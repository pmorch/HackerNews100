<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 16 Jul 2024 23:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[DevRel at HuggingFace (132 pts)]]></title>
            <link>https://dx.tips/huggingface</link>
            <guid>40979221</guid>
            <pubDate>Tue, 16 Jul 2024 18:56:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dx.tips/huggingface">https://dx.tips/huggingface</a>, See on <a href="https://news.ycombinator.com/item?id=40979221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><blockquote>
<p>Following the <a target="_blank" href="https://dx.tips/zirp">ZIRP DevRel</a> post, the community has had many great discussions on where devrel needs to go next. DXTips exists to share this tacit niche industry knowledge. <strong>DX@X</strong> is our new async interview series we are starting with DevRel leaders to get more perspectives on the state of the art in DX and DevRel. We're excited to kick it off with <strong><a target="_blank" href="https://x.com/osanseviero">Omar Sanseviero</a>, Chief Llama Officer at HuggingFace</strong>!</p>
<p>HuggingFace is well known for being incredible stewards of the open source ML community, building critical infrastructure at hypergrowth (growing from 780k to 2.3m repos on the HF Hub in the past year) and doing so <a target="_blank" href="https://analyticsindiamag.com/ai-news-updates/hugging-face-announces-profitability-with-free-and-open-source-models/">profitably</a>. They are also a rare startup whose large online community also translates to <a target="_blank" href="https://x.com/search?q=huggingface%20woodstock&amp;src=recent_search_click&amp;f=top"><em>massive</em> multi-thousand people meetups</a> all <a target="_blank" href="https://x.com/search?q=huggingface%20station%20f&amp;src=typed_query&amp;f=top">over the world</a>.</p>
<p><strong>Request for Suggestions: Who else would you like to hear from? Let us know on <a target="_blank" href="https://x.com/dxtipshq">X/Twitter</a> and join our <a target="_blank" href="https://dx.tips/newsletter">Newsletter</a> to get the next issue!</strong></p>
</blockquote>
<h2 id="heading-introduction-to-omar-and-huggingface">Introduction to Omar and HuggingFace</h2>
<blockquote>
<p><strong>Intro:</strong> <em>Hey Omar! Let’s assume people know the surface level of HuggingFace - it’s the largest AI community which collaborates on open source models, datasets, and applications, with paid compute and enterprise solutions. What does a Chief Llama Officer do at HF?</em></p>
</blockquote>
<p><a target="_blank" href="https://x.com/osanseviero/media">Memes</a>! More seriously, my title might translate to “<strong>Head of Platform and Community</strong>” at another company, although the scope of what I do is quite broad. There are two aspects to my role:</p>
<ul>
<li><strong>Leadership</strong>: Within HF, my role involves horizontal and vertical leadership. Vertically, I direct a family of teams (Dev Advocacy Engineering, On-device ML, Moonshot Factory, Argilla - our most recent acquisition). Horizontally, our team sits at the intersection of Open Source, Product, and the external community (+ sometimes research). In my day-to-day, I aim to identify high-impact potential areas, connect dots across teams at HF and the community, and unblock people to succeed.</li>
<li><strong>IC</strong>: HF has a very bottom-up leadership culture. This, combined with a meeting-less async culture (<a target="_blank" href="https://x.com/mervenoyann/status/1692111147783143751">example</a>, <a target="_blank" href="https://x.com/SashaMTL/status/1773344913502929014">example</a>, <a target="_blank" href="https://x.com/osanseviero/status/1573055162070999061">example</a>, <a target="_blank" href="https://www.hbs.edu/faculty/Pages/item.aspx?num=63185">old HBS case study</a>), allows folks in leadership positions to dedicate significant time to technical and meaningful contributions to different projects. A significant part of my role involves collaborating with partners to release new models, such as the latest Llama and Gemma models. Each release is unique, intense, and fun, and I quite enjoy being deeply involved in the entire process.</li>
</ul>
<blockquote>
<p><strong>Followup</strong>: <em>What do you think is under-appreciated about HF’s open source work?</em></p>
</blockquote>
<p>Hugging Face is a community-centric company. It's hard to exaggerate how community-centric we are. Some examples:</p>
<ul>
<li>We prioritize giving the spotlight to community members and collaborators as much as possible.</li>
<li>Provide compute and no-strings-attached cash grants (including but not limited to <a target="_blank" href="https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai">the $10m ZeroGPU program</a>) to community members/communities (for example, in the past, Eleuther, Boris from Dall-e Mini, and lucidrains have been sponsored by HF, allowing them to keep doing their cool work without financial constraints).</li>
<li>Help maintain open-source libraries (eg <a target="_blank" href="https://github.com/UKPLab/sentence-transformers">sentence transformers</a> and <a target="_blank" href="https://github.com/bitsandbytes-foundation/bitsandbytes">bitsandbytes</a>) from other groups and closely collaborate with other tools (eg <a target="_blank" href="https://github.com/EleutherAI/lm-evaluation-harness/tree/main">LM Eval Harness</a> and <a target="_blank" href="https://github.com/mlfoundations/open_clip">OpenCLIP</a>)</li>
</ul>
<p>Our approach to working with other groups and open-source platforms and libraries is always collaborative. We view ourselves as "the Switzerland" of the ML community, actively contributing to and supporting the ML ecosystem. We want the community to be successful and grow the pie.</p>
<p>So, one aspect of HF that I think is underappreciated is the extent of the support and collaboration with the community. Many see the outputs—like models and libraries—but might not realize the significant behind-the-scenes effort that the team puts into fostering the thriving ecosystem.</p>
<h2 id="heading-devrel-at-huggingface-metrics-and-velocity">DevRel at HuggingFace: Metrics and Velocity</h2>
<blockquote>
<p><strong>Credibility/Success:</strong> <em>What are some “real” metrics that you track that point to HF’s devrel success?</em></p>
</blockquote>
<p>DevRel comes in all kinds of flavors in the industry. Some DevRel teams are part of a marketing function, and some are within a monetization-team function. At Hugging Face, DevRel sits between the open-source and product organizations and is primarily an engineering function.</p>
<p>This means <strong>HF DevRel's goal is to see increased usage of the Hub platform and open-source tools</strong> rather than focusing on revenue as our primary goal. Two of our north stars are the <strong>number of repositories on HF and logged-in usage of the Hub</strong> platform. For example, the Hub has 2.3 million repositories today, compared to 780k repositories a year ago. (Of course, we can look at everything with more granularity, e.g., the number of Spaces, which grew from 187k in June last year to 650k this year).</p>
<p><img loading="lazy" src="https://gist.github.com/user-attachments/assets/ca97fe47-127d-4bc6-9d05-b4f2c0fdf863" alt="image"></p>
<p>Each team member works on different topics (e.g., Computer Vision, Audio ML, ML for 3D CV, etc.), so we jointly define some metrics that we would like to see move based on our efforts. <strong>We prioritize usage-based</strong> (number of repos, downloads, installs) <strong>over visibility-driven</strong> (GitHub stars, Twitter likes, views), which are also valuable but not the main motivation of our work.</p>
<p>That said, I'm skeptical of cultures that overemphasize metrics (of course, this is nuanced and depends on a lot of context). From my experience at Google and looking at other startups, I've seen the downsides of measuring too much too early. Metrics are an imperfect proxy for impact and are game-able. <strong>Cultures prioritizing metrics above all risk losing sight of user needs and making wrong decisions</strong> (e.g., to improve metrics for their performance review rather than genuine user benefit). Some DevRel activities might not have immediate metric changes but have long-term impact.</p>
<p>One of the most rewarding moments was after two years of building connections with Spanish-speaking folks, we <a target="_blank" href="https://platzi.com/blog/ayuda-a-mejorar-los-llm-en-espanol-en-7-sencillos-pasos/">initiated</a> an exciting Alpaca translation effort involving Argilla, Platzi (a Colombian edtech), and many community super-users. This 'Avengers assemble' moment is becoming more frequent as we foster stronger relationships with practitioners, communities, and organizations. Examples of these are <a target="_blank" href="https://x.com/_lewtun/status/1778429536264188214">Zephyr ORPO</a> (KAIST + HF + Argilla), <a target="_blank" href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">QLoRA</a> (University of Washington), and the very recent <a target="_blank" href="https://huggingface.co/blog/winning-aimo-progress-prize">AI Math Olympiad winner</a> (NuminaM + HF).</p>
<blockquote>
<p><strong>DevRel @ HF:</strong> <em>What was your reaction to <a target="_blank" href="https://dx.tips/zirp">the ZIRP DevRel article</a>? What’s different at HF?</em></p>
</blockquote>
<p>As mentioned before, DevRel comes in all kinds of flavors. There were things that I could relate to. Specially two points:</p>
<ul>
<li>I'm a bit skeptical of the impact of traveling to conferences. While conferences can be impactful if approached strategically, the highest impact usually doesn't come from giving a talk. Instead, it's often the connections made and behind-the-scenes collaborations which require a different mindset. We support conference travel (and people do that a lot), but we encourage team members to attend with an impact-driven mindset, ready to achieve some concrete things beyond attending an event.</li>
<li>Lack of OKRs. <strong>We do not have OKRs</strong>. The ML ecosystem moves incredibly fast, so we need to be nimble and action-driven. Gemini Nano was added to Chrome? Let's figure out how to run it and <a target="_blank" href="https://x.com/xenovacom/status/1810356703826977183">release some docs</a>. Model 504B is coming out next month; let's make sure it's usable by the community. Although exciting, this comes with cons: priorities can and will change, planning becomes challenging, and maintaining focus can be difficult in the chaos of the current ML space.</li>
</ul>
<p>That said, I think HF DevRel has been successful overall for a couple of reasons:</p>
<ul>
<li>It's an <strong>engineering-centric</strong> function. Day-to-day activities might involve fine-tuning models to get a training script right, collaborating on a research project, or finding out why a model is not quantizing well to 4 bits. Our users are engineers and researchers, so it's essential that we are in their world to understand them.</li>
<li>It's a <strong>decentralized</strong> function. Although we have a dedicated DevRel team, <strong>everyone at HF is expected to do activities usually associated with DevRel</strong>. Although we have a DevRel team, everyone at HF, from research to success engineers, is involved in doing DevRel-like activities themselves, so you'll see everyone engaging in social media, creating content (youtube, blog posts, etc), giving presentations, etc. If you build a feature/tool, you're responsible of its visibility and growth (of course, with support/guidance from others). Marketing your own work could involve writing a blog post or a technical deep dive, crafting some beautiful notebooks, and yes, sometimes making memes. If you visit <a target="_blank" href="https://huggingface.co/blog">HF blog</a>, you'll see content from all across the company. Rather than "outsourcing" these responsibilities to a third team (either a marketing or a DevRel team in many companies), HF members are encouraged and expected to own their work, end to end. <a target="_blank" href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb</a> is an amazing example of how this can be successful.</li>
</ul>
<p>The two points combined lead to very genuine and scalable relationships. Rather than a competitive culture, we've fostered a culture in which people are excited to collaborate both internally and externally and ready to amplify the amazing work being done by the community.</p>
<blockquote>
<p><strong>Followup question on DevRel Velocity:</strong> <em>I notice that OKRs very rarely prioritize moving fast. What has worked/not worked in encouraging your team members to move fast (other than the obvious intrinsic motivation)?</em></p>
</blockquote>
<ul>
<li><strong>What works well</strong>: Beyond intrinsic motivation, which is indeed a strong factor, collective momentum plays a big role. Being surrounded by a group of smart, driven individuals working on the latest ML advancements creates an environment where progress is both expected and contagious. This collaborative atmosphere builds some sense of urgency and encourages everyone to push forward together.</li>
<li><strong>What does not work well</strong>: On the flip side, a lack of structured planning and clear OKRs can affect some people. While flexibility is desired a lot in the industry, it can lead to ambiguity and confusion about expectations, making it harder for new team members to get up to speed quickly. This can result in onboarding challenges and potential mismatches in cultural fit. Each team is a bit different, but there's a balance between agility and more structured goal-setting that can help everyone thrive.</li>
</ul>

<blockquote>
<p><strong>Open Source Engineering and Community:</strong> <em>HF maintains a -lot- of open source work, and only (~200?) employees. How do you organize the different projects you work on, and how does the community engagement work?</em></p>
</blockquote>
<p>Yes, we're a relatively small team (215 persons), and maintain a large number of libraries ourselves: demos (Gradio), data (datasets, Argilla, distilabel), modeling (transformers, diffusers, timm, peft, Candle, tokenizers, accelerate, parler TTS, transformers.js), production (TGI and TEI), and research related (lerobot, alignment handbook), plus support community libraries (bitsandbytes and sentence-transformers and others as mentioned above).</p>
<p>There are some key strategies that have worked well for us</p>
<ul>
<li><strong>Strong async culture</strong>. We mostly communicate through Slack and GitHub, enabling collaboration across different projects. This fosters transparency, allowing everyone to gain visibility into other projects.</li>
<li><strong>Flexible organizational and role boundaries.</strong> The organizational structure is flexible, allowing people from different teams to contribute where needed. For example, when we were preparing for Llama 2 release, people from all kinds of teams contributed to make sure the model was in good shape and usable by the community. It's quite powerful to see different teams working organically to make things happen without having to go through bureaucracy or process management.</li>
<li>(other points mentioned above, such as being collaboration and community centric)</li>
<li><strong>Pragmatic</strong>. Let me dive into this one more in the next point :)</li>
</ul>
<blockquote>
<p><strong>Prioritization:</strong> <em>There’s a lot of interesting directions in ML and only so much time/resources. How do you decide -what- to invest in? And what to cut? Because you're decentralized - what do managers decide vs leave to ICs?</em></p>
</blockquote>
<p>That's a great question and likely one of our biggest challenges. As you said, there are many interesting directions, and the ecosystem is changing quickly. We see new players, from new libraries and startups to new organizations releasing models.</p>
<p>In general, I like to apply the concept of exploration/exploitation from Reinforcement Learning. This involves two main stages:</p>
<ol>
<li><strong>Exploration</strong>: We do small projects or comms to gauge their potential impact and community interest. This allows us to experiment without having too many people working on it or committing lots of time.</li>
<li><strong>Exploitation</strong>: Based on the knowledge gained from the exploration stage, we focus our efforts on things we are more confident will have a significant impact. This involves scaling up successful projects and allocating more resources to areas with proven value.</li>
</ol>
<p>Of course, it's never as simple as that (the ϵ is variable), and it's usually cyclical (exploration -&gt; exploitation -&gt; exploration), but it's a good mental framework to have. Some projects are heavy in exploration by nature (for example, exploring a very niche domain or community), and others might require a larger time investment (which tends to happen in research-oriented projects).</p>
<p>The second point, related to the above, is pragmatism. That means being willing to pause or stop projects if they aren't having the expected impact. For example, investing days to make a YouTube video with a few hundred views may not be a worthwhile investment unless it leads to some very valuable or targeted outcome. It can be sad to spend some weeks building an open-source library and then see no engagement or adoption. What is worse, however, is to keep pushing and pushing for a tool that might lack product-market fit.</p>
<p>Failure is a part of the process for all of us. The key is to learn from it, understand what went well and what didn't, and know when to pivot or stop. This pragmatic approach helps us stay focused on what truly matters.</p>
<blockquote>
<p><strong>Followup Question:</strong> <em>Let’s apply Explore-Exploit. Just to pick on a specific, visible example that has caught my (swyx’s) eye recently, <a target="_blank" href="https://x.com/reach_vb/">VB (Vaibhav)</a> has staked out a very notable position as “the audio guy” on AI Twitter. Always the first to have great takes on anything in audio, shipping insanely-fast-whisper and the TTS Arena, and goodness knows what else I don’t even know about. He of course also does <a target="_blank" href="https://x.com/reach_vb/status/1806343018640781675">other open source AI work eg on LLMs</a>. Was there a top down decision to focus on Audio? It must have been… But I’m also equally sure that audio doesn’t drive nearly as much revenue for HF as, say, LLMs or Diffusion models (Apolinario). So… great hire, but how did you decide to invest in audio in the first place? Is there any calculation driven by the GTM/Product/Sales side of HF?</em></p>
</blockquote>
<p>It might sound surprising, but audio (with VB) was a very validated area we wanted to invest in, while diffusion/art (<a target="_blank" href="https://x.com/multimodalart">Poli</a>) was a very experimental area.</p>
<p><img loading="lazy" src="https://gist.github.com/user-attachments/assets/56f995f4-8f48-467f-9463-2d4582e1f730" alt="image"></p>
<p><strong>For audio</strong>, back in 2022, we saw a significant wave of OS libraries (SpeechBrain, ESPNet, Asteroid, etc) and interesting research (Whisper, XLS-R by Meta, etc). We were actively organizing community sprints with free GPUs to help people fine-tune speech recognition models in their languages. There was a lot going on that led to the decision to hire a DA for the role (apart from the MLE in the open source team already working on the topic). VB was working in audio in his masters and had already engaged with us through different efforts. Despite being somewhat junior in the open ML space, his <strong>very</strong> strong alignment with the open ML culture and mindset allowed him to scale his impact. Since joining, VB has expanded beyond audio, leading different collaborations and integrations, including recent work with Georgi on llama.cpp. Now, VB is even <a target="_blank" href="https://x.com/reach_vb/status/1810977320275943852">hiring an intern</a> to support the ML ecosystem for audio!</p>
<p><img loading="lazy" src="https://gist.github.com/user-attachments/assets/030763e7-ce88-4cab-9371-4120525f1461" alt="image"></p>
<p><strong>For diffusion/art</strong>, Poli was our first Moonshot MLE hired to make "ML for art as accessible and open source as possible." This was before the hype around Stable Diffusion. We hired him because of his strong cultural alignment, his contributions to early HF Spaces and him being a Gradio super-user. At that time, while more experimental, the impact on Spaces and the potential of diffusion models (like latent diffusion by CompVis) showed promising signs. As a power (and somewhat early) user for Spaces, he also brought lots of product ideas on making Spaces more successful.</p>
<p>In summary, our decision to invest in audio was based on clear community and research validation as well as growth potential. In contrast, MLxArt was a more experimental exploration that showed early impacts and ended up being a very high impact area.</p>
<p>Sometimes both intercept! Talking about AI x music <a target="_blank" href="https://x.com/iamwill/status/1696546638863749154">with will.i.am</a> is definitely a highlight of last year.</p>
<p><img loading="lazy" src="https://gist.github.com/user-attachments/assets/655f586e-61f1-471c-a4d2-20c44ec621ec" alt="image"></p>

<blockquote>
<p><strong>Open Questions:</strong> <em>What are you looking for help with? What questions do you want answered that would help you get to your “next level” (whatever that means to you)?</em></p>
</blockquote>
<p>Hugging Face's core audience has traditionally been people with ML experience, but we've seen more and more <strong>developers without an ML background who want to incorporate ML into their projects or learn about ML</strong>. These developers often feel overwhelmed by the complexity of ML and the speed of the ecosystem. While the community has introduced new tools and APIs to simplify things, and we have exciting features coming soon, there's still much to be done to lower the entry barriers further. I'm looking for <strong>insights and suggestions on how we can make our tools even more accessible to non-ML developers</strong>. (<em>Editor: some might call these <a target="_blank" href="https://www.latent.space/p/ai-engineer">AI Engineers</a>?</em>)</p>
<p>Additionally, <strong>we're expanding our team and are looking for individuals with strong developer empathy and technical skills based in the Bay Area</strong>. If you're interested or know someone who might be, <a target="_blank" href="https://x.com/osanseviero">please reach out</a>!</p>
<blockquote>
<p><strong>Request for Startups/Tools:</strong> <em>You <a target="_blank" href="https://argilla.io/blog/argilla-joins-hugggingface/">recently acquired Argilla</a> for collaborating on high quality datasets — what else do you wish people worked on? (that would be useful to the ecosystem from your POV)</em></p>
</blockquote>
<p>Some topics I'm interested in (not necessarily for a startup):</p>
<p>In <strong>Research</strong>: </p>
<ul>
<li>more distillation experiments and OS tooling</li>
<li>densification of sparse (MoE) models</li>
<li>quantization (sub-1-bit for MoEs, &lt;8-bit fine-tuning), tooling on speculative decoding strategies, more people trying the KTO alignment algorithm (which removes the need of preference data for RLHF/PPO/DPO)</li>
<li>true multimodality (2+ input modalities and 2+ output modalities, e.g., text+image+video to text+image in a unified model)</li>
</ul>
<p>There are trends in all of this already.</p>
<p>More generally: We want <strong>more developer-friendly ML tooling</strong> (i.e. making it super easy for any developer to use ML, not just LLMs).  If you come from a background in which you can speak both the language of a discipline (biochemistry, chemistry, material sciences, health) and ML, and communicate and work well with both audiences, you're a unicorn and can do very impactful things, not just in the ML domain, but in other industries.</p>
<blockquote>
<p><em>Thank you for your time, Omar!</em></p>
</blockquote>
<hr>
<p><strong>CTA from DXTips: Who else would you like to hear from? Let us know on <a target="_blank" href="https://x.com/dxtipshq">X/Twitter</a> and join our <a target="_blank" href="https://dx.tips/newsletter">Newsletter</a> to get the next issue!</strong></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I am starting an AI+Education company (391 pts)]]></title>
            <link>https://twitter.com/karpathy/status/1813263734707790301</link>
            <guid>40978731</guid>
            <pubDate>Tue, 16 Jul 2024 17:57:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/karpathy/status/1813263734707790301">https://twitter.com/karpathy/status/1813263734707790301</a>, See on <a href="https://news.ycombinator.com/item?id=40978731">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[XLSTMTime: Long-Term Time Series Forecasting with xLSTM (127 pts)]]></title>
            <link>https://arxiv.org/abs/2407.10240</link>
            <guid>40978372</guid>
            <pubDate>Tue, 16 Jul 2024 17:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2407.10240">https://arxiv.org/abs/2407.10240</a>, See on <a href="https://news.ycombinator.com/item?id=40978372">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2407.10240">View PDF</a></p><blockquote>
            <span>Abstract:</span>In recent years, transformer-based models have gained prominence in multivariate long-term time series forecasting (LTSF), demonstrating significant advancements despite facing challenges such as high computational demands, difficulty in capturing temporal dynamics, and managing long-term dependencies. The emergence of LTSF-Linear, with its straightforward linear architecture, has notably outperformed transformer-based counterparts, prompting a reevaluation of the transformer's utility in time series forecasting. In response, this paper presents an adaptation of a recent architecture termed extended LSTM (xLSTM) for LTSF. xLSTM incorporates exponential gating and a revised memory structure with higher capacity that has good potential for LTSF. Our adopted architecture for LTSF termed as xLSTMTime surpasses current approaches. We compare xLSTMTime's performance against various state-of-the-art models across multiple real-world da-tasets, demonstrating superior forecasting capabilities. Our findings suggest that refined recurrent architectures can offer competitive alternatives to transformer-based models in LTSF tasks, po-tentially redefining the landscape of time series forecasting.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Musleh Alharthi [<a href="https://arxiv.org/show-email/df681557/2407.10240">view email</a>]      <br>    <strong>[v1]</strong>
        Sun, 14 Jul 2024 15:15:00 UTC (848 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Private Browsing 2.0 (156 pts)]]></title>
            <link>https://webkit.org/blog/15697/private-browsing-2-0/</link>
            <guid>40977945</guid>
            <pubDate>Tue, 16 Jul 2024 16:23:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webkit.org/blog/15697/private-browsing-2-0/">https://webkit.org/blog/15697/private-browsing-2-0/</a>, See on <a href="https://news.ycombinator.com/item?id=40977945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                
                <p>When <a href="https://en.wikipedia.org/wiki/Private_browsing#History">we invented</a> Private Browsing back in 2005, our aim was to provide users with an easy way to keep their browsing private from anyone who shared the same device. We created a mode where users do not leave any local, persistent traces of their browsing. Eventually all other browsers shipped the same feature. At times, this is called “ephemeral browsing.”</p>
<p>We baked in cross-site tracking prevention in all Safari browsing through our cookie policy, starting with Safari 1.0 in 2003. And we’ve increased privacy protections incrementally over the last 20 years. (Learn more by reading <a href="https://webkit.org/tracking-prevention/">Tracking Prevention in Webkit</a>.) Other popular browsers have not been as quick to follow our lead in tracking prevention but there is progress.</p>
<p>Apple believes that users should not be tracked across the web without their knowledge or their consent. Entering Private Browsing is a strong signal that the user wants the best possible protection against privacy invasions, while still being able to enjoy and utilize the web. Staying with the 2005 definition of private mode as only being ephemeral, such as <a href="https://support.google.com/chrome/answer/9845881?hl=en#zippy=%2Chow-incognito-mode-works%2Chow-incognito-mode-protects-your-privacy">Chrome’s Incognito Mode</a>, simply doesn’t cut it anymore. Users expect and deserve more.</p>
<p>So, we decided to take Private Browsing further and add even more protection beyond the normal Safari experience. Last September, we added a whole new level of privacy protections to Private Browsing in Safari 17.0. And we enhanced it even further in Safari 17.2 and Safari 17.5. Plus, when a user enables them, all of the new safeguards are available in regular Safari browsing too.</p>
<p>With this work we’ve enhanced web privacy immensely and hope to set a new industry standard for what Private Browsing should be.</p>
<h2>Enhanced Private Browsing in a Nutshell</h2>
<p>These are the protections and defenses added to Private Browsing in Safari 17.0:</p>
<ul>
<li>Link Tracking Protection</li>
<li>Blocking network loads of known trackers, including CNAME-cloaked known trackers</li>
<li>Advanced Fingerprinting Protection</li>
<li>Extensions with website or history access are off by default</li>
</ul>
<p>In addition, we added these protections and defenses in all browsing modes:</p>
<ul>
<li>Capped lifetime of cookies set in responses from cloaked third-party IP addresses</li>
<li>Partitioned SessionStorage</li>
<li>Partitioned blob URLs (starting in Safari 17.2)</li>
</ul>
<p>We also expanded Web AdAttributionKit (formerly Private Click Measurement) as a replacement for tracking parameters in URL to help developers understand the performance of their marketing campaigns even under Private Browsing.</p>
<figure><picture><source srcset="https://www.webkit.org/wp-content/uploads/safari-private-browsing-dark.png" media="(prefers-color-scheme: dark)"><img fetchpriority="high" decoding="async" src="https://www.webkit.org/wp-content/uploads/safari-private-browsing-light.png" alt="Screenshot of Private Browsing in Safari" width="2704" height="1628" srcset="https://webkit.org/wp-content/uploads/safari-private-browsing-light.png 2704w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-300x181.png 300w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-1024x617.png 1024w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-768x462.png 768w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-1536x925.png 1536w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-2048x1233.png 2048w" sizes="(max-width: 2704px) 100vw, 2704px"></picture><figcaption>Private Browsing in Safari</figcaption></figure>
<p>However, before we dive into these new and enhanced privacy protections, let’s first consider an important aspect of these changes: website compatibility risk.</p>
<h2>The Risk of Breaking Websites and How We Mitigate It</h2>
<p>There are many ideas for how to protect privacy on the web, but unfortunately many of them may break the user’s experience. Like security protections in real life, a balance must be struck. The new Private Browsing goes right up to the line, attempting to never break websites. But of course there is a risk that some parts of some sites won’t work. To solve this, we give users affordances to reduce privacy protections on a per-site basis. Such a change in privacy protections is only remembered while browsing within a site. This option is a last resort when a web page is not usable due to the privacy protections.</p>
<figure><picture><source srcset="https://www.webkit.org/wp-content/uploads/webkit-tracking-prevention-dark.png" media="(prefers-color-scheme: dark)"><img decoding="async" src="https://www.webkit.org/wp-content/uploads/webkit-tracking-prevention-light.png" alt="Reload menu with Reload Reducing Privacy Protections selected" width="2564" height="1544" srcset="https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light.png 2564w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-300x181.png 300w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-1024x617.png 1024w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-768x462.png 768w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-1536x925.png 1536w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-2048x1233.png 2048w" sizes="(max-width: 2564px) 100vw, 2564px"></picture><figcaption>Reload Reducing Privacy Protections</figcaption></figure>
<p>All of the new privacy protections in Private Browsing are also available in regular browsing. On iOS, iPadOS and visionOS go to Settings &gt; Apps &gt; Safari &gt; Advanced &gt; Advanced Tracking and Fingerprinting Protection and enable “All Browsing”. On macOS go to Safari &gt; Settings &gt; Advanced and enable  “Use advanced tracking and fingerprinting protection”:</p>
<figure><picture><source srcset="https://www.webkit.org/wp-content/uploads/safari-advanced-tracking-protection-dark.png" media="(prefers-color-scheme: dark)"><img decoding="async" width="1700" height="1155" src="https://www.webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light.png" alt="Safari Advanced Settings with &quot;Use advanced tracking and fingerprinting protection in all browsing&quot; selected" srcset="https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light.png 1700w, https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light-300x204.png 300w, https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light-1024x696.png 1024w, https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light-768x522.png 768w, https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light-1536x1044.png 1536w" sizes="(max-width: 1700px) 100vw, 1700px"></picture><figcaption>Use advanced tracking and fingerprinting protection in all browsing from Safari Advanced Settings</figcaption></figure>
<p>Let’s now walk through how these enhancements work.</p>
<h2>Link Tracking Protection</h2>
<p>Safari’s Private Browsing implements two new protections against tracking information in the destination URL when the user navigates between different websites. The specific parts of the URL covered are query parameters and the fragment. The goal of these protections is to make it more difficult for third-party scripts running on the destination site to correlate user activity across websites by reading the URL.</p>
<p>Let’s consider an example where the user clicks a link on <code>clickSource.example</code>, which takes them to <code>clickDestination.example.</code> The URL looks like this:</p>
<pre><code>https://clickDestination.example/article?known_tracking_param=123&amp;campaign=abc&amp;click_val=456
</code></pre>
<p>Safari removes a subset of query parameters that have been identified as being used for pervasive cross-site tracking granular to users or clicks. This is done <em>prior to</em> navigation, such that these values are never propagated over the network. If <code>known_tracking_param</code> above represents such a query parameter, the URL that’s used for navigation will be:</p>
<pre><code>https://clickDestination.example/article?campaign=abc&amp;click_val=456
</code></pre>
<p>As its name suggests, the <code>campaign</code> above represents a parameter that’s only used for campaign attribution, as opposed to click or user-level tracking. Safari allows such parameters to pass through.</p>
<p>Finally, on the destination site after a cross-site navigation, all third-party scripts that attempt to read the full URL (e.g. using <code>location.search</code>, <code>location.href</code>, or <code>document.URL</code>) will get a version of the URL that has no query parameters or fragment. In our example, this script-exposed value is simply:</p>
<pre><code>https://clickDestination.example/article
</code></pre>
<p>In a similar vein, Safari also hides cross-site any <code>document.referrer</code> from script access in Private Browsing.</p>
<h2>Web AdAttributionKit in Private Browsing</h2>
<p>Web AdAttributionKit (formerly Private Click Measurement) is a way for advertisers, websites, and apps to implement ad attribution and click measurement in a privacy-preserving way. You can <a href="https://webkit.org/blog/11529/introducing-private-click-measurement-pcm/">read more about it here</a>. Alongside the new suite of enhanced privacy protections in Private Browsing, Safari also brings a version of Web AdAttributionKit to Private Browsing. This allows click measurement and attribution to continue working in a privacy-preserving manner.</p>
<p>Web AdAttributionKit in Private Browsing works the same way as it does in normal browsing, but with some limits:</p>
<ul>
<li>Attribution is scoped to individual Private Browsing tabs, and transfers attribution across new tabs opened when clicking on links. However, attribution is not preserved through other indirect means of navigation: for instance, copying a link and pasting in a new tab. In effect, this behaves similarly to how Web AdAttributionKit works for <a href="https://webkit.org/blog/12042/pcm-for-in-app-direct-response-advertising/">Direct Response Advertising</a>.</li>
<li>Since Private Browsing doesn’t persist any data, pending attribution requests are discarded when the tab is closed.</li>
</ul>
<h2>Blocking Network Loads of Known Trackers</h2>
<p>Safari 17.0 also comes with an automatically enabled content blocker in Private Browsing, which blocks network loads to known trackers. While Intelligent Tracking Prevention has long blocked all third party cookies, blocking trackers’ network requests from leaving the user’s device in the first place ensures that no personal information or tracking parameters are exfiltrated through the URL itself.</p>
<p>This automatically enabled content blocker is compiled using data from DuckDuckGo and from the EasyPrivacy filtering rules from EasyList. The requests flagged by this content blocker are only entries that are flagged as trackers by <em>both</em> DuckDuckGo and EasyPrivacy. In doing so, Safari intentionally allows most ads to continue loading even in Private Browsing.</p>
<p>Private Browsing also blocks cloaked network requests to known tracking domains. They otherwise have the ability to save third party cookies in a first-party context. This protection requires macOS Sonoma or iOS 17. By cloaked we mean subdomains mapped to a third-party server via CNAME cloaking or third-party IP address cloaking. See also the “Defending Against Cloaked First Party IP Addresses” section below.</p>
<p>When Safari blocks a network request to a known tracker, a console message of this form is logged, and can be viewed using Web Inspector:</p>
<pre><code>`<span>Blocked</span> <span>connection</span> <span>to</span> <span>known</span> <span>tracker</span><span>:</span> <span>tracker</span>.<span>example</span>` 
</code></pre>
<h2>Network Privacy Enhancements</h2>
<p>Safari 15.0 started hiding IP addresses from known trackers by default. Private Browsing in Safari 17.0 adds the following protections for all users:</p>
<ul>
<li><strong>Encrypted DNS</strong>. DNS queries are used to resolve server hostnames into IP addresses, which is a necessary function of accessing the internet. However, DNS is traditionally unencrypted, and allows network operators to track user activity or redirect users to other servers. Private Browsing uses <a href="https://en.wikipedia.org/wiki/DNS_over_HTTPS#Oblivious_DNS_over_HTTPS">Oblivious DNS over HTTPS</a> by default, which encrypts and proxies DNS queries to protect the privacy and integrity of these lookups.</li>
<li><strong>Proxying unencrypted HTTP</strong>. Any unencrypted HTTP resources loaded in Private Browsing will use the same multi-hop proxy network used to hide IP addresses from trackers. This ensures that attackers in the local network cannot see or modify the content of Private Browsing traffic.</li>
</ul>
<p>Additionally, for iCloud+ subscribers who have iCloud Private Relay turned on, Private Browsing takes privacy to the next level with these enhancements:</p>
<ul>
<li><strong>Separate sessions per tab</strong>. Every tab that the user opens in Private Browsing now uses a separate session to the iCloud Private Relay proxies. This means that web servers won’t be able to tell if two tabs originated on the same device. Each session is assigned egress IP addresses independently. Note that this doesn’t apply to parent-child windows that need a programmatic relationship, such as popups and their openers.</li>
<li><strong>Geolocation privacy by default</strong>. Private Browsing uses an IP location based on your country and time zone, not a more specific location.</li>
<li><strong>Warnings before revealing IP address</strong>. When accessing a server that is not accessible on the public internet, such as a local network server or an internal corporate server, Safari cannot use iCloud Private Relay. In Private Browsing, Safari now displays a warning requesting that the user consents to revealing their IP address to the server before loading the page.</li>
</ul>
<h2>Extensions in Private Browsing</h2>
<p>Safari 17.0 also boosts the privacy of Extensions in Private Browsing. Extensions that can access website data and browsing history are now off by default in Private Browsing. Users can still choose to allow an extension to run in Private Browsing and gain all of the extension’s utility. Extensions that don’t access webpage contents or browsing history, like Content Blockers, are turned on by default in Private Browsing when turned on in Safari.</p>
<h2>Advanced Fingerprinting Protection</h2>
<p>With Safari and subsequently <a href="https://wiki.mozilla.org/Security/Anti_tracking_policy">other</a> <a href="https://www.chromium.org/Home/chromium-privacy/privacy-sandbox/#turning-down-third-party-cookies">browsers</a> restricting stateful tracking (e.g. cross-site cookies), many trackers have turned to stateless tracking, often referred to as <em>fingerprinting</em>.</p>
<h3>Types of Fingerprinting</h3>
<p>We distinguish these types of fingerprinting:</p>
<ul>
<li><strong>Device fingerprinting</strong>. This is about building a fingerprint based on device characteristics, including hardware and the current operating system and browser. It can also include connected peripherals if they are allowed to be detected. Such a fingerprint cannot be changed by the user through settings or web extensions.</li>
<li><strong>Network and geographic position fingerprinting</strong>. This is about building a fingerprint based on how the device connects to the Internet and any means of detecting its geographic position. It could be done by measuring roundtrip speeds of network requests or simply using the IP address as an identifier.</li>
<li><strong>User settings fingerprinting</strong>. This is about reading the state of user settings such as dark/light mode, locale, font size adjustments, and window size on platforms where the user can change it. It also includes detecting web extensions and accessibility tools. We find this kind of fingerprinting to be extra hurtful since it exploits how users customize their web experience to fit their needs.</li>
<li><strong>User behavior fingerprinting</strong>. This is about detecting recurring patterns in how the user behaves. It could be how the mouse pointer is used, how quickly they type in form fields, or how they scroll.</li>
<li><strong>User traits fingerprinting</strong>. This is about figuring out things about the user, such as their interests, age, health status, financial status, and educational background. Those gleaned traits can contribute to a unique ID but also can be used directly to target them with certain content, adjust prices, or tailor messages.</li>
</ul>
<h3>Fingerprint Stability</h3>
<p>A challenge for any tracker trying to create a fingerprint is how stable the fingerprint will be over time. Software version fingerprinting changes with software updates, web extension fingerprinting changes with extension updates and enablement/disablement, user settings change when the user wants, multiple users of the same device means behavior fingerprints change, and roaming devices may change network and geographic position a lot.</p>
<h3>Fingerprinting Privacy Problem 1: Cross-Site Tracking</h3>
<p>Fingerprints can be used to track the user across websites. If successful, it defeats tracking preventions such as storage partitioning and link decoration filtering.</p>
<p>There are two types of solutions to this problem:</p>
<ol>
<li>Make the fingerprint be shared among many users, so called herd immunity.</li>
<li>Make the fingerprint unique per website, typically achieved via randomized noise injection.</li>
</ol>
<h3>Fingerprinting Privacy Problem 2: Per-Site User Recall</h3>
<p>Less talked about is the fingerprinting problem of per-site user recall. Web browsers offer at least two ways for the user to reset their relationship with a website: Clear website data or use Private Browsing. Both make a subsequent navigation to a website start out fresh.</p>
<p>But fingerprinting defeats this and allows a website to remember the user even though they’ve opted to clear website data or use Private Browsing.</p>
<p>There are two types of solutions to this problem:</p>
<ol>
<li>Make the fingerprint be shared among many users, so called herd immunity.</li>
<li>Make the fingerprint unique per website, and generate a new unique fingerprint for every fresh start.</li>
</ol>
<h3>Fingerprinting Privacy Problem 3: Per-Site Visitor Uniqueness</h3>
<p>The ultimate anti fingerprinting challenge in our view is to address a specific user’s uniqueness when visiting a specific website. Here’s a simple example:</p>
<p>Having the locale setting to US/EN for American English may provide ample herd immunity in many cases. But what happens when a user with that setting visits an Icelandic government website or a Korean reading club website? They may find themselves in a very small “herd” on that particular website and combined with just a few more fingerprinting touch points they can be uniquely identified.</p>
<p>Addressing per-site visitor uniqueness is not possible in general by a browser unless it knows what the spread of visitors looks like for individual websites.</p>
<h3>Fingerprinting Protections at a High Level</h3>
<p>We view cross-site tracking and per-site user recall as privacy problems to be addressed by browsers.</p>
<p><strong>Our approach</strong>:<br>
Make the fingerprint unique per website, and generate a new unique fingerprint for every fresh start such as at website data removal.</p>
<p><strong>Our tools</strong>:</p>
<ul>
<li>Use multi-hop proxies to hide IP addresses and defend against network and geographic position fingerprinting.</li>
<li>Limit the number of fingerprintable web APIs whenever possible. This could mean altering the APIs, gating them behind user permissions, or not implementing them.</li>
<li>Inject small amounts of noise in return values of fingerprintable web APIs.</li>
</ul>
<h3>Fingerprinting Protection Details</h3>
<p>Safari’s new advanced fingerprinting protections make it difficult for scripts to <strong>reliably</strong> extract <strong>high-entropy</strong> data through the use of several web APIs:</p>
<ol>
<li>To make it more difficult to <strong>reliably</strong> extract details about the user’s configuration, Safari injects noise into various APIs: namely, during 2D canvas and WebGL readback, and when reading <code>AudioBuffer</code> samples using WebAudio.</li>
<li>To reduce the overall <strong>entropy</strong> exposed through other APIs, Safari also overrides the results of certain web APIs related to window or screen metrics to fixed values, such that fingerprinting scripts that call into these APIs for users with different screen or window configurations will get the same results, even if the users’ underlying configurations are different.</li>
</ol>
<h4>2D Canvas and WebGL</h4>
<p>Many modern web browsers use a computer’s graphics processing unit (GPU) to accelerate rendering graphics. The Web’s Canvas API (2D Canvas) and WebGL API give a web page the tools it needs for rendering arbitrary images and complex scenes using the GPU, and analyzing the result. These APIs are valuable for the web platform, but they allow the web page to learn unique details about the underlying hardware without asking for consent. With Safari’s advanced fingerprinting protections enabled, Safari applies tiny amounts of noise to pixels on the canvas that have been painted using drawing commands. These modifications reduce the value of a fingerprint when using these APIs without significantly impacting the rendered graphics.</p>
<p>It’s important to emphasize that:</p>
<ol>
<li>This noise injection only happens in regions of the canvas where drawing occurs.</li>
<li>The amount of noise injected is extremely small, and (mostly) should not result in observable differences or artifacts.</li>
</ol>
<p>This strategy helps mitigate many of the compatibility issues that arise from this kind of noise injection, while still maintaining robust fingerprinting mitigations.</p>
<p>In Safari 17.5, we’ve bolstered these protections by additionally injecting noise when reading back data from offscreen canvas in both service workers and shared workers.</p>
<h4>Web Audio</h4>
<p>Similarly, when reading samples using the WebAudio API — via <code>AudioBuffer.getChannelData()</code> — a tiny amount of noise is applied to each sample to make it very difficult to reliably measure OS differences. In practice, these differences are already extremely minor. Typically due to slight differences in the order of operations when applying FFT or IFFT. As such, a relatively low amount of noise can make it substantially more difficult to obtain a stable fingerprint.</p>
<p>In Safari 17.5, we made audio noise injection more robust in the following ways:</p>
<ul>
<li>The injected noise now applies consistently to the same values in a given audio buffer — this means a looping  <code>AudioSourceNode</code> that contains a single high-entropy sample can’t be used to average out the injected noise and obtain the original value quickly.</li>
<li>Instead of using a uniform distribution for the injected noise, we now use normally-distributed noise. The mean of this distribution converges much more slowly on the original value, when compared to the average of the minimum and maximum value in the case of uniformly-distributed noise.</li>
<li>Rather than using a low, fixed amount of noise (0.1%), we’ve refactored the noise injection mechanism to support arbitrary levels of noise injection. This allows us to easily fine-tune noise injection, such that the magnitude of noise increases when using audio nodes that are known to reveal subtle OS or hardware differences through minute differences in sample values.</li>
</ul>
<p>This noise injection also activates when using Audio Worklets (e.g. <code>AudioWorkletNode</code>) to read back audio samples.</p>
<h4>Screen/Window Metrics</h4>
<p>Lastly, for various web APIs that currently directly expose window and screen-related metrics, Safari takes a different approach: instead of the noise-injection-based mitigations described above, entropy is reduced by fixing the results to either hard-coded values, or values that match other APIs.</p>
<ul>
<li><code>screen.width</code> / <code>screen.height</code>: The screen size is fixed to the values of <code>innerWidth</code> and <code>innerHeight</code>.</li>
<li><code>screenX</code> / <code>screenY</code>: The screen position is fixed to <code>(0, 0)</code>.</li>
<li><code>outerWidth</code> / <code>outerHeight</code>: Like screen size, these values are fixed to <code>innerWidth</code> and <code>innerHeight</code>.</li>
</ul>
<p>These mitigations also apply when using media queries to indirectly observe the screen size.</p>
<h2>Don’t Add Fingerprintable APIs to the Web, Like The Topics API</h2>
<p>We have worked for many years with the standards community on improving user privacy of the web platform. There are existing web APIs that are fingerprintable, such as Canvas, and reining in their fingerprintability is a long journey. Especially since we want to ensure existing websites can continue to work well.</p>
<p>It is key for the future privacy of the web to not compound the fingerprinting problem with new, fingerprintable APIs. There are cases where the tradeoff tells us that a rich web experience or enhanced accessibility motivates some level of fingerprintability. But in general, our position is that we should progress the web without increasing fingerprintability.</p>
<p>A recent example where we opposed a new proposal is the Topics API which is now <a href="https://developers.google.com/privacy-sandbox/relevance/topics">shipping in the Chrome browser</a>. We provided <a href="https://github.com/WebKit/standards-positions/issues/111">extensive critical feedback</a> as part of the standards process and we’d like to highlight a few pieces here.</p>
<h3>The Topics API in a Nutshell</h3>
<p>From the <a href="https://github.com/patcg-individual-drafts/topics">proposal</a>:</p>
<pre><code><span>// document.browsingTopics() returns an array of up to three topic objects in random order.
</span><span>const</span> <span>topics</span> <span>=</span> <span>await</span> <span>document</span>.<span>browsingTopics</span>();
</code></pre>
<p>Any JavaScript can call this function on a webpage. Yes, that includes tracker scripts, advertising scripts, and data broker scripts.</p>
<p>The topics come from a predefined list of hundreds of topics. It’s not the user who picks from these topics, but instead Chrome will record the user’s browsing history over time and deduce interests from it. The user doesn’t get told upfront which topics Chrome has tagged them with or which topics it exposes to which parties. It all happens in the background and by default.</p>
<p>The intent of the API is to help advertisers target users with ads based on each user’s interests even though the current website does not necessarily imply that they have those interests.</p>
<h3>The Fingerprinting Problem With the Topics API</h3>
<p>A new <a href="https://arxiv.org/html/2403.19577v1">research paper</a> by Yohan Beugin and Patrick McDaniel from University of Wisconsin-Madison goes into detail on Chrome’s actual implementation of the Topics API.</p>
<p>The authors use large scale real user browsing data (voluntarily donated) to show both how the 5% noise supposed to provide plausible deniability for users can be defeated, and how the Topics API can be used to fingerprint and re-identify users.</p>
<blockquote><p>
  “We conclude that an important part of the users from this real dataset are re-identified across websites through only the observations of their topics of interest in our experiment. Thus, the real users from our dataset can be fingerprinted through the Topics API. Moreover, as can be seen, the information leakage and so, privacy violation worsen over time as more users are uniquely re-identified.” —Beugin and McDaniel, University of Wisconsin-Madison
</p></blockquote>
<p>The paper was published at the <a href="https://ieeexplore.ieee.org/document/10579537">2024 IEEE Security and Privacy Workshops (SPW)</a> in May.</p>
<h3>Further Privacy Problems With the Topics API</h3>
<p>Re-identifying and tracking users is not the only privacy problem with the Topics API. There is also the profiling of users’ cross-site activity. Here’s an example using topics on <a href="https://github.com/patcg-individual-drafts/topics/blob/main/taxonomy_v2.md">Chrome’s predefined list</a>.</p>
<p>Imagine in May 2024 you go to <code>news.example</code> where you are a subscriber and have provided your email address. Embedded on the website, <code>dataBroker.example</code>. The data broker has gleaned your email address from the login form and calls the Topics API to learn that you currently have these interests:</p>
<ul>
<li>Flowers</li>
<li>Event &amp; Studio Photography</li>
<li>Luxury Travel</li>
</ul>
<p>In May 2026 you go to <code>news.example</code> where <code>dataBroker.example</code> calls the Topics API and is told that you now have these interests:</p>
<ul>
<li>Children’s Clothing</li>
<li>Family Travel</li>
<li>Toys</li>
</ul>
<p>Finally, in May 2029 you go to <code>news.example</code> where <code>dataBroker.example</code> calls the Topics API and is told that you have these interests:</p>
<ul>
<li>Legal Services</li>
<li>Furnished Rentals</li>
<li>Child Care</li>
</ul>
<p>You haven’t told any website with access to your email address anything that’s been going on in your family life. But the data broker has been able to read your shifting interests and store them in their permanent profile of you — while you were reading the news.</p>
<p>Now imagine what advanced machine learning and artificial intelligence can deduce about you based on various combinations of interest signals. What patterns will emerge when data brokers and trackers can compare and contrast across large portions of the population? Remember that they can combine the output of the Topics API with any other data points they have available, and it’s the analysis of all of it together that feeds the algorithms that try to draw conclusions about you.</p>
<p>We think the web should not expose such information across websites and we don’t think the browser, i.e. <em>the user agent</em>, should facilitate any such data collection or use.</p>
<h2>Privacy Enhancements in Both Browsing Modes</h2>
<p>Our defenses against cloaked third-party IP addresses and our partitioning of SessionStorage and blob URLs are enabled by default in both regular browsing and Private Browsing. Here’s how those protections work.</p>
<h3>Defending Against Cloaked First Party IP Addresses</h3>
<p>In 2020, Intelligent Tracking Prevention (ITP) gained the ability to <a href="https://webkit.org/blog/11338/cname-cloaking-and-bounce-tracking-defense">cap the expiry of cookies set in third-party CNAME-cloaked HTTP responses to 7 days</a>.</p>
<p>This defense did not mitigate cases where IP aliasing is used to cloak third party requests under first party subdomains. ITP now also applies a 7-day cap to the expiry of cookies in responses from cloaked third-party IP addresses. Detection of third-party IP addresses is heuristic, and may change in the future. Currently, two IP addresses are considered different parties if any of the following criteria are met:</p>
<ol>
<li>One IP address is IPv4, while the other is IPv6.</li>
<li>If both addresses are IPv4, the length of the common subnet mask is less than 16 bits (half of the full address length).</li>
<li>If both addresses are IPv6, the length of the common subnet mask is less than 64 bits (also half of the full address length).</li>
</ol>
<h3>Partitioned SessionStorage and Blob URLs</h3>
<p>Websites have many options for how they store information over longer time periods. <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage">Session Storage</a> is a storage area in Safari that is scoped to the current tab. When a tab in Safari is closed, all of the session storage associated with it is destroyed. Beginning in Safari 16.1 cross-site Session Storage is partitioned by first-party web site.</p>
<p>Similarly, <a href="https://developer.mozilla.org/en-US/docs/Web/API/Blob">Blobs</a> are a storage type that allow websites to store raw, file-like data in the browser. A blob can hold almost anything, from simple text to something larger and more complex like a video file. A unique URL can be <a href="https://developer.mozilla.org/en-US/docs/Web/API/URL/createObjectURL_static">created for a blob</a>, and that URL can be used to gain access to the associated blob, as long as the blob still exists. These URLs are often referred to as Blob URLs, and a Blob URL’s lifetime is scoped to the document that creates it. Beginning in Safari 17.2, cross-site Blob URLs are partitioned by first-party web site, and first-party Blob URLs are not usable by third parties.</p>
<h2>Setting a New Industry Standard</h2>
<p>The additional privacy protections of Private Browsing in Safari 17.0, Safari 17.2 and Safari 17.5 set a new bar for user protection. We’re excited for all Safari users and the web itself to benefit from this work!</p>
<h2>Feedback</h2>
<p>We love hearing from you! To share your thoughts on Private Browsing 2.0, find John Wilander on Mastodon at <a href="https://mastodon.social/@wilander">@wilander@mastodon.social</a> or send a reply on X to <a href="https://x.com/webkit">@webkit</a>. You can also <a href="https://www.linkedin.com/in/apple-webkit/">follow WebKit on LinkedIn</a>. If you run into any issues, we welcome your <a href="https://feedbackassistant.apple.com/">feedback</a> on Safari UI (learn more about <a href="https://developer.apple.com/bug-reporting/">filing Feedback</a>), or your <a href="https://bugs.webkit.org/">WebKit bug report</a> about web technologies or Web Inspector.</p>

                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I quit my job and made an automatic time tracker (108 pts)]]></title>
            <link>https://taimapp.io</link>
            <guid>40977453</guid>
            <pubDate>Tue, 16 Jul 2024 15:28:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taimapp.io">https://taimapp.io</a>, See on <a href="https://news.ycombinator.com/item?id=40977453">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>     <main><header></header>    <section><div><p><img src="https://taimapp.io/tray.png" alt="Taim logo"></p><h2 data-svelte-h="svelte-1s4m87e">Automated time tracking software<br>to save you time</h2> <p data-svelte-h="svelte-ppsrs9">Forgetting to start a timer is an issue of the past. You do your job, we keep track of it.</p> <div data-svelte-h="svelte-r1ro5t"><p><a href="#pricing">Pre-order now</a></p><p>macOS Ventura 13.1+, Win 10+* is recommended</p></div></div> <div><p><img src="https://taimapp.io/taimapp-preview.png" alt="Automatic time tracking software interface"></p>  </div> </section> <div><div id="section-one"><h2>Struggling with Tracking<br> Your Work Hours?</h2> <p>Freelancers often face the hassle of manually starting and stopping timers, leading to inaccurate time tracking and billing. 
        Missed hours and overcharges can harm your reputation and client trust.</p></div> <div> <div data-svelte-h="svelte-mv45p8"> <p>Documenting
        <span>11:30 - 13.00</span></p></div>      <div><div data-svelte-h="svelte-e0k7ia"> <p>My Project 1
        <span>09:00 - 10.00</span></p></div>  <div data-svelte-h="svelte-1v8rkjr"> <p>Mega Corp Inc.
        <span>11.30 - 13.00</span></p></div></div></div> <div id="product"><h2>Why Choose Taim?</h2> <p>You have full control over your sessions. You can either record sessions manually or automatically.
      Need to change the duration or date of a session? No problem. You can easily edit sessions to reflect the correct information.</p> </div> <div id="features"><h2>Control your sessions</h2> <p>It doesn't matter if you work on a personal project or on a client's project, you can adjust settings accordingly.</p> <div id="cards"><div><p><img src="https://taimapp.io/features/toggle-billing.png" alt="Toggle Billing"></p><div><h3>Toggle Billing</h3> <p>Select whether a session is billable &amp; paid or not, to make your invoicing process easier.</p> </div></div><div><p><img src="https://taimapp.io/features/application-flow.png" alt="Application Flow"></p><div><h3>Application Flow</h3> <p>Easily see an overview of your activity, and choose what to log.</p> </div></div><div><p><img src="https://taimapp.io/features/time-modifying.png" alt="Modify time &amp; sessions"></p><div><h3>Modify time &amp; sessions</h3> <p>Edit your session data by adding or removing time &amp; data whenever needed.</p> </div></div><div><p><img src="https://taimapp.io/features/notes.png" alt="Notes"></p><div><h3>Notes</h3> <p>Add shareable notes to your sessions &amp; projects to keep track of important details.</p> </div></div><div><p><img src="https://taimapp.io/features/ai-time-tracker.png" alt="Learns from you"></p><div><h3>Learns from you</h3> <p>More work equals bigger brains. Over time Taim can start to log sessions automatically.</p> </div></div><div><p><img src="https://taimapp.io/features/filtering.png" alt="Advanced Filtering"></p><div><h3>Advanced Filtering</h3> <p>Filter time your tracked time by date, statuses, project, tags, and more.</p> </div></div></div> </div></div> <div><h2>Resource efficient</h2> <p>Designed to consume low power, storage. Just like any native application.</p> <div data-svelte-h="svelte-1gpxnfw"><p><span>CPU USAGE: 5-15%</span>
        Other apps</p>  <svg viewBox="0 0 1653 512" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2222_2)"><path d="M0 332.453L12.5227 333.066C25.0455 333.679 50.0909 334.904 75.1364 338.357C100.182 341.809 125.227 347.488 150.273 326.69C175.318 305.892 200.364 258.617 225.409 258.952C250.455 259.287 275.5 307.231 300.545 344.666C325.591 382.101 350.636 409.026 375.682 424.843C400.727 440.659 425.773 445.367 450.818 452.882C475.864 460.398 500.909 470.721 525.955 468.756C551 466.791 576.045 452.538 601.091 384.897C626.136 317.256 651.182 196.226 676.227 186.558C701.273 176.889 726.318 278.582 751.364 270.579C776.409 262.576 801.455 144.879 826.5 132.177C851.545 119.475 876.591 211.769 901.636 207.823C926.682 203.877 951.727 103.691 976.773 78.7498C1001.82 53.8091 1026.86 104.114 1051.91 122.007C1076.95 139.901 1102 125.383 1127.05 141.174C1152.09 156.964 1177.14 203.064 1202.18 197.931C1227.23 192.797 1252.27 136.431 1277.32 148.65C1302.36 160.868 1327.41 241.672 1352.45 285.831C1377.5 329.99 1402.55 337.503 1427.59 316.721C1452.64 295.94 1477.68 246.863 1502.73 186.553C1527.77 126.243 1552.82 54.6998 1577.86 27.2078C1602.91 -0.284178 1627.95 16.2749 1640.48 24.5544L1653 32.8339" stroke="url(#paint0_linear_2222_2)" stroke-width="3"></path><path d="M0 452.057L13.3492 452.457C26.6984 452.856 53.3968 453.656 80.0952 454.901C106.794 456.147 133.492 457.838 160.19 454.227C186.889 450.615 213.587 441.7 240.286 442.322C266.984 442.943 293.683 453.1 320.381 461.152C347.079 469.204 373.778 475.151 400.476 478.873C427.175 482.595 453.873 484.092 480.571 486.151C507.27 488.21 533.968 490.832 560.667 490.992C587.365 491.153 614.063 488.852 640.762 475.858C667.46 462.864 694.159 439.177 720.857 437.794C747.556 436.412 774.254 457.334 800.952 456.285C827.651 455.236 854.349 432.216 881.048 430.226C907.746 428.236 934.444 447.276 961.143 447.04C987.841 446.804 1014.54 427.291 1041.24 422.85C1067.94 418.408 1094.63 429.038 1121.33 433.176C1148.03 437.314 1174.73 434.96 1201.43 438.677C1228.13 442.394 1254.83 452.181 1281.52 451.707C1308.22 451.233 1334.92 440.497 1361.62 443.499C1388.32 446.5 1415.02 463.239 1441.71 472.637C1468.41 482.036 1495.11 484.095 1521.81 480.487C1548.51 476.878 1575.21 467.602 1601.9 456.077C1628.6 444.551 1655.3 430.776 1668.65 423.888L1682 417" stroke="url(#paint1_linear_2222_2)" stroke-width="3"></path></g><defs><linearGradient id="paint0_linear_2222_2" x1="0" y1="240.5" x2="1653" y2="240.5" gradientUnits="userSpaceOnUse"><stop stop-color="#A91576" stop-opacity="0"></stop><stop offset="0.255" stop-color="#A91576"></stop><stop offset="0.765" stop-color="#870505"></stop><stop offset="1" stop-color="#870505" stop-opacity="0"></stop></linearGradient><linearGradient id="paint1_linear_2222_2" x1="0" y1="454" x2="1682" y2="454" gradientUnits="userSpaceOnUse"><stop stop-color="#18A0FB" stop-opacity="0"></stop><stop offset="0.205" stop-color="#18A0FB"></stop><stop offset="0.775" stop-color="#53023C"></stop><stop offset="1" stop-color="#53023C" stop-opacity="0"></stop></linearGradient><clipPath id="clip0_2222_2"><rect width="1653" height="512" fill="white"></rect></clipPath></defs></svg></div></div> <div id="fullcontrol"><h2>Time tracking software with full control</h2> <p>Taim is a time tracking software that works just like you want it to. Clicks and calculations that would otherwise take a lot of time are made instantly for you.</p> <div><div><p><span><span>CSV</span> Jun 9 - Jun 10 sessions</span></p></div><div><p><span><span>PDF</span> Paid sessions this week</span></p></div><div><p><span><span>XLS</span> Billable sessions last month</span></p></div></div></div> <div id="section-three"><h2>Work Smarter, Not Harder</h2> <p>Customize Taim to your needs, if you want to track time manually, you can do that. 
        If you want to track time automatically, you can do that as well.
         It is up to you to decide how you want to track your time.</p> </div>  <div><div id="pricing"><h2>Pay once, use forever</h2> <p>Taim is a one-time purchase. You get all the features in every plan.</p> <div data-svelte-h="svelte-k74s6c"><p><span>PRE-SALE IS LIVE!</span> <span>Currently Taim is available to purchase as a presale. We are planning to launch Taim to the public in early autumn (Sep-Oct), be sure to checkout our <a href="https://taimapp.io/roadmap">roadmap.</a></span></p></div> <div> <div><div><h3>Individual</h3> <p>Early-bird 56% off</p></div> <p>One-time payment for a single user.</p>   <ul role="list"><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Pay once, use forever</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>1 macOS device</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>All features</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Free updates for 12 months</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Local storage</span> </li></ul>  </div><div><p><h3>Teams</h3> </p> <p>Great for multi-devices setups &amp; teams.</p>   <ul role="list"><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Pay per seat for your team.</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Unlimited devices</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>All features</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>App updates during the subscription</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Cloud storage</span> </li></ul>  </div></div></div> <div><h2>Questions &amp; Answers</h2> <p>Get answers to your questions. For additional questions, please get in touch.</p> </div></div></main>  
			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Codestral Mamba (343 pts)]]></title>
            <link>https://mistral.ai/news/codestral-mamba/</link>
            <guid>40977103</guid>
            <pubDate>Tue, 16 Jul 2024 14:44:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/codestral-mamba/">https://mistral.ai/news/codestral-mamba/</a>, See on <a href="https://news.ycombinator.com/item?id=40977103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Following the publishing of the Mixtral family, Codestral Mamba is another step in our effort to study and provide new architectures. It is available for free use, modification, and distribution, and we hope it will open new perspectives in architecture research. Codestral Mamba was designed with help from Albert Gu and Tri Dao.</p><p>Unlike Transformer models, <a href="https://arxiv.org/abs/2312.00752">Mamba models</a> offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length. It allows users to engage with the model extensively with quick responses, irrespective of the input length. This efficiency is especially relevant for code productivity use cases—this is why we trained this model with advanced code and reasoning capabilities, enabling it to perform on par with SOTA transformer-based models.</p><p><img src="https://mistral.ai/images/news/codestral-mamba/codestral-mamba-benchmarks.png" alt="Detailed Codestral Mamba benchmarks" width="100%"></p><p>We have tested Codestral Mamba on in-context retrieval capabilities up to 256k tokens. We expect it to be a great local code assistant!</p><p>You can deploy Codestral Mamba using the <a href="https://github.com/mistralai/mistral-inference">mistral-inference</a> SDK, which relies on the reference implementations from Mamba’s GitHub repository. The model can also be deployed through <a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/mamba">TensorRT-LLM</a>. For local inference, keep an eye out for support in llama.cpp. You may download the raw weights from <a href="https://huggingface.co/mistralai/mamba-codestral-7B-v0.1">HuggingFace</a>.</p><p>For easy testing, we made Codestral Mamba available on <a href="https://console.mistral.ai/">la Plateforme</a> (<code>codestral-mamba-2407</code>), alongside its big sister, Codestral 22B. While Codestral Mamba is available under the Apache 2.0 license, Codestral 22B is available under a <a href="https://mistral.ai/contact/">commercial license</a> for self-deployment or a community license for testing purposes.</p><p><strong>Important:</strong> This is an instructed model, with 7,285,403,648 parameters.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Godotcaml for Godot 4.2 (110 pts)]]></title>
            <link>https://fizzixnerd.com/blog/2024-06-24-announcing-godotcaml/</link>
            <guid>40975509</guid>
            <pubDate>Tue, 16 Jul 2024 11:25:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fizzixnerd.com/blog/2024-06-24-announcing-godotcaml/">https://fizzixnerd.com/blog/2024-06-24-announcing-godotcaml/</a>, See on <a href="https://news.ycombinator.com/item?id=40975509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Hello!  Today I’m releasing a project on which I’ve been working, that is in an early stage of development, into the open source world.  It is integration and bindings to Godot (currently just 4.2) from a new language: OCaml.  It is called Godotcaml.  Details below!</p>
<h2 id="why-godot">Why Godot?</h2>
<p>There are many reasons to choose Godot, but the reason I’ll focus on is that it provides a full game-development IDE from which you can develop production quality 2D and 3D games.  It’s suitable for small-to-medium-sized teams, quite mature, and very fun to use; I think most devs have a secret inclination to “one day” make a video game.  If there is one piece of advice I could give to those devs, it’s choose a good engine that already exists, unless you want to be stuck in Vulkan hell for 6-12 months.  Godot is a good engine, and happily, already exists, and is free and open source — so is a good first choice, even if you abandon it for something different later.</p>
<h2 id="why-ocaml">Why OCaml?</h2>
<p>While my greatest loved language is Haskell, there are some specific reasons that it is somewhat unsuitable for game development.  Instead of listing those, however, I will instead take a more positive approach and talk about why OCaml is an excellent language for game development.</p>
<ul>
<li>
<p><strong>Garbage Collected by Default:</strong> This may shock game devs used to the C++ lyfe, but will come as no surprise to people who’ve used Godot and/or Unity.  Programmers are just more productive when there is a garbage collector, and programmer time is what is usually most valuable — not machine time.  Now, you can write some pretty cool allocation-free OCaml code too — but that’s an optimization that you should measure your need for before you commit to it.</p>
</li>
<li>
<p><strong>Functional by Default:</strong> I love functional programming, and I love the kind of code you can write in an ML-like curried functional language (such as OCaml or Haskell).  So <em>if</em> I were to bind Godot to a new language, it would have to be a functional one.  However, excellent bindings (<code>gdext</code>) for Rust already exist, and it can be used as a workable functional language.  That being said, Rust has the borrowchecker and an aversion to garbage collection, and put simply, I don’t think it makes a particularly good game scripting language, even though it is a wonderful systems programming language.  I think OCaml can one day prove to be a more efficient vehicle for experienced functional programmers to create a game in.  (No hate intended!  This is just how I <em>feel</em>.)</p>
</li>
<li>
<p><strong>Eager by Default:</strong> I absolutely adore lazy APIs.  I might actually be in the minority now in the Haskell community that I think that laziness was not a mistake, but was an excellent choice because of the ergonomics it provides.  However, it’s definitely true that it makes it somewhat less straight-forward to reason about the runtime performance of your code, and indeed to debug it — at least without specialized knowledge that is, in my experience, rather rare to have.  OCaml is eager by default, and I think that’s probably better for a soft-realtime system, unless you’re using some specialized framework (e.g. one that I don’t know whether exists or not!).</p>
</li>
<li>
<p><strong>Side-effects for When You Need Them:</strong> If the world was all written in one language, Haskell would make a pretty good choice — not perfect by any stretch, but pretty good.  However, we live in the real world, where a C FFI is the glue holding this pot of spaghetti we call an operating system together.  Because of that, when doing FFI heavy code, a beautiful language that makes it slightly more tedious to work with side-effects is less preferable in my experience than a language that simply encourages you to think before you use them, but still easily allows you to do things like have global mutable references at the top level.</p>
</li>
<li>
<p><strong>PPXes For CodeGen Help:</strong> I’m not totally sold on the pervasive use of PPXes for OCaml code, but one thing they are definitely useful for is code gen, and you would need a <em>lot</em> of tedious hand-written code if you wanted to interact with Godot directly by hand.  PPXes lie somewhere between Rust macros and TemplateHaskell in their power, but most problems with codegen were able to be solved to my satisfaction using the wonderful PpxLib and context-free extenders.  For example, here is the definition of a simple Godot class that inherits from the stock <code>Node</code> class, and provides a successor function for Godot <code>int</code>s:</p>
</li>
</ul>
<pre is:raw="" tabindex="0"><code><span><span>module</span><span>%</span><span>gclass </span><span>MyClass</span><span> </span><span>=</span><span> struct</span></span>
<span><span>  </span><span>[</span><span>%%</span><span>ginherits </span><span>Node]</span></span>
<span></span>
<span><span>  </span><span>let</span><span>%</span><span>gfunc </span><span>succ</span><span> </span><span>=</span><span> </span></span>
<span><span>    </span><span>[|</span><span> </span><span>ClassMethodFlags</span><span>.</span><span>default </span><span>|]</span></span>
<span><span>      (module </span><span>BuiltinClass0</span><span>.</span><span>Int</span><span>)</span></span>
<span><span>      (module </span><span>Class</span><span>.</span><span>Node</span><span>)</span></span>
<span><span>      (module </span><span>BuiltinClass0</span><span>.</span><span>Int</span><span>) </span></span>
<span><span>      (</span><span>fun</span><span> </span><span>i</span><span> </span><span>_self</span><span> </span><span>-&gt;</span><span> </span><span>Int64</span><span>.</span><span>(i </span><span>+</span><span> </span><span>1</span><span>L))</span></span>
<span><span>end</span></span></code></pre>
<p>Whether or not you like the use of PPXes in general, it is tough to argue that this code isn’t at least <em>short</em>, especially if I were to show you the amount of work you’d have to do without those <code>%</code>s!</p>
<ul>
<li><strong>More:</strong> If you’re reading this post, you probably already like OCaml already, so I’ll leave it there at “it’s a really nice pragmatic functional language, and I thought it would be a good candidate”!</li>
</ul>
<h2 id="what-can-it-do">What Can It Do?</h2>
<p>This is an extremely early stage of development, but basically at this point it is possible to:</p>
<ol>
<li>Call any builtin Godot utility function or method (static, virtual, or otherwise) from OCaml easily, and with documentation comments for the original function intact an available through your favourite OCaml LSP implementation.</li>
<li>Use Godot (binary) operators in a natural way from OCaml. (Unary operators are currently broken, which I will be investigating!)</li>
<li>Construct Godot values from OCaml easily, and from OCaml analogues if they exist (e.g. I incur a dependency on <code>Gg</code> for low-dimensional vector math)</li>
<li>Marshalling in and out of all these functions to/from the OCaml analogues.  That is, a method that is in Godot on an object of type <code>ClassyClass</code> taking an <code>int</code> parameter and returning an <code>int</code> will appear in Godotcaml as <code>int64 -&gt; ClassyClass.t structure ptr -&gt; int64</code>, where the <code>ClassyClass.t structure ptr</code> is the “pointer to the Godot object”, commonly called <code>self</code>.  (Note that this is always the <em>last</em> argument, to facilitate pipeline-style programming when GDScript programmers have a method-chaining interface.)</li>
<li>Naturally define a new Godot class in OCaml that inherits from an existing Godot-registered class.  (Currently NOT tested with classes defined in GDScript and/or externally.)</li>
<li>Most of the code-gen for custom engines that define new stock/builtin types and classes, etc.</li>
<li>Simulated inheritence for stock (and easily extendable to user-defined) classes using module inclusion:  That is if <code>Derived</code> inherits from <code>Base</code>, then simply include <code>Base</code> in the module representing <code>Derived</code>, and you get access to all the methods from <code>Base</code> without explicit casting (or in the case of Rust’s <code>gdext</code>, object composition).</li>
<li>Naturally define a new Godot method in OCaml and have it called from GDScript or another Godot-bound language. (ergonomics still WIP).</li>
</ol>
<h2 id="todo-or-what-cant-it-do">TODO (Or, What Can’t It Do):</h2>
<ol>
<li><strong>Signals:</strong> I’m still cooking ideas for how best to do this, but user-defined signals are not currently nicely supported, and even built in ones are not nice to call or interact with right now.  I’d also like them to be type-safe, so there’s that.  Very WIP, but fixable with some thought and work.</li>
<li><strong>Garbage Collection:</strong> Right now, if a OCaml reference is stored in a, say, GDScript variable, and contains no references in the OCaml world, then it might be collected out from under you.  This is fixable because of Godot’s wonderful reference-counting hooks and OCaml finalisers, I just haven’t gotten around to it yet.</li>
<li><strong>Nice Interface to Various Kinds of Methods:</strong> As of the writing of this blog post, it is only possible to define methods of arity <code>1 + Self</code> from OCaml.  No static methods, no virtual methods  This will be fixed soon, but I wanted to get the stuff in the hands of interested enthusiasts as soon as I was able to call OCaml functions from Godot.</li>
<li><strong>Real First Class Modules for Method Definitions:</strong> Taking again the example above, we can write</li>
</ol>
<pre is:raw="" tabindex="0"><code><span><span>let</span><span>%</span><span>gfunc </span><span>f</span><span> </span><span>=</span><span> </span></span>
<span><span>  </span><span>[|</span><span> </span><span>ClassMethodFlags</span><span>.</span><span>default </span><span>|]</span></span>
<span><span>  (module </span><span>ArgumentGodotTypeModule</span><span>)</span></span>
<span><span>  (module </span><span>SelfGodotTypeModule</span><span>)</span></span>
<span><span>  (module </span><span>ReturnValueGodotTypeModule</span><span>)</span></span>
<span><span>  (</span><span>fun</span><span> </span><span>x</span><span> </span><span>self</span><span> </span><span>-&gt;</span><span> </span><span>(* implementation here ...*)</span><span> </span><span>()</span><span>)</span></span></code></pre>
<p>making it seem like you could write, say</p>
<pre is:raw="" tabindex="0"><code><span><span>let</span><span> </span><span>m</span><span> </span><span>=</span><span> (module </span><span>SomeGodotTypeModule</span><span>)</span></span>
<span></span>
<span><span>let</span><span>%</span><span>gfunc </span><span>g</span><span> </span><span>=</span><span> </span><span>[|</span><span> </span><span>ClassMethodFlags</span><span>.</span><span>default </span><span>|]</span><span> m m m (</span><span>fun</span><span> </span><span>x</span><span> </span><span>y</span><span> </span><span>-&gt;</span><span> x)</span></span></code></pre>
<p>or something, but that wouldn’t work.  This is due to the way I generate code, but basically you have to consider the <code>module</code> as part of the “syntax” for <code>gfunc</code>; it directly takes the packed module out of the expression and codegens using whatever the <code>module</code> operator is applied to (i.e. at <em>parse</em> time, not <em>run</em> time).  This is pretty messed up and not ideal, but the way I justify it to myself is that <code>let%gfunc</code> introduces it’s own syntactic form for declaring a <em>type signature</em> and <em>implementation</em> of a method.  This is, as far as I can tell, <em>not fixable</em>, but I’d love to hear your thoughts if you think it is.  (Briefly, the problem is that if you try to use real first-class modules, their types escape the scope of the function because the implementation function contains them.)</p>
<ol start="5">
<li><strong>General Clean-up:</strong> This implementation was designed sort of ad-hoc and in-the-moment, so some of the stuff doesn’t quite make sense in the module architecture.  This is fixable but lower priority until I iron out the rest of the implementation details of the other features.</li>
<li><strong>Better Build System Integration:</strong> I don’t know dune very well, so I got it <em>working</em>, but it’s not exactly nice to use and develop on.  Lots of ad-hoc calls to <code>dune exec ./gen_api.exe</code> when something has changed, and then trying to remember to format with <code>ocamlformat -i *.ml</code> — that sort of thing; I’m sure dune can help with it, but I didn’t invest the time into learning properly (but I will).  Fixable.</li>
<li><strong>Hot-Reloading:</strong> This should be possible, as Rust somehow manages it in Godot 4.2+ but I haven’t even begun to look into it.  Right now, if you change an OCaml file and recompile the extension, you probably need to restart the editor to see the effects.  Fixable.</li>
<li><strong>Name-mangling for Custom Operators:</strong> I just haven’t done this, but it wouldn’t be hard (and would probably make a good first issue, if you’re looking to contribute).  Right now custom operators defined as <code>gfunc</code> methods in OCaml probably can’t be used from GDScript, or most other languages.  Perhaps at all!  I haven’t even tried.  Fixable.</li>
<li><strong>Finishing the C API:</strong> These represent a work-in-progress set of bindings that are by no means complete at the moment.  That needs to change eventually.  Fixable.</li>
<li><strong>Embedding a TopLevel:</strong> I’d like to be able to interact with the Godot world, well, interactively, from an OCaml Toplevel.  This is on my backburner, but it’s harder than it looks at first, because the shared_object you have to build must of course be native code, but Toplevel and friends are (seemingly?) only available as bytecode.  Ping me if you have ideas here! Fixability unknown!</li>
<li><strong>Reliably Not Segfault:</strong> This is an unsafe C api, and so it’s extremely sharp, and the interface is very rough around the edges, as I figure out what exactly each value is supposed to actually do.  DO NOT TRY TO MAKE A PRODUCTION GAME IN GODOTCAML RIGHT NOW!  I’m going to fix things, but they definitely aren’t ready for prime-time at the moment.  This is fixable, but will take time and testing.</li>
<li><strong>Testing:</strong> Speaking of testing, I have none.  If you’d like to contribute here, I’d be happy to hear from you; I’m personally going to prioritize other above areas until things are a little more stable in the API, so I’m not constantly changing things and fixing <em>broken tests</em> (i.e. as opposed to <em>broken code that is being tested</em>).  Fixable.</li>
<li><strong>Type Safety Concerns:</strong> Right now all classes have the same object type.  This makes it nice for when you’re inheriting from them, as module inclusion “just works”, but obviously have negative effects on the type safety of the system.  Destructive updates of the module types during inclusion is one possible solution, but this requires you to have a module type for every Godot class from which you wish to inherit — a tall order.  I believe this is fixable with some thought — and indeed, it <em>must</em> be fixed in my eyes, even if it means more code gen for the module signatures — but I haven’t given it much thought beyond that.</li>
<li><strong>Support Multiple Native Type Sizes:</strong> Godot’s api supports multiple configurations, depending on if you want float64 or float32, and 64-bit and 32-bit systems.  Right now, I’m concentrating on the float64 + 64-bit configuration, but this should be expanded at some point in the future.  Fixable.</li>
</ol>
<p>For more, check the issue page on GitHub, as that is where I’ll be doing the development.</p>
<h2 id="to-be-continued">To Be Continued</h2>
<p>More details and a setup guide to come!  If you’d like to get involved, I’d love to hear from you — best place to find me is either GitHub or the OCaml Discourse currently.  Beware, the code is pretty funky at the moment, but it’ll get there!</p>
<p>Best,</p>
<p><em>Matt</em></p>

        <p><span>#open-source</span><span>#ocaml</span><span>#godot</span><span>#godotcaml</span><span>#announcement</span>
        </p>
      </div><div>
    <p><img alt="Author Photo" width="96" height="96" src="https://fizzixnerd.com/_astro/fizzixnerd_Z1RMnIC.png" loading="eager" decoding="async">
    </p>
    <div>
      <p>
          About Matt Walker
        </p>
      <p>Matt Walker is a software engineer with a love for all things Functional, DevOps, and Typed, currently residing in Toronto, Canada.</p>
    </div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Live Stream of VC Funded Startups – Use It for Research and Sales (503 pts)]]></title>
            <link>https://old.reddit.com/r/SaaSMarketing/comments/1e4ktjy/i_creatd_a_tool_to_track_all_live_vc_investments/</link>
            <guid>40975351</guid>
            <pubDate>Tue, 16 Jul 2024 10:59:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/SaaSMarketing/comments/1e4ktjy/i_creatd_a_tool_to_track_all_live_vc_investments/">https://old.reddit.com/r/SaaSMarketing/comments/1e4ktjy/i_creatd_a_tool_to_track_all_live_vc_investments/</a>, See on <a href="https://news.ycombinator.com/item?id=40975351">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="searchexpando"><p><label>limit my search to r/SaaSMarketing</label></p><div id="moresearchinfo"><p>use the following search parameters to narrow your results:</p><dl><dt>subreddit:<i>subreddit</i></dt><dd>find submissions in "subreddit"</dd><dt>author:<i>username</i></dt><dd>find submissions by "username"</dd><dt>site:<i>example.com</i></dt><dd>find submissions from "example.com"</dd><dt>url:<i>text</i></dt><dd>search for "text" in url</dd><dt>selftext:<i>text</i></dt><dd>search for "text" in self post contents</dd><dt>self:yes (or self:no)</dt><dd>include (or exclude) self posts</dd><dt>nsfw:yes (or nsfw:no)</dt><dd>include (or exclude) results marked as NSFW</dd></dl><p>e.g. <code>subreddit:aww site:imgur.com dog</code></p><p><a href="https://www.reddit.com/wiki/search">see the search faq for details.</a></p></div><p><a href="https://www.reddit.com/wiki/search" id="search_showmore">advanced search: by author, subreddit...</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Engineer's Guide to Deep Learning: Understanding the Transformer Model (240 pts)]]></title>
            <link>https://www.interdb.jp/dl/</link>
            <guid>40974193</guid>
            <pubDate>Tue, 16 Jul 2024 07:01:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.interdb.jp/dl/">https://www.interdb.jp/dl/</a>, See on <a href="https://news.ycombinator.com/item?id=40974193">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body-inner" tabindex="-1">
          <article>
            


<h2 id="the-engineers-guide-to-deep-learning">The Engineer’s Guide To Deep Learning</h2>
<figure><a href="https://www.flickr.com/photos/x-ray_delta_one/35257686756">
    <img src="https://www.interdb.jp/dl/metropolis.jpg" width="480"> </a>
</figure>


<p>We are in the third golden age of AI.</p>
<p>In the previous two golden ages (1950s-1960s and the 1980s),
our expectations outpaced the capabilities of the technology at the time, leading to disappointment.
In contrast, the AI technology of the current golden age, which began in the mid-2010s, has consistently exceeded our expectations.</p>
<p>Among AI technologies, the Transformer, introduced in 2017, stands as a groundbreaking breakthrough.
Initially developed as a machine translation model, its impact has extended to permeate nearly every field.
Today, the Transformer model is considered essential knowledge for modern engineers.</p>
<p>The first goal of this document is to provide the shortest path for engineers to understand the Transformer.</p>
<h6 id="what-is-this-document">What is this document</h6>
<ul>
<li>A concise guidebook:<br>
This document provides just enough information to learn the Transformer.</li>
</ul>
<!--
Unlike a textbook, it avoids detailed explanations.
However, I will explain where appropriate documentation is not available or where readers find the material difficult to understand.
-->
<h6 id="what-this-document-provides">What this document provides</h6>
<ul>
<li>
<p>Working <a href="https://github.com/s-hironobu/guide2dl/tree/main" target="_blank">Python code examples</a> for hands-on learning:<br>
To enhance comprehension, this document provides working Python code examples that readers can run themselves.</p>
</li>
<li>
<p>References for further exploration:<br>
This document introduces readers to a variety of documentation options, recognizing that different individuals find different resources more accessible.</p>
</li>
</ul>

<div>
  <p>Contents</p>
  <div>

<ul>
<li>Part 1: <a href="https://www.interdb.jp/dl/part01.html">Neural Networks</a><br>Introduces the fundamental concepts of neural networks.</li>
<li>Part 2: <a href="https://www.interdb.jp/dl/part02.html">Recurrent Neural Networks (RNNs)</a><br>Explores RNNs, including LSTM and GRU.</li>
<li>Part 3: <a href="https://www.interdb.jp/dl/part03.html">Natural Language Processing (NLP) and Attention Mechanisms</a><br>Provides the essential principles of NLP, encompassing machine translation and attention mechanisms.</li>
<li>Part 4: <a href="https://www.interdb.jp/dl/part04.html">Transformer</a><br>Unravels the Transformer model.</li>
<li>Appendix: <a href="https://www.interdb.jp/dl/L-00">Basic Knowledge</a><br>Provides the minimum knowledge of Python and mathematics required to understand the Transformer.</li>
</ul>
</div>
</div>

<div>
    <p>
    <label for="expand-05cf400c8aed19a98bb9d3ea186aea44">
        <i></i>
        <i></i>
        Change History (since 21st May, 2024)
    </label></p>
</div>
<h5 id="next-goal">Next goal</h5>
<p>Many Transformer-based technologies are currently being developed.
There will definitely be another major breakthrough in the near future.
I might write about them if I have time.</p>
<h5 id="copyright">Copyright</h5>
<p>© Copyright ALL Right Reserved, Hironobu SUZUKI.</p>
<p>For any inquiries regarding the use of this document or any of its figures, please contact me after reading the following FAQ:</p>

<div>
    <p>
    <label for="expand-988bd57a55344cb0d300647b4c8666b0">
        <i></i>
        <i></i>
        FAQ
    </label></p><div id="expandcontent-988bd57a55344cb0d300647b4c8666b0">

<p>Since publishing my content, I’ve been fortunate to receive a lot of positive feedback, which is truly gratifying.
However, I’ve also encountered a few instances where people tried to misuse my content for self-promotion in the past.</p>
<!--
(For more details, refer to the FAQ in "[the internals of PostgreSQL](https://www.interdb.jp/pg/)")
-->
<p>These experiences have shaped the approach I’ve outlined below:</p>
<ol>
<li>Who can use this document freely?<br>
If you are a teacher or a student belonging to an educational organization, you can freely use this document and figures in your study.
Anyone can use this document and figures with noncommercial meetings and lectures, if you state the link to this site and the copyright; otherwise, contact me.</li>
<li>Is it available for commercial contents?<br>
This content can be used under two options:
<ul>
<li>Revenue Share:
You can leverage this content after a revenue share agreement is signed.
Under this agreement, you’ll share 20% of the sales generated from using this content including the github repository.</li>
<li>Full Buyout:
In very rare cases, I consider requests for full commercial use of all content on this site (and the github repository).
For a complete buyout of all content rights, the cost is €10,000,000.</li>
</ul>
</li>
<li>Why doesn’t the author waive the copyright of this document or use the creative commons license?<br>
I’d like to ask you what problems you have by that I keep on having the copyright of my document.</li>
</ol>
</div>
</div>
<p>When you send me an email, please <strong>provide at least two SNS addresses (e.g. LinkedIn, Twitter) for verification purposes</strong>.
Due to the XZ backdoor incident, I no longer accept contact from anonymous individuals.</p>
<p><span><span><i></i></span><span>Exception</span></span> Educational institutions can use this document freely.</p>

<h6 id="hironobu-suzuki">Hironobu SUZUKI</h6>
<p>I am a software programmer/engineer, the author of:</p>
<ul>
<li><a href="http://www.interdb.jp/pg/" target="_blank">The Internals of PostgreSQL</a></li>
<li><a href="http://www.interdb.jp/dl/" target="_blank">The Engineer’s Guide To Deep Learning</a></li>
<li><a href="https://github.com/s-hironobu/pg_plan_inspector" target="_blank">pg_plan_inspector</a></li>
<li><a href="https://github.com/s-hironobu/pg_tuner" target="_blank">pg_tuner</a></li>
</ul>
<p>I graduated from graduate school in information engineering (M.S. in Information Engineering),
had worked for several companies as a software developer and technical manager/director,
and published seven books (4 PostgreSQL books and 3 MySQL books) in Japanese and a Chinese book.</p>
<p>As a director of the Japan PostgreSQL Users Group (2010-2016),
I organized the largest (non-commercial) technical seminar/lecture on PostgreSQL in Japan for more than six years,
and also served as the program committee chair of the Japan PostgreSQL Conference in 2013 and as a member in 2008 and 2009.
In June 2022, <a href="https://postgresql.life/post/hironobu_suzuki/" target="_blank">my interview article</a> was published.</p>
<p>Cuando era joven, vivió en Sudamérica por unos años. Recientemente, a veces vuelve a allí.</p>
<p>I am looking for a new job, applying ML and AI technologies to DBMS.</p>
<p>I’m interested in History, Animal Rights, Cosmology, Social Issues, Environment Issues. I play the piano and guitar. Vegetarian. I love animals, music, science.</p>

            
          </article>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI illegally barred staff from airing safety risks, whistleblowers say (146 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2024/07/13/openai-safety-risks-whistleblower-sec/</link>
            <guid>40974154</guid>
            <pubDate>Tue, 16 Jul 2024 06:51:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2024/07/13/openai-safety-risks-whistleblower-sec/">https://www.washingtonpost.com/technology/2024/07/13/openai-safety-risks-whistleblower-sec/</a>, See on <a href="https://news.ycombinator.com/item?id=40974154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="PBIKSWH2CJH27NQS6BVBMESB24" data-el="text" dir="null">OpenAI whistleblowers have filed a complaint with the Securities and Exchange Commission alleging the artificial intelligence company illegally prohibited its employees from <a href="https://www.washingtonpost.com/technology/2024/06/04/openai-employees-ai-whistleblowers/?itid=lk_inline_manual_2" target="_blank">warning </a>regulators about the grave risks its technology may pose to humanity, calling for an investigation.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="I2BEYH4RENHTBLYB2G5VYLGMDU" data-el="text" dir="null">The whistleblowers said OpenAI issued its employees overly restrictive employment, severance and nondisclosure agreements that could have led to penalties against workers who raised concerns about OpenAI to federal regulators,<a href="https://www.washingtonpost.com/documents/83df0e55-546c-498a-9efc-06fac591904e.pdf?itid=lk_inline_manual_4" target="_blank"> according to a seven-page letter</a> sent to the SEC commissioner earlier this month that referred to the formal complaint. The letter was obtained exclusively by The Washington Post.</p></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="U7EFJOVSBJANHMID5ZODY4GNNI" data-el="text" dir="null">OpenAI made staff sign employee agreements that required them to waive their federal rights to whistleblower compensation, the letter said. These agreements also required OpenAI staff to get prior consent from the company if they wished to disclose information to federal authorities. OpenAI did not create exemptions in its employee nondisparagement clauses for disclosing securities violations to the SEC.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="WADFSEVK5NHI3JTJURT65MRGLI" data-el="text" dir="null">These overly broad agreements violated long-standing federal laws and regulations meant to protect whistleblowers who wish to reveal damning information about their company anonymously and without fear of retaliation, the letter said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="57HMR3A3IJG7ND4UJF2ZKP76AY" data-el="text" dir="null">“These contracts sent a message that ‘we don’t want … employees talking to federal regulators,’” said one of the whistleblowers, who spoke on the condition of anonymity for fear of retaliation. “I don’t think that AI companies can build technology that is safe and in the public interest if they shield themselves from scrutiny and dissent.”</p><div role="group" aria-roledescription="carousel" data-qa="article-body"><h2>GET CAUGHT UP<p>Stories to keep you informed</p></h2></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="QBGXLIXUFJGQFD7HASNP3JPDIY" data-el="text" dir="null">In a statement, Hannah Wong, a spokesperson for OpenAI said, “Our whistleblower policy protects employees’ rights to make protected disclosures. Additionally, we believe rigorous debate about this technology is essential and have already made important changes to our departure process to remove nondisparagement terms.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="MLHFUMOBBFEELIFOVC4BKMCAQY" data-el="text" dir="null">The whistleblowers’ letter comes amid concerns that OpenAI, which started as a<b> </b>nonprofit with an altruistic mission, is putting profit before safety in creating its technology. The Post <a href="https://www.washingtonpost.com/technology/2024/07/12/openai-ai-safety-regulation-gpt4/?itid=lk_inline_manual_13" target="_blank">reported Friday</a> that OpenAI rushed out its latest AI model that fuels ChatGPT to meet a May release date set by company leaders, despite employee concerns that the company “failed” to live up to its own security testing protocol that it said would keep its AI safe from catastrophic harms, like teaching users to build bioweapons or helping hackers develop new kinds of cyberattacks. In a statement, OpenAI spokesperson Lindsey Held said the company “didn’t cut corners on our safety process, though we recognize the launch was stressful for our teams.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="TH3SN445UVE7PC4L3TFGYHYC5U" data-el="text" dir="null">Tech companies’ strict confidentiality agreements have long vexed workers and regulators. During the #MeToo movement and national protests in response to the murder of George Floyd, <a href="https://www.washingtonpost.com/news/powerpost/paloma/the-technology-202/2020/07/02/the-technology-202-here-s-what-facebook-ad-boycott-organizers-plan-to-tell-mark-zuckerberg/5efcc6f388e0fa7b44f6c2fd/?itid=lk_inline_manual_14" target="_blank">workers warned</a> that such legal agreements limited their ability to report sexual misconduct or racial discrimination. Regulators, meanwhile, have worried that the terms muzzle tech employees who could alert them to misconduct in the opaque<b> </b>tech sector, especially amid allegations that companies’ algorithms promote content that undermines elections, public health and children’s safety.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="PBRT3ZCHC10B7ATVQ4TZ16V3MW" data-el="text" dir="null">The rapid advance of artificial intelligence sharpened<b> </b>policymakers’ concerns about the power of the tech industry, prompting a flood of calls for regulation. In the United States, AI companies are largely operating in a legal vacuum, and policymakers say they cannot effectively create new AI policies without the help of whistleblowers, who can help explain the potential threats posed by the fast-moving technology.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="1MJTVFK93N5C5ANZ20PUGG5TW8" data-el="text" dir="null">“OpenAI’s policies and practices appear to cast a chilling effect on whistleblowers’ right to speak up and receive due compensation for their protected disclosures,” said Sen. Chuck Grassley (R-Iowa) in a statement to The Post. “In order for the federal government to stay one step ahead of artificial intelligence, OpenAI’s nondisclosure agreements must change.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="652ZX455XZAFFCRJF54SSPIDIU" data-el="text" dir="null">A copy of the letter, addressed to SEC chairman Gary Gensler, was sent to Congress. The Post obtained the whistleblower letter from Grassley’s office.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="TNEC468CUD38VEBX5T95KAUFTW" data-el="text" dir="null">The official complaints referred to<b> </b>in the letter were submitted to the SEC in June. Stephen Kohn, a lawyer representing the OpenAI whistleblowers, said the SEC has responded to the complaint.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="7YWSZPAHU5ESVBEY4UCH3TVOAQ" data-el="text" dir="null">It could not be determined whether the SEC has launched an investigation. The agency declined to comment.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="25MGV72VVNFVPAU2QHSNMBBD7U" data-el="text" dir="null">The SEC must take “swift and aggressive” steps to address these illegal agreements, the letter says, as they might be relevant to the wider AI sector and could violate the October <a href="https://www.washingtonpost.com/technology/2023/10/30/biden-artificial-intelligence-executive-order/?itid=lk_inline_manual_24" target="_blank">White House executive order</a> that demands AI companies develop the technology safely.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="YF6YXKL6QJABXFSKJOMEJK2LIA" data-el="text" dir="null">“At the heart of any such enforcement effort is the recognition that insiders … must be free to report concerns to federal authorities,” the letter said. “Employees are in the best position to detect and warn against the types of dangers referenced in the Executive Order and are also in the best position to help ensure that AI benefits humanity, instead of having the opposite effect.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="VBLIBHJZ7RH67L564Q47DMO2TI" data-el="text" dir="null">These agreements threatened employees with criminal prosecutions if they reported violations of law to federal authorities under trade secret laws, Kohn said. Employees were instructed to keep company information confidential and threatened with “severe sanctions” without recognition of their right to report such information to the government, he said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="MTDF47KSXFFCHGKVEQ6P24ZLXQ" data-el="text" dir="null">“In terms of oversight of AI, we are at the very beginning,” Kohn said. “We need employees to step forward, and we need OpenAI to be open.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="63IGWQ6QEBERXD7HMFWTBTM6BY" data-el="text" dir="null">The SEC should require OpenAI to produce every employment, severance and investor agreement that contains nondisclosure clauses to ensure they don’t violate federal laws, the letter said. Federal regulators should require OpenAI to notify all past and current employees of the violations the company committed as well as notify them that they have the right to confidentially and anonymously report any violations of law to the SEC. The SEC should issue fines to OpenAI for “each improper agreement” under SEC law and direct OpenAI to cure the “chilling effect” of its past practices, according to the whistleblowers letter.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="T4GVHMFCIJBUZH4UKMEZ5GO34Y" data-el="text" dir="null">Multiple tech employees, including <a href="https://www.washingtonpost.com/technology/2021/10/26/frances-haugen-facebook-whistleblower-documents/?itid=lk_inline_manual_32" target="_blank">Facebook whistleblower Frances Haugen</a>, have filed complaints with the SEC, which established a whistleblower program in the wake of the 2008 financial crisis.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="5GFZHILTJRHHNJUGOYTAWL5ZKE" data-el="text" dir="null">Fighting back against Silicon Valley’s use of NDAs to “monopolize information” has been a protracted battle, said Chris Baker, a San Francisco lawyer. He won a $27 million settlement for Google employees in December against claims that the tech giant used onerous confidentiality agreements to block whistleblowing and other protected activity. Now tech companies are increasingly fighting back with clever ways to deter speech, he said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="VQNXXU4DJ97DQ4N4YTY49QZ82C" data-el="text" dir="null">“Employers have learned that the cost of leaks is sometimes way greater than the cost of litigation, so they are willing to take the risk,” Baker said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[For advertising: Firefox now collects user data by default (535 pts)]]></title>
            <link>https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html</link>
            <guid>40974112</guid>
            <pubDate>Tue, 16 Jul 2024 06:41:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html">https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html</a>, See on <a href="https://news.ycombinator.com/item?id=40974112">Hacker News</a></p>
Couldn't get https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Peter Buxtun, whistleblower who exposed Tuskegee syphilis study, has died (229 pts)]]></title>
            <link>https://www.theguardian.com/us-news/article/2024/jul/15/peter-buxtun-tuskegee-whistleblower-dies</link>
            <guid>40973422</guid>
            <pubDate>Tue, 16 Jul 2024 03:17:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/article/2024/jul/15/peter-buxtun-tuskegee-whistleblower-dies">https://www.theguardian.com/us-news/article/2024/jul/15/peter-buxtun-tuskegee-whistleblower-dies</a>, See on <a href="https://news.ycombinator.com/item?id=40973422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Peter Buxtun, the whistleblower who revealed that the US government allowed hundreds of Black men in rural <a href="https://www.theguardian.com/us-news/alabama" data-link-name="in body link" data-component="auto-linked-tag">Alabama</a> to go untreated for syphilis in what became known as the Tuskegee study, has died. He was 86.</p><p>Buxtun died on 18 May of Alzheimer’s disease in Rocklin, California, according to his attorney, Minna Fernan.</p><p>Buxtun is revered as a hero to public health scholars and ethicists for his role in bringing to light the most notorious medical research scandal in US history. Documents that Buxtun provided to the Associated Press, and its subsequent investigation and reporting, led to a public outcry that ended the study in 1972.</p><p>Forty years earlier, in 1932, federal scientists began studying 400 Black men in Tuskegee, Alabama, who were infected with syphilis. When antibiotics became available in the 1940s that could treat the disease, federal health officials ordered that the drugs be withheld. The study became an observation of how the disease ravaged the body over time.</p><p>In the mid-1960s, Buxtun was a federal public health employee working in San Francisco when he overheard a co-worker talking about the study. The research was not exactly a secret – about a dozen medical journal articles about it had been published in the previous 20 years. But hardly anyone had raised any concerns about how the experiment was being conducted.</p><p>“This study was completely accepted by the American medical community,” said Ted Pestorius of the US Centers for Disease Control and Prevention, speaking at a 2022 program marking the 50th anniversary of the end of the study.</p><p>Buxtun had a different reaction. After learning more about the study, he raised ethical concerns in a 1966 letter to officials at the CDC. In 1967, he was summoned to a meeting in Atlanta, where he was chewed out by agency officials for what they deemed to be impertinence. Repeatedly, agency leaders rejected his complaints and his call for the men in Tuskegee to be treated.</p><p>He left the US Public Health Service and attended law school, but the study ate at him. In 1972, he provided documents about the research to Edith Lederer, an AP reporter he had met in San Francisco. Lederer passed the documents to the AP investigative reporter Jean Heller, telling her colleague: “I think there might be something here.”</p><p>Heller’s story was published on 25 July 1972, leading to congressional hearings, a class-action lawsuit that resulted in a $10m settlement and the study’s termination about four months later. In 1997, President Bill Clinton formally apologized for the study, calling it “shameful”.</p><p>The leader of a group dedicated to the memory of the study participants said on Monday they were grateful to Buxtun for exposing the experiment.</p><p>“We are thankful for his honesty and his courage,” said Lille Tyson Head, whose father was in the study.</p><p>Buxtun was born in Prague in 1937. His father was Jewish, and his family immigrated to the US in 1939 from Nazi-occupied Czechoslovakia, eventually settling in Irish Bend, Oregon, on the Columbia River.</p><p>In his complaints to federal health officials, he drew comparisons between the Tuskegee study and medical experiments Nazi doctors had conducted on Jews and other prisoners. Federal scientists did not believe they were guilty of the same kind of moral and ethical sins, but after the Tuskegee study was exposed, the government put in place new rules about how it conducts medical research. Today, the study is often blamed for the unwillingness of some African Americans to participate in medical research.</p><p>“Peter’s life experiences led him to immediately identify the study as morally indefensible and to seek justice in the form of treatment for the men. Ultimately, he could not relent,” said the CDC’s Pestorius.</p><p>Buxtun attended the University of Oregon, served in the US army as a combat medic and psychiatric social worker and joined the federal health service in 1965.</p><p>Buxtun went on to write, give presentations and win awards for his involvement in the Tuskegee study. A global traveler, he collected and sold antiques, especially military weapons and swords and gambling equipment from California’s gold rush era.</p><p>He also spent more than 20 years trying to recover his family’s properties confiscated by the Nazis and was partly successful.</p><p>“Peter was wise, witty, classy and unceasingly generous,” said David M Golden, a close friend of Buxtun’s for over 25 years. “He was a staunch advocate for personal freedoms and spoke often against prohibition, whether it be drugs, prostitution or firearms.”</p><p>Another longtime friend Angie Bailie said she attended many of Buxtun’s presentations about Tuskegee.</p><p>“Peter never ended a single talk without fighting back tears,” she said</p><p>Buxtun himself could be self-effacing about his actions, saying he did not anticipate the vitriolic reaction of some health officials when he started questioning the study’s ethics.</p><p>At a Johns Hopkins University forum in 2018, Buxtun was asked where he got the moral strength to blow the whistle.</p><p>“It wasn’t strength,” he said. “It was stupidity.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exo: Run your own AI cluster at home with everyday devices (332 pts)]]></title>
            <link>https://github.com/exo-explore/exo</link>
            <guid>40973339</guid>
            <pubDate>Tue, 16 Jul 2024 02:55:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/exo-explore/exo">https://github.com/exo-explore/exo</a>, See on <a href="https://news.ycombinator.com/item?id=40973339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/exo-explore/exo/raw/main/docs/exo-logo-black-bg.jpg">
  <img alt="exo logo" src="https://github.com/exo-explore/exo/raw/main/docs/exo-logo-transparent.png" width="50%" height="50%">
</picture></themed-picture>
<p dir="auto">exo: Run your own AI cluster at home with everyday devices. Maintained by <a href="https://x.com/exolabs_" rel="nofollow">exo labs</a>.</p>

</div>
<hr>
<p dir="auto">Forget expensive NVIDIA GPUs, unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, Linux, pretty much any device!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Involved</h2><a id="user-content-get-involved" aria-label="Permalink: Get Involved" href="#get-involved"></a></p>
<p dir="auto">exo is <strong>experimental</strong> software. Expect bugs early on. Create issues so they can be fixed. The <a href="https://x.com/exolabs_" rel="nofollow">exo labs</a> team will strive to resolve issues quickly.</p>
<p dir="auto">We also welcome contributions from the community. We have a list of bounties in <a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing" rel="nofollow">this sheet</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wide Model Support</h3><a id="user-content-wide-model-support" aria-label="Permalink: Wide Model Support" href="#wide-model-support"></a></p>
<p dir="auto">exo supports LLaMA and other popular models.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dynamic Model Partitioning</h3><a id="user-content-dynamic-model-partitioning" aria-label="Permalink: Dynamic Model Partitioning" href="#dynamic-model-partitioning"></a></p>
<p dir="auto">exo optimally splits up models based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Automatic Device Discovery</h3><a id="user-content-automatic-device-discovery" aria-label="Permalink: Automatic Device Discovery" href="#automatic-device-discovery"></a></p>
<p dir="auto">exo will automatically discover other devices using the best method available. Zero manual configuration.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">ChatGPT-compatible API</h3><a id="user-content-chatgpt-compatible-api" aria-label="Permalink: ChatGPT-compatible API" href="#chatgpt-compatible-api"></a></p>
<p dir="auto">exo provides a ChatGPT-compatible API for running models. It's a one-line change in your application to run models on your own hardware using exo.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Device Equality</h3><a id="user-content-device-equality" aria-label="Permalink: Device Equality" href="#device-equality"></a></p>
<p dir="auto">Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices connect p2p. As long as a device is connected somewhere in the network, it can be used to run models.</p>
<p dir="auto">Exo supports different partitioning strategies to split up a model across devices. The default partitioning strategy is <a href="https://github.com/exo-explore/exo/blob/main/topology/ring_memory_weighted_partitioning.py">ring memory weighted partitioning</a>. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.</p>
<themed-picture data-catalyst-inline="true"><picture>
  <img alt="ring topology" src="https://github.com/exo-explore/exo/raw/main/docs/ring-topology.png" width="30%" height="30%">
</picture></themed-picture>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The current recommended way to install exo is from source.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">From source</h3><a id="user-content-from-source" aria-label="Permalink: From source" href="#from-source"></a></p>
<p dir="auto">Python&gt;=3.12.0 is required because of <a href="https://github.com/exo-explore/exo/issues/5" data-hovercard-type="issue" data-hovercard-url="/exo-explore/exo/issues/5/hovercard">issues with asyncio</a> in previous versions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/exo-explore/exo.git
cd exo
pip install -r requirements.txt"><pre>git clone https://github.com/exo-explore/exo.git
<span>cd</span> exo
pip install -r requirements.txt</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example Usage on Multiple MacOS Devices</h3><a id="user-content-example-usage-on-multiple-macos-devices" aria-label="Permalink: Example Usage on Multiple MacOS Devices" href="#example-usage-on-multiple-macos-devices"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Device 1:</h4><a id="user-content-device-1" aria-label="Permalink: Device 1:" href="#device-1"></a></p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Device 2:</h4><a id="user-content-device-2" aria-label="Permalink: Device 2:" href="#device-2"></a></p>

<p dir="auto">That's it! No configuration required - exo will automatically discover the other device(s).</p>
<p dir="auto">The native way to access models running on exo is using the exo library with peer handles. See how in <a href="https://github.com/exo-explore/exo/blob/main/examples/llama3_distributed.py">this example for Llama 3</a>.</p>
<p dir="auto">exo also starts a ChatGPT-compatible API endpoint on <a href="http://localhost:8000/" rel="nofollow">http://localhost:8000</a>. Note: this is currently only supported by tail nodes (i.e. nodes selected to be at the end of the ring topology). Example request:</p>
<div data-snippet-clipboard-copy-content="curl http://localhost:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
     &quot;model&quot;: &quot;llama-3-70b&quot;,
     &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the meaning of exo?&quot;}],
     &quot;temperature&quot;: 0.7
   }'"><pre><code>curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3-70b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X POST http://localhost:8001/api/v1/chat -H &quot;Content-Type: application/json&quot; -d '{&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the meaning of life?&quot;}]}'"><pre>curl -X POST http://localhost:8001/api/v1/chat -H <span><span>"</span>Content-Type: application/json<span>"</span></span> -d <span><span>'</span>{"messages": [{"role": "user", "content": "What is the meaning of life?"}]}<span>'</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inference Engines</h2><a id="user-content-inference-engines" aria-label="Permalink: Inference Engines" href="#inference-engines"></a></p>
<p dir="auto">exo supports the following inference engines:</p>
<ul dir="auto">
<li>✅ <a href="https://github.com/exo-explore/exo/blob/main/inference/mlx/sharded_inference_engine.py">MLX</a></li>
<li>✅ <a href="https://github.com/exo-explore/exo/blob/main/inference/tinygrad/inference.py">tinygrad</a></li>
<li>🚧 <a href="https://github.com/exo-explore/exo/blob/main/TODO">llama.cpp</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Networking Modules</h2><a id="user-content-networking-modules" aria-label="Permalink: Networking Modules" href="#networking-modules"></a></p>
<ul dir="auto">
<li>✅ <a href="https://github.com/exo-explore/exo/blob/main/networking/grpc">GRPC</a></li>
<li>🚧 <a href="https://github.com/exo-explore/exo/blob/main/TODO">Radio</a></li>
<li>🚧 <a href="https://github.com/exo-explore/exo/blob/main/TODO">Bluetooth</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Known Issues</h2><a id="user-content-known-issues" aria-label="Permalink: Known Issues" href="#known-issues"></a></p>
<ul dir="auto">
<li>🚧 As the library is evolving so quickly, the iOS implementation has fallen behind Python. This is being addressed, and longer term we will push out an approach that will unify the implementations so we don't have to maintain separate implementations.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Review of Linux on Surface Pro 4 (191 pts)]]></title>
            <link>https://www.binwang.me/2024-07-12-A-Review-of-Linux-on-Surface-Pro-4.html</link>
            <guid>40973123</guid>
            <pubDate>Tue, 16 Jul 2024 02:03:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.binwang.me/2024-07-12-A-Review-of-Linux-on-Surface-Pro-4.html">https://www.binwang.me/2024-07-12-A-Review-of-Linux-on-Surface-Pro-4.html</a>, See on <a href="https://news.ycombinator.com/item?id=40973123">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_content">
<article id="post">
  <header>
    
    
      <p>Posted on 12 Jul 2024, tagged <code>Linux</code><code>Surface</code><code>Microsoft</code><code>Operating system</code><code>tech</code></p>
    
  </header>

  <h2 id="background">Background</h2>

<p>I bought a Surface Pro 4 at 2016. It has an Intel Core m3-6Y30 CPU and 4GB memory. The spec is not that impressive even compared to an average laptop released years earlier. On the other hand, the form factor is very attractive to me: at a very low price, you get a tablet with a beautiful HiDPI 2k screen, a pressure sensitive stylus and an useable keyboard. It is on the heavier side if used as a tablet, but compared to other laptops, it’s very light. It served me very well for my limited use cases. The blog <a href="https://www.binwang.me/2016-11-28-Config-Development-Environment-on-Windows.html">Build a Unix Like Environment on Windows</a> was written at that era. Some years later, I bought a more powerful laptop when I needed to work while traveling. So I gave the Surface away to a family member.</p>

<p>However, during the past years, I couldn’t stop thinking about having a Linux tablet. At first I checked <a href="https://pine64.org/devices/pinetab/">Pinetab</a>, then I realized I had a Surface which would be perfect if I could install Linux on it. I searched online and found some successful stories. So when I <a href="https://www.binwang.me/2024-03-19-Travel-Back-to-China.html">travelled back to my hometown</a> at the beginning of this year, I brought the Surface back with me and started to experiment with it.</p>

<h2 id="use-cases">Use Cases</h2>

<p>Before I go further, I need to mention my intended use cases:</p>

<ul>
  <li>Browse Internet. Mainly <a href="https://www.rssbrain.com/">RSS Brain</a>, the RSS reader I built by myself.</li>
  <li>Media consumption: watch videos from my Samba share and online websites like Youtube.</li>
  <li>PDF reading: reading only is enough for me but it’s better if I can take notes in the PDF.</li>
  <li>Sketches: I don’t have a habit to do handwriting notes even at students era. Nowadays it’s more efficient and readable/searchable to take text notes with Markdown. However, I do like drawing sketches on paper when brain storming or resolving some hard problem. Moving it to digital has a lot of benefits if it works.</li>
  <li>Drawing: this is a good to have feature. I don’t really have needs to draw things but it’s always fun. Especially with the development of AI, if I draw something and send it to a more powerful machine to generate images, it could open doors to many possibilities.</li>
</ul>

<h2 id="installation">Installation</h2>

<p>The installation of Linux is actually very easy. I tried two distros and the installation process went very smooth for both of them. The distros I tried are <a href="https://endeavouros.com/">EndeavourOS</a> and Fedora workstation 40.</p>

<p>The installation steps are well documented in <a href="https://github.com/linux-surface/linux-surface/wiki/Installation-and-Setup#installation">linux-surface’s wiki</a>. <a href="https://github.com/linux-surface/linux-surface">linux-surface</a> is the Linux kernel and tools for Surface devices. The wiki page has its installation steps as well.</p>

<p>In general, if only used as a laptop, the experience is almost perfect even without the linux-surface kernel. But using it as a tablet is another story.</p>

<h2 id="what-works">What Works</h2>

<p>Let’s talk about what works first. Even without linux-surface kernel, almost everything works except touch screen and stylus. That includes things like wireless network, bluetooth, keyboard, power profile, UI scaling for Hi-DPI and so on. Multi touch and pressure sensitive stylus works as well (sort of, see sections below) after installed linux-surface kernel. Battery life is good enough: about 5-6 hours of light usage like web browsing, PDF reading, and about 3 hours of video watching. (Just some estimated time from my experience, no serious benchmarking was done).</p>

<p>On the software side, automatic screen rotation is enabled on both distros I tried. KDE with EndeavourOS is very fast and responsive. When the keyboard is detached, it enters tablet mode which makes some UI larger and more user friendly with touch gestures. For example, you can just touch on a folder to open it in Dolphin instead of double click it.</p>

<p>For Gnome, it’s less responsive than KDE but the UI is really beautiful when used as a tablet. I was never a fan after Gnome 3 but I guess the UI changes it made makes more sense on a tablet than on a laptop or a desktop. The overall layout really reminds you about the iPad or Android tablet (in a good way), but with the power of a real desktop OS at the same time. I would really like it if it uses less resource.</p>

<p>Even though the overall experience is positive and has the potential to meet all my use cases, one serious problem made it very unusable and made me gave up Linux on Surface at the end.</p>

<h2 id="the-problems-in-both-distros">The Problems in Both Distros</h2>

<p>The deal breaker problem is touch recognition. The problem is in the surface-linux tools so it affects all the distros. The biggest problem is ghost touch: touches are registered randomly even when I do nothing. I tried a lot of workarounds including the ones mentioned in <a href="https://github.com/linux-surface/linux-surface/wiki/Surface-Pro-5">linux-surface’s wiki page</a>, but none of them actually resolved it completely. Sometimes it’s fixed after reboot but reappeared after next reboot. Sometimes it get fixed for a period of time but reappeared after a system upgrade. Sometimes the touch screen doesn’t work at all after resume from sleep. The randomness and the serious of the problem is really annoying so I gave up using it with Linux at last.</p>

<p>Other than the ghost touching, another big problem about touch recognition is palm rejection. It’s really annoying when draw things with the pen. In iptsd (surface-linux’s deamon for touch recognition), there is a configuration to disable touch screen when using a pen but it doesn’t work well. So it makes drawing very unusable.</p>

<p>Both KDE and Gnome has virtual keyboards when the physical keyboard is detached, and works most of the time despite the problems I’ll mention in the following sections. But if you have setup disk encryption with a password, there is no virtual keyboard when you input the disk password, so a physical keyboard is always needed during the boot. Which can be annoying but not really a deal breaker.</p>

<p>The last big problem is battery drain during sleep. It uses about 30% battery for one night even it has been put into sleep. I had similar issues for other laptops. I believe there maybe some configurations I can tune to fix that. But after I gave up Linux on it because of the ghost touch, I didn’t dig deeper into that.</p>

<p>Other than the problems shared by both distros, each distro/desktop environment also has their own problems.</p>

<h2 id="the-problems-in-kde-with-endeavouros">The Problems in KDE with EndeavourOS</h2>

<p>The biggest problem in KDE other than the ones I talked above, is the virtual keyboard. It’s buggy and not very stable. Sometimes it kept pop up and sometimes it doesn’t show up. It’s annoying especially at the login screen: if it’s not popped up you will still need a physical keyboard, which prevent it to be a real tablet. Sometimes when the keyboard is popped up, the panel at the bottom cannot be touched. The bugs happened randomly that makes it hard to be properly reported.</p>

<p>Another problem is the touch gesture for right click. Naturally, with a touch screen, long press should be treated like a right click. But that is not the case for KDE. So a lot of operations just cannot be done without a mouse when you need a right click.</p>

<p>Resize a window is also very tricky with touch only operation: you need to touch on the boarder precisely on the first try.</p>

<p>At last, the scroll behaviour is not very smooth. It makes me a little bit dizzy just by scrolling through web pages and PDFs.</p>

<p>So I thought give another distro and desktop environment a try, to see if they can resolve my problems.</p>

<h2 id="the-problems-in-gnome-with-fedora-workstation-40">The Problems in Gnome with Fedora Workstation 40</h2>

<p>I choose Fedora because it comes with Gnome, and I had good experience with it before. After the installation, the first impression is it’s much slower than KDE with EndeavourOS. I found it enables swap and ZRam by default so I disabled them. It’s better but still slower than KDE. It uses more memory at around 40-50% percentage while idel. And I got a lot of OOM kills which almost never happened with KDE on EndeavourOS.</p>

<p>Maybe because of the slowness, it’s also buggy for lots of operations. For example, when switch to the workspace view from PDF viewer with 4 fingers swipe up, the PDF keeps scrolling at the background. And when scroll in the file manager, the context menu keeps popping up.</p>

<p>Other than the slowness, there is a problem on the virtual keyboard as well: the backspace key doesn’t work properly. I found a workaround by install a third-party Gnome addon, but sometimes the old keyboard still popped up.</p>

<h2 id="go-back-to-windows-10">Go Back to Windows 10</h2>

<p>I’d say if the touch recognition works well enough, all the other problems are acceptable with KDE. But with those problems, I finally decided to fallback to Windows 10 again. It works well enough, just as I remembered from years ago. However I abandoned OneNotes and some other Microsoft products and use the following software instead:</p>

<ul>
  <li>Firefox as the browser.</li>
  <li>Nextcloud to sync the files.</li>
  <li>Samba for video sharing.</li>
  <li>Built in video player for local video playing.</li>
  <li>Krita for drawing and sketches.</li>
  <li>Drawboard PDF for PDF reading.</li>
</ul>

<p>It’s pretty disappointing that this device cannot be used with Linux properly. But using Windows is still better to just let the device sitting there doing nothing. Maybe I will re-evaluate it after Windows 10 is end of life next year.</p>


</article>



<!--
<section id="comment">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'crazy-hot-ice'; // required: replace example with your forum shortname
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
-->



<!-- MathJax -->




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Struggling with poor memory and executive function. What to do? (114 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40972596</link>
            <guid>40972596</guid>
            <pubDate>Tue, 16 Jul 2024 00:14:50 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40972596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40972596">
      <td><span></span></td>      <td><center><a id="up_40972596" href="https://news.ycombinator.com/vote?id=40972596&amp;how=up&amp;goto=item%3Fid%3D40972596"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40972596">Ask HN: Struggling with poor memory and executive function. What to do?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40972596">106 points</span> by <a href="https://news.ycombinator.com/user?id=regainmemory">regainmemory</a> <span title="2024-07-16T00:14:50"><a href="https://news.ycombinator.com/item?id=40972596">21 hours ago</a></span> <span id="unv_40972596"></span> | <a href="https://news.ycombinator.com/hide?id=40972596&amp;goto=item%3Fid%3D40972596">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Struggling%20with%20poor%20memory%20and%20executive%20function.%20What%20to%20do%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40972596&amp;auth=b7efd20b14c0cb691a7ce2e53168b8f16fc441cc">favorite</a> | <a href="https://news.ycombinator.com/item?id=40972596">100&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>In my late 30s and have always struggled to effectively build a career, network, life. Has only occurred to me that this may due to what seems to be a deficiency in my memory. I've had a wealth of experiences, both good and bad, but few have found their way into my mental models of how the world works, and so I keep making the same mistakes or am unable to effectively navigate my way to a specific goal.</p><p>From learning new topics &amp; skills, to learning how to network, to learning the dynamics of how an organization and how to navigate various relationships, to making well-reasoned and effective decisions, my mind often feels like mush, totally blinded to the realities of the world. I feel I've been stuck both cognitively and emotionally at a late-teen stage. Poor emotional regulation, difficulties with thinking in nuanced details, constantly flying at 1000 feet.</p><p>The older I get with no improvement, the more it feels my goals keep drifting farther away. I want to get fit, I want to read more, I want to develop skills, I want to build relationships, I want to be an entrepreneur. These are things many of my colleagues have been working towards for years. It feels like I just wasn't given the playbook, and worse, am incapable of piecing one together.</p><p>Have any of you dealt with this? Any advice?
Are there coaches that can help?</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Second Law of Thermodynamics (2011) (132 pts)]]></title>
            <link>https://franklambert.net/secondlaw.com/</link>
            <guid>40972589</guid>
            <pubDate>Tue, 16 Jul 2024 00:12:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://franklambert.net/secondlaw.com/">https://franklambert.net/secondlaw.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40972589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><center>

<table>
  <tbody><tr>
    <td nowrap=""><span face="Arial">This site shows that some ancient
    questions about "things going wrong" in our lives </span></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; have
    surprisingly simple answers in modern basic chemistry </span></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    (even things happening to us which cause that painful cry of "Why me?")</span></td>
  </tr>
  <tr>
    <td nowrap=""></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">Still more important to one's
    philosophy about life, these chemical ideas can startle </span></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; us
    into seeing how fortunate we all are: that things <i>don't</i> go wrong more often! </span></td>
  </tr>
  <tr>
    <td nowrap=""></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">We'll talk mainly about down-to-earth <em>chemical reactions</em> - like burning and rusting - </span></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and the behavior of things - common solid objects of wood, metal, and bone,</span></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; not about complex computer chips or programs going wrong</span></td>
  </tr>  
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    (nor about&nbsp; personal relations that fall apart. Even chemistry has limits.)</span></td>
  </tr>
  <tr>
    <td nowrap=""></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">Simple chemical reactions often are involved in
    annoying or deadly happenings to us: </span></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
    being in a fire in Malibu, flying the X-1 (that broke the sound barrier – but had a</span></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp;
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  dangerous corroded battery cable), or things just “go wrong”:&nbsp; our tire blows out</span></td>
  </tr>
  <tr>
    <td nowrap=""><span face="Arial">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    a friend riding a horse and is thrown – ending up with a&nbsp; broken neck. </span></td>
  </tr>
</tbody></table>
</center></div><p><span face="Arial"><em><big><span color="#FF0000">Life is hard.</span> <span color="#0000FF">But it's harder if you don't know how the material world works! </span></big></em></span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The All-American Delusion of the Polygraph (239 pts)]]></title>
            <link>https://lithub.com/what-the-all-american-delusion-of-the-polygraph-says-about-our-relationship-to-fact-and-fiction/</link>
            <guid>40972437</guid>
            <pubDate>Mon, 15 Jul 2024 23:39:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/what-the-all-american-delusion-of-the-polygraph-says-about-our-relationship-to-fact-and-fiction/">https://lithub.com/what-the-all-american-delusion-of-the-polygraph-says-about-our-relationship-to-fact-and-fiction/</a>, See on <a href="https://news.ycombinator.com/item?id=40972437">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								        
										<p>A few weeks before the release of my first book, a memoir about my mother’s murder, I had to take a polygraph exam. The two things were not in fact related, but that was easy to forget once I found myself strapped in a chair in a windowless room on the fourth floor of a federal building in El Paso, with some polygrapher I’d just met sitting behind me, asking questions.</p><p><span>Article continues below</span></p>
<p>I’d met my examiner, whom I’ll call Kevin, that morning. The federal scheduler had insisted on a 9:00 am appointment even though I lived four hours away, which meant I’d spent the previous night alone in a Motel 6 by the highway in El Paso, eating Del Taco and reflecting on the decisions that had led me to spending the night alone in a Motel 6 by the highway in El Paso, eating Del Taco. Technically, I was there because I’d applied for a job with Customs and Border Protection. But the truth seemed much more complicated than that.</p>
<p>I didn’t get much sleep, and showed up at the federal building early, dressed in what I imagined the government meant by comfortable clothing: black dress pants, plain white oxford, no tie. I looked like a banquet waiter. One other guy was in the waiting room when I walked in. As we sat there past the scheduled time of our appointments, we struck up a desultory conversation. Like me, he’d been in the hiring process for years, had driven down from Albuquerque the night before, and seemed nervous. He asked if I’d done any research on the polygraph. I said no, and asked him the same question. He said no. We were getting our first lies out of the way.</p>
<p><span>The lie detector was like any true story in America: the facts didn’t matter as long as a lot of people believed it. </span></p><p>The government’s guidelines had repeatedly stressed that we should not do any research before our polygraph. Their insistence struck me as odd: if the machine detects lies, why would it matter? I’d spent much of the previous week Googling polygraphs. I was in the middle of denying it to my new friend when the door opened and Kevin appeared.</p>
<p>He was around sixty, short, portly, bald, with a silly goatee and wire-rimmed glasses, wearing a baggy gray suit and a shirt in one of those colors I never can keep straight, puce or mauve or periwinkle, a little too festive for the occasion. Kevin was squinty and smug, with an air of hollow authority that reminded me of my middle-school principal. He didn’t even step into the waiting room, just swung the door open and shouted my full name. I stood and shook the hand he extended. He squeezed too hard and said I could call him by his first name, as if he were doing me some great favor.</p>
<p>Kevin led me down a drab hallway to a door on the left that led to my first disappointment. I’d been expecting the sort of tableau you see in cop movies, some dank cellar with a dangling bulb and a two-way mirror on the wall for me to stare defiantly into. Instead we entered a bare, sterile room with office chairs on either side of a desk. Wires ran from the computer on the desk across the room to a hard-backed chair festooned with cuffs and straps and sensors.</p>
<p>We sat in the normal chairs. Kevin leaned back in his, twirled a pencil, and said, “Let’s get started.” I knew this bit, the casual tone and performative warmth. I was a college professor, sort of, and I did this same bit on the first day of classes, trying to make my students trust me. In that first fluorescent moment, staring into Kevin’s beady eyes, I had a premonition: I was going to fail my polygraph exam.</p>
<p>*</p>
<p>Joan Didion once wrote that it’s easy to see the beginnings of things, and harder to see the ends. That has not been my experience. Ends are obvious: divorce, death, getting fired. Beginnings, on the other hand, seem subjective. If ends are facts, beginnings are truth: relative, random, subject to belief.</p>
<p>The story of my exam, for example, could begin two years before I met Kevin, when President Obama signed an act requiring polygraph screening for applicants to certain federal agencies. Or two years before that, when I started writing a memoir and began to understand what it means to tell the truth. Or it could begin with the polygraph itself, the kind of story America likes best: a simple one that isn’t all that true. The polygraph’s most commonly credited inventor is John Larson, an employee of the Berkeley Police Department, who developed a new device for interrogations in 1921. Larson was twenty-nine at the time, and, like me, a strange candidate for law enforcement: he might have been America’s first cop with a PhD.</p>
<p>His degree was in physiology, the science of the body’s systems. The then-prevailing scientific belief saw crime as biological, either hereditary or the result of a physical defect. Larson explored both possibilities. His undergraduate work tried to find familial patterns in fingerprints that predicted criminality. In grad school, Larson shifted his focus to thyroid deficiencies. The results were disappointing, so he turned to machines. Larson read an article about using blood pressure to detect deception, and decided to improve on its author’s technique by designing a machine that could do so more objectively. The polygraph was born.</p>
<p>The truer story is, as usual, more complicated. Larson’s machine was not so much an invention as it was an amalgam of existing devices. He didn’t believe it detected lies and didn’t call it a polygraph: Larson referred to his machine as an “emotion recorder.” His protégé and rival, Leonarde Keeler, would later come up with the term polygraph to help commercialize the device.</p>
<p>Polygraph organizations like to say the word means “many writings,” which is halfway true. It’s a neologist portmanteau of the Greek terms meaning exactly that, and the machine does indeed create many writings. But to claim the word means only and exactly one thing is to make the same mistake with language polygraphers habitually make with facts: believing that they’re static and absolute. Language, like truth, is neither. Words evolve and change over time and mean different things in different contexts. Polygraph has six different definitions, according to the <em>Oxford English Dictionary</em>—three of which predate the machine—and they range from a letter grouping in cryptography to a person imitating another to a writer of various works. (Which I guess makes this an essay about a polygraph taking a polygraph.)</p>
<p>More to the point, polygraph does not mean “lie detector.” Larson himself repudiated that term for the rest of his life. But that fact didn’t get in the way of a good story. Once the polygraph was adopted by police across America and heralded in the popular media, it took on a mythical new name: the lie detector. And as soon as the lie detector became famous, a bunch of men fought with each other for decades—mostly in their memoirs—over who was its true inventor. In his book <em>The Truth Machine: A Social History of the Lie Detector</em>, Geoffrey C. Bunn devotes an entire chapter to the question of who invented the device and offers enough credible candidates to field a baseball team: everyone from Carl Jung, who helped pioneer the field of psychology, to Étienne-Jules Marey, who did the same for cinematography. As Bunn puts it, with magisterial restraint, it was a “curious and notable fact that the lie detector’s principal actors mistrusted each other intensely.” Maybe they should have taken a polygraph to resolve the question.</p>
<p>*</p>
<p>My polygraph test was the final step in a process that had begun three years earlier, when I started applying for government jobs. At the time, I was living in San Francisco, teaching at Stanford, and was nearly finished with the book. While my job sounded impressive, it paid forty thousand dollars a year and was located in one of the most expensive areas of the country. I was in the second year of a two-year contract and had been trying unsuccessfully to get a real teaching job for years, something tenure-track, or at least a lecturer gig in a more affordable city, one with benefits and some semblance of job security.</p>
<p>When I was “on the market,” as they say, I applied to teaching jobs in Fairbanks, Alaska; Birmingham, Alabama; Spokane, Washington; Camden, New Jersey; Cullowhee, North Carolina; and on and on. I spent a lot of time back then browsing Wikipedia pages for cities I’d never been to, trying to convince myself I could live there. It was a moot point. Despite dozens of interviews, I couldn’t get a professor job. My memory has condensed the experience into one vivid example, when the writing faculty of a football powerhouse down South made me fly to Chicago, in January, on my own dime, so I could sit in their hotel suite sweating through my freshly pressed suit while they asked a series of oddly combative questions for an hour and a half, after which they never bothered to contact me again to tell me that I didn’t get the job I wouldn’t have taken if my life depended on it.</p>
<p>Somewhere along the way, I decided to give up on academia and find a less demoralizing line of work, something with better pay, more stability, maybe even a union. I applied to be a technical writer, an FBI agent, a cop. I didn’t have any luck with those, either. My brother suggested the Border Patrol. He was an agent, and so were a half-dozen of my childhood friends and my former baseball coach. They were always hiring. The starting pay was nearly double what I made at Stanford, with much better benefits, and I might be able to move back to Arizona.</p>
<p>Most of my old friends from home thought joining the Border Patrol was a good idea. I could be closer to family, make a living, maybe even buy a house. My social circle in San Francisco was another story. By then, I’d been in academia for more than a decade, surrounded by white liberals from wealthy backgrounds. When they heard I was trying to join the Border Patrol, my new friends all said the same thing: <em>why?</em></p>
<p>I didn’t understand the question, and soon determined it was a matter of perspective. For people who grew up wealthy, or even middle-class—whatever that means—the proper career path seems to be to find a job that’s rewarding, fulfilling, whatever. I’d done every job I’d ever had for one reason, the same reason I was trying to join the Border Patrol: money. Did I want to drive around the border in an SUV detaining people? Of course not. But I didn’t want to help tech billionaires write their memoirs, either. At least the Border Patrol paid a living wage.</p>
<p>But the Border Patrol is <em>racist</em>, my friends said. And that might be true. But cries of racism rang hollow coming from people who worked in academia, one of the whitest industries in America. The Border Patrol is a hell of a lot more diverse than the average writing faculty.</p>
<p>Their underlying point was right, though: if I joined the Border Patrol, I’d be complicit. But I was already complicit. The longer I spent in higher ed, the more it seemed like an engine driving American inequality. At least the Border Patrol gave working-class people a path to economic mobility without being saddled by a lifetime of student loan debt. My brother had begun his career at the same time I’d started grad school. Nine years later, he made twice as much as I did, owned a house and a new Acura, had health insurance and a retirement account and the whole American dream. Meanwhile, in the three years since I applied to the Border Patrol, I’d taken another teaching job in Albuquerque, making forty-five grand, with shitty benefits and no long-term security. Besides, I still hadn’t decided to actually join the Border Patrol. I hadn’t been offered the job.</p>
<p>By the time I went to El Paso, I’d passed a four-hour written exam, a physical, a fitness test, an oral interview, and a background check so intensive that they’d talked to my coworkers, every neighbor in my apartment building, and my high school teachers. Once I passed the background check, I waited more than a year for the call from the polygraph office.</p>
<p>That call would be the first sign that the government and I had different ideas about truth. First, I got an email from a scheduler saying he’d been trying to reach me. I hadn’t received any messages, so I knew that wasn’t true, but had no way to prove it. I called the office and spoke to another man, who said he was a quality control agent, and that he’d call me back soon with a date and time for my appointment. He never did. Then I received a letter saying I’d been removed from consideration for refusing to take the polygraph. I called Mr. Quality Control. He accused me of lying, but grudgingly scheduled my appointment. So there I was, in a building full of liars, about to have mine detected.</p>
<p>*</p>
<p>Kevin started off by reminding me of the rules. One of them was that I could not discuss my test with anyone afterward. I’m sure he would have told me that I couldn’t write about it either, although I made sure not to ask. Clearly the government believed, despite all historical evidence to the contrary, in its power to control information. I believed my right to free speech was inalienable. In that sense, I guess this essay is a lie detector, too: we’re going to find out who was right.</p>
<p>Kevin ran me through some questions that might be on the test. Some were pedestrian, my name and address and so on. Others were bizarre: questions about bestiality, child porn, terrorism.</p>
<p>When he got to a question about my past drug use, Kevin’s tone changed. His smile fell and he held eye contact. The interrogation had begun. I told him the same well-rehearsed thing I’d told my background investigator: <em>I experimented with drugs a few times in high school and college</em>. It was a lie. Kevin seemed to detect it.</p>
<p>“What do you mean by few?” he asked.</p>
<p>“Not many.”</p>
<p>“Could it mean five?”</p>
<p>“I guess it <em>could</em>.”</p>
<p>“Don’t be a smartass.”</p>
<p>I looked around the room. It couldn’t be just me and Kevin. Surely there was a witness somewhere, a hidden camera, a recorder, anything to prove we actually said what his report would say we had. Later I would learn that federal polygraph protocol requires examiners to make audio recordings of exams, and that many examiners have been accused of ignoring that requirement, as well as an array of other sordid and unprofessional practices. I don’t remember Kevin saying anything about a recording, or seeing a device. But there’s no way to say for sure.</p>
<p>Kevin sighed elaborately and asked the maximum number of times I could have used illegal drugs. “Was it six? Eight?”</p>
<p>“Sure,” I said. “It was a long time ago.”</p>
<p>“Could it be ten?” he asked, with a smug little grin, and finally I caught on.</p>
<p>“No. Definitely not ten.” If Kevin wanted a fixed and certain truth, I’d give him one.</p>
<p>He wrote “6–8 times” on his notepad.</p>
<p>“I said it could have been that many, not that it <em>was</em>.”</p>
<p>Kevin said to let him do his job, and a shroud of dread descended. The test hadn’t even begun, and I’d already found myself at epistemological loggerheads with the federal government. Kevin thought he’d convinced me to tell the truth, which was that I’d done drugs between six and eight times in my life. I thought there was no truth to tell. I didn’t remember how many times it was. A few minutes earlier, Kevin had said there was no maximum threshold for drug use that would disqualify me—a bald-faced lie I had not yet detected—but I wasn’t about to say that the real number was closer to a hundred. So the lie became the truth: I’d done drugs six to eight times.</p>
<p><span>The polygraph works a lot like a memoir. It doesn’t find the truth, it creates it.</span></p><p>Almost immediately, I began to believe it. As I sat there watching Kevin scribble notes, a handful of specific drug experiences returned to me. The first time I got high and drunk at the same time, in eighth grade, and spent the night in my friend David’s bathroom with my head on the toilet seat while he tried to convince his mom that I had food poisoning. When Charlie taught me the trick where you blow the smoke through a paper towel tube with a fabric softener sheet rubber-banded over the end. The first line of meth I ever snorted, off a Bone Thugs CD case in Jeremy’s bedroom. The cooks at my first restaurant job going around at closing time on busy nights, handing out key bumps. The eight ball I went in on with two ex-con dishwashers at my second restaurant job, who told me they’d pay me back double in a week, but then Mike got stabbed and I realized I wasn’t cut out for that life. I never saw that money or the drugs, so maybe that one didn’t count.</p>
<p>But that was all during one relatively short period of my life, when I was a shithead teenager. I’d come so far since then. I was a college professor; technically a visiting assistant professor, but still. My memoir was about to come out. The sitting president at the time had admitted to using marijuana and cocaine in <em>his</em> memoir. Why should I be banned from a job for being a small-time delinquent twenty years ago? Remove those few wayward years and it was true enough that I’d only done drugs a few times in high school and college. As we say in the memoir business, it was <em>my</em> truth.</p>
<p>*</p>
<p>My new profession, memoirist, had a complicated relationship with truth, to put it mildly. Fake-memoir scandals have erupted more or less continuously as long as America has existed, from James Frey and his contemporaries, to the so-called autobiographies of Howard Hughes and Davy Crockett, to fantastical captivity narratives of the colonial era and dubious accounts of European explorers in the New World. Sometimes it seems like every notable American figure wrote a fictionalized memoir—even Wyatt Earp, the patron saint of my hometown. (<em>Frontier Marshal</em>. It’s a hoot.) I fudged plenty of facts myself, combining real people into composite characters, changing the order of events. Most memoirists do the same. The point of the genre isn’t accuracy or precision. The point is to tell a good story.</p>
<p>The same applies to the polygraph. There’s no real evidence for the machine’s accuracy. Its purpose is to monitor the body’s physical response to stimuli, but the body’s response to lying is indistinguishable from its response to any other stimulus. Even the telltale spike in the polygraph chart, itself largely a myth created by TV and movies, can indicate anything from a heart problem to sexual attraction. (Indeed, the machine’s inventors used it to detect both of those things. Larson married one of his first test subjects, and Keeler discovered a heart defect while testing the machine on himself.)</p>
<p>But from the early years of the machine to the present day, its proponents have told the same story of an infallible machine that detects lies. Polygraph organizations routinely estimate its accuracy at nearly 100 percent. Most of those estimates are invented out of thin air, and the few based on data suffer from an obvious sampling error. As early as 1939, Walter Summers—yet another purported inventor of a lie detector—pointed out the fundamental flaw on which all polygraph statistics are based: they “fail to relate the number of instances in which deception was actually practiced in a manner which eluded the examiner and the instrument.” You can’t detect the lies you can’t detect.</p>
<p>Independent studies suggest a polygraph exam is roughly as accurate as a coin flip, and that polygraph operators find as many as half of innocent subjects guilty. The scientific case against the polygraph is so compelling that the National Academy of Sciences, the American Psychological Association, the Congressional Office of Technology Assessment, and the United States Supreme Court have dismissed it as unreliable. A federal law forbids using the polygraph to screen applicants to private companies. For most jobs in America, an exam like mine would’ve been against the law.</p>
<p>In fact, the only people who seem to believe the polygraph is accurate are its operators. Every study I’ve found that supports the machine’s ability to detect deception was funded or performed by polygraphers. They’ve formed half a dozen different organizations dedicated to spreading the lie of the lie detector. Their websites are ironically similar to the polygraph itself: archaic, slipshod, rife with bias and bullshit. The International League of Polygraph Examiners calls the device’s invention “officially one of the greatest of all time,” and claims the accuracy of contemporary polygraphs is close to 100 percent. The American Polygraph Association, which claims to be the largest organization of polygraphers, has a section of its website devoted to Polygraph Validity Research.</p>
<p>It begins by stating the organization “<em>believes</em> that scientific evidence supports the validity of polygraph examinations.” The site includes a link to what seems to be the entire basis of that belief, the “Meta-Analytic Survey of Criterion Accuracy of Validated Polygraph Techniques.” The document was prepared by a team of polygraphers, and it reads about how you’d expect. I made it far enough to learn a few astounding facts, including that until 2012, the American Polygraph Association did not require members to use methods supported by published research. In other words, for the first ninety-one years of its existence, polygraphers literally had no scientific standards. Luckily for me, they came up with some just in time for my exam.</p>
<p>The lie detector was like any true story in America: the facts didn’t matter as long as a lot of people believed it. And we wanted to believe it: the notion of a lie detector existed long before the polygraph did. Tellingly, it first appeared in fiction. Bunn traces its first known usage to Charles Walk’s 1909 detective novel <em>The Yellow Circle</em>, in which a character fantasizes about having a machine he calls a lie detector: “With its aid one can plumb the bottomless pits of a chap’s subconscious mind, and fathom all the mysteries of his subliminal ego.” In 1914, seven years before anyone claimed to have invented a polygraph, G. K. Chesterton mocked the notion of a lie detector in his mystery story <em>The Mistake of the Machine</em>, comparing it to the Dark Ages belief that blood would flow from a victim’s body if their murderer touched it. Bunn lists many other instances of lie detectors appearing in fiction long before anyone claimed to have invented one.</p>
<p>Meanwhile, the popular American media seemed obsessed with the idea of machines that could see inside our heads, hearts, and souls. Fourteen years before Larson’s polygraph debuted, the <em>New York Times</em> ran an article rhapsodizing about Jung’s electric psychometer. That “mysterious little machine” purported to detect emotions, not lies; still, the article foretold a future when it would be used to detect guilt, making criminal courts superfluous. Four years later, the same paper ran a two-page profile of the “big-hearted” men, Edward Johnstone and Henry Goddard, who’d been doing the “self-sacrificing work” of testing the psychometer on developmentally disabled children at an institution in New Jersey. Under Johnstone’s supervision, Goddard hooked kids up to a machine like Jung’s. Their intent was to study and eradicate “feeble-mindedness”; Johnstone was a noted member of the American eugenics movement. But the article breathlessly predicted a future in which the lie detector would replace the “impedimenta” of American justice, like judges and juries.</p>
<p>When the story started circulating that a cop with a PhD had invented a lie-detecting machine, a myth became a widely reported fact almost overnight. Within a year, the<em> San Francisco Examiner</em> claimed, “everyone has heard of the ‘lie detector.’” By then, polygraph results had already been banned from American courts, by a judge who was less enthused about the prospect of a machine replacing juries. It would be the first of many official dismissals of the polygraph. But it didn’t dampen the media’s fascination with the so-called lie detector—and, by extension, the American public’s.</p>
<p>That fascination was furthered by the polygraph’s sister inventions, psychology and cinematography, which were created by some of the same men at around the same time. In addition to his psychometer, Jung also helped create the field of modern psychology. Marey developed a different forerunner of the polygraph, as well as chronophotography, an important step in the development of cinema. Marston pioneered the idea of detecting lies based on physical responses, and later became a Hollywood censor and wrote a guide for aspiring screenwriters. His academic mentor, Hugo Münsterberg, helped lay the theoretical groundwork for the polygraph, and also published one of the earliest works of film theory.</p>
<p>The rise of psychology helped drive a cultural fascination with discovering the inner workings of the mind—especially the criminal mind—a desire the lie detector satisfied. Meanwhile, cinematography, the notion that we could document and preserve reality itself, had embarked on its ongoing project of destroying our cultural distinctions between fiction and fact.</p>
<p>Throughout its history, the polygraph has moved freely between the two realms. As it spread to police departments across the country, who used it to investigate real crimes, the machine also began to appear in advertisements and movies. The machine may have made its screen debut in the 1926 silent-film serial <em>Officer 444</em>, alongside Vollmer, who played an idealized version of himself, a criminologist using science against evil. By the 1930s, Marston was using the polygraph to screen-test Hollywood films, including <em>Frankenstein</em>—ironic, considering that Larson once compared his invention to the Monster—and to sell razor blades, gasoline, and cigarettes. In 1941, Marston invented Wonder Woman, a female superhero whose primary power was a Lasso of Truth, similar to a lie detector. Marston called the comic “psychological propaganda,” and it was hugely effective: within a year, Wonder Woman had her own comic book with a circulation of half a million. In 1946, Keeler starred in a noir movie alongside his version of the machine. The first TV show called <em>Lie Detector</em> debuted in the fifties; there have since been a handful of others, both scripted and reality, not to mention a continuous stream of polygraph appearances in film and television. While writing this essay, I watched lie detectors play prominent roles in two different TV shows. In one, the polygraph is accurate; in the other, it isn’t.</p>
<p>*</p>
<p>Kevin slid a blank sheet of paper across the desk and told me to draw the number five inside a black circle. I did, and slid it back. Kevin drew other numbers in other circles and said now he was going to hook me up to the machine.</p>
<p>He told me to take off my shoes and empty my pockets, then directed me to the chair. I sat on one sensor and put my feet on two others. Kevin wrapped two cords around my chest, slid a sphygmomanometer over my left arm, and stuck metal clamps on my right index and ring fingers. As he pumped up the cuff, Kevin asked if I was comfortable. He didn’t seem to be joking.</p>
<p>He sat behind his desk and said he was going to point to all the numbers on the paper and ask if I’d written them. I should say no every time, even for the one I’d written. He did. I did. He unhooked me, led me to the desk, and pointed to the lie on the computer screen. It looked like a lot of squiggly lines to me.</p>
<p>“Now we can take a break,” Kevin said. I looked at the clock, which wasn’t visible from the polygraph chair; we’d only been in the room for half an hour. Kevin smiled inscrutably. “Bathroom and water only. Be back in ten minutes.”</p>
<p>I wandered into the hallway, drank from a fountain, leaned against another beige wall, and tried to calm down. I was not then in a great place, psychologically speaking. I stayed up until sunrise a few nights a week, spent days on end inside my apartment, often went blank with anxiety in front of my classes, and was preoccupied by a vivid and persistent vision of myself swan-diving off my balcony. I would later be diagnosed with various conditions and embark upon a reasonably successful therapeutic journey, but right then, in that hallway, I was freaking the fuck out. My shirt clung wetly to my chest, where I could see my heart beating as if it was trying to escape, like the alien in <em>Alien</em>. If I had a heart attack in the chair, what would that look like on the polygraph readout?</p>
<p>I’d tried to learn techniques for managing anxiety. Most of them didn’t work— picture a beach, my ass—but a shrink I’d briefly seen had suggested imagining the worst possible outcome, and that seemed helpful. The idea was that you embrace the notion of failure and realize it wouldn’t be so bad, thereby relieving the pressure not to fail.</p>
<p>I tried it. What if I failed the poly? I’d go back to Albuquerque, keep teaching, apply for more jobs. This was my backup plan, which made me luckier than pretty much everyone else applying. Then again, that was not the worst-case situation. One problem with that exercise is that I could always come up with something worse. What if I got in a car accident on my tired four-hour drive and spent the rest of my pain-filled life alone in my rented house in Albuquerque? What if I passed the poly, got the job, and actually took it—got sent to some borderland armpit like Ajo or Wellton where I’d have to herd other human beings into the back of trucks? Woke up two or ten or thirty years down the road and didn’t recognize myself?</p>
<p>By the time Kevin came to get me, a few minutes sooner than the ten I’d been promised, I’d almost accepted my imminent failure. If my anxiety didn’t make me fail the test, something else would. I remembered something from my sorta-research about Catholics failing the polygraph at higher rates. Technically I was Catholic, baptized and confirmed, now lapsed, but that only made things worse. Maybe I’d ask Kevin how to become a polygrapher. How much training did it require? Did he enjoy it? How much money did he make? Later, I’d search around online and find out that the average polygraph examiner makes even less than I did at the time. Then again, the training only takes ten weeks, and there are actually jobs in that field. I’d been training for years to be a nonfiction professor and still had no idea what truth actually meant; maybe I should’ve just taken a polygraph course and become an official federal arbiter of facts, an asshole demigod like Kevin.</p>
<p>I shouldn’t be so hard on Kevin. Judging by his clothes and demeanor, he probably came from a similar background to mine. Maybe polygraphing was his version of teaching, a thing he did to pay the bills because it was better than his other options. Maybe he had a whole life to maintain, a family, a little house in some cul-de-sac on the West Side, two Toyotas and a swing set.</p>
<p>While Kevin strapped me back into the chair, I wondered what he told himself at night, trying to sleep, after watching applicants lose their best hope for a career to his machine. Kevin seemed like a smart guy, way too smart to believe in the simpleminded fantasy of a machine that detects lies. But that wasn’t his decision. It was his employer’s. And why is our government the only major employer in the world that uses polygraphs to screen prospective hires?</p>
<p>*</p>
<p>The answer to that question is based on a lie. Even the United States government isn’t dumb enough to believe the polygraph works. The machine’s real purpose is symbolic, as an icon of the power of the state. Law enforcement agencies don’t use the machine to detect lies. They use it to coerce confessions.</p>
<p>In its early days, the polygraph was considered a more humane version of the infamous “third degree,” the interrogation procedure it largely replaced, which involved beating the shit out of a suspect until they confessed. The third degree was itself a variation of another quintessentially American tactic, outright torture. The parallels between torture and the polygraph are obvious: the latter’s arcane parts and procedures, its use of restraints and stimuli, the gratuitous periods of waiting for what the subject knows is coming. The polygraph creates the very stress it’s designed to detect, then presents it as evidence of deception, which often leads its subject to confess. If the subject confesses, that confession effectively <em>becomes</em> the truth, whether it’s true or false or somewhere in between.</p>
<p>And the polygraph has a long history of coercing false confessions. It may begin with its maiden voyage in 1921, when Larson tested his new device on the residents of an all-female Berkeley dorm that had experienced a rash of petty thefts. Thanks in part to the polygraph, a suspect admitted to most of the thefts and withdrew from the university. But the crimes continued, and Larson himself doubted the veracity of her confession.</p>
<p>Not long after, Larson tested a man named Henry Wilkens who was accused of having his wife killed. The polygraph helped to exonerate Wilkens despite evidence of his guilt. After that, police began to doubt the polygraph’s utility, and some departments refused to use it. (The media had no such qualms: it continued to trumpet the infallibility of the “electric detective.”)</p>
<p>A year after the Wilkens case, a young Black man named James Frye retracted his confession to killing a Washington, DC, doctor, claiming it was coerced. Using his variation of the lie detector, Marston examined Frye and declared him innocent. But the judge prevented Marston from testifying as an expert at trial, and an appeals court upheld the ruling, instituting what became known as the Frye rule, which has largely prevented polygraph results from being admissible in American courts ever since.</p>
<p>But the machine remains useful for extracting confessions. And the conflation of confessions and truth is yet another lie, one that’s kept the polygraph alive for the last century as a peculiarly American delusion. Confessions are usually presumed to be true and treated as such in legal settings. But recent research suggests that false confessions are common, especially in the context of police interrogations.</p>
<p>Despite a growing body of evidence, including hundreds of exonerations based on DNA evidence, most people don’t believe in false confessions. A recent article in the <em>Journal of the American Academy of Psychiatry and the Law</em> explains why:</p>
<p>Most lay people believe in what has been referred to as the myth of psychological interrogation: that an innocent person will not falsely confess to police unless he is physically tortured or mentally ill…the myth of psychological interrogation persists because most people do not know what occurs during police interrogations, and because they wrongly assume that individuals do not act against their self-interest or engage in self-destructive behavior, such as falsely confessing to a crime that they did not commit.</p>
<p>The likelihood of a false confession increases when interrogators use certain tactics, especially elements of the so-called Reid Technique, a procedure created in the 1950s by John E. Reid. I didn’t know it at the time, but Kevin used elements of the Reid Technique in my test. He conducted it in a small, barely furnished, cold room; seated me in a hard, armless, straight-backed chair; and repeatedly encroached on my personal space.</p>
<p>Reid first used his technique (along with a polygraph) in 1955, to extract a confession from a man named Darrel Parker who was suspected of killing his wife. Parker was convicted and sentenced to life in prison. He served fifteen years before being released on appeal because Parker’s confession was ruled to have been coerced. Eighteen years after Parker’s release, a man on death row for other crimes confessed to the murder; he did so by showing his lawyers a passage of his memoirs that described the murder in detail, a passage the legal system apparently assumed to be true.</p>
<p>Neither those nor the Reid Technique’s subsequent high-profile failures, including the $2m settlement of a 2012 civil case by a wrongly convicted man named Juan Rivera, have prevented it from being adopted by police departments across America. The company founded in Reid’s name, John E. Reid and Associates, claims its technique is “the most widely used approach to question subjects in the world,” and recently registered a trademark on the term.</p>
<p>The Reid Technique™ involves a number of tactics, from creating an anxiety-inducing environment to a list of specific steps. According to Saul Kassin, perhaps the foremost American expert on false confessions, the purpose of those tactics is to “get suspects to incriminate themselves by increasing the anxiety associated with denial, plunging the subject into a state of despair and then minimizing the perceived consequences of confession.”</p>
<p>Like the polygraph, the Reid Technique isn’t designed to find the truth. Its purpose is to coerce confessions. Research suggests the Reid Technique may actually make interrogators <em>worse</em> at detecting truth. In an independent study, interrogators trained in the Reid Technique were less accurate, although “they were more confident and cited more reasons for their judgments.”</p>
<p><span>Myths exist for a reason, to explain collective phenomena, to explain us to ourselves.</span></p><p>The polygraph itself is not required for the Reid Technique, but it helps. Together, they have a long and checkered history of producing false confessions. In 2013, the <em>Chicago Tribune</em> found a pattern of false confessions obtained via polygraph exams and the Reid Technique, by examiners who routinely ignored accepted standards, including failing or refusing to record interrogations.</p>
<p>*</p>
<p>Kevin said the first battery of questions would cover my character, and asked if I had any questions. I did, but too many, and where to start? So I said no, and Kevin started the exam.</p>
<p>He asked me a battery of eight questions four times in different orders. By the time I typed notes on my phone after the test, I’d already forgotten one of them. The other seven were:</p>
<p>1. Have you misrepresented your past drug use?<br>
2. Have you lied about participating in serious crimes?<br>
3. Have you falsified info on forms?<br>
4. Have you ever cheated to get ahead in your personal life?<br>
5. Have you ever made disparaging comments about your supervisor?<br>
6. Is the light on?<br>
7. Have you taken a drink of water today?</p>
<p>Except for the last two, all of them seemed open to interpretation. For instance, I absolutely had misrepresented my past drug use, but only the number of times, not the drugs or the fact of doing them. And could my estimate be a lie when there’s no way to know the exact answer? What crimes are considered serious? What counts as cheating? Who gets to say? Has any employed person in America not made a single disparaging comment about a supervisor?</p>
<p>The first time through the questions, I tried to follow Kevin’s direction to answer quickly, yes or no, and to abide by his somewhat contradictory instructions to breathe normally while staying absolutely still. The second time through, my voice began to crack, and I swallowed.</p>
<p>“Stop!”</p>
<p>I turned my head to see who he was yelling at.</p>
<p>“Stay completely still!”</p>
<p>I turned back to the wall and tried to comply. Kevin kept yelling, asking combative and rhetorical questions: was I <em>trying</em> to beat the test, did I <em>want</em> to fail? I tried to calm myself by imagining something peaceful, although that was probably considered a countermeasure, and anyway, it didn’t work: I visualized ripping off the electrodes and punching Kevin. I tried the box-breathing technique I’d once learned from a veteran stepdad. That worked better. Possibly too well. Soon I caught myself nodding off.</p>
<p>That probably sounds like a lie. How could someone under that much stress be sleepy? Have you ever been interrogated? I don’t mean metaphorically, having a difficult conversation, confessing something to a spouse, parent, priest, boss. I mean actually interrogated, by a professional. No lawyer, no witnesses, nobody on your side. And he has a machine that says he’s right, not to mention the backing of the Department of Homeland Security, a vast and unaccountable agency built on the lie that policing and surveilling Americans will protect us from terrorism. Suddenly, this part of the Border Patrol application process made a grim kind of sense: I was getting a little taste of how an immigrant might feel. Except I deserved it. I’d signed up for this.</p>
<p>If you haven’t been in that situation, maybe you think, like I did before it was proven otherwise, that you’d be one of the exceptions. You’d beat the polygraph, like people do on TV. But that’s the thing: you can’t beat the polygraph, because the polygraph isn’t a lie detector, isn’t a test, isn’t even a machine. It’s a fact, part of a story power tells itself to justify its power. Maybe you can beat the machine— they don’t detect lies, so it’s not that hard—but you can’t beat an entire country that believes in it.</p>
<p>As Kevin went through the questions again, I sank into a fugue, part paranoia and part exhaustion, and lost track of time. Not what time it was—the whole idea of time. There was no past or future, only an endless present of sitting in that windowless room, strapped to a chair, wired to a machine, staring at a beige wall while a stranger I couldn’t see asked the same questions, over and over. I forgot why I was there, who I was, the truth and what it meant. At some point, I heard a sort of flutter, and my vision vibrated and jumped, as if someone had changed the reel. My mind floated up to the corner of the room and observed the proceedings from a cool remove. My memory of the rest of the test is from a vantage point outside of my body.</p>
<p>Jung defined this phenomenon as dissociation, the loss of a fixed and coherent identity. Reports of similar experiences were largely ignored in early psychology, but more recent studies suggest dissociation is fairly common, and can be triggered by drugs, trauma, stress, or nothing at all. Jung said dissociative states could prevent a subject from recalling important facts, among other things. “We talk about being able to control ourselves,” he wrote. “But self-control is a rare and remarkable virtue.”</p>
<p>The polygraph showed me I was neither rare nor remarkable. By the final time through the battery, I no longer knew what was true. For example: the first three times Kevin asked if I’d ever disparaged a supervisor, I’d rationalized, telling myself “disparaged” was a strong word. It means to regard as having little worth, and older definitions meant to dishonor or degrade; did Kevin know it came from the Old French <em>disparagier</em>, to marry unequally? Certainly I’d made fun of some bosses, and respected few, but I hadn’t disparaged my supervisors, per se. The fourth time he asked me the question, a crystalline memory popped into my detached head, a moment a few years before when I told my then-girlfriend that my then-boss was a fucking idiot.</p>
<p>“No,” I said. Kevin moved on.</p>
<p>Later I would learn that I wasn’t the first lie detector subject to report experiencing dissociation. I’m not even the first one to write about it. As an undergraduate at Harvard, Gertrude Stein worked in a laboratory run by Hugo Münsterberg, who came up with the earliest scientific rationale for lie detection. Stein’s first published work, an 1894 essay originally written for her sophomore composition class titled “In the Psychological Laboratory,” was an account of her experiences in the lab, including an instance of being connected to one of Münsterberg’s primitive predecessors of the polygraph. The essay’s third person narration and distinctive syntax are both harbingers of Stein’s future work, and her knowledge of the machine seems to have informed her later experiments in “automatic writing.” (It’s also worth noting that her autobiography has fictional elements.)</p>
<p>But her account interests me for other reasons. She describes the experience of being subjected to an exam in front of a group of students like so:</p>
<p>Strange fancies begin to crowd upon her, she feels that the silent pen is writing on and on forever. Her record is there she cannot escape it and the group about her begin to assume the shape of mocking fiends gloating over her imprisoned misery. Suddenly she starts, they have suddenly loosened a metronome directly behind her, to observe the effect, so now the morning’s work is over.</p>
<p>What it describes sounds like dissociation, or exactly what I felt when I was subjected to the polygraph.</p>
<p>The scientific literature suggests dissociation during polygraph exams is fairly common. In 1996, the polygrapher Donald J. Krapohl wrote an article for <em>Polygraph</em>, the official organ of the American Polygraph Association, that addresses the phenomenon. The article is typical of polygraphers’ attempts at justifying their profession in the sense that it’s paranoid, authoritarian, proto-fascist, and presents the opinions and experiences of a single polygraph operator as if they’re commandments carved into tablets. Krapohl begins with a blithe, moralistic tirade about “the phenomenon of mendacity” that “pervades every class and culture.” Lying, he claims, is endemic to certain types of people, having “served to defend or expand the interests of uncounted generations of monarchs, merchants, spouses, debtors, knaves, and saints.”</p>
<p>Kraphol attempts to codify four classes of countermeasures. The first, Physical Countermeasures, includes any instance in which a polygraph subject “use[s] movements in hopes of masking their reactions or misdirecting the examiner.” Of course, a polygraph subject might move for any number of reasons during an exam, including as a natural reaction to the very discomfort and stress it’s designed to cause; no examiner or machine can determine <em>why</em>. And one wonders why a test so supposedly accurate can be fooled by something as simple as flexing a muscle.</p>
<p>The second category, “Mental Countermeasures,” includes imagery, hypnosis, biofeedback training, placebos, and even personality. Notably, dissociation is considered a mental countermeasure. Here, again, is the rub: if the subject dissociates during a polygraph, who’s to say whether it’s intentional? Even Krapohl acknowledges that “the outward appearance of a dissociating subject is quite similar to that of a cooperating test subject.” In other words, nobody can tell if another person is dissociating, much less why. Even the person dissociating may not know; I certainly didn’t at the time. Studies suggest dissociation is often an unconscious response to intense stress of the sort the polygraph is designed to create in its subject. The polygraph works by stressing you out, but if the stress it causes in turn causes the subject to dissociate, they can be failed for trying to cheat.</p>
<p>*</p>
<p>Kevin gave me another break. I spent it in the waiting area, staring out over the rooftops of El Paso. The window faced east, so I couldn’t see the border, but the border is like the truth: you know when you’re close to it. After a few minutes, Kevin came and led me back into the interrogation room, where he sat me down and said I’d failed. The machine detected deception in my answers to either the drug question or the falsifying information question. I was amazed: he couldn’t even tell which question I’d lied about? And why hadn’t he detected my lie about disparaging my boss?</p>
<p>Kevin went on to say that I must have researched tactics to defeat the poly. My swallowing seemed to bother him to the point that he considered it evidence of deception. I said my mouth had been dry, but Kevin ignored me, telling stories about his brother who’d done drugs and other applicants who said they’d gotten high a thousand times. I didn’t know it at the time, but his psychological tactics were all elements of the Reid Technique: repeated, unwavering assertions of guilt; attempts to excuse or minimize the suspected crime; constant interruptions; professed sympathy; and, of course, outright lies.</p>
<p>Kevin said he was almost positive that my drug use wasn’t over the threshold. Earlier, he’d told me there was no threshold, and it dawned on me that Kevin wasn’t much of an interrogator without his machine. He kept fishing, accusing me of various lies, interrupting whenever I tried to deny, suggesting things I may have forgotten: didn’t I ever do any pills when I was a bartender? Did I really only do meth once or twice? He began pointing to the computer screen and picking up other deceptions. He detected possible lies in my academic record—I told him I’d gotten a 3.0 GPA at a state school, and why the hell would anybody lie about that?—and questioned whether I really had a master’s degree, even though I’d provided transcripts as part of my background investigation.</p>
<p>He accused, I denied, and it became the worst kind of male interaction, a matter of pride. He was lying, I was lying, everything we said was both true and false, depending how you looked at it. I wasn’t even hooked up to the machine anymore, so I didn’t understand why we were still talking. He’d already said that I failed. Couldn’t we call it a day?</p>
<p>As Kevin’s one-man theater dragged on, I thought about the long drive ahead of me, and began to understand why people make false confessions. It’s not because you don’t know the truth. It’s because the truth doesn’t matter. When the person across the table has all the power, what’s the point of arguing? It was pretty clear by then, a few hours into a test I’d already failed, that Kevin didn’t give a damn what was true. He wouldn’t be satisfied until I confessed. But fuck Kevin and his machine. I wasn’t confessing.</p>
<p>Kevin paused, and I thought we might finally be done until he asked about my mother’s murder: was there anything I hadn’t told him about <em>that</em>? It broke the spell. I dissociated in reverse, came fully into my body. My chest relaxed, my heart quieted, and I saw the situation clearly for the first time. Kevin was just some dickhead with a grift, doing a job based on lies for a government that practically invented them. Did I know anything about my mother’s murder? I’d spent five years writing about it. I was the world’s foremost authority on the subject. But I wasn’t telling Kevin. If he wanted to know, he could buy the fucking book.</p>
<p>I asked if I was free to leave. He shrugged. As I stood and went for the door, he asked if I would come back for another test with him if necessary. Sure, I said. I’d love to. It was the last thing I would ever say to Kevin, and I wanted to make sure it was a lie.</p>
<p>When I got home, I checked the internet forum and read a post by the guy who’d been sitting with me in the waiting room. He’d failed, too. His story was almost exactly the same as mine, except his test took twice as long. He must have tried harder than I did to tell the truth.</p>
<p>*</p>
<p>Three weeks later, my book came out, and I once again found myself answering questions. Whenever someone asked about truth, I’d say that I consulted the historical record when possible, but that the book was mostly based on memory. Sometimes, despite myself, that old cliche slipped out: <em>it’s my truth</em>. I’d think about the polygraph, Kevin’s endless questions, his assumption that a fixed, detectable truth existed in my memory.</p>
<p>Writing a book based on memory showed me it’s less a font of truth than a river of lies. We may tell ourselves stories in order to live, as Didion famously said, but nobody ever quotes the rest of that passage: “We live entirely, especially if we are writers, by the imposition of a narrative line upon disparate images, by the ‘ideas’ with which we have learned to freeze the shifting phantasmagoria which is our actual experience.” That sounds a lot like lying.</p>
<p>Even if we mean to tell the truth, the existing science suggests that memory is almost as unreliable as the polygraph. In 1885, the German psychologist Hermann Ebbinghaus did tests on himself and came up with his famed “forgetting curve,” a chart that showed we forget more than half of information within a few days. More recent studies have found that autobiographical memory—the deliberate recollection of facts, ideas, and experiences from one’s life—is not only inaccurate, but suggestible and frequently false. And factors such as depression and trauma, both psychological and physical, have been found to degrade autobiographical memory.</p>
<p>Then there’s the issue of stress, which also seems to have a range of effects on memory. Stress hormones like cortisol and adrenaline have been shown to aid in memory consolidation, which is, more or less, the process of storing recently learned information as memories for long-term recall. But those memories can change each time they’re remembered, through a process called reconsolidation: once accessed, the memory has to be rewritten, and it can be rewritten differently, revised just like a scene in a memoir. While stress may aid in memory consolidation, it has a profoundly negative effect on reconsolidation. Trauma has its own story to tell.</p>
<p><span>We’re all polygraphers, staring at the screens of our truth machines, proving ourselves right.</span></p><p>By the time I took the polygraph, my memory had already been rewritten. After five years of accessing and re-accessing memories of my mother and her death, I’d recently begun to understand how dangerous it is to write a book based primarily on memory. I don’t mean the truth: accuracy is overrated, not to mention impossible. The real danger is the sacrifice you have to make. By writing your memories, and rewriting them again and again, draft after draft, you replace them, erase them. By the time I finished my book, after revising every word half a dozen times, I didn’t remember my mother anymore. She was pages, scenes, sentences.</p>
<p>The polygraph works a lot like a memoir. It doesn’t find the truth, it creates it. First the exam makes you doubt or forget your memories. Then, by forcing you to re-access them again and again under stress, it literally rewrites them. Since my polygraph exam, I’ve believed that I did drugs between six and eight times before then, even though my rational mind knows that isn’t true. My experience of being polygraphed showed me that not only does the polygraph not detect lies, it manufactures them.</p>
<p>More than two million polygraph exams are given every year in America; it’s a two-billion-dollar industry. No other country in the world uses the poly to nearly the extent that we do, and most don’t use it at all. Why are we the only ones who build machines to detect the truth, and believe that they can, despite all evidence to the contrary? Why do we need to believe in a truth so simple it can be detected by a machine?</p>
<p>The lie detector is a myth. Everyone who doesn’t make money off of polygraphs agrees on that. But myths exist for a reason, to explain collective phenomena, to explain us to ourselves. Myths create a sense of community through shared belief. Judging by our obsession with lie detectors—and the fact that we continue to call them that, more than a century into this charade—the myth of a simple, detectable truth is one of the few beliefs most Americans do still share.</p>
<p>The lie detector came straight out of science fiction, and drifted into the realm of fact at the beginning of a century in which a succession of groundbreaking technologies would shatter and reshape our cultural conceptions of what was possible: Edison’s bulbs, Bell’s telephone, Ford’s mass-produced cars, the Wright Brothers’ airplane. By the time the polygraph came along, a credulous American public was used to tales of revolutionary, terrifying innovations that were going to change their lives forever. Some of those technologies were transforming reality itself.</p>
<p>Electricity was spreading across the country, quite literally changing the way people saw the world. So were telephone and radio access, which did the same for how we heard it. The polygraph’s siblings, cinema and psychology, were transforming the way people saw themselves. Meanwhile, Modernism—the artistic movement responding to those changes—was roiling the arts in every medium, revising our expressions of lived and imagined reality; the United States government banned James Joyce’s <em>Ulysses</em> the same year the polygraph debuted.</p>
<p>How it must have felt to be a human then. A man my age in 1921 might have remembered reading dime Westerns by candlelight as a boy, riding a horse to school, a time before voices—much less humans—could travel through the air. He’d seen the Progressive era rewrite the rules of society, including the victory of women’s suffrage the year before. He might have fought in the first World War. By the time the polygraph was invented, he may have owned a car or had access to a telephone, if he was wealthy. But he still got his news from newspapers or neighbors. He didn’t own a screen. Can we blame him for believing in a simple kind of truth that a new device could find?</p>
<p>A century later, I’m awash in new technologies, and deeply confused about what to believe. I was a young adult when smartphones arrived, in college when Facebook debuted, and dimly remember an early childhood before the internet changed everything. In the last few years, as of this writing, the COVID-19 pandemic has altered American life in ways nobody comprehends. Nobody trusts the media anymore, and appalling numbers of Americans refuse to believe in fundamental, verifiable facts. Meanwhile, conspiracies ricochet around the internet, gathering believers.</p>
<p>It’s hard not to see this moment as a parallel of the polygraph’s invention, another time when technology has done a number on whatever shared sense of truth America once had. That erosion has worsened our cultural and political divisions, which often hinge on what kind of truth we believe in: relative and constructed, or absolute and fixed. A century ago, the polygraph was born from the latter belief. But the notion of objective truth seems quaint and naïve in our era of fake news and algorithms, when the only truth that still exists is ours, a custom reality created for us and delivered to our devices. We’re all memoirists now, shouting our stories into the void. We’re all polygraphers, staring at the screens of our truth machines, proving ourselves right.</p>
<p>*</p>
<p>A few months after the exam, I got the official news: not only had I been found unsuitable for employment, I also had no right to appeal the decision and was barred from reapplying for a minimum of three years. The consequences of my polygraph exam were finally clear, the only real truth revealed in the whole process: I was never going to be a federal agent.</p>
<p>I wasn’t alone. I soon found out that Customs and Border Protection job applicants had a polygraph failure rate of 68.1 percent, more than double that of other law enforcement agencies. The CBP commissioner at the time said those statistics showed the polygraph was working and blamed the quality of the applicants. It struck me as a strange rhetorical strategy to suggest that his agency attracted applicants so much worse than those of any other law enforcement body in America, but what did I know: I was one of those applicants. One article described polygraph subjects being accused with no evidence of cheating on their wives and having cartel connections, in exams lasting eight hours or longer. Other law enforcement agencies called CBP’s conduct excessive, and Jeff Flake, then a Republican Senator from Arizona, suggested that operators were being forced to fail applicants to justify their own jobs.</p>
<p>A few months after my exam, I got an email officially notifying me of my failure. I replied to the email and asked for a copy of the polygraph report. The CBP representative told me I’d have to file a Freedom of Information Act request. I did. They ignored my request for four months, in violation of FOIA. When I threatened legal action, they denied my request for other reasons, both of which were lies.</p>
<p>I filed another FOIA request and was still waiting for a response when the Office of Personnel Management announced that it had suffered a data breach. The personnel files of millions of federal employees and applicants had been stolen by Chinese hackers. An investigation ensued, and the Inspector General accused OPM officials of lying about the hack. Multiple high-ranking officials, including the director of OPM, resigned in the aftermath.</p>
<p>Eventually I received a letter from the replacement director alerting me that I’d been affected by the hack. (In the letter, she admitted that her own background check had also been compromised, as if that was supposed to make me feel better.) The most sensitive imaginable document about my life—one that included all of my personal and financial information, as well as a complete history of my jobs, residences, and relationships—had been stolen. The government’s solution was to offer free credit monitoring to those affected.</p>
<p>As of this writing, it’s been more than a decade since my polygraph exam, and the government still hasn’t sent me a copy of the report. But at least now I know the truth is out there. It’s on a hard drive somewhere in China.</p>
<p>__________________________________</p>
<p><img fetchpriority="high" decoding="async" data-attachment-id="239683" data-permalink="https://lithub.com/what-the-all-american-delusion-of-the-polygraph-says-about-our-relationship-to-fact-and-fiction/45-2-cover-front-706x1024/" data-orig-file="https://s26162.pcdn.co/wp-content/uploads/2024/07/45-2-Cover-FRONT-706x1024-1.png" data-orig-size="706,1024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="45-2-Cover-FRONT-706×1024" data-image-description="" data-image-caption="" data-medium-file="https://s26162.pcdn.co/wp-content/uploads/2024/07/45-2-Cover-FRONT-706x1024-1-207x300.png" data-large-file="https://s26162.pcdn.co/wp-content/uploads/2024/07/45-2-Cover-FRONT-706x1024-1.png" src="https://s26162.pcdn.co/wp-content/uploads/2024/07/45-2-Cover-FRONT-706x1024-1-207x300.png" alt="" width="207" height="300" srcset="https://s26162.pcdn.co/wp-content/uploads/2024/07/45-2-Cover-FRONT-706x1024-1-207x300.png 207w, https://s26162.pcdn.co/wp-content/uploads/2024/07/45-2-Cover-FRONT-706x1024-1-41x60.png 41w, https://s26162.pcdn.co/wp-content/uploads/2024/07/45-2-Cover-FRONT-706x1024-1-34x50.png 34w, https://s26162.pcdn.co/wp-content/uploads/2024/07/45-2-Cover-FRONT-706x1024-1.png 706w" sizes="(max-width: 207px) 100vw, 207px"></p>
<p><em>“The Memoirist and the Lie Detector” by <span>Justin</span>&nbsp;<span>St</span>.&nbsp;</em><span><em>Germain appears in the latest issue of</em> <a href="https://www.nereview.com/vol-45-no-2-2024/" target="_blank">New England Review</a>.</span></p>
										
																				
																		
										<div id="about_the_author">
												<p><a href="https://lithub.com/author/justinstgermain1/"><img src="https://s26162.pcdn.co/wp-content/uploads/2024/07/Justin-St-Germain-courtesy-of-William-B-Bledsoe-100x100.jpg" width="100" height="100" alt="Justin St. Germain"></a></p>
												
											</div>

										
									</div></div>]]></description>
        </item>
    </channel>
</rss>