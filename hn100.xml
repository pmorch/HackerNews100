<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 05 Jul 2024 00:30:34 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Japan introduces enormous humanoid robot to maintain train lines (109 pts)]]></title>
            <link>https://www.theguardian.com/world/article/2024/jul/04/japan-train-robot-maintain-railway-lines</link>
            <guid>40877648</guid>
            <pubDate>Thu, 04 Jul 2024 20:02:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/article/2024/jul/04/japan-train-robot-maintain-railway-lines">https://www.theguardian.com/world/article/2024/jul/04/japan-train-robot-maintain-railway-lines</a>, See on <a href="https://news.ycombinator.com/item?id=40877648">Hacker News</a></p>
Couldn't get https://www.theguardian.com/world/article/2024/jul/04/japan-train-robot-maintain-railway-lines: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Batteries: How cheap can they get? (159 pts)]]></title>
            <link>https://aukehoekstra.substack.com/p/batteries-how-cheap-can-they-get</link>
            <guid>40877337</guid>
            <pubDate>Thu, 04 Jul 2024 19:20:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aukehoekstra.substack.com/p/batteries-how-cheap-can-they-get">https://aukehoekstra.substack.com/p/batteries-how-cheap-can-they-get</a>, See on <a href="https://news.ycombinator.com/item?id=40877337">Hacker News</a></p>
Couldn't get https://aukehoekstra.substack.com/p/batteries-how-cheap-can-they-get: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Insights from over 10,000 comments on "Ask HN: Who Is Hiring" using GPT-4o (193 pts)]]></title>
            <link>https://tamerc.com/posts/ask-hn-who-is-hiring/</link>
            <guid>40877136</guid>
            <pubDate>Thu, 04 Jul 2024 18:50:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tamerc.com/posts/ask-hn-who-is-hiring/">https://tamerc.com/posts/ask-hn-who-is-hiring/</a>, See on <a href="https://news.ycombinator.com/item?id=40877136">Hacker News</a></p>
Couldn't get https://tamerc.com/posts/ask-hn-who-is-hiring/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Gravitational wave researchers cast new light on Antikythera mechanism mystery (109 pts)]]></title>
            <link>https://www.gla.ac.uk/news/headline_1086643_en.html</link>
            <guid>40877042</guid>
            <pubDate>Thu, 04 Jul 2024 18:33:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gla.ac.uk/news/headline_1086643_en.html">https://www.gla.ac.uk/news/headline_1086643_en.html</a>, See on <a href="https://news.ycombinator.com/item?id=40877042">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img loading="lazy" src="https://www.gla.ac.uk/media/Media_1086641_smxx.jpg" alt="Antikythera mechanism in National Archaeological Museum, Athens, Greece." title="" width="650" height="433"></p><p>&nbsp;The mechanism was discovered in 1901 by divers exploring a sunken shipwreck near the Aegean island of Antikythera. Although the shoebox-sized mechanism had broken into fragments and eroded, it quickly became clear that it contained a complex series of gears which were unusually intricately-tooled. </p><p>&nbsp;Decades of subsequent research and analysis have established that the mechanism dates from the second century BCE and functioned as a kind of hand-operated mechanical computer. Exterior dials connected to the internal gears allowed users to predict eclipses and calculate the astronomical positions of planets on any given date with an accuracy unparalleled by any other known contemporary device.</p><p>&nbsp;In 2020, new X-ray images of one of the mechanism‚Äôs rings, known as the calendar ring, revealed fresh details of regularly spaced holes that sit beneath the ring. Since the ring was broken and incomplete, however, it wasn‚Äôt clear how just how many holes were there originally. Initial analysis by Antikythera researcher Chris Budiselic and colleagues suggested it was likely somewhere between 347 and 367.</p><p>&nbsp;Now, in a new paper published today in the <em>Horological Journal</em>, the Glasgow researchers describe how they used two statistical analysis techniques to reveal new details about the calendar ring. They show that the ring is vastly more likely to have had 354 holes, corresponding to the lunar calendar, than 365 holes, which would have followed the Egyptian calendar. The analysis also shows that 354 holes is hundreds of times more probable than a 360-hole ring, which previous research had suggested as a possible count. </p><p>&nbsp;Professor Graham Woan, of the University of Glasgow‚Äôs School of Physics &amp; Astronomy, is one of the authors of the paper. He said: ‚ÄúTowards the end of last year, a colleague pointed to me to data acquired by YouTuber Chris Budiselic, who was looking to make a replica of the calendar ring and was investigating ways to determine just how many holes it contained. </p><p>&nbsp;‚ÄúIt struck me as an interesting problem, and one that I thought I might be able to solve in a different way during the Christmas holidays, so I set about using some statistical techniques to answer the question.‚Äù</p><p>&nbsp;Professor Woan used a technique called Bayesian analysis, which uses probability to quantify uncertainty based on incomplete data, to calculate the likely number of holes in the mechanism using the positions of the surviving holes and the placement of the ring‚Äôs surviving six fragments. His results showed strong evidence that the mechanism‚Äôs calendar ring contained either 354 or 355 holes.</p><p>&nbsp;At the same time, one of Professor Woan‚Äôs colleagues at the University‚Äôs Institute for Gravitational Research, Dr Joseph Bayley, had also heard about the problem. He adapted techniques used by their research group to analyse the signals picked up by the LIGO gravitational wave detectors, which measure the tiny ripples in spacetime, caused by massive astronomical events like the collision of black holes, as they pass through the Earth, to scrutinise the calendar ring.</p><p>&nbsp;The Markov Chain Monte Carlo and nested sampling methods Woan and Bayley used provided a comprehensive probabilistic set of results, again suggested that the ring most likely contained 354 or 355 holes in a circle of radius 77.1mm, with an uncertainty of about 1/3 mm. It also reveals that the holes were precisely positioned with extraordinary accuracy, with an average radial variation of just 0.028mm between each hole.</p><p>&nbsp;Bayley, a co-author of the paper, is a research associate at the School of Physics &amp; Astronomy. He said: ‚ÄúPrevious studies had suggested that the calendar ring was likely to have tracked the lunar calendar, but the dual techniques we‚Äôve applied in this piece of work greatly increase the likelihood that this was the case. </p><p>&nbsp;‚ÄúIt‚Äôs given me a new appreciation for the Antikythera mechanism and the work and care that Greek craftspeople put into making it ‚Äì the precision of the holes‚Äô positioning would have required highly accurate measurement techniques and an incredibly steady hand to punch them.</p><p>&nbsp;Professor Woan added: ‚ÄúIt‚Äôs a neat symmetry that we‚Äôve adapted techniques we use to study the universe today to understand more about a mechanism that helped people keep track of the heavens nearly two millennia ago. </p><p>&nbsp;‚ÄúWe hope that our findings about the Antikythera mechanism, although less supernaturally spectacular than those made by Indiana Jones, will help deepen our understanding of how this remarkable device was made and used by the Greeks.‚Äù</p><p>&nbsp;The paper, titled ‚Äò<a href="https://bhi.co.uk/wp-content/uploads/2024/06/07-HJJuly24-AOTM-2.pdf">An Improved Calendar Ring Hole-Count for the Antikythera Mechanism: A Fresh Analysis</a>‚Äô, is published in <em>Horological Journal.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SCIM: Ncurses based, Vim-like spreadsheet (177 pts)]]></title>
            <link>https://github.com/andmarti1424/sc-im</link>
            <guid>40876848</guid>
            <pubDate>Thu, 04 Jul 2024 18:04:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/andmarti1424/sc-im">https://github.com/andmarti1424/sc-im</a>, See on <a href="https://news.ycombinator.com/item?id=40876848">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/andmarti1424/sc-im/dev/logo.png"><img src="https://raw.githubusercontent.com/andmarti1424/sc-im/dev/logo.png" alt="sc-im" height="25%" width="25%"></a>
</h2><a id="user-content---" aria-label="Permalink: " href="#--"></a></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">NOTE 06/01/2023:</h2><a id="user-content-note-06012023" aria-label="Permalink: NOTE 06/01/2023:" href="#note-06012023"></a></p>
<p dir="auto">This project needs some help.
This is a one person project and lost sponsoring in the last months. There are only just a few left. I want to still maintain and develop sc-im, but I am the only income in my family and its becoming difficult to work as much as I would want.
If you can make a donation (see at the bottom), please do. Your help would be really appreciated!!
Thanks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">sc-im</h2><a id="user-content-sc-im" aria-label="Permalink: sc-im" href="#sc-im"></a></p>
<p dir="auto">Spreadsheet Calculator Improvised, aka sc-im, is an ncurses based, vim-like spreadsheet calculator.</p>
<p dir="auto">sc-im is based on <a href="https://en.wikipedia.org/wiki/Sc_(spreadsheet_calculator)" rel="nofollow">sc</a>, whose original authors are James Gosling and Mark Weiser, and mods were later added by Chuck Martin.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Some of the features of sc-im</h2><a id="user-content-some-of-the-features-of-sc-im" aria-label="Permalink: Some of the features of sc-im" href="#some-of-the-features-of-sc-im"></a></p>
<ul dir="auto">
<li>Vim movements commands for editing cell content.</li>
<li>UNDO / REDO.</li>
<li>65.536 rows and 702 columns supported. (The number of rows can be expanded to 1.048.576 if wished).</li>
<li>CSV / TAB delimited / XLSX file import and export. ODS import. Markdown export.</li>
<li>Key-mappings.</li>
<li>Autobackup.</li>
<li>Direct color support - specifing the RGB values, screen colors can be customized by user, even at runtime.</li>
<li>Colorize cells or give them format such as bold, italic or underline.</li>
<li>Wide character support. The following alphabets are supported: English, Spanish, French, Italian, German, Portuguese, Russian, Ukrainian, Greek, Turkish, Czech, Japanese, Chinese.</li>
<li>Sort of rows.</li>
<li>Filter of rows.</li>
<li>Subtotals.</li>
<li>Cell shifting.</li>
<li>Clipboard support.</li>
<li>GNUPlot interaction.</li>
<li>Scripting support with LUA. Also with triggers and c dynamic linked modules.</li>
<li>Implement external functions in the language you prefer and use them in SC-IM.</li>
<li>Use SC-IM as a non-interactive calculator, reading its input from an external script.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>=</td>
<td>Insert a numeric value</td>
</tr>
<tr>
<td>\</td>
<td>Insert a text value</td>
</tr>
<tr>
<td>e</td>
<td>Edit a numeric value</td>
</tr>
<tr>
<td>E</td>
<td>Edit a string value</td>
</tr>
<tr>
<td>x</td>
<td>Delete current cell content</td>
</tr>
<tr>
<td>:q</td>
<td>Quit the app</td>
</tr>
<tr>
<td>:h</td>
<td>See help</td>
</tr>
<tr>
<td>:w filename.sc</td>
<td>Save current spreadsheet in sc format</td>
</tr>
<tr>
<td>j</td>
<td>Move down</td>
</tr>
<tr>
<td>k</td>
<td>Move up</td>
</tr>
<tr>
<td>h</td>
<td>Move left</td>
</tr>
<tr>
<td>l</td>
<td>Move right</td>
</tr>
<tr>
<td>goab12</td>
<td>go to cell AB12</td>
</tr>
<tr>
<td>u</td>
<td>undo last change</td>
</tr>
<tr>
<td>C-r</td>
<td>redo last change undone</td>
</tr>
<tr>
<td>yy</td>
<td>Copy current cell</td>
</tr>
<tr>
<td>v</td>
<td>select a range using cursor/hjkl keys</td>
</tr>
<tr>
<td>p</td>
<td>paste a previously yanked cell or range</td>
</tr>
<tr>
<td>ir</td>
<td>insert row</td>
</tr>
<tr>
<td>ic</td>
<td>insert column</td>
</tr>
<tr>
<td>dr</td>
<td>delete row</td>
</tr>
<tr>
<td>dc</td>
<td>delete column</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/andmarti1424/sc-im/blob/main/screenshots/scim6.png?raw=true"><img src="https://github.com/andmarti1424/sc-im/raw/main/screenshots/scim6.png?raw=true" alt="demo image"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/andmarti1424/sc-im/blob/main/screenshots/scim-plot-graph.gif?raw=true"><img src="https://github.com/andmarti1424/sc-im/raw/main/screenshots/scim-plot-graph.gif?raw=true" alt="demo image" data-animated-image=""></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/andmarti1424/sc-im/blob/main/screenshots/scim5.png?raw=true"><img src="https://github.com/andmarti1424/sc-im/raw/main/screenshots/scim5.png?raw=true" alt="demo image"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/andmarti1424/sc-im/blob/main/screenshots/scim4.png?raw=true"><img src="https://github.com/andmarti1424/sc-im/raw/main/screenshots/scim4.png?raw=true" alt="demo image"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/andmarti1424/sc-im/blob/main/screenshots/scimp2.png?raw=true"><img src="https://github.com/andmarti1424/sc-im/raw/main/screenshots/scimp2.png?raw=true" alt="demo image"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/andmarti1424/sc-im/blob/main/screenshots/scimp3.png?raw=true"><img src="https://github.com/andmarti1424/sc-im/raw/main/screenshots/scimp3.png?raw=true" alt="demo image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dependencies</h3><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Requirements:</p>
<ul dir="auto">
<li><code>ncurses</code> (best if compiled with wide chars support)</li>
<li><code>bison</code> or <code>yacc</code></li>
<li><code>gcc</code></li>
<li><code>make</code></li>
<li><code>pkg-config</code> and <code>which</code> (for make to do its job)</li>
</ul>
</li>
<li>
<p dir="auto">Optionals:</p>
<ul dir="auto">
<li><code>tmux</code> / <code>xclip</code> / <code>pbpaste</code> (for clipboard copy/paste)</li>
<li><code>gnuplot</code> (for plots)</li>
<li><code>libxlsxreader</code> (for xls support)</li>
<li><code>xlsxwriter</code> (for xlsx export support)</li>
<li><code>libxml-2.0</code> and <code>libzip</code> (for xlsx/ods import support)</li>
<li><code>lua</code> (for Lua scripting)</li>
<li>threads support (in case you want to test this in Minix, just disable autobackup and HAVE_PTHREAD)</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual</h3><a id="user-content-manual" aria-label="Permalink: Manual" href="#manual"></a></p>
<ul dir="auto">
<li>Edit <a href="https://github.com/andmarti1424/sc-im/blob/main/src/Makefile"><code>src/Makefile</code></a> according to your system and needs:</li>
</ul>

<ul dir="auto">
<li>Run <code>make</code>:</li>
</ul>

<ul dir="auto">
<li>Optional: You can install the binary <code>sc-im</code> in your system by typing with a privileged user:</li>
</ul>

<p dir="auto"><h3 tabindex="-1" dir="auto">Building on OS X</h3><a id="user-content-building-on-os-x" aria-label="Permalink: Building on OS X" href="#building-on-os-x"></a></p>
<p dir="auto">You can follow the instructions as above, but if you would like Lua scripting
support, you will need to install Lua 5.1, which you can do with,</p>

<p dir="auto">And then follow the instructions as above.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebrew for OSX users</h3><a id="user-content-homebrew-for-osx-users" aria-label="Permalink: Homebrew for OSX users" href="#homebrew-for-osx-users"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Ubuntu with XLSX import &amp; export</h3><a id="user-content-ubuntu-with-xlsx-import--export" aria-label="Permalink: Ubuntu with XLSX import &amp; export" href="#ubuntu-with-xlsx-import--export"></a></p>
<p dir="auto">See <a href="https://github.com/andmarti1424/sc-im/wiki/Ubuntu-with-XLSX-import-&amp;-export">this wiki page</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Other distros / OS</h3><a id="user-content-other-distros--os" aria-label="Permalink: Other distros / OS" href="#other-distros--os"></a></p>
<p dir="auto">Please check <a href="https://github.com/andmarti1424/sc-im/wiki/">wiki pages</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The <code>scimrc</code> file can be used to configure <code>sc-im</code>. The file should be placed in the <code>~/.config/sc-im</code> directory.</p>
<p dir="auto">Here is an example <code>~/.config/sc-im/scimrc</code> :</p>
<div data-snippet-clipboard-copy-content="set autocalc
set numeric
set numeric_decimal=0
set overlap
set xlsx_readformulas"><pre><code>set autocalc
set numeric
set numeric_decimal=0
set overlap
set xlsx_readformulas
</code></pre></div>
<p dir="auto">Other configuration variables are listed in the <a href="https://raw.githubusercontent.com/andmarti1424/sc-im/freeze/src/doc" rel="nofollow">help file</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Issues and questions</h3><a id="user-content-issues-and-questions" aria-label="Permalink: Issues and questions" href="#issues-and-questions"></a></p>
<p dir="auto">Please open an issue if you find a bug.
If you are now sure if its a bug, please take a look at the discussions and/or ask there.
If you have a question please check out current discussions and if you still are in doubt, open a discussion as well.
If you want to ask for a feature request, the same, check out current discussions.
Thank you!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tutorial</h3><a id="user-content-tutorial" aria-label="Permalink: Tutorial" href="#tutorial"></a></p>
<p dir="auto"><a href="https://github.com/jonnieey/Sc-im-Tutorial">sc-im tutorial</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Related projects</h3><a id="user-content-related-projects" aria-label="Permalink: Related projects" href="#related-projects"></a></p>
<ul dir="auto">
<li><a href="https://github.com/mipmip/vim-scimark">vim-scimark</a> - Vim plugin, edit embedded markdown tables with sc-im in vim terminal.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Helping us</h3><a id="user-content-helping-us" aria-label="Permalink: Helping us" href="#helping-us"></a></p>
<p dir="auto">Want to help?  You can help us with one or more of the following:</p>
<ul dir="auto">
<li>giving sc-im a star on GitHub</li>
<li>taking screenshots / creating screencasts showing sc-im</li>
<li>making a donation (see below).</li>
<li>telling if you use it / like it. I really don't have a clue if this app is used by someone.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Donations</h3><a id="user-content-donations" aria-label="Permalink: Donations" href="#donations"></a></p>
<p dir="auto">If you like sc-im please support its development by making a DONATION with Patreon or PayPal.
It would really help a lot.</p>
<a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=U537V8SNQQ45J" rel="nofollow">
<img src="https://camo.githubusercontent.com/141ac4e931e15772077ad435ddf38b274bbd148d451af0b7849074db1534f6a0/68747470733a2f2f7777772e70617970616c6f626a656374732e636f6d2f656e5f55532f692f62746e2f62746e5f646f6e6174655f4c472e676966" data-animated-image="" data-canonical-src="https://www.paypalobjects.com/en_US/i/btn/btn_donate_LG.gif">
</a>
<p dir="auto">If you wish to make a donation, please click the above button or just send money to <a href="mailto:scim.spreadsheet@gmail.com">scim.spreadsheet@gmail.com</a> via PayPal, choosing "Goods and Services".
or with Patreon.</p>
<p dir="auto">Thank you!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Who Wants a Penpal? (112 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40876568</link>
            <guid>40876568</guid>
            <pubDate>Thu, 04 Jul 2024 17:25:27 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40876568">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=40876568: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mechanical computer relies on kirigami cubes, not electronics (118 pts)]]></title>
            <link>https://news.ncsu.edu/2024/06/kirigami-mechanical-computer/</link>
            <guid>40875924</guid>
            <pubDate>Thu, 04 Jul 2024 16:03:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.ncsu.edu/2024/06/kirigami-mechanical-computer/">https://news.ncsu.edu/2024/06/kirigami-mechanical-computer/</a>, See on <a href="https://news.ycombinator.com/item?id=40875924">Hacker News</a></p>
Couldn't get https://news.ncsu.edu/2024/06/kirigami-mechanical-computer/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The sad state of property-based testing libraries (105 pts)]]></title>
            <link>https://stevana.github.io/the_sad_state_of_property-based_testing_libraries.html</link>
            <guid>40875559</guid>
            <pubDate>Thu, 04 Jul 2024 15:15:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stevana.github.io/the_sad_state_of_property-based_testing_libraries.html">https://stevana.github.io/the_sad_state_of_property-based_testing_libraries.html</a>, See on <a href="https://news.ycombinator.com/item?id=40875559">Hacker News</a></p>
Couldn't get https://stevana.github.io/the_sad_state_of_property-based_testing_libraries.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Twilio Confirms Data Breach After Hackers Leak 33M Authy User Phone Numbers (412 pts)]]></title>
            <link>https://www.securityweek.com/twilio-confirms-data-breach-after-hackers-leak-33m-authy-user-phone-numbers/</link>
            <guid>40874341</guid>
            <pubDate>Thu, 04 Jul 2024 12:26:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.securityweek.com/twilio-confirms-data-breach-after-hackers-leak-33m-authy-user-phone-numbers/">https://www.securityweek.com/twilio-confirms-data-breach-after-hackers-leak-33m-authy-user-phone-numbers/</a>, See on <a href="https://news.ycombinator.com/item?id=40874341">Hacker News</a></p>
Couldn't get https://www.securityweek.com/twilio-confirms-data-breach-after-hackers-leak-33m-authy-user-phone-numbers/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Jeffrey Snover and the Making of PowerShell (231 pts)]]></title>
            <link>https://corecursive.com/building-powershell-with-jeffrey-snover/</link>
            <guid>40874013</guid>
            <pubDate>Thu, 04 Jul 2024 11:26:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corecursive.com/building-powershell-with-jeffrey-snover/">https://corecursive.com/building-powershell-with-jeffrey-snover/</a>, See on <a href="https://news.ycombinator.com/item?id=40874013">Hacker News</a></p>
Couldn't get https://corecursive.com/building-powershell-with-jeffrey-snover/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Region-specific Machines pricing (102 pts)]]></title>
            <link>https://community.fly.io/t/fresh-produce-region-specific-machines-pricing/20690</link>
            <guid>40873001</guid>
            <pubDate>Thu, 04 Jul 2024 07:17:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.fly.io/t/fresh-produce-region-specific-machines-pricing/20690">https://community.fly.io/t/fresh-produce-region-specific-machines-pricing/20690</a>, See on <a href="https://news.ycombinator.com/item?id=40873001">Hacker News</a></p>
Couldn't get https://community.fly.io/t/fresh-produce-region-specific-machines-pricing/20690: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Finding near-duplicates with Jaccard similarity and MinHash (163 pts)]]></title>
            <link>https://blog.nelhage.com/post/fuzzy-dedup/</link>
            <guid>40872438</guid>
            <pubDate>Thu, 04 Jul 2024 05:04:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.nelhage.com/post/fuzzy-dedup/">https://blog.nelhage.com/post/fuzzy-dedup/</a>, See on <a href="https://news.ycombinator.com/item?id=40872438">Hacker News</a></p>
Couldn't get https://blog.nelhage.com/post/fuzzy-dedup/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Building a data compression utility in Haskell using Huffman codes (184 pts)]]></title>
            <link>https://lazamar.github.io/haskell-data-compression-with-huffman-codes/</link>
            <guid>40872332</guid>
            <pubDate>Thu, 04 Jul 2024 04:29:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lazamar.github.io/haskell-data-compression-with-huffman-codes/">https://lazamar.github.io/haskell-data-compression-with-huffman-codes/</a>, See on <a href="https://news.ycombinator.com/item?id=40872332">Hacker News</a></p>
Couldn't get https://lazamar.github.io/haskell-data-compression-with-huffman-codes/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The saddest "Just Ship It" story ever (2020) (231 pts)]]></title>
            <link>https://www.kitze.io/posts/saddest-just-ship-it-story-ever</link>
            <guid>40872182</guid>
            <pubDate>Thu, 04 Jul 2024 03:50:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kitze.io/posts/saddest-just-ship-it-story-ever">https://www.kitze.io/posts/saddest-just-ship-it-story-ever</a>, See on <a href="https://news.ycombinator.com/item?id=40872182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I know, I know, at this point you want "Just Ship It" to be an actual person so you could punch it in the face. As an Indie Maker, that sentence can be super frustrating because it's tired, it's cliche, and your response is always "HEY! You don't understand... it's not that easy".  </p>&nbsp;<p>I agree, it's not easy, but it's always the right thing to do. </p>&nbsp;<p>Here's why.  </p>&nbsp;<p>I started building an app on 01.01.2018. It was New Year's Eve and we just had the crappiest night ever. Yes, imagine a night <em>so bad</em> that at midnight you decide "you know what, fuck it, I'm gonna work on WEB DEVELOPMENT".<br>That bad.  </p>&nbsp;<p>The MVP was ready in a few days. I'm not that good of a coder, it's just a simple app. The 0.0.1 alpha version was more than ready. I could've released it, share it with a couple of people, and call it a day. I could've done that with every single version that I made, at any point from 2018 until now. I just wanted to add one more thing. One more feature. Just this one more thing and people will like it. One more screen and everything is gonna make sense. I swear, just this one last thing and it's ready.  </p>&nbsp;<p>BAM, last moment decision from the world's biggest dumbass:
"People wouldn't use this if it doesn't have a proper native mobile app for it. Time to learn React Native and spend a few months on that ü§¶Ô∏è"   </p>&nbsp;<p>God, if time machines were real, past-Kitze would be shoved in a toilet so hard right now. </p>&nbsp;<p>After 2 years of development, juggling between the fucking horror that's the web platform, React Native, Expo, GraphQL, bitching about how there's no ideal tech stack, the good old jQuery and Filezilla days, switching to other projects, releasing <a target="_blank" rel="noopener " href="https://sizzy.co/">other apps</a>, losing passion, finding passion, coming back to the app, etc. etc. etc...  </p>&nbsp;<p>I just dropped it.  </p>&nbsp;<p>I was still using it but I stopped developing it and just dropped the idea of releasing the app, ever.</p>&nbsp;<p>After a while, I was using it, but I realized that I'm missing a lot of features, so I'd either have to go back to developing it, or I'd have to find an alternative.   </p>&nbsp;<p>And boy did I find one.  </p>&nbsp;<p>I was scrolling their landing page and I was happy and furious at the same time. Someone solved the problem that I was solving. It was like someone literally read my mind and started coding. WHAT.  </p>&nbsp;<p>I have previously sent a video of my app to a couple of people (closest I came to shipping it) so I started getting suspicious if someone actually shared the video of my app with these people because they were solving literally the same problem, and they most of the features that I had.  </p>&nbsp;<p>I started getting this overwhelming happy, sad, and panicky feeling. I literally cannot explain how I felt while scrolling their page.</p>&nbsp;<p>One moment I am scrolling their list of features giggling like a little kid with a 48$ bill in a candy store (yea I know 48$ bills don't make sense, but JavaScript doesn't make sense and you're still using it), one moment I want to find these people and THROW THEM IN A PIT OF LIONS.   </p>&nbsp;<p>FUCK.    </p>&nbsp;<p>It's not their fault. I was just slow. I didn't ship on time. I'm gonna go ahead and tattoo "JUST SHIP IT" on my forehead. Nah I wouldn't be able to see it there. On my arm maybe. Nvm, let me go back to scrolling their landing page.  </p>&nbsp;<p>Fuck. They have solved everything that I wanted to solve, and WAY more. Hey, maybe I should be happy? I don't have to code anymore. Yay!? No more web platform? BLISS. Oh crap... the world is never gonna see my app though. But at least I don't have to see React Native anymore. NICE! Wait... BUT I WASTED SO MUCH TIME ON IT. FUCK.  A bunch of mixed feelings.  </p>&nbsp;<p>Here comes the saddest part, so grab a pack of tissues.   </p>&nbsp;<p>After a little bit of hesitation, I made an account. I watched the videos in their help center. Every time I caught myself smiling about a clever way they implemented something I slapped myself. NO. Bad Kitze. You shouldn't like this. THEY'RE COMPETITORS. <em>sigh</em> Sure buddy, whatever you tell yourself. Competitors to a shitty codebase sitting on your hard drive.  </p>&nbsp;<p>For 2 frickin' years, I thought it's too early to release my app because it's clunky, buggy, it's missing features, blah, blah, blah. No one would ever use it, right? I was so wrong.  </p>&nbsp;<p>I started using their app.  </p>&nbsp;<p>Even though they were working on it for the past few years it's still slow, buggy, and super unpolished, it doesn't matter, because they shipped.   </p>&nbsp;<p>Their mobile app is terrible and it needs 10 seconds to sync. It doesn't matter, they shipped. And I'm looking forward to every single update they release.</p>&nbsp;<p>Their backlog of things to do is huge, but it doesn't matter, they ship every single week, and the app is growing along with the community.  </p>&nbsp;<p>"But Kitze, even though tHeY sHiPpEd no one would pay for something unpolished and broken, right?"  </p>&nbsp;<p>Oh, Indie Hackers. So clever, yet so naive.  </p>&nbsp;<p>Today my 30-day trial has expired. A tear rolled down my cheek for every single digit of my credit card that I entered in their app. I am officially not only a subscriber, but also a fan. Every time I'll get a payment notification it's gonna feel like stepping on a lego ... glued to a knife. My bank might as well change the notification from "You have paid 5$ to ThatCompany" to "You never shipped, loser".   </p>&nbsp;<p>My app is officially dead.  </p>&nbsp;<p>99% of you are in the same boat right now, but hopefully just a few weeks into your project. Don't be a dumbass like me. Take a breath, roll your eyes at the cliche saying, but please...  </p>&nbsp;<p>Just Ship It.  </p>&nbsp;<p>P.S I would totally release and show you my app but ... it's not ready yet.  </p>&nbsp;<hr><p>Thanks for reading this! I wrote another article about <a target="_blank" rel="noopener " href="https://kitze.io/posts/github-stars-wont-pay-your-rent">my successful shipping story</a>, so I'd appreciate if you give it a read!</p>&nbsp;</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Vision Pro owners, are you still using it? (142 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40872102</link>
            <guid>40872102</guid>
            <pubDate>Thu, 04 Jul 2024 03:30:24 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40872102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="40872203"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872203" href="https://news.ycombinator.com/vote?id=40872203&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Yes using it regularly multiple times a week.</p><p>I watch tons of media on it. My background was in film production and I firmly believe this is the best current way to watch films at home , as long as you don‚Äôt mind doing so alone.</p><p>The only better experience visually is a laser projector with active shutter glasses. I literally exclaimed out loud when I saw some of my shots on here for the first time. Depth for stereo movies adds so much, but you lose so much vibrancy and light with passive glasses. This solves both issues. I get why James Cameron said it was a religious experience. For fellow film makers, this is the highest quality way that I‚Äôve experienced my own work.</p><p>It also is probably the only place at home to experience these movies at that quality. Nobody else has 4k 3D HDR with HFR. Nobody.</p><p>So as a previous film buff, it‚Äôs worth it alone for me for that.</p><p>However I also use it for work regularly. I join industry meetings with it, I multitask regularly. I spend more time on the couch working off my laptop with this as my screen now.</p><p>The passthrough and eyesight features have been surprisingly great for being with my family. While people think it‚Äôs sad that I‚Äôm doing my own thing in the headset, the reality is that we all do our own hobbies in the evening after work. I can now spend that time with my partner and interact with them while they do their thing.</p><p>I think it‚Äôll take a while for Apple and the app developers to really get into the swing of things, but it‚Äôs been a huge, positive change for me.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872241"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872241" href="https://news.ycombinator.com/vote?id=40872241&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>That‚Äôs a great insight.</p><p>I have the Vision Pro and do like the quality and sound from my Sony a90j and Sonos system a bit more, but the size of the screen in the Vision Pro is amazing</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40872306"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872306" href="https://news.ycombinator.com/vote?id=40872306&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I‚Äôm still using Vision Pro for about 10-15 hours each week, but the bulk of that is spent mirroring my Mac or having focused writing time using the Obsidian iPad app‚Äîthere really aren‚Äôt apps that take full advantage of the platform yet (and I don‚Äôt watch a lot of movies). Still, it‚Äôs been the best way to stay productive away from my desk. The launch was a bit rocky with bugs and missing features, but the recent updates to mirroring and keyboard/mouse support are starting to hint that Apple is focusing in on productivity as a first-class use case. I‚Äôm okay paying the premium knowing that this platform has the potential to keep heading in this direction. It feels like the hardware has a decent amount of headroom.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872196"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872196" href="https://news.ycombinator.com/vote?id=40872196&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>I sent it back before the return deadline, would have considered keeping it if it had supported showing more than one display of my MacBook. I know by now they've released some sort of a "very wide display" in a VisionOS update, but back then it didn't really make sense. I thought, ok, I'll just try using one Mac desktop and all the other apps, messages &amp; browser would be the Vision OS native ones open side by side. Then it turned out I had to connect my bluetooth mouse/keyboard to the Vision device instead (if I wanted to type something into the browser/Messages) and it was too much friction.</p><p>I did like the brief period of working (or just browsing stuff) while lying on the couch, but I knew from the beginning that I didn't need an even lazier position for staring at a screen all day.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872176"><td></td></tr>
            <tr id="40872180"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872180" href="https://news.ycombinator.com/vote?id=40872180&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I bought and returned mine. Without decent window management integration in OSX it feels limited to only "fun" use cases. If it were able to pair with a Bluetooth keyboard and sync to my MacBook with many screens... I would buy one again immediately.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872215"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872215" href="https://news.ycombinator.com/vote?id=40872215&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>It does pair with a Bluetooth keyboard and VisionOS 2 introduces an ultrawide monitor mode that‚Äôs equivalent to two displays, fwiw.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872251"><td></td></tr>
                        <tr id="40872161"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872161" href="https://news.ycombinator.com/vote?id=40872161&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Interesting this is top of HN but no comments.</p><p>A lot more folks interested in the answer than able to answer perhaps.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872183"><td></td></tr>
            <tr id="40872179"><td></td></tr>
                <tr id="40872279"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40872279" href="https://news.ycombinator.com/vote?id=40872279&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>Also, importantly it‚Äôs had a very limited release. US only till this last week. Supposedly only 450k units available this year in total, which in the grand scheme is a very low distribution likelihood among a given demographic.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872211"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40872211" href="https://news.ycombinator.com/vote?id=40872211&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Truly, just too many other things I would rather spend $4000 on than a first generation Apple product. Hopefully the next revision can start at $2000, more palatable.</p><p>Something like the Quest link cable and SteamVR support would also be tremendous, but I'm not holding my breath on that.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40872187"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872187" href="https://news.ycombinator.com/vote?id=40872187&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I use it as a fancy monitor that I can strap to my face and fits in a suitcase. sometimes I want to work lying down and it's great for that. It being based on ipados makes it kinda useless aside from the display.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872247"><td></td></tr>
            <tr id="40872245"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872245" href="https://news.ycombinator.com/vote?id=40872245&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I've mostly been using mine to watch TV/movies, though when I get some extra time I've been meaning to try the AVLR port that makes it work with SteamVR as a VR headset with joycons serving as controllers.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872261"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872261" href="https://news.ycombinator.com/vote?id=40872261&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>Yes, but in a fairly specific niche. I've found that the primary use case is for travel. It is really quite compact if you disconnect the straps. Being able to have the functional equivalent to desktop monitors is really quite handy for doing work on the road. It does struggles when not using a laptop with it despite having tried to make that work a couple times.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872262"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872262" href="https://news.ycombinator.com/vote?id=40872262&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Yes, but I'll use it a lot more when 2.0 comes out, so I can see my keyboard in environments, which is my biggest complaint.</p><p>Mostly it's the best cinema screen I've ever viewed in my life. "Avatar 2", in 3D and at 48fps, is an absolutely stunning viewing experience. I wish high-framerate movies were more common. They look incredible.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872178"><td></td></tr>
                <tr id="40872194"><td></td></tr>
                <tr id="40872204"><td></td></tr>
            <tr id="40872207"><td></td></tr>
                <tr id="40872253"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_40872253" href="https://news.ycombinator.com/vote?id=40872253&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>It generally includes the desk and everything that's on it, the chair, and the whole area surrounding the desk, and sometimes the whole room.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="40872269"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872269" href="https://news.ycombinator.com/vote?id=40872269&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Nope I have not been using mind. It is sitting next to my Oculus Quest now.</p><p>I wish there were more interesting things to do in it.</p><p>Honestly having that strapped to my head for any long thing (such as a movie) is a bit much. It's also heavy-ish and the field of view is still quite limited.</p><p>But most importantly there is really not much to do in it once the novelty wears off.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872288"><td></td></tr>
            <tr id="40872217"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872217" href="https://news.ycombinator.com/vote?id=40872217&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>I hope Apple makes a comparison product to Meta Ray Bans which i use daily.  Though today using my Ray Bans today and asking it questions got tedious and annoying as the reliability of "Hey Meta," working is 60 to 65 percent of the time.  I did wish for a disply to get the info instead of having to ask questions and intently listen.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872189"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872189" href="https://news.ycombinator.com/vote?id=40872189&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Huge VR fan here. I tried the demo in the Apple Store. It was cool and all, but nothing game changing. From a company like Apple - somewhat disappointing even. The technology just isn't there yet for a compelling product.</p><p>Apple should wait until they have this in a glasses form factor before hyping it up any more.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872228"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872228" href="https://news.ycombinator.com/vote?id=40872228&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>Yes. That's what Carmack said when he quit Oculus. The headgear has to get down to swim goggle size to get any traction, and eyeglass size to go mainstream.</p><p>Apple got it down from a brick on your head to a half-brick, but that's not good enough.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872284"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_40872284" href="https://news.ycombinator.com/vote?id=40872284&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>It doesn‚Äôt seem like they were even prioritizing size and weight. They made it out of heavy glass and aluminum, and sacrificed size to add the front-facing screen.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="40872220"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40872220" href="https://news.ycombinator.com/vote?id=40872220&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div>
                  <p>For all the people laughing at the Vision Pro and saying it's useless, just remember that when the first iPhone came out people were saying the same thing. It's often the second or third generation of a product where it really starts to shine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="40872327"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872327" href="https://news.ycombinator.com/vote?id=40872327&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>I‚Äôm happy to wait and see what they come up with. Tempted to get a quest in the meantime. AVP was too heavy, quest a bit too limiting.</p><p>I‚Äôll say that I was very happy with my G1 instead of an iPhone back in the day.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40872264"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40872264" href="https://news.ycombinator.com/vote?id=40872264&amp;how=up&amp;goto=item%3Fid%3D40872102"></a></center>    </td><td><br><div><p>There are also plenty of products that just aren‚Äôt that good or useful from the beginning and it doesn‚Äôt matter how many revisions it gets.</p><p>Also, Apples version is on its first generation but it‚Äôs not like VR as a whole is, ‚Äúmodern‚Äù VR has been around for a decade.</p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sans-IO: The secret to effective Rust for network services (202 pts)]]></title>
            <link>https://www.firezone.dev/blog/sans-io</link>
            <guid>40872020</guid>
            <pubDate>Thu, 04 Jul 2024 03:05:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.firezone.dev/blog/sans-io">https://www.firezone.dev/blog/sans-io</a>, See on <a href="https://news.ycombinator.com/item?id=40872020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>At Firezone, we use Rust<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup> to build secure remote access that scales, be it
from your Android phone, MacOS computer or Linux server. At the core of each app
sits a connectivity library ‚Äî aptly named
<a href="https://www.github.com/firezone/firezone/tree/main/rust/connlib"><code>connlib</code></a>
‚Äî that manages network connections and WireGuard tunnels to secure your
traffic. After several iterations, we‚Äôve landed on a design that we are
extremely happy with. It gives us fast and exhaustive tests, deep customisation
and overall high assurance that it does what we want it to do.</p>
<p><code>connlib</code> is built in Rust and the design we are talking about is known as
sans-IO. Rust's premise of speed and memory-safety makes it a great choice for
building network services. Most parts of our Rust stack aren't particularly
surprising: We use the <code>tokio</code> runtime for asynchronous tasks, <code>tungstenite</code> for
WebSockets, <code>boringtun</code> for the WireGuard implementation, <code>rustls</code> to encrypt
traffic with the API, etc. Yet, once you go beneath the surface of the library,
you will discover something that is perhaps unusual: There are almost no calls
to <code>tokio::spawn</code>, all communication is multiplexed via a single UDP socket and
the same APIs appear to repeat themselves across various layers:
<code>handle_timeout</code>, <code>poll_transmit</code>, <code>handle_input</code>, and so on.</p>
<p>These are the tell-tale signs of a sans-IO design. Instead of sending and
receiving bytes via a socket in multiple places, our protocols are implemented
as pure state machines. Even time is abstracted away: every function that needs
to know the current time receives an <code>Instant</code> parameter instead of calling
<code>Instant::now</code> itself. This pattern isn't something that we invented! The Python
world even has a dedicated <a href="https://sans-io.readthedocs.io/">website</a> about it.
In Rust, it is used by libraries such as:</p>
<ul>
<li><a href="https://github.com/quinn-rs/quinn/tree/main/quinn-proto"><code>quinn</code></a>, an
independent QUIC implementation.</li>
<li><a href="https://github.com/cloudflare/quiche/tree/master/quiche"><code>quiche</code></a>,
cloudflare's QUIC implementation.</li>
<li><a href="https://github.com/algesten/str0m"><code>str0m</code></a>, a sans-IO WebRTC implementation.</li>
</ul>
<p>In this post, we'll go over some of the problems with doing IO the traditional
way, followed by transitioning that to a sans-IO design and the reasons why we
think it is a good idea. As it turns out, Rust lends itself particularly well to
this pattern.</p>
<h2 id="rusts-async-model--the-function-colouring-debate"><a href="#rusts-async-model--the-function-colouring-debate">Rust's async model &amp; the "function colouring" debate</a></h2>
<p>If you've been around the Rust space for a while, you will have likely come
across the "function colouring" debate. In a nutshell, it discusses the
constraint that async functions can only be called from other async functions,
thus "colouring" them. There are various takes on this but what stands out for
me is that the ability to suspend execution and resume later is a pretty
important part of function's API contract. The fact that Rust enforces this at
compile-time is a good thing.</p>
<p>A result of this constraint is that an async function deep down in your stack
"forces" every calling function to also become async in order to <code>.await</code> the
inner function. This can be problematic if the code you want to call isn't
actually yours but a dependency that you are pulling in.</p>
<p>Some people see this as a problem, and they would like to write code that is
agnostic over the "asyncness" of their dependencies. That concern has merit.
Ultimately, at the very bottom of each async call stack sits a <code>Future</code> that
needs to suspend on something. Usually, this is some form of IO, like writing to
a socket, reading from a file, waiting for time to advance, etc. The majority of
async functions however don't actually perform async work themselves. Instead,
they are only async because they depend on other async functions. The code
around those inner async functions would usually also work in a blocking
context, but the author of your dependency happened to pick the async variant.</p>
<p>Let's look at an example of this problem. Firezone's connectivity library
<code>connlib</code> uses <a href="https://datatracker.ietf.org/doc/html/rfc8445">ICE</a> for NAT
traversal and as part of that, we utilise STUN to discover our server-reflexive
candidate, i.e. our public address. STUN is a binary message format and a STUN
binding is a pretty simple protocol: Send a UDP packet to server, server notes
the IP + port it sees as the sending socket and send a UDP packet back
containing that address.</p>
<p>Here is how we could implement this using <code>tokio</code>'s <code>UdpSocket</code> (thank you to
Cloudflare for the public STUN server):</p>
<div><pre><code><span>#[tokio::main]</span>
<span>async</span> <span>fn</span> <span>main</span>() <span>-&gt;</span> anyhow::<span>Result</span>&lt;()&gt; {
    <span>let</span> <span>socket</span> = UdpSocket::<span>bind</span>(<span>"0.0.0.0:0"</span>).<span>await</span>?;
    socket.<span>connect</span>(<span>"stun.cloudflare.com:3478"</span>).<span>await</span>?;
    socket.<span>send</span>(&amp;<span>make_binding_request</span>()).<span>await</span>?;

    <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[<span>0u8</span>; <span>100</span>];
    <span>let</span> <span>num_read</span> = socket.<span>recv</span>(&amp;<span>mut</span> buf).<span>await</span>?;
    <span>let</span> <span>address</span> = <span>parse_binding_response</span>(&amp;buf[..num_read]);

    <span>println!</span>(<span>"Our public IP is: {address}"</span>);

    <span>Ok</span>(())
}
</code></pre></div>
<p>This could be also be written using blocking IO from the standard library:</p>
<div><pre><code><span>fn</span> <span>main</span>() <span>-&gt;</span> anyhow::<span>Result</span>&lt;()&gt; {
    <span>let</span> <span>socket</span> = UdpSocket::<span>bind</span>(<span>"0.0.0.0:0"</span>)?;
    socket.<span>connect</span>(<span>"stun.cloudflare.com:3478"</span>)?;
    socket.<span>send</span>(&amp;<span>make_binding_request</span>())?;

    <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[<span>0u8</span>; <span>100</span>];
    <span>let</span> <span>num_read</span> = socket.<span>recv</span>(&amp;<span>mut</span> buf)?;
    <span>let</span> <span>address</span> = <span>parse_binding_response</span>(&amp;buf[..num_read]);

    <span>println!</span>(<span>"Our public IP is: {address}"</span>);

    <span>Ok</span>(())
}
</code></pre></div>
<div><p>You can find all of these snippets as working programs in the following
repository: <a href="https://github.com/firezone/sans-io-blog-example">https://github.com/firezone/sans-io-blog-example</a>.</p></div>
<p>Notice how this code is virtually identical apart from the use of <code>async</code>? If we
wanted to write a library that allows you to perform STUN, we'd have to decide
on one of them or include both. There are lots of opinions out there as to what
the "best" way of solving this duplication is. Writing sans-IO code is one of
them.</p>
<h2 id="introducing-sans-io"><a href="#introducing-sans-io">Introducing sans-IO</a></h2>
<p>The core idea of sans-IO is similar to the dependency inversion principle from
the OOP world. Whilst some OOP code out there might be a bit extreme in terms of
following patterns (looking at you
<a href="https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/aop/framework/AbstractSingletonProxyFactoryBean.html"><code>AbstractSingletonProxyFactoryBean</code></a>),
I've found it helpful to explicitly spell some of these things out to really get
to the bottom of a particular design.</p>
<p>The dependency inversion principle says that policies (what to do) should not
depend on implementation details (how to do it). Instead, both components should
depend and communicate via abstractions. In other words, the piece of code that
decides to send a message on the network (i.e. the policy) should not depend on
the code that actually sends the message (i.e. the implementation).</p>
<p>That is the heart of the issue in the above example: We are composing our policy
code on top of a UDP socket and thus, forcing everything upwards to either be
<code>async</code> in the <code>tokio</code> example or deal with blocking IO in the <code>std</code> case. The
policy code is the same, yet it is the one we want to test and perhaps share
with others via libraries, regardless of whether or not we use blocking or
non-blocking IO.</p>
<h2 id="applying-dependency-inversion"><a href="#applying-dependency-inversion">Applying dependency inversion</a></h2>
<p>How do we apply the dependency inversion principle then? We introduce
abstractions! When we call <code>UdpSocket::send</code>, what data are we actually passing?
The payload, a <code>SocketAddr</code> and ‚Äî implicitly ‚Äî the socket itself.
The socket can also be identified by means of a <code>SocketAddr</code>: The one we bound
to earlier in our application. Let's package these three things up into an
abstraction. Meet <code>Transmit</code>:</p>
<div><pre><code><span>pub</span> <span>struct</span> <span>Transmit</span> {
    src: SocketAddr,
    dst: SocketAddr,
    payload: <span>Vec</span>&lt;<span>u8</span>&gt;
}
</code></pre></div>
<p>Anywhere where we'd like to send data over our <code>UdpSocket</code>, we should instead
emit a <code>Transmit</code>. But that is only one half of the solution. Where does the
<code>Transmit</code> go? We need to execute this <code>Transmit</code> somewhere! This is the 2nd
half of any sans-IO application. Recall the definition of the
dependency-inversion principle: Policies should not depend on implementations,
instead both should depend on abstractions. <code>Transmit</code> is our abstraction, and
we already know that we need to rewrite our policy code to use it. The actual
implementation details, i.e. our <code>UdpSocket</code> also needs to be made aware of our
new abstraction.</p>
<p>This is where event loops come in. sans-IO code needs to be "driven", almost
similarly as to how a <code>Future</code> in Rust is lazy and needs to be polled by a
runtime to make progress.</p>
<p>Event loops are the implementation of our side-effects and will actually call
<code>UdpSocket::send</code>. That way, the rest of the code turns into a state machine
that only expresses, what should happen at a given moment.</p>
<h3 id="the-state-machine"><a href="#the-state-machine">The state machine</a></h3>
<p>The state machine diagram for our STUN binding request looks like this:</p>
<p><img alt="A UML state diagram for a STUN binding request." loading="lazy" width="500" height="500" decoding="async" data-nimg="1" srcset="https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine1.svg&amp;w=640&amp;q=75 1x, https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine1.svg&amp;w=1080&amp;q=75 2x" src="https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine1.svg&amp;w=1080&amp;q=75"></p><p>Without executing the side-effect of sending a message directly, we need to
rewrite our code to resemble what it actually is: This state machine. As we can
see in our diagram, we have 2 states (not counting entry and exit states):
<code>Sent</code> &amp; <code>Received</code>. These are mutually-exclusive, so we can model them as an
enum:</p>
<div><pre><code><span>enum</span> <span>State</span> {
    Sent,
    Received { address: SocketAddr },
}
</code></pre></div>
<p>Now, that we've laid out our data structure, let's add some functionality to it!</p>
<div><pre><code><span>struct</span> <span>StunBinding</span> {
    state: State,
    buffered_transmits: VecDeque&lt;Transmit&gt;,
}

<span>impl</span> <span>StunBinding</span> {
    <span>fn</span> <span>new</span>(server: SocketAddr) <span>-&gt;</span> <span>Self</span> {
        <span>Self</span> {
            state: State::Sent,
            buffered_transmits: VecDeque::<span>from</span>([Transmit {
                dst: server,
                payload: <span>make_binding_request</span>(),
            }]),
        }
    }

    <span>fn</span> <span>handle_input</span>(&amp;<span>mut</span> <span>self</span>, packet: &amp;[<span>u8</span>]) {
        <span>// Error handling is left as an exercise to the reader ...</span>
        <span>let</span> <span>address</span> = <span>parse_binding_response</span>(packet);

        <span>self</span>.state = State::Received { address };
    }

    <span>fn</span> <span>poll_transmit</span>(&amp;<span>mut</span> <span>self</span>) <span>-&gt;</span> <span>Option</span>&lt;Transmit&gt; {
        <span>self</span>.buffered_transmits.<span>pop_front</span>()
    }

    <span>fn</span> <span>public_address</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>Option</span>&lt;SocketAddr&gt; {
        <span>match</span> <span>self</span>.state {
            State::Sent =&gt; <span>None</span>,
            State::Received { address } =&gt; <span>Some</span>(address),
        }
    }
}
</code></pre></div>
<p>The <code>handle_input</code> function is like the inverse to <code>Transmit</code>. We will use it to
feed incoming data to our state machine, i.e. the result of <code>UdpSocket::recv</code>.
We also add a few auxiliary functions to actually construct a new instance of
our state machine and to query things from it. With this in place, we now have a
state machine that models the behaviour of our program without performing any IO
itself.</p>
<h3 id="the-event-loop"><a href="#the-event-loop">The event loop</a></h3>
<p>Without an event loop, this state machine does nothing. For this example, we can
get away with a pretty simple event loop:</p>
<div><pre><code><span>fn</span> <span>main</span>() <span>-&gt;</span> anyhow::<span>Result</span>&lt;()&gt; {
    <span>let</span> <span>socket</span> = UdpSocket::<span>bind</span>(<span>"0.0.0.0:0"</span>)?;
    <span>let</span> <span>server</span> = <span>"stun.cloudflare.com:3478"</span>
        .<span>to_socket_addrs</span>()?
        .<span>next</span>()
        .<span>context</span>(<span>"Failed to resolve hostname"</span>)?;
    <span>let</span> <span>mut </span><span>binding</span> = StunBinding::<span>new</span>(server);

    <span>let</span> <span>address</span> = <span>loop</span> {
        <span>if</span> <span>let</span> <span>Some</span>(transmit) = binding.<span>poll_transmit</span>() {
            socket.<span>send_to</span>(&amp;transmit.payload, transmit.dst)?;
            <span>continue</span>;
        }

        <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[<span>0u8</span>; <span>100</span>];
        <span>let</span> <span>num_read</span> = socket.<span>recv</span>(&amp;<span>mut</span> buf)?;

        binding.<span>handle_input</span>(&amp;buf[..num_read]);

        <span>if</span> <span>let</span> <span>Some</span>(address) = binding.<span>public_address</span>() {
            <span>break</span> address;
        }
    };

    <span>println!</span>(<span>"Our public IP is: {address}"</span>);

    <span>Ok</span>(())
}
</code></pre></div>
<p>Notice how the event loop is slightly more generic than the previous versions?
The event loop does not make any assumptions about the details of the STUN
binding protocol. It doesn't know that it is request-response for example! From
the event loop's perspective, multiple message could be necessary before we can
figure out our public address.</p>
<p>UDP is an unreliable protocol, meaning our packets could get lost in transit. To
mitigate this, STUN mandates retransmission timers. As it turns out, adding time
to this event loop is fairly trivial.</p>
<h3 id="abstracting-time"><a href="#abstracting-time">Abstracting time</a></h3>
<p>What do we mean when we talk about abstracting time? In most cases, especially
in network protocols, access to the current time is needed to check whether some
amount of time has passed. For example, has it been more than 5s since we sent
our request? Another common one is keep-alive messages: Has it been more than
30s since we sent our last keep-alive?</p>
<p>In all these cases, we don't actually need to know the current <em>wall clock</em>
time. All we need is a <code>Duration</code> to a previous point in time. Rust provides us
with a very convenient abstraction here: <code>Instant</code>. <code>Instant</code> doesn't expose the
current time, but it allows us to measure the <code>Duration</code> between two <code>Instant</code>s.
We can extend our state machine with two APIs that are generic enough to cover
all our time-based needs: <code>poll_timeout</code> and <code>handle_timeout</code>:</p>
<div><pre><code><span>impl</span> <span>StunBinding</span> {
    <span>// ...</span>

    <span>/// Notifies `StunBinding` that time has advanced to `now`.</span>
    <span>fn</span> <span>handle_timeout</span>(&amp;<span>mut</span> <span>self</span>, now: Instant) {}

    <span>/// Returns the timestamp when we next expect `handle_timeout` to be called.</span>
    <span>fn</span> <span>poll_timeout</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>Option</span>&lt;Instant&gt; {
        <span>None</span>
    }

    <span>// ...</span>
}
</code></pre></div>
<p>Similar to <code>handle_input</code> and <code>poll_timeout</code>, these APIs are the abstraction
between our protocol code and the event loop:</p>
<ul>
<li><code>poll_timeout</code>: Used by the event loop to schedule a timer for a wake-up.</li>
<li><code>handle_timeout</code>: Used by the event loop to notify the state machine that a
timer has expired.</li>
</ul>
<p>For demonstration purposes, let's say we want to send a new binding request
every 5s after we have received the last one. Here is how one could implement
this:</p>
<div><pre><code><span>impl</span> <span>StunBinding</span> {
    <span>// ...</span>

    <span>/// Notifies `StunBinding` that time has advanced to `now`.</span>
    <span>fn</span> <span>handle_timeout</span>(&amp;<span>mut</span> <span>self</span>, now: Instant) {
        <span>let</span> <span>last_received_at</span> = <span>match</span> <span>self</span>.state {
            State::Sent =&gt; <span>return</span>,
            State::Received { at, .. } =&gt; at,
        };

        <span>if</span> now.<span>duration_since</span>(last_received_at) &lt; Duration::<span>from_secs</span>(<span>5</span>) {
            <span>return</span>;
        }

        <span>self</span>.buffered_transmits.<span>push_front</span>(Transmit {
            dst: <span>self</span>.server,
            payload: <span>make_binding_request</span>(),
        });
        <span>self</span>.state = State::Sent;
    }

    <span>/// Returns the timestamp when we next expect `handle_timeout` to be called.</span>
    <span>fn</span> <span>poll_timeout</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>Option</span>&lt;Instant&gt; {
        <span>match</span> <span>self</span>.state {
            State::Sent =&gt; <span>None</span>,
            State::Received { at, .. } =&gt; <span>Some</span>(at + Duration::<span>from_secs</span>(<span>5</span>)),
        }
    }

    <span>// ...</span>
}
</code></pre></div>
<p>The only other changes I've made are adding an <code>at</code> field to the
<code>State::Received</code> variant that gets set to the current time upon <code>handle_input</code>:</p>
<div><pre><code><span>impl</span> <span>StunBinding</span> {
    <span>fn</span> <span>handle_input</span>(&amp;<span>mut</span> <span>self</span>, packet: &amp;[<span>u8</span>], now: Instant) {
        <span>let</span> <span>address</span> = <span>parse_binding_response</span>(packet);

        <span>self</span>.state = State::Received { address, at: now };
    }
}
</code></pre></div>
<p>This is an updated version of our state diagram:</p>
<p><img alt="A UML state diagram for a STUN binding request that is being refreshed every 5s." loading="lazy" width="500" height="800" decoding="async" data-nimg="1" srcset="https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine2.svg&amp;w=640&amp;q=75 1x, https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine2.svg&amp;w=1080&amp;q=75 2x" src="https://www.firezone.dev/_next/image?url=%2Fimages%2Fblog%2Fsans-io%2Fstun-binding-state-machine2.svg&amp;w=1080&amp;q=75"></p><p>The event loop also changed slightly. Instead of exiting once we know our public
IP, we'll now loop until the user quits the program:</p>
<div><pre><code>    <span>loop</span> {
        <span>if</span> <span>let</span> <span>Some</span>(transmit) = binding.<span>poll_transmit</span>() {
            socket.<span>send_to</span>(&amp;transmit.payload, transmit.dst).<span>await</span>?;
            <span>continue</span>;
        }

        <span>let</span> <span>mut </span><span>buf</span> = <span>vec!</span>[<span>0u8</span>; <span>100</span>];

        tokio::<span>select!</span> {
            <span>Some</span>(time) = &amp;<span>mut</span> timer =&gt; {
                binding.<span>handle_timeout</span>(time);
            },
            res = socket.<span>recv</span>(&amp;<span>mut</span> buf) =&gt; {
                <span>let</span> <span>num_read</span> = res?;
                binding.<span>handle_input</span>(&amp;buf[..num_read], Instant::<span>now</span>());

            }
        }

        timer.<span>reset_to</span>(binding.<span>poll_timeout</span>());

        <span>if</span> <span>let</span> <span>Some</span>(address) = binding.<span>public_address</span>() {
            <span>println!</span>(<span>"Our public IP is: {address}"</span>);
        }
    }
</code></pre></div>
<h2 id="the-premise-of-sans-io"><a href="#the-premise-of-sans-io">The premise of sans-IO</a></h2>
<p>So far, all of this seems like a very excessive overhead for sending a few UDP
packets back and forth. Surely, the 10 line example introduced at the start is
preferable over this state machine and the event loop! The example might be, but
recall the debate around function colouring. In a code snippet without
dependencies like the above example, using <code>async</code> seems like a no-brainer and
really easy. The problem arises once you want to bring in dependencies.
Composing your functionality (i.e. policy) on top of those dependencies imposes
their decisions around async vs blocking IO on you. Libraries like <code>str0m</code> or
<code>quinn-proto</code> which are written in the sans-IO way don't do that. Instead, they
are pure state machines and thus the decision about async vs blocking IO or
which async runtime to use is deferred to the application.</p>
<p>Freedom to use either blocking or non-blocking IO isn't the only benefit to
this. sans-IO design also compose very well, tend to have very flexible APIs,
are easy to test and play well with Rust's features. Let's explore these
additional benefits one by one.</p>
<h3 id="easy-composition"><a href="#easy-composition">Easy composition</a></h3>
<p>Take another look at the API of <code>StunBinding</code>. The main functions exposed to the
event loop are: <code>handle_timeout</code>, <code>handle_input</code>, <code>poll_transmit</code> and
<code>poll_timeout</code>. None of these are specific to the domain of STUN! Most network
protocols can be implemented with these or some variation of them. As a result,
it is very easy to compose these state machines together: want to query 5 STUN
servers for your public IP? No problem. Just make 5 <code>StunBinding</code>s and call them
in order<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>.</p>
<p>In the case of Firezone, you can see this in the example of
<a href="https://github.com/firezone/firezone/tree/main/rust/connlib/snownet"><code>snownet</code></a>,
a library that combines ICE and WireGuard and thereby exposes "magic" IP tunnels
that work in any network setup to the rest of the application.</p>
<p><code>snownet</code> builds on top of <code>str0m</code>, a sans-IO WebRTC library and <code>boringtun</code>, an
(almost<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>) sans-IO WireGuard implementation. We don‚Äôt need the majority of the
WebRTC stack though. The only thing we are interested in is the <code>IceAgent</code> which
implements <a href="https://datatracker.ietf.org/doc/html/rfc8445">RFC 8445</a>. ICE uses a
clever algorithm that ensures two agents, deployed into arbitrary network
environments find the most optimal communication path to each other. The result
of ICE is a pair of socket addresses that we then use to setup a WireGuard
tunnel. Because <code>str0m</code> is built in a sans-IO fashion, only using the <code>IceAgent</code>
is shockingly trivial: you simply only import that part of the library and
compose its state machine into your existing code. In <code>snownet</code>, a
<a href="https://github.com/firezone/firezone/blob/a5b7507932e9d27e3fc9ed5be7428b9937f2f828/rust/connlib/snownet/src/node.rs#L1289-L1306">connection</a>
simply houses an <code>IceAgent</code> and a wireguard tunnel, dispatching incoming
messages to either one or the other.</p>
<h3 id="flexible-apis"><a href="#flexible-apis">Flexible APIs</a></h3>
<p>sans-IO code needs to be "driven" by an event loop of some sorts because it
"just" expresses the state of the system but doesn‚Äôt cause any side-effects
itself. The event loop is responsible for "querying" the state (like
<code>poll_transmit</code>), executing it and also passing new input to the state machine
(<code>handle_timeout</code> and <code>handle_input</code>). To some people, this may appear as
unnecessary boilerplate but it comes with a great benefit: flexibility.</p>
<ul>
<li>Want to make use of <code>sendmmsg</code> to reduce the number of syscalls when sending
packets? No problem.</li>
<li>Want to multiplex multiple protocols over a single socket? No problem.</li>
</ul>
<p>Writing the event loop yourself is an opportunity to be able to tune our code to
exactly what we want it to do. This also makes maintenance easier for library
authors: They can focus on correctly implementing protocol functionality instead
of having debates around async runtimes or exposing APIs to set socket options.</p>
<p>A good example here is <code>str0m</code>‚Äôs stance on enumerating network interfaces: This
is an IO concern and up to the application on how to achieve it. <code>str0m</code> only
provides an API to add the socket addresses as an ICE candidate to the current
state. As a result, we are able to easily implement optimisations such as
gathering TURN candidates prior to any connection being made, thus reducing
Firezone's connection-setup latency.</p>
<div><p>In ICE, both parties gather candidates (sockets) and then test connectivity
between them. See <a href="https://datatracker.ietf.org/doc/html/rfc8445#section-5.1.1">https://datatracker.ietf.org/doc/html/rfc8445#section-5.1.1</a>
for details.</p></div>
<h3 id="testing-at-the-speed-of-light"><a href="#testing-at-the-speed-of-light">Testing at the speed of light</a></h3>
<p>sans-IO code is essentially side-effect free and thus lends itself extremely
well for (unit) tests. Due to sockets and time being abstracted away, it becomes
a breeze to write tests that advance time by 5 minutes in an instant. All we
need to do is pass a modified <code>Instant</code> to our function and assert, how the code
behaves. To see a real world example of this,
<a href="https://github.com/firezone/firezone/blob/53557f46e452c0fe5195a4326873753a356c6005/rust/connlib/snownet/tests/lib.rs#L123-L127">check out</a>
how we test that <code>snownet</code> closes idle connections after 5 minutes.</p>
<p>Similarly, actually sending data over a socket takes (a little bit of) time and
more importantly, requires allocation of ports etc. In a sans-IO world, "sending
data" in a test is as simple as taking a <code>Transmit</code> from party B and calling
<code>handle_input</code> on the state of party A. No need to go through a network socket!</p>
<p>At Firezone, we took this idea one step further. We implemented a reference
state machine that describes how we want <code>connlib</code> to work. This reference state
machine is used as the source of truth in our tests. We then leverage
<code>proptest</code>'s support for
<a href="https://proptest-rs.github.io/proptest/proptest/state-machine.html">state machine testing</a>
to deterministically sample and execute thousands of scenarios on every CI run
and compare the reference state machine with <code>connlib</code>'s actual state. The
details of this go beyond the scope of this post, so stay tuned for a followup
about that topic in particular too! The key take-away here is that a sans-IO
design enables these kind of tests.</p>
<h3 id="edge-cases-and-io-failures"><a href="#edge-cases-and-io-failures">Edge-cases and IO failures</a></h3>
<p>Not only can we easily test how our code reacts at certain points in time but
the lack of any IO also makes it really easy to test for IO failures and/or
weird behaviours!</p>
<ul>
<li>What happens if this packets gets dropped and we never receive a response?</li>
<li>What happens if we get a malformed response?</li>
<li>What happens if the RTT to the server is really long?</li>
<li>What happens if we don't have a functional IPv6 interface?</li>
<li>What happens if we <em>only</em> have an IPv6 interface?</li>
</ul>
<p>By decoupling our protocol implementation from the actual IO side-effects, we
are forced go back to the drawing board and design our state machine to be
resilient against these problems. Consequently, detecting and dealing with
errors simply becomes part of state machine's input handling which leads to more
robust code and makes it less likely for edge-cases to only be considered as an
after-thought.</p>
<h2 id="rust--sans-io-a-match-made-in-heaven"><a href="#rust--sans-io-a-match-made-in-heaven">Rust + sans-IO: A match made in heaven?</a></h2>
<p>Rust forces us to declare, which component or function in our code owns a
certain value. A common example for these are buffers: When reading from a
<code>UdpSocket</code>, we need to provide a <code>&amp;mut [u8]</code> as a place for the actual bytes
being received. Only the owner of a value can declare it mutable and thus either
mutate itself or temporarily hand out mutable references to other functions.
<code>UdpSocket</code> follows this design: It doesn't declare a buffer on its own,
instead, it only requires temporary, mutable access to it when it is actually
reading from the socket. The explicit modelling of ownership and mutability are
integral to how Rust works and what enable features like the borrow-checker.</p>
<p>In a sans-IO design we only have synchronous APIs, i.e. none of the functions on
a state machines ever block on IO or time. Instead, they are just data
structures.</p>
<p>Those two aspects work exceptionally well together. We can use <code>&amp;mut</code> liberally
to express state changes and thus leverage the borrow-checker to ensure our code
is sound. In comparison, <code>async</code> Rust and <code>&amp;mut</code> almost feel somewhat at odds
with each other.</p>
<p>In Rust, <code>async</code> functions are just syntax sugar for a data structure that
implements <code>Future</code>. Spawning a <code>Future</code> into a runtime<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup> like <code>tokio</code>
requires this data structure to be <code>'static</code> and therefore, it cannot contain
any references, including <code>&amp;mut</code>. To mutate state that isn't local to the
<code>Future</code>, you basically have two options:</p>
<ul>
<li>Use reference-counted pointers and a mutex, i.e. <code>Arc&lt;Mutex&lt;T&gt;&gt;</code></li>
<li>Use "actors" and connect them via channels, i.e. spawn multiple tasks with
loops that read and write to channels</li>
</ul>
<p>Both of these options have a runtime overhead: Locks can result in contention
and sending messages through channels requires copying. In addition, multiple
tasks running inside a runtime operate in a non-deterministic order which can
easily lead to race conditions and in the worst case, deadlocks. It appears that
with either of these options, we arrive at a design that feels brittle, is prone
to deadlocks and no longer employs zero-cost abstractions, yet avoiding all of
these is one of the reasons we wanted to use Rust in the first place!</p>
<p>In the sans-IO world, these problems don't exist. Our protocol code doesn't
spawn any tasks and thus, <code>&amp;mut self</code> is all we need to mutate state. Without
tasks or threads, we also don't need synchronisation primitives like <code>Mutex</code>.
Without channels, there is no need to copy data: The state machine can simply
directly reference the buffer we passed to the socket.</p>
<p>Last but not least, we've also found that ever since we moved to sans-IO, our
code became much easier to understand. No more tracking down of: Where is the
other end of this channel? What if the channel is closed? Which other code is
locking this <code>Mutex</code>? Instead, it is all just nested state machines and regular
function calls.</p>
<h2 id="the-downsides"><a href="#the-downsides">The downsides</a></h2>
<p>There are no silver-bullets and sans-IO is no exception to this. Whilst writing
your own event loop gives you great control, it can also result in subtle bugs
that are initially hard to find.</p>
<p>For example, a bug in the state machine where the value returned from
<code>poll_timeout</code> is not advanced can lead to a busy-looping behaviour in the event
loop.</p>
<p>Also, sequential workflows require more code to be written. In Rust, <code>async</code>
functions compile down to state machines, with each <code>.await</code> point representing
a transition to a different state. This makes it easy for developers to write
sequential code together with non-blocking IO. Without <code>async</code>, we need to write
our own state machines for expressing the various steps. How annoying this will
be in practise depends on your problem domain. Modelling a request-response
protocol is not very difficult as we've seen in the example of a <code>StunBinding</code>.
On the other hand, if need to express larger, sequential workflows, manually
modelling them out as state machines could become tedious.</p>
<p>Finally, the sans-IO design is not particularly wide-spread (yet) in the Rust
community. As a result, there are very few libraries out there that follow it.
Most of them will either implement blocking or non-blocking IO instead of
sans-IO.</p>
<h2 id="closing"><a href="#closing">Closing</a></h2>
<p>Writing sans-IO code is unusual at first but really enjoyable once you get the
hang of it. In part, this is because Rust provides great tools for modelling
state machines. More so, the fact that sans-IO forces you to handle errors as
you would any other input simply feels like the way networking code should be
written.</p>
<p>That being said, there are additional ways of writing async Rust not discussed
in this post. The most notable of those being structured concurrency which sits
somewhere "in the middle" between sans-IO and the async Rust portrayed in this
post. Read <a href="https://without.boats/blog/let-futures-be-futures/">this article</a>
from withoutboats for more on that topic.</p>
<p>Many thanks to <a href="https://github.com/algesten">@algesten</a> for providing feedback
on drafts of this post.</p>
<section data-footnotes="true">
<ol>
<li id="user-content-fn-1">
<p>For more details on Firezone's tech stack, see
<a href="https://www.firezone.dev/kb/architecture/tech-stack">this article</a> in our architecture docs. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">‚Ü©</a></p>
</li>
<li id="user-content-fn-4">
<p>Be sure to implement proper multiplexing of STUN messages at this point.
Hint: Use the <code>TransactionId</code> and/or the server's address. <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 2">‚Ü©</a></p>
</li>
<li id="user-content-fn-3">
<p><code>boringtun</code> does call <code>Instant::now</code> internally and is thus unfortunately
partly impure, see <a href="https://github.com/cloudflare/boringtun/issues/391">https://github.com/cloudflare/boringtun/issues/391</a>. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">‚Ü©</a></p>
</li>
<li id="user-content-fn-2">
<p>Technically, a thread-per-core runtime could allow non-<code>'static</code> <code>Future</code>s. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 4">‚Ü©</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion Forcing: Next-Token Prediction Meets Full-Sequence Diffusion (158 pts)]]></title>
            <link>https://boyuan.space/diffusion-forcing/</link>
            <guid>40871783</guid>
            <pubDate>Thu, 04 Jul 2024 02:09:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boyuan.space/diffusion-forcing/">https://boyuan.space/diffusion-forcing/</a>, See on <a href="https://news.ycombinator.com/item?id=40871783">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
              
              
              <p><sup>*</sup> Work done while being a visiting student at MIT.
              </p>

              <br>
              <div>
                <ul>
                  <li>
                    <img src="https://boyuan.space/diffusion-forcing/static/images/mit.jpg" alt="MIT Logo">
                    <span><sup>1</sup>MIT</span>
                  </li>
                </ul>
              </div>
              
              
              <p><span>TL;DR:</span> Diffusion
                Forcing combines the strength of full-sequence diffusion models
                and next-token models, acting as either or a mix at sampling
                time for different applications without retraining.
              </p>
            </div>

    <div>
            <h2>Abstract</h2>
            <p>
              This paper presents Diffusion Forcing, a new training paradigm
              where a diffusion model is trained to denoise a set of tokens with
              independent per-token noise levels. We apply Diffusion Forcing to
              sequence generative modeling by training a causal next-token
              prediction model to generate one or several future tokens without
              fully diffusing past ones. Our approach is shown to combine the
              strengths of next-token prediction models, such as variable-length
              generation, with the strengths of full-sequence diffusion models,
              such as the ability to guide sampling to desirable trajectories.
              Our method offers a range of additional capabilities, such as (1)
              rolling-out sequences of continuous tokens, such as video, with
              lengths past the training horizon, where baselines diverge and (2)
              new sampling and guiding schemes that uniquely profit from
              Diffusion Forcing's variable-horizon and causal architecture, and
              which lead to marked performance gains in decision-making and
              planning tasks. In addition to its empirical success, our method
              is proven to optimize a variational lower bound on the likelihoods
              of all subsequences of tokens drawn from the true joint
              distribution.
              </p>
          </div>

    

    <div>
            <h2>Diffusion Forcing</h2>
            <p>
                Diffusion Forcing enjoys key strengths of both next-token
                autoregressive models and full-sequence diffusion models. By
                training Diffusion Forcing once, one can flexibly control its
                behavior at sampling time to simultaneously perform flexible and
                compositional geneation like next-token models, and perform
                sequence level guidance like full-sequence diffusion models.
              </p>
            
            <p><img src="https://boyuan.space/diffusion-forcing/static/images/abilities.png" alt="Abilities of teacher forcing, full-sequence diffusion, and Diffusion Forcing.">
            </p>
            <p>
                Diffusion Forcing achieves so by training sequence diffusion but
                allowing each token to have a different noise level. One can
                view noises in diffusion as varying levels of masking and
                establish a unified view: full-sequence diffusion denoise all
                frames at once with the same noise level, while next-token
                prediction denoises next frame at a time with zero noise in its
                past tokens.
              </p>
            
            <p><img src="https://boyuan.space/diffusion-forcing/static/images/method.png" alt="Diffusion Forcing method.">
            </p>
            
            <p>
                As a result, one can use different noise levels across a
                sequence at sampling time to achieve flexible behaviors such as
                stablizing auto-regressive rollout, guidance over long horizon
                or planning with causal uncertainty.
              </p>
            
            <p><img src="https://boyuan.space/diffusion-forcing/static/images/usage.png" alt="Diffusion Forcing usage.">
            </p>
          </div>

    

    <div>
            <h2>Video Prediction</h2>
            <p>
                We provide a list of synthesized videos directly generated by
                models (without VAE / superresolution). The below results are
                sampled without cherry-picking.
              </p>

            
            <p>
                Video Prediction by Diffusion Forcing (ours) and baselines in
                DMLab dataset (0.25x speed). Teacher forcing easily blows up
                while causal full-sequence diffusion models suffer from serious
                consistency issues. Diffusion Forcing can achieve stable and and
                consistent video prediction. PNG visualizations are provided
                below to reflect the original quality of generated samples.
              </p>

            <div>
              <p><img src="https://boyuan.space/diffusion-forcing/static/images/dmlab_df_0.png" height="auto" width="100%"></p><hr>

              <p><img src="https://boyuan.space/diffusion-forcing/static/images/dmlab_df_1.png" height="auto" width="100%"></p><hr>
            </div>

            
            <p>
                Video Prediction by Diffusion Forcing (ours) and baselines in
                Minecraft dataset (0.5x speed). Teacher forcing easily blows up
                while causal full-sequence diffusion models suffer from serious
                consistency issues. Diffusion Forcing can achieve stable and and
                consistent video prediction. PNG visualizations are provided
                below to reflect the original quality of generated samples.
              </p>
            <div>
              <p><img src="https://boyuan.space/diffusion-forcing/static/images/minecraft_df_0.png" height="auto" width="100%"></p><hr>

              <p><img src="https://boyuan.space/diffusion-forcing/static/images/minecraft_df_1.png" height="auto" width="100%">
            </p></div>
          </div>

    

    <div>
            <h2>
              Stablizing Infinite Rollout without Sliding Window
            </h2>
            <p>
                In addition, one can rollout much longer videos with our method
                than the maximum sequence length it's trained on. Remarkly, we
                can do this without Sliding Window. That is, we rollout RNN
                without ever resetting the latent z to initial latent z0,
                showing stablization effect of Diffusion Forcing thanks to its
                stablization effect. Videos are compressed for loading speed.
                The results are sampled without cherry-picking.
              </p>
            <video id="dmlab_long" autoplay="" muted="" loop="" playsinline="" width="100%">
              <source src="https://boyuan.space/diffusion-forcing/static/videos/video_prediction/dmlab_long_compressed.mp4" type="video/mp4">
            </video>

            <div>
              <p>
                <b>Quality of the video is decreased due to mp4 compression of
                  long videos! We provide PNG visualizations below to reflect
                  original quality of generated samples longer than training
                  horizon.</b>
              </p>
              <p>
                Diffusion Forcing (ours) trained on 36 frames can rollout for
                2000 frames or more on DMLab dataset, without sliding window
                thanks to its stablization effect. Videos are compressed for
                loading speed. Original dataset resolution is 64x64.
              </p>
            </div>
            <p><img src="https://boyuan.space/diffusion-forcing/static/images/df_dmlab_long_0.png" height="auto" width="100%">
            </p>

            <video id="minecraft_long" autoplay="" muted="" loop="" playsinline="" width="100%">
              <source src="https://boyuan.space/diffusion-forcing/static/videos/video_prediction/minecraft_long_compressed.mp4" type="video/mp4">
            </video>

            <div>
              <p>
                <b>Quality of the video is decreased due to mp4 compression of
                  long videos! We provide PNG visualizations below to reflect
                  original quality of generated samples longer than training
                  horizon.
                </b>
              </p>
              <p>
                Diffusion Forcing (ours) trained on 72 frames rolloutss for 2000
                frames or more on Minecraft dataset without blowing up, without
                sliding window. Original dataset resolution is 128x128. In
                certain scenarios, the agent will get stuck in front of two
                block high dirt or stone blocks until it switches direction,
                which is an instrinsics issue of the dataset collection.
              </p>
            </div>

            <p><img src="https://boyuan.space/diffusion-forcing/static/images/df_minecraft_long_0.png" height="auto" width="100%">
            </p>
          </div>

    <hr>

    <div>
            <h2>Diffusion Planning</h2>
            <p>
                Similar to prior works like Diffuser, we can use test-time
                guidance to make our diffusion sequence a planner. However, we
                explictly model the causal relationship by defining each token
                as [a_t, o_{t+1}]. By doing so, we have a belief over action to
                take and the observation it's leading to, but can also update
                this belief to posterior estimation when new observation is made
                after the action is taken.
              </p>
            <video id="planning" autoplay="" controls="" muted="" loop="" playsinline="" width="100%">
              <source src="https://boyuan.space/diffusion-forcing/static/videos/planning/planning.mp4" type="video/mp4">
            </video>
            <p>
                Visualization of the diffusion planning process of Diffusion
                Forcing as a decision-making framework. To model the causal
                uncertainty of future, diffusion forcing's plan can have near
                future at lower noise level while having far future at higher
                noise level.
              </p>
          </div>

    <div>
            <h2>
              Long Horizon Imitation Learning
            </h2>

            <p>
                Many real world tasks are not markovian and requires long
                horizon memory to accomplish. In our real robot task, a robot
                arm is asked to swap the slots of two fruits using a third slot.
                Since the fruits are input in random slots at the beginning, one
                cannot determine the next steps from a single observation
                without knowledge of the initial placement of the fruits.
              </p>
            <p><img src="https://boyuan.space/diffusion-forcing/static/images/robot.png" height="auto" width="100%">
            </p>
            <p>
                We simply remove guidance from the planning experiments and
                jointly diffuses action-observation sequences to perform
                feedback control.
              </p>
            
            <p>
                The above video shows multiple continuous successes before a
                failure happens. One can observe that the robot is able to
                accomplish the task even when the fruit location is randomized
                by the previous run. On the other hand, we tried SOTA imitation
                learning techniques Diffusion Forcing but it cannot perform the
                task due to non-markovianess.
              </p>
            
            <p>
                In addition, diffusion forcing can be prompted to treat incoming
                observation as noisy ones to be robust to unseen distractions at
                test time. In the video above, we illustrate our distraction
                method of randomly throwing a shopping bag into the field of
                view.
              </p>
          </div>

    

    <div id="BibTeX">
        <h2>BibTeX</h2>
        <pre><code>
@misc{chen2024diffusionforcingnexttokenprediction,
      title={Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion}, 
      author={Boyuan Chen and Diego Marti Monso and Yilun Du and Max Simchowitz and Russ Tedrake and Vincent Sitzmann},
      year={2024},
      eprint={2407.01392},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.01392}, 
}
        </code></pre>
      </div>
  

</div>]]></description>
        </item>
    </channel>
</rss>