<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 16 May 2025 15:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Sci-Net (164 pts)]]></title>
            <link>https://sci-hub.se/sci-net</link>
            <guid>44004625</guid>
            <pubDate>Fri, 16 May 2025 12:30:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sci-hub.se/sci-net">https://sci-hub.se/sci-net</a>, See on <a href="https://news.ycombinator.com/item?id=44004625">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                <p>
                    I regularly receive requests from Sci-Hub users to help them download some paper that cannot be opened through Sci-Hub. The number of such requests increased in the past two years, since Sci-Hub database updates were paused. The opposite also happens: users ask whether they can upload to Sci-Hub some paper that they have bought or downloaded via university subscription.
                </p>

                <p>Sci-Hub was never designed to accept uploads from users. From the very beginning, it was implemented as an autonomous system that scans and downloads research papers from various universities ‚Äê without human in the loop. This automatic method turned out to be quite effective, and Sci-Hub was able to reach coverage &gt; 90% in its best days. However, even then some paper remained unreachable.</p>

                <p>Now there exists a platform where users can request unavailable papers, and upload them: <a target="_blank" href="https://sci-net.xyz/">sci-net.xyz</a></p>

                

                <p>The user interface is simple: an input box where a DOI of the requested paper should be inserted. Once the user clicks the button, Sci-Net automatically checks if the requested paper is open access or already available on Sci-Hub. If the paper is found, the link to it will be displayed. If not, then user will be able to create a new request.</p>

                <p>On the index page, the list of requests is shown, that can be filtered by subjects and publisher. If you happen to have access to some paper in the list, you can click on the request and upload the PDF. There is a nice built-in functionality to remove watermarks from PDF to hide the university where paper was downloaded. That way Sci-Net protects the anonymity of the uploader.</p>

                <p>The 'Library' section will show all your requests and uploads, and in the 'Upload' section you can contribute your papers to the platform: just drop the PDF file and the system will automatically detect the DOI. If the DOI is not available yet on Sci-Net / Sci-Hub / open access, the file will get uploaded. The article then will become accessible to everyone by URL sci-net.xyz/DOI including people who are not registered on the platform.</p>

                <p>The platform is currently in the active development stage, and much more features are upcoming.</p>

                

                <p>The part that really makes Sci-Net stand out is the usage of decentralized tokens: Sci-Hub meme coins -- to reward knowledge sharing. When creating a request, you can specify the amount of tokens uploader will receive for sharing the paper. However, the tokens will not be transferred after uploading the PDF right away, but only after you check the solution and click the 'Accept' button. The tokens subtracted from your account will be added to the uploader.</p>

                <p>Sci-Net will ask for a minimum amount of 1000 Sci-Hub tokens to register. The tokens will appear on your account after registration and will be used to reward uploaders. That is the most controversial part: some people argue that Sci-Net introduces paywall similar to publisher.</p>

                <p>Even though both might appear similar at a first glance, the differences are profound. Compared to insane publisher paywalls, the entry tax on Sci-Net is symbolic and is not higher than an equivalent of a cup of coffee in most countries. The next difference is that in traditional approach, profits are made by publisher, and ordinary researcher has no control on how the money are used. On Sci-Net, you're using tokens directly to reward uploaders. Payments go to fellow researchers, not to the platform.</p>

                <p>But the most important is that publishers will charge for access to the same paper again and again. Sci-Net will only do that once when paper is uploaded and after that, it will remain free forever and for everyone -- even users who are not registered on the platform will be able to access it. That is, all Sci-Net transactions contribute directly to public domain, and the number of knowledge accessible to everyone grows with Sci-Net usage.</p>

                <p>The more people use Sci-Hub token, the more valuable it becomes. That way Sci-Net indirectly contributes to maintenance and further development of Sci-Hub.</p>

                <p>The only downside is that obtaining Sci-Hub tokens on the Solana network can be a non-trivial puzzle for a user who are new to crypto. But that only makes the process more interesting.</p>

                

                <p>Overall, Sci-Net is a platform that every researcher today must use and contribute to -- together, we can gradually make the idea of open knowledge a reality.</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ground control to Major Trial (354 pts)]]></title>
            <link>https://virtualize.sh/blog/ground-control-to-major-trial/</link>
            <guid>44004388</guid>
            <pubDate>Fri, 16 May 2025 12:03:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://virtualize.sh/blog/ground-control-to-major-trial/">https://virtualize.sh/blog/ground-control-to-major-trial/</a>, See on <a href="https://news.ycombinator.com/item?id=44004388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h3 id="%F0%9F%9A%80-of-trials-and-tribulations">üöÄ Of trials and tribulations</h3><p>Open Source is beautiful. Messy, thankless, powerful ‚Äî and occasionally just plain weird.</p><p>At <a href="https://vates.tech/?ref=virtualize.sh" rel="noreferrer">Vates</a>, we‚Äôve already shared some of the pain points that come with maintaining open source projects (see: <a href="https://virtualize.sh/blog/the-reality-of-oss-maintainer-fatigue/" rel="noopener">OSS maintainer fatigue</a>). And more recently, the rise of AI-assisted fake contributions and ‚Äúsecurity reports‚Äù has added a whole new layer of entropy. (<a href="https://daniel.haxx.se/blog/2024/01/02/the-i-in-llm-stands-for-intelligence/?ref=virtualize.sh" rel="noopener">this blog post</a> captures it perfectly.)</p><p>But today, we‚Äôre here for something a little more‚Ä¶ grounded.</p><h3 id="%F0%9F%A7%91%E2%80%8D%F0%9F%9A%80-the-curious-case-of-the-infinite-trial">üßë‚ÄçüöÄ The curious case of the infinite trial</h3><p>Let‚Äôs set the stage. Picture a semi-governmental company. Around $130 million in annual revenue. They build and operate very expensive things ‚Äî in space. Hundreds of physical hosts. Nearly 4,000 VMs. Most of their IT stack, in fact, runs on our platform.</p><div><p>üöÄ</p><p>You might say we‚Äôre <i><em>mission critical</em></i>. Fitting, since the <b><strong>XCP-ng logo is literally a rocket</strong></b>, and <b><strong>Xen Orchestra‚Äôs is a satellite</strong></b>. We didn‚Äôt expect someone to take that so literally.</p></div><p>Are they paying customers?</p><p>No.</p><p>Are they using the fully open-source version, from source?</p><p>Also no.</p><p>Instead, they discovered our <strong>Xen Orchestra Appliance</strong> (XOA): a turnkey virtual machine, with Xen Orchestra pre-installed, regularly tested, easy to deploy and update (and yes, still running fully on-prem). A supported and stable experience, designed for teams that don‚Äôt want to <code>git pull</code> on <code>master</code> branch in production.</p><p>But they didn‚Äôt want to pay for it. So they came up with a <em>creative</em> workaround: abusing our <strong>30-day trial </strong>(initially 15 days until recently), over and over again.</p><p>It all started back in <strong>April 2015</strong> ‚Äî yes, a full decade ago. At first, they used their <strong>corporate emails</strong> to request trials. One here, one there. Nothing suspicious. But over the years, the pattern grew. More emails. More trials. Enough that, when we looked back, we realized we could chart it. Literally. Here's what the <em>"creative licensing strategy"</em> has looked like over time:</p><figure><img src="https://virtualize.sh/blog/content/images/2025/05/image-1.png" alt="" loading="lazy" width="988" height="574" srcset="https://virtualize.sh/blog/content/images/size/w600/2025/05/image-1.png 600w, https://virtualize.sh/blog/content/images/2025/05/image-1.png 988w" sizes="(min-width: 720px) 720px"><figcaption><b><strong>Number of accounts we could confidently tie to the company.</strong></b> <span>Probably not even the full picture ‚Äî they were </span><i><em>very</em></i><span> committed.</span></figcaption></figure><p>As you can imagine, we ended up with what looked like the entire staff directory. Developers, sysadmins, managers‚Ä¶ pretty sure we even had the janitor signed up for a trial at some point.</p><p>When those ran out, they switched to <strong>personal Outlook or Gmail addresses</strong>. Every time: starting with a new (real!) person with their‚Ä¶ personal email, a new 30-day trial. And then go incrementally with it. <code>johndoe01@outlook.com</code>, then <code>johndoe02@outlook.com</code>‚Ä¶ We're now well past <code>johndoe60</code>. Same company name, every time‚Ä¶ which is impressive considering the field isn‚Äôt even required in order to register your account. Hard to say if it was a mistake, a flex, or just their way of making sure we didn‚Äôt miss who was milking the trials.</p><p>Yes, they‚Äôre that committed. Committed to <em>not</em> paying.</p><h3 id="%F0%9F%94%8D-a-quick-reminder-on-the-options">üîç A quick reminder on the options</h3><p>For the record, we‚Äôre fine with free users. That‚Äôs the spirit of open source. You can follow <a href="https://docs.xen-orchestra.com/installation?ref=virtualize.sh#from-the-sources" rel="noopener">our documentation</a>, build from source, and enjoy all the features for free. No stability guarantee, no pro support, <strong>but everything is there</strong>.</p><p>The appliance version (XOA) ‚Äî the one they keep trialing ‚Äî is our supported, maintained, pre-packaged VM. It‚Äôs tested. It updates with a single click. It saves time and reduces risk. That‚Äôs what we sell. And that‚Äôs what they keep pirouetting around with their email dance.</p><p>To me, that‚Äôs a pretty blatant breach of the unwritten ‚Äúmoral contract‚Äù of Open Source. I touched on that in more detail here:</p><figure><a href="https://virtualize.sh/blog/the-moral-contract/"><div><p>The moral contract</p><p>Balancing openness and revenue: navigating the moral contract in the evolving world of free software.</p><p><img src="https://virtualize.sh/blog/content/images/icon/faviconV2-3" alt=""><span>Olivier Lambert</span></p></div><p><img src="https://virtualize.sh/blog/content/images/thumbnail/trees-gedc9162a0_1280.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><h3 id="%F0%9F%A7%A0-the-real-puzzle">üß† The real puzzle</h3><p>We even helped them. In good faith. Like we often do with users evaluating our solution ‚Äî we don‚Äôt waste time scrutinizing who‚Äôs asking; we just try to help. That‚Äôs the spirit of trials, after all. So yes, we answered their questions, offered guidance, and even spent nearly a full day supporting them during one of their early ‚Äúwe‚Äôre just testing, maybe planning to buy‚Äù moments. You know the tune.</p><p>But over the years, the pattern started to stand out. Familiar questions, familiar setups‚Ä¶ and when we finally looked into it, the pieces clicked. A quick search through our records revealed over <strong>at least 60 separate accounts</strong> tied to the same organization.</p><p>So we reached out.</p><p>They vaguely apologized and claimed they‚Äôd switch to using the source version instead.</p><p>Which ‚Äî fine. Not ideal, but technically within the rules. What stung more was their complete disinterest in any kind of professional support ‚Äî even when we simply brought up the idea of a volume discount (!). They shut it down immediately. <strong>Apparently, sending satellites into orbit is easier than entertaining the thought of paying for open source support.</strong></p><p>And did they actually switch to the source?</p><p>Of course not.</p><p>They just kept going ‚Äî now using personal Outlook addresses and incrementing the email handles like they were running a script.</p><p>And here's the kicker: if this was a typical SaaS platform with no way to self-host, maybe you‚Äôd understand the logic of trying to game the system. But <strong>you can self-host everything</strong>. For free. No limits. Just a slightly less comfy upgrade experience.</p><p>They chose to go out of their way ‚Äî repeatedly ‚Äî to fake trials <strong>rather than</strong> read a short doc and type a few commands.</p><p><strong>What does that say about the value of our virtual appliance? Maybe more than any of our marketing</strong> ü§î</p><h3 id="%F0%9F%92%AD-what-now">üí≠ What now?</h3><p>We‚Äôre not going to waste days chasing them. But at some point, this goes beyond saving a few bucks: it becomes performance art.</p><p>Just to be sure you get it: they are using <strong>personal Outlook or Gmail accounts</strong>. Used to repeatedly request trials of a product that‚Äôs open source and free to self-host. For the last 10 years. All while voluntarily filling in their <strong>actual company name</strong> in the registration form. Was it a mistake? A subconscious cry for help? Or just an artistic commitment to trolling? We may never know.</p><p>What we do know: this kind of behavior chips away at what makes Open Source sustainable. When you're operating a satellite fleet and running 4,000 VMs on our stack, maybe (just maybe?) it‚Äôs time to stop treating the trial system like a vending machine.</p><p>We'll likely introduce smarter limits to prevent this kind of trial farming. Not to block honest users, but to make sure we can spend our energy where it actually matters: building great software, supporting real users, and keeping Open Source alive and well.</p><p>And if someone from That Company is reading this: you still have time to do the right thing. You‚Äôve got the rocket science down. Now try ethics.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grok's white genocide fixation caused by 'unauthorized modification' (134 pts)]]></title>
            <link>https://www.theverge.com/news/668220/grok-white-genocide-south-africa-xai-unauthorized-modification-employee</link>
            <guid>44004250</guid>
            <pubDate>Fri, 16 May 2025 11:50:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/668220/grok-white-genocide-south-africa-xai-unauthorized-modification-employee">https://www.theverge.com/news/668220/grok-white-genocide-south-africa-xai-unauthorized-modification-employee</a>, See on <a href="https://news.ycombinator.com/item?id=44004250">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img alt="Dominic Preston" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197794/dominic-preston.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197794/dominic-preston.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/197794/dominic-preston.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></p><p><a href="https://www.theverge.com/authors/dominic-preston">Dominic Preston</a> <span>is a news editor with over a decade‚Äôs experience in journalism. He previously worked at <em>Android Police</em> and <em>Tech Advisor</em>.</span></p></div><div id="zephr-anchor"><p>After xAI‚Äôs chatbot Grok spent a few hours on Wednesday telling every X user that would listen that the claim of white genocide in South Africa is highly contentious, the company has blamed the behavior on an ‚Äúunauthorized modification‚Äù to Grok‚Äôs code.</p><p>Wednesday‚Äôs <a href="https://www.theverge.com/news/667179/x-twitter-grok-ai-white-genocide-claims">hours-long outburst</a> saw Grok insert discussion of alleged white genocide in South Africa into various responses on X, no matter the topic. Grok discussed white farmers‚Äô deaths in reply to a video of a cat drinking water, related the song ‚ÄúKill the Boer‚Äù to a question about <em>Spongebob Squarepants</em>, and <a href="https://bsky.app/profile/adashtra.bsky.social/post/3lp5pjb5r722c">broke down the issue in full patois</a>. Even OpenAI CEO Sam Altman <a href="https://www.theverge.com/ai-artificial-intelligence/667885/grok-ai-white-genocide-sam-altman">got in on the action</a>, poking fun at the rival chatbot‚Äôs public breakdown.</p><p>In a <a href="https://x.com/xai/status/1923183620606619649">statement on X</a> the company said that someone had modified the AI bot‚Äôs system prompt, ‚Äúwhich directed Grok to provide a specific response on a political topic.‚Äù That modification ‚Äúviolated xAI‚Äôs internal policies and core values,‚Äù and the company says it has ‚Äúconducted a thorough investigation‚Äù and is implementing new measures to improve ‚Äútransparency and reliability.‚Äù</p><p>Those measures include publishing Grok‚Äôs system level prompts <a href="https://github.com/xai-org/grok-prompts">publicly on GitHub</a>, launching a 24/7 monitoring team to catch issues like this more quickly, and adding ‚Äúadditional checks and measures to ensure that xAI employees can‚Äôt modify the prompt without review.‚Äù</p><p>xAI has had this problem before. The company <a href="https://www.theverge.com/news/618109/grok-blocked-elon-musk-trump-misinformation">blamed an unnamed ex-OpenAI employee</a> in February for pushing a change to Grok‚Äôs prompts that saw the chatbot disregard any sources that accused Elon Musk or Donald Trump of spreading misinformation. At the time xAI‚Äôs head of engineering, Igor Babuschkin, <a href="https://x.com/ibab/status/1893778039634563094">said the employee</a> had been able to make the change ‚Äúwithout asking anyone at the company for confirmation.‚Äù</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama violating llama.cpp license for over a year (120 pts)]]></title>
            <link>https://github.com/ollama/ollama/issues/3185</link>
            <guid>44003741</guid>
            <pubDate>Fri, 16 May 2025 10:36:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ollama/ollama/issues/3185">https://github.com/ollama/ollama/issues/3185</a>, See on <a href="https://news.ycombinator.com/item?id=44003741">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="markdown-body" data-team-hovercards-enabled="true" data-turbolinks="false"><h3 dir="auto">What is the issue?</h3>
<p dir="auto">ollama uses projects like llama.cpp as a statically linked dependency. The terms of the MIT license require that it distribute the copyright notice in both source and binary form. Yet if I <code>grep</code> for "Georgi Gerganov" on my Linux and Windows installation folders for ollama, the copyright notices are nowhere to be found. This is a violation of the terms of the license which should be rectified.</p>
<h3 dir="auto">What did you expect to see?</h3>
<p dir="auto">I expected the copyright notices of dependent projects to be at minimum present within the binary artifacts. Some people online are of the opinion that a mention of llama.cpp should be present in places like the README as well, although the license doesn't strictly require that.</p>
<h3 dir="auto">Steps to reproduce</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">Are there any recent changes that introduced the issue?</h3>
<div data-snippet-clipboard-copy-content="grep -iR 'Georgi Gerganov' AppData/Local/Programs/Ollama/
grep -R 'Georgi Gerganov' /usr/local/bin/ollama
etc."><pre><code>grep -iR 'Georgi Gerganov' AppData/Local/Programs/Ollama/
grep -R 'Georgi Gerganov' /usr/local/bin/ollama
etc.
</code></pre></div>
<h3 dir="auto">OS</h3>
<p dir="auto">Other</p>
<h3 dir="auto">Architecture</h3>
<p dir="auto">Other</p>
<h3 dir="auto">Platform</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">Ollama version</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">GPU</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">GPU info</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">CPU</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">Other software</h3>
<p dir="auto"><em>No response</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After months of coding with LLMs, I'm going back to using my brain (257 pts)]]></title>
            <link>https://albertofortin.com/writing/coding-with-ai</link>
            <guid>44003700</guid>
            <pubDate>Fri, 16 May 2025 10:29:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://albertofortin.com/writing/coding-with-ai">https://albertofortin.com/writing/coding-with-ai</a>, See on <a href="https://news.ycombinator.com/item?id=44003700">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                            <p>A few months ago I needed to build a new infrastructure for my SaaS, as the current PHP+MySQL combo was not fit for purpose anymore. I was excited about the opportunity to make the most of all the new LLMs I'd been playing with, so I set aside my SWE hat and I started acting as a product manager, chatting with Claude about best practices, doing some research on my own and then coming up with a plan, after many back and forths. I ended up choosing Go+Clickhouse.</p><p>When it was time to start coding, I asked Claude to give me a big and complicated markdown file outlining my existing infrastructure, my desired new infrastructure, what I‚Äôm trying to achieve, why I'm doing it, etc.</p><p>So I put it all inside Cursor Notepads and I start prompting away. Cursor writes code, I build it and test it. I'm quite happy with how things are going, the codebase isn't the tidiest but things seems to work. I'm aiming for speed more than code cleanliness - my SaaS business customers told me they need specific data and this new infrastructure is the only way I can deliver it. I have a few more potential customers who are waiting for me to tell them this is ready so they can purchase a plan. Every day I don‚Äôt have this ready, I‚Äôm literally losing money.</p><p>But then a few weeks go by, and the cracks start to show. I start getting frustrated. Every day I feel like I‚Äôm really close to the end product, then I find another issue that sets me back for days. <br>I justify it thinking that I‚Äôve never used Go or Clickhouse before, so it makes sense that it takes me a bit longer than I‚Äôm used to to fix those issues. The problems continue though, and I‚Äôm getting more frustrated. Cursor is not helping as much anymore. I paste in error messages, and I get a fix in response but then something breaks somewhere else. The more detailed the problem, the harder it is for the LLM to provide the actual solution. So I start actually looking at the code more closely, trying to understand it. <br>I‚Äôve been a software engineer for 15 years, and I studied C++ and Java in school, so I do have a rough idea of what‚Äôs happening in these Go files. But I have no concept of Go or Clickhouse best practices.<br>I start educating myself a bit more about those. I read some documentation, articles, I watch a YouTube video about Clickhouse. I become more inquisitive with Claude, asking detailed questions and challenging its answers.</p><p>One morning, I decide to actually inspect closely what‚Äôs all this code that Cursor has been writing. It‚Äôs not like I was blindly prompting without looking at the end result, but I was optimizing for speed and I hadn‚Äôt actually sat down just to review the code. I was just building building building.</p><p>So I do a ‚Äúcoding review‚Äù session. And <strong>the horror ensues.</strong></p><p>Two service files, in the same directory, with similar names, clearly doing a very similar thing. But the method names are different. The props are not consistent. One is called "WebAPIprovider", the other one "webApi". They represent the same exact parameter. The same method is redeclared multiple times across different files. The same config file is being called in different ways and retrieved with different methods.</p><p>No consistency, no overarching plan. It‚Äôs like I'd asked 10 junior-mid developers to work on this codebase, with no Git access, locking them in a room without seeing what the other 9 were doing.</p><p>And before you ask, yes, I was feeding context to LLMs, lots of it. I was mostly using Gemini specifically for its larger context window. Every time I needed a different iteration of the same kind of file I gave specific instructions to take example from those. But it wasn‚Äôt enough.</p><p><strong>Taking a step back</strong></p><p>By now it‚Äôs clear that I need to change my approach. I‚Äôm a software engineer first and foremost, so it‚Äôs stupid not to make the most of my skills. I‚Äôve been teaching myself more about Go and Clickhouse, and I‚Äôve taken a step back from building at full speed. I‚Äôm going through files and rewriting code. Not everything, just the things that make me want to vomit. The language might be different but I know in my head what I want things to look like and how to organise them.</p><p>Since I‚Äôve taken a step back, debugging has become easier. Maybe I‚Äôm not as fast but I don‚Äôt have this weird feeling of ‚Äú<em>I kinda wrote this code but I actually have no idea what‚Äôs in it</em>‚Äù. I‚Äôm still using LLMs, but for dumber things: "rename all occurrences of this parameter", or "here‚Äôs some pseudo code, give me the Go equivalent".</p><p>The hardest part in all of this has been resisting the urge to use AI. I have this amazing tool available and it could write these 10 files in a minute. I‚Äôm wasting time by not using it! And that‚Äôs when it hit me. <strong>I‚Äôve not been using my brain as much. </strong>I'm subconsciously defaulting to AI for all things coding.<br>I‚Äôve been using pen and paper less. As soon as I need to plan a new feature, my first thought is asking o4-mini-high how to do it, instead of my neurons. I hate this. And I‚Äôm changing it.</p><p>So yes, I‚Äôm worried about the impact of AI, but I‚Äôm not worried about the jobs, I‚Äôm worried about losing my mental sharpness, my ability to plan out features and write tidy and functional code.</p><p>So I‚Äôm taking a big step back, really limiting how much AI I use. I‚Äôm defaulting to pen and paper, I‚Äôm defaulting to coding the first draft of that function on my own. And if I‚Äôm not sure I‚Äôll use an LLM to check if that‚Äôs a good solution, if that‚Äôs a good naming convention, or how I can finish that last part.<br>But I‚Äôm not asking it to write new things from scratch, to come up with ideas or to write a whole new plan. I‚Äôm writing the plan. I‚Äôm the senior dev. The LLM is the assistant.</p><p><strong>A happy medium</strong></p><p>Now that I've changed my approach, I‚Äôm not frustrated with LLMs anymore. I have once again very low expectations, so when they do something well it‚Äôs a nice surprise. I‚Äôm trying to be smart about how I use them, they're such a great tool for learning for example. I‚Äôm leveraging them to learn Go, to upskill myself. And then I apply this new knowledge when I code.</p><p>But I feel worried for non coders. I almost feel like they‚Äôre in a worse position now with AI than they were in the no-code era. At least a no-code tool has been written by a human with common sense, and even if its features are limited, at least there‚Äôs some sort of structure.</p><p>‚ÄúVibe coding‚Äù, or whatever ‚Äúcoding with AI without knowing how to code‚Äù is called, is as of today a recipe for disaster, if you‚Äôre building anything that‚Äôs not a quick prototype.</p><p>I cannot imagine what coding with Cursor must feel like, for a non coder. Or maybe I can. Walls of code you don‚Äôt understand, error after error that you keep pasting into a chat box, which gives you back more code, which makes things even messier and complicated and, at some point, just wrong. And impossible to fix.</p><p><strong>A note for AI enthusiasts</strong></p><p>I can hear the horde of AI coding experts screaming at me: you should have used &lt;newest-model&gt;! You should have used Cursor rules! You should have followed this 15 step gamechanger workflow I found on Reddit the other day!</p><p>I have. I tried, I really did. There‚Äôs some things that AI cannot do just yet.</p><p>And while I haven‚Äôt tried every single combination of tools, agentic workflows, etc, I remain confident in my statement.If you don‚Äôt believe me, try being someone with no prior knowledge of Clickhouse, asking an LLM to write a complicated query involving multiple 100M+ rows tables, without triggering memory errors, on a server with limited RAM. <br>It just can‚Äôt, even if you give it the whole SQL schema, even if you link the latest Clickhouse documentation, even if you outline in granular detail your business needs and infrastructure constraints. Newest Gemini can‚Äôt, o4-mini-high can‚Äôt, o3 can‚Äôt, Sonnet 3.7 can‚Äôt.</p><p>Also, if I need hours to set up a complicated system just so the AI doesn‚Äôt build a house of cards instead of the app I needed, is it really worth it? Especially when there‚Äôs no consistency in the model's performance, so even if you find the absolutely perfect workflow, it doesn‚Äôt last for long. Or it stops working as soon as you need it for something slightly different.</p><p>Let me be clear: I'm writing this as someone super excited about new tech, someone who loves to be an early adopter, someone who is still really enthusiastic about AI. I think and hope we‚Äôll get there eventually, but as of right now we‚Äôre in a very weird time, where the tools look amazing, everyone tells you they‚Äôre amazing, but they‚Äôre actually good-but-not-great, and they're potentially making us dumber.</p><p>It‚Äôs a confusing experience. It‚Äôs like I need to get somewhere and I can either walk, or jump on this spaceship that travels at 900mph, but its controls are half in Hungarian and half in Ancient Greek. With enough trial and error I can probably get the spaceship to take me to my destination, but that‚Äôs a lot of work in itself and at the end of the day I‚Äôm left wondering if I should have just walked.</p><p>To make it all worse, I feel like we‚Äôre constantly being gaslighted by benchmarks, by influencers who can now sell you this new magic shovel, and by a flurry of companies that are trying to convince us that it‚Äôs an ‚Äúagent‚Äù and definitely not just another cron job.</p><p>Some days I feel like we‚Äôre also being gaslit by LLM providers. Just look at any AI related subreddit, and you‚Äôll see people having completely opposite experiences, with the same exact model, with the same prompt, on the same day. If you code with AI long enough you‚Äôll be able to relate. One day it‚Äôs amazing, the next day it‚Äôs incredibly stupid.</p><p>Are they throttling the GPUs? Are these tools just impossible to control? What the fuck is going on?</p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leeks and Leaks (103 pts)]]></title>
            <link>https://daniel.haxx.se/blog/2025/05/16/leeks-and-leaks/</link>
            <guid>44003447</guid>
            <pubDate>Fri, 16 May 2025 09:42:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.haxx.se/blog/2025/05/16/leeks-and-leaks/">https://daniel.haxx.se/blog/2025/05/16/leeks-and-leaks/</a>, See on <a href="https://news.ycombinator.com/item?id=44003447">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>On the completely impossible situation of blocking the Tor <code>.onion</code> TLD to avoid leaks, but at the same time not block it to make users able to do what they want.</p>



<h2>dot-onion leaks</h2>



<p>The onion TLD is a Tor specific domain that does not mean much to the world outside of Tor. If you try to resolve one of the domains in there with a <em>normal </em>resolver, you will get told that the domain does not exist.</p>



<p>If you ask your ISP‚Äôs resolver for such a hostname, you also advertise your intentions to speak with that domain to a sizeable portion of the Internet. DNS is inherently commonly insecure and has a habit of getting almost broadcast to lots of different actors. <em>A user on this IP address intends to interact with this .onion site.</em></p>



<p>It is thus of utter importance for Tor users to never use the <em>normal</em> DNS system for resolving .onion names.</p>



<h2>Remedy: refuse them</h2>



<p>To help preventing DNS leaks as mentioned above, Tor people engaged with the IETF and the cooperation ultimately ended up with the publication of <a href="https://datatracker.ietf.org/doc/html/rfc7686">RFC 7686</a>: <em>The ‚Äú.onion‚Äù Special-Use Domain Name</em>. Published in 2015.</p>



<p>This document details how libraries and software should approach handling of this special domain. It basically says that software should to a large degree <em>refuse</em> to resolve such hostnames.</p>



<h2>curl</h2>



<p>In <a href="https://github.com/curl/curl/issues/543">November 2015 we were made aware</a> of the onion RFC and how it says we should filter these domains. At the time nobody seemed keen to work on this, and the problem was added to the <a href="https://curl.se/docs/knownbugs.html">known bugs document</a>.</p>



<p>Eight years later the issue was still lingering in that document and curl had still not done anything about it, when Matt Jolly emerged from the shadows and brought <a href="https://github.com/curl/curl/pull/10705">a PR that finally implemented the filtering</a> the RFC says we should do. Merged into curl on March 30, 2023. Shipped in the curl 8.1.0 release that shipped in May 17, 2023. Two years ago.</p>



<p>Since that release, curl users would not accidentally leak their .onion use.</p>



<p>A curl user that uses Tor would <em>obviously</em> use a SOCKS proxy to access the Tor network like they always have and then curl would let the Tor server do the name resolving as that is the entity here that knows about Tor and the dot onion domain etc.</p>



<p>That‚Äôs the thing with using a proxy like this. A network client like curl can hand the full host name to the proxy server and let that do the resolving magic instead of it being done by the client. It avoids leaking names to local name resolvers.</p>



<h2>Controversy</h2>



<p>It did not take long after curl started support the RFC that Tor themselves had pushed for, when Tor users with <em>creative</em> network setups realized and had opinions.</p>



<p>A <a href="https://github.com/curl/curl/pull/11236">proposal appeared</a> to provide an override of the filter based on an environment variable for users who have a setup where the normal name resolver is already protected somehow and is known to be okay. Environment variables make for horrible APIs and the discussion did not end up in any particular consensus for other solutions so this suggestion did not make it through into code.</p>



<p>This issue has subsequently popped up a few more times when users have had some issues, but no fix or solution has been merged. curl remains blocking this domain. Usually people realize that when using the SOCKS proxy correctly, the domain name works as expected and that has then been the end of the discussions.</p>



<h2>oniux</h2>



<p>This week the Tor project announced a new product of their: <a href="https://blog.torproject.org/introducing-oniux-tor-isolation-using-linux-namespaces/">oniux</a>: <em>a command-line utility providing Tor network isolation for third-party applications using Linux namespaces</em>.</p>



<p>On their introductory web page explaining this new tool they even show it off using a curl command line:</p>



<pre>$ oniux curl http://2gzyxa5ihm7wid.onion/index.html</pre>



<p>(I decided to shorten the hostname here for illustration purposes.)</p>



<p>Wait a second, isn‚Äôt that a .onion hostname in that URL? Now what does curl do with .onion host names since that release two years ago?</p>



<p>Correct: that illustrated command line only works with old curl versions from before we implemented support for RFC 7686. From before we tried to do in curl what Tor indirectly suggested we should do. So now, when we try to do the right thing, curl does not work with this new tool Tor themselves launched!</p>



<p>At least we can‚Äôt say we get do live a dull life.</p>



<h2>Hey this doesn‚Äôt work!</h2>



<p>No really? Tell us all about it. Of course there was immediately <a href="https://github.com/curl/curl/issues/17363">an issue submitted</a> against curl for this, quite rightfully. That tool was quite clearly made for a use case like this.</p>



<p>So how do we fix this?</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The first year of free-threaded Python (144 pts)]]></title>
            <link>https://labs.quansight.org/blog/free-threaded-one-year-recap</link>
            <guid>44003445</guid>
            <pubDate>Fri, 16 May 2025 09:42:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://labs.quansight.org/blog/free-threaded-one-year-recap">https://labs.quansight.org/blog/free-threaded-one-year-recap</a>, See on <a href="https://news.ycombinator.com/item?id=44003445">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Last week, the CPython developers rolled out CPython 3.14.0b1. This week, PyCon
2025 kicks off in Pittsburgh, PA. Both events mark a significant milestone for
the effort to ship and stabilize free-threaded Python.</p>
<p>This is the story of the first year of that effort and how our team at
Quansight played a key role in enabling experimental use of the free-threaded
build with real production workflows that depend on a complex set of
dependencies.</p>

<p>Support for free-threaded Python unlocks the full compute power of modern
hardware with multicore CPUs and GPUs now commonplace. In the GIL-enabled
build, making full use of parallel algorithms that exploit all available
compute resources in Python requires workarounds and careful tuning. The Python
<code>threading</code> module is often not used because the GIL prevents useful parallel
scaling. Instead, many reach for <code>multiprocessing</code>, but spawning processes is
expensive and communicating across processes often requires making expensive
copies of data that would not be necessary in a multithreaded program where
data is implicitly shared between threads.</p>
<p>Having said that, it is not possible for packages that ship compiled code in
their release distributions to support the free-threaded build out of the box.
Any package shipping native code (many Python packages do that) need to be
audited to ensure the package builds and does not suffer thread-safety issues
that are impossible on the GIL-enabled build.</p>
<p>Disabling the GIL required deep structural changes to the CPython
interpreter. Fully supporting the free-threaded build in existing packages
similarly requires fixing structural issues that until now were not big
problems. Things like use of global state in the implementation of a C extension
for convenience or for performance are no longer safe, since the GIL does not
protect simultaneous access from Python to the global state, allowing undefined
behavior via data races. While it is possible to trigger thread safety issues
like this using the <code>threading</code> module even with the GIL, most of the time the
GIL prevented these issues from surfacing. The free-threaded build makes fixing
these issues much more pressing.</p>
<h2 id="major-accomplishments">Major accomplishments</h2>
<p>Alongside the Python runtime team at Meta, we made significant contributions to
enable support for free-threaded Python in a long list of packages and
projects, including:</p>
<ul>
<li>Packaging and project workflow tools like meson, meson-python, the
setup-python GitHub Actions workflow, packaging, pip, and setuptools.</li>
<li>Bindings generators like Cython, pybind11, f2py, and PyO3.</li>
<li>Foundational packages in the PyData ecosystem like NumPy, SciPy, PyArrow,
Matplotlib, pandas, scikit-learn, and scikit-image.</li>
<li>Top dependencies by PyPI downloads like Pillow, PyYAML, yarl, multidict,
and frozenlist.</li>
</ul>
<p>We are also currently looking at popular packages that don't yet ship support,
including CFFI, cryptography, PyNaCl, aiohttp, SQLAlchemy, and grpcio as well
as popular libraries for machine learning workflows like safetensors and
tokenizers.</p>
<p>CPython core developers on our team also contributed several major improvements
that will ship in CPython 3.14:</p>
<ul>
<li>The Python <code>warnings</code> module is now thread-safe by default on the
free-threaded build. It can be made thread-safe on the GIL-enabled build
with a configuration option or runtime command-line flag.</li>
<li>Significant thread safety issues in <code>asyncio</code> have been fixed. Our
benchmarks indicate substantially improved parallel scaling of code using
asyncio with a thread pool runner.</li>
<li>Thread safety overhaul in the <code>ctypes</code> module.</li>
<li>Substantial performance improvements for the free-threaded garbage collector.</li>
<li>Helped implement the deferred reference counting scheme used by the
free-threaded interpreter in 3.14.</li>
<li>Implemented several specializations for the adaptive specializing
interpreter and supported shipping optimizations that bring the
single-threaded performance of free-threaded CPython 3.14 within spitting
distance of the GIL-enabled build.</li>
<li>A huge number of smaller bugfixes and thread safety improvements.</li>
</ul>
<p>We've also written a <a href="https://py-free-threading.github.io/">comprehensive guide</a>
for supporting free-threaded Python in existing apps and packages gleaned from
our experiences. Our hope is that the documentation we've written can be a
valuable resource for the "long tail" of packages that people will want to
update to support free-threaded Python in the coming years.</p>
<p>You can also read more about this effort from the team at Meta on the
<a href="https://engineering.fb.com/2025/05/05/developer-tools/enhancing-the-python-ecosystem-with-type-checking-and-free-threading/">Meta engineering blog</a>.</p>
<h2 id="what-is-the-state-of-the-free-threaded-python-ecosystem">What is the state of the free-threaded Python ecosystem?</h2>
<p>At this time last year, when Python 3.13.0b1 shipped, the wider ecosystem of
Python packages was more or less completely broken on the free-threaded build.
Trying to <code>pip install</code> anything but the simplest package with no dependencies
or only pure-Python dependencies would likely lead to build errors. Most of
these issues were not due to fundamental problems but because of unsupported
default options or minor assumptions broken on the free-threaded build.</p>
<p>Together with package maintainers and other contributors in the community,
we have fixed many of these issues and today things are much better. With the
release of Cython 3.1.0, which ships official support for the free-threaded
build, we also helped fix one of the most significant sources of build issues.</p>
<p>We are currently working on packages that ship compiled code but still do not
yet ship free-threaded wheels. You can track our progress using our manually
updated <a href="https://py-free-threading.github.io/tracking/">status tracking table</a>
or using Hugo van Kemenade's <a href="https://hugovk.github.io/free-threaded-wheels/">automatically updated
tracker</a>.</p>
<h3 id="challenges">Challenges</h3>
<p>As of today, the free-threaded Python build is ready to experiment with. We
need more reports of bad performance and bugs from people with real-world
workflows. Significant performance improvements are possible, particularly in
workflows that make use of multiprocessing and are paying the costs inherent to
that approach. However, many packages still need detailed auditing to discover
thread safety issues. Many Python libraries ship mutable data structures that
will not behave correctly under shared mutating and with no or minimal
documentation on thread safety or multithreaded performance.</p>
<p>As in any change of this magnitude that affects an entire programming language
package ecosystem, we are hitting cases where popular packages do not have the
resources needed to deal with changes needed to support free-threading. This is
particularly true of large legacy packages where few people or even no one
fully understands the code. As a community, we need to understand these issues
in our dependency trees and work towards sustainable maintenance for critical
packages.</p>
<h3 id="how-can-you-help">How can you help?</h3>
<p>Take a look at the <a href="https://py-free-threading.github.io/contributing/">contribution guide</a>
we've added to the main free-threading guide. We're tracking ecosystem-wide
issues and writing the content of the free-threaded guide in the
<a href="https://github.com/Quansight-Labs/free-threaded-compatibility">free-threaded-compatibility</a>
repository hosted on the Quansight-Labs GitHub org.</p>
<p>We also helped launch a <a href="https://discord.gg/rqgHCDqdRr">community Discord</a> to host
discussions about supporting the free-threaded build. Come join us if you're
interested in helping out!</p>
<h2 id="come-to-our-talk-at-pycon">Come to our talk at PyCon!</h2>
<p>I will be giving <a href="https://us.pycon.org/2025/schedule/presentation/42/">a talk at
PyCon</a> with my teammate
<a href="https://github.com/lysnikolaou">Lysandros Nikolaou</a>. If you'll be attending
the conference, please come and watch. We'll be sharing details from our
experiences porting packages to support the free-threaded build. We're hopeful
that the recording on YouTube will be a lasting valuable resource for the
visual learners of the world.</p>
<p>Personally, I believe the free-threaded build is the future of the language,
and am excited that I get to work full-time on enabling that. I'm also hopeful
that the work we're doing now will enable future work in the long tail of
packages used every day by millions of developers and dramatically improve the
performance of the language.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BuyMeACoffee silently dropped support for many countries, and nobody cares (280 pts)]]></title>
            <link>https://zverok.space/blog/2024-08-08-bmac-snafu.html</link>
            <guid>44002333</guid>
            <pubDate>Fri, 16 May 2025 06:23:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zverok.space/blog/2024-08-08-bmac-snafu.html">https://zverok.space/blog/2024-08-08-bmac-snafu.html</a>, See on <a href="https://news.ycombinator.com/item?id=44002333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><strong>Silent changes in payment methods on big creator funding platforms raise some unpleasant questions.</strong></p>

<p><strong>UPD Aug 13, 2024:</strong> The kind-of-official reply from BuyMeACoffee, and <a href="#upd-aug-13-2024">my response to it</a>.</p>

<p><strong>UPD Aug 14, 2024:</strong> A support email to one of the users gives BuyMeACoffee‚Äôs reasons, and also they <a href="#upd-aug-14-2024">blocked me on X (Twitter)</a>.</p>

<h2 id="what-happened">What happened</h2>

<p>Recently, many Ukrainian creators have reported problems with payouts from <a href="https://buymeacoffee.com/">BuyMeACoffee</a>, a creator funding/crowdfunding platform.</p>

<p>At first, the reported support answers were typical corporative ‚Äúwe are sorry that we don‚Äôt care,‚Äù citing ‚Äúcompliance‚Äù and ‚Äúpolicy updates.‚Äù</p>

<blockquote data-conversation="none"><p lang="uk" dir="ltr">–í –æ—Ä–∏–≥—ñ–Ω–∞–ª—ñ, —â–æ–± –≤–µ—Å—Ç–µ—Ä–Ω–∏ —á–∏—Ç–∞–ª–∏ —è–∫—ñ —Ç–∞–º —î –Ω—ñ–∫—á–µ–º–∏ <a href="https://t.co/JxoGgamYLq">pic.twitter.com/JxoGgamYLq</a></p>‚Äî Kar√• Íë≠üá∫üá¶ (@BzickOff) <a href="https://twitter.com/BzickOff/status/1819341225024983310?ref_src=twsrc%5Etfw">August 2, 2024</a></blockquote>


<p>Those reports were widely shared, and our predominantly feeling was ‚Äúshocked but not surprised‚Äù: it is not the first time when a big platform has decided that they are too busy to try to distinguish non-occupied parts of a large country with a 30+m population from occupied parts, and decided they don‚Äôt care. Just recently, for example, the Mercury payment system <strong><a href="https://x.com/United24media/status/1815707715307479244">just blacklisted the entirety of Ukraine</a></strong> alongside Iran and North Korea.</p>

<p>But in a few days, it turned out (again, from support messages shared on Twitter) that BuyMeACoffee <strong>just dropped support for Payoneer</strong> (which works in Ukraine), leaving Stripe (which doesn‚Äôt) as the <strong>only payout method</strong>.</p>

<blockquote data-conversation="none"><p lang="uk" dir="ltr">4. –ë–æ –±–∞–π –º—ñ –∫–æ—Ö–≤–µ –º–∞–ª–∞ –≤ –≥—É–∑–Ω–æ —Å–≤–æ—ó—Ö —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤ —Ç–∞ –∑–∞–∫—Ä–∏–ª–∞ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–≤–æ–¥–∏—Ç–∏ –∫–æ—à—Ç–∏ –Ω–∞–ø—Ä—è–º—É –Ω–∞ –∫–∞—Ä—Ç—É —á–µ—Ä–µ–∑ Wise, –∞ —Ç–∞–∫–æ–∂ —á–µ—Ä–µ–∑ Payoneer. –Ñ–¥–∏–Ω–æ –º–æ–∂–ª–∏–≤–∏–π –º–µ—Ç–æ–¥ –≤–∏–≤–æ–¥—É —Å—Ç–∞—î —Å–µ—Ä–≤—ñ—Å Stripe —è–∫–∏–π –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π –≤ –£–∫—Ä–∞—ó–Ω—ñ —Ç–∞ –¥–æ 14 —Å–µ—Ä–ø–Ω—è –≤—Å—ñ –∞–≤—Ç–æ—Ä–∏ –∑–º–æ–∂—É—Ç—å –≤–≤–æ—Å—Ç–∞–Ω–Ω—î –≤–∏–≤–µ—Å—Ç–∏ —Å–≤–æ—ó –∫–æ—à—Ç–∏. <a href="https://t.co/XIQdNHVllu">pic.twitter.com/XIQdNHVllu</a></p>‚Äî AdrianZP (@AdrianZPcity) <a href="https://twitter.com/AdrianZPcity/status/1821252119875436595?ref_src=twsrc%5Etfw">August 7, 2024</a></blockquote>


<h2 id="did-they-though">Did they, though?</h2>

<p>Currently BuyMeACoffee‚Äôs <a href="https://help.buymeacoffee.com/en/articles/6258038-supported-countries-for-payouts-on-buy-me-a-coffee">support page</a> simply states:</p>

<blockquote>
  <p>Buy Me a Coffee supports payouts to creators in several countries, facilitated through our payment provider, Stripe.</p>
</blockquote>

<p>According to the Internet Archive, in <a href="https://web.archive.org/web/20240229183241/https://help.buymeacoffee.com/en/collections/1907909-payments">February</a>, there was another link to a <a href="https://web.archive.org/web/20231028113814/https://help.buymeacoffee.com/en/articles/6258038-payment-supported-countries">page</a> that listed Stripe and Payoneer, while <a href="https://web.archive.org/web/20240521195730/https://help.buymeacoffee.com/en/collections/1907909-payments">May‚Äôs snapshot</a> already has only Stripe-related links.</p>

<p>So, the change <em>in the documentation</em> happened somewhere between February (that‚Äôs when I personally used it for the last time) and May: Internet Archive doesn‚Äôt provide other snapshots between those two dates.</p>

<p>So, at least the support‚Äôs private answers were aligned with the documentation.</p>

<p><strong>At the same time, there are a couple of funny facts about this deprecation.</strong></p>

<p><strong>First,</strong>  I know for a fact that some Ukrainian creators were receiving payouts via Payoneer somewhere around ~1 month ago; I even saw a credible claim about last week! (I just sent mine, which trickled from a moderately popular blog, will see whether it will come through; it is funny if it will. At least my configured Payoneer payout method is currently active in the BMaC dashboard.)</p>

<p>But more importantly, <strong>there was NO communication about the change</strong>. You wouldn‚Äôt find anything on <a href="https://x.com/buymeacoffee">their Twitter</a> or in the <a href="https://building.buymeacoffee.com/changelog/">public changelog</a> (screenshotted at 2024-08-08 11:05:30 GMT+0300‚Äîjust in case something will magically appear there retroactively):</p>

<p><img src="https://zverok.space/img/2024-08-08-bmac/image00.png" alt="">
<img src="https://zverok.space/img/2024-08-08-bmac/image01.png" alt="">
<img src="https://zverok.space/img/2024-08-08-bmac/image02.png" alt="">
<img src="https://zverok.space/img/2024-08-08-bmac/image03.png" alt=""></p>

<p>As you can see, <strong>there is nothing about payout updates</strong> after the <a href="https://building.buymeacoffee.com/changelog/improved-payout-experience">Nov‚Äô23 post</a>, which mentioned Payoneer and Wise as available options alongside Stripe.</p>

<p>I assure you that <strong>there was no email communication to the creators either</strong>.</p>

<h2 id="how-bad-is-this">How bad is this?</h2>

<p>The difference is public lists of supported countries between <a href="https://www.payoneer.com/resources/global-payment-capabilities/">Payoneer</a> and <a href="https://help.buymeacoffee.com/en/articles/6258038-supported-countries-for-payouts-on-buy-me-a-coffee">Stripe</a> are <strong>95 (ninety-five)</strong> countries and territories. I am not sure that all 95 of those don‚Äôt have working Stripe (say, maybe the Faroe Islands just listed on Payoneer separately while being actually served by Stripe via Danish banks?), but it seems to be <em>a lot</em> anyway.</p>

<p>I‚Äôd like to know what the experience of BMaC users from those countries is like. But what I know for sure is that in Ukraine, a lot of people use the platform as a source of income (sometimes even a primary one). My subscriptions, for example, include but are not limited to:</p>

<ul>
  <li>A singer-songwriter turned paramedic who uses BMaC donations to record her songs on short vacations between duties;</li>
  <li>A writer and culturology scholar turned soldier who funds his studies about Ukrainian culture and history;</li>
  <li>A girl whose two brothers have fallen on the frontlines, and she funds her absolutely unique book club to connect to people and share reading experience with them;</li>
  <li>A small business owner from Kharkiv who uses a ‚Äúbuild in public‚Äù approach for a small coffee shop (and recently got mobilized into Armed Forces);</li>
  <li>‚Ä¶and so on.</li>
</ul>

<p>None of them have BMaC as their primary income, and still, it is a sign of support from their peers and the possibility to continue doing what‚Äôs important to them.</p>

<p><strong>There is no way for any of us to receive the money accumulated on BMaC</strong> anymore. What looks like ‚Äútechnical‚Äù change for some American is realistically <strong>prohibitive</strong> for those not privileged enough to live in Stripe-supported countries. The money is still ‚Äúthere‚Äù, so probably no lawyer can say BMaC has ‚Äústolen them‚Äù‚Äîyou just can‚Äôt neither receive ‚Äúyour‚Äù money nor, at least, give them back to those who sent them.</p>

<p>Note that some BMaC accounts have a lot of supporters, and many of those use ‚Äúyearly‚Äù payments as a sign of their support‚Äîand all of it is currently in some technological limbo.</p>

<p>Now, I am just a mere developer, and I don‚Äôt know much about payout regulations and legal facts. I <em>might</em> charitably assume there was a good reason for dropping Payoneer support (maybe a business one, or maybe it is actually related to some government regulations). The fact itself is extremely distressing for many, but hey, that‚Äôs their business, what do we know.</p>

<p><strong>But the way this policy was implemented‚Äîno prior notice, no choice, no explicit communication of the reasons and possibilities‚Äîis absolutely fascinating, to say the least.</strong> And, honestly, support‚Äôs evasive behavior (and inconsistent reports about ‚Äúsome Payoneer payouts might still work‚Äù) just adds insult to injury.</p>

<p>I am not sure that any service that handles people‚Äôs money as their <em>primary</em> activity, and that does it this way, can be ever trusted. Just sayin‚Äô.</p>

<p>Have a good day, and have a screenshot of the latest message from BuyMeACoffee‚Äôs ever-positive Twitter!</p>

<p><img src="https://zverok.space/img/2024-08-08-bmac/image04.png" alt=""></p>

<p>Such a nice vibe! <em>(Neither their primary account nor <a href="https://x.com/jijosunny">the founder‚Äôs personal one</a>, which both have a history of replying to all relevant tweets, are currently giving any signs they are aware of the situation.)</em></p>

<p>PS: As a side note, <a href="https://en.ain.ua/2023/04/10/what-is-wrong-with-patreon-and-why-ukrainians-urge-canceling-it/">here is a story</a> about why Patreon is not an option for Ukrainians. And <a href="https://www.patreon.com/wargonzoo">here</a> is russian war correspondent‚Äôs Patreon account, mentioned in the story, still proudly present on the platform (though seemingly dormant).</p>

<h2 id="upd-aug-13-2024">UPD: Aug 13, 2024</h2>

<p>So, folks! After a week of poking, we have a kind-of-official answer from <a href="https://x.com/buymeacoffee">@buymeacoffee</a>!</p>

<p>Let‚Äôs unpack it!</p>

<blockquote data-conversation="none"><div lang="en" dir="ltr"><p>This is simply not true. Let us clarify:</p><p>1. We never hold funds. If for some reason we cannot payout (potential fraud, bank account issues), the amount is refunded.</p><p>2. Not one Ukrainian creator's payout is held. We paid out to thousands of Ukrainian creators this week. Please‚Ä¶</p></div>‚Äî Buy Me a Coffee (@buymeacoffee) <a href="https://twitter.com/buymeacoffee/status/1823085434605605055?ref_src=twsrc%5Etfw">August 12, 2024</a></blockquote>


<p><strong>0.</strong> For some reason, this answer is a reply to one of those repeatedly asked them. Not even a standalone X post, nor a blog/changelog entry, nor a newsletter to creators.</p>

<p>So, if you aren‚Äôt a creator who constantly buggers them on X-formerly-Twitter (or follows all the discussions), you might still be unaware of the policy change or the existence of some answers.</p>

<p>The official feeds are still clear of the topic.</p>

<blockquote>
  <ol>
    <li>We never hold funds. If [‚Ä¶] we cannot payout [‚Ä¶] the amount is refunded.</li>
  </ol>
</blockquote>

<p>I believe this <strong>intention</strong> (well, anything less would be a legal suicide), but I would still like to hear:
a. How the ‚Äúcannot payout‚Äù decided and whether ‚Äúthe creator doesn‚Äôt have payout method we like, they should try another one‚Äù qualifies
b. A success story of real refunding (say, $5 to thousands of people) and whether it really works or just a claim.</p>

<blockquote>
  <ol>
    <li>Not one Ukrainian creator‚Äôs payout is held. [‚Ä¶] Please reply with the creator @ if you know of any.</li>
  </ol>
</blockquote>

<p>I <strong>want to believe</strong> that. In my original thread, and many other threads and discussions, I saw a LOT of people with negative experience of interaction with support.</p>

<p>Hopefully, <strong>ALL</strong> of Ukrainian creators who reported payout problems and unhelpful support responses are paid now? (And <strong>hopefully</strong>, it is not by means of ‚Äújust find an EU friend who will agree to receive a payment for you‚Äù?.. Because I saw stories like that).</p>

<p>Also, dear BuyMeACoffee, ‚Äúplease @-us if you know a creator who has problems‚Äù would be <strong>a lot more persuasive</strong> if it wasn‚Äôt in the effing <em>replies under just one tweet</em>.</p>

<p>Just sayin‚Äô</p>

<blockquote>
  <ol>
    <li>Almost all of our creators use our standard payout method powered by Stripe.</li>
  </ol>
</blockquote>

<p>Yeah, the so-called ‚ÄúFirst World‚Äù. But I get it. I do.</p>

<blockquote>
  <p>We‚Äôve decided to focus on this for [‚Ä¶] reasons which are important for the sustainability of the business.</p>
</blockquote>

<p>That‚Äôs your business, and that‚Äôs your right to do so. As inconvenient as it might be for us, we have no say here. But the way it was communicated‚Ä¶</p>

<blockquote>
  <p>we‚Äôre planning to continue offering an alternative payout method [‚Ä¶] for countries that are not supported. As we figure this out, we can confirm that it is business as usual for creators from Ukraine</p>
</blockquote>

<p>It would be such a nice message if communicated clearly &amp; timely üôè</p>

<blockquote>
  <p>We‚Äôd appreciate it if you stop spamming our socials, spreading misinformation about us such as ‚Äúwe blocked withdrawals to Ukraine‚Äù. We‚Äôll have no option but to block you and move on.</p>
</blockquote>

<p>You know what?</p>

<p><strong>We‚Äôd appreciate</strong> if you would be clear, honest, and public about the changes you make.</p>

<p><strong>We‚Äôd appreciate</strong> your support not changing the story from the initial ‚Äúit‚Äôs compliance standards‚Äù, to further ‚Äúyour payout method is not supported, it is your problem‚Äù to final (after the public outrage) ‚Äúoh of course we would help everyone üôè‚Äù.</p>

<p><strong>We‚Äôd appreciate</strong> the public communication of the situation (and no, the reply in one thread in X that ends with passive aggression is not it, thank you very much!)</p>

<p><strong>We‚Äôd appreciate</strong> having this convo, like, a couple of weeks ago.</p>

<p><strong>In conclusion</strong>, I don‚Äôt believe BMaC being malicious or scammy in this situation (and I <em>do want</em> to believe they are looking for a solution that will work for us, too). But the amount of neglect and disregard that was displayed in the way the change was implemented and communicated, and in the interactions with support, makes it almost a textbook case of <strong>bad communication</strong>.</p>

<h2 id="upd-aug-14-2024">UPD: Aug 14, 2024</h2>

<p>One of the Ukrainian creators received an unusually honest explanation of the reasons behind dropping support for Payoneer and Wise: ‚ÄúIncompatible with our upcoming features.‚Äù So, like, a bit more convenience for the First World users, at a price of excluding those not that privileged. (Note also that in this communication there is no sign of that promise of that ‚Äúplanning to continue offering an alternative payout method [‚Ä¶] for countries that are not supported‚Äù which was sweetening their <strong>public</strong> communication.)</p>

<blockquote><div lang="en" dir="ltr"><p>I am disabling <a href="https://twitter.com/buymeacoffee?ref_src=twsrc%5Etfw">@buymeacoffee</a> account tomorrow.</p><p>The company is irresponsibly punches its Ukrainian clients in the guts by disabling Wise and making payouts impossible.</p><p>This isn‚Äôt just about switching platforms; each Ukrainian creator will lose their entire subscriber base. This‚Ä¶ <a href="https://t.co/BPnuAgcutk">pic.twitter.com/BPnuAgcutk</a></p></div>‚Äî Kate from Kharkiv (@BohuslavskaKate) <a href="https://twitter.com/BohuslavskaKate/status/1823303795322822679?ref_src=twsrc%5Etfw">August 13, 2024</a></blockquote>


<p><strong>And another update:</strong> After several days of persistent, yet polite, poking their public X account, asking for public statement (more public than a reply in a thread), BuyMeACoffee <a href="https://x.com/zverok/status/1823757570240340466">just blocked me</a>, reportedly ‚Äúfor spreading misinformation‚Äù:</p>

<p><img src="https://zverok.space/img/2024-08-08-bmac/image05.png" alt=""></p>

<p>Note there is new, previously never stated, pieces of information: about Payoneer working till Nov.1‚Äînot ‚Äúdo your last payout in August,‚Äù as it was previously; and about Wise which they will ‚Äúcontinue to offer‚Äù (currently, to the best of my understanding, they aren‚Äôt).</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Awful German Language (1880) (134 pts)]]></title>
            <link>https://faculty.georgetown.edu/jod/texts/twain.german.html</link>
            <guid>44001765</guid>
            <pubDate>Fri, 16 May 2025 04:09:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://faculty.georgetown.edu/jod/texts/twain.german.html">https://faculty.georgetown.edu/jod/texts/twain.german.html</a>, See on <a href="https://news.ycombinator.com/item?id=44001765">Hacker News</a></p>
<div id="readability-page-1" class="page">
<h2>Mark Twain</h2>
<h3>from <i>A Tramp Abroad</i></h3>

<p>A little learning makes the whole world kin.<br>
--Proverbs xxxii, 7. 

</p><p>I went often to look at the collection of curiosities in Heidelberg
Castle, and one day I surprised the keeper of it with my German.  I spoke
entirely in that language.  He was greatly interested; and after I had
talked a while he said my German was very rare, possibly a "unique"; and
wanted to add it to his museum. 

</p><p>If he had known what it had cost me to acquire my art, he would also
have known that it would break any collector to buy it.  Harris and I had
been hard at work on our German during several weeks at that time, and
although we had made good progress, it had been accomplished under great
difficulty and annoyance, for three of our teachers had died in the mean
time.  A person who has not studied German can form no idea of what a
perplexing language it is. 

</p><p>Surely there is not another language that is so slipshod and
systemless, and so slippery and elusive to the grasp.  One is washed about
in it, hither and thither, in the most helpless way; and when at last he
thinks he has captured a rule which offers firm ground to take a rest on
amid the general rage and turmoil of the ten parts of speech, he turns
over the page and reads, "Let the pupil make careful note of the following
EXCEPTIONS." He runs his eye down and finds that there are more exceptions
to the rule than instances of it.  So overboard he goes again, to hunt for
another Ararat and find another quicksand.  Such has been, and continues
to be, my experience.  Every time I think I have got one of these four
confusing "cases" where I am master of it, a seemingly insignificant
preposition intrudes itself into my sentence, clothed with an awful and
unsuspected power, and crumbles the ground from under me.  For instance,
my book inquires after a certain bird--(it is always inquiring after
things which are of no sort of no consequence to anybody): "Where is the
bird?" Now the answer to this question--according to the book--is that the
bird is waiting in the blacksmith shop on account of the rain.  Of course
no bird would do that, but then you must stick to the book.  Very well, I
begin to cipher out the German for that answer.  I begin at the wrong end,
necessarily, for that is the German idea.  I say to myself, "REGEN (rain)
is masculine--or maybe it is feminine--or possibly neuter--it is too much
trouble to look now.  Therefore, it is either DER (the) Regen, or DIE
(the) Regen, or DAS (the) Regen, according to which gender it may turn out
to be when I look.  In the interest of science, I will cipher it out on
the hypothesis that it is masculine.  Very well--then THE rain is DER
Regen, if it is simply in the quiescent state of being MENTIONED, without
enlargement or discussion--Nominative case; but if this rain is lying
around, in a kind of a general way on the ground, it is then definitely
located, it is DOING SOMETHING--that is, RESTING (which is one of the
German grammar's ideas of doing something), and this throws the rain into
the Dative case, and makes it DEM Regen.  However, this rain is not
resting, but is doing something ACTIVELY,--it is falling--to interfere
with the bird, likely--and this indicates MOVEMENT, which has the effect
of sliding it into the Accusative case and changing DEM Regen into DEN
Regen." Having completed the grammatical horoscope of this matter, I
answer up confidently and state in German that the bird is staying in the
blacksmith shop "wegen (on account of) DEN Regen." Then the teacher lets
me softly down with the remark that whenever the word "wegen" drops into a
sentence, it ALWAYS throws that subject into the GENITIVE case, regardless
of consequences--and therefore this bird stayed in the blacksmith shop
"wegen DES Regens."

</p><p>N.B.--I was informed, later, by a higher authority, that there was an
"exception" which permits one to say "wegen DEN Regen" in certain peculiar
and complex circumstances, but that this exception is not extended to
anything BUT rain. 

</p><p>There are ten parts of speech, and they are all troublesome.  An
average sentence, in a German newspaper, is a sublime and impressive
curiosity; it occupies a quarter of a column; it contains all the ten
parts of speech--not in regular order, but mixed; it is built mainly of
compound words constructed by the writer on the spot, and not to be found
in any dictionary--six or seven words compacted into one, without joint or
seam--that is, without hyphens; it treats of fourteen or fifteen different
subjects, each enclosed in a parenthesis of its own, with here and there
extra parentheses, making pens with pens: finally, all the parentheses and
reparentheses are massed together between a couple of king-parentheses,
one of which is placed in the first line of the majestic sentence and the
other in the middle of the last line of it--AFTER WHICH COMES THE VERB,
and you find out for the first time what the man has been talking about;
and after the verb--merely by way of ornament, as far as I can make
out--the writer shovels in "HABEN SIND GEWESEN GEHABT HAVEN GEWORDEN
SEIN," or words to that effect, and the monument is finished.  I suppose
that this closing hurrah is in the nature of the flourish to a man's
signature--not necessary, but pretty.  German books are easy enough to
read when you hold them before the looking-glass or stand on your head--so
as to reverse the construction--but I think that to learn to read and
understand a German newspaper is a thing which must always remain an
impossibility to a foreigner. 

</p><p>Yet even the German books are not entirely free from attacks of the
Parenthesis distemper--though they are usually so mild as to cover only a
few lines, and therefore when you at last get down to the verb it carries
some meaning to your mind because you are able to remember a good deal of
what has gone before.  Now here is a sentence from a popular and excellent
German novel--which a slight parenthesis in it.  I will make a perfectly
literal translation, and throw in the parenthesis-marks and some hyphens
for the assistance of the reader--though in the original there are no
parenthesis-marks or hyphens, and the reader is left to flounder through
to the remote verb the best way he can: 

</p><p>"But when he, upon the street, the (in-satin-and-silk-covered-
now-very-unconstrained-after-the-newest-fashioned-dressed) government
counselor's wife MET," etc., etc. <a href="#1">[1]</a>

</p><p>That is from THE OLD MAMSELLE'S SECRET, by Mrs. Marlitt.  And that
sentence is constructed upon the most approved German model.  You observe
how far that verb is from the reader's base of operations; well, in a
German newspaper they put their verb away over on the next page; and I
have heard that sometimes after stringing along the exciting preliminaries
and parentheses for a column or two, they get in a hurry and have to go to
press without getting to the verb at all.  Of course, then, the reader is
left in a very exhausted and ignorant state. 

</p><p>We have the Parenthesis disease in our literature, too; and one may see
cases of it every day in our books and newspapers: but with us it is the
mark and sign of an unpracticed writer or a cloudy intellect, whereas with
the Germans it is doubtless the mark and sign of a practiced pen and of
the presence of that sort of luminous intellectual fog which stands for
clearness among these people.  For surely it is NOT clearness--it
necessarily can't be clearness.  Even a jury would have penetration enough
to discover that.  A writer's ideas must be a good deal confused, a good
deal out of line and sequence, when he starts out to say that a man met a
counselor's wife in the street, and then right in the midst of this so
simple undertaking halts these approaching people and makes them stand
still until he jots down an inventory of the woman's dress.  That is
manifestly absurd.  It reminds a person of those dentists who secure your
instant and breathless interest in a tooth by taking a grip on it with the
forceps, and then stand there and drawl through a tedious anecdote before
they give the dreaded jerk.  Parentheses in literature and dentistry are
in bad taste. 

</p><p>The Germans have another kind of parenthesis, which they make by
splitting a verb in two and putting half of it at the beginning of an
exciting chapter and the OTHER HALF at the end of it.  Can any one
conceive of anything more confusing than that? These things are called
"separable verbs." The German grammar is blistered all over with separable
verbs; and the wider the two portions of one of them are spread apart, the
better the author of the crime is pleased with his performance.  A
favorite one is REISTE AB--which means departed.  Here is an example which
I culled from a novel and reduced to English: 

</p><p>"The trunks being now ready, he DE- after kissing his mother and
sisters, and once more pressing to his bosom his adored Gretchen, who,
dressed in simple white muslin, with a single tuberose in the ample folds
of her rich brown hair, had tottered feebly down the stairs, still pale
from the terror and excitement of the past evening, but longing to lay her
poor aching head yet once again upon the breast of him whom she loved more
dearly than life itself, PARTED."

</p><p>However, it is not well to dwell too much on the separable verbs.  One
is sure to lose his temper early; and if he sticks to the subject, and
will not be warned, it will at last either soften his brain or petrify it. 
Personal pronouns and adjectives are a fruitful nuisance in this language,
and should have been left out.  For instance, the same sound, SIE, means
YOU, and it means SHE, and it means HER, and it means IT, and it means
THEY, and it means THEM.  Think of the ragged poverty of a language which
has to make one word do the work of six--and a poor little weak thing of
only three letters at that.  But mainly, think of the exasperation of
never knowing which of these meanings the speaker is trying to convey. 
This explains why, whenever a person says SIE to me, I generally try to
kill him, if a stranger. 

</p><p>Now observe the Adjective.  Here was a case where simplicity would have
been an advantage; therefore, for no other reason, the inventor of this
language complicated it all he could.  When we wish to speak of our "good
friend or friends," in our enlightened tongue, we stick to the one form
and have no trouble or hard feeling about it; but with the German tongue
it is different.  When a German gets his hands on an adjective, he
declines it, and keeps on declining it until the common sense is all
declined out of it.  It is as bad as Latin.  He says, for instance: 

</p><p>SINGULAR

</p><p>Nominative--Mein gutER Freund, my good friend.<br> 
Genitives--MeinES GutEN FreundES, of my good friend. <br>
Dative--MeinEM gutEN Freund, to my good friend. <br>
Accusative--MeinEN gutEN Freund, my good friend. <br>

</p><p>PLURAL

</p><p>N.--MeinE gutEN FreundE, my good friends.  G.--MeinER gutEN
FreundE, of my good friends.  D.--MeinEN gutEN FreundEN,
to my good friends.  A.--MeinE gutEN FreundE, my good friends. 

</p><p>Now let the candidate for the asylum try to memorize
those variations, and see how soon he will be elected. 
One might better go without friends in Germany than take
all this trouble about them.  I have shown what a bother
it is to decline a good (male) friend; well this is
only a third of the work, for there is a variety of new
distortions of the adjective to be learned when the object
is feminine, and still another when the object is neuter. 
Now there are more adjectives in this language than there
are black cats in Switzerland, and they must all be as
elaborately declined as the examples above suggested. 
Difficult?--troublesome?--these words cannot describe it. 
I heard a Californian student in Heidelberg say, in one of
his calmest moods, that he would rather decline two drinks
than one German adjective. 

</p><p>The inventor of the language seems to have taken pleasure
in complicating it in every way he could think of. 
For instance, if one is casually referring to a house,
HAUS, or a horse, PFERD, or a dog, HUND, he spells these
words as I have indicated; but if he is referring to them
in the Dative case, he sticks on a foolish and unnecessary
E and spells them HAUSE, PFERDE, HUNDE.  So, as an added
E often signifies the plural, as the S does with us,
the new student is likely to go on for a month making
twins out of a Dative dog before he discovers his mistake;
and on the other hand, many a new student who could ill
afford loss, has bought and paid for two dogs and only
got one of them, because he ignorantly bought that dog
in the Dative singular when he really supposed he was
talking plural--which left the law on the seller's side,
of course, by the strict rules of grammar, and therefore
a suit for recovery could not lie. 

</p><p>In German, all the Nouns begin with a capital letter. 
Now that is a good idea; and a good idea, in this language,
is necessarily conspicuous from its lonesomeness.  I consider
this capitalizing of nouns a good idea, because by reason
of it you are almost always able to tell a noun the minute
you see it.  You fall into error occasionally, because you
mistake the name of a person for the name of a thing,
and waste a good deal of time trying to dig a meaning
out of it.  German names almost always do mean something,
and this helps to deceive the student.  I translated
a passage one day, which said that "the infuriated tigress
broke loose and utterly ate up the unfortunate fir forest"
(Tannenwald). When I was girding up my loins to doubt this,
I found out that Tannenwald in this instance was a
man's name. 

</p><p>Every noun has a gender, and there is no sense or system
in the distribution; so the gender of each must be
learned separately and by heart.  There is no other way. 
To do this one has to have a memory like a memorandum-book.
In German, a young lady has no sex, while a turnip has. 
Think what overwrought reverence that shows for the turnip,
and what callous disrespect for the girl.  See how it
looks in print--I translate this from a conversation
in one of the best of the German Sunday-school books:

</p><p>"Gretchen. Wilhelm, where is the turnip?

</p><p>"Wilhelm. She has gone to the kitchen. 

</p><p>"Gretchen. Where is the accomplished and beautiful English
maiden?

</p><p>Wilhelm.  It has gone to the opera."

</p><p>To continue with the German genders: a tree is male, its buds
are female, its leaves are neuter; horses are sexless,
dogs are male, cats are female--tomcats included, of course;
a person's mouth, neck, bosom, elbows, fingers, nails, feet,
and body are of the male sex, and his head is male
or neuter according to the word selected to signify it,
and NOT according to the sex of the individual who wears
it--for in Germany all the women either male heads or
sexless ones; a person's nose, lips, shoulders, breast,
hands, and toes are of the female sex; and his hair,
ears, eyes, chin, legs, knees, heart, and conscience
haven't any sex at all.  The inventor of the language
probably got what he knew about a conscience from hearsay. 

</p><p>Now, by the above dissection, the reader will see that in
Germany a man may THINK he is a man, but when he comes to look
into the matter closely, he is bound to have his doubts;
he finds that in sober truth he is a most ridiculous mixture;
and if he ends by trying to comfort himself with the
thought that he can at least depend on a third of this
mess as being manly and masculine, the humiliating second
thought will quickly remind him that in this respect
he is no better off than any woman or cow in the land. 

</p><p>In the German it is true that by some oversight of the inventor
of the language, a Woman is a female; but a Wife (Weib)
is not--which is unfortunate.  A Wife, here, has no sex;
she is neuter; so, according to the grammar, a fish
is HE, his scales are SHE, but a fishwife is neither. 
To describe a wife as sexless may be called under-description;
that is bad enough, but over-description is surely worse. 
A German speaks of an Englishman as the ENGL√ÑNDER; to change
the sex, he adds INN, and that stands for Englishwoman--
ENGL√ÑNDERINN. That seems descriptive enough, but still
it is not exact enough for a German; so he precedes the
word with that article which indicates that the creature
to follow is feminine, and writes it down thus: "die
Engl√§nderinn,"--which means "the she-Englishwoman."
I consider that that person is over-described.

</p><p>Well, after the student has learned the sex of a great
number of nouns, he is still in a difficulty, because he
finds it impossible to persuade his tongue to refer
to things as "he" and "she," and "him" and "her," which
it has been always accustomed to refer to it as "it."
When he even frames a German sentence in his mind,
with the hims and hers in the right places, and then works
up his courage to the utterance-point, it is no use--
the moment he begins to speak his tongue files the track
and all those labored males and females come out as "its."
And even when he is reading German to himself, he always
calls those things "it," where as he ought to read in this way:

</p><p>TALE OF THE FISHWIFE AND ITS SAD FATE <a href="#2">[2]</a>

</p><p>It is a bleak Day.  Hear the Rain, how he pours, and the Hail,
how he rattles; and see the Snow, how he drifts along,
and of the Mud, how deep he is! Ah the poor Fishwife,
it is stuck fast in the Mire; it has dropped its Basket
of Fishes; and its Hands have been cut by the Scales
as it seized some of the falling Creatures; and one Scale
has even got into its Eye.  and it cannot get her out. 
It opens its Mouth to cry for Help; but if any Sound comes
out of him, alas he is drowned by the raging of the Storm. 
And now a Tomcat has got one of the Fishes and she
will surely escape with him.  No, she bites off a Fin,
she holds her in her Mouth--will she swallow her? No,
the Fishwife's brave Mother-dog deserts his Puppies and
rescues the Fin--which he eats, himself, as his Reward. 
O, horror, the Lightning has struck the Fish-basket;
he sets him on Fire; see the Flame, how she licks the
doomed Utensil with her red and angry Tongue; now she
attacks the helpless Fishwife's Foot--she burns him up,
all but the big Toe, and even SHE is partly consumed;
and still she spreads, still she waves her fiery Tongues;
she attacks the Fishwife's Leg and destroys IT; she attacks
its Hand and destroys HER also; she attacks the Fishwife's Leg
and destroys HER also; she attacks its Body and consumes HIM;
she wreathes herself about its Heart and IT is consumed;
next about its Breast, and in a Moment SHE is a Cinder;
now she reaches its Neck--He goes; now its Chin--
IT goes; now its Nose--SHE goes.  In another Moment,
except Help come, the Fishwife will be no more. 
Time presses--is there none to succor and save? Yes! Joy,
joy, with flying Feet the she-Englishwoman comes! But alas,
the generous she-Female is too late: where now is
the fated Fishwife? It has ceased from its Sufferings,
it has gone to a better Land; all that is left of it
for its loved Ones to lament over, is this poor smoldering
Ash-heap. Ah, woeful, woeful Ash-heap! Let us take him
up tenderly, reverently, upon the lowly Shovel, and bear
him to his long Rest, with the Prayer that when he rises
again it will be a Realm where he will have one good square
responsible Sex, and have it all to himself, instead of
having a mangy lot of assorted Sexes scattered all over him
in Spots. 

</p><hr>

<p>There, now, the reader can see for himself that this pronoun
business is a very awkward thing for the unaccustomed tongue. 
I suppose that in all languages the similarities of look
and sound between words which have no similarity in meaning
are a fruitful source of perplexity to the foreigner. 
It is so in our tongue, and it is notably the case in
the German.  Now there is that troublesome word VERM√ÑHLT:
to me it has so close a resemblance--either real or
fancied--to three or four other words, that I never know
whether it means despised, painted, suspected, or married;
until I look in the dictionary, and then I find it means
the latter.  There are lots of such words and they are
a great torment.  To increase the difficulty there are
words which SEEM to resemble each other, and yet do not;
but they make just as much trouble as if they did. 
For instance, there is the word VERMIETHEN (to let,
to lease, to hire); and the word VERHEIRATHEN (another way
of saying to marry). I heard of an Englishman who knocked
at a man's door in Heidelberg and proposed, in the best
German he could command, to "verheirathen" that house. 
Then there are some words which mean one thing when you
emphasize the first syllable, but mean something very
different if you throw the emphasis on the last syllable. 
For instance, there is a word which means a runaway,
or the act of glancing through a book, according to the
placing of the emphasis; and another word which signifies
to ASSOCIATE with a man, or to AVOID him, according to
where you put the emphasis--and you can generally depend
on putting it in the wrong place and getting into trouble. 

</p><p>There are some exceedingly useful words in this language. 
SCHLAG, for example; and ZUG.  There are three-quarters
of a column of SCHLAGS in the dictonary, and a column
and a half of ZUGS.  The word SCHLAG means Blow, Stroke,
Dash, Hit, Shock, Clap, Slap, Time, Bar, Coin, Stamp, Kind,
Sort, Manner, Way, Apoplexy, Wood-cutting, Enclosure,
Field, Forest-clearing. This is its simple and EXACT
meaning--that is to say, its restricted, its fettered meaning;
but there are ways by which you can set it free,
so that it can soar away, as on the wings of the morning,
and never be at rest.  You can hang any word you please
to its tail, and make it mean anything you want to. 
You can begin with SCHLAG-ADER, which means artery,
and you can hang on the whole dictionary, word by word,
clear through the alphabet to SCHLAG-WASSER, which means
bilge-water--and including SCHLAG-MUTTER, which means
mother-in-law.

</p><p>Just the same with ZUG.  Strictly speaking, ZUG means Pull,
Tug, Draught, Procession, March, Progress, Flight, Direction,
Expedition, Train, Caravan, Passage, Stroke, Touch, Line,
Flourish, Trait of Character, Feature, Lineament, Chess-move,
Organ-stop, Team, Whiff, Bias, Drawer, Propensity, Inhalation,
Disposition: but that thing which it does NOT mean--when
all its legitimate pennants have been hung on, has not been
discovered yet. 

</p><p>One cannot overestimate the usefulness of SCHLAG and ZUG. 
Armed just with these two, and the word ALSO, what cannot
the foreigner on German soil accomplish? The German word
ALSO is the equivalent of the English phrase "You know,"
and does not mean anything at all--in TALK, though it
sometimes does in print.  Every time a German opens his
mouth an ALSO falls out; and every time he shuts it he bites
one in two that was trying to GET out. 

</p><p>Now, the foreigner, equipped with these three noble words,
is master of the situation.  Let him talk right along,
fearlessly; let him pour his indifferent German forth,
and when he lacks for a word, let him heave a SCHLAG into
the vacuum; all the chances are that it fits it like a plug,
but if it doesn't let him promptly heave a ZUG after it;
the two together can hardly fail to bung the hole; but if,
by a miracle, they SHOULD fail, let him simply say ALSO!
and this will give him a moment's chance to think of the
needful word.  In Germany, when you load your conversational
gun it is always best to throw in a SCHLAG or two and a ZUG
or two, because it doesn't make any difference how much
the rest of the charge may scatter, you are bound to bag
something with THEM.  Then you blandly say ALSO, and load
up again.  Nothing gives such an air of grace and elegance
and unconstraint to a German or an English conversation
as to scatter it full of "Also's" or "You knows."

</p><p>In my note-book I find this entry:

</p><p>July 1.--In the hospital yesterday, a word of thirteen
syllables was successfully removed from a patient--a
North German from near Hamburg; but as most unfortunately
the surgeons had opened him in the wrong place, under the
impression that he contained a panorama, he died. 
The sad event has cast a gloom over the whole community. 

</p><p>That paragraph furnishes a text for a few remarks about
one of the most curious and notable features of my
subject--the length of German words.  Some German words
are so long that they have a perspective.  Observe these
examples:

</p><p>Freundschaftsbezeigungen. 

</p><p>Dilettantenaufdringlichkeiten. 

</p><p>Stadtverordnetenversammlungen. 

</p><p>These things are not words, they are alphabetical processions. 
And they are not rare; one can open a German newspaper
at any time and see them marching majestically across
the page--and if he has any imagination he can see
the banners and hear the music, too.  They impart
a martial thrill to the meekest subject.  I take a
great interest in these curiosities.  Whenever I come
across a good one, I stuff it and put it in my museum. 
In this way I have made quite a valuable collection. 
When I get duplicates, I exchange with other collectors,
and thus increase the variety of my stock.  Here rare
some specimens which I lately bought at an auction sale
of the effects of a bankrupt bric-a-brac hunter:

</p><p>Generalstaatsverordnetenversammlungen. 

</p><p>Alterthumswissenschaften. 

</p><p>Kinderbewahrungsanstalten. 

</p><p>Unabhaengigkeitserklaerungen. 

</p><p>Wiedererstellungbestrebungen. 

</p><p>Waffenstillstandsunterhandlungen. 

</p><p>Of course when one of these grand mountain ranges goes
stretching across the printed page, it adorns and ennobles
that literary landscape--but at the same time it is a great
distress to the new student, for it blocks up his way;
he cannot crawl under it, or climb over it, or tunnel
through it.  So he resorts to the dictionary for help,
but there is no help there.  The dictionary must draw
the line somewhere--so it leaves this sort of words out. 
And it is right, because these long things are hardly
legitimate words, but are rather combinations of words,
and the inventor of them ought to have been killed. 
They are compound words with the hyphens left out. 
The various words used in building them are in the dictionary,
but in a very scattered condition; so you can hunt
the materials out, one by one, and get at the meaning
at last, but it is a tedious and harassing business. 
I have tried this process upon some of the above examples. 
"Freundshaftsbezeigungen" seems to be "Friendship
demonstrations,"
which is only a foolish and clumsy way of saying "demonstrations
of friendship." "Unabhaengigkeitserklaerungen" seems
to be "Independencedeclarations," which is no improvement
upon "Declarations of Independence," so far as I can see. 
"Generalstaatsverordnetenversammlungen" seems to be
"General-statesrepresentativesmeetings," as nearly as I
can get at it--a mere rhythmical, gushy euphuism for
"meetings of the legislature," I judge.  We used to have
a good deal of this sort of crime in our literature,
but it has gone out now.  We used to speak of a things as a
"never-to-be-forgotten" circumstance, instead of cramping
it into the simple and sufficient word "memorable" and then
going calmly about our business as if nothing had happened. 
In those days we were not content to embalm the thing
and bury it decently, we wanted to build a monument over it. 

</p><p>But in our newspapers the compounding-disease lingers
a little to the present day, but with the hyphens left out,
in the German fashion.  This is the shape it takes:
instead of saying "Mr. Simmons, clerk of the county and
district courts, was in town yesterday," the new form put
it thus: "Clerk of the County and District Courts Simmons
was in town yesterday." This saves neither time nor ink,
and has an awkward sound besides.  One often sees a remark
like this in our papers: "MRS. Assistant District Attorney
Johnson returned to her city residence yesterday for the season."
That is a case of really unjustifiable compounding;
because it not only saves no time or trouble, but confers
a title on Mrs. Johnson which she has no right to. 
But these little instances are trifles indeed, contrasted
with the ponderous and dismal German system of piling
jumbled compounds together.  I wish to submit the following
local item, from a Mannheim journal, by way of illustration:

</p><p>"In the daybeforeyesterdayshortlyaftereleveno'clock Night,
the inthistownstandingtavern called 'The Wagoner' was downburnt. 
When the fire to the onthedownburninghouseresting Stork's
Nest reached, flew the parent Storks away.  But when
the bytheraging, firesurrounded Nest ITSELF caught Fire,
straightway plunged the quickreturning Mother-Stork into
the Flames and died, her Wings over her young ones outspread."

</p><p>Even the cumbersome German construction is not able to
take the pathos out of that picture--indeed, it somehow
seems to strengthen it.  This item is dated away back
yonder months ago.  I could have used it sooner, but I
was waiting to hear from the Father-stork. I am still waiting. 

</p><p>"ALSO!" If I had not shown that the German is a
difficult language, I have at least intended to do so. 
I have heard of an American student who was asked how he
was getting along with his German, and who answered
promptly: "I am not getting along at all.  I have worked
at it hard for three level months, and all I have got
to show for it is one solitary German phrase--'ZWEI GLAS'"
(two glasses of beer). He paused for a moment, reflectively;
then added with feeling: "But I've got that SOLID!"

</p><p>And if I have not also shown that German is a harassing
and infuriating study, my execution has been at fault,
and not my intent.  I heard lately of a worn and sorely
tried American student who used to fly to a certain German
word for relief when he could bear up under his aggravations
no longer--the only word whose sound was sweet and
precious to his ear and healing to his lacerated spirit. 
This was the word DAMIT.  It was only the SOUND that
helped him, not the meaning; <a href="#3">[3]</a> and so, at last, when he
learned that the emphasis was not on the first syllable,
his only stay and support was gone, and he faded away
and died. 

</p><p>I think that a description of any loud, stirring,
tumultuous episode must be tamer in German than in English. 
Our descriptive words of this character have such
a deep, strong, resonant sound, while their German
equivalents do seem so thin and mild and energyless. 
Boom, burst, crash, roar, storm, bellow, blow, thunder,
explosion; howl, cry, shout, yell, groan; battle, hell. 
These are magnificent words; the have a force and magnitude
of sound befitting the things which they describe. 
But their German equivalents would be ever so nice to sing
the children to sleep with, or else my awe-inspiring ears
were made for display and not for superior usefulness
in analyzing sounds.  Would any man want to die in a
battle which was called by so tame a term as a SCHLACHT?
Or would not a comsumptive feel too much bundled up,
who was about to go out, in a shirt-collar and a seal-ring,
into a storm which the bird-song word GEWITTER was employed
to describe? And observe the strongest of the several
German equivalents for explosion--AUSBRUCH.  Our word
Toothbrush is more powerful than that.  It seems to me
that the Germans could do worse than import it into their
language to describe particularly tremendous explosions with. 
The German word for hell--H√∂lle--sounds more like HELLY
than anything else; therefore, how necessary chipper,
frivolous, and unimpressive it is.  If a man were told
in German to go there, could he really rise to thee
dignity of feeling insulted?

</p><p>Having pointed out, in detail, the several vices of
this language, I now come to the brief and pleasant task
of pointing out its virtues.  The capitalizing of the nouns
I have already mentioned.  But far before this virtue stands
another--that of spelling a word according to the sound of it. 
After one short lesson in the alphabet, the student can tell
how any German word is pronounced without having to ask;
whereas in our language if a student should inquire of us,
"What does B, O, W, spell?" we should be obliged to reply,
"Nobody can tell what it spells when you set if off by itself;
you can only tell by referring to the context and finding
out what it signifies--whether it is a thing to shoot
arrows with, or a nod of one's head, or the forward end of a
boat."

</p><p>There are some German words which are singularly
and powerfully effective.  For instance, those which
describe lowly, peaceful, and affectionate home life;
those which deal with love, in any and all forms,
from mere kindly feeling and honest good will toward
the passing stranger, clear up to courtship; those which
deal with outdoor Nature, in its softest and loveliest
aspects--with meadows and forests, and birds and flowers,
the fragrance and sunshine of summer, and the moonlight
of peaceful winter nights; in a word, those which deal with
any and all forms of rest, respose, and peace; those also
which deal with the creatures and marvels of fairyland;
and lastly and chiefly, in those words which express pathos,
is the language surpassingly rich and affective.  There are
German songs which can make a stranger to the language cry. 
That shows that the SOUND of the words is correct--it
interprets the meanings with truth and with exactness;
and so the ear is informed, and through the ear, the heart. 

</p><p>The Germans do not seem to be afraid to repeat a word
when it is the right one.  they repeat it several times,
if they choose.  That is wise.  But in English, when we
have used a word a couple of times in a paragraph,
we imagine we are growing tautological, and so we are weak
enough to exchange it for some other word which only
approximates exactness, to escape what we wrongly fancy
is a greater blemish.  Repetition may be bad, but surely
inexactness is worse. 

</p><hr>

<p>There are people in the world who will take a great
deal of trouble to point out the faults in a religion
or a language, and then go blandly about their business
without suggesting any remedy.  I am not that kind
of person.  I have shown that the German language
needs reforming.  Very well, I am ready to reform it. 
At least I am ready to make the proper suggestions. 
Such a course as this might be immodest in another; but I
have devoted upward of nine full weeks, first and last,
to a careful and critical study of this tongue, and thus
have acquired a confidence in my ability to reform it
which no mere superficial culture could have conferred
upon me. 

</p><p>In the first place, I would leave out the Dative case. 
It confuses the plurals; and, besides, nobody ever knows
when he is in the Dative case, except he discover it
by accident--and then he does not know when or where it
was that he got into it, or how long he has been in it,
or how he is going to get out of it again.  The Dative case
is but an ornamental folly--it is better to discard it. 

</p><p>In the next place, I would move the Verb further up
to the front.  You may load up with ever so good a Verb,
but I notice that you never really bring down a subject
with it at the present German range--you only cripple it. 
So I insist that this important part of speech should be
brought forward to a position where it may be easily seen
with the naked eye. 

</p><p>Thirdly, I would import some strong words from the English
tongue--to swear with, and also to use in describing
all sorts of vigorous things in a vigorous ways. <a href="#4">[4]</a>

</p><p>Fourthly, I would reorganizes the sexes, and distribute
them accordingly to the will of the creator.  This as
a tribute of respect, if nothing else. 

</p><p>Fifthly, I would do away with those great long
compounded words; or require the speaker to deliver
them in sections, with intermissions for refreshments. 
To wholly do away with them would be best, for ideas are
more easily received and digested when they come one at
a time than when they come in bulk.  Intellectual food
is like any other; it is pleasanter and more beneficial
to take it with a spoon than with a shovel. 

</p><p>Sixthly, I would require a speaker to stop when he is done,
and not hang a string of those useless "haven sind gewesen
gehabt haben geworden seins" to the end of his oration. 
This sort of gewgaws undignify a speech, instead of adding
a grace.  They are, therefore, an offense, and should
be discarded. 

</p><p>Seventhly, I would discard the Parenthesis.  Also the
reparenthesis,
the re-reparenthesis, and the re-re-re-re-re-reparentheses,
and likewise the final wide-reaching all-enclosing
king-parenthesis. I would require every individual,
be he high or low, to unfold a plain straightforward tale,
or else coil it and sit on it and hold his peace. 
Infractions of this law should be punishable with death. 

</p><p>And eighthly, and last, I would retain ZUG and SCHLAG,
with their pendants, and discard the rest of the vocabulary. 
This would simplify the language. 

</p><p>I have now named what I regard as the most necessary
and important changes.  These are perhaps all I could
be expected to name for nothing; but there are other
suggestions which I can and will make in case my proposed
application shall result in my being formally employed
by the government in the work of reforming the language. 

</p><p>My philological studies have satisfied me that a gifted person
ought to learn English (barring spelling and pronouncing)
in thirty hours, French in thirty days, and German
in thirty years.  It seems manifest, then, that the
latter tongue ought to be trimmed down and repaired. 
If it is to remain as it is, it ought to be gently
and reverently set aside among the dead languages,
for only the dead have time to learn it. 

</p><p>A FOURTH OF JULY ORATION IN THE GERMAN TONGUE, DELIVERED AT
A BANQUET OF THE ANGLO-AMERICAN CLUB OF STUDENTS BY THE
AUTHOR OF THIS BOOK

</p><p>Gentlemen: Since I arrived, a month ago, in this
old wonderland, this vast garden of Germany, my English
tongue has so often proved a useless piece of baggage
to me, and so troublesome to carry around, in a country
where they haven't the checking system for luggage, that I
finally set to work, and learned the German language. 
Also! Es freut mich dass dies so ist, denn es muss,
in ein haupts:achlich degree, h:oflich sein, dass man
auf ein occasion like this, sein Rede in die Sprache des
Landes worin he boards, aussprechen soll.  Daf:ur habe ich,
aus reinische Verlegenheit--no, Vergangenheit--no, I
mean Hoflichkeit--aus reinishe Hoflichkeit habe ich
resolved to tackle this business in the German language,
um Gottes willen! Also! Sie m√ºssen so freundlich sein,
und verzeih mich die interlarding von ein oder zwei
Englischer Worte, hie und da, denn ich finde dass die
deutsche is not a very copious language, and so when
you've really got anything to say, you've got to draw
on a language that can stand the strain. 

</p><p>Wenn haber man kann nicht meinem Rede Verstehen, so werde
ich ihm sp:ater dasselbe :ubersetz, wenn er solche Dienst
verlangen wollen haben werden sollen sein h:atte. (I don't
know what wollen haben werden sollen sein h√§tte means,
but I notice they always put it at the end of a German
sentence--merely for general literary gorgeousness,
I suppose.)

</p><p>This is a great and justly honored day--a day which is
worthy of the veneration in which it is held by the true
patriots of all climes and nationalities--a day which
offers a fruitful theme for thought and speech; und meinem
Freunde--no, meinEN FreundEN--meinES FreundES--well,
take your choice, they're all the same price; I don't
know which one is right--also! ich habe gehabt haben
worden gewesen sein, as Goethe says in his Paradise
Lost--ich--ich--that is to say--ich--but let us change cars. 

</p><div><p>Also! Die Anblich so viele Grossbrittanischer und Amerikanischer
hier zusammengetroffen in Bruderliche concord, ist zwar
a welcome and inspiriting spectacle.  And what has moved you
to it? Can the terse German tongue rise to the expression of
this impulse? Is it Freundschaftsbezeigungenstadtverordneten-
versammlungenfamilieneigenth:umlichkeiten? Nein,
o nein! This is a crisp and noble word, but it fails
to pierce the marrow of the impulse which has gathered
this friendly meeting and produced diese Anblick--eine
Anblich welche ist gut zu sehen--gut f√ºr die Augen
in a foreign land and a far country--eine Anblick solche
als in die gew:ohnliche Heidelberger phrase nennt man ein
"sch√∂nes Aussicht!" Ja, freilich nat√ºrlich wahrscheinlich
ebensowohl! Also! Die Aussicht auf dem Konigsstuhl
mehr gr:osser ist, aber geistlische sprechend nicht so
sch√∂n, lob' Gott! Because sie sind hier zusammengetroffen,
in Bruderlichem concord, ein grossen Tag zu feirn,
whose high benefits were not for one land and one locality,
but have conferred a measure of good upon all lands
that know liberty today, and love it.  Hundert Jahre
vor¬®ber, waren die Engl√§nder und die Amerikaner Feinde;
aber heut sind sie herzlichen Freunde, Gott sei Dank!
May this good-fellowship endure; may these banners here
blended in amity so remain; may they never any more wave
over opposing hosts, or be stained with blood which
was kindred, is kindred, and always will be kindred,
until a line drawn upon a map shall be able to say:
"THIS bars the ancestral blood from flowing in the veins
of the descendant!"

</p><p>



























NOTES

</p></div><p><a name="1">1.  Wenn er aber auf der Strasse der in Sammt und Seide
geh√ºllten jetz sehr ungenirt nach der neusten mode
gekleideten Regierungsrathin begegnet.

</a></p><p><a name="2">2.  I capitalize the nouns, in the German (and ancient
English) fashion.


</a></p><p><a name="3">3.  It merely means, in its general sense, "herewith."

</a></p><p><a name="4">4.  "Verdammt," and its variations and enlargements, are words
which have plenty of meaning, but the SOUNDS are so mild and
ineffectual that German ladies can use them without sin.  German
ladies who could not be induced to commit a sin by any persuasion
or compulsion, promptly rip out one of these harmless little
words when they tear their dresses or don't like the soup.  It
sounds about as wicked as our "My gracious." German ladies are
constantly saying, "Ach! Gott!" "Mein Gott!" "Gott in Himmel!"
"Herr Gott" "Der Herr Jesus!" etc.  They think our ladies have
the same custom, perhaps; for I once heard a gentle and lovely
old German lady say to a sweet young American girl: "The two
languages are so alike--how pleasant that is; we say 'Ach! Gott!'
you say 'Goddamn.'"
</a></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama's new engine for multimodal models (295 pts)]]></title>
            <link>https://ollama.com/blog/multimodal-models</link>
            <guid>44001087</guid>
            <pubDate>Fri, 16 May 2025 01:43:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/blog/multimodal-models">https://ollama.com/blog/multimodal-models</a>, See on <a href="https://news.ycombinator.com/item?id=44001087">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p><img src="https://ollama.com/public/blog/multimodal.png" alt="multimodality Ollama"></p>

<p>Ollama now supports multimodal models via Ollama‚Äôs new engine, starting with new vision multimodal models:</p>

<ul>
<li><a href="https://ollama.com/library/llama4">Meta Llama 4</a></li>
<li><a href="https://ollama.com/library/gemma3">Google Gemma 3</a></li>
<li><a href="https://ollama.com/library/qwen2.5vl">Qwen 2.5 VL</a></li>
<li><a href="https://ollama.com/library/mistral-small3.1">Mistral Small 3.1</a></li>
<li>and more <a href="https://ollama.com/search?c=vision">vision models</a>.</li>
</ul>

<h2>General Multimodal Understanding &amp; Reasoning</h2>

<h3>Llama 4 Scout</h3>

<pre><code>ollama run llama4:scout
</code></pre>

<p>(Note: this is a 109 billion parameter, mixture-of-experts model.)</p>

<p>Example: asking location-based questions about a video frame:</p>

<p><img src="https://ollama.com/public/blog/multimodal-example1-min.png" alt="video frame analysis example"></p>

<p><img src="https://ollama.com/public/blog/multimodal-example1-answer-min.png" alt="video frame analysis example answer"></p>

<p>You can then ask follow-up questions:</p>

<pre><code>ollama@ollamas-computer ~ % ollama run llama4:scout
&gt;&gt;&gt; what do you see in this image?  /Users/ollama/Downloads/multimodal-example1.png 
Added image '/Users/ollama/Downloads/multimodal-example1.png'
The image depicts a scenic waterfront area with a prominent clock tower at its center. The clock tower, likely the Ferry Building Clock Tower in San Francisco, features a tall structure with a clock face on each side and an American flag flying atop. 

In the foreground, there are several buildings, including what appears to be a marketplace or shopping center, with a parking lot and a street lined with cars. The background showcases a large body of water, possibly a bay or harbor, with a bridge visible in the distance. The sky above is blue with white clouds, suggesting a daytime setting with pleasant weather conditions.

Overall, the image presents a picturesque view of a waterfront area, highlighting the iconic clock tower and the surrounding architecture, as well as the natural beauty of the bay and sky.

&gt;&gt;&gt; how far is this building from the stanford campus in palo alto? 
The distance between the Ferry Building in San Francisco and the Stanford University campus in Palo Alto is approximately 35 miles (56 kilometers). The drive typically takes around 45-60 minutes, depending on traffic conditions.

&gt;&gt;&gt; what's the best way to get there? 
The best way to get to Stanford University from the Ferry Building in San Francisco depends on your preferences and budget. Here are a few options:

1. **By Car**: Take US-101 South to CA-85 South, then continue on CA-101 South. The journey takes around 45-60 minutes, depending on traffic.

2. **By Train and Shuttle**: Take Caltrain from the San Francisco Station (4th and King Streets) to the Palo Alto Station. From there, you can take the Stanford University shuttle (Red and White) to campus. The total travel time is around 1 hour and 15 minutes.

3. **By Ride-hailing Services**: Uber or Lyft from San Francisco to Stanford University takes around 45-60 minutes, depending on traffic.

Please check for any service disruptions or schedule changes before your trip.
</code></pre>

<h3>Gemma 3</h3>

<pre><code>ollama run gemma3
</code></pre>

<p>Example of giving multiple images and asking about their relations.</p>

<p>This demonstrates how a user can input multiple images at once, or do so via follow up prompts and ask about the images.</p>

<p><img src="https://ollama.com/public/blog/multi-images.png" alt="multiple images"></p>

<p><img src="https://ollama.com/public/blog/gemma3-answer.png" alt="gemma 3 answers"></p>

<pre><code>ollama@ollamas-computer ~ % ollama run gemma3
&gt;&gt;&gt; /Users/ollama/Downloads/img1.png /Users/ollama/Downloads/img2.png /Users/ollama/Downloads/img3.png /Users/
... ollama/Downloads/img4.png tell me what animal appears in all 4 images? 

Added image '/Users/ollama/Downloads/img1.png'
Added image '/Users/ollama/Downloads/img2.png'
Added image '/Users/ollama/Downloads/img3.png'
Added image '/Users/ollama/Downloads/img4.png'

The animal featured in all four images is a **llama**.

&gt;&gt;&gt; is there a marine mammal in one of the images? 
Yes! You are absolutely right to spot that. 

In the first image, there‚Äôs a **dolphin** (whale) boxing with the llama.

&gt;&gt;&gt; in the boxing image, is the llama more likely to win or the whale? 

That‚Äôs a fantastic question! Given the image, the llama is *much* more likely to win. 

Here‚Äôs why:

*   **Punching Power:** The llama is clearly throwing a punch with a closed fist, demonstrating intent and force.

*   **Whale‚Äôs Reaction:** The whale is clearly recoiling in pain and fear.

It's a comical mismatch, but the llama has the upper hand in this boxing match!
</code></pre>

<h2>Document scanning</h2>

<h3>Qwen 2.5 VL</h3>

<pre><code>ollama run qwen2.5vl
</code></pre>

<p>Example of using Qwen 2.5 VL for character recognition.</p>

<p><img src="https://ollama.com/public/blog/check-example.jpg" alt="check example"></p>

<p><img src="https://ollama.com/public/blog/check-example-answer-min.png" alt="check answer"></p>

<p>Example understanding and translating vertical Chinese spring couplets to English.</p>

<p><img src="https://ollama.com/public/blog/spring-couplets.jpg" alt="spring couplets example"></p>

<p><img src="https://ollama.com/public/blog/spring-couplets-answer.png" alt="spring couplets answer"></p>

<h2>Ollama‚Äôs new multimodal engine</h2>

<p><img src="https://ollama.com/public/blog/ollama-car.png" alt="Ollama‚Äôs new car"></p>

<p>Ollama has so far relied on the <a href="https://github.com/ggml-org/llama.cpp">ggml-org/llama.cpp</a> project for model support and has instead focused on ease of use and model portability.</p>

<p>As more multimodal models are released by major research labs, the task of supporting these models the way Ollama intends became more and more challenging.</p>

<p>We set out to support a new engine that makes multimodal models first-class citizens, and getting Ollama‚Äôs partners to contribute more directly the community - the GGML tensor library.</p>

<p><strong>What does this mean?</strong></p>

<p>To sum it up, this work is to improve the reliability and accuracy of Ollama‚Äôs local inference, and to set the foundations for supporting future modalities with more capabilities - i.e. speech, image generation, video generation, longer context sizes, improved tool support for models.</p>

<p>Let‚Äôs break down a couple specific areas:</p>

<h4>Model modularity</h4>

<p>Our goal is to confine each model‚Äôs ‚Äúblast radius‚Äù to itself‚Äîimproving reliability and making it easier for creators and developers to integrate new models.</p>

<p>Today, <em>ggml/llama.cpp</em> offers first-class support for text-only models. For multimodal systems, however, the <strong>text decoder</strong> and <strong>vision encoder</strong> are split into separate models and executed independently. Passing image embeddings from the vision model into the text model therefore demands model-specific logic in the orchestration layer that can break specific model implementations.</p>

<p>Within Ollama, each model is fully self-contained and can expose its own projection layer, aligned with how that model was trained. This isolation lets model creators implement and ship their code without patching multiple files or adding cascading <code>if</code> statements. They no longer need to understand a shared multimodal projection function or worry about breaking other models‚Äîthey can focus solely on their own model and its training.</p>

<p>Examples of how some models are implemented are available on <a href="https://github.com/ollama/ollama/tree/main/model/models">Ollama‚Äôs GitHub repository</a>.</p>

<h4>Accuracy</h4>

<p>Large images produce large number of tokens which may exceed the batch size. Processing this correctly with the right positional information is challenging specifically when a single image crosses boundaries.</p>

<p>Ollama adds metadata as it processes images to help improve accuracy.</p>

<p>Some examples:</p>

<ul>
<li><p>Should causal attention be on / off?</p></li>

<li><p>Is it possible to split the image embeddings into batches for processing, and if possible, what are the boundaries when accounting for quality of output, and the computer being used for inference? If an image is split in the wrong place, the quality of output goes down. This is usually defined by the model, and can be checked in its paper?</p></li>
</ul>

<p>Many other local inference tools implement this differently; while a similar result may be achieved, it does not follow how the models were designed and trained.</p>

<h4>Memory management</h4>

<p><strong>Image caching</strong></p>

<p>Once an image is processed, Ollama caches it so later prompts are faster; the image remains in cache while it is still being used and is not discarded for memory-cleanup limits.</p>

<p><strong>Memory estimation &amp; KV cache optimizations</strong></p>

<p>Ollama collaborates with hardware manufacturers and an operating system partner to make sure the correct hardware metadata is detected for Ollama to better estimate and optimize for memory usage. For many firmware releases, partners will validate/test it against Ollama to minimize regression and to benchmark against new features.</p>

<p>Ollama has some KV cache optimizations to improve how memory can be efficiently used. Ollama configures causal attention at the individual model level instead of configuring as a group.</p>

<p>Examples:</p>

<ul>
<li><p>Google DeepMind‚Äôs Gemma 3 leverages sliding window attention, and Ollama can leverage that to allocate a subset or a portion of the model‚Äôs context length to improve performance, and because of the memory efficiency, this means we can increase the context length of the model on the same system or use the remaining memory for higher concurrency.</p></li>

<li><p>To uniquely support <strong>Meta‚Äôs Llama 4 Scout and Maverick models</strong>, Ollama has implemented chunked attention, attention tuning to support longer context size, specific 2D rotary embedding, and in the mixture-of-experts type of model.</p></li>
</ul>

<p>If a model‚Äôs attention layer isn‚Äôt fully implemented, such as sliding window attention or chunked attention, it may still <em>‚Äòwork‚Äô</em>. However, because this isn‚Äôt how the model was trained, the end user may begin to see erratic or degraded output by the model itself over time. This becomes especially prominent the longer the context / sequence due to cascading effects.</p>

<h3>What‚Äôs next</h3>

<ul>
<li>Support longer context sizes</li>
<li>Support thinking / reasoning</li>
<li>Tool calling with streaming responses</li>
<li>Enabling computer use</li>
</ul>

<h3>Acknowledgements</h3>

<p><strong>Models</strong></p>

<p>Thank you to the teams working to release openly available and powerful vision models - Google DeepMind, Meta Llama, Alibaba Qwen, Mistral, IBM Granite, and the many countless people &amp; labs in the community.</p>

<p><strong>GGML</strong></p>

<p>Thank you to the GGML team for the tensor library that powers Ollama‚Äôs inference ‚Äì accessing GGML directly from Go has given a portable way to design custom inference graphs and tackle harder model architectures not available before in Ollama.</p>

<p><strong>Hardware partners</strong></p>

<p>Thank you to hardware partners for collaborating and helping to improve inference on a large number of devices - NVIDIA, AMD, Qualcomm, Intel, and Microsoft.</p>

    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I was a Theranos whistleblower. Here's what I think Elizabeth Holmes is up to (118 pts)]]></title>
            <link>https://www.statnews.com/2025/05/15/theranos-whistleblower-tyler-shultz-commentary-elizabeth-holmes-billy-evans-haemanthus-startup/</link>
            <guid>44001041</guid>
            <pubDate>Fri, 16 May 2025 01:34:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.statnews.com/2025/05/15/theranos-whistleblower-tyler-shultz-commentary-elizabeth-holmes-billy-evans-haemanthus-startup/">https://www.statnews.com/2025/05/15/theranos-whistleblower-tyler-shultz-commentary-elizabeth-holmes-billy-evans-haemanthus-startup/</a>, See on <a href="https://news.ycombinator.com/item?id=44001041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I always knew Elizabeth Holmes would have a second act. But I‚Äôm shocked it‚Äôs starting while she‚Äôs still behind bars.</p>



<p>Elizabeth is now serving an 11-year sentence for defrauding investors in Theranos and has been <a href="https://www.statnews.com/2024/01/19/elizabeth-holmes-theranos-banned-government-health-programs/">banned</a> from participating in federal health programs. The company was built on her claim that, at 19 years old, she had invented a device capable of running hundreds of tests from just a single drop of blood. It was a bold vision that captivated the world, and a story everyone wanted to believe ‚Äî <a href="https://www.statnews.com/2019/05/01/from-protegee-to-whistleblower-a-former-theranos-scientist-says-elizabeth-holmes-should-come-forward-and-apologize/">including me</a>.</p>



<p>Now her partner, Billy Evans, has founded a company called <a href="https://www.statnews.com/2025/05/12/elizabeth-holmes-theranos-partner-new-biotech-startup-haemanthus-analysts-reaction/">Haemanthus</a>, which also touts a diagnostic blood-testing device that uses only small amounts of blood. While Elizabeth has no legal affiliation with the company, her fingerprints and pricks are all over it. Even the naming convention feels familiar: Instead of combining the Greek roots of therape√≠a and diagnosis like she did with Theranos, this time it‚Äôs haema and anthos<em> </em>‚Äî Greek for ‚Äúblood‚Äù and ‚Äúflower.‚Äù&nbsp;</p>


<figure><a href="https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434.jpg"><img decoding="async" width="1024" height="1536" src="https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-1024x1536.jpg" alt="" srcset="https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-1024x1536.jpg 1024w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-328x492.jpg 328w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-768x1152.jpg 768w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-500x750.jpg 500w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-2048x3072.jpg 2048w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-1600x2400.jpg 1600w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-640x960.jpg 640w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-384x576.jpg 384w" sizes="(min-width: 625px) and (max-width: 767px) 719px, (min-width: 768px) and (max-width: 899px) 835px, (min-width: 900px) and (max-width: 1023px) 635px, (min-width: 1024px) and (max-width: 1199px) 803px, (min-width: 1200px) and (max-width: 1439px) 994px, (min-width: 1440px) 982px, 576px"></a><figcaption><span>Tyler Shultz</span><wbr><cite>STAT</cite></figcaption></figure>



<p>Since she reported to prison about two years ago, by far the most common question people have asked me is: ‚ÄúDid Elizabeth start Theranos with the intention to revolutionize health care, or did she intend to commit fraud from the start?‚Äù I think the answer is neither. While I wasn‚Äôt there in the early days, I believe Elizabeth started Theranos in the pursuit of fame. She wanted to be the female Steve Jobs. She wanted to be known as the youngest self-made female billionaire, fly in private jets, advise the president, rub shoulders with Mark Zuckerberg and Elon Musk. She wanted to be on the cover of magazines. And she got what she wanted. Just a few months ago, she was on the cover of People magazine. Let me repeat: Even from prison, she‚Äôs landing on the cover of People magazine.</p>



<p>As the news of Haemanthus broke, I started to hear a similar question: ‚ÄúDo you think Haemanthus intends to revolutionize health care, or do you think it‚Äôs another fraud?‚Äù Once again ‚Äî neither. I think it‚Äôs just another chapter of her narrative.&nbsp;</p>



<p>Anyone who thought this story was over doesn‚Äôt know Elizabeth and her ability to craft a narrative. When she leaves prison, there‚Äôs no doubt she‚Äôll manage to appear remorseful without admitting guilt. And if she‚Äôs credited with launching a company from a prison cell? That‚Äôs more legendary than the Hewlett-Packard garage or Mark Zuckerberg‚Äôs dorm room. It‚Äôs a story so compelling, people will want it to be true.</p>



<p>Elizabeth is more than a masterful storyteller. She writes the story. Every detail is meticulously chosen to build her narrative, captivate her audience, and distort reality. The black turtleneck, deep voice, college dropout ‚Äî the parallels with Steve Jobs were no accident.&nbsp;</p>





<p>The very first time I met her, Elizabeth told my grandfather, George Shultz (who served in the&nbsp;Cabinet in the Nixon and Reagan administrations and became a member of the Theranos board), and then-Sen. Dianne Feinstein that Quest and LabCorp would eventually try to take her down. She was planting the seeds of a conspiracy years before she would need to lean on it. She also told us Theranos would never go public, preempting pressure from investors. She completely controlled the narrative from day one.&nbsp;</p>



<p>The Elizabeth I knew would openly talk about her decision to not have friends, to never date, to never have kids, all because she was devoted to Theranos. (This, of course, was another lie. She was dating Sunny Balwani, the COO and president of the company.) However, when motherhood made for a useful character reboot, she became one ‚Äî twice. Her first child arrived just days before her trial was set to begin, and her second between her conviction and sentencing.&nbsp;</p>



<p>Before her sentencing, the <a href="https://www.nytimes.com/2023/05/07/business/elizabeth-holmes-theranos-interview.html" target="_blank" rel="noopener">New York Times</a> wrote about how she had become ‚ÄúLiz,‚Äù the devoted mother and volunteer who had ditched the black turtleneck. The article even included carefully posed photographs of Elizabeth with her two children. I feel profound sadness for those kids. Most parents would try to shield their children from that type of attention, but Elizabeth put them front and center, using them the same way she used to use her black turtleneck. They were props to support the narrative she was writing.&nbsp;</p>



<p>Similarly, I suspect that Haemanthus‚Äô purpose is not to build a product that will improve the lives of people (or their pets ‚Äî materials for potential investors say that they plan to start with veterinary medicine). Nor is it to make VC returns.&nbsp;</p>


		
		


<p>The purpose is to build something that works, so that she can point to it and say, ‚ÄúSee, I would have figured it out. Theranos was never a fraud.‚Äù If Haemanthus‚Äô technology does indeed work and she eventually makes that claim, whatever reporter she‚Äôs chosen to interview her needs to ask, ‚ÄúHow many tens of thousands of people were you willing to hurt on the path to building something that worked?‚Äù&nbsp;</p>



<p>This raises another question I‚Äôve been asked this week: ‚ÄúDo you think the technology will work?‚Äù Probably not, but I don‚Äôt have enough information to really know. Raman spectroscopy has been used for nearly 100 years and hasn‚Äôt achieved the resolution needed to perform this type of blood testing from a few drops of blood. Maybe two additional light sources, as described in Haemanthus‚Äô patent, combined with AI solves the resolution problems, but consider me a skeptic until a third party validates its method. Over the past few years a number of people have reached out to me saying they‚Äôve invented ‚Äúthe Theranos that works,‚Äù and several of them have concepts similar to this: apply AI to a mountain of noise and get a thousand test results. While many actually do have innovative approaches, none have come close to realizing the Theranos vision.&nbsp;</p>





<p>It‚Äôs been reported that Haemanthus aims to raise $50 million, and I have little doubt it‚Äôll succeed. Billy Evans is independently wealthy, an heir to a Southern California hotel chain, so it‚Äôs likely a significant portion will be self-funded. And I can think of at least one outside investor who might get involved. Tim Draper, Elizabeth‚Äôs former neighbor and one of Theranos‚Äô earliest backers, has maintained an unusual and unwavering loyalty to her. Even after her conviction, he continues to publicly defend her. Draper seems to be as invested in the narrative as she is, and is in a position to fund a company simply for the sake of that narrative.&nbsp;</p>



<p>My advice to prospective Haemanthus investors is this: Ask yourself what motivates Elizabeth Holmes. Money? Possibly. But when she walks out of prison, she‚Äôll be returning to a partner who‚Äôs already a millionaire many times over. Mission? I think she‚Äôs thoroughly proven that she doesn‚Äôt care about improving the lives of patients. The real question is: Will Elizabeth serve the purpose of your fund ‚Äî or by investing in her, will you simply be serving her purpose of writing her rise-from-the-ashes narrative?</p>



<p>Failed founders deserve second chances. But at Theranos, Elizabeth had a thousand chances ‚Äî opportunities to listen to her scientists, to tell the truth to her board and investors, to change course and right the ship. At each opportunity she consistently showed poor judgement and a lack of character. Even when the Wall Street Journal began reporting on the company, she still had hundreds of millions in the bank, world-class scientists on payroll, and the most prestigious board in Silicon Valley. In that moment, she had more resources, talent, and opportunity than most founders will ever see ‚Äî and she blew it.</p>



<p>She‚Äôs not Edison. This isn‚Äôt going to work on the thousand-and-first try.</p>



<p><em><a href="https://www.tyler-shultz.com/" target="_blank" rel="noopener">Tyler Shultz</a> is a scientist, founder, and whistleblower best known for exposing fraud at Theranos and advocating for ethics in innovation.</em></p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Teal ‚Äì A statically-typed dialect of Lua (187 pts)]]></title>
            <link>https://teal-language.org/</link>
            <guid>44000759</guid>
            <pubDate>Fri, 16 May 2025 00:40:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://teal-language.org/">https://teal-language.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44000759">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="below-the-fold">

<!---------------------------------------------------------------------------->

<h2>About Teal</h2>
<p>
Teal is a statically-typed dialect of <a href="https://www.lua.org/">Lua</a>.
It extends Lua with type annotations, allowing you to specify arrays, maps and records,
as well as interfaces, union types and generics.
</p>
<p>
It aims to fill a niche similar to that of TypeScript in the JavaScript world,
but adhering to Lua's spirit of minimalism, portability and embeddability.
</p>
<p>
Is it implemented as a compiler, <a href="https://github.com/teal-language/tl">tl</a>,
which compiles <tt>.tl</tt> source code into <tt>.lua</tt> files.
</p>

<!---------------------------------------------------------------------------->

<h2>A taste of Teal</h2>
<p>
Here is a quick taste of what Teal code looks like:
</p>
<pre><b>local function</b> add(a: <span>number</span>, b: <span>number</span>): <span>number</span>
   <b>return</b> a + b
<b>end</b>

<b>local</b> s = add(1, 2)
print(s)
</pre>
<p>
...or check out this page's source code!
</p>
<p>
You can also play with the Teal compiler right from your browser,
using the <a href="https://teal-playground.netlify.app/" target="_blank">Teal Playground</a>.
</p>

<!---------------------------------------------------------------------------->

<h2>Install Teal</h2>
<p>
If you're using <a href="https://luarocks.org/">LuaRocks</a>, you can install the compiler with:
</p>
<pre>luarocks install tl
</pre>
<p>
You can also find <a href="https://github.com/teal-language/tl/releases">pre-compiled
binaries</a> for Linux and Windows.</p>
<p>
To build larger projects, you probably won't want to run <tt>tl</tt> on each
file individually. We recommend using <a href="https://github.com/teal-language/cyan">Cyan</a>,
the build tool designed for Teal.
</p>
<p>
You can also install <a href="https://github.com/teal-language/vscode-teal/">vscode-teal</a> for
Visual Studio Code integration, <a href="https://github.com/teal-language/teal-language-server/">teal-language-server</a>
for NeoVim and others, and <a href="https://github.com/teal-language/awesome-teal">more</a>.
</p>

<!---------------------------------------------------------------------------->

<h2>Documentation</h2>
<p>
You can browse the Teal documentation <a href="https://teal-language.org/book/index.html">online</a>.
</p>
<p>
Here are some recorded talks discussing the history of Lua and types,
outlining the motivations behind Teal and talking about the project's progress:
</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=OPyBQRndLUk">Minimalism versus types (2019)</a></li>
<li><a href="https://www.youtube.com/watch?v=HfnjUCRzRKU">Minimalistic typed Lua is here (2020)</a></li>
<li><a href="https://www.youtube.com/watch?v=OqXbnaDR8QY">What's next for Teal (2021)</a></li>
<li><a href="https://www.youtube.com/watch?v=OPyBQRndLUk">Five years of Teal: minimalism versus growth in language design (2024)</a></li>
<li><a href="https://www.youtube.com/watch?v=Uq_8bckDxaU">What should Teal be? - musings on FOSS project directions (2025)</a></li>
</ul>



<!---------------------------------------------------------------------------->

<h2>Community</h2>
<p>
Development happens on <a href="https://github.com/teal-language">GitHub</a>.
</p>
<p>
We have a <a href="https://github.com/teal-language/tl/discussions">community forum</a> hosted by GitHub.
You can also chat with us via Matrix at <a href="https://matrix.to/#/#teal-language_community:gitter.im">#teal-language_community:gitter.im</a>.
</p>
<p>
Teal is a project started by <a href="https://hisham.hm/">Hisham Muhammad</a>,
developed by a <a href="https://github.com/teal-language/tl/graphs/contributors">growing number of contributors</a>
and is written using Teal itself!
</p>

<!---------------------------------------------------------------------------->

<h2>License</h2>
<p>
Teal is free and open source software.<br>Its <a href="https://github.com/teal-language/tl/blob/master/LICENSE">license</a> is MIT, the same as Lua.
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA keeps ancient Voyager 1 spacecraft alive with Hail Mary thruster fix (310 pts)]]></title>
            <link>https://www.theregister.com/2025/05/15/voyager_1_survives_with_thruster_fix/</link>
            <guid>44000700</guid>
            <pubDate>Fri, 16 May 2025 00:29:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/05/15/voyager_1_survives_with_thruster_fix/">https://www.theregister.com/2025/05/15/voyager_1_survives_with_thruster_fix/</a>, See on <a href="https://news.ycombinator.com/item?id=44000700">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>NASA has revived a set of thrusters on the nearly 50-year-old Voyager 1 spacecraft after declaring them inoperable over two decades ago.&nbsp;</p>
<p>It's a nice long-distance engineering win for the team at NASA's Jet Propulsion Laboratory, responsible for keeping the <a href="https://www.theregister.com/2024/11/09/voyager_closes_in_on_a/">venerable Voyager spacecraft</a> flying - and a critical one at that, as clogging fuel lines threatened to derail the backup thrusters currently in use.&nbsp;</p>
<p>The things you have to deal with when your spacecraft is operating more than four decades beyond its original mission plan, eh? Voyager 1 launched in 1977.</p>

    

<p>JPL <a href="https://www.jpl.nasa.gov/news/nasas-voyager-1-revives-backup-thrusters-before-command-pause/?utm_source=iContact&amp;utm_medium=email&amp;utm_campaign=1-nasajpl&amp;utm_content=voyager20250514" rel="nofollow">reported</a> Wednesday that the maneuver, completed in March, restarted Voyager 1's primary roll thrusters, which are used to keep the spacecraft aligned with a tracking star. That guide star helps keep its high-gain antenna aimed at Earth, now over 15.6 billion miles away, and far beyond the reach of any telescope.</p>

        


        

<p>Those primary roll thrusters stopped working in 2004 after a pair of internal heaters lost power. Voyager engineers long believed they were broken and unfixable. The backup roll thrusters in use are now at risk due to residue buildup in their fuel lines, which could cause failure as early as this fall.</p>
<p>Without roll thrusters, Voyager 1 would lose its ability to stay properly oriented and eventually drift out of contact. To make matters worse, the only antenna on Earth with enough power to send commands to the Voyager probes - the 230-foot-wide DSS-43 dish in Australia - is undergoing an upgrade shutdown until next February, with only a couple of brief operational windows in August and December.</p>

        

<p>While other dishes around the world can still receive data from the Voyagers, those windows are the only opportunities to send commands to the spacecraft, which remain the most distant human-made objects in existence.</p>
<ul>

<li><a href="https://www.theregister.com/2025/05/14/saudi_cubesat_sls/">Saudi CubeSat gets golden ticket on doomed SLS rocket</a></li>

<li><a href="https://www.theregister.com/2025/05/08/nasa_jpl_director_quits/">NASA JPL boss bails for 'personal reasons' as budget cuts bite</a></li>

<li><a href="https://www.theregister.com/2025/05/02/nasa_funding_slashed/">Trump wants to fire quarter of NASA budget into black hole ‚Äì and not in a good way</a></li>

<li><a href="https://www.theregister.com/2025/05/08/nasa_budget_reaction/">ESA feeling weightless and unwanted amid proposed NASA cuts</a></li>
</ul>
<h3>Do and maybe die; don't do and definitely die</h3>
<p>With a hard deadline in place, the Voyager team had few options. So it turned to the primary roll thrusters that failed in 2004 - with a big IF in mind.&nbsp;</p>
<p><em>If</em> those heaters weren't actually dead, and <em>if</em> a power switch had been flipped by a circuit disturbance, then restoring that switch might bring the thrusters back online and preserve control of the spacecraft if the backup system failed.</p>
<p>The idea was to restore heater power to the primary roll thrusters, then let Voyager 1 drift far enough from its guide star that its onboard system would automatically fire the thrusters to correct its course.</p>
<p>If the heaters were still off when the dormant thrusters auto-fired due to attitude drift, "it could trigger a small explosion," JPL noted. And since it takes the radio signal more than 23 hours to travel from Voyager back to Earth, the team wouldn't know for an entire day.&nbsp;</p>
<blockquote>

<p>It was yet another miracle save for Voyager</p>
</blockquote>
<p>Fortunately, as the return signal arrived, the Voyager team saw signs that the thruster heaters were back online and the effort was a success.&nbsp;</p>
<p>"These thrusters were considered dead. And that was a legitimate conclusion," Voyager mission propulsion leader Todd Barber said in JPL's report. "It's just that one of our engineers had this insight that maybe there was this other possible cause and it was fixable. It was yet another miracle save for Voyager."</p>

        

<p>The Voyager missions have faced many challenges since leaving Earth more than 47 years ago. Most recently, Voyager 1 spent months <a href="https://www.theregister.com/2024/03/14/voyager_1_not_dead/">returning gibberish</a> instead of usable data before being fixed, and both probes have had to <a href="https://www.theregister.com/2025/03/06/voyager_power_down/">switch off</a> <a href="https://www.theregister.com/2024/10/03/voyager_2_plasma_instrument/">scientific instruments</a> as power dwindles and systems fail. <a href="https://www.theregister.com/2024/09/12/voyager_power_issues/">Thruster</a> <a href="https://www.theregister.com/2017/12/04/voyager_1_mission_extension_thanks_assmbler_code_and_thrusters_unused_since_1980/">issues</a> have also been a problem for Voyager 1 before, but the little craft that could <a href="https://www.theregister.com/2024/06/14/voyager_1_restored/">continue to soldier on</a> at the edge of interstellar space.</p>
<p>Both spacecraft will go dark eventually, closing our most distant eye into the universe beyond our solar system. But not quite yet.&nbsp;¬Æ</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A leap year check in three instructions (390 pts)]]></title>
            <link>https://hueffner.de/falk/blog/a-leap-year-check-in-three-instructions.html</link>
            <guid>43999748</guid>
            <pubDate>Thu, 15 May 2025 21:57:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hueffner.de/falk/blog/a-leap-year-check-in-three-instructions.html">https://hueffner.de/falk/blog/a-leap-year-check-in-three-instructions.html</a>, See on <a href="https://news.ycombinator.com/item?id=43999748">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>
      With the following code, we can check whether a year 0 ‚â§ <i>y</i> ‚â§ 102499 is a leap year with only about 3 CPU instructions:
    </p>
  
    <pre><code>bool is_leap_year_fast(uint32_t y) {
    return ((y * 1073750999) &amp; 3221352463) &lt;= 126976;
}</code></pre>
      
    <p>How does this work? The answer is surprisingly complex. This article explains it, mostly to have some fun with bit-twiddling; at the end, I'll briefly discuss the practical use.</p>
      
    <p>This is how a leap year check is typically implemented:</p>

    <pre><code>bool is_leap_year(uint32_t y) {
    if ((y % 4) != 0) return false;
    if ((y % 100) != 0) return true;
    if ((y % 400) == 0) return true;
    return false;
}</code></pre>

    <p>We are using the <a href="https://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar">proleptic Gregorian calendar</a>, which extends the Gregorian calendar backward from its introduction in 1582 and includes a year 0. Thus, we don't need to treat years before 1582 any differently. For simplicity, we ignore negative years and use an unsigned year.</p>

    <h2>Optimizing the standard approach</h2>

    <p>Let's first do some simple speedups so that we get a good baseline. I'm not sure where to assign credit ‚Äì these tricks have probably been reinvented independently many times.</p>

    <p>We can replace <code>(y % 100) != 0</code> by <code>(y % 25) != 0</code>: we already know that <code>y</code> is a multiple of 2<sup>2</sup>, so if it is also a multiple of 5<sup>2</sup>, it is a multiple of 2<sup>2</sup> ‚ãÖ 5<sup>2</sup> = 100. Similarly, we can replace <code>(y % 400) == 0</code> by <code>(y % 16) == 0</code>: we already know that <i>y</i> is a multiple of 5<sup>2</sup>, so if it is also a multiple of 2<sup>4</sup>, it is a multiple of 5<sup>2</sup> ‚ãÖ 2<sup>4</sup> = 400.</p>

    <pre><code>bool is_leap_year1(uint32_t y) {
    if ((y % 4) != 0) return false;
    if ((y % 25) != 0) return true;
    if ((y % 16) == 0) return true;
    return false;
}</code></pre>
      
    <p>This is useful because we can now replace modulo by 4 and by 16 with bit masking. There is a further trick well-known to compiler implementors on how to lower the modulo by 25. Compiling <code>(x % 25) != 0</code> with gcc and translating back to C, we get <code>x * 3264175145 &gt; 171798691</code>. With multiplication at a typical latency of 3 cycles and modulo of at least 20 cycles, this is a great improvement. I will only give an intuition on how it works; more details can be found in
    </p><ul>
      <li><a href="https://doi.org/10.1002/spe.2689">Faster remainder by direct computation: Applications to compilers and software libraries</a> paper by Daniel Lemire, Owen Kaser, and Nathan Kurz for lowering modulo by a constant in general;</li>
      <li><a href="https://doi.org/10.1002/spe.3172">Euclidean affine functions and their application to calendar algorithms</a> by Cassio Neri and Lorenz Schneider more specifically for calendar calculations; and</li>
      <li><a href="https://davecturner.github.io/2020/08/07/leapyear-optimization.html">Identifying leap years</a> by David Turner for the leap year check (including formal proofs!).</li>
    </ul>

    <p>Where do the magic numbers come from? We have</p>

    <p><span>2<sup>32</sup> ‚ãÖ 19/25 = 3264175144.96 (exactly).</span>
    </p>

    <p>So by multiplying with 3264175145, we calculate approximately the fractional part of multiplying with (19/25). If this was an exact multiplication, we would get a fractional part of zero for multiples of 25. We multiply with a number that is too high by 0.04 though, so the error can be up to 0.04 ‚ãÖ (2<sup>32</sup> - 1) = 171798691.8, which is where the second magic number comes from.</p>

    <p>This trick doesn't work quite as nicely for <code>x % 100</code>, where we need one more fixup instruction, so the reduction from <code>y % 100</code> to <code>y % 25</code> was helpful.</p>

    <p>Our leap year check now looks like this:
    </p><pre><code>bool is_leap_year2(uint32_t y) {
    if ((y &amp; 3) != 0) return false;
    if (y * 3264175145u &gt; 171798691u) return true;
    if ((y &amp; 15) == 0) return true;
    return false;
}</code></pre>

    <p>Note that modern compilers like gcc or clang will produce <a href="https://godbolt.org/z/GeK6Trfx6">something</a> like <code>is_leap_year2</code> from <code>is_leap_year1</code>, so there is not much point in doing this in C source, but it might be useful in other programming languages.</p>

    <p>This code typically compiles to branching assembly. In practice, inputs are typically fairly predictable, so that is not necessarily bad. If we want to avoid branch misprediction penalties, at the price of also slowing down the best case, we can shuffle things around a bit and get <a href="https://godbolt.org/z/PWs8saMYd">branchless code</a> (and also a decent code golf candidate):</p>
    <pre><code>bool is_leap_year3(uint32_t y) {
    return !(y &amp; ((y % 25) ? 3 : 15));
}</code></pre>

    <p>If you want to see some more calendar calculation speedups, make sure to check out <a href="https://hueffner.de/falk/blog/Optimizing%20with%20Novel%20Calendrical%20Algorithms">https://jhpratt.dev/blog/optimizing-with-novel-calendrical-algorithms/</a> by Jacob Pratt.

    </p><h2>Finding a bit-twiddling approach</h2>

    <p>Can we improve the leap year calculation by giving up on correctness for all inputs? After all, we typically don't care whether the year 3584536493 is a leap year; indeed, Python, C#, and Go only support years 0 (or 1) to 9999 (at which point the drift relative to the seasons is already more than 4 days). My thought was that if something shorter exists, it will basically look like some weird hashing using magic constants, so I wanted to try some small forms and guess the constants by brute force. The form <code>(y * f) &lt;= t</code> seems useful but is not powerful enough yet. One of my candidates was adding a mask: <code>((y * f) &amp; m) &lt;= t</code>. Now we have 96 bits to guess, which is not feasible by brute force alone. Let's use <a href="https://github.com/Z3Prover/z3">z3</a>, a solver that supports bitvector constraints that is perfectly suited for this:</p>

    <pre><code>import z3

BITS = 32
f, m, t, y = z3.BitVecs('f m t y', BITS)

def target(y):
    return z3.And((y &amp; 3) == 0, z3.Or(z3.URem(y, 25) != 0, (y &amp; 15) == 0))

def candidate(x):
    return z3.ULE((x * f) &amp; m, t)

solver = z3.Solver()
solver.add(z3.ForAll(y, z3.Implies(z3.ULE(y, 400),
                                   candidate(y) == target(y))))

if solver.check() == z3.sat:
    print(f'found solution: {solver.model()}')
else:
    print('no solution found')
</code></pre>

    <p>In a few seconds, this finds constants that yield the correct result for a nontrivial year range. Extending the range, I eventually found constants that yield the correct result from year 0 up to the year 102499, in about half an hour of computation time, and proved that this is the optimum for 32 bits:</p>

    <pre><code>bool is_leap_year_fast(uint32_t y) {
    const uint32_t f = 1073750999u;
    const uint32_t m = 3221352463u;
    const uint32_t t = 126976u;
    return ((y * f) &amp; m) &lt;= t;
}</code></pre>

    <h2>Explanation</h2>

    <p>How does it work? It seems surprising, almost magic, that we can squeeze all these computations into three instructions. However, the considerations above give us most of the tools to understand it.</p>

    <p>Here's our constants in binary, with four relevant bit ranges indicated by letters:</p>

    <pre>   <span>A</span>                 <span>D</span>
f: <span>01</span>0000000000000000<span>10001111010111</span>
m: <span>11</span>0000000000000<span>11111</span>00000000<span>1111</span>
t: <span>00</span>0000000000000<span>11111</span>00000000<span>0000</span>
                  <span>B</span>            <span>C</span>
</pre>

    <p>Let's first consider for the product <i>p</i> := <i>y</i> ‚ãÖ <i>f</i> what the masking with <i>m</i> and comparison with <i>t</i> does. In block <i>A</i>, the bits in <i>t</i> are 0, and therefore, the result is false as soon as any bit in <i>A</i> is set in <i>p</i>. Otherwise, block <i>B</i> becomes relevant. Here all bits in <i>t</i> are 1, so the result is true as soon as any bit in <i>B</i> is unset in <i>p</i>. Otherwise, for block <i>C</i> we require that all bits are unset in <i>p</i>. In this way, a bunch of comparisons of bit ranges are all united into a single <code>&lt;=</code>.</p>

    <p>Thus, we could rewrite <code>is_leap_year_fast</code> as follows:
    </p><pre><code>bool is_leap_year_fast2(uint32_t y) {
    uint32_t p = y * 1073750999u;
    const uint32_t A = 0b11000000000000000000000000000000;
    const uint32_t B = 0b00000000000000011111000000000000;
    const uint32_t C = 0b00000000000000000000000000001111;
    if ((p &amp; A) != 0) return false;
    if ((p &amp; B) != B) return true;
    if ((p &amp; C) == 0) return true;
    return false;
}</code></pre>

    <p>That looks suspiciously like <code>is_leap_year2</code>! And indeed, the three conditions have exactly the same purpose. We'll show
      </p><ol>
	<li> <code>(p &amp; A) != 0</code> triggers when <code>(y % 4) != 0</code>;</li>
	<li> <code>(p &amp; B) != B</code> triggers when <code>(y % 100) != 0</code>;</li>
	<li> <code>(p &amp; C) == 0</code> triggers when <code>(y % 16) == 0</code> (and therefore <code>(y % 400) == 0</code>, since we already know <i>y</i> is a multiple of 25).</li>
      </ol>

    <h3>Two easy cases: (1) and (3)</h3>
          
    <p>(1): The 1 bit in <i>A</i> in <i>f</i> reproduces the two low bits of <i>y</i> into <i>p</i> at <i>A</i>. This cannot be clobbered by the result of the multiplication with the bits in <i>D</i>: the maximum value we can get is 102499 ‚ãÖ (f &amp; D) = 940428325, which has only 30 bits. Thus, checking that A is zero in p is equivalent to checking whether <i>y</i> is 0 modulo 4.</p>

    <p>(3): Checking that none of the lowest 4 bits is set in <i>p</i> is checking whether <i>p</i> is 0 modulo 16. However, we actually want to check <i>y</i>. This is no problem: It suffices to look only at the lowest 4 bits of <i>f</i>, and <i>f</i> is 1111<sub>2</sub> = 15 there. Multiplying with 15 = 3 ‚ãÖ 5 introduces no additional factor of 2, so divisibility by 16 is unchanged.
    

    </p><h3>The interesting case: (2)</h3>

    <p>Next, let's try to find out for which numbers <i>p</i> &amp; <i>B</i> ‚â† <i>B</i>. For this, the 1 bit in <i>f</i> &amp; <i>A</i> does not play a role, so consider the bits in <i>f</i> &amp; <i>D</i>. They are 10001111010111<sub>2</sub> = 9175. Let's see for which numbers the test hits.</p>

    <pre><code>&gt;&gt;&gt; B = 0b00000000000000011111000000000000
&gt;&gt;&gt; s = [y for y in range(5000) if ((y * 9175) &amp; B) == B]
&gt;&gt;&gt; for i in range(0, len(s), 16): print(*(f'{n:4d}' for n in s[i:i+16]))
  14   57   71  100  114  157  171  200  214  257  271  300  314  357  371  400
 414  457  471  500  514  557  571  600  614  657  671  700  714  757  771  800
 814  857  871  900  914  957  971 1000 1014 1057 1071 1100 1114 1157 1171 1200
1214 1257 1271 1300 1314 1357 1371 1400 1414 1457 1471 1500 1514 1557 1571 1600
1614 1657 1671 1700 1714 1757 1771 1800 1814 1857 1871 1900 1914 1957 1971 2000
2014 2057 2071 2100 2114 2157 2171 2200 2214 2257 2271 2300 2314 2357 2371 2400
2414 2457 2471 2500 2514 2557 2571 2600 2614 2657 2671 2700 2714 2757 2771 2800
2814 2857 2871 2900 2914 2957 2971 3000 3014 3057 3071 3100 3114 3157 3171 3200
3214 3257 3271 3300 3314 3357 3371 3400 3414 3457 3471 3500 3514 3557 3571 3600
3614 3657 3671 3700 3714 3757 3771 3800 3814 3857 3871 3900 3914 3957 3971 4000
4014 4057 4071 4100 4114 4157 4200 4214 4257 4300 4314 4357 4400 4414 4457 4500
4514 4557 4600 4614 4657 4700 4714 4757 4800 4814 4857 4900 4914 4957
</code></pre>

    <p>Multiples of 100 are here as desired, but also a bunch of other numbers. This is not a problem as long as none of them are a multiple of 4, since those were already filtered out in the previous step. Also, 0 is missing, but that's not a problem either, since 0 is also a multiple of 400.</p>

    <p>Let's try to understand the pattern. At first glance, it looks very simple: we have *14, *57, *71, and *00. However, from 4171 on, *71 drops out (did you notice?). We also have new patterns popping up later. Let's analyze this. The following Python program</p>
<pre><code>def test(y):
    B = 126976
    return ((y * 9175) &amp; B) == B

active = set()
for y in range(120000):
    r = y % 100
    if test(y):
        if r not in active:
            print(f'{y:6}: started *{r:02}')
            active.add(r)
    else:
        if r in active:
            print(f'{y:6}: stopped *{r:02}')
            active.remove(r)
</code></pre><p>
outputs
</p><pre><code>    14: started *14
    57: started *57
    71: started *71
   100: started *00
  4171: stopped *71
 32843: started *43
 36914: stopped *14
 65586: started *86
 69657: stopped *57
 98329: started *29
102500: stopped *00
</code></pre>

    <p>So, from 102500 on, we no longer catch multiples of 100, which explains why 102499 is the last number for which <code>is_leap_year_fast</code> returns the correct result. We also see that below that, no number is a multiple of 4 except for the multiples of 100 (conveniently, we can check that knowing only the last two decimal digits). This concludes the proof of condition (2), if we trust this brute-force enumeration; but let's continue and understand better why we get exactly these numbers.</p>

    <p>Let's look into why we're getting the multiples of 100 first. The factor 9175 is close to a multiple of 1/100 in 17-bit fixed point representation:</p>
    <p><span>2<sup>17</sup> ‚ãÖ 7/100 = 9175.04 (exactly).</span>
    </p>

    <p>Multiplying a multiple of 100 by 9175.04 gives an integer (a multiple of 7) at bits 17 and above and 17 zero bits below that. E.g.</p>
    <p><span>9175.04 ‚ãÖ 500 = 10001100000000000000000<sub>2</sub>, with 100011<sub>2</sub> = 35 = 5 ‚ãÖ 7.</span>
    </p>

    <p>Multiplying a multiple of 100 by 9175 gives slightly less:</p>
    <p><span>9175 ‚ãÖ 500 = 10001100000000000000000<sub>2</sub> ‚àí 500 ‚ãÖ 0.04 = 10001011111111111101100<sub>2</sub>.</span>
    </p>

    <p>In general, subtracting a little bit from a number ending in a lot of zeroes creates a number that ends in a lot of ones, except at the very end. Here, we check the 5 bits in <i>B</i>. For <i>y</i> a multiple of 100, these are guaranteed be all 1 until the accumulated error reaches the lower end of <i>B</i>, which happens only after <i>y</i> = 2<sup>12</sup> / 0.04 = 102400, so this fits.</p>

    <p>Now where do the other numbers like 14, 57, and 71 come from? Let's look at it from a different angle. We have 9175 = 2<sup>17</sup> ‚ãÖ 0.06999969482421875 (exactly) and <i>B</i> = 2<sup>17</sup> ‚ãÖ 0.96875, so
    </p><p><span><i>p</i> &amp; <i>B</i> </span>             <span>= <i>B</i></span>
      <span>‚áî {<i>y</i> ‚ãÖ 0.06999969482421875} </span>  <span>‚â• 0.96875, where {<i>x</i>} is the fractional part of <i>x</i></span>
      <span>‚áî 6.999969482421875<i>y</i> mod 100 </span> <span>‚â• 96.875</span>
    </p>

    <p>This is another way to see why multiples of 100 are accepted: For them, 7<i>y</i> mod 100 is 0, so 6.999969482421875<i>y</i> mod 100 comes out slightly below 100, and drops below 96.875 only after <i>y</i> = (100 ‚àí 96.875) / (7 ‚àí 6.999969482421875) = 102400.

    </p><p>To understand the other numbers showing up in our sequence, let's first consider what the solutions would be if we had a clean 7 in this inequality:</p>
    <p><span>7y mod 100 </span> <span>‚â• 96.875</span>
      <span>‚áî 7y mod 100 </span> <span>‚àà {97, 98, 99}.</span>
    </p>

    <p>To find the solutions to this, we first need the modular inverse of 7 modulo 100, that is, the number <i>x</i> such that 7<i>x</i> mod 100 = 1. We can calculate it by using the extended Euclidean algorithm, or just using <a href="https://www.wolframalpha.com/input?i=modular+inverse+of+7+modulo+100">some online calculator</a>, which tells us the result is 43. Then the solutions are 43 ‚ãÖ 97 (mod 100), 43 ‚ãÖ 98 (mod 100), and 43 ‚ãÖ 99 (mod 100), which is 71, 14, and 57 (mod 100), respectively. This explains why we're initially seeing numbers of the form *14, *57, and *71. It also explains why we stop seeing e.g. *71 after 4071: While 7 ‚ãÖ 4171 = 29197, we have 6.999969482421875 ‚ãÖ 4171 = 29196.872711181640625, which (modulo 100) is less than 96.875. Similarly, 32843 pops up because the accumulated error (7 ‚àí 6.999969482421875) ‚ãÖ 32843 = 1.002288818359375 exceeds one. With a bit more effort here, we could manually reproduce the output of the Python program above and check that none of these numbers is a multiple of 4.</p>

    <h2>Extension to other bitwidths</h2>

    <p>Now that we understand how the trick works, we can try to find parameters for other bitwidths. The variable parts are the location of block <i>B</i> and the fraction of 100 in <i>f</i> &amp; <i>D</i>.</p>

    <pre><code>uint64_t score(uint64_t f, uint64_t m, uint64_t t) {
      for (uint64_t y = 0; ; y++)
          if ((((y * f) &amp; m) &lt;= t) != is_leap_year(y))
              return y;
  }
  
  int main() {
      uint64_t best_score = 0;
      for (int k = 0; k &lt; BITS; k++) {
          for (int k2 = 0; k2 &lt; k; k2++) {
              uint64_t t = (1ULL &lt;&lt; k) - (1ULL &lt;&lt; k2);
              uint64_t m = (0b11ULL &lt;&lt; (BITS - 2)) | t | 0b1111;
              for (int n = 0; n &lt; 100; n++) {
                  uint64_t f = (0b01ULL &lt;&lt; (BITS - 2)) | (((1ULL &lt;&lt; k) * n) / 100);
                  uint64_t new_score = score(f, m, t);
                  if (new_score &gt; best_score) {
                      printf("%llu %llu %llu: %llu (%d %d %d)\n",
                             f, m, t, new_score, k, k - k2, n);
                      best_score = new_score;
                  }
              }
          }
      }
      return 0;
  }
  </code></pre>

    <p>For <code>BITS = 64</code>, in about 7 minutes we find <i>f</i> = 4611686019114582671, <i>m</i> = 13835058121854156815, <i>t</i> = 66571993088, which is correct up to <i>y</i> = 5965232499. This is nice because 5965232499 &gt; 2<sup>32</sup>, so any 32-bit year can be tested correctly with this variant.</p>

    <p>Is this the highest we can go with 64 bit? Perhaps there's some other constants that work even better? I couldn't immediately find a way to prove it, so I employed the tried-and-true method of getting someone else to do it for me by <a href="https://codegolf.stackexchange.com/q/275505/116815">posting it to the Code Golf StackExchange</a>. Indeed after only 1h user ovs posted a very good result, and two days later user Exalted Toast posted a <a href="https://codegolf.stackexchange.com/a/275541">proof</a> that 5965232499 is indeed the best possible range for 64 bits, also employing the z3 solver.</p>

    <h2>Benchmark</h2>

    <p>Getting meaningful benchmarks is tricky, since the function takes just such a tiny amount of time, and moreover for the branching versions execution time depends on the patterns of the input. Let's try two extreme cases: always the year 2025, and completely random years. These are the results of the <a href="https://hueffner.de/falk/blog/benchmark.cc">benchmark</a> on an i7-8700K (Coffee Lake, 4.7 GHz), compiled wirh <code>g++ -O3 -fno-tree-vectorize</code>:</p>

    <table>
      <thead>
          <tr>
              <th></th>
              <th>2025 (ns)</th>
              <th>random (ns)</th>
          </tr>
      </thead>
      <tbody>
            <tr>
              <td><code>is_leap_year</code></td>
              <td>0.65</td>
              <td>2.61</td>
            </tr>
            <tr>
              <td><code>is_leap_year2</code></td>
              <td>0.65</td>
              <td>2.75</td>
            </tr>
            <tr>
              <td><code>is_leap_year3</code></td>
              <td>0.67</td>
              <td>0.88</td>
            </tr>
            <tr>
              <td><code>is_leap_year_fast</code></td>
              <td>0.69</td>
              <td>0.69</td>
          </tr>
       </tbody>
    </table>

    <p>Some weird things stand out here:</p>
    <ul>
    <li><code>is_leap_year2</code> is actually marginally slower than <code>is_leap_year</code> in the random case. This is surprising because <code>y % 100</code> needs one instruction more than the trick in <code>is_leap_year2</code>.</li>
    <li><code>is_leap_year3</code> is a bit slower for random data than for the fixed value. This is surprising because it doesn't do any branching.</li>
    </ul>

    <p>I have no explanation for this other than benchmarking is hard.</p>

    <p>The new function <code>is_leap_year_fast</code> is 3.8 times faster than the standard implementation for random data, and about 6% slower for the perfectly predictable input. Overall, that seems pretty solid.</p>


    <h2>Conclusion</h2>

    <p>So, is this really worthwhile? Should we replace e.g. the <a href="https://github.com/python/cpython/blob/5cdd49b3f4cbdcf0472a65fd0c723912c3d48211/Modules/_datetimemodule.c#L416">CPython datetime implementation</a> with this trick? Well, it depends. Most years being queried in practice will be the current year, or at least fairly predictable, in which case we don't have a great advantage. To fully justify a change, ideally we would have a benchmark with realistic data that employs a leap year check as a subroutine, rather than just microbenchmarking. I would be happy to hear about any such results!</p>

    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Initialization in C++ is bonkers (2017) (181 pts)]]></title>
            <link>https://blog.tartanllama.xyz/initialization-is-bonkers/</link>
            <guid>43999492</guid>
            <pubDate>Thu, 15 May 2025 21:27:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.tartanllama.xyz/initialization-is-bonkers/">https://blog.tartanllama.xyz/initialization-is-bonkers/</a>, See on <a href="https://news.ycombinator.com/item?id=43999492">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        


        

        <p><span itemprop="articleBody"><p>C++ pop quiz time: what are the values of <code>a.a</code> and <code>b.b</code> on the last line in <code>main</code> of this program?</p>

<figure><pre><code data-lang="cpp"><span>#include</span> <span>&lt;iostream&gt;</span><span>
</span>
<span>struct</span> <span>foo</span> <span>{</span>
    <span>foo</span><span>()</span> <span>=</span> <span>default</span><span>;</span>
    <span>int</span> <span>a</span><span>;</span>
<span>};</span>

<span>struct</span> <span>bar</span> <span>{</span>
    <span>bar</span><span>();</span>
    <span>int</span> <span>b</span><span>;</span>
<span>};</span>

<span>bar</span><span>::</span><span>bar</span><span>()</span> <span>=</span> <span>default</span><span>;</span>

<span>int</span> <span>main</span><span>()</span> <span>{</span>
    <span>foo</span> <span>a</span><span>{};</span>
    <span>bar</span> <span>b</span><span>{};</span>
    <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>a</span><span>.</span><span>a</span> <span>&lt;&lt;</span> <span>' '</span> <span>&lt;&lt;</span> <span>b</span><span>.</span><span>b</span><span>;</span>
<span>}</span></code></pre></figure>

<p>The answer is that <code>a.a</code> is <code>0</code> and <code>b.b</code> is indeterminate, so reading it is undefined behaviour. Why? Because initialization in C++ is bonkers.</p>

<hr>

<h4 id="default--value--and-zero-initialization">Default-, value-, and zero-initialization</h4>

<p>Before we get into the details which cause this, I‚Äôll introduce the concepts of default-, value- and zero-initialization. Feel free to skip this section if you‚Äôre already familiar with these.</p>

<figure><pre><code data-lang="cpp"><span>T</span> <span>global</span><span>;</span>       <span>//zero-initialization, then default-initialization</span>

<span>void</span> <span>foo</span><span>()</span> <span>{</span>
    <span>T</span> <span>i</span><span>;</span>         <span>//default-initialization</span>
    <span>T</span> <span>j</span><span>{};</span>       <span>//value-initialization (C++11)</span>
    <span>T</span> <span>k</span> <span>=</span> <span>T</span><span>();</span>   <span>//value-initialization</span>
    <span>T</span> <span>l</span> <span>=</span> <span>T</span><span>{};</span>   <span>//value-initialization (C++11)</span>
    <span>T</span> <span>m</span><span>();</span>       <span>//function-declaration</span>

    <span>new</span> <span>T</span><span>;</span>       <span>//default-initialization</span>
    <span>new</span> <span>T</span><span>();</span>     <span>//value-initialization</span>
    <span>new</span> <span>T</span><span>{};</span>     <span>//value-initialization (C++11)</span>
<span>}</span>

<span>struct</span> <span>A</span> <span>{</span> <span>T</span> <span>t</span><span>;</span> <span>A</span><span>()</span> <span>:</span> <span>t</span><span>()</span> <span>{}</span> <span>};</span> <span>//t is value-initialized</span>
<span>struct</span> <span>B</span> <span>{</span> <span>T</span> <span>t</span><span>;</span> <span>B</span><span>()</span> <span>:</span> <span>t</span><span>{}</span> <span>{}</span> <span>};</span> <span>//t is value-initialized (C++11)</span>
<span>struct</span> <span>C</span> <span>{</span> <span>T</span> <span>t</span><span>;</span> <span>C</span><span>()</span>       <span>{}</span> <span>};</span> <span>//t is default-initialized</span></code></pre></figure>

<p>The rules for these different initialization forms are fairly complex, so I‚Äôll give a simplified outline of the C++11 rules (C++14 even changed some of them, so those value-initialization forms can be aggregate initialization). If you want to understand all the details of these forms, check out the relevant cppreference.com articles<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>, or see the standards quotes at the bottom of the article.</p>

<ul>
  <li><strong>default-initialization</strong> ‚Äì If <code>T</code> is a class, the default constructor is called; if it‚Äôs an array, each element is default-initialized; otherwise, no initialization is done, resulting in indeterminate values.</li>
  <li><strong>value-initialization</strong> ‚Äì If <code>T</code> is a class, the object is default-initialized (after being zero-initialized if <code>T</code>‚Äôs default constructor is not user-provided/deleted); if it‚Äôs an array, each element is value-initialized; otherwise, the object is zero-initialized.</li>
  <li><strong>zero-initialization</strong> ‚Äì Applied to static and thread-local variables before any other initialization. If <code>T</code> is scalar (arithmetic, pointer, enum), it is initialized from <code>0</code>; if it‚Äôs a class type, all base classes and data members are zero-initialized; if it‚Äôs an array, each element is zero-initialized.</li>
</ul>

<p>Taking the simple example of <code>int</code> as <code>T</code>, <code>global</code> and all of the value-initialized variables will have the value <code>0</code>, and all other variables will have an indeterminate value. Reading these indeterminate values results in undefined behaviour.</p>

<hr>

<h4 id="back-to-our-original-example">Back to our original example</h4>

<p>Now we have the necessary knowledge to understand what‚Äôs going on in my original example. Essentially, the behaviours of <code>foo</code> and <code>bar</code> are changed by the different location of <code>=default</code> on their constructors. Again, the relevant standards passages are down at the bottom of the page if you want them, but the jist is this:</p>

<p>Since the constructor for <code>foo</code> is defaulted on its first declaration, it is not technically <em>user-provided</em> ‚Äì I‚Äôll explain what this term means shortly, just accept this standardese for now. The constructor for <code>bar</code>, conversely, is only defaulted at its definition, so it <em>is</em> user-provided. Put another way, if you don‚Äôt want your constructor to be user-provided, be sure to write <code>=default</code> when you declare it rather than define it like that elsewhere. This rule makes sense when you think about it: without having access to the definition of a constructor, a translation unit can‚Äôt know if it is going to be a simple compiler-generated one, or if it‚Äôs going to send a telegram to the Moon to retrieve some data and block until it gets a response.</p>

<p>The default constructor being user-provided has a few consequences for the class type. For example, you can‚Äôt default-initialize a const-qualified object if it lacks a user-provided constructor, the notion being that if the object should only be set once, it better be initialised with something reasonable:</p>

<figure><pre><code data-lang="cpp"><span>const</span> <span>int</span> <span>my_int</span><span>;</span>            <span>//ill-formed, no user-provided constructor</span>
<span>const</span> <span>std</span><span>::</span><span>string</span> <span>my_string</span><span>;</span> <span>//well-formed, has a user-provided constructor</span>

<span>const</span> <span>foo</span> <span>my_foo</span><span>;</span> <span>//ill-formed, no user-provided constructor</span>
<span>const</span> <span>bar</span> <span>my_bar</span><span>;</span> <span>//well-formed, has a user-provided constructor</span></code></pre></figure>

<p>Additionally, in order to be <a href="http://en.cppreference.com/w/cpp/concept/TrivialType">trivial</a> (and therefore <a href="http://en.cppreference.com/w/cpp/concept/PODType">POD</a>) or an <a href="http://en.cppreference.com/w/cpp/language/aggregate_initialization">aggregate</a>, a class must have no user-provided constructors. Don‚Äôt worry if you don‚Äôt know those terms, it suffices to know that whether your constructors are user-provided or not modifies some of the restrictions of what you can do with that class and how it acts.</p>

<p>For our first example, however, we‚Äôre interested in how user-provided constructors interact with initialization rules. The language mandates that both <code>a</code> and <code>b</code> are value-initialized, but only <code>a</code> is additionally zero-initialized. Zero-initialization for <code>a</code> gives <code>a.a</code> the value <code>0</code>, whereas <code>b.b</code> is not initialized at all, giving us undefined behaviour if we attempt to read it. This is a very subtle distinction which has inadvertently changed our program from executing safely to summoning nasal demons/eating your cat/ordering pizza/your favourite undefined behaviour metaphor.</p>

<p>Fortunately, there‚Äôs a simple solution. At the risk of repeating advice which has been given many times before, <strong>initialize your variables.</strong></p>

<p><b>Seriously.</b></p>

<p><b>Do it.</b></p>

<p><b>INITIALIZE YOUR GORRAM VARIABLES.</b></p>

<p>If the designer of <code>foo</code> and <code>bar</code> decides that they should be default constructible, they should initialize their contents with some sensible values. If they decide that they should <em>not</em> be default constructible, they should delete the constructors to avoid issues.</p>

<figure><pre><code data-lang="cpp"><span>struct</span> <span>foo</span> <span>{</span>
    <span>foo</span><span>()</span> <span>:</span> <span>a</span><span>{</span><span>0</span><span>}</span> <span>{}</span> <span>//initialize to 0 explicitly</span>
    <span>int</span> <span>a</span><span>;</span>
<span>};</span>

<span>struct</span> <span>bar</span> <span>{</span>
    <span>bar</span><span>()</span> <span>=</span> <span>delete</span><span>;</span> <span>//delete constructor</span>
    <span>//insert non-default constructor which does something sensible here</span>
    <span>int</span> <span>b</span><span>;</span>
<span>};</span></code></pre></figure>

<p>Internalising this way of thinking about initialization is key to writing unsurprising code. If you‚Äôve profiled your code and found a bottleneck caused by unnecessary initialization, then sure, optimise it, but you best be certain that the extra performance is worth the possible headaches and money spent to keep the code safe.</p>

<p>If you still aren‚Äôt convinced that C++ initialization rules are crazy-complex, take a minute to think of all the forms of initialization you can think of. My answers after the line.</p>

<hr>

<p>Done? How many did you come up with? In perusal of the standard, I counted <em>eighteen</em> different forms of initialization<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>. Here they are with a short example/description:</p>

<ul>
  <li>default: <code>int i;</code></li>
  <li>value: <code>int i{};</code></li>
  <li>zero: <code>static int i;</code></li>
  <li>constant: <code>static int i = some_constexpr_function();</code></li>
  <li>static: zero- or constant-initialization</li>
  <li>dynamic: not static initialization</li>
  <li>unordered: dynamic initialization of class template static data members which are not explicitly specialized</li>
  <li>ordered: dynamic initialization of other non-local variables with static storage duration</li>
  <li>non-trivial: when a class or aggregate is initialized by a non-trivial constructor</li>
  <li>direct: <code>int i{42}; int j(42);</code></li>
  <li>copy: <code>int i = 42;</code></li>
  <li>copy-list: <code>int i = {42};</code></li>
  <li>direct-list: <code>int i{42};</code></li>
  <li>list: either copy-list or direct-list</li>
  <li>aggregate: <code>int is[3] = {0,1,2};</code></li>
  <li>reference: <code>const int&amp; i = 42; auto&amp;&amp; j = 42;</code></li>
  <li>implicit: default or value</li>
  <li>explicit: direct, copy, or list</li>
</ul>

<p>Don‚Äôt try to memorise all of these rules; therein lies madness. Just be careful, and keep in mind that C++‚Äôs initialization rules are there to pounce on you when you least expect it. Explicitly initialize your variables, and if you ever fall in to the trap of thinking C++ is a sane language, remember this:</p>

<p><strong>In C++, you can give your program undefined behaviour by changing the point at which you tell the compiler to generate something it was probably going to generate for you anyway.</strong></p>

<hr>

<h4 id="appendix-standards-quotes">Appendix: Standards quotes</h4>

<p>All quotes from N4140 (essentially C++14).</p>

<blockquote para="[dcl.fct.def.default]/5">
  <p>Explicitly-defaulted functions and implicitly-declared functions are collectively called defaulted functions,
and the implementation shall provide implicit definitions for them (12.1 12.4, 12.8), which might mean
defining them as deleted. <strong>A function is user-provided if it is user-declared and not explicitly defaulted or
deleted on its first declaration.</strong> A user-provided explicitly-defaulted function (i.e., explicitly defaulted after its first declaration) is defined at the point where it is explicitly defaulted; if such a function is implicitly defined
as deleted, the program is ill-formed.</p>
</blockquote>

<blockquote para="[dcl.init]/6-8">
  <p>To <em>zero-initialize</em> an object or reference of type <code>T</code> means:</p>

  <ul>
    <li>if <code>T</code> is a scalar type (3.9), the object is initialized to the value obtained by converting the integer literal
0 (zero) to T</li>
    <li>if <code>T</code> is a (possibly cv-qualified) non-union class type, each non-static data member and each base-class subobject is zero-initialized and padding is initialized to zero bits;</li>
    <li>if <code>T</code> is a (possibly cv-qualified) union type, the object‚Äôs first non-static named data member is zero-initialized and padding is initialized to zero bits;</li>
    <li>if <code>T</code> is an array type, each element is zero-initialized;</li>
    <li>if <code>T</code> is a reference type, no initialization is performed.</li>
  </ul>

  <p>To <em>default-initialize</em> an object of type <code>T</code> means:</p>

  <ul>
    <li>if <code>T</code> is a (possibly cv-qualified) class type (Clause 9), the default constructor (12.1) for <code>T</code> is called (and the initialization is ill-formed if <code>T</code> has no default constructor or overload resolution (13.3) results in an
ambiguity or in a function that is deleted or inaccessible from the context of the initialization);</li>
    <li>if <code>T</code> is an array type, each element is default-initialized;</li>
    <li>otherwise, no initialization is performed.
If a program calls for the default initialization of an object of a const-qualified type <code>T</code>, <code>T</code> shall be a class type with a user-provided default constructor.</li>
  </ul>

  <p>To <em>value-initialize</em> an object of type <code>T</code> means:</p>

  <ul>
    <li>if <code>T</code> is a (possibly cv-qualified) class type (Clause 9) with either no default constructor (12.1) or a default constructor that is user-provided or deleted, then the object is default-initialized;</li>
    <li>if <code>T</code> is a (possibly cv-qualified) class type without a user-provided or deleted default constructor, then the object is zero-initialized and the semantic constraints for default-initialization are checked, and if <code>T</code> has a non-trivial default constructor, the object is default-initialized;</li>
    <li>if <code>T</code> is an array type, then each element is value-initialized;</li>
    <li>otherwise, the object is zero-initialized.</li>
  </ul>
</blockquote>

<blockquote para="[basic.start.init]/2">
  <p>Variables with static storage duration (3.7.1) or thread storage duration (3.7.2) shall be zero-initialized (8.5)
before any other initialization takes place. [‚Ä¶]</p>
</blockquote>

</span></p><p><small>
          <i></i>
            c++
          </small>
        </p>
        

      </div><p>
        I <i></i> feedback.<br>
        Let me know what you think of this article on twitter <a href="https://www.twitter.com/TartanLlama">@TartanLlama</a> or leave a comment below!
      </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stack Overflow is almost dead (121 pts)]]></title>
            <link>https://blog.pragmaticengineer.com/stack-overflow-is-almost-dead/</link>
            <guid>43999125</guid>
            <pubDate>Thu, 15 May 2025 20:41:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pragmaticengineer.com/stack-overflow-is-almost-dead/">https://blog.pragmaticengineer.com/stack-overflow-is-almost-dead/</a>, See on <a href="https://news.ycombinator.com/item?id=43999125">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
    <article>

        

        <section id="content">
            <p><em>Originally published in </em><a href="https://newsletter.pragmaticengineer.com/p/the-pulse-134?ref=blog.pragmaticengineer.com" rel="noreferrer"><em>The Pragmatic Engineer Newsletter</em></a><em>.</em></p><p>Four months ago, we asked <a href="https://blog.pragmaticengineer.com/are-llms-making-stackoverflow-irrelevant/" rel="noreferrer">Are LLMs making Stack Overflow irrelevant?</a> Data at the time suggested that the answer is likely "yes:"</p><figure><img src="https://blog.pragmaticengineer.com/content/images/2025/05/image-6.png" alt="" loading="lazy" width="1456" height="859" srcset="https://blog.pragmaticengineer.com/content/images/size/w600/2025/05/image-6.png 600w, https://blog.pragmaticengineer.com/content/images/size/w1000/2025/05/image-6.png 1000w, https://blog.pragmaticengineer.com/content/images/2025/05/image-6.png 1456w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Number of questions asked per month on StackOverflow. Data source: </em></i><a href="https://gist.github.com/hopeseekr/f522e380e35745bd5bdc3269a9f0b132?ref=blog.pragmaticengineer.com#file-stackoverflow-new-questions-over-time-2009-2024-csv" target="_blank" rel="noopener noreferrer nofollow"><i><em>this Gist</em></i></a></figcaption></figure><p>Since then, things at Stack Overflow went from bad to worse. The volume of questions asked has nearly dried up, new data shows:</p><figure><img src="https://blog.pragmaticengineer.com/content/images/2025/05/image-4.png" alt="" loading="lazy" width="936" height="744" srcset="https://blog.pragmaticengineer.com/content/images/size/w600/2025/05/image-4.png 600w, https://blog.pragmaticengineer.com/content/images/2025/05/image-4.png 936w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Questions have slumped to levels last seen when Stack Overflow launched in 2009. Source: Stack Overflow Data Explorer (SEDE) / Marc Gravell&nbsp;</em></i><a href="https://x.com/marcgravell?ref=blog.pragmaticengineer.com" rel=""><i><em>on X</em></i></a></figcaption></figure><p>This graph&nbsp;<a href="https://x.com/marcgravell/status/1922922817143660783?ref=blog.pragmaticengineer.com">was shared</a>&nbsp;by&nbsp;<a href="https://x.com/marcgravell?ref=blog.pragmaticengineer.com">Marc Gravell</a>, a top 10 all-time contributor to Stack Overflow. Let‚Äôs look closer at the data:</p><figure><img src="https://blog.pragmaticengineer.com/content/images/2025/05/image-5.png" alt="" loading="lazy" width="1456" height="727" srcset="https://blog.pragmaticengineer.com/content/images/size/w600/2025/05/image-5.png 600w, https://blog.pragmaticengineer.com/content/images/size/w1000/2025/05/image-5.png 1000w, https://blog.pragmaticengineer.com/content/images/2025/05/image-5.png 1456w" sizes="(min-width: 720px) 720px"><figcaption><span>Decline started around 2014</span></figcaption></figure><p>A few things stand out:</p><ul><li><strong>2014:</strong>&nbsp;questions started to decline, which was also when Stack Overflow&nbsp;<a href="https://meta.stackoverflow.com/questions/251175/stack-overflow-is-not-yet-a-vast-wasteland-a-history-of-moderator-tooling?ref=blog.pragmaticengineer.com">significantly improved moderator efficiency</a>. From then, questions were closed faster, many more were closed, and ‚Äúlow quality‚Äù questions were removed more efficiently. This tallies with my memory of feeling that site moderators had gone on a power trip by closing legitimate questions.&nbsp;<em>I stopped asking questions around this time because the site felt unwelcome.</em></li><li><strong>March 2020:&nbsp;</strong>a big jump in traffic due to pandemic-induced lockdowns and forced remote working. Instead of asking colleagues, devs Googled and visited Stack Overflow for help</li><li><strong>June 2020:</strong>&nbsp;questions start to decline, faster than before.&nbsp;<em>Even though we did not know at the time, this was stilll two years from ChatGPT launching!</em></li><li><strong>June 2021:</strong>&nbsp;Stack Overflow sold for $1.8B to private equity investor, Prosus.&nbsp;<em>In hindsight, the founders ‚Äì Jeff Atwood and Joel Spolsky ‚Äì sold with near-perfect timing, before terminal decline.</em></li><li><strong>November 2022:</strong>&nbsp;as soon as ChatGPT came out, the number of questions asked declined rapidly. ChatGPT is faster and it‚Äôs trained on StackOverflow data, so the quality of answers is similar. Plus, ChatGPT is polite and answers all questions, in contrast to StackOverflow moderators.</li><li><strong>May 2025:</strong>&nbsp;the number of monthly questions is as low as when Stack Overflow launched in 2009.</li></ul><p>In January, I asked&nbsp;<a href="https://newsletter.pragmaticengineer.com/i/154495287/are-llms-making-stackoverflow-irrelevant?ref=blog.pragmaticengineer.com">if LLMs are making Stack Overflow irrelevant</a>. We now have an answer, and sadly, it‚Äôs a ‚Äúyes.‚Äù The question seems to be&nbsp;<em>when</em>&nbsp;Stack Overflow will wind down operations, or the owner sells the site for comparative pennies, not&nbsp;<em>if&nbsp;</em>it will happen.</p><p>Even without LLMs, it‚Äôs possible StackOverflow would have eventually faded into irrelevance ‚Äì perhaps driven by moderation policy changes or something else that started in 2014. LLMs have certainly accelerated its fall. It's a true shame for a site that helped so many developers get "unstuck" ‚Äì while successfully gamifying helping other developers on the internet in the early 2010s.</p><p>I'll certainly miss having a space on the internet to ask questions and receive help ‚Äì not from an AI, but from fellow, human developers. While Stack Overflow's days are likely numbered: I'm sure we'll see spaces where developers hang out and help each other continue to be popular ‚Äì whether they are in the form of Discord servers, WhatsApp or Telegram groups, or something else.</p><p><em>Update on 15 May: updated the last two paragraphs to make it a more positive outlook. I really did love StackOverflow from when it launched, and it made a big and positive difference in my professional growth in those early years ‚Äì I still remember the pride of getting my first upvote on first a question, and eventually on more and more answers as well. Too bad that all good things come to an end. Thanks to Andrew for his thoughtful note.</em></p><hr><p>This was one out of five topics from latest <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-134?ref=blog.pragmaticengineer.com" rel="noreferrer">The Pulse issue</a>. <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-134?ref=blog.pragmaticengineer.com" rel="noreferrer">The full issue</a> additionally covers:</p><ul><li><strong>Industry pulse.&nbsp;</strong>Google‚Äôs CEO doing customer support, coding model recommendations from Cursor, AI dev tools company valuations soar, OpenAI still a nonprofit ‚Äì but with more clarity on stock, and will we get an answer to whether copyrighted materials can be used to train AI models?</li><li><strong>Could big job cuts at Microsoft become more regular?&nbsp;</strong>6,000 people (about 3% of staff) let go at Microsoft. Based on the company‚Äôs history, mass layoffs happen more than in the past. Satya Nadella is an empathetic leader, but also doesn‚Äôt shy away from axing jobs.</li><li><strong>Google: high performers get more bonus, low performers get less.</strong>&nbsp;Not exactly a controversial change, but another example of the search giant becoming similar to other tech companies. Places like Uber have implemented this approach before.</li><li><strong>Notes on rolling out Cursor and Claude Code.&nbsp;</strong>A 40-person dev team at Workforce.com with a Ruby on Rails codebase started to use AI tools and agents. Results so far are pretty good: productivity gains are real if modest, and there‚Äôs l<a href="https://ghiculescu.substack.com/p/nobody-codes-here-anymore?ref=blog.pragmaticengineer.com">earnings on how to best use them</a>&nbsp;from cofounder,&nbsp;<a href="https://ghiculescu.substack.com/p/nobody-codes-here-anymore?ref=blog.pragmaticengineer.com">Alex Ghiculescu</a>.</li></ul><p><a href="https://newsletter.pragmaticengineer.com/p/the-pulse-134?ref=blog.pragmaticengineer.com" rel="noreferrer"><strong>Read the full issue here.</strong></a></p>


            <!-- Newsletter -->
            <p>
              <a href="https://newsletter.pragmaticengineer.com/about">Subscribe to my weekly newsletter</a> to get articles like this in your inbox. It's a pretty good read - and the <a href="https://substack.com/top/technology">#1 tech newsletter</a> on Substack.
            </p>
            
        </section>

        
    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tek ‚Äì A music making program for 24-bit Unicode terminals (157 pts)]]></title>
            <link>https://codeberg.org/unspeaker/tek</link>
            <guid>43998707</guid>
            <pubDate>Thu, 15 May 2025 19:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codeberg.org/unspeaker/tek">https://codeberg.org/unspeaker/tek</a>, See on <a href="https://news.ycombinator.com/item?id=43998707">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		




	


		

		
		<div>
			<p><span><span aria-label="crab">ü¶Ä</span> a colorful music making program for your linux terminal <span aria-label="crab">ü¶Ä</span></span>
				
			</p>
		</div>
		<div id="repo-topics">
			
			<p><a href="https://codeberg.org/explore/repos?q=daw&amp;topic=1">daw</a><a href="https://codeberg.org/explore/repos?q=groovebox&amp;topic=1">groovebox</a><a href="https://codeberg.org/explore/repos?q=jack&amp;topic=1">jack</a><a href="https://codeberg.org/explore/repos?q=jackaudio&amp;topic=1">jackaudio</a><a href="https://codeberg.org/explore/repos?q=lv2&amp;topic=1">lv2</a><a href="https://codeberg.org/explore/repos?q=music&amp;topic=1">music</a><a href="https://codeberg.org/explore/repos?q=rust&amp;topic=1">rust</a><a href="https://codeberg.org/explore/repos?q=sampler&amp;topic=1">sampler</a><a href="https://codeberg.org/explore/repos?q=sequencer&amp;topic=1">sequencer</a><a href="https://codeberg.org/explore/repos?q=tui&amp;topic=1">tui</a>
			
		</p></div>
		
		

		
		








	
	



	
	



	
	



	
	



	
	



	
	



	
	



	
	



	
	



	
	





		
		
<div>
	<div>
		
			<a href="https://codeberg.org/unspeaker/tek/commits/branch/main">
				 <b>1751</b> commits
			</a>
			<a href="https://codeberg.org/unspeaker/tek/branches">
				 <b>4</b> branches
			</a>
			
				<a href="https://codeberg.org/unspeaker/tek/tags">
					 <b>11</b> tags
				</a>
			
			<p><span data-tooltip-content="git: 5 MiB, lfs: 0 B">
				
				 <b>5</b> MiB
			</span>
		
	</p></div>
	
	<a id="language-stats-bar" data-panel="#language-stats-legend">
		
		
		
		
		
		
		
		
		
	</a>
	<div id="language-stats-legend">
		
		<p><i></i>
			<span>
				
					Rust
				
			</span>
			98.4%
		</p>
		
		<p><i></i>
			<span>
				
					Just
				
			</span>
			0.9%
		</p>
		
		<p><i></i>
			<span>
				
					Shell
				
			</span>
			0.4%
		</p>
		
		<p><i></i>
			<span>
				
					Nix
				
			</span>
			0.3%
		</p>
		
	</div>
	
</div>


		
		
		
		<div>
			<div>
				






	








				
					
					
					
					
					<a id="new-pull-request" role="button" href="https://codeberg.org/unspeaker/tek/compare/main...main" data-tooltip-content="Compare">
						
					</a>
				
				
				
					<p><a href="https://codeberg.org/unspeaker/tek/find/branch/main">Find a file</a>
				

				

				
				
			</p></div>
			<div>
				
				
					
					<div id="cite-repo-modal">
	<p>
		Cite this repository
	</p>
	<div id="citation-panel">
				<p><span id="citation-copy-bibtex" data-text="">
BibTeX
</span>


</p><a id="goto-citation-btn" href="https://codeberg.org/unspeaker/tek/src/main/" data-tooltip-content="Find a file">
					
				</a>
			</div>
	
</div>

				
				
			</div>
		</div>
		
			
			
			
			
			
			<table id="repo-files-table">
	<thead>
		<tr>
			<th colspan="2">
				<div>
						
	
		<p><img loading="lazy" alt="" src="https://codeberg.org/avatars/d519d36199e89735a26401f8b318761bba46b0fec030ce83b49580fcbb91ec3c?size=48" title="ü§´‚ãØüòâ" width="24" height="24">
		
			<a title="unspeaker" href="https://codeberg.org/unspeaker"><strong>unspeaker</strong></a>
		
	
	<a rel="nofollow" href="https://codeberg.org/unspeaker/tek/commit/9aeb792f7dc0d4ee115d619d62916710ca1a3e9f">
		<span>9aeb792f7d</span>
		
	</a></p><a data-tippy="commit-statuses" href="https://codeberg.org/unspeaker/tek/actions/runs/1551/jobs/0">
			

	






		</a>
	
	<div>
	<p>
		
			Some checks are pending
		

		
	</p>

	<div>
				

	






				<p>/ build (push) <span>Waiting to run</span></p>
				<div>
					
					<p><span><a href="https://codeberg.org/unspeaker/tek/actions/runs/1551/jobs/0">Details</a></span>
				</p></div>
			</div>
</div>


	
	<p><span title="wip: new old arranger scenes"><span><a href="https://codeberg.org/unspeaker/tek/commit/9aeb792f7dc0d4ee115d619d62916710ca1a3e9f">wip: new old arranger scenes</a></span>
		
	</span>


					</p></div>
			</th>
			<th><relative-time prefix="" tense="past" datetime="2025-05-15T23:06:15+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 23:06:15 +03:00</relative-time></th>
		</tr>
	</thead>
	<tbody>
		
		
			
			
			
			<tr data-entryname=".forgejo" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.forgejo/workflows" title=".forgejo/workflows">
									
									
									<span>.forgejo</span>/workflows
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/0fa6d31c7e463db0be8d7c2645ba84c908e4d90e">ci: disable cache</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-19T23:07:37+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-19 23:07:37 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname=".old" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.old" title=".old">
									
									
									.old
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/e3b12a1d3638b7a7fa351ccd78cbfe751ef69675">add old tengri demo</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-08T22:06:40+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-08 22:06:40 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="build" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/build" title="build">
									
									
									build
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/8e9d7dc9a1b6f1dde51107d63834437a35b3e3ea">document build directory</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-20T00:46:25+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-20 00:46:25 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="config" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/config" title="config">
									
									
									config
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/9aeb792f7dc0d4ee115d619d62916710ca1a3e9f">wip: new old arranger scenes</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-15T23:06:15+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 23:06:15 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="crates" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/crates" title="crates">
									
									
									crates
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/9aeb792f7dc0d4ee115d619d62916710ca1a3e9f">wip: new old arranger scenes</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-15T23:06:15+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 23:06:15 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="deps" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/deps" title="deps">
									
									
									deps
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/094d5dd451792f39990a00d8f4b632d25c2631e3">wip: reimplement arranger because i'm sick of that shit</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-15T00:01:26+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 00:01:26 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="target" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/target" title="target">
									
									
									target
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/4ba88bfd6dce48a627bb4c6f2a1cb679688280ef">groovebox: layout</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-15T14:45:06+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 14:45:06 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname=".dockerignore" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.dockerignore" title=".dockerignore">.dockerignore</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/c439528cfca459b4f794ba3ea49cee8318bb8568">add containerized release build</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-19T01:20:41+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-19 01:20:41 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname=".gitignore" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.gitignore" title=".gitignore">.gitignore</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/4d3f308c1054eecb6bca065e4ec8e8457b310c3b">add stub target</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-20T00:42:22+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-20 00:42:22 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname=".gitmodules" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.gitmodules" title=".gitmodules">.gitmodules</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/393634a1a4768ff494f3129b7c08df35f4966cb3">local dockerized build</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-19T03:23:09+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-19 03:23:09 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="architecture.svg" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/architecture.svg" title="architecture.svg">architecture.svg</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/47c34d0077c3c91ffe8fad857c0b280baab45a15">move some docs to root</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2024-12-30T21:56:52+01:00" data-tooltip-content="" data-tooltip-interactive="true">2024-12-30 21:56:52 +01:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="Cargo.lock" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/Cargo.lock" title="Cargo.lock">Cargo.lock</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/6ce83fb27a60f0a2532520a39596adc253081b2d">wip: down to 25 errors woo</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-14T02:13:16+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-14 02:13:16 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="Cargo.toml" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/Cargo.toml" title="Cargo.toml">Cargo.toml</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/6286d6982460f65394e2125833f3349baf49aa99">wip: device: reenable lv2 support</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-04T18:23:44+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-04 18:23:44 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="CONTRIBUTING.md" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/CONTRIBUTING.md" title="CONTRIBUTING.md">CONTRIBUTING.md</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/393634a1a4768ff494f3129b7c08df35f4966cb3">local dockerized build</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-19T03:23:09+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-19 03:23:09 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="Justfile" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/Justfile" title="Justfile">Justfile</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/d647fc68e942300c6ba0109633df32ad4fe87581">just: connect to all firefox jack outputs</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-11T19:18:19+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-11 19:18:19 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="LICENSE" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/LICENSE" title="LICENSE">LICENSE</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/0c6484d733df4567fd8dfba48847c033a2a78e91">add license</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-03-24T03:18:18+02:00" data-tooltip-content="" data-tooltip-interactive="true">2025-03-24 03:18:18 +02:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="README.md" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/README.md" title="README.md">README.md</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/5c74ffd91611ee346f91a224d17e7f401758f693">docs: remove mention of wasd keybinds</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-09T22:02:09+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-09 22:02:09 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="shell.nix" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/shell.nix" title="shell.nix">shell.nix</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/3837dbd47bff70ecf07c92e7b63119fe2e500486">add thiserror to edn module; 38.57% total cov</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-02-26T15:09:44+02:00" data-tooltip-content="" data-tooltip-interactive="true">2025-02-26 15:09:44 +02:00</relative-time></td>
			</tr>
		
	</tbody>
</table>

	<div id="readme">

	

	<h4>
		<div>
			
				
				<p><strong><a href="#readme">README.md</a></strong>
			
		</p></div>
		
	</h4>
	<div>
			
				
<p dir="auto">a music making program for <a href="https://sw.kovidgoyal.net/kitty/" rel="nofollow">24-bit unicode terminals</a>.</p>
<p dir="auto">written in <a href="https://www.rust-lang.org/" rel="nofollow">rust</a>
with <a href="https://ratatui.rs/" rel="nofollow">ratatui</a> on <a href="https://docs.rs/crossterm/latest/crossterm/" rel="nofollow">crossterm</a>
for <a href="https://jackaudio.org/" rel="nofollow">jack</a> and <a href="https://www.pipewire.org/" rel="nofollow">pipewire</a>.</p>
<p dir="auto"><strong>tek</strong> is available as <a href="https://codeberg.org/unspeaker/tek#building-from-source" rel="nofollow">source</a>,
<a href="https://codeberg.org/unspeaker/tek/releases" rel="nofollow">statically linked binaries</a>, and on the
<a href="https://codeberg.org/unspeaker/tek#arch-linux" rel="nofollow">aur</a>.</p>
<p dir="auto">author is reachable via <a href="https://mastodon.social/@unspeaker" rel="nofollow"><strong>mastodon</strong> <code>@unspeaker@mastodon.social</code></a>
or <a href="https://matrix.to/#/@unspeaker:matrix.org" rel="nofollow"><strong>matrix</strong> <code>@unspeaker:matrix.org</code></a></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://codeberg.org/unspeaker/tek/attachments/5014ff4d-9ece-4862-90de-3bc6573eacf6" target="_blank" rel="nofollow noopener"><img src="https://codeberg.org/unspeaker/tek/attachments/5014ff4d-9ece-4862-90de-3bc6573eacf6" alt="Screenshot of Arranger Mode"></a></td>
<td><a href="https://codeberg.org/unspeaker/tek/releases/download/0.2.0-rc.7/Screenshot%20From%202025-01-02%2023-18-05.png" target="_blank" rel="nofollow noopener"><img src="https://codeberg.org/unspeaker/tek/releases/download/0.2.0-rc.7/Screenshot%20From%202025-01-02%2023-18-05.png" alt="Screenshot of Groovebox Mode"></a></td>
</tr>
</tbody>
</table>
<h2 id="user-content-usage" dir="auto">usage</h2>
<ul dir="auto">
<li><strong>requirements:</strong> linux; jack or pipewire; 24-bit terminal (i use <code>kitty</code>)</li>
<li><strong>recommended:</strong> midi controller; samples in wav format; lv2 plugins.</li>
</ul>
<h2 id="user-content-keymaps" dir="auto">keymaps</h2>
<ul dir="auto">
<li>Arranger:
<ul dir="auto">
<li>arrows: navigate</li>
<li>tab: enter editor</li>
<li><code>q</code>: enqueue clip</li>
<li>space: play/pause</li>
</ul>
</li>
<li>Editor:
<ul dir="auto">
<li>arrows: navigate</li>
<li><code>,</code> / <code>.</code>: change note length</li>
<li>enter: write note</li>
<li><code>-</code> / <code>=</code>: zoom midi editor</li>
<li><code>z</code>: zoom lock/unlock</li>
<li>del: delete</li>
</ul>
</li>
<li>Global:
<ul dir="auto">
<li>esc: options menu</li>
<li>f1: help/command list</li>
<li>f2: rename</li>
<li>f6: save</li>
<li>f9: load</li>
</ul>
</li>
</ul>
<h2 id="user-content-installation" dir="auto">installation</h2>
<h3 id="user-content-binary-download" dir="auto">binary download</h3>
<p dir="auto">you can download <a href="https://codeberg.org/unspeaker/tek/releases/tag/0.2.0" rel="nofollow">tek 0.2.0 "almost static"</a>
from codeberg releases. this standalone binary release, should work on any glibc-based system.</p>
<h3 id="user-content-from-distro-repositories" dir="auto">from distro repositories</h3>
<p dir="auto"><a href="https://repology.org/project/tek/versions" rel="nofollow"><img src="https://repology.org/badge/vertical-allrepos/tek.svg" alt="Packaging status"></a></p>
<h4 id="user-content-arch-linux" dir="auto">arch linux</h4>
<p dir="auto"><a href="https://aur.archlinux.org/packages/tek" rel="nofollow">tek 0.2.0-rc7</a> is available as a package in the AUR.
you can install it using your preferred AUR helper (e.g. <code>paru</code>):</p>
<pre><code>paru -S tek
</code></pre><h3 id="user-content-building-from-source" dir="auto">building from source</h3>
<p dir="auto">requires docker.</p>
<pre><code>git clone --recursive -b 0.2 https://codeberg.org/unspeaker/tek
cd tek                             # enter directory
cat bin/release-glibc.sh           # preview build script
sudo bin/release-glibc.sh          # run build script
sudo cp bin/tek /usr/local/bin/tek # install
</code></pre><h2 id="user-content-design-goals" dir="auto">design goals</h2>
<ul dir="auto">
<li>
<p dir="auto">inspired by trackers and hardware sequencers,
but with the critical feature that 90s samplers lack:
able to <strong>resample, i.e. record while playing!</strong></p>
</li>
<li>
<p dir="auto"><strong>pop-up scratchpad for musical ideas.</strong>
low resource consumption, can stay open in background.
but flexible enough to allow expanding on compositions</p>
</li>
<li>
<p dir="auto"><strong>human- and machine- readable project format</strong>
simple representation for project data
enable scripting and remapping.</p>
</li>
</ul>

			
		</div>
</div>



		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The unreasonable effectiveness of an LLM agent loop with tool use (400 pts)]]></title>
            <link>https://sketch.dev/blog/agent-loop</link>
            <guid>43998472</guid>
            <pubDate>Thu, 15 May 2025 19:33:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sketch.dev/blog/agent-loop">https://sketch.dev/blog/agent-loop</a>, See on <a href="https://news.ycombinator.com/item?id=43998472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      <p><em>2025-05-15</em> by Philip Zeyliger</p>
      <p>
  <span>My co-workers and I have been working on an AI Programming Assistant called</span>
  <span><a href="https://sketch.dev/">Sketch</a></span>
  <span> for the last few months. The thing I've been most surprised by is how shockingly simple the main loop of using an LLM with tool use is:</span>
</p>

<pre><code>def loop(llm):
    msg = user_input()
    while True:
        output, tool_calls = llm(msg)
        print("Agent: ", output)
        if tool_calls:
            msg = [ handle_tool_call(tc) for tc in tool_calls ]
        else:
            msg = user_input()
</code></pre>

<p>
  <span>There's some pomp and circumstance to make the above work (</span><span><a href="https://sketch.dev/blog/agent_loop.py">here's the full script</a></span><span>)</span>
  <span>, but the core idea is the above 9 lines. Here, <code>llm()</code> is a function that sends the system prompt, the conversation so far, and the next message to the LLM API.</span>
</p>

<p>
  <span>Tool use is the fancy term for "the LLM returns some output that corresponds to a schema," and, in the full script, we tell the LLM (in its system prompt and tool description prompts) that it has access to <code>bash</code>.</span>
</p>

<p>
  <span>With just that one very general purpose tool, the current models (we use Claude 3.7 Sonnet extensively) can nail many problems, some of them in "one shot." Whereas I used to look up an esoteric git operation and then cut and paste, now I just ask Sketch to do it. Whereas I used to handle git merges manually, now I let Sketch take a first pass. Whereas I used to change a type and go through the resulting type checker errors one by one (or, let's be real, with <code>perl -pie</code> ridiculousness), I give it a shot with Sketch. If appropriately prompted, the agentic loop can be persistent. If you don't have some tool installed, it'll install it. If your `grep` has different command line options, it adapts. (It can also be infuriating! "Oh, this test doesn't pass... let's just skip it," it sometimes says, maddeningly.)</span>
</p>

<p>
  <span>For many workflows, agentic tools specialize. Sketch's quiver of tools is not just <code>bash</code>, as we've found that a handful of extra tools improve the quality, speed up iterations, and facilitate better developer workflows. Tools that let the LLM edit text correctly are surprisingly tricky. Seeing the LLM struggle with <code>sed</code> one-liners re-affirms that visual (as opposed to line) editors are a marvel.</span>
</p>

<p>
  <span>I have no doubt that agent loops will get incorporated into more day to day automation tedium that's historically been too specific for general purpose tools and too esoteric and unstable to automate traditionally. I keep thinking of how much time I've spent correlating stack traces with git commits, and how good LLMs are at doing a first pass on it. We'll be seeing more custom, <span>ad hoc</span>, throw-away LLM agent loops in our <code>bin/</code> directories. Grab your favorite bearer token and give it a shot.</span>
</p>

<p>
<span><i>Also published at <span><a href="https://philz.dev/blog/agent-loop/">philz.dev/blog/agent-loop/</a>.</span></i></span><i>
</i></p><i>
</i> 
      <p>
        <a href="https://sketch.dev/">sketch.dev</a> ¬∑
        <a href="https://merde.ai/">merde.ai</a> ¬∑ <a href="https://pi.dev/">pi.dev</a>
      </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The current state of TLA‚Å∫ development (129 pts)]]></title>
            <link>https://ahelwer.ca/post/2025-05-15-tla-dev-status/</link>
            <guid>43998115</guid>
            <pubDate>Thu, 15 May 2025 18:53:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ahelwer.ca/post/2025-05-15-tla-dev-status/">https://ahelwer.ca/post/2025-05-15-tla-dev-status/</a>, See on <a href="https://news.ycombinator.com/item?id=43998115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
      <article role="main">
        <p>The <a href="https://conf.tlapl.us/2025-etaps/">2025 TLA‚Å∫ Community Event</a> was held last week on May 4th at McMaster University in Hamilton, Ontario, Canada.
It was a satellite event to <a href="https://etaps.org/2025/">ETAPS 2025</a>, which I also attended, and plan to write about in the near future.
I gave a talk somewhat-hucksterishly titled <em>It‚Äôs never been easier to write TLA‚Å∫ tooling!</em> which I will spin into a general account of the state of TLA‚Å∫ development here.
The conference talks were all recorded, so if you‚Äôd like this blog post in video form you can watch it below:</p>

    <p>
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube-nocookie.com/embed/KrhZebeRn90?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" title="YouTube video"></iframe>
    </p>

<h2 id="outline">Outline</h2>
<p>The thesis of this post is that almost all the dreams &amp; desires we have for TLA‚Å∫ are downstream of making it easy to develop in or on TLA‚Å∫ language tooling.
We break this down into three parts:</p>
<ol>
<li>Overview of existing TLA‚Å∫ language tooling</li>
<li>Overcoming the legacy code challenge</li>
<li>Some TLA‚Å∫ development ideas for the near and middle future</li>
</ol>
<p>I‚Äôm very optimistic about the future of TLA‚Å∫, despite the challenges.
We now have the <a href="https://foundation.tlapl.us/">TLA‚Å∫ Foundation</a>, which has very generously been paying me a comfortable living wage to work on the tools for the past six months.
I will talk about some of the things I‚Äôve accomplished.
Being paid to work on FOSS is a blessed existence; if that sounds appealing to you as well, the TLA‚Å∫ Foundation is <a href="https://foundation.tlapl.us/grants/2024-grant-program/index.html">looking to fund more contributors</a>!</p>

<p>This is a summary of what TLA‚Å∫ language tooling exists, to stoke the imagination for what tools you could contribute to or build upon.</p>
<h3 id="parsers">Parsers</h3>
<p>Parsers are of course the foundation of any language tooling, because your tools need to know what some source code actually says.
We have a decent stable of them:</p>
<ul>
<li><a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools">SANY</a> (Syntactic ANalYzer) - our flagship parser, a Java-based recursive descent parser generated by <a href="https://github.com/javacc/javacc">JavaCC</a>.
This is the only parser that really handles all the syntax and does a lot of semantic checks (including level-checking, which is the closest thing TLA‚Å∫ has to type checking) that other parsers don‚Äôt.</li>
<li><a href="https://github.com/tlaplus/tlapm">The TLAPM parser</a> - a parser-combinator-based OCaml parser, this is used by the TLA‚Å∫ Proof System.
It handles almost all TLA‚Å∫ syntax and does some rudimentary semantic analysis.</li>
<li><a href="https://github.com/tlaplus-community/tree-sitter-tlaplus/">tree-sitter-tlaplus</a> - I wrote &amp; <a href="https://ahelwer.ca/post/2023-01-11-tree-sitter-tlaplus/">presented this</a> at the 2021 TLA‚Å∫ Community Event.
Like all <a href="https://tree-sitter.github.io/tree-sitter/">tree-sitter</a> grammars, it uses a JavaScript-based DSL to generate a LR(1) parser in C along with some handwritten C to handle context-sensitive syntax.
This makes it very portable, and a strong tree-sitter ecosystem has developed with nice bindings for Python, Rust, JavaScript, and other languages.
This parser only handles syntax, no semantic checks, but that‚Äôs good enough for a lot of tools.
Its LR(1) nature also makes it a great static analyzer for ambiguity introduced by <a href="https://github.com/tlaplus/rfcs/issues/9">proposed TLA‚Å∫ language extensions</a>.</li>
</ul>
<p>Those are the big three.
There are a couple others floating around, in varying stages of completion: <a href="https://pypi.org/project/tla/">one in Python</a>, <a href="https://github.com/ret/specifica">Haskell</a>, <a href="https://github.com/bugarela/tla-transmutation">Haskell (again)</a>, and <a href="https://codeberg.org/tlaplus/rstla">Rust</a> (I got halfway through writing that last one myself before getting lost in the sauce of the Rust type system &amp; burning out).</p>
<h3 id="interpreters">Interpreters</h3>
<p>Moving up the stack.
For the longest time we had only one interpreter, and now we have two!</p>
<ul>
<li>The one used by the <a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools/src/tlc2">TLC</a> (Temporal Logic Checker) finite model checker - it‚Äôs written in Java and uses SANY for parsing.
It also now has a REPL.</li>
<li><a href="https://github.com/will62794/spectacle">Spectacle</a> - presented by <a href="https://will62794.github.io/">William Schultz</a> at the <a href="https://conf.tlapl.us/2024/">2024 TLA‚Å∫ conference</a>,  this is a web-based interpreter written in JavaScript using tree-sitter-tlaplus as a parser.
I think this tool is really neat!
It has a large focus on sharing specs &amp; traces with colleagues.</li>
</ul>
<h3 id="model-checkers">Model Checkers</h3>
<p>Again moving up the stack.
We now have <em>three</em> model checkers!</p>
<ul>
<li><a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools/src/tlc2">TLC</a> - mentioned previously, is the only fully-featured model checker.
It‚Äôs a finite state model checker, with all the benefits (predictable &amp; understandable performance) &amp; drawbacks (combinatoric state explosion) that implies.
It is the only checker that fully handles liveness, in particular refinement - a banner TLA‚Å∫ language feature, and arguably its <em>raison d‚Äô√™tre</em>.
It is written in Java and uses SANY for parsing.</li>
<li><a href="https://github.com/apalache-mc/apalache">Apalache</a> - a symbolic model checker that uses <a href="https://github.com/Z3Prover/z3">Z3</a> to model-check TLA‚Å∫ specifications.
It handles some liveness properties but not all.
It is written in Scala, and also uses SANY for parsing.</li>
<li><a href="https://informal.systems/">Spectacle</a> - this was recently updated to do basic safety checking by running a breadth-first search of the model‚Äôs state space, in the browser!
Once you do the difficult work of developing a TLA‚Å∫ interpreter it is not too much extra effort to add safety checking (liveness checking is where it really starts to get hairy).
As covered above, this is written in JavaScript and uses tree-sitter-tlaplus for parsing.</li>
</ul>
<h3 id="others">Others</h3>
<p>Here‚Äôs a grab bag of other notable TLA‚Å∫ language tools:</p>
<ul>
<li><a href="https://github.com/tlaplus/tlapm">TLAPM</a> - a system for validating proofs written in TLA‚Å∫‚Äôs proof language.
It‚Äôs written in OCaml and uses its own parser, described above.
TLA‚Å∫ proofs are translated to obligations for discharge by an array of backend provers including <a href="https://www.cl.cam.ac.uk/research/hvg/Isabelle/">Isabelle</a>, <a href="https://github.com/Z3Prover/z3">Z3</a>, and <a href="https://github.com/zenon-prover/zenon">Zenon</a>.</li>
<li><a href="https://apalache-mc.org/docs/apalache/typechecker-snowcat.html">The Snowcat Type Checker</a> - this is bundled with Apalache and verifies TLA‚Å∫ type annotations encoded in comments.
I think it‚Äôs a very neat project that is underutilized.
Modulo some weirdness around conversions between functions and sequences, many industry-produced TLA‚Å∫ specs have variables spanning a small set of values and would benefit from catching type errors at parse time.</li>
<li><a href="https://github.com/FedericoPonzi/tlaplus-formatter">Tlaplus-formatter</a> &amp; <a href="https://github.com/domodwyer/tlafmt">tlafmt</a> - we now have not one but <em>two</em> TLA‚Å∫ formatters!
The former is written in Java and uses SANY for parsing, while the latter is written in Rust and uses tree-sitter-tlaplus.</li>
<li><a href="https://github.com/tlaplus/tlapm/tree/e9b8bb51818f0b454384e8d94fe614899a0aaa78/lsp">LSP Server</a> - written by <a href="https://github.com/kape1395">Karolis Petrauskas</a>, this is aimed at improving support for the TLA‚Å∫ proof language in the TLA‚Å∫ VS Code extension.
It is written in OCaml on top of the TLAPM parser.</li>
<li><a href="https://github.com/tlaplus-community/tlauc">TLAUC</a> - the TLA‚Å∫ Unicode Converter, written by myself in Rust using tree-sitter-tlaplus.
This converts your TLA‚Å∫ specs between their LaTeX-like ASCII and Unicode representations.
I introduced <a href="https://ahelwer.ca/post/2024-05-28-tla-unicode/">Unicode support to TLA‚Å∫</a> at the <a href="https://conf.tlapl.us/2024/">2024 TLA‚Å∫ conference</a>.
Write your TLA‚Å∫ specs in Unicode!
It‚Äôs great!</li>
<li><a href="https://github.com/tlaplus/vscode-tlaplus/">VS Code Extension</a> - slowly replacing the Eclipse-based TLA‚Å∫ Toolbox as the IDE of choice, this is written in TypeScript and uses regexes to cobble together some concept of TLA‚Å∫ parsing.
Don‚Äôt get me wrong, it‚Äôs really a nice project!
It recently <a href="https://github.com/tlaplus/vscode-tlaplus/pull/367">bundled SANY</a> so is offloading more and more functionality onto a real parser.</li>
</ul>
<h3 id="summing-it-up">Summing it up</h3>
<p>I think the current state of the TLA‚Å∫ tooling ecosystem is <em>strong</em>.
This section was quite long once I wrote it all out.
People don‚Äôt just want to use TLA‚Å∫, they want to make it better!
That‚Äôs a really good energy to have in the community.
A lot of people don‚Äôt like being tied to the JVM but alternatives are slowly taking shape.</p>
<h2 id="overcoming-the-legacy-code-challenge">Overcoming the Legacy Code Challenge</h2>
<p>Many working software engineers have read (or at least heard of) this book:</p>
<p><img src="https://ahelwer.ca/post/2025-05-15-tla-dev-status/legacy-code.jpg" alt=""></p>
<p>It has a concise &amp; objective definition of legacy code that I like: legacy code is code without tests.
However, as I‚Äôve grown in my career I‚Äôve arrived at a fuzzier definition: legacy code is code not in living knowledge.
The codebase or even a particular module does not exist in totality inside the heads of the people who work on it.
We‚Äôve probably all written software from scratch and know how easy it is to make massive changes because you know how every part works.
If you hand the same task to a codebase newcomer you can reasonably expect it to take 10-100x as long to effect the same changes, and the codebase end state will be much less coherent.</p>
<p>We have a large shortfall of living knowledge in the TLA‚Å∫ project, which I will further elaborate in the next section.
The thesis of this section is simple: we must overcome this challenge if TLA‚Å∫ is to <em><strong>thrive</strong></em>.
TLA‚Å∫ will trundle along on inertia for quite a while, but eventually find its way into irrelevance if we do not conquer the challenge before us.</p>
<p>One last note on living knowledge: it is interlinked with tests.
Once living knowledge is lost it must be rebuilt from scratch, and that is only possible if you can write tests for the codebase.
Poke &amp; prod it, see how it flows and is interlinked.
Most of <em>Working Effectively with Legacy Code</em> deals with how to bootstrap your way to safely making changes in a codebase that lacks tests, when writing tests <em>requires</em> changing the codebase to make it testable.</p>
<h3 id="the-challenge-for-tla">The Challenge for TLA‚Å∫</h3>
<p>First, a quick history:
Leslie Lamport published the paper <a href="https://lamport.azurewebsites.net/pubs/pubs.html#old-tla-src"><em>A Temporal Logic of Actions</em></a> in April of 1990.
Jean-Charles Gregoir wrote the initial SANY TLA‚Å∫ parser in the late 1990s.
The TLC model checker was written over 25 years ago, announced in the 1999 paper <a href="https://lamport.azurewebsites.net/pubs/pubs.html#yuanyu-model-checking"><em>Model Checking TLA‚Å∫ Specifications</em></a> by Yuan Yu and Panagiotis Manolios which described writing an interpreter &amp; safety checker for TLA‚Å∫.
By 2002 the liveness checker <a href="https://lamport.azurewebsites.net/pubs/pubs.html#spec-and-verifying">had been developed</a>.
The tools were subsequently heavily modified around 2010 for the release of <a href="https://lamport.azurewebsites.net/tla/tla2.html">TLA‚Å∫ 2</a>, which added the proof language.
Around this time Inria &amp; Microsoft Research partnered to write TLAPM so the proof language could be formally checked.
Inria funding lapsed in the late 2010s and TLAPM development slowed dramatically.
Development on the Apalache symbolic model checker began in the late 2010s, and accelerated with <a href="https://ahelwer.ca/post/2025-05-15-tla-dev-status/informal.systems">Informal Systems</a> bringing it in-house in the early 2020s, but development has similarly slowed dramatically since Informal Systems spun out Apalache near the end of 2024.
The <a href="https://foundation.tlapl.us/">TLA‚Å∫ Foundation</a> was founded to support TLA‚Å∫ as it was spun out from Microsoft Research in the leadup to Leslie Lamport‚Äôs retirement at the end of 2024.</p>
<p>We will focus on the challenge of maintaining SANY and TLC.
By comparison to SANY &amp; TLC, TLAPM and Apalache are much less approachable by non-academic software engineers and I would be interested in watching talks on how their development can be ramped up &amp; made sustainable.</p>
<p>So, we have a large quarter-century-old Java codebase that is difficult to unit test due to prolific global static state, where all the original authors have long since left the project, and where the three regular core contributors lack knowledge of its many parts.
Each of us focuses on a different area: I tend to work on the parser, <a href="https://github.com/lemmy">Markus Kuppe</a> tends to work on the model checker, and <a href="https://calvin.loncaric.us/">Calvin Loncaric</a> tends to work on the interpreter &amp; is also our resident true Java expert.
There are many things we want to do but are restrained by the unknowns.
This all sounds somewhat dire, so why am I optimistic?</p>
<h3 id="the-dream-and-our-blessings">The Dream and Our Blessings</h3>
<p>Let‚Äôs start with a dream: <strong>a world where we are bold and unafraid to make large changes to the core TLA‚Å∫ tools</strong>.
How do we get there?</p>
<p>Thankfully, we start from a position of strength.
Language tooling is a blessed area of software engineering.
It is one of the few places where program requirements are relatively complete &amp; unambiguous.
It is actually possible to <em>complete</em> a parser!
Usually starting a software project is kind of like casting a curse upon yourself, where you must continually provide it your labor or it will disappear from relevance and all your work will be for nought.
Not so here.
Of course, conventional language tooling demands do become stronger over time.
Parsers have to be useful in a language server context now, which requires better error recovery than was previously standard.
But these are from relatively slow changes in development culture and only crop up every decade or so.</p>
<h3 id="concrete-strategies">Concrete Strategies</h3>
<p>Enough faffing around.
Here are the three strategies I think will bring us from dream to reality:</p>
<ol>
<li><strong>Tests</strong> - Of course!
The TLA‚Å∫ Foundation has been funding me to put a <em>ton</em> of effort into developing <a href="https://github.com/tlaplus/rfcs/tree/2a772d9dd11acec5d7dedf30abfab91a49de48b8/language_standard/tests">standardized implementation-independent test suites</a> for TLA‚Å∫ parsers, then applying them to the existing parsers.
We now have a nice source-code-input/expected-AST-output syntax suite applied to the three prominent parsers (good for conformance!) along with a semantic-level test suite with lots of in-language assertions about levels and references (thanks to <a href="https://lobste.rs/s/58jah7/what_implementation_independent_test">lobste.rs users</a> for some great suggestions on its design) and - my favorite - a set of 150ish TLA‚Å∫ parse inputs that should each trigger a specific standardized error code in any conformant parser.
Combined, I think these make every TLA‚Å∫ parser open for development!
We are no longer constrained to analyzing bugs by git-bisecting &amp; comparing inscrutable code; just write a test and trace it in the debugger!</li>
<li><strong>Developer Onboarding</strong> - We must go beyond documentation.
Treat the codebase as a fixed artifact to be studied &amp; learned by any new contributor, then develop a syllabus for it.
The TLA‚Å∫ Foundation has been funding me for this in various ways, such as writing <a href="https://github.com/tlaplus/tlaplus/blob/e61495931507f5ad5ec232be9e65fe8f2a1bf379/DEVELOPING.md">developing</a> &amp; <a href="https://github.com/tlaplus/tlaplus/blob/e61495931507f5ad5ec232be9e65fe8f2a1bf379/CONTRIBUTING.md">contributing</a> guides, highlighting good first issues in <a href="https://foundation.tlapl.us/blog/index.html">monthly development updates</a>, and - most substantially - writing a long tutorial on <a href="https://docs.tlapl.us/creating:start">how to write your <em>own</em> minimal TLA‚Å∫ model checker</a>!
This last one is very fun and is based on the excellent free online textbook <a href="https://craftinginterpreters.com/"><em>Crafting Interpreters</em></a> by Robert Nystrom.
Give it a look!</li>
<li><strong>Grants &amp; Stipends</strong> - The fundamental contradiction of FOSS development is that every substantial FOSS project requires <em>full-time</em> commitment for long periods of time, but people need to eat and it‚Äôs hard to make money giving something away for free.
When I first got into this game I thought Linux was something people developed on their evenings and weekends.
That‚Äôs true for some people, but the overwhelming majority of code is contributed by people working on Linux full-time!
A lot of digital ink has been spilled about how to manage the fundamental FOSS contradiction.
Foundations with corporate sponsorship are one method, and that‚Äôs what we have.
The TLA‚Å∫ Foundation can fund people to work on the tools, and they are.
Interested?
<a href="https://foundation.tlapl.us/grants/2024-grant-program/index.html">Join us</a>!</li>
</ol>
<h3 id="summing-it-up-1">Summing it up</h3>
<p>So, we are starting from a position of strength and have concrete strategies to overcome the remaining challenges.
We should also recognize that our struggle is shared.
People in the TLA‚Å∫ community occasionally say things like ‚ÄúTLA‚Å∫ is not a programming language‚Äù, but we face the same challenges as the developers of every other programming language out there!
Even the exotic parts of TLA‚Å∫ are ‚Äújust‚Äù algorithms.
By which I mean learning algorithms is relatively easy, fun, &amp; contained compared to the challenge of developing an unknown codebase.
As complicated as the liveness checking algorithm is, it‚Äôs ‚Äújust‚Äù an algorithm on graphs that an ordinary developer can learn in a few days of study, given a suitable tutorial.
And it‚Äôs our job to write that tutorial and make it fun.</p>
<h2 id="whats-next">What‚Äôs Next?</h2>
<p>I ended the talk with a few ideas I had for TLA‚Å∫ tool development in the near future.
One, inspired by attendance at <a href="https://bugbash.antithesis.com/">Antithesis BugBash</a> last month, is generative testing for the TLA‚Å∫ tools!
It‚Äôs all well &amp; good to lock me in an office to crank out hundreds of test cases, but what if that process could be automated?
It would certainly be more fun to develop than a large static test corpus.
Actually it‚Äôs good to have both: static test cases to nail down the core language functionality, and generative tests to explore the nooks &amp; crannies.</p>
<p>I also tentatively proposed simplifications to the TLA‚Å∫ syntax.
I‚Äôm a bit of a weird guy who <em>likes</em> the TLA‚Å∫ syntax (especially in its Unicode form) but other people do struggle with it, as <a href="https://www.hillelwayne.com/">Hillel Wayne</a> can attest from his work teaching TLA‚Å∫ both online and offline.
I‚Äôm not a purist who is against evolving the syntax.
I have what I think is <a href="https://github.com/tlaplus/rfcs/issues/10">quite a nice RFC</a> to combine &amp; disambiguate the set filter &amp; set map syntax.
Larger changes will have to be debated.
This is design &amp; consensus work; the actual code changes will likely not be substantial in comparison.</p>
<p>I also want to improve the SANY API &amp; consumption experience so people can easily build their own tools instead of requesting features be upstreamed.
A new API has <a href="https://github.com/tlaplus/tlaplus/pull/1125">already been prototyped</a>, and all SANY unit tests are currently exercising it.
Markus has also put effort into automatically <a href="https://oss.sonatype.org/content/repositories/snapshots/org/lamport/tla2tools/">publishing a Java package</a> from the rolling release.
Finally, we need to write a collection of how-to guides for consuming &amp; developing against the package.
There are actually quite a few projects &amp; features in TLA‚Å∫ land that exist but have not had documentation written &amp; disseminated for them.
I think I could spend an entire year writing only tutorials &amp; documentation.
I am coming closer to being persuaded that would actually be a good use of time, maybe even the best use of my time for TLA‚Å∫.</p>
<h3 id="the-one-billion-states-per-minute-initiative">The One Billion States Per Minute Initiative</h3>
<p>I wanted to end the talk with a bang so here it is: I think we can increase the TLC model checker throughput to 1 billion states per minute (1000x speedup) by writing a <em>bytecode interpreter</em>.
Currently, TLC clocks about 1 million states per minute.
Calvin has done some <em>very</em> preliminary prototyping with transpiling TLA‚Å∫ specs to C++ then compiling &amp; running them as little breadth-first search programs, and saw some head-turning numbers.
The TLC interpreter is a tree-walk interpreter written in Java.
As optimized as it is, there‚Äôs a limit to what can be done with the format.
This would be a bold, substantial project but I think it is worth exploring.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Thanks for reading!
I hope this has invigorated &amp; inspired you to consider developing your own TLA‚Å∫ tooling or contribute to the existing tools, while giving a nice overview of the current state of things.</p>
<h2 id="links--discussions">Links &amp; Discussions</h2>
<p><a href="https://cse.buffalo.edu/~demirbas/">Murat Demirbas</a> wrote <a href="https://muratbuffalo.blogspot.com/2025/05/modular-verification-of-mongodb.html">a blog post</a> version of his talk at the TLA‚Å∫ community event, on using TLA‚Å∫ to specify cross-shard transactions in MongoDB.
He also wrote a post <a href="https://muratbuffalo.blogspot.com/2025/05/notes-from-tla-community-event.html">summarizing other talks</a> from the event.</p>
<p><a href="https://emptysqua.re/blog/about/">A. Jesse Jiryu Davis</a> wrote <a href="https://emptysqua.re/blog/are-we-serious-about-statistical-properties-tlaplus/">a blog post</a> version of his TLA‚Å∫ community event talk, on the possibility of adding statistical &amp; performance modeling capabilities to TLA‚Å∫.
He also posted <a href="https://emptysqua.re/blog/2025-tlaplus-community-event/">his notes</a> about other talks at the event.</p>
<p>Discuss this post here:</p>
<ul>
<li><a href="https://lobste.rs/s/auqf51/current_state_tla_development">lobste.rs</a></li>
<li><a href="https://discuss.systems/@ahelwer/114513355067720408">Mastodon</a></li>
<li><a href="https://news.ycombinator.com/item?id=43998115">Hacker News</a></li>
<li><a href="https://www.linkedin.com/posts/ahelwer_the-current-state-of-tla-development-activity-7328855373866504192-uv45">LinkedIn</a></li>
</ul>


        
          
        

        

        
      </article>

      
        
      


      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windsurf SWE-1: Our First Frontier Models (134 pts)]]></title>
            <link>https://windsurf.com/blog/windsurf-wave-9-swe-1</link>
            <guid>43998049</guid>
            <pubDate>Thu, 15 May 2025 18:47:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://windsurf.com/blog/windsurf-wave-9-swe-1">https://windsurf.com/blog/windsurf-wave-9-swe-1</a>, See on <a href="https://news.ycombinator.com/item?id=43998049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="section-container-wide"><main><div><h2>Stay up to date on the latest Windsurf news</h2><form><div><p><label for="email-address">Email address</label></p></div><p>Please enter a valid email address</p></form></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard Law paid $27 for a copy of Magna Carta. It's an original (186 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/15/world/europe/harvard-law-magna-carta-original.html</link>
            <guid>43997830</guid>
            <pubDate>Thu, 15 May 2025 18:26:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/15/world/europe/harvard-law-magna-carta-original.html">https://www.nytimes.com/2025/05/15/world/europe/harvard-law-magna-carta-original.html</a>, See on <a href="https://news.ycombinator.com/item?id=43997830">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/15/world/europe/harvard-law-magna-carta-original.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Baby Is Healed with First Personalized Gene-Editing Treatment (1037 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html</link>
            <guid>43997636</guid>
            <pubDate>Thu, 15 May 2025 18:06:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html">https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html</a>, See on <a href="https://news.ycombinator.com/item?id=43997636">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Tinfoil (YC X25): Verifiable Privacy for Cloud AI (130 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43996555</link>
            <guid>43996555</guid>
            <pubDate>Thu, 15 May 2025 16:19:00 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43996555">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <tbody><tr id="43996555">
      <td><span></span></td>      <td><center><a id="up_43996555" href="https://news.ycombinator.com/vote?id=43996555&amp;how=up&amp;goto=item%3Fid%3D43996555"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=43996555">Launch HN: Tinfoil (YC X25): Verifiable Privacy for Cloud AI</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_43996555">101 points</span> by <a href="https://news.ycombinator.com/user?id=FrasiertheLion">FrasiertheLion</a> <span title="2025-05-15T16:19:00 1747325940"><a href="https://news.ycombinator.com/item?id=43996555">10 hours ago</a></span> <span id="unv_43996555"></span> | <a href="https://news.ycombinator.com/hide?id=43996555&amp;goto=item%3Fid%3D43996555">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Tinfoil%20%28YC%20X25%29%3A%20Verifiable%20Privacy%20for%20Cloud%20AI&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=43996555&amp;auth=3ccb71ca1f7cf63db9871486b2141cb96771e089">favorite</a> | <a href="https://news.ycombinator.com/item?id=43996555">81&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hello HN! We‚Äôre Tanya, Sacha, Jules and Nate from Tinfoil: <a href="https://tinfoil.sh/">https://tinfoil.sh</a>. We host models and AI workloads on the cloud while guaranteeing zero data access and retention. This lets us run open-source LLMs like Llama, or Deepseek R1 on cloud GPUs without you having to trust us‚Äîor any cloud provider‚Äîwith private data.</p><p>Since AI performs better the more context you give it, we think solving AI privacy will unlock more valuable AI applications, just how TLS on the Internet enabled e-commerce to flourish knowing that your credit card info wouldn't be stolen by someone sniffing internet packets.</p><p>We come from backgrounds in cryptography, security, and infrastructure. Jules did his PhD in trusted hardware and confidential computing at MIT, and worked with NVIDIA and Microsoft Research on the same, Sacha did his PhD in privacy-preserving cryptography at MIT, Nate worked on privacy tech like Tor, and I (Tanya) was on Cloudflare's cryptography team. We were unsatisfied with band-aid techniques like PII redaction (which is actually undesirable in some cases like AI personal assistants) or ‚Äúpinky promise‚Äù security through legal contracts like DPAs. We wanted a real solution that replaced trust with provable security.</p><p>Running models locally or on-prem is an option, but can be expensive and inconvenient. Fully Homomorphic Encryption (FHE) is not practical for LLM inference for the foreseeable future. The next best option is using secure enclaves: a secure environment on the chip that no other software running on the host machine can access. This lets us perform LLM inference in the cloud while being able to prove that no one, not even Tinfoil or the cloud provider, can access the data. And because these security mechanisms are implemented in hardware, there is minimal performance overhead.</p><p>Even though we (Tinfoil) control the host machine, we do not have any visibility into the data processed inside of the enclave. At a high level, a secure enclave is a set of cores that are reserved, isolated, and locked down to create a sectioned off area. Everything that comes out of the enclave is encrypted: memory and network traffic, but also peripheral (PCIe) traffic to other devices such as the GPU. These encryptions are performed using secret keys that are generated inside the enclave during setup, which never leave its boundaries. Additionally, a ‚Äúhardware root of trust‚Äù baked into the chip lets clients check security claims and verify that all security mechanisms are in place.</p><p>Up until recently, secure enclaves were only available on CPUs. But NVIDIA confidential computing recently added these hardware-based capabilities to their latest GPUs, making it possible to run GPU-based workloads in a secure enclave.</p><p>Here‚Äôs how it works in a nutshell:</p><p>1. We publish the code that should run inside the secure enclave to Github, as well as a hash of the compiled binary to a transparency log called Sigstore</p><p>2. Before sending data to the enclave, the client fetches a signed document from the enclave which includes a hash of the running code signed by the CPU manufacturer. It then verifies the signature with the hardware manufacturer to prove the hardware is genuine. Then the client fetches a hash of the source code from a transparency log (Sigstore) and checks that the hash equals the one we got from the enclave. This lets the client get verifiable proof that the enclave is running the exact code we claim.</p><p>3. With the assurance that the enclave environment is what we expect, the client sends its data to the enclave, which travels encrypted (TLS) and is only decrypted inside the enclave.</p><p>4. Processing happens entirely within this protected environment. Even an attacker that controls the host machine can‚Äôt access this data.
We believe making end-to-end verifiability a ‚Äúfirst class citizen‚Äù is key. Secure enclaves have traditionally been used to remove trust from the cloud provider, not necessarily from the application provider. This is evidenced by confidential VM technologies such as Azure Confidential VM allowing ssh access by the host into the confidential VM. Our goal is to provably remove trust both from ourselves, aka the application provider, as well as the cloud provider.</p><p>We encourage you to be skeptical of our privacy claims. Verifiability is our answer. It‚Äôs not just us saying it‚Äôs private; the hardware and cryptography let you check. Here‚Äôs a guide that walks you through the verification process: <a href="https://docs.tinfoil.sh/verification/attestation-architecture">https://docs.tinfoil.sh/verification/attestation-architectur...</a>.</p><p>People are using us for analyzing sensitive docs, building copilots for proprietary code, and processing user data in agentic AI applications without the privacy risks that previously blocked cloud AI adoption.</p><p>We‚Äôre excited to share Tinfoil with HN!</p><p>* Try the chat (<a href="https://tinfoil.sh/chat">https://tinfoil.sh/chat</a>): It verifies attestation with an in-browser check. Free, limited messages, $20/month for unlimited messages and additional models</p><p>* Use the API (<a href="https://tinfoil.sh/inference">https://tinfoil.sh/inference</a>): OpenAI API compatible interface. $2 / 1M tokens</p><p>* Take your existing Docker image and make it end to end confidential by deploying on Tinfoil. Here's a demo of how you could use Tinfoil to run a deepfake detection service that could run securely on people's private videos: <a href="https://www.youtube.com/watch?v=_8hLmqoutyk" rel="nofollow">https://www.youtube.com/watch?v=_8hLmqoutyk</a>. Note: This feature is not currently self-serve.</p><p>* Reach out to us at contact@tinfoil.sh if you want to run a different model or want to deploy a custom application, or if you just want to learn more!</p><p>Let us know what you think, we‚Äôd love to hear about your experiences and ideas in this space!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td></td></tr>  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Don't Like NumPy (436 pts)]]></title>
            <link>https://dynomight.net/numpy/</link>
            <guid>43996431</guid>
            <pubDate>Thu, 15 May 2025 16:05:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynomight.net/numpy/">https://dynomight.net/numpy/</a>, See on <a href="https://news.ycombinator.com/item?id=43996431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
    
  <section>
    <p>They say you can‚Äôt truly hate someone unless you loved them first. I don‚Äôt know if that‚Äôs true as a general principle, but it certainly describes my relationship with NumPy.</p>

<p><a href="https://numpy.org/">NumPy</a>, by the way, is some software that does computations on arrays in Python. It‚Äôs insanely popular and has had a huge influence on all the popular machine learning libraries like PyTorch. These libraries share most of the same issues I discuss below, but I‚Äôll stick to NumPy for concreteness.</p>

<p>NumPy makes easy things easy. Say <code>A</code> is a <code>5√ó5</code> matrix, <code>x</code> is a length-5 vector, and you want to find the vector <em>y</em> such that <code>Ay=x</code>. In NumPy, that would be:</p>

<div><pre><code><span>y</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span> <span>x</span><span>)</span>
</code></pre></div>

<p>So elegant! So clear!</p>

<p>But say the situation is even a <em>little</em> more complicated. Say <code>A</code> is a stack of 100 <code>5√ó5</code> matrices, given as a <code>100√ó5√ó5</code> array. And say <code>x</code> is a stack of 100 length-5 vectors, given as a <code>100√ó5</code> array. And say you want to solve <code>A·µ¢y·µ¢=x·µ¢</code> for <code>1‚â§i‚â§100</code>.</p>

<p>If you could use loops, this would be easy:</p>

<div><pre><code><span>y</span> <span>=</span> <span>np</span><span>.</span><span>empty_like</span><span>(</span><span>x</span><span>)</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>100</span><span>):</span>
    <span>y</span><span>[</span><span>i</span><span>,:]</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>[</span><span>i</span><span>,:,:],</span> <span>x</span><span>[</span><span>i</span><span>,:])</span>
</code></pre></div>

<p>But you can‚Äôt use loops. To some degree, this is a limitation of loops being slow in Python. But nowadays, everything is GPU and if you‚Äôve got big arrays, you probably don‚Äôt want to use loops in any language. To get all those transistors firing, you need to call special GPU functions that will sort of split up the arrays into lots of little pieces and process them in parallel.</p>

<p>The good news is that NumPy knows about those special routines (at least if you use <a href="https://github.com/jax-ml/jax">JAX</a> or <a href="https://cupy.dev/">CuPy</a>), and if you call <code>np.linalg.solve</code> correctly, it will use them.</p>

<p>The bad news is that no one knows how do that.</p>

<p>Don‚Äôt believe me? OK, which of these is right?</p>

<div><pre><code><span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>,</span><span>axis</span><span>=</span><span>0</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>,</span><span>axes</span><span>=</span><span>[[</span><span>1</span><span>,</span><span>2</span><span>],</span><span>1</span><span>])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>T</span><span>,</span> <span>x</span><span>.</span><span>T</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>T</span><span>,</span> <span>x</span><span>).</span><span>T</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span> <span>x</span><span>[</span><span>None</span><span>,:,:])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>[:,:,</span><span>None</span><span>])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>[:,:,</span><span>None</span><span>])[:,:,</span><span>0</span><span>]</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>[:,:,:,</span><span>None</span><span>],</span><span>x</span><span>[:,</span><span>None</span><span>,</span><span>None</span><span>,:])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>transpose</span><span>([</span><span>1</span><span>,</span><span>2</span><span>,</span><span>0</span><span>]),</span><span>x</span><span>[:,:,</span><span>None</span><span>]).</span><span>T</span>
</code></pre></div>

<p>No one knows. And let me show you something else. Here‚Äôs the <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html">documentation</a>:</p>

<p><img src="https://dynomight.net/img/numpy/solve.png" alt="np.linalg.solve"></p>

<p>Read that. Meditate on it. Now, notice: You <em>still</em> don‚Äôt know how to solve <code>A·µ¢y·µ¢=x·µ¢</code> for all <code>i</code> at once. Is it even possible? Did I lie when I said it was?</p>

<p>As far as I can tell, what people actually do is try random variations until one seems to work.</p>

<h2 id="why-numpy-bad">Why NumPy bad</h2>

<p>NumPy is all about applying operations to arrays. When the arrays have 2 or fewer dimensions, everything is fine. But if you‚Äôre doing something even mildly complicated, you inevitably find yourself with some operation you want to apply to some dimensions of array <code>A</code>, some other dimensions of array <code>B</code>, and some <em>other</em> dimensions of array <code>C</code>. And NumPy has no theory for how to express that.</p>

<p>Let me show you what I mean. Suppose:</p>

<ul>
  <li><code>A</code> is a <code>K√óL√óM</code> array</li>
  <li><code>B</code> is a <code>L√óN</code> array</li>
  <li><code>C</code> is a <code>K√óM</code> array</li>
</ul>

<p>And say that for each <code>k</code> and <code>n</code>, you‚Äôd like to compute the mean over the <code>L</code> and <code>M</code> dimensions. That is, you want</p>

<p>&nbsp;&nbsp;  <code>D<sub>kn</sub> = 1/(LM) √ó ‚àë<sub>lm</sub> A<sub>klm</sub> B<sub>ln</sub> C<sub>km</sub>.</code></p>

<p>To do that, you‚Äôve got two options. The first is to use grotesque dimension alignment tricks:</p>

<div><pre><code><span>D</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span>
        <span>np</span><span>.</span><span>mean</span><span>(</span>
            <span>A</span><span>[:,:,:,</span><span>None</span><span>]</span> <span>*</span>
            <span>B</span><span>[</span><span>None</span><span>,:,</span><span>None</span><span>,:]</span> <span>*</span>
            <span>C</span><span>[:,</span><span>None</span><span>,:,</span><span>None</span><span>],</span>
        <span>axis</span><span>=</span><span>1</span><span>),</span>
    <span>axis</span><span>=</span><span>1</span><span>)</span>
</code></pre></div>

<p>The hell, you ask? Why is <code>None</code> everywhere? Well, when indexing an array in NumPy, you can write <code>None</code> to insert a new dimension. <code>A</code> is <code>K√óL√óM</code>, but <code>A[:,:,:,None]</code> is <code>K√óL√óM√ó</code><strong><code>1</code></strong>. Similarly, <code>B[None,:,None,:]</code> is <strong><code>1</code></strong><code>√óL√ó</code><strong><code>1</code></strong><code>√óN</code> and <code>C[:,None,:,None]</code> is <code>K√ó</code><strong><code>1</code></strong><code>√óM√ó</code><strong><code>1</code></strong>. When you multiply these together, NumPy ‚Äúbroadcasts‚Äù all the size-1 dimensions to give a <code>K√óL√óM√óN</code> array. Then, the <code>np.mean</code> calls average over the <code>L</code> and <code>M</code> dimensions.</p>

<p>I think this is bad. I‚Äôve been using NumPy for years and I still find it impossible to write code like that without <em>always</em> making mistakes.</p>

<p>It‚Äôs also borderline-impossible to read. To prove this, I just flipped a coin and introduced a bug above if and only if the coin was tails. Is there a bug? Are you <em>sure</em>? No one knows.</p>

<p>Your second option is to desperately try to be clever. Life is short and precious, but if you spend a lot of yours reading the NumPy documentation, you might eventually realize that there‚Äôs a function called <a href="https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html"><code>np.tensordot</code></a>, and that it‚Äôs possible to make it do much of the work:</p>

<div><pre><code><span>D</span> <span>=</span> <span>(</span><span>1</span><span>/</span><span>L</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>mean</span><span>(</span>
                <span>np</span><span>.</span><span>tensordot</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>,</span> <span>axes</span><span>=</span><span>[</span><span>1</span><span>,</span><span>0</span><span>])</span> <span>*</span>
                <span>C</span><span>[:,:,</span><span>None</span><span>],</span>
            <span>axis</span><span>=</span><span>1</span><span>)</span>
</code></pre></div>

<p>That‚Äôs correct. (I promise.) But why does it work? What exactly is <code>np.tensordot</code> doing? If you saw that code in some other context, would you have the slightest idea what was happening?</p>

<p>Here‚Äôs how I‚Äôd do it, if only I could use loops:</p>

<div><pre><code><span>D</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>K</span><span>,</span><span>N</span><span>))</span>  
<span>for</span> <span>k</span> <span>in</span> <span>range</span><span>(</span><span>K</span><span>):</span>  
    <span>for</span> <span>n</span> <span>in</span> <span>range</span><span>(</span><span>N</span><span>):</span>  
        <span>a</span> <span>=</span> <span>A</span><span>[</span><span>k</span><span>,:,:]</span>  
        <span>b</span> <span>=</span> <span>B</span><span>[:,</span><span>n</span><span>]</span>  
        <span>c</span> <span>=</span> <span>C</span><span>[</span><span>k</span><span>,:]</span>  
        <span>assert</span> <span>a</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>L</span><span>,</span><span>M</span><span>)</span>  
        <span>assert</span> <span>b</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>L</span><span>,)</span>  
        <span>assert</span> <span>c</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>M</span><span>,)</span>  
        <span>D</span><span>[</span><span>k</span><span>,</span><span>n</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span><span>a</span> <span>*</span> <span>b</span><span>[:,</span><span>None</span><span>]</span> <span>*</span> <span>c</span><span>[</span><span>None</span><span>,:])</span>
</code></pre></div>

<p>People who‚Äôve written too much NumPy may find that clunky. I suspect that‚Äôs a wee bit of Stockholm Syndrome. But surely we can agree that it‚Äôs <em>clear</em>.</p>

<p>In practice, things are often even worse. Say that <code>A</code> had shape <code>M√óK√óL</code> rather than <code>K√óL√óM</code>. With loops, no big deal. But NumPy requires you to write monstrosities like <code>A.transpose([1,2,0])</code>. Or should that be <code>A.transpose([2,0,1])</code>? What shapes do those produce? No one knows.</p>

<p>Loops were better.</p>

<h2 id="ok-i-lied">OK I lied</h2>

<p>There is a third option:</p>

<div><pre><code><span>D</span> <span>=</span> <span>1</span><span>/</span><span>(</span><span>L</span><span>*</span><span>M</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'klm,ln,km-&gt;kn'</span><span>,</span> <span>A</span><span>,</span> <span>B</span><span>,</span> <span>C</span><span>)</span>
</code></pre></div>

<p>If you‚Äôve never seen Einstein summation before, that might look terrifying. But remember, our goal is to find</p>

<p>&nbsp;&nbsp; D<sub>kn</sub> = 1/(LM) √ó ‚àë<sub>lm</sub> A<sub>klm</sub> B<sub>ln</sub> C<sub>km</sub>.</p>

<p>The string in the above code basically gives labels to the indices in each of the three inputs (<code>klm,ln,km</code>) and the target indices for the output (<code>-&gt;kn</code>). Then, <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html"><code>np.einsum</code></a> multiplies together the corresponding elements of the inputs and sums over all indices that aren‚Äôt in the output.</p>

<p>Personally, I think <code>np.einsum</code> is one of the rare parts of NumPy that‚Äôs actually good. The strings are a bit tedious, but they‚Äôre worth it, because the overall function is easy(ish) to understand, is completely explicit, and is quite general and powerful.</p>

<p>Except, how does <code>np.einsum</code> achieve all this? It uses indices. Or, more precisely, it introduces a tiny <em>domain-specific language</em> based on indices. It doesn‚Äôt suffer from NumPy‚Äôs design flaws because it refuses to play by NumPy‚Äôs normal rules.</p>

<p>But <code>np.einsum</code> only does a few things. (<a href="https://github.com/arogozhnikov/einops">Einops</a> does a few more.) What if you want to apply some other function over various dimensions of some arrays? There is no <code>np.linalg.einsolve</code>. And if you create your own function, there‚Äôs <em>certainly</em> no ‚ÄúEinstein‚Äù version of <em>it</em>.</p>

<p>I think <code>np.einsum</code>‚Äôs goodness shows that NumPy went somewhere.</p>

<h2 id="intermission">Intermission</h2>

<p>Here‚Äôs a <a href="https://www.nga.gov/artworks/45858-burning-old-south-church-bath-maine">painting</a> which feels analogous to our subject.</p>

<p><img src="https://dynomight.net/img/numpy/burning.jpg" alt=""></p>

<h2 id="where-did-numpy-go-wrong">Where did NumPy go wrong?</h2>

<p>Here‚Äôs what I want from an array language. I ain‚Äôt particular about syntax, but it would be nice if:</p>

<ol>
  <li>When you want to do something, it‚Äôs ‚Äúobvious‚Äù how to do it.</li>
  <li>When you read some code, it‚Äôs ‚Äúobvious‚Äù what it does.</li>
</ol>

<p>Wouldn‚Äôt that be nice? I think NumPy doesn‚Äôt achieve these because of its original sin: It took away indices and replaced them with broadcasting. And broadcasting cannot fill indices‚Äô shoes.</p>

<h2 id="i-dont-love-numpy-broadcasting">I don‚Äôt love NumPy broadcasting</h2>

<p>NumPy‚Äôs core trick is broadcasting. Take this code:</p>

<div><pre><code><span>A</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>1</span><span>,</span><span>2</span><span>],[</span><span>3</span><span>,</span><span>4</span><span>],[</span><span>5</span><span>,</span><span>6</span><span>]])</span>
<span>B</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>10</span><span>,</span><span>20</span><span>])</span>
<span>C</span> <span>=</span> <span>A</span> <span>*</span> <span>B</span>
<span>print</span><span>(</span><span>C</span><span>)</span>
</code></pre></div>

<p>This outputs:</p>

<div><pre><code>[[ 10  40]
 [ 30  80]
 [ 50 120]]
</code></pre></div>

<p>Here, <code>A</code> is a <code>3√ó2</code> array, and <code>B</code> is a length-<code>2</code> array. When you multiply them together, <code>B</code> is ‚Äúbroadcast‚Äù to the shape of <code>A</code>, meaning the first column of <code>A</code> is multiplied with <code>B[0]=10</code> and the second is multiplied with <code>B[1]=20</code>.</p>

<p>In simple cases, this seems good. But I don‚Äôt love it. One reason is that, as we saw above, you often have to do gross things to the dimensions to get them to line up.</p>

<p>Another reason is that it isn‚Äôt explicit or legible. Sometimes <code>A*B</code> multiplies element-by-element, and sometimes it does more complicated things. So every time you see <code>A*B</code>, you have to figure out which case in the <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">broadcasting conventions</a> is getting triggered.</p>

<p>But the real problem with broadcasting is how it infects everything else. I‚Äôll explain below.</p>

<h2 id="i-dont-like-numpy-indexing">I don‚Äôt like NumPy indexing</h2>

<p>Here‚Äôs a riddle. Take this code:</p>

<div><pre><code><span>A</span> <span>=</span> <span>np</span><span>.</span><span>ones</span><span>((</span><span>10</span><span>,</span><span>20</span><span>,</span><span>30</span><span>,</span><span>40</span><span>))</span>
<span>i</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>])</span>
<span>j</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>0</span><span>],[</span><span>1</span><span>]])</span>
<span>B</span> <span>=</span> <span>A</span><span>[:,</span><span>i</span><span>,</span><span>j</span><span>,:]</span>
</code></pre></div>

<p>What shape does <code>B</code> have?</p>

<p>It turns out the answer is <code>10√ó2√ó3√ó40</code>. That‚Äôs because the <code>i</code> and <code>j</code> indices get broadcast to a shape of <code>2√ó3</code> and then something something mumble mumble mumble. Try to convince yourself it makes sense.</p>

<p>Done? OK, now try these:</p>

<div><pre><code><span>C</span> <span>=</span> <span>A</span><span>[:,:,</span><span>i</span><span>,</span><span>j</span><span>]</span>
<span>D</span> <span>=</span> <span>A</span><span>[:,</span><span>i</span><span>,:,</span><span>j</span><span>]</span>
<span>E</span> <span>=</span> <span>A</span><span>[:,</span><span>1</span><span>:</span><span>4</span><span>,</span><span>j</span><span>,:]</span>
</code></pre></div>

<p>What shapes do these have?</p>

<ul>
  <li>
    <p><code>C</code> is <code>10√ó20√ó2√ó3</code>. This seems logical, given what happened with <code>B</code> above.</p>
  </li>
  <li>
    <p>What about <code>D</code>? It is <code>2√ó3√ó10√ó30</code>. Now, for some reason, the <code>2</code> and <code>3</code> go at the beginning?</p>
  </li>
  <li>
    <p>And what about <code>E</code>? Well, ‚Äúslices‚Äù in Python exclude the endpoint, so <code>1:4</code> is equivalent to <code>[1,2,3]</code> which is equivalent to <code>i</code>, and so <code>E</code> is the same as <code>B</code>. Hahaha, just kidding! <code>E</code> is <code>10√ó3√ó2√ó1√ó40</code>.</p>
  </li>
</ul>

<p>Yes, that is what happens. Try it if you don‚Äôt believe me! I understand why NumPy does this, because I‚Äôve absorbed <a href="https://numpy.org/doc/stable/user/basics.indexing.html">this 5000 word document</a> that explains how NumPy indexing works. But I want that time back.</p>

<details>
  <summary>
For fun, I tried asking a bunch of AI models to figure out what shapes those arrays have. Here were the results:
</summary>

  <p>I used this query:</p>

  <blockquote>
    <p>Take this python code</p>

    <p>A = np.ones((10,20,30,40))<br>
i = np.array([1,2,3])<br>
j = np.array([[0],[1]])<br>
B = A[:,i,j,:]<br>
C = A[:,:,i,j]<br>
D = A[:,i,:,j]<br>
E = A[:,1:4,j,:]</p>

    <p>what shapes do B, C, D, and E have?</p>
  </blockquote>

  <p>Claude 3.7 used ‚Äúextended thinking‚Äù. Here are all the incorrect outputs:</p>

  <table>
    <thead>
      <tr>
        <th>AI</th>
        <th><code>B</code></th>
        <th><code>C</code></th>
        <th><code>D</code></th>
        <th><code>E</code></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>GPT 4.1</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó2√ó3√ó30</td>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td>Grok 3</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó3√ó30√ó2</td>
        <td>10√ó3√ó2√ó40</td>
      </tr>
      <tr>
        <td>Claude 3 Opus</td>
        <td>10√ó3√ó2√ó30</td>
        <td>10√ó20√ó3√ó2</td>
        <td>10√ó3√ó30√ó2</td>
        <td>10√ó3√ó2√ó40</td>
      </tr>
      <tr>
        <td>Llama 4 Maverick</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó3√ó30√ó2</td>
        <td>10√ó3√ó2√ó40</td>
      </tr>
      <tr>
        <td>o3</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó2√ó3√ó30</td>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td>Claude 3.7</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó3√ó30√ó2</td>
        <td>10√ó3√ó2√ó40</td>
      </tr>
    </tbody>
  </table>

</details>

<table>
  <thead>
    <tr>
      <th>AI</th>
      <th><code>B</code></th>
      <th><code>C</code></th>
      <th><code>D</code></th>
      <th><code>E</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT 4.1</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>‚úîÔ∏è</td>
    </tr>
    <tr>
      <td>Grok 3</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Claude 3 Opus</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Llama 4 Maverick</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>o3</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>‚úîÔ∏è</td>
    </tr>
    <tr>
      <td>Claude 3.7</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Gemini 2.5 Pro</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
    </tr>
    <tr>
      <td>DeepSeek R1</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
    </tr>
  </tbody>
</table>

<p>(DeepSeek‚Äôs chain of thought used ‚Äúwait‚Äù 76 times. It got everything right the first time, but when I tried it again, it somehow got <code>B</code>, <code>C</code>, and <code>D</code> all wrong, but <code>E</code> right.)</p>

<p>This is insane. Using basic features should not require solving crazy logic puzzles.</p>

<p>You might think, ‚ÄúOK, I‚Äôll just limit myself to indexing in simple ways.‚Äù Sounds good, except sometimes you <em>need</em> advanced indexing. And even if you‚Äôre doing something simple, you still need to be careful to avoid the crazy cases.</p>

<p>This again makes everything non-legible. Even if you‚Äôre just reading code that uses indexing in a simple way, how do you <em>know</em> it‚Äôs simple? If you see <code>A[B,C]</code>, that could be doing almost anything. To understand it, you need to remember the shapes of <code>A</code>, <code>B</code>, and <code>C</code> and work through all the cases. And, of course, <code>A</code>, <code>B</code>, and <code>C</code> are often produced by <em>other</em> code, which you <em>also</em> need to think about‚Ä¶</p>

<h2 id="i-dont-like-numpy-functions">I don‚Äôt like NumPy functions</h2>

<p>Why did NumPy end up with a <code>np.linalg.solve(A,B)</code> function that‚Äôs so confusing? I imagine they first made it work when <code>A</code> is a 2D array and and <code>b</code> is a 1D or 2D array, just like the mathematical notation of <code>A‚Åª¬πb</code> or <code>A‚Åª¬πB</code>.</p>

<p>So far so good. But then someone probably came along with a 3D array.  If you could use loops, the solution would be ‚Äúuse the old function with loops‚Äù. But you can‚Äôt use loops. So there were basically three options:</p>

<ol>
  <li>They could add some extra <code>axes</code> argument, so the user can specify which dimensions to operate over. Maybe you could write <code>solve(A,B,axes=[[1,2],1])</code>.</li>
  <li>They could create different  functions with different names for different situations. Maybe <code>solve_matrix_vector</code> would do one thing, <code>solve_tensor_matrix</code> would do another.</li>
  <li>They could add a Convention: Some arbitrary choice for how <code>solve</code> will internally try to line up the dimensions. Then it‚Äôs the user‚Äôs problem to figure out and conform to those Conventions.</li>
</ol>

<p>All these options are bad, because none of them can really cope with the fact that there are a combinatorial number of different cases. NumPy chose: All of them. Some functions have <code>axes</code> arguments. Some have different versions with different names. Some have Conventions. Some have Conventions <em>and</em> <code>axes</code> arguments. And some don‚Äôt provide any vectorized version at all.</p>

<p>But the <em>biggest</em> flaw of NumPy is this: Say you create a function that solves some problem with arrays of some given shape. Now, how do you apply it to particular dimensions of some larger arrays? The answer is: You re-write your function from scratch in a much more complex way. The basic principle of programming is abstraction‚Äîsolving simple problems and then using the solutions as building blocks for more complex problems. NumPy doesn‚Äôt let you do that.</p>

<h2 id="attention-please">Attention please</h2>

<p>One last example to show you what I‚Äôm talking about. Whenever I whine about NumPy, people always want to see an example with <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">self-attention</a>, the core trick behind modern language models. So fine. Here‚Äôs an implementation, which I humbly suggest is better than all 227 versions I found when I searched for ‚Äúself-attention numpy‚Äù:</p>

<div><pre><code><span># self attention by your friend dynomight
</span>
<span>input_dim</span> <span>=</span> <span>4</span>  
<span>seq_len</span> <span>=</span> <span>4</span>  
<span>d_k</span> <span>=</span> <span>5</span>  
<span>d_v</span> <span>=</span> <span>input_dim</span>  
  
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
<span>W_q</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_k</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_v</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_v</span><span>)</span>  
  
<span>def</span> <span>softmax</span><span>(</span><span>x</span><span>,</span> <span>axis</span><span>):</span>  
    <span>e_x</span> <span>=</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>x</span> <span>-</span> <span>np</span><span>.</span><span>max</span><span>(</span><span>x</span><span>,</span> <span>axis</span><span>=</span><span>axis</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>))</span>  
    <span>return</span> <span>e_x</span> <span>/</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>e_x</span><span>,</span> <span>axis</span><span>=</span><span>axis</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>)</span>  
  
<span>def</span> <span>attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>):</span>  
    <span>d_k</span> <span>=</span> <span>W_k</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span>  
    <span>Q</span> <span>=</span> <span>X</span> <span>@</span> <span>W_q</span>  
    <span>K</span> <span>=</span> <span>X</span> <span>@</span> <span>W_k</span>  
    <span>V</span> <span>=</span> <span>X</span> <span>@</span> <span>W_v</span>  
    <span>scores</span> <span>=</span> <span>Q</span> <span>@</span> <span>K</span><span>.</span><span>T</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>d_k</span><span>)</span>  
    <span>attention_weights</span> <span>=</span> <span>softmax</span><span>(</span><span>scores</span><span>,</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>  
    <span>return</span> <span>attention_weights</span> <span>@</span> <span>V</span>  
</code></pre></div>

<p>This is fine. Some of the <code>axis</code> stuff is a little obscure, but whatever.</p>

<p>But what language models really need is <em>multi-head</em> attention, where you sort of do attention several times in parallel and then merge the results. How do we do that?</p>

<p>First, let‚Äôs imagine we lived in a sane world where we were allowed to use abstractions. Then you could just call the previous function in a loop:</p>

<div><pre><code><span># multi-head self attention by your friend dynomight
# if only we could use loops
</span>
<span>n_head</span> <span>=</span> <span>2</span>
  
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
<span>W_q</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_k</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_v</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_v</span><span>)</span>  
<span>W_o</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>d_v</span><span>,</span> <span>input_dim</span> <span>//</span> <span>n_head</span><span>)</span>

<span>def</span> <span>multi_head_attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>,</span> <span>W_o</span><span>):</span>  
    <span>projected</span> <span>=</span> <span>[]</span>  
    <span>for</span> <span>n</span> <span>in</span> <span>range</span><span>(</span><span>n_head</span><span>):</span>  
        <span>output</span> <span>=</span> <span>attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>[</span><span>n</span><span>,:,:],</span> <span>W_k</span><span>[</span><span>n</span><span>,:,:],</span> <span>W_v</span><span>[</span><span>n</span><span>,:,:])</span>  
        <span>my_proj</span> <span>=</span> <span>output</span> <span>@</span> <span>W_o</span><span>[</span><span>n</span><span>,:,:]</span>  
        <span>projected</span><span>.</span><span>append</span><span>(</span><span>my_proj</span><span>)</span>  
    <span>projected</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>projected</span><span>)</span>  
  
    <span>output</span> <span>=</span> <span>[]</span>  
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>seq_len</span><span>):</span>  
        <span>my_output</span> <span>=</span> <span>np</span><span>.</span><span>ravel</span><span>(</span><span>projected</span><span>[:,</span><span>i</span><span>,:])</span>  
        <span>output</span><span>.</span><span>append</span><span>(</span><span>my_output</span><span>)</span>  
    <span>return</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>final</span><span>)</span>  
</code></pre></div>

<p>Looks stupid, right? Yes‚Äîthank you! Cleverness is bad.</p>

<p>But we don‚Äôt live in a sane world. So instead you need to do this:</p>

<div><pre><code><span># multi-head self attention by your friend dynomight
# all vectorized and bewildering
</span>
<span>def</span> <span>multi_head_attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>,</span> <span>W_o</span><span>):</span>  
    <span>d_k</span> <span>=</span> <span>W_k</span><span>.</span><span>shape</span><span>[</span><span>-</span><span>1</span><span>]</span>  
    <span>Q</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hij-&gt;hsj'</span><span>,</span> <span>X</span><span>,</span> <span>W_q</span><span>)</span>  
    <span>K</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hik-&gt;hsk'</span><span>,</span> <span>X</span><span>,</span> <span>W_k</span><span>)</span>  
    <span>V</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hiv-&gt;hsv'</span><span>,</span> <span>X</span><span>,</span> <span>W_v</span><span>)</span>  
    <span>scores</span> <span>=</span> <span>Q</span> <span>@</span> <span>K</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>d_k</span><span>)</span>  
    <span>weights</span> <span>=</span> <span>softmax</span><span>(</span><span>scores</span><span>,</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>
    <span>output</span> <span>=</span> <span>weights</span> <span>@</span> <span>V</span>  
    <span>projected</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'hsv,hvd-&gt;hsd'</span><span>,</span> <span>output</span><span>,</span> <span>W_o</span><span>)</span>  
    <span>return</span> <span>projected</span><span>.</span><span>transpose</span><span>(</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>2</span><span>).</span><span>reshape</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
</code></pre></div>

<p>Ha! Hahahahahaha!</p>

<h2 id="so-what-then">So what then?</h2>

<p>To be clear, I‚Äôm only suggesting that NumPy is ‚Äúthe worst array language other than all the other array languages‚Äù. What‚Äôs the point of complaining if I don‚Äôt have something better to suggest?</p>

<p>Well, actually I do have something better to suggest. I‚Äôve made a prototype of a ‚Äúbetter‚Äù NumPy that I think retains all the power while eliminating all the sharp edges. I thought this would just be a short motivational introduction, but after I started writing, the evil took hold of me and here we are 3000 words later.</p>

<p>Also, it‚Äôs probably wise to keep some distance between one‚Äôs raving polemics and one‚Äôs constructive array language API proposals. So I‚Äôll cover my new thing next time.</p>

  </section>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coinbase says hackers bribed staff to steal customer data, demanding $20M ransom (348 pts)]]></title>
            <link>https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html</link>
            <guid>43996307</guid>
            <pubDate>Thu, 15 May 2025 15:52:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html">https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html</a>, See on <a href="https://news.ycombinator.com/item?id=43996307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107132326" data-test="InlineImage"><p>Jakub Porzycki | Nurphoto | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/COIN/">Coinbase</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Thursday reported that cybercriminals bribed overseas support agents to steal customer data to use in social engineering attacks. The incident may cost Coinbase up to $400 million to fix, the company estimated.</p><p>The crypto exchange operator received an email on May 11 from someone claiming they obtained information about certain Coinbase customer accounts as well as other internal Coinbase documentation, including materials relating to customer service and account management systems, Coinbase reported in a <a href="https://www.sec.gov/ix?doc=/Archives/edgar/data/0001679788/000167978825000094/coin-20250514.htm" target="_blank">Securities and Exchange Commission filing</a>.</p><p>The company's shares were down more than 6% in morning trading.</p><p>The email demanded money in exchange for not publicly disclosing the information, but Coinbase says it has not paid the demand and is cooperating with law enforcement on the investigation of the incident.</p><p>Although passwords and private keys were not compromised, affected data included sensitive data such as names, addresses, phone numbers and emails; masked bank account numbers and identifiers as well as the last four digits of Social Security numbers; government ID images and account balances, the company said.</p><p>"Cyber criminals bribed and recruited a group of rogue overseas support agents to steal Coinbase customer data to facilitate social engineering attacks," the company said in a <a href="https://www.coinbase.com/blog/protecting-our-customers-standing-up-to-extortionists" target="_blank">blog post</a>. "These insiders abused their access to customer support systems to steal the account data for a small subset of customers. No passwords, private keys, or funds were exposed and Coinbase Prime accounts are untouched. We will reimburse customers who were tricked into sending funds to the attacker."</p><p>Coinbase had detected the breach independently in previous months, per the filing. It immediately terminated the employees involved, warned customers whose information may have been accessed and enhanced its fraud monitoring protections.</p><p>The threat actor paid overseas contractors and employees in support rolls to obtain the information, it said.</p><p>"We're cooperating closely with law enforcement to pursue the harshest penalties possible and will not pay the $20 million ransom demand we received," the company said in the blog. "Instead we are establishing a $20 million reward fund for information leading to the arrest and conviction of the criminals responsible for this attack."</p><p>Coinbase operates the largest crypto exchange in the U.S. In the past week it <a href="https://www.cnbc.com/2025/05/08/coinbase-acquires-crypto-derivatives-exchange-deribit-for-2point9-billion.html">announced an acquisition</a> that is expected to help it expand its global reach and gained <a href="https://www.cnbc.com/2025/05/13/coinbase-stock-enters-sp-500-a-watershed-moment-for-crypto-industry-analysts-say.html">entry to the benchmark S&amp;P 500</a> stock index, which will take effect next week. On the earnings call last week, CEO Brian Armstrong discussed his ambition to make Coinbase "the <a href="https://www.cnbc.com/2025/05/11/coinbase-aims-to-be-worlds-number-1-financial-service-app-in-10-years.html">No. 1 financial services app</a> in the world" in the next five to 10 years.</p></div></div></div>]]></description>
        </item>
    </channel>
</rss>