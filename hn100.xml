<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 19 Aug 2023 02:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[2009scape (122 pts)]]></title>
            <link>https://2009scape.org/</link>
            <guid>37183069</guid>
            <pubDate>Fri, 18 Aug 2023 22:40:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://2009scape.org/">https://2009scape.org/</a>, See on <a href="https://news.ycombinator.com/item?id=37183069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
          <div id="left">
            <!--MAIN BUTTON: LEFT (DISCORD)-->
            <p><a href="https://forum.2009scape.org/" target="_blank">
              <img src="https://2009scape.org/site/2009scape-resources/img/buttons/btn-forums2.webp" alt="Join us on the forums" width="268" height="151">
              <span></span>
            </a></p><!--WEBSITE FEATURES-->
            <div id="features">
                <div>
                  <p><a href="https://2009scape.org/site/game_guide/play.html"><img src="https://2009scape.org/site/img/2009img/main/home/feature_kbsearch_icon.webp" width="57" height="57" alt="Getting Started"></a></p><p>Getting Started</p>
                  
                </div>
                <div>
                  <p><a href="https://2009scape.org/services/m=hiscore/hiscores.html?world=2"><img src="https://2009scape.org/site/img/2009img/main/home/feature_poll_icon.webp" width="57" height="57" alt="Hiscores"></a></p><p>Community Hiscores</p>
                  <p>See who's on top &amp; check your skill levels! View the <a href="https://2009scape.org/services/m=hiscore/hiscores.html?world=2">Hiscores</a>.</p>
                </div>
                <div>
                  <p><a href="https://gitlab.com/2009scape/2009scape/-/issues"><img src="https://2009scape.org/site/2009scape-resources/img/icons/feature-bugreport.webp" width="57" height="57" alt="Report a Bug"></a></p><p>Report a Bug</p>
                  <p>Found a bug in game? Send us a <a href="https://gitlab.com/2009scape/2009scape/-/issues" target="_blank">Bug report</a>!</p>
                </div>
                <div>
                  <p><a href="https://2009scape.org/site/classicapplet/playclassic.html"><img src="https://2009scape.org/site/2009scape-resources/img/icons/feature-openrsc.webp" width="57" height="57" alt="Open RuneScape Classic"></a></p><p>OpenRSC</p>
                  <p>Experience RuneScape Classic in its original glory: <a href="https://2009scape.org/site/classicapplet/playclassic.html">OpenRSC</a>!</p>
                </div>
                <div>
                  <p><a target="_blank" href="https://github.com/2009scape/2009Scape-mobile#readme"><img src="https://2009scape.org/site/2009scape-resources/img/icons/aog_icon.jpeg" width="57" height="57" alt="AOG"></a></p><p>2009Scape Mobile</p>
                  <p>Enjoy our project on the go with the Android <a target="_blank" href="https://github.com/2009scape/2009Scape-mobile#readme">
                    mobile client!</a>
                  </p>
                </div>
              </div>
            <!--GAME GUIDE-->
            <div id="articles">
                  <p><img alt="Featured Article" src="https://2009scape.org/site/2009scape-resources/img/titles/title-article-featured.webp" width="164" height="9">
                  </p>
                  <div>
                      <p><a href="https://discord.gg/YY7WSttN7H"><img alt="" src="https://2009scape.org/site/img/2009img/main/kbase/aow_icons/discord-promo.webp" width="249" height="87"></a></p>
                      <p>Join us on Discord</p>
                      <p> 2009Scape has an active community on our Discord Server. Discuss the game, ask questions, or just hang out! 
                        <a href="https://discord.gg/YY7WSttN7H">Join Now!</a>
                      </p>
                    </div>
                  
                </div>
          </div>
          <div id="right">
            <!--MAIN BUTTON: PLAY/CLIENT DOWNLOAD-->
            <p><a href="https://2009scape.org/site/game_guide/play.html" id="playbutton">
              <img src="https://2009scape.org/site/2009scape-resources/img/buttons/btn-play.webp" alt="Play 2009Scape" width="480" height="151">
              <span></span>
            </a></p><div id="recentnews">
                
                
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                
                  
                    
                  
                
                  
                
                  
                    
                      
                        
                          <!--LATEST UPDATE ITEM-->
                          <div>
                                <!--UPDATE TITLE & DATE-->
                                <div>
                                  <h3>Clock Tower Quest</h3>
                                  <p><span>12-August-2023</span>
                                </p></div>
                                <!--UPDATE IMAGE-->
                                
                                <p><a href="https://2009scape.org/services/m=news/archives/2023-08-12.html"><img alt="Latest update image" src="https://2009scape.org/site/2009scape-resources/img/updates/update-monthly-08.webp" width="215" height="137"></a></p><!--UPDATE CONTENT TEASER-->
                              <p>  Clock Tower quest, combat sounds and deep wilderness balancing...
                                <!--READ MORE LINK-->
                                <br><a href="https://2009scape.org/services/m=news/archives/2023-08-12.html">Read more...</a>
                              </p>
                            </div>
                        <!--END LATEST UPDATE ITEM-->
                      
                    
                  
                
                  
                    
                      
                        
                        <!--RECENT UPDATE ITEM-->
                        <div>
                            <!--UPDATE TITLE & DATE-->
                            <div>
                              <h3>Death Plateau, New Random Events and Deep Wilderness PvP</h3>
                              <p><span>06-August-2023</span>
                            </p></div>
                            <!--UPDATE ICON (Use most relevant to this post!)
                            --
                            GENERIC // generic1.webp, generic2.webp, generic3.webp
                            SETTINGS & TWEAKS // settings1.webp, settings2.webp, settings3.webp
                            OTHER // account.webp, award.webp, bugfix.webp, combat.webp, hiscores.webp, money,webp, random.webp, website.webp -->
                            <p><img src="https://2009scape.org/site/2009scape-resources/img/updates/icons/generic1.webp" width="65" height="70" alt="Generic feature icon"></p><!--UPDATE CONTENT TEASER-->
                              <p>A new quest, new random events, deep wilderness PvP zone, champion challenge and a HUGE list of other changes...
                                <!--READ MORE LINK-->
                                <br><a href="https://2009scape.org/services/m=news/archives/2023-08-06.html">Read more..</a>
                              </p>
                            </div>
                        
                    
                  
                
                  
                    
                      
                        
                        <!--RECENT UPDATE ITEM-->
                        <div>
                            <!--UPDATE TITLE & DATE-->
                            <div>
                              <h3>Small Changes in Gielinor</h3>
                              <p><span>06-June-2023</span>
                            </p></div>
                            <!--UPDATE ICON (Use most relevant to this post!)
                            --
                            GENERIC // generic1.webp, generic2.webp, generic3.webp
                            SETTINGS & TWEAKS // settings1.webp, settings2.webp, settings3.webp
                            OTHER // account.webp, award.webp, bugfix.webp, combat.webp, hiscores.webp, money,webp, random.webp, website.webp -->
                            <p><img src="https://2009scape.org/site/2009scape-resources/img/updates/icons/generic1.webp" width="65" height="70" alt="Generic feature icon"></p><!--UPDATE CONTENT TEASER-->
                              <p>A small hotfix update.
                                <!--READ MORE LINK-->
                                <br><a href="https://2009scape.org/services/m=news/archives/2023-06-06.html">Read more..</a>
                              </p>
                            </div>
                        
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
                  
                    
                  
                
              </div>
          </div>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dirty downside of 'return to office'; ending WFH could make climate crisis worse (163 pts)]]></title>
            <link>https://www.businessinsider.com/return-to-office-remote-work-from-home-commute-companies-climate-2023-8</link>
            <guid>37180392</guid>
            <pubDate>Fri, 18 Aug 2023 18:53:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/return-to-office-remote-work-from-home-commute-companies-climate-2023-8">https://www.businessinsider.com/return-to-office-remote-work-from-home-commute-companies-climate-2023-8</a>, See on <a href="https://news.ycombinator.com/item?id=37180392">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <p>Rachel really hates sitting in traffic. Knowing she's about to spend up to an hour and a half in her car that day staring at taillights in front of her makes it all the more difficult to wake up in the morning.</p><p>She had a remote position over the past few years, but landed a new job in February at a nonprofit, which came with a catch: The job required her to make the hike to its Silver Spring, Maryland, office at least two days a week — forcing her to commute 30 to 45 minutes each way.&nbsp;</p><p>Rachel, who was granted a pseudonym to speak freely about her employer's policy, is among millions of workers getting back into the swing of commuting thanks to return-to-office mandates. Like many of them, Rachel said she doesn't see the point. But it's not just the time suck that bothers her, it's the environmental impact of her new routine.</p><p>"I get kind of furious when I drive to work and see the road choked with traffic," Rachel told me. "It's incredibly harmful to the environment. And offices also generate so much waste, like paper and plastic cups and utensils."&nbsp;</p><p>Rachel's concerns about the environment have been largely ignored in <a href="https://www.businessinsider.com/companies-making-workers-employees-return-to-office-rto-wfh-hybrid-2023-1" data-analytics-product-module="body_link" rel="">the battle over return-to-office mandates</a> — at least publicly. CEOs at Amazon, Google, and JPMorgan Chase argue that in-person collaboration and random watercooler conversations keep people more engaged. Opponents of the shift have emphasized the ability of workers to be more productive when they set their own schedule. But grappling with the broader planetary effects of these decisions is a failure on the part of corporations, James Elfer, the founder of More Than Now, told me.&nbsp;</p><p>"It's shocking that this isn't part of the conversation, especially at companies that claim to care about sustainability," Elfer, whose firm conducts behavioral-science experiments to improve workplaces, said. "It's a missed opportunity to explore an employer's colossal influence on our behavior."&nbsp;</p><p>Whether mandatory return-to-office policies will make the climate crisis worse is an important question, especially as scientists predict that <a href="https://www.insider.com/summer-2023-heat-wave-tracker-temperature-records-2023-5" data-analytics-product-module="body_link" rel="">2023 will be the hottest year on record</a>. Transportation accounts for about 15% of greenhouse-gas emissions warming the planet, with gas-powered cars, trucks, and buses contributing an outsize amount. But determining whether working from an office is worse for the planet isn't that simple. There are myriad factors that could tip the scales the other way, such as where people live, the amount of energy people use at home, the food they eat, the things they buy, and the extra trips they take. We are in the midst of a major upheaval in how people work, and Fortune 500 companies <a href="https://www.prnewswire.com/news-releases/fortune-announces-2022-fortune-500-list-301552608.html#:~:text=The%20revenue%20threshold%20for%202022,employ%2029.7%20million%20people%20worldwide." target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">employ nearly 30 million people around the world</a>. Failing to find a balance between boosting productivity at the office and protecting the planet risks making the crisis worse.</p><p>"Collectively, these decisions mean millions of employees working in profoundly different ways," Elfer said. "The lack of attention feels irresponsible."</p><h2>Commuting cost</h2><p>In the US, the transportation sector belches out the most greenhouse gases of any industry, and <a href="https://www.epa.gov/greenvehicles/fast-facts-transportation-greenhouse-gas-emissions" target="_blank" rel="noopener" data-analytics-product-module="body_link">passenger vehicles are largely responsible</a>, generating the equivalent of 374 million metric tons of carbon dioxide in 2021. While all that driving isn't just from commuting, about three-fourths of the 154 million Americans who do travel to work go by car. On average, they spend nearly an hour on the road each day, according to US Census data from 2021. The <a href="https://www.gov.uk/government/statistics/transport-statistics-great-britain-2022/transport-statistics-great-britain-2022-domestic-travel" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">trends are similar in the UK</a>, where cars and taxis account for half of the transportation sector's emissions, and some 68% percent of people who commute to work did so by car for an average travel time of one hour a day.&nbsp;</p><p>So it stands to reason that cutting out commutes would help keep the air cleaner. Ty Colman, a cofounder and the chief revenue officer at Optera, a carbon-accounting firm that helps organizations quantify their emissions, said that in general, a fully remote company with no offices has the lowest impact-per-employee per year, at less than 1 metric ton of carbon-dioxide equivalent. That includes the uptick in energy used to power computers, keep the lights on, and maintain a comfortable temperature at home. Employees with a hybrid-work policy that allows them to be remote three days a week emit about 1.4 metric tons per year, which increases to 1.7 metric tons under a fully in-office policy. The firm's method is based on the Greenhouse Gas Protocol, the most widely used standard among organizations, cities, and countries to track their emissions. The company adjusted a wide array of variables to see how different behaviors affected the amount of emissions per employee, Colman said, and fully in-office always produced the highest emissions unless a large majority of people made their commute by public transit — then a hybrid schedule could actually have a greater environmental impact.&nbsp;</p><p>The <a href="https://www.iea.org/commentaries/working-from-home-can-save-energy-and-reduce-emissions-but-how-much" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">International Energy Agency in 2020 found</a> that for people who commute more than 4 miles per day by car, their emissions would decrease if they worked from home. Overall, if people who can telework did so just one day a week, it would cut 24 million metric tons of emissions a year — equivalent to what the Greater London area produces each year. The IEA said that represented a "notable decline" but was small in the context of the amount emissions must drop to meet global climate goals.&nbsp;</p><blockquote><q>"I get kind of furious when I drive to work and see the road choked with traffic. It's incredibly harmful to the environment."</q></blockquote><p>The onset of the pandemic in 2020 provided a real-world opportunity to research what happens when millions of people stop commuting and work from home. Ralf Martin, an associate professor at Imperial College Business School in London, did just that. His team collected smart-meter data from a representative sample of 1,164 UK households and surveyed 452 of them about how their daily patterns changed during July and August 2020. Most people reported a commute of fewer than 20 miles and traveled by car. On average, they worked from home three days a week and commuted two, as some were essential workers and had to go in. Even as people sat at home charging laptops and taking video calls, overall household emissions dropped by 33% — electricity use went up by 6%, while gas consumption decreased by 9.5%. That rise in electricity didn't generate as big a spike in emissions as expected, Martin said, for two reasons: First, demand was more spread out during the day. In the pre-COVID world, household emissions were clustered in the early morning and the evenings, forcing more inefficient oil and gas plants to come online to meet the spikes in demand.</p><p>"During these morning and evening peaks is when energy generation is the dirtiest," Martin said. "The high demand leads to some of the more marginal, dirtier power plants getting switched on so the carbon intensity of the power grid goes up."</p><p>The second factor was a bit more odd: Since people weren't going into the office and spending time around colleagues, they probably care less about hygiene. "It's also possible people didn't shower so much," Martin added. "That's less energy for heating water."&nbsp;</p><p>Even though the study only covered the early part of the pandemic, it illustrated how a shift away from office work can have some positive effects for the planet. But since we are no longer in lockdown, global emissions have rebounded to higher than pre-pandemic levels because people <a href="https://www.businessinsider.com/delta-airlines-expects-memorial-day-travel-beat-pre-pandemic-levels-2023-5" data-analytics-product-module="body_link" rel="">are driving and flying again</a>, the IEA said. The added trouble is that many people used the pandemic as a reason to <a href="https://www.businessinsider.com/millennials-gen-z-leaving-cities-for-suburbs-amid-pandemic-2020-11" data-analytics-product-module="body_link" rel="">move to the suburbs</a>. That potentially means longer and more expensive commutes as employers demand people be in the office again — making a strong case for more flexibility to work from home.</p><p>"The bottom line here is that the immediate effect was this big savings in emissions," Martin said. "But what seems to be emerging is a danger that transport emissions are actually increasing as people move further away from their place of work and engage in hybrid working."</p><h2>Keeping the lights on</h2><p>Despite the intuitive nature of the argument, most experts said that sending everyone back to home offices is not a silver bullet for combating the climate crisis. If a person's commute was shorter than 4 miles or they relied on public transportation, the IEA report found, teleworking could actually increase their emissions because of the extra energy used at home. Other behaviors make a difference in this calculation, as well: What time of year is it? During the winter, energy use tends to be higher as people turn on the heat, though in the US it's actually higher in the summer because air conditioning is so common. Are people wasting more food or running their electronic devices unnecessarily? Running more errands? Taking more weekend trips?</p><p>In 2021, More Than Now outlined a road map for how companies could <a href="https://hbr.org/2022/03/is-remote-work-actually-better-for-the-environment" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">determine the optimal mix of commuting and WFH</a> for their employees. The firm's initial research found that employees' net-sustainability impact depended on travel, the energy and digital devices they use, waste management, and local infrastructure.</p><p>"Each was dramatically influenced by remote work and had pros and cons when it comes to our environmental footprint," Elfer said. "There was no clear answer to whether work from home was better or worse for the environment in general terms."</p><blockquote><q>It's shocking that this isn't part of the conversation, especially at companies that claim to care about sustainability</q></blockquote><p>Tailoring people's behavior to ensure that working from home actually cuts emissions can be hard, especially without robust data from employees on how they spend their time. Even without this information, some institutions are encouraging people to be more sustainable at home, including American University in Washington, DC. Megan Litke, the director of sustainability at American, said her office conducts a survey each year of faculty and staff to track their commuting habits. However, they haven't found a way to accurately measure what people are doing at home.</p><p>"We've taken the approach that we don't need a firm number to start taking action on it," Litke said. "We provide people with a list of actions they can do, broken out by morning routine, lunch break, and afternoon stretch."</p><p>A <a href="https://www.american.edu/about/sustainability/get-involved/green-home-guide.cfm" target="_blank" rel="noopener" data-analytics-product-module="body_link">Green Home Guide suggests</a> people avoid plastic K-Cups for coffee and eat meat- and dairy-free meals to lower their carbon footprint, and it offers tips for recycling, composting, and buying green cleaning products. Energy-saving techniques are also included, such as putting desks near a window to take advantage of daylight and setting electronic devices to go into sleep mode after a certain window of time.</p><p>"It's important to recognize that working from home isn't a perfect environmental solution," Litke said. "We have to think about it holistically."</p><h2>Head in the sand</h2><p>While scientists and experts are trying to find the perfect mix of work-from-home and in-office time to help the planet, CEOs seem less concerned. Many companies — even ones that say <a href="https://www.businessinsider.com/climate-crisis-inequality-racial-justice-energy-technology-innovation-2022-11" data-analytics-product-module="body_link" rel="">they are dedicated to helping the planet</a> — don't seem interested in trying to figure out the answer, Efler said. More Than Now published its road map in 2021 in hopes that companies would want to partner to study the trade-offs, but in the two years since his organization started looking into the issue, it's been mostly crickets from the corporate world.</p><p>Operta's Colman said that companies are quietly considering "what emissions will be relative to various work policies." It may not be center stage in the debate because employees' commutes can represent a small portion of a company's overall emissions, Colman said, but Optera, which has clients including Target, Dell, and Williams-Sonoma, has run the numbers for dozens of companies.&nbsp;</p><p>Given the importance of the question at hand and the volume of the environmental promises made by many large corporations, the relative hush when it comes to commutes is perplexing.</p><p>Insider asked Amazon, Apple, Google, and JPMorgan Chase — all of which have ambitious climate goals — whether they considered the potential environmental trade-offs in crafting their return-to-office mandates. All declined to comment or didn't respond<strong>.</strong> In their latest sustainability reports, Amazon, Apple, and Google account for employee commutes in their overall carbon footprint. JPMorgan does not. Google <a href="https://www.gstatic.com/gumdrop/sustainability/google-2023-environmental-report.pdf" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">said commuting and teleworking contributes</a> 2% of the total emissions created by the company. Amazon <a href="https://affiliate.insider.com/?h=7648d69efeb3a0b75f2d9a47cc6073534dd497afbfcd48f8d275d6e4bd385f6f&amp;postID=64d2530e29ea7163e0de2990&amp;site=bi&amp;u=https%3A%2F%2Fsustainability.aboutamazon.com%2Fcarbon-methodology.pdf&amp;amazonTrackingID=null&amp;platform=browser&amp;sc=false&amp;disabled=false" target="_blank" rel="noopener" data-analytics-product-module="body_link">said it counts emissions</a> from company-provided shuttles and certain employer-subsidized transit but doesn't break out a specific total. Apple <a href="https://affiliate.insider.com/?h=de5fff59f18e5d767aaeb71155e925f76f0af4afa82283a29fec5087968a5e89&amp;postID=64d2530e29ea7163e0de2990&amp;site=bi&amp;u=https%3A%2F%2Fwww.apple.com%2Fenvironment%2Fpdf%2FApple_Environmental_Progress_Report_2022.pdf&amp;amazonTrackingID=null&amp;platform=browser&amp;sc=false&amp;disabled=false" target="_blank" rel="noopener" data-analytics-product-module="body_link">said employee commute and business travel</a> accounts for 0.5% of its emissions, which it offsets by purchasing carbon credits from projects that protect and restore nature. While these numbers may seem small, they do add up, and optimizing how employees commute could be an easy win for companies that ostensibly care about the future of the planet.&nbsp;</p><blockquote><q>It's important to recognize that working from home isn't a perfect environmental solution. We have to think about it holistically.</q></blockquote><p>Schneider Electric, which makes hardware and software to manage energy in buildings, is focused on making its own offices as sustainable as possible and offering employees flexibility to work from home. Tony Johnson, the director of hub sites and workplace strategy, said these efforts began around 2015. Since then, Schneider Electric has shrunk its offices from 300 to about 200 in North America.&nbsp;</p><p>"We recognized that not everybody wants or needs to be in the office every day of the week," he said. "At the same time, it's hard to build relationships that drive innovation virtually."&nbsp;</p><p>Schneider asks its 35,000 employees in North America to be in the office at least two days a week and is pursuing electric vehicles for those who regularly drive to meet with customers. The company's <a href="https://www.se.com/ww/en/assets/564/document/396659/2022-sustainability-report.pdf" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">2022 sustainability report shows</a> that emissions from its own offices and manufacturing sites around the world are decreasing, while emissions from employee commuting increased. Both categories account for a small portion of Schneider Electric's overall carbon footprint.&nbsp;&nbsp;&nbsp;&nbsp;</p><p>"This is a complicated problem, and we're learning as we go," Johnson said. "Several years ago, no one was talking about how their vendors were impacting the planet. Now, we're trying to make sure our furniture supplier recycles or refurbishes."&nbsp;</p><p>For Rachel, finding another job that is fully remote would be ideal. She already negotiated with her current employer to be in the office two days a week — as opposed to five — and doesn't think they will budge any further. That's left Rachel thinking about the ripple effects of commuting to work — the environmental impact, the weight on her mental health, even the wear and tear on her cars and the roads. For all these reasons, she's determined that going into the office simply isn't worth it.&nbsp;</p><p>"I think we go in because our CEO likes to have people to talk to," she said. "I don't think our presence is generating amazing ideas or networking. My boss just likes a more traditional work environment."</p><hr><p><em><a href="https://www.businessinsider.com/author/catherine-boudreau" data-analytics-product-module="body_link" rel="">Catherine Boudreau</a> is senior sustainability reporter at Insider.&nbsp;</em></p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dude,Where's My Donations? Wikimedia gives another $1M to non-Wikimedia projects (110 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-15/News_and_notes</link>
            <guid>37179587</guid>
            <pubDate>Fri, 18 Aug 2023 17:59:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-15/News_and_notes">https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-08-15/News_and_notes</a>, See on <a href="https://news.ycombinator.com/item?id=37179587">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
<h3><span id="Wikimedia_Foundation_gives_away_about_.241_million_in_grants_to_counter_racial_bias_and_discrimination"></span><span id="Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bias_and_discrimination" data-mw-thread-id="h-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia-signpost-article-title"><span data-mw-comment-start="" id="h-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia-signpost-article-title"></span>Wikimedia Foundation gives away about $1 million in grants to counter racial bias and discrimination<span data-mw-comment-end="h-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia-signpost-article-title"></span></span></h3>
</p><div>
<p>In 2021, the Wikimedia Foundation <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2021-09-26/News_from_Diff" title="Wikipedia:Wikipedia Signpost/2021-09-26/News from Diff">announced</a> the first grants of a "<a href="https://meta.wikimedia.org/wiki/Knowledge_Equity_Fund" title="m:Knowledge Equity Fund">Knowledge Equity Fund</a>" created in June of the previous year. This involved about a million dollars of WMF funds being given, in the form of grants, to a number of external charitable and advocacy organizations.
</p><p>This proved controversial; as we <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2022-10-31/News_and_notes" title="Wikipedia:Wikipedia Signpost/2022-10-31/News and notes">covered last October</a>, a "somewhat-viral Twitter thread" questioned the relevance of these organizations to Wikimedia projects and values, and while some of the controversy was certainly political in nature, many of the grant recipients seemed unrelated to Wikimedia projects, prompting further discussion on mailing lists. One concern was the lack of community input into the process that led to the fund's creation. Another was the use of money which was generally solicited on the grounds of being necessary to fund Wikimedia projects, meaning that many donors likely did not know or intend for their funds to be given to unrelated organizations. <a href="https://meta.wikimedia.org/wiki/Knowledge_Equity_Fund#Round_1_-_Narrative_and_Financial_reports" title="meta:Knowledge Equity Fund">Two of the grant recipients</a> from the first round seem to have not shared financial reports detailing how the money was spent.
</p><p>The Wikimedia Foundation has <a href="https://diff.wikimedia.org/2023/08/03/announcing-the-second-round-of-grantees-from-the-wikimedia-foundation-knowledge-equity-fund/">announced</a> a second round of grantees this month, saying in its announcement:
</p>
<blockquote><p>Equity – more specifically, knowledge equity – underpins our movement's vision of a world in which every human can share in the sum of all knowledge. It encourages us to consider the knowledge and communities that have been left out of the historical record, both intentionally and unintentionally. This is an important pillar of the Wikimedia movement’s strategic direction, our forward-looking approach to prepare for the Wikimedia of 2030. 
</p><p>There can be many reasons behind these gaps in knowledge, derived from systemic social, political and technical challenges that prevent all people from being able to access and contribute to free knowledge projects like Wikimedia equally. In 2021, the Wikimedia Foundation launched the Knowledge Equity Fund specifically to address gaps in the Wikimedia movement's vision of free knowledge caused by racial bias and discrimination, that have prevented populations around the world from participating equally. The fund is a part of the Wikimedia Foundation’s Annual Plan for the 2023-24 fiscal year to support knowledge equity by supporting regional and thematic strategies, and helping close knowledge gaps. Building on learnings from its first round of grants, today the Equity Fund is welcoming its second round of grantees.
</p><p>This second round includes seven grantees that span four regions, including the Fund's first-ever grantees in Asia. This diverse group of grantees was chosen from an initial pool of 42 nominations, which were received from across the Wikimedia movement through an open survey in 2022 and 2023. Each grantee aligns with one of Fund's five focus areas, identified to address persistent structural barriers that prevent equitable access and participation in open knowledge. They are also recognized nonprofits with a proven track record of impact in their region. The Knowledge Equity Fund was initially conceived in response to global demands for racial equity, and the global reach of these new grantees is testament to and in recognition of the systemic impact of racial inequity in affecting participation in knowledge across the world.
</p>
</blockquote>
<p>The grants announced are as follows:
</p>
<h4><span id=".24290.2C000_USD_to_Black_Cultural_Archives.2C_United_Kingdom"></span><span id="$290,000_USD_to_Black_Cultural_Archives,_United_Kingdom" data-mw-thread-id="h-$290,000_USD_to_Black_Cultural_Archives,_United_Kingdom-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$290,000_USD_to_Black_Cultural_Archives,_United_Kingdom-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$290,000 USD to Black Cultural Archives, United Kingdom<span data-mw-comment-end="h-$290,000_USD_to_Black_Cultural_Archives,_United_Kingdom-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Black Cultural Archives is a Black-led archive and heritage center that preserves and gives access to the histories of African and Caribbean people in the UK. Their goals with this grant for the coming year include increasing research into their collections, as well as increasing the breadth of their collections for research. 
</p>
<h4><span id=".24200.2C000_USD_to_Aliansi_Masyarakat_Adat_Nusantara.2C_Indonesia"></span><span id="$200,000_USD_to_Aliansi_Masyarakat_Adat_Nusantara,_Indonesia" data-mw-thread-id="h-$200,000_USD_to_Aliansi_Masyarakat_Adat_Nusantara,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$200,000_USD_to_Aliansi_Masyarakat_Adat_Nusantara,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$200,000 USD to Aliansi Masyarakat Adat Nusantara, Indonesia<span data-mw-comment-end="h-$200,000_USD_to_Aliansi_Masyarakat_Adat_Nusantara,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>The Aliansi Masyarakat Adat Nusantara, or the Alliance of the Indigenous Peoples of the Archipelago (AMAN for short), is a non-profit organization based in Indonesia that works on human rights and advocacy issues for indigenous people. 
</p>
<h4><span id=".24160.2C000_USD_to_Criola.2C_Brazil"></span><span id="$160,000_USD_to_Criola,_Brazil" data-mw-thread-id="h-$160,000_USD_to_Criola,_Brazil-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$160,000_USD_to_Criola,_Brazil-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$160,000 USD to Criola, Brazil<span data-mw-comment-end="h-$160,000_USD_to_Criola,_Brazil-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Criola is a civil society organization, based in Rio de Janeiro, dedicated to advocating for the rights of Black women in Brazilian society. They prioritize knowledge production, research, and skills development as part of their work. They are also part of a national and international network of human rights, justice and advocacy organization focused on promoting racial equity.
</p>
<h4><span id=".24100.2C000_USD_to_Data_for_Black_Lives.2C_United_States"></span><span id="$100,000_USD_to_Data_for_Black_Lives,_United_States" data-mw-thread-id="h-$100,000_USD_to_Data_for_Black_Lives,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$100,000_USD_to_Data_for_Black_Lives,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$100,000 USD to Data for Black Lives, United States<span data-mw-comment-end="h-$100,000_USD_to_Data_for_Black_Lives,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Data for Black Lives is a movement of activists, organizers, and scientists committed to the mission of using data to create concrete and measurable change in the lives of Black people. They will use the grant in part to launch a Movement Scientists Fellowship matching racial justice leaders with machine learning research engineers to develop data-based machine learning applications to drive change in the areas of climate, genetics, and economic justice.
</p>
<h4><span id=".2475.2C000_USD_to_Create_Caribbean_Research_Institute.2C_Commonwealth_of_Dominica"></span><span id="$75,000_USD_to_Create_Caribbean_Research_Institute,_Commonwealth_of_Dominica" data-mw-thread-id="h-$75,000_USD_to_Create_Caribbean_Research_Institute,_Commonwealth_of_Dominica-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$75,000_USD_to_Create_Caribbean_Research_Institute,_Commonwealth_of_Dominica-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$75,000 USD to Create Caribbean Research Institute, Commonwealth of Dominica<span data-mw-comment-end="h-$75,000_USD_to_Create_Caribbean_Research_Institute,_Commonwealth_of_Dominica-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Create Caribbean Research Institute is the first digital humanities center in the Caribbean. The grant will be used to expand Create Caribbean’s Create and Code technology education program to enable children ages 5-16 to develop information and digital literacy as well as coding skills.
</p>
<h4><span id=".2470.2C000_USD_to_Filipino_American_National_Historical_Society.2C_United_States"></span><span id="$70,000_USD_to_Filipino_American_National_Historical_Society,_United_States" data-mw-thread-id="h-$70,000_USD_to_Filipino_American_National_Historical_Society,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$70,000_USD_to_Filipino_American_National_Historical_Society,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$70,000 USD to Filipino American National Historical Society, United States<span data-mw-comment-end="h-$70,000_USD_to_Filipino_American_National_Historical_Society,_United_States-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>The Filipino American National Historical Society, or FANHS, has a mission to gather, document and share Filipino American history through its 42 community based chapters. The grant will support continuing and growing FANHS’ scholarship and advocacy on accurate historical representations of Filipino Americans and counter distorted and effaced ethnic history.
</p>
<h4><span id=".2450.2C000_USD_to_Project_Multatuli.2C_Indonesia"></span><span id="$50,000_USD_to_Project_Multatuli,_Indonesia" data-mw-thread-id="h-$50,000_USD_to_Project_Multatuli,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"><span data-mw-comment-start="" id="h-$50,000_USD_to_Project_Multatuli,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span>$50,000 USD to Project Multatuli, Indonesia<span data-mw-comment-end="h-$50,000_USD_to_Project_Multatuli,_Indonesia-Wikimedia_Foundation_gives_away_about_$1_million_in_grants_to_counter_racial_bia"></span></span></h4>
<p>Project Multatuli is an organization dedicated to non-profit journalism, especially for underreported topics, ranging from indigenous people to marginalized issues. Their goal is to produce data-based, deeply researched news stories to promote inclusive journalism and amplify the voices of marginalized communities. 
</p><p>For further background on the grantees, see the <a href="https://diff.wikimedia.org/2023/08/03/announcing-the-second-round-of-grantees-from-the-wikimedia-foundation-knowledge-equity-fund/">Wikimedia announcement</a>. –&nbsp;<span><a href="https://en.wikipedia.org/wiki/User:Jayen466" title="User:Jayen466">AK</a>, <a href="https://en.wikipedia.org/wiki/User:JPxG" title="User:JPxG">JG</a></span>
</p>
<h3><span id="Jimbo_promises_improvements_for_Wikimedia_Endowment.27s_lackluster_transparency"></span><span id="Jimbo_promises_improvements_for_Wikimedia_Endowment's_lackluster_transparency" data-mw-thread-id="h-Jimbo_promises_improvements_for_Wikimedia_Endowment's_lackluster_transparency-signpost-article-title"><span data-mw-comment-start="" id="h-Jimbo_promises_improvements_for_Wikimedia_Endowment's_lackluster_transparency-signpost-article-title"></span>Jimbo promises improvements for Wikimedia Endowment's lackluster transparency<span data-mw-comment-end="h-Jimbo_promises_improvements_for_Wikimedia_Endowment's_lackluster_transparency-signpost-article-title"></span></span></h3>
<p>On the subject of financial transparency regarding the Wikimedia Endowment: here is what the <a href="https://meta.wikimedia.org/wiki/Wikimedia_Endowment/Meetings/January_27,_2022" title="m:Wikimedia Endowment/Meetings/January 27, 2022">minutes of the January 2022</a> board meeting had to say about it. Not exactly a wealth of detail, but we do at least get a financial summary:
</p>
<blockquote><p><b>8) Fundraising update</b>
</p><ul><li>Overview, lead by Caitlin Virtue</li>
<li>Review of Fundraising Report, lead by Amy Parker</li>
<li>Summary: As of December 31, 2021, the Endowment held $105.4 million. There is currently $99.33 million in the investment account and $6.07 million in cash. An additional $8 million raised in December will be transferred to the Endowment in January 2022.</li></ul>
</blockquote>
<p>This summary was the last time the Endowment Board meeting minutes contained a dollar figure for the Endowment's total value (cash plus investments). Requests for an updated figure in February <span><a href="https://meta.wikimedia.org/w/index.php?title=Talk:Wikimedia_Endowment&amp;oldid=25284029#January_2023_board_meeting">remained unanswered in July</a></span>. 
</p><p>A couple of weeks ago, the Wikimedia Foundation's <a href="https://en.wikipedia.org/wiki/User:JAntonio_(WMF)" title="User:JAntonio (WMF)">Jayde Antonio</a> <span><a href="https://meta.wikimedia.org/w/index.php?title=Wikimedia_Endowment/Meetings/January_19,_2023&amp;oldid=24524042">posted</a></span> the approved minutes for the January 19, 2023 Endowment Board meeting to the its page on <a href="https://en.wikipedia.org/wiki/Meta-Wiki" title="Meta-Wiki">Meta</a>. Noticeable here is the lack of any substantial new information – apart from noting the approval of the Endowment grants which were <a href="https://diff.wikimedia.org/2023/04/13/launching-the-first-grants-from-the-wikimedia-endowment-to-support-technical-innovation-in-wikimedia-projects/">announced publicly back in April</a>, they essentially just repeat the boilerplate meeting agenda posted months ago. 
</p><p>For example, the meeting's <a href="https://meta.wikimedia.org/wiki/Wikimedia_Endowment/Meetings/January_19,_2023#Agenda" title="meta:Wikimedia Endowment/Meetings/January 19, 2023">agenda</a> (posted in February 2023) contained the following item:
</p>
<blockquote><p>6:25 - 6:55 pm UTC: Fundraising Update (Board Chair, Jimmy Wales and Endowment Director, Amy Parker)
</p><ul><li>FY22-23 year to date update</li>
<li>Campaign strategy</li></ul>
</blockquote>
<p>The <a href="https://meta.wikimedia.org/wiki/Wikimedia_Endowment/Meetings/January_19,_2023#Minutes" title="meta:Wikimedia Endowment/Meetings/January 19, 2023">minutes</a> approved by the Endowment's board, led by Jimbo Wales, repeated the same point almost verbatim when they were added in July:
</p>
<blockquote><p><b>Fundraising Update</b> (Amy Parker)
</p><ul><li>FY22-23 year to date update</li>
<li>Presentation of campaign strategy</li></ul>
</blockquote>
<p>Following a <a href="https://en.wikipedia.org/wiki/Special:PermanentLink/1169861874#Endowment" title="Special:PermanentLink/1169861874">query</a> on his user talk page about the Endowment's apparent secrecy, Jimbo <a href="https://en.wikipedia.org/wiki/Special:Diff/1169793872" title="Special:Diff/1169793872">appeared to criticize</a> the minutes approved by him and his board:
</p>
<blockquote><p>At the meeting we discussed, to universal agreement, that we should publish more information and more often [...] the discussion about publishing more information and more often came about in no small part because the January minutes were something that I felt were not good enough in terms of being open and informative. (A financial report is forthccoming – I haven't seen it yet – but delayed because the relevant person creating it has taken a bit of family leave.)
</p></blockquote>
<p>This is a strange comment, as it would seem entirely within the power of the board to determine what information the minutes of its own meetings should contain. He later clarified: "The minutes of the previous board meetings are not written in realtime in the board meeting. They are a legal document prepared in advance and reviewed by the legal team and staff."
</p><p>Following that discussion, however, Wales did provide a <a href="https://meta.wikimedia.org/wiki/Talk:Wikimedia_Endowment#Update_on_endowment" title="m:Talk:Wikimedia Endowment">more meaningful update on Meta-Wiki</a>:
</p>
<blockquote><p>In official business, the Board moved to hire KPMG as our independent auditor for the new entity, approved a spending policy for the Endowment, approved an operational budget of $2.09 million, and approved a grantmaking budget of $2.91 million for FY 2023-24.  We also set the target of $11.5 million in revenue between fundraising and investment income this fiscal year. We ended the last fiscal year with $118 million in the Wikimedia Endowment and are projecting to grow the corpus by approximately $6.5 million depending on market performance and after expenses.
</p></blockquote>
<p>How much of this $118 million is held by the Tides Foundation, and how much by the new 501(c)(3) organization, is unknown. The Wikimedia Foundation has been keen to emphasize that the Endowment is now a transparent 501(c)(3) non-profit, fulfilling a promise first made in 2017, but the Endowment website itself continues to say:
</p>
<blockquote><p>The Endowment has been established, with an initial contribution by Wikimedia Foundation, as a Collective Action Fund at Tides Foundation (Tax ID# 51-0198509).
</p></blockquote>
<p>Jimmy Wales also uploaded a <a href="https://meta.wikimedia.org/wiki/File:Wikimedia_Endowment_2023-24_Plan_-_2023-08-12_release_date.pdf" title="m:File:Wikimedia Endowment 2023-24 Plan - 2023-08-12 release date.pdf">document</a> to Meta-Wiki titled "Wikimedia Endowment 2023-24 Plan". This provides information on fundraising goals, an operational timeline, and the Endowment's budget for 2023–2024. It <a href="https://meta.wikimedia.org/w/index.php?title=File%3AWikimedia_Endowment_2023-24_Plan_-_2023-08-12_release_date.pdf&amp;page=11">mentions</a> $1.8 million in annual expenses in the most recent financial year (similar to the figure mentioned in the <a href="https://meta.wikimedia.org/wiki/Wikimedia_Endowment/Meetings/July_21,_2022" title="m:Wikimedia Endowment/Meetings/July 21, 2022">minutes for the July 2022 board meeting</a>), including $400,000 for unspecified professional services. It envisages the Endowment standing at $130.4 million by the end of the 2023–2024 fiscal year. 
</p><p>Even with the information now provided, the Wikimedia Endowment has never published a statement detailing its revenue and expenses for any year of its existence. Its actual receipts and spending from 2016 to the present day, including the fees paid to Tides, are completely opaque. The Wikimedia Endowment, the Wikimedia movement's richest affiliate, remains some way away from delivering the level of transparency ordinarily expected of Wikimedia affiliates. 
</p><p><i>See also:</i>
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/User_talk:Jimbo_Wales#Endowment" title="User talk:Jimbo Wales">User talk:Jimbo Wales#Endowment</a></li>
<li><a href="https://meta.wikimedia.org/wiki/Talk:Wikimedia_Endowment#Update_on_endowment" title="m:Talk:Wikimedia Endowment">m:Talk:Wikimedia Endowment#Update on endowment</a></li></ul>
<p>–&nbsp;<span><a href="https://en.wikipedia.org/wiki/User:Jayen466" title="User:Jayen466">AK</a></span>
</p>
<h3><span id="Wikifunctions_goes_live" data-mw-thread-id="h-Wikifunctions_goes_live-signpost-article-title"><span data-mw-comment-start="" id="h-Wikifunctions_goes_live-signpost-article-title"></span>Wikifunctions goes live<span data-mw-comment-end="h-Wikifunctions_goes_live-signpost-article-title"></span></span></h3>
</div><div>
<p>The Wikimedia Foundation has <a href="https://diff.wikimedia.org/2023/08/07/wikifunctions-is-starting-up/">announced</a> that after three years of development, its <a href="https://en.wikipedia.org/wiki/Wikifunctions" title="Wikifunctions">Wikifunctions</a> project is slowly beginning to roll out.
</p>
<blockquote><p>Wikifunctions, the newest Wikimedia project, is a new space to collaboratively create and maintain a library of functions. You can think of these functions like recipes for a meal—they take inputs and produce an output (a reliable answer). You might have experienced something similar when using a search engine to find the distance between two locations, the volume of an object, converting two units, and more.
</p></blockquote>
<p>The announcement describes Wikifunctions as "a core component of the larger" <a href="https://en.wikipedia.org/wiki/Abstract_Wikipedia" title="Abstract Wikipedia">Abstract Wikipedia</a>, a project designed to have volunteers writing simple Wikipedia articles in code that can then be translated into human languages. Both projects are spearheaded by <a href="https://en.wikipedia.org/wiki/Denny_Vrande%C4%8Di%C4%87" title="Denny Vrandečić">Denny Vrandečić</a>, the former project lead of <a href="https://en.wikipedia.org/wiki/Wikidata" title="Wikidata">Wikidata</a> and a past Google employee. You can learn more about how Wikifunctions works in this short video on <a href="https://commons.wikimedia.org/wiki/File:Wikifunctions_in_7_minutes.webm" title="commons:File:Wikifunctions in 7 minutes.webm">Commons</a> and <a rel="nofollow" href="https://www.youtube.com/watch?v=bHy63VOp0RQ">YouTube</a>.
</p><p>A technical evaluation published in December 2022 had criticized this "decision to make Abstract Wikipedia depend on Wikifunctions, a new programming language and runtime environment, invented by the Abstract Wikipedia team, with design goals that exceed the scope of Abstract Wikipedia itself, and architectural issues that are incompatible with the standards of correctness, performance, and usability that Abstract Wikipedia requires." However, Vrandečić's team disputed such criticisms and rejected the evaluation's recommendations, which had included decoupling Wikifunctions from Abstract Wikipedia, and having it based on the existing <a href="https://en.wikipedia.org/wiki/Wikipedia:Lua" title="Wikipedia:Lua">Lua</a> programming language that is already integrated into MediaWiki and widely used by Wikipedia editors (see <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-01-01/Technology_report" title="Wikipedia:Wikipedia Signpost/2023-01-01/Technology report">detailed <i>Signpost</i> coverage</a>).
– <span>AK</span>, <span><a href="https://en.wikipedia.org/wiki/User:HaeB" title="User:HaeB">H</a></span>
</p>
<h3><span id="Wikimania_Singapore" data-mw-thread-id="h-Wikimania_Singapore-signpost-article-title"><span data-mw-comment-start="" id="h-Wikimania_Singapore-signpost-article-title"></span>Wikimania Singapore<span data-mw-comment-end="h-Wikimania_Singapore-signpost-article-title"></span></span></h3>
<figure typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Wikimania_2023_Singapore_Header_logo.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Wikimania_2023_Singapore_Header_logo.svg/800px-Wikimania_2023_Singapore_Header_logo.svg.png" decoding="async" width="800" height="272" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Wikimania_2023_Singapore_Header_logo.svg/1200px-Wikimania_2023_Singapore_Header_logo.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Wikimania_2023_Singapore_Header_logo.svg/1600px-Wikimania_2023_Singapore_Header_logo.svg.png 2x" data-file-width="2560" data-file-height="870"></a><figcaption></figcaption></figure>
<p><a href="https://en.wikipedia.org/wiki/Wikimania" title="Wikimania">Wikimania</a> 2023 is taking place in Singapore this week, from 16 to 19 August, with some workshop, hackathon and pre-conference activities happening on 15 August. Event partners include <a href="https://en.wikipedia.org/wiki/UNESCO" title="UNESCO">UNESCO</a>, <a href="https://en.wikipedia.org/wiki/Google" title="Google">Google</a>, <a href="https://en.wikipedia.org/wiki/Creative_Commons" title="Creative Commons">Creative Commons</a> and <a href="https://en.wikipedia.org/wiki/Mozilla" title="Mozilla">Mozilla</a> as well as a number of Singaporean partners like <a href="https://en.wikipedia.org/wiki/NETS_(company)" title="NETS (company)">NETS</a> and the <a href="https://en.wikipedia.org/wiki/National_Library_Board" title="National Library Board">National Library Board</a>.
</p><p>While this year is the first time since 2019 that the Wikimedia movement's annual conference is happening as an in-person event again, it is also open to <a href="https://wikimania.wikimedia.org/wiki/2023:Registration">remote participation</a>.  The full schedule can be found <a rel="nofollow" href="https://pretalx.com/wm2023/schedule/">here</a>.
</p><p><i>The Signpost</i> wishes all those who travel to Wikimania safe journey and a great conference!
</p>
<h3><span id="Brief_notes" data-mw-thread-id="h-Brief_notes-signpost-article-title"><span data-mw-comment-start="" id="h-Brief_notes-signpost-article-title"></span>Brief notes<span data-mw-comment-end="h-Brief_notes-signpost-article-title"></span></span></h3>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Police Are Getting DNA Data from People Who Think They Opted Out (183 pts)]]></title>
            <link>https://theintercept.com/2023/08/18/gedmatch-dna-police-forensic-genetic-genealogy/</link>
            <guid>37179094</guid>
            <pubDate>Fri, 18 Aug 2023 17:28:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theintercept.com/2023/08/18/gedmatch-dna-police-forensic-genetic-genealogy/">https://theintercept.com/2023/08/18/gedmatch-dna-police-forensic-genetic-genealogy/</a>, See on <a href="https://news.ycombinator.com/item?id=37179094">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    
<p><u>CeCe Moore, an</u> actress and director-turned-genetic genealogist, stood behind a lectern at New Jersey’s Ramapo College in late July. Propelled onto the national stage by the popular PBS show “<a href="https://www.pbs.org/weta/finding-your-roots">Finding Your Roots</a>,” Moore was delivering the keynote address for the inaugural conference of forensic genetic genealogists at Ramapo, one of only two institutions of higher education in the U.S. that offer instruction in the field. It was a new era, Moore told the audience, a turning point for solving crime, and they were in on the ground floor. “We’ve created this tool that can accomplish so much,” she said.</p>



<p>Genealogists like Moore hunt for relatives and build family trees just as traditional genealogists do, but with a twist: They work with law enforcement agencies and use commercial DNA databases to search for people who can help them identify unknown human remains or perpetrators who left DNA at a crime scene.</p>



<p>The field exploded in 2018 after the <a href="https://www.vcstar.com/story/news/local/communities/ventura/2018/04/26/golden-state-killer-joseph-deangelo-dna-ventura-double-homicide/554069002/">arrest</a> of Joseph James DeAngelo as the notorious Golden State Killer, responsible for more than a dozen murders across California. DNA evidence collected from a 1980 double murder was analyzed and uploaded to a commercial database; a hit to a distant relative helped a genetic genealogist build an elaborate family tree that ultimately coalesced on DeAngelo. Since then, <a href="https://data.mendeley.com/datasets/jcycgvhm96/1">hundreds</a> of cold cases have been solved using the technique. Moore, among the field’s biggest evangelists, boasts of having personally helped close more than 200 cases.</p>



<p>The practice is not without controversy. It involves combing through the genetic information of hundreds of thousands of innocent people in search of a perpetrator. And its practitioners operate without meaningful guardrails, save for “interim” guidance <a href="https://www.justice.gov/olp/page/file/1204386/download">published</a> by the Department of Justice in 2019.</p>



<p>The last five years have been like the “Wild West,” Moore acknowledged, but she was proud to be among the founding members of the <a href="https://www.iggab.org/">Investigative Genetic Genealogy Accreditation Board</a>, which is developing professional standards for practitioners. “With this incredibly powerful tool comes immense responsibility,” she solemnly told the audience. The practice relies on public trust to convince people not only to upload their private genetic information to commercial databases, but also to allow police to rifle through that information. If you’re doing something you wouldn’t want blasted on the front page of the New York Times, Moore said, you should probably rethink what you’re doing. “If we lose public trust, we will lose this tool.” </p>



<p>Despite those words of caution, Moore is one of several high-profile genetic genealogists who exploited a loophole in a commercial database called GEDmatch, allowing them to search the DNA of individuals who explicitly opted out of sharing their genetic information with police.</p>



<!-- BLOCK(newsletter)[0](%7B%22componentName%22%3A%22NEWSLETTER%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(newsletter)[0] -->



<p>The loophole, which a source demonstrated for The Intercept, allows genealogists working with police to manipulate search fields within a DNA comparison tool to trick the system into showing opted-out profiles. In records of communications reviewed by The Intercept, Moore and two other forensic genetic genealogists discussed the loophole and how to trigger it. In a separate communication, one of the genealogists described hiding the fact that her organization had made an identification using an opted-out profile.</p>



<p>The communications are a disturbing example of how genetic genealogists and their law enforcement partners, in their zeal to close criminal cases, skirt privacy rules put in place by DNA database companies to protect their customers. How common these practices are remains unknown, in part because police and prosecutors have fought to keep details of genetic investigations from being turned over to criminal defendants. As commercial DNA databases grow, and the use of forensic genetic genealogy as a crime-fighting tool expands, experts say the genetic privacy of millions of Americans is in jeopardy.</p>



<p>Moore did not respond to The Intercept’s requests for comment.</p>



<!-- BLOCK(pullquote)[1](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22left%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="left"><!-- CONTENT(pullquote)[1] -->“If we can’t trust these practitioners, we certainly cannot trust law enforcement.”<!-- END-CONTENT(pullquote)[1] --></blockquote><!-- END-BLOCK(pullquote)[1] -->



<p>To Tiffany Roy, a DNA expert and lawyer, the fact that genetic genealogists have accessed private profiles — while simultaneously preaching about ethics — is troubling. “If we can’t trust these practitioners, we certainly cannot trust law enforcement,” she said. “These investigations have serious consequences; they involve people who have never been suspected of a crime.” At the very least, law enforcement actors should have a warrant to conduct a genetic genealogy search, she said. “Anything less is a serious violation of privacy.”</p>


<!-- BLOCK(photo)[2](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22bleed%22%2C%22bleed%22%3A%22large%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[2] --> <p><img decoding="async" fetchpriority="high" width="5514" height="3676" src="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg" alt="MEGYN KELLY TODAY -- Pictured: (l-r) CeCe Moore and Megyn Kelly on Tuesday, August 14, 2018 -- (Photo by: Zach Pagano/NBCU Photo Bank/NBCUniversal via Getty Images via Getty Images)" srcset="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg 5514w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=300,200 300w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=768,512 768w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=1024,683 1024w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=1536,1024 1536w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=2048,1365 2048w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=540,360 540w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-1017209950.jpg?resize=1000,667 1000w" sizes="(max-width: 5514px) 100vw, 5514px"></p><p>CeCe Moore appears as a guest on “Megyn Kelly Today” on Aug. 14, 2018.</p>
<p>
Photo: Zach Pagano/NBCU Photo Bank/NBCUniversal via Getty Images</p><!-- END-CONTENT(photo)[2] --></div><!-- END-BLOCK(photo)[2] -->


<h2 id="h-the-wild-west">The Wild West</h2>



<p>Forensic genetic genealogy evolved from the direct-to-consumer DNA testing craze that took hold roughly a decade ago. Companies like 23andMe and Ancestry offered DNA analysis and a database where results could be uploaded and searched against millions of other profiles, offering consumers a powerful new tool to dig into their heritage through genetics.</p>



<p>It wasn’t long before entrepreneurial genealogists realized this information could also be used to solve criminal cases, especially those that had gone cold. While the arrest of the Golden State Killer captured national attention, it was not the first case solved by forensic genetic genealogy. Two weeks earlier, genetic genealogists Margaret Press and Colleen Fitzpatrick joined officials in Ohio to <a href="https://www.limaohio.com/top-stories/2018/04/16/buckskin-girl-identified/">announce</a> that “groundbreaking work” had allowed authorities to identify a young woman whose body was found by the side of a road back in 1981. Formerly known as “Buckskin Girl” for the handmade pullover she wore, Marcia King was given her name back through genetic genealogy. “Everyone said it couldn’t be done,” Press <a href="https://www.youtube.com/watch?v=dYTI1zvfCfk">said</a>.</p>



<!-- BLOCK(cta)[3](%7B%22componentName%22%3A%22CTA%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(cta)[3] -->



<p>The type of consumer DNA information used in forensic genetic genealogy is far different from that uploaded to the <a href="https://www.fbi.gov/how-we-can-help-you/dna-fingerprint-act-of-2005-expungement-policy/codis-and-ndis-fact-sheet">Combined DNA Index System</a>, or CODIS, a decades-old network administered by the FBI. The DNA entered in CODIS comes from individuals convicted of or arrested for serious crimes and is often referred to as “junk” DNA: short pieces of unique genetic code that don’t carry any individual health or trait information. “It’s not telling us how the person looks. It’s not telling us about their heritage or their phenotypic traits,” Roy said. “It’s a string of numbers, like a telephone number.”</p>



<p>In contrast, the DNA testing offered by direct-to-consumer companies is “as sensitive as it gets,” Roy said. “It tells you about your origins. It tells you about your relatives and your parentage, and it tells you about your disease propensity.” And it has serious reach: While CODIS searches the DNA of people already identified by the criminal justice system, the commercial databases have the potential to search through the DNA of everyone else.</p>



<p>Individuals can upload their test results to any number of databases; at present, there are five main commercial portals. Ancestry and 23andMe are the biggest players in the field, with databases containing roughly 23 million and 14 million profiles. Individuals must test with the companies to gain access to their databases; neither allow DNA results obtained from a different testing service. Both <a href="https://www.ancestry.com/c/legal/privacystatement#:~:text=We%20do%20not%20allow%20law,the%20law%20from%20doing%20so.">Ancestry</a> and <a href="https://www.23andme.com/privacy/">23andMe</a> forbid police, and the genetic genealogists who work with them, from accessing their data for crime-fighting purposes. “We do not allow law enforcement to use Ancestry’s service to investigate crimes or to identify human remains” absent a valid court order, Ancestry’s privacy policy notes. The two companies provide regular <a href="https://www.ancestry.com/c/transparency">transparency</a> reports <a href="https://www.23andme.com/transparency-report/">documenting</a> law enforcement requests for user information.</p>



<p>MyHeritage, home to some 7 million DNA profiles, similarly bars law enforcement searches, but it does allow individuals to upload DNA results obtained from other sources.</p>



<p>And then there are <a href="https://www.familytreedna.com/">FamilyTreeDNA</a> and <a href="https://www.gedmatch.com/about/">GEDmatch</a>, which grant police access but give users the choice of opting in or out. Both allow anyone to upload their DNA results and have <a href="https://www.wired.com/story/genetic-genealogy-nonprofit-dna-database/">upward</a> of 1.8 million profiles. But neither company routinely publicizes the number of customers who have opted in, said Leah Larkin, a veteran genetic genealogist and privacy advocate from California. Larkin writes about issues in the field — including forensic genetic genealogy, which she does not practice — on her website <a href="https://thednageek.com/">the DNA Geek</a>. Larkin estimates that roughly 700,000 GEDmatch profiles are opted in. She suspects that even more are opted in on FamilyTreeDNA; opting in is the default for the company’s U.S. customers and “it’s not obvious how to opt out.”</p>



<p>But even opting out of law enforcement searches doesn’t guarantee that a profile won’t be accessed: A loophole in GEDmatch offers users working with law enforcement agencies a back door to accessing protected profiles. A source showed The Intercept how to exploit the loophole; it was not an obvious weakness or one that could be triggered mistakenly. Rather, it was a back door that required experience with the platform’s various tools to open.</p>



<p>GEDmatch’s parent company, Verogen, did not respond to a request for comment.</p>


<!-- BLOCK(photo)[4](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22none%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[4] --> <p><img decoding="async" width="3000" height="1907" src="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg" alt="CITRUS HEIGHTS, CA - APRIL 25:  Law enforcement officials leave the home of accused rapist and killer Joseph James DeAngelo on April 24, 2018 in Citrus Heights, California. Sacramento District Attorney Anne Marie Schubert was joined by law enforcement officials from across California to announce the arrest of 72 year-old Joseph James DeAngelo who is believed to be the the East Area Rapist, also known as the Golden State Killer, who killed at least 12, raped over 45 people and burglarized hundreds of homes throughout California in the 1970s and 1980s.  (Photo by Justin Sullivan/Getty Images)" srcset="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg 3000w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=300,191 300w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=768,488 768w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=1024,651 1024w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=1536,976 1536w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=2048,1302 2048w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=540,343 540w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-951591636.jpg?resize=1000,636 1000w" sizes="(max-width: 3000px) 100vw, 3000px"></p><p>Law enforcement officials leave the home of accused serial killer Joseph James DeAngelo in Citrus Heights, Calif., on April 24, 2018.</p>
<p>
Photo: Justin Sullivan/Getty Images</p><!-- END-CONTENT(photo)[4] --></div><!-- END-BLOCK(photo)[4] -->


<h2 id="h-an-open-secret">An Open Secret</h2>



<p>In forensic genetic genealogy circles, the GEDmatch loophole had long been an open secret, sources told The Intercept, one that finally surfaced publicly during the Ramapo College conference in late July.</p>



<p>Roy, the DNA expert, was giving a presentation titled “In the Hot Seat,” a primer for genealogists on what to expect if called to testify in a criminal case. There was a clear and simple theme: “Do not lie,” Roy said. “The minute you’re caught in a lie is the minute that it’s going to be difficult for people to use your work.”</p>



<p>As part of the session, David Gurney, a professor of law and society at Ramapo and director of the <a href="https://www.forensicmag.com/592484-NJ-College-Launches-World-s-First-Investigative-Genetic-Genealogy-Center/">college’s</a> nascent <a href="https://www.ramapo.edu/igg/">Investigative Genetic Genealogy Center</a>, joined Roy for a mock questioning of Cairenn Binder, a genealogist who heads up the center’s <a href="https://www.ramapo.edu/igg/certificate-program/">certificate program</a>.</p>



<p>Gurney, simulating direct examination, walked Binder through a series of friendly questions. Did she have access to DNA evidence or genetic code during her investigations? No, she replied. Could she see everyone who’d uploaded DNA to the databases? No, she said, only those who’d opted in to law enforcement searches.</p>



<p>Roy, playing the part of opposing counsel, was pointed in her cross-examination: Was Binder aware of the GEDmatch loophole? And had she used it? Yes, Binder said. “How many times?” Roy asked.</p>



<p>“A handful,” Binder replied. “Maybe up to a dozen.”</p>



<p>Binder’s answers quickly made their way into a private Facebook group for genetic genealogy enthusiasts, prompting a response from the DNA Doe Project, a volunteer-driven organization led by Press, one of the women who identified the Buckskin Girl. Before joining Ramapo College, Binder had worked for the DNA Doe Project.</p>



<p>In a statement posted to the Facebook group, Pam Lauritzen, the project’s communications director, said the loophole was an artifact of changes GEDmatch implemented in 2019, when it made opting out the default for all profiles. “While we knew that the intent of the change was to make opted-out users unavailable, some volunteers with the DNA Doe Project continued to use the reports that allowed access to profiles that were opted out,” she wrote. That use was neither “encouraged nor discouraged,” she continued. Still, she claimed the access was somehow “in compliance” with GEDmatch’s terms of service — which at the time promised that DNA uploaded for law enforcement purposes would only be matched with customers who’d opted in — and that the loophole was closed “years ago.”</p>



<p>It was a curious statement, particularly given that Press, the group’s co-founder, was among the genealogists who discussed the GEDmatch loophole in communications reviewed by The Intercept. In 2020, she described the DNA Doe Project using an opted-out profile to make an identification — and devising a way to keep that quiet.</p>



<p>Press referred The Intercept’s questions to the DNA Doe Project, which declined to comment.</p>



<p>In July 2020, GEDmatch was hacked, which resulted in all 1.45 million profiles then contained in the database to be briefly opted in to law enforcement matching; at the time, BuzzFeed News <a href="https://www.buzzfeednews.com/article/peteraldhous/hackers-gedmatch-dna-privacy">reported</a>, just 280,000 profiles had opted in. GEDmatch was taken offline “until such time that we can be absolutely sure that user data is protected against potential attacks,” Verogen <a href="https://www.facebook.com/officialGEDmatch/posts/226701508802584">wrote</a> on Facebook.</p>



<p>In the wake of the hack, a genetic genealogist named Joan Hanlon was asked by Verogen to beta test a new version of the site. According to records of a conversation reviewed by The Intercept, Press and Moore, the featured speaker at the Ramapo conference, discussed with Hanlon their tricks to access opted-out profiles and whether the new website had plugged all backdoor access. It hadn’t. It’s unclear if anyone told Verogen; as of this month, the back door was still open.</p>



<p>Hanlon did not respond to The Intercept’s requests for comment.</p>



<p>In January 2021, GEDmatch changed its terms of service to opt everyone in for searches involving unidentified human remains, making the back door irrelevant for genealogists who only worked on Doe cases, but not those working with authorities to identify perpetrators of violent crimes.<br></p>



<h2 id="h-undisclosed-methods">Undisclosed Methods</h2>



<p>Exploitation of the GEDmatch loophole isn’t the only example of genetic genealogists and their law enforcement partners playing fast and loose with the rules.</p>



<p>Law enforcement officers have used genetic genealogy to solve crimes that aren’t eligible for genetic investigation per company terms of service and Justice Department guidelines, which say the practice should be reserved for violent crimes like rape and murder only when all other “reasonable” avenues of investigation have failed. In May, CNN <a href="https://edition.cnn.com/2023/05/09/australia/australia-william-leslie-arnold-cold-case-intl-hnk-dst/index.html">reported</a> on a U.S. marshal who used genetic genealogy to solve a decades-old prison break in Nebraska. There is no prison break exception to the eligibility rules, Larkin noted in a <a href="https://thednageek.com/rules-we-dont-need-no-stinkin-rules/">post</a> on her website. “This case should never have used forensic genetic genealogy in the first place.”</p>



<!-- BLOCK(pullquote)[5](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22right%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="right"><!-- CONTENT(pullquote)[5] -->“This case should never have used forensic genetic genealogy in the first place.”<!-- END-CONTENT(pullquote)[5] --></blockquote><!-- END-BLOCK(pullquote)[5] -->



<p>A month later, Larkin wrote about another violation, this time in a California case. The FBI and the Riverside County Regional Cold Case Homicide Team had identified the victim of a 1996 homicide using the MyHeritage database — an explicit violation of the company’s terms of service, which <a href="https://mobileapi.myheritage.com/terms-and-conditions">make clear</a> that using the database for law enforcement purposes is “strictly prohibited” absent a court order.</p>



<p>“The case presents an example of ‘noble cause bias,’” Larkin <a href="https://thednageek.com/noble-cause-no-rules/">wrote</a>, “in which the investigators seem to feel that their objective is so worthy that they can break the rules in place to protect others.”</p>



<p>MyHeritage did not respond to a request for comment. The Riverside County Sheriff’s Office referred questions to the Riverside district attorney’s office, which declined to comment on an ongoing investigation. The FBI also declined to comment.</p>



<p>Violations have even come from inside the DNA testing companies. Back in 2019, GEDmatch co-founder Curtis Rogers unilaterally made an exception to the terms of service, without notifying the site’s users, to allow police to search for someone suspected of assault in Utah. It was a tough call, Rogers <a href="https://www.buzzfeednews.com/article/peteraldhous/genetic-genealogy-parabon-gedmatch-assault">told</a> BuzzFeed News, but the case in question “was as close to a homicide as you can get.”</p>



<p>It appears that violations have also spread to Ancestry, which prohibits the use of its DNA data for law enforcement purposes unless the company is legally compelled to provide access. Genetic genealogists told The Intercept that they are aware of examples in which genealogists working with police have provided AncestryDNA testing kits to the possible relatives of suspects — what’s known as “target testing” — or asked customers for access to preexisting accounts as a way to unlock the off-limits data.</p>



<p>A spokesperson for Ancestry did not answer The Intercept’s questions about efforts to unlock DNA data for law enforcement purposes via a third party. Instead, in a statement, the company reiterated its commitment to maintaining the privacy of its users. “Protecting our customers’ privacy and being good stewards of their data is Ancestry’s highest priority,” it read. The company did not respond to follow-up questions.</p>



<p>As it turns out, the genetic genealogy work in the Golden State Killer case was also questionable: The break that led to DeAngelo came after genealogist Barbara Rae-Venter uploaded DNA from the double murder to MyHeritage, according to the <a href="https://www.latimes.com/california/story/2020-12-08/man-in-the-window">Los Angeles Times</a>. Rae-Venter told the Times that she didn’t notify the company about what she was doing but that her actions were approved by Steve Kramer, the FBI’s Los Angeles division counsel at the time. “In his opinion, law enforcement is entitled to go where the public goes,” Rae-Venter told the paper.</p>



<p>Just how prevalent these practices are may never fully be known, in part because police and prosecutors regularly seek to shield genetic investigations from being vetted in court. They argue that what they obtain from forensic genetic genealogy is merely a tip, like information provided by an informant, and is exempt from disclosure to criminal defendants.</p>



<p>That’s exactly what’s happening in Idaho, where Bryan Kohberger is awaiting trial for the 2022 <a href="https://www.idahostatesman.com/news/local/crime/article275698001.html">murder</a> of four university students. <a href="https://www.idahostatesman.com/news/local/crime/article276611776.html">For months</a>, the state failed to disclose that it had used forensic genetic genealogy to identify Kohberger as a suspect. A probable cause <a href="https://s3.us-west-2.amazonaws.com/isc.coi/CR29-22-2805/122922+Affidavit+-+Exhibit+A+-+Statement+of+Brett-Payne.pdf">statement</a> methodically laying out the evidence that led cops to his door conspicuously omitted any mention of genetic genealogy. Kohberger’s defense team has asked to see documents related to the genealogy work as it prepares for an October trial, but the state has refused, saying the defense has no right to any information about the genetic genealogy it used to crack the case.</p>



<p>Prosecutors said it was the FBI that did the genetic genealogy work, and few records were created in the process, leaving little to turn over. But the state also <a href="https://s3.us-west-2.amazonaws.com/isc.coi/CR29-22-2805/061623+States+Motion+for+Protective+Order.pdf">argued</a> that it couldn’t turn over information because the family tree the FBI created was extensive — including “the names and personal information of … hundreds of innocent relatives” — and the privacy of those individuals needed to be maintained. According to the state, it shouldn’t even have to say which genetic database — or databases — it used.</p>



<p>Kohberger’s attorneys argue that the state’s position is preposterous and keeps them from ensuring that the work undertaken to find Kohberger was above board. “It would appear that the state is acknowledging that the companies are providing personal information to the state and that those companies and the government would suffer if the public were to realize it,” one of Kohberger’s attorneys <a href="https://s3.us-west-2.amazonaws.com/isc.coi/CR29-22-2805/062323+Objection+to+States+Motion+for+Protective+Order.pdf">wrote</a>. “The statement by the government implies that the databases searched may be ones that law enforcement is specifically barred from, which explains why they do not want to disclose their methods.”</p>



<p>A hearing on the issue is scheduled for August 18.</p>


<!-- BLOCK(photo)[6](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22bleed%22%2C%22bleed%22%3A%22large%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[6] --> <p><img decoding="async" width="4633" height="3141" src="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg" alt="LITTLETON, CO - JUNE 27: Patrick Meeker show his family tree on Ancestry.com, June 24, 2016. Meeker used Ancestry.com's DNA test to track down his birth parents. (Photo by RJ Sangosti/The Denver Post via Getty Images)" srcset="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg 4633w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=300,203 300w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=768,521 768w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=1024,694 1024w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=1536,1041 1536w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=2048,1388 2048w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=540,366 540w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-543306496.jpg?resize=1000,678 1000w" sizes="(max-width: 4633px) 100vw, 4633px"></p><p>An AncestryDNA user points to his family tree on Ancestry.com on June 24, 2016.</p>
<p>
Photo: RJ Sangosti/The Denver Post via Getty Images</p><!-- END-CONTENT(photo)[6] --></div><!-- END-BLOCK(photo)[6] -->


<h2>“A Search of All of Us”</h2>



<p>Natalie Ram, a law professor at the University of Maryland Carey School of Law and an expert in genetic privacy, believes forensic genetic genealogy is a giant fishing expedition that fails the particularity requirement of the Fourth Amendment: that law enforcement searches be targeted and based on individualized suspicion. Finding a match to crime scene DNA by searching through millions of genetic profiles is the opposite of targeted. Forensic genetic genealogy, according to Ram, “is fundamentally a search of all of us every time they do it.”</p>



<p>While proponents of forensic genetic genealogy say the individuals they’re searching have willingly uploaded their genetic information and opted in to law enforcement access, Ram and others aren’t so sure that’s the case, even when practitioners adhere to terms of service. If the consent is truly informed and voluntary, “then I think that it would be ethical, lawful, permissible for law enforcement to use that DNA … to identify those individuals who did the volunteering,” Ram said. But that’s not who is being identified in these cases. Instead, it’s relatives — and sometimes very distant relatives. “Our genetic associations are involuntary. They’re profoundly involuntary. They’re involuntary in a way that almost nothing else is. And they’re also immutable,” she said. “I can estrange myself from my family and my siblings and deprive them of information about what I’m doing in my life. And yet their DNA is informative on me.”</p>



<p>Jennifer Lynch, general counsel at the Electronic Frontier Foundation, agrees. “We’re putting other people’s privacy on the line when we’re trying to upload our own genetic information,” she said. “You can’t consent for another person. And there’s just not an argument that you have consented for your genetic information to be in a database when it’s your brother who’s uploaded the information, or when it’s somebody you don’t even know who is related to you.”</p>



<!-- BLOCK(promote-related-post)[7](%7B%22componentName%22%3A%22PROMOTE_RELATED_POST%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22relatedPostNumber%22%3A1%7D) -->

<!-- END-BLOCK(promote-related-post)[7] -->



<p>To date, efforts to rein in the practice as a violation of the Fourth Amendment have presented some problems. A person whose arrest was built on a foundation of genetic genealogy, for example, might have been harmed by the genealogical fishing expedition but lack standing to bring a case; in the strictest sense, it wasn’t their DNA that was searched. In contrast, a third cousin whose DNA was used to identify a suspect could have standing to bring a suit, but they might be hard-pressed to prove they were harmed by the search.</p>



<p>If police are getting hits to suspects by violating companies’ terms of service — using databases that bar police searching — that “raises some serious Fourth Amendment questions” because no expectation of privacy has been waived, Ram said. Of course, ferreting out such violations would require that the information be disclosed in court, which isn’t happening.</p>



<p>At present, the only real regulators of the practice are the database owners: private companies that can change hands or terms of service with little notice. GEDmatch, which has at least once bent its terms to accommodate police, was started by two genealogy hobbyists and then sold to the biotech company Verogen, which in turn was acquired last winter by another biotech company, Qiagen. Experts like Ram and Lynch worry about the implications of so much sensitive information held in for-profit hands — and readily exploited by police. The “platforms right now are the most powerful regulators we have for most Americans,” Ram said. Police regulate “after a fashion, in a fashion, by what they do. They tell us what they’re willing to do by what they actually do,” she added. “But by the way, that’s like law enforcement making rules for itself, so not exactly a diverse group of stakeholders.”</p>



<p>For now, Ram said, the best way to regulate forensic genetic genealogy is by statute. In 2021, Maryland lawmakers passed a <a href="https://mgaleg.maryland.gov/2021RS/chapters_noln/Ch_681_hb0240E.pdf">comprehensive law</a> to restrain the practice. It requires police to obtain a warrant before conducting a genetic genealogy search — certifying that the case is an eligible violent felony and that all other reasonable avenues of investigation have failed — and notify the court before gathering DNA evidence to confirm the suspect identified via genetic genealogy is, in fact, the likely perpetrator. Currently, police use surreptitious methods to collect DNA without judicial oversight: mining a person’s garbage, for example, for items expected to contain biological evidence. In the Golden State Killer case, DeAngelo was implicated by DNA on a discarded tissue.</p>



<p>The Maryland law also requires police to obtain consent from any third party whose DNA might help solve a crime. In the Kohberger case, police searched his parents’ garbage, <a href="https://slate.com/technology/2023/01/bryan-kohberger-university-idaho-murders-forensic-genealogy.html">collecting trash</a> with DNA on it that the lab believed belonged to Kohberger’s father. In a notorious Florida case, <a href="https://www.nbcnews.com/news/us-news/they-lied-us-mom-says-police-deceived-her-get-her-n1140696">police lied</a> to a suspect’s parents to get a DNA sample from the mother, telling her they were trying to identify a person found dead whom they believed was her relative. Those methods are barred under the Maryland law.</p>



<p><a href="https://leg.mt.gov/bills/2021/billhtml/HB0602.htm">Montana</a> and <a href="https://le.utah.gov/xcode/Title53/Chapter10/53-10-S403.7.html?v=C53-10-S403.7_2023050320230503">Utah</a> have also passed laws governing forensic genetic genealogy, though neither is as strict as Maryland’s.</p>


<!-- BLOCK(photo)[8](%7B%22componentName%22%3A%22PHOTO%22%2C%22entityType%22%3A%22RESOURCE%22%7D)(%7B%22scroll%22%3Afalse%2C%22align%22%3A%22none%22%2C%22width%22%3A%22auto%22%7D) --><div><!-- CONTENT(photo)[8] --> <p><img decoding="async" loading="lazy" width="4920" height="3551" src="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg" alt="MyHeritage UK Ltd. DNA kits are displayed for sale at the 2017 RootsTech Conference in Salt Lake City, Utah, U.S., on Thursday, Feb. 9, 2017. The four-day conference is a genealogy event focused on discovering and sharing family connections across generations through technology. Photographer: George Frey/Bloomberg via Getty Images" srcset="https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg 4920w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=300,217 300w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=768,554 768w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=1024,739 1024w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=1536,1109 1536w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=2048,1478 2048w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=540,390 540w, https://theintercept.com/wp-content/uploads/2023/08/GettyImages-634432394.jpg?resize=1000,722 1000w" sizes="(max-width: 4920px) 100vw, 4920px"></p><p>MyHeritage DNA kits are displayed at the RootsTech conference in Salt Lake City on Feb. 9, 2017.</p>
<p>
Photo: George Frey/Bloomberg via Getty Images</p><!-- END-CONTENT(photo)[8] --></div><!-- END-BLOCK(photo)[8] -->


<h2>Solving Crime Before It Happens</h2>



<p>The rise of direct-to-consumer DNA testing and forensic genetic genealogy raises another issue: the looming reality of a de facto national DNA database that can identify large swaths of the U.S. population, regardless of whether those individuals have uploaded their genetic information. In 2018, researchers led by the former chief science officer at MyHeritage <a href="https://www.science.org/doi/10.1126/science.aau4832">predicted</a> that a database of roughly 3 million people could identify nearly 100 percent of U.S. citizens of European descent. “Such a database scale is foreseeable for some third-party websites in the near future,” they concluded.</p>



<!-- BLOCK(pullquote)[9](%7B%22componentName%22%3A%22PULLQUOTE%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%22pull%22%3A%22right%22%7D) --><blockquote data-shortcode-type="pullquote" data-pull="right"><!-- CONTENT(pullquote)[9] -->“All of a sudden, we have a national DNA database, and we didn’t ever have any kind of debate about whether we wanted that in our society.”<!-- END-CONTENT(pullquote)[9] --></blockquote><!-- END-BLOCK(pullquote)[9] -->



<p>“All of a sudden, we have a national DNA database,” said Lynch, “and we didn’t ever have any kind of debate about whether we wanted that in our society.” A national database in “private hands,” she added.</p>



<p>By the time people started worrying about this as a policy issue, it was “too late,” Moore said during her address at the Ramapo conference. “By the time the vast majority of the public learned about genetic genealogy, we’d been quietly building this incredibly powerful tool for human identification behind the scenes,” she said. “People sort of laughed, like, ‘Oh, hobbyists … you do your genealogy, you do your adoption,’ and we were allowed to build this tool without interference.”</p>



<p>Moore advocated for involving forensic genetic genealogy earlier in the investigative process. Doing so, she argued, could focus police on guilty parties more quickly and save innocent people from needless law enforcement scrutiny. In fact, she told the audience, she believes that forensic genetic genealogy can help to eradicate crime. “We can stop criminals in their tracks,” she said. “I really believe we can stop serial killers from existing, stop serial rapists from existing.”</p>



<p>“We are an army. We can do this! So repeat after me,” Moore said, before leading the audience in a chant. “No more serial killers!”</p>



<p><strong>Update: August 18, 2023, 3:55 p.m. ET</strong></p>



<p><em>After this article was published, Margaret Press, founder of the DNA Doe Project, released a <a href="https://dnadoeproject.org/statement-from-Margaret-Press/">statement</a> in response to The Intercept’s findings. Press acknowledged that between May 2019 and January 2021, the organization’s leadership and volunteers made use of GEDmatch tools that provided access to DNA profiles that were opted out of law enforcement searches, which she described as “a bug in the software.” Press stated:</em></p>



<blockquote>
<p><em>We have always been committed to abide by the Terms of Service for the databases we used, and take our responsibility to our law enforcement and medical examiner partner agencies extremely seriously. In hindsight, it’s clear we failed to consider the critically important need for the public to be able to trust that their DNA data will only be shared and used with their permission and under the restrictions they choose. We should have reported these bugs to GEDmatch and stopped using the affected reports until the bugs were fixed. Instead, on that first day when we found that all of the profiles were set to opt-out, I discouraged our team from reporting them at all. I now know I was wrong and I regret my words and actions.</em></p>
</blockquote>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zaum (105 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Zaum</link>
            <guid>37178251</guid>
            <pubDate>Fri, 18 Aug 2023 16:41:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Zaum">https://en.wikipedia.org/wiki/Zaum</a>, See on <a href="https://news.ycombinator.com/item?id=37178251">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div id="mw-content-text" lang="en" dir="ltr">
<p>This article is about the Russian Futurist concept. For the American band, see <a href="https://en.wikipedia.org/wiki/Zaum_(band)" title="Zaum (band)">Zaum (band)</a>.</p>
<p>Not to be confused with <a href="https://en.wikipedia.org/wiki/ZA/UM" title="ZA/UM">ZA/UM</a>.</p>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Zangezi.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Zangezi.jpg/220px-Zangezi.jpg" decoding="async" width="220" height="321" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Zangezi.jpg/330px-Zangezi.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/7/71/Zangezi.jpg 2x" data-file-width="411" data-file-height="600"></a><figcaption><a href="https://en.wikipedia.org/wiki/Velimir_Khlebnikov" title="Velimir Khlebnikov">Khlebnikov's</a> book <i><a href="https://en.wikipedia.org/wiki/Zangezi" title="Zangezi">Zangezi</a></i> (1922)</figcaption></figure>
<p><b>Zaum</b> (<a href="https://en.wikipedia.org/wiki/Russian_language" title="Russian language">Russian</a>: <span lang="ru">за́умь</span>, <small><a href="https://en.wikipedia.org/wiki/Literal_translation" title="Literal translation">lit.</a> </small>'transrational') are the linguistic experiments in <a href="https://en.wikipedia.org/wiki/Sound_symbolism" title="Sound symbolism">sound symbolism</a> and <a href="https://en.wikipedia.org/wiki/Artistic_language" title="Artistic language">language creation</a> of <a href="https://en.wikipedia.org/wiki/Cubo-Futurism" title="Cubo-Futurism">Russian Cubo-Futurist</a> poets such as <a href="https://en.wikipedia.org/wiki/Velimir_Khlebnikov" title="Velimir Khlebnikov">Velimir Khlebnikov</a> and <a href="https://en.wikipedia.org/wiki/Aleksei_Kruchenykh" title="Aleksei Kruchenykh">Aleksei Kruchenykh</a>. Zaum is a non-referential <a href="https://en.wikipedia.org/wiki/Phonetics" title="Phonetics">phonetic</a> entity with its own <a href="https://en.wikipedia.org/wiki/Ontology" title="Ontology">ontology</a>. The language consists of <a href="https://en.wikipedia.org/wiki/Neologism" title="Neologism">neologisms</a> that mean nothing. Zaum is a language organized through phonetic analogy and rhythm.<sup id="cite_ref-:0_1-0"><a href="#cite_note-:0-1">[1]</a></sup> Zaum literature cannot contain any <a href="https://en.wikipedia.org/wiki/Onomatopoeia" title="Onomatopoeia">onomatopoeia</a> or <a href="https://en.wikipedia.org/wiki/Psychopathology" title="Psychopathology">psychopathological states</a>.<sup id="cite_ref-:1_2-0"><a href="#cite_note-:1-2">[2]</a></sup>
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Usage">Usage</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=1" title="Edit section: Usage">edit</a><span>]</span></span></h2>
<p>Aleksei Kruchenykh created Zaum in order to show that language was indefinite and indeterminate.<sup id="cite_ref-:1_2-1"><a href="#cite_note-:1-2">[2]</a></sup>
</p><p>Kruchenykh stated that when creating Zaum, he decided to forgo <a href="https://en.wikipedia.org/wiki/Grammar" title="Grammar">grammar</a> and <a href="https://en.wikipedia.org/wiki/Syntax" title="Syntax">syntax</a> rules. He wanted to convey the disorder of life by introducing disorder into the language. Kruchenykh considered Zaum to be the manifestation of a spontaneous non-codified language.<sup id="cite_ref-:0_1-1"><a href="#cite_note-:0-1">[1]</a></sup>
</p><p>Khelinbov believed that the purpose of Zaum was to find the essential meaning of <a href="https://en.wikipedia.org/wiki/Root_(linguistics)" title="Root (linguistics)">word roots</a> in <a href="https://en.wikipedia.org/wiki/Consonant" title="Consonant">consonantal sounds</a>. He believed such knowledge could help create a new <a href="https://en.wikipedia.org/wiki/Universal_language" title="Universal language">universal language</a> based on reason.<sup id="cite_ref-:0_1-2"><a href="#cite_note-:0-1">[1]</a></sup>
</p><p>Examples of zaum include Kruchenykh's poem "<a href="https://en.wikipedia.org/wiki/Dyr_bul_shchyl" title="Dyr bul shchyl">Dyr bul shchyl</a>",<sup id="cite_ref-FOOTNOTEJanecek199649_3-0"><a href="#cite_note-FOOTNOTEJanecek199649-3">[3]</a></sup> Kruchenykh's libretto for the Futurist opera <i><a href="https://en.wikipedia.org/wiki/Victory_over_the_Sun" title="Victory over the Sun">Victory over the Sun</a></i> with music by <a href="https://en.wikipedia.org/wiki/Mikhail_Matyushin" title="Mikhail Matyushin">Mikhail Matyushin</a> and stage design by <a href="https://en.wikipedia.org/wiki/Kazimir_Malevich" title="Kazimir Malevich">Kazimir Malevich</a>,<sup id="cite_ref-FOOTNOTEJanecek1996111_4-0"><a href="#cite_note-FOOTNOTEJanecek1996111-4">[4]</a></sup> and Khlebnikov's so-called "<a href="https://en.wikipedia.org/wiki/Language_of_the_birds" title="Language of the birds">language of the birds</a>", "<a href="https://en.wikipedia.org/wiki/Language_of_the_gods" title="Language of the gods">language of the gods</a>" and "language of the stars".<sup id="cite_ref-FOOTNOTEJanecek1996137–138_5-0"><a href="#cite_note-FOOTNOTEJanecek1996137–138-5">[5]</a></sup> The poetic output is perhaps comparable to that of the contemporary <a href="https://en.wikipedia.org/wiki/Dadaism" title="Dadaism">Dadaism</a> but the linguistic theory or metaphysics behind zaum was entirely devoid of the gentle reflexive irony of that movement and in all seriousness intended to recover the <a href="https://en.wikipedia.org/wiki/Sound_symbolism" title="Sound symbolism">sound symbolism</a> of a lost <a href="https://en.wikipedia.org/wiki/Glottogony" title="Glottogony">aboriginal tongue</a>.<sup id="cite_ref-FOOTNOTEJanecek199679_6-0"><a href="#cite_note-FOOTNOTEJanecek199679-6">[6]</a></sup> Exhibiting traits of a Slavic <a href="https://en.wikipedia.org/wiki/National_mysticism" title="National mysticism">national mysticism</a>, Kruchenykh aimed at recovering the primeval Slavic mother-tongue in particular.
</p><p>Kruchenykh would author many poems and <a href="https://en.wikipedia.org/wiki/Mimeograph" title="Mimeograph">mimeographed</a> <a href="https://en.wikipedia.org/wiki/Pamphlet" title="Pamphlet">pamphlets</a> written in Zaum. These pamphlets combine poetry, illustrations, and theory.<sup id="cite_ref-:0_1-3"><a href="#cite_note-:0-1">[1]</a></sup>
</p><p>In modern times, since 1962 <a href="https://en.wikipedia.org/wiki/Serge_Segay" title="Serge Segay">Serge Segay</a> was creating zaum poetry.<sup id="cite_ref-У_Голубой_Лагуны_7-0"><a href="#cite_note-У_Голубой_Лагуны-7">[7]</a></sup> <a href="https://en.wikipedia.org/wiki/Ry_Nikonova" title="Ry Nikonova">Rea Nikonova</a> started creating zaum verses probably a bit later, around 1964.<sup id="cite_ref-Zhumati_8-0"><a href="#cite_note-Zhumati-8">[8]</a></sup> Their zaum poetry can be seen e.g. in issues of the famous "Transponans" <a href="https://en.wikipedia.org/wiki/Samizdat" title="Samizdat">samizdat</a> magazine.<sup id="cite_ref-Transponans_9-0"><a href="#cite_note-Transponans-9">[9]</a></sup> In 1990, contemporary <a href="https://en.wikipedia.org/wiki/Avant-garde" title="Avant-garde">avant-garde</a> poet Sergei Biriukov has founded an association of poets called the "Academy of Zaum" in <a href="https://en.wikipedia.org/wiki/Tambov" title="Tambov">Tambov</a>.
</p><p>The use of Zaum peaked from 1916 to 1920 during <a href="https://en.wikipedia.org/wiki/World_War_I" title="World War I">World War I</a>. At this time, Zaumism took root as a movement primarily involved in <a href="https://en.wikipedia.org/wiki/Visual_arts" title="Visual arts">visual arts</a>, <a href="https://en.wikipedia.org/wiki/Literature" title="Literature">literature</a>, <a href="https://en.wikipedia.org/wiki/Poetry" title="Poetry">poetry</a>, <a href="https://en.wikipedia.org/wiki/Art_manifesto" title="Art manifesto">art manifestoes</a>, <a href="https://en.wikipedia.org/wiki/Art_theory" title="Art theory">art theory</a>, <a href="https://en.wikipedia.org/wiki/Theatre" title="Theatre">theatre</a>, and <a href="https://en.wikipedia.org/wiki/Graphic_design" title="Graphic design">graphic design</a>,<sup id="cite_ref-FOOTNOTEJanecek1984149–206_10-0"><a href="#cite_note-FOOTNOTEJanecek1984149–206-10">[10]</a></sup> and concentrated its <a href="https://en.wikipedia.org/wiki/Anti_war" title="Anti war">anti war</a> politic through a rejection of the prevailing standards in <a href="https://en.wikipedia.org/wiki/Art" title="Art">art</a> through <a href="https://en.wikipedia.org/wiki/Anti-art" title="Anti-art">anti-art</a> cultural works. Zaum activities included public gatherings, demonstrations, and publications. The movement influenced later styles, <a href="https://en.wikipedia.org/wiki/Avant-garde" title="Avant-garde">Avant-garde</a> and <a href="https://en.wikipedia.org/wiki/Downtown_music" title="Downtown music">downtown music</a> movements, and groups including <a href="https://en.wikipedia.org/wiki/Surrealism" title="Surrealism">surrealism</a>, <a href="https://en.wikipedia.org/wiki/Nouveau_r%C3%A9alisme" title="Nouveau réalisme">nouveau réalisme</a>, <a href="https://en.wikipedia.org/wiki/Pop_Art" title="Pop Art">Pop Art</a> and <a href="https://en.wikipedia.org/wiki/Fluxus" title="Fluxus">Fluxus</a>.<sup id="cite_ref-FOOTNOTEKnowlson1996217_11-0"><a href="#cite_note-FOOTNOTEKnowlson1996217-11">[11]</a></sup>
</p>
<h2><span id="Etymology_and_meaning">Etymology and meaning</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=2" title="Edit section: Etymology and meaning">edit</a><span>]</span></span></h2>
<p>Coined by Kruchenykh in 1913,<sup id="cite_ref-FOOTNOTEJanecek19962_12-0"><a href="#cite_note-FOOTNOTEJanecek19962-12">[12]</a></sup> the word <i>zaum</i> is made up of the Russian prefix <a href="https://en.wiktionary.org/wiki/%D0%B7%D0%B0" title="wikt:за">за</a> "beyond, behind" and noun <a href="https://en.wiktionary.org/wiki/%D1%83%D0%BC" title="wikt:ум">ум</a> "the mind, <i><a href="https://en.wikipedia.org/wiki/Nous" title="Nous">nous</a></i>" and has been translated as "transreason", "transration" or "beyonsense."<sup id="cite_ref-FOOTNOTEJanecek19961_13-0"><a href="#cite_note-FOOTNOTEJanecek19961-13">[13]</a></sup> According to scholar Gerald Janecek, <i>zaum</i> can be defined as experimental poetic language characterized by indeterminacy in meaning.<sup id="cite_ref-FOOTNOTEJanecek19961_13-1"><a href="#cite_note-FOOTNOTEJanecek19961-13">[13]</a></sup>
</p><p>Kruchenykh, in "Declaration of the Word as Such (1913)", declares zaum "a language which does not have any definite meaning, a transrational language" that "allows for fuller expression" whereas, he maintains, the common language of everyday speech "binds".<sup id="cite_ref-FOOTNOTEJanecek199678_14-0"><a href="#cite_note-FOOTNOTEJanecek199678-14">[14]</a></sup> He further maintained, in "Declaration of Transrational Language (1921)", that zaum "can provide a universal poetic language, born organically, and not artificially, like <a href="https://en.wikipedia.org/wiki/Esperanto" title="Esperanto">Esperanto</a>."<sup id="cite_ref-FOOTNOTEKruchenykh2005183_15-0"><a href="#cite_note-FOOTNOTEKruchenykh2005183-15">[15]</a></sup>
</p>
<h2><span id="Major_zaumiks">Major zaumiks</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=3" title="Edit section: Major zaumiks">edit</a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Velimir_Khlebnikov" title="Velimir Khlebnikov">Velimir Khlebnikov</a><sup id="cite_ref-:1_2-2"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Aleksei_Kruchenykh" title="Aleksei Kruchenykh">Aleksei Kruchenykh</a><sup id="cite_ref-:1_2-3"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Ilia_Zdanevich" title="Ilia Zdanevich">Ilia Zdanevich</a><sup id="cite_ref-:1_2-4"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Igor_Terentiev" title="Igor Terentiev">Igor Terentev</a><sup id="cite_ref-:1_2-5"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li>Aleksandr Tufanov<sup id="cite_ref-:1_2-6"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Kazimir_Malevich" title="Kazimir Malevich">Kazimir Malevich</a><sup id="cite_ref-:1_2-7"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Olga_Rozanova" title="Olga Rozanova">Olga Razanova</a><sup id="cite_ref-:1_2-8"><a href="#cite_note-:1-2">[2]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Varvara_Stepanova" title="Varvara Stepanova">Varvara Stepanova</a><sup id="cite_ref-:1_2-9"><a href="#cite_note-:1-2">[2]</a></sup></li></ul>
<h2><span id="Notes">Notes</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=4" title="Edit section: Notes">edit</a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-:0-1"><span>^ <a href="#cite_ref-:0_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:0_1-3"><sup><i><b>d</b></i></sup></a></span> <span><cite id="CITEREFTerras1985">Terras, Victor (1985). <i>Handbook of Russian Literature</i>. London: Yale University Press. p.&nbsp;530. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-030-004-868-1" title="Special:BookSources/978-030-004-868-1"><bdi>978-030-004-868-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Handbook+of+Russian+Literature&amp;rft.place=London&amp;rft.pages=530&amp;rft.pub=Yale+University+Press&amp;rft.date=1985&amp;rft.isbn=978-030-004-868-1&amp;rft.aulast=Terras&amp;rft.aufirst=Victor&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></span>
</li>
<li id="cite_note-:1-2"><span>^ <a href="#cite_ref-:1_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:1_2-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:1_2-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:1_2-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-:1_2-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-:1_2-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-:1_2-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-:1_2-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-:1_2-9"><sup><i><b>j</b></i></sup></a></span> <span><cite id="CITEREFKostelanetz2013">Kostelanetz, Richard (2013). <i>A Dictionary of the Avant-Gardes</i>. New York: Taylor&amp;Francis. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-113-680-619-3" title="Special:BookSources/978-113-680-619-3"><bdi>978-113-680-619-3</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+Dictionary+of+the+Avant-Gardes&amp;rft.place=New+York&amp;rft.pub=Taylor%26Francis&amp;rft.date=2013&amp;rft.isbn=978-113-680-619-3&amp;rft.aulast=Kostelanetz&amp;rft.aufirst=Richard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></span>
</li>
<li id="cite_note-FOOTNOTEJanecek199649-3"><span><b><a href="#cite_ref-FOOTNOTEJanecek199649_3-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;49.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek1996111-4"><span><b><a href="#cite_ref-FOOTNOTEJanecek1996111_4-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;111.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek1996137–138-5"><span><b><a href="#cite_ref-FOOTNOTEJanecek1996137–138_5-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, pp.&nbsp;137–138.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek199679-6"><span><b><a href="#cite_ref-FOOTNOTEJanecek199679_6-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;79.</span>
</li>
<li id="cite_note-У_Голубой_Лагуны-7"><span><b><a href="#cite_ref-У_Голубой_Лагуны_7-0">^</a></b></span> <span><i><a href="https://en.wikipedia.org/w/index.php?title=%D0%9A%D1%83%D0%B7%D1%8C%D0%BC%D0%B8%D0%BD%D1%81%D0%BA%D0%B8%D0%B9,_%D0%9A%D0%BE%D0%BD%D1%81%D1%82%D0%B0%D0%BD%D1%82%D0%B8%D0%BD_%D0%9A%D0%BE%D0%BD%D1%81%D1%82%D0%B0%D0%BD%D1%82%D0%B8%D0%BD%D0%BE%D0%B2%D0%B8%D1%87&amp;action=edit&amp;redlink=1" title="Кузьминский, Константин Константинович (page does not exist)">Кузьминский К.</a>, Ковалёв Г.</i> <a rel="nofollow" href="http://www.kkk-bluelagoon.ru/tom5b/transpoety.htm">Антология новейшей русской поэзии у Голубой Лагуны</a>. — Т. 5Б.</span>
</li>
<li id="cite_note-Zhumati-8"><span><b><a href="#cite_ref-Zhumati_8-0">^</a></b></span> <span><cite id="CITEREFЖумати1999">Жумати, Т. П. (1999). "<span></span>"Уктусская школа" (1965-1974)&nbsp;: К истории уральского андеграунда". <i>Известия Уральского государственного университета</i>. <b>13</b>: 125–127.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=%D0%98%D0%B7%D0%B2%D0%B5%D1%81%D1%82%D0%B8%D1%8F+%D0%A3%D1%80%D0%B0%D0%BB%D1%8C%D1%81%D0%BA%D0%BE%D0%B3%D0%BE+%D0%B3%D0%BE%D1%81%D1%83%D0%B4%D0%B0%D1%80%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE+%D1%83%D0%BD%D0%B8%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D1%82%D0%B5%D1%82%D0%B0&amp;rft.atitle=%22%D0%A3%D0%BA%D1%82%D1%83%D1%81%D1%81%D0%BA%D0%B0%D1%8F+%D1%88%D0%BA%D0%BE%D0%BB%D0%B0%22+%281965-1974%29+%3A+%D0%9A+%D0%B8%D1%81%D1%82%D0%BE%D1%80%D0%B8%D0%B8+%D1%83%D1%80%D0%B0%D0%BB%D1%8C%D1%81%D0%BA%D0%BE%D0%B3%D0%BE+%D0%B0%D0%BD%D0%B4%D0%B5%D0%B3%D1%80%D0%B0%D1%83%D0%BD%D0%B4%D0%B0&amp;rft.volume=13&amp;rft.pages=125-127&amp;rft.date=1999&amp;rft.aulast=%D0%96%D1%83%D0%BC%D0%B0%D1%82%D0%B8&amp;rft.aufirst=%D0%A2.+%D0%9F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></span>
</li>
<li id="cite_note-Transponans-9"><span><b><a href="#cite_ref-Transponans_9-0">^</a></b></span> <span><cite><a rel="nofollow" href="https://samizdatcollections.library.utoronto.ca/islandora/object/samizdat:transponans">"Журнал теории и практики "Транспонанс": Комментированное электронное издание / Под ред. И. Кукуя. - A Work in Progress | Project for the Study of Dissidence and Samizdat"</a>. <i>samizdatcollections.library.utoronto.ca</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=samizdatcollections.library.utoronto.ca&amp;rft.atitle=%D0%96%D1%83%D1%80%D0%BD%D0%B0%D0%BB+%D1%82%D0%B5%D0%BE%D1%80%D0%B8%D0%B8+%D0%B8+%D0%BF%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B8+%22%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D0%BF%D0%BE%D0%BD%D0%B0%D0%BD%D1%81%22%3A+%D0%9A%D0%BE%D0%BC%D0%BC%D0%B5%D0%BD%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%BE%D0%B5+%D1%8D%D0%BB%D0%B5%D0%BA%D1%82%D1%80%D0%BE%D0%BD%D0%BD%D0%BE%D0%B5+%D0%B8%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5+%2F+%D0%9F%D0%BE%D0%B4+%D1%80%D0%B5%D0%B4.+%D0%98.+%D0%9A%D1%83%D0%BA%D1%83%D1%8F.+-+A+Work+in+Progress+%26%23124%3B+Project+for+the+Study+of+Dissidence+and+Samizdat&amp;rft_id=https%3A%2F%2Fsamizdatcollections.library.utoronto.ca%2Fislandora%2Fobject%2Fsamizdat%3Atransponans&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></span>
</li>
<li id="cite_note-FOOTNOTEJanecek1984149–206-10"><span><b><a href="#cite_ref-FOOTNOTEJanecek1984149–206_10-0">^</a></b></span> <span><a href="#CITEREFJanecek1984">Janecek 1984</a>, pp.&nbsp;149–206.</span>
</li>
<li id="cite_note-FOOTNOTEKnowlson1996217-11"><span><b><a href="#cite_ref-FOOTNOTEKnowlson1996217_11-0">^</a></b></span> <span><a href="#CITEREFKnowlson1996">Knowlson 1996</a>, p.&nbsp;217.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek19962-12"><span><b><a href="#cite_ref-FOOTNOTEJanecek19962_12-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;2.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek19961-13"><span>^ <a href="#cite_ref-FOOTNOTEJanecek19961_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-FOOTNOTEJanecek19961_13-1"><sup><i><b>b</b></i></sup></a></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;1.</span>
</li>
<li id="cite_note-FOOTNOTEJanecek199678-14"><span><b><a href="#cite_ref-FOOTNOTEJanecek199678_14-0">^</a></b></span> <span><a href="#CITEREFJanecek1996">Janecek 1996</a>, p.&nbsp;78.</span>
</li>
<li id="cite_note-FOOTNOTEKruchenykh2005183-15"><span><b><a href="#cite_ref-FOOTNOTEKruchenykh2005183_15-0">^</a></b></span> <span><a href="#CITEREFKruchenykh2005">Kruchenykh 2005</a>, p.&nbsp;183.</span>
</li>
</ol></div>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=5" title="Edit section: References">edit</a><span>]</span></span></h2>
<ul><li><cite id="CITEREFJanecek1984">Janecek, Gerald (1984), <i>The Look of Russian Literature: Avant-Garde Visual Experiments 1900-1930</i>, Princeton: Princeton University Press, <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0691014579" title="Special:BookSources/978-0691014579"><bdi>978-0691014579</bdi></a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Look+of+Russian+Literature%3A+Avant-Garde+Visual+Experiments+1900-1930&amp;rft.place=Princeton&amp;rft.pub=Princeton+University+Press&amp;rft.date=1984&amp;rft.isbn=978-0691014579&amp;rft.aulast=Janecek&amp;rft.aufirst=Gerald&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></li>
<li><cite id="CITEREFJanecek1996">Janecek, Gerald (1996), <i>Zaum: The Transrational Poetry of Russian Futurism</i>, San Diego: San Diego State University Press, <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1879691414" title="Special:BookSources/978-1879691414"><bdi>978-1879691414</bdi></a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Zaum%3A+The+Transrational+Poetry+of+Russian+Futurism&amp;rft.place=San+Diego&amp;rft.pub=San+Diego+State+University+Press&amp;rft.date=1996&amp;rft.isbn=978-1879691414&amp;rft.aulast=Janecek&amp;rft.aufirst=Gerald&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></li>
<li><cite id="CITEREFKruchenykh2005">Kruchenykh, Aleksei (2005), Anna Lawton; Herbert Eagle (eds.), "Declaration of Transrational Language", <i>Words in Revolution: Russian Futurist Manifestoes 1912-1928</i>, Washington: New Academia Publishing, <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0974493473" title="Special:BookSources/978-0974493473"><bdi>978-0974493473</bdi></a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Words+in+Revolution%3A+Russian+Futurist+Manifestoes+1912-1928&amp;rft.atitle=Declaration+of+Transrational+Language&amp;rft.date=2005&amp;rft.isbn=978-0974493473&amp;rft.aulast=Kruchenykh&amp;rft.aufirst=Aleksei&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></li>
<li><cite id="CITEREFKnowlson1996">Knowlson, J. (1996), <i>The Continuing Influence of Zaum</i>, London: Bloomsbury</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Continuing+Influence+of+Zaum&amp;rft.place=London&amp;rft.pub=Bloomsbury&amp;rft.date=1996&amp;rft.aulast=Knowlson&amp;rft.aufirst=J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AZaum"></span></li></ul>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zaum&amp;action=edit&amp;section=6" title="Edit section: External links">edit</a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="http://www.thing.net/~grist/l%26d/kruch/lkrucht1.htm">Chapter Nine of G. Janecek, <i>Zaum: The Transrational Poetry of Russian Futurism</i></a></li>
<li><a rel="nofollow" href="http://sdsupress.sdsu.edu/codex.html">Janecek's <i>Zaum</i>, published by San Diego State University Press</a></li>
<li><a rel="nofollow" href="http://users.belgacom.net/nachtschimmen/zaumpaper.htm">Lecture by Z. Laskewicz: "Zaum: Words Without Meaning or Meaning Without Words? Towards a Musical Understanding of Language"</a></li>
<li><a rel="nofollow" href="http://jacketmagazine.com/27/reed.html">'Locating Zaum: Mnatsakanova on Khlebnikov'</a> an essay by Brian Reed</li>
<li><a rel="nofollow" href="http://www.vavilon.ru/texts/purin3-8.html">Article by A. Purin: "Meaning and Zaum"</a> <span>(in Russian)</span></li>
<li><a rel="nofollow" href="http://www.tstu.ru/koi/kultur/literary/lit_tamb/az.htm">Tambov Academy of Zaum</a>, Cyrillic KOI8-R encoding <span>(in Russian)</span></li>
<li><a rel="nofollow" href="http://www.transfurism.com/serge-segay-books">Samizdat books and artist' books by Serge Segay, some with zaum and visual poetry</a></li>
<li><a rel="nofollow" href="http://www.transfurism.com/ry-nikonova-books">Samizdat books and artist' books by Ry Nikonova, some with zaum and visual poetry</a></li></ul>


<!-- 
NewPP limit report
Parsed by mw1379
Cached time: 20230818235036
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.568 seconds
Real time usage: 0.693 seconds
Preprocessor visited node count: 1564/1000000
Post‐expand include size: 71743/2097152 bytes
Template argument size: 1448/2097152 bytes
Highest expansion depth: 9/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 42062/5000000 bytes
Lua time usage: 0.380/10.000 seconds
Lua memory usage: 22397093/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  569.425      1 -total
 21.89%  124.644      3 Template:Navbox
 21.44%  122.061      1 Template:Reflist
 18.11%  103.134      1 Template:Futurism
 17.23%   98.085      1 Template:Lang-ru
 16.01%   91.181      2 Template:Cite_book
 14.54%   82.821     11 Template:Sfn
 12.29%   69.970      1 Template:Short_description
  7.33%   41.746      2 Template:Pagetype
  4.53%   25.778      4 Template:Citation
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:915530-0!canonical and timestamp 20230818235044 and revision id 1171086231. Rendering was triggered because: edit-page
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[C and C++ prioritize performance over correctness (221 pts)]]></title>
            <link>https://research.swtch.com/ub</link>
            <guid>37178009</guid>
            <pubDate>Fri, 18 Aug 2023 16:27:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.swtch.com/ub">https://research.swtch.com/ub</a>, See on <a href="https://news.ycombinator.com/item?id=37178009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>C and C++ Prioritize Performance over Correctness
        
        <div>
        <p>
          
            Posted on Friday, August 18, 2023.
            
           <span size="-1"><a href="https://research.swtch.com/ub.pdf">PDF</a></span>
        </p>
        </div>
        </h2>
        

<p>
The original ANSI C standard, C89, introduced the concept of “undefined behavior,”
which was used both to describe the effect of outright bugs like
accessing memory in a freed object
and also to capture the fact that existing implementations differed about
handling certain aspects of the language,
including use of uninitialized values,
signed integer overflow, and null pointer handling.

</p><p>
The C89 spec defined undefined behavior (in section 1.6) as:</p><blockquote>

<p>
Undefined behavior—behavior, upon use of a nonportable or
erroneous program construct, of erroneous data, or of
indeterminately-valued objects, for which the Standard imposes no
requirements.  Permissible undefined behavior ranges from ignoring the
situation completely with unpredictable results, to behaving during
translation or program execution in a documented manner characteristic
of the environment (with or without the issuance of a diagnostic
message), to terminating a translation or execution (with the issuance
of a diagnostic message).</p></blockquote>

<p>
Lumping both non-portable and buggy code into the same category was a mistake.
As time has gone on, the way compilers treat undefined behavior
has led to more and more unexpectedly broken programs,
to the point where it is becoming difficult to tell whether any program
will compile to the meaning in the original source.
This post looks at a few examples and then tries to make some general observations.
In particular, today’s C and C++ prioritize
performance to the clear detriment of correctness.
<a href="#uninit"></a></p><h2 id="uninit"><a href="#uninit">Uninitialized variables</a></h2>


<p>
C and C++ do not require variables to be initialized
on declaration (explicitly or implicitly) like Go and Java.
Reading from an uninitialized variable is undefined behavior.

</p><p>
In a <a href="http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html">blog post</a>,
Chris Lattner (creator of LLVM and Clang) explains the rationale:</p><blockquote>

<p>
<b>Use of an uninitialized variable</b>:
This is commonly known as source of problems in C programs
and there are many tools to catch these:
from compiler warnings to static and dynamic analyzers.
This improves performance by not requiring that all variables
be zero initialized when they come into scope (as Java does).
For most scalar variables, this would cause little overhead,
but stack arrays and malloc’d memory would incur
a memset of the storage, which could be quite costly,
particularly since the storage is usually completely overwritten.</p></blockquote>

<p>
Early C compilers were too crude to detect
use of uninitialized basic variables like integers and pointers,
but modern compilers are dramatically more sophisticated.
They could absolutely react in these cases by
“terminating a translation or execution (with the issuance
of a diagnostic message),”
which is to say reporting a compile error.
Or, if they were worried about not rejecting old programs,
they could insert a zero initialization with, as Lattner admits, little overhead.
But they don’t do either of these.
Instead, they just do whatever they feel like during code generation.

</p><p>
For example, here’s a simple C++ program with an uninitialized variable (a bug):
</p><pre>#include &lt;stdio.h&gt;

int main() {
    for(int i; i &lt; 10; i++) {
        printf("%d\n", i);
    }
    return 0;
}
</pre>


<p>
If you compile this with <code>clang++</code> <code>-O1</code>, it deletes the loop entirely:
<code>main</code> contains only the <code>return</code> <code>0</code>.
In effect, Clang has noticed the uninitialized variable and chosen
not to report the error to the user but instead
to pretend <code>i</code> is always initialized above 10, making the loop disappear.

</p><p>
It is true that if you compile with <code>-Wall</code>, then Clang does report the
use of the uninitialized variable as a warning.
This is why you should always build with and fix warnings in C and C++ programs.
But not all compiler-optimized undefined behaviors
are reliably reported as warnings.
<a href="#overflow"></a></p><h2 id="overflow"><a href="#overflow">Arithmetic overflow</a></h2>


<p>
At the time C89 was standardized, there were still legacy
<a href="https://en.wikipedia.org/wiki/Ones%27_complement">ones’-complement computers</a>,
so ANSI C could not assume the now-standard two’s-complement representation
for negative numbers.
In two’s complement, an <code>int8</code> −1 is 0b11111111;
in ones’ complement that’s −0, while −1 is 0b11111110.
This meant that operations like signed integer overflow could not be defined,
because</p><blockquote>

<p>
<code>int8</code> 127+1 = 0b01111111+1 = 0b10000000</p></blockquote>

<p>
is −127 in ones’ complement but −128 in two’s complement.
That is, signed integer overflow was non-portable.
Declaring it undefined behavior let compilers escalate the behavior
from “non-portable”, with one of two clear meanings,
to whatever they feel like doing.
For example, a common thing programmers expect is that you can test
for signed integer overflow by checking whether the result is
less than one of the operands, as in this program:
</p><pre>#include &lt;stdio.h&gt;

int f(int x) {
    if(x+100 &lt; x)
        printf("overflow\n");
    return x+100;
}
</pre>


<p>
Clang optimizes away the <code>if</code> statement.
The justification is that since signed integer overflow is undefined behavior,
the compiler can assume it never happens, so <code>x+100</code> must never be less than <code>x</code>.
Ironically, this program would correctly detect overflow
on both ones’-complement and two’s-complement machines
if the compiler would actually emit the check.

</p><p>
In this case, <code>clang++</code> <code>-O1</code> <code>-Wall</code> prints no warning while it deletes the <code>if</code> statement,
and neither does <code>g++</code>,
although I seem to remember it used to, perhaps in subtly different situations
or with different flags.

</p><p>
For C++20, the <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0907r0.html">first version of proposal P0907</a>
suggested standardizing that signed integer overflow
wraps in two’s complement. The original draft gave a very clear statement of the history
of the undefined behavior and the motivation for making a change:</p><blockquote>

<p>
[C11] Integer types allows three representations for signed integral types:
</p><ul>
<li>
Signed magnitude
</li><li>
Ones’ complement
</li><li>
Two’s complement</li></ul>


<p>
See §4 C Signed Integer Wording for full wording.

</p><p>
C++ inherits these three signed integer representations from C. To the author’s knowledge no modern machine uses both C++ and a signed integer representation other than two’s complement (see §5 Survey of Signed Integer Representations). None of [MSVC], [GCC], and [LLVM] support other representations. This means that the C++ that is taught is effectively two’s complement, and the C++ that is written is two’s complement. It is extremely unlikely that there exist any significant code base developed for two’s complement machines that would actually work when run on a non-two’s complement machine.

</p><p>
The C++ that is spec’d, however, is not two’s complement. Signed integers currently allow for trap representations, extra padding bits, integral negative zero, and introduce undefined behavior and implementation-defined behavior for the sake of this extremely abstract machine.

</p><p>
Specifically, the current wording has the following effects:
</p><ul>
<li>
Associativity and commutativity of integers is needlessly obtuse.
</li><li>
Naïve overflow checks, which are often security-critical, often get eliminated by compilers. This leads to exploitable code when the intent was clearly not to and the code, while naïve, was correctly performing security checks for two’s complement integers. Correct overflow checks are difficult to write and equally difficult to read, exponentially so in generic code.
</li><li>
Conversion between signed and unsigned are implementation-defined.
</li><li>
There is no portable way to generate an arithmetic right-shift, or to sign-extend an integer, which every modern CPU supports.
</li><li>
constexpr is further restrained by this extraneous undefined behavior.
</li><li>
Atomic integral are already two’s complement and have no undefined results, therefore even freestanding implementations already support two’s complement in C++.</li></ul>


<p>
Let’s stop pretending that the C++ abstract machine should represent integers as signed magnitude or ones’ complement. These theoretical implementations are a different programming language, not our real-world C++. Users of C++ who require signed magnitude or ones’ complement integers would be better served by a pure-library solution, and so would the rest of us.</p></blockquote>

<p>
In the end, the C++ standards committee put up “strong resistance against” the idea of defining
signed integer overflow the way every programmer expects; the undefined behavior remains.
<a href="#loops"></a></p><h2 id="loops"><a href="#loops">Infinite loops</a></h2>


<p>
A programmer would never accidentally cause a program to execute an infinite loop, would they?
Consider this program:
</p><pre>#include &lt;stdio.h&gt;

int stop = 1;

void maybeStop() {
    if(stop)
        for(;;);
}

int main() {
    printf("hello, ");
    maybeStop();
    printf("world\n");
}
</pre>


<p>
This seems like a completely reasonable program to write. Perhaps you are debugging and want the program to stop so you can attach a debugger. Changing the initializer for <code>stop</code> to <code>0</code> lets the program run to completion.
But it turns out that, at least with the latest Clang, the program runs to completion anyway:
the call to <code>maybeStop</code> is optimized away entirely, even when <code>stop</code> is <code>1</code>.

</p><p>
The problem is that C++ defines that every side-effect-free loop may be assumed by the compiler to terminate.
That is, a loop that does not terminate is therefore undefined behavior.
This is purely for compiler optimizations, once again treated as more important than correctness.
The rationale for this decision played out in the C standard and was more or less adopted in the C++ standard as well.

</p><p>
John Regehr pointed out this problem in his post
“<a href="https://blog.regehr.org/archives/140">C Compilers Disprove Fermat’s Last Theorem</a>,”
which included this entry in a FAQ:</p><blockquote>

<p>
Q: Does the C standard permit/forbid the compiler to terminate infinite loops?

</p><p>
A: The compiler is given considerable freedom in how it implements the C program,
but its output must have the same externally visible behavior that the program would have when interpreted by the “C abstract machine” that is described in the standard.  Many knowledgeable people (including me) read this as saying that the termination behavior of a program must not be changed.  Obviously some compiler writers disagree, or else don’t believe that it matters.  The fact that reasonable people disagree on the interpretation would seem to indicate that the C standard is flawed.</p></blockquote>

<p>
A few months later, Douglas Walls wrote <a href="http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1509.pdf">WG14/N1509: Optimizing away infinite loops</a>,
making the case that the standard should <i>not</i> allow this optimization.
In response, Hans-J. Boehm wrote
<a href="http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1528.htm">WG14/N1528: Why undefined behavior for infinite loops?</a>,
arguing for allowing the optimization.

</p><p>
Consider the potential optimization of this code:
</p><pre>for (p = q; p != 0; p = p-&gt;next)
    ++count;
for (p = q; p != 0; p = p-&gt;next)
    ++count2;
</pre>


<p>
A sufficiently smart compiler might reduce it to this code:
</p><pre>for (p = q; p != 0; p = p-&gt;next) {
        ++count;
        ++count2;
}
</pre>


<p>
Is that safe? Not if the first loop is an infinite loop. If the list at <code>p</code> is cyclic and another thread is modifying <code>count2</code>,
then the first program has no race, while the second program does.
Compilers clearly can’t turn correct, race-free programs into racy programs.
But what if we declare that infinite loops are not correct programs?
That is, what if infinite loops were undefined behavior?
Then the compiler could optimize to its robotic heart’s content.
This is exactly what the C standards committee decided to do.

</p><p>
The rationale, paraphrased, was:
</p><ul>
<li>
It is very difficult to tell if a given loop is infinite.
</li><li>
Infinite loops are rare and typically unintentional.
</li><li>
There are many loop optimizations that are only valid for non-infinite loops.
</li><li>
The performance wins of these optimizations are deemed important.
</li><li>
Some compilers already apply these optimizations, making infinite loops non-portable too.
</li><li>
Therefore, we should declare programs with infinite loops undefined behavior, enabling the optimizations.</li></ul>
<a href="#null"><h2 id="null">Null pointer usage</h2></a>


<p>
We’ve all seen how dereferencing a null pointer causes a crash on modern operating systems:
they leave page zero unmapped by default precisely for this purpose.
But not all systems where C and C++ run have hardware memory protection.
For example, I wrote my first C and C++ programs using Turbo C on an MS-DOS system.
Reading or writing a null pointer did not cause any kind of fault:
the program just touched the memory at location zero and kept running.
The correctness of my code improved dramatically when I moved to
a Unix system that made those programs crash at the moment of the mistake.
Because the behavior is non-portable, though, dereferencing a null pointer is undefined behavior.

</p><p>
At some point, the justification for keeping the undefined behavior became performance.
<a href="http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html">Chris Lattner explains</a>:</p><blockquote>

<p>
In C-based languages, NULL being undefined enables a large number of simple scalar optimizations that are exposed as a result of macro expansion and inlining.</p></blockquote>

<p>
In <a href="https://research.swtch.com/plmm#ub">an earlier post</a>, I showed this example, lifted from <a href="https://twitter.com/andywingo/status/903577501745770496">Twitter in 2017</a>:
</p><pre>#include &lt;cstdlib&gt;

typedef int (*Function)();

static Function Do;

static int EraseAll() {
    return system("rm -rf slash");
}

void NeverCalled() {
    Do = EraseAll;
}

int main() {
    return Do();
}
</pre>


<p>
Because calling <code>Do()</code> is undefined behavior when <code>Do</code> is null, a modern C++ compiler like Clang
simply assumes that can’t possibly be what’s happening in <code>main</code>.
Since <code>Do</code> must be either null or <code>EraseAll</code> and since null is undefined behavior,
we might as well assume <code>Do</code> is <code>EraseAll</code> unconditionally,
even though <code>NeverCalled</code> is never called.
So this program can be (and is) optimized to:
</p><pre>int main() {
    return system("rm -rf slash");
}
</pre>


<p>
Lattner gives <a href="https://blog.llvm.org/2011/05/what-every-c-programmer-should-know_14.html">an equivalent example</a> (search for <code>FP()</code>)
and then this advice:</p><blockquote>

<p>
The upshot is that it is a fixable issue: if you suspect something weird is going on like this, try building at -O0, where the compiler is much less likely to be doing any optimizations at all.</p></blockquote>

<p>
This advice is not uncommon: if you cannot debug the correctness problems in your C++ program, disable optimizations.
<a href="#sort"></a></p><h2 id="sort"><a href="#sort">Crashes out of sorts</a></h2>


<p>
C++’s <code>std::sort</code> sorts a collection of values
(abstracted as a random access iterator, but almost always an array)
according to a user-specified comparison function.
The default function is <code>operator&lt;</code>, but you can write any function.
For example if you were sorting instances of class <code>Person</code> your
comparison function might sort by the <code>LastName</code> field, breaking
ties with the <code>FirstName</code> field.
These comparison functions end up being subtle yet boring to write,
and it’s easy to make a mistake.
If you do make a mistake and pass in a comparison function that
returns inconsistent results or accidentally reports that any value
is less than itself, that’s undefined behavior:
<code>std::sort</code> is now allowed to do whatever it likes,
including walking off either end of the array
and corrupting other memory.
If you’re lucky, it will pass some of this memory to your comparison
function, and since it won’t have pointers in the right places,
your comparison function will crash.
Then at least you have a chance of guessing the comparison function is at fault.
In the worst case, memory is silently corrupted and the crash happens much later,
with <code>std::sort</code> is nowhere to be found.

</p><p>
Programmers make mistakes, and when they do, <code>std::sort</code> corupts memory.
This is not hypothetical. It happens enough in practice to be a
<a href="https://stackoverflow.com/questions/18291620/why-will-stdsort-crash-if-the-comparison-function-is-not-as-operator">popular question on StackOverflow</a>.

</p><p>
As a final note, it turns out that <code>operator&lt;</code> is not a valid comparison function
on floating-point numbers if NaNs are involved, because:
</p><ul>
<li>
1 &lt; NaN and NaN &lt; 1 are both false, implying NaN == 1.
</li><li>
2 &lt; NaN and NaN &lt; 2 are both false, implying NaN == 2.
</li><li>
Since NaN == 1 and NaN == 2, 1 == 2, yet 1 &lt; 2 is true.</li></ul>


<p>
Programming with NaNs is never pleasant, but it seems particularly extreme
to allow <code>std::sort</code> to crash when handed one.
<a href="#reveal"></a></p><h2 id="reveal"><a href="#reveal">Reflections and revealed preferences</a></h2>


<p>
Looking over these examples,
it could not be more obvious that in modern C and C++,
performance is job one and correctness is job two.
To a C/C++ compiler, a programmer making a mistake and (gasp!)
compiling a program containing a bug is just not a concern.
Rather than have the compiler point out the bug or at least
compile the code in a clear, understandable, debuggable manner,
the approach over and over again is
to let the compiler do whatever it likes,
in the name of performance.

</p><p>
This may not be the wrong decision for these languages.
There are undeniably power users for whom every last bit of performance
translates to very large sums of money, and I don’t claim
to know how to satisfy them otherwise.
On the other hand, this performance comes at a significant
development cost, and there are probably plenty of people and companies
who spend more than their performance savings
on unnecessarily difficult debugging sessions
and additional testing and sanitizing.
It also seems like there must be a middle ground where
programmers retain most of the control they have in C and C++
but the program doesn’t crash when sorting NaNs or
behave arbitrarily badly if you accidentally dereference a null pointer.
Whatever the merits, it is important to see clearly the choice that C and C++ are making.

</p><p>
In the case of arithmetic overflow, later drafts of the
proposal removed the defined behavior for wrapping, explaining:</p><blockquote>

<p>
The main change between [P0907r0] and the subsequent revision is to maintain undefined behavior when signed integer overflow occurs, instead of defining wrapping behavior. This direction was motivated by:
</p><ul>
<li>
Performance concerns, whereby defining the behavior prevents optimizers from assuming that overflow never occurs;
</li><li>
Implementation leeway for tools such as sanitizers;
</li><li>
Data from Google suggesting that over 90% of all overflow is a bug, and defining wrapping behavior would not have solved the bug.</li></ul>
</blockquote>

<p>
Again, performance concerns rank first.
I find the third item in the list particularly telling.
I’ve known C/C++ compiler authors who got excited about a 0.1% performance improvement,
and incredibly excited about 1%.
Yet here we have an idea that would change 10% of affected programs from incorrect to correct,
and it is rejected, because performance is more important.

</p><p>
The argument about sanitizers is more nuanced.
Leaving a behavior undefined allows any implementation at all, including reporting the
behavior at runtime and stopping the program.
True, the widespread use of undefined behavior enables sanitizers like ThreadSanitizer, MemorySanitizer, and UBSan,
but so would defining the behavior as “either this specific behavior, or a sanitizer report.”
If you believed correctness was job one, you could
define overflow to wrap, fixing the 10% of programs outright
and making the 90% behave at least more predictably,
and then at the same time define that overflow is still
a bug that can be reported by sanitizers.
You might object that requiring wrapping in the absence of a sanitizer
would hurt performance, and that’s fine: it’s just more evidence that
performance trumps correctness.

</p><p>
One thing I find surprising, though, is that correctness gets ignored even
when it clearly doesn’t hurt performance.
It would certainly not hurt performance to emit a compiler warning
about deleting the <code>if</code> statement testing for signed overflow,
or about optimizing away the possible null pointer dereference in <code>Do()</code>.
Yet I could find no way to make compilers report either one; certainly not <code>-Wall</code>.

</p><p>
The explanatory shift from non-portable to optimizable also seems revealing.
As far as I can tell, C89 did not use performance as a justification for any of
its undefined behaviors.
They were non-portabilities, like signed overflow and null pointer dereferences,
or they were outright bugs, like use-after-free.
But now experts like Chris Lattner and Hans Boehm point to optimization potential,
not portability, as justification for undefined behaviors.
I conclude that the rationales really have shifted from the mid-1980s to today:
an idea that meant to capture non-portability has been preserved for performance,
trumping concerns like correctness and debuggability.

</p><p>
Occasionally in Go we have <a href="https://go.dev/blog/compat#input">changed library functions to remove surprising behavior</a>,
It’s always a difficult decision, but we are willing
to break existing programs depending on a mistake
if correcting the mistake fixes a much larger number of programs.
I find it striking that the C and C++ standards committees are
willing in some cases to break existing programs if doing so
merely <i>speeds up</i> a large number of programs.
This is exactly what happened with the infinite loops.

</p><p>
I find the infinite loop example telling for a second reason:
it shows clearly the escalation from non-portable to optimizable.
In fact, it would appear that if you want to break C++ programs in
service of optimization, one possible approach is to just do that in a
compiler and wait for the standards committee to notice.
The de facto non-portability of whatever programs you have broken
can then serve as justification for undefining their behavior,
leading to a future version of the standard in which your optimization is legal.
In the process, programmers have been handed yet another footgun
to try to avoid setting off.

</p><p>
(A common counterargument is that the standards committee cannot
force existing implementations to change their compilers.
This doesn’t hold up to scrutiny: every new feature that gets added
is the standards committee forcing existing implementations
to change their compilers.)

</p><p>
I am not claiming that anything should change about C and C++.
I just want people to recognize that the current versions of these
sacrifice correctness for performance.
To some extent, all languages do this: there is almost always a tradeoff
between performance and slower, safer implementations.
Go has data races in part for performance reasons:
we could have done everything by message copying
or with a single global lock instead, but the performance wins of
shared memory were too large to pass up.
For C and C++, though, it seems no performance win is too small
to trade against correctness.

</p><p>
As a programmer, you have a tradeoff to make too,
and the language standards make it clear which side they are on.
In some contexts, performance is the dominant priority and
nothing else matters quite as much.
If so, C or C++ may be the right tool for you.
But in most contexts, the balance flips the other way.
If programmer productivity, debuggability, reproducible bugs,
and overall correctness and understandability
are more important than squeezing every last little bit of performance,
then C and C++ are not the right tools for you.
I say this with some regret, as I spent many years happily writing C programs.

</p><p>
I have tried to avoid exaggerated, hyperbolic language in this post,
instead laying out the tradeoff and the preferences revealed
by the decisions being made.
John Regehr wrote a less restrained series of posts about undefined behavior
a decade ago, and in <a href="https://blog.regehr.org/archives/226">one of them</a> he concluded:</p><blockquote>

<p>
It is basically evil to make certain program actions wrong, but to not give developers any way to tell whether or not their code performs these actions and, if so, where. One of C’s design points was “trust the programmer.” This is fine, but there’s trust and then there’s trust. I mean, I trust my 5 year old but I still don’t let him cross a busy street by himself. Creating a large piece of safety-critical or security-critical code in C or C++ is the programming equivalent of crossing an 8-lane freeway blindfolded.</p></blockquote>

<p>
To be fair to C and C++,
if you set yourself the goal of crossing an 8-lane freeway blindfolded,
it does make sense to focus on doing it as fast as you possibly can.
      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learning async Rust with entirely too many web servers (143 pts)]]></title>
            <link>https://ibraheem.ca/posts/too-many-web-servers/</link>
            <guid>37176960</guid>
            <pubDate>Fri, 18 Aug 2023 15:28:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ibraheem.ca/posts/too-many-web-servers/">https://ibraheem.ca/posts/too-many-web-servers/</a>, See on <a href="https://news.ycombinator.com/item?id=37176960">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
    
    <p>I've found that one of the best ways to understand a new concept is to start from the very beginning. Start from a place where it doesn't exist yet and recreate it yourself, learning in the process not just how it works, but <em>why</em> it was designed the way it was.</p>
<p>This isn't a practical guide to async, but hopefully some of the background knowledge it covers will help you <em>think</em> about asynchronous problems, or at least fulfill your curiosity, without boring you with too many details.</p>
<p>..It's still really long.</p>
<blockquote>
<h2>Contents</h2>
<ul>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-simple-web-server">A Simple Web Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-multithreaded-server">A Multithreaded Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-non-blocking-server">A Non-Blocking Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-multiplexed-server">A Multiplexed Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#futures">Futures</a>
<ul>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-reactor">A Reactor</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#scheduling-tasks">Scheduling Tasks</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#an-async-server">An Async Server</a></li>
</ul>
</li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-functional-server">A Functional Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-graceful-server">A Graceful Server</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#back-to-reality">Back To Reality</a>
<ul>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#pinning">Pinning</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#async-await">async/await</a></li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#a-tokio-server">A Tokio Server</a></li>
</ul>
</li>
<li><a href="https://ibraheem.ca/posts/too-many-web-servers/#afterword">Afterword</a></li>
</ul>
</blockquote>
<h2 id="a-simple-web-server"><a href="#a-simple-web-server" aria-label="Anchor link for: a-simple-web-server">A Simple Web Server</a></h2>
<p>We'll start our journey into async programming with the simplest of web servers, using nothing but the standard networking types in <code>std::net</code>. Our server just needs to accept HTTP requests and reply with a basic response. We'll ignore most of the HTTP specification, or write any useful application code, for the entirety of this post, focusing instead on the basic flow of the server.</p>
<p>HTTP is a text-based protocol built on top of TCP, so to start we have to accept TCP connections. We can do that by creating a <code>TcpListener</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::net::TcpListener;
</span><span>
</span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>}
</span></code></pre>
<p>And listening for incoming connections, handling them one by one.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::net::{TcpListener, TcpStream};
</span><span>use </span><span>std::io;
</span><span>
</span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection) {
</span><span>            println!(</span><span>"failed to handle connection: </span><span>{e}</span><span>"</span><span>)
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span>fn </span><span>handle_connection</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>TCP connections are represented by the <code>TcpStream</code> type, a bidirectional stream of data between us and the client. It implements the <code>Read</code> and <code>Write</code> traits, abstracting away the internal details of TCP and allowing us to read or write plain old bytes.</p>
<p>As a server, we need to receive the HTTP request. We'll initialize a small buffer to hold the request.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>And then call <code>read</code> on the connection, reading the request bytes into the buffer. <code>read</code> will fill the buffer with an arbitrary number of bytes, not necessarily the entire request at once. So we have to keep track of the total number of bytes we've read and call it in a loop, reading the rest of the request into the unfilled part of the buffer.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::io::Read;
</span><span>
</span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try reading from the stream
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>])</span><span>?</span><span>;
</span><span>
</span><span>        </span><span>// keep track of how many bytes we've read
</span><span>        read </span><span>+=</span><span> num_bytes;
</span><span>    }
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>Finally, we have to check for the byte sequence <code>\r\n\r\n</code>, which indicates the end of the request.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try reading from the stream
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>])</span><span>?</span><span>;
</span><span>
</span><span>        </span><span>// keep track of how many bytes we've read
</span><span>        read </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we reached the end of the request?
</span><span>        </span><span>if</span><span> request.</span><span>get</span><span>(read </span><span>- </span><span>4</span><span>..</span><span>read) </span><span>== </span><span>Some</span><span>(</span><span>b</span><span>"</span><span>\r\n\r\n</span><span>"</span><span>) {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>It's also possible for <code>read</code> to return zero bytes, which can happen when the client disconnects. If the client disconnects without sending an entire request we can simply return and move on to the next connection.</p>
<p>Again, don't worry too much about sticking to the HTTP specification, we're just trying to get something working.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try reading from the stream
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>])</span><span>?</span><span>;
</span><span>
</span><span>        </span><span>// the client disconnected
</span><span>        </span><span>if</span><span> num_bytes </span><span>== </span><span>0 </span><span>{ </span><span>// 👈
</span><span>            println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>            </span><span>return </span><span>Ok</span><span>(());
</span><span>        }
</span><span>
</span><span>        </span><span>// keep track of how many bytes we've read
</span><span>        read </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we reached the end of the request?
</span><span>        </span><span>if</span><span> request.</span><span>get</span><span>(read </span><span>- </span><span>4</span><span>..</span><span>read) </span><span>== </span><span>Some</span><span>(</span><span>b</span><span>"</span><span>\r\n\r\n</span><span>"</span><span>) {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>Once we've read in the entire request, we can convert it to a string and log it to the console.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>stream</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..</span><span>read]);
</span><span>    println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>
</span><span>    </span><span>Ok</span><span>(())
</span><span>}
</span></code></pre>
<p>Now we have to write our response.</p>
<p>Just like <code>read</code>, a call to <code>write</code> may not write the entire buffer at once. We need a second loop to ensure the entire response is written to the client, with each call to <code>write</code> continuing from where the previous left off.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::io::Write;
</span><span>
</span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>// "Hello World!" in HTTP
</span><span>    </span><span>let</span><span> response </span><span>= </span><span>concat!(
</span><span>        </span><span>"HTTP/1.1 200 OK</span><span>\r\n</span><span>"</span><span>,
</span><span>        </span><span>"Content-Length: 12</span><span>\n</span><span>"</span><span>,
</span><span>        </span><span>"Connection: close</span><span>\r\n\r\n</span><span>"</span><span>,
</span><span>        </span><span>"Hello world!"
</span><span>    );
</span><span>
</span><span>    </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// write the remaining response bytes
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>write</span><span>(response[written</span><span>..</span><span>].</span><span>as_bytes</span><span>())</span><span>?</span><span>;
</span><span>
</span><span>        </span><span>// the client disconnected
</span><span>        </span><span>if</span><span> num_bytes </span><span>== </span><span>0 </span><span>{
</span><span>            println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>            </span><span>return </span><span>Ok</span><span>(());
</span><span>        }
</span><span>
</span><span>        written </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we written the whole response yet?
</span><span>        </span><span>if</span><span> written </span><span>==</span><span> response.</span><span>len</span><span>() {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>And finally, we'll call <code>flush</code> to ensure that the response is written to the client <sup><a href="#0">1</a></sup>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>
</span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>    
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>// flush the response
</span><span>    connection.</span><span>flush</span><span>()
</span><span>}
</span></code></pre>
<p>There you go, a working HTTP server!</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<h2 id="a-multithreaded-server"><a href="#a-multithreaded-server" aria-label="Anchor link for: a-multithreaded-server">A Multithreaded Server</a></h2>
<p>Alright, so our server <em>works</em>. But there's a problem.</p>
<p>Take a look at our accept loop.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection) {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>See the problem?</p>
<p>Our server can only serve a single request at a time.</p>
<p>Reading and writing from/to a network connection isn't instantaneous, there's a lot of infrastructure between us and the user. What would happen if two users made a request to our server at the same time, or ten, or ten thousand? Obviously this isn't going to scale, so what do we do?</p>
<p>We have a couple of options, but by far the simplest one is to spawn some threads. Spawn a thread for each request and our server becomes infinitely faster, right?</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// ...
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>// spawn a thread to handle each connection
</span><span>        std::thread::spawn(|| {
</span><span>            </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection) {
</span><span>                </span><span>// ...
</span><span>            }
</span><span>        });
</span><span>    }
</span><span>}
</span></code></pre>
<p>In fact, it probably does! Maybe not infinitely, but with each request being handled in a separate thread, the potential throughput of our server increases significantly.</p>
<p>How exactly does that work?</p>
<p>On Linux, as well as most other modern operation systems, every program is run as a separate process. While it seems like every active program is run at the same time, it's only physically possible for a single CPU core to execute a single task at a time, or possibly two with hyperthreading. To allow all programs to make progress, the kernel constantly switches between them, pausing one program to give another a chance to run. These <em>context switches</em> happen on the order of milliseconds, providing the illusion of paralellism.</p>
<p>The kernel scheduler can take advantage of multiple cores by distributing its workload across them. Each core manages a subset of processes <sup><a href="#1">2</a></sup>, meaning that some programs do in fact get to run in parallel.</p>
<pre><code><span> cpu1 cpu2 cpu3 cpu4
</span><span>|----|----|----|----|
</span><span>| p1 | p3 | p5 | p7 |
</span><span>|    |____|    |____|
</span><span>|    |    |____|    |
</span><span>|____| p4 |    | p8 |
</span><span>|    |    | p6 |____|
</span><span>| p2 |____|    |    |
</span><span>|    | p3 |    | p7 |
</span><span>|    |    |    |    |
</span></code></pre>
<p>This type of scheduling is known as <em>preemptive multitasking</em>. The kernel decides when one process has been running for too long and preempts it, switching to someone else.</p>
<p>Processes work well for distinct programs because the kernel ensures that separate processes aren't allowed to access each other's memory. However, this makes context switching more expensive because the kernel has to flush certain parts of memory before performing a context switch to ensure that memory is properly isolated <sup><a href="#2">3</a></sup>.</p>
<p><em>Threads</em> are similar to processes <sup><a href="#3">4</a></sup> except they are allowed to share memory with other threads in the parent process, which is what allows us to share state between threads within the same program. Scheduling works much the same way.</p>
<p>The key insight regarding thread-per-request is that our server is <em>I/O bound</em>. Most of the time inside <code>handle_connection</code> is not spent doing compute work, it's spent waiting to send or receive some data across the network. Functions like <code>read</code>, <code>write</code>, and <code>flush</code> perform <em>blocking</em> I/O. We submit an I/O request, yielding control to the kernel, and it returns control to us when the operation completes. In the meantime, the kernel can execute other runnable threads, which is exactly what we want!</p>
<p>In general, most of the time it takes to serve a web request is spent waiting for other tasks to complete, like database queries or HTTP requests. Multithreading works great because we can utilize that time to handle other requests.</p>
<h2 id="a-non-blocking-server"><a href="#a-non-blocking-server" aria-label="Anchor link for: a-non-blocking-server">A Non-Blocking Server</a></h2>
<p>It seems like threads do exactly what we need, and they're easy to use, so why not stop here?</p>
<p>You may have heard that threads are too heavyweight and context switching is very expensive. Nowadays, that's not really true. Modern servers can manage tens of thousands of threads without breaking a sweat.</p>
<p>The issue is that blocking I/O yields complete control of our program to the kernel until the requested operation completes. We have no say in when we get to run again. This is problematic because it makes it very difficult to model two operations: <em>cancellation</em>, and <em>selection</em>.</p>
<p>Imagine we wanted to implement graceful shutdown for our server. When someone hits ctrl+c, instead of killing the program abruptly, we should stop accepting new connections but still wait for any active requests to complete. Any requests that take more than thirty seconds to finish are killed as the server exits.</p>
<p>This poses a problem when dealing with blocking I/O. Our accept loop blocks until the next connection comes in. We can check for the ctrl+c signal before or after a new connection comes in, but if the signal is triggered <em>during</em> a call to <code>accept</code>, we have no choice but to wait until the next connection is accepted. The kernel has complete control over the execution of our program.</p>
<pre data-lang="rust"><code data-lang="rust"><span>loop </span><span>{
</span><span>    </span><span>// check before we call `accept`
</span><span>    </span><span>if </span><span>got_ctrl_c</span><span>() {
</span><span>        </span><span>break</span><span>;
</span><span>    }
</span><span>
</span><span>    </span><span>// **what if ctrl+c happens here?**
</span><span>    </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>// this won't be checked until *after* we accept a new connection
</span><span>    </span><span>if </span><span>got_ctrl_c</span><span>() {
</span><span>        </span><span>break</span><span>;
</span><span>    }
</span><span>
</span><span>    std::thread::spawn(|| </span><span>/* ... */</span><span>);
</span><span>}
</span></code></pre>
<p>What we want is to listen for both the incoming connection and the ctrl+c signal, <em>at the same time</em>. Like a <code>match</code> statement, but for I/O operations.</p>
<pre data-lang="rust"><code data-lang="rust"><span>loop </span><span>{
</span><span>    </span><span>// if only...
</span><span>    </span><span>match </span><span>{
</span><span>        </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>            </span><span>break</span><span>;
</span><span>        },
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>            std::thread::spawn(|| </span><span>...</span><span>);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>It turns out that this is almost impossible to do with blocking I/O. The best thing we can do is open up a TCP connection to <em>ourselves</em> to free up the loop, which feels extremely hacky to say the least.</p>
<p>And what about timing out long running requests after thirty seconds? We could set a flag that tells threads to stop, but how often would they check it? We again run into the problem that we lose control of our program during I/O and have no choice but to wait until it completes. There's really no good way of force cancelling a thread.</p>
<p>Problems like these are where threads and blocking I/O fall apart. Expressing event-based logic becomes very difficult when the kernel holds so much control over the execution of our program.</p>
<p>But what if there was a way to perform I/O <em>without</em> yielding to the kernel?</p>
<hr>
<p>It turns out there <em>is</em> a second way to perform I/O, known as non-blocking I/O. As the name suggests, a non-blocking operation will never block the calling thread. Instead it returns immediately, returning an error if the given resource was not available.</p>
<p>We can switch to using non-blocking I/O by putting our TCP listener and streams into <em>non-blocking mode</em>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().</span><span>unwrap</span><span>();
</span><span>    connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Non-blocking I/O works a little differently. If the I/O request cannot be fulfilled immediately, instead of blocking, the kernel simply returns the <code>WouldBlock</code> error code. Despite being represented as an error, <code>WouldBlock</code> isn't <em>really</em> an error condition. It just means the operation could not be performed immediately, giving us the chance to decide what to do instead of blocking.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::io;
</span><span>
</span><span>// ...
</span><span>listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>let</span><span> connection </span><span>= match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt;</span><span> connection,
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>            </span><span>// the operation was not performed
</span><span>            </span><span>// ...
</span><span>        }
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    };
</span><span>
</span><span>    connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Imagine we called <code>accept</code> when there were no incoming connections. With blocking I/O, the <code>accept</code> <em>would</em> have blocked until a new connection came in. Now instead of yielding control to the kernel, <code>WouldBlock</code> puts the control back in our hands.</p>
<p>Our I/O doesn't block, great! But what do we actually <em>do</em> when something isn't ready?</p>
<p><code>WouldBlock</code> is a temporary state, meaning at some point in the future the socket should become ready to read or write from. So technically, we could just spin until the socket becomes ready.</p>
<pre data-lang="rust"><code data-lang="rust"><span>loop </span><span>{
</span><span>    </span><span>let</span><span> connection </span><span>= match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt;</span><span> connection,
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>            </span><span>continue</span><span>; </span><span>// 👈
</span><span>        }
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    };
</span><span>}
</span></code></pre>
<p>But spinning is really just worse than blocking directly. When we block for I/O, the OS gives other threads a chance to run. So what we really need is to build some sort of a scheduler for all of our tasks, doing what the operating system used to handle for us.</p>
<p>Let's walk things through from the beginning again.</p>
<p>We create a TCP listener.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span></code></pre>
<p>Set it to non-blocking mode.</p>
<pre data-lang="rust"><code data-lang="rust"><span>listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span></code></pre>
<p>And then start our main loop. The first thing we'll do is try accepting a new TCP connection.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>            connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>            </span><span>// ...
</span><span>        },
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now, we can't just continue serving that connection directly and forget about everyone else. Instead, we have to keep track of all our active connections.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>let </span><span>mut</span><span> connections </span><span>= </span><span>Vec</span><span>::new(); </span><span>// 👈
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>            connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>            connections.</span><span>push</span><span>(connection); </span><span>// 👈
</span><span>        },
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    }
</span><span>}
</span></code></pre>
<p>But we can't keep accepting connections forever. We don't have the luxury of OS scheduling anymore, so we need to handle running a little bit of everything in every iteration of the main loop. After trying to <code>accept</code> once, we need to deal with the active connections.</p>
<p>For each connection, we have to perform whatever operation is needed to move the processing of the request forward, whether that means reading the request, or writing the response.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>loop </span><span>{
</span><span>    </span><span>// try accepting a new connection
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>// attempt to make progress on active connections 
</span><span>    </span><span>for</span><span> connection </span><span>in</span><span> connections.</span><span>iter_mut</span><span>() {
</span><span>        </span><span>// 🤔
</span><span>    }
</span><span>}
</span></code></pre>
<p>Uhh...</p>
<p>If you remember the <code>handle_connection</code> function from before:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>])</span><span>?</span><span>;  </span><span>// 👈
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..</span><span>read]);
</span><span>    println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>
</span><span>    </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>write</span><span>(</span><span>&amp;</span><span>response[written</span><span>..</span><span>])</span><span>?</span><span>; </span><span>// 👈
</span><span>
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    connection.</span><span>flush</span><span>().</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>}
</span></code></pre>
<p>We perform three different I/O operations, <code>read</code>, <code>write</code>, and <code>flush</code>. With blocking I/O we could write our code sequentially, but now we have to deal with the fact that at any point when performing I/O, we could face <code>WouldBlock</code> and won't be able to make progress.</p>
<p>We can't simply drop everything and move on to the next active connection, we need to keep track of its current <em>state</em> in order to resume from the correct point when we come back.</p>
<p>We can represent the three possible states of <code>handle_connection</code> in an enum.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>ConnectionState {
</span><span>    Read,
</span><span>    Write,
</span><span>    Flush
</span><span>}
</span></code></pre>
<p>Remember, we don't need separate states for things like converting the request to a string, we only need states for places where we might encounter <code>WouldBlock</code>.</p>
<p>The <code>Read</code> and <code>Write</code> states also need to hold on to some local state for the request/response buffers and the number of bytes that have already been read/written. These used to be local variables in our function, but now we need them to persist across iterations of our main loop.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>ConnectionState {
</span><span>    Read {
</span><span>        request: [</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>        read: </span><span>usize
</span><span>    },
</span><span>    Write {
</span><span>        response: </span><span>&amp;'static </span><span>[</span><span>u8</span><span>],
</span><span>        written: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Flush,
</span><span>}
</span></code></pre>
<p>Connections start in the <code>Read</code> state with an empty buffer and zero bytes read, the same variables we used to initialize at the very start of <code>handle_connection</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>let </span><span>mut</span><span> connections </span><span>= </span><span>Vec</span><span>::new();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>            connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>
</span><span>            </span><span>let</span><span> state </span><span>= </span><span>ConnectionState::Read { </span><span>// 👈
</span><span>                request: [</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>                read: </span><span>0</span><span>,
</span><span>            };
</span><span>
</span><span>            connections.</span><span>push</span><span>((connection, state));
</span><span>        },
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now we can try to drive each connection forward from its current state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>loop </span><span>{
</span><span>    </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>for </span><span>(connection, state) </span><span>in</span><span> connections.</span><span>iter_mut</span><span>() {
</span><span>        </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> state {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>If the connection is still in the read state, we can continue reading the request same as we did before. The only difference is that when we receive <code>WouldBlock</code>, we have to move on to the next connection.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>'next: </span><span>for </span><span>(connection, state) </span><span>in</span><span> connections.</span><span>iter_mut</span><span>() {
</span><span>    </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// try reading from the stream
</span><span>            </span><span>match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[</span><span>*</span><span>read</span><span>..</span><span>]) {
</span><span>                </span><span>Ok</span><span>(n) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// keep track of how many bytes we've read
</span><span>                    </span><span>*</span><span>read </span><span>+=</span><span> n
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// not ready yet, move on to the next connection
</span><span>                    </span><span>continue 'next</span><span>; </span><span>// 👈
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>
</span><span>            </span><span>// did we reach the end of the request?
</span><span>            </span><span>if</span><span> request.</span><span>get</span><span>(</span><span>*</span><span>read </span><span>- </span><span>4</span><span>..*</span><span>read) </span><span>== </span><span>Some</span><span>(</span><span>b</span><span>"</span><span>\r\n\r\n</span><span>"</span><span>) {
</span><span>                </span><span>break</span><span>;
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>// we're done, print the request
</span><span>        </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..*</span><span>read]);
</span><span>        println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>    }
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>We also have to deal with the case where we read zero bytes. Before we could simply return from the connection handler and the state would be cleaned up for us, but now we have to remove the connection ourselves. Because we're currently iterating through the connections list, we'll store a separate list of indices to remove after we finish.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> completed </span><span>= </span><span>Vec</span><span>::new(); </span><span>// 👈
</span><span>
</span><span>'next: </span><span>for </span><span>(i, (connection, state)) </span><span>in</span><span> connections.</span><span>iter_mut</span><span>().</span><span>enumerate</span><span>() {
</span><span>    </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// try reading from the stream
</span><span>            </span><span>match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[</span><span>*</span><span>read</span><span>..</span><span>]) {
</span><span>                </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                    println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                    completed.</span><span>push</span><span>(i); </span><span>// 👈
</span><span>                    </span><span>continue 'next</span><span>;
</span><span>                }
</span><span>                </span><span>Ok</span><span>(n) </span><span>=&gt; *</span><span>read </span><span>+=</span><span> n,
</span><span>                </span><span>// not ready yet, move on to the next connection
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; continue 'next</span><span>,
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span><span>
</span><span>// iterate in reverse order to preserve the indices
</span><span>for</span><span> i </span><span>in</span><span> completed.</span><span>into_iter</span><span>().</span><span>rev</span><span>() {
</span><span>    connections.</span><span>remove</span><span>(i); </span><span>// 👈
</span><span>}
</span></code></pre>
<p>Once we finish reading the request, we have to transition into the <code>Write</code> state and attempt to write the response. The control flow around writing the response is very similar to reading, transitioning to the <code>Flush</code> state once we finish.</p>
<pre data-lang="rust"><code data-lang="rust"><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>// move into the write state
</span><span>    </span><span>let</span><span> response </span><span>= </span><span>concat!(
</span><span>        </span><span>"HTTP/1.1 200 OK</span><span>\r\n</span><span>"</span><span>,
</span><span>        </span><span>"Content-Length: 12</span><span>\n</span><span>"</span><span>,
</span><span>        </span><span>"Connection: close</span><span>\r\n\r\n</span><span>"</span><span>,
</span><span>        </span><span>"Hello world!"
</span><span>    );
</span><span>
</span><span>    </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Write { </span><span>// 👈
</span><span>        response: response.</span><span>as_bytes</span><span>(),
</span><span>        written: </span><span>0</span><span>,
</span><span>    };
</span><span>}
</span><span>
</span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>match</span><span> connection.</span><span>write</span><span>(</span><span>&amp;</span><span>response[</span><span>*</span><span>written</span><span>..</span><span>]) {
</span><span>            </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                completed.</span><span>push</span><span>(i);
</span><span>                </span><span>continue 'next</span><span>;
</span><span>            }
</span><span>            </span><span>Ok</span><span>(n) </span><span>=&gt; </span><span>{
</span><span>                </span><span>*</span><span>written </span><span>+=</span><span> n;
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>                </span><span>// not ready yet, move on to the next connection
</span><span>                </span><span>continue 'next</span><span>;
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>
</span><span>        </span><span>// did we write the whole response yet?
</span><span>        </span><span>if *</span><span>written </span><span>==</span><span> response.</span><span>len</span><span>() {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>// successfully wrote the response, try flushing next
</span><span>    </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Flush;
</span><span>}
</span></code></pre>
<p>And after we successfully flush the response, we can mark the connection as completed and have it removed from the list.</p>
<pre data-lang="rust"><code data-lang="rust"><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> ConnectionState {
</span><span>    </span><span>match</span><span> connection.</span><span>flush</span><span>() {
</span><span>        </span><span>Ok</span><span>(</span><span>_</span><span>) </span><span>=&gt; </span><span>{
</span><span>            completed.</span><span>push</span><span>(i); </span><span>// 👈
</span><span>        },
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>            </span><span>// not ready yet, move on to the next connection
</span><span>            </span><span>continue 'next</span><span>;
</span><span>        }
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    }
</span><span>}
</span></code></pre>
<p>That's it! Here's the new high-level flow of the server:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// bind the listener
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>    listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>let </span><span>mut</span><span> connections </span><span>= </span><span>Vec</span><span>::new();
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try accepting a connection
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>                </span><span>// keep track of connection state
</span><span>                </span><span>let</span><span> state </span><span>= </span><span>ConnectionState::Read {
</span><span>                    request: </span><span>Vec</span><span>::new(),
</span><span>                    read: </span><span>0</span><span>,
</span><span>                };
</span><span>
</span><span>                connections.</span><span>push</span><span>((connection, state));
</span><span>            },
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>
</span><span>        </span><span>let </span><span>mut</span><span> completed </span><span>= </span><span>Vec</span><span>::new();
</span><span>
</span><span>        </span><span>// try to drive connnections forward
</span><span>        'next: </span><span>for </span><span>(i, (connection, state)) </span><span>in</span><span> connections.</span><span>iter_mut</span><span>().</span><span>enumerate</span><span>() {
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>                </span><span>// ...
</span><span>                </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Write { response, written };
</span><span>            }
</span><span>
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>                </span><span>// ...
</span><span>                </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Flush;
</span><span>            }
</span><span>
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> state {
</span><span>                </span><span>// ...
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>// remove any connections that completed, iterating in reverse order
</span><span>        </span><span>// to preserve the indices
</span><span>        </span><span>for</span><span> i </span><span>in</span><span> completed.</span><span>into_iter</span><span>().</span><span>rev</span><span>() {
</span><span>            connections.</span><span>remove</span><span>(i);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now that we have to manage scheduling ourselves.. things are a lot more complicated.</p>
<p>And now for the moment of truth...</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<p>It works!</p>
<h2 id="a-multiplexed-server"><a href="#a-multiplexed-server" aria-label="Anchor link for: a-multiplexed-server">A Multiplexed Server</a></h2>
<p>Our server can now handle running multiple requests concurrently on a single thread. Nothing ever blocks. If some operation <em>would</em> have blocked, it remembers the current state and moves on to run something else, much like the kernel scheduler was doing for us. However, our new design introduces two new problems.</p>
<p>The first problem is that everything runs on the main thread, utilizing only a single CPU core. We're doing the best we can to use that one core efficiently, but we're still only running a single thing at a time. With threads spread across multiple cores, we could be doing much more.</p>
<p>There's a bigger problem though.</p>
<p>Our main loop isn't actually very efficient.</p>
<p>We're making an I/O request to the kernel for <em>every</em> single active connection, <em>every</em> single iteration of the loop, to check if it's ready. A call to <code>read</code> or <code>write</code>, even if it returns <code>WouldBlock</code> and doesn't actually perform any I/O, is still a syscall. Syscalls aren't cheap. We might have 10k active connections but only 500 of them are ready. Calling <code>read</code> or <code>write</code> 10k times when only 500 of them will actually do anything is a massive waste of CPU cycles.</p>
<p>As the number of connections scales, our loop becomes less and less efficient, wasting more time doing useless work.</p>
<p>How do we fix this? With blocking I/O the kernel was able to schedule things efficiently because it <em>knows</em> when resources become ready. With non-blocking I/O, we don't know without checking. But checking is expensive.</p>
<p>What we need is an efficient way to keep track of all of our active connections, and somehow get notified when they become ready.</p>
<p>It turns out, we aren't the first to run into this problem. Every operating system provides a solution for exactly this. On Linux, it's called <code>epoll</code>.</p>
<blockquote>
<p><code>epoll(7)</code> - I/O event notification facility</p>
<p>The epoll API performs a similar task to <code>poll(2)</code>: monitoring multiple file descriptors to see if I/O is possible on any of them. The epoll API can be used either as an edge-triggered or a level-triggered interface and scales well to large numbers of watched file descriptors.</p>
</blockquote>
<p>Sounds perfect! Let's try using it.</p>
<p><code>epoll</code> is a family of Linux system calls that let us work with a set of non-blocking sockets. It isn't terribly ergonomic to use directly, so we'll be using the <code>epoll</code> crate, a thin wrapper around the C interface.</p>
<p>To start, we'll initialize an epoll instance using the <code>create</code> function.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ```toml
</span><span>// [dependencies]
</span><span>// epoll = "4.3"
</span><span>// ```
</span><span>
</span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> epoll </span><span>= </span><span>epoll::create(</span><span>false</span><span>).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>}
</span></code></pre>
<p><code>epoll::create</code> returns a file descriptor that represents the newly created epoll instance. You can think of it as a set of file descriptors that we can add or remove from.</p>
<blockquote>
<p>In Linux/Unix, everything is considered a file. Actual files on the file system, TCP sockets, and external devices are all files that you can <code>read</code>/<code>write</code> to. A <em>file descriptor</em> is an integer that represents an open "file" in the system. We'll be working with file descriptors a lot throughout the rest of the article.</p>
</blockquote>
<p>The first file descriptor we have to add is the TCP listener. We can modify the epoll set with the <code>epoll::ctl</code> command. To add to it, we'll use the <code>EPOLL_CTL_ADD</code> flag.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>epoll::{Event, Events, ControlOptions::</span><span>*</span><span>};
</span><span>use </span><span>std::os::fd::AsRawFd;
</span><span>
</span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>    listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>// add the listener to epoll
</span><span>    </span><span>let</span><span> event </span><span>= </span><span>Event::new(Events::</span><span>EPOLLIN</span><span>, listener.</span><span>as_raw_fd</span><span>() </span><span>as _</span><span>);
</span><span>    epoll::ctl(epoll, </span><span>EPOLL_CTL_ADD</span><span>, listener.</span><span>as_raw_fd</span><span>(), event).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>}
</span></code></pre>
<p>We pass in the file descriptor of the resource we are registering, the TCP listener, along with an <code>Event</code>. An event has two parts, the <em>interest</em> flag, and the data field. The interest flag gives us a way to tell epoll which I/O events we are interested in. In the case of the TCP listener, we want to be notified when new connections come <em>in</em>, so we pass the <code>EPOLLIN</code> flag.</p>
<p>The data field lets us store an ID that will uniquely identify each resource. Remember, a file descriptor is a unique integer for a given file, so we can just use that. You'll see why this is important in the next step.</p>
<p>Now for the main loop. This time, no spinning. Instead we can call <code>epoll::wait</code>.</p>
<blockquote>
<p><code>epoll_wait(2)</code>- wait for an I/O event on an epoll file descriptor</p>
<p>The <code>epoll_wait()</code> system call waits for events on the <code>epoll(7)</code> instance referred to by the file descriptor epfd. The buffer pointed to by events is used to return information from the ready list about file descriptors in the interest list that have some events available.</p>
<p>A call to <code>epoll_wait()</code> will block until either:</p>
<ul>
<li>a file descriptor delivers an event;</li>
<li>the call is interrupted by a signal handler; or</li>
<li>the timeout expires.</li>
</ul>
</blockquote>
<p><code>epoll::wait</code> is the magical part of epoll. It lets us block until <em>any</em> of the events we registered become ready, and tells us which ones did. Right now that's just until a new connection comes in, but soon we'll use this same call to block for read, write, and flush events that we were previously spinning for.</p>
<p>The fact that <code>epoll::wait</code> is "blocking" might put you off, but remember, it only blocks if there is nothing else to do, where previously we would have been spinning and making pointless syscalls. This idea of blocking on multiple operations simultaneously is known as <em>I/O multiplexing</em>.</p>
<p><code>epoll::wait</code> accepts a list of events that it will populate with information about the file descriptors that became ready. It then returns the number of events that were added.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>loop </span><span>{
</span><span>    </span><span>let </span><span>mut</span><span> events </span><span>= </span><span>[Event::new(Events::empty(), </span><span>0</span><span>); </span><span>1024</span><span>];
</span><span>    </span><span>let</span><span> timeout </span><span>= -</span><span>1</span><span>; </span><span>// block forever, until something happens
</span><span>    </span><span>let</span><span> num_events </span><span>= </span><span>epoll::wait(epoll, timeout, </span><span>&amp;mut</span><span> events).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>
</span><span>    </span><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>Each event contains the data field associated with the resource that became ready.</p>
<pre data-lang="rust"><code data-lang="rust"><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>    </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Remember when we used the file descriptor for the data field? We can use it to check whether the event is for the TCP listener, which means there's an incoming connection ready to accept:</p>
<pre data-lang="rust"><code data-lang="rust"><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>    </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>    </span><span>// is the listener ready?
</span><span>    </span><span>if</span><span> fd </span><span>==</span><span> listener.</span><span>as_raw_fd</span><span>() {
</span><span>        </span><span>// try accepting a connection
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>                </span><span>// ...
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>If the call still returns <code>WouldBlock</code> for whatever reason, we can just move one and wait for the next event.</p>
<p>Now we have to register the new connection in epoll, just like we did the listener.</p>
<pre data-lang="rust"><code data-lang="rust"><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>    </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>    </span><span>// is the listener ready?
</span><span>    </span><span>if</span><span> fd </span><span>==</span><span> listener.</span><span>as_raw_fd</span><span>() {
</span><span>        </span><span>// try accepting a connection
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>                 </span><span>let</span><span> fd </span><span>=</span><span> connection.</span><span>as_raw_fd</span><span>();
</span><span>
</span><span>                 </span><span>// register the connection with epoll
</span><span>                 </span><span>let</span><span> event </span><span>= </span><span>Event::new(Events::</span><span>EPOLLIN </span><span>| </span><span>Events::</span><span>EPOLLOUT</span><span>, fd </span><span>as _</span><span>);
</span><span>                 epoll::ctl(epoll, </span><span>EPOLL_CTL_ADD</span><span>, fd, event).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>This time we set both <code>EPOLLIN</code> and <code>EPOLLOUT</code>, because we are interested in both read and write events, depending on the state of the connection.</p>
<p>Now that we register connections, we'll get events for both the TCP listener and individual connections. We need to store connections and their states in a way that we can look up by file descriptor.</p>
<p>Instead of a list, we can use a <code>HashMap</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> connections </span><span>= </span><span>HashMap::new();
</span><span>
</span><span>loop </span><span>{
</span><span>    </span><span>// ...
</span><span>    'next: </span><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>        </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>        </span><span>// is the listener ready?
</span><span>        </span><span>if</span><span> fd </span><span>==</span><span> listener.</span><span>as_raw_fd</span><span>() {
</span><span>            </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>                </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// ...
</span><span>
</span><span>                    </span><span>let</span><span> state </span><span>= </span><span>ConnectionState::Read {
</span><span>                        request: [</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>                        read: </span><span>0</span><span>,
</span><span>                    };
</span><span>
</span><span>                    connections.</span><span>insert</span><span>(fd, (connection, state)); </span><span>// 👈
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{}
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>
</span><span>            </span><span>continue 'next</span><span>;
</span><span>        }
</span><span>
</span><span>        </span><span>// otherwise, a connection must be ready
</span><span>        </span><span>let </span><span>(connection, state) </span><span>=</span><span> connections.</span><span>get_mut</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>(); </span><span>// 👈
</span><span>    }
</span><span>}
</span></code></pre>
<p>Once we have the ready connection and it's state, we can try to make progress the exact same way we did last time. Nothing changes about the way we read or write from the stream, the only difference is that we only ever <em>do</em> I/O when epoll tells us to.</p>
<p>Before we had to check every single connection to see if something became ready, but now <code>epoll</code> handles that for us, so we avoid any useless syscalls.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// ...
</span><span>
</span><span>// epoll told us this connection is ready
</span><span>let </span><span>(connection, state) </span><span>=</span><span> connections.</span><span>get_mut</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>();
</span><span>
</span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>    </span><span>// connection.read...
</span><span>    </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Write { response, written };
</span><span>}
</span><span>
</span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>    </span><span>// connection.write...
</span><span>    </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Flush;
</span><span>}
</span><span>
</span><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> state {
</span><span>    </span><span>// connection.flush...
</span><span>}
</span></code></pre>
<p>Once we've finished reading, writing, and flushing the response, we remove the connection from our map and drop it, which automatically unregisters it from epoll.</p>
<pre data-lang="rust"><code data-lang="rust"><span>for</span><span> fd </span><span>in</span><span> completed {
</span><span>    </span><span>let </span><span>(connection, _state) </span><span>=</span><span> connections.</span><span>remove</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>();
</span><span>    </span><span>// unregister from epoll
</span><span>    </span><span>drop</span><span>(connection);
</span><span>}
</span></code></pre>
<p>And that's it! Here's the new high-level flow of our server:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// create epoll
</span><span>    </span><span>let</span><span> epoll </span><span>= </span><span>epoll::create(</span><span>false</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>// bind the listener
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>/* ... */</span><span>.
</span><span>
</span><span>    </span><span>// add the listener to epoll
</span><span>    </span><span>let</span><span> event </span><span>= </span><span>Event::new(Events::</span><span>EPOLLIN</span><span>, listener.</span><span>as_raw_fd</span><span>() </span><span>as _</span><span>);
</span><span>    epoll::ctl(epoll, </span><span>EPOLL_CTL_ADD</span><span>, listener.</span><span>as_raw_fd</span><span>(), event).</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>let </span><span>mut</span><span> connections </span><span>= </span><span>HashMap::new();
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>mut</span><span> events </span><span>= </span><span>[Event::new(Events::empty(), </span><span>0</span><span>); </span><span>1024</span><span>];
</span><span>
</span><span>        </span><span>// block until epoll wakes us up
</span><span>        </span><span>let</span><span> num_events </span><span>= </span><span>epoll::wait(epoll, </span><span>0</span><span>, </span><span>&amp;mut</span><span> events).</span><span>unwrap</span><span>();
</span><span>        </span><span>let </span><span>mut</span><span> completed </span><span>= </span><span>Vec</span><span>::new();
</span><span>
</span><span>        'next: </span><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>            </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>            </span><span>// is the listener ready?
</span><span>            </span><span>if</span><span> fd </span><span>==</span><span> listener.</span><span>as_raw_fd</span><span>() {
</span><span>                </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>                    </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                        </span><span>// ...
</span><span>
</span><span>                        </span><span>// add the connection to epoll
</span><span>                        </span><span>let</span><span> event </span><span>= </span><span>Event::new(Events::</span><span>EPOLLIN </span><span>| </span><span>Events::</span><span>EPOLLOUT</span><span>, fd </span><span>as _</span><span>);
</span><span>                        epoll::ctl(epoll, </span><span>EPOLL_CTL_ADD</span><span>, fd, event).</span><span>unwrap</span><span>();
</span><span>
</span><span>                        </span><span>// keep track of connection state
</span><span>                        </span><span>let</span><span> state </span><span>= </span><span>ConnectionState::Read {
</span><span>                            request: [</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>                            read: </span><span>0</span><span>,
</span><span>                        };
</span><span>
</span><span>                        connections.</span><span>insert</span><span>(fd, (connection, state));
</span><span>                }
</span><span>
</span><span>                </span><span>continue 'next</span><span>;
</span><span>            }
</span><span>
</span><span>            </span><span>// otherwise, a connection is ready
</span><span>            </span><span>let </span><span>(connection, state) </span><span>=</span><span> connections.</span><span>get_mut</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>();
</span><span>
</span><span>            </span><span>// try to drive it forward based on its state
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Read { request, read } </span><span>=</span><span> state {
</span><span>                </span><span>// connection.read...
</span><span>                </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Write {
</span><span>                    response: response.</span><span>as_bytes</span><span>(),
</span><span>                    written: </span><span>0</span><span>,
</span><span>                };
</span><span>            }
</span><span>
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Write { response, written } </span><span>=</span><span> state {
</span><span>                </span><span>// connection.write...
</span><span>                </span><span>*</span><span>state </span><span>= </span><span>ConnectionState::Flush;
</span><span>            }
</span><span>
</span><span>            </span><span>if </span><span>let </span><span>ConnectionState::Flush </span><span>=</span><span> state {
</span><span>                </span><span>// connection.flush...
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>for</span><span> fd </span><span>in</span><span> completed {
</span><span>            </span><span>let </span><span>(connection, _state) </span><span>=</span><span> connections.</span><span>remove</span><span>(</span><span>&amp;</span><span>fd).</span><span>unwrap</span><span>();
</span><span>            </span><span>// unregister from epoll
</span><span>            </span><span>drop</span><span>(connection);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>And...</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<p>It works!</p>
<h2 id="futures"><a href="#futures" aria-label="Anchor link for: futures">Futures</a></h2>
<p>Alright, our server can now process multiple requests concurrently on a single thread. And thanks to epoll, it's pretty efficient at doing so. But there's still a problem.</p>
<p>We got so caught up in gaining control over the execution of our tasks, and then scheduling them efficiently ourselves, that in the process the complexity of our code has increased dramatically.</p>
<p>What went from a simple, sequential accept loop has become a massive event loop managing multiple state machines.</p>
<p>And it's not pretty.</p>
<p>Making our original server multi-threaded was as simple as adding a single line of code in <code>thread::spawn</code>. If you think about it, our server is still a set of concurrent tasks, we just manage them all messily in a giant loop.</p>
<p>This doesn't seem very scalable. The more features we add to our program, the more complex the loop becomes, because everything is so tightly coupled together.</p>
<p>What if we could write an abstraction like <code>thread::spawn</code> that let us write our tasks as individual units, and handle the scheduling and event handling for all tasks in a single place, regaining some of that sequential control flow?</p>
<blockquote>
<p>This idea is generally referred to as <em>asynchronous programming</em>.</p>
</blockquote>
<p>Let's take a look at the signature of <code>thread::spawn</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>fn </span><span>spawn</span><span>&lt;F, T&gt;(</span><span>f</span><span>: F) -&gt; JoinHandle&lt;T&gt;
</span><span>where
</span><span>    F: FnOnce() -&gt; T + Send + </span><span>'static</span><span>,
</span><span>    T: Send + </span><span>'static</span><span>;
</span></code></pre>
<p><code>thread::spawn</code> takes a closure. Our version of <code>thread::spawn</code> however, could not take a closure, because we aren't the operating system and can't arbitrarily preempt code at will. We need to somehow represent a non-blocking, resumable task.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// fn spawn&lt;T: Task&gt;(task: T);
</span><span>
</span><span>trait </span><span>Task {}
</span></code></pre>
<p>Handling a request is a task. Reading or writing from/to the connection is also a task. A task is really just a piece of code that needs to be run, representing a value that will resolve sometime in the <em>future</em>.</p>
<p><code>Future</code>, that's a nice name isn't it.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>self</span><span>) -&gt; </span><span>Self::</span><span>Output;
</span><span>}
</span></code></pre>
<p>Hmm.. that signature doesn't really work. Having <code>run</code> return the output directly means it must be blocking, which is what we're trying so hard to avoid. We instead want a way to attempt to drive the future forward without blocking, like we've been doing with all our state machines in the event loop.</p>
<p>What we're really doing when we try to run a future is asking it if the value is ready yet, <em>polling</em> it, and giving it a chance to make progress.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>self</span><span>) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>That looks more like it.</p>
<p>Except wait, <code>poll</code> can't take <code>self</code> if we want to call it multiple times, it should probably take a reference. A mutable reference, if we want to mutate the internal state of the task as it makes progress, like <code>ConnectionState</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>Alright, imagine a scheduler that runs these new futures.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>for</span><span> future </span><span>in &amp;</span><span>self.tasks {
</span><span>                future.</span><span>poll</span><span>();
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>That doesn't look right.</p>
<p>After initiating the future, the scheduler should only try to call <code>poll</code> when the given future is able to make progress, like when epoll returns an event. But how do we know when that happens?</p>
<p>If the future represents an I/O operation, we know it's able to make progress when epoll tells us it is. The problem is the scheduler won't know which epoll event corresponds to which future, because the future handles everything internally in <code>poll</code>.</p>
<p>What we need is for the scheduler to pass each future an ID, so that the future can register any I/O resources with epoll using the same ID, instead of their file descriptors. That way the scheduler has a way of mapping epoll events to runnable futures.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>fn </span><span>spawn</span><span>&lt;T&gt;(</span><span>&amp;</span><span>self</span><span>, </span><span>mut </span><span>future</span><span>: T) {
</span><span>        </span><span>let</span><span> id </span><span>= </span><span>rand</span><span>();
</span><span>        </span><span>// poll the future once to get it started, passing in it's ID
</span><span>        future.</span><span>poll</span><span>(event.id);
</span><span>        </span><span>// store the future
</span><span>        self.tasks.</span><span>insert</span><span>(id, future);
</span><span>    }
</span><span>
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>self</span><span>) {
</span><span>        </span><span>// ...
</span><span>
</span><span>        </span><span>for</span><span> event </span><span>in</span><span> epoll_events {
</span><span>            </span><span>// poll the future associated with this event 
</span><span>            </span><span>let</span><span> future </span><span>= </span><span>self.tasks.</span><span>get</span><span>(</span><span>&amp;</span><span>event.id).</span><span>unwrap</span><span>();
</span><span>            future.</span><span>poll</span><span>(event.id);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>You know, it would be nice if there was a more generic way to tell the scheduler about progress than tying every future to epoll. We might have different types of futures that make progress in other ways, like a timer running on a background thread, or a channel that needs to notify tasks that a message is available.</p>
<p>What if we gave the futures themselves more control? Instead of just an ID, what if we give every future a way to wake itself up, notifying the scheduler that it's ready to make progress?</p>
<p>A simple callback should do the trick.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[derive(Clone)]
</span><span>struct </span><span>Waker(Arc&lt;dyn </span><span>Fn</span><span>() </span><span>+ </span><span>Send </span><span>+ </span><span>Sync</span><span>&gt;</span><span>);
</span><span>
</span><span>impl </span><span>Waker {
</span><span>    </span><span>fn </span><span>wake</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        (self.</span><span>0</span><span>)()
</span><span>    }
</span><span>}
</span><span>
</span><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>The scheduler can provide each future a callback, that when called, updates the scheduler's state for that future, marking it as ready. That way our scheduler is completely disconnected from epoll, or any other individual notification system.</p>
<p><code>Waker</code> is thread-safe, allowing us to use background threads to wake futures. Right now all of our tasks are connected to epoll anyways, but this will come in handy later.</p>
<h2 id="a-reactor"><a href="#a-reactor" aria-label="Anchor link for: a-reactor">A Reactor</a></h2>
<p>Consider a future that reads from a TCP connection. It receives a <code>Waker</code> that needs to be called when epoll returns the relevant <code>EPOLLIN</code> event, but the future won't be running when that happens, it will be idle in the scheduler's queue. Obviously, the future can't wake itself up, someone else has to.</p>
<p>All I/O futures need a way to give their wakers to epoll. In fact, they need more than that, they need some sort of background service that drives epoll, so we can register wakers with it.</p>
<p>This service is commonly known as a <em>reactor</em>.</p>
<p>The reactor is a simple object holding the epoll descriptor and a map of tasks keyed by file descriptor, just like we had before. The difference is that instead of the map holding the TCP connections themselves, it holds the wakers.</p>
<pre data-lang="rust"><code data-lang="rust"><span>thread_local! {
</span><span>    </span><span>static </span><span>REACTOR</span><span>: Reactor </span><span>= </span><span>Reactor::new();
</span><span>}
</span><span>
</span><span>struct </span><span>Reactor {
</span><span>    epoll: RawFd,
</span><span>    tasks: RefCell&lt;HashMap&lt;RawFd, Waker&gt;&gt;,
</span><span>}
</span><span>
</span><span>impl </span><span>Reactor {
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>() -&gt; Reactor {
</span><span>        Reactor {
</span><span>            epoll: epoll::create(</span><span>false</span><span>).</span><span>unwrap</span><span>(),
</span><span>            tasks: RefCell::new(HashMap::new()),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>To keep things simple, the reactor is a thread-local object, mutated through a <code>RefCell</code>. This is important because the reactor will be modified and accessed by different tasks throughout the program.</p>
<p>The reactor needs to support a couple simple operations.</p>
<p>Adding a task:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Reactor {
</span><span>    </span><span>// Add a file descriptor with read and write interest.
</span><span>    </span><span>//
</span><span>    </span><span>// `waker` will be called when the descriptor becomes ready.
</span><span>    </span><span>pub </span><span>fn </span><span>add</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>fd</span><span>: RawFd, </span><span>waker</span><span>: Waker) {
</span><span>        </span><span>let</span><span> event </span><span>= </span><span>epoll::Event::new(Events::</span><span>EPOLLIN </span><span>| </span><span>Events::</span><span>EPOLLOUT</span><span>, fd </span><span>as </span><span>u64</span><span>);
</span><span>        epoll::ctl(self.epoll, </span><span>EPOLL_CTL_ADD</span><span>, fd, event).</span><span>unwrap</span><span>();
</span><span>        self.tasks.</span><span>borrow_mut</span><span>().</span><span>insert</span><span>(fd, waker);
</span><span>    }
</span><span>}
</span></code></pre>
<p>Removing a task:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Reactor {
</span><span>    </span><span>// Remove the given descriptor from epoll.
</span><span>    </span><span>//
</span><span>    </span><span>// It will no longer receive any notifications.
</span><span>    </span><span>pub </span><span>fn </span><span>remove</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>fd</span><span>: RawFd) {
</span><span>        self.tasks.</span><span>borrow_mut</span><span>().</span><span>remove</span><span>(</span><span>&amp;</span><span>fd);
</span><span>    }
</span><span>}
</span></code></pre>
<p>And driving epoll.</p>
<p>We'll be running the reactor in a loop, just like we were running epoll in a loop before. It works exactly the same way, except all the reactor has to do is wake the associated future for every event. Remember, this will trigger the scheduler to run the future later, and continue the cycle.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Reactor {
</span><span>    </span><span>// Drive tasks forward, blocking forever until an event arrives.
</span><span>    </span><span>pub </span><span>fn </span><span>wait</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>       </span><span>let </span><span>mut</span><span> events </span><span>= </span><span>[Event::new(Events::empty(), </span><span>0</span><span>); </span><span>1024</span><span>];
</span><span>       </span><span>let</span><span> timeout </span><span>= -</span><span>1</span><span>; </span><span>// forever
</span><span>       </span><span>let</span><span> num_events </span><span>= </span><span>epoll::wait(self.epoll, timeout, </span><span>&amp;mut</span><span> events).</span><span>unwrap</span><span>();
</span><span>
</span><span>       </span><span>for</span><span> event </span><span>in &amp;</span><span>events[</span><span>..</span><span>num_events] {
</span><span>           </span><span>let</span><span> fd </span><span>=</span><span> event.data </span><span>as </span><span>i32</span><span>;
</span><span>
</span><span>           </span><span>// wake the task
</span><span>           </span><span>if </span><span>let Some</span><span>(waker) </span><span>= </span><span>self.tasks.</span><span>borrow</span><span>().</span><span>get</span><span>(</span><span>&amp;</span><span>fd) {
</span><span>               waker.</span><span>wake</span><span>();
</span><span>           }
</span><span>       }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Great, now we have a simple reactor interface.</p>
<p>But all of this is still a little abstract. What does it really mean to call <code>wake</code>?</p>
<h2 id="scheduling-tasks"><a href="#scheduling-tasks" aria-label="Anchor link for: scheduling-tasks">Scheduling Tasks</a></h2>
<p>We have a reactor, now we need a scheduler to run our tasks.</p>
<p>One thing to keep in mind is that the scheduler must be global and thread-safe because wakers are <code>Send</code>, meaning <code>wake</code> may be called concurrently from other threads.</p>
<pre data-lang="rust"><code data-lang="rust"><span>static </span><span>SCHEDULER</span><span>: Scheduler </span><span>=</span><span> Scheduler { </span><span>/* ... */ </span><span>};
</span><span>
</span><span>#[derive(Default)]
</span><span>struct </span><span>Scheduler {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>We want to be able to spawn tasks onto our scheduler, just like we could spawn threads. For now, we'll only handle spawning tasks that don't return anything, to avoid having to implement a version of <code>JoinHandle</code>.</p>
<p>To start, we'll probably need some sort of list of tasks to run, guarded by a <code>Mutex</code> to be thread-safe.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Scheduler {
</span><span>    tasks: Mutex&lt;</span><span>Vec</span><span>&lt;</span><span>Box</span><span>&lt;dyn Future </span><span>+ </span><span>Send</span><span>&gt;&gt;&gt;
</span><span>}
</span><span>
</span><span>impl </span><span>Scheduler {
</span><span>    </span><span>pub </span><span>fn </span><span>spawn</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>task</span><span>: impl Future&lt;Output = ()&gt; + Send + </span><span>'static</span><span>) {
</span><span>        self.tasks.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>push</span><span>(</span><span>Box</span><span>::new(task));
</span><span>    }
</span><span>
</span><span>    </span><span>pub </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>for</span><span> task </span><span>in</span><span> tasks.</span><span>borrow_mut</span><span>().</span><span>iter_mut</span><span>() {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Remember, futures are only polled when they are able to make progress. They should always be able to make progress at the start, but after that we don't touch them until someone calls <code>wake</code>.</p>
<p>There are a couple of ways we could go about this. We could just store a <code>HashMap</code> of tasks with a status flag that indicates whether or not the task was woken, but that means we would have to iterate through the entire map to find out which tasks are runnable. While this isn't incredibly expensive, there is a better way.</p>
<p>Instead of storing every spawned task in the map, we'll only store runnable tasks in a queue.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::collections::VecDeque;
</span><span>
</span><span>type </span><span>SharedTask </span><span>= </span><span>Arc&lt;Mutex&lt;dyn Future&lt;Output = ()&gt; </span><span>+ </span><span>Send</span><span>&gt;&gt;;
</span><span>
</span><span>#[derive(Default)]
</span><span>struct </span><span>Scheduler {
</span><span>    runnable: Mutex&lt;VecDeque&lt;SharedTask&gt;&gt;,
</span><span>}
</span></code></pre>
<p>The types will make sense soon.</p>
<p>When a task is spawned, it's pushed onto the back of the queue:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>pub </span><span>fn </span><span>spawn</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>task</span><span>: impl Future&lt;Output = ()&gt; + Send + </span><span>'static</span><span>) {
</span><span>        self.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>push_back</span><span>(Arc::new(Mutex::new(task)));
</span><span>    }
</span><span>}
</span></code></pre>
<p>The scheduler pops tasks off the queue one by one and calls <code>poll</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// pop a runnable task off the queue
</span><span>            </span><span>let</span><span> task </span><span>= </span><span>self.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>pop_front</span><span>();
</span><span>
</span><span>            </span><span>if </span><span>let Some</span><span>(task) </span><span>=</span><span> task {
</span><span>                </span><span>// and poll it
</span><span>                task.</span><span>try_lock</span><span>().</span><span>unwrap</span><span>().</span><span>poll</span><span>(waker);
</span><span>            }
</span><span>        }
</span><span>    }
</span></code></pre>
<blockquote>
<p>Notice that we don't even really need a <code>Mutex</code> around the task because it's only going to be accessed by the main thread, but removing it would mean <code>unsafe</code>. We'll settle with <code>try_lock().unwrap()</code> for now.</p>
</blockquote>
<p>Now for the important bit, the waker. The beautiful part of our run queue is that when a task is woken, it's simply pushed back onto the queue.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Scheduler {
</span><span>    </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// pop a runnable task off the queue
</span><span>            </span><span>let</span><span> task </span><span>= </span><span>self.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>pop_front</span><span>();
</span><span>
</span><span>            </span><span>if </span><span>let Some</span><span>(task) </span><span>=</span><span> task {
</span><span>                </span><span>let</span><span> t2 </span><span>=</span><span> task.</span><span>clone</span><span>();
</span><span>
</span><span>                </span><span>// create a waker that pushes the task back on
</span><span>                </span><span>let</span><span> wake </span><span>= </span><span>Arc::new(</span><span>move || </span><span>{
</span><span>                    </span><span>SCHEDULER</span><span>.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>push_back</span><span>(t2.</span><span>clone</span><span>());
</span><span>                });
</span><span>
</span><span>                </span><span>// poll the task
</span><span>                task.</span><span>try_lock</span><span>().</span><span>unwrap</span><span>().</span><span>poll</span><span>(Waker(wake));
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>This is why the task needed to be reference counted — it's not owned by the scheduler, it's referenced by the queue, as well as wherever the waker is being stored. In fact the same task might be on the queue multiple times at once, and the waker might be cloned all over the place.</p>
<p>Once we've dealt with all runnable tasks, we need to block on the reactor until another task becomes ready <sup><a href="#4">5</a></sup>. Once a task becomes ready, the reactor will call <code>wake</code> and push the future back onto our queue for us to run it again, continuing the cycle.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>// pop a runnable task off the queue
</span><span>            </span><span>let Some</span><span>(task) </span><span>= </span><span>self.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>pop_front</span><span>() </span><span>else </span><span>{ </span><span>break </span><span>};
</span><span>            </span><span>let</span><span> t2 </span><span>=</span><span> task.</span><span>clone</span><span>();
</span><span>
</span><span>            </span><span>// create a waker that pushes the task back on
</span><span>            </span><span>let</span><span> wake </span><span>= </span><span>Arc::new(</span><span>move || </span><span>{
</span><span>                </span><span>SCHEDULER</span><span>.runnable.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>push_back</span><span>(t2.</span><span>clone</span><span>());
</span><span>            });
</span><span>
</span><span>            </span><span>// poll the task
</span><span>            task.</span><span>lock</span><span>().</span><span>unwrap</span><span>().</span><span>poll</span><span>(Waker(wake));
</span><span>        }
</span><span>
</span><span>        </span><span>// if there are no runnable tasks, block on epoll until something becomes ready
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| reactor.</span><span>wait</span><span>()); </span><span>// 👈
</span><span>    }
</span><span>}
</span></code></pre>
<p>Beautiful.</p>
<p>...ignoring the <code>Arc&lt;Mutex&lt;T&gt;&gt;</code> clutter.</p>
<p>Alright! Together, the scheduler and reactor form a <em>runtime</em> for our futures. The scheduler keeps tracks of which tasks are runnable and polls them, and the reactor marks tasks as runnable when epoll tells us something they are interested in becomes ready.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span><span>
</span><span>static </span><span>SCHEDULER</span><span>: Scheduler </span><span>=</span><span> Scheduler { </span><span>/* ... */ </span><span>};
</span><span>
</span><span>// The scheduler.
</span><span>#[derive(Default)]
</span><span>struct </span><span>Scheduler {
</span><span>    runnable: Mutex&lt;VecDeque&lt;SharedTask&gt;&gt;,
</span><span>}
</span><span>
</span><span>type </span><span>SharedTask </span><span>= </span><span>Arc&lt;Mutex&lt;dyn Future&lt;Output = ()&gt; </span><span>+ </span><span>Send</span><span>&gt;&gt;;
</span><span>
</span><span>impl </span><span>Scheduler {
</span><span>    </span><span>pub </span><span>fn </span><span>spawn</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>task</span><span>: impl Future&lt;Output = ()&gt; + Send + </span><span>'static</span><span>);
</span><span>    </span><span>pub </span><span>fn </span><span>run</span><span>(</span><span>&amp;</span><span>self</span><span>);
</span><span>}
</span><span>
</span><span>thread_local! {
</span><span>    </span><span>static </span><span>REACTOR</span><span>: Reactor </span><span>= </span><span>Reactor::new();
</span><span>}
</span><span>
</span><span>// The reactor.
</span><span>struct </span><span>Reactor {
</span><span>    epoll: RawFd,
</span><span>    tasks: RefCell&lt;HashMap&lt;RawFd, Waker&gt;&gt;,
</span><span>}
</span><span>
</span><span>impl </span><span>Reactor {
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>() -&gt; Reactor;
</span><span>    </span><span>pub </span><span>fn </span><span>add</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>fd</span><span>: RawFd, </span><span>waker</span><span>: Waker);
</span><span>    </span><span>pub </span><span>fn </span><span>remove</span><span>(</span><span>&amp;</span><span>self</span><span>, </span><span>fd</span><span>: RawFd);
</span><span>    </span><span>pub </span><span>fn </span><span>wait</span><span>(</span><span>&amp;</span><span>self</span><span>);
</span><span>}
</span></code></pre>
<p>We've written the runtime, now let's try to use it!</p>
<h2 id="an-async-server"><a href="#an-async-server" aria-label="Anchor link for: an-async-server">An Async Server</a></h2>
<p>It's time to actually write the tasks that our scheduler is going to run. Like before, we'll use enums as state machines to manage the different states of our program. The difference is that this time, each task will manage it's own state independent from other tasks, instead of having the entire program revolve around a messy event loop.</p>
<p>To start everything off, we need to write the main task. This task will be pushed on and off the scheduler's run queue for the entirety of our program.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Main::Start);
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>enum </span><span>Main {
</span><span>    Start,
</span><span>}
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>Main {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>Our task starts off just like before, creating the TCP listener and setting it to non-blocking mode.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// impl Future for Main {
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>        listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>    }
</span><span>
</span><span>    </span><span>None
</span><span>}
</span></code></pre>
<p>Now we need to register the listener with epoll. We can do that using our new <code>Reactor</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// impl Future for Main {
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>// ...
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>    }
</span><span>}
</span></code></pre>
<p>Notice how we give the reactor the waker provided to us by the scheduler. When a connection comes, epoll will return an event and the <code>Reactor</code> will wake the task, causing the scheduler to push our task back onto the queue and <code>poll</code> us again. The waker keeps everything connected.</p>
<p>We now need a second state for the next time we're run, <code>Accept</code>. The main task will stay in the <code>Accept</code> state for the rest of the program, attempting to accept new connections.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Main {
</span><span>    Start,
</span><span>    Accept { listener: TcpListener }, </span><span>// 👈
</span><span>}
</span><span>
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>// ...
</span><span>        </span><span>*</span><span>self </span><span>= </span><span>Main::Accept { listener };
</span><span>    }
</span><span>
</span><span>    </span><span>if </span><span>let </span><span>Main::Accept { listener } </span><span>= </span><span>self {
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>                </span><span>return </span><span>None</span><span>;
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>None
</span><span>}
</span></code></pre>
<p>If the listener is not ready, we can simply return <code>None</code>. Remember, this tells the scheduler the future is not yet ready, and it will be rescheduled once the reactor wakes us.</p>
<p>If we do accept a new connection, we need to again set it to non-blocking mode.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>if </span><span>let </span><span>Main::Accept { listener } </span><span>= </span><span>self {
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>(); </span><span>// 👈 
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>None
</span><span>}
</span></code></pre>
<p>And now we need to spawn a new task to handle the request.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>if </span><span>let </span><span>Main::Accept { listener } </span><span>= </span><span>self {
</span><span>        </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Handler { </span><span>// 👈
</span><span>                    connection,
</span><span>                    state: HandlerState::Start,
</span><span>                });
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>The handler task looks similar to before, but now it manages the connection itself along with its current state, which is identical to <code>ConnectionState</code> from earlier.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Handler {
</span><span>    connection: TcpStream,
</span><span>    state: HandlerState,
</span><span>}
</span><span>
</span><span>enum </span><span>HandlerState {
</span><span>    Start,
</span><span>    Read {
</span><span>        request: [</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>        read: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Write {
</span><span>        response: </span><span>&amp;'static </span><span>[</span><span>u8</span><span>],
</span><span>        written: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Flush,
</span><span>}
</span></code></pre>
<p>The handler task starts by registering its connection with the reactor to be notified when the connection is ready to read/write to. Again, it passes the waker so that the scheduler knows when to run it again.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Future </span><span>for </span><span>Handler {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Start </span><span>= </span><span>self.state {
</span><span>            </span><span>// start by registering our connection for notifications
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>                reactor.</span><span>add</span><span>(self.connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>            });
</span><span>
</span><span>            self.state </span><span>= </span><span>HandlerState::Read {
</span><span>                request: [</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>                read: </span><span>0</span><span>,
</span><span>            };
</span><span>        }
</span><span>
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>The <code>Read</code>, <code>Write</code>, and <code>Flush</code> states work exactly the same as before, but now when we encounter <code>WouldBlock</code>, we can simply return <code>None</code>, knowing that we'll be run again once our future is woken.</p>
<pre data-lang="rust"><code data-lang="rust"><span>// impl Future for Handler {
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>    </span><span>if </span><span>let </span><span>HandlerState::Start </span><span>= </span><span>self.state {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>
</span><span>    </span><span>// read the request
</span><span>    </span><span>if </span><span>let </span><span>HandlerState::Read { request, read } </span><span>= &amp;mut </span><span>self.state {
</span><span>        </span><span>loop </span><span>{
</span><span>            </span><span>match </span><span>self.connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]) {
</span><span>                </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                    println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                    </span><span>return </span><span>Some</span><span>(());
</span><span>                }
</span><span>                </span><span>Ok</span><span>(n) </span><span>=&gt; *</span><span>read </span><span>+=</span><span> n,
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>, </span><span>// 👈
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>
</span><span>            </span><span>// did we reach the end of the request?
</span><span>            </span><span>let</span><span> read </span><span>= *</span><span>read;
</span><span>            </span><span>if</span><span> read </span><span>&gt;= </span><span>4 </span><span>&amp;&amp; &amp;</span><span>request[read </span><span>- </span><span>4</span><span>..</span><span>read] </span><span>== </span><span>b</span><span>"</span><span>\r\n\r\n</span><span>" </span><span>{
</span><span>                </span><span>break</span><span>;
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>// we're done, print the request
</span><span>        </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..*</span><span>read]);
</span><span>        println!(</span><span>"</span><span>{}</span><span>"</span><span>, request);
</span><span>
</span><span>        </span><span>// and move into the write state
</span><span>        </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>
</span><span>        self.state </span><span>= </span><span>HandlerState::Write {
</span><span>            response: response.</span><span>as_bytes</span><span>(),
</span><span>            written: </span><span>0</span><span>,
</span><span>        };
</span><span>    }
</span><span>
</span><span>    </span><span>// write the response
</span><span>    </span><span>if </span><span>let </span><span>HandlerState::Write { response, written } </span><span>= &amp;mut </span><span>self.state {
</span><span>        </span><span>// self.connection.write...
</span><span>
</span><span>        </span><span>// successfully wrote the response, try flushing next
</span><span>        self.state </span><span>= </span><span>HandlerState::Flush;
</span><span>    }
</span><span>
</span><span>    </span><span>// flush the response
</span><span>    </span><span>if </span><span>let </span><span>HandlerState::Flush </span><span>= </span><span>self.state {
</span><span>        </span><span>match </span><span>self.connection.</span><span>flush</span><span>() {
</span><span>            </span><span>Ok</span><span>(</span><span>_</span><span>) </span><span>=&gt; </span><span>{}
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>, </span><span>// 👈
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Notice how much nicer things are when tasks are independent, encapsulated objects?</p>
<p>At the end of the task's lifecycle, it removes its connection from the reactor and returns <code>Some</code>. It will never be run again after that point.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>        reactor.</span><span>remove</span><span>(self.connection.</span><span>as_raw_fd</span><span>());
</span><span>    });
</span><span>
</span><span>    </span><span>Some</span><span>(())
</span><span>}
</span></code></pre>
<p>Perfect! Our new server looks a lot nicer. Individual tasks are completely independent of each other, and we can spawn new tasks just like threads.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Main::Start);
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>// main task: accept loop
</span><span>enum </span><span>Main {
</span><span>    Start,
</span><span>    Accept { listener: TcpListener },
</span><span>}
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>Main {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;()&gt; {
</span><span>        </span><span>if </span><span>let </span><span>Main::Start </span><span>= </span><span>self {
</span><span>            </span><span>// ...
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>                reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>            });
</span><span>
</span><span>            </span><span>*</span><span>self </span><span>= </span><span>Main::Accept { listener };
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>Main::Accept { listener } </span><span>= </span><span>self {
</span><span>            </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>                </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// ...
</span><span>                    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Handler {
</span><span>                        connection,
</span><span>                        state: HandlerState::Start,
</span><span>                    });
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>None
</span><span>    }
</span><span>}
</span><span>
</span><span>// handler task: handles every connection
</span><span>struct </span><span>Handler {
</span><span>    connection: TcpStream,
</span><span>    state: HandlerState,
</span><span>}
</span><span>
</span><span>enum </span><span>HandlerState {
</span><span>    Start,
</span><span>    Read {
</span><span>        request: [</span><span>u8</span><span>; </span><span>1024</span><span>],
</span><span>        read: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Write {
</span><span>        response: </span><span>&amp;'static </span><span>[</span><span>u8</span><span>],
</span><span>        written: </span><span>usize</span><span>,
</span><span>    },
</span><span>    Flush,
</span><span>}
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>Handler {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Start </span><span>= </span><span>self.state {
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>                reactor.</span><span>add</span><span>(self.connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>            });
</span><span>
</span><span>            self.state </span><span>= </span><span>HandlerState::Read { </span><span>/* .. */ </span><span>};
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Read { request, read } </span><span>= &amp;mut </span><span>self.state {
</span><span>            </span><span>// ...
</span><span>            self.state </span><span>= </span><span>HandlerState::Write { </span><span>/* .. */ </span><span>};
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Write { response, written } </span><span>= &amp;mut </span><span>self.state {
</span><span>            </span><span>// ...
</span><span>            self.state </span><span>= </span><span>HandlerState::Flush;
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>HandlerState::Flush </span><span>= </span><span>self.state {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>remove</span><span>(self.connection.</span><span>as_raw_fd</span><span>());
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(())
</span><span>    }
</span><span>}
</span></code></pre>
<p>Andd....</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<p>It works!</p>
<h2 id="a-functional-server"><a href="#a-functional-server" aria-label="Anchor link for: a-functional-server">A Functional Server</a></h2>
<p>With this new future abstraction, our server is <em>much</em> nicer than before. Futures get to manage their state independently, the scheduler gets to run tasks without worrying about epoll, and tasks can be spawned and woken without worrying about any of the lower level details of the scheduler. It really is a much nicer programming model.</p>
<p>It is nice that tasks are encapsulated, but we still have to write everything in a state-machine like way. Granted, Rust makes this pretty easy to do with enums, but could we do better?</p>
<p>Looking at the two futures we've written, they have a lot in common. Each future has a number of states. At each state, some code is run. If that code completes successfully, we transition into the next state. If it encounters <code>WouldBlock</code>, we return <code>None</code>, indicating that the future is not yet ready.</p>
<p>This seems like something we can abstract over.</p>
<p>What we need is a way to create a future from some block of code, and a way to <em>combine</em> two futures, chaining them together.</p>
<p>Given a block of code, we need to be able to construct a future... sound like a job for a closure?</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>future_fn</span><span>(</span><span>f</span><span>: F) -&gt; impl Future
</span><span>where
</span><span>    F: Fn(),
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>The closure probably also needs to mutate local state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>future_fn</span><span>(</span><span>f</span><span>: F) -&gt; impl Future
</span><span>where
</span><span>    F: FnMut(),
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>And it also needs access to the waker.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>future_fn</span><span>(</span><span>f</span><span>: F) -&gt; impl Future
</span><span>where
</span><span>    F: FnMut(Waker),
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>And.. it needs to return a value. An optional value, in case it's not ready yet. In fact, we can just copy the signature of <code>poll</code>, because that's really what this closure is 😅</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll_fn</span><span>&lt;F, T&gt;(</span><span>f</span><span>: F) -&gt; impl Future&lt;Output = T&gt;
</span><span>where
</span><span>    F: FnMut(Waker) -&gt; </span><span>Option</span><span>&lt;T&gt;,
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Implementing <code>poll_fn</code> doesn't seem too hard, we just need a wrapper struct that implements <code>Future</code> and delegates <code>poll</code> to the closure.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>poll_fn</span><span>&lt;F, T&gt;(</span><span>f</span><span>: F) -&gt; impl Future&lt;Output = T&gt;
</span><span>where
</span><span>    F: FnMut(Waker) -&gt; </span><span>Option</span><span>&lt;T&gt;,
</span><span>{
</span><span>    </span><span>struct </span><span>FromFn&lt;F&gt;(F);
</span><span>
</span><span>    </span><span>impl</span><span>&lt;F, T&gt; Future </span><span>for </span><span>FromFn&lt;F&gt;
</span><span>    </span><span>where
</span><span>        F: FnMut(Waker) -&gt; </span><span>Option</span><span>&lt;T&gt;,
</span><span>    {
</span><span>        </span><span>type </span><span>Output </span><span>=</span><span> T;
</span><span>
</span><span>        </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>            (self.</span><span>0</span><span>)(waker)
</span><span>        }
</span><span>    }
</span><span>
</span><span>    FromFn(f)
</span><span>}
</span></code></pre>
<p>Alright. Let's try reworking the main task to use the new <code>poll_fn</code> helper. We can easily stick the code of the <code>Main::Start</code> state into a closure.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>listen</span><span>());
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> start </span><span>= </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>        listener.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    });
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Remember, <code>Main::Start</code> never waits on any I/O, so it's immediately ready with the listener.</p>
<p>We can also use <code>poll_fn</code> to write the <code>Main::Accept</code> future.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> start </span><span>= </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    });
</span><span>
</span><span>    </span><span>let</span><span> accept </span><span>= </span><span>poll_fn</span><span>(|_| </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>            connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>            </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Handler {
</span><span>                connection,
</span><span>                state: HandlerState::Start,
</span><span>            });
</span><span>
</span><span>            </span><span>None
</span><span>        }
</span><span>        </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>        </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>    });
</span><span>}
</span></code></pre>
<p>On the other hand, <code>accept</code> always returns <code>None</code> because we <em>want</em> it to be called every time a new connection comes in. It runs for the entirety of our program.</p>
<p>We have our two task states, now we need a way to chain them together.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>chain</span><span>&lt;F1, F2&gt;(</span><span>future1</span><span>: F1, </span><span>future2</span><span>: F2) -&gt; impl Future&lt;Output = F2::Output&gt;
</span><span>where
</span><span>    F1: Future,
</span><span>    F2: Future
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Hm, that doesn't really work.</p>
<p>The second future will need to access the output of the first, the TCP listener.</p>
<p>Instead of chaining the second future directly, we have to chain a closure over the first future's output. That way the closure can use the output of the first future to construct the second.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>chain</span><span>&lt;T1, F, T2&gt;(</span><span>future1</span><span>: T1, </span><span>chain</span><span>: F) -&gt; impl Future&lt;Output = T2::Output&gt;
</span><span>where
</span><span>    T1: Future,
</span><span>    F: FnOnce(T1::Output) -&gt; T2,
</span><span>    T2: Future
</span><span>{
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>That seems better.</p>
<p>We might as well be fancy and have <code>chain</code> be a method on the <code>Future</code> trait. That way we can call <code>.chain</code> as a postfix method on any future.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>fn </span><span>chain</span><span>&lt;F, T&gt;(</span><span>self</span><span>, </span><span>chain</span><span>: F) -&gt; Chain&lt;</span><span>Self</span><span>, F, T&gt;
</span><span>    </span><span>where
</span><span>        F: FnOnce(</span><span>Self::</span><span>Output) -&gt; T,
</span><span>        T: Future,
</span><span>        </span><span>Self</span><span>: Sized,
</span><span>    {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span><span>
</span><span>enum </span><span>Chain&lt;T1, F, T2&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>That looks right, let's try implementing it!</p>
<p>The <code>Chain</code> future is a generalization of our state machines, so it itself is a mini state machine. It starts off by polling the first future, holding onto the transition closure for when it finishes.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Chain&lt;T1, F, T2&gt; {
</span><span>    First { future1: </span><span>T1</span><span>, transition: F },
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;T1, F, T2&gt; Future </span><span>for </span><span>Chain&lt;T1, F, T2&gt;
</span><span>where
</span><span>    T1: Future,
</span><span>    F: FnOnce(T1::Output) -&gt; T2,
</span><span>    T2: Future,
</span><span>{
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>T2</span><span>::Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>Chain::First { future1, transition } </span><span>= </span><span>self {
</span><span>            </span><span>// poll the first future
</span><span>            </span><span>match</span><span> future1.</span><span>poll</span><span>(waker) {
</span><span>                </span><span>// ...
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Once the first future is finished, it constructs the second future using the <code>transition</code> closure, and starts polling it:</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Chain&lt;T1, F, T2&gt; {
</span><span>    First { future1: </span><span>T1</span><span>, transition: F },
</span><span>    Second { future2: </span><span>T2 </span><span>},
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;T1, F, T2&gt; Future </span><span>for </span><span>Chain&lt;T1, F, T2&gt;
</span><span>where
</span><span>    T1: Future,
</span><span>    F: FnOnce(T1::Output) -&gt; T2,
</span><span>    T2: Future,
</span><span>{
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>T2</span><span>::Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>Chain::First { future1, transition } </span><span>= </span><span>self {
</span><span>            </span><span>// poll the first future
</span><span>            </span><span>match</span><span> future1.</span><span>poll</span><span>(waker.</span><span>clone</span><span>()) {
</span><span>                </span><span>Some</span><span>(value) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>// first future is done, transition into the second
</span><span>                    </span><span>let</span><span> future2 </span><span>= </span><span>(transition)(value); </span><span>// 👈
</span><span>                    </span><span>*</span><span>self </span><span>= </span><span>Chain::Second { future2 };
</span><span>                }
</span><span>                </span><span>// first future is not ready, return
</span><span>                </span><span>None </span><span>=&gt; return </span><span>None</span><span>,
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>Chain::Second { future2 } </span><span>= </span><span>self {
</span><span>            </span><span>// first future is already done, poll the second
</span><span>            </span><span>return</span><span> future2.</span><span>poll</span><span>(waker); </span><span>// 👈
</span><span>        }
</span><span>
</span><span>        </span><span>None
</span><span>    }
</span><span>}
</span></code></pre>
<p>Notice how the same <code>waker</code> is used to poll both futures. This means that notifications for both futures will be propagated to the <code>Chain</code> parent future, depending on its state.</p>
<p>Hm... that doesn't actually seem to work:</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0507</span><span>]: cannot </span><span>move</span><span> out of `</span><span>*</span><span>transition` which is behind a mutable reference
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>182</span><span>:</span><span>33
</span><span>    </span><span>|
</span><span>182 </span><span>|    </span><span>let</span><span> future2 </span><span>= </span><span>(transition)(value);
</span><span>    |                  ^^^^^^^^^^^^ move occurs because `*transition` has type `F`,
</span><span>                                    which does not implement the `</span><span>Copy</span><span>` </span><span>trait
</span></code></pre>
<p>Oh right, <code>transition</code> is an <code>FnOnce</code> closure, meaning it is consumed the first time it is called. We only ever call it once based on our state machine, but the compiler doesn't know that.</p>
<p>We can wrap it in an <code>Option</code> and use <code>take</code> to call it, replacing it with <code>None</code> and allowing us to get an owned value. This is a common pattern when working with state machines.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Chain&lt;T1, F, T2&gt; {
</span><span>    First { future1: </span><span>T1</span><span>, transition: </span><span>Option</span><span>&lt;F&gt; }, </span><span>// 👈
</span><span>    Second { future2: </span><span>T2 </span><span>},
</span><span>}
</span><span>
</span><span>// ...
</span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>    </span><span>if </span><span>let </span><span>Chain::First { future1, transition } </span><span>= </span><span>self {
</span><span>        </span><span>match</span><span> future1.</span><span>poll</span><span>(waker.</span><span>clone</span><span>()) {
</span><span>            </span><span>Some</span><span>(value) </span><span>=&gt; </span><span>{
</span><span>                </span><span>let</span><span> future2 </span><span>= </span><span>(transition.</span><span>take</span><span>().</span><span>unwrap</span><span>())(value); </span><span>// 👈
</span><span>                </span><span>*</span><span>self </span><span>= </span><span>Chain::future2 { future2 };
</span><span>            }
</span><span>            </span><span>None </span><span>=&gt; return </span><span>None</span><span>,
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Perfect. Now the <code>chain</code> method can simply construct our <code>Chain</code> future in it's starting state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Future {
</span><span>    </span><span>// ...
</span><span>    </span><span>fn </span><span>chain</span><span>&lt;F, T&gt;(</span><span>self</span><span>, </span><span>transition</span><span>: F) -&gt; Chain&lt;</span><span>Self</span><span>, F, T&gt;
</span><span>    </span><span>where
</span><span>        F: FnOnce(</span><span>Self::</span><span>Output) -&gt; T,
</span><span>        T: Future,
</span><span>        </span><span>Self</span><span>: Sized,
</span><span>    {
</span><span>        Chain::First {
</span><span>            future1: self,
</span><span>            transition: </span><span>Some</span><span>(transition),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Alright. Where were we... oh right, the main future!</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> start </span><span>= </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    });
</span><span>
</span><span>    </span><span>let</span><span> accept </span><span>= </span><span>poll_fn</span><span>(|_| </span><span>match</span><span> listener.</span><span>accept</span><span>() {
</span><span>        </span><span>// ...
</span><span>    });
</span><span>}
</span></code></pre>
<p>We can combine the two futures using our new <code>chain</code> method:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| { </span><span>// 👈
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>Huh, that seems really nice! Gone is our manual state machine, our listen method can now be expressed in terms of simple closures!</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>listen</span><span>());
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>        </span><span>// ...
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(Handler {
</span><span>                    connection,
</span><span>                    state: HandlerState::Start,
</span><span>                });
</span><span>
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>It's too good to be true!</p>
<p>We can convert the connection handler to a closure-based future just like we did with the main one. To start we'll separate it out into a function that returns a <code>Future</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>handle</span><span>(connection)); </span><span>// 👈
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>}
</span><span>
</span><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>The first state, <code>HandlerState::Start</code>, is a simple <code>poll_fn</code> closure that registers the connection with the reactor and immediately returns.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(())
</span><span>    })
</span><span>}
</span></code></pre>
<p>The second state, <code>HandlerState::Read</code>, can be chained on quite easily. It initializes its local request state on the stack and <em>moves</em> it into the future, allowing the future to own its state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>        </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>]; </span><span>// 👈
</span><span>
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>loop </span><span>{
</span><span>                </span><span>// try reading from the stream
</span><span>                </span><span>match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]) {
</span><span>                    </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                        println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                        </span><span>return </span><span>Some</span><span>(());
</span><span>                    }
</span><span>                    </span><span>Ok</span><span>(n) </span><span>=&gt;</span><span> read </span><span>+=</span><span> n,
</span><span>                    </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>                    </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>                }
</span><span>
</span><span>                </span><span>// did we reach the end of the request?
</span><span>                </span><span>let</span><span> read </span><span>=</span><span> read;
</span><span>                </span><span>if</span><span> read </span><span>&gt;= </span><span>4 </span><span>&amp;&amp; &amp;</span><span>request[read </span><span>- </span><span>4</span><span>..</span><span>read] </span><span>== </span><span>b</span><span>"</span><span>\r\n\r\n</span><span>" </span><span>{
</span><span>                    </span><span>break</span><span>;
</span><span>                }
</span><span>            }
</span><span>
</span><span>            </span><span>// we're done, print the request
</span><span>            </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..</span><span>read]);
</span><span>            println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>
</span><span>            </span><span>Some</span><span>(())
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p><code>HandlerState::Write</code> and <code>HandlerState::Flush</code> can be chained on the same way.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>// REACTOR.register...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>// connection.read...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>        </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>loop </span><span>{
</span><span>                </span><span>match</span><span> connection.</span><span>write</span><span>(response[written</span><span>..</span><span>].</span><span>as_bytes</span><span>()) {
</span><span>                    </span><span>Ok</span><span>(</span><span>0</span><span>) </span><span>=&gt; </span><span>{
</span><span>                        println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>                        </span><span>return </span><span>Some</span><span>(());
</span><span>                    }
</span><span>                    </span><span>Ok</span><span>(n) </span><span>=&gt;</span><span> written </span><span>+=</span><span> n,
</span><span>                    </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; return </span><span>None</span><span>,
</span><span>                    </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>                }
</span><span>
</span><span>                </span><span>// did we write the whole response yet?
</span><span>                </span><span>if</span><span> written </span><span>==</span><span> response.</span><span>len</span><span>() {
</span><span>                    </span><span>break</span><span>;
</span><span>                }
</span><span>            }
</span><span>
</span><span>            </span><span>Some</span><span>(())
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>match</span><span> connection.</span><span>flush</span><span>() {
</span><span>                </span><span>Ok</span><span>(</span><span>_</span><span>) </span><span>=&gt; </span><span>{}
</span><span>                </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>{
</span><span>                    </span><span>return </span><span>None</span><span>;
</span><span>                }
</span><span>                </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>            };
</span><span>
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| reactor.</span><span>remove</span><span>(connection.</span><span>as_raw_fd</span><span>());
</span><span>            </span><span>Some</span><span>(())
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>It's beautiful.</p>
<p>Uhhhh...</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0382</span><span>]: </span><span>use</span><span> of moved value: `connection`
</span><span>  </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>59</span><span>:</span><span>12
</span><span>   </span><span>|
</span><span>51 </span><span>| </span><span>fn </span><span>handle</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>   </span><span>|           -------------- move</span><span> occurs because `connection` has </span><span>type</span><span> `TcpStream`, which does not implement the `</span><span>Copy</span><span>` </span><span>trait
</span><span>52 </span><span>|     </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>   |             ------------- value moved into closure here
</span><span>53 </span><span>|         </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>54 </span><span>|</span><span>             reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>   |                          ---------- variable moved due to use in closure
</span><span>...
</span><span>59 </span><span>|     </span><span>.</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>   |            ^^^^^^^^ value used here after move
</span><span>...
</span><span>66 </span><span>|                 match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]) {
</span><span>   |                       ---------- use occurs due to use in closure
</span><span>
</span><span>error[</span><span>E0382</span><span>]: </span><span>use</span><span> of moved value: `connection`
</span><span>// ...
</span></code></pre>
<p>Hmm....</p>
<p>All of our futures use <code>move</code> closures, meaning they take ownership of the connection. There can only be one owner of the connection though. Guess they shouldn't be <code>move</code> closures?</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0373</span><span>]: closure may outlive the current function, but it borrows `connection`, which is owned by the current function
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>52</span><span>:</span><span>13
</span><span>    </span><span>|
</span><span>52  </span><span>|     </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>    |             ^^^^^^^^ may outlive borrowed value `connection`
</span><span>53  </span><span>|         </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>54  </span><span>|</span><span>             reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>    |                          ---------- `connection` is borrowed here
</span><span>    </span><span>|
</span><span>note: closure is returned here
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>52</span><span>:</span><span>5
</span><span>    </span><span>|
</span><span>52  </span><span>| /     </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>53  </span><span>| |         </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>54  </span><span>| |</span><span>             reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>55  </span><span>| |         </span><span>});
</span><span>...   |
</span><span>128 </span><span>| |         </span><span>})
</span><span>129 </span><span>| |     </span><span>})
</span><span>    </span><span>| |</span><span>______</span><span>^
</span><span>help: to force the closure to take ownership of `connection` (and any other referenced variables), </span><span>use</span><span> the `</span><span>move</span><span>` keyword
</span><span>    </span><span>|
</span><span>52  </span><span>|     </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>    |             ++++
</span></code></pre>
<p>That doesn't seem to work either. The <code>connection</code> needs to live <em>somewhere</em>. What if we only move it into the first future, and have the rest of the futures borrow it?</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0382</span><span>]: </span><span>use</span><span> of moved value: `connection`
</span><span>  </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>59</span><span>:</span><span>12
</span><span>   </span><span>|
</span><span>51 </span><span>| </span><span>fn </span><span>handle</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>   </span><span>|           -------------- move</span><span> occurs because `connection` has </span><span>type</span><span> `TcpStream`, which does not implement the `</span><span>Copy</span><span>` </span><span>trait
</span><span>52 </span><span>|     </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>   |             ------------- value moved into closure here
</span><span>53 </span><span>|         </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>54 </span><span>|</span><span>             reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>   |                          ---------- variable moved due to use in closure
</span><span>...
</span><span>59 </span><span>|     </span><span>.</span><span>chain</span><span>(|_| {
</span><span>   |            ^^^ value used here after move
</span><span>...
</span><span>66 </span><span>|                 match</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]) {
</span><span>   |                       ---------- use occurs due to use in closure
</span></code></pre>
<p>Nope, that doesn't work either.</p>
<p>Under the hood, our chained futures look something like this. The first future owns the connection, and the rest borrow from it.</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Handle {
</span><span>    Start {
</span><span>        connection: TcpStream,
</span><span>    }
</span><span>    Read {
</span><span>        connection: </span><span>&amp;'???</span><span> TcpStream
</span><span>    }
</span><span>}
</span></code></pre>
<p>Which of course, doesn't make much sense. Once the state transitions into <code>Read</code>, the connection from <code>Start</code> is dropped, and we have nothing to reference.</p>
<p>So how did this work when we were writing futures manually?</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>Handler {
</span><span>    connection: TcpStream,
</span><span>    state: HandlerState,
</span><span>}
</span><span>
</span><span>enum </span><span>HandlerState { </span><span>/* ... */ </span><span>}
</span></code></pre>
<p>Right, the connection lived in the outer struct. Maybe we can write another one of those future helpers that allows us to reference some data stored in an outer future?</p>
<p>Something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>WithData&lt;D, F&gt; {
</span><span>    data: D,
</span><span>    future: F,
</span><span>}
</span></code></pre>
<p>Seems simple enough. We should be able to construct the future such that it can capture a reference to the data. We can use a closure, just like we did with <code>chain</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;D, F&gt; WithData&lt;D, F&gt; {
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>(</span><span>data</span><span>: D, </span><span>construct</span><span>: impl Fn(&amp;</span><span>D</span><span>) -&gt; F) -&gt; WithData&lt;D, F&gt; {
</span><span>        </span><span>let</span><span> future </span><span>= </span><span>construct</span><span>(</span><span>&amp;</span><span>data);
</span><span>        WithData { data, future }
</span><span>    }
</span><span>}
</span></code></pre>
<p><code>WithData</code> can implement <code>Future</code> by simply delegating to the inner future:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;D, F&gt; Future </span><span>for </span><span>WithData&lt;D, F&gt;
</span><span>where
</span><span>    F: Future,
</span><span>{
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>F::Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        self.future.</span><span>poll</span><span>(waker)
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now we should be able to wrap our future in <code>WithData</code>, giving <code>connection</code> a place to live even after it is returned.. and everything should work!</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    WithData::new(connection, |</span><span>connection</span><span>| {
</span><span>        </span><span>from_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>            </span><span>// ...
</span><span>        })
</span><span>        .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>            </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>            </span><span>from_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>                </span><span>// ...
</span><span>            })
</span><span>        })
</span><span>        .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>            </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>            </span><span>from_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>                </span><span>// ...
</span><span>            })
</span><span>        })
</span><span>        .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>from_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>                </span><span>// ...
</span><span>            })
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>A little funky, but if it works...</p>
<pre data-lang="rust"><code data-lang="rust"><span>error: lifetime may not live long enough
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>main.rs:</span><span>53</span><span>:</span><span>9
</span><span>    </span><span>|
</span><span>52  </span><span>|       </span><span>WithData::new(connection, |</span><span>connection</span><span>| {
</span><span>    |                                  ----------- return type of closure `Chain&lt;Chain&lt;Chain&lt;impl Future&lt;Output = ()&gt;, [closure@src/bin/play.</span><span>rs</span><span>:60:16: 86:10], impl Future&lt;Output = ()&gt;&gt;, [closure@src/bin/play.</span><span>rs</span><span>:87:16: 113:10], impl Future&lt;Output = ()&gt;&gt;, [closure@src/bin/play.</span><span>rs</span><span>:114:16: 130:10], impl Future&lt;Output = ()&gt;&gt;` contains a lifetime `'2`
</span><span>    </span><span>|                                  |
</span><span>    </span><span>|</span><span>                                  has </span><span>type</span><span> `</span><span>&amp;'</span><span>1</span><span> TcpStream`
</span><span>53  </span><span>| /         </span><span>from_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>54  </span><span>| |             </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>55  </span><span>| |</span><span>                 reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>56  </span><span>| |             </span><span>});
</span><span>...   |
</span><span>129 </span><span>| |             </span><span>})
</span><span>130 </span><span>| |         </span><span>})
</span><span>    </span><span>| |</span><span>__________</span><span>^</span><span> returning this value requires that `</span><span>'</span><span>1</span><span>` must outlive `</span><span>'</span><span>2</span><span>`
</span></code></pre>
<p>It couldn't be that easy could it.</p>
<p>What a weird error message too.</p>
<blockquote>
<pre><code><span>return type of closure `Chain&lt;Chain&lt;Chain&lt;impl Future&lt;Output = ()&gt;, [closure
</span><span>@src/bin/play.rs:60:16: 86:10], impl Future&lt;Output = ()&gt;&gt;, [closure@src/bin/
</span><span>play.rs:87:16: 113: 10], impl Future&lt;Output = ()&gt;&gt;, [closure@src/bin/play.rs
</span><span>:114:16: 130:10], impl Future&lt;Output = ()&gt;&gt;` contains a lifetime `'2`
</span></code></pre>
</blockquote>
<p>Alright, so, the giant chained future we pass to <code>WithData</code> contains a reference to the connection.</p>
<p>That's what we want, for the future to borrow the connection, right?</p>
<blockquote>
<pre><code><span>`connection` has type `&amp;'1 TcpStream` ... returning this value requires
</span><span>that `'1` must outlive `'2`
</span></code></pre>
</blockquote>
<p>Hmmm, nowhere in our <code>WithData</code> struct did we actually specify that the future borrows from the data. It seems Rust can't figure out the lifetimes without that. So.. we should probably add a lifetime to <code>WithData</code> right?</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>WithData&lt;</span><span>'data</span><span>, D, F&gt; {
</span><span>    data: D,
</span><span>    future: F,
</span><span>}
</span></code></pre>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0392</span><span>]: parameter `</span><span>'data</span><span>` is never used
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>bin</span><span>/</span><span>play.rs:</span><span>160</span><span>:</span><span>17
</span><span>    </span><span>|
</span><span>160 </span><span>| </span><span>struct </span><span>WithData&lt;</span><span>'data</span><span>, D, F&gt; {
</span><span>    |                 ^^^^^ unused parameter
</span><span>    |
</span><span>    = help: consider removing `</span><span>'data</span><span>`, referring to it in a field, or using a marker such as `PhantomData`
</span></code></pre>
<p>Adding <code>PhantomData</code> seems like an easy fix.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>WithData&lt;</span><span>'data</span><span>, D, F&gt; {
</span><span>    data: D,
</span><span>    future: F,
</span><span>    _data: PhantomData&lt;</span><span>&amp;'data</span><span> D&gt;,
</span><span>}
</span></code></pre>
<p>The future does reference <code>&amp;'data D</code>, so that sort of makes sense. Now in the constructor, we should say that the future borrows from the data:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;</span><span>'data</span><span>, D, F&gt; WithData&lt;</span><span>'data</span><span>, D, F&gt;
</span><span>where
</span><span>    F: Future + </span><span>'data</span><span>, </span><span>// 👈
</span><span>{
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>(
</span><span>        </span><span>data</span><span>: D,
</span><span>        </span><span>construct</span><span>: impl Fn(&amp;'</span><span>data D</span><span>) -&gt; F, </span><span>// 👈
</span><span>    ) -&gt; WithData&lt;</span><span>'data</span><span>, D, F&gt; {
</span><span>        </span><span>let</span><span> future </span><span>= </span><span>construct</span><span>(</span><span>&amp;</span><span>data);
</span><span>
</span><span>        WithData {
</span><span>            data,
</span><span>            future,
</span><span>            _data: PhantomData,
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>And that should work, right? All the lifetimes are written out and make sense:</p>
<pre data-lang="rust"><code data-lang="rust"><span>error[</span><span>E0597</span><span>]: `data` does not live long enough
</span><span>   </span><span>-</span><span>-&gt; src</span><span>/</span><span>bin</span><span>/</span><span>play.rs:</span><span>172</span><span>:</span><span>30
</span><span>    </span><span>|
</span><span>167 </span><span>| </span><span>impl</span><span>&lt;</span><span>'data</span><span>, D, F&gt; WithData&lt;</span><span>'data</span><span>, D, F&gt;
</span><span>    |      ----- lifetime `</span><span>'data</span><span>` defined here
</span><span>...
</span><span>172 |         let future = construct(</span><span>&amp;</span><span>data);
</span><span>    |                      ----------^^^^^-
</span><span>    |                      |         |
</span><span>    |                      |         borrowed value does not live long enough
</span><span>    |                      argument requires that `data` is borrowed for `</span><span>'data</span><span>`
</span><span>...
</span><span>178 |     }
</span><span>    |     - `data` dropped here while still borrowed
</span><span>
</span><span>error[E0505]: cannot move out of `data` because it is borrowed
</span><span>   --&gt; src/bin/play.rs:174:13
</span><span>    |
</span><span>167 | impl&lt;</span><span>'data</span><span>, D, F&gt; WithData&lt;</span><span>'data</span><span>, D, F&gt;
</span><span>    |      ----- lifetime `</span><span>'data</span><span>` defined here
</span><span>...
</span><span>172 |         let future = construct(</span><span>&amp;</span><span>data);
</span><span>    |                      ----------------
</span><span>    |                      |         |
</span><span>    |                      |         borrow of `data` occurs here
</span><span>    |                      argument requires that `data` is borrowed for `</span><span>'data</span><span>`
</span><span>173 |         WithData {
</span><span>174 </span><span>|</span><span>             data,
</span><span>    |             ^^^^ move out of `data` occurs here
</span></code></pre>
<p>Or... not.</p>
<p>Why doesn't this work?</p>
<blockquote>
<pre><code><span>`data` dropped here while still borrowed
</span></code></pre>
</blockquote>
<p>Wait a minute, that's the same error message we got when we removed <code>move</code> from our future closures?! But the data <em>does</em> have a place to live now... doesn't it?</p>
<p>Hmm.. actually, the second error is telling us that moving <code>data</code> is wrong too:</p>
<blockquote>
<pre><code><span>cannot move out of `data` because it is borrowed
</span></code></pre>
</blockquote>
<p>That... actually makes sense. The future we construct borrows the <code>data</code> that lives on the stack. Once we move it, it's <strong>no longer in the same place on the stack</strong>. Its address changes, so the future's reference to the data is actually invalidated.</p>
<pre><code><span>## Before Moving
</span><span>
</span><span>       ┌─────────────┐
</span><span>       │0101001010010│
</span><span>data:  │001...       │ ◄──────── &amp;future.data
</span><span>       │             │
</span><span>       │             │
</span><span>       └─────────────┘
</span><span>
</span><span>
</span><span>## After Moving
</span><span>
</span><span>             ???       ◄──────── &amp;future.data
</span><span>
</span><span>
</span><span>       ┌─────────────┐
</span><span>       │0101001010010│
</span><span>data:  │001...       │
</span><span>       │             │
</span><span>       │             │
</span><span>       └─────────────┘
</span></code></pre>
<p>We gave the data a place to live, but we didn't give it a <em>stable</em> place to live. It turns out, this is a well-known problem in Rust. What we're trying to create is called a <em>self-referential struct</em>, and it's not possible to do safely.</p>
<p>Back when our entire future state was in the <code>Handler</code> struct, there was no self-referencing going on. Everything just worked off the <code>Handler</code>. But now that we're trying to split our futures up into subtasks, we need a way for them to access the data independently.</p>
<p>So is it not possible?</p>
<p>Well...</p>
<p>We <em>could</em> allocate the data l using <code>Rc</code> and clone the pointer into each of the futures. That way the futures get a stable pointer to the data on the heap, and it's only deallocated after all the futures complete.</p>
<p>The code is about to get pretty ugly.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> connection </span><span>= </span><span>Rc::new(connection); </span><span>// 👈
</span><span>    </span><span>let</span><span> read_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>    </span><span>let</span><span> write_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>    </span><span>let</span><span> flush_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>// ...
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let</span><span> connection </span><span>= &amp;*</span><span>read_connection_ref;
</span><span>
</span><span>            </span><span>loop </span><span>{
</span><span>                </span><span>match </span><span>(</span><span>&amp;mut</span><span> connection).</span><span>read</span><span>(</span><span>&amp;mut</span><span> request) {
</span><span>                    </span><span>// ...
</span><span>                }
</span><span>            }
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>// ...
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let</span><span> connection </span><span>= &amp;*</span><span>write_connection_ref;
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>let</span><span> connection </span><span>= &amp;*</span><span>flush_connection_ref;
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>Oh no..</p>
<pre><code><span>error[E0277]: `Rc&lt;TcpStream&gt;` cannot be sent between threads safely
</span><span>   --&gt; src/main.rs:90:37
</span><span>    |
</span><span>90  |     SCHEDULER.spawn(handle(connection));
</span><span>    |               ----- ^^^^^^^^^^^^^^^^^^ `Rc&lt;TcpStream&gt;` cannot be sent between threads safely
</span><span>    |               |
</span><span>    |               required by a bound introduced by this call
</span><span>...
</span><span>100 |     fn handle(mut connection: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    |                                             ------------------------ within this `impl Future&lt;Output = ()&gt;`
</span></code></pre>
<p>Using <code>Rc</code> in our handler makes it <code>!Send</code>. Even though the connection is only used internally within the future and futures are only ever run by the main thread, we need to use an <code>Arc</code> to satisfy the compiler.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> connection </span><span>= </span><span>Arc::new(connection); </span><span>// 👈
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>A little sad, but at least it compiles.</p>
<p>Our server has no more manual state machines and is looking pretty clean.</p>
<p>A <em>lot</em> cleaner than when we started with epoll manually, even with the messy <code>Arc</code> business.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>listen</span><span>());
</span><span>    </span><span>SCHEDULER</span><span>.</span><span>run</span><span>();
</span><span>}
</span><span>
</span><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>// ...
</span><span>
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(listener.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(listener)
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>handle</span><span>(connection));
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>}
</span><span>
</span><span>fn </span><span>handle</span><span>(</span><span>connection</span><span>: TcpStream) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> connection </span><span>= </span><span>Arc::new(connection);
</span><span>    </span><span>let</span><span> read_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>    </span><span>let</span><span> write_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>    </span><span>let</span><span> flush_connection_ref </span><span>=</span><span> connection.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>            reactor.</span><span>add</span><span>(connection.</span><span>as_raw_fd</span><span>(), waker);
</span><span>        });
</span><span>
</span><span>        </span><span>Some</span><span>(())
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>        </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>        </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>// ...
</span><span>        })
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |_| </span><span>{
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>            </span><span>// ...
</span><span>            </span><span>REACTOR</span><span>.</span><span>with</span><span>(|</span><span>reactor</span><span>| {
</span><span>                reactor.</span><span>remove</span><span>(flush_connection_ref.</span><span>as_raw_fd</span><span>());
</span><span>            });
</span><span>
</span><span>            </span><span>Some</span><span>(())
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>And of course...</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ curl localhost:3000
</span><span># =&gt; Hello world!
</span></code></pre>
<p>It works!</p>
<h2 id="a-graceful-server"><a href="#a-graceful-server" aria-label="Anchor link for: a-graceful-server">A Graceful Server</a></h2>
<p>Whew, that was a lot.</p>
<p>One last thing before we finish. To put our task model to the test, we can finally implement the graceful shutdown mechanism we discussed earlier.</p>
<blockquote>
<p>Imagine we wanted to implement graceful shutdown for our server. When someone hits the keys ctrl+c, instead of killing the program abruptly, we should stop accepting new connections and wait for any active requests to complete. Any requests that take more than 30 seconds to handle are killed as the server exits.</p>
</blockquote>
<p>There are a couple things we have to do to set this up. Firstly, we have to actually detect the signal. On Linux, ctrl+c triggers the <code>SIGINT</code> signal, so we can use the <code>signal_hook</code> crate to wait until the signal is received.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>signal_hook::consts::signal::</span><span>SIGINT</span><span>;
</span><span>use </span><span>signal_hook::iterator::Signals;
</span><span>
</span><span>fn </span><span>ctrl_c</span><span>() {
</span><span>    </span><span>let </span><span>mut</span><span> signal </span><span>= </span><span>Signals::new(</span><span>&amp;</span><span>[</span><span>SIGINT</span><span>]).</span><span>unwrap</span><span>();
</span><span>    </span><span>let</span><span> _ctrl_c </span><span>=</span><span> signal.</span><span>forever</span><span>().</span><span>next</span><span>().</span><span>unwrap</span><span>();
</span><span>}
</span></code></pre>
<p>There's a problem though. <code>forever().next()</code> blocks the thread until the signal is received. Now that our server is async, that means calling <code>ctrl_c()</code> on the main thread will block the entire program.</p>
<p>Instead, we need to represent the ctrl+c signal as a <em>future</em> that resolves when it is received. Something like this.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>ctrl_c</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>        </span><span>// ...
</span><span>    })
</span><span>}
</span></code></pre>
<p>So how do we listen for the signal asynchronously?</p>
<p>We could register a signal handler with epoll, but we could also use this as an opportunity to learn about handling blocking tasks in an async program. There will be times when the only way to get what you want is through a blocking API, but you can't simply call it on the main thread. Instead, you can run the blocking work on a separate thread, and notify the main thread when it completes.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>spawn_blocking</span><span>(</span><span>blocking_work</span><span>: impl FnOnce()) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>// run the blocking work on a separate thread
</span><span>    std::thread::spawn(</span><span>move || </span><span>{
</span><span>        </span><span>blocking_work</span><span>();
</span><span>    });
</span><span>
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ???
</span><span>    }))
</span><span>}
</span></code></pre>
<p>The question is, how do we know when the work is done?</p>
<p>The blocking work is run on a separate thread, outside of the future. It needs access to the waker so it can notify the future when it completes. We only get access to the waker when the future is first polled, so the state needs to start out as <code>None</code>.</p>
<p>We also need a flag that tells the future that the work has completed, in case the work completes before the future is even polled.</p>
<p>These two pieces of state can be stored inside a <code>Mutex</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let</span><span> state: Arc&lt;Mutex&lt;(</span><span>bool</span><span>, </span><span>Option</span><span>&lt;Waker&gt;)&gt;&gt; </span><span>= </span><span>Arc::default();
</span></code></pre>
<p>Once the thread completes the work, it must set the flag to true and call <code>wake</code> if a waker has been stored. It's fine if a waker hasn't been stored yet, the future will see the flag when it's first polled and return immediately.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>spawn_blocking</span><span>(</span><span>blocking_work</span><span>: impl FnOnce() + Send + </span><span>'static</span><span>) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> state: Arc&lt;Mutex&lt;(</span><span>bool</span><span>, </span><span>Option</span><span>&lt;Waker&gt;)&gt;&gt; </span><span>= </span><span>Arc::default();
</span><span>    </span><span>let</span><span> state_handle </span><span>=</span><span> state.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>// run the blocking work on a separate thread
</span><span>    std::thread::spawn(</span><span>move || </span><span>{
</span><span>        </span><span>// run the work
</span><span>        </span><span>blocking_work</span><span>();
</span><span>
</span><span>        </span><span>// mark the task as done
</span><span>        </span><span>let </span><span>(done, waker) </span><span>= &amp;mut *</span><span>state_handle.</span><span>lock</span><span>().</span><span>unwrap</span><span>();
</span><span>        </span><span>*</span><span>done </span><span>= </span><span>true</span><span>;
</span><span>
</span><span>        </span><span>// wake the waker
</span><span>        </span><span>if </span><span>let Some</span><span>(waker) </span><span>=</span><span> waker.</span><span>take</span><span>() {
</span><span>            waker.</span><span>wake</span><span>();
</span><span>        }
</span><span>    });
</span><span>
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    }))
</span><span>}
</span></code></pre>
<p>Now the future needs to access the state and check if the work has completed yet. If not, it stores its waker and returns <code>None</code>, to be woken later when the work does complete.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>spawn_blocking</span><span>(</span><span>blocking_work</span><span>: impl FnOnce() + Send + </span><span>'static</span><span>) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> state: Arc&lt;Mutex&lt;(</span><span>bool</span><span>, </span><span>Option</span><span>&lt;Waker&gt;)&gt;&gt; </span><span>= </span><span>Arc::default();
</span><span>    </span><span>let</span><span> state_handle </span><span>=</span><span> state.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>// run the blocking work on a separate thread
</span><span>    std::thread::spawn(</span><span>move || </span><span>{
</span><span>        </span><span>// ...
</span><span>    });
</span><span>
</span><span>    </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| match &amp;mut *</span><span>state.</span><span>lock</span><span>().</span><span>unwrap</span><span>() {
</span><span>        </span><span>// work is not completed, store our waker and come back later
</span><span>        (</span><span>false</span><span>, state) </span><span>=&gt; </span><span>{
</span><span>            </span><span>*</span><span>state </span><span>= </span><span>Some</span><span>(waker);
</span><span>            </span><span>None
</span><span>        }
</span><span>        </span><span>// the work is completed
</span><span>        (</span><span>true</span><span>, </span><span>_</span><span>) </span><span>=&gt; </span><span>Some</span><span>(()),
</span><span>    })
</span><span>
</span><span>}
</span></code></pre>
<p>The future returned by <code>spawn_blocking</code> serves as an asynchronous version of <code>JoinHandle</code>. We can wait asynchronously on the main thread while the blocking work is run on a separate thread.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>ctrl_c</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>spawn_blocking</span><span>(|| {
</span><span>        </span><span>let </span><span>mut</span><span> signal </span><span>= </span><span>Signals::new(</span><span>&amp;</span><span>[</span><span>SIGINT</span><span>]).</span><span>unwrap</span><span>();
</span><span>        </span><span>let</span><span> _ctrl_c </span><span>=</span><span> signal.</span><span>forever</span><span>().</span><span>next</span><span>().</span><span>unwrap</span><span>();
</span><span>    })
</span><span>}
</span></code></pre>
<p><code>spawn_blocking</code> is an extremely convenient abstraction for dealing with blocking APIs in an async program.</p>
<p>Alright, we now have a future that waits for the ctrl+c signal!</p>
<p>If you remember from back when our server used blocking I/O, we wondered how to watch for the signal in a way that aborts the connection listener loop immediately after the signal arrives. We realized we needed some way to listen for both incoming connections, and the ctrl+c signal, <em>at the same time</em>.</p>
<p>Because <code>accept</code> was blocking, it wasn't that simple. But with futures, it's actually possible!</p>
<p>We can implement this as another future wrapper. Given two futures, we should be able to create a wrapper future that selects between either of the future's outputs, depending on which future completed first.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>select</span><span>&lt;L, R&gt;(</span><span>left</span><span>: L, </span><span>right</span><span>: R) -&gt; Select&lt;L, R&gt; {
</span><span>    Select { left, right }
</span><span>}
</span><span>
</span><span>struct </span><span>Select&lt;L, R&gt; {
</span><span>    left: L,
</span><span>    right: R
</span><span>}
</span><span>
</span><span>enum </span><span>Either&lt;L, R&gt; {
</span><span>    Left(L),
</span><span>    Right(R)
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;L, R&gt; Future </span><span>for </span><span>Select&lt;L, R&gt; {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>Either&lt;L, R&gt;;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Self::</span><span>Output {
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>It turns out that the implementation of the select future is really simple. We just attempt to poll both futures and return when the first one resolves.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;L, R&gt; Future </span><span>for </span><span>Select&lt;L, R&gt; {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>Either&lt;</span><span>L::</span><span>Output, </span><span>R::</span><span>Output&gt;;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>waker</span><span>: Waker) -&gt; </span><span>Option</span><span>&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let Some</span><span>(output) </span><span>= </span><span>self.left.</span><span>poll</span><span>(waker.</span><span>clone</span><span>()) {
</span><span>            </span><span>return </span><span>Some</span><span>(Either::Left(output));
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let Some</span><span>(output) </span><span>= </span><span>self.right.</span><span>poll</span><span>(waker) {
</span><span>            </span><span>return </span><span>Some</span><span>(Either::Right(output));
</span><span>        }
</span><span>
</span><span>        </span><span>None
</span><span>    }
</span><span>}
</span></code></pre>
<p>Because we pass the same waker to both futures, any progress in either future will notify us, and we can check if either of them completed.</p>
<p>It really is that simple.</p>
<p>Now back to our main program.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).</span><span>unwrap</span><span>();
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>handle</span><span>(connection));
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>We can combine the TCP listener task with the ctrl+c listener using our new <code>select</code> combinator. This way we can listen for both at the same time:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>let</span><span> listen </span><span>= </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// ...
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(</span><span>handle</span><span>(connection));
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        });
</span><span>
</span><span>        </span><span>select</span><span>(listen, </span><span>ctrl_c</span><span>())
</span><span>    })
</span><span>}
</span></code></pre>
<p>The TCP listener task never resolves — remember, it represents the loop from our synchronous server. <code>ctrl_c()</code> can resolve though, so we need to chain on another task to handle the shutdown signal.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>listener</span><span>| {
</span><span>        </span><span>let</span><span> listen </span><span>= </span><span>/* ... */</span><span>;
</span><span>        </span><span>select</span><span>(listen, </span><span>ctrl_c</span><span>())
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>_ctrl_c</span><span>| </span><span>graceful_shutdown</span><span>())
</span><span>}
</span><span>
</span><span>fn </span><span>graceful_shutdown</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Now we need to implement the rest of our shutdown logic. Once the shutdown signal is received, we wait at most thirty seconds for any active requests to complete before shutting down.</p>
<p>This sounds like another use case for <code>select</code>! Either thirty seconds elapse, or all active requests complete.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>graceful_shutdown</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> timer </span><span>= </span><span>/* ... */</span><span>;
</span><span>    </span><span>let</span><span> request_counter </span><span>= </span><span>/* ... */</span><span>;
</span><span>
</span><span>    </span><span>select</span><span>(timer, request_counter).</span><span>chain</span><span>(|_| {
</span><span>        </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>            </span><span>// graceful shutdown process complete, now we actually exit
</span><span>            println!(</span><span>"Graceful shutdown complete"</span><span>);
</span><span>            std::process::exit(</span><span>0</span><span>)
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>All we need to do now is create the two futures for our shutdown conditions.</p>
<p>First we need a timer. Of course, we can't simply call <code>thread::sleep</code> because it's a blocking function. But we <em>could</em> run it through <code>spawn_blocking</code>, and use the handle to represent our timer future.</p>
<blockquote>
<p>Note that there <em>are</em> ways to build async timers around epoll, but that's out of scope for this article.</p>
</blockquote>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::thread;
</span><span>use </span><span>std::time::Duration;
</span><span>
</span><span>fn </span><span>graceful_shutdown</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> timer </span><span>= </span><span>spawn_blocking</span><span>(|| thread::sleep(Duration::from_secs(</span><span>30</span><span>)));
</span><span>    </span><span>let</span><span> request_counter </span><span>= </span><span>/* ... */</span><span>;
</span><span>
</span><span>    </span><span>select</span><span>(timer, request_counter).</span><span>chain</span><span>(|_| {
</span><span>        </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>            </span><span>// graceful shutdown process complete, now we actually exit
</span><span>            println!(</span><span>"Graceful shutdown complete"</span><span>);
</span><span>            std::process::exit(</span><span>0</span><span>)
</span><span>        })
</span><span>    })
</span><span>}
</span></code></pre>
<p>That was simple enough.</p>
<p>Now for the main shutdown condition. For us to know when all active requests are completed, we'll need a counter for active requests.</p>
<p>We can keep the counter local to our <code>listen</code> future, and increment/decrement it whenever tasks are spawned, or complete.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> tasks </span><span>= </span><span>Arc::new(Mutex::new(</span><span>0</span><span>));
</span><span>
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |</span><span>listener</span><span>| </span><span>{
</span><span>        </span><span>let</span><span> listen </span><span>= </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                </span><span>// increment the counter
</span><span>                </span><span>*</span><span>tasks.</span><span>lock</span><span>().</span><span>unwrap</span><span>() </span><span>+= </span><span>1</span><span>; </span><span>// 👈
</span><span>
</span><span>                </span><span>let</span><span> handle_connection </span><span>= </span><span>handle</span><span>(connection).</span><span>chain</span><span>(|_| {
</span><span>                    </span><span>poll_fn</span><span>(|_| {
</span><span>                        </span><span>// decrement the counter
</span><span>                        </span><span>*</span><span>tasks.</span><span>lock</span><span>().</span><span>unwrap</span><span>() </span><span>-= </span><span>1</span><span>; </span><span>// 👈
</span><span>                        </span><span>Some</span><span>(())
</span><span>                    })
</span><span>                });
</span><span>
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(handle_connection);
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        });
</span><span>
</span><span>        </span><span>select</span><span>(listen, </span><span>ctrl_c</span><span>())
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>_ctrl_c</span><span>| </span><span>graceful_shutdown</span><span>())
</span><span>}
</span></code></pre>
<p>Notice how the decrement of the counter is chained onto the connection handler, so the decrement actually happens from within the spawned task, after it completes.</p>
<p>A task counter is great, but we need a little more than that. We can't just check for <code>tasks == 0</code> in a loop, we need the shutdown handler to be <em>notified</em> when the last task completes.</p>
<p>And for that, the connection handler that completes needs access to the shutdown handler's waker.</p>
<p>What we need is similar to the <code>spawn_blocking</code> solution we created earlier, except instead of a boolean flag, we need a counter. We can wrap up all this state into a small struct.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[derive(Default)]
</span><span>struct </span><span>Counter {
</span><span>    state: Mutex&lt;(</span><span>usize</span><span>, </span><span>Option</span><span>&lt;Waker&gt;)&gt;,
</span><span>}
</span><span>
</span><span>impl </span><span>Counter {
</span><span>    </span><span>fn </span><span>increment</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>let </span><span>(count, </span><span>_</span><span>) </span><span>= &amp;mut *</span><span>self.state.</span><span>lock</span><span>().</span><span>unwrap</span><span>();
</span><span>        </span><span>*</span><span>count </span><span>+= </span><span>1</span><span>;
</span><span>    }
</span><span>
</span><span>    </span><span>fn </span><span>decrement</span><span>(</span><span>&amp;</span><span>self</span><span>) {
</span><span>        </span><span>let </span><span>(count, waker) </span><span>= &amp;mut *</span><span>self.state.</span><span>lock</span><span>().</span><span>unwrap</span><span>();
</span><span>        </span><span>*</span><span>count </span><span>-= </span><span>1</span><span>;
</span><span>
</span><span>        </span><span>// we were the last task
</span><span>        </span><span>if *</span><span>count </span><span>== </span><span>0 </span><span>{
</span><span>            </span><span>// wake the waiting task
</span><span>            </span><span>if </span><span>let Some</span><span>(waker) </span><span>=</span><span> waker.</span><span>take</span><span>() {
</span><span>                waker.</span><span>wake</span><span>();
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>
</span><span>
</span><span>    </span><span>fn </span><span>wait_for_zero</span><span>(</span><span>self</span><span>: Arc&lt;</span><span>Self</span><span>&gt;) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>        </span><span>poll_fn</span><span>(</span><span>move |</span><span>waker</span><span>| </span><span>{
</span><span>            </span><span>match &amp;mut *</span><span>self.state.</span><span>lock</span><span>().</span><span>unwrap</span><span>() {
</span><span>                </span><span>// work is completed
</span><span>                (</span><span>0</span><span>, </span><span>_</span><span>) </span><span>=&gt; </span><span>Some</span><span>(()),
</span><span>                </span><span>// work is not completed, store our waker and come back later
</span><span>                (</span><span>_</span><span>, state) </span><span>=&gt; </span><span>{
</span><span>                    </span><span>*</span><span>state </span><span>= </span><span>Some</span><span>(waker);
</span><span>                    </span><span>None
</span><span>                }
</span><span>            }
</span><span>        })
</span><span>    }
</span><span>}
</span></code></pre>
<p>When <code>wait_for_zero</code> is first called it stores its waker in the counter state before returning. Now the task that calls <code>decrement</code> and sees that it was the last active task can simply call <code>wake</code>, notifying the caller of <code>wait_for_zero</code>.</p>
<p>When the shutdown handler is woken, it will see that the counter is at zero and shut down the program.</p>
<p>Now we can replace our manual counter with the <code>Counter</code> object.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>listen</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>let</span><span> tasks </span><span>= </span><span>Arc::new(Counter::default()); </span><span>// 👈
</span><span>    </span><span>let</span><span> tasks_ref </span><span>= </span><span>Arc::new(Counter::default());
</span><span>
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>// ...
</span><span>    })
</span><span>    .</span><span>chain</span><span>(</span><span>move |</span><span>listener</span><span>| </span><span>{
</span><span>        </span><span>let</span><span> listen </span><span>= </span><span>poll_fn</span><span>(</span><span>move |_| match</span><span> listener.</span><span>accept</span><span>() {
</span><span>            </span><span>Ok</span><span>((connection, </span><span>_</span><span>)) </span><span>=&gt; </span><span>{
</span><span>                connection.</span><span>set_nonblocking</span><span>(</span><span>true</span><span>).</span><span>unwrap</span><span>();
</span><span>
</span><span>                </span><span>// increment the counter
</span><span>                tasks.</span><span>increment</span><span>(); </span><span>// 👈
</span><span>
</span><span>                </span><span>let</span><span> tasks </span><span>=</span><span> tasks.</span><span>clone</span><span>();
</span><span>                </span><span>let</span><span> handle_connection </span><span>= </span><span>handle</span><span>(connection).</span><span>chain</span><span>(|_| {
</span><span>                    </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>{
</span><span>                        </span><span>// decrement the counter
</span><span>                        tasks.</span><span>decrement</span><span>(); </span><span>// 👈
</span><span>                        </span><span>Some</span><span>(())
</span><span>                    })
</span><span>                });
</span><span>
</span><span>                </span><span>SCHEDULER</span><span>.</span><span>spawn</span><span>(handle_connection);
</span><span>                </span><span>None
</span><span>            }
</span><span>            </span><span>Err</span><span>(e) </span><span>if</span><span> e.</span><span>kind</span><span>() </span><span>== </span><span>io::ErrorKind::WouldBlock </span><span>=&gt; </span><span>None</span><span>::&lt;()&gt;,
</span><span>            </span><span>Err</span><span>(e) </span><span>=&gt; </span><span>panic!(</span><span>"{e}"</span><span>),
</span><span>        });
</span><span>
</span><span>        </span><span>select</span><span>(listen, </span><span>ctrl_c</span><span>())
</span><span>    })
</span><span>    .</span><span>chain</span><span>(|</span><span>_ctrl_c</span><span>| </span><span>graceful_shutdown</span><span>(tasks_ref)) </span><span>// 👈
</span><span>}
</span></code></pre>
<p>And our graceful shutdown handler can use <code>wait_for_zero</code> to wait until all the active tasks are complete. Once they are, or the timer elapses, the graceful shutdown is met and the program will exit.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>graceful_shutdown</span><span>(</span><span>tasks</span><span>: Arc&lt;Counter&gt;) -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>poll_fn</span><span>(|</span><span>waker</span><span>| {
</span><span>        </span><span>let</span><span> timer </span><span>= </span><span>spawn_blocking</span><span>(|| thread::sleep(Duration::from_secs(</span><span>30</span><span>)));
</span><span>        </span><span>let</span><span> request_counter </span><span>=</span><span> tasks.</span><span>wait_for_zero</span><span>(); </span><span>// 👈
</span><span>        </span><span>select</span><span>(timer, request_counter)
</span><span>    }).</span><span>chain</span><span>(|_| {
</span><span>        </span><span>// graceful shutdown process complete, now we actually exit
</span><span>        println!(</span><span>"Graceful shutdown complete"</span><span>);
</span><span>        std::process::exit(</span><span>0</span><span>)
</span><span>    })
</span><span>}
</span></code></pre>
<p>And that's it!</p>
<p>Now if you start the server and hit ctrl+c, it will exit immediately, without blocking for another connection.</p>
<pre data-lang="sh"><code data-lang="sh"><span>$ cargo run
</span><span>^C
</span><span># =&gt; Graceful shutdown complete
</span><span>$ </span><span>|
</span></code></pre>
<h2 id="looking-back"><a href="#looking-back" aria-label="Anchor link for: looking-back">Looking Back</a></h2>
<p>Well, that was quite the journey.</p>
<p>Our server is looking pretty good now. From threads, to an epoll event loop, to futures and closure combinators, we've come a long way. There is some manual work that we could abstract over even further, but overall our program is relatively clean.</p>
<p>Compared to our original multithreaded program, our code is still clearly more complex. However, it's also a lot more powerful. Composing futures is trivial, and we were able to express complex control flow that would have been very difficult to do with threads. We can even still call out to blocking functions without interrupting our async runtime.</p>
<p>There must be a price to pay for all this power, right?</p>
<h2 id="back-to-reality"><a href="#back-to-reality" aria-label="Anchor link for: back-to-reality">Back To Reality</a></h2>
<p>Now that we've thoroughly explored concurrency and async ourselves, let's see how it works in the real world.</p>
<p>The standard library defines a trait like <code>Future</code>, which looks remarkably similar to the trait we designed.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>self</span><span>: Pin&lt;</span><span>&amp;mut </span><span>Self</span><span>&gt;, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>However, there are a few noticeable differences.</p>
<p>The first is that instead of a <code>Waker</code> argument, <code>poll</code> takes a <code>&amp;mut Context</code>. It turns out this isn't much of a difference at all, because <code>Context</code> is simply a wrapper around a <code>Waker</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Context&lt;'</span><span>_</span><span>&gt; {
</span><span>    </span><span>pub </span><span>fn </span><span>from_waker</span><span>(</span><span>waker</span><span>: </span><span>&amp;'a</span><span> Waker) -&gt; Context&lt;</span><span>'a</span><span>&gt;  { </span><span>/* ... */ </span><span>}
</span><span>    </span><span>pub </span><span>fn </span><span>waker</span><span>(</span><span>&amp;</span><span>self</span><span>) -&gt; </span><span>&amp;'a</span><span> Waker  { </span><span>/* ... */ </span><span>}
</span><span>}
</span></code></pre>
<p>And <code>Waker</code>, along with a few other utility methods, has the familiar <code>wake</code> method.</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>Waker {
</span><span>    </span><span>pub </span><span>fn </span><span>wake</span><span>(</span><span>self</span><span>) { </span><span>/* ... */ </span><span>}
</span><span>}
</span></code></pre>
<p>Constructing a <code>Waker</code> is a little more complicated, but it's essentially a manual trait object just like the <code>Arc&lt;dyn Fn()&gt;</code> we used for our version. All of that happens <a href="https://doc.rust-lang.org/std/task/struct.Waker.html#method.from_raw">through the <code>RawWaker</code> type</a>, which you can check out yourself.</p>
<p>The second difference is that instead of returning an <code>Option</code>, <code>poll</code> returns a new type called <code>Poll</code>.. which is really just a rebranding of <code>Option</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>enum </span><span>Poll&lt;T&gt; {
</span><span>    Ready(T),
</span><span>    Pending,
</span><span>}
</span></code></pre>
<p>The final difference is a little more complicated.</p>
<h2 id="pinning"><a href="#pinning" aria-label="Anchor link for: pinning">Pinning</a></h2>
<p>Instead of <code>poll</code> taking a mutable reference to <code>self</code>, it takes a <em>pinned</em> mutable reference to self — <code>Pin&lt;&amp;mut Self&gt;</code>.</p>
<p>What is <code>Pin</code>, you ask?</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[derive(Copy, Clone)]
</span><span>pub </span><span>struct </span><span>Pin&lt;P&gt; {
</span><span>    pointer: P,
</span><span>}
</span></code></pre>
<p>Huh. That doesn't seem very useful.</p>
<p>It turns out, what makes <code>Pin</code> special is how you create one:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;P: Deref&gt; Pin&lt;P&gt; {
</span><span>    </span><span>pub </span><span>fn </span><span>new</span><span>(</span><span>pointer</span><span>: P) -&gt; Pin&lt;P&gt; </span><span>where </span><span>P::</span><span>Target: Unpin { </span><span>/* ... */ </span><span>}
</span><span>    </span><span>pub unsafe </span><span>fn </span><span>new_unchecked</span><span>(</span><span>pointer</span><span>: P) -&gt; Pin&lt;P&gt;  { </span><span>/* ... */ </span><span>}
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;P: Deref&gt; Deref </span><span>for </span><span>Pin&lt;P&gt; {
</span><span>    </span><span>type </span><span>Target </span><span>= </span><span>P::Target;
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;P: Deref&gt; DerefMut </span><span>for </span><span>Pin&lt;P&gt;
</span><span>where
</span><span>    </span><span>P::</span><span>Target: Unpin
</span><span>{
</span><span>    </span><span>type </span><span>Target </span><span>= </span><span>P::Target;
</span><span>}
</span></code></pre>
<p>So you can only create a <code>Pin&lt;&amp;mut T&gt;</code> safely if <code>T</code> is <code>Unpin</code>... what's <code>Unpin</code>?</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub</span><span> auto </span><span>trait </span><span>Unpin {}
</span><span>
</span><span>/// A marker type which does not implement `Unpin`.
</span><span>pub </span><span>struct </span><span>PhantomPinned;
</span><span>impl </span><span>!Unpin for PhantomPinned {}
</span></code></pre>
<p><code>Unpin</code> seems to be automatically implemented for all types except <code>PhantomPinned</code>. So creating a <code>Pin</code> is safe, except for types that contain <code>PhantomPinned</code>? And <code>Pin</code> just dereferences to <code>T</code> normally? All of this seems a little useless. </p>
<p>There is a point to it all though, and it goes back to a problem we ran into earlier. Remember when we tried creating a self-referential struct to hold our task state but it wouldn't work, so we ended up having to allocate our task state with an <code>Arc</code>? It was a bit unfortunate, and it turns out that you actually <em>can</em> create self-referential structs with a little bit of unsafe code, and avoid that <code>Arc</code> allocation.</p>
<p>The problem is that you can't just go handing out a self-referential struct in general, because as we realized, moving a self-referential struct breaks its internal references and is <em>unsound</em>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>SelfReferential {
</span><span>    counter: </span><span>u8</span><span>, </span><span>// (X)
</span><span>    state: FutureState 
</span><span>}
</span><span>
</span><span>enum </span><span>FutureState  {
</span><span>    First { counter_ptr: </span><span>*mut </span><span>u8 </span><span>} </span><span>// self-referentially points to `counter` (X)
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> future1 </span><span>= </span><span>SelfReferential::new();
</span><span>future1.</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span><span>
</span><span>let </span><span>mut</span><span> moved </span><span>=</span><span> future1; </span><span>// move it
</span><span>// unsound! `counter_ptr` still point to the old stack location of `counter`
</span><span>moved.</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span></code></pre>
<p>This is where <code>Pin</code> comes in. You can only create a <code>Pin&lt;&amp;mut T&gt;</code> if you <strong>guarantee that the <code>T</code> will stay in a stable location until it is dropped</strong>, meaning that any self-references will remain valid.</p>
<p>For most types, <code>Pin</code> doesn't mean anything, which is why <code>Unpin</code> exists. <code>Unpin</code> essentially tells <code>Pin</code> that a type is not self-referential, so pinning it is completely safe and always valid. <code>Pin</code> will even hand out mutable references to <code>Unpin</code> types and let you use <code>mem::swap</code> or <code>mem::replace</code> to move them around. Because you can't safely create a self-referential struct, <code>Unpin</code> is the default and implemented by types automatically.</p>
<p>If you did want to create a self-referential future though, you can use the <code>PhantomPinned</code> marker struct to make it <code>!Unpin</code>. Pinning a <code>!Unpin</code> type requires <code>unsafe</code>, so because <code>poll</code> requires <code>Pin&lt;&amp;mut Self&gt;</code>, it cannot be called safely on a self-referential future.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> future </span><span>= </span><span>SelfReferential::new();
</span><span>
</span><span>// SAFETY: we never move `future`
</span><span>let</span><span> pinned </span><span>= unsafe </span><span>{ Pin::new_unchecked(</span><span>&amp;mut</span><span> future) };
</span><span>pinned.</span><span>poll</span><span>(</span><span>/* ... /*);
</span></code></pre>
<p>Notice that you can move around the future all you want before pinning it because the self-references are only created after you first call <code>poll</code>. Once you do pin it though, you <em>must</em> uphold the <code>Pin</code> safety contract.</p>
<p>There are a couple safe ways of creating a pin though, even for <code>!Unpin</code> types.</p>
<p>The first way is with <code>Box::pin</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>let </span><span>mut</span><span> future1: Pin&lt;</span><span>Box</span><span>&lt;SelfReferential&gt;&gt; </span><span>= </span><span>Box</span><span>::pin(SelfReferential::new());
</span><span>future1.</span><span>as_mut</span><span>().</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span><span>
</span><span>let </span><span>mut</span><span> moved </span><span>=</span><span> future1;
</span><span>moved.</span><span>as_mut</span><span>().</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span></code></pre>
<p>At first glance this may seem unsound, but remember, <code>Box</code> is an allocation. Once the future is allocated it has a stable location on the heap, so you can move around the <code>Box</code> pointer all you want, the internal references will remain stable.</p>
<p>The second way you can safely create a pin is with the <code>pin!</code> macro.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>std::pin::pin;
</span><span>
</span><span>let </span><span>mut</span><span> future1: Pin&lt;</span><span>&amp;mut</span><span> SelfReferential&gt; </span><span>= </span><span>pin!(SelfReferential::new());
</span><span>future1.</span><span>as_mut</span><span>().</span><span>poll</span><span>(</span><span>/* ... */</span><span>);
</span></code></pre>
<p>With <code>pin!</code>, you can safely pin a struct without even allocating it! The trick is that <code>pin!</code> takes ownership of the future, making it impossible to access except through the <code>Pin&lt;&amp;mut T&gt;</code>, which remember, will never give you a mutable reference if <code>T</code> isn't <code>Unpin</code>. The <code>T</code> is completely hidden and thus safe from being tampered with.</p>
<p><code>Pin</code> is a common point of confusion around futures, but once you understand why it exists, the solution is pretty ingenious.</p>
<h2 id="async-await"><a href="#async-await" aria-label="Anchor link for: async-await">async/await</a></h2>
<p>Alright, that's the standard <code>Future</code> trait.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>trait </span><span>Future {
</span><span>    </span><span>type </span><span>Output;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>self</span><span>: Pin&lt;</span><span>&amp;mut </span><span>Self</span><span>&gt;, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt;;
</span><span>}
</span></code></pre>
<p>So how do we use it?</p>
<p>The <a href="https://docs.rs/futures"><code>futures</code></a> crate is where all the useful helpers live. Functions like <code>poll_fn</code> that we wrote before, and combinators like <code>map</code> and <code>and_then</code>, which we called <code>chain</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>futures::{FutureExt, future::{ready, poll_fn}};
</span><span>
</span><span>let</span><span> future </span><span>= </span><span>poll_fn</span><span>(|_| Poll::Ready(</span><span>1</span><span>))
</span><span>    .</span><span>and_then</span><span>(|</span><span>x</span><span>| </span><span>poll_fn</span><span>(|_| Poll::Ready(x </span><span>+ </span><span>1</span><span>)))
</span><span>    .</span><span>and_then</span><span>(|</span><span>x</span><span>| </span><span>poll_fn</span><span>(|_| println!(</span><span>"</span><span>{x}</span><span>"</span><span>)));
</span></code></pre>
<p>But even with these helpers, as we found out, it's a little cumbersome to write async code. It's still a shift from the simple synchronous code we're used to. Maybe not as drastic as a manual <code>epoll</code> event loop, but still a big change.</p>
<p>It turns out there's actually another way to write futures in Rust, with the async/await syntax.</p>
<p>Instead of using <code>poll_fn</code> to create futures, you can attach the <code>async</code> keyword to functions:</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>foo</span><span>() -&gt; </span><span>usize </span><span>{
</span><span>    </span><span>1
</span><span>}
</span></code></pre>
<p>An <em>async</em> function is really just a function that returns an async block:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>foo</span><span>() -&gt; impl Future&lt;Output = </span><span>usize</span><span>&gt; {
</span><span>    async { </span><span>1 </span><span>}
</span><span>}
</span></code></pre>
<p>Which is really just a function that returns a <code>poll_fn</code> future:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>foo</span><span>() -&gt; impl Future&lt;Output = </span><span>usize</span><span>&gt; {
</span><span>    </span><span>poll_fn</span><span>(|| Poll::Ready(</span><span>1</span><span>))
</span><span>}
</span></code></pre>
<p>The magic comes with the <code>await</code> keyword. <code>await</code> waits for the completion of another future, propagating <code>Poll::Pending</code> until the future is resolved.</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>foo</span><span>() {
</span><span>    </span><span>let</span><span> one </span><span>= </span><span>one</span><span>().await;
</span><span>    </span><span>let</span><span> two </span><span>= </span><span>two</span><span>().await;
</span><span>    assert_eq!(one </span><span>+ </span><span>1</span><span>, two);
</span><span>}
</span><span>
</span><span>async </span><span>fn </span><span>two</span><span>() -&gt; </span><span>usize </span><span>{
</span><span>    </span><span>one</span><span>().await </span><span>+ </span><span>1
</span><span>}
</span><span>
</span><span>async </span><span>fn </span><span>one</span><span>() -&gt; </span><span>usize </span><span>{
</span><span>    </span><span>1
</span><span>}
</span></code></pre>
<p>Under the hood, the compiler transforms this into manual state machines, similar to the ones we created with those combinators:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>foo</span><span>() -&gt; impl Future&lt;Output = ()&gt; {
</span><span>    </span><span>one</span><span>()
</span><span>        .</span><span>and_then</span><span>(|</span><span>one</span><span>| </span><span>two</span><span>().</span><span>and_then</span><span>(</span><span>move |</span><span>two</span><span>| </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>Poll::Ready((one, two)))))
</span><span>        .</span><span>and_then</span><span>(|(</span><span>one</span><span>, </span><span>two</span><span>)| </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>Poll::Ready(assert_eq!(one, two </span><span>+ </span><span>1</span><span>))))
</span><span>}
</span><span>
</span><span>fn </span><span>two</span><span>() -&gt; impl Future&lt;Output = </span><span>usize</span><span>&gt; {
</span><span>    </span><span>one</span><span>().</span><span>and_then</span><span>(|</span><span>one</span><span>| </span><span>poll_fn</span><span>(</span><span>move |_| </span><span>Poll::Ready(one </span><span>+ </span><span>1</span><span>)))
</span><span>}
</span><span>
</span><span>fn </span><span>one</span><span>() -&gt; impl Future&lt;Output = </span><span>usize</span><span>&gt; {
</span><span>    </span><span>poll_fn</span><span>(|_| Poll::Ready(</span><span>1</span><span>))
</span><span>}
</span></code></pre>
<p>Which, as we know all too well, translates into a huge manual state machine that looks something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>FooFuture {
</span><span>    One(OneFuture),
</span><span>    Two(</span><span>usize</span><span>, TwoFuture),
</span><span>}
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>FooFuture {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>();
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>if </span><span>let </span><span>FooFuture::One(f) </span><span>= </span><span>self {
</span><span>            </span><span>match</span><span> f.</span><span>poll</span><span>(cx) {
</span><span>                Poll::Ready(one) </span><span>=&gt; *</span><span>self </span><span>= </span><span>Self</span><span>::Two(one, TwoFuture(OneFuture)),
</span><span>                Poll::Pending </span><span>=&gt; return </span><span>Poll::Pending,
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>if </span><span>let </span><span>FooFuture::Two(one, f) </span><span>= </span><span>self {
</span><span>            </span><span>match</span><span> f.</span><span>poll</span><span>(cx) {
</span><span>                Poll::Ready(two) </span><span>=&gt; </span><span>{
</span><span>                    assert_eq!(</span><span>*</span><span>one </span><span>+ </span><span>1</span><span>, two);
</span><span>                    </span><span>return </span><span>Poll::Ready(());
</span><span>                }
</span><span>                Poll::Pending </span><span>=&gt; return </span><span>Poll::Pending,
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>None
</span><span>    }
</span><span>}
</span><span>
</span><span>struct </span><span>TwoFuture(OneFuture);
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>TwoFuture {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>usize</span><span>;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        </span><span>match </span><span>self.</span><span>0.</span><span>poll</span><span>(waker) {
</span><span>            Poll::Ready(one) </span><span>=&gt; </span><span>Poll::Ready(one </span><span>+ </span><span>1</span><span>),
</span><span>            Poll::Pending </span><span>=&gt; </span><span>Poll::Pending,
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span>struct </span><span>OneFuture;
</span><span>
</span><span>impl </span><span>Future </span><span>for </span><span>OneFuture {
</span><span>    </span><span>type </span><span>Output </span><span>= </span><span>usize</span><span>;
</span><span>
</span><span>    </span><span>fn </span><span>poll</span><span>(</span><span>&amp;mut </span><span>self</span><span>, </span><span>cx</span><span>: </span><span>&amp;mut </span><span>Context&lt;'</span><span>_</span><span>&gt;) -&gt; Poll&lt;</span><span>Self::</span><span>Output&gt; {
</span><span>        Poll::Ready(</span><span>1</span><span>)
</span><span>    }
</span><span>}
</span></code></pre>
<p>... but async/await removes all of that headache. Of course, we aren't actually doing any I/O here so the futures are mostly useless, but you can imagine how helpful this would be for our web server.</p>
<p>In fact, it's even better than the combinators. With <code>async</code> functions, you can hold onto local state across <code>await</code> points!</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>foo</span><span>() {
</span><span>    </span><span>let</span><span> x </span><span>= </span><span>vec![</span><span>1</span><span>, </span><span>2</span><span>, </span><span>3</span><span>];
</span><span>    </span><span>bar</span><span>(</span><span>&amp;</span><span>x).await;
</span><span>    </span><span>baz</span><span>(</span><span>&amp;</span><span>x).await;
</span><span>    println!(</span><span>"</span><span>{x:?}</span><span>"</span><span>);
</span><span>}
</span></code></pre>
<p>After going through implementing futures ourselves, we can really appreciate the convenience of this. Under the hood the compiler has to generate a self-referential future to give <code>bar</code> and <code>baz</code> access to the state.</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>FooFuture {
</span><span>    x: </span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt;, </span><span>// (X)
</span><span>    state: FooFutureState,
</span><span>}
</span><span>
</span><span>enum </span><span>FooFutureState {
</span><span>    Bar(BarFuture),
</span><span>    Baz(BazFuture),
</span><span>}
</span><span>
</span><span>struct </span><span>BarFuture { x: </span><span>*mut </span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt; </span><span>/* pointer to (X)! */ </span><span>}
</span><span>struct </span><span>BazFuture { x: </span><span>*mut </span><span>Vec</span><span>&lt;</span><span>i32</span><span>&gt; </span><span>/* pointer to (X)! */ </span><span>}
</span></code></pre>
<p>The compiler takes care of all the unsafe code involved in this, allowing us to work with local state just like we would in a regular function. For this reason, the futures generated by <code>async</code> blocks or functions are <code>!Unpin</code>.</p>
<p>async/await removes any complexity that remained with writing futures compared to synchronous code. After implementing futures manually, it almost feels like magic!</p>
<h2 id="a-tokio-server"><a href="#a-tokio-server" aria-label="Anchor link for: a-tokio-server">A Tokio Server</a></h2>
<p>So far we've only been looking at how <code>Future</code> <em>works</em>, we haven't discussed how to actually run one, or do any I/O. The thing is, the standard library doesn't provide any of that, it only provides the bare essential types and traits to get started.</p>
<p>If you want to actually write an async application, you have to use an external runtime. The most popular general purpose runtime is <code>tokio</code>. <code>tokio</code> provides a task scheduler, a reactor, and a pool to run blocking tasks, just like we wrote earlier, but it also provides timers, async channels, and various other useful types and utilities for async code. On top of that, <code>tokio</code> is multi-threaded, distributing async tasks to take advantage of all your CPU cores. The core ideas behind <code>tokio</code> are very similar to the async runtime we wrote ourselves, but you can read more about it's design <a href="https://tokio.rs/blog/2019-10-scheduler">in this excellent blog post</a>.</p>
<p>It's time to write our final web server, this time using the standard <code>Future</code> trait and tokio.</p>
<p>Tokio applications begin with the <code>#[tokio::main]</code> macro. Under the hood, this macro spins up the runtime and runs the async code in <code>main</code>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[tokio::main]
</span><span>async </span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Tokio does it's best to mirror the standard library for most of it's types. For example, <code>tokio::net::TcpListener</code> works exactly like <code>std::net::TcpListener</code>, except with <code>async</code> methods. Any interactions with epoll and the reactor are hidden under the hood.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>tokio::net::{TcpListener, TcpStream};
</span><span>
</span><span>#[tokio::main]
</span><span>async </span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).await.</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().await.</span><span>unwrap</span><span>();
</span><span>
</span><span>        </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection).await {
</span><span>            println!(</span><span>"failed to handle connection: </span><span>{e}</span><span>"</span><span>)
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span>async </span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>That isn't exactly right though, we need to spawn the connection handler. We can do so with the <code>tokio::spawn</code> function, which takes a future.</p>
<pre data-lang="rust"><code data-lang="rust"><span>#[tokio::main]
</span><span>async </span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).await.</span><span>unwrap</span><span>();
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> listener.</span><span>accept</span><span>().await.</span><span>unwrap</span><span>();
</span><span>
</span><span>        tokio::spawn(async </span><span>move </span><span>{ </span><span>// 👈
</span><span>            </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection).await {
</span><span>                println!(</span><span>"failed to handle connection: </span><span>{e}</span><span>"</span><span>)
</span><span>            }
</span><span>        });
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now for the connection handler. With the <code>AsyncReadExt</code> trait and the <code>await</code> keyword, we can read from the TCP stream almost exactly like we did before.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>tokio::io::AsyncRead;
</span><span>
</span><span>async </span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>let </span><span>mut</span><span> read </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> request </span><span>= </span><span>[</span><span>0</span><span>u8</span><span>; </span><span>1024</span><span>];
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// try reading from the stream
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>read</span><span>(</span><span>&amp;mut</span><span> request[read</span><span>..</span><span>]).await</span><span>?</span><span>; </span><span>// 👈
</span><span>
</span><span>        </span><span>// the client disconnected
</span><span>        </span><span>if</span><span> num_bytes </span><span>== </span><span>0 </span><span>{
</span><span>            println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>            </span><span>return </span><span>Ok</span><span>(());
</span><span>        }
</span><span>
</span><span>        </span><span>// keep track of how many bytes we've read
</span><span>        read </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we reached the end of the request?
</span><span>        </span><span>if</span><span> request.</span><span>get</span><span>(read </span><span>- </span><span>4</span><span>..</span><span>read) </span><span>== </span><span>Some</span><span>(</span><span>b</span><span>"</span><span>\r\n\r\n</span><span>"</span><span>) {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>let</span><span> request </span><span>= </span><span>String</span><span>::from_utf8_lossy(</span><span>&amp;</span><span>request[</span><span>..</span><span>read]);
</span><span>    println!(</span><span>"</span><span>{request}</span><span>"</span><span>);
</span><span>
</span><span>    </span><span>// ...
</span><span>}
</span></code></pre>
<p>Writing the response works the same as well.</p>
<pre data-lang="rust"><code data-lang="rust"><span>async </span><span>fn </span><span>handle_connection</span><span>(</span><span>mut </span><span>connection</span><span>: TcpStream) -&gt; io::</span><span>Result</span><span>&lt;()&gt; {
</span><span>    </span><span>// ...
</span><span>
</span><span>    </span><span>// "Hello World!" in HTTP
</span><span>    </span><span>let</span><span> response </span><span>= </span><span>/* ... */</span><span>;
</span><span>    </span><span>let </span><span>mut</span><span> written </span><span>= </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        </span><span>// write the remaining response bytes
</span><span>        </span><span>let</span><span> num_bytes </span><span>=</span><span> connection.</span><span>write</span><span>(response[written</span><span>..</span><span>].</span><span>as_bytes</span><span>()).await</span><span>?</span><span>; </span><span>// 👈
</span><span>
</span><span>        </span><span>// the client disconnected
</span><span>        </span><span>if</span><span> num_bytes </span><span>== </span><span>0 </span><span>{
</span><span>            println!(</span><span>"client disconnected unexpectedly"</span><span>);
</span><span>            </span><span>return </span><span>Ok</span><span>(());
</span><span>        }
</span><span>
</span><span>        written </span><span>+=</span><span> num_bytes;
</span><span>
</span><span>        </span><span>// have we written the whole response yet?
</span><span>        </span><span>if</span><span> written </span><span>==</span><span> response.</span><span>len</span><span>() {
</span><span>            </span><span>break</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    connection.</span><span>flush</span><span>().await
</span><span>}
</span></code></pre>
<p>Well that was easy.</p>
<p>If you notice, our program is exactly the same as our original server, with the exception of a couple uses of the <code>async</code> and <code>await</code> keywords. With async/await, we really can have our cake and eat it too.</p>
<p>Now to implement graceful shutdown.</p>
<p>The first step is to identify the ctrl+c signal. With tokio, this is as simple as using the <code>tokio::signal::ctrl_c</code> function, an async function that returns once the ctrl+c signal is received. We can also use tokio's <code>select!</code> macro, a more powerful version of the <code>select</code> combinator we implemented earlier.</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub</span><span> async </span><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> listener </span><span>= </span><span>TcpListener::bind(</span><span>"localhost:3000"</span><span>).await.</span><span>unwrap</span><span>();
</span><span>    </span><span>let</span><span> state </span><span>= </span><span>Arc::new((AtomicUsize::new(</span><span>0</span><span>), Notify::new()));
</span><span>
</span><span>    </span><span>loop </span><span>{
</span><span>        select! {
</span><span>            </span><span>// new incoming connection
</span><span>            result </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>                </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> result.</span><span>unwrap</span><span>();
</span><span>
</span><span>                tokio::spawn(async </span><span>move </span><span>{
</span><span>                    </span><span>// ..
</span><span>                });
</span><span>            }
</span><span>            </span><span>// ctrl+c signal
</span><span>            shutdown </span><span>= </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>                </span><span>let</span><span> timer </span><span>= </span><span>/* ... */</span><span>;
</span><span>                </span><span>let</span><span> request_counter </span><span>= </span><span>/* .. */</span><span>;
</span><span>
</span><span>                select! {
</span><span>                    </span><span>_ =</span><span> timer </span><span>=&gt; </span><span>{}
</span><span>                    </span><span>_ =</span><span> request_counter </span><span>=&gt; </span><span>{}
</span><span>                }
</span><span>
</span><span>                println!(</span><span>"Gracefully shutting down."</span><span>);
</span><span>                </span><span>return</span><span>;
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p><code>select!</code> runs the branch for whichever future completes first and cancels the other branches, allowing us to select between incoming connections and the ctrl+c signal, and run the appropriate code for each.</p>
<p>Now we need to create the graceful shutdown condition.</p>
<p>For the timer, we can use tokio's asynchronous <code>sleep</code> function. Under the hood this hooks into a custom timer system, a much more efficient version of our <code>spawn_blocking</code> timers. You can read more about how that works <a href="https://tokio.rs/blog/2018-03-timers">in this other excellent post</a>.</p>
<pre data-lang="rust"><code data-lang="rust"><span>select! {
</span><span>    </span><span>// new incoming connection
</span><span>    result </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>    </span><span>// ctrl+c signal
</span><span>    shutdown </span><span>= </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>let</span><span> timer </span><span>= </span><span>tokio::time::sleep(Duration::from_secs(</span><span>30</span><span>));
</span><span>        </span><span>let</span><span> request_counter </span><span>= </span><span>/* .. */</span><span>;
</span><span>
</span><span>        select! {
</span><span>            </span><span>_ =</span><span> timer </span><span>=&gt; </span><span>{}
</span><span>            </span><span>_ =</span><span> request_counter </span><span>=&gt; </span><span>{}
</span><span>        }
</span><span>
</span><span>        println!(</span><span>"Gracefully shutting down."</span><span>);
</span><span>        </span><span>return</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now for the active request counter. Instead of managing wakers manually, we can use a simple counter and take advantage of tokio's <code>Notify</code> type, which allows tasks to notify each other, or wait to be notified.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>tokio::sync::Notify;
</span><span>
</span><span>let</span><span> state </span><span>= </span><span>Arc::new((AtomicUsize::new(</span><span>0</span><span>), Notify::new()));
</span></code></pre>
<p>When a request comes in, we increment the counter, and when it completes, we decrement it. If the counter reaches zero, the last active task calls <code>notify_one</code>, which will wake up the main thread, letting it know that all active tasks have completed.</p>
<pre data-lang="rust"><code data-lang="rust"><span>select! {
</span><span>    result </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>let </span><span>(connection, </span><span>_</span><span>) </span><span>=</span><span> result.</span><span>unwrap</span><span>();
</span><span>        </span><span>let</span><span> state </span><span>=</span><span> state.</span><span>clone</span><span>();
</span><span>
</span><span>    </span><span>// increment the counter
</span><span>    state.</span><span>0.</span><span>fetch_add</span><span>(</span><span>1</span><span>, Ordering::Relaxed);
</span><span>
</span><span>    tokio::spawn(async </span><span>move </span><span>{
</span><span>        </span><span>if </span><span>let Err</span><span>(e) </span><span>= </span><span>handle_connection</span><span>(connection).await {
</span><span>            </span><span>// ...
</span><span>        }
</span><span>
</span><span>        </span><span>// decrement the counter
</span><span>        </span><span>let</span><span> count </span><span>=</span><span> state.</span><span>0.</span><span>fetch_sub</span><span>(</span><span>1</span><span>, Ordering::Relaxed);
</span><span>        </span><span>if</span><span> count </span><span>== </span><span>1 </span><span>{
</span><span>            </span><span>// we were the last active task
</span><span>            state.</span><span>1.</span><span>notify_one</span><span>();
</span><span>        }
</span><span>    });
</span><span>    }
</span><span>
</span><span>    shutdown </span><span>= </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>The shutdown handler can now simply select between the timer and <code>Notify::notified</code>, which will resolve when someone calls <code>notify_one</code>, indicating that the last active request has completed.</p>
<pre data-lang="rust"><code data-lang="rust"><span>select! {
</span><span>    result </span><span>=</span><span> listener.</span><span>accept</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>// ...
</span><span>    }
</span><span>    shutdown </span><span>= </span><span>ctrl_c</span><span>() </span><span>=&gt; </span><span>{
</span><span>        </span><span>// a 30 second timer
</span><span>        </span><span>let</span><span> timer </span><span>= </span><span>tokio::time::sleep(Duration::from_secs(</span><span>30</span><span>));
</span><span>        </span><span>// notified by the last active task
</span><span>        </span><span>let</span><span> notification </span><span>=</span><span> state.</span><span>1.</span><span>notified</span><span>();
</span><span>
</span><span>        </span><span>// if the count isn't zero, we have to wait
</span><span>        </span><span>if</span><span> state.</span><span>0.</span><span>load</span><span>(Ordering::Relaxed) </span><span>!= </span><span>0 </span><span>{
</span><span>            </span><span>// wait for either the timer or notification to resolve
</span><span>            select! {
</span><span>                </span><span>_ =</span><span> timer </span><span>=&gt; </span><span>{}
</span><span>                </span><span>_ =</span><span> notification </span><span>=&gt; </span><span>{}
</span><span>            }
</span><span>        }
</span><span>
</span><span>        println!(</span><span>"Gracefully shutting down."</span><span>);
</span><span>        </span><span>return</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<p>Beautiful, isn't it?</p>
<p>With <code>tokio</code> and async/await we don't even have to think about wakers, reactors, or anything else that goes on under the hood. All the building blocks are provided for us, we just have to put them together.</p>
<h2 id="afterword"><a href="#afterword" aria-label="Anchor link for: afterword">Afterword</a></h2>
<p>Whew, that was quite the journey!</p>
<p>We started from the simplest web server, tried multithreading, and then worked our way up to a custom asynchronous runtime built on epoll. All to implement graceful shutdown.</p>
<p>Then we circled back and implemented graceful shutdown with <code>tokio</code> in just a few extra lines of code.</p>
<p>Hopefully this article has helped you appreciate the power of async Rust, as well as taught you a little more about how it works under the hood. All the code for this repository <a href="https://github.com/ibraheemdev/too-many-web-servers">is available on GitHub</a>.</p>
<br>
<hr>






</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deja Vu: The FBI Proves Again It Can’t Be Trusted with Section 702 (304 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2023/07/deja-vu-fbi-proves-again-it-cant-be-trusted-section-702</link>
            <guid>37176717</guid>
            <pubDate>Fri, 18 Aug 2023 15:07:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2023/07/deja-vu-fbi-proves-again-it-cant-be-trusted-section-702">https://www.eff.org/deeplinks/2023/07/deja-vu-fbi-proves-again-it-cant-be-trusted-section-702</a>, See on <a href="https://news.ycombinator.com/item?id=37176717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span>We all deserve privacy in our communications, and part of that is trusting that the government will only access them within the limits of the law. But at this point, it’s crystal clear that the FBI doesn’t believe that either our rights nor the limitations that Congress has placed upon the bureau matter when it comes to the vast amount of information about us collected under FISA Section 702.&nbsp; <br></span></p>
<p><span>How many times will the FBI get caught with their hand in the cookie jar of our </span><span>constitutionally protected private communications </span><span>without losing these invasive and unconstitutional powers?</span></p>
<p><span>The latest exhibit in this is in yet another newly declassified opinion of the Foreign Intelligence Surveillance Court (FISC). This opinion further reiterates </span><a href="https://www.eff.org/deeplinks/2023/05/newly-public-fisc-opinion-best-evidence-why-congress-must-end-section-702"><span>what we already know</span></a><span>, that the Federal Bureau of Investigation simply cannot be trusted with conducting foreign intelligence queries on American persons.&nbsp; Regardless of the rules, or consistent FISC disapprovals, the FBI continues to act in a way that shows no regard for privacy and civil liberties. <br></span></p>
<p><span>According to the </span><a href="https://www.intel.gov/assets/documents/702%20Documents/declassified/2023/FISC_2023_FISA_702_Certifications_Opinion_April11_2023.pdf"><span>declassified FISC ruling</span></a><span>, despite paper reforms which the FBI has touted that it put into place to respond to the last time it was caught violating U.S. law, the Bureau conducted four queries for the communications of a state senator and a U.S. senator.&nbsp; And they did so without even meeting their own&nbsp; already-inadequate standards for these kinds of searches. <br></span></p>
<p><span>How many times will the FBI get caught with their hand in the cookie jar of our </span><a href="https://www.eff.org/deeplinks/2023/06/foreign-intelligence-surveillance-court-has-made-mockery-constitutional-right"><span>constitutionally protected private communications</span></a><span> </span><span>without losing these invasive and unconstitutional powers?</span></p>
<p><span>Specifically, this disclosure concerns Section 702 of the 2008 Foreign Intelligence Surveillance Amendments Act, which authorizes the collection of overseas communications that can be queried by intelligence agencies in national security investigations under the oversight of the FISC. The FBI has access to the collected information, but only for limited purposes—purposes which it routinely and grossly oversteps.</span></p>
<p><span>Apart from the FBI’s apparent failure to even abide by its own rules, the bigger problem with this arrangement—even under the law—is that we live in a globalized world where U.S. persons regularly communicate with people in other countries. This creates a massive pool of digital communications in which one side of the conversation is an American on U.S. soil. The FBI, investigating crimes in the U.S., has spent the better part of 15 years sifting through these communications without even a warrant. So the fact that they cannot even abide by their own rules, much less the ones set by Congress, is a big deal.<br></span></p>
<p><a href="https://www.eff.org/deeplinks/2023/03/section-702s-unconstitutional-domestic-spying-program-must-end"><span>But now we have a chance to close this unconstitutional loophole</span></a><span> and block the FBI—or any other government agency—from searching any of our communications without a warrant. Section 702 is set to expire in December 2023.&nbsp; Sadly, both the FBI and Biden Administration have signaled that they are all in when it comes to trying to keep open the FBI’s warrantless backdoor searches of 702 data. They like their hands fully in the cookie jar and at this point are likely confident that, even when they get caught, the FISC won’t take any serious steps to stop them.</span></p>
<p><span>But they won’t get that renewal without a fight. After several hearings in the House Judiciary Committee, it is clear that there is </span><a href="https://cyberscoop.com/congress-fbi-section-702/"><span>bipartisan support</span></a><span> </span><span>for the idea that Section 702 must drastically change, or else face termination (called sunsetting in DC) entirely. The Privacy and Civil Liberties Oversight Board (PCLOB), which has been unwilling to seriously take on 702 violations, even suggested before Congress that some </span><a href="https://www.eff.org/deeplinks/2023/04/congressional-hearing-pclob-members-suggest-bare-minimum-702-reforms"><span>bare minimum of changes</span></a><span> </span><span>should be made to the surveillance programs in order to protect the privacy rights of Americans. <br></span></p>
<p><span>While we think it’s time for 702 to end entirely, and for any future programs to start from scratch in protecting the privacy of digital communications. EFF will continue to fight to make sure that any bill that does renew Section 702 closes the government’s warrantless access to U.S. communications, minimizes the amount of data collected, and increases transparency. Anything less than that would signal a continued indifference, or contempt, to our right to privacy.<br></span></p>
<p><span>This recent disclosure proves, in a </span><i><span>Groundhog Day</span></i><span>-like fashion, that the FBI is not going to suddenly become good at self-control when it comes to access to our data. If the privacy of our communications—including communications with people abroad—is going to actually matter, Section 702 must be irrevocably changed or jettisoned entirely.</span></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ask vs. Guess Culture (620 pts)]]></title>
            <link>https://jeanhsu.substack.com/p/ask-vs-guess-culture</link>
            <guid>37176703</guid>
            <pubDate>Fri, 18 Aug 2023 15:06:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jeanhsu.substack.com/p/ask-vs-guess-culture">https://jeanhsu.substack.com/p/ask-vs-guess-culture</a>, See on <a href="https://news.ycombinator.com/item?id=37176703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Have you had someone ask you for a favor that seemed unreasonable — a referral you didn’t want to make, a long-term stay at your place, a sizable cash loan? But because they asked, you felt obliged to seriously consider it, to try to meet their request, even if it put you in a space of discomfort? Maybe you carry out the favor, but it sours your relationship, and when it all comes out, that person says, “Well why’d you agree to it? You could have just said no!”</p><p>But you feel resentful that that person even put you in a position to have to say, “Sorry we’re a bit busy that week so don’t have space for you to stay with us,” or “I can’t loan you that money at the moment”?</p><p>Congratulations, you’ve just encountered a clash between ask culture and guess culture.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png" width="1456" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d15f5ae-994c-49b5-99a6-5efff02b7eca_1456x816.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>The idea of ask vs guess culture was </span><a href="https://ask.metafilter.com/55153/Whats-the-middle-ground-between-FU-and-Welcome#830421" rel="">shared online</a><span> in 2007 by a user </span><em>tangerine</em><span> on Metafilter. When I first read it years ago, a lightbulb moment went off, and many frustrations and conflicts I had while growing up made much more sense in this framework. </span></p><p>Despite this idea’s longevity, I find that it’s still a new-to-many and incredibly useful concept to revisit, so here’s a little exploration of ask vs guess culture at home and at work.</p><p><span>Ask culture and guess culture are </span><em>vastly</em><span> different in behavior and expectations. Here are some highlights:</span></p><ul><li><p>Ask for what you want, even if it seems out of reach or like a big unreasonable request</p></li><li><p>Take care of your own needs, and others will take care of theirs</p></li><li><p>It’s fine to make requests that people will probably say no to</p></li><li><p>People say yes to requests that you truly feel good about, say no to ones they don’t</p></li></ul><ul><li><p>Only ask for something if you’re already pretty sure the other person will say yes</p></li><li><p>Read an abundance of indirect contextual cues to determine if your request is reasonable to make</p></li><li><p>It’s rude to put someone in a position where they have to say no to you</p></li><li><p>If the appropriate feelers and context are set, you will never have to make your request at all.</p></li></ul><p>It’s easiest to understand the differences between ask culture and guess culture through examples, so here are two examples with a moving situation — you’re moving soon and hope to save a few bucks with the help of your friends.</p><p>You post on Facebook sharing that you’re moving and make a list of things you could use help with: moving boxes and tape, packing help, usage of a truck/van, and physical labor on moving day. You reach out to a few local friends asking if they’re available on moving day. A few people respond on Facebook with moving supplies, and a friend comes over to help with packing, but no one is available to help on moving day, so you end up renting a moving van and hiring a few movers.</p><p>Your friend is typically free on the weekends, so you ask them if they’re available to help you on moving day. You ask another friend what they’re up to, and they have family visiting, so you don’t mention that you need help with moving. Another friend has access to a pickup truck, and you dropped off some soup recently when they were sick, so you mention that you’re moving next weekend. They ask if you’d like to borrow their truck, which you defer saying you don’t want to inconvenience them, but when they offer again, you accept.</p><p>Depending on whether you gravitate more towards ask or guess culture as your default, one of these scenarios may sound very uncomfortable. </p><p>If you’re more a guess-culture person, asking people for help without knowing their circumstances can feel rude or intrusive. Broadcasting publicly your need for help can feel awkward and vulnerable.</p><p>If you’re more of an ask-culture person, the guess-culture example of juggling everyone’s specific scenarios and the historical context of favors probably seems exhausting. Dropping hints in the hopes that you won’t even have to make your request can feel extra passive and manipulative.</p><p>I probably operate somewhere in-between ask and guess culture&nbsp;— defaulting to guess culture when I’m low-functioning but aspiring to be more and more ask-culture. </p><p>I was raised deeply in guess culture, as many Asians and Asian-Americans are. The Japanese proverb that “the nail that sticks up gets nailed down” reinforces the idea of social collectivism and keeping your individual needs and wants to yourself&nbsp;— values that are shared by many Asian culture. My parents rarely had to make explicit asks of me, because the expectations around values and behavior were communicated through indirect messaging, often by tone of voice or through stories about other people. </p><p>Western society is very much ask culture. A classic example can be found in proverbs. “A squeaky wheel gets the grease” is an American proverb, enforcing the ideas of individualism and that asking for what you want will benefit you. </p><p>The generational clash between ask and guess culture can be frustrating and exhausting. Years ago, my brother and I were in San Diego visiting our aunt and uncle. The plan was for my grandma to come down from Los Angeles, so we could all spend time together, but our grandma had last minute minor surgery to recover from and had to stay put in LA. “So we should drive up to see her, right?” my brother and I discussed. But all of the older relatives insisted we did not, suggesting that instead we see the sights in San Diego, that we take the kids to Sea World, that the traffic would be awful and that a 2 hour drive would turn into 5 hours, that it’d be dangerous. </p><p><span>This all seemed ridiculous to us, so instead we drove the two hours, keeping our plan </span><em>secret</em><span> until we pulled up into our grandma’s driveway, so that no one could resist and thwart our plan. We had a lovely visit, and my mom later thanked us for making the drive. </span></p><p>This is guess culture — and it’s a lot of saying not really what you actually want, and it’s a lot of reading between the lines to try to figure out what people want.</p><p><span>Deciding what to eat for dinner with guess-culture people isn’t as simple as asking people what they want to eat for dinner, because they will not tell you what they </span><strong>actually</strong><span> want to want to eat for dinner. They will say “oh, whatever you want,” or “whatever is easiest.” And when you insist that you really really want to know what they want to eat for dinner, and if it’s too much work, you’ll do something else instead, the response you receive will already be a compromised version of what they want, taking into account the preferences of everyone else in the house, what the kids will eat, and the leftovers in the fridge.</span></p><p>Thoughtful? Yes. But frustrating if you actually want an honest answer of what someone wants. You may be better off listing options and gauging their response to each one.</p><p>For guess-culture people, thinking about what it is you want can feel absolutely foreign, and for me, it’s been a years-long practice to continue to tap into and understand what I want, before I then try to take others’ needs into account.</p><p>At a high level, western corporate work operates almost entirely in ask culture. But people working at these companies often operate in or were raised in guess culture, which as you might expect, is ripe for feeling misunderstood and frustrated.</p><p><span>Last week, I shared about normalizing </span><a href="https://jeanhsu.substack.com/p/do-you-keep-your-wishes-secret" rel="">sharing we want in life and at work</a><span>, so that they might actually be supported in coming true. Ask vs guess culture is another lens at looking at asking for what you want at work.</span></p><p>Being guess-culture in an ask-culture work environment looks like hoping someone will tap you to become a manager because you’re clearly the best person for the job. </p><p>It also looks like being frustrated when others loudly express enthusiasm about taking on a new project on the roadmap and are given the opportunity to lead it, when you were also interested in it and maybe dropped some hints about it being somewhat interesting.</p><p>At a certain point, guess culture is not going to work for you, and you’ll feel under-acknowledged and overlooked. If you want to get more of what you want out of your work situation, you’ll have to lean more into ask culture. </p><p>But ask culture is vulnerable, because the requests you’re making are ones that feel out of reach, and requires being ok with people saying no to you, often. It requires putting things out there that you want help with, and trusting that people will say no to you instead of helping you resentfully.</p><p>Maybe just understanding the framework is already helpful, but here are a few small ways you can start to nudge yourself into ask culture.</p><ul><li><p>Ask for help on something you’re feeling stuck on. Guess-culture people will worry that they’re interrupting someone, or someone will be annoyed if they’re in the middle of something. If it feels more comfortable, you could say, “Let me know when is good for a half-hour working session today or tomorrow.”</p></li><li><p>Want to publish something on the company blog or give a talk at an upcoming event? Try asking. If “Hey can I give a talk at the next event?” feels too uncomfortable, try “Hey I’m really interested in giving a talk at a future event. What are you looking for?” or “I’d love to give a talk about &lt;topic&gt;, what do you think?”</p></li><li><p>Get more comfortable with people saying no to you. If people are not saying no to you, you’re probably still only asking for things that you already know people will say yes to (which is guess culture). Ask for more budget, ask for an uncomfortable amount of PTO, ask for professional development budget, ask to purchase only vaguely-work-related books on your company card.</p></li><li><p><span>Ask yourself, </span><a href="https://jeanhsu.substack.com/p/if-you-could-have-your-way" rel="">“If I could have my way…”</a><span>, which is a useful hack to bypass thinking about others’ needs and honing in on exactly what you want. Use this to think about your role, your next project, your work schedule, your title, your salary and equity, your team. From that thought exercise, ask for some things you want.</span></p></li></ul><p><em>Does this framework behind ask vs guess culture provide any clarity to you around past or current frustrations? When have you experienced clashes between ask and guess culture?</em></p><p data-attrs="{&quot;url&quot;:&quot;https://jeanhsu.substack.com/p/ask-vs-guess-culture/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://jeanhsu.substack.com/p/ask-vs-guess-culture/comments" rel=""><span>Leave a comment</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft's backwards compatibility is insane (209 pts)]]></title>
            <link>https://twitter.com/mikko/status/1692503249595584526</link>
            <guid>37176666</guid>
            <pubDate>Fri, 18 Aug 2023 15:04:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mikko/status/1692503249595584526">https://twitter.com/mikko/status/1692503249595584526</a>, See on <a href="https://news.ycombinator.com/item?id=37176666">Hacker News</a></p>
Couldn't get https://twitter.com/mikko/status/1692503249595584526: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Revealed: WHO aspartame safety panel linked to alleged Coca-Cola front group (155 pts)]]></title>
            <link>https://www.theguardian.com/business/2023/aug/17/who-panel-aspartame-diet-coke-guidelines</link>
            <guid>37176555</guid>
            <pubDate>Fri, 18 Aug 2023 14:56:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/business/2023/aug/17/who-panel-aspartame-diet-coke-guidelines">https://www.theguardian.com/business/2023/aug/17/who-panel-aspartame-diet-coke-guidelines</a>, See on <a href="https://news.ycombinator.com/item?id=37176555">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>In May, the World Health Organization issued an alarming <a href="https://www.who.int/news/item/15-05-2023-who-advises-not-to-use-non-sugar-sweeteners-for-weight-control-in-newly-released-guideline" data-link-name="in body link">report</a> that declared widely used non-sugar sweeteners like aspartame are likely ineffective for weight loss, and long term consumption may increase the risk of diabetes, cardiovascular diseases and mortality in adults.</p><p>A few months later, WHO declared aspartame, a key ingredient in Diet Coke, to be a “possible carcinogen”, then quickly issued a third report that seemed to contradict its previous findings – people could continue consuming the product at levels determined to be safe decades ago, before new science cited by WHO raised health concerns.</p><p>That contradiction stems from beverage industry corruption of the review process by consultants tied to an alleged Coca-Cola front group, the public health advocacy group US Right-To-Know said in a recent <a href="https://usrtk.org/sweeteners/coca-cola-front-group-who-review-of-aspartame/" data-link-name="in body link">report</a>.</p><p>It uncovered eight WHO panelists involved with assessing safe levels of aspartame consumption who are beverage industry consultants who currently or previously worked with the alleged Coke front group, International Life Sciences Institute (Ilsi).</p><p>Their involvement in developing intake guidelines represents “an obvious conflict of interest”, said Gary Ruskin, US Right-To-Know’s executive director. “Because of this conflict of interest, [the daily intake] conclusions about aspartame are not credible, and the public should not rely on them,” he added.</p><p>Aspartame was first approved for use in the US in the early 1980s over the objection of some researchers who warned of potential health risks. In recent years, as <a href="http://www.mpwhi.com/soffritti_2010_20896_fta.pdf" data-link-name="in body link">evidence</a> of health threats has mounted, industry has ramped up a PR <a href="https://www.nbcnews.com/healthmain/amid-falling-sales-coke-launches-campaign-defend-sweetener-6c10920355" data-link-name="in body link">campaign</a> to downplay the issues.</p><p>In the World Health Organization’s 14 July aspartame hazard and risk <a href="https://www.iarc.who.int/news-events/aspartame-hazard-and-risk-assessment-results-released/" data-link-name="in body link">assessments</a>, its cancer research arm, the International Agency for Research on Cancer (Iarc) <a href="https://www.who.int/news/item/14-07-2023-aspartame-hazard-and-risk-assessment-results-released" data-link-name="in body link">classified</a> aspartame as “<a href="https://www.thelancet.com/journals/lanonc/article/PIIS1470-2045(23)00341-8/fulltext" data-link-name="in body link">possibly carcinogenic</a>”. That same day, WHO’s Joint Expert Committee on Food Additives (Jecfa), which makes consumption recommendations, <a href="https://cdn.who.int/media/docs/default-source/food-safety/jecfa/summary-and-conclusions/jecfa96-summary-and-conclusions.pdf?sfvrsn=f7b61f6c_4&amp;download=true" data-link-name="in body link">reaffirmed</a> the acceptable daily intake of 40 mg/kg of body weight.</p><p>Ilsi describes itself as a nonprofit that conducts “science for the public good”, but it was founded in 1978 by a Coca-Cola executive who simultaneously worked for the company through 2021, US Right-To-Know found. Other Coca-Cola executives have worked with the group, and US Right-To-Know <a href="https://usrtk.org/pesticides/ilsi-is-a-food-industry-lobby-group/" data-link-name="in body link">detailed</a> tax returns that show millions in donations from Coca-Cola and other beverage industry players. Coke <a href="https://www.bloomberg.com/news/articles/2021-01-13/coca-cola-severs-longtime-ties-with-pro-sugar-industry-group#xj4y7vzkg" data-link-name="in body link">ended</a> its official membership with the group in 2021.</p><p>Over the years, Ilsi representatives have sought to shape food policy worldwide, and Ruskin, who has written multiple peer-reviewed papers on the group, characterized the aspartame controversy as a “masterpiece in how Ilsi worms its way into these regulatory processes”.</p><p>US Right To Know identified six out of 13 Jefca panel members with ties to the industry group. After it released its report, the WHO acknowledged two more of its members with industry ties.</p><p>In a statement to the Guardian, a WHO spokesperson defended the industry consultants’ inclusion in the review process.</p><p>“For the meeting on aspartame, Jefca selected the experts likely to make the best contributions to the debate,” said spokesperson Fadéla Chaib. She said WHO’s guidelines only require disclosure of conflicts of interest within the last four years.</p><p>“To our knowledge, the experts you listed by name have not participated in any Ilsi activities for at least 10 years,” she said.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-13">skip past newsletter promotion</a><p id="EmailSignup-skip-link-13" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>But that partially contradicts a statement made by WHO just weeks before to the news outlet <a href="https://www.leparisien.fr/sciences/nouvelles-recommandations-sur-laspartame-les-liaisons-dangereuses-de-certains-experts-avec-coca-et-pepsi-19-07-2023-J3BYCFSOJJG7ZGHJEHFOVR5XFM.php" data-link-name="in body link">Le Parisien</a> in which it acknowledged two people currently working with Ilsi were involved in the process. The Guardian had also asked about those two people identified in the Parisien story but were not listed “by name” in its email.</p><p>The WHO told Le Parisien: “We regret that this interest was not declared by these two experts either in the written statement or orally at the opening of the meeting.”</p><p>WHO’s inclusion of Ilsi-tied consultants in its review process is especially alarming because WHO has in place “much higher standards” to ensure there are no conflicts of interest in its processes, Ruskin said. He noted WHO only relies on publicly available, peer-reviewed science, while excluding corporate interest studies.</p><p>Ruskin said the move also marks a change in direction for WHO, which in 2015 distanced itself from Ilsi when its executive board found the group to be a “private entity” and voted to <a href="https://usrtk.org/wp-content/uploads/2018/07/ILSI-Official-Relations-WHO.pdf" data-link-name="in body link">discontinue</a> its official relationship.</p><p>Ruskin said the damage has been done. In the “avalanche” of media coverage of WHO’s designation of aspartame as a possible carcinogen, many outlets noted WHO’s split decision, or reported that WHO <a href="https://www.theguardian.com/society/2023/jul/14/aspartame-is-safe-in-limited-amounts-say-experts-after-cancer-warning" data-link-name="in body link">found the product to be safe</a>. Those reports did not note Ilsi’s fingerprints on the safety assessment, Ruskin said.</p><p>“So much of the tone of it has been ‘There was a split decision at WHO and we shouldn’t be concerned, so go ahead and drink all you want,’” he said. “That has so poorly served the public.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why GNU su does not support the `wheel' group (2002) (104 pts)]]></title>
            <link>https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_149.html</link>
            <guid>37175754</guid>
            <pubDate>Fri, 18 Aug 2023 14:03:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_149.html">https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_149.html</a>, See on <a href="https://news.ycombinator.com/item?id=37175754">Hacker News</a></p>
<div id="readability-page-1" class="page">

<a name="SEC149"></a>
<table>
<tbody><tr><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_148.html#SEC148"> &lt; </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_150.html#SEC151"> &gt; </a>]</td>
<td> &nbsp; </td><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_144.html#SEC144"> &lt;&lt; </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_144.html#SEC144"> Up </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_150.html#SEC151"> &gt;&gt; </a>]</td>
<td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils.html#SEC_Top">Top</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_toc.html#SEC_Contents">Contents</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_186.html#SEC187">Index</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_abt.html#SEC_About"> ? </a>]</td>
</tr></tbody></table>
<hr size="1">
<h2> 22.5 <code>su</code>: Run a command with substitute user and group id </h2>
<!--docid::SEC149::-->
<p>

<code>su</code> allows one user to temporarily become another user.  It runs a
command (often an interactive shell) with the real and effective user
id, group id, and supplemental groups of a given <var>user</var>. Synopsis:
</p><p>

<table><tbody><tr><td>&nbsp;</td><td><pre>su [<var>option</var>]<small>...</small> [<var>user</var> [<var>arg</var>]<small>...</small>]
</pre></td></tr></tbody></table></p><p>

<a name="IDX1807"></a>
<a name="IDX1808"></a>
<a name="IDX1809"></a>
If no <var>user</var> is given, the default is <code>root</code>, the super-user.
The shell to use is taken from <var>user</var>'s <code>passwd</code> entry, or
`<tt>/bin/sh</tt>' if none is specified there.  If <var>user</var> has a
password, <code>su</code> prompts for the password unless run by a user with
effective user id of zero (the super-user).
</p><p>

<a name="IDX1810"></a>
<a name="IDX1811"></a>
<a name="IDX1812"></a>
<a name="IDX1813"></a>
<a name="IDX1814"></a>
By default, <code>su</code> does not change the current directory.
It sets the environment variables <code>HOME</code> and <code>SHELL</code>
from the password entry for <var>user</var>, and if <var>user</var> is not
the super-user, sets <code>USER</code> and <code>LOGNAME</code> to <var>user</var>.
By default, the shell is not a login shell.
</p><p>

Any additional <var>arg</var>s are passed as additional arguments to the
shell.
</p><p>

<a name="IDX1815"></a>
GNU <code>su</code> does not treat `<tt>/bin/sh</tt>' or any other shells specially
(e.g., by setting <code>argv[0]</code> to `<samp>-su</samp>', passing <code>-c</code> only
to certain shells, etc.).
</p><p>

<a name="IDX1816"></a>
<code>su</code> can optionally be compiled to use <code>syslog</code> to report
failed, and optionally successful, <code>su</code> attempts.  (If the system
supports <code>syslog</code>.)  However, GNU <code>su</code> does not check if the
user is a member of the <code>wheel</code> group; see below.
</p><p>

The program accepts the following options.  Also see <a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_2.html#SEC2">2. Common options</a>.
</p><dl compact="">
<dt>`<samp>-c <var>command</var></samp>'
</dt><dd></dd><dt>`<samp>--command=<var>command</var></samp>'
</dt><dd><a name="IDX1817"></a>
<a name="IDX1818"></a>
Pass <var>command</var>, a single command line to run, to the shell with
a <code>-c</code> option instead of starting an interactive shell.
</dd><dt>`<samp>-f</samp>'
</dt><dd></dd><dt>`<samp>--fast</samp>'
</dt><dd><a name="IDX1819"></a>
<a name="IDX1820"></a>
<a name="IDX1821"></a>
<a name="IDX1822"></a>
<a name="IDX1823"></a>
Pass the <code>-f</code> option to the shell.  This probably only makes sense
if the shell run is <code>csh</code> or <code>tcsh</code>, for which the <code>-f</code>
option prevents reading the startup file (`<tt>.cshrc</tt>').  With
Bourne-like shells, the <code>-f</code> option disables file name pattern
expansion (globbing), which is not likely to be useful.
</dd><dt>`<samp>-</samp>'
</dt><dd></dd><dt>`<samp>-l</samp>'
</dt><dd></dd><dt>`<samp>--login</samp>'
</dt><dd><a name="IDX1824"></a>
<a name="IDX1825"></a>
<a name="IDX1826"></a>
<a name="IDX1827"></a>
<a name="IDX1828"></a>
<a name="IDX1829"></a>
Make the shell a login shell.  This means the following.  Unset all
environment variables except <code>TERM</code>, <code>HOME</code>, and <code>SHELL</code>
(which are set as described above), and <code>USER</code> and <code>LOGNAME</code>
(which are set, even for the super-user, as described above), and set
<code>PATH</code> to a compiled-in default value.  Change to <var>user</var>'s home
directory.  Prepend `<samp>-</samp>' to the shell's name, intended to make it
read its login startup file(s).
</dd><dt>`<samp>-m</samp>'
</dt><dd></dd><dt>`<samp>-p</samp>'
</dt><dd></dd><dt>`<samp>--preserve-environment</samp>'
</dt><dd><a name="IDX1830"></a>
<a name="IDX1831"></a>
<a name="IDX1832"></a>
<a name="IDX1833"></a>
<a name="IDX1834"></a>
<a name="IDX1835"></a>
Do not change the environment variables <code>HOME</code>, <code>USER</code>,
<code>LOGNAME</code>, or <code>SHELL</code>.  Run the shell given in the environment
variable <code>SHELL</code> instead of the shell from <var>user</var>'s passwd
entry, unless the user running <code>su</code> is not the superuser and
<var>user</var>'s shell is restricted.  A <em>restricted shell</em> is one that
is not listed in the file `<tt>/etc/shells</tt>', or in a compiled-in list
if that file does not exist.  Parts of what this option does can be
overridden by <code>--login</code> and <code>--shell</code>.
</dd><dt>`<samp>-s <var>shell</var></samp>'
</dt><dd></dd><dt>`<samp>--shell=<var>shell</var></samp>'
</dt><dd><a name="IDX1836"></a>
<a name="IDX1837"></a>
Run <var>shell</var> instead of the shell from <var>user</var>'s passwd entry,
unless the user running <code>su</code> is not the superuser and <var>user</var>'s
shell is restricted (see `<samp>-m</samp>' just above).
</dd></dl>
<h2> Why GNU <code>su</code> does not support the `<samp>wheel</samp>' group </h2>
<!--docid::SEC150::-->
<p>

(This section is by Richard Stallman.)
</p><p>

<a name="IDX1841"></a>
<a name="IDX1842"></a>
Sometimes a few of the users try to hold total power over all the
rest.  For example, in 1984, a few users at the MIT AI lab decided to
seize power by changing the operator password on the Twenex system and
keeping it secret from everyone else.  (I was able to thwart this coup
and give power back to the users by patching the kernel, but I
wouldn't know how to do that in Unix.)
</p><p>

However, occasionally the rulers do tell someone.  Under the usual
<code>su</code> mechanism, once someone learns the root password who
sympathizes with the ordinary users, he or she can tell the rest.  The
"wheel group" feature would make this impossible, and thus cement the
power of the rulers.
</p><p>

I'm on the side of the masses, not that of the rulers.  If you are
used to supporting the bosses and sysadmins in whatever they do, you
might find this idea strange at first.
</p><hr size="1">
<table>
<tbody><tr><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_148.html#SEC148"> &lt; </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_150.html#SEC151"> &gt; </a>]</td>
<td> &nbsp; </td><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_144.html#SEC144"> &lt;&lt; </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_144.html#SEC144"> Up </a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_150.html#SEC151"> &gt;&gt; </a>]</td>
<td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td> &nbsp; </td><td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils.html#SEC_Top">Top</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_toc.html#SEC_Contents">Contents</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_186.html#SEC187">Index</a>]</td>
<td>[<a href="https://ftp.gnu.org/old-gnu/Manuals/coreutils-4.5.4/html_node/coreutils_abt.html#SEC_About"> ? </a>]</td>
</tr></tbody></table>
<br>  
<span size="-1">
This document was generated
by <i>Jeff Bailey</i> on <i>December, 28  2002</i>
using <a href="http://www.mathematik.uni-kl.de/~obachman/Texi2html"><i>texi2html</i></a>
</span>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sargablock: Bricks from Seaweed (141 pts)]]></title>
            <link>https://fortomorrow.org/explore-solutions/sargablock</link>
            <guid>37175721</guid>
            <pubDate>Fri, 18 Aug 2023 14:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fortomorrow.org/explore-solutions/sargablock">https://fortomorrow.org/explore-solutions/sargablock</a>, See on <a href="https://news.ycombinator.com/item?id=37175721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="presentation-panel" role="tabpanel" aria-labelledby="presentation-tab"><p>My journey began in 2015 when sargassum seaweed first started washing up on the shores of the Riviera Maya. Where others saw a problem, I saw an opportunity to turn it into its own sustainable solution, including placing it in service of people who need it the most.</p><p><img src="https://fortomorrowprod.blob.core.windows.net/assets/default-path/Sargablock3_10db329dbe.png" alt=""></p><p>I started collecting sargassum seaweed to use as fertilizer for my business, Blue-Green Nursery, and selling it in small amounts to my clients. Soon I obtained permits and within a year was employing about 300 families to clean the beaches for local hotels and resorts. But then it occurred to me that we could turn sargassum seaweed into construction bricks as it was already being used to make products like plates and other things. Inspired by the memory of my family’s little adobe house, I developed Sargablock, an architectural brick made from the sargassum seaweed that spoils our beaches between April and October.</p><p><img src="https://fortomorrowprod.blob.core.windows.net/assets/default-path/Sargablock1_d10b266802.png" alt=""></p><p>&nbsp;I adjusted a machine designed to make adobe bricks so that it can process a mix of 40% sargassum and 60% other organic materials for the Sargablock. The machine can turn out 1,000 blocks a day, and after four hours of baking in the sun, they are dried and ready to be used. After we built Casa Angelita, the first sargassum house named after my mother, Sargablock became one of the first seaweed projects to get off to a solid start in our state of Quintana Roo. From there, I was determined to use sargassum seaweed as a low-cost building material to build affordable housing throughout the Riviera Maya so that families can live in their own homes.</p><p><img src="https://fortomorrowprod.blob.core.windows.net/assets/default-path/15_sargassum_house_001_fc796ef165.jpeg" alt="">&nbsp;</p><p>Now we have a first artisanal factory in Mahahual, a tourist town where a dock for cruise ships operates, and we are creating jobs. A sargassum house could last 120 years, and we would like to have 10 houses finished by the end of the year, which will be donated to underprivileged families. My vision goes beyond turning a profit; I would like to see a country where local entrepreneurs create thriving, sustainable businesses that give back to their communities.</p><p>People from countries like Belize, Jamaica, Puerto Rico, the Dominican Republic, Barbados, Malaysia and the United States are contacting us for help in doing what we are doing. They’re all hit harder every day by the seaweed washing up on their beaches. It is maybe nature's way of telling us to protect our seas. We must be aware of what is happening on the beaches. The sea is very wise and is telling us something. With the contamination we have created over so many years it tells us to take care of what we have.</p><p><strong>Read more:</strong> Exposure article: Sea Change (<a href="https://undplac.exposure.co/sea-change" target="_blank">English</a> and <a href="https://undplac.exposure.co/un-cambio-para-el-oceano" target="_blank">Spanish</a>) - published on the 13th of February, 2020) by UNDP Mexico Environmental and Resilience Unit. Text by Emily Mkrtichian, Edited by Andrea Egan</p><p><em>Photography: Emily Mkrtichian for UNDP Mexico, with exception of the image of sargassum on the beach courtesy of Elena Tarassova.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mister Rogers Had a Point: Routinely Greeting Six Neighbors Maximizes Wellbeing (253 pts)]]></title>
            <link>https://www.goodnewsnetwork.org/mister-rogers-had-a-point-routinely-greeting-six-neighbors-maximizes-wellbeing-outcomes/</link>
            <guid>37175432</guid>
            <pubDate>Fri, 18 Aug 2023 13:36:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.goodnewsnetwork.org/mister-rogers-had-a-point-routinely-greeting-six-neighbors-maximizes-wellbeing-outcomes/">https://www.goodnewsnetwork.org/mister-rogers-had-a-point-routinely-greeting-six-neighbors-maximizes-wellbeing-outcomes/</a>, See on <a href="https://news.ycombinator.com/item?id=37175432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <figure id="attachment_196771" aria-describedby="caption-attachment-196771"><img decoding="async" src="https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy.jpg" alt="" width="800" height="400" srcset="https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy.jpg 800w, https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy-326x163.jpg 326w, https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy-768x384.jpg 768w, https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy-696x348.jpg 696w, https://www.goodnewsnetwork.org/wp-content/uploads/2023/08/Mr-Fred-Rogers-CC-2.0.-Intergalactically-Speaking-retireved-via-Flickr-Copy-530x265.jpg 530w" sizes="(max-width: 800px) 100vw, 800px"><figcaption id="caption-attachment-196771">Mr Fred Rogers – CC 2.0. Intergalactically Speaking, retrieved via Flickr&nbsp;</figcaption></figure>
<p>It’s a beautiful day in the neighborhood, haven’t you heard? Mister Rogers said so—and now his simple advice on how to be a good person has been backed by sophisticated polling data.</p>
<p>As part of the Gallup National Health and Well-Being Index, saying hello to more than 1 neighbor was shown to correlate with greater self-perception of well-being.</p>
<p>Averaged across five dimensions that included career, communal, physical, financial, and social well-being, the increase which greeting a neighbor had led to around a 2-point increase on a scale of 0-100 up until the sixth neighbor, at which point further greetings had no measured impact.</p>
<p>Interestingly, when the well-being scores are looked at individually and not averaged together, the sixth neighbor is where the perception of well-being in life peaks for social and communal well-being, but not financial well-being.</p>
<p>No; perception of financial well-being kept on climbing and climbing, only to cease at the 11th such greeting; a profound revelation—repeated positive social interaction benefited perception of personal finance even more than personal sense of community.</p>
<p>Men were more likely to greet neighbors than women, as were people with children under the age of 18 in the household, and people with a household income of more than $120k a year.</p>
<p>Individuals aged 40 to 65+ were the most common greeters of neighbors, and 27% of the over 4,000 participants greeted 5 neighbors or more in a day.</p>
<p><strong>MORE INSPIRING POLLS: </strong><a title="Survey Reveals Americans are Retiring Earlier Than They’d Expected" href="https://www.goodnewsnetwork.org/survey-reveals-americans-are-retiring-earlier-than-theyd-expected/" rel="bookmark">Survey Reveals Americans are Retiring Earlier Than They’d Expected</a></p>
<p>“Recent Gallup research in partnership with Meta has shown that the U.S. compares favorably with other nations around the world in social interactions,” the polling company states, “with those in the U.S. more likely than those in countries such as Mexico, India, and France to interact with the people who live near them.”</p>
<p>“Notably, greeting neighbors is also linked to career wellbeing (liking what you do each day), physical wellbeing (having energy to get things done), and financial wellbeing (managing your money well),” the report continued. “The associations found among these latter three elements are likely more multifaceted in nature and could be reinforced in part through the correlations found with social and community wellbeing.”</p>
<p><strong>WATCH what used to be our daily reminder below…&nbsp;</strong></p>
 <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/FhAJnx5uwUU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> 
<p><strong>SHARE This Study (And Fred Rogers) Wisdom With Your Friends (And Neighbors)…</strong></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Low dose radiation cancer 2x worse than predicted by LNT model (146 pts)]]></title>
            <link>https://www.bmj.com/content/382/bmj-2022-074520</link>
            <guid>37175228</guid>
            <pubDate>Fri, 18 Aug 2023 13:20:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bmj.com/content/382/bmj-2022-074520">https://www.bmj.com/content/382/bmj-2022-074520</a>, See on <a href="https://news.ycombinator.com/item?id=37175228">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Broadband monopolies push bill that would crush your ability to stand up to them (224 pts)]]></title>
            <link>https://www.techdirt.com/2023/08/18/with-hr-3557-broadband-monopolies-are-pushing-a-bill-that-would-crush-your-towns-ability-to-stand-up-to-them/</link>
            <guid>37175200</guid>
            <pubDate>Fri, 18 Aug 2023 13:17:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/08/18/with-hr-3557-broadband-monopolies-are-pushing-a-bill-that-would-crush-your-towns-ability-to-stand-up-to-them/">https://www.techdirt.com/2023/08/18/with-hr-3557-broadband-monopolies-are-pushing-a-bill-that-would-crush-your-towns-ability-to-stand-up-to-them/</a>, See on <a href="https://news.ycombinator.com/item?id=37175200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-418603">


<h3>from the <i>this-is-why-we-can't-have-nice-things</i> dept</h3>

<p>For thirty-plus years, giant telecom monopolies have worked tirelessly to crush all broadband competition. At the same time, they’ve lobbied state and federal governments so extensively, that the vast majority of politicians are feckless cardboard cutouts with little real interest in market or consumer health. </p>
<p>The result has been fairly obvious: Americans pay some of the highest prices in the developed world for sluggish, slow broadband with <a href="https://www.techdirt.com/2023/06/23/broadband-isp-customer-satisfaction-ranked-second-worst-above-only-gas-stations/" data-type="link" data-id="https://www.techdirt.com/2023/06/23/broadband-isp-customer-satisfaction-ranked-second-worst-above-only-gas-stations/">historically abysmal customer service</a>. </p>
<p>Telecom lobbyists love to insist that often-shitty U.S. broadband is the envy of the modern world (it isn’t). They also love to argue that the <strong>only</strong> reason U.S. broadband isn’t even <em>more awesome</em> is because of “too much government regulation,” unnecessary red tape, and “bureaucracy.”</p>
<p>In this way they get to have their cake (enjoy unchecked monopoly power free from competition or regulatory oversight) and eat it too (demonize <strong>any</strong> government effort to do <strong>anything</strong> about monopoly power as the antithesis of progress). It’s an endless cycle where your broadband monopoly gets more powerful and the government gets weaker. Often under the pretense of “deregulation” and “reform.”</p>
<p>It’s a pretty successful con. Telecom giants like Comcast and AT&amp;T have effectively lobotomized the country’s top federal telecom regulator (the FCC) for going on the <a href="https://www.techdirt.com/2023/06/02/telecom-lobbyists-have-had-the-fcc-under-their-boot-heel-for-7-straight-years-and-nobody-much-seems-to-care/" data-type="link" data-id="https://www.techdirt.com/2023/06/02/telecom-lobbyists-have-had-the-fcc-under-their-boot-heel-for-7-straight-years-and-nobody-much-seems-to-care/">better part of a decade</a>. They also have the majority of Congress under their bootheel, to the point where we can’t pass <a href="https://www.techdirt.com/2017/03/28/consumer-broadband-privacy-protections-are-dead/" data-type="link" data-id="https://www.techdirt.com/2017/03/28/consumer-broadband-privacy-protections-are-dead/">even basic consumer protection reforms</a>, or <a href="https://www.techdirt.com/2023/03/07/telecom-monopolies-win-again-gigi-sohn-forced-to-withdraw-from-fcc-nomination/" data-type="link" data-id="https://www.techdirt.com/2023/03/07/telecom-monopolies-win-again-gigi-sohn-forced-to-withdraw-from-fcc-nomination/">nominate and seat <strong>any</strong> popular reformers</a> to federal agencies. </p>
<p>Even when you see “antitrust reform” <a href="https://www.techdirt.com/2023/02/10/gop-stops-pretending-it-ever-actually-cared-about-antitrust-reform/" data-type="link" data-id="https://www.techdirt.com/2023/02/10/gop-stops-pretending-it-ever-actually-cared-about-antitrust-reform/">performatively mentioned</a> in Congress, telecom is never included. And this is all before the looming right wing Supreme Court’s <a href="https://www.politico.com/news/2023/05/01/supreme-court-chevron-doctrine-climate-change-00094670" data-type="link" data-id="https://www.politico.com/news/2023/05/01/supreme-court-chevron-doctrine-climate-change-00094670">Chevron Deference ruling</a> strips away any remaining independent authority the FCC (and every other federal regulatory agency) has left. </p>
<p>With the feds rendered useless on consumer protection and monopoly busting, telecom lobbyists have increasingly taken aim at the last vestiges of state and local government power. </p>
<p>During the Trump FCC, that involved stripping away your town’s or <a href="https://www.techdirt.com/2019/11/20/46-cities-sue-fcc-trampling-their-rights/" data-type="link" data-id="https://www.techdirt.com/2019/11/20/46-cities-sue-fcc-trampling-their-rights/">city’s local authority </a>over everything from cell tower placement to utility pole rates (again under the pretense that <em>mean ‘ole Mayors</em>, not unchecked monopolization and consolidation, is what’s holding back better broadband). </p>
<p>The Ajit Pai era net neutrality repeal tried to ban states (<a href="https://www.techdirt.com/2022/01/28/courts-again-shoot-down-telecom-lobbys-attempt-to-kill-state-level-net-neutrality-rules/" data-type="link" data-id="https://www.techdirt.com/2022/01/28/courts-again-shoot-down-telecom-lobbys-attempt-to-kill-state-level-net-neutrality-rules/">unsuccessfully</a>) from protecting broadband consumers. <a href="https://broadbandnow.com/report/municipal-broadband-roadblocks" data-type="link" data-id="https://broadbandnow.com/report/municipal-broadband-roadblocks">16 States have passed laws</a> banning your town or city from building its own broadband. House Republicans even tried to <a href="https://www.techdirt.com/2021/02/19/new-bill-tries-to-ban-community-broadband-during-pandemic/" data-type="link" data-id="https://www.techdirt.com/2021/02/19/new-bill-tries-to-ban-community-broadband-during-pandemic/">ban community-owned broadband networks</a> entirely <em>in the middle of a pandemic that painfully showcased the importance of affordable, reliable broadband</em>. </p>
<p>The goal throughout is obvious: AT&amp;T, Verizon, Comcast, Charter, CenturyLink and other telecom giants want to be free to rip you off with high prices and substandard service. They want to be free of all meaningful competition. And they want local, state, and federal governments absolutely powerless to do anything about it. If there are <strong>any</strong> rules, they want to write them to their personal benefit.</p>
<p>It’s a campaign they’ve been winning for decades.</p>
<p>Nineteen proposed new telecom laws just stumbled forth from the the House Energy and Commerce Committee. One of them, <a href="https://www.congress.gov/bill/118th-congress/house-bill/3557" data-type="link" data-id="https://www.congress.gov/bill/118th-congress/house-bill/3557">HR 3557</a> (the American Broadband Deployment Act of 2023) focus on “pre-empting” any remaining local government authority over telecom giants. This is being presented, once again, as a “<a href="https://www.nlc.org/article/2023/06/08/house-committee-advances-communications-infrastructure-preemption-bill/" data-type="link" data-id="https://westviewnews.org/2023/07/21/alert-house-bill-3557-overrides-local-governments-in-push-to-streamline-deployment-of-wireless-broadband/westview-news/">streamlining</a>” of unnecessarily bureaucratic government power. </p>
<p>In reality, HR 3557 would make it all but impossible for local governments to have much of a say in telecom infrastructure placement (regardless of environmental or historical impact), negotiate fair rates with telecom giants for things like rights of way, or utility pole usage, or have much of a any role in terms of consumer protection. <a href="https://potsandpansbyccg.com/2023/08/14/preempting-local-government/" data-type="link" data-id="https://potsandpansbyccg.com/2023/08/14/preempting-local-government/">From Doug Dawson, a widely respected industry consultant</a>:</p>
<blockquote>
<p><em>[HR 3557] “Eliminates cable franchise renewals and eliminates the ability of local governments to require rules such as an ISP having to serve the whole community, the local government requiring PEG channels, <span>or the local government requiring customer service standards</span>.”</em></p>
</blockquote>
<p>It’s worth noting that the human beings actually involved in local governments <a href="https://www.nlc.org/article/2023/06/08/house-committee-advances-communications-infrastructure-preemption-bill/" data-type="link" data-id="https://www.nlc.org/article/2023/06/08/house-committee-advances-communications-infrastructure-preemption-bill/">weren’t invited to the single hearing on this subject</a>, which speaks volumes:</p>
<blockquote>
<p><em>“When the committee&nbsp;<a href="https://energycommerce.house.gov/events/communications-and-technology-subcommittee-legislative-hearing-breaking-barriers-streamlining-permitting-to-expedite-broadband-deployment">held an initial hearing</a>&nbsp;on broadband permitting streamlining, including a draft of the American Broadband Deployment Act, no state or local government was invited to testify.”</em></p>
</blockquote>
<p>In telecom policy conversations, local governments are always framed as some kind of faceless bureaucrats hell bent on ruining everybody’s good time for no coherent reason. But a lot of the systems telecom lobbyists are trying to dismantle serve important functions:</p>
<blockquote>
<p><em>“Local permitting and cable franchising processes are intended to make sure that communications infrastructure is deployed equitably and in the public interest, that work is done safely and in a way that protects valuable public resources, including the rights of way. Local governments are the stewards of these finite public resources.”</em></p>
</blockquote>
<p>It’s no coincidence that this new lobbying push comes as local governments around the country increasingly eye building their own fiber networks with an eye on affordability and even coverage (something that’s <a href="https://www.techdirt.com/2023/05/31/community-owned-broadband-network-again-tops-list-of-most-popular-isps/" data-type="link" data-id="https://www.techdirt.com/2023/05/31/community-owned-broadband-network-again-tops-list-of-most-popular-isps/">very popular</a> among long-neglected consumers). As Dawson notes, AT&amp;T and Comcast certainly don’t want your piddly Mayor standing in the way of them doing whatever they want:</p>
<blockquote>
<p><em>“The biggest killer is that the law would give holders of franchise agreements the ability to cancel the agreement without losing any rights-of-ways included in the agreement. This would also kill local franchise fees, a major source of revenue for many governments. Perhaps the most severe provision is that franchise contract holders can eliminate any contract provisions they deem to be commercially infeasible.”</em></p>
</blockquote>
<p>To be clear: some of the telecom bills winding through Congress are actually helpful. One would renew the FCC’s authority to conduct spectrum auctions, bizarrely lapsed due to a <a href="https://www.techdirt.com/2023/03/15/congress-lets-the-fccs-spectrum-auction-authority-lapse-for-no-good-reason/" data-type="link" data-id="https://www.techdirt.com/2023/03/15/congress-lets-the-fccs-spectrum-auction-authority-lapse-for-no-good-reason/">recent bout of government incompetence</a>. Another <a href="https://communitynets.org/content/isps-large-and-small-push-tax-exempt-broadband-grants" data-type="link" data-id="https://communitynets.org/content/isps-large-and-small-push-tax-exempt-broadband-grants">would make broadband grants tax exempt</a> (a boon to telecoms big and small). </p>
<p>But as Dawson quite correctly notes, legislation like HR 3557 is seeded in there in the hopes it worms its way into a broader bill under the pretense of a broader telecom reform package. I’d be genuinely surprised if the bill itself wasn’t written by an AT&amp;T or Comcast lawyer (probably <a href="https://www.techdirt.com/2015/04/16/alec-threatens-to-sue-critics-that-point-out-it-helps-keep-broadband-uncompetitive/" data-type="link" data-id="https://www.techdirt.com/2015/04/16/alec-threatens-to-sue-critics-that-point-out-it-helps-keep-broadband-uncompetitive/">using ALEC as a proxy</a>):</p>
<blockquote>
<p><em>“This bill is going for a home run to eliminate local regulations these big companies don’t like. I’ve written recently about regulatory capture, and this is an ultimate example of changing the laws to get what the big monopoly providers want…This bill is the ultimate example of the biggest companies in telecom flexing their power and influence to bypass some of the last vestiges of regulation.”</em></p>
</blockquote>
<p>As the federal government becomes (quite intentionally) more dysfunctional, feckless, and corrupt, most meaningful telecom policy fights have shifted to the state or local level. In town after town, locals now find themselves fighting block by block against monopoly power, with less and less meaningful support from the feckless and captured federal government. Now, industry is eyeing the killing blow. </p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/american-broadband-deployment-act/" rel="tag">american broadband deployment act</a>, <a href="https://www.techdirt.com/tag/broadband/" rel="tag">broadband</a>, <a href="https://www.techdirt.com/tag/community-broadband/" rel="tag">community broadband</a>, <a href="https://www.techdirt.com/tag/corruption/" rel="tag">corruption</a>, <a href="https://www.techdirt.com/tag/fcc/" rel="tag">fcc</a>, <a href="https://www.techdirt.com/tag/hr-3557/" rel="tag">hr 3557</a>, <a href="https://www.techdirt.com/tag/local-government/" rel="tag">local government</a>, <a href="https://www.techdirt.com/tag/monopoly/" rel="tag">monopoly</a>, <a href="https://www.techdirt.com/tag/municipal-broadband/" rel="tag">municipal broadband</a>, <a href="https://www.techdirt.com/tag/pre-emption/" rel="tag">pre-emption</a>, <a href="https://www.techdirt.com/tag/regulatory-reform/" rel="tag">regulatory reform</a>, <a href="https://www.techdirt.com/tag/telecom/" rel="tag">telecom</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Node.js 20.6.0 will include built-in support for .env files (169 pts)]]></title>
            <link>https://twitter.com/kom_256/status/1692225622091706389</link>
            <guid>37174916</guid>
            <pubDate>Fri, 18 Aug 2023 12:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/kom_256/status/1692225622091706389">https://twitter.com/kom_256/status/1692225622091706389</a>, See on <a href="https://news.ycombinator.com/item?id=37174916">Hacker News</a></p>
Couldn't get https://twitter.com/kom_256/status/1692225622091706389: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[You probably don’t need to fine-tune LLMs (149 pts)]]></title>
            <link>https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/</link>
            <guid>37174850</guid>
            <pubDate>Fri, 18 Aug 2023 12:41:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/">https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/</a>, See on <a href="https://news.ycombinator.com/item?id=37174850">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-268">

                            
    
                        <div>
                            
<p><em>This post is targeted towards folks focused on building LLM applications (as opposed to research).</em></p>



<p>If you’re a builder, it’s important to know what’s available in your toolbox, and the right time to use a given tool. Depending on what you’re doing, there are probably ones you use more often (hammer, screwdriver), and ones that you use less often (say, a hacksaw).</p>



<p>A lot of very smart people are experimenting with LLMs right now — resulting in a pretty jam-packed toolbox, acronyms and all (fine-tuning, RLHF, RAG, chain-of-thought, etc). It’s easy to get stuck in the decision paralysis stage of “what technical approach do I use”, even if your ultimate goal is to “build an app for X”.</p>



<p>On their own, people often run into issues with base model LLMs — “the model didn’t return what I wanted” or “the model hallucinated, its answer makes no sense” or “the model doesn’t know anything about Y because it wasn’t trained on it”.</p>



<p>People sometimes turn to a fairly involved technique called fine-tuning, in hopes that it will solve all of the above. <strong>In this post, we’ll talk about why fine-tuning is probably not necessary for your app.</strong></p>



<p>Specifically, people often think of “fine-tuning” when they want one or both of the following:<br></p>



<ul>
<li><strong>Additional structure/style: </strong>They want<strong> </strong>the LLM to do a more specific task (beyond open-ended question answering) + provide answers in a desired format
<ul>
<li>This can be done with <strong>few-shot prompting</strong><strong><br></strong></li>
</ul>
</li>



<li><strong>Additional source knowledge:</strong> They want the base LLM to answer questions about things it may not have been trained on (and consequently is unaware of), and which is not publicly available on the internet (e.g. even to GPT-4).
<ul>
<li>This can be done with  <strong>retrieval-augmented generation (RAG)</strong>.</li>
</ul>
</li>
</ul>



<p><strong>A combination of these two techniques is actually sufficient for most use cases.</strong></p>



<h2>Why People <em>Think</em> Fine-tuning Might Be Helpful</h2>



<details><summary> <strong>What even is fine-tuning?</strong> 🤔</summary>
<div><p><em>Fine-tuning involves taking a pre-trained LLM (e.g. GPT-3.5, LLaMA 2) and further training it on a smaller, domain-specific dataset to make it more specialized for that particular task or data.<br></em><br><em>Out-of-the-box language models have already been pre-trained on open-source data (e.g. <a href="https://commoncrawl.org/">Common Crawl</a>) — the LLM interface you interact with already has “fixed weights” within its transformer/neural network architecture, which do not update based on the queries you feed it (aka, though there can be session history provided as context, it doesn’t “learn” on the fly).<p>When you fine-tune a model, you are essentially “unlocking” these weights and allowing them to update based on whatever new training data you feed it (e.g. a collection of legal cases, or company earnings reports, or a specific user’s tweets). These new weights should allow the model to better handle tasks related to that new domain.</p></em></p><p><em>As of Aug 2023:</em></p></div>



<ul>
<li><em>OpenAI only supports fine-tuning for its GPT-3 models (not the newer GPT-3.5 and GPT-4 models that back ChatGPT) (see <a href="https://platform.openai.com/docs/guides/fine-tuning">how-to guide</a>)</em></li>



<li><em>LLaMA 2 has similar performance to ChatGPT and is open-source, so it is typically the one that people have been using to fine-tune (while maintaining chat capabilities)</em></li>
</ul>
</details>







<p>Base LLMs have a lot of abilities (question answering, summarization, etc) that you likely want to leverage in your app, but you may find them too generic (or unaware of) your particular use case.</p>



<p>You might be drawn to fine-tuning because you believe “more training” can help your LLM application eke out better accuracy on your target task. Intuitively this makes sense — why wouldn’t it be best to adjust the model based on text from your specific domain?</p>



<p>However, the following table explains why it is sufficient, easier, and often preferable to apply other techniques to the existing base model:</p>


<figure>
<table>
<thead>
<tr>
<th>Initial Motivation for Fine-tuning &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</th>
<th>Why a Base LLM is Sufficient</th>
</tr>
</thead>
<tbody>
<tr>
<td><span>It’s cheaper than training a model from scratch, and leverages a base LLM’s existing training.</span></td>
<td>
<p><span>Yes, but comparing to “starting from scratch” fails to consider approaches that don’t require any retraining (and retraining requires access to resources like GPUs).</span></p>
<p><b>Also, the context window length — referring to the LLM Input token limit — has only been longer and longer</b><span>.</span> <span>This length can be allocated not just to your question/task description itself, but also to (1) conversation history (2) examples of output (3) additional info the LLM wasn’t trained on.</span><span><br></span><span><br></span><b>Longer context windows and possible version improvements (e.g. GPT-5)</b> <b>mean that a lot of improvements from fine-tuning will become baseline within a few months — so why invest all those time/computational resources now?</b></p>
</td>
</tr>
<tr>
<td><span>The base model doesn’t have access to private knowledge bases.</span></td>
<td>
<p><span>There is a technique called </span><b>retrieval-augmented generation (RAG)</b><span>, where documents can be stored as embeddings in vector databases, queried for based on semantic meaning, and passed into the base model prompt via the context window.</span></p>
</td>
</tr>
<tr>
<td><span>The base model doesn’t return answers in the desired style or format.</span></td>
<td>
<p><span>Actually, this can be done with a more specific prompt — and the odds of consistently formatted answers can be improved with </span><b>few-shot prompting</b><span>, an approach that provides the base model with examples (think: </span><a href="https://blog.prepscholar.com/sat-analogies-and-comparisons-why-removed-what-replaced-them"><span>SAT analogies</span></a><span>) within the context window.</span></p>
</td>
</tr>
<tr>
<td><span>Fine-tuning means that you don’t need to provide additional context in each prompt – this will save token usage per query.</span></td>
<td>
<p><span>Perhaps in the long run, but token usage is incredibly cheap and this is usually not an issue. (</span> <span>$0.06 / 1K tokens if using 32K context, AKA $1.92 for ~24,000 words)</span><span><br></span><span><br></span><span>Retraining/fine-tuning can cost hundreds of dollars, may still require some prompt engineering, and does not guarantee that your LLM will provide accurate answers.</span></p>
</td>
</tr>
<tr>
<td><span>Fine-tuning will result in more accurate, domain-specific results.</span></td>
<td>
<p><span>Fine-tuning does </span><b>not</b><span> prevent LLM from hallucinating – in fact, it may be more reliable to provide a source to the base model (e.g. from a vector database), and ask clear questions about it (and have the model indicate when the answer is not found).</span><span><br></span><span><br></span><span>Also there have been some </span><a href="https://openreview.net/forum?id=zmXJUKULDzh"><span>experiments</span></a><span> which have shown that LLM’s cross-functional abilities (summarization, classification, generation, etc) can actually degrade because fine-tuning results in overtraining.</span></p>
</td>
</tr>
</tbody>
</table>
</figure>


<p>Just to reiterate,<strong> fine-tuning (except in some rare cases) negates most of the resource-saving benefits from recent LLMs — the reasons that people are flocking to this technology in the first place.</strong> The biggest reason why NLP was hard to do before late 2022 was because you needed to collect data, label data, train models, host infra — and all that requires hiring an ML ops and eng team!</p>



<p>Now with using LLMs out of the box, the startup cost is incredibly low. There are a whole bunch of orgs that never would have done NLP if not for LLMs making the bar so low. <strong>Is it worth investing your eng time into fine-tuning when state-of-the-art is advancing so quickly?</strong> Sure, you’ll have a slight competitive advantage if your model has better accuracy/quality — but will you still think so a few months later when other companies get the same boosted functionality with GPT-5, no effort required?</p>



<p>This is why we recommend that you focus your attention on lighter-touch approaches like few-shot prompting and retrieval augmented generation (RAG).</p>



<h2>Alternative 1 (for Structure): Base LLM + Few-Shot Prompting</h2>



<p>Although most popular LLMs have been trained to respond in a Q&amp;A format, you may want them to perform a specific task (e.g. sentiment analysis, or a <a href="https://www.tidepool.so/2023/07/26/5-top-product-applications-of-llms/">range of applications</a>) or to output answers in a particular format (e.g. JSON).</p>



<p>While you can provide these instructions directly in the prompt, the LLM’s response is probabilistic, not deterministic. There isn’t a guarantee that it will answer in the way that you expect (and perhaps this is why people sometimes think fine-tuning is needed).&nbsp;</p>



<p>However, you can ensure more consistency with the following:</p>



<ul>
<li>Using basic heuristics to vet that its response falls into a domain of known/desired outputs</li>



<li>Providing examples of input/output pairs within the context window (what’s passed into the LLM, in addition to the main query) — it’s almost as if you are providing a small number of training data within the real-time query. This is <strong>few-shot prompting.</strong></li>
</ul>



<p>For example, the following prompt provides examples (taken from the <a href="https://huggingface.co/datasets/yelp_review_full"><code>yelp_review_full</code></a> dataset) so that the LLM (ChatGPT in this case) knows how to classify user reviews:</p>



<p><strong>Context for prompt:</strong></p>



<pre><code>You will be given inputs (reviews for various businesses, e.g. from Yelp) and your job will be to (1) classify the sentiment (one of [“positive”, “negative”, “neutral”, “mixed”]) and (2) try to determine the sort of place the review is for. You will output results in JSON format (with the keys “sentiment” and “business_type”).

Input: “This location never disappoints!! Food is always consistently great, and if you come at the right time, (witching hours) you may see the cook singing and dancing along with the music in the back. And it is awesome! ! Love this place!!”

Output: {“sentiment”: “positive”, “business_type”: “restaurant”}

Input: “I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\n\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an $800.00 bill for the blood work. I can't pay for my bill because I'm a student and don't have any cash flow at this current time. I can't believe the Doctor wouldn't give me a heads up to make sure my insurance would cover work that wasn't necessary and was strictly preventative. The office can't do anything to help me cover the bill. In addition, the office staff said the onus is on me to make sure my insurance covers visits. Frustrating situation!”

Output: {“sentiment”: “negative”, “business_type”: “doctor’s office”}

Input: “Good beer selection. Understaffed for a light Monday night crowd, it wasn't her fault she was the only server. But it took about an hour to get our sandwiches. Mine was one of the best reubens I've ever had.”

Output: {“sentiment”: “mixed”, “business_type”: “restaurant”}

Input: “Place was alright for a one-night stay. Nothing special, room was a bit old, but fine as a pitstop on our long roadtrip.”

Output: {“sentiment”: “neutral”, “business_type”: “hotel”}

Input: &lt;INPUT&gt;

Output:</code></pre>



<p><strong>Example of raw input (inserted into the &lt;INPUT&gt; placeholder in the prompt above):</strong></p>



<pre><code>This place is absolute garbage...  Half of the tees are not available, including all the grass tees.  It is cash only, and they sell the last bucket at 8, despite having lights.  And if you finish even a minute after 8, don't plan on getting a drink.  The vending machines are sold out (of course) and they sell drinks inside, but close the drawers at 8 on the dot.  There are weeds grown all over the place.  I noticed some sort of batting cage, but it looks like those are out of order as well.  Someone should buy this place and turn it into what it should be.</code></pre>



<p><strong>ChatGPT (turbo-3.5) output:</strong></p>



<pre><code>{
“sentiment”: “negative”,
“business_type”: “golf range”
}
</code></pre>



<p>Usually the base model is sufficient to extrapolate what you want from the examples you provide (“few-shot”). However, if you need the model to perform a task using specialized knowledge that it may not have been trained on, few-shot prompting won’t be enough.</p>



<h2>Alternative 2 (for Knowledge): Base LLM + Retrieval Augmented Generation (RAG)</h2>



<p>By itself, an LLM can’t answer questions about content it hasn’t been trained on. However, it has an <a href="https://help.openai.com/en/articles/7127966-what-is-the-difference-between-the-gpt-4-models">extended context window of 32,000 tokens</a> — essentially, 24,000 words of “memory”.&nbsp; This memory can include:</p>



<ul>
<li>Previous conversation history (if any)</li>



<li>Additional info needed that the LLM hasn’t been trained on</li>



<li>Your actual query</li>
</ul>



<p>The supported context window length has been trending bigger and bigger (to the point where you really can provide <strong>pages</strong> of source material in the real-time query).</p>



<p>Say that you want to use an LLM to do smarter search over your own internal documentation. Your total number of docs &gt; 12,000 words by a long shot. However, if you have another means of figuring out which doc (and for longer docs, which “chunk” of a particular doc) likely has what you’re looking for, you can provide that “chunk” as context for your question to the LLM.</p>



<p>Luckily, vector DBs do just that — assuming that you’ve already split your docs into LLM-friendly chunks and stored these in the DB in vector form, you can subsequently:</p>



<ol>
<li>Encode your search query as a vector</li>



<li>Query the DB using to find the top X chunks most related to (1)</li>



<li>Provide each of these to the LLM along with the original search query, to see if it can determine the answer with that extra context</li>
</ol>



<p>The following diagram illustrates how you’d store your documents into a vector DB:</p>



<figure><img decoding="async" src="https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-1024x761.png" alt="" width="614" height="456" srcset="https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-1024x761.png 1024w, https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-300x223.png 300w, https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-768x571.png 768w, https://www.tidepool.so/wp-content/uploads/2023/08/seed_db-650x483.png 650w, https://www.tidepool.so/wp-content/uploads/2023/08/seed_db.png 1482w" sizes="(max-width: 614px) 100vw, 614px"></figure>



<p>This second diagram illustrates how you’d query the vector DB and provide this as context to the LLM model:</p>



<figure><img decoding="async" src="https://www.tidepool.so/wp-content/uploads/2023/08/read_db-1024x761.png" alt="" width="615" height="457" srcset="https://www.tidepool.so/wp-content/uploads/2023/08/read_db-1024x761.png 1024w, https://www.tidepool.so/wp-content/uploads/2023/08/read_db-300x223.png 300w, https://www.tidepool.so/wp-content/uploads/2023/08/read_db-768x571.png 768w, https://www.tidepool.so/wp-content/uploads/2023/08/read_db-650x483.png 650w, https://www.tidepool.so/wp-content/uploads/2023/08/read_db.png 1482w" sizes="(max-width: 615px) 100vw, 615px"></figure>



<p>Below is some pseudocode for the second part (query logic), using the vector DB <a href="https://docs.pinecone.io/docs/overview">Pinecone</a> in this case. <strong>FYI this won’t actually run,</strong> but is meant to give you a sense of what’s needed:</p>



<figure><img decoding="async" loading="lazy" width="1024" height="761" src="https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-1024x761.png" alt="" srcset="https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-1024x761.png 1024w, https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-300x223.png 300w, https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-768x570.png 768w, https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM-650x483.png 650w, https://www.tidepool.so/wp-content/uploads/2023/08/Screen-Shot-2023-08-17-at-7.52.58-AM.png 1516w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<h2>When You Might Actually Fine-tune</h2>



<p>Given how far you can get with the alternatives mentioned above, there aren’t that many truly valid cases of fine-tuning. You might still consider it if:</p>



<ul>
<li>You have <strong>super stringent</strong> accuracy requirements for a certain task that justifies putting in a lot of engineering and ops resources (a heavily-resourced company like Bloomberg can do <a href="https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/">this</a>)</li>



<li>You really really care about <a href="https://twitter.com/karpathy/status/1666182244107689985">fast edge inference</a> (e.g. an LLM model running locally on your phone). In which case you’re probably not going to fine-tune an LLM, you’ll probably want to use a BERT type model because it’s lighter and more task-generalizable (<a href="https://datascience.stackexchange.com/questions/123053/why-does-everyone-use-bert-in-research-instead-of-llama-or-gpt-or-palm-etc">src</a>).</li>



<li>If few-shot and RAG combined do not get you the performance you want (e.g. a more involved style transfer task) and you really want to make it better…even then, as we’ve mentioned, the move here might be “welp let’s wait until the latest LLM version update”</li>
</ul>



<h2>Conclusion</h2>



<p>To close this out with a callback to the original toolbox metaphor, you can consider a base LLM model to be like a Swiss army knife — it is sufficient and adaptable to most use cases.</p>



<p>Given the upfront time and computational resources required for fine-tuning, we’d recommend starting with this base first (along with the supplemental techniques mentioned) and seeing how far you get with it!</p>
                        </div><!-- .entry-content -->

                                                <!-- .entry-footer -->
                        
                    </article><p><a href="https://www.tidepool.so/author/jyao/" title="View Jessica Yao’s posts" rel="author"><img src="https://secure.gravatar.com/avatar/4bba728520d0f0874ff296af346c0859?s=450&amp;d=mm&amp;r=g"></a>            <span>
                <h4> </h4>
                                <p> Software engineer and occasional technical writer. Former early employee at Aquarium! 🐬</p>
            </span>
        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Worldcoin ignored initial order to stop iris scans in Kenya, records show (189 pts)]]></title>
            <link>https://techcrunch.com/2023/08/15/worldcoin-in-kenya/</link>
            <guid>37174758</guid>
            <pubDate>Fri, 18 Aug 2023 12:31:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/08/15/worldcoin-in-kenya/">https://techcrunch.com/2023/08/15/worldcoin-in-kenya/</a>, See on <a href="https://news.ycombinator.com/item?id=37174758">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Months before<a href="https://techcrunch.com/2023/08/02/kenya-suspends-worldcoin-scans-over-security-privacy-and-financial-concerns/?tpcc=tcplustwitter"> Kenya finally banned iris scans by Sam Altman’s crypto startup Worldcoin,</a> the Office of the Data Protection Commissioner (ODPC) had ordered its parent company, Tools for Humanity, to stop collecting personal data.</p>
<p>The ODPC had in May this year instructed the crypto startup to stop iris scans and the collection of facial recognition and other personal data in Kenya, a letter sent to Worldcoin and seen by TechCrunch shows.</p>
<p>Tools for Humanity, the company building Worldcoin, did not stop taking biometric data until early this month when Kenya’s ministry of interior and administration, a more powerful entity, suspended it following its official launch. Worldcoin’s official launch led to a spike in the number of people queuing up to have their eyeballs scanned in exchange for “free money,” drawing the attention of authorities.</p>
<p>The letter shows that ODPC had instructed Worldcoin to cease collecting data for intruding on individuals’ privacy by gathering biometric data without a well-established and compelling justification. Further, it said Worldcoin had failed to obtain valid consent from people before scanning their irises, saying its agents failed to inform its subjects about the data security and privacy measures it took, and how the data collected would be used or processed.</p>
<p>“Your client is hereby instructed to cease the collection of all facial recognition data and iris scans, from your subscribers. This cessation should be implemented without delay and should include all ongoing and future data processing activities,” said Rose Mosero, in a letter to Tools for Humanity that outlined the concerns. The letter, addressed to Ariana Issaias of Coulson Harney (Bowmans), the law firm representing the crypto startup, also restricted Worldcoin from processing the collected data further and instructed the safe storage of collected information.</p>
<p>Details of ODPC’s attempt to stop the collection of biometric data have emerged in a new petition filed before the High Court by the data protection authority.</p>
<p>The deputy data commissioner of compliance, Oscar Otieno, in an affidavit filed in court in August, said that it started the “assessment” of the respondents (Tools for Humanity and Sense Marketing Limited) in 2022. In May this year, it carried out further inquiry on their processing activities and directed that they cease processing sensitive personal data immediately.</p>
<p>“The applicant (ODPC) is aware that despite the suspension and directive to cease processing of personal data, the respondents continued to process the said personal data. It took the public directive by the cabinet ministry of interior and coordination to halt the operations of the respondents (Tools for Humanity and Sense Marketing),” said Otieno in the affidavit.</p>

<p>The ODPC sought the court’s help to have Worldcoin compelled to preserve the data it collected from Kenyans, as it finalizes (the multi-agency) investigations around security, privacy, and the legality of using “financial incentive” to obtain biometric data.</p>
<p>Since the filing of the petition, the High Court has <a href="https://www.businessdailyafrica.com/bd/economy/keep-off-kenyans-eyes-court-orders-worldcoin-as-probe-on--4335544" target="_blank" rel="noopener">barred Worldcoin from collecting data from Kenyans</a> and directed it, its agents, its representatives and its employees to preserve all information collected locally from April 19 to August 8.</p>
<p>This has emerged after Worldcoin activities were suspended in Kenya on August 2, by Kithure Kindiki, the country’s cabinet secretary for interior and national administration. Kindiki said the ban will remain in place until the authorities determine “the absence of any risks to the general public whatsoever.”</p>
<p>“Relevant security, financial service and data protection agencies have commenced inquiries and investigations to establish the authenticity and legality of the aforesaid activities, and the safety and protection of the data being harvested, and how the harvesters intend to use the data,” said Kindiki.</p>
<p>Worldcoin claims to be creating a new “human identity (World ID) and financial network” through iris scans done by “Orb,” the company’s spherical scanners to “verify your World ID,” and its own cryptocurrency “WLD.”</p>
<p>Kenya was one of the first countries where Worldcoin launched sign-ups and had been one of the biggest markets for takeup. After the global official launch at the end of July, locals who had received the tokens could sell them for USDT (the stablecoin pegged to the U.S. dollar) on crypto exchanges or to “brokers” in exchange for cash. In Kenya, that promise of “free money” quickly spread across the country, leading to an influx of people at the recruitment (Orb) stations, which drew the attention of top government officials, leading to the suspension of Worldcoin iris scans. The crypto startups said it <a href="https://techcrunch.com/2023/08/03/worldcoin-plans-to-resume-iris-scans-in-kenya/">hopes to resume activities in Kenya soon</a>.</p>
<p>Alongside the many issues that <a href="https://newsletter.mollywhite.net/p/worldcoin-a-solution-in-search-of" target="_blank" rel="noopener">skeptical peers in the technology industry</a> have been raising about the Worldcoin project and its bigger business ambitions, there are growing concerns about how those efforts to build a biometric database using the promise of free cryptocurrency have exploited economically disadvantaged people. Also, some of these issues have been there in plain sight. An MIT Technology Review <a href="https://www.technologyreview.com/2022/04/06/1048981/worldcoin-cryptocurrency-biometrics-web3/" target="_blank" rel="noopener">investigation</a> — published last year — found that it “used deceptive marketing practices, was collecting more personal data than it acknowledged, and failed to obtain meaningful informed consent.”</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Textual: Rapid Application Development Framework for Python (224 pts)]]></title>
            <link>https://github.com/Textualize/textual</link>
            <guid>37174657</guid>
            <pubDate>Fri, 18 Aug 2023 12:22:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Textualize/textual">https://github.com/Textualize/textual</a>, See on <a href="https://news.ycombinator.com/item?id=37174657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Textualize/textual/main/imgs/textual.png"><img src="https://raw.githubusercontent.com/Textualize/textual/main/imgs/textual.png" alt="Textual splash image"></a></p>
<p dir="auto"><a href="https://discord.gg/Enf6Z3qhVr" rel="nofollow"><img src="https://camo.githubusercontent.com/7974ed333e6b924da47e6798cae37e22776ed404c8d84789d7c2780a641f0691/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f31303236323134303835313733343631303732" alt="Discord" data-canonical-src="https://img.shields.io/discord/1026214085173461072"></a></p>
<h2 tabindex="-1" dir="auto">Textual</h2>
<p dir="auto">Textual is a <em>Rapid Application Development</em> framework for Python.</p>
<p dir="auto">Build sophisticated user interfaces with a simple Python API. Run your apps in the terminal and (coming soon) a web browser!</p>
<details>
  <summary> 🎬 Demonstration </summary>
  <hr>
<p dir="auto">A quick run through of some Textual features.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-22.at.19.00.48.mov">Screen.Recording.2022-10-22.at.19.00.48.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/197355913-65d3c125-493d-4c05-a590-5311f16c40ff.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/197355913-65d3c125-493d-4c05-a590-5311f16c40ff.mov" controls="controls" muted="muted">

  </video>
</details>

 </details>
<h2 tabindex="-1" dir="auto">About</h2>
<p dir="auto">Textual adds interactivity to <a href="https://github.com/Textualize/rich">Rich</a> with an API inspired by modern web development.</p>
<p dir="auto">On modern terminal software (installed by default on most systems), Textual apps can use <strong>16.7 million</strong> colors with mouse support and smooth flicker-free animation. A powerful layout engine and re-usable components makes it possible to build apps that rival the desktop and web experience.</p>
<h2 tabindex="-1" dir="auto">Compatibility</h2>
<p dir="auto">Textual runs on Linux, macOS, and Windows. Textual requires Python 3.7 or above.</p>
<h2 tabindex="-1" dir="auto">Installing</h2>
<p dir="auto">Install Textual via pip:</p>

<p dir="auto">If you plan on developing Textual apps, you should also install the development tools with the following command:</p>

<p dir="auto">See the <a href="https://textual.textualize.io/getting_started/" rel="nofollow">docs</a> if you need help getting started.</p>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto">Run the following command to see a little of what Textual can do:</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Textualize/textual/main/imgs/demo.png"><img src="https://raw.githubusercontent.com/Textualize/textual/main/imgs/demo.png" alt="Textual demo"></a></p>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">Head over to the <a href="http://textual.textualize.io/" rel="nofollow">Textual documentation</a> to start building!</p>
<h2 tabindex="-1" dir="auto">Join us on Discord</h2>
<p dir="auto">Join the Textual developers and community on our <a href="https://discord.gg/Enf6Z3qhVr" rel="nofollow">Discord Server</a>.</p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<p dir="auto">The Textual repository comes with a number of examples you can experiment with or use as a template for your own projects.</p>
<details>
  <summary> 🎬 Code browser </summary>
  <hr>
<p dir="auto">This is the <a href="https://github.com/Textualize/textual/blob/main/examples/code_browser.py">code_browser.py</a> example which clocks in at 61 lines (<em>including</em> docstrings and blank lines).</p>
<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-21.at.12.41.15.mov">Screen.Recording.2022-10-21.at.12.41.15.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/197188237-88d3f7e4-4e5f-40b5-b996-c47b19ee2f49.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/197188237-88d3f7e4-4e5f-40b5-b996-c47b19ee2f49.mov" controls="controls" muted="muted">

  </video>
</details>

 </details>
<details>
  <summary> 📷 Calculator </summary>
  <hr>
<p dir="auto">This is <a href="https://github.com/Textualize/textual/blob/main/examples/calculator.py">calculator.py</a> which demonstrates Textual grid layouts.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Textualize/textual/main/imgs/calculator.png"><img src="https://raw.githubusercontent.com/Textualize/textual/main/imgs/calculator.png" alt="calculator screenshot"></a></p>
</details>
<details>
  <summary> 🎬 Stopwatch </summary>
  <hr>
<p dir="auto">This is the Stopwatch example from the <a href="https://textual.textualize.io/tutorial/" rel="nofollow">tutorial</a>.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-22.at.21.12.22.mov">Screen.Recording.2022-10-22.at.21.12.22.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/197360718-0c834ef5-6285-4d37-85cf-23eed4aa56c5.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/197360718-0c834ef5-6285-4d37-85cf-23eed4aa56c5.mov" controls="controls" muted="muted">

  </video>
</details>

</details>
<h2 tabindex="-1" dir="auto">Reference commands</h2>
<p dir="auto">The <code>textual</code> command has a few sub-commands to preview Textual styles.</p>
<details>
  <summary> 🎬 Easing reference </summary>
  <hr>
<p dir="auto">This is the <em>easing</em> reference which demonstrates the easing parameter on animation, with both movement and opacity. You can run it with the following command:</p>

<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-17.at.11.38.13.mov">Screen.Recording.2022-10-17.at.11.38.13.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/196157100-352852a6-2b09-4dc8-a888-55b53570aff9.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/196157100-352852a6-2b09-4dc8-a888-55b53570aff9.mov" controls="controls" muted="muted">

  </video>
</details>

 </details>
<details>
  <summary> 🎬 Borders reference </summary>
  <hr>
<p dir="auto">This is the borders reference which demonstrates some of the borders styles in Textual. You can run it with the following command:</p>

<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-17.at.11.44.24.mov">Screen.Recording.2022-10-17.at.11.44.24.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/196158235-4b45fb78-053d-4fd5-b285-e09b4f1c67a8.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/196158235-4b45fb78-053d-4fd5-b285-e09b4f1c67a8.mov" controls="controls" muted="muted">

  </video>
</details>

</details>
<details>
  <summary> 🎬 Colors reference </summary>
  <hr>
<p dir="auto">This is a reference for Textual's color design system.</p>

<details open="">
  <summary>
    
    <span aria-label="Video description Screen.Recording.2022-10-22.at.19.07.20.mov">Screen.Recording.2022-10-22.at.19.07.20.mov</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/554369/197357417-2d407aac-8969-44d3-8250-eea45df79d57.mov" data-canonical-src="https://user-images.githubusercontent.com/554369/197357417-2d407aac-8969-44d3-8250-eea45df79d57.mov" controls="controls" muted="muted">

  </video>
</details>

</details>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Short session expiration does not help security (550 pts)]]></title>
            <link>https://www.sjoerdlangkemper.nl/2023/08/16/session-timeout/</link>
            <guid>37173339</guid>
            <pubDate>Fri, 18 Aug 2023 09:45:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sjoerdlangkemper.nl/2023/08/16/session-timeout/">https://www.sjoerdlangkemper.nl/2023/08/16/session-timeout/</a>, See on <a href="https://news.ycombinator.com/item?id=37173339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <article>

  

  <div>
  <p>When logged into a web application, the session does not remain valid forever. Typically, the session expires after a fixed time after login, or after the user has been idle for some time. How long should these times be?</p>

<!-- Photo source: https://commons.wikimedia.org/wiki/File:Time_Expired_(237355699).jpeg -->

<h2 id="introduction">Introduction</h2>

<blockquote>
  <p>Roses are red, <br>
Violets are blue, <br>
Sessions expire, <br>
Just to spite you.</p>
</blockquote>

<p>In some web applications, sessions expire. You are logged out after a while and need to authenticate again. Current security advice is to use quite short session timeouts, such as after 15 minutes of inactivity. However, most mobile apps and big web applications such as Gmail or GitHub don’t adhere to this. You can be logged in seemingly forever without authenticating again. Are these insecure? Do Google and Microsoft know better than NIST and OWASP?</p>

<h2 id="threat-model">Threat model</h2>

<p>The threat model involves an attacker gaining unauthorized access to a user’s active session. This could happen through various means, such as stealing session cookies, exploiting session fixation vulnerabilities, or by using the same device as the victim.</p>

<p>However, would session takeover be prevented by expiring the session after 15 minutes of inactivity?</p>

<h3 id="xss">XSS</h3>

<p>If an attacker steals a session cookie with XSS or session fixation, the attacker immediately gains access to a valid session and can keep performing requests to keep the session alive. An absolute timeout would limit the amount of time the attacker has, but realistically this wouldn’t really hinder any attacker. By definition they steal a valid session token, and can use it immediately. Presumably they are going to immediately make themselves admin, or wire all your bitcoin to their account. Since this is a targeted attack, the attacker knows about session timeouts of the application and can automate their attack to strike before the session times out.</p>

<h3 id="logged-token">Logged token</h3>

<p>If an attacker sees an old session token in the logs, or on your hard drive after they steal your computer, the session timeout probably prevented session takeover. This is an argument for session timeouts, but not necessarily for short session timeouts. Also, it would be better to protect against this by securing the logs or using hard drive encryption.</p>

<h3 id="shared-computers">Shared computers</h3>

<p>Perhaps you used the shared computer in the library to access your web application, and forgot to log out. The next user of that computer could reopen the web application and take over your session.</p>

<p>Is this a thing? Are shared computers without user separation a thing? If so, these shouldn’t be used to access web applications with sensitive information at all, no matter how short the session expiry time is. The device may already be compromised, or the <a href="https://textslashplain.com/2023/05/16/how-do-random-credentials-mysteriously-appear/">browser may remember your password</a>, or sensitive information remains in the browser cache.</p>

<p>Even if internet cafes still exist, some applications are used strictly within an company from company devices. Or people use their own mobile device to access the application. For most web applications, the threat of shared public computers is not realistic.</p>

<h3 id="the-attacker-has-access-to-your-device">The attacker has access to your device</h3>

<p>You forgot to lock your computer when you went to lunch, and the attacker sat down at your desk and gained access to your machine.</p>

<p>In this case, session expiration may prevent them from gaining access to your session, if they weren’t fast enough. However, they now have access to your email, Slack, password vault, SSH agent, browser, and files. They don’t need your active session, they can just create a new one. Either by using the password vault or using the “forgot password” to mail a password reset mail.</p>

<p>One situation in which immediate access to the web application could still be prevented, if when 2FA is enabled and you took your phone or yubikey with you to lunch. But even then, the attacker could install a browser extension that sends your credentials to them the next time you log in.</p>

<h2 id="reauthentication-is-risky">Reauthentication is risky</h2>

<p>Perhaps you prefer short sessions just to be on the safe side. However, short sessions have disadvantages, both in user experience and in security. If someone needs to log in again every 15 minutes, they are going to make authenticating as easy as possible. That means keeping the password vault open, choosing an easier password, or putting the password on the clipboard every time. Reauthentication comes with its own risks. A shorter expiration time does not automatically reduce the overall risk.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Session tokens are pretty secure. The threats described above are easily fixed with other measures, such as disk encryption, locking your computer, or HttpOnly cookies.</p>

<p>Even so, if someone compromises your session, you’re screwed whether it lasts five minutes or for ever. Attacks that are prevented by short session timeouts are really rare.</p>

<p>Finally, short session timeouts come with security and user experience costs.</p>

<p>Facebook, Google, Amazon and GitHub have sessions that never expire. They think it’s an acceptable risk. I think they are right.</p>

<h2 id="read-more">Read more</h2>

<ul>
  <li><a href="https://auth0.com/blog/balance-user-experience-and-security-to-retain-customers/">Balance User Experience and Security to Retain Customers</a></li>
  <li><a href="https://chromium.googlesource.com/chromium/src/+/master/docs/security/faq.md#why-arent-physically_local-attacks-in-chromes-threat-model">Why aren‘t physically-local attacks in Chrome’s threat model?</a></li>
</ul>

  </div>

</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WTF is going on with R7RS Large? (180 pts)]]></title>
            <link>http://dpk.io/r7rswtf</link>
            <guid>37173231</guid>
            <pubDate>Fri, 18 Aug 2023 09:32:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://dpk.io/r7rswtf">http://dpk.io/r7rswtf</a>, See on <a href="https://news.ycombinator.com/item?id=37173231">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>The reaction to John Cowan’s <a href="https://groups.google.com/g/scheme-reports-wg2/c/xGd0_eeKmGI/m/q-xM5fbuAQAJ">resignation as chair of the R<sup>7</sup>RS Large working group</a> has been somewhat wider than I think anyone expected, making it onto general interest programming forums. Scheme may be a small island, but when it has an earthquake, it makes waves through the whole archipelago of programming languages. For the most part, these waves caused only confusion about what all this means.

</p><p>What’s happening? Is Scheme dying? Will R<sup>7</sup>RS carry on? Find out below!

</p><p>Note for the sake of disclosure of interests: I’ve explicitly invited the steering committee to consider me a candidate to replace John Cowan. This is also purely my own view on what has happened, not anyone else’s.

</p><h2>The story so far</h2>

<p>In the beginning, Scheme was created. This made a lot of people very happy and has widely been regarded as an excellent idea.

</p><p>Scheme progressed from a master’s research project through to standardized language through interest in using it in introductory computer science education (think <a href="https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/6515/sicp.zip/index.html"><cite>Structure and Interpretation of Computer Programs</cite></a>), programming language research, and as a minimal dialect of Lisp for environments with limited resources. An informal group evolved the language through revisions known as R<sup><var>n</var></sup>RS (a mathematical pun with a reference to Algol hidden in it), up to R<sup>5</sup>RS in 1998, which was the first programming language to standardize a hygienic macro system.<a href="#fn-clmacro" rel="footnote" id="fnref-clmacro">*</a>&nbsp;<a href="#fn-rust" rel="footnote" id="fnref-rust">†</a>

</p><p>To further evolve the language, a process called <a href="https://srfi.schemers.org/">SRFI</a> was set up, which basically provided the equivalent of Python PEPs or whathaveyou. Some SRFIs, like <a href="https://srfi.schemers.org/srfi-1/srfi-1.html">SRFI&nbsp;1</a> (a comprehensive library of list utility procedures) were wildly popular and many implementations took them up; others found no adoption at all. So it goes.

</p><p>To move beyond R<sup>5</sup>RS, a Steering Committee was formed in 2004 which appointed editors to specify an R<sup>6</sup>RS. When they were done in 2006, the reaction from the Scheme community was&nbsp;… mixed. Compared to any previous new R<sup><var>n</var></sup>RS, the R<sup>6</sup>RS represented the biggest development from its the preceding R<sup><var>n</var>−1</sup>RS yet. Some people loved it and still love it. For others, it seemed to be the worst thing to ever happen to Scheme. Generally, their criticisms boiled down to: it’s too big for certain use cases; there are too many things implementations are required or forbidden to do, limiting implementations’ ability to create their own extensions to the language; some changes from existing Scheme practice seem gratuitous (for example, instead of adopting SRFI&nbsp;1, or even a subset of it, R<sup>6</sup>RS created its own library with much of the same functionality under incompatible names); it messes up the ‘spirit of Scheme’ (as understood by those who criticized the report). Some implementations added support for it; others refused to touch it with a barge pole.<a href="#fn-sixrejection" rel="footnote" id="fnref-sixrejection">‡</a> Even some implementations which supported it wilfully ignored its strictest requirements.

</p><p>A new Steering Committee was elected by those interested in Scheme in 2009, which recognized that Scheme was trying to serve two purposes which couldn’t be accommodated in one report. Those who liked R<sup>5</sup>RS liked it because it was small and flexible; those who liked R<sup>6</sup>RS wanted something bigger and more practical for writing real, production programs on modern computers. Thus, they planned to revise the language again and split it into two parts: small and large. New working groups were appointed, and the first one delivered <a href="https://github.com/johnwcowan/r7rs-spec/blob/errata/spec/r7rs.pdf">the report on the R<sup>7</sup>RS small language</a><sup>(PDF)</sup> in 2013. This was mostly well-received by those who appreciated the simpler nature of R<sup>5</sup>RS, and pretty much every actively maintained implementation which had rejected R<sup>6</sup>RS adopted R<sup>7</sup>RS&nbsp;small.

</p><h2>The current issues</h2>

<p>Getting the large language done has been a much slower process, in part because of its increased scope. In contrast to R<sup>6</sup>RS (which used the SRFI process to gather feedback on some, but not all of its new features), R<sup>7</sup>RS Large was going to be almost entirely SRFI-based. This meant adopting popular existing SRFIs, but also writing a whole lot of new ones. Anyone in the Scheme community was invited to vote on which ones made it into the language. To attempt to speed things up, in 2022 an <a href="https://codeberg.org/scheme/r7rs/issues">issue tracker</a> was established (until then, to-do items had been in a document maintained by the chair, John Cowan). At the same time, the community identified that there were two groups with slightly different concerns about what R<sup>7</sup>RS Large would look like: some were concerned that the small language itself didn’t impose strict enough semantics for a safe high-level programming language,<a href="#fn-bounds" rel="footnote" id="fnref-bounds">§</a> and wanted other cool new features like delimited continuations; others wanted a language with a large standard library like Python’s. For the most part, those in the former group were concerned that most progress so far had been on the standard libraries, and there was an emphasis on compatibility with the small language which seemed to undermine the idea of new core language features. So the report was split into ‘Foundations’, with stronger error checking than the small language and other new features including an advanced macro system, and ‘Batteries’, which would give us the big standard library others wanted. In theory two groups would be formed, each working on their part.

</p><p>In practice, the working group for R<sup>7</sup>RS&nbsp;Large has consisted of two groups split along very different lines:

</p><ul>
  <li>a small self-selected group (which, I hasten to add, still always welcomes new participants) of people actively contributing to discussion of individual features and proposing new SRFIs and such; plus
  </li><li>a larger community of interested Schemers who come out to vote when we ask them which of these proposals they actually want to see in the new language (likewise, anyone can join this community by sending a statement explaining their interest in Scheme to the mailing list).
</li></ul>

<p>Since these changes in 2022, the small group actively working on the language has overwhelmingly focussed on the Foundations. Those who expressed that their main interest was in the Batteries have got themselves involved in Foundations work, and (to the extent Batteries work has continued) vice versa.

</p><p>Debates on the Foundations have got bogged down a lot in questions of R<sup>6</sup>RS compatibility, even though the Foundations report at the outset was going to ensure, at least, that nothing would prevent an R<sup>6</sup>RS implementation from <em>also</em> supporting R<sup>7</sup>RS. The details of this have proven the most controversial points: should we base all our new Foundations features on R<sup>6</sup>RS? Should R<sup>6</sup>RS in its entirety become part of the Foundations, even though some of R<sup>6</sup>RS is clearly Batteries stuff, like that list library I mentioned earlier? Should R<sup>7</sup>RS implementations be <em>required</em> to support R<sup>6</sup>RS as well? Some of these arguments have undermined the settlement reached in 2022, because Batteries people don’t want the Foundations people to work on (what they perceive to be) the assumption that the Batteries will never be done. On the other side, it’s argued that simply incorporating everything from R<sup>6</sup>RS&nbsp;— possibly in revised form&nbsp;— would settle a lot of questions about the Foundations without having to debate them.

</p><p>There are also procedural disagreements: Marc Nieper-Wißkirchen, who was in theory responsible for the Foundations, is not a fan of putting things to a vote of the larger community, because he feels a report created by democratic vote won’t necessarily be a coherent whole in the end. He wants to work by unanimous consensus, i.e. ‘as long as nobody objects to something, it goes in’. Unanimous consensus of the editors was actually how reports up to and including R<sup>5</sup>RS were written (explaining the smaller size of those reports), but this stance does rule out one option for resolving these fundamental disagreements where unanimity is not possible.

</p><h2>What happens now?</h2>

<p>According to the charter for the working group for the large language, the Steering Committee now has to make a decision: either they appoint a new chair who they think can fix things; or they dissolve the working group and give up on the R<sup>7</sup>RS&nbsp;Large language.

</p><p>The number of people who could credibly become the new chair is limited; the number of people who want it even fewer. When John Cowan informed me he intended to resign (the weekend before he announced it publically), I immediately suggested a successor, whom John thought was a good choice to recommend to the Steering Committee. But, when asked, that person just as immediately turned it down. Thus John was not able to recommend any course of action in his resignation letter. The credible choices of a new chair are, in my view: someone from within that smaller active working group (one of whom, as mentioned, already said they don’t want to be chair); or someone from the Steering Committee itself. There are a few possible candidates in the wider Scheme community, but they seem even more unlikely to want to take on the task: it would be cool to have Guy Steele as chair, but I think he’s still busy with Java or something. So far I appear to be the only person mad enough to actually want to try to take on the responsibility of fixing things (though it’s possible someone else asked the Steering Committee privately to consider them a candidate, too).

</p><p>Equally, though, dissolving the working group seems premature. Everyone who is working on the language is still excited about it and wants to see it completed: work on proposals and specs has continued even while we are without a chair to present them to the voting community. It seems wrong to cancel the project in these circumstances, even if any future chair’s efforts to resolve the conflicts might be seen as ‘last ditch attempts’ before actually, finally giving up.

</p><p>Somewhat concerningly, there’s also a third possibility. All of the members of the Steering Committee have become somewhat less active in the Scheme community in recent years. Though we’ve seen occasional input from individual members, the Steering Committee as a body hasn’t taken any action since 2013 when the small language report was ratified. When the changes made in 2022 were being discussed, we got no guidance from the Committee at all. In short, we’re not sure the Steering Committee exists as a single, functioning entity any more.

</p><p>So it’s possible that the Steering Committee does <em>nothing,</em> which would amount to <i>de facto</i> ending the process, but in the least satisfying possible way. The working group would still technically exist, but without a chair. This might be the end of Scheme standardization altogether, though I hope we could at least find some source of authority to give <em>some</em> kind of future specification its seal of approval.<a href="" rel="footnote" id="fnref-vatican">‖</a>

</p><p>What I wrote in my <a href="https://groups.google.com/g/scheme-reports-wg2/c/TUbmlPY9lR0">statement requesting the Steering Committee consider me a candidate for new chair</a> is true: Scheme has an outsized importance in the world of programming languages compared to its actual use in industry, because it has so often pioneered features that later broke into the mainstream. Proper tail calls as the fundamental means of iteration is now standard in functional programming. Transformation to continuation-passing style as the basis of compilation likewise. Hygienic macros&nbsp;— now part of Rust and <a href="https://peps.python.org/pep-0638/">even being considered in Python</a>&nbsp;— were pioneered by researchers who used Scheme as their test bed. That research became part of the Scheme standard in R<sup>5</sup>RS and R<sup>6</sup>RS. In functional programming, a lot of research is being done at the moment in algebraic effects, which might revolutionize how side-effects like I/O are done in languages like Haskell&nbsp;— and all this work is based on research into delimited control operators, research which was done by Schemers. That’s research we want to put into the standard in R<sup>7</sup>RS&nbsp;Large. Above all, Scheme continues to be the first language of reference for all those who believe, in the words of the introduction to the reports, that ‘programming languages should be designed not by piling feature on top of feature, but by removing the restrictions and limitations that make additional features seem necessary’.

</p><p>It would be a damned shame if this were the end of the road for standard Scheme, but it doesn’t have to be so. I have my plan to save it, but let’s wait and see what the Steering Committee has to say.

  </p><address><a href="http://dpk.io/">Daphne Preston-Kendal</a>, 18 August 2023</address>
  
<hr>

<section id="footnotes">
  <p id="fn-clmacro">*&nbsp;Unfortunately, the standard hygienic macro system in R<sup>5</sup>RS was quite underpowered because it represented a minimum consensus about how macros should work. So the entire concept of hygienic macros in general was pooh-poohed by a lot of Common Lispers mostly due to a misconception that that was <em>all</em> that hygienic macros could do, creating a rift in the Lisp community&nbsp;– even though all Scheme implementations in practice provide a lower-level, more powerful macro system in which you can do everything you can do with unhygienic macros <em>and more</em>. <a href="#fnref-clmacro">↑</a>
  </p><p id="fn-rust">†&nbsp;Nearly twenty years later, hygienic macros&nbsp;– and program-structural macros in general&nbsp;– first made their way into a mainstream programming language when Rust took off. Hooray! <a href="#fnref-rust">↑</a>
  </p><p id="fn-sixrejection">‡&nbsp;A synopsis of all the specific reasons people voted against acception the R<sup>6</sup>RS report, together with responses from John Cowan as contributor to R<sup>7</sup>RS, can be found on the wiki page <a href="https://small.r7rs.org/wiki/SixRejection/">SixRejection</a>. Note that the responses are not any official statement of any working group, and in some cases have become obsolete because the decisions they reference have been superseded. <a href="#fnref-sixrejection">↑</a>
  </p><p id="fn-bounds">§&nbsp;Indeed, the small language doesn’t even enforce bounds checking, even though almost all implementations do it. Such are the sacrifices for a language which might be expected to work on a tiny microcontroller. <a href="#fnref-bounds">↑</a>
  </p><p id="fn-vatican">‖&nbsp;The situation reminds me somewhat of the <a href="https://en.wikipedia.org/wiki/First_Vatican_Council">First Council of the Vatican,</a> which was suspended in 1870 after the unifiers of Italy invaded Rome, but was still technically in session until it was formally closed in 1960 in preparation for a new, Second Council of the Vatican (the one your traditionalist Catholic aunt complains about all the time). Who will formally dissolve the R<sup>7</sup>RS working group eighty years from now to produce R<sup>8</sup>RS? <a href="#fnref-vatican">↑</a>
</p></section>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Career advice for young system programmers (172 pts)]]></title>
            <link>https://glaubercosta-11125.medium.com/career-advice-for-young-system-programmers-c7443f2d3edf</link>
            <guid>37172815</guid>
            <pubDate>Fri, 18 Aug 2023 08:33:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://glaubercosta-11125.medium.com/career-advice-for-young-system-programmers-c7443f2d3edf">https://glaubercosta-11125.medium.com/career-advice-for-young-system-programmers-c7443f2d3edf</a>, See on <a href="https://news.ycombinator.com/item?id=37172815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure></figure><div><h2 id="27e6">For some of us, the backend vs frontend discussion just doesn’t cut it. What do do, then?</h2><div><a rel="noopener follow" href="https://glaubercosta-11125.medium.com/?source=post_page-----c7443f2d3edf--------------------------------"><div aria-hidden="false"><p><img alt="Glauber Costa" src="https://miro.medium.com/v2/resize:fill:88:88/0*_vxmqP0UCaArfAxZ" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><p id="5cbb">Much of the advice I see online targeted at “developers” is really bad. And the funny thing is: it is bad advice, even if it is completely true.</p><p id="f60d">That’s because in developers’ minds, it is very easy to say that a “developer” is someone who does the same things I do. But in practice, there are many kinds of developers, focusing on drastically different problems.</p><p id="08ec">Much of the advice online takes the form of “who needs this anyway?”. As a quick detour, that is one of the reasons I appreciate <a href="https://blog.turso.tech/were-proud-partners-with-theprimeagen-155d549f4b14" rel="noopener ugc nofollow" target="_blank">ThePrimeagen</a> so much: he is the rare kind of influencer that is always happy to take a step back, and invite his audience to ask “ok, let’s understand the problem and the domain first”, as opposed to “whoever is doing this or that is doing it wrong”.</p><h2 id="9b4d">It is hard online for system programmers</h2><p id="f5bd">In common discourse, the main groups of developers are “frontend” and “backend” developers. But even those are just a part of the whole. A part I was never interested in since my early days in the late 1990s.</p><p id="8699">There are other developers. The ones writing databases, operating systems, compilers, and foundational building blocks that, if they do their jobs right, end up becoming almost invisible to the average programmer.</p><h2 id="89ce">Advice for systems programmers</h2><p id="6e51">Lots of the advice that you see on Twitter (X?), is especially bad when it comes to systems level software. There’s a certain level of <em>“move fast and break things” </em>that you would never accept from your OS Kernel. The whole discussion of whether or not you should write unit tests sound frankly quite stupid from the point of view of a compiler author or some core Open Source library. I could go on.</p><p id="3ed1">My goal in this article is to give some pointers to young engineers that want to build a career doing systems programming, based on my personal experiences. People in different circumstances may have different experiences and you should listen to them too.</p><h2 id="b679">Do Open Source</h2><p id="d633">Barriers to entry in systems programming are usually higher. Writing a Kernel, a Compiler, or a Database takes years. You can write toy versions of those, but that’s still a large surface area that will take at least months to complete, and you’re now in an environment that is so unrealistic, that you’re not really learning much.</p><p id="6c9a">It is possible to get a job somewhere and learn, but that’s much harder. We live in a society that requires 5 years of experience for tools that only exist for 3 years for entry level jobs. Finding a job that will give you access to world class systems programming challenges, albeit possible, is challenging.</p><p id="4848">Thankfully, lots of cornerstone systems software have their code wide open. If you are interested in compilers, you don’t have to struggle imagining how very basic things could work in your toy compiler. You can just go and play with LLVM.</p><p id="a786">Pick a project that you like, that is industry-relevant, and is well known for its high standards. For me, in the early 2000s, that was the Linux Kernel. Today, I am the founder of Turso and libSQL, that you can contribute to.</p><p id="9e9f">But there are many other projects I can recommend, led by people that I know personally and can vouch for, like <a href="https://github.com/scylladb/scylladb" rel="noopener ugc nofollow" target="_blank">ScyllaDB</a> and its companion <a href="https://github.com/scylladb/seastar" rel="noopener ugc nofollow" target="_blank">Seastar Framework</a> (C++), <a href="https://github.com/redpanda-data/redpanda" rel="noopener ugc nofollow" target="_blank">RedPanda</a> (C++), <a href="https://github.com/tigerbeetle/tigerbeetle" rel="noopener ugc nofollow" target="_blank">TigerBeetle</a> (Zig). The list is very data oriented because that’s what I know best, but there are a variety of projects out there that will fit your interests.</p><h2 id="bc57">Do meaningful Open Source work</h2><p id="0a24">For this to work, “doing Open Source” needs to be more than a stamp. It’s easy to get your name in a project contributor’s list with trivial commits. But if your goal is to learn, at some point you have to break past this.</p><p id="22da">Getting started with a big project can be daunting, and you may feel there’s no good place to start. The good news is that when I say “do Open Source”, that doesn’t mean you need to <em>show </em>your Open Source work to anybody.</p><p id="b644">Remember I said toy projects didn’t help much? That’s because the time-to-value is huge. But a toy project <em>inside </em>an established project? That helps!</p><p id="5886">In my early days I focused more on <em>modifying </em>Linux than <em>contributing</em> to Linux. As an example, I wrote a really basic, and frankly useless, filesystem. You could write to files, but they always returned a static string.</p><p id="878f">I wanted to understand filesystems. With my toy fs, I could get started on a real kernel. I learned about the Linux VFS layer, modules, core interfaces and Linux Kernel concepts, and more importantly: the variety of unpredictable ways in which a program that is in complete control of the hardware can just <em>break</em>, and developed the experience on how to debug and fix those issues.</p><p id="3071">A couple of months later, I made my first (very small) contribution to the ext4 filesystem.</p><h2 id="1806">Does Open Source really help you get hired?</h2><p id="a6b2">The brutally honest reality is that I receive so many applications for people interested in working at our company, that your usual bullet list of technologies mean very little. Yes, interviews should help sifting candidates, but even doing an interview uses up time and resources.</p><p id="c69a">Open Source does help. But these days, getting your name listed as a contributor in an Open Source project is very easy. So your contributions have to be to be meaningful.</p><h2 id="dbe8">Hiring V</h2><p id="6e50">We recently hired someone from our Open Source community. I figured it would be helpful to share a bit of that story from our perspective, in the hopes it can help you too!</p><p id="1e8e">When we released our private beta in February 2023. <a href="https://twitter.com/iavins" rel="noopener ugc nofollow" target="_blank">Avinash</a>, from Bangalore, India (he goes by V), joined it right away. This immediately set him apart: I receive hundreds of standard job applications over a month, and I don’t have time to go through all of them. But I always have time to talk to my users.</p><figure></figure><p id="8d40">Our Discord community has lots of people who join, ask a question, and then ask for a job. I am never interested in talking to these people. But that wasn’t v’s case. He was genuinely interested in what we were doing. He kept engaging with us consistently, and giving feedback on what he’d like to see next.</p><figure></figure><p id="c12b">Not only was he giving us good feedback — that’s easy to do — he started contributing to our tooling. In fact, he quickly became the third top contributor to our Go client. Our Go client is not the most active project in the world, but his contributions were beyond trivial.</p><figure></figure><p id="def6">Then came the tipping point. We have an experimental open source initiative to replace the SQLite default storage engine with an <a href="https://github.com/penberg/tihku" rel="noopener ugc nofollow" target="_blank">MVCC implementation</a>. That’s hard stuff. V started contributing to that too. He <a href="https://github.com/penberg/tihku/issues/15" rel="noopener ugc nofollow" target="_blank">filed and fixed</a> a bug that caused the storage engine to corrupt data.</p><p id="7264">Here’s where it gets interesting: our implementation was correct, as described by the original research paper we based the implementation on. He found a bug <em>in the paper</em>, contacted the authors, who realized the mistake and updated the paper. Then he fixed the implementation.</p><figure></figure><p id="30ad">No other image on the internet could do justice to what I thought at the moment, except for the one and only, Tiffany Gomas.</p><figure></figure><p id="ef74">Companies have a hard time finding great talent. And being a good match for a company goes beyond your coding ability. Are you aligned with the vision? Do you understand what we’re trying to do? Because of that, hiring someone is always a risk, even if a person excels in a technical interview (which as I said earlier, are costly to run on their own).</p><p id="6aa5">It later became known to us that V was looking to switch jobs, and we had just opened a position for our Go backend.</p><p id="3256">The only disagreement we had internally was whether or not we should even interview this guy. My position as the CEO was: why would we even waste time interviewing him? What do we hope to learn? Just send the offer!</p><figure></figure><p id="86d1">I guess people are set in their ways. It <em>feels</em> strange to hire someone without an interview. And because companies sometimes like to give people take home assignments, Sarna, one of our engineers, brilliantly crafted a humorous take on why it was indeed, pointless to even interview V:</p><figure></figure><p id="889e">And so, we hired V.</p><h2 id="c37a">Summary</h2><p id="8d5a">If you are interested in the plumbing of our software industry — systems level programming, much of the career advice online won’t apply to you. I hope in this article I could give you some useful advice on how to stand out from the crowd.</p><p id="43db">I showed you a real example of someone who did stand out, and got hired by us. If you want to chat with V, you can join our <a href="https://discord.gg/GHNN9CNAZe" rel="noopener ugc nofollow" target="_blank">Discord channel</a> today!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pg_later: Asynchronous Queries for Postgres (171 pts)]]></title>
            <link>https://tembo.io/blog/introducing-pg-later/</link>
            <guid>37172689</guid>
            <pubDate>Fri, 18 Aug 2023 08:15:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tembo.io/blog/introducing-pg-later/">https://tembo.io/blog/introducing-pg-later/</a>, See on <a href="https://news.ycombinator.com/item?id=37172689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p>We’re working on asynchronous query execution in Postgres and have packaged the work up in an extension we’re calling <a href="https://github.com/tembo-io/pg_later" target="_blank" rel="noopener noreferrer">pg_later</a>. If you’ve used <a href="https://docs.snowflake.com/developer-guide/python-connector/python-connector-example#examples-of-asynchronous-queries" target="_blank" rel="noopener noreferrer">Snowflake’s asynchronous queries</a>, then you might already be familiar with this type of feature. Submit your queries to Postgres now, and come back later and get the query’s results.</p><p>Visit <a href="https://github.com/tembo-io/pg_later" target="_blank" rel="noopener noreferrer">pg_later</a>’s Github repository and give it a star!</p><p><img loading="lazy" alt="elephant-tasker" src="https://tembo.io/assets/images/elephant-3d5154dac74be08748e278a9ad3c7a0e.png" title="elephant-tasker" width="882" height="878"></p><h2 id="why-async-queries">Why async queries?<a href="#why-async-queries" aria-label="Direct link to Why async queries?" title="Direct link to Why async queries?">​</a></h2><p>Imagine that you’ve initiated a long-running maintenance job. You step away while it is executing, only to come back and discover it was interrupted hours ago due to your laptop shutting down. You don’t want this to happen again, so you spend some time googling or asking your favorite LLM how to run the command in the background with screen or tmux. Having asynchronous query support from the beginning would have saved you a bunch of time and headache!</p><p>Asynchronous processing is a useful development pattern in software engineering. It has advantages such as improved resource utilization, and unblocking of the main execution thread.</p><p>Some examples where async querying can be useful are:</p><ul><li>For a DBA running ad-hoc maintenance.</li><li>Developing in interactive environments such as a Jupyter notebook. Rather than submit a long-running query only to have your notebook hang idly and then crash, you can use asynchronous tasks to avoid blocking your Notebook, or simply come back later to check on your task.</li><li>Having a long-running analytical query, for example fulfilling an ad-hoc request like seeing how many new users signed up each day for the past month. You can submit that query and have it run in the background while you continue other work.</li></ul><h2 id="extending-postgres-with-async-features">Extending Postgres with async features<a href="#extending-postgres-with-async-features" aria-label="Direct link to Extending Postgres with async features" title="Direct link to Extending Postgres with async features">​</a></h2><p>At Tembo, we’ve built a similar feature for Postgres and published it as an extension called <strong>pg_later</strong>. With <strong>pg_later</strong>, you can dispatch a query to your Postgres database and, rather than waiting for the results, your program can return and retrieve the results at your convenience.</p><p>A common example is manually executing VACUUM on a table. Typically one might execute VACUUM in one session, and then use another session to check the status of the VACUUM job via <code>pg_stat_progress_vacuum</code>. pg_later gives you the power to do that in a single session. You can use it to queue up any long-running analytical or administrative task on your Postgres database.</p><h2 id="stacking-postgres-extensions">Stacking Postgres Extensions<a href="#stacking-postgres-extensions" aria-label="Direct link to Stacking Postgres Extensions" title="Direct link to Stacking Postgres Extensions">​</a></h2><p><strong>pg_later</strong> is built on top of <a href="https://tembo.io/blog/introducing-pgmq" target="_blank" rel="noopener noreferrer">PGMQ</a>, another one of Tembo's open source extensions. Once a user submits a query, <strong>pg_later</strong> seamlessly enqueues the request in a Postgres-managed message queue. This mechanism then processes the query asynchronously, ensuring no unnecessary wait times or hold-ups.</p><p>The <strong>pg_later</strong> <a href="https://www.postgresql.org/docs/current/bgworker.html" target="_blank" rel="noopener noreferrer">background worker</a> picks up the query from the queue and executes it. The results are persisted by being written to a table as <a href="https://www.postgresql.org/docs/current/functions-json.html" target="_blank" rel="noopener noreferrer">JSONB</a> and can be easily retrieved using the <strong>pg_later</strong> API. You can simply reference the unique job id given upon query submission, and retrieve the result set, or query the table directly. By default, the results are retained forever, however we are building retention policies as a feature into <strong>pg_later</strong>.</p><p><img loading="lazy" alt="diagram" src="https://tembo.io/assets/images/diagram-d6cbc048f614b1157a4ebdcdf011e537.png" title="diagram" width="2146" height="800"></p><h2 id="using-pg_later">Using pg_later<a href="#using-pg_later" aria-label="Direct link to Using pg_later" title="Direct link to Using pg_later">​</a></h2><p>To get started, check out our project’s <a href="https://github.com/tembo-io/pg_later/blob/main/README.md" target="_blank" rel="noopener noreferrer">README</a> for a guide on installing the extension.</p><p>First, you need to initialize the extension. This handles the management of PGMQ objects like a job queue and some metadata tables.</p><p>You're now set to dispatch your queries. Submit the query using pglater.exec, and be sure to take note of the <strong>job_id</strong> that is returned. In this case, it’s the first job so the <strong>job_id</strong> <strong>is 1</strong>.</p><div><pre tabindex="0"><code><span><span>select</span><span> pglater</span><span>.</span><span>exec</span><span>(</span><span></span><br></span><span><span>  </span><span>'select * from pg_available_extensions limit 2'</span><span></span><br></span><span><span></span><span>)</span><span> </span><span>as</span><span> job_id</span><span>;</span><br></span></code></pre></div><p>And whenever you're ready, your results are a query away:</p><div><pre tabindex="0"><code><span><span>select</span><span> pglater</span><span>.</span><span>fetch_results</span><span>(</span><span>1</span><span>)</span><span>;</span><br></span></code></pre></div><div><pre tabindex="0"><code><span><span>{</span><br></span><span><span>  "query": "select * from pg_available_extensions limit 2",</span><br></span><span><span>  "job_id": 1,</span><br></span><span><span>  "result": [</span><br></span><span><span>    {</span><br></span><span><span>      "name": "pg_later",</span><br></span><span><span>      "comment": "pg_later:  Run queries now and get results later",</span><br></span><span><span>      "default_version": "0.0.6",</span><br></span><span><span>      "installed_version": "0.0.6"</span><br></span><span><span>    },</span><br></span><span><span>    {</span><br></span><span><span>      "name": "pgmq",</span><br></span><span><span>      "comment": "Distributed message queues",</span><br></span><span><span>      "default_version": "0.10.1",</span><br></span><span><span>      "installed_version": "0.10.1"</span><br></span><span><span>    }</span><br></span><span><span>  ],</span><br></span><span><span>  "status": "success"</span><br></span><span><span>}</span><br></span></code></pre></div><h2 id="up-next">Up next<a href="#up-next" aria-label="Direct link to Up next" title="Direct link to Up next">​</a></h2><p><code>pg_later</code> is a new project and still under development. A few features that we are excited to build:</p><ul><li>Status and progress of in-flight queries</li><li>Security and permission models for submitted queries</li><li>Cursor support for finished jobs (fetch results row by row)</li><li>Kill a query that is in the queue or is currently in flight</li><li>Support for transactions</li><li>Configurable concurrency levels for background works to increase the throughput of jobs</li><li>Push notifications for completed and failed jobs</li><li>Retention policies for completed jobs</li></ul><p>Give us a <a href="https://github.com/tembo-io/pg_later" target="_blank" rel="noopener noreferrer">star</a> and try out pg_later by running the example in the README. If you run into issues, please create an <a href="https://github.com/tembo-io/pg_later/issues" target="_blank" rel="noopener noreferrer">issue</a>. We would greatly welcome contributions to the project as well.</p><p>Please stay tuned for a follow up post on benchmarking PGMQ vs leading open source message queues.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[London Then and Now: Aerial Shots Show City Grow over Past Two Decades (210 pts)]]></title>
            <link>https://londonist.com/london/art-and-photography/aerial-london-then-and-now</link>
            <guid>37172598</guid>
            <pubDate>Fri, 18 Aug 2023 08:01:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://londonist.com/london/art-and-photography/aerial-london-then-and-now">https://londonist.com/london/art-and-photography/aerial-london-then-and-now</a>, See on <a href="https://news.ycombinator.com/item?id=37172598">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
	
	
		

		<div id="main-container">
	<article data-id="56742" itemscope="" itemtype="http://schema.org/WebPage">
		
		
		
		
		



		
		
		
		<div id="body-block" itemscope="" itemtype="http://schema.org/NewsArticle">
			<p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
				<span itemprop="name">Will Noble</span>
			</span>
			<span itemprop="headline name">London Then And Now: Aerial Shots Show City Grow Over Past Two Decades</span></p><meta itemprop="datePublished" content="2023-08-15T12:00:00+01:00">
			
				<div itemprop="articleBody"><p>London's filling out at such a pace, it's only when you stop for a second and look back on recent history that you appreciate just how much it's transformed. Aerial photographer extraordinaire Jason Hawkes has released a set of photos he took from AS355 helicopters — showing London's prodigious transformation between the 2000s and 2023. All we can say is... my, how you've grown!</p>
<figure id="aa4a133c75c622c4cd8d" data-id="227772"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-8229.jpg" alt="The Olympic stadium only half constructed" itemprop="image">
<figcaption>Queen Elizabeth Olympic Park, 2010</figcaption>
</figure>
<figure id="2461ed0966426eca54cd" data-id="227775"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-8904390_1.jpg" alt="The Olympic Park from above looking verdant" itemprop="image">
<figcaption>Queen Elizabeth Olympic Park, 2020</figcaption>
</figure>
<figure id="6de40bd0ee066ea2aa1b" data-id="227776"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-2007-2.jpg" alt="Battersea Power Station looking dishevelled and roofless" itemprop="image">
<figcaption>Battersea Power Station, 2007</figcaption>
</figure>
<figure id="357df130166a0dca8f42" data-id="227777"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9001526.jpg" alt="The power station totally refreshed with white chimneys and green roof gardens" itemprop="image">
<figcaption>Battersea Power Station, 2023</figcaption>
</figure>
<figure id="9aa15ad54b8523a7d582" data-id="227778"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-10.jpg" alt="A panorama of the Thames with St Paul's and the Tate Modern visible" itemprop="image">
<figcaption>River Thames at Dusk, 2009</figcaption>
</figure>
<figure id="ac0a0eda97c79509a935" data-id="227779"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9102769.jpg" alt="The same shot as above, but with a lot more skyscrapers in the City" itemprop="image">
<figcaption>River Thames at Dusk, 2023</figcaption>
</figure>
<figure id="5a8baaae506ffa99a700" data-id="227781"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-1942.jpg" alt="Aerial shot of the City, with the Gherkin dominating" itemprop="image">
<figcaption>The City, 2006</figcaption>
</figure>
<figure id="f6b2c5d7b5c6df6869f6" data-id="227780"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9003706.jpg" alt="Same image as above, but now with a huge cluster of skyscrapers" itemprop="image">
<figcaption>The City, 2023</figcaption>
</figure>
<figure id="a927653e1a4f31ed3a34" data-id="227782"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-2007.jpg" alt="Aerial view dominated by the verdant Oval ground" itemprop="image">
<figcaption>The Oval, 2007</figcaption>
</figure>
<figure id="b61862c25926debfb2a7" data-id="227783"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-741487.jpg" alt="The same shot as above, but now much more built up" itemprop="image">
<figcaption>The Oval, 2023</figcaption>
</figure>
<figure id="fdb44a8fadfcffcc2cdb" data-id="227784"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-4888.jpg" alt="Wembley Stadium being built with lots of construction going on around it" itemprop="image">
<figcaption>Wembley Stadium, 2013</figcaption>
</figure>
<figure id="4f96fec9dfd7a476a491" data-id="227785"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-817970.jpg" alt="Wembley Stadium surrounded by high rises" itemprop="image">
<figcaption>Wembley Stadium, 2023</figcaption>
</figure>
<figure id="bcbf036f170e0c73d025" data-id="227786"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-2949.jpg" alt="The Canary Wharf buildings with a lot of construction work going on around them" itemprop="image">
<figcaption>Isle of Dogs, 2016</figcaption>
</figure>
<figure id="c440c366152e5b06d1eb" data-id="227787"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9204214.jpg" alt="The same shot as above, but with dozens more skyscrapers" itemprop="image">
<figcaption>Isle of Dogs, 2023</figcaption>
</figure>
<figure id="00ba0b7893102c88f0fb" data-id="227788"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-6162.jpg" alt="Shot of the City looking distinctly under built - The Shard only partially constructed in the background" itemprop="image">
<figcaption>The City, 2011</figcaption>
</figure>
<figure id="a541ec20c9dfd4135507" data-id="227789"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-773702.jpg" alt="The City now looking much more built up - The Shard completed" itemprop="image">
<figcaption>The City, 2023</figcaption>
</figure>
<figure id="0fdf50107c4be4d66974" data-id="227791"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-4255.jpg" alt="Nine Elms scored with railway tracks" itemprop="image">
<figcaption>Nine Elms, 2013</figcaption>
</figure>
<figure id="bf46a8c10fe2e98ed218" data-id="227793"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-874804.jpg" alt="Nine Elms looking a lot more built up" itemprop="image">
<figcaption>Nine Elms, 2023</figcaption>
</figure>
<figure id="49bbefa483c8d1b4edd8" data-id="227794"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-0018.jpg" alt="Tower Bridge with a notably low-rise landscape around it" itemprop="image">
<figcaption>Tower Bridge, 2006</figcaption>
</figure>
<figure id="ebda01acccd253bb4425" data-id="227795"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9102864.jpg" alt="Tower Bridge now surrounded by lots more buildings - including the Shard" itemprop="image">
<figcaption>Tower Bridge, 2023</figcaption>
</figure>
<figure id="4f13c811ce14d1c298ad" data-id="227796"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-5338.jpg" alt="The Emirates being constructed, with Highbury Stadium - still open - in the foreground" itemprop="image">
<figcaption>Highbury Stadium and The Emirates, 2005</figcaption>
</figure>
<figure id="b5085c03bac580c6873e" data-id="227797"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-9302962.jpg" alt="Highbury has now been converted into flats, with the Emirates completed" itemprop="image">
<figcaption>The old Highbury Stadium and The Emirates, 2023</figcaption>
</figure>
<figure id="180fd3d6c97eec19ccd6" data-id="227799"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-70.jpg" alt="A sweep in the Thames by the Tower of London - showing an under developed landscape" itemprop="image">
<figcaption>Tower of London, 2002</figcaption>
</figure>
<figure id="545b2077d2bec2b3fd86" data-id="227800"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-773654.jpg" alt="The same shot as above, but far more built up" itemprop="image">
<figcaption>Tower of London, 2023</figcaption>
</figure>
<figure id="fe05a74a7f03e5082d1a" data-id="227792"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-0625.jpg" alt="" itemprop="image">
<figcaption>King's Cross, 2015</figcaption>
</figure>
<figure id="8ec6cff34375121d9fc3" data-id="227802"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-775471.jpg" alt="Another aerial shot of King's Cross - this time a lot more built up" itemprop="image">
<figcaption>King's Cross, 2023</figcaption>
</figure>
<figure id="f65b456b85adc8e9117f" data-id="227803"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-10-2.jpg" alt="A sunset over the Greenwich peninsula - with the O2 by the water - but not much more development" itemprop="image">
<figcaption>North Greenwich, 2007</figcaption>
</figure>
<figure id="80a18bc29db8d8daa87d" data-id="227804"><img src="https://assets.londonist.com/uploads/2023/08/i730/jasonhawkes-8903803.jpg" alt="The same shot as above - now with more development o the peninsula and across the water in the City" itemprop="image">
<figcaption>North Greenwich, 2023</figcaption>
</figure>
<p><em>All images © <a href="https://www.jasonhawkes.com/">Jason Hawkes</a></em></p>
</div>
			
			
			
				<p><em>Last Updated 17 August 2023</em></p>
		</div>
		
		
		
		

	
  	
		

	


<p><em>Continued below.</em></p>


		
		



		
		
		
		

	
    


    
	</article>
		
	<section id="side-bar">
				<h2>TRENDING</h2>
					<div data-id="51814" data-date="2023-06-23 10:45:00 +0100">
	
	
		<p><a href="https://londonist.com/london/sport/womens-world-cup-2023-screenings-where-to-watch-pubs-bars-showing-matches">
			<img alt="Where To Watch The England Vs Spain Women's World Cup Final In London" src="https://assets.londonist.com/uploads/2023/06/i300x150/womens_world_cup_boxpark_2023_screenings.jpg">
</a></p>
	
</div>
					<div data-id="52164" data-date="2022-04-25 10:30:00 +0100">
	
	
		<p><a href="https://londonist.com/london/pubs/pubs-bars-football-watch-premier-league-championship">
			<img alt="The Best Pubs And Bars For Watching Football In London" src="https://assets.londonist.com/uploads/2023/01/i300x150/london_food_and_drink_photography_-_peckham_levels_london_2022_-_nic_crilly-hargrave-35.jpg">
</a></p>
	
</div>
					<div data-id="52152" data-date="2023-07-28 10:54:00 +0100">
	
	
		<p><a href="https://londonist.com/london/beyond-london/day-trips-from-london-august">
			<img alt="13 Summery Day Trips From London: August 2023" src="https://assets.londonist.com/uploads/2023/07/i300x150/imberbus32.jpg">
</a></p>
	
</div>
					<div data-id="52145" data-date="2023-07-27 09:06:00 +0100">
	
	
		<p><a href="https://londonist.com/london/things-to-do/things-to-do-in-london-in-august">
			<img alt="40+ Summery Things To Do In London This Month: August 2023" src="https://assets.londonist.com/uploads/2023/07/i300x150/summer_sounds_kings_cross.jpg">
</a></p>
	
</div>
			</section>
</div>
  
    
    
  	


  
	
	
	
	
    
	
	
	


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft AI suggests Ottawa food bank as a “cannot miss” tourist spot (186 pts)]]></title>
            <link>https://arstechnica.com/information-technology/2023/08/microsoft-ai-suggests-food-bank-as-a-cannot-miss-tourist-spot-in-canada/</link>
            <guid>37171955</guid>
            <pubDate>Fri, 18 Aug 2023 06:22:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/information-technology/2023/08/microsoft-ai-suggests-food-bank-as-a-cannot-miss-tourist-spot-in-canada/">https://arstechnica.com/information-technology/2023/08/microsoft-ai-suggests-food-bank-as-a-cannot-miss-tourist-spot-in-canada/</a>, See on <a href="https://news.ycombinator.com/item?id=37171955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      large language mistakes    —
</h4>
            
            <h2 itemprop="description">AI-penned Microsoft Travel article recommends food bank as a must-see destination.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/unnamed-800x450.jpg" alt="A photo of the Ottawa Food Bank warehouse.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/unnamed.jpg" data-height="608" data-width="1080">Enlarge</a> <span>/</span> A photo of the Ottawa Food Bank warehouse.</p><p>Ottawa Food Bank</p></figcaption>  </figure>

  




<!-- cache hit 8:single/related:ad20a06507d0a4b24f52bd4cce223ddc --><!-- empty -->
<p>Late last week, MSN.com's Microsoft Travel section <a href="https://archive.is/FGP9q">posted</a> an AI-generated article about the "cannot miss" attractions of Ottawa that includes the <a href="https://www.ottawafoodbank.ca/">Ottawa Food Bank</a>, a real charitable organization that feeds struggling families. In its recommendation text, Microsoft's AI model wrote, "Consider going into it on an empty stomach."</p>

<p>Titled, "<a href="https://www.msn.com/en-gb/lifestyle/travel/headed-to-ottawa-here-s-what-you-shouldn-t-miss/ar-AA1faajY">Headed to Ottawa? Here's what you shouldn't miss!</a>," (<a href="https://archive.is/FGP9q">archive here</a>) the article extols the virtues of the Canadian city and recommends attending the <a href="https://en.wikipedia.org/wiki/Winterlude">Winterlude</a> festival (which only takes place in February), visiting an Ottawa Senators game, and skating in "The World's Largest Naturallyfrozen Ice Rink" (sic).</p>
<p>As the No. 3 destination on the list, Microsoft Travel suggests visiting the Ottawa Food Bank, likely drawn from a summary found online but capped with an unfortunate turn of phrase.</p>
<blockquote><p>The organization has been collecting, purchasing, producing, and delivering food to needy people and families in the Ottawa area since 1984. We observe how hunger impacts men, women, and children on a daily basis, and how it may be a barrier to achievement. People who come to us have jobs and families to support, as well as expenses to pay. Life is already difficult enough. Consider going into it on an empty stomach.</p></blockquote>
<p>That last line is an example of the kind of empty platitude (or embarrassing mistaken summary) one can easily find in AI-generated writing, inserted thoughtlessly because the AI model behind the article cannot understand the context of what it is doing.</p>
  <div>
    <ul>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/08/ottawa_food_bank-150x150.jpg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/ottawa_food_bank.jpg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/08/ottawa_food_bank.jpg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/08/ottawa_food_bank.jpg 2560" data-sub-html="#caption-1961649">
          <figure>
            
                          <figcaption id="caption-1961649">
                <span></span>
                                  <p>
                    A screenshot of the "Ottawa Food Bank" blurb in Microsoft Travel's AI-generated article.                  </p>
                                                  <p><span></span>
                                          Ars Technica                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/08/ottawa_article_screenshot_1-150x150.jpg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/ottawa_article_screenshot_1.jpg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/08/ottawa_article_screenshot_1-980x773.jpg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/08/ottawa_article_screenshot_1-1440x1135.jpg 2560" data-sub-html="#caption-1961648">
          <figure>
            
                          <figcaption id="caption-1961648">
                <span></span>
                                  <p>
                    A screenshot of the AI-generated Microsoft Travel Ottawa article.                  </p>
                                                  <p><span></span>
                                          Ars Technica                                      </p>
                              </figcaption>
                      </figure>
        </li>
          </ul>
  </div>

<p>The article is credited to "Microsoft Travel," and it is likely the product of a <a href="https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/">large language model</a> (LLM), a type of AI model trained on a vast scrape of text found on the Internet. Microsoft partner OpenAI made waves with LLMs called GPT-3 in 2020 and <a href="https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/">GPT-4</a> in 2023, both of which can imitate human writing styles but have <a href="https://arstechnica.com/information-technology/2023/08/an-iowa-school-district-is-using-chatgpt-to-decide-which-books-to-ban/">frequently</a> been used for <a href="https://arstechnica.com/information-technology/2023/04/gpt-4-will-hunt-for-trends-in-medical-records-thanks-to-microsoft-and-epic/">unsuitable tasks</a>, according to critics.</p>                                            
                                                        
<p>Since the announcement of <a href="https://arstechnica.com/information-technology/2023/01/openai-and-microsoft-reaffirm-shared-quest-for-powerful-ai-with-new-investment/">deep investments</a> and collaborations with ChatGPT-maker OpenAI in January and the emergence of <a href="https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/">Bing Chat</a> the month after, Microsoft has been experimenting with integrating AI-generated content into its online publications and services, such as adding <a href="https://webcache.googleusercontent.com/search?q=cache:iZqBnsQIN_wJ:https://www.msn.com/en-in/money/news/microsoft-adds-ai-generated-stories-to-its-bing-search-for-visual-auditory-learners/ar-AA1951sC%3Focid%3DFinanceShimLayer&amp;cd=15&amp;hl=en&amp;ct=clnk&amp;gl=us&amp;client=firefox-b-1-d">AI-generated stories to Bing Search</a> and including AI-generated <a href="https://www.theverge.com/2023/5/23/23732821/microsoft-store-bing-ai-hub-apps-build">App review summaries</a> on the Microsoft Store. "Microsoft Travel" appears to be another production use of generative AI technology.</p>

<p>First <a href="https://bsky.app/profile/parismarx.bsky.social/post/3k56cljqydb2b">noticed</a> by tech author Paris Marx on Bluesky, the post on the Ottawa Food Bank began to gain traction on social media late Thursday. In response to Marx's post, frequent LLM critic Emily Bender <a href="https://bsky.app/profile/emilymbender.bsky.social/post/3k56ebui7ih26">noted</a>, "I can't find anything on that page that marks it overtly as AI-generated. Seems like a major failing on two of their <a href="https://www.microsoft.com/en-us/ai/principles-and-approach#tabs-pill-bar-occ736_tab4">'Responsible AI' principles</a>."</p>
<p>Bender also pointed toward Microsoft policies; one is "Transparency," and reads, "How might people misunderstand, misuse, or incorrectly estimate the capabilities of the system?" and the other is "Accountability," which states, "How can we create oversight so that humans can be accountable and in control?"</p>
<p>Judging by the Ottawa article's content, it's more than likely that a human was not responsible for writing the article and that no one fully reviewed its content before publication either, which means that Microsoft is publishing AI-generated content on the Internet with little-to-no oversight.</p>
<p>Microsoft was not immediately available for comment by press time.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Railway Oriented Programming (268 pts)]]></title>
            <link>https://fsharpforfunandprofit.com/rop/</link>
            <guid>37171943</guid>
            <pubDate>Fri, 18 Aug 2023 06:20:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fsharpforfunandprofit.com/rop/">https://fsharpforfunandprofit.com/rop/</a>, See on <a href="https://news.ycombinator.com/item?id=37171943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>This page contains links to the slides and code from my talk “Railway Oriented Programming”.</p>
<p>Here’s the blurb for the talk:</p>
<blockquote>
<div><p>Many examples in functional programming assume that you are always on the “happy path”.
But to create a robust real world application you must deal with validation, logging,
network and service errors, and other annoyances.
</p><p>

So, how do you handle all this in a clean functional way?
</p><p>

This talk will provide a brief introduction to this topic,
using a fun and easy-to-understand railway analogy.</p></div>
</blockquote>
<p>I am also planning to upload some posts on these topics soon. Meanwhile, please see the <a href="https://fsharpforfunandprofit.com/series/a-recipe-for-a-functional-app.html">recipe for a functional app</a> series, which covers similar ground.</p>
<p>If you want to to see some real code, I have created
<a href="https://github.com/swlaschin/Railway-Oriented-Programming-Example">this project on Github that compares normal C# with F# using the ROP approach</a></p>
<p>WARNING: This is a useful approach to error handling, but please don’t take it to extremes! See my post on <a href="https://fsharpforfunandprofit.com/posts/against-railway-oriented-programming/">“Against Railway-Oriented Programming”</a>.</p>



<p>
    <h2 id="videos">
        Videos
        <a data-clipboard-text="https://fsharpforfunandprofit.com/rop/#videos" aria-label="Anchor Videos" href="#videos">
            <svg><use xlink:href="#link"></use></svg>
        </a>
    </h2>
</p>

<p>I presented on this topic at NDC London 2014 (click image to view video)</p>
<p><a href="https://goo.gl/Lv5ZAo"><img src="https://fsharpforfunandprofit.com/rop/rop427.jpg" alt="Video from NDC London 2014">
</a></p>
<p>Other videos of this talk are available are from <a href="http://vimeo.com/97344498">NDC Oslo 2014</a>
and <a href="https://skillsmatter.com/skillscasts/4964-railway-oriented-programming">Functional Programming eXchange, 2014</a></p>



<p>
    <h2 id="slides">
        Slides
        <a data-clipboard-text="https://fsharpforfunandprofit.com/rop/#slides" aria-label="Anchor Slides" href="#slides">
            <svg><use xlink:href="#link"></use></svg>
        </a>
    </h2>
</p>

<p>Slides from Functional Programming eXchange, March 14, 2014</p>



<p>The powerpoint slides are also available from <a href="https://github.com/swlaschin/RailwayOrientedProgramming">Github</a>. Feel free to borrow from them!</p>
<hr>
<p>
If you like my way of explaining things with pictures, take a look at <a href="https://fsharpforfunandprofit.com/books">my "Domain Modeling Made Functional" book!</a>
It's a friendly introduction to Domain Driven Design, modeling with types, and functional programming.
</p>
<hr>
<p>
    <h2 id="relationship-to-the-either-monad-and-kleisli-composition">
        Relationship to the Either monad and Kleisli composition
        <a data-clipboard-text="https://fsharpforfunandprofit.com/rop/#relationship-to-the-either-monad-and-kleisli-composition" aria-label="Anchor Relationship to the Either monad and Kleisli composition" href="#relationship-to-the-either-monad-and-kleisli-composition">
            <svg><use xlink:href="#link"></use></svg>
        </a>
    </h2>
</p>

<p>Any Haskellers reading this will immediately recognize this approach as the <a href="http://book.realworldhaskell.org/read/error-handling.html"><code>Either</code> type</a>,
specialized to use a list of a custom error type for the Left case. In Haskell, something like: <code>type TwoTrack a b = Either [a] (b,[a])</code></p>
<p>I’m certainly not trying to claim that I invented this approach at all (although I do lay claim to the silly analogy).  So why did I not use the standard Haskell terminology?</p>
<p>First, <strong>this post is not trying to be a monad tutorial</strong>, but is instead focused on solving the specific problem of error handling.</p>
<p>Most people coming to F# are not familiar with monads. I’d rather present an approach that is visual, non-intimidating, and generally more intuitive for many people.</p>
<p>I am strong believer in a <a href="https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/">“begin with the concrete, and move to the abstract”</a>
pedagogical approach. In my experience, once you are familiar with this particular approach, the higher level abstractions are easier to grasp later.</p>
<p>Second, I would be incorrect to claim that my two-track type with bind is a monad anyway – a monad is more complicated than that, and I just didn’t want to get into the monad laws here.</p>
<p>Third, and most importantly, <code>Either</code> is too general a concept. <strong>I wanted to present a recipe, not a tool</strong>.</p>
<p>For example, if I want a recipe for making a loaf of bread, saying “just use flour and an oven” is not very helpful.</p>
<p>And so, in the same way, if I want a recipe for handling errors, saying “just use Either with bind” is not very helpful.</p>
<p>So, in this approach, I’m presenting a whole series of techniques:</p>
<ul>
<li>Using a list of custom error types on both the left and right sides of Either (rather than, say, <code>Either String a</code>).</li>
<li>“bind” (<code>&gt;&gt;=</code>) for integrating monadic functions into the pipeline.</li>
<li>Kleisli composition (<code>&gt;=&gt;</code>) for composing monadic functions.</li>
<li>“map” (<code>fmap</code>) for integrating non-monadic functions into the pipeline.</li>
<li>“tee” for integrating unit functions into the pipeline (because F# doesn’t use the IO monad).</li>
<li>Mapping from exceptions to error cases.</li>
<li><code>&amp;&amp;&amp;</code> for combining monadic functions in parallel (e.g. for validation).</li>
<li>The benefits of custom error types for domain driven design.</li>
<li>And obvious extensions for logging, domain events, compensating transactions, and more.</li>
</ul>
<p>I hope you can see that this is a more comprehensive approach than “just use the Either monad”!</p>
<p>My goal here is to provide a template that is versatile enough to be
used in almost all situations, yet constrained enough to enforce a consistent style.
That is, there is basically only one way to write the code. This is extremely helpful to anyone who has to maintain the code later,
as they can immediately understand how it is put together.</p>
<p>I’m not saying that this is the <em>only</em> way to do it. But I think this approach is a good start.</p>
<p>As an aside, even in the Haskell community <a href="http://www.randomhacks.net/2007/03/10/haskell-8-ways-to-report-errors/">there is no consistent approach to error handling</a>, which
can make things <a href="http://programmers.stackexchange.com/questions/252977/cleanest-way-to-report-errors-in-haskell">confusing for beginners</a>.
I know that there is a <a href="http://www.fpcomplete.com/school/starting-with-haskell/basics-of-haskell/10_Error_Handling">lot of content</a>
about the individual <a href="http://hackage.haskell.org/package/errors">error handling techniques</a>, but I’m not aware of a document that brings all these tools
together in a comprehensive way.</p>



<p>
    <h2 id="how-can-i-use-this-in-my-own-code">
        How can I use this in my own code?
        <a data-clipboard-text="https://fsharpforfunandprofit.com/rop/#how-can-i-use-this-in-my-own-code" aria-label="Anchor How can I use this in my own code?" href="#how-can-i-use-this-in-my-own-code">
            <svg><use xlink:href="#link"></use></svg>
        </a>
    </h2>
</p>

<ul>
<li>If you want a ready-made F# library that works with NuGet, check out the <a href="https://fsprojects.github.io/Chessie/">Chessie project</a>.</li>
<li>If you want to see a sample web-service using these techniques, <a href="https://github.com/swlaschin/Railway-Oriented-Programming-Example">I have created a project on GitHub</a>.</li>
<li>You can also <a href="https://fsharpforfunandprofit.com/posts/railway-oriented-programming-carbonated/">see the ROP approach applied to FizzBuzz</a>!</li>
</ul>
<p>F# does not have type classes, and so you don’t really have a reusable way to do monads (although the <a href="https://github.com/fsprojects/fsharpx/blob/master/src/FSharpx.Core/ComputationExpressions/Monad.fs">FSharpX library</a>
has a useful approach).  This means the <code>Rop.fs</code> library defines all its functions from scratch.
(In some ways though, this isolation can be helpful because there are no external dependencies at all.)</p>



<p>
    <h2 id="further-reading">
        Further reading
        <a data-clipboard-text="https://fsharpforfunandprofit.com/rop/#further-reading" aria-label="Anchor Further reading" href="#further-reading">
            <svg><use xlink:href="#link"></use></svg>
        </a>
    </h2>
</p>

<blockquote>
<p><em>“One bind does not a monad make” – Aristotle</em></p>
</blockquote>
<p>As I mentioned above, one reason why I stayed away from monads is that defining a monad correctly is <em>not</em> just a matter of implementing “bind” and “return”.
It is an algebraic structure that needs to obey the monad laws (which in turn are just the <a href="https://fsharpforfunandprofit.com/posts/monoids-without-tears/">monoid laws</a> in a specific situation)
and that was a path I did not want to head down in this particular talk.</p>
<p>However if you are interested in more detail on <code>Either</code> and Kleisi composition, here are some links that might be useful:</p>
<ul>
<li><strong>Monads in general</strong>.
<ul>
<li><a href="http://stackoverflow.com/questions/44965/what-is-a-monad">Stack overflow answer on monads</a></li>
<li><a href="http://stackoverflow.com/questions/2704652/monad-in-plain-english-for-the-oop-programmer-with-no-fp-background/2704795#2704795">Stack overflow answer by Eric Lippert</a></li>
<li><a href="http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html">Monads in pictures</a></li>
<li><a href="http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html">“You Could Have Invented Monads”</a></li>
<li><a href="https://www.haskell.org/tutorial/monads.html">Haskell tutorial</a></li>
<li><a href="http://ncatlab.org/nlab/show/monad">The hardcore definition on nLab</a></li>
</ul>
</li>
<li><strong>The <code>Either</code> monad</strong>.
<ul>
<li><a href="http://www.fpcomplete.com/school/starting-with-haskell/basics-of-haskell/10_Error_Handling">School of Haskell</a></li>
<li><a href="http://book.realworldhaskell.org/read/error-handling.html">Real World Haskell on error handling</a> (halfway down)</li>
<li><a href="http://learnyouahaskell.com/for-a-few-monads-more">LYAH on error handling</a> (halfway down)</li>
</ul>
</li>
<li><strong>Kleisli categories and composition</strong>
<ul>
<li><a href="http://www.fpcomplete.com/user/Lkey/kleisli">Post at FPComplete</a></li>
<li><a href="http://bartoszmilewski.com/2014/12/23/kleisli-categories/">Post by Bartosz Milewski</a></li>
<li><a href="http://ncatlab.org/nlab/show/Kleisli+category">The hardcore definition on nLab</a></li>
</ul>
</li>
<li><strong>Comprehensive error handling approaches</strong>
<ul>
<li><a href="http://www.randomhacks.net/2007/03/10/haskell-8-ways-to-report-errors/">Item 5 in this post</a></li>
<li>I’m not aware of other approaches that cover all the techniques discussed in this talk.
If you do know of any, ping me in the comments and I’ll update this page.</li>
</ul>
</li>
</ul>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TypeScript Is Surprisingly OK for Compilers (249 pts)]]></title>
            <link>https://matklad.github.io/2023/08/17/typescript-is-surprisingly-ok-for-compilers.html</link>
            <guid>37171801</guid>
            <pubDate>Fri, 18 Aug 2023 05:58:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2023/08/17/typescript-is-surprisingly-ok-for-compilers.html">https://matklad.github.io/2023/08/17/typescript-is-surprisingly-ok-for-compilers.html</a>, See on <a href="https://news.ycombinator.com/item?id=37171801">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>

    
<p><span>There are two main historical trends when choosing an implementation language for something</span>
<span>compiler-shaped.</span></p>
<p><span>For more language-centric tasks, like a formal specification, or a toy hobby language, OCaml makes</span>
<span>most sense. See, for example, </span><a href="https://plzoo.andrej.com/"><span>plzoo</span></a><span> or </span><a href="https://github.com/WebAssembly/spec/tree/653938a88c6f40eb886d5980ca315136eb861d03/interpreter"><span>WebAssembly reference</span>
<span>interpreter</span></a><span>.</span></p>
<p><span>For something implementation-centric and production ready, C++ is often chosen: LLVM, clang, v8,</span>
<span>HotSpot are all C++.</span></p>
<p><span>These days, Rust is a great new addition to the landscape. It is influenced most directly by ML and</span>
<span>C++, combines their strengths, and even brings something new of its own to the table, like seamless,</span>
<span>safe multithreading. Still, Rust leans heavily towards production readiness side of the spectrum.</span>
<span>While some aspects of it, like a </span>“<span>just works</span>”<span> build system, help with prototyping as well, there</span>’<span>s</span>
<span>still extra complexity tax due to the necessity to model physical layout of data. The usual advice,</span>
<span>when you start building a compiler in Rust, is to avoid pointers and use indexes. Indexes are great!</span>
<span>In large codebase, they allow greater decoupling (side tables can stay local to relevant modules),</span>
<span>improved performance (an index is  </span><code>u32</code><span> and nudges you towards struct-of-arrays layouts), and more</span>
<span>flexible computation strategies (indexes are easier to serialize or plug into incremental</span>
<span>compilation framework). But they do make programming-in-the-small significantly more annoying, which</span>
<span>is a deal-breaker for hobbyist tinkering.</span></p>
<p><span>But OCaml is crufty! Is there something better? Today, I realized that TypeScript might actually be</span>
<span>OK? It is not really surprising, given how the language works, but it never occured to me to think</span>
<span>about TypeScript as an ML equivalent before.</span></p>
<p><span>So, let</span>’<span>s write a tiny-tiny typechecker in TS!</span></p>
<p><span>Of course, we start with </span><a href="https://deno.land/"><span>deno</span></a><span>. See </span><a href="https://matklad.github.io/2023/02/12/a-love-letter-to-deno.html"><em><span>A Love Letter to</span>
<span>Deno</span></em></a><span> for more details, but the</span>
<span>TL;DR is that deno provides out-of-the-box experience for TypeScript. This is a pain point for</span>
<span>OCaml, and something that Rust does better than either OCaml or C++. But deno does this better than</span>
<span>Rust! It</span>’<span>s just a single binary, it comes with linting and formatting, there</span>’<span>s no compilation step,</span>
<span>and there are built-in task runner and watch mode. A dream setup for quick PLT hacks!</span></p>
<p><span>And then there</span>’<span>s TypeScript itself, with its sufficiently flexible, yet light-ceremony type system.</span></p>
<p><span>Let</span>’<span>s start with defining an AST. As we are hacking, we won</span>’<span>t bother with making it an IDE-friendly</span>
<span>concrete syntax tree, or incremental-friendly </span>“<span>only store relative offsets</span>”<span> tree, and will just tag</span>
<span>AST nodes with locations in file:</span></p>

<figure>


<pre><code><span><span>export</span> <span>interface</span> <span>Location</span> {</span>
<span>  <span>file</span>: <span>string</span>;</span>
<span>  <span>line</span>: <span>number</span>;</span>
<span>  <span>column</span>: <span>number</span>;</span>
<span>}</span></code></pre>

</figure>
<p><span>Even here, we already see high-level nature of TypeScript </span>—<span> string is just a </span><code>string</code><span>, there</span>’<span>s no</span>
<span>thinking about </span><code>usize</code><span> vs </span><code>u32</code><span> as numbers are just </span><code>number</code><span>s.</span></p>
<p><span>Usually, an expression is defined as a sum-type. As we want to tag each expression with a location,</span>
<span>that representation would be slightly inconvenient for us, so we split things up a bit:</span></p>

<figure>


<pre><code><span><span>export</span> <span>interface</span> <span>Expr</span> {</span>
<span>    <span>location</span>: <span>Location</span>;</span>
<span>    <span>kind</span>: <span>ExprKind</span>;</span>
<span>}</span>
<span></span>
<span><span>export</span> <span>type</span> <span>ExprKind</span> = <span>ExprBool</span> | <span>ExprInt</span> | ... ;</span></code></pre>

</figure>
<p><span>One more thing </span>—<span> as we are going for something quick, we</span>’<span>ll be storing inferred types directly in</span>
<span>the AST nodes. Still, we want to keep raw and type-checked AST separate, so what we are going to do</span>
<span>here is to parametrize the </span><code>Expr</code><span> over associated data it stores. A freshly parsed expression would</span>
<span>use </span><code>void</code><span> as data, and the type checker will set it to </span><code>Type</code><span>. Here</span>’<span>s what we get:</span></p>

<figure>


<pre><code><span><span>export</span> <span>interface</span> <span>Expr</span>&lt;T&gt; {</span>
<span>  <span>location</span>: <span>Location</span>;</span>
<span>  <span>data</span>: T;</span>
<span>  <span>kind</span>: <span>ExprKind</span>&lt;T&gt;;</span>
<span>}</span>
<span></span>
<span><span>export</span> <span>type</span> <span>ExprKind</span>&lt;T&gt; =</span>
<span>  | <span>ExprBool</span>&lt;T&gt;</span>
<span>  | <span>ExprInt</span>&lt;T&gt;</span>
<span>  | <span>ExprBinary</span>&lt;T&gt;</span>
<span>  | <span>ExprControl</span>&lt;T&gt;;</span></code></pre>

</figure>
<p><span>A definition of </span><code>ExprBinary</code><span> could look like this:</span></p>

<figure>


<pre><code><span><span>export</span> <span>interface</span> <span>ExprBinary</span>&lt;T&gt; {</span>
<span>  <span>op</span>: <span>BinaryOp</span>;</span>
<span>  <span>lhs</span>: <span>Expr</span>&lt;T&gt;;</span>
<span>  <span>rhs</span>: <span>Expr</span>&lt;T&gt;;</span>
<span>}</span>
<span></span>
<span><span>export</span> <span>enum</span> <span>BinaryOp</span> {</span>
<span>  <span>Add</span>, <span>Sub</span>, <span>Mul</span>, <span>Div</span>,</span>
<span>  <span>Eq</span>, <span>Neq</span>,</span>
<span>  <span>Lt</span>, <span>Gt</span>, <span>Le</span>, <span>Ge</span>,</span>
<span>}</span></code></pre>

</figure>
<p><span>Note how I don</span>’<span>t introduce separate types for, e.g, </span><code>AddExpr</code><span> and </span><code>SubExpr</code><span> </span>—<span> all binary</span>
<span>expressions have the same shape, so one type is enough!</span></p>
<p><span>But we need a tiny adjustment here. Our </span><code>Expr</code><span> kind is defined as a union type. To match a value of</span>
<span>a union type a bit of runtime type information is needed. However, it</span>’<span>s one of the core properties</span>
<span>of TypeScript that it doesn</span>’<span>t add any runtime behaviors. So, if we want to match on expression kinds</span>
<span>(and we for sure want!), we need to give a helping hand to the compiler and include a bit of RTTI</span>
<span>manually. That would be the </span><code>tag</code><span> field:</span></p>

<figure>


<pre><code><span><span>export</span> <span>interface</span> <span>ExprBinary</span>&lt;T&gt; {</span>
<span>  <span>tag</span>: <span>"binary"</span>;</span>
<span>  <span>op</span>: <span>BinaryOp</span>;</span>
<span>  <span>lhs</span>: <span>Expr</span>&lt;T&gt;;</span>
<span>  <span>rhs</span>: <span>Expr</span>&lt;T&gt;;</span>
<span>}</span></code></pre>

</figure>
<p><code>tag: "binary"</code><span> means that the only possible runtime value for </span><code>tag</code><span> is the string </span><code>"binary"</code><span>.</span></p>
<p><span>Similarly to various binary expressions, boolean literal and int literal expressions have </span><em><span>almost</span></em>
<span>identical shape.  Almost, because the payload (</span><code>boolean</code><span> or </span><code>number</code><span>) is different. TypeScript</span>
<span>allows us to neatly abstract this over:</span></p>

<figure>


<pre><code><span><span>export</span> <span>type</span> <span>ExprBool</span>&lt;T&gt; = <span>ExprLiteral</span>&lt;T, <span>boolean</span>, <span>"bool"</span>&gt;;</span>
<span><span>export</span> <span>type</span> <span>ExprInt</span>&lt;T&gt; = <span>ExprLiteral</span>&lt;T, <span>number</span>, <span>"int"</span>&gt;;</span>
<span></span>
<span><span>export</span> <span>interface</span> <span>ExprLiteral</span>&lt;T, V, <span>Tag</span>&gt; {</span>
<span>  <span>tag</span>: <span>Tag</span>;</span>
<span>  <span>value</span>: V;</span>
<span>}</span></code></pre>

</figure>
<p><span>Finally, for control-flow expressions we only add </span><code>if</code><span> for now:</span></p>

<figure>


<pre><code><span><span>export</span> <span>type</span> <span>ExprControl</span>&lt;T&gt; = <span>ExprIf</span>&lt;T&gt;;</span>
<span></span>
<span><span>export</span> <span>interface</span> <span>ExprIf</span>&lt;T&gt; {</span>
<span>  <span>tag</span>: <span>"if"</span>;</span>
<span>  <span>cond</span>: <span>Expr</span>&lt;T&gt;;</span>
<span>  <span>then_branch</span>: <span>Expr</span>&lt;T&gt;;</span>
<span>  <span>else_branch</span>: <span>Expr</span>&lt;T&gt;;</span>
<span>}</span></code></pre>

</figure>
<p><span>This concludes the definition of the ast! Let</span>’<span>s move on to the type inference! Start with types:</span></p>

<figure>


<pre><code><span><span>type</span> <span>Type</span> = <span>TypeBool</span> | <span>TypeInt</span>;</span>
<span></span>
<span><span>interface</span> <span>TypeBool</span> {</span>
<span>  <span>tag</span>: <span>"Bool"</span>;</span>
<span>}</span>
<span><span>const</span> <span>TypeBool</span>: <span>TypeBool</span> = { <span>tag</span>: <span>"Bool"</span> };</span>
<span></span>
<span><span>interface</span> <span>TypeInt</span> {</span>
<span>  <span>tag</span>: <span>"Int"</span>;</span>
<span>}</span>
<span><span>const</span> <span>TypeInt</span>: <span>TypeInt</span> = { <span>tag</span>: <span>"Int"</span> };</span></code></pre>

</figure>
<p><span>Our types are really simple, we could have gone with </span><span><code>type Type = "Int" | "Bool"</code><span>,</span></span><span> but</span>
<span>lets do this a bit more enterprisy! We define separate types for integer and boolean types. As these</span>
<span>types are singletons, we also provide canonical definitions. And here is another TypeScript-ism.</span>
<span>Because TypeScript fully erases types, everything related to types lives in a separate namespace. So</span>
<span>you can have a type and a value sharing the same name. Which is exactly what we use to define the</span>
<span>singletons!</span></p>
<p><span>Finally, we can take advantage of our associated-data parametrized expression and writhe the</span>
<span>signature of</span></p>

<figure>


<pre><code><span><span>function</span> <span>infer_types</span>(<span>expr: ast.Expr&lt;<span>void</span>&gt;</span>): ast.<span>Expr</span>&lt;<span>Type</span>&gt;</span></code></pre>

</figure>
<p><span>As it says on the tin, </span><code>inter_types</code><span> fills in </span><code>Type</code><span> information into the void! Let</span>’<span>s fill in the</span>
<span>details!</span></p>

<figure>


<pre><code><span><span>function</span> <span>infer_types</span>(<span>expr: ast.Expr&lt;<span>void</span>&gt;</span>): ast.<span>Expr</span>&lt;<span>Type</span>&gt; {</span>
<span>  <span>switch</span> (expr.<span>kind</span>.<span>tag</span>) {</span>
<span>    cas</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>If at this point we hit Enter, the editor completes:</span></p>

<figure>


<pre><code><span><span>function</span> <span>infer_types</span>(<span>expr: ast.Expr&lt;<span>void</span>&gt;</span>): ast.<span>Expr</span>&lt;<span>Type</span>&gt; {</span>
<span>  <span>switch</span> (expr.<span>kind</span>.<span>tag</span>) {</span>
<span>    <span>case</span> <span>"bool"</span>:</span>
<span>    <span>case</span> <span>"int"</span>:</span>
<span>    <span>case</span> <span>"binary"</span>:</span>
<span>    <span>case</span> <span>"if"</span>:</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>There</span>’<span>s one problem though. What we really want to write here is something like</span>
<span><code>const inferred_type = switch(..)</code><span>,</span></span>
<span>but in TypeScript </span><code>switch</code><span> is a statement, not an expression.</span>
<span>So let</span>’<span>s define a generic visitor!</span></p>

<figure>


<pre><code><span><span>export</span> <span>type</span> <span>Visitor</span>&lt;T, R&gt; = {</span>
<span>  <span>bool</span>(<span>kind</span>: <span>ExprBool</span>&lt;T&gt;): R;</span>
<span>  <span>int</span>(<span>kind</span>: <span>ExprInt</span>&lt;T&gt;): R;</span>
<span>  <span>binary</span>(<span>kind</span>: <span>ExprBinary</span>&lt;T&gt;): R;</span>
<span>  <span>if</span>(<span>kind</span>: <span>ExprIf</span>&lt;T&gt;): R;</span>
<span>};</span>
<span></span>
<span><span>export</span> <span>function</span> visit&lt;T, R&gt;(</span>
<span>  <span>expr</span>: <span>Expr</span>&lt;T&gt;,</span>
<span>  <span>v</span>: <span>Visitor</span>&lt;T, R&gt;,</span>
<span>): R {</span>
<span>  <span>switch</span> (expr.<span>kind</span>.<span>tag</span>) {</span>
<span>    <span>case</span> <span>"bool"</span>: <span>return</span> v.<span>bool</span>(expr.<span>kind</span>);</span>
<span>    <span>case</span> <span>"int"</span>: <span>return</span> v.<span>int</span>(expr.<span>kind</span>);</span>
<span>    <span>case</span> <span>"binary"</span>: <span>return</span> v.<span>binary</span>(expr.<span>kind</span>);</span>
<span>    <span>case</span> <span>"if"</span>: <span>return</span> v.<span>if</span>(expr.<span>kind</span>);</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Armed with the </span><code>visit</code><span>, we can ergonomically match over the expression:</span></p>

<figure>


<pre><code><span><span>function</span> <span>infer_types</span>(<span>expr: ast.Expr&lt;<span>void</span>&gt;</span>): ast.<span>Expr</span>&lt;<span>Type</span>&gt; {</span>
<span>  <span>const</span> ty = <span>visit</span>(expr, {</span>
<span>    <span>bool</span>: <span>() =&gt;</span> <span>TypeBool</span>,</span>
<span>    <span>int</span>: <span>() =&gt;</span> <span>TypeInt</span>,</span>
<span>    <span>binary</span>: <span>(<span>kind: ast.ExprBinary&lt;<span>void</span>&gt;</span>) =&gt;</span> <span>result_type</span>(kind.<span>op</span>),</span>
<span>    <span>if</span>: (<span>kind</span>: ast.<span>ExprIf</span>&lt;<span>void</span>&gt;) {</span>
<span>      ...</span>
<span>    },</span>
<span>  });</span>
<span>  ...</span>
<span>}</span>
<span></span>
<span><span>function</span> <span>result_type</span>(<span>op: ast.BinaryOp</span>): <span>Type</span> {</span>
<span>  <span>switch</span> (op) { <span>// A tad verbose, but auto-completed!</span></span>
<span>    <span>case</span> ast.<span>BinaryOp</span>.<span>Add</span>: <span>case</span> ast.<span>BinaryOp</span>.<span>Sub</span>:</span>
<span>    <span>case</span> ast.<span>BinaryOp</span>.<span>Mul</span>: <span>case</span> ast.<span>BinaryOp</span>.<span>Div</span>:</span>
<span>      <span>return</span> <span>TypeInt</span></span>
<span></span>
<span>    <span>case</span> ast.<span>BinaryOp</span>.<span>Eq</span>: <span>case</span> ast.<span>BinaryOp</span>.<span>Neq</span>:</span>
<span>      <span>return</span> <span>TypeBool</span></span>
<span></span>
<span>    <span>case</span> ast.<span>BinaryOp</span>.<span>Lt</span>: <span>case</span> ast.<span>BinaryOp</span>.<span>Gt</span>:</span>
<span>    <span>case</span> ast.<span>BinaryOp</span>.<span>Le</span>: <span>case</span> ast.<span>BinaryOp</span>.<span>Ge</span>:</span>
<span>      <span>return</span> <span>TypeBool</span></span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Before we go further, let</span>’<span>s generalize this visiting pattern a bit! Recall that our expressions are</span>
<span>parametrized by the type of associated data, and type-checker-shaped transformations are essentially an</span>
<code>Expr&lt;U&gt; -&gt; Expr&lt;V&gt;</code>
<span>transformation.</span></p>
<p><span>Let</span>’<span>s make this generic!</span></p>

<figure>


<pre><code><span><span>export</span> <span>function</span> transform&lt;U, V&gt;(<span>expr</span>: <span>Expr</span>&lt;U&gt;, <span>v</span>: <span>Visitor</span>&lt;V, V&gt;): <span>Expr</span>&lt;V&gt; {</span></code></pre>

</figure>
<p><span>Transform maps an expression carrying </span><code>T</code><span> into an expression carrying </span><code>V</code><span> by applying an </span><code>f</code>
<span>visitor. Importantly, it</span>’<span>s </span><code>Visitor&lt;V, V&gt;</code><span>, rather than a </span><code>Visitor&lt;U, V&gt;</code><span>. This is</span>
<span>counter-intuitive, but correct </span>—<span> we run transformation bottom up, transforming the leaves first.</span>
<span>So, when the time comes to visit an interior node, all subexpression will have been transformed!</span></p>
<p><span>The body of </span><code>transform</code><span> is wordy, but regular, rectangular, and auto-comptletes itself:</span></p>

<figure>


<pre><code><span><span>export</span> <span>function</span> transform&lt;U, V&gt;(<span>expr</span>: <span>Expr</span>&lt;U&gt;, <span>v</span>: <span>Visitor</span>&lt;V, V&gt;): <span>Expr</span>&lt;V&gt; {</span>
<span>  <span>switch</span> (expr.<span>kind</span>.<span>tag</span>) {</span>
<span>    <span>case</span> <span>"bool"</span>:</span>
<span>      <span>return</span> {</span>
<span>        <span>location</span>: expr.<span>location</span>,</span>
<span>        <span>data</span>: v.<span>bool</span>(expr.<span>kind</span>),</span>
<span>        <span>kind</span>: expr.<span>kind</span>, <i data-value="1"></i></span>
<span>      };</span>
<span>    <span>case</span> <span>"int"</span>:</span>
<span>      <span>return</span> {</span>
<span>        <span>location</span>: expr.<span>location</span>,</span>
<span>        <span>data</span>: v.<span>int</span>(expr.<span>kind</span>),</span>
<span>        <span>kind</span>: expr.<span>kind</span>,</span>
<span>      };</span>
<span>    <span>case</span> <span>"binary"</span>: {</span>
<span>      <span>const</span> <span>kind</span>: <span>ExprBinary</span>&lt;V&gt; = { <i data-value="2"></i></span>
<span>        <span>tag</span>: <span>"binary"</span>,</span>
<span>        <span>op</span>: expr.<span>kind</span>.<span>op</span>,</span>
<span>        <span>lhs</span>: <span>transform</span>(expr.<span>kind</span>.<span>lhs</span>, v),</span>
<span>        <span>rhs</span>: <span>transform</span>(expr.<span>kind</span>.<span>rhs</span>, v),</span>
<span>      };</span>
<span>      <span>return</span> {</span>
<span>        <span>location</span>: expr.<span>location</span>,</span>
<span>        <span>data</span>: v.<span>binary</span>(kind), <i data-value="2"></i></span>
<span>        <span>kind</span>: kind,</span>
<span>      };</span>
<span>    }</span>
<span>    <span>case</span> <span>"if"</span>: {</span>
<span>      <span>const</span> <span>kind</span>: <span>ExprIf</span>&lt;V&gt; = {</span>
<span>        <span>tag</span>: <span>"if"</span>,</span>
<span>        <span>cond</span>: <span>transform</span>(expr.<span>kind</span>.<span>cond</span>, v),</span>
<span>        <span>then_branch</span>: <span>transform</span>(expr.<span>kind</span>.<span>then_branch</span>, v),</span>
<span>        <span>else_branch</span>: <span>transform</span>(expr.<span>kind</span>.<span>else_branch</span>, v),</span>
<span>      };</span>
<span>      <span>return</span> {</span>
<span>        <span>location</span>: expr.<span>location</span>,</span>
<span>        <span>data</span>: v.<span>if</span>(kind),</span>
<span>        <span>kind</span>: kind,</span>
<span>      };</span>
<span>    }</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<ol>
<li>
<p><span>Note how here </span><code>expr.kind</code><span> is both </span><code>Expr&lt;U&gt;</code><span> and  </span><code>Expr&lt;V&gt;</code><span> </span>—<span> literals don</span>’<span>t depend on this type</span>
<span>parameter, and TypeScript is smart enough to figure this out without us manually re-assembling</span>
<span>the same value with a different type.</span></p>
</li>
<li>
<p><span>This is where that magic with </span><code>Visitor&lt;V, V&gt;</code><span> happens.</span></p>
</li>
</ol>
<p><span>The code is pretty regular here though! So at this point we might actually recall that TypeScript is</span>
<span>a dynamically-typed language, and write a generic traversal using </span><code>Object.keys</code><span>, </span><em><span>while keeping the</span>
<span>static function signature in-place</span></em><span>. I don</span>’<span>t think we need to do it here, but there</span>’<span>s comfort in</span>
<span>knowing that it</span>’<span>s possible!</span></p>
<p><em><span>Now</span></em><span> implementing type inference should be a breeze! We need some way to emit type errors though.</span>
<span>With TypeScript, it would be trivial to accumulate errors into an array as a side-effect, but let</span>’<span>s</span>
<span>actually represent type errors as instances of a specific type, </span><code>TypeError</code><span> (pun intended):</span></p>

<figure>


<pre><code><span><span>type</span> <span>Type</span> = <span>TypeBool</span> | <span>TypeInt</span> | <span>TypeError</span>;</span>
<span></span>
<span><span>interface</span> <span>TypeError</span> {</span>
<span>  <span>tag</span>: <span>"Error"</span>;</span>
<span>  <span>location</span>: ast.<span>Location</span>;</span>
<span>  <span>message</span>: <span>string</span>;</span>
<span>}</span></code></pre>

</figure>
<p><span>To check ifs and binary expressions, we would also need an utility for comparing types:</span></p>

<figure>


<pre><code><span><span>function</span> <span>type_equal</span>(<span>lhs: Type, rhs: Type</span>): <span>boolean</span> {</span>
<span>  <span>if</span> (lhs.<span>tag</span> == <span>"Error"</span> || rhs.<span>tag</span> == <span>"Error"</span>) <span>return</span> <span>true</span>;</span>
<span>  <span>return</span> lhs.<span>tag</span> == rhs.<span>tag</span>;</span>
<span>}</span></code></pre>

</figure>
<p><span>We make the </span><code>Error</code><span> type equal to any other type to prevent cascading failures. With all that</span>
<span>machinery in place, our type checker is finally:</span></p>

<figure>


<pre><code><span><span>function</span> <span>infer_types</span>(<span>expr: ast.Expr&lt;<span>void</span>&gt;</span>): ast.<span>Expr</span>&lt;<span>Type</span>&gt; {</span>
<span>  <span>return</span> ast.<span>transform</span>(expr, {</span>
<span>    <span>bool</span>: (): <span><span>Type</span> =&gt;</span> <span>TypeBool</span>,</span>
<span>    <span>int</span>: (): <span><span>Type</span> =&gt;</span> <span>TypeInt</span>,</span>
<span></span>
<span>    <span>binary</span>: (<span>kind</span>: ast.<span>ExprBinary</span>&lt;<span>Type</span>&gt;, <span>location</span>: ast.<span>Location</span>): <span><span>Type</span> =&gt;</span> {</span>
<span>      <span>if</span> (!<span>type_equal</span>(kind.<span>lhs</span>.<span>data</span>, kind.<span>rhs</span>.<span>data</span>)) {</span>
<span>        <span>return</span> {</span>
<span>          <span>tag</span>: <span>"Error"</span>,</span>
<span>          location,</span>
<span>          <span>message</span>: <span>"binary expression operands have different types"</span>,</span>
<span>        };</span>
<span>      }</span>
<span>      <span>return</span> <span>result_type</span>(kind.<span>op</span>);</span>
<span>    },</span>
<span></span>
<span>    <span>if</span>: (<span>kind</span>: ast.<span>ExprIf</span>&lt;<span>Type</span>&gt;, <span>location</span>: ast.<span>Location</span>): <span><span>Type</span> =&gt;</span> {</span>
<span>      <span>if</span> (!<span>type_equal</span>(kind.<span>cond</span>.<span>data</span>, <span>TypeBool</span>)) {</span>
<span>        <span>return</span> {</span>
<span>          <span>tag</span>: <span>"Error"</span>,</span>
<span>          location,</span>
<span>          <span>message</span>: <span>"if condition is not a boolean"</span>,</span>
<span>        };</span>
<span>      }</span>
<span>      <span>if</span> (!<span>type_equal</span>(kind.<span>then_branch</span>.<span>data</span>, kind.<span>else_branch</span>.<span>data</span>)) {</span>
<span>        <span>return</span> {</span>
<span>          <span>tag</span>: <span>"Error"</span>,</span>
<span>          location,</span>
<span>          <span>message</span>: <span>"if branches have different types"</span>,</span>
<span>        };</span>
<span>      }</span>
<span>      <span>return</span> kind.<span>then_branch</span>.<span>data</span>;</span>
<span>    },</span>
<span>  });</span>
<span>}</span>
<span></span>
<span><span>function</span> <span>result_type</span>(<span>op: ast.BinaryOp</span>): <span>Type</span> {</span>
<span>    ...</span>
<span>}</span></code></pre>

</figure>
<p><span>Astute reader will notice that our visitor functions now take an extra </span><code>ast.Location</code><span> argument.</span>
<span>TypeScript allows using this argument only in cases where it is needed, cutting down verbosity.</span></p>
<p><span>And that</span>’<span>s all for today! The end result is pretty neat and concise. It took some typing to get there,</span>
<span>but TypeScript autocompletion really helps with that! What</span>’<span>s more important, there was very little</span>
<span>fighting with the language, and the result feels quite natural and directly corresponds to the shape</span>
<span>of the problem.</span></p>
<p><span>I am not entirely sure in the conclusion just yet, but I think I</span>’<span>ll be using TypeScript as my tool</span>
<span>of choice for various small language hacks. It is surprisingly productive due to the confluence of</span>
<span>three aspects:</span></p>
<ul>
<li>
<span>deno is a perfect scripting runtime! Small, hermetic, powerful, and optimized for effective</span>
<span>development workflows.</span>
</li>
<li>
<span>TypeScript tooling is great </span>—<span> the IDE is helpful and productive (and deno makes sure that it</span>
<span>also requires zero configuration)</span>
</li>
<li>
<span>The language is powerful both at runtime and at compile time. You can get pretty fancy with types,</span>
<span>but you can also just escape to dynamic world if you need some very high-order code.</span>
</li>
</ul>
<hr>
<p><span>Just kidding, here</span>’<span>s one more cute thing. Let</span>’<span>s say that we want to have lots of syntactic sugar,</span>
<span>and also want type-safe desugaring. We could tweak our setup a bit for that: instead of </span><code>Expr</code><span> and</span>
<code>ExprKind</code><span> being parametrized over associated data, we circularly parametrize </span><code>Expr</code><span> by the whole</span>
<code>ExprKind</code><span> and  vice verse:</span></p>

<figure>


<pre><code><span><span>interface</span> <span>Expr</span>&lt;K&gt; {</span>
<span>  <span>location</span>: <span>Location</span>,</span>
<span>  <span>kind</span>: K,</span>
<span>}</span>
<span></span>
<span><span>interface</span> <span>ExprBinary</span>&lt;E&gt; {</span>
<span>  <span>op</span>: <span>BinaryOp</span>,</span>
<span>  <span>lhs</span>: E,</span>
<span>  <span>rhs</span>: E,</span>
<span>}</span></code></pre>

</figure>
<p><span>This allows expressing desugaring in a type-safe manner!</span></p>

<figure>


<pre><code><span><span>// Fundamental, primitive expressions.</span></span>
<span><span>type</span> <span>ExprKindCore</span>&lt;E&gt; =</span>
<span>    <span>ExprInt</span>&lt;E&gt; | <span>ExprBinary</span>&lt;E&gt; | <span>ExprIf</span>&lt;E&gt;</span>
<span></span>
<span><span>// Expressions which are either themselves primitive,</span></span>
<span><span>// or can be desugared to primitives.</span></span>
<span><span>type</span> <span>ExprKindSugar</span>&lt;E&gt; = <span>ExprKindCore</span>&lt;E&gt;</span>
<span>    | <span>ExprCond</span>&lt;E&gt; | <span>ExprUnless</span>&lt;E&gt;</span>
<span></span>
<span><span>type</span> <span>ExprCore</span> = <span>Expr</span>&lt;<span>ExprKindCore</span>&lt;<span>ExprCore</span>&gt;&gt;;</span>
<span><span>type</span> <span>ExprSugar</span> = <span>Expr</span>&lt;<span>ExprKindSugar</span>&lt;<span>ExprSugar</span>&gt;&gt;;</span>
<span></span>
<span><span>// Desugaring works by reducing the set of expression kinds.</span></span>
<span><span>function</span> <span>desugar</span>(<span>expr: ExprSugar</span>): <span>ExprCore</span></span>
<span></span>
<span><span>// A desugaring steps takes a (potentially sugar) expression,</span></span>
<span><span>// whose subexpression are already desugared,</span></span>
<span><span>// and produces an equivalent core expression.</span></span>
<span><span>function</span> <span>desugar_one</span>(<span></span></span>
<span><span>    expr: ExprKindSugar&lt;ExprCore&gt;,</span></span>
<span><span></span>): <span>ExprKindCore</span>&lt;<span>ExprCore</span>&gt;</span></code></pre>

</figure>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[micro – A Modern Alternative to nano (312 pts)]]></title>
            <link>https://micro-editor.github.io/</link>
            <guid>37171294</guid>
            <pubDate>Fri, 18 Aug 2023 04:06:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://micro-editor.github.io/">https://micro-editor.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=37171294">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <div>
                    <p>
                        <h2>Features</h2>
                    </p>
                </div>
                <div>
                    <h4>Easy to Use</h4>
                    <p>Micro's number one feature is being easy to install (it's just a static binary with no dependencies) and easy to use.</p>

                    <h4>Highly Customizable</h4>
                    <p>Use a simple json format to configure your options and rebind keys to your liking. If you need more power, you can use Lua to configure the editor further.</p>

                    <h4>Colors and Highlighting</h4>
                    <p>Micro supports over 75 languages and has 7 default colorschemes to choose from. Micro supports 16, 256, and truecolor themes. Syntax files and colorschemes are also very simple to make.</p>

                    <h4>Multiple Cursors</h4>
                    <p>Micro has support for Sublime-style multiple cursors, giving you lots of editing power directly in your terminal.</p>
                </div>

                <div>
                    <h4>Plugin System</h4>
                    <p>Micro supports a full-blown plugin system. Plugins are written in Lua and there is a plugin manager to automatically download and install your plugins for you.</p>

                    <h4>Common Keybindings</h4>
                    <p>Micro's keybindings are what you would expect from a simple-to-use editor. You can also rebind any of the bindings without problem in the <code>bindings.json</code> file.</p>

                    <h4>Mouse Support</h4>
                    <p>Micro has full support for the mouse. This means you can click and drag to select text, double click select by word, and triple click to select by line.</p>

                    <h4>Terminal Emulator</h4>
                    <p>Run a real interactive shell from within micro. You could open up a split with code on one side and bash on the other -- all from within micro.</p>
                </div>

                <div>
                    
                    <p>And much more! Check out the full list of features <a href="https://github.com/zyedidia/micro#features">here</a> as well as the built-in help system also viewable online <a href="https://github.com/zyedidia/micro/tree/master/runtime/help">here</a>.</p>
                </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[China’s property giant Evergrande files for bankruptcy protection in Manhattan (313 pts)]]></title>
            <link>https://www.cnbc.com/2023/08/18/china-property-developer-evergrande-files-for-bankruptcy-protection-in-us.html</link>
            <guid>37171187</guid>
            <pubDate>Fri, 18 Aug 2023 03:43:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/08/18/china-property-developer-evergrande-files-for-bankruptcy-protection-in-us.html">https://www.cnbc.com/2023/08/18/china-property-developer-evergrande-files-for-bankruptcy-protection-in-us.html</a>, See on <a href="https://news.ycombinator.com/item?id=37171187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107278273" data-test="InlineImage"><p>A residential complex constructed by Evergrande in Huai'an, Jiangsu, China, on July 20, 2023.</p><p>Future Publishing | Future Publishing | Getty Images</p></div><div><p>China's heavily indebted property giant Evergrande Group on Thursday filed for Chapter 15 bankruptcy protection in a U.S. court.</p><p>In a filing to the Manhattan bankruptcy court, the firm referenced restructuring proceedings in Hong Kong, the Cayman Islands and the British Virgin Islands.</p><p>CNBC has reached out to Evergrande for comment but did not hear back.</p><p>The world's most indebted property developer <a href="https://www.cnbc.com/2023/07/18/chinas-evergrande-posts-81-billion-loss-over-the-past-two-years.html">defaulted in 2021</a> and announced an offshore debt restructuring program <a href="https://www1.hkexnews.hk/listedco/listconews/sehk/2023/0322/2023032201427.pdf" target="_blank">in March</a>. Trading of Evergrande shares have been suspended since March 2022.</p></div><div><p>The <a href="https://urldefense.com/v3/__https:/www.uscourts.gov/services-forms/bankruptcy/bankruptcy-basics/chapter-15-bankruptcy-basics__;!!PIZeeW5wscynRQ!obbh4pbt4o4Tm8TrYWi_1iL-lSb5iT0-Zup4wlPCG85TOUHwJIX4z52FDS68RauoDKWkX8XBze0tbunjMr27wjb1$" target="_blank">Chapter 15 bankruptcy</a>&nbsp;protection allows a U.S. bankruptcy court to intervene in cross-border insolvency case involving foreign companies that are undergoing restructuring from creditors. It aims to protect the debtors' assets and facilitate the rescue of businesses that are in financial trouble.</p><p>Tianji Holdings, an affiliate of Evergrande, and its subsidiary Scenery Journey, also filed for Chapter 15 protection in a Manhattan bankruptcy court, according to the filing.</p></div><h2><a id="headline0"></a>Property sector fallout</h2><div id="Placeholder-ArticleBody-Video-106949184" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000212054" aria-labelledby="Placeholder-ArticleBody-Video-106949184"><p><img src="https://image.cnbcfm.com/api/v1/image/106949189-GettyImages-1235417902.jpg?v=1632894181&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Chinese property giant Evergrande has a huge debt problem – here's why you should care"><span></span><span></span></p></div><div><p>China's massive real estate sector has long been<strong>&nbsp;</strong>a vital engine of growth for the world's second-largest economy, and accounts for as much as 30% of the country's gross domestic product.</p><p>Despite <a href="https://www.cnbc.com/2023/08/18/chinas-property-troubles-worsen-ramping-calls-for-bolder-policy-help.html">recent policy signals,</a> investor worries linger.&nbsp;In late July,&nbsp;<a href="https://www.cnbc.com/2023/07/26/china-signals-more-support-for-real-estate-with-a-big-change-in-tone.html">its top leaders indicated a shift</a>&nbsp;toward greater support for the property sector, paving the way for local governments to implement specific policies.</p><p>In July, <a href="https://www.cnbc.com/2023/07/18/chinas-evergrande-posts-81-billion-loss-over-the-past-two-years.html">Evergrande&nbsp;posted a combined loss</a> of $81 billion over the past two years, after struggling to finish projects and repay suppliers and lenders.</p><p>Net losses for 2021 and 2022 were 476 billion yuan ($66.36 billion) and 105.9 billion yuan ($14.76 billion), respectively, as a result of property write-downs, return of lands, losses on financial assets and financing costs, the company said.</p><p>The bankruptcy filing was signed by Jimmy Fong, who listed himself as a "foreign representative" of China Evergrande Group. A "scheme creditors" meting is set for Wednesday at the Hong Kong office of Sidley Austin, the U.S. based law firm representing Evergrande, the petition added.</p><p><em>— CNBC's Evelyn Cheng and Elliot Smith contributed to this story.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fp-go: Functional Programming Library for Golang (196 pts)]]></title>
            <link>https://github.com/IBM/fp-go</link>
            <guid>37171149</guid>
            <pubDate>Fri, 18 Aug 2023 03:33:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/IBM/fp-go">https://github.com/IBM/fp-go</a>, See on <a href="https://news.ycombinator.com/item?id=37171149">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Functional programming library for golang</h2>
<p dir="auto"><strong>🚧 Work in progress! 🚧</strong> Despite major version 1 because of <a data-error-text="Failed to load title" data-id="589023119" data-permission-text="Title is private" data-url="https://github.com/semantic-release/semantic-release/issues/1507" data-hovercard-type="issue" data-hovercard-url="/semantic-release/semantic-release/issues/1507/hovercard" href="https://github.com/semantic-release/semantic-release/issues/1507">semantic-release/semantic-release#1507</a>. Trying to not make breaking changes, but devil is in the details.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/IBM/fp-go/blob/main/resources/images/logo.png"><img src="https://github.com/IBM/fp-go/raw/main/resources/images/logo.png" alt="logo"></a></p>
<p dir="auto">This library is strongly influenced by the awesome <a href="https://github.com/gcanti/fp-ts">fp-ts</a>.</p>
<h2 tabindex="-1" dir="auto">Getting started</h2>
<div dir="auto" data-snippet-clipboard-copy-content="go get github.com/IBM/fp-go"><pre>go get github.com/IBM/fp-go</pre></div>
<p dir="auto">Refer to the <a href="https://github.com/IBM/fp-go/blob/main/samples">samples</a>.</p>
<h2 tabindex="-1" dir="auto">Design Goal</h2>
<p dir="auto">This library aims to provide a set of data types and functions that make it easy and fun to write maintainable and testable code in golang. It encourages the following patterns:</p>
<ul dir="auto">
<li>write many small, testable and pure functions, i.e. functions that produce output only depending on their input and that do not execute side effects</li>
<li>offer helpers to isolate side effects into lazily executed functions (IO)</li>
<li>expose a consistent set of composition to create new functions from existing ones
<ul dir="auto">
<li>for each data type there exists a small set of composition functions</li>
<li>these functions are called the same across all data types, so you only have to learn a small number of function names</li>
<li>the semantic of functions of the same name is consistent across all data types</li>
</ul>
</li>
</ul>
<h3 tabindex="-1" dir="auto">How does this play with the <a href="https://the-zen-of-go.netlify.app/" rel="nofollow">🧘🏽 Zen Of Go</a>?</h3>
<h4 tabindex="-1" dir="auto">🧘🏽 Each package fulfils a single purpose</h4>
<p dir="auto">✔️ Each of the top level packages (e.g. Option, Either, ReaderIOEither, ...) fulfils the purpose of defining the respective data type and implementing the set of common operations for this data type.</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Handle errors explicitly</h4>
<p dir="auto">✔️ The library makes a clear distinction between that operations that cannot fail by design and operations that can fail. Failure is represented via the <code>Either</code> type and errors are handled explicitly by using <code>Either</code>'s monadic set of operations.</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Return early rather than nesting deeply</h4>
<p dir="auto">✔️ We recommend to implement simple, small functions that implement one feature and that would typically not invoke other functions. Interaction with other functions is done by function composition and the composition makes sure to run one function after the other. In the error case the <code>Either</code> monad makes sure to skip the error path.</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Leave concurrency to the caller</h4>
<p dir="auto">✔️ All pure are synchronous by default. The I/O operations are asynchronous per default.</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Before you launch a goroutine, know when it will stop</h4>
<p dir="auto">🤷🏽 This is left to the user of the library since the library itself will not start goroutines on its own. The Task monad offers support for cancellation via the golang context, though.</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Avoid package level state</h4>
<p dir="auto">✔️ No package level state anywhere, this would be a significant anti-pattern</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Simplicity matters</h4>
<p dir="auto">✔️ The library is simple in the sense that it offers a small, consistent interface to a variety of data types. Users can concentrate on implementing business logic rather than dealing with low level data structures.</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Write tests to lock in the behaviour of your package’s API</h4>
<p dir="auto">🟡 The programming pattern suggested by this library encourages writing test cases. The library itself also has a growing number of tests, but not enough, yet. TBD</p>
<h4 tabindex="-1" dir="auto">🧘🏽 If you think it’s slow, first prove it with a benchmark</h4>
<p dir="auto">✔️ Absolutely. If you think the function composition offered by this library is too slow, please provide a benchmark.</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Moderation is a virtue</h4>
<p dir="auto">✔️ The library does not implement its own goroutines and also does not require any expensive synchronization primitives. Coordination of IO operations is implemented via atomic counters without additional primitives.</p>
<h4 tabindex="-1" dir="auto">🧘🏽 Maintainability counts</h4>
<p dir="auto">✔️ Code that consumes this library is easy to maintain because of the small and concise set of operations exposed. Also the suggested programming paradigm to decompose an application into small functions increases maintainability, because these functions are easy to understand and if they are pure, it's often sufficient to look at the type signature to understand the purpose.</p>
<p dir="auto">The library itself also comprises many small functions, but it's admittedly harder to maintain than code that uses it. However this asymmetry is intended because it offloads complexity from users into a central component.</p>
<h2 tabindex="-1" dir="auto">Comparation to Idiomatic Go</h2>
<p dir="auto">In this section we discuss how the functional APIs differ from idiomatic go function signatures and how to convert back and forth.</p>
<h3 tabindex="-1" dir="auto">Pure functions</h3>
<p dir="auto">Pure functions are functions that take input parameters and that compute an output without changing any global state and without mutating the input parameters. They will always return the same output for the same input.</p>
<h4 tabindex="-1" dir="auto">Without Errors</h4>
<p dir="auto">If your pure function does not return an error, the idiomatic signature is just fine and no changes are required.</p>
<h4 tabindex="-1" dir="auto">With Errors</h4>
<p dir="auto">If your pure function can return an error, then it will have a <code>(T, error)</code> return value in idiomatic go. In functional style the return value is <a href="https://pkg.go.dev/github.com/IBM/fp-go/either" rel="nofollow">Either[error, T]</a> because function composition is easier with such a return type. Use the <code>EitherizeXXX</code> methods in <a href="https://pkg.go.dev/github.com/IBM/fp-go/either" rel="nofollow">"github.com/IBM/fp-go/either"</a> to convert from idiomatic to functional style and <code>UneitherizeXXX</code> to convert from functional to idiomatic style.</p>
<h3 tabindex="-1" dir="auto">Effectful functions</h3>
<p dir="auto">An effectful function (or function with a side effect) is one that changes data outside the scope of the function or that does not always produce the same output for the same input (because it depends on some external, mutable state). There is no special way in idiomatic go to identify such a function other than documentation. In functional style we represent them as functions that do not take an input but that produce an output. The base type for these functions is <a href="https://pkg.go.dev/github.com/IBM/fp-go/io" rel="nofollow">IO[T]</a> because in many cases such functions represent <code>I/O</code> operations.</p>
<h4 tabindex="-1" dir="auto">Without Errors</h4>
<p dir="auto">If your effectful function does not return an error, the functional signature is <a href="https://pkg.go.dev/github.com/IBM/fp-go/io" rel="nofollow">IO[T]</a></p>
<h4 tabindex="-1" dir="auto">With Errors</h4>
<p dir="auto">If your effectful function can return an error, the functional signature is <a href="https://pkg.go.dev/github.com/IBM/fp-go/ioeither" rel="nofollow">IOEither[error, T]</a>. Use <code>EitherizeXXX</code> from <a href="https://pkg.go.dev/github.com/IBM/fp-go/ioeither" rel="nofollow">"github.com/IBM/fp-go/ioeither"</a> to convert an idiomatic go function to functional style.</p>
<h3 tabindex="-1" dir="auto">Go Context</h3>
<p dir="auto">Functions that take a <a href="https://pkg.go.dev/context" rel="nofollow">context</a> are per definition effectful because they depend on the context parameter that is designed to be mutable (it can e.g. be used to cancel a running operation). Furthermore in idiomatic go the parameter is typically passed as the first parameter to a function.</p>
<p dir="auto">In functional style we isolate the <a href="https://pkg.go.dev/context" rel="nofollow">context</a> and represent the nature of the effectful function as an <a href="https://pkg.go.dev/github.com/IBM/fp-go/ioeither" rel="nofollow">IOEither[error, T]</a>. The resulting type is <a href="https://pkg.go.dev/github.com/IBM/fp-go/context/readerioeither" rel="nofollow">ReaderIOEither[T]</a>, a function taking a <a href="https://pkg.go.dev/context" rel="nofollow">context</a> that returns a function without parameters returning an <a href="https://pkg.go.dev/github.com/IBM/fp-go/either" rel="nofollow">Either[error, T]</a>. Use the <code>EitherizeXXX</code> methods from <a href="https://pkg.go.dev/github.com/IBM/fp-go/context/readerioeither" rel="nofollow">"github.com/IBM/fp-go/context/readerioeither"</a> to convert an idiomatic go function with a <a href="https://pkg.go.dev/context" rel="nofollow">context</a> to functional style.</p>
<h2 tabindex="-1" dir="auto">Implementation Notes</h2>
<h3 tabindex="-1" dir="auto">Generics</h3>
<p dir="auto">All monadic operations are implemented via generics, i.e. they offer a type safe way to compose operations. This allows for convenient IDE support and also gives confidence about the correctness of the composition at compile time.</p>
<p dir="auto">Downside is that this will result in different versions of each operation per type, these versions are generated by the golang compiler at build time (unlike type erasure in languages such as Java of TypeScript). This might lead to large binaries for codebases with many different types. If this is a concern, you can always implement type erasure on top, i.e. use the monadic operations with the <code>any</code> type as if generics were not supported. You loose type safety, but this might result in smaller binaries.</p>
<h3 tabindex="-1" dir="auto">Ordering of Generic Type Parameters</h3>
<p dir="auto">In go we need to specify all type parameters of a function on the global function definition, even if the function returns a higher order function and some of the type parameters are only applicable to the higher order function. So the following is not possible:</p>
<div dir="auto" data-snippet-clipboard-copy-content="func Map[A, B any](f func(A) B) [R, E any]func(fa ReaderIOEither[R, E, A]) ReaderIOEither[R, E, B]"><pre><span>func</span> <span>Map</span>[<span>A</span>, <span>B</span> <span>any</span>](<span>f</span> <span>func</span>(<span>A</span>) <span>B</span>) [<span>R</span>, <span>E</span> <span>any</span>]<span>func</span>(<span>fa</span> <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>A</span>]) <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>B</span>]</pre></div>
<p dir="auto">Note that the parameters <code>R</code> and <code>E</code> are not needed by the first level of <code>Map</code> but only by the resulting higher order function. Instead we need to specify the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="func Map[R, E, A, B any](f func(A) B) func(fa ReaderIOEither[R, E, A]) ReaderIOEither[R, E, B]"><pre><span>func</span> <span>Map</span>[<span>R</span>, <span>E</span>, <span>A</span>, <span>B</span> <span>any</span>](<span>f</span> <span>func</span>(<span>A</span>) <span>B</span>) <span>func</span>(<span>fa</span> <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>A</span>]) <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>B</span>]</pre></div>
<p dir="auto">which overspecifies <code>Map</code> on the global scope. As a result the go compiler will not be able to auto-detect these parameters, it can only auto detect <code>A</code> and <code>B</code> since they appear in the argument of <code>Map</code>. We need to explicitly pass values for these type parameters when <code>Map</code> is being used.</p>
<p dir="auto">Because of this limitation the order of parameters on a function matters. We want to make sure that we define those parameters that cannot be auto-detected, first, and the parameters that can be auto-detected, last. This can lead to inconsistencies in parameter ordering, but we believe that the gain in convenience is worth it. The parameter order of <code>Ap</code> is e.g. different from that of <code>Map</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="func Ap[B, R, E, A any](fa ReaderIOEither[R, E, A]) func(fab ReaderIOEither[R, E, func(A) B]) ReaderIOEither[R, E, B]"><pre><span>func</span> <span>Ap</span>[<span>B</span>, <span>R</span>, <span>E</span>, <span>A</span> <span>any</span>](<span>fa</span> <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>A</span>]) <span>func</span>(<span>fab</span> <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>func</span>(<span>A</span>) <span>B</span>]) <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>B</span>]</pre></div>
<p dir="auto">because <code>R</code>, <code>E</code> and <code>A</code> can be determined from the argument to <code>Ap</code> but <code>B</code> cannot.</p>
<h3 tabindex="-1" dir="auto">Use of the <a href="https://go.googlesource.com/proposal/+/master/design/47781-parameterized-go-ast.md" rel="nofollow">~ Operator</a></h3>
<p dir="auto">The FP library attempts to be easy to consume and one aspect of this is the definition of higher level type definitions instead of having to use their low level equivalent. It is e.g. more convenient and readable to use</p>

<p dir="auto">than</p>
<div dir="auto" data-snippet-clipboard-copy-content="func(R) func() Either.Either[E, A]"><pre><span>func</span>(<span>R</span>) <span>func</span>() Either.<span>Either</span>[<span>E</span>, <span>A</span>]</pre></div>
<p dir="auto">although both are logically equivalent. At the time of this writing the go type system does not support generic type aliases, only generic type definition, i.e. it is not possible to write:</p>
<div dir="auto" data-snippet-clipboard-copy-content="type ReaderIOEither[R, E, A any] = RD.Reader[R, IOE.IOEither[E, A]]"><pre><span>type</span> <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>A</span> <span>any</span>] <span>=</span> RD.<span>Reader</span>[<span>R</span>, <span>IOE</span>.<span>IOEither</span>[<span>E</span>, <span>A</span>]]</pre></div>
<p dir="auto">only</p>
<div dir="auto" data-snippet-clipboard-copy-content="type ReaderIOEither[R, E, A any] RD.Reader[R, IOE.IOEither[E, A]]"><pre><span>type</span> <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>A</span> <span>any</span>] RD.<span>Reader</span>[<span>R</span>, <span>IOE</span>.<span>IOEither</span>[<span>E</span>, <span>A</span>]]</pre></div>
<p dir="auto">This makes a big difference, because in the second case the type <code>ReaderIOEither[R, E, A any]</code> is considered a completely new type, not compatible to its right hand side, so it's not just a shortcut but a fully new type.</p>
<p dir="auto">From the implementation perspective however there is no reason to restrict the implementation to the new type, it can be generic for all compatible types. The way to express this in go is the <a href="https://go.googlesource.com/proposal/+/master/design/47781-parameterized-go-ast.md" rel="nofollow">~</a> operator. This comes with some quite complicated type declarations in some cases, which undermines the goal of the library to be easy to use.</p>
<p dir="auto">For that reason there exist sub-packages called <code>Generic</code> for all higher level types. These packages contain the fully generic implementation of the operations, preferring abstraction over usability. These packages are not meant to be used by end-users but are meant to be used by library extensions. The implementation for the convenient higher level types specializes the generic implementation for the particular higher level type, i.e. this layer does not contain any business logic but only <em>type magic</em>.</p>
<h3 tabindex="-1" dir="auto">Higher Kinded Types</h3>
<p dir="auto">Go does not support higher kinded types (HKT). Such types occur if a generic type itself is parametrized by another generic type. Example:</p>
<p dir="auto">The <code>Map</code> operation for <code>ReaderIOEither</code> is defined as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="func Map[R, E, A, B any](f func(A) B) func(fa ReaderIOEither[R, E, A]) ReaderIOEither[R, E, B]"><pre><span>func</span> <span>Map</span>[<span>R</span>, <span>E</span>, <span>A</span>, <span>B</span> <span>any</span>](<span>f</span> <span>func</span>(<span>A</span>) <span>B</span>) <span>func</span>(<span>fa</span> <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>A</span>]) <span>ReaderIOEither</span>[<span>R</span>, <span>E</span>, <span>B</span>]</pre></div>
<p dir="auto">and in fact the equivalent operations for all other mondas follow the same pattern, we could try to introduce a new type for <code>ReaderIOEither</code> (without a parameter) as a HKT, e.g. like so (made-up syntax, does not work in go):</p>
<div dir="auto" data-snippet-clipboard-copy-content="func Map[HKT, R, E, A, B any](f func(A) B) func(HKT[R, E, A]) HKT[R, E, B]"><pre><span>func</span> <span>Map</span>[<span>HKT</span>, <span>R</span>, <span>E</span>, <span>A</span>, <span>B</span> <span>any</span>](<span>f</span> <span>func</span>(<span>A</span>) <span>B</span>) <span>func</span>(<span>HKT</span>[<span>R</span>, <span>E</span>, <span>A</span>]) <span>HKT</span>[<span>R</span>, <span>E</span>, <span>B</span>]</pre></div>
<p dir="auto">this would be the completely generic method signature for all possible monads. In particular in many cases it is possible to compose functions independent of the concrete knowledge of the actual <code>HKT</code>. From the perspective of a library this is the ideal situation because then a particular algorithm only has to be implemented and tested once.</p>
<p dir="auto">This FP library addresses this by introducing the HKTs as individual types, e.g. <code>HKT[A]</code> would be represented as a new generic type <code>HKTA</code>. This loses the correlation to the type <code>A</code> but allows to implement generic algorithms, at the price of readability.</p>
<p dir="auto">For that reason these implementations are kept in the <code>internal</code> package. These are meant to be used by the library itself or by extensions, not by end users.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microplastics detected in the marine air from Norway to the high Arctic (102 pts)]]></title>
            <link>https://scienceswitch.com/2023/08/18/microplastics-detected-in-the-marine-air-from-norway-to-the-high-arctic/</link>
            <guid>37170911</guid>
            <pubDate>Fri, 18 Aug 2023 02:47:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scienceswitch.com/2023/08/18/microplastics-detected-in-the-marine-air-from-norway-to-the-high-arctic/">https://scienceswitch.com/2023/08/18/microplastics-detected-in-the-marine-air-from-norway-to-the-high-arctic/</a>, See on <a href="https://news.ycombinator.com/item?id=37170911">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

							<figure data-shortcode="caption" id="attachment_41361" aria-describedby="caption-attachment-41361"><img data-attachment-id="41361" data-permalink="https://scienceswitch.com/2023/08/18/microplastics-detected-in-the-marine-air-from-norway-to-the-high-arctic/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources-2/" data-orig-file="https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg" data-orig-size="1104,621" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Quantitative chemical analysis finds microplastics are present even in Arctic air, originating from diverse land and ocean sources" data-image-description="<p>Quantitative chemical analysis finds microplastics are present even in Arctic air, originating from diverse land and ocean sources</p>
" data-image-caption="" data-medium-file="https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg?w=300" data-large-file="https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg?w=616" src="https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg?w=616" alt="Quantitative chemical analysis finds microplastics are present even in Arctic air, originating from diverse land and ocean sources" srcset="https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg 1104w, https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg?w=150&amp;h=84 150w, https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg?w=300&amp;h=169 300w, https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg?w=768&amp;h=432 768w, https://scienceswitch.files.wordpress.com/2023/08/quantitative-chemical-analysis-finds-microplastics-are-present-even-in-arctic-air-originating-from-diverse-land-and-ocean-sources.jpg?w=1024&amp;h=576 1024w" sizes="(max-width: 1104px) 100vw, 1104px"><figcaption id="caption-attachment-41361">Quantitative chemical analysis finds microplastics are present even in Arctic air, originating from diverse land and ocean sources</figcaption></figure>
<p>You’d think the crisp ocean air would be pristine, even far from shore. But alarmingly, scientists have discovered microplastic particles floating thousands of miles from land.</p>
<p>An investigative team from Germany and Norway took air samples along the Norwegian coast up to the remote Arctic. Using specialized equipment aboard a research vessel, they measured plastic particles 12 meters above the sea surface. Their findings, published in <a href="https://www.nature.com/articles/s41467-023-39340-5"><em>Nature Communications</em></a>, reveal microplastics are ubiquitous – even in the planet’s most isolated polar regions.</p>
<h2>Quantifying Plastics in the Marine Atmosphere</h2>
<p>Back on shore, the researchers used an advanced technique called <a href="https://www.emsl.pnnl.gov/science/instruments-resources/pyrolysis-gas-chromatography-mass-spectrometry" target="_blank" rel="noopener">pyrolysis-gas chromatography-mass spectrometry</a> to identify and quantify different polymer types. This involved thermally degrading the samples and analyzing the gas byproducts.</p>
<p>The results showed a concerning mixture of microplastics. Polyester fibers shed likely from clothing were present in all samples. Tiny tire rubber particles also constituted a major source, worn off from driving and braking. Measured concentrations reached up to 37.5 nanograms (that’s a billionth of a gram!) per cubic meter of air.</p>
<h2>Sources Both On Land and Sea</h2>
<p>To elucidate the sources, the team conducted air mass back trajectory modeling. Their <a href="https://uol.de/en/news/article/how-microplastics-end-up-in-the-marine-atmosphere-8176" target="_blank" rel="noopener">analyses indicate</a> microplastics originate from direct terrestrial sources as well as re-emission from the ocean itself. Bits of plastic debris floating on the sea surface get ejected back into the air through wave turbulence and bubble bursting.</p>
<p>Chemicals specific to marine coatings were also detected, implicating sea vessels as contributors through painted surfaces weathering in the harsh open ocean environment.</p>
<h2>A Complex Picture of Pervasive Plastic</h2>
<p>While further research is urgently needed, these unprecedented findings provide initial insight into microplastics circulating through the atmosphere worldwide. From populated coasts to the furthest Arctic reaches, the results reveal a complex portrait of plastic particles aloft across our interconnected oceans.</p>
<p>Ultimately, mitigating this pollution will require reducing plastic waste generation and improving recycling. In the meantime, take a deep breath of seaside air, but be aware you may be inhaling a tiny bit of discarded plastic as well.</p>
<hr>
<p><strong>FAQs</strong></p>
<h2>How small are microplastics?</h2>
<p>Microplastics are less than <a href="https://oceanservice.noaa.gov/facts/microplastics.html" target="_blank" rel="noopener">5 mm in size</a> – smaller than a sesame seed. The ones found in this study were just a few thousandths of a millimeter.</p>
<h2>How do microplastics get into the air?</h2>
<p>Wave action and bubbles can fling bits of plastic debris from the ocean surface into the air. Rain washes plastic from land into the sea, where it can then be tossed back up. Winds also carry plastics long distances.</p>
<h2>Are microplastics only in the oceans?</h2>
<p>No, they are transported all over the planet. Plastics have been found in Arctic snow, suggesting they can travel through the atmosphere to remote regions.</p>
<h2>What can be done about microplastics?</h2>
<p>Preventing plastic waste is key. Individuals can cut down on single-use plastics, while companies and governments need to prioritize sustainability. Better recycling, waste management and reducing plastic production are also important solutions.</p>

			
			
							
						</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GIL removal and the Faster CPython project (274 pts)]]></title>
            <link>https://lwn.net/Articles/939981/</link>
            <guid>37170771</guid>
            <pubDate>Fri, 18 Aug 2023 02:20:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/939981/">https://lwn.net/Articles/939981/</a>, See on <a href="https://news.ycombinator.com/item?id=37170771">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
           <div><b>Please consider subscribing to LWN</b><p>Subscriptions are the lifeblood of LWN.net.  If you appreciate this
       content and would like to see more of it, your subscription will
       help to ensure that LWN continues to thrive.  Please visit
       <a href="https://lwn.net/subscribe/">this page</a> to join up and keep LWN on
       the net.</p></div>
           </center>
           
<p>
The Python global interpreter lock (GIL) has long been a barrier to
increasing the performance of programs by using multiple threads—the GIL
serializes access to the interpreter's virtual machine such that only one thread
can be executing Python code at any given time.  There are other mechanisms
to provide 
concurrency for the language, but the specter of the GIL—and its reality as
well—have often been cited as a major negative for Python.  Back in October
2021, Sam Gross <a href="https://lwn.net/ml/python-dev/CAGr09bSrMNyVNLTvFq-h6t38kTxqTXfgxJYApmbEWnT71L74-g@mail.gmail.com/">introduced</a>
a <a href="https://lwn.net/Articles/872869/">proof-of-concept, no-GIL version of the
language</a>. It was met with a lot of excitement at the time, but
seemed to languish to a certain extent for more than a year; now, the Python
Steering 
Council has <a href="https://lwn.net/Articles/939568/">announced its intent to accept the
no-GIL feature</a>.  It will still be some time before it lands in a
released Python version—and there is the possibility that it all has to be
rolled back at some point—but there are several companies backing the
effort, which gives it all a good chance to succeed.
</p>

<p>
After its introduction in&nbsp;2021, and the discussion around that, the next public
appearance for the feature was at the <a href="https://pyfound.blogspot.com/2022/05/the-2022-python-language-summit_01678898482.html">2022
Python Language Summit</a> in April. Gross gave a <a href="https://pyfound.blogspot.com/2022/05/the-2022-python-language-summit-python_11.html">talk</a> about
his <a href="https://github.com/colesbury/nogil">no-GIL fork</a> in the
hopes of getting some tacit agreement on proceeding with the work.  That
agreement 
was not forthcoming, in part because the full details and implications of
a no-GIL interpreter were not really known.
Meanwhile, the Faster CPython project, which <a href="https://lwn.net/Articles/857754/">came about in mid-2021</a> had been working along
on its plan to increase the <i>single-threaded</i> performance of the
interpreter. 
Mark Shannon <a href="https://pyfound.blogspot.com/2022/05/the-2022-python-language-summit_2.html">reported
on the status of that effort</a> at the&nbsp;2022 summit as well.  He also
authored <a href="https://peps.python.org/pep-0659/">PEP&nbsp;659</a>
("Specialized Adaptive Interpreter") that describes the kinds of changes
being made, some of which have found their way into Python&nbsp;3.10
and&nbsp;3.11. 
</p>

<p>
At this year's PyCon, two of the Faster CPython team gave talks describing
the techniques they have been using to improve the performance of the
interpreter: Brandt Bucher <a href="https://lwn.net/Articles/930705/">looked at adaptive
instructions</a>, while Shannon  <a href="https://lwn.net/Articles/931197/">described memory layout improvements and other
optimizations</a>. 
Given the GIL, nearly all existing Python
programs are single threaded, so improving the performance of
those programs will effectively speed up the entire Python world.  One of
the concerns that has been heard about no-GIL Python is what its impact on
single-threaded programs would be.
</p>

<h4>PEP 703</h4>

<p>
In January 2023, Łukasz Langa <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional/22606">posted</a>
the first version of <a href="https://peps.python.org/pep-0703/">PEP&nbsp;703</a> ("Making the
Global Interpreter Lock Optional in CPython") that is authored by Gross;
Langa is sponsoring the PEP 
as a core developer.  As might be guessed, that set off a lengthy thread,
with, once again, a lot of excitement.  There were also some concerns expressed
with regard to the implications of not having a GIL, especially for Python
extensions written in C; since the GIL protects that code from 
many concurrency problems, removing it might well lead to bugs. 
</p> 

<p>
One
thing that everyone wants to avoid is another "flag day" transition like
that of Python&nbsp;2 to&nbsp;3.  The huge and unfortunate impact of
Python&nbsp;3 being incompatible with its predecessor was not foreseen—the
core developers vastly underestimated the growing popularity of the
language, for one thing—but that mistake will not be repeated.  Any switch
to remove the GIL will need to smoothly work with code that is not (yet)
ready for it.
</p>

<p>
There was a <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional/22606/25">question</a>
from Shannon about "<q>what people think is a acceptable slowdown for
single-threaded code</q>".  To a large extent, that question went
unanswered in the thread, but he had estimated an impact "<q>in the 15-20%
range, but it could be more, depending on the impact on PEP 659</q>".
</p>

<p>
Another Faster CPython team member, Eric Snow, <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional/22606/84">posted</a>
a lengthy analysis with a bunch of questions, which he summarized as:
"<q>tl;dr I'm really excited by this proposal but have significant
concerns, which I genuinely hope the PEP can address.</q>"  He noted that
he was the author of a "competing" concurrency option in <a href="https://peps.python.org/pep-0684/">PEP&nbsp;684</a> ("A
Per-Interpreter GIL"), along with the related <a href="https://peps.python.org/pep-0683/">PEP&nbsp;683</a> ("Immortal
Objects, Using a Fixed Refcount"), though he does not truly see multiple
sub-interpreters, each with their own GIL, as being incompatible with the
no-GIL work.  Much of his concern was focused on the impacts on the C
extensions (which is also a problem for PEP&nbsp;684, though to a lesser
extent), but single-threaded performance was also mentioned.  Gross <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional/22606/97">replied</a>
that the impact on the extensions was not completely negative:
</p><blockquote>
There are also substantial benefits to extension module maintainers. The
PEP includes quotes from a number of maintainers of widely used C API
extensions who suffer the complexity of working around the GIL. For
example, Zachary DeVito, PyTorch core developer, wrote "On three separate
occasions in the past couple of months… I spent an order-of-magnitude more
time figuring out how to work around GIL limitations than actually solving
the particular problem." 
</blockquote>


<h4>Updated PEP</h4>

<p>
The thread had mostly run its course by the end of January.
In early May, Gross <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503">posted</a>
an updated version of PEP&nbsp;703, along with an <a href="https://github.com/colesbury/nogil-3.12">implementation based on the
in-progress Python&nbsp;3.12</a>.  There was just one <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/2">response</a>
early on 
(which Gross <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/3">replied
to</a>).  On May&nbsp;12, Gross <a href="https://github.com/python/steering-council/issues/188">asked the
steering council to decide on the PEP</a>.  As it turned out, there was
still a lot more discussion to go before any decision would be made.
</p>

<p>
On June 2, Shannon <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/9">posted
a performance assessment</a> of the PEP with some pretty eye-opening
numbers (that were disputed) on the impact of the changes; his estimates
of the impact ranged from&nbsp;11 to&nbsp;30%.
He also noted that removing the GIL had some negative impacts on the
existing and planned Faster CPython work:
</p><blockquote>
The adaptive specializing interpreter relies on the GIL; it is not
thread-friendly. 
If NoGIL is accepted, then some redesign of our optimization strategy will
be necessary. 
While this is perfectly possible, it does have a cost.
The effort spent on this redesign and resulting implementation is not being
spent on actual optimizations. 
</blockquote>


<p>
Shannon has noted that he is not a fan of the free-threading, shared-memory
concurrency model; his assessment ends with a suggestion that
sub-interpreters provide a better concurrency solution with fewer of the
performance and other concerns that no-GIL brings.  Others, <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/13">including
steering council member Gregory P. Smith</a> found that analysis to be
somewhat oversimplified.  Langa <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/17">posted
benchmark numbers</a> that showed considerably less impact than Shannon's
estimates. Langa <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/18">followed
that up</a> with some additional results that correspond closely with what
Gross had reported in the PEP.
</p>

<p>
Guido van Rossum, who  heads up the Faster CPython team, wanted to
ensure that everyone 
learned from the mistakes made in the past:
</p><blockquote>
If there's one lesson we've learned from the Python 2 to 3 transition, it's
that it would have been very beneficial if Python 2 and 3 code could
<i>coexist</i> in the same Python interpreter. We blew it that time, and it
set us 
back by about a decade. 
<p>
Let's not blow it this time. If we're going forward with nogil (and I'm not
saying we are, but I can't exclude it), let's make sure there is a way to
be able to import extensions requiring the GIL in a nogil interpreter
without any additional shenanigans – neither the application code nor the
extension module should have to be modified in any way [...]
</p></blockquote>


<p>
Meanwhile, Smith replied to Gross's steering-council request (and <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/20">copied
it to the forum thread</a>): 
</p><blockquote>
The steering council is going to take its time on this. A huge thank you
for working to keep it up to date! We're not ready to simply pronounce on
703 as it has a HUGE blast radius. 
<p>
[...] That does not mean "no" to this. There is demand for it. (<i>personally,
I've wanted this since forever!</i>) It's just that it won't be easy and we'll
need to consider the entire ecosystem and how to smoothly allow such a
change to happen without breaking the world. 
</p><p>
I'm glad to see the continued discuss thread with faster-cpython folks in
particular piping up. The intersection between this work and ongoing single
threaded performance improvements will always be high and we don't want to
hamper that in the near term. 
</p></blockquote>


<p>
Gross <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/28">largely
disagreed</a> with Shannon's assessment and, in particular, with his
characterization of threading.  He was also, seemingly, <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/43">somewhat
unhappy</a> with Smith's reply:
</p><blockquote>
You wrote that the Steering Council's decision does not mean "no," but the
steering council has not set a bar for acceptance, stated what evidence is
actually needed, nor said when a final decision will be made. Given the
expressed demand for PEP 703, it makes sense to me 
for the steering committee to develop a timeline for identifying the
factors it may need to consider and for determining the steps that would be
required for the change to happen smoothly. 
<p>
Without these timelines and milestones in place, I would like to explain
that the effect of the Steering Council's answer is a "no" in practice. I
have been funded to work on this for the past few years with the milestone
of submitting the PEP along with a comprehensive implementation to convince
the Python community. Without specific concerns or a clear bar for
acceptance, I (and my funding organization) will have to treat the current
decision-in-limbo as a "no" and will be unable to pursue the PEP further. 
</p></blockquote>


<p>
That obviously put pressure on the council, as did the users who were
clamoring for a no-GIL Python, but the decision is clearly not a simple
one.  On June&nbsp;14, more pressure was applied from the Faster CPython
team.  Van Rossum <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional-3-12-updates/26503/118">described</a>
some of
the costs of no-GIL, but also expressed concern about waiting for a
decision:
</p><blockquote>
We've had a group discussion about how our work would be affected by free
threading. Our key conclusion is that merging nogil will set back our
current results by a significant amount of time, and in addition will
reduce our velocity in the future. We don't see this as a reason to reject
nogil – it's just a new set of problems we would have to overcome, and we
expect that our ultimate design would be quite different as a result. But
there is a significant cost, and it's not a one-time cost. We could use
help from someone (Sam?) who has experience thinking about the problems
posed by the new environment. 
<p>
[...] In the meantime we're treading water, unsure whether to put our
efforts in continuing with the current plan, or in designing a new,
thread-safe optimization architecture. 
</p></blockquote>


<h4>Fast, free threading</h4>

<p>
The next day, Shannon <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903">started
a new thread</a> (titled: "A fast, free threading Python") that described
three possible options for a way forward.  It started with a lengthy
description of the tradeoffs for optimization of a dynamic language like
Python. Of the three aspects that he thinks need to be considered,
single-threaded performance, parallelism, and mutability, the last has
mostly been glossed over in earlier discussions, "<q>but it is key</q>":
</p><blockquote>
It isn't quite:
<blockquote>
    Performance, parallelism, mutability: pick two.
</blockquote>
but more like:
<blockquote>
    Performance, parallelism, mutability: pick one to restrict.
</blockquote>
</blockquote>


<p>
He also cautioned that there are some unknowns:
</p><blockquote>
Performing the optimizations necessary to make Python fast in a
free-threading environment will need some original research. That makes it
more costly and a lot more risky. 
</blockquote>


<p>
The options for the steering council amount to choosing a fast
single-threaded interpreter as currently planned, a no-GIL free-threading
interpreter with an unknown (but non-zero) impact on single-threaded
performance, or both at the same time.  His preference is for both,
but he is concerned that the council might choose no-GIL without also
committing to the rest of the work needed:
</p><blockquote>
Please don't choose option 2 [no-GIL] hoping that we will get option 3
[both], because 
"someone will sort out the performance issue". They won't, unless the
resources are there. 
<p>
If we must choose option 1 [current Faster CPython plans] or 2, then I
think it has to be option 1. 
It gives us a speedup at much lower cost in CPUs and energy,
by doing things more efficiently rather than just throwing lots of cores at
the problem. 
</p></blockquote>


<p>
Marc-André Lemburg <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/2">asked</a>
about a phased approach, where, effectively, GIL or no-GIL were chosen at
the command line; over time, the two could slowly be merged. "<q>Or would
this not be feasible because the 'slow merge' would actually require
redesigning the whole specialization approach?</q>"  Smith <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/30">replied</a> 
that he thinks that is more or less what PEP&nbsp;703 is proposing; even
though Shannon basically recommended against it, Smith thinks pursuing
both at once is possible:
</p><blockquote>
I'd more or less expect work on specialization for to proceed in parallel
without worrying if those benefits cannot yet be available in a free
threaded build for a few of releases. Turning it mostly into an additional
code maintenance and test matrix burden on the CPython core dev side to
keep both our still-primary single threaded GIL based interpreter and the
experimental free threaded build working. 
<p>
I figure this is basically exactly what Mark claims <i>not</i> to
want. Presumably 
due to the interim added build and maintenance complexity. But also seems
like the most likely way to get to his "both" option 3 that I suspect we
all <i>magically wish would just happen</i>. 
</p></blockquote>


<p>
Smith <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/31">followed
that up</a> by noting that free threading will need to addressed at some
point; even if the Faster CPython plans work out and Python&nbsp;3.15 is
five times faster than Python&nbsp;3.10, nobody will "<q>be satisfied at 'just
5x' in the end</q>".   Van Rossum <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/34">agreed</a>,
but was also concerned that the council "<q>might be betting on hope as a
strategy</q>" by choosing no-GIL and hoping for the best.
</p><blockquote>
Like Mark, I hope that you're choosing (3) – like Mark says, it's clearly
the best option. But we will need to be honest about it, and accept that we
need more resources to improve single-threaded performance. (And, as I
believe someone already pointed out, it will also be harder to do future
maintenance on CPython's C code, since so much of it is now exposed to
potential race conditions. This is a problem for a language that's for a
large part maintained by volunteers.) 
</blockquote>


<p>
The talk of "more resources" led Itamar Oren to 
<a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/41">wonder</a>
what that means: "<q>It's not clear to me to what extent the SC
[steering council] is in a position to tie PEP acceptance or rejection to
allocation of funding.</q>"  Van Rossum <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/45">replied</a>
that Microsoft was committed to continue funding the team and that "<q>our
charter 
is not limited to single-threaded performance work</q>", but that there is
extra work to do in a no-GIL world:
</p><blockquote>
Meanwhile, we can start adapting the specialization and optimization work
to a no-GIL world, with the goal of obtaining Mark's Option 3 (free
threading and faster per-thread performance). Ideally we would reach a
state where we can make no-GIL the one and only build mode without a drop
in single-threaded performance (important for apps that haven't been
re-architected, e.g. apps that currently use multi-processing, or
algorithms that are hard to parallelize). 
<p>
It is this latter step (getting to Option 3) that requires extra resources
– for example, it would be great if Meta or another tech company could
spare some engineers with established CPython internals experience to help
the core dev team with this work.
</p><p>
Finally, I want to re-emphasize that while Microsoft has a team using the
Faster CPython moniker, we don't intend to own CPython performance – we
believe in good citizenship and want to contribute in a way that puts our
skills and experience to the best possible use for the Python community. 
</p></blockquote>


<p>
Van Rossum did not just choose Meta out of a hat, here; Gross works for the
company, which presumably funded his no-GIL work, and the <a href="https://github.com/facebookincubator/cinder">Cinder</a> CPython fork
is maintained by a team at Meta.  Carl Meyer <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/46">said</a>
that he expected the Cinder team to work on no-GIL Python.  In fact, on
July&nbsp;7, Meyer <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/99">announced</a>
that Meta would fund work on the no-GIL interpreter:
</p><blockquote>
If PEP 703 is accepted, Meta can commit to support in the form of three
engineer-years (from engineers experienced working in CPython internals)
between the acceptance of PEP 703 and the end of 2025, to collaborate with
the core dev team on landing the PEP 703 implementation smoothly in CPython
and on ongoing improvements to the compatibility and performance of nogil
CPython. 
</blockquote>


<p>
On July 19, Anaconda <a href="https://discuss.python.org/t/pep-703-making-the-global-interpreter-lock-optional/22606/121">followed
suit</a>. Stan Seibert said that the company would fund work on the
"<q>packaging challenges that will be associated with adopting PEP 703,
including any work on pip, cibuildwheel, and conda-forge that will be
needed to get nogil-compatible packages into the hands of the Python
community</q>".  Some of that funding commitment likely helped the council
reach a verdict, but the results of a <a href="https://discuss.python.org/t/poll-feedback-to-the-sc-on-making-cpython-free-threaded-and-pep-703/28540">core-developer
poll on no-GIL</a> also pushed the council in the direction of accepting the
PEP.   That poll showed 87% of 46 voters thought that free-threaded Python
should be actively pursued and 63% of 38 voters said that they were willing
to help support and maintain a no-GIL Python based on PEP&nbsp;703.
</p>

<h4>Steering council decision</h4>

<p>
On July 28, council member Thomas Wouters <a href="https://discuss.python.org/t/a-steering-council-notice-about-pep-703-making-the-global-interpreter-lock-optional-in-cpython/30474">announced</a>
that the council would be accepting PEP&nbsp;703, though it was "<q>still
working on the acceptance details</q>".  The idea would be to introduce the
no-GIL version of the interpreter in order to give everyone a chance to
figure out what pieces are missing, so that they can be filled in before
no-GIL becomes the default and, eventually, the only, version of Python.
The time frame for that transition is estimated to be around five years,
but there will be no repeat of earlier mistakes:
</p><blockquote>
We do not want another Python 3 situation, so any changes in third-party
code needed to accommodate no-GIL builds should just work in with-GIL
builds (although backward compatibility with older Python versions will
still need to be addressed). This is not Python 4. We are still considering
the requirements we want to place on ABI compatibility and other details
for the two builds and the effect on backward compatibility. 
</blockquote>


<p>
As was noted in the various discussions, there is more to removing the GIL
than simply adopting a PEP.  Wouters made it clear that the core developers
will need to gain experience with no-GIL Python so that they can lead the
rest of the community:
</p><blockquote>
We will probably need to figure out new C APIs and Python APIs as we sort
out thread safety in existing code. We also need to bring along the rest of
the Python community as we gain those insights and make sure the changes we
want to make, and the changes we want them to make, are palatable. 
</blockquote>


<p>
If the Python community finds that the switch is "<q>just
going to be too disruptive for too little gain</q>", the council wants to
be able to change its mind anytime before declaring no-GIL as the default
mode for the language.  He outlined the steps that the council sees,
starting with a short-term (perhaps for Python&nbsp;3.13, which is due in
October 2024) experimental no-GIL build of the interpreter that core
developers and others can try out.  In the medium term, no-GIL would be a
supported option, but not the default; when that happens depends a lot on
how quickly the community adopts and supports the no-GIL build.  In the
long term, 
no-GIL would be the default build and the GIL would be completely excised
("<q>without unnecessarily breaking backward compatibility</q>").   Along
the way, periodic reviews will be needed:
</p><blockquote>
Throughout the process we (the core devs, not just the SC) will need to
re-evaluate the progress and the suggested timelines. We don't want this to
turn into another ten year backward compatibility struggle, and we want to
be able to call off PEP 703 and find another solution if it looks to become
problematic, and so we need to regularly check that the continued work is
worth it. 
</blockquote>


<p>
As might be guessed, that spawned multiple congratulatory and
excited-for-the-future responses, though there are a few who think that
keeping the GIL would be a better choice for the language.  The
announcement presumably 
also sent the Faster CPython folks back to their drawing boards; though
there were some accusations of turf wars in the discussions, that did not
really seem to be the case.  The Faster CPython team simply wanted to
ensure that all of the costs were taken into consideration;  overall, the
team seems quite excited to work on surmounting the challenges of producing
a no-GIL 
interpreter, with minimal (or, ideally, no) performance impact on
single-threaded 
code.
</p>

<p>
It is quite a turning point in the history of the language, but the work is
(obviously) not done yet.  There is a huge amount of researching, coding,
testing, experimenting,
documenting, and so on between here and a no-GIL-only version of the language
in, say, Python&nbsp;3.17 in October&nbsp;2028. One guesses that the work
will not be done, then, either—there will be more optimizations to be
found and applied if there is still funding available to do so.
Meanwhile, we have yet to dig into the details of the PEP itself; that will
come soon. We will be keeping an eye on the no-GIL development process as
it plays 
out over the coming years as well.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/PythonIndex/">Python</a></td><td><a href="https://lwn.net/Archives/PythonIndex/#CPython">CPython</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/PythonIndex/">Python</a></td><td><a href="https://lwn.net/Archives/PythonIndex/#Global_interpreter_lock_GIL">Global interpreter lock (GIL)</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/PythonIndex/">Python</a></td><td><a href="https://lwn.net/Archives/PythonIndex/#Python_Enhancement_Proposals_PEP-PEP_703">Python Enhancement Proposals (PEP)/PEP 703</a></td></tr>
            </tbody></table><br clear="all">
<hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/939981/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FreeBSD Bhyve Virtualization (128 pts)]]></title>
            <link>https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/</link>
            <guid>37170683</guid>
            <pubDate>Fri, 18 Aug 2023 02:05:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/">https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/</a>, See on <a href="https://news.ycombinator.com/item?id=37170683">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>The Bhyve FreeBSD hypervisor (called/spelled <i>‘beehive’</i> usually) was created almost 10 years ago. Right now it offers speed and features that other similar solutions provide – such as KVM/VMware/XEN. You can check all the details in the <a href="https://freebsd.org/handbook">FreeBSD Handbook</a> for details. One of the last things Bhyve lacks is so called live migration between physical hosts but save state and resume from saved state are in the works currently so not long before that live migration. Up until recently I used mostly VirtualBox for my small virtualization needs. Recently I started to evaluate Bhyve and this time I am very pleased – the FreeBSD VirtualBox integration is not perfect anyway – for example – the USB passthru does not work since several years – and even when it worked – it was limited to USB 1.x speeds only. Also because of FreeBSD policy of <tt><b>pkg(8)</b></tt> packages building process – the VirtualBox packages remain broken for 3 months after each *.1 or upper release (*.2/*.3/…). The other impulse that forced me to switch from VirtualBox to Bhyve was the VirtualBox (in)stability. I often needed to restart crashed VirtualBox VMs because they failed for some unspecified reason.</p>
<p>One of the Bhyve features that I especially liked was that by default Bhyve only uses memory that guest system wanted to allocate. For example a FreeBSD virtual machine with <strong>2 GB RAM</strong> set will use after boot about <strong>70 MB RAM</strong> 🙂</p>
<p>Another great feature I really liked about Bhyve was that I could suspend the host machine with all the VMs started – both on my <a href="https://vermaden.wordpress.com/2022/04/14/freebsd-13-1-on-thinkpad-w520/">ThinkPad W520</a> laptop and <a href="https://vermaden.wordpress.com/2023/07/30/amd-based-freebsd-desktop/">AMD Based FreeBSD Desktop</a> and then it all successfully resumed. With VirtualBox you would have to poweroff all VMs because if you suspend with running VirtualBox VMs – it will just crash – its not possible to do suspend/resume cycle with VirtualBox.</p>
<p>The <i>Table of Contents</i> for this article is as follows:</p>
<ul>
<li><strong>FreeBSD Bhyve Virtualization</strong></li>
<li><strong>Bhyve Managers</strong></li>
<li><strong>Bhyve versus KVM and VMware</strong></li>
<li><strong>Bhyve libvirt/virt-manager GUI</strong></li>
<li><strong>vm-bhyve</strong>
<ul>
<li><strong>Install/Setup</strong></li>
<li><strong>Networking</strong>
<ul>
<li><strong>Server/Desktop LAN Bridge</strong></li>
<li><strong>Laptop WiFi NAT</strong></li>
<li><strong>Networking Restart</strong></li>
</ul>
</li>
<li><strong>Datastores</strong></li>
<li><strong>Templates</strong></li>
<li><strong>NVMe</strong></li>
<li><strong>ISO Images</strong></li>
<li><strong>Guest OS Install</strong>
<ul>
<li><strong>FreeBSD</strong></li>
<li><strong>Linux</strong></li>
<li><strong>Windows 7</strong></li>
<li><strong>Windows 10</strong>
<ul>
<li><strong>Force Windows 10 Offline Account</strong></li>
<li><strong>Windows 10 Bloat Removers</strong></li>
</ul>
</li>
<li><strong>Windows 11</strong></li>
</ul>
</li>
<li><strong>Dealing with Locked VMs</strong></li>
<li><strong>Disk Resize</strong></li>
</ul>
</li>
<li><strong>Summary</strong></li>
</ul>
<h2><strong>Bhyve Managers</strong></h2>
<p>While VirtualBox has quite usable QT based GUI – the Bhyve does not have anything like that. I once seen some GUI QT prototype for Bhyve but it was very basic – so forget about that currently. There are however several web interfaces such as TrueNAS CORE or CBSD/CloneOS. There are also several terminal managers such as <tt><b>vm-bhyve</b></tt>. The older one <tt><b>iohyve</b></tt> has not been maintained for at least 6 long years. There is also libvirt Bhyve driver but more on that later.</p>
<h2>Bhyve versus KVM and VMware</h2>
<p><i>Klara Systems</i> compared Bhyve to <a href="https://klarasystems.com/articles/virtualization-showdown-freebsd-bhyve-linux-kvm/">KVM</a> and <i>Benjamin Bryan</i> compared it against <a href="https://b3n.org/vmware-vs-bhyve-performance-comparison/">VMware</a> hypervisor. While Bhyve remains competitive against both of them there are two important aspects from <i>Klara Systems</i> that stand out and are worth repeating here.</p>
<p>First – using <tt><b>nvme</b></tt> driver is a lot faster then using more traditional <tt><b>virtio-blk</b></tt> or <tt><b>ahci-hd</b></tt> backends.</p>
<p><img data-attachment-id="5221" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/bhyve-nvme/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/bhyve-nvme.png" data-orig-size="630,360" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="bhyve-nvme" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/bhyve-nvme.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/bhyve-nvme.png?w=630" src="https://vermaden.files.wordpress.com/2023/08/bhyve-nvme.png?w=960" alt="bhyve-nvme" srcset="https://vermaden.files.wordpress.com/2023/08/bhyve-nvme.png 630w, https://vermaden.files.wordpress.com/2023/08/bhyve-nvme.png?w=150&amp;h=86 150w, https://vermaden.files.wordpress.com/2023/08/bhyve-nvme.png?w=300&amp;h=171 300w" sizes="(max-width: 630px) 100vw, 630px"></p>
<p>Second – and this one seems strange – using a <b>raw file</b> is faster then using ZFS <b>zvol</b> device.</p>
<p><img data-attachment-id="5222" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/bhyve-raw-zvol/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/bhyve-raw-zvol.png" data-orig-size="630,360" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="bhyve-raw-zvol" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/bhyve-raw-zvol.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/bhyve-raw-zvol.png?w=630" src="https://vermaden.files.wordpress.com/2023/08/bhyve-raw-zvol.png?w=960" alt="bhyve-raw-zvol" srcset="https://vermaden.files.wordpress.com/2023/08/bhyve-raw-zvol.png 630w, https://vermaden.files.wordpress.com/2023/08/bhyve-raw-zvol.png?w=150&amp;h=86 150w, https://vermaden.files.wordpress.com/2023/08/bhyve-raw-zvol.png?w=300&amp;h=171 300w" sizes="(max-width: 630px) 100vw, 630px"></p>
<p>To summarize these thoughts – just just file on a disk like <tt><b>disk0.img</b></tt> and use <tt><b>nvme</b></tt> as storage backend everytime the guest operating system supports it.</p>
<h2>Bhyve libvirt/virt-manager GUI</h2>
<p>Theoretically the libvirt virtualization API supports Bhyve as one of its backends and the details about Bhyve driver – <a href="https://libvirt.org/drvbhyve.html">https://libvirt.org/drvbhyve.html</a> – are available here. I have tried it with <tt><b>virt-manager</b></tt> and after some basic configuration I was able to start FreeBSD 13.2 installation … but it got frozen at the kernel messages and nothing more happened.</p>
<p><img data-attachment-id="5223" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/virt-manager-boot-menu/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-boot-menu.png" data-orig-size="1024,827" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="virt-manager-boot-menu" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-boot-menu.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-boot-menu.png?w=960" src="https://vermaden.files.wordpress.com/2023/08/virt-manager-boot-menu.png?w=960" alt="virt-manager-boot-menu" srcset="https://vermaden.files.wordpress.com/2023/08/virt-manager-boot-menu.png 1024w, https://vermaden.files.wordpress.com/2023/08/virt-manager-boot-menu.png?w=150&amp;h=121 150w, https://vermaden.files.wordpress.com/2023/08/virt-manager-boot-menu.png?w=300&amp;h=242 300w, https://vermaden.files.wordpress.com/2023/08/virt-manager-boot-menu.png?w=768&amp;h=620 768w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p>… and the moment it hanged below. I have tried multiple times with the same effect.</p>
<p><img data-attachment-id="5224" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/virt-manager-hang/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-hang.png" data-orig-size="1024,827" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="virt-manager-hang" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-hang.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-hang.png?w=960" src="https://vermaden.files.wordpress.com/2023/08/virt-manager-hang.png?w=960" alt="virt-manager-hang" srcset="https://vermaden.files.wordpress.com/2023/08/virt-manager-hang.png 1024w, https://vermaden.files.wordpress.com/2023/08/virt-manager-hang.png?w=150&amp;h=121 150w, https://vermaden.files.wordpress.com/2023/08/virt-manager-hang.png?w=300&amp;h=242 300w, https://vermaden.files.wordpress.com/2023/08/virt-manager-hang.png?w=768&amp;h=620 768w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p>I really liked the virtual machine settings window of <tt><b>virt-manager</b></tt>.</p>
<p><img data-attachment-id="5225" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/virt-manager-machine-settings/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-machine-settings.png" data-orig-size="1024,827" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="virt-manager-machine-settings" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-machine-settings.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/virt-manager-machine-settings.png?w=960" src="https://vermaden.files.wordpress.com/2023/08/virt-manager-machine-settings.png?w=960" alt="virt-manager-machine-settings" srcset="https://vermaden.files.wordpress.com/2023/08/virt-manager-machine-settings.png 1024w, https://vermaden.files.wordpress.com/2023/08/virt-manager-machine-settings.png?w=150&amp;h=121 150w, https://vermaden.files.wordpress.com/2023/08/virt-manager-machine-settings.png?w=300&amp;h=242 300w, https://vermaden.files.wordpress.com/2023/08/virt-manager-machine-settings.png?w=768&amp;h=620 768w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<h2>vm-bhyve</h2>
<p>While You can use Bhyve directly with <tt><b>bhyve(8)</b></tt> and <tt><b>bhyvectl(8)</b></tt> commands – which I was doing in the past – after trying the <tt><b>vm-bhyve</b></tt> both on the desktop and server space – I really liked it and this is what I currently use. I just moved from <tt><b>vm-bhyve</b></tt> package to the newer <tt><b>vm-bhybe-devel</b></tt> one.</p>
<p>The <strong><tt>vm(8)</tt></strong> command is simple and provides all needed use cases.</p>
<pre><b>host # <span>vm help</span></b>
vm-bhyve: Bhyve virtual machine management v1.6-devel (rev. 106001)
Usage: vm ...
    version
    init
    set [setting=value] [...]
    get [all|setting] [...]
    switch list
    switch info [name] [...]
    switch create [-t type] [-i interface] [-n vlan-id] [-m mtu] [-a address/prefix-len] [-b bridge] [-p] 
    switch vlan  
    switch nat  
    switch private  
    switch add  
    switch remove  
    switch destroy 
    datastore list
    datastore add  
    datastore remove 
    datastore add  
    list
    info [name] [...]
    create [-d datastore] [-t template] [-s size] [-m memory] [-c vCPUs] 
    install [-fi]  
    start [-fi]  [...]
    stop  [...]
    restart 
    console  [com1|com2]
    configure 
    rename  
    add [-d device] [-t type] [-s size|switch] 
    startall
    stopall
    reset  [-f] 
    poweroff [-f] 
    destroy [-f] 
    passthru
    clone  
    snapshot [-f] 
    rollback [-r] 
    iso [url]
    img [url]
    image list
    image create [-d description] [-u] 
    image destroy 
    image provision [-d datastore]  

</pre>
<h2>Install/Setup</h2>
<p>We need only several packages to add.</p>
<pre><strong>host # <span>pkg install -y \</span></strong>
<span><strong>         vm-bhyve-devel \</strong></span>
<span><strong>         uefi-edk2-bhyve-csm \</strong></span>
<span><strong>         bhyve-firmware \</strong></span>
<span><strong>         edk2-bhyve \</strong></span>
<span><strong>         dnsmasq \</strong></span>
<span><strong>         grub2-bhyve \</strong></span>
<span><strong>         tigervnc-viewer \</strong></span>
<span><strong>         rdesktop
</strong></span></pre>
<p>The setup is pretty easy also.</p>
<p>First we need to add several <tt><b>vm_*</b></tt> settings into the main FreeBSD <tt><b>/etc/rc.conf</b></tt> file.</p>
<pre>  vm_enable=YES
  vm_dir="zfs:zroot/vm"
  vm_list=""
  vm_delay=3

</pre>
<p>Keep in mind that you will later use the <tt><b>vm_list=""</b></tt> for the list of VMs that you would like to be started at boot. Like <tt><b>vm_list="freebsd13 freebsd14uefi"</b></tt> for example. Then the <tt><b>vm list</b></tt> command would place <strong>[1]</strong> in at the <tt><b>freebsd13</b></tt> name (as its first) and <strong>[2]</strong> in the <tt><b>freebsd14uefi</b></tt> name as this one is second on the list. See below.</p>
<pre><strong>host # <span>vm list</span></strong>
NAME           DATASTORE  LOADER     CPU  MEMORY  VNC           AUTO     STATE
almalinux8     default    uefi       2    2G      0.0.0.0:5908  No       Running (11819)
<strong><span>freebsd13      default    bhyveload  1    256M    -             Yes [1]  Running (2342)</span></strong>
freebsd14      default    bhyveload  1    256M    -             No       Stopped
<strong><span>freebsd14uefi  default    uefi       2    8G      -             Yes [2]  Running (35394)</span></strong>
windows10      default    uefi       2    2G      -             No       Stopped
windows7       default    uefi       2    2G      -             No       Stopped
</pre>
<p>We need to create a dedicated ZFS dataset for our VMs. You can also use directory on UFS – check <tt><b>vm-bhyve</b></tt> documentation.</p>
<pre><strong>host # <span>zfs create -o mountpoint=/vm zroot/vm
</span></strong></pre>
<p>We will also copy the available templates to our new <tt><b>/vm</b></tt> dir.</p>
<pre><strong>host # <span>cp -a /usr/local/share/examples/vm-bhyve /vm/.templates
</span></strong></pre>
<p>Remember to check <tt><b>/vm/.templates/config.sample</b></tt> as it has the documentation for all available options.</p>
<pre><strong>host # <span>head -12 /vm/.templates/config.sample</span></strong>
# This is a sample configuration file containing all supported options
# Please do not try and use this file itself for a guest
# For any option that contains a number in the name, such as "network0_type",
# you can add additional devices of that type by creating a new set of
# variables using the next number in sequence, e.g "network1_type"
#
# Please make sure all option names are specified in lowercase and
# at the beginning of the line. If there is any whitespace before
# the option name, the line will be ignored.
# The '#' character signifies the start of a comment, even within
# double-quotes, and so cannot be used inside any values.
</pre>
<p>We can now start initialize the <tt><b>vm-bhyve</b></tt>.</p>
<pre><strong>host # <span>service vm start
</span></strong></pre>
<h2>Networking</h2>
<p>There as many network setups as many FreeBSD has network capabilities – a lot! I this guide I will cover two most typical network setups for Bhyve. One would be the most server (or desktop) oriented – as it requires a LAN card to be used. The other one I would call a laptop one – that one would provide network connectivity using <tt><b>wlan0</b></tt> WiFi interface.</p>
<p>No matter which one we will choose – we need to enable port forwarding on our FreeBSD host. Do that with these two commands.</p>
<pre><strong>host # <span>sysrc gateway_enable=YES</span>

host # <span>sysctl net.inet.ip.forwarding=1</span>

host # <span>echo net.link.tap.up_on_open=1 &gt;&gt; /etc/sysctl.conf</span>

host # <span>sysctl net.link.tap.up_on_open=1
</span></strong></pre>
<p>I assume that our FreeBSD <tt><b>host</b></tt> system would use <tt><b>10.0.0.10/24</b></tt> IP address and that <tt><b>10.0.0.1</b></tt> would be its default gateway.</p>
<p>Your <tt><b>host</b></tt> system main <tt><b>/etc/rc.conf</b></tt> file can looks as follows then.</p>
<pre><strong>host # <span>cat /etc/rc.conf</span></strong>
# NETWORK
  hostname=host
  ifconfig_re0="inet 10.0.0.10/24 up"
  defaultrouter="10.0.0.1"
  gateway_enable=YES

# DAEMONS
  sshd_enable=YES
  zfs_enable=YES

# BHYVE
  vm_enable="YES"
  vm_dir="zfs:zroot/vm"
  vm_list=""
  vm_delay="3"
</pre>
<h3>Server/Desktop LAN Bridge</h3>
<p>We will use <tt><b>10.0.0.0/24</b></tt> network – the same that our <tt><b>host</b></tt> system uses. We will need one bridge/switch named <b><tt>vm-public</tt></b> without any address on it. Information about the switches is kept in the <tt><b>/vm/.config/system.conf</b></tt> file.</p>
<pre><strong>host # <span>vm switch create public</span>

host # <span>vm switch list</span></strong>
NAME    TYPE      IFACE      ADDRESS      PRIVATE  MTU  VLAN  PORTS
public  standard  vm-public  -            no       -    -     -

<strong>host # <span>cat /vm/.config/system.conf</span></strong> 
switch_list="public"
type_public="standard"
</pre>
<p>To be honest the networking part setup is complete.</p>
<p>When you will be setting up your Bhyve VMs you will either use static <tt><b>10.0.0.0/24</b></tt> IP address space or just use DHCP and the one that is already on your network will take care of the rest (assuming you have one).</p>
<p>If you do not have one you may use <tt><b>dnsmasq</b></tt> service to do that easily.</p>
<pre><strong>host # <span>cat /usr/local/etc/dnsmasq.conf</span></strong>
port=0
no-resolv
server=1.1.1.1
except-interface=lo0
bind-interfaces
local-service
dhcp-authoritative
interface=vm-public
dhcp-range=10.0.0.69,10.0.0.96

<strong>host # <span>service dnsmasq enable</span>

host # <span>service dnsmasq start
</span></strong></pre>
<p>That should do.</p>
<h3>Laptop WiFi NAT</h3>
<p>This is one of the cases where VirtualBox has one more feature over Bhyve. With VirtualBox its possible to use bridge mode over WiFi interface. Its not possible with Bhyve currently. I have submitted a proposal to <i>FreeBSD Foundation</i> to implement such configuration – especially as open source VirtualBox code already exists. Time will tell if it will be implemented or if there would be more important tasks to take care of.</p>
<p>We will use <tt><b>10.1.1.0/24</b></tt> network for our VM needs. We will also need only one <tt><b>vm-bhyve</b></tt> switch that we will use – and it will be the <b><tt>vm-public</tt></b> one with <tt><b>10.1.1.1/24</b></tt> address – we will be using it as a gateway for our VMs in NAT. Information about the switches is kept in the <tt><b>/vm/.config/system.conf</b></tt> file.</p>
<pre><strong>host # <span>vm switch create -a 10.1.1.1/24 public</span>

host # <span>vm switch list</span></strong>
NAME    TYPE      IFACE      ADDRESS      PRIVATE  MTU  VLAN  PORTS
public  standard  vm-public  10.1.1.1/24  no       -    -     -

<strong>host # <span>cat /vm/.config/system.conf</span></strong> 
switch_list="public"
type_public="standard"
addr_public="10.1.1.1/24"

</pre>
<p>Now the NAT part – we will do that with very simple <tt><b>pf(4)</b></tt> config.</p>
<pre><strong>host # <span>cat /etc/pf.conf</span></strong>
# SKIP LOOPBACK
  set skip on lo0

# bhyve(8) VMS NAT 
  nat on wlan0 from {10.1.1.1/24} to any -&gt; (wlan0)

# PASS IN/OUT ALL
  pass in all
  pass out all

<strong>host # <span>service pf enable</span>

host # <span>service pf start
</span></strong></pre>
<p>You can check the stats of that <tt><b>pf(4)</b></tt> rules like that.</p>
<pre><strong>host # <span>pfctl -Psn -vv</span></strong>
No ALTQ support in kernel
ALTQ related functions disabled
@0 nat on wlan0 inet from 10.1.1.0/24 to any -&gt; (wlan0) round-robin
  [ Evaluations: 18774     Packets: 362277    Bytes: 352847937   States: 0     ]
  [ Inserted: uid 0 pid 69837 State Creations: 38    ]

</pre>
<p>Feel free to add all your <tt><b>pf(4)</b></tt> rules into the <tt><b>/etc/pf.conf</b></tt> file.</p>
<p>Now the DHCP server. For simplicity of the setup we will use <tt><b>dnsmasq</b></tt> daemon – but nothing prevents you from setting up a <a href="https://vermaden.wordpress.com/2018/08/12/highly-available-dhcp-server-on-freebsd/">Highly Available DHCP Server</a> instead using <tt><b>isc-dhcp44-server</b></tt> package.</p>
<pre><strong>host # <span>cat /usr/local/etc/dnsmasq.conf</span></strong>
port=0
no-resolv
server=1.1.1.1
except-interface=lo0
bind-interfaces
local-service
dhcp-authoritative
interface=vm-public
dhcp-range=10.1.1.11,10.1.1.99

<strong>host # <span>service dnsmasq enable</span>

host # <span>service dnsmasq start
</span></strong></pre>
<p>Now you should be ready to setup Bhyve VMs on your laptop.</p>
<h3>Networking Restart</h3>
<p>Sometimes – when for example you laptop will boot without network connectivity – the <tt><b>tap(4)</b></tt> interfaces sometimes do not went <tt><b>UP</b></tt>.</p>
<p><img data-attachment-id="5238" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/bhyve-networking-restart/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/bhyve-networking-restart.png" data-orig-size="850,540" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="bhyve-networking-restart" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/bhyve-networking-restart.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/bhyve-networking-restart.png?w=850" src="https://vermaden.files.wordpress.com/2023/08/bhyve-networking-restart.png?w=960" alt="bhyve-networking-restart" srcset="https://vermaden.files.wordpress.com/2023/08/bhyve-networking-restart.png 850w, https://vermaden.files.wordpress.com/2023/08/bhyve-networking-restart.png?w=150&amp;h=95 150w, https://vermaden.files.wordpress.com/2023/08/bhyve-networking-restart.png?w=300&amp;h=191 300w, https://vermaden.files.wordpress.com/2023/08/bhyve-networking-restart.png?w=768&amp;h=488 768w" sizes="(max-width: 850px) 100vw, 850px"></p>
<p>There is simple fix tor that problem – <tt><b>bhyve-network-restart.sh</b></tt> script.</p>
<p>Its shown below.</p>
<pre><span># ADD IP ADDRESS TO EACH vm-bhyve SWITCH</span>
vm switch list \
  | sed 1d \
  | while read NAME TYPE IFACE ADDRESS PRIVATE MTU VLAN PORTS a0 a1 a2 a3 a4 a5 a6 a7 a8 a9
    do
      if [ "${ADDRESS}" != "-" ]
      then
             vm switch address ${NAME} ${ADDRESS}
        echo vm switch address ${NAME} ${ADDRESS}
      fi
    done        

<span># SET TO 'up' ALL vm-bhyve SWITCH MEMBERS</span>
vm switch list \
  | sed 1d \
  | awk '{print $1}' \
  | while read SWITCH
    do
      ifconfig vm-${SWITCH} \
        | awk '/member:/ {print $2}' \
        | while read INTERFACE
          do
                 ifconfig ${INTERFACE} up
            echo ifconfig ${INTERFACE} up
          done
    done


</pre>
<p>Execute it everytime you lost connectivity with your VMs and you are done.</p>
<h2>Datastores</h2>
<p>While <tt><b>vm-bhyve</b></tt> supports multiple datastores – you will only need one – the <tt><b>default</b></tt> one.</p>
<pre><strong>host # <span>vm datastore list</span></strong>
NAME            TYPE        PATH                      ZFS DATASET
default         zfs         /vm                       zroot/vm

</pre>
<h2>Snapshots and Clones</h2>
<p>The <tt><b>vm-bhyve</b></tt> also supports snapshots and clones of the VMs disks. Generally they are just ZFS snapshots and clones.</p>
<h2>Templates</h2>
<p>While <tt><b>vm-bhyve</b></tt> comes with several handy templates – they are incomplete – and small several changes makes the game more playable.</p>
<h2>NVMe</h2>
<p>First – we will implement the things that we know work faster – the <tt><b>nvme</b></tt> type for disk images instead of <tt><b>virt-blk</b></tt> or <b><tt>ahci-hd</tt></b> ones. Of course not all operating systems have support for such devices – for them we will use the latter options.</p>
<p>A fast way to change it to <tt><b>nvme</b></tt> is below.</p>
<pre><strong>host # <span>sed -i '' s.virtio-blk.nvme.g /vm/.templates/freebsd.conf
</span></strong></pre>
<h2>ISO Images</h2>
<p>Each VM needs an ISO image from which it will be installed. Of course you can also just create new VM and copy the disk contents from other server or use one of the FreeBSD images.</p>
<p>There are two ways to feed <tt><b>vm-bhyve</b></tt> with ISO images.</p>
<p>One is to fetch them from some URL.</p>
<pre><strong>host # <span>vm iso http://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/13.2/FreeBSD-13.2-RELEASE-amd64-disc1.iso</span></strong>

<strong>host # <span>vm iso</span></strong>
DATASTORE           FILENAME
default             FreeBSD-13.2-RELEASE-amd64-disc1.iso

</pre>
<p>The other way is to just simple copy ISO file to the <tt><b>/vm/.iso</b></tt> directory.</p>
<pre><strong>host # <span>cp /home/vermaden/download/ubuntu-mate-23.04-desktop-amd64.iso /vm/.iso/</span></strong>

<strong>host # <span>vm iso</span></strong>
DATASTORE           FILENAME
default             FreeBSD-13.2-RELEASE-amd64-disc1.iso
default             ubuntu-mate-23.04-desktop-amd64.iso
</pre>
<h2>Guest OS Install</h2>
<p>Generally each VM install is very similar as shown below.</p>
<pre><strong>host # <span>vm create -t TEMPLATE NAME</span>

host # <span>vm install MAME ISO
</span>
host # <span>vm console NAME

</span></strong></pre>
<p>Example for FreeBSD is below.</p>
<pre><strong>host # <span>vm create -t freebsd freebsd13</span>

host # <span>vm install freebsd13 FreeBSD-13.2-RELEASE-amd64-disc1.iso</span></strong>
Starting freebsd13
  * found guest in /vm/freebsd13
  * booting...

<strong>host # <span>vm console freebsd13
</span></strong></pre>
<p>You will probably see something like that below.</p>
<p><img data-attachment-id="5226" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/freebsd-loader-menu/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/freebsd-loader-menu.png" data-orig-size="800,500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="freebsd-loader-menu" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/freebsd-loader-menu.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/freebsd-loader-menu.png?w=800" src="https://vermaden.files.wordpress.com/2023/08/freebsd-loader-menu.png?w=960" alt="freebsd-loader-menu" srcset="https://vermaden.files.wordpress.com/2023/08/freebsd-loader-menu.png 800w, https://vermaden.files.wordpress.com/2023/08/freebsd-loader-menu.png?w=150&amp;h=94 150w, https://vermaden.files.wordpress.com/2023/08/freebsd-loader-menu.png?w=300&amp;h=188 300w, https://vermaden.files.wordpress.com/2023/08/freebsd-loader-menu.png?w=768&amp;h=480 768w" sizes="(max-width: 800px) 100vw, 800px"></p>
<p>Then you do the installation in the text mode and after reboot you have your running FreeBSD VM.</p>
<pre><strong>host # <span>vm list</span></strong>
NAME          DATASTORE  LOADER     CPU  MEMORY  VNC  AUTO     STATE
freebsd13     default    bhyveload  1    256M    -    Yes [1]  Running (85315)
</pre>
<p>Some more info to display can be shown with <tt><b>info</b></tt> argument.</p>
<pre><strong>host # <span>vm info freebsd13</span></strong>
------------------------
Virtual Machine: freebsd13
------------------------
  state: stopped
  datastore: default
  loader: bhyveload
  uuid: a91287a1-39d3-11ee-b73d-f0def1d6aea1
  cpu: 1
  memory: 256M

  network-interface
    number: 0
    emulation: virtio-net
    virtual-switch: public
    fixed-mac-address: 58:9c:fc:0b:98:30
    fixed-device: -

  virtual-disk
    number: 0
    device-type: file
    emulation: nvme
    options: -
    system-path: /vm/freebsd13/disk0.img
    bytes-size: 21474836480 (20.000G)
    bytes-used: 885089280 (844.086M)

  snapshots
    zroot/vm/freebsd13@fresh    85.2M   Mon Aug 14 11:18 2023

<strong>host # <span>env EDITOR=cat vm configure freebsd13</span></strong>
loader="bhyveload"
cpu=1
memory=256M
network0_type="virtio-net"
network0_switch="public"
disk0_type="nvme"
disk0_name="disk0.img"
uuid="a91287a1-39d3-11ee-b73d-f0def1d6aea1"
network0_mac="58:9c:fc:0b:98:30"
</pre>
<p>If you want to edit and not only display the VM config use this.</p>
<pre><strong>host # <span>vm configure freebsd13
</span></strong></pre>
<h3>FreeBSD</h3>
<p>FreeBSD can be boot in two ways. One is with <tt><b>bhyveload</b></tt> which may be translated to legacy BIOS boot. You can also of course boot FreeBSD un UEFI mode.</p>
<pre><strong>host # <span>cat /vm/.templates/freebsd.conf</span></strong>
loader="bhyveload"
cpu=1
memory=256M
network0_type="virtio-net"
network0_switch="public"
disk0_type="nvme"
disk0_name="disk0.img"
</pre>
<p>The above will use <tt><b>bhyveload</b></tt> and it mostly works … but sometimes if you want to install a lot newer version under Bhyve the loader may not have all the needed features. I was hit by this problem recently where I used FreeBSD 13.2-RELEASE for the FreeBSD <tt><b>host</b></tt> system and wanted to try 14.0-ALPHA1 version.</p>
<p>I described the details of this problem here – <a href="https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=273099">FreeBSD Bug 273099</a> – in a BUG report.</p>
<p>This is how such error looks like:</p>
<pre>| FreeBSD/amd64 User boot lua, Revision 1.2
| <span>ZFS: unsupported feature: com.klarasystems:vdev_zaps_v2</span>
| <span>ERROR: cannot open /boot/lua/loader.lua: no such file or directory.
</span>| 
| Type '?' for a list of commands, 'help' for more detailed help.
| OK 
</pre>
<p>To overcome that you will need latest (more up to date then 14.0-ALPHA1 version) FreeBSD sources and below commands.</p>
<pre><strong>host # <span>pkg install gitup</span>

host # <span>cp /usr/local/etc/gitup.conf.sample /usr/local/etc/gitup.conf</span>

host # <span>gitup current</span>

host # <span>cd /usr/src/stand</span>

host # <span>make</span>

host # <span>find /usr/obj -type f -name userboot_lua.so
</span></strong>/usr/obj/usr/src/amd64.amd64/stand/userboot/userboot_lua/userboot_lua.so

<strong>host # <span>cp /usr/obj/usr/src/amd64.amd64/stand/userboot/userboot_lua/userboot_lua.so /vm/userboot_lua.so

</span></strong></pre>
<p>Now – we need to add <tt><b>bhyveload_loader="/vm/userboot_lua.so"</b></tt> option to out FreeBSD 14.0-ALPHA1 machine config.</p>
<pre><strong>host # <span>cat /vm/freebsd14/freebsd14.conf</span></strong>
loader="bhyveload"
<strong><span>bhyveload_loader="/vm/userboot_lua.so"</span></strong>
cpu=1
memory=256M
network0_type="virtio-net"
network0_switch="public"
disk0_type="nvme"
disk0_name="disk0.img"
uuid="975bca2a-39c4-11ee-b73d-f0def1d6aea1"
network0_mac="58:9c:fc:03:67:47"
</pre>
<p>Now it will boot properly.</p>
<p>Of course it was very easy to overcome that using UEFI boot instead.</p>
<pre><strong>host # <span>cat /vm/freebsd14uefi/freebsd14uefi.conf</span></strong>
loader="uefi"
cpu=1
memory=256M
network0_type="virtio-net"
network0_switch="public"
disk0_type="nvme"
disk0_name="disk0.img"
uuid="35ca42b7-7f28-43eb-afd9-2488c5ec83cf"
network0_mac="58:9c:fc:0a:16:4b"
</pre>
<h3>Linux</h3>
<p>By default for Linux the <tt><b>grub</b></tt> way is the proposed way. I do not use it as at it olny allows console access – and even many so called enterprice grade Linux distributions such as AlmaLinux or Rocky have graphical installer that needs/wants graphical display … and that is only available in <tt><b>uefi</b></tt> mode.</p>
<p>Maybe for Alpine or Void Linux such approach may be usable … but <tt><b>uefi</b></tt> will also work very well – thus I do not see ANY advantages of using <tt><b>grub</b></tt> way here.</p>
<p>I will show you the next example based on AlmaLinux 8.x install but the same worked properly with Ubuntu Mate for example.</p>
<p>First the default template.</p>
<pre><strong>host # <span>cat /vm/.templates/linux.conf</span></strong>
loader="uefi"
cpu=2
memory=4G
network0_type="virtio-net"
network0_switch="public"
disk0_type="nvme"
disk0_name="disk0.img"
xhci_mouse="yes"
graphics="yes"
</pre>
<p>The above added <tt><b>xhci_mouse="yes"</b></tt> uses more precise <tt><b>xhci(4)</b></tt> USB 3.x mouse driver and <tt><b>graphics="yes"</b></tt> forces the exposure of VNC connection.</p>
<p>With such template the installation looks like that.</p>
<pre><strong>host # <span>cp AlmaLinux-8.8-x86_64-minimal.iso /vm/.iso/</span>

host # <span>vm create -t linux almalinux8</span>

host # <span>vm install almalinux8 AlmaLinux-8.8-x86_64-minimal.iso</span></strong>
Starting almalinux8
  * found guest in /vm/almalinux8
  * booting...

<strong>host # <span>vm list</span></strong>
NAME           DATASTORE  LOADER     CPU  MEMORY  VNC           AUTO  STATE
almalinux8     default    uefi       2    4G      0.0.0.0:5900  No    Running (11819)

<strong>host % <span>vncviewer -SendClipboard -AcceptClipboard -LowColorLevel -QualityLevel 6 :5900 &amp;

</span></strong></pre>
<p>The last <tt><b>vncviewer(1)</b></tt> command is executed as regular user. It comes from <tt><b>net/tigervnc-viewer</b></tt> package.</p>
<p>If you will be connecting to some external server then use IP address in the command.</p>
<pre><strong>host % <span>vncviewer -SendClipboard -AcceptClipboard -LowColorLevel -QualityLevel 6 10.0.0.66::5900 &amp;
</span></strong></pre>
<p>After the Linux system is installed you may specify the exact VNC port or IP address. Also the screen resolution or enable/disable waiting for the VNC connection.</p>
<pre>graphics_port="5900"
graphics_listen="0.0.0.0"
graphics_res="1400x900"
graphics_wait="no"
</pre>
<h3>Windows 7</h3>
<p>A lot of people will criticize me for this one – as Windows 7 is not an officially supported version anymore. I do not care about that when I want to use some <tt><b>localhost</b></tt> software … or older software that works better on older version. Not to mention that its one of the last Windows versions that does not force online Microsoft account down your throat. It also uses less resources and is more responsive.</p>
<p>First – the template – similar to the Linux one.</p>
<pre><strong>host # <span>cat /vm/.templates/windows7.conf          </span></strong> 
loader="uefi"
graphics="yes"
cpu=2
memory=2G
ahci_device_limit="8"
network0_type="e1000"
network0_switch="public"
disk0_type="ahci-hd"
disk0_name="disk0.img"
disk0_opts="sectorsize=512"
utctime="no"
bhyve_options="-s 8,hda,play=/dev/dsp,rec=/dev/dsp"
</pre>
<p>If you set the <tt><b>xhci_mouse="yes"</b></tt> option with Windows 7 – you will end up without a working mouse in VNC connection and you will have to make all the install and configuration by keyboard only.</p>
<p>One may think about adding <tt><b>xhci_mouse="yes"</b></tt> after installation when you will already have working RDP connection – but that would also reqiure additional drivers. In theory – the device <tt><b>VEN_8086&amp;DEV_1E31</b></tt> name is recognized as <b>Intel USB 3.0 eXtensible Host Controller</b> … but for some reason anytime I wanted to install it – the Windows 7 system crashed and instantly rebooted.</p>
<p>The other even more imporant thing is having the <tt><b>disk0_opts="sectorsize=512"</b></tt> option. Without it the Windows 7 instaler will fail with the following error.</p>
<p><img data-attachment-id="5227" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/win-7-install-error-no-512-blocks/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/win-7-install-error-no-512-blocks.png" data-orig-size="1024,768" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="win-7-install-error-NO-512-BLOCKS" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/win-7-install-error-no-512-blocks.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/win-7-install-error-no-512-blocks.png?w=960" src="https://vermaden.files.wordpress.com/2023/08/win-7-install-error-no-512-blocks.png?w=960" alt="win-7-install-error-NO-512-BLOCKS" srcset="https://vermaden.files.wordpress.com/2023/08/win-7-install-error-no-512-blocks.png 1024w, https://vermaden.files.wordpress.com/2023/08/win-7-install-error-no-512-blocks.png?w=150&amp;h=113 150w, https://vermaden.files.wordpress.com/2023/08/win-7-install-error-no-512-blocks.png?w=300&amp;h=225 300w, https://vermaden.files.wordpress.com/2023/08/win-7-install-error-no-512-blocks.png?w=768&amp;h=576 768w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p>The last option <tt><b>bhyve_options="-s 8,hda,play=/dev/dsp,rec=/dev/dsp"</b></tt> enables audio.</p>
<p>The install procedure is also similar to Linux.</p>
<pre><strong>host # <span>cp win_7_amd64_sp1_en.iso /vm/.iso/</span></strong>

<strong>host # <span>vm iso</span></strong>
DATASTORE           FILENAME
default             win_7_amd64_sp1_en.iso

<strong>host # <span>vm create -t windows7 -s 40G windows7</span></strong>

<strong>host # <span>vm install windows7 win_7_amd64_sp1_en.iso</span></strong>
Starting windows7
  * found guest in /vm/windows7
  * booting...

<strong>host # <span>vm list</span></strong>
NAME           DATASTORE  LOADER     CPU  MEMORY  VNC           AUTO  STATE
windows7       default    uefi       2    2G      0.0.0.0:5900  No    Running (11819)

<strong>host % <span>vncviewer -SendClipboard -AcceptClipboard -LowColorLevel -QualityLevel 6 :5900 &amp;

</span></strong></pre>
<p>After the install we should enable RDP connections for more features. Rememeber to select <i>any version</i> option.</p>
<p><img data-attachment-id="5228" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/win-7-computer-properties-advanced-remote/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/win-7-computer-properties-advanced-remote.png" data-orig-size="1024,768" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="win-7-computer-properties-advanced-remote" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/win-7-computer-properties-advanced-remote.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/win-7-computer-properties-advanced-remote.png?w=960" src="https://vermaden.files.wordpress.com/2023/08/win-7-computer-properties-advanced-remote.png?w=960" alt="win-7-computer-properties-advanced-remote" srcset="https://vermaden.files.wordpress.com/2023/08/win-7-computer-properties-advanced-remote.png 1024w, https://vermaden.files.wordpress.com/2023/08/win-7-computer-properties-advanced-remote.png?w=150&amp;h=113 150w, https://vermaden.files.wordpress.com/2023/08/win-7-computer-properties-advanced-remote.png?w=300&amp;h=225 300w, https://vermaden.files.wordpress.com/2023/08/win-7-computer-properties-advanced-remote.png?w=768&amp;h=576 768w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p>You can add one or more CD-ROM drives with following options in the <tt><b>configure</b></tt> argument.</p>
<pre>disk1_type="ahci-cd"
disk1_dev="custom"
disk1_name="/vm/.iso/virtio-drivers.iso"
</pre>
<p>It would be easier for RDP connections to have static IP instead of a DHCP one.</p>
<p><img data-attachment-id="5229" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/win-7-network-adapter-ipv4-static/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/win-7-network-adapter-ipv4-static.png" data-orig-size="1024,768" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="win-7-network-adapter-IPv4-static" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/win-7-network-adapter-ipv4-static.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/win-7-network-adapter-ipv4-static.png?w=960" src="https://vermaden.files.wordpress.com/2023/08/win-7-network-adapter-ipv4-static.png?w=960" alt="win-7-network-adapter-IPv4-static" srcset="https://vermaden.files.wordpress.com/2023/08/win-7-network-adapter-ipv4-static.png 1024w, https://vermaden.files.wordpress.com/2023/08/win-7-network-adapter-ipv4-static.png?w=150&amp;h=113 150w, https://vermaden.files.wordpress.com/2023/08/win-7-network-adapter-ipv4-static.png?w=300&amp;h=225 300w, https://vermaden.files.wordpress.com/2023/08/win-7-network-adapter-ipv4-static.png?w=768&amp;h=576 768w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p>Now as we have the static <tt><b>10.1.1.7</b></tt> IP address we can use RDP connection with <tt><b>rdesktop(1)</b></tt> command.</p>
<pre><strong>host % <span>rdesktop -u buser -p bpass -P -N -z -g 1800x1000 -a 24 -r sound:local -r disk:HOME=/home/vermaden 10.1.1.7</span></strong>
Autoselecting keyboard map 'en-us' from locale

ATTENTION! The server uses and invalid security certificate which can not be trusted for
the following identified reasons(s);

 1. Certificate issuer is not trusted by this system.

     Issuer: CN=vbox


Review the following certificate info before you trust it to be added as an exception.
If you do not trust the certificate the connection atempt will be aborted:

    Subject: CN=vbox
     Issuer: CN=vbox
 Valid From: Mon Aug 14 00:58:25 2023
         To: Mon Feb 12 23:58:25 2024

  Certificate fingerprints:

       sha1: 4ad853c40a8aa0560af315b691038202506e07ce
     sha256: 44ec8f7650486aef6261aea42da99caba4e84d7bc58341c0ca1bb8e28b81d222


Do you trust this certificate (yes/no)? <span><strong>yes</strong></span>
Connection established using SSL.
</pre>
<p>There are several useful options here.</p>
<p>The <tt><b>-u buser</b></tt> and <tt><b>-p bpass</b></tt> will take care of credentials.</p>
<p>The <tt><b>-P</b></tt> option enables caching of bitmaps to disk (persistent bitmap caching). This improves performance (especially on low bandwidth connections) and reduces network traffic.</p>
<p>The <tt><b>-N</b></tt> option enables numlock synchronization between the X11 server and remote RDP session.</p>
<p>The <tt><b>-z</b></tt> enables compression of the RDP datastream.</p>
<p>The <tt><b>-g 1800x1000</b></tt> and <tt><b>-a 24</b></tt> specifies resolution and color depth rate.</p>
<p>The <tt><b>-r disk:HOME=/home/vermaden</b></tt> enables transparent sharing of your home directory and additional share is shown in My Computer in the Windows 7 machine – very handy for sharing files between the <tt><b>host</b></tt> and guest VM as chown below.</p>
<p><img data-attachment-id="5230" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/win-7-sharing/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/win-7-sharing.png" data-orig-size="860,600" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="win-7-sharing" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/win-7-sharing.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/win-7-sharing.png?w=860" src="https://vermaden.files.wordpress.com/2023/08/win-7-sharing.png?w=960" alt="win-7-sharing" srcset="https://vermaden.files.wordpress.com/2023/08/win-7-sharing.png 860w, https://vermaden.files.wordpress.com/2023/08/win-7-sharing.png?w=150&amp;h=105 150w, https://vermaden.files.wordpress.com/2023/08/win-7-sharing.png?w=300&amp;h=209 300w, https://vermaden.files.wordpress.com/2023/08/win-7-sharing.png?w=768&amp;h=536 768w" sizes="(max-width: 860px) 100vw, 860px"></p>
<p>The last one option <b><tt>-r sound:local</tt></b> specifies that the audio will be realized on the guest VM – this will only work if you added the <tt><b>bhyve_options="-s 8,hda,play=/dev/dsp,rec=/dev/dsp"</b></tt> to the Windows 7 Bhyve config. Alternatively without that <tt><b>hda(4)</b></tt> emulation you can use <tt><b>-r sound:remote</b></tt> option – this would use RDP protocol to transfer audio events from the guest machine to your <tt><b>host</b></tt> machine and then audio will be played then locally on your <tt><b>host</b></tt> machine.</p>
<h3>Windows 10</h3>
<p>Finally a supported version.</p>
<p>Template is similar to the Windows 7 one.</p>
<pre><strong>host # <span>cat /vm/.templates/windows10.conf</span></strong>
loader="uefi"
graphics="yes"
xhci_mouse="yes"
cpu=2
memory=2G
ahci_device_limit="8"
network0_type="e1000"
network0_switch="public"
disk0_type="nvme"
disk0_name="disk0.img"
utctime="no"
bhyve_options="-s 8,hda,play=/dev/dsp,rec=/dev/dsp"
</pre>
<p>The Windows 10 supports the <tt><b>xhci_mouse="yes"</b></tt> so we enable and keep it all the time.</p>
<p>The Windows 10 does not need the <tt><b>disk0_opts="sectorsize=512"</b></tt> option.</p>
<p>As Windows 10 is newer – the <tt><b>nvme</b></tt> can (and should) be used for performance reasons.</p>
<p>The last option <tt><b>bhyve_options="-s 8,hda,play=/dev/dsp,rec=/dev/dsp"</b></tt> enables audio.</p>
<p>The install procedure is also similar to Windows 7.</p>
<pre><strong>host # <span>cp win_10_amd64_en_LTSC.iso /vm/.iso/</span></strong>

<strong>host # <span>vm iso</span></strong>
DATASTORE           FILENAME
default             win_10_amd64_en_LTSC.iso

<strong>host # <span>vm create -t windows10 -s 40G windows10</span></strong>

<strong>host # <span>vm install windows10 win_10_amd64_en_LTSC.iso</span></strong>
Starting windows10
  * found guest in /vm/windows10
  * booting...

<strong>host # <span>vm list</span></strong>
NAME           DATASTORE  LOADER     CPU  MEMORY  VNC           AUTO  STATE
windows10      default    uefi       2    2G      0.0.0.0:5900  No    Running (11819)

<strong>host % <span>vncviewer -SendClipboard -AcceptClipboard -LowColorLevel -QualityLevel 6 :5900 &amp;

</span></strong></pre>
<p>After the install we should enable RDP connections for more features. Remember to select <i>any version</i> option.</p>
<p><img data-attachment-id="5231" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/win-10-advanced-settings-remote/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/win-10-advanced-settings-remote.png" data-orig-size="1024,768" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="win-10-advanced-settings-remote" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/win-10-advanced-settings-remote.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/win-10-advanced-settings-remote.png?w=960" src="https://vermaden.files.wordpress.com/2023/08/win-10-advanced-settings-remote.png?w=960" alt="win-10-advanced-settings-remote" srcset="https://vermaden.files.wordpress.com/2023/08/win-10-advanced-settings-remote.png 1024w, https://vermaden.files.wordpress.com/2023/08/win-10-advanced-settings-remote.png?w=150&amp;h=113 150w, https://vermaden.files.wordpress.com/2023/08/win-10-advanced-settings-remote.png?w=300&amp;h=225 300w, https://vermaden.files.wordpress.com/2023/08/win-10-advanced-settings-remote.png?w=768&amp;h=576 768w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p>You can add one or more CD-ROM drives with following options in the <tt><b>configure</b></tt> argument.</p>
<pre>disk1_type="ahci-cd"
disk1_dev="custom"
disk1_name="/vm/.iso/virtio-drivers.iso"
</pre>
<p>It would be easier for RDP connections to have static IP instead of a DHCP one.</p>
<p><img data-attachment-id="5232" data-permalink="https://vermaden.wordpress.com/2023/08/18/freebsd-bhyve-virtualization/win-10-network-settings-adapter-properties-ipv4-static/" data-orig-file="https://vermaden.files.wordpress.com/2023/08/win-10-network-settings-adapter-properties-ipv4-static.png" data-orig-size="1024,768" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="win-10-network-settings-adapter-properties-IPv4-static" data-image-description="" data-image-caption="" data-medium-file="https://vermaden.files.wordpress.com/2023/08/win-10-network-settings-adapter-properties-ipv4-static.png?w=300" data-large-file="https://vermaden.files.wordpress.com/2023/08/win-10-network-settings-adapter-properties-ipv4-static.png?w=960" src="https://vermaden.files.wordpress.com/2023/08/win-10-network-settings-adapter-properties-ipv4-static.png?w=960" alt="win-10-network-settings-adapter-properties-IPv4-static" srcset="https://vermaden.files.wordpress.com/2023/08/win-10-network-settings-adapter-properties-ipv4-static.png 1024w, https://vermaden.files.wordpress.com/2023/08/win-10-network-settings-adapter-properties-ipv4-static.png?w=150&amp;h=113 150w, https://vermaden.files.wordpress.com/2023/08/win-10-network-settings-adapter-properties-ipv4-static.png?w=300&amp;h=225 300w, https://vermaden.files.wordpress.com/2023/08/win-10-network-settings-adapter-properties-ipv4-static.png?w=768&amp;h=576 768w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<p>Now as we have the static <tt><b>10.1.1.8</b></tt> IP address we can use RDP connection with <tt><b>rdesktop(1)</b></tt> command.</p>
<pre><strong>host % <span>rdesktop -u buser -p bpass -P -N -z -g 1600x900 -a 24 -r sound:local -r disk:HOME=/home/vermaden 10.1.1.8</span></strong>
Autoselecting keyboard map 'en-us' from locale

ATTENTION! The server uses and invalid security certificate which can not be trusted for
the following identified reasons(s);

 1. Certificate issuer is not trusted by this system.

     Issuer: CN=DESKTOP-HKJ3H6T


Review the following certificate info before you trust it to be added as an exception.
If you do not trust the certificate the connection atempt will be aborted:

    Subject: CN=DESKTOP-HKJ3H6T
     Issuer: CN=DESKTOP-HKJ3H6T
 Valid From: Mon Aug 14 10:33:41 2023
         To: Tue Feb 13 09:33:41 2024

  Certificate fingerprints:

       sha1: 967d5cdb164e53f7eb4c5c17b0343f2f279fb709
     sha256: c08b732122a39c44d91fac2a9093724da12d2f3e6ea51613245d13cf762f4cd2


Do you trust this certificate (yes/no)? <span><strong>yes
</strong></span></pre>
<p>Options are the same as with Windows 7 and they are described in the <b>Windows 7</b> section.</p>
<h4>Force Windows 10 Offline Account</h4>
<p>To force creation of local account instead of forced online account you need to boot the Windows 10 without network.</p>
<p>Do the following steps to do that.</p>
<pre><strong>host # <span>yes | vm poweroff windows10</span>

host # <span>vm configure windows10</span></strong>
<span>- network0_type="e1000"
- network0_switch="public"</span>

<strong>host # <span>vm start windows10

</span></strong></pre>
<p>Now create the offline account.</p>
<p>After creating it poweroff the Windows 10 VM.</p>
<pre><strong>host # <span>vm configure windows10</span></strong>
<span>+ network0_type="e1000"
+ network0_switch="public"</span>

<strong>host # <span>vm start windows10
</span></strong></pre>
<p>Now you have local account on Windows 10 system.</p>
<h4>Windows 10 Bloat Removers</h4>
<p>You may consider using on of the known Windows 10 bloat removers available here:</p>
<ul>
<li><strong><a href="https://github.com/Sycnex/Windows10Debloater" rel="nofollow">https://github.com/Sycnex/Windows10Debloater</a></strong></li>
<li><strong><a href="https://github.com/W4RH4WK/Debloat-Windows-10" rel="nofollow">https://github.com/W4RH4WK/Debloat-Windows-10</a></strong></li>
<li><strong><a href="https://github.com/LeDragoX/Win-Debloat-Tools" rel="nofollow">https://github.com/LeDragoX/Win-Debloat-Tools</a></strong></li>
<li><strong><a href="https://github.com/ChrisTitusTech/winutil" rel="nofollow">https://github.com/ChrisTitusTech/winutil</a></strong></li>
</ul>
<h3>Windows 11</h3>
<p>The setup/install of Windows 11 is the same as Windows 10.</p>
<h2>Dealing with Locked VMs</h2>
<p>Lets assume that our <tt><b>host</b></tt> system crashed.</p>
<p>The <tt><b>vm-bhyve</b></tt> will left <tt><b>run.lock</b></tt> files in the machines dirs.</p>
<pre><strong>host # <span>ls -l /vm/freebsd14uefi</span></strong>
total 1389223K
-rw-r--r-- 1 root wheel          32 2023-08-16 23:36 console
-rw------- 1 root wheel 21474836480 2023-08-16 23:46 disk0.img
-rw-r--r-- 1 root wheel         200 2023-08-16 23:35 freebsd14uefi.conf
<span>-rw-r--r-- 1 root wheel          11 2023-08-16 23:36 run.lock</span>
-rw-r--r-- 1 root wheel        5583 2023-08-16 23:36 vm-bhyve.log

<strong>host # <span>vm list</span></strong>
NAME           DATASTORE  LOADER     CPU  MEMORY  VNC  AUTO     STATE
almalinux8     default    uefi       2    2G      -    No       Stopped
freebsd13      default    bhyveload  1    256M    -    Yes [1]  Running (19258)
freebsd13alt   default    bhyveload  1    256M    -    No       Stopped
freebsd14      default    bhyveload  1    256M    -    No       Stopped
<span>freebsd14uefi  default    uefi       2    8G      -    No       Locked (w520.local)</span>
windows10ltsc  default    uefi       2    2G      -    No       Stopped
windows7       default    uefi       2    2G      -    No       Stopped

<strong>host # <span>rm /vm/freebsd14uefi/run.lock</span>

host # <span>vm list</span></strong>
NAME           DATASTORE  LOADER     CPU  MEMORY  VNC  AUTO     STATE
almalinux8     default    uefi       2    2G      -    No       Stopped
freebsd13      default    bhyveload  1    256M    -    Yes [1]  Running (19258)
freebsd13alt   default    bhyveload  1    256M    -    No       Stopped
freebsd14      default    bhyveload  1    256M    -    No       Stopped
<span>freebsd14uefi  default    uefi       2    8G      -    No       Stopped</span>
windows10ltsc  default    uefi       2    2G      -    No       Stopped
windows7       default    uefi       2    2G      -    No       Stopped
</pre>
<p>Now you may want to start the locked machine properly.</p>
<h2>Disk Resize</h2>
<p>By default <tt><b>vm-bhyve</b></tt> will create disks with 20 GB in size.</p>
<p>To resize the Bhyve virtual machine disk we would use <tt><b>truncate(1)</b></tt> command.</p>
<pre><strong>host # <span>vm stop freebsd13</span>

host # <span>cd /vm/freebsd13</span>

host # <span>truncate -s 40G disk0.img</span>

host # <span>vm start freebsd13
</span></strong></pre>
<p>If you are not sure about that – you may work on a copy instead.</p>
<pre><strong>host # <span>vm stop freebsd13</span></strong>

<strong>host # <span>truncate -s 40G disk0.img.NEW</span></strong>

<strong>host # <span>dd bs=1m if=disk0.img of=disk0.img.NEW conv=notrunc status=progress</span></strong>
  20865613824 bytes (21 GB, 19 GiB) transferred 43.002s, 485 MB/s
20480+0 records in
20480+0 records out
21474836480 bytes transferred in 43.454036 secs (494196586 bytes/sec)

<strong>host # <span>mv disk0.img disk0.img.BACKUP</span></strong>

<strong>host # <span>mv disk0.img.NEW disk0.img</span></strong>

<strong>host # <span>vm start freebsd13
</span></strong></pre>
<p>Now we need to resize the filesystem inside the VM.</p>
<pre><strong>freebsd13 # <span>lsblk</span></strong>
DEVICE         MAJ:MIN SIZE TYPE                                          LABEL MOUNT
nvd0             0:90   40G GPT                                               - -
  nvd0p1         0:91  512K freebsd-boot                           gpt/gptboot0 -
           -:-   492K -                                                 - -
  nvd0p2         0:92  2.0G freebsd-swap                              gpt/swap0 SWAP
  nvd0p3         0:93   18G freebsd-zfs                                gpt/zfs0 
           -:-   1.0M -                                                 - -

<strong>freebsd13 # <span>geom disk list</span></strong>
Geom name: nvd0
Providers:
1. Name: nvd0
   Mediasize: 42949672960 (40G)
   Sectorsize: 512
   Mode: r2w2e3
   descr: bhyve-NVMe
   lunid: 589cfc2081410001
   ident: NVME-4-0
   rotationrate: 0
   fwsectors: 0
   fwheads: 0

<strong>freebsd13 # <span>gpart show</span></strong>
=&gt;      40  41942960  nvd0  GPT  (40G) [CORRUPT]
        40      1024     1  freebsd-boot  (512K)
      1064       984        - free -  (492K)
      2048   4194304     2  freebsd-swap  (2.0G)
   4196352  37744640     3  freebsd-zfs  (18G)
  41940992      2008        - free -  (1.0M)

<strong>freebsd13 # <span>gpart recover nvd0</span></strong>
nvd0 recovered

<strong>freebsd13 # <span>gpart show</span></strong>
=&gt;      40  83886000  nvd0  GPT  (40G)
        40      1024     1  freebsd-boot  (512K)
      1064       984        - free -  (492K)
      2048   4194304     2  freebsd-swap  (2.0G)
   4196352  37744640     3  freebsd-zfs  (<span><strong>18G</strong></span>)
  41940992  41945048        - free -  (20G)

<strong>freebsd13 # <span>gpart resize -i 3 -a 1m nvd0</span></strong>
nvd0p3 resized

<strong>freebsd13 # <span>gpart show</span></strong>
=&gt;      40  83886000  nvd0  GPT  (40G)
        40      1024     1  freebsd-boot  (512K)
      1064       984        - free -  (492K)
      2048   4194304     2  freebsd-swap  (2.0G)
   4196352  79687680     3  freebsd-zfs  (<span><strong>38G</strong></span>)
  83884032      2008        - free -  (1.0M)

<strong>freebsd13 # <span>zpool status</span></strong>
  pool: zroot
 state: ONLINE
config:

        NAME        STATE     READ WRITE CKSUM
        zroot       ONLINE       0     0     0
          nvd0p3    ONLINE       0     0     0

<strong>freebsd13 # <span>zpool list</span></strong>
NAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
zroot  <span><strong>17.5G</strong></span>  17.0G   544M        -         -    87%    96%  1.00x    ONLINE  -

<strong>freebsd13 # <span>zpool set autoexpand=on zroot</span></strong>

<strong>freebsd13 # <span>zpool online -e zroot nvd0p3</span></strong>

<strong>freebsd13 # <span>zpool list</span></strong>
NAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT
zroot  <strong><span>37.5G</span></strong>  17.0G  20.5G        -         -    41%    45%  1.00x    ONLINE  -

</pre>
<h2>Summary</h2>
<p>I hope I was able to provide all needed information.</p>
<p>Let me know in comments if I missed something.</p>
<p>EOF</p>
			
			
								</div></div>]]></description>
        </item>
    </channel>
</rss>