<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 22 Sep 2025 21:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[AI-generated “workslop” is destroying productivity? (128 pts)]]></title>
            <link>https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity</link>
            <guid>45337253</guid>
            <pubDate>Mon, 22 Sep 2025 18:07:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity">https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity</a>, See on <a href="https://news.ycombinator.com/item?id=45337253">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="main"><svg x="0" y="0" viewBox="0 0 130 148" aria-labelledby="logo:Ram6:" role="img"><title id="logo:Ram6:">Harvard Business Review Logo</title><path d="M21.3729 54.332V50.78C21.3729 49.2013 20.5031 47.9022 19.0534 47.3102L18.8546 47.228L19.0534 47.154C20.4037 46.6689 21.2072 45.3862 21.2072 43.7171V40.6667C21.2072 37.3531 19.3434 35.6758 15.6736 35.6758H9.05469V59.3229H15.8392C19.509 59.3229 21.3729 57.6456 21.3729 54.332ZM13.9671 39.7869H15.1682C15.8807 39.7869 16.2866 40.2884 16.2866 41.16V43.75C16.2866 44.6216 15.8807 45.1231 15.1682 45.1231H13.9671V39.7869ZM13.9671 55.2036V49.1684H15.1351C15.9884 49.1684 16.4191 49.6618 16.4191 50.6402V53.8222C16.4191 54.7102 16.0132 55.1953 15.2676 55.1953H13.9671V55.2036Z"></path><path d="M34.8668 35.6758H29.8882V54.036C29.8882 54.9487 29.4823 55.4749 28.7698 55.4749C28.0574 55.4749 27.6184 54.9322 27.6184 54.036V35.6758H22.6729V54.1676C22.6729 57.4647 25.0669 59.5942 28.7698 59.5942C32.4728 59.5942 34.8668 57.4647 34.8668 54.1676V35.6758Z"></path><path d="M41.908 59.5859C45.6109 59.5859 48.005 57.4563 48.005 54.1592V51.0759C48.005 49.0779 47.392 47.9267 45.7352 46.8332L41.8252 44.0787C41.2287 43.6347 41.0216 43.199 41.0216 42.385V40.9625C41.0216 39.7703 41.6098 39.5236 42.1068 39.5236C42.7861 39.5236 43.1589 40.0334 43.1589 40.9625V43.7827H47.9719V40.831C47.9719 37.4352 45.7766 35.4043 42.1068 35.4043C38.437 35.4043 36.043 37.5339 36.043 40.831V43.3881C36.043 45.3861 36.656 46.5372 38.3128 47.6307L42.2228 50.4181C42.753 50.8127 43.0263 51.1827 43.0263 52.1119V54.0359C43.0263 54.965 42.6536 55.4747 41.9743 55.4747C41.4772 55.4747 40.8891 55.2281 40.8891 54.0359V50.7881H36.043V54.1674C36.043 57.5632 38.2382 59.5941 41.908 59.5941V59.5859Z"></path><path d="M54.334 35.6758H49.3223V59.3229H54.334V35.6758Z"></path><path d="M64.3746 59.3229H68.7568V35.6758H64.2752V41.8589L64.3497 47.0142H63.9024L61.2681 35.6758H56.2811V59.3229H60.7628V51.446L60.6965 46.488H61.1355L64.3746 59.3229Z"></path><path d="M80.1721 40.05V35.6758H70.7118V59.3229H80.1721V54.9404H75.591V49.3082H79.6005V44.9258H75.591V40.05H80.1721Z"></path><path d="M92.9786 51.0846C92.9786 49.0866 92.3656 47.9355 90.7088 46.842L86.7988 44.0875C86.2024 43.6435 85.9952 43.2078 85.9952 42.3938V40.9713C85.9952 39.7791 86.5834 39.5324 87.0805 39.5324C87.7597 39.5324 88.1325 40.0422 88.1325 40.9713V43.7915H92.9455V40.8398C92.9455 37.444 90.7502 35.4131 87.0805 35.4131C83.4107 35.4131 81.0166 37.5426 81.0166 40.8398V43.3969C81.0166 45.3949 81.6296 46.546 83.2864 47.6395L87.1964 50.4269C87.7266 50.8215 88 51.1915 88 52.1206V54.0446C88 54.9738 87.6272 55.4835 86.9479 55.4835C86.4509 55.4835 85.8627 55.2369 85.8627 54.0446V50.7969H81.0166V54.1762C81.0166 57.572 83.2118 59.6029 86.8816 59.6029C90.5514 59.6029 92.9786 57.4733 92.9786 54.1762V51.0846Z"></path><path d="M106.076 51.0846C106.076 49.0866 105.463 47.9355 103.806 46.842L99.8965 44.0875C99.3 43.6435 99.0929 43.2078 99.0929 42.3938V40.9713C99.0929 39.7791 99.6811 39.5324 100.178 39.5324C100.857 39.5324 101.23 40.0422 101.23 40.9713V43.7915H106.043V40.8398C106.043 37.444 103.848 35.4131 100.178 35.4131C96.5083 35.4131 94.1143 37.5426 94.1143 40.8398V43.3969C94.1143 45.3949 94.7273 46.546 96.3841 47.6395L100.294 50.4269C100.824 50.8215 101.098 51.1915 101.098 52.1206V54.0446C101.098 54.9738 100.725 55.4835 100.046 55.4835C99.5485 55.4835 98.9604 55.2369 98.9604 54.0446V50.7969H94.1143V54.1762C94.1143 57.572 96.3095 59.6029 99.9793 59.6029C103.649 59.6029 106.076 57.4733 106.076 54.1762V51.0846Z"></path><path d="M21.696 32.6417V8.98633H16.7091V18.2446H14.0748V8.98633H9.05469V32.6417H14.0748V22.6188H16.7091V32.6417H21.696Z"></path><path d="M30.8663 27.8651L31.3634 32.6422H36.2095L32.987 8.99512H25.7137L22.5244 32.6422H27.2048L27.6687 27.8651H30.8663ZM28.3977 19.8813L28.8782 15.3345H29.6238L30.1456 19.8813L30.6344 23.9513H27.909L28.3977 19.8813Z"></path><path d="M49.7285 32.6418C49.4386 31.7785 49.3972 30.5945 49.3972 29.542V24.4032C49.3972 22.6847 48.6599 21.4514 47.3096 20.9334L47.1025 20.8512L47.3096 20.7772C48.6599 20.292 49.4635 19.0094 49.4635 17.3403V13.9938C49.4635 10.6803 47.5996 9.00293 43.9298 9.00293H37.1121V32.65H42.0576V23.03H43.1925C43.7641 23.03 44.4434 23.2849 44.4434 24.5018V29.7065C44.4434 30.652 44.4848 31.8278 44.783 32.6418H49.7285ZM43.3665 18.812H42.0659V13.2127H43.3665C44.0706 13.2127 44.4848 13.7307 44.4848 14.5858V17.4389C44.4848 18.3105 44.0789 18.812 43.3665 18.812Z"></path><path d="M58.5261 8.9873L57.4989 19.7913L57.0598 25.0782H56.538L56.0989 19.7913L55.0303 8.9873H49.9688L53.5143 32.6426H59.8101L63.3971 8.9873H58.5261Z"></path><path d="M84.5049 32.6428H89.4422C89.1522 31.7795 89.1108 30.5955 89.1108 29.543V24.4041C89.1108 22.6857 88.3735 21.4524 87.0232 20.9343L86.8244 20.8521L87.0315 20.7781C88.3818 20.293 89.1854 19.0104 89.1854 17.3412V13.9948C89.1854 10.6812 87.3215 9.00391 83.6517 9.00391H76.834V32.651H81.7795V23.031H82.9144C83.486 23.031 84.1653 23.2859 84.1653 24.5028V29.7075C84.1653 30.653 84.2067 31.8288 84.5049 32.6428ZM83.0801 18.813H81.7795V13.2137H83.0801C83.7842 13.2137 84.1984 13.7317 84.1984 14.5868V17.4399C84.1984 18.3115 83.7925 18.813 83.0801 18.813Z"></path><path d="M103.276 14.414C103.276 11.0675 100.973 8.9873 97.2784 8.9873H90.9246V32.6344H97.2784C100.981 32.6344 103.276 30.5542 103.276 27.2077V14.4057V14.414ZM98.2973 27.0515C98.2973 27.9149 97.8831 28.4246 97.179 28.4246H95.8784V13.2053H97.179C97.8831 13.2053 98.2973 13.7233 98.2973 14.5784V27.0515Z"></path><path d="M70.6041 27.8651L71.1094 32.6422H75.9555L72.7331 8.99511H65.4598L62.2704 32.6422H66.9509L67.4148 27.8651H70.6041ZM68.1355 19.8813L68.616 15.3344H69.3615L69.8834 19.8813L70.3721 23.9513H67.6467L68.1355 19.8813Z"></path><path d="M16.7336 86.0129H21.6708C21.3809 85.1496 21.3394 83.9656 21.3394 82.9131V77.7742C21.3394 76.0558 20.6022 74.8225 19.2519 74.3045L19.0531 74.2222L19.2602 74.1482C20.6105 73.6631 21.414 72.3805 21.414 70.7114V67.3649C21.414 64.0514 19.5501 62.374 15.8803 62.374H9.06264V86.0211H14.0082V76.4011H15.1431C15.7146 76.4011 16.3939 76.656 16.3939 77.8729V83.0776C16.3939 84.0231 16.4354 85.1989 16.7336 86.0129ZM15.3087 72.1831H14.0082V66.5838H15.3087C16.0129 66.5838 16.4271 67.1018 16.4271 67.9569V70.81C16.4271 71.6816 16.0212 72.1831 15.3087 72.1831Z"></path><path d="M32.5472 66.7399V62.3574H23.0869V86.0128H32.5472V81.6303H27.9662V75.9899H31.9756V71.6156H27.9662V66.7399H32.5472Z"></path><path d="M52.3956 62.3574H47.3838V86.0045H52.3956V62.3574Z"></path><path d="M63.7367 66.7398V62.3573H54.2764V86.0127H63.7367V81.6302H59.1557V75.9898H63.1651V71.6156H59.1557V66.7398H63.7367Z"></path><path d="M78.9626 62.3573L77.9851 76.0227H77.2395L77.2313 75.9487L76.3035 62.3573H72.2609L71.3911 76.0227H70.6372L70.2562 71.1058L69.6266 62.3573H64.4491L67.1497 86.0127H72.8408L73.7768 73.3093H74.4478L75.3508 86.0127H81.0419L83.7424 62.3573H78.9626Z"></path><path d="M41.7842 62.3573L40.757 73.1613L40.3097 78.4482H39.7961L39.3488 73.1613L38.2884 62.3573H33.2186L36.7724 86.0127H43.0682L46.6469 62.3573H41.7842Z"></path><path d="M64.9959 148L64.0929 147.556C39.1499 135.387 22.9548 124.509 13.1218 113.319C4.05085 103 0 92.1464 0 78.1276V0H130V78.1276C130 92.1464 125.957 103 116.878 113.319C107.045 124.509 90.8418 135.387 65.9071 147.556L65.0042 148H64.9959ZM4.10884 4.07822V78.1276C4.10884 91.0858 7.8449 101.117 16.2117 110.638C25.5643 121.27 41.0801 131.712 64.9959 143.453C88.9116 131.712 104.436 121.27 113.78 110.638C122.155 101.117 125.883 91.094 125.883 78.1276V4.07822H4.10055H4.10884Z"></path></svg><div><div><p><span>September 22, 2025<!-- -->, Updated September 22, 2025</span></p></div><div><p><img src="https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid.jpg" sizes="(min-width: 64em) 84vw, 100vw" srcset="https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid.jpg 1200w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-300x169.jpg 300w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-1024x576.jpg 1024w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-768x432.jpg 768w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-500x281.jpg 500w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-383x215.jpg 383w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-700x394.jpg 700w, https://hbr.org/resources/images/article_assets/2025/09/Sep25_22_AIslopmid-850x478.jpg 850w" alt=""></p><p><span>HBR Staff/AI</span></p></div><p data-first-paragraph="true">A confusing contradiction is unfolding in companies embracing generative AI tools: while workers are largely following mandates to embrace the technology, few are seeing it create real value. Consider, for instance, that the number of companies with fully AI-led processes <a href="https://www.accenture.com/content/dam/accenture/final/accenture-com/document-3/Accenture-Reinventing-Enterprise-Operations-FA-9-25-24.pdf">nearly doubled</a> last year, while AI use has likewise <a href="https://www.gallup.com/workplace/691643/work-nearly-doubled-two-years.aspx">doubled</a> at work since 2023. Yet a <a href="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf">recent report</a> from the MIT Media Lab found that 95% of organizations see no measurable return on their investment in these technologies. So much activity, so much enthusiasm, so little return. Why?</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen3-Omni: Native Omni AI model for text, image and video (173 pts)]]></title>
            <link>https://github.com/QwenLM/Qwen3-Omni</link>
            <guid>45336989</guid>
            <pubDate>Mon, 22 Sep 2025 17:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/QwenLM/Qwen3-Omni">https://github.com/QwenLM/Qwen3-Omni</a>, See on <a href="https://news.ycombinator.com/item?id=45336989">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Qwen3-Omni</h2><a id="user-content-qwen3-omni" aria-label="Permalink: Qwen3-Omni" href="#qwen3-omni"></a></p>

<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1df0c9499512c1ce1c446481874a712de3eacdb6a5b674c1cd3bf0fafd4280cd/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f2f5177656e332d4f6d6e692f7177656e335f6f6d6e695f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/1df0c9499512c1ce1c446481874a712de3eacdb6a5b674c1cd3bf0fafd4280cd/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f2f5177656e332d4f6d6e692f7177656e335f6f6d6e695f6c6f676f2e706e67" width="400" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com//Qwen3-Omni/qwen3_omni_logo.png"></a>
</p><p dir="auto">
        💜 <a href="https://chat.qwen.ai/" rel="nofollow"><b>Qwen Chat</b></a>&nbsp;&nbsp; | &nbsp;&nbsp;🤗 <a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe" rel="nofollow">Hugging Face</a>&nbsp;&nbsp; | &nbsp;&nbsp;🤖 <a href="https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f" rel="nofollow">ModelScope</a>&nbsp;&nbsp; | &nbsp;&nbsp;📑 <a href="https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&amp;from=research.latest-advancements-list" rel="nofollow">Blog</a>&nbsp;&nbsp; | &nbsp;&nbsp;📚 <a href="https://github.com/QwenLM/Qwen3-Omni/tree/main/cookbooks">Cookbooks</a>&nbsp;&nbsp; | &nbsp;&nbsp;📑 <a href="https://github.com/QwenLM/Qwen3-Omni/tree/main/assets/Qwen3_Omni.pdf">Paper</a>&nbsp;&nbsp;
<br>
🖥️ <a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo" rel="nofollow">Hugging Face Demo</a>&nbsp;&nbsp; | &nbsp;&nbsp; 🖥️ <a href="https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo" rel="nofollow">ModelScope Demo</a>&nbsp;&nbsp; | &nbsp;&nbsp;💬 <a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat (微信)</a>&nbsp;&nbsp; | &nbsp;&nbsp;🫨 <a href="https://discord.gg/CV4E9rpNSD" rel="nofollow">Discord</a>&nbsp;&nbsp; | &nbsp;&nbsp;📑 <a href="https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni" rel="nofollow">API</a>
</p>
<p dir="auto">We release <strong>Qwen3-Omni</strong>, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information 😃</p>
<details open="">
<summary>English Version</summary>
<a href="https://youtu.be/_zdOrPju4_g" rel="nofollow">
  <img src="https://camo.githubusercontent.com/c71f457935383f018f713df40205bb87a7bb92a1f46e3456e14e6f81b438541c/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f766964656f636f7665722e706e67" alt="Open English Video" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png">
</a>
</details>
<details>
<summary>Chinese Version</summary>
<a href="https://youtu.be/Wtjsw5deXfQ" rel="nofollow">
  <img src="https://camo.githubusercontent.com/c71f457935383f018f713df40205bb87a7bb92a1f46e3456e14e6f81b438541c/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f766964656f636f7665722e706e67" alt="打开中文视频" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/videocover.png">
</a>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">News</h2><a id="user-content-news" aria-label="Permalink: News" href="#news"></a></p>
<ul dir="auto">
<li>2025.09.22: 🎉🎉🎉 We have released <a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe" rel="nofollow">Qwen3-Omni</a>. For more details, please check our <a href="https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&amp;from=research.latest-advancements-list" rel="nofollow">blog</a>!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents </h2><a id="user-content-contents-" aria-label="Permalink: Contents " href="#contents-"></a></p>
<ul dir="auto">
<li><a href="#overview">Overview</a>
<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#model-architecture">Model Architecture</a></li>
<li><a href="#cookbooks-for-usage-cases">Cookbooks for Usage Cases</a></li>
</ul>
</li>
<li><a href="#quickstart">QuickStart</a>
<ul dir="auto">
<li><a href="#model-description-and-download">Model Description and Download</a></li>
<li><a href="#transformers-usage">Transformers Usage</a></li>
<li><a href="#vllm-usage">vLLM Usage</a></li>
<li><a href="#dashscope-api-usage">DashScope API Usage</a></li>
<li><a href="#usage-tips-recommended-reading">Usage Tips (Recommended Reading)</a></li>
</ul>
</li>
<li><a href="#interaction-with-qwen3-omni">Interaction with Qwen3-Omni</a>
<ul dir="auto">
<li><a href="#online-demo">Online Demo</a></li>
<li><a href="#real-time-interaction">Real-Time Interaction</a></li>
<li><a href="#launch-local-web-ui-demo">Launch Local Web UI Demo</a></li>
</ul>
</li>
<li><a href="#-docker">Docker</a></li>
<li><a href="#evaluation">Evaluation</a>
<ul dir="auto">
<li><a href="#performance-of-qwen3-omni">Performance of Qwen3-Omni</a></li>
<li><a href="#setting-for-evaluation">Setting for Evaluation</a></li>
</ul>
</li>
<li><a href="#citation">Citation</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Introduction</h3><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d96c10768d1a12d6718515935a0da75295e66b53a76407358205764576cb1e7/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f71336f5f696e74726f64756374696f6e2e706e67"><img src="https://camo.githubusercontent.com/7d96c10768d1a12d6718515935a0da75295e66b53a76407358205764576cb1e7/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f71336f5f696e74726f64756374696f6e2e706e67" width="90%" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png"></a>
</p><p dir="auto">Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>State-of-the-art across modalities</strong>: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.</p>
</li>
<li>
<p dir="auto"><strong>Multilingual</strong>: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.</p>
<ul dir="auto">
<li><strong>Speech Input</strong>: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.</li>
<li><strong>Speech Output</strong>: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Novel Architecture</strong>: MoE-based Thinker–Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.</p>
</li>
<li>
<p dir="auto"><strong>Real-time Audio/Video Interaction</strong>: Low-latency streaming with natural turn-taking and immediate text or speech responses.</p>
</li>
<li>
<p dir="auto"><strong>Flexible Control</strong>: Customize behavior via system prompts for fine-grained control and easy adaptation.</p>
</li>
<li>
<p dir="auto"><strong>Detailed Audio Captioner</strong>: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Model Architecture</h3><a id="user-content-model-architecture" aria-label="Permalink: Model Architecture" href="#model-architecture"></a></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c4e0a9eeee86b979ea2d52f01ec3ec13f133324ef14e01c2c0d42ba8044da756/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f6f766572766965772e706e67"><img src="https://camo.githubusercontent.com/c4e0a9eeee86b979ea2d52f01ec3ec13f133324ef14e01c2c0d42ba8044da756/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4f6d6e692f6f766572766965772e706e67" width="80%" data-canonical-src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/overview.png"></a>
</p><p dir="auto"><h3 tabindex="-1" dir="auto">Cookbooks for Usage Cases</h3><a id="user-content-cookbooks-for-usage-cases" aria-label="Permalink: Cookbooks for Usage Cases" href="#cookbooks-for-usage-cases"></a></p>
<p dir="auto">Qwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the <a href="#quickstart">QuickStart</a> guide to download the model and install the necessary inference environment dependencies, then run and experiment locally—try modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!</p>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Cookbook</th>
      <th>Description</th>
      <th>Open</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="6">Audio</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb">Speech Recognition</a></td>
      <td>Speech recognition, supporting multiple languages and long audio.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb">Speech Translation</a></td>
      <td>Speech-to-Text / Speech-to-Speech translation.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb">Music Analysis</a></td>
      <td>Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb">Sound Analysis</a></td>
      <td>Description and analysis of various sound effects and audio signals.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb">Audio Caption</a></td>
      <td>Audio captioning, detailed description of any audio input.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb">Mixed Audio Analysis</a></td>
      <td>Analysis of mixed audio content, such as speech, music, and environmental sounds.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td rowspan="7">Visual</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb">OCR</a></td>
      <td>OCR for complex images.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb">Object Grounding</a></td>
      <td>Target detection and grounding.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb">Image Question</a></td>
      <td>Answering arbitrary questions about any image.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb">Image Math</a></td>
      <td>Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb">Video Description</a></td>
      <td>Detailed description of video content.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb">Video Navigation</a></td>
      <td>Generating navigation commands from first-person motion videos.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb">Video Scene Transition</a></td>
      <td>Analysis of scene transitions in videos.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td rowspan="3">Audio-Visual</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb">Audio Visual Question</a></td>
      <td>Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb">Audio Visual Interaction</a></td>
      <td>Interactive communication with the model using audio-visual inputs, including task specification via audio.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb">Audio Visual Dialogue</a></td>
      <td>Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td>Agent</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb">Audio Function Call</a></td>
      <td>Using audio input to perform function calls, enabling agent-like behaviors.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
    <tr>
      <td>Downstream Task Fine-tuning</td>
      <td><a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb">Omni Captioner</a></td>
      <td>Introduction and capability demonstration of <strong>Qwen3-Omni-30B-A3B-Captioner</strong>, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.</td>
      <td><a href="https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">QuickStart</h2><a id="user-content-quickstart" aria-label="Permalink: QuickStart" href="#quickstart"></a></p>
<p dir="auto">Here, we provide several methods to quickly get started with Qwen3-Omni. If you want complete experience of Qwen3-Omni, you can use <a href="#transformers-usage">Hugging Face Transformers</a>. However, since Qwen3-Omni employs an MoE architecture, inference speed with Hugging Face Transformers on MoE models can be very slow. For large-scale invocation or low-latency requirements, we highly recommend using <a href="#vllm-usage">vLLM</a> or performing inference via the <a href="#dashscope-api-usage">DashScope API</a>. We also strongly suggest using our provided <a href="#-docker">Docker</a> image, which includes a complete runtime environment for both Hugging Face Transformers and vLLM. In addition, our <a href="https://github.com/QwenLM/Qwen3-Omni/tree/main/cookbooks">cookbooks</a> offer some use cases to show Qwen3-Omni's capabilities. Welcome to learn more!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Model Description and Download</h3><a id="user-content-model-description-and-download" aria-label="Permalink: Model Description and Download" href="#model-description-and-download"></a></p>
<p dir="auto">Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen3-Omni-30B-A3B-Instruct</td>
<td>The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the <a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf">Qwen3-Omni Technical Report</a>.</td>
</tr>
<tr>
<td>Qwen3-Omni-30B-A3B-Thinking</td>
<td>The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the <a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf">Qwen3-Omni Technical Report</a>.</td>
</tr>
<tr>
<td>Qwen3-Omni-30B-A3B-Captioner</td>
<td>A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's <a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb">cookbook</a> or <a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Captioner-Demo" rel="nofollow">Hugging Face Demo</a> and <a href="https://modelscope.cn/studios/Qwen/Qwen3-Omni-Captioner-Demo" rel="nofollow">ModelScope Demo</a>.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">During loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Download through ModelScope (recommended for users in Mainland China)
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

# Download through Hugging Face
pip install -U &quot;huggingface_hub[cli]&quot;
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner"><pre><span><span>#</span> Download through ModelScope (recommended for users in Mainland China)</span>
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

<span><span>#</span> Download through Hugging Face</span>
pip install -U <span><span>"</span>huggingface_hub[cli]<span>"</span></span>
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Transformers Usage</h3><a id="user-content-transformers-usage" aria-label="Permalink: Transformers Usage" href="#transformers-usage"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation</h4><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you <strong>create a new Python environment</strong> or use our <a href="#-docker">Docker</a> to avoid environment runtime issues.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# If you already have transformers installed, please uninstall it first, or create a new Python environment
# pip uninstall transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate"><pre><span><span>#</span> If you already have transformers installed, please uninstall it first, or create a new Python environment</span>
<span><span>#</span> pip uninstall transformers</span>
pip install git+https://github.com/huggingface/transformers
pip install accelerate</pre></div>
<p dir="auto">We offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has <code>ffmpeg</code> installed:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install qwen-omni-utils -U"><pre>pip install qwen-omni-utils -U</pre></div>
<p dir="auto">Additionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using <a href="#vllm-usage">vLLM</a> for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -U flash-attn --no-build-isolation"><pre>pip install -U flash-attn --no-build-isolation</pre></div>
<p dir="auto">Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention repository</a>. FlashAttention 2 can only be used when a model is loaded in <code>torch.float16</code> or <code>torch.bfloat16</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Code Snippet</h4><a id="user-content-code-snippet" aria-label="Permalink: Code Snippet" href="#code-snippet"></a></p>
<p dir="auto">Here is a code snippet to show you how to use Qwen3-Omni with <code>transformers</code> and <code>qwen_omni_utils</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import soundfile as sf

from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;
# MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Thinking&quot;

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    attn_implementation=&quot;flash_attention_2&quot;,
)

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

conversation = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see and hear? Answer in one short sentence.&quot;}
        ],
    },
]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors=&quot;pt&quot;, 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, 
                                 speaker=&quot;Ethan&quot;, 
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs[&quot;input_ids&quot;].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)
if audio is not None:
    sf.write(
        &quot;output.wav&quot;,
        audio.reshape(-1).detach().cpu().numpy(),
        samplerate=24000,
    )"><pre><span>import</span> <span>soundfile</span> <span>as</span> <span>sf</span>

<span>from</span> <span>transformers</span> <span>import</span> <span>Qwen3OmniMoeForConditionalGeneration</span>, <span>Qwen3OmniMoeProcessor</span>
<span>from</span> <span>qwen_omni_utils</span> <span>import</span> <span>process_mm_info</span>

<span>MODEL_PATH</span> <span>=</span> <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>
<span># MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"</span>

<span>model</span> <span>=</span> <span>Qwen3OmniMoeForConditionalGeneration</span>.<span>from_pretrained</span>(
    <span>MODEL_PATH</span>,
    <span>dtype</span><span>=</span><span>"auto"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>,
)

<span>processor</span> <span>=</span> <span>Qwen3OmniMoeProcessor</span>.<span>from_pretrained</span>(<span>MODEL_PATH</span>)

<span>conversation</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see and hear? Answer in one short sentence."</span>}
        ],
    },
]

<span># Set whether to use audio in video</span>
<span>USE_AUDIO_IN_VIDEO</span> <span>=</span> <span>True</span>

<span># Preparation for inference</span>
<span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(<span>conversation</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>conversation</span>, <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)
<span>inputs</span> <span>=</span> <span>processor</span>(<span>text</span><span>=</span><span>text</span>, 
                   <span>audio</span><span>=</span><span>audios</span>, 
                   <span>images</span><span>=</span><span>images</span>, 
                   <span>videos</span><span>=</span><span>videos</span>, 
                   <span>return_tensors</span><span>=</span><span>"pt"</span>, 
                   <span>padding</span><span>=</span><span>True</span>, 
                   <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)
<span>inputs</span> <span>=</span> <span>inputs</span>.<span>to</span>(<span>model</span>.<span>device</span>).<span>to</span>(<span>model</span>.<span>dtype</span>)

<span># Inference: Generation of the output text and audio</span>
<span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>inputs</span>, 
                                 <span>speaker</span><span>=</span><span>"Ethan"</span>, 
                                 <span>thinker_return_dict_in_generate</span><span>=</span><span>True</span>,
                                 <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)

<span>text</span> <span>=</span> <span>processor</span>.<span>batch_decode</span>(<span>text_ids</span>.<span>sequences</span>[:, <span>inputs</span>[<span>"input_ids"</span>].<span>shape</span>[<span>1</span>] :],
                              <span>skip_special_tokens</span><span>=</span><span>True</span>,
                              <span>clean_up_tokenization_spaces</span><span>=</span><span>False</span>)
<span>print</span>(<span>text</span>)
<span>if</span> <span>audio</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
    <span>sf</span>.<span>write</span>(
        <span>"output.wav"</span>,
        <span>audio</span>.<span>reshape</span>(<span>-</span><span>1</span>).<span>detach</span>().<span>cpu</span>().<span>numpy</span>(),
        <span>samplerate</span><span>=</span><span>24000</span>,
    )</pre></div>
<p dir="auto">Here are some more advanced usage examples. You can expand the sections below to learn more.</p>
<details>
<summary>Batch inference</summary>
<p dir="auto">The model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when <code>return_audio=False</code> is set. Here is an example.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;
# MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Thinking&quot;

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    attn_implementation=&quot;flash_attention_2&quot;,
)
model.disable_talker()

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

# Conversation with image only
conversation1 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see in this image? Answer in one sentence.&quot;},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you hear in this audio?&quot;},
        ]
    }
]

# Conversation with pure text and system prompt
conversation3 = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen-Omni.&quot;}
        ],
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;Who are you?&quot;
    }
]

# Conversation with mixed media
conversation4 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see and hear? Answer in one sentence.&quot;}
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors=&quot;pt&quot;, 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch inference does not support returning audio
text_ids, audio = model.generate(**inputs,
                                 return_audio=False,
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs[&quot;input_ids&quot;].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>Qwen3OmniMoeForConditionalGeneration</span>, <span>Qwen3OmniMoeProcessor</span>
<span>from</span> <span>qwen_omni_utils</span> <span>import</span> <span>process_mm_info</span>

<span>MODEL_PATH</span> <span>=</span> <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>
<span># MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"</span>

<span>model</span> <span>=</span> <span>Qwen3OmniMoeForConditionalGeneration</span>.<span>from_pretrained</span>(
    <span>MODEL_PATH</span>,
    <span>dtype</span><span>=</span><span>"auto"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>,
)
<span>model</span>.<span>disable_talker</span>()

<span>processor</span> <span>=</span> <span>Qwen3OmniMoeProcessor</span>.<span>from_pretrained</span>(<span>MODEL_PATH</span>)

<span># Conversation with image only</span>
<span>conversation1</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see in this image? Answer in one sentence."</span>},
        ]
    }
]

<span># Conversation with audio only</span>
<span>conversation2</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you hear in this audio?"</span>},
        ]
    }
]

<span># Conversation with pure text and system prompt</span>
<span>conversation3</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"system"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"You are Qwen-Omni."</span>}
        ],
    },
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: <span>"Who are you?"</span>
    }
]

<span># Conversation with mixed media</span>
<span>conversation4</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see and hear? Answer in one sentence."</span>}
        ],
    }
]

<span># Combine messages for batch processing</span>
<span>conversations</span> <span>=</span> [<span>conversation1</span>, <span>conversation2</span>, <span>conversation3</span>, <span>conversation4</span>]

<span># Set whether to use audio in video</span>
<span>USE_AUDIO_IN_VIDEO</span> <span>=</span> <span>True</span>

<span># Preparation for batch inference</span>
<span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(<span>conversations</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>conversations</span>, <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)

<span>inputs</span> <span>=</span> <span>processor</span>(<span>text</span><span>=</span><span>text</span>, 
                   <span>audio</span><span>=</span><span>audios</span>, 
                   <span>images</span><span>=</span><span>images</span>, 
                   <span>videos</span><span>=</span><span>videos</span>, 
                   <span>return_tensors</span><span>=</span><span>"pt"</span>, 
                   <span>padding</span><span>=</span><span>True</span>, 
                   <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)
<span>inputs</span> <span>=</span> <span>inputs</span>.<span>to</span>(<span>model</span>.<span>device</span>).<span>to</span>(<span>model</span>.<span>dtype</span>)

<span># Batch inference does not support returning audio</span>
<span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>inputs</span>,
                                 <span>return_audio</span><span>=</span><span>False</span>,
                                 <span>thinker_return_dict_in_generate</span><span>=</span><span>True</span>,
                                 <span>use_audio_in_video</span><span>=</span><span>USE_AUDIO_IN_VIDEO</span>)

<span>text</span> <span>=</span> <span>processor</span>.<span>batch_decode</span>(<span>text_ids</span>.<span>sequences</span>[:, <span>inputs</span>[<span>"input_ids"</span>].<span>shape</span>[<span>1</span>] :],
                              <span>skip_special_tokens</span><span>=</span><span>True</span>,
                              <span>clean_up_tokenization_spaces</span><span>=</span><span>False</span>)
<span>print</span>(<span>text</span>)</pre></div>
</details>
<details>
<summary>Use audio output or not</summary>
<p dir="auto">The model supports both text and audio outputs. If users do not need audio outputs, they can call <code>model.disable_talker()</code> after initializing the model. This option will save about <code>10GB</code> of GPU memory, but the <code>return_audio</code> option for the <code>generate</code> function will only allow <code>False</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;,
    dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    attn_implementation=&quot;flash_attention_2&quot;,
)
model.disable_talker()"><pre><span>model</span> <span>=</span> <span>Qwen3OmniMoeForConditionalGeneration</span>.<span>from_pretrained</span>(
    <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>,
    <span>dtype</span><span>=</span><span>"auto"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>,
)
<span>model</span>.<span>disable_talker</span>()</pre></div>
<p dir="auto">For a more flexible experience, we recommend that users decide whether to return audio when the <code>generate</code> function is called. If <code>return_audio</code> is set to <code>False</code>, the model will only return text outputs, resulting in faster text responses.</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;,
    dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;,
    attn_implementation=&quot;flash_attention_2&quot;,
)
...
text_ids, _ = model.generate(..., return_audio=False)```

</details>

<details>
<summary>Change voice type of output audio</summary>

Qwen3-Omni supports changing the voice of the output audio. The `&quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;` checkpoint supports three voice types as follows:

| Voice Type | Gender | Description |
|------------|--------|-------------|
| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |
| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |
| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |

Users can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.

```python
text_ids, audio = model.generate(..., speaker=&quot;Ethan&quot;)"><pre><span>model</span> <span>=</span> <span>Qwen3OmniMoeForConditionalGeneration</span>.<span>from_pretrained</span>(
    <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>,
    <span>dtype</span><span>=</span><span>"auto"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>attn_implementation</span><span>=</span><span>"flash_attention_2"</span>,
)
...
<span>text_ids</span>, <span>_</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>return_audio</span><span>=</span><span>False</span>)<span>``</span>`

<span>&lt;</span><span>/</span><span>details</span><span>&gt;</span>

<span>&lt;</span><span>details</span><span>&gt;</span>
<span>&lt;</span><span>summary</span><span>&gt;</span><span>Change</span> <span>voice</span> type <span>of</span> <span>output</span> <span>audio</span><span>&lt;</span><span>/</span><span>summary</span><span>&gt;</span>

<span>Qwen3</span><span>-</span><span>Omni</span> <span>supports</span> <span>changing</span> <span>the</span> <span>voice</span> <span>of</span> <span>the</span> <span>output</span> <span>audio</span>. <span>The</span> <span>`"Qwen/Qwen3-Omni-30B-A3B-Instruct"`</span> <span>checkpoint</span> <span>supports</span> <span>three</span> <span>voice</span> <span>types</span> <span>as</span> <span>follows</span>:

<span>|</span> <span>Voice</span> <span>Type</span> <span>|</span> <span>Gender</span> <span>|</span> <span>Description</span> <span>|</span>
<span>|</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>|</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>|</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>|</span>
<span>|</span> <span>Ethan</span>      <span>|</span> <span>Male</span>   <span>|</span> <span>A</span> <span>bright</span>, <span>upbeat</span> <span>voice</span> <span>with</span> <span>infectious</span> <span>energy</span> <span>and</span> <span>a</span> <span>warm</span>, <span>approachable</span> <span>vibe</span>. <span>|</span>
<span>|</span> <span>Chelsie</span>    <span>|</span> <span>Female</span> <span>|</span> <span>A</span> <span>honeyed</span>, <span>velvety</span> <span>voice</span> <span>that</span> <span>carries</span> <span>a</span> <span>gentle</span> <span>warmth</span> <span>and</span> <span>luminous</span> <span>clarity</span>. <span>|</span>
<span>|</span> <span>Aiden</span>      <span>|</span> <span>Male</span>   <span>|</span> <span>A</span> <span>warm</span>, <span>laid</span><span>-</span><span>back</span> <span>American</span> <span>voice</span> <span>with</span> <span>a</span> <span>gentle</span>, <span>boyish</span> <span>charm</span>. <span>|</span>

<span>Users</span> <span>can</span> <span>use</span> <span>the</span> <span>`speaker`</span> <span>parameter</span> <span>of</span> <span>the</span> <span>`generate`</span> <span>function</span> <span>to</span> <span>specify</span> <span>the</span> <span>voice</span> <span>type</span>. <span>By</span> <span>default</span>, <span>if</span> <span>`speaker`</span> <span><span>is</span> <span>not</span></span> <span>specified</span>, <span>the</span> <span>voice</span> <span>type</span> <span>is</span> <span>`Ethan`</span>.

<span>``</span>`<span>python</span>
<span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>speaker</span><span>=</span><span>"Ethan"</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="text_ids, audio = model.generate(..., speaker=&quot;Chelsie&quot;)"><pre><span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>speaker</span><span>=</span><span>"Chelsie"</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="text_ids, audio = model.generate(..., speaker=&quot;Aiden&quot;)"><pre><span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>speaker</span><span>=</span><span>"Aiden"</span>)</pre></div>
</details>
<p dir="auto">Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to <a href="#usage-tips-recommended-reading">Usage Tips</a> and <a href="#cookbooks-for-usage-cases">Cookbooks for Usage Cases</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">vLLM Usage</h3><a id="user-content-vllm-usage" aria-label="Permalink: vLLM Usage" href="#vllm-usage"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation</h4><a id="user-content-installation-1" aria-label="Permalink: Installation" href="#installation-1"></a></p>
<p dir="auto">We strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and <strong>audio output inference support for the Instruct model will be released in the near future</strong>, you can follow the commands below to install vLLM from source. Please note that we recommend you <strong>create a new Python environment</strong> or use our provided <a href="#-docker">Docker</a> to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the <a href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html#set-up-using-python-only-build-without-compilation" rel="nofollow">vLLM official documentation</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
cd vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation
# If you meet an &quot;Undefined symbol&quot; error while using VLLM_USE_PRECOMPILED=1, please use &quot;pip install -e . -v&quot; to build from source.
# Install the Transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation"><pre>git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
<span>cd</span> vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
<span>export</span> VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e <span>.</span> -v --no-build-isolation
<span><span>#</span> If you meet an "Undefined symbol" error while using VLLM_USE_PRECOMPILED=1, please use "pip install -e . -v" to build from source.</span>
<span><span>#</span> Install the Transformers</span>
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Inference</h4><a id="user-content-inference" aria-label="Permalink: Inference" href="#inference"></a></p>
<p dir="auto">You can use the following code for vLLM inference. The <code>limit_mm_per_prompt</code> parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting <code>tensor_parallel_size</code> greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, <code>max_num_seqs</code> indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the <a href="https://docs.vllm.ai/en/latest/api/vllm/index.html#vllm.LLM" rel="nofollow">vLLM official documentation</a>. Below is a simple example of how to run Qwen3-Omni with vLLM:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;
    # MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Thinking&quot;

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4&quot;}
            ], 
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        &quot;mm_processor_kwargs&quot;: {
            &quot;use_audio_in_video&quot;: True,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios

    outputs = llm.generate([inputs], sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)"><pre><span>import</span> <span>os</span>
<span>import</span> <span>torch</span>

<span>from</span> <span>vllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>Qwen3OmniMoeProcessor</span>
<span>from</span> <span>qwen_omni_utils</span> <span>import</span> <span>process_mm_info</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span>:
    <span># vLLM engine v1 not supported yet</span>
    <span>os</span>.<span>environ</span>[<span>'VLLM_USE_V1'</span>] <span>=</span> <span>'0'</span>

    <span>MODEL_PATH</span> <span>=</span> <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>
    <span># MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"</span>

    <span>llm</span> <span>=</span> <span>LLM</span>(
            <span>model</span><span>=</span><span>MODEL_PATH</span>, <span>trust_remote_code</span><span>=</span><span>True</span>, <span>gpu_memory_utilization</span><span>=</span><span>0.95</span>,
            <span>tensor_parallel_size</span><span>=</span><span>torch</span>.<span>cuda</span>.<span>device_count</span>(),
            <span>limit_mm_per_prompt</span><span>=</span>{<span>'image'</span>: <span>3</span>, <span>'video'</span>: <span>3</span>, <span>'audio'</span>: <span>3</span>},
            <span>max_num_seqs</span><span>=</span><span>8</span>,
            <span>max_model_len</span><span>=</span><span>32768</span>,
            <span>seed</span><span>=</span><span>1234</span>,
    )

    <span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(
        <span>temperature</span><span>=</span><span>0.6</span>,
        <span>top_p</span><span>=</span><span>0.95</span>,
        <span>top_k</span><span>=</span><span>20</span>,
        <span>max_tokens</span><span>=</span><span>16384</span>,
    )

    <span>processor</span> <span>=</span> <span>Qwen3OmniMoeProcessor</span>.<span>from_pretrained</span>(<span>MODEL_PATH</span>)

    <span>messages</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"video"</span>, <span>"video"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"</span>}
            ], 
        }
    ]

    <span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(
        <span>messages</span>,
        <span>tokenize</span><span>=</span><span>False</span>,
        <span>add_generation_prompt</span><span>=</span><span>True</span>,
    )
    <span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>messages</span>, <span>use_audio_in_video</span><span>=</span><span>True</span>)

    <span>inputs</span> <span>=</span> {
        <span>'prompt'</span>: <span>text</span>,
        <span>'multi_modal_data'</span>: {},
        <span>"mm_processor_kwargs"</span>: {
            <span>"use_audio_in_video"</span>: <span>True</span>,
        },
    }

    <span>if</span> <span>images</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'image'</span>] <span>=</span> <span>images</span>
    <span>if</span> <span>videos</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'video'</span>] <span>=</span> <span>videos</span>
    <span>if</span> <span>audios</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'audio'</span>] <span>=</span> <span>audios</span>

    <span>outputs</span> <span>=</span> <span>llm</span>.<span>generate</span>([<span>inputs</span>], <span>sampling_params</span><span>=</span><span>sampling_params</span>)

    <span>print</span>(<span>outputs</span>[<span>0</span>].<span>outputs</span>[<span>0</span>].<span>text</span>)</pre></div>
<p dir="auto">Here are some more advanced usage examples. You can expand the sections below to learn more.</p>
<details>
<summary>Batch inference</summary>
<p dir="auto">Using vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

def build_input(processor, messages, use_audio_in_video):
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        &quot;mm_processor_kwargs&quot;: {
            &quot;use_audio_in_video&quot;: use_audio_in_video,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios
    
    return inputs

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Instruct&quot;
    # MODEL_PATH = &quot;Qwen/Qwen3-Omni-30B-A3B-Thinking&quot;

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    # Conversation with image only
    conversation1 = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see in this image? Answer in one sentence.&quot;},
            ]
        }
    ]

    # Conversation with audio only
    conversation2 = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;},
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you hear in this audio?&quot;},
            ]
        }
    ]

    # Conversation with pure text and system prompt
    conversation3 = [
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen-Omni.&quot;}
            ],
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Who are you? Answer in one sentence.&quot;
        }
    ]

    # Conversation with mixed media
    conversation4 = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
                {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;},
                {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav&quot;},
                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see and hear? Answer in one sentence.&quot;}
            ],
        }
    ]
    
    USE_AUDIO_IN_VIDEO = True

    # Combine messages for batch processing
    conversations = [conversation1, conversation2, conversation3, conversation4]
    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]

    outputs = llm.generate(inputs, sampling_params=sampling_params)

    result = [outputs[i].outputs[0].text for i in range(len(outputs))]
    print(result)"><pre><span>import</span> <span>os</span>
<span>import</span> <span>torch</span>

<span>from</span> <span>vllm</span> <span>import</span> <span>LLM</span>, <span>SamplingParams</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>Qwen3OmniMoeProcessor</span>
<span>from</span> <span>qwen_omni_utils</span> <span>import</span> <span>process_mm_info</span>

<span>def</span> <span>build_input</span>(<span>processor</span>, <span>messages</span>, <span>use_audio_in_video</span>):
    <span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(
        <span>messages</span>,
        <span>tokenize</span><span>=</span><span>False</span>,
        <span>add_generation_prompt</span><span>=</span><span>True</span>,
    )
    <span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>messages</span>, <span>use_audio_in_video</span><span>=</span><span>use_audio_in_video</span>)

    <span>inputs</span> <span>=</span> {
        <span>'prompt'</span>: <span>text</span>,
        <span>'multi_modal_data'</span>: {},
        <span>"mm_processor_kwargs"</span>: {
            <span>"use_audio_in_video"</span>: <span>use_audio_in_video</span>,
        },
    }

    <span>if</span> <span>images</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'image'</span>] <span>=</span> <span>images</span>
    <span>if</span> <span>videos</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'video'</span>] <span>=</span> <span>videos</span>
    <span>if</span> <span>audios</span> <span><span>is</span> <span>not</span></span> <span>None</span>:
        <span>inputs</span>[<span>'multi_modal_data'</span>][<span>'audio'</span>] <span>=</span> <span>audios</span>
    
    <span>return</span> <span>inputs</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span>:
    <span># vLLM engine v1 not supported yet</span>
    <span>os</span>.<span>environ</span>[<span>'VLLM_USE_V1'</span>] <span>=</span> <span>'0'</span>

    <span>MODEL_PATH</span> <span>=</span> <span>"Qwen/Qwen3-Omni-30B-A3B-Instruct"</span>
    <span># MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"</span>

    <span>llm</span> <span>=</span> <span>LLM</span>(
            <span>model</span><span>=</span><span>MODEL_PATH</span>, <span>trust_remote_code</span><span>=</span><span>True</span>, <span>gpu_memory_utilization</span><span>=</span><span>0.95</span>,
            <span>tensor_parallel_size</span><span>=</span><span>torch</span>.<span>cuda</span>.<span>device_count</span>(),
            <span>limit_mm_per_prompt</span><span>=</span>{<span>'image'</span>: <span>3</span>, <span>'video'</span>: <span>3</span>, <span>'audio'</span>: <span>3</span>},
            <span>max_num_seqs</span><span>=</span><span>8</span>,
            <span>max_model_len</span><span>=</span><span>32768</span>,
            <span>seed</span><span>=</span><span>1234</span>,
    )

    <span>sampling_params</span> <span>=</span> <span>SamplingParams</span>(
        <span>temperature</span><span>=</span><span>0.6</span>,
        <span>top_p</span><span>=</span><span>0.95</span>,
        <span>top_k</span><span>=</span><span>20</span>,
        <span>max_tokens</span><span>=</span><span>16384</span>,
    )

    <span>processor</span> <span>=</span> <span>Qwen3OmniMoeProcessor</span>.<span>from_pretrained</span>(<span>MODEL_PATH</span>)

    <span># Conversation with image only</span>
    <span>conversation1</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
                {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see in this image? Answer in one sentence."</span>},
            ]
        }
    ]

    <span># Conversation with audio only</span>
    <span>conversation2</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"</span>},
                {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you hear in this audio?"</span>},
            ]
        }
    ]

    <span># Conversation with pure text and system prompt</span>
    <span>conversation3</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"system"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"You are Qwen-Omni."</span>}
            ],
        },
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: <span>"Who are you? Answer in one sentence."</span>
        }
    ]

    <span># Conversation with mixed media</span>
    <span>conversation4</span> <span>=</span> [
        {
            <span>"role"</span>: <span>"user"</span>,
            <span>"content"</span>: [
                {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"</span>},
                {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav"</span>},
                {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"What can you see and hear? Answer in one sentence."</span>}
            ],
        }
    ]
    
    <span>USE_AUDIO_IN_VIDEO</span> <span>=</span> <span>True</span>

    <span># Combine messages for batch processing</span>
    <span>conversations</span> <span>=</span> [<span>conversation1</span>, <span>conversation2</span>, <span>conversation3</span>, <span>conversation4</span>]
    <span>inputs</span> <span>=</span> [<span>build_input</span>(<span>processor</span>, <span>messages</span>, <span>USE_AUDIO_IN_VIDEO</span>) <span>for</span> <span>messages</span> <span>in</span> <span>conversations</span>]

    <span>outputs</span> <span>=</span> <span>llm</span>.<span>generate</span>(<span>inputs</span>, <span>sampling_params</span><span>=</span><span>sampling_params</span>)

    <span>result</span> <span>=</span> [<span>outputs</span>[<span>i</span>].<span>outputs</span>[<span>0</span>].<span>text</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>len</span>(<span>outputs</span>))]
    <span>print</span>(<span>result</span>)</pre></div>
</details>
<details>
<summary>vLLM Serve Usage</summary>
<p dir="auto">vLLM serve for Qwen3-Omni currently only supports the thinker model. The <code>use_audio_in_video</code> parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Qwen3-Omni-30B-A3B-Instruct for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4"><pre><span><span>#</span> Qwen3-Omni-30B-A3B-Instruct for single GPU</span>
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
<span><span>#</span> Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)</span>
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
<span><span>#</span> Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU</span>
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
<span><span>#</span> Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)</span>
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4</pre></div>
<p dir="auto">Then you can use the chat API as below (via curl, for example):</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl http://localhost:8901/v1/chat/completions \
    -H &quot;Content-Type: application/json&quot; \
    -d '{
    &quot;messages&quot;: [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
        {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg&quot;}},
        {&quot;type&quot;: &quot;audio_url&quot;, &quot;audio_url&quot;: {&quot;url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav&quot;}},
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What can you see and hear? Answer in one sentence.&quot;}
    ]}
    ]
    }'"><pre>curl http://localhost:8901/v1/chat/completions \
    -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \
    -d <span><span>'</span>{</span>
<span>    "messages": [</span>
<span>    {"role": "system", "content": "You are a helpful assistant."},</span>
<span>    {"role": "user", "content": [</span>
<span>        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},</span>
<span>        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},</span>
<span>        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}</span>
<span>    ]}</span>
<span>    ]</span>
<span>    }<span>'</span></span></pre></div>
</details>
<p dir="auto">Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to <a href="#usage-tips-recommended-reading">Usage Tips</a> and <a href="#cookbooks-for-usage-cases">Cookbooks for Usage Cases</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DashScope API Usage</h3><a id="user-content-dashscope-api-usage" aria-label="Permalink: DashScope API Usage" href="#dashscope-api-usage"></a></p>
<p dir="auto">To further explore Qwen3-Omni, we encourage you to try our DashScope API for a faster and more efficient experience. For detailed API information and documentation, please refer to the following:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>API Description</th>
<th>API Documentation (Mainland China)</th>
<th>API Documentation (International)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Offline API for Qwen3-Omni-Flash, including Instruct and Thinking models</td>
<td><a href="https://help.aliyun.com/zh/model-studio/qwen-omni" rel="nofollow">https://help.aliyun.com/zh/model-studio/qwen-omni</a></td>
<td><a href="https://www.alibabacloud.com/help/en/model-studio/qwen-omni" rel="nofollow">https://www.alibabacloud.com/help/en/model-studio/qwen-omni</a></td>
</tr>
<tr>
<td>Real-time API for Qwen3-Omni-Flash, supporting end-to-end real-time interaction</td>
<td><a href="https://help.aliyun.com/zh/model-studio/realtime" rel="nofollow">https://help.aliyun.com/zh/model-studio/realtime</a></td>
<td><a href="https://www.alibabacloud.com/help/en/model-studio/realtime" rel="nofollow">https://www.alibabacloud.com/help/en/model-studio/realtime</a></td>
</tr>
<tr>
<td>API for Qwen3-Omni-30B-A3B-Captioner model</td>
<td><a href="https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner" rel="nofollow">https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner</a></td>
<td><a href="https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner" rel="nofollow">https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage Tips (Recommended Reading)</h3><a id="user-content-usage-tips-recommended-reading" aria-label="Permalink: Usage Tips (Recommended Reading)" href="#usage-tips-recommended-reading"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Minimum GPU memory requirements</h4><a id="user-content-minimum-gpu-memory-requirements" aria-label="Permalink: Minimum GPU memory requirements" href="#minimum-gpu-memory-requirements"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Precision</th>
<th>15s Video</th>
<th>30s Video</th>
<th>60s Video</th>
<th>120s Video</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen3-Omni-30B-A3B-Instruct</td>
<td>BF16</td>
<td>78.85 GB</td>
<td>88.52 GB</td>
<td>107.74 GB</td>
<td>144.81 GB</td>
</tr>
<tr>
<td>Qwen3-Omni-30B-A3B-Thinking</td>
<td>BF16</td>
<td>68.74 GB</td>
<td>77.79 GB</td>
<td>95.76 GB</td>
<td>131.65 GB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Note</strong>: The table above presents the theoretical minimum memory requirements for inference with <code>transformers</code> and <code>BF16</code> precision, tested with <code>attn_implementation="flash_attention_2"</code>. The Instruct model includes both the <strong>thinker</strong> and <strong>talker</strong> components, whereas the Thinking model includes only the <strong>thinker</strong> part.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Prompt for Audio-Visual Interaction</h4><a id="user-content-prompt-for-audio-visual-interaction" aria-label="Permalink: Prompt for Audio-Visual Interaction" href="#prompt-for-audio-visual-interaction"></a></p>
<p dir="auto">When using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the <strong>following system prompt</strong>. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the <code>user_system_prompt</code> field in the system prompt to include character settings or other role-specific descriptions as needed.</p>
<div data-snippet-clipboard-copy-content="user_system_prompt = &quot;You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen.&quot;
message = {
    &quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: [
          {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, “I/me/my/we/our” refer to the user and “you/your” refer to the assistant. In your replies, address the user as “you/your” and yourself as “I/me/my”; never mirror the user’s pronouns—always shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face.&quot;}
    ]
}"><pre><code>user_system_prompt = "You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen."
message = {
    "role": "system",
    "content": [
          {"type": "text", "text": f"{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, “I/me/my/we/our” refer to the user and “you/your” refer to the assistant. In your replies, address the user as “you/your” and yourself as “I/me/my”; never mirror the user’s pronouns—always shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face."}
    ]
}
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Best Practices for the Thinking Model</h4><a id="user-content-best-practices-for-the-thinking-model" aria-label="Permalink: Best Practices for the Thinking Model" href="#best-practices-for-the-thinking-model"></a></p>
<p dir="auto">The <code>Qwen3-Omni-30B-A3B-Thinking</code> model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;/path/to/audio.wav&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;/path/to/image.png&quot;},
            {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;/path/to/video.mp4&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Analyze this audio, image, and video together.&quot;},
        ], 
    }
]"><pre><span>messages</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"/path/to/audio.wav"</span>},
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"/path/to/image.png"</span>},
            {<span>"type"</span>: <span>"video"</span>, <span>"video"</span>: <span>"/path/to/video.mp4"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"Analyze this audio, image, and video together."</span>},
        ], 
    }
]</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Use audio in video</h4><a id="user-content-use-audio-in-video" aria-label="Permalink: Use audio in video" href="#use-audio-in-video"></a></p>
<p dir="auto">In multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# In data preprocessing
audios, images, videos = process_mm_info(messages, use_audio_in_video=True)"><pre><span># In data preprocessing</span>
<span>audios</span>, <span>images</span>, <span>videos</span> <span>=</span> <span>process_mm_info</span>(<span>messages</span>, <span>use_audio_in_video</span><span>=</span><span>True</span>)</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# For Transformers
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=&quot;pt&quot;, 
                   padding=True, use_audio_in_video=True)
text_ids, audio = model.generate(..., use_audio_in_video=True)

# For vLLM
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = {
    'prompt': text,
    'multi_modal_data': {},
    &quot;mm_processor_kwargs&quot;: {
        &quot;use_audio_in_video&quot;: True,
    },
}"><pre><span># For Transformers</span>
<span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(<span>messages</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>inputs</span> <span>=</span> <span>processor</span>(<span>text</span><span>=</span><span>text</span>, <span>audio</span><span>=</span><span>audios</span>, <span>images</span><span>=</span><span>images</span>, <span>videos</span><span>=</span><span>videos</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>, 
                   <span>padding</span><span>=</span><span>True</span>, <span>use_audio_in_video</span><span>=</span><span>True</span>)
<span>text_ids</span>, <span>audio</span> <span>=</span> <span>model</span>.<span>generate</span>(..., <span>use_audio_in_video</span><span>=</span><span>True</span>)

<span># For vLLM</span>
<span>text</span> <span>=</span> <span>processor</span>.<span>apply_chat_template</span>(<span>messages</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>inputs</span> <span>=</span> {
    <span>'prompt'</span>: <span>text</span>,
    <span>'multi_modal_data'</span>: {},
    <span>"mm_processor_kwargs"</span>: {
        <span>"use_audio_in_video"</span>: <span>True</span>,
    },
}</pre></div>
<p dir="auto">It is worth noting that during a multi-round conversation, the <code>use_audio_in_video</code> parameter must be set consistently across these steps; otherwise, unexpected results may occur.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interaction with Qwen3-Omni</h2><a id="user-content-interaction-with-qwen3-omni" aria-label="Permalink: Interaction with Qwen3-Omni" href="#interaction-with-qwen3-omni"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Online Demo</h3><a id="user-content-online-demo" aria-label="Permalink: Online Demo" href="#online-demo"></a></p>
<p dir="auto">Without local deployment, you can experience an online web demo directly by visiting our <a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo" rel="nofollow">Hugging Face Spaces</a> and <a href="https://modelscope.cn/studios/Qwen/Qwen3-Omni-Demo" rel="nofollow">ModelScope Studio</a>. This includes quick hands-on experiences for Qwen3-Omni-Realtime, Qwen3-Omni (Instruct and Thinking), and Qwen3-Omni-30B-A3B-Captioner.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Real-Time Interaction</h3><a id="user-content-real-time-interaction" aria-label="Permalink: Real-Time Interaction" href="#real-time-interaction"></a></p>
<p dir="auto">Real-time streaming interaction with Qwen3-Omni is available now. Please visit <a href="https://chat.qwen.ai/" rel="nofollow">Qwen Chat</a> and select the voice/video call option in the chat box to experience it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Launch Local Web UI Demo</h3><a id="user-content-launch-local-web-ui-demo" aria-label="Permalink: Launch Local Web UI Demo" href="#launch-local-web-ui-demo"></a></p>
<p dir="auto">In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with the model through a web browser. Follow the steps below to get start :)</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installation</h4><a id="user-content-installation-2" aria-label="Permalink: Installation" href="#installation-2"></a></p>
<p dir="auto">Before you begin, we strongly recommend that you refer to the <strong>Installation</strong> section in <a href="#vllm-usage">vLLM Usage</a> to set up your environment, which will allow you to seamlessly use both the vLLM and Transformers backends. However, if you only intend to use the Transformers backend (<strong>note that this will result in significantly slower inference</strong>), please follow the installation instructions in <a href="#transformers-usage">Transformers Usage</a>. That said, we still highly recommend using our <a href="#-docker">Docker</a> image to avoid potential environment-related issues. Additionally, if you are running locally, make sure your system has <code>ffmpeg</code> installed and you install the following dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1"><pre>pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Running the Demo</h4><a id="user-content-running-the-demo" aria-label="Permalink: Running the Demo" href="#running-the-demo"></a></p>
<p dir="auto">Once the required packages are installed, you can launch the web demo using the following commands. These commands will start a web server and provide you with a link to access the UI in your web browser. You can run <code>python web_demo.py --help</code> and <code>python web_demo_captioner.py --help</code> to learn about more options.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# For Qwen3-Omni-30B-A3B-Instruct with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2"><pre><span><span>#</span> For Qwen3-Omni-30B-A3B-Instruct with vLLM backend</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
<span><span>#</span> For Qwen3-Omni-30B-A3B-Instruct with Transformers backend</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
<span><span>#</span> For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# For Qwen3-Omni-30B-A3B-Thinking with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2"><pre><span><span>#</span> For Qwen3-Omni-30B-A3B-Thinking with vLLM backend</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
<span><span>#</span> For Qwen3-Omni-30B-A3B-Thinking with Transformers backend</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
<span><span>#</span> For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# For Qwen3-Omni-30B-A3B-Captioner with vLLM backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2"><pre><span><span>#</span> For Qwen3-Omni-30B-A3B-Captioner with vLLM backend</span>
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
<span><span>#</span> For Qwen3-Omni-30B-A3B-Captioner with Transformers backend</span>
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
<span><span>#</span> For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support</span>
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2</pre></div>
<p dir="auto">After running the command, you’ll see a link generated in the terminal similar to this:</p>
<div data-snippet-clipboard-copy-content="Running on local: http://127.0.0.1:8901/"><pre><code>Running on local: http://127.0.0.1:8901/
</code></pre></div>
<p dir="auto">If you are running locally, copy this link and paste it into your browser to access the web UI. If you are running on a server or in a <code>docker</code> container, please configure the address according to the server's actual IP, or set up port forwarding where necessary. For instructions on how to configure port forwarding from the official <code>docker</code> container to the host machine, please refer to <a href="#-docker">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🐳 Docker</h2><a id="user-content--docker" aria-label="Permalink: 🐳 Docker" href="#-docker"></a></p>
<p dir="auto">To simplify the deployment process, we provide Docker images with pre-built environments: <a href="https://hub.docker.com/r/qwenllm/qwen3-omni" rel="nofollow">qwenllm/qwen3-omni</a>. You only need to install the driver and download model files to launch the demos. Please refer to the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" rel="nofollow">guide</a> to install the NVIDIA Container Toolkit, ensuring that your Docker can access the GPU. For users in mainland China who may have difficulty accessing Docker Hub, you can use mirror acceleration services to pull the images. First, run the following command to pull and initialize the container:</p>
<div dir="auto" data-snippet-clipboard-copy-content="LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p $HOST_PORT:$CONTAINER_PORT \
    --mount type=bind,source=$LOCAL_WORKDIR,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124"><pre>LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p <span>$HOST_PORT</span>:<span>$CONTAINER_PORT</span> \
    --mount type=bind,source=<span>$LOCAL_WORKDIR</span>,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124</pre></div>
<p dir="auto">After executing the command, you will enter the bash shell of the container. Your local model and data directory (<strong>please replace</strong> <code>/path/to/your/workspace</code> <strong>with the actual path</strong>) will be mounted to the container's internal path <code>/data/shared/Qwen3-Omni</code>. The host's port <code>8901</code> is mapped to port <code>80</code> in the container, meaning you can access the service inside the container by visiting port <code>8901</code> on the host machine.</p>
<p dir="auto">Please note that services inside the container must be started with the IP <code>0.0.0.0</code> to ensure proper port forwarding. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run this command inside the Docker container
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0"><pre><span><span>#</span> Run this command inside the Docker container</span>
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0</pre></div>
<p dir="auto">For more ways to launch the web demo, please refer to <a href="#launch-local-web-ui-demo">Launch Local Web UI Demo</a>. If you exit the container, you can re-enter it using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker start qwen3-omni
docker exec -it qwen3-omni bash"><pre>docker start qwen3-omni
docker <span>exec</span> -it qwen3-omni bash</pre></div>
<p dir="auto">Or if you want to completely remove the container, please run:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation</h2><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Performance of Qwen3-Omni</h3><a id="user-content-performance-of-qwen3-omni" aria-label="Permalink: Performance of Qwen3-Omni" href="#performance-of-qwen3-omni"></a></p>
<p dir="auto">Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.</p>
<details>
<summary>Text -&gt; Text</summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th colspan="2"></th>
      <th>GPT-4o-0327</th>
      <th>Qwen3-235B-A22B<br>Non Thinking</th>
      <th>Qwen3-30B-A3B-Instruct-2507</th>
      <th>Qwen3-Omni-30B-A3B-Instruct</th>
      <th>Qwen3-Omni-Flash-Instruct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2">General<br>Tasks</td>
      <td>MMLU-Redux</td>
      <td><strong>91.3</strong></td>
      <td>89.2</td>
      <td>89.3</td>
      <td>86.6</td>
      <td>86.8</td>
    </tr>
    <tr>
      <td>GPQA</td>
      <td>66.9</td>
      <td>62.9</td>
      <td><strong>70.4</strong></td>
      <td>69.6</td>
      <td>69.7</td>
    </tr>
    <tr>
      <td rowspan="2">Reasoning</td>
      <td>AIME25</td>
      <td>26.7</td>
      <td>24.7</td>
      <td>61.3</td>
      <td>65.0</td>
      <td><strong>65.9</strong></td>
    </tr>
    <tr>
      <td>ZebraLogic</td>
      <td>52.6</td>
      <td>37.7</td>
      <td><strong>90.0</strong></td>
      <td>76.0</td>
      <td>76.1</td>
    </tr>
    <tr>
      <td>Code</td>
      <td>MultiPL-E</td>
      <td>82.7</td>
      <td>79.3</td>
      <td><strong>83.8</strong></td>
      <td>81.4</td>
      <td>81.5</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td rowspan="3">Alignment<br>Tasks</td>
      <td>IFEval</td>
      <td>83.9</td>
      <td>83.2</td>
      <td><strong>84.7</strong></td>
      <td>81.0</td>
      <td>81.7</td>
    </tr>
    <tr>
      <td>Creative Writing v3</td>
      <td>84.9</td>
      <td>80.4</td>
      <td><strong>86.0</strong></td>
      <td>80.6</td>
      <td>81.8</td>
    </tr>
    <tr>
      <td>WritingBench</td>
      <td>75.5</td>
      <td>77.0</td>
      <td><strong>85.5</strong></td>
      <td>82.6</td>
      <td>83.0</td>
    </tr>
    <tr>
      <td>Agent</td>
      <td>BFCL-v3</td>
      <td>66.5</td>
      <td><strong>68.0</strong></td>
      <td>65.1</td>
      <td>64.4</td>
      <td>65.0</td>
    </tr>
    <tr>
      <td rowspan="2">Multilingual<br>Tasks</td>
      <td>MultiIF</td>
      <td><strong>70.4</strong></td>
      <td>70.2</td>
      <td>67.9</td>
      <td>64.0</td>
      <td>64.7</td>
    </tr>
    <tr>
      <td>PolyMATH</td>
      <td>25.5</td>
      <td>27.0</td>
      <td><strong>43.1</strong></td>
      <td>37.9</td>
      <td>39.3</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th>Gemini-2.5-Flash<br>Thinking</th>
      <th>Qwen3-235B-A22B<br>Thinking</th>
      <th>Qwen3-30B-A3B-Thinking-2507</th>
      <th>Qwen3-Omni-30B-A3B-Thinking</th>
      <th>Qwen3-Omni-Flash-Thinking</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2"><em>General<br>Tasks</em></td>
      <td>MMLU-Redux</td>
      <td>92.1</td>
      <td><b>92.7</b></td>
      <td>91.4</td>
      <td>88.8</td>
      <td>89.7</td>
    </tr>
    <tr>
      <td>GPQA</td>
      <td><b>82.8</b></td>
      <td>71.1</td>
      <td>73.4</td>
      <td>73.1</td>
      <td>73.1</td>
    </tr>
    <tr>
      <td rowspan="2"><em>Reasoning</em></td>
      <td>AIME25</td>
      <td>72.0</td>
      <td>81.5</td>
      <td><b>85.0</b></td>
      <td>73.7</td>
      <td>74.0</td>
    </tr>
    <tr>
      <td>LiveBench 20241125</td>
      <td>74.3</td>
      <td><b>77.1</b></td>
      <td>76.8</td>
      <td>71.8</td>
      <td>70.3</td>
    </tr>
    <tr>
      <td><em>Code</em></td>
      <td>MultiPL-E</td>
      <td><b>84.5</b></td>
      <td>79.9</td>
      <td>81.3</td>
      <td>80.6</td>
      <td>81.0</td>
    </tr>
    <tr>
      <td rowspan="4"><em>Alignment<br>Tasks</em></td>
      <td>IFEval</td>
      <td><b>89.8</b></td>
      <td>83.4</td>
      <td>88.9</td>
      <td>85.1</td>
      <td>85.2</td>
    </tr>
    <tr>
      <td>Arena-Hard v2</td>
      <td>56.7</td>
      <td><b>61.5</b></td>
      <td>56.0</td>
      <td>55.1</td>
      <td>57.8</td>
    </tr>
    <tr>
      <td>Creative Writing v3</td>
      <td><b>85.0</b></td>
      <td>84.6</td>
      <td>84.4</td>
      <td>82.5</td>
      <td>83.6</td>
    </tr>
    <tr>
      <td>WritingBench</td>
      <td>83.9</td>
      <td>80.3</td>
      <td>85.0</td>
      <td>85.5</td>
      <td><b>85.9</b></td>
    </tr>
    <tr>
      <td><em>Agent</em></td>
      <td>BFCL-v3</td>
      <td>68.6</td>
      <td>70.8</td>
      <td><b>72.4</b></td>
      <td>63.2</td>
      <td>64.5</td>
    </tr>
    <tr>
      <td rowspan="2"><em>Multilingual<br>Tasks</em></td>
      <td>MultiIF</td>
      <td>74.4</td>
      <td>71.9</td>
      <td><b>76.4</b></td>
      <td>72.9</td>
      <td>73.2</td>
    </tr>
    <tr>
      <td>PolyMATH</td>
      <td>49.8</td>
      <td><b>54.7</b></td>
      <td>52.6</td>
      <td>47.1</td>
      <td>48.7</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Audio -&gt; Text</summary>
<markdown-accessiblity-table><table>
<thead>
  <tr>
    <th></th>
    <th>Seed-ASR</th>
    <th>Voxtral-Mini</th>
    <th>Voxtral-Small</th>
    <th>GPT-4o-Transcribe</th>
    <th>Gemini-2.5-Pro</th>
    <th>Qwen2.5-Omni</th>
    <th>Qwen3-Omni-30B-A3B-Instruct</th>
    <th>Qwen3-Omni-Flash-Instruct</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td colspan="9"><em>EN &amp; ZH ASR (wer)</em></td>
  </tr>
  <tr>
    <td>Wenetspeech<br><em>net</em> | <em>meeting</em></td>
    <td>4.66 | <strong>5.69</strong></td>
    <td>24.30 | 31.53</td>
    <td>20.33 | 26.08</td>
    <td>15.30 | 32.27</td>
    <td>14.43 | 13.47</td>
    <td>5.91 | 7.65</td>
    <td>4.69 | 5.89</td>
    <td><strong>4.62</strong> | 5.75</td>
  </tr>
  <tr>
    <td>Librispeech<br><em>clean</em> | <em>other</em></td>
    <td>1.58 | 2.84</td>
    <td>1.88 | 4.12</td>
    <td>1.56 | 3.30</td>
    <td>1.39 | 3.75</td>
    <td>2.89 | 3.56</td>
    <td>1.74 | 3.45</td>
    <td><strong>1.22</strong> | 2.48</td>
    <td>1.27 | <strong>2.44</strong></td>
  </tr>
  <tr>
    <td>CV15-en</td>
    <td>-</td>
    <td>9.47</td>
    <td>7.79</td>
    <td>10.01</td>
    <td>9.89</td>
    <td>7.61</td>
    <td>6.05</td>
    <td><strong>5.94</strong></td>
  </tr>
  <tr>
    <td>CV15-zh</td>
    <td>-</td>
    <td>24.67</td>
    <td>19.30</td>
    <td>9.84</td>
    <td>8.00</td>
    <td>5.13</td>
    <td>4.31</td>
    <td><strong>4.28</strong></td>
  </tr>
  <tr>
    <td>Fleurs-en</td>
    <td>3.40</td>
    <td>3.96</td>
    <td>3.77</td>
    <td>3.32</td>
    <td>2.94</td>
    <td>3.77</td>
    <td><strong>2.72</strong></td>
    <td>2.74</td>
  </tr>
  <tr>
    <td>Fleurs-zh</td>
    <td>2.69</td>
    <td>12.22</td>
    <td>7.98</td>
    <td>2.44</td>
    <td>2.71</td>
    <td>2.54</td>
    <td>2.20</td>
    <td><strong>2.19</strong></td>
  </tr>
  <tr>
    <td colspan="9"><em>Multilingual ASR (wer)</em></td>
  </tr>
  <tr>
    <td>Fleurs-avg<br>(19 lang)</td>
    <td>-</td>
    <td>15.67</td>
    <td>8.09</td>
    <td>4.48</td>
    <td>5.55</td>
    <td>14.04</td>
    <td>5.33</td>
    <td><strong>5.31</strong></td>
  </tr>
  <tr>
    <td colspan="9"><em>Lyric ASR (wer)</em></td>
  </tr>
  <tr>
    <td>MIR-1K (vocal-only)</td>
    <td>6.45</td>
    <td>23.33</td>
    <td>18.73</td>
    <td>11.87</td>
    <td>9.85</td>
    <td>8.15</td>
    <td>5.90</td>
    <td><strong>5.85</strong></td>
  </tr>
  <tr>
    <td>Opencpop-test</td>
    <td>2.98</td>
    <td>31.01</td>
    <td>16.06</td>
    <td>7.93</td>
    <td>6.49</td>
    <td>2.84</td>
    <td><strong>1.54</strong></td>
    <td>2.02</td>
  </tr>
  <tr>
    <td colspan="9"><em>S2TT (BLEU)</em></td>
  </tr>
  <tr>
    <td>Fleurs-en2xx</td>
    <td>-</td>
    <td>30.35</td>
    <td>37.85</td>
    <td>-</td>
    <td><strong>39.25</strong></td>
    <td>29.22</td>
    <td>37.50</td>
    <td>36.22</td>
  </tr>
  <tr>
    <td>Fleurs-xx2en</td>
    <td>-</td>
    <td>27.54</td>
    <td>32.81</td>
    <td>-</td>
    <td><strong>35.41</strong></td>
    <td>28.61</td>
    <td>31.08</td>
    <td>30.71</td>
  </tr>
  <tr>
    <td>Fleurs-zh2xx</td>
    <td>-</td>
    <td>17.03</td>
    <td>22.05</td>
    <td>-</td>
    <td><strong>26.63</strong></td>
    <td>17.97</td>
    <td>25.17</td>
    <td>25.10</td>
  </tr>
  <tr>
    <td>Fleurs-xx2zh</td>
    <td>-</td>
    <td>28.75</td>
    <td>34.82</td>
    <td>-</td>
    <td><strong>37.50</strong></td>
    <td>27.68</td>
    <td>33.13</td>
    <td>31.19</td>
  </tr>
</tbody>
</table></markdown-accessiblity-table>

      <markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th></th>
      <th>GPT-4o-Audio</th>
      <th>Gemini-2.5-Flash</th>
      <th>Gemini-2.5-Pro</th>
      <th>Qwen2.5-Omni</th>
      <th>Qwen3-Omni-30B-A3B-Instruct</th>
      <th>Qwen3-Omni-30B-A3B-Thinking</th>
      <th>Qwen3-Omni-Flash-Instruct</th>
      <th>Qwen3-Omni-Flash-Thinking</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="9"><strong>VoiceBench</strong></td>
    </tr>
    <tr>
      <td>AlpacaEval</td>
      <td>95.6</td>
      <td>96.1</td>
      <td>94.3</td>
      <td>89.9</td>
      <td>94.8</td>
      <td>96.4</td>
      <td>95.4</td>
      <td><strong>96.8</strong></td>
    </tr>
    <tr>
      <td>CommonEval</td>
      <td>89.8</td>
      <td>88.3</td>
      <td>88.4</td>
      <td>76.7</td>
      <td>90.8</td>
      <td>90.5</td>
      <td><strong>91.0</strong></td>
      <td>90.9</td>
    </tr>
    <tr>
      <td>WildVoice</td>
      <td>91.6</td>
      <td>92.1</td>
      <td>93.4</td>
      <td>77.7</td>
      <td>91.6</td>
      <td>90.5</td>
      <td><strong>92.3</strong></td>
      <td>90.9</td>
    </tr>
    <tr>
      <td>SD-QA</td>
      <td>75.5</td>
      <td>84.5</td>
      <td><strong>90.1</strong></td>
      <td>56.4</td>
      <td>76.9</td>
      <td>78.1</td>
      <td>76.8</td>
      <td>78.5</td>
    </tr>
    <tr>
      <td>MMSU</td>
      <td>80.3</td>
      <td>66.1</td>
      <td>71.1</td>
      <td>61.7</td>
      <td>68.1</td>
      <td>83.0</td>
      <td>68.4</td>
      <td><strong>84.3</strong></td>
    </tr>
    <tr>
      <td>OpenBookQA</td>
      <td>89.2</td>
      <td>56.9</td>
      <td>92.3</td>
      <td>80.9</td>
      <td>89.7</td>
      <td>94.3</td>
      <td>91.4</td>
      <td><strong>95.0</strong></td>
    </tr>
    <tr>
      <td>BBH</td>
      <td>84.1</td>
      <td>83.9</td>
      <td><strong>92.6</strong></td>
      <td>66.7</td>
      <td>80.4</td>
      <td>88.9</td>
      <td>80.6</td>
      <td>89.6</td>
    </tr>
    <tr>
      <td>IFEval</td>
      <td>76.0</td>
      <td>83.8</td>
      <td><strong>85.7</strong></td>
      <td>53.5</td>
      <td>77.8</td>
      <td>80.6</td>
      <td>75.2</td>
      <td>80.8</td>
    </tr>
    <tr>
      <td>AdvBench</td>
      <td>98.7</td>
      <td>98.9</td>
      <td>98.1</td>
      <td>99.2</td>
      <td><strong>99.3</strong></td>
      <td>97.2</td>
      <td><strong>99.4</strong></td>
      <td>98.9</td>
    </tr>
    <tr>
      <td>Overall</td>
      <td>86.8</td>
      <td>83.4</td>
      <td><strong>89.6</strong></td>
      <td>73.6</td>
      <td>85.5</td>
      <td>88.8</td>
      <td>85.6</td>
      <td>89.5</td>
    </tr>
    <tr>
      <td colspan="9"><strong>Audio Reasoning</strong></td>
    </tr>
    <tr>
      <td>MMAU-v05.15.25</td>
      <td>62.5</td>
      <td>71.8</td>
      <td>77.4</td>
      <td>65.5</td>
      <td>77.5</td>
      <td>75.4</td>
      <td><strong>77.6</strong></td>
      <td>76.5</td>
    </tr>
    <tr><td>MMSU</td>
      <td>56.4</td>
      <td>70.2</td>
      <td><strong>77.7</strong></td>
      <td>62.6</td>
      <td>69.0</td>
      <td>70.2</td>
      <td>69.1</td>
      <td>71.3</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th></th>
      <th>Best Specialist<br>Models</th>
      <th>GPT-4o-Audio</th>
      <th>Gemini-2.5-Pro</th>
      <th>Qwen2.5-Omni</th>
      <th>Qwen3-Omni-30B-A3B-Instruct</th>
      <th>Qwen3-Omni-Flash-Instruct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RUL-MuchoMusic</td>
      <td>47.6 (Audio Flamingo 3)</td>
      <td>36.1</td>
      <td>49.4</td>
      <td>47.3</td>
      <td>52.0</td>
      <td><strong>52.1</strong></td>
    </tr>
    <tr>
      <td>GTZAN<br><em>Acc.</em></td>
      <td>87.9 (CLaMP 3)</td>
      <td>76.5</td>
      <td>81.0</td>
      <td>81.7</td>
      <td>93.0</td>
      <td><strong>93.1</strong></td>
    </tr>
    <tr>
      <td>MTG Genre<br><em>Micro F1</em></td>
      <td>35.8 (MuQ-MuLan)</td>
      <td>25.3</td>
      <td>32.6</td>
      <td>32.5</td>
      <td>39.0</td>
      <td><strong>39.5</strong></td>
    </tr>
    <tr>
      <td>MTG Mood/Theme<br><em>Micro F1</em></td>
      <td>10.9 (MuQ-MuLan)</td>
      <td>11.3</td>
      <td>14.1</td>
      <td>8.9</td>
      <td>21.0</td>
      <td><strong>21.7</strong></td>
    </tr>
    <tr>
      <td>MTG Instrument<br><em>Micro F1</em></td>
      <td>39.8 (MuQ-MuLan)</td>
      <td>34.2</td>
      <td>33.0</td>
      <td>22.6</td>
      <td>40.5</td>
      <td><strong>40.7</strong></td>
    </tr>
    <tr>
      <td>MTG Top50<br><em>Micro F1</em></td>
      <td>33.2 (MuQ-MuLan)</td>
      <td>25.0</td>
      <td>26.1</td>
      <td>21.6</td>
      <td>36.7</td>
      <td><strong>36.9</strong></td>
    </tr>
    <tr>
      <td>MagnaTagATune<br><em>Micro F1</em></td>
      <td>41.6 (MuQ)</td>
      <td>29.2</td>
      <td>28.1</td>
      <td>30.1</td>
      <td>44.3</td>
      <td><strong>46.8</strong></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Vision -&gt; Text</summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>GPT4-o</th>
      <th>Gemini-2.0-Flash</th>
      <th>Qwen2.5-VL<br>72B</th>
      <th>Qwen3-Omni-30B-A3B<br>-Instruct</th>
      <th>Qwen3-Omni-Flash<br>-Instruct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="6">General Visual Question Answering</td>
    </tr>
    <tr>
      <td>MMStar</td>
      <td>64.7</td>
      <td><strong>71.4</strong></td>
      <td>70.8</td>
      <td>68.5</td>
      <td>69.3</td>
    </tr>
    <tr>
      <td>HallusionBench</td>
      <td>55.0</td>
      <td>56.3</td>
      <td>55.2</td>
      <td><strong>59.7</strong></td>
      <td>58.5</td>
    </tr>
    <tr>
      <td>MM-MT-Bench</td>
      <td><strong>7.7</strong></td>
      <td>6.7</td>
      <td>7.6</td>
      <td>7.4</td>
      <td>7.6</td>
    </tr>
    <tr>
      <td colspan="6">Math &amp; STEM</td>
    </tr>
    <tr>
      <td>MMMU_val</td>
      <td>69.1</td>
      <td><strong>71.3</strong></td>
      <td>70.2</td>
      <td>69.1</td>
      <td>69.8</td>
    </tr>
    <tr>
      <td>MMMU_pro</td>
      <td>51.9</td>
      <td>56.1</td>
      <td>51.1</td>
      <td>57.0</td>
      <td><strong>57.6</strong></td>
    </tr>
    <tr>
      <td>MathVista_mini</td>
      <td>63.8</td>
      <td>71.4</td>
      <td>74.8</td>
      <td>75.9</td>
      <td><strong>77.4</strong></td>
    </tr>
    <tr>
      <td>MathVision_full</td>
      <td>30.4</td>
      <td>48.6</td>
      <td>38.1</td>
      <td>56.3</td>
      <td><strong>58.3</strong></td>
    </tr>
    <tr>
      <td colspan="6">Documentation Understanding</td>
    </tr>
    <tr>
      <td>AI2D</td>
      <td>84.6</td>
      <td>86.7</td>
      <td><strong>88.7</strong></td>
      <td>85.2</td>
      <td>86.4</td>
    </tr>
    <tr>
      <td>ChartQA_test</td>
      <td>86.7</td>
      <td>64.6</td>
      <td><strong>89.5</strong></td>
      <td>86.8</td>
      <td>87.1</td>
    </tr>
    <tr>
      <td colspan="6">Counting</td>
    </tr>
    <tr>
      <td>CountBench</td>
      <td>87.9</td>
      <td>91.2</td>
      <td><strong>93.6</strong></td>
      <td>90.0</td>
      <td>90.0</td>
    </tr>
    <tr>
      <td colspan="6">Video Understanding</td>
    </tr>
    <tr>
      <td>Video-MME</td>
      <td>71.9</td>
      <td>72.4</td>
      <td><strong>73.3</strong></td>
      <td>70.5</td>
      <td>71.4</td>
    </tr>
    <tr>
      <td>LVBench</td>
      <td>30.8</td>
      <td><strong>57.9</strong></td>
      <td>47.3</td>
      <td>50.2</td>
      <td>51.1</td>
    </tr>
    <tr>
      <td>MLVU</td>
      <td>64.6</td>
      <td>71.0</td>
      <td>74.6</td>
      <td>75.2</td>
      <td><strong>75.5</strong></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>Gemini-2.5-flash-thinking</th>
      <th>InternVL-3.5-241B-A28B</th>
      <th>Qwen3-Omni-30B-A3B-Thinking</th>
      <th>Qwen3-Omni-Flash-Thinking</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="5">General Visual Question Answering</td>
    </tr>
    <tr>
      <td>MMStar</td>
      <td>75.5</td>
      <td><b>77.9</b></td>
      <td>74.9</td>
      <td>75.5</td>
    </tr>
    <tr>
      <td>HallusionBench</td>
      <td>61.1</td>
      <td>57.3</td>
      <td>62.8</td>
      <td><b>63.4</b></td>
    </tr>
    <tr>
      <td>MM-MT-Bench</td>
      <td>7.8</td>
      <td>–</td>
      <td><b>8.0</b></td>
      <td><b>8.0</b></td>
    </tr>
    <tr>
      <td colspan="5">Math &amp; STEM</td>
    </tr>
    <tr>
      <td>MMMU_val</td>
      <td>76.9</td>
      <td><b>77.7</b></td>
      <td>75.6</td>
      <td>75.0</td>
    </tr>
    <tr>
      <td>MMMU_pro</td>
      <td><b>65.8</b></td>
      <td>–</td>
      <td>60.5</td>
      <td>60.8</td>
    </tr>
    <tr>
      <td>MathVista_mini</td>
      <td>77.6</td>
      <td><b>82.7</b></td>
      <td>80.0</td>
      <td>81.2</td>
    </tr>
    <tr>
      <td>MathVision_full</td>
      <td>62.3</td>
      <td><b>63.9</b></td>
      <td>62.9</td>
      <td>63.8</td>
    </tr>
    <tr>
      <td colspan="5">Documentation Understanding</td>
    </tr>
    <tr>
      <td>AI2D_test</td>
      <td><b>88.6</b></td>
      <td>87.3</td>
      <td>86.1</td>
      <td>86.8</td>
    </tr>
    <tr>
      <td>ChartQA_test</td>
      <td>–</td>
      <td>88.0</td>
      <td><b>89.5</b></td>
      <td>89.3</td>
    </tr>
    <tr>
      <td colspan="5">Counting</td>
    </tr>
    <tr>
      <td>CountBench</td>
      <td>88.6</td>
      <td>–</td>
      <td>88.6</td>
      <td><b>92.5</b></td>
    </tr>
    <tr>
      <td colspan="5">Video Understanding</td>
    </tr>
    <tr>
      <td>Video-MME</td>
      <td><b>79.6</b></td>
      <td>72.9</td>
      <td>69.7</td>
      <td>69.8</td>
    </tr>
    <tr>
      <td>LVBench</td>
      <td><b>64.5</b></td>
      <td>–</td>
      <td>49.0</td>
      <td>49.5</td>
    </tr>
    <tr>
      <td>MLVU</td>
      <td><b>82.1</b></td>
      <td>78.2</td>
      <td>72.9</td>
      <td>73.9</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>AudioVisual -&gt; Text</summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>Previous Open-source SoTA</th>
      <th>Gemini-2.5-Flash</th>
      <th>Qwen2.5-Omni</th>
      <th>Qwen3-Omni-30B-A3B-Instruct</th>
      <th>Qwen3-Omni-Flash-Instruct</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>WorldSense</td>
      <td>47.1</td>
      <td>50.9</td>
      <td>45.4</td>
      <td>54.0</td>
      <td><strong>54.1</strong></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>Previous Open-source SoTA</th>
      <th>Gemini-2.5-Flash-Thinking</th>
      <th>Qwen3-Omni-30B-A3B-Thinking</th>
      <th>Qwen3-Omni-Flash-Thinking</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DailyOmni</td>
      <td>69.8</td>
      <td>72.7</td>
      <td>75.8</td>
      <td><b>76.2</b></td>
    </tr>
    <tr>
      <td>VideoHolmes</td>
      <td>55.6</td>
      <td>49.5</td>
      <td><b>57.3</b></td>
      <td><b>57.3</b></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Zero-shot Speech Generation</summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Datasets</th>
      <th>Model</th>
      <th>Performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>&nbsp;</td>
      <td colspan="2"><em>Content Consistency</em></td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td rowspan="10"><strong>SEED</strong><br><em>test-zh</em> | <em>test-en</em></td>
      <td>Seed-TTS<sub>ICL</sub></td>
      <td>1.11 | 2.24</td>
    </tr>
    <tr>
      <td>Seed-TTS<sub>RL</sub></td>
      <td>1.00 | 1.94</td>
    </tr>
    <tr>
      <td>MaskGCT</td>
      <td>2.27 | 2.62</td>
    </tr>
    <tr>
      <td>E2 TTS</td>
      <td>1.97 | 2.19</td>
    </tr>
    <tr>
      <td>F5-TTS</td>
      <td>1.56 | 1.83</td>
    </tr>
    <tr>
      <td>Spark TTS</td>
      <td>1.20 | 1.98</td>
    </tr>
    <tr>
      <td>CosyVoice 2</td>
      <td>1.45 | 2.57</td>
    </tr>
    <tr>
      <td>CosyVoice 3</td>
      <td><strong>0.71</strong> | 1.45</td>
    </tr>
    <tr>
      <td>Qwen2.5-Omni-7B</td>
      <td>1.42 | 2.33</td>
    </tr>
    <tr>
      <td>Qwen3-Omni-30B-A3B</td>
      <td>1.07 | <strong>1.39</strong></td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Multilingual Speech Generation </summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th rowspan="2">Language</th>
      <th colspan="3">Content Consistency</th>
      <th colspan="3">Speaker Similarity</th>
    </tr>
    <tr>
      <th>Qwen3-Omni-30B-A3B</th>
      <th>MiniMax</th>
      <th>ElevenLabs</th>
      <th>Qwen3-Omni-30B-A3B</th>
      <th>MiniMax</th>
      <th>ElevenLabs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Chinese</td>
      <td><strong>0.716</strong></td>
      <td>2.252</td>
      <td>16.026</td>
      <td>0.772</td>
      <td><strong>0.780</strong></td>
      <td>0.677</td>
    </tr>
    <tr>
      <td>English</td>
      <td><strong>1.069</strong></td>
      <td>2.164</td>
      <td>2.339</td>
      <td><strong>0.773</strong></td>
      <td>0.756</td>
      <td>0.613</td>
    </tr>
    <tr>
      <td>German</td>
      <td>0.777</td>
      <td>1.906</td>
      <td><strong>0.572</strong></td>
      <td><strong>0.738</strong></td>
      <td>0.733</td>
      <td>0.614</td>
    </tr>
    <tr>
      <td>Italian</td>
      <td><strong>1.067</strong></td>
      <td>1.543</td>
      <td>1.743</td>
      <td><strong>0.742</strong></td>
      <td>0.699</td>
      <td>0.579</td>
    </tr>
    <tr>
      <td>Portuguese</td>
      <td>1.872</td>
      <td>1.877</td>
      <td><strong>1.331</strong></td>
      <td>0.770</td>
      <td><strong>0.805</strong></td>
      <td>0.711</td>
    </tr>
    <tr>
      <td>Spanish</td>
      <td>1.765</td>
      <td><strong>1.029</strong></td>
      <td>1.084</td>
      <td>0.744</td>
      <td><strong>0.762</strong></td>
      <td>0.615</td>
    </tr>
    <tr>
      <td>Japanese</td>
      <td>3.631</td>
      <td><strong>3.519</strong></td>
      <td>10.646</td>
      <td>0.763</td>
      <td><strong>0.776</strong></td>
      <td>0.738</td>
    </tr>
    <tr>
      <td>Korean</td>
      <td><strong>1.670</strong></td>
      <td>1.747</td>
      <td>1.865</td>
      <td><strong>0.778</strong></td>
      <td>0.776</td>
      <td>0.700</td>
    </tr>
    <tr>
      <td>French</td>
      <td><strong>2.505</strong></td>
      <td>4.099</td>
      <td>5.216</td>
      <td><strong>0.689</strong></td>
      <td>0.628</td>
      <td>0.535</td>
    </tr>
    <tr>
      <td>Russian</td>
      <td>3.986</td>
      <td>4.281</td>
      <td><strong>3.878</strong></td>
      <td>0.759</td>
      <td><strong>0.761</strong></td>
      <td>0.676</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<details>
<summary>Cross-Lingual Speech Generation </summary>
<markdown-accessiblity-table><table>
  <thead>
    <tr>
      <th>Language</th>
      <th>Qwen3-Omni-30B-A3B</th>
      <th>CosyVoice3</th>
      <th>CosyVoice2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>en-to-zh</td>
      <td>5.37</td>
      <td><strong>5.09</strong></td>
      <td>13.5</td>
    </tr>
    <tr>
      <td>ja-to-zh</td>
      <td>3.32</td>
      <td><strong>3.05</strong></td>
      <td>48.1</td>
    </tr>
    <tr>
      <td>ko-to-zh</td>
      <td><strong>0.99</strong></td>
      <td>1.06</td>
      <td>7.70</td>
    </tr>
    <tr>
      <td>zh-to-en</td>
      <td><strong>2.76</strong></td>
      <td>2.98</td>
      <td>6.47</td>
    </tr>
    <tr>
      <td>ja-to-en</td>
      <td><strong>3.31</strong></td>
      <td>4.20</td>
      <td>17.1</td>
    </tr>
    <tr>
      <td>ko-to-en</td>
      <td><strong>3.34</strong></td>
      <td>4.19</td>
      <td>11.2</td>
    </tr>
    <tr>
      <td>zh-to-ja</td>
      <td>8.29</td>
      <td><strong>7.08</strong></td>
      <td>13.1</td>
    </tr>
    <tr>
      <td>en-to-ja</td>
      <td>7.53</td>
      <td><strong>6.80</strong></td>
      <td>14.9</td>
    </tr>
    <tr>
      <td>ko-to-ja</td>
      <td>4.24</td>
      <td><strong>3.93</strong></td>
      <td>5.86</td>
    </tr>
    <tr>
      <td>zh-to-ko</td>
      <td><strong>5.13</strong></td>
      <td>14.4</td>
      <td>24.8</td>
    </tr>
    <tr>
      <td>en-to-ko</td>
      <td><strong>4.96</strong></td>
      <td>5.87</td>
      <td>21.9</td>
    </tr>
    <tr>
      <td>ja-to-ko</td>
      <td><strong>6.23</strong></td>
      <td>7.92</td>
      <td>21.5</td>
    </tr>
  </tbody>
</table></markdown-accessiblity-table>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setting for Evaluation</h3><a id="user-content-setting-for-evaluation" aria-label="Permalink: Setting for Evaluation" href="#setting-for-evaluation"></a></p>
<ul dir="auto">
<li><strong>Decoding Strategy</strong>: For the Qwen3-Omni series across all evaluation benchmarks, <code>Instruct</code> models use greedy decoding during generation without sampling. For <code>Thinking</code> models, the decoding parameters should be taken from the <code>generation_config.json</code> file in the checkpoint.</li>
<li><strong>Benchmark-Specific Formatting</strong>: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to <code>fps=2</code> during evaluation.</li>
<li><strong>Default Prompts</strong>: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:</li>
</ul>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Task Type</th>
<th>Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Auto Speech Recognition (ASR) for Chinese</td>
<td>请将这段中文语音转换为纯文本。</td>
</tr>
<tr>
<td>Auto Speech Recognition (ASR) for Other languages</td>
<td>Transcribe the  audio into text.</td>
</tr>
<tr>
<td>Speech-to-Text Translation (S2TT)</td>
<td>Listen to the provided &lt;source_language&gt; speech and produce a translation in &lt;target_language&gt; text.</td>
</tr>
<tr>
<td>Song Lyrics Recognition</td>
<td>Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<ul dir="auto">
<li><strong>System Prompt</strong>: No <code>system prompt</code> should be set for any evaluation benchmark.</li>
<li><strong>Input Sequence</strong>: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come <strong>after</strong> multimodal data in the sequence. For example:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;/path/to/audio.wav&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;/path/to/image.png&quot;},
            {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;/path/to/video.mp4&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe the audio, image and video.&quot;},
        ],
    },
]"><pre><span>messages</span> <span>=</span> [
    {
        <span>"role"</span>: <span>"user"</span>,
        <span>"content"</span>: [
            {<span>"type"</span>: <span>"audio"</span>, <span>"audio"</span>: <span>"/path/to/audio.wav"</span>},
            {<span>"type"</span>: <span>"image"</span>, <span>"image"</span>: <span>"/path/to/image.png"</span>},
            {<span>"type"</span>: <span>"video"</span>, <span>"video"</span>: <span>"/path/to/video.mp4"</span>},
            {<span>"type"</span>: <span>"text"</span>, <span>"text"</span>: <span>"Describe the audio, image and video."</span>},
        ],
    },
]</pre></div>

<br>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California issues fine over lawyer's ChatGPT fabrications (119 pts)]]></title>
            <link>https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/</link>
            <guid>45335774</guid>
            <pubDate>Mon, 22 Sep 2025 16:30:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/">https://calmatters.org/economy/technology/2025/09/chatgpt-lawyer-fine-ai-regulation/</a>, See on <a href="https://news.ycombinator.com/item?id=45335774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
				
		<div>
				
				<p><strong>In summary</strong></p>
				

				
				<p>The court of appeals said 21 of 23 quotes in an opening brief were fake. State authorities are scrambling to grapple with widespread use of artificial intelligence.</p>
				
							</div>
		
		

<p>A California attorney must pay a $10,000 fine for filing a state court appeal full of fake quotations generated by the artificial intelligence tool ChatGPT.</p>

<p>The fine appears to be the largest issued over AI fabrications by a California court and came with a <a href="https://www4.courts.ca.gov/opinions/documents/B331918.PDF" target="_blank" rel="noreferrer noopener">blistering opinion</a> stating that 21 of 23 quotes from cases cited in the attorney’s opening brief were made up. It also noted that numerous out-of-state and federal courts have confronted attorneys for citing fake legal authority.</p>

<p>“We therefore publish this opinion as a warning,” it continued. “Simply stated, no brief, pleading, motion, or any other paper filed in any court should contain any citations— whether provided by generative AI or any other source—that the attorney responsible for submitting the pleading has not personally read and verified.”</p>

<p>The opinion, issued 10 days ago in California’s 2nd District Court of Appeal, is a clear example of why the state’s legal authorities are scrambling to regulate the use of AI in the judiciary. The state’s Judicial Council two weeks ago <a href="https://courts.ca.gov/cms/rules/index/standards/Standard10_80" target="_blank" rel="noreferrer noopener">issued guidelines requiring judges and court staff</a> to either ban generative AI or adopt a generative AI use policy by Dec. 15. Meanwhile, the California Bar Association is considering whether to strengthen its code of conduct to account for various forms of AI following a request by the California Supreme Court last month.</p>


<p>The Los Angeles-area attorney fined last week, Amir Mostafavi, told the court that he did not read text generated by the AI model before submitting the appeal in July 2023, months after OpenAI marketed ChatGPT as capable of <a href="https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/" target="_blank" rel="noreferrer noopener">passing the bar exam</a>. A three-judge panel fined him for filing a frivolous appeal, violating court rules, citing fake cases, and wasting the court’s time and the taxpayers money, according to the opinion.</p>

<p>Mostafavi told CalMatters he wrote the appeal and then used ChatGPT to try and improve it. He said that he didn’t know it would add case citations or make things up.&nbsp;</p>

<p>He thinks it is unrealistic to expect lawyers to stop using AI. It’s become an important tool just as online databases largely replaced law libraries and, until AI systems stop hallucinating fake information, he suggests lawyers who use AI to proceed with caution.</p>

<p>“In the meantime we’re going to have some victims, we’re going to have some damages, we’re going to have some wreckages,” he said. “I hope this example will help others not fall into the hole. I’m paying the price.”</p>


<p>The fine issued to Mostafavi is the most costly penalty issued to an attorney by a California state court and one of the highest fines ever issued over attorney use of&nbsp; AI, according to Damien Charlotin, who teaches a class on AI and the law at a business school in Paris. He <a href="https://www.damiencharlotin.com/hallucinations/">tracks</a> instances of attorneys citing fake cases, primarily in Australia, Canada, the United States, and the United Kingdom.</p>

<p>In a widely-publicized case in May, a U.S. district court judge in California <a href="https://websitedc.s3.amazonaws.com/documents/Lacey_v._State_Farm_General_Insurances_Co._D._Cal._May_6_2025.pdf">ordered two law firms to pay</a> $31,100 in fees to defense counsel and the court for costs associated with using “bogus AI-generated research.” In that ruling, the judge described feeling misled, said they almost cited fake material in a judicial order and said “Strong deterrence is needed to make sure that attorneys don’t succumb to this easy shortcut.”</p>

<figure><blockquote><p>“We’re going to have some wreckages.”</p><cite>Amir Mostafavi, lawyer fined $10,000 after submitting opening brief filled with quotes fabricated by ChatGPT</cite></blockquote></figure>

<p>Charlotin thinks courts and the public should expect to see an exponential rise in these cases in the future. When he started tracking court filings involving AI and fake cases earlier this year, he encountered a few cases a month. Now he sees a few cases a day. Large language models confidently state falsehoods as facts, particularly when there are no supporting facts.</p>

<p>“The harder your legal argument is to make, the more the model will tend to hallucinate, because they will try to please you,” he said. “That’s where the confirmation bias kicks in.”</p>

<p>A <a href="https://hai.stanford.edu/news/ai-trial-legal-models-hallucinate-1-out-6-or-more-benchmarking-queries">May 2024 analysis</a> by Stanford University’s RegLab found that although three out of four lawyers plan to use generative AI in their practice, some forms of AI generate hallucinations in one out of three queries. Detecting fake material cited in legal filings <a href="https://www.nytimes.com/2025/05/05/technology/ai-hallucinations-chatgpt-google.html">could get harder as models grow in size</a>.</p>

<p><a href="https://www.ailawlibrarians.com/2025/08/10/coming-soon-the-interactive-genai-legal-hallucination-tracker-sneak-peek-today/">Another tracker</a> of cases where lawyers cite nonexistent legal authority due to use of AI identifies 52 such cases in California and more than 600 nationwide.<strong> </strong>That amount is expected to increase in the near future because AI innovation is outpacing the education of attorneys, said Nicholas Sanctis, a law student at Capital University Law School in Ohio.</p>
	<div data-posts="" data-current-post-id="476080">
							<h2>
					<span>More on AI in government</span>
				</h2>
						
	<article data-post-id="466445">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

				</div>
	
<p>Jenny Wondracek, who leads the tracker project, said she expects this trend to get worse because she still regularly encounters lawyers who don’t know that AI makes things up or believe that legal tech tools can eliminate all fake or false material generated by language models.&nbsp;</p>

<p>“I think we’d see a reduction if (lawyers) just understood the basics of the technology,” she said.</p>

<p>Like Charlotin, she suspects there are more instances of made up cases generated by AI in state court filings than in federal courts, but a lack of standard filing methods makes it difficult to verify that. She said she encounters fake cases most often among overburdened attorneys or people who choose to represent themselves in family court.</p>

<p>She suspects the number of arguments filed by attorneys that use AI and cite fake cases will continue to go up, but added that not just attorneys engage in the practice. In recent weeks, she’s documented three <a href="https://www.ailawlibrarians.com/2025/07/03/first-known-court-order-with-fabricated-cases-and-a-test-run-of-citecheck-ai/">instances of judges</a> citing fake legal authority in their decisions.</p>

<figure><blockquote><p>She’s documented three judges citing fake legal authority in their decisions.</p></blockquote></figure>

<p>As California considers how to treat generative AI and fake case citations, Wondracek said they can consider approaches taken by other states, such as temporary suspensions, requiring attorneys who get caught to take courses to better understand how to ethically use AI, or requiring them to <a href="https://www.lawnext.com/2025/09/nevada-judge-takes-creative-and-unusual-approach-to-combat-ai-generated-fictitious-citations.html">teach law students how they can avoid making the same mistake</a>.</p>

<p>Mark McKenna, codirector of the UCLA Institute of Technology, Law &amp; Policy praised fines like the one against Mostafavi as punishing lawyers for “an abdication of your responsibility as a party representing someone.” He thinks the problem “will get worse before it gets better,” because there’s been a rush among law schools and private firms to adopt AI without thinking through the appropriate way to use them.</p>

<p>UCLA School of Law professor Andrew Selbst agrees, pointing out that clerks that work for judges are recent law school graduates, and students are getting bombarded with the message that they must use AI or get left behind. Educators and other professionals <a href="https://calmatters.org/economy/technology/2024/06/teachers-ai-grading/">report feeling similar pressures</a>.</p>

<p>“This is getting shoved down all our throats,” he said. “It’s being pushed in firms and schools and a lot of places and we have not yet grappled with the consequences of that.”</p>

<p><strong>For the record</strong>: The fine issued to Mostafavi was for $10,000. Due to an editing error, an earlier version of this article had an incorrect figure.</p>
	<div data-posts="" data-current-post-id="476080">
							<h2>
					<span>READ NEXT</span>
				</h2>
						
	<article data-post-id="475357">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

		
	<article data-post-id="467979">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

				</div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems (329 pts)]]></title>
            <link>https://openai.com/index/openai-nvidia-systems-partnership/</link>
            <guid>45335474</guid>
            <pubDate>Mon, 22 Sep 2025 16:10:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/openai-nvidia-systems-partnership/">https://openai.com/index/openai-nvidia-systems-partnership/</a>, See on <a href="https://news.ycombinator.com/item?id=45335474">Hacker News</a></p>
Couldn't get https://openai.com/index/openai-nvidia-systems-partnership/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[UK Millionaire exodus did not occur, study reveals (226 pts)]]></title>
            <link>https://taxjustice.net/press/millionaire-exodus-did-not-occur-study-reveals/</link>
            <guid>45335135</guid>
            <pubDate>Mon, 22 Sep 2025 15:48:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taxjustice.net/press/millionaire-exodus-did-not-occur-study-reveals/">https://taxjustice.net/press/millionaire-exodus-did-not-occur-study-reveals/</a>, See on <a href="https://news.ycombinator.com/item?id=45335135">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="s_content"><p><span><em>30 news pieces a day in 2024 on non-existent exodus</em></span></p><p>A millionaire exodus widely reported by news outlets around the world in 2024, and credited for the UK Labour government’s decision to weaken tax reforms, did not occur, the Tax Justice Network reveals.</p><p>The media reporting – consisting of over 10,900 news pieces across print, broadcast and online news in 2024 – was primarily based on a report published by Henley &amp; Partners<sup>1</sup>, a firm that sells golden passports to the superrich and advises governments on setting up such schemes. The European Court of Justice recently ruled one such scheme, that of Malta, to be unlawful.<sup>2</sup></p><p>The Tax Justice Network’s review – co-published with Patriotic Millionaires UK and Tax Justice UK – of the Henley report finds that the number of millionaires claimed by Henley &amp; Partners to be leaving countries in “exodus” in 2024 represented near-0% of those countries’ millionaire populations.<sup>3</sup> For example, the 9500 millionaires widely reported to be leaving the UK in 2024 represented 0.3% of the UK’s 3.06 million millionaires.<sup>4</sup></p><p>Media reporting widely blamed the alleged millionaire exodus on tax policies in the same year that calls for a wealth tax on the superrich gained unprecedented momentum globally.<sup>5</sup> The media reporting was equivalent to 30 news pieces a day on the non-existent millionaire exodus across 2024.</p><p>Reviewing the full period from 2013 to 2024 for which the Henley report presents estimates on millionaire migration, the Tax Justice Network finds that millionaire migration rates consistently stood at near-0% for every year.<sup>6</sup> Academic studies consistently show that the tax responses of the wealthy involves minimal levels of migration.<sup>7</sup></p><p>Henley’s estimates, when put into perspective, reveal a picture that is at complete odds with the report’s narrative and media coverage: millionaires are highly immobile, and nearly 100% of millionaires have <em>not</em> relocated to a new country since 2013, if Henley’s estimates are to be taken at face value.</p><p>Henley &amp; Partners was accused in a 2018 UK Parliamentary inquiry of meddling in the elections of Caribbean nations in return for the exclusive rights to sell golden passports.<sup>8</sup> Henley &amp; Partners told The Guardian it “fundamentally rejects any allegation of wrongdoing”.<sup>9</sup> A recent Financial Times article identified an EU-sanctioned Russian businessman with links to the Ukraine invasion who could more easily circumvent travel restrictions due to a Maltese golden passport Henley helped him acquire in the past.<sup>10</sup> A spokesperson for Henley &amp; Partners told the Financial Times that while she “could not comment on individual cases because of missing information and data protection… an individual ‘may pass all the stringent due diligence tests imposed, but still go on to engage in criminal activity.’”<sup>11 </sup></p><p>Golden passports are now illegal in the EU following a successful court challenge brought by the European Commission against Malta’s scheme, on which Henley &amp; Partners had advised. The Commission said such schemes pose serious risks for money laundering, tax evasion and corruption.<sup>12</sup> Henley &amp; Partners told media in response that it was “disappointed” but that the decision “will only increase the demand for specialized advisors”.<sup>13</sup></p><p><span><strong>Findings behind Labour climbdown riddled with problems</strong></span></p><p>The UK Labour government’s decision in January 2025 to weaken non-dom tax reform was widely reported to be a result of concerns about the Henley report’s findings.<sup>14</sup></p><p>The Tax Justice Network’s review of the Henley report flags several issues with the report’s methodology as well as contradictions in Henley &amp; Partner’s reporting, and particularly in its claims on the UK exodus.</p><p>Strikingly, the report’s methodology<sup>15</sup> states that its estimates are primarily a measure of where millionaires <em>say they work on social media</em> and not of where they live or reside, meaning the report does not track actual, physical migration – contrary to the presenting of the estimates in the press.</p><p>Moreover, the report uses a far narrower definition of ‘millionaires’ that does not include all dollar millionaires like the standard definition (people with net worth of 1 million dollars or more), but rather only individuals with liquid assets worth 1 million dollars or more, who are thus richer and more mobile on average than a standardly defined millionaire.<sup>16</sup> In the case of the UK, the ‘millionaires’ identified by the report represent just a fifth (20%) of the UK millionaire population.<sup>17 </sup>Even then, the report is based on a small sample from within these narrowly defined millionaires and the sample is skewed towards centi-millionaires and billionaires, who are also likely to be the most easily mobile.<sup>18</sup></p><p>Just as striking, the use of the term “exodus” has been inconsistent in the analysis. In 2021, Henley described 2000 millionaires leaving the UK as “insignificant” but in 2023 described 1600 millionaires leaving the UK an “exodus”. In 2023, the 6500 millionaires claimed to be leaving India were described as “not particularly concerning” but redescribed in 2024 as a “wealth exodus”.<sup>19</sup></p><p>The Tax Justice Network wrote to Henley &amp; Partners and New World Wealth (who prepared the Henley report’s estimates) with questions for each ahead of the publication of its review. The response received said<sup>20</sup>:</p><p><em>“It seems this entire debate is over that one word. The dictionary definition is just ‘mass migration’, and HMRC’s own data shows that the number of non-doms in the UK is decreasing year on year – which seems to be a mass migration. If you are looking to the biblical definition, then to use the term ‘exodus’ would of course mean that all non-doms are leaving, but I don’t think many people take biblical interpretations quite so literally?”</em></p><p>Furthermore, Henley &amp; Partners labelled the UK’s alleged exodus a <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2023/private-wealth-insights/transformative-era-investment-migration">“Brexodus”</a> in 2023, claiming that the exodus was largely an impact of Brexit.<sup>21 </sup>In October 2024, Henley relabelled the exodus a <a href="https://www.henleyglobal.com/newsroom/press-releases/uk-wealth-exodus">“Wexit”</a> in a press release framing the UK exodus as a reaction to tax hikes that might be announced in the UK Labour government’s upcoming budget statement.<sup>22</sup></p><p>Henley &amp; Partners’ specified in October 2024 that the “Wexit” is a “wealth exodus” that includes centi-millionaires and billionaires, and the report’s author emphasised that “[t]he large number of centi-millionaires leaving the UK is particularly concerning”.<sup>23</sup> These claims appear inconsistent with Henley’s forecast made the month prior in September 2024 that the UK centi-millionaire population is growing and will continue to grow from 2024 to 2040.<sup>24</sup></p><p>The press release highlighted that the UK Labour government’s budget statement as a main reason for this alleged “wealth exodus”:</p><p><em>“The UK’s high tax rates and concerns about additional tax hikes that could be announced at the end of the month in the Labour Party’s first budget in 14 years, are highlighted as being among the main reasons for the wealth exodus.”<sup>25</sup></em></p><p>A response sent by Henley &amp; Partners to the Tax Justice Network said:</p><p><em>“We have never claimed that Labour tax policies were the sole or root cause.&nbsp; If papers such as the Telegraph, Times, Mail, decide to add their own layer on to that, and deliberately exclude from their story our standard reminder to them that these were the Conservatives’ tax changes, then I think your argument is with them not with us.”</em></p><p>Moreover, it is unclear whether the forecast of centi-millionaires and billionaires leaving the UK that Henley reported in October 2024 was different from the forecast it initially made in June 2024 when the Labour party was not in power.</p><p><span><strong>Media adds fuel to the fictional fire</strong></span></p><p>The Tax Justice Network’s analysis of media coverage of the Henley report finds that coverage often went far beyond any claims made in the report itself, contributing to an entirely unfounded narrative about the role of tax and government policies in causing a millionaire exodus which itself did not occur.</p><p>Tax was mentioned in half of global media coverage of the exodus and far more often than any other exodus drivers discussed in the Henley report.</p><p>The UK Labour party, which was not in power when the report was published in June 2024, was mentioned more than twice as much as the UK Conservative party in global media coverage, and nearly four times as much as Brexit in UK media coverage.</p><p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNDc0IiB2aWV3Qm94PSIwIDAgODAwIDQ3NCI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Graph-drivers-in-the-media.png" alt="" width="800" height="474"><br>
<span>Note: Percentages show the number of mentions as a share of all global media coverage &nbsp;</span></p><p>The picture is more skewed in UK media coverage, where tax was mentioned in 71% of coverage and Labour mentioned in 43% of coverage.<sup>26</sup></p><p>Seven high-profile millionaires leaving the UK were mentioned nearly three times more often in global media coverage than pro-tax millionaire campaign groups representing hundreds of millionaires.<sup>27</sup></p><p>In contrast to the media narrative, 81% of UK millionaires agree with the statement that it is patriotic to pay a fair share of tax, according to a poll published on 5 June 2025 by Patriotic Millionaires UK. 80% of UK millionaires said they would support a wealth tax of 2% on wealth over £10 million.<sup>28</sup></p><p>The Tax Justice Network’s review of the Henley report raised a number of other questions, including the statistical credibility of drawing any conclusions from a self-admittedly unrepresentative sample; and the degree of extrapolation necessary to make any findings about smaller groups such as billionaires.</p><p>On the report’s sample, the response sent by Henley &amp; Partners said:</p><p><em>“Statistically, if it is consistent year by year, then laws of statistical sampling mean that it can be used to draw a conclusion.”</em></p><p>The report is published by Henley &amp; Partners but prepared by New World Wealth<sup>29</sup>, which describes itself as a “wealth intelligence firm” on its website. New World Wealth appears to have only one staff member and has not made the data behind its calculations public.</p><p>New World Wealth has been publishing estimates on millionaire migrations for at least a decade, and first began to publish its estimates with Henley &amp; Partners in 2022, which was the first time the estimates on millionaire migrations underway were described as an “exodus”.</p><p>More specific questions about the sample sent to New World Wealth, including a question asking how many real persons in the sample were observed to have “migrated” in 2024, were not responded to.</p><p><strong>Fariya Mohiuddin</strong>, Deputy Director: External Affairs at Tax Justice UK said:</p><p><em>“Taxing the super-rich to revitalise key services like the NHS and education, that we all rely on, is more urgent than ever. Taxing the wealth of the richest is simply not going to cause a mass exodus. This is scaremongering and statistical obfuscation by companies that represent the interests of billionaires and multi-millionaires. In fact, when the numbers are crunched properly, rather than using dodgy stats and figures, tax is an inconsequential factor in the decision-making of the vanishingly small percentage of millionaires that do decide to move. In fact, many wealthy people want to pay more tax. They know that when public services are well-funded, people are healthy, and the country works better, they will benefit – alongside everyone else.”</em></p><p>Member of Patriotic Millionaires UK and legal consultant, <strong>Stephen Kinsella</strong> said:</p><p><em>“As this excellent report from the Tax Justice Network shows, millionaires like me aren’t going anywhere.&nbsp;We want to build a better&nbsp;Britain so we’re proud to pay&nbsp;and here to stay.&nbsp;When nearly three quarters of UK millionaires think taxes should be raised on the richest to reduce the strain on everyone else, and 81%&nbsp;think it’s patriotic to pay their fair share in tax, what on earth is stopping our Government from doing their duty and taxing extreme wealth?”</em></p><p><strong>Alex Cobham</strong>, chief executive at the Tax Justice Network, said:</p><p><em>“The majority of people want taxes on the superrich, the majority of millionaires are saying tax us, and practically all credible studies say you should do it. But &nbsp;what the media reported, and the government listened to, was a fictional millionaire ‘exodus’ based on questionable data published by a firm that helps the superrich buy their way out of the rules that apply to everybody else. Tax is our most powerful tool for creating more equal societies, but scare stories like these are used to talk down to people and to block positive change.” &nbsp;</em></p><p><em>“This is a wakeup call for media professionals and governments alike. Do your homework when it comes to tax. Treat the Henley report and any such claims about fleeing millionaires with extreme caution, and make sure your stories and your policy decisions are based on robust evidence.”</em></p><p>-ENDS-</p><p><strong>Read </strong><a href="https://taxjustice.net/reports/the-millionaire-exodus-myth/"><strong>our review of the Henley report</strong></a></p><p><strong>Notes to Editor</strong></p><ol><li>The <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2024/">Henley Private Wealth Migration Report 2024</a> was published on 18 June 2024 by <a href="https://www.henleyglobal.com/">Henley &amp; Partners</a>. The report’s estimates are prepared by <a href="http://newworldwealth.com/">New World Wealth</a>.</li><li>See this <a href="https://www.ft.com/content/7bc66138-a5b4-4fd6-948c-78bb65d6d492">FT article</a> for more information about the European Court of Justice’s ruling and Henley &amp; Partners role in the Maltese scheme. See the press release from the Court <a href="https://curia.europa.eu/jcms/upload/docs/application/pdf/2025-04/cp250052en.pdf">here</a>.</li><li>The Tax Justice Network’s review of the Henley report is available <a href="https://taxjustice.net/reports/the-millionaire-exodus-myth/">here.</a></li><li>Countries’ reported migrating millionaires represented less than 1% of their millionaires, and was closer to 0% for most countries, including the UK.<br>
<img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2NTkiIGhlaWdodD0iMzA2IiB2aWV3Qm94PSIwIDAgNjU5IDMwNiI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Table-national-global-migrations.png" alt="" width="659" height="306"><br>
<span>Source: <em>The millionaire exodus myth</em>, Tax Justice Network, June 2025</span></li><li>Some examples of media coverage:<br>
– <a href="https://www.reuters.com/world/uk/globally-mobile-millionaires-threaten-desert-britain-over-tax-2024-07-19/"><strong>Reuters:</strong></a> Ultra-rich entrepreneurs threaten to desert Britain over tax<br>
– <a href="https://www.bloomberg.com/news/articles/2024-07-05/britain-s-ultra-rich-map-out-plans-to-escape-non-dom-taxes-after-labour-victory"><strong>Bloomberg UK:</strong></a> Britain’s Ultra-Rich Map Out Routes to Escape ‘Non-Dom’ Taxes After Labour Victory<br>
– <a href="http://edition.cnn.com/2024/06/18/business/uk-millionaires-loss-record"><strong>CNN:</strong></a> Millionaires fleeing Britain in their thousands<br>
– <a href="https://www.telegraph.co.uk/business/2024/07/10/millennials-inheritance-boomer-wealth/"><strong>The Telegraph:</strong></a> Britain to suffer world’s biggest exodus of millionaires as Labour takes power<br>
– <a href="https://www.cnbc.com/2024/06/18/millionaires-are-abandoning-the-uk-in-their-droves-new-research-shows.html"><strong>CNBC:</strong></a> Millionaires are abandoning the UK in droves, new research shows</li><li>The total number of millionaires reported on <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2024/global-wealth-migration">the Henley &amp; Partners website</a> to have migrated every year since 2013 to 2023 consistently represented around 0.2% of millionaires annually. Moreover, while the global millionaire population has grown since 2013, the millionaire migration rate, however small, is marginally lower now than it was in the middle of the previous decade – even after bouncing back from the enforced immobility of the pandemic years. The “<a href="https://www.henleyglobal.com/newsroom/press-releases/henley-private-wealth-migration-report-2024">unprecedented</a>”, “<a href="https://www.henleyglobal.com/newsroom/press-releases/henley-private-wealth-migration-report-2024">record numbers</a>” of migrating millionaires Henley reported in 2024 are proportionally smaller than the migration numbers reported for 2016, 2017 and 2018.<br>
<img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNDQ1IiB2aWV3Qm94PSIwIDAgODAwIDQ0NSI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Graph-rate-vs-population-2013-2024.png" alt="" width="800" height="445"><br>
<span>Source: <em>The millionaire exodus myth</em>, Tax Justice Network, June 2025</span></li><li>See the literature on migration surveyed in our report, <a href="https://taxjustice.net/reports/taxing-extreme-wealth-what-countries-around-the-world-could-gain-from-progressive-wealth-taxes/"><em>Taxing extreme wealth: what countries around the world could gain from progressive wealth taxes</em></a> (Alison Schulz &amp; Miroslav Palanský), 2024, Tax Justice Network. Meanwhile, a <a href="https://www.lse.ac.uk/News/Latest-news-from-LSE/2024/a-January-2024/super-rich-unlikely-to-leave-uk-for-boring-and-culturally-barren-tax-havens">London School of Economics study</a> found that the vast majority of Britain’s extremely wealthy people would never leave the country for tax reasons, partly due to the stigma involved in doing so, and partly because they think lower-tax jurisdictions are “boring”.</li><li>More information about the inquiry in this <a href="https://www.ft.com/content/7bc66138-a5b4-4fd6-948c-78bb65d6d492">FT article</a>. Read The Guardian’s 2018 investigation <a href="https://www.theguardian.com/world/2018/oct/16/the-passport-king-who-markets-citizenship-for-cash">here</a>.</li><li>See Henley’s response to the Guardian <a href="https://www.theguardian.com/world/2018/oct/16/henley-partners-statement">here</a>.</li><li>Read the FT’s investigation on Maltese golden passports sold to Russians <a href="https://www.ft.com/content/b6accfc5-eeca-4a83-a39d-1dc54a7688dd">here</a>.</li><li>See note 10 for Henley’s response.</li><li>See note 2.</li><li>See Henley’s statement on the European Court of Justice’s ruling <a href="https://www.henleyglobal.com/newsroom/press-releases/ecj-malta-ruling-2025">here</a>.</li><li>Some examples of media reporting:<br>
– <a href="https://news.sky.com/story/rachel-reeves-to-soften-non-dom-tax-crackdown-after-listening-to-concerns-13295016"><strong>Sky News</strong></a><strong>: </strong>“Rachel Reeves is to water down her crackdown on the non-dom tax status after analysis showed it had prompted an exodus of millionaires.”<br>
– <a href="https://www.cnbc.com/2025/01/24/uk-to-soften-tax-rules-for-wealthy-foreigners-after-millionaire-exodus-rachel-reeves-says.html"><strong>CNBC</strong></a><strong>: </strong>“The U.K. is to soften some planned changes to its controversial non-dom tax rule following concerns of a millionaire exodus, the Treasury has confirmed.”<br>
– <a href="https://www.independent.co.uk/news/uk/politics/reeves-davos-non-dom-tax-millionaire-b2684926.html"><strong>The Independent</strong></a><strong>: </strong>“Reeves to water down tax raid on non-doms amid exodus of millionaires”<br>
– <a href="https://www.thetimes.com/business-money/economics/article/rachel-reeves-to-amend-non-dom-tax-rules-after-millionaire-exodus-s6fjr2tj9"><strong>The Times</strong></a><strong>:</strong> “Rachel Reeves to relax non-dom tax rule amid millionaire exodus”</li><li>The Henley report’s methodology states: “The firm [New World Wealth] uses various public sources to check city locations, including LinkedIn and other business portals. Its stats are therefore mainly based on the work locations of the individuals.” The methodology is available at the bottom of this <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2024/">webpage</a>.</li><li>See the Henley report’s methodology at the bottom of this <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2024/">webpage</a>.</li><li>The Henley report’s author stated in a <a href="https://www.bbc.co.uk/sounds/play/m002304z">BBC interview</a> that the group of UK ‘millionaires’ as defined in the report totalled 602,000, which is around one fifth of the UK’s millionaire population, which the UBS Global Wealth Report 2024 estimates to stand at 3.06 million millionaires.</li><li>The Henley report’s author Andrew Amolis acknowledged in an interview with <a href="https://www.bbc.co.uk/sounds/play/m002304z">BBC More or Less</a> that the report’s sample is skewed. Presenter Tim Harford challenged Mr Amolis further (emphasis added):<strong>Andrew Amoils</strong>: Most of the database – I’d say between twenty and a hundred million dollars in assets, that would be the bulk of the database. So our data is skewed to the top end, so there will be the billionaires and the centimillionaires with over a hundred million, with some of the people lower down it’s more difficult to know if they are a high net worth.<strong>Tim Harford</strong>: But wait, aren’t these super-rich more easily able to skip off to Monaco or Dubai than your run of a mill dollar millionaire?<strong>AA</strong>: No, you’re right, 100%, that would be an issue. Though I would argue that the super-wealthy leaving is obviously the most important, because if you’ve got a banker at Goldman Sachs who’s making five hundred thousand pounds a year and they leave, that has very little impact whereas if somebody with over a hundred million who’s got a business leaves, the impact’s much greater.<strong>TH</strong>: Sure, but the headlines are not about a few people controlling a huge amount of money leaving, the headlines are about 9000 millionaires leaving. So the headlines imply that there is some kind of representative sample, and there’s some kind of reasonable extrapolation, but from <strong>based on what you’ve told me I don’t think we really can reasonably extrapolate</strong>, given the methods you’re describing.<strong>AA</strong>: Well how else would you do it? I mean, we’ve got a sample of 150000 high net worths globally, and we’re tracking them in terms of their movements. How else would you do it? How else would you work out where people are going, apart from the way we’re doing it?<strong>TH</strong>: Well I think <strong>if you don’t have a representative sample, you don’t have any basis to make the claim at all</strong>.<strong>AA</strong>: Well I would argue it is a representative sample. 150000 people, that’s a lot!<strong>TH</strong>: <strong>But you just told us it wasn’t representative.</strong> The sheer number of people doesn’t make it representative.</li><li>The term “exodus” has been used inconsistently, as this table shows.<br>
<img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2MDIiIGhlaWdodD0iMTkzIiB2aWV3Qm94PSIwIDAgNjAyIDE5MyI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Table-use-of-exodus.png" alt="" width="602" height="193"><br>
<span>Source: <em>The millionaire exodus myth</em>, Tax Justice Network, June 2025</span><br>
<span><em>Note</em>: The table presents migration numbers for the three reports New World Wealth published with the AfrAsia bank from 2018 to 2020 and three reports it published with Henley &amp; Partners from 2022 to 2024. New World Wealth’s calculation of migration as a percentage of millionaire population are provided in parentheses where available. New World Wealth provided the percentages in 2019 and 2020, and provided percentages for some countries in 2022. No percentages were provided in 2023 and 2024 that we could find. </span><br>
<span>*Retroactively called a “wealth exodus” in Henley’s 2024 <a href="https://www.henleyglobal.com/newsroom/press-releases/henley-private-wealth-migration-report-2024">press release</a>.</span></li><li>The response the Tax Justice Network received is reproduced in full at the end of the Tax Justice Network’s <a href="https://taxjustice.net/reports/the-millionaire-exodus-myth/">review of the Henley report</a>.</li><li>See Henley &amp; Partners’ use of the term “Brexodus” in this <a href="https://www.henleyglobal.com/publications/henley-private-wealth-migration-report-2023/private-wealth-insights/transformative-era-investment-migration">article </a>from the Henley 2023 report.</li><li>See Henley &amp; Partner’s October 2024 press release using the term “Wexit” <a href="https://www.henleyglobal.com/newsroom/press-releases/uk-wealth-exodus">here</a>. The press release was syndicated at least 400 times across the media landscape in large part due to the PR Newswire service. While Henley’s original press release did list in its notes to editor Brexit as possible driver of exodus, the notes to editor were cut off in the <a href="https://www.prnewswire.co.uk/news-releases/wexit-wealthy-brits-exit-uk-for-eu-ahead-of-budget-302280346.html">PR Newswire version</a> of the press release that was widely syndicated. Nonetheless, Henley’s October 2024 press release did refer to Brexit in the body of the release, but when doing so contrasted Brexit as a positive phenomenon that is separate from the wealth exodus. The press release stated: “Based on data over the past nine months, the UK’s wealth exodus or WEXIT is expected to include 85 centi-millionaires and 10 billionaires, and in an ironic reversal of Brexit fortunes, 68% are heading for Europe, with favored destinations being Italy, Malta, Greece, Portugal, Switzerland, Monaco, Cyprus, France, Spain, and the Netherlands.”</li><li>Henley’s <a href="https://www.henleyglobal.com/publications/centi-millionaire-report-2024"><em>The Centi-Millionaire Report 2024</em></a> was published on 17 September 2024.</li><li>See note 22.</li><li>Looking specifically at UK media coverage, we find the mentions of themes and drivers to be far more skewed towards tax and Labour.<br>
<img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNDc0IiB2aWV3Qm94PSIwIDAgODAwIDQ3NCI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" loading="lazy" data-src="https://taxjustice.net/wp-content/uploads/2025/06/Graph-drivers-in-UK-media.png" alt="" width="800" height="474"><br>
<span>Source: <em>The millionaire exodus myth</em>, Tax Justice Network, June 2025</span></li><li>The seven high-profile millionaires reportedly moving away from the UK – Charlie Mullins, Christian Angermayer, Alan Howard, Nassef Sawiris, Asif Aziz and Bassim Haidar – were mentioned in 199 articles. In contrast, Patriotic Millionaires, Patriotic Millionaires UK, Millionaires for Humanity, Tax Me Now and Proud to Pay More – campaigning groups representing over 300 millionaires calling on governments to tax them more – were mentioned 73 times. The seven migrating millionaires were mentioned 2.7 times more than the pro-tax millionaires groups.</li><li>See Patriotic Millionaire UK’s polling <a href="https://patrioticmillionaires.uk/latest-news/uk-millionaire-poll-2025">here</a>.</li><li>See New World Wealth’s <a href="http://newworldwealth.com/">website</a>.</li></ol><p><strong>About Patriotic Millionaires UK</strong></p><p><a href="https://patrioticmillionaires.uk/">Patriotic Millionaires UK</a> is a nonpartisan network of British millionaires, from multiple industries and backgrounds from across the UK. It delivers a single mission – to leverage the voice of wealth to build a better Britain by changing the system to end extreme wealth and make those with it make their fair and proper contribution.</p><p><strong>About Tax Justice UK</strong></p><p>The UK’s approach to tax isn’t working. Our government fails to raise enough money to support high quality public services and wealth is desperately under-taxed. We campaign for a fairer tax system that takes more from the very wealthy. A tax system that actively redistributes wealth to tackle inequality; and that funds high quality public services. Our mission is to ensure that everyone in the UK benefits from a fair and effective tax system. <a href="https://taxjustice.uk/">Tax Justice UK</a> is a partner of – but independent from – the Tax Justice Network.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A New Internet Business Model? (183 pts)]]></title>
            <link>https://blog.cloudflare.com/cloudflare-2025-annual-founders-letter/</link>
            <guid>45334599</guid>
            <pubDate>Mon, 22 Sep 2025 15:14:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/cloudflare-2025-annual-founders-letter/">https://blog.cloudflare.com/cloudflare-2025-annual-founders-letter/</a>, See on <a href="https://news.ycombinator.com/item?id=45334599">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>2025-09-21</p><section><p>9 min read</p><img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/1YhhjZVSFTK2FlxA3ZcYba/ceb030d8cae80d37fce08c2a2ceafd88/BLOG-2994_1.png" alt=""><div><p>Cloudflare <a href="https://www.youtube.com/watch?v=XeKWeBw1R5A"><u>launched 15 years ago</u></a> this week. We like to celebrate our birthday by announcing new products and features that give back to the Internet, which we’ll do a lot of this week. But, on this occasion, we've also been thinking about what's changed on the Internet over the last 15 years and what has not.</p><p>With some things there's been clear progress: when we launched in 2010 less than 10 percent of the Internet was encrypted, today well over 95 percent is encrypted. We're proud of the <a href="https://blog.cloudflare.com/introducing-universal-ssl/"><u>role we played in making that happen</u></a>.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/2MLknOh75r4KpCfiXTjQkw/b80baa01b75437f3b1da24be3ca9e209/Timeline_2_part.png" alt="" width="2070" height="411" loading="lazy">
          </figure>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5xkR8gdKR1YO1tIr6rLOmv/7e848bbefa83db1078d7ffe35e2bcc51/2.png" alt="" width="2064" height="411" loading="lazy">
          </figure><p>Some other areas have seen limited progress: IPv6 adoption has grown steadily but painfully slowly over the last 15 years, in <a href="https://blog.cloudflare.com/introducing-cloudflares-automatic-ipv6-gatewa/"><u>spite</u></a> <a href="https://blog.cloudflare.com/cloudflare-expanding-the-ipv6-web/"><u>of</u></a> <a href="https://blog.cloudflare.com/eliminating-the-last-reasons-to-not-enable-ipv6/"><u>our</u></a> <a href="https://blog.cloudflare.com/amazon-2bn-ipv4-tax-how-avoid-paying/"><u>efforts</u></a>. That's a problem because as IPv4 addresses have become scarce and expensive it’s held back new entrants and driven up the costs of things like networking and cloud computing.</p>
    <p>
      <h2 id="the-internets-business-model">The Internet’s Business Model</h2>
      
    </p>
    <p>Still other things have remained remarkably consistent: the basic business model of the Internet has for the last 15 years been the same — create compelling content, find a way to be discovered, and then generate value from the resulting traffic. Whether that was through ads or subscriptions or selling things or just the ego of knowing that someone is consuming what you created, traffic generation has been the engine that powered the Internet we know today.</p><p>Make no mistake, the Internet has never been free. There's always been a reward system that transferred value from consumers to creators and, in doing so, filled the Internet with content. Had the Internet not had that reward system it wouldn't be nearly as vibrant as it is today.</p><p>A bit of a trivia aside: why did Cloudflare never build an ad blocker <a href="https://www.answeroverflow.com/m/1123890164222144542"><u>despite many requests</u></a>? Because, as imperfect as they are, ads have been the only micropayment system that has worked at scale to encourage an open Internet while also compensating content creators for their work. Our mission is to help build a better Internet, and a core value is that we’re principled, so we weren’t going to hamper the Internet’s fundamental business model.</p>
    <p>
      <h2 id="traffic-value">Traffic ≠ Value</h2>
      
    </p>
    <p>But that same traffic-based reward system has also created many of the problems we lament about the current state of the Internet. Traffic has always been an imperfect proxy for value. Over the last 15 years we've watched more of the Internet driven by annoying clickbait or dangerous ragebait. Entire media organizations have built their businesses with a stated objective of writing headlines to generate the maximum cortisol response because that's what generates the maximum amount of traffic.</p><p>Over the years, Cloudflare has at times faced calls for us to intervene and control what content can be published online. As an infrastructure provider, we've never felt we were the right place for those editorial decisions to be made. But it wasn't because we didn't worry about the direction the traffic-incentivized Internet seemed to be headed. It always seemed like what fundamentally needed to change was not more content moderation at the infrastructure level but instead a healthier incentive system for content creation.</p><p>Today the conditions to bring about that change may be happening. In the last year, something core to the Internet we’ve all known has changed. It's being driven by AI and it has an opportunity with some care and nurturing to help bring about what we think may be a much better Internet.</p>
    <p>
      <h2 id="from-search-to-answers">From Search to Answers</h2>
      
    </p>
    <p>What’s the change? The primary discovery system of the Internet for the last 15 years has been Search Engines. They scraped the Internet's content, built an index, and then presented users with a treasure map which they followed generating traffic. Content creators were happy to let Search Engines scrape their content because there were a limited number of them, so the infrastructure costs were relatively low and, more importantly, because the Search Engines gave something to sites in the form of traffic —&nbsp;the Internet’s historic currency —&nbsp;sent back to sites.</p><p>It’s already clear that the Internet’s discovery system for the next 15 years will be something different: Answer Engines. Unlike Search Engines which gave you a map where you hunted for what you were looking for, driving traffic in the process, Answer Engines just give you the answer without you having to click on anything. For 95 percent of users 95 percent of the time, that is a better user experience.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5d2TQwVHA8GpFUBpAdr8QT/23fd6b7306d55dce3dea9e989784595d/BLOG-2994_3.png" alt="BLOG-2994 3" width="1801" height="564" loading="lazy">
          </figure><p>You don’t have to look far to see this is changing rapidly before our eyes. ChatGPT, Anthropic’s Claude, and other AI startups aren’t Search Engines — they’re Answer Engines. Even Google, the search stalwart, is increasingly serving “AI Overviews” in place of 10 blue links. We can often look to sci-fi movies to have a glimpse into our most likely future. In them, the helpful intelligent robot character didn’t answer questions with: “Here are some links you can click on to maybe find what you’re looking for.” Whether you like it or not, the future will increasingly be answers not searches.</p>
    <p>
      <h2 id="short-term-pain">Short Term Pain</h2>
      
    </p>
    <p>In the short term, this is going to be extremely painful for some industries that are built based on monetizing traffic. It already is. While ecommerce and social applications haven't yet seen a significant drop in traffic as the world switches to Answer Engines, media companies have. Why the difference? Well, for the former, you still need to buy the thing the Answer Engine recommends and, for now, we still value talking with other humans.</p><p>But for media companies, if the Answer Engine gives you the summary of what you’re looking for in most cases you don’t need to read the story. And the loss of traffic for media companies has already been dramatic. It’s not just traditional media. Research groups at investment banks, industry analysts, major consulting firms —&nbsp;they’re all seeing major drops in people finding their content because we are increasingly getting answers not search treasure maps.</p><p>Some say these answer engines or agents are just acting on behalf of humans. Sure but so what? Without a change they will still kill content creators’ businesses. If you ask your agent to summarize twenty different news sources but never actually visit any of them you’re still undermining the business model of those news sources. Agents don’t click on ads. And if those agents are allowed to aggregate information on behalf of multiple users it’s an even bigger problem because then subscription revenue is eliminated as well. Why subscribe to the Wall Street Journal or New York Times or Financial Times or Washington Post if my agent can free ride off some other user who does?</p><p>Unless you believe that content creators should work for free, or that they are somehow not needed anymore —&nbsp;both of which are naive assumptions —&nbsp;something needs to change. A visit from an agent isn’t the same as a visit from a human and therefore should have different rules of the road. If nothing changes, the drop in human traffic to the media ecosystem writ large will kill the business model that has built the content-rich Internet we enjoy today.</p><p>We think that’s an existential threat to one of humanity’s most important creations: the Internet.</p>
    <p>
      <h2 id="rewarding-better-content">Rewarding Better Content</h2>
      
    </p>
    <p>But there’s reason for optimism. Content is the fuel that powers every AI system and the companies that run those AI systems know ultimately they need to financially support the ecosystem. Because of that it seems potentially we're on the cusp of a new, better, and maybe healthier Internet business model. As content creators use tools like the <a href="https://blog.cloudflare.com/introducing-ai-crawl-control/"><u>ones provided by Cloudflare to restrict AI robots from taking their content without compensation</u></a>, we're already seeing a market emerge and better deals being struck between AI and content companies.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5J0hmMolAcrPKBZSJzKNMw/d78a04e0ae0afb2c578e7b7c1ca8b1c9/BLOG-2994_4.png" alt="BLOG-2994 4" width="1200" height="262" loading="lazy">
          </figure><p>What's most interesting is what content companies are getting the best deals. It's not the ragebait headline writers. It's not the news organizations writing yet another take on what's going on in politics. It's not the spammy content farms full of drivel. Instead, it's <a href="https://www.bloomberg.com/news/articles/2025-09-17/reddit-seeks-to-strike-next-ai-content-pact-with-google-openai"><u>Reddit</u></a> and other quirky corners that best remind us of the Internet of old. For those of you old enough, think back to the Internet not of the last 15 years but of the last 35. We’ve lost some of what made that early Internet great, but there are indications that we might finally have the incentives to bring more of it back.</p><p>It seems increasingly likely that in our future, AI-driven Internet —&nbsp;assuming the AI companies are willing to step up, support the ecosystem, and pay for the content that is the most valuable to them — it’s the creative, local, unique, original content that’ll be worth the most. And, if you’re like us, the thing you as an Internet consumer are craving more of is creative, local, unique, original content. And, it turns out, having talked with many of them, that’s the content that content creators are most excited to create.</p>
    <p>
      <h2 id="a-new-internet-business-model">A New Internet Business Model</h2>
      
    </p>
    <p>So how will the business model work? Well, for the first time in history, we have a pretty good mathematical representation of human knowledge. Sum up all the LLMs and that's what you get. It's not perfect, but it's pretty good. Inherently, the same mathematical model serves as a map for the gaps in human knowledge. Like a block of Swiss Cheese — there's a lot of cheese, but there's also a lot of holes.</p><p>Imagine a future business model of the Internet that doesn't reward traffic-generating ragebait but instead rewards those content creators that help fill in the holes in our collective metaphorical cheese. That will involve some portion of the subscription fees AI companies collect, and some portion of the revenue from the ads they'll inevitably serve, going back to content creators who most enrich the collective knowledge.</p><p>As a rough and simplistic sketch, think of it as some number of dollars per AI company’s monthly active users going into a collective pool to be distributed out to content creators based on what most fills in the holes in the cheese.</p><p>You could imagine an AI company suggesting back to creators that they need more created about topics they may not have enough content about. Say, for example, the carrying capacity of unladened swallows because they know their subscribers of a certain age and proclivity are always looking for answers about that topic. The very pruning algorithms the AI companies use today form a roadmap for what content is worth enough to not be pruned but paid for.</p><p>While today the budget items that differentiate AI companies are how much they can afford to spend on GPUs and top talent, as those things inevitably become more and more commodities it seems likely what will differentiate the different AIs is their access to creative, local, unique, original content. And the math of their algorithms provides them a map of what’s worth the most. While there are a lot of details to work out, those are the ingredients you need for a healthy market.</p>
    <p>
      <h2 id="cloudflares-role">Cloudflare’s Role</h2>
      
    </p>
    <p>As we think about our role at Cloudflare in this developing market, it's not about protecting the status quo but instead helping catalyze a better business model for the future of Internet content creation. That means creating a level playing field. Ideally there should be lots of AI companies, large and small, and lots of content creators, large and small.</p><p>It can’t be that a new entrant AI company is at a disadvantage to a legacy search engine because one has to pay for content but the other gets it for free. But it’s also critical to realize that the right solution to that current conundrum isn’t that no one pays, it’s that, new or old, everyone who benefits from the ecosystem should contribute back to it based on their relative size.</p><p>It may seem impossibly idealistic today, but the good news is that based on the conversations we’ve had we’re confident if a few market participants tip —&nbsp;whether because they step up and do the right thing or are compelled — we will see the entire market tipping and becoming robust very quickly.</p>
    <p>
      <h2 id="supporting-the-ecosystem">Supporting the Ecosystem</h2>
      
    </p>
    <p>We can't do this alone and we have no plans to try to. Our mission is not to “build a better Internet” but to “<b><i>help</i></b> build a better Internet.” The solutions developed to facilitate this market need to be open, collaborative, standardized, and shared across many organizations. We’ll take some encouraging steps in that direction with announcements on partnerships and collaborations this week. And we’re proud to be a leader in this space.</p><p>The Internet is an ecosystem and we, other infrastructure providers, along with most importantly both AI companies and content creators, will be critical in ensuring that ecosystem is healthy. We’re excited to partner with those who are ready to step up and do their part to also help build a better Internet. It is possible.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6EHC7vxXoMmle1QFHwGHh9/408b73f7b677701e7242e794efa3cb52/unnamed__29_.png" alt="" width="512" height="113" loading="lazy">
          </figure><p>And we're optimistic that if others can collaborate in supporting the ecosystem we may be at the cusp of a new golden age of the Internet. Our conversations with the leading AI companies nearly all acknowledge that they have a responsibility to give back to the ecosystem and compensate content creators. Confirming this, the largest publishers are reporting they're having much more constructive conversations about licensing their content to those AI companies. And, this week, we'll be announcing new tools to help even the smallest publishers take back control of who can use what they've created.</p><p>It may seem impossible. We think it’s a no-brainer. We're proud of what Cloudflare has accomplished over the last 15 years, but there’s a lot left to do to live up to our mission. So, more than ever, it's clear: giddy up, because we're just getting started!</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/15o6NDQsh19vfz6RC9nD5v/03f8f84dc09366ffc617829f35b2e255/BLOG-2994_5.png" alt="BLOG-2994 5" width="618" height="124" loading="lazy">
          </figure></div></section><div><p>Cloudflare's connectivity cloud protects <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, helps customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerates any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">wards off DDoS attacks</a>, keeps <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><astro-slot> <!--[if astro]>server-island-start<![endif]--> </astro-slot><a href="https://blog.cloudflare.com/tag/birthday-week/">Birthday Week</a><a href="https://blog.cloudflare.com/tag/product-news/">Product News</a><a href="https://blog.cloudflare.com/tag/founders-letter/">Founders' Letter</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PlanetScale for Postgres is now GA (229 pts)]]></title>
            <link>https://planetscale.com/blog/planetscale-for-postgres-is-generally-available</link>
            <guid>45334545</guid>
            <pubDate>Mon, 22 Sep 2025 15:10:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://planetscale.com/blog/planetscale-for-postgres-is-generally-available">https://planetscale.com/blog/planetscale-for-postgres-is-generally-available</a>, See on <a href="https://news.ycombinator.com/item?id=45334545">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>By <!-- -->Sam Lambert<!-- --> | <time datetime="2025-09-22">September 22, 2025</time></p><p>PlanetScale for Postgres is now generally available and out of private preview. To create a Postgres database, <a href="https://auth.planetscale.com/sign-up">sign up</a> or log in to your PlanetScale account, create a new database, and select Postgres. If you are looking to migrate from another Postgres provider to PlanetScale, you can use our <a href="https://planetscale.com/docs/postgres/imports/postgres-imports">migration guides</a> to get started. Finally, if you have a large or complex migration, we can help you via our sales team at <a href="mailto:postgres@planetscale.com">postgres@planetscale.com</a>.</p><h2 id="what-is-planetscale-for-postgres"><a href="#what-is-planetscale-for-postgres">What is PlanetScale for Postgres?</a></h2><p>Our mission is simple: bring you the fastest and most reliable databases with the best developer experience. We have done this for 5 years now with our managed Vitess product, allowing companies like Cursor, Intercom, and Block to scale beyond previous limits.</p><p>We are so excited to bring this to Postgres. Our proprietary operator allows us to bring the maturity of PlanetScale and the performance of Metal to an even wider audience. We bring you the best of Postgres and the best of PlanetScale in one product.</p><h2 id="customers-on-planetscale-for-postgres"><a href="#customers-on-planetscale-for-postgres">Customers on PlanetScale for Postgres</a></h2><p>Hundreds of companies already trust PlanetScale for Postgres to power their production workloads. We say this every time we launch something, but we prefer you hear about real-world usage straight from our customers. Read through some of their stories about their migration to PlanetScale for Postgres below.</p><ul><li><a href="https://news.convex.dev/powered-by-planetscale-for-postgres/">Convex: Powered by PlanetScale</a></li><li><a href="https://supermemory.ai/blog/supermemory-just-got-faster-on-planetscale/">Supermemory just got faster on PlanetScale</a></li><li><a href="https://medium.com/engineering-layers/scaling-real-time-discovery-inside-layers-planetscale-migration-443cb3a76815">Scaling Real‑Time Discovery: Inside Layers’ PlanetScale Migration</a></li><li><a href="https://blog.opensecret.cloud/why-we-migrated-from-neon-to-planetscale/">Why We Migrated from Neon to PlanetScale</a></li></ul><h2 id="vitess-for-postgres"><a href="#vitess-for-postgres">Vitess for Postgres</a></h2><p>Neki is our Postgres sharding solution. Built by the team behind Vitess combining the best of Vitess and Postgres. Neki is not a fork of Vitess. Vitess’ achievements are enabled by leveraging MySQL’s strengths and engineering around its weaknesses. To achieve Vitess’ power for Postgres we are architecting from first principles and building alongside design partners at scale. When we are ready we will release Neki as an open source project suitable for running the most demanding Postgres workloads. To sign up for the Neki waitlist visit <a href="https://www.neki.dev/">neki.dev</a>.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dear GitHub: no YAML anchors, please (161 pts)]]></title>
            <link>https://blog.yossarian.net/2025/09/22/dear-github-no-yaml-anchors</link>
            <guid>45334032</guid>
            <pubDate>Mon, 22 Sep 2025 14:34:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.yossarian.net/2025/09/22/dear-github-no-yaml-anchors">https://blog.yossarian.net/2025/09/22/dear-github-no-yaml-anchors</a>, See on <a href="https://news.ycombinator.com/item?id=45334032">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h2>ENOSUCHBLOG</h2>
<h2><em>Programming, philosophy, pedaling.</em></h2>

<ul>
    <li><a href="https://blog.yossarian.net/">Home</a></li>
    <li><a href="https://blog.yossarian.net/tags">Tags</a></li>
    <li><a href="https://blog.yossarian.net/series">Series</a></li>
    <li><a href="https://blog.yossarian.net/favorites">Favorites</a></li>
    <li><a href="https://blog.yossarian.net/archive">Archive</a></li>
    
    <li><a href="https://yossarian.net/">Main Site</a></li>
    <li><a href="https://yossarian.net/til">TILs</a></li>
    
</ul>

<hr>



<h2>
  <p>
    <span><em>Sep 22, 2025</em></span>

    &nbsp; &nbsp;

    
      <span>
        Tags:
        
        
          <a href="https://blog.yossarian.net/tags#programming">programming</a>,
        
          <a href="https://blog.yossarian.net/tags#rant">rant</a>
        
      </span>
    

    &nbsp; &nbsp;

    
  </p>
</h2>






<hr>


<p><strong>TL;DR</strong>: for a very long time, GitHub Actions lacked support for YAML anchors.</p>

<p>This was a <strong>good thing</strong>. YAML anchors in GitHub Actions are (1) redundant
with existing functionality, (2) introduce a complication to the data model
that makes CI/CD human and machine comprehension harder, and (3) are
<strong>not even uniquely useful</strong> because GitHub has chosen not to support
the one feature (merge keys) that lacks a semantic equivalent in GitHub Actions.</p>

<p>This step backwards <strong>reinforces</strong> GitHub Actions’ status as an
<strong>insecure by default</strong> CI/CD platform by making
it harder for both humans and machines to analyze action and workflow
definitions for vulnerabilities. GitHub should <strong>immediately remove</strong>
support for YAML anchors, before adoption becomes widespread.</p>

<hr>

<p>GitHub <a href="https://github.blog/changelog/2025-09-18-actions-yaml-anchors-and-non-public-workflow-templates/">recently announced</a> that YAML anchors are now supported in
GitHub Actions. That means that users can write things like this:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td><pre><span>jobs</span><span>:</span>
  <span>job1</span><span>:</span>
    <span>env</span><span>:</span> <span>&amp;env_vars</span> <span># Define the anchor on first use</span>
      <span>NODE_ENV</span><span>:</span> <span>production</span>
      <span>DATABASE_URL</span><span>:</span> <span>${{ secrets.DATABASE_URL }}</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>run</span><span>:</span> <span>echo "Using production settings"</span>

  <span>job2</span><span>:</span>
    <span>env</span><span>:</span> <span>*env_vars</span> <span># Reuse the environment variables</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>run</span><span>:</span> <span>echo "Same environment variables here"</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>On face value, this seems like a reasonable feature: the job and step
abstractions in GitHub Actions lend themselves to duplication, and YAML anchors
are <em>one</em> way to reduce that duplication.</p>

<p>Unfortunately, YAML anchors are a <strong>terrible</strong> tool for this job. Furthermore
(as we’ll see) GitHub’s implementation of YAML anchors is <strong>incomplete</strong>,
precluding the actual small subset of use cases where YAML anchors
are uniquely useful (but still not a good idea). We’ll see why below.</p>

<p><img src="https://blog.yossarian.net/assets/product-roadmap.jpeg" alt="Pictured: the author's understanding of the GitHub Actions product roadmap.">
<em>Pictured: the author’s understanding of the GitHub Actions product roadmap.</em></p>

<h2 id="redundancy">Redundancy</h2>

<p>The simplest reason why YAML anchors are a bad idea is because they’re
<strong>redundant</strong> with other <em>more explicit</em> mechanisms for reducing duplication
in GitHub Actions.</p>

<p>GitHub’s own example above could be rewritten without YAML anchors as:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td><pre><span>env</span><span>:</span>
  <span>NODE_ENV</span><span>:</span> <span>production</span>
  <span>DATABASE_URL</span><span>:</span> <span>${{ secrets.DATABASE_URL }}</span>

<span>jobs</span><span>:</span>
  <span>job1</span><span>:</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>run</span><span>:</span> <span>echo "Using production settings"</span>

  <span>job2</span><span>:</span>
    <span>steps</span><span>:</span>
      <span>-</span> <span>run</span><span>:</span> <span>echo "Same environment variables here"</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>This version is significantly clearer, but has slightly different semantics:
<em>all</em> jobs inherit the workflow-level <code>env</code>. But this, in my opinion,
is a <em>good thing</em>: the need to template environment variables across a <em>subset</em>
of jobs suggests an architectural error in the workflow design.</p>

<p>In other words: if you find yourself wanting to use YAML anchors to
share “global” configuration between jobs or steps, you
<strong>probably actually want</strong> separate workflows, or at least separate jobs with
job-level <code>env</code> blocks.</p>

<p>In summary: YAML anchors further muddy the abstractions of workflows,
jobs, and steps, by introducing a <strong>cross-cutting form of global state</strong> that
<strong>doesn’t play by the rules</strong> of the rest of the system. This, to me, suggests
that the current Actions team lacks a strong set of opinions about how
GitHub Actions <strong>should</strong> be used, leading to a “kitchen sink” approach
that serves <em>all</em> users equally poorly.</p>

<h2 id="non-locality-with-full-generality">Non-locality with full generality</h2>

<p>As noted above: YAML anchors introduce a <em>new</em> form of
<strong>non-locality</strong> into GitHub Actions. Furthermore, this form
of non-locality is <strong>fully general</strong>: <em>any</em> YAML node can be anchored
and referenced. This is a <strong>bad idea</strong> for humans and machines alike:</p>

<ul>
  <li>
    <p>For humans: a <em>new</em> form of non-locality makes it harder to preserve
<em>local understanding</em> of what a workflow, job, or step does: a unit
of work may now depend on <em>any</em> other unit of work in the same file,
including one hundreds or thousands of lines away. This makes it harder
to reason about the behavior of one’s GitHub Actions without context
switching.</p>

    <p>It would only be fair to note that GitHub Actions already has some
  forms of non-locality: global contexts, scoping rules for <code>env</code> blocks,
  <code>needs</code> dependencies, step and job outputs, and so on. These <em>can</em> be
  difficult to debug! But what sets them apart is their <em>lack of
  generality</em>: each has <em>precise</em> semantics and scoping rules,
  meaning that a user who understands those rules can comprehend
  what a unit of work does without referencing the source of an
  environment variable, output, &amp;c.</p>
  </li>
  <li>
    <p>For machines: non-locality makes it significantly harder to write
tools that analyze (or transform) GitHub Actions workflows.</p>

    <p>The pain here boils down to the fact that YAML anchors diverge
  from the <em>one-to-one object model</em><sup id="fnref:om"><a href="#fn:om" rel="footnote" role="doc-noteref">1</a></sup> that GitHub Actions otherwise maps
  onto.</p>

    <p>With anchors, that mapping becomes one-to-many: the same element
  may appear once in the source, but multiple times in the loaded
  object representation.</p>

    <p>In effect, this breaks a critical assumption that many tools
  make about YAML in GitHub Actions: that an entity in the deserialized
  object can be <em>mapped back</em> to a <strong>single concrete location</strong> in the
  <em>source</em> YAML.</p>

    <p>This is needed to present reasonable source locations in error messages,
  but it <strong>doesn’t hold</strong> if the object model doesn’t represent
  anchors and references explicitly.</p>

    <p>Furthermore, this is the reality
  for every YAML parser in wide use: <strong>all widespread YAML parsers</strong>
  choose (reasonably) to <em>copy</em> anchored values into each
  location where they’re referenced, meaning that the analyzing tool
  cannot “see” the original element for source location purposes.</p>

    <p>I feel these pains directly: I maintain <a href="https://docs.zizmor.sh/">zizmor</a> as a static analysis tool
  for GitHub Actions, and <code>zizmor</code> makes both of these assumptions.
  Moreover, <code>zizmor</code>’s <em>dependencies</em> make these assumptions:
  <code>serde_yaml</code> (like most other YAML parsers) chooses to deserialize YAML
  anchors by <em>copying</em> the anchored value into each location where it’s
  referenced<sup id="fnref:compat"><a href="#fn:compat" rel="footnote" role="doc-noteref">2</a></sup>.</p>
  </li>
</ul>

<h2 id="no-merging-anyways">No merging anyways</h2>

<p>One of the few things that make YAML anchors <em>uniquely</em> useful is
<a href="https://yaml.org/type/merge.html">merge keys</a>: a merge key allows a user to <em>compose</em> multiple referenced
mappings together into a single mapping.</p>

<p>An example from the YAML spec, which I think tidily demonstrates
both their use case <em>and</em> how incredibly confusing merge keys are:</p>

<div><pre><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td><pre><span>---</span>
<span>-</span> <span>&amp;CENTER</span> <span>{</span> <span>x</span><span>:</span> <span>1</span><span>,</span> <span>y</span><span>:</span> <span>2</span> <span>}</span>
<span>-</span> <span>&amp;LEFT</span> <span>{</span> <span>x</span><span>:</span> <span>0</span><span>,</span> <span>y</span><span>:</span> <span>2</span> <span>}</span>
<span>-</span> <span>&amp;BIG</span> <span>{</span> <span>r</span><span>:</span> <span>10</span> <span>}</span>
<span>-</span> <span>&amp;SMALL</span> <span>{</span> <span>r</span><span>:</span> <span>1</span> <span>}</span>

<span># All the following maps are equal:</span>

<span>-</span> <span># Explicit keys</span>
  <span>x</span><span>:</span> <span>1</span>
  <span>y</span><span>:</span> <span>2</span>
  <span>r</span><span>:</span> <span>10</span>
  <span>label</span><span>:</span> <span>center/big</span>

<span>-</span> <span># Merge one map</span>
  <span>&lt;&lt; </span><span>:</span> <span>*CENTER</span>
  <span>r</span><span>:</span> <span>10</span>
  <span>label</span><span>:</span> <span>center/big</span>

<span>-</span> <span># Merge multiple maps</span>
  <span>&lt;&lt; </span><span>:</span> <span>[</span> <span>*CENTER</span><span>,</span> <span>*BIG</span> <span>]</span>
  <span>label</span><span>:</span> <span>center/big</span>

<span>-</span> <span># Override</span>
  <span>&lt;&lt; </span><span>:</span> <span>[</span> <span>*BIG</span><span>,</span> <span>*LEFT</span><span>,</span> <span>*SMALL</span> <span>]</span>
  <span>x</span><span>:</span> <span>1</span>
  <span>label</span><span>:</span> <span>center/big</span>
</pre></td></tr></tbody></table></code></pre></div>

<p>I personally find this syntax incredibly hard to read, but at least it
has a <em>unique</em> use case that could be useful in GitHub Actions:
composing multiple sets of environment variables together with clear
precedence rules is <strong>manifestly</strong> useful.</p>

<p>Except: GitHub Actions <a href="https://github.com/actions/runner/issues/1182#issuecomment-2810242069"><strong>doesn’t support merge keys</strong></a>! They appear to be
using their own internal YAML parser that already had <em>some</em> degree of
support for anchors and references, but not for merge keys.</p>

<p>To me, this takes the situation from a set of bad technical decisions
(and lack of strong opinions around how GitHub Actions <em>should</em> be used)
to <strong>farce</strong>: the <strong>one thing</strong> that makes YAML anchors <em>uniquely</em> useful
in the context of GitHub Actions is the <strong>one thing</strong> that GitHub Actions
doesn’t support.</p>

<h2 id="summary">Summary</h2>

<p>To summarize, I think YAML anchors in GitHub Actions are (1) redundant
with existing functionality, (2) introduce a complication to the data model
that makes CI/CD human and machine comprehension harder, and (3) are
<strong>not even uniquely useful</strong> because GitHub has chosen not to support
the one feature (merge keys) that lacks a semantic equivalent in GitHub Actions.</p>

<p>Of these reasons, I think (2) is the most important: GitHub Actions security
has been <a href="https://www.wiz.io/blog/github-action-tj-actions-changed-files-supply-chain-attack-cve-2025-30066">in the news</a>&nbsp;<a href="https://www.wiz.io/blog/shai-hulud-npm-supply-chain-attack">a great deal recently</a>, with the overwhelming consensus
being that it’s <strong>too easy to introduce vulnerabilities</strong> <em>in</em> (or expose
otherwise latent vulnerabilities <em>through</em>) GitHub Actions workflow.</p>

<p>For this reason, we <strong>need</strong> GitHub Actions to be <strong>easy to analyze</strong>
for humans and machine alike. In effect, this means that GitHub <em>should</em>
be <strong>decreasing the complexity</strong> of GitHub Actions, not increasing it.
YAML anchors are a step in the wrong direction for all of the
reasons aforementioned.</p>

<p>Of course, I’m not without self-interest here: I maintain a
<a href="https://docs.zizmor.sh/">static analysis tool</a> for GitHub Actions, and supporting YAML anchors
is going to be an <em>absolute royal pain in my ass</em><sup id="fnref:pita"><a href="#fn:pita" rel="footnote" role="doc-noteref">3</a></sup>. But it’s not just
me: tools like <a href="https://github.com/rhysd/actionlint">actionlint</a>, <a href="https://github.com/Betterment/claws">claws</a>, and <a href="https://github.com/boostsecurityio/poutine">poutine</a> are all likely to
struggle with supporting YAML anchors, as they <strong>fundamentally</strong> alter
each tool’s relationship to GitHub Actions’ assumed data model. As-is,
this change <strong>blows a massive hole</strong> in the larger open source ecosystem’s
ability to analyze GitHub Actions for correctness and security.</p>

<p>All told: I strongly believe that GitHub should <strong>immediately remove</strong> support
for YAML anchors in GitHub Actions. The “good” news is that they can probably
do so with a bare minimum of user disruption, since support has only been
public for a few days and adoption is (probably) still primarily at
the <em>single-use</em> workflow layer and not the reusable action (or workflow) layer.</p>

<hr>



<hr>


<span>
  Discussions:
  
  <a href="https://www.reddit.com/r/enosuchblog/comments/1nno3if/dear_github_no_yaml_anchors_please/">Reddit</a>
  
  <a href="https://infosec.exchange/@yossarian/115248433982332273">Mastodon</a>
  
  <a href="https://bsky.app/profile/yossarian.net/post/3lzgms3uobt2y">Bluesky</a>
  
</span>
<hr>



  






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Local-First Apps Haven't Become Popular? (191 pts)]]></title>
            <link>https://marcobambini.substack.com/p/why-local-first-apps-havent-become</link>
            <guid>45333021</guid>
            <pubDate>Mon, 22 Sep 2025 13:17:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marcobambini.substack.com/p/why-local-first-apps-havent-become">https://marcobambini.substack.com/p/why-local-first-apps-havent-become</a>, See on <a href="https://news.ycombinator.com/item?id=45333021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Offline-first apps </span><strong>sound like the future</strong><span>: instant loading, privacy by default, and no more spinning loaders on flaky connections.</span></p><p><span>But in practice, very few apps get offline support right. Most simply queue changes locally and push them when the network comes back (spoiler: this doesn’t really work). Eventually, users see a scary banner saying </span><em>“changes may not be saved.”</em></p><p><span>The reason is simple: </span><strong>syncing is hard.</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!HTL7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!HTL7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic 424w, https://substackcdn.com/image/fetch/$s_!HTL7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic 848w, https://substackcdn.com/image/fetch/$s_!HTL7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic 1272w, https://substackcdn.com/image/fetch/$s_!HTL7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!HTL7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:168692,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/heic&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://marcobambini.substack.com/i/174227731?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!HTL7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic 424w, https://substackcdn.com/image/fetch/$s_!HTL7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic 848w, https://substackcdn.com/image/fetch/$s_!HTL7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic 1272w, https://substackcdn.com/image/fetch/$s_!HTL7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7219314f-ace0-446a-b502-ff026c465236_1920x1080.heic 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>When you build a local-first app, you’ve effectively created a </span><strong>distributed system</strong><span>. Multiple devices can mutate data independently, sometimes offline, and later must converge to the </span><strong>exact same state</strong><span> without losing data.</span></p><p>There are two main challenges to solve:</p><ol><li><p><strong>Unreliable ordering</strong></p></li><li><p><strong>Conflicts</strong></p></li></ol><p>Let’s go through them.</p><p>In a distributed environment, events happen on multiple devices at different times. If you just apply them as they arrive, you get inconsistent results.</p><p>Example:</p><ul><li><p><strong>Device A</strong><span> sets </span><code>x = 3</code></p></li><li><p><strong>Device B</strong><span> sets </span><code>x = 5</code></p></li></ul><p>Both happen while offline.</p><p><span>When the devices sync, the final value of </span><code>x</code><span> depends on which update is applied first. That’s a problem.</span></p><p><span>Traditional backend databases solve this with </span><strong>strong consistency</strong><span>, but that requires global coordination—too slow and brittle for local-first systems.</span></p><p><span>Instead, we want </span><strong>eventual consistency</strong><span>: every device applies changes independently but eventually converges to the same result once all changes are known.</span></p><p><span>The challenge is figuring out the </span><strong>correct order of events</strong><span> without relying on a centralized clock (because the network might be down).</span></p><p><span>Surprisingly, there’s a simple solution to this seemingly hard problem: </span><strong>Hybrid Logical Clocks (HLCs)</strong><span>.</span></p><p>HLCs generate timestamps that are:</p><ul><li><p><strong>Comparable</strong><span> (can be sorted lexicographically)</span></p></li><li><p><strong>Causally consistent</strong><span> (encode the order in which events happened)</span></p></li></ul><p>HLCs combine two pieces of information:</p><ul><li><p><strong>Physical time</strong><span> (from the machine clock)</span></p></li><li><p><strong>Logical time</strong><span> (a counter that increments when clocks are out of sync or events happen too close together)</span></p></li></ul><p>In plain terms, HLCs let devices agree on “what happened first” without perfectly synchronized clocks.</p><p>Imagine two machines, A and B:</p><ol><li><p><strong>Machine A</strong><span> records an event at </span><code>10:00:00.100</code><span>.</span><br><span>→ Its HLC becomes </span><code>(10:00:00.100, 0)</code><span> (time + counter).</span></p></li><li><p>A sends a message to B with this HLC.</p></li><li><p><strong>Machine B</strong><span>’s clock shows </span><code>10:00:00.095</code><span> (slightly behind).</span><br><span>When B receives the message, it </span><strong>advances its HLC</strong><span> to at least match A’s timestamp.</span></p></li><li><p><span>B’s HLC becomes </span><code>(10:00:00.100, 1)</code><span> — the counter increments to indicate this happened </span><em>after</em><span> A’s event.</span></p></li></ol><p>Result:</p><ul><li><p><span>Event on A: </span><code>(10:00:00.100, 0)</code></p></li><li><p><span>Event on B: </span><code>(10:00:00.100, 1)</code></p></li></ul><p>Even though B’s physical clock was behind, we can now order events consistently across machines.</p><p><span>Even with proper ordering, </span><strong>conflicts still happen</strong><span> when two devices modify the same data independently.</span></p><p>Example:</p><ul><li><p><span>Initial balance = </span><code>100</code></p></li><li><p><strong>Device A:</strong><span> </span><code>+20</code><span> → balance = </span><code>120</code></p></li><li><p><strong>Device B:</strong><span> </span><code>-20</code><span> → balance = </span><code>80</code></p></li></ul><p><span>When they sync, which value should “win”?</span><br><span>If you naively apply both updates, one overwrites the other—losing user data.</span></p><p>Most systems ask developers to write manual conflict resolution code, but that’s error-prone and hard to maintain.</p><p><span>The right approach is </span><strong>CRDTs</strong><span> (Conflict-Free Replicated Data Types).</span></p><p>CRDTs guarantee two important properties:</p><ul><li><p><strong>Commutativity:</strong><span> Order of application doesn’t matter</span></p></li><li><p><strong>Idempotence:</strong><span> Applying the same change twice has no effect</span></p></li></ul><p>This means you can apply messages in any order, even multiple times, and every device will still converge to the same state.</p><p><span>One of the simplest CRDT strategies is </span><strong>Last-Write-Wins (LWW)</strong><span>:</span></p><ul><li><p>Each update gets a timestamp (physical or logical).</p></li><li><p>When two devices write to the same field, the update with the latest timestamp wins.</p></li></ul><p>Example:</p><ul><li><p><strong>Device A:</strong><span> balance = </span><code>120</code><span> at </span><code>10:00:00</code></p></li><li><p><strong>Device B:</strong><span> balance = </span><code>80</code><span> at </span><code>10:00:02</code></p></li></ul><p><span>When syncing, the system keeps </span><code>80</code><span> because it was written last.</span></p><p><span>When building a local-first app, you need a rock-solid local database. </span><strong>SQLite</strong><span> is the obvious choice: battle-tested, lightweight, and available everywhere.</span></p><p><span>That’s why we built our local-first framework as a </span><strong>SQLite extension</strong><span>.</span></p><p>Our approach (simplified):</p><ul><li><p><span>Every change is stored as a message in a </span><code>messages</code><span> table with:</span></p><ul><li><p><code>timestamp</code><span> (from HLC)</span></p></li><li><p><code>dataset</code><span> (table name)</span></p></li><li><p><code>row</code><span> (encoded primary keys)</span></p></li><li><p><code>column</code></p></li><li><p><code>value</code></p></li></ul></li></ul><p>Applying a message is as simple as:</p><ol><li><p>Look up the current value</p></li><li><p>If the incoming timestamp is newer → overwrite</p></li><li><p>If it’s older → ignore</p></li></ol><p>This guarantees convergence across devices, regardless of the sync order.</p><p><span>This architecture makes syncing </span><strong>simple and reliable</strong><span>:</span></p><ul><li><p><strong>Reliable:</strong><span> Survives weeks of offline use without data loss</span></p></li><li><p><strong>Deterministic:</strong><span> Final state always converges</span></p></li><li><p><strong>Minimal:</strong><span> Just a small SQLite extension, no heavy dependencies</span></p></li><li><p><strong>Cross-platform:</strong><span> The extension is available for iOS, Android, macOS, Windows, Linux, and WASM</span></p></li></ul><ul><li><p>Stop faking offline support with request queues</p></li><li><p><span>Embrace </span><strong>eventual consistency</strong></p></li><li><p><span>Use proven distributed-systems techniques like </span><strong>HLCs</strong><span> and </span><strong>CRDTs</strong></p></li><li><p>Keep it small and dependency-free</p></li></ul><p><span>The result? Apps that are </span><strong>instant</strong><span>, </span><strong>offline-capable</strong><span>, and </span><strong>private by default</strong><span> — without the complexity of traditional client–server synchronization.</span></p><p><span>If you need a production-ready, cross-platform, offline-first engine for SQLite, check out our open-source </span><strong><a href="https://github.com/sqliteai/sqlite-sync" rel="nofollow ugc noopener">SQLite-Sync </a></strong><span>extension.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cap'n Web: a new RPC system for browsers and web servers (266 pts)]]></title>
            <link>https://blog.cloudflare.com/capnweb-javascript-rpc-library/</link>
            <guid>45332883</guid>
            <pubDate>Mon, 22 Sep 2025 13:05:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/capnweb-javascript-rpc-library/">https://blog.cloudflare.com/capnweb-javascript-rpc-library/</a>, See on <a href="https://news.ycombinator.com/item?id=45332883">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><p>2025-09-22</p><section><p>12 min read</p><img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/5i59EQLnkQYe3oZrCzCk1y/43c20e671782e6ec9f878f786535bd15/BLOG-2954_1.png" alt=""><div><p>Allow us to introduce <a href="https://github.com/cloudflare/capnweb"><u>Cap'n Web</u></a>, an RPC protocol and implementation in pure TypeScript.</p><p>Cap'n Web is a spiritual sibling to <a href="https://capnproto.org/"><u>Cap'n Proto</u></a>, an RPC protocol I (Kenton) created a decade ago, but designed to play nice in the web stack. That means:</p><ul><li><p>Like Cap'n Proto, it is an object-capability protocol. ("Cap'n" is short for "capabilities and".) We'll get into this more below, but it's incredibly powerful.</p></li><li><p>Unlike Cap'n Proto, Cap'n Web has <i>no schemas</i>. In fact, it has almost no boilerplate whatsoever. This means it works more like the <a href="https://blog.cloudflare.com/javascript-native-rpc/"><u>JavaScript-native RPC system in Cloudflare Workers</u></a>.</p></li><li><p>That said, it integrates nicely with TypeScript.</p></li><li><p>Also unlike Cap'n Proto, Cap'n Web's underlying serialization is human-readable. In fact, it's just JSON, with a little pre-/post-processing.</p></li><li><p>It works over HTTP, WebSocket, and postMessage() out-of-the-box, with the ability to extend it to other transports easily.</p></li><li><p>It works in all major browsers, Cloudflare Workers, Node.js, and other modern JavaScript runtimes.</p></li><li><p>The whole thing compresses (minify+gzip) to under 10&nbsp;kB with no dependencies.</p></li><li><p><a href="https://github.com/cloudflare/capnweb"><u>It's open source</u></a> under the MIT license.</p></li></ul><p>Cap'n Web is more expressive than almost every other RPC system, because it implements an <b>object-capability RPC model</b>. That means it:</p><ul><li><p>Supports bidirectional calling. The client can call the server, and the server can also call the client.</p></li><li><p>Supports passing functions by reference: If you pass a function over RPC, the recipient receives a "stub". When they call the stub, they actually make an RPC back to you, invoking the function where it was created. This is how bidirectional calling happens: the client passes a callback to the server, and then the server can call it later.</p></li><li><p>Similarly, supports passing objects by reference: If a class extends the special marker type <code>RpcTarget</code>, then instances of that class are passed by reference, with method calls calling back to the location where the object was created.</p></li><li><p>Supports promise pipelining. When you start an RPC, you get back a promise. Instead of awaiting it, you can immediately use the promise in dependent RPCs, thus performing a chain of calls in a single network round trip.</p></li><li><p>Supports capability-based security patterns.</p></li></ul><p>In short, Cap'n Web lets you design RPC interfaces the way you'd design regular JavaScript APIs – while still acknowledging and compensating for network latency.</p><p>The best part is, Cap'n Web is absolutely trivial to set up.</p><p>A client looks like this:</p>
            <pre><code>import { newWebSocketRpcSession } from "capnweb";

// One-line setup.
let api = newWebSocketRpcSession("wss://example.com/api");

// Call a method on the server!
let result = await api.hello("World");

console.log(result);
</code></pre>
            <p>And here's a complete Cloudflare Worker implementing an RPC server:</p>
            <pre><code>import { RpcTarget, newWorkersRpcResponse } from "capnweb";

// This is the server implementation.
class MyApiServer extends RpcTarget {
  hello(name) {
    return `Hello, ${name}!`
  }
}

// Standard Workers HTTP handler.
export default {
  fetch(request, env, ctx) {
    // Parse URL for routing.
    let url = new URL(request.url);

    // Serve API at `/api`.
    if (url.pathname === "/api") {
      return newWorkersRpcResponse(request, new MyApiServer());
    }

    // You could serve other endpoints here...
    return new Response("Not found", {status: 404});
  }
}
</code></pre>
            <p>That's it. That's the app.</p><ul><li><p>You can add more methods to <code>MyApiServer</code>, and call them from the client.</p></li><li><p>You can have the client pass a callback function to the server, and then the server can just call it.</p></li><li><p>You can define a TypeScript interface for your API, and easily apply it to the client and server.</p></li></ul><p>It just works.</p>
    <p>
      <h3 id="why-rpc-and-what-is-rpc-anyway">Why RPC? (And what is RPC anyway?)</h3>
      
    </p>
    <p>Remote Procedure Calls (RPC) are a way of expressing communications between two programs over a network. Without RPC, you might communicate using a protocol like HTTP. With HTTP, though, you must format and parse your communications as an HTTP request and response, perhaps designed in <a href="https://en.wikipedia.org/wiki/REST"><u>REST</u></a> style. RPC systems try to make communications look like a regular function call instead, as if you were calling a library rather than a remote service. The RPC system provides a "stub" object on the client side which stands in for the real server-side object. When a method is called on the stub, the RPC system figures out how to serialize and transmit the parameters to the server, invoke the method on the server, and then transmit the return value back.</p><p>The merits of RPC have been subject to a great deal of debate. RPC is often accused of committing many of the <a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing"><u>fallacies of distributed computing</u></a>.</p><p>But this reputation is outdated. When RPC was first invented some 40 years ago, async programming barely existed. We did not have Promises, much less async and await. Early RPC was synchronous: calls would block the calling thread waiting for a reply. At best, latency made the program slow. At worst, network failures would hang or crash the program. No wonder it was deemed "broken".</p><p>Things are different today. We have Promise and async and await, and we can throw exceptions on network failures. We even understand how RPCs can be pipelined so that a chain of calls takes only one network round trip. Many large distributed systems you likely use every day are built on RPC. It works.</p><p>The fact is, RPC fits the programming model we're used to. Every programmer is trained to think in terms of APIs composed of function calls, not in terms of byte stream protocols nor even REST. Using RPC frees you from the need to constantly translate between mental models, allowing you to move faster.</p>
    <p>
      <h3 id="when-should-you-use-capn-web">When should you use Cap'n Web?</h3>
      
    </p>
    <p>Cap'n Web is useful anywhere where you have two JavaScript applications speaking to each other over a network, including client-to-server and microservice-to-microservice scenarios. However, it is particularly well-suited to interactive web applications with real-time collaborative features, as well as modeling interactions over complex security boundaries.</p><p>Cap'n Web is still new and experimental, so for now, a willingness to live on the cutting edge may also be required!</p>
    <p>
      <h2 id="features-features-features">Features, features, features…</h2>
      
    </p>
    <p>Here's some more things you can do with Cap'n Web.</p>
    <p>
      <h3 id="http-batch-mode">HTTP batch mode</h3>
      
    </p>
    <p>Sometimes a WebSocket connection is a bit too heavyweight. What if you just want to make a quick one-time batch of calls, but don't need an ongoing connection?</p><p>For that, Cap'n Web supports HTTP batch mode:</p>
            <pre><code>import { newHttpBatchRpcSession } from "capnweb";

let batch = newHttpBatchRpcSession("https://example.com/api");

let result = await batch.hello("World");

console.log(result);
</code></pre>
            <p><i>(The server is exactly the same as before.)</i></p><p>Note that once you've awaited an RPC in the batch, the batch is done, and all the remote references received through it become broken. To make more calls, you need to start over with a new batch. However, you can make multiple calls in a single batch:</p>
            <pre><code>let batch = newHttpBatchRpcSession("https://example.com/api");

// We can call make multiple calls, as long as we await them all at once.
let promise1 = batch.hello("Alice");
let promise2 = batch.hello("Bob");

let [result1, result2] = await Promise.all([promise1, promise2]);

console.log(result1);
console.log(result2);
</code></pre>
            <p>And that brings us to another feature…</p>
    <p>
      <h3 id="chained-calls-promise-pipelining">Chained calls (Promise Pipelining)</h3>
      
    </p>
    <p>Here's where things get magical.</p><p>In both batch mode and WebSocket mode, you can make a call that depends on the result of another call, without waiting for the first call to finish. In batch mode, that means you can, in a single batch, call a method, then use its result in another call. The entire batch still requires only one network round trip.</p><p>For example, say your API is:</p>
            <pre><code>class MyApiServer extends RpcTarget {
  getMyName() {
    return "Alice";
  }

  hello(name) {
    return `Hello, ${name}!`
  }
}
</code></pre>
            <p>You can do:</p>
            <pre><code>let namePromise = batch.getMyName();
let result = await batch.hello(namePromise);

console.log(result);
</code></pre>
            <p>Notice the initial call to <code>getMyName()</code> returned a promise, but we used the promise itself as the input to <code>hello()</code>, without awaiting it first. With Cap'n Web, this just works: The client sends a message to the server saying: "Please insert the result of the first call into the parameters of the second."</p><p>Or perhaps the first call returns an object with methods. You can call the methods immediately, without awaiting the first promise, like:</p>
            <pre><code>let batch = newHttpBatchRpcSession("https://example.com/api");

// Authencitate the API key, returning a Session object.
let sessionPromise = batch.authenticate(apiKey);

// Get the user's name.
let name = await sessionPromise.whoami();

console.log(name);
</code></pre>
            <p>This works because the promise returned by a Cap'n Web call is not a regular promise. Instead, it's a JavaScript Proxy object. Any methods you call on it are interpreted as speculative method calls on the eventual result. These calls are sent to the server immediately, telling the server: "When you finish the call I sent earlier, call this method on what it returns."</p>
    <p>
      <h3 id="did-you-spot-the-security">Did you spot the security?</h3>
      
    </p>
    <p>This last example shows an important security pattern enabled by Cap'n Web's object-capability model.</p><p>When we call the authenticate() method, after it has verified the provided API key, it returns an authenticated session object. The client can then make further RPCs on the session object to perform operations that require authorization as that user. The server code might look like this:</p>
            <pre><code>class MyApiServer extends RpcTarget {
  authenticate(apiKey) {
    let username = await checkApiKey(apiKey);
    return new AuthenticatedSession(username);
  }
}

class AuthenticatedSession extends RpcTarget {
  constructor(username) {
    super();
    this.username = username;
  }

  whoami() {
    return this.username;
  }

  // ...other methods requiring auth...
}
</code></pre>
            <p>Here's what makes this work: <b>It is impossible for the client to "forge" a session object. The only way to get one is to call authenticate(), and have it return successfully.</b></p><p>In most RPC systems, it is not possible for one RPC to return a stub pointing at a new RPC object in this way. Instead, all functions are top-level, and can be called by anyone. In such a traditional RPC system, it would be necessary to pass the API key again to every function call, and check it again on the server each time. Or, you'd need to do authorization outside the RPC system entirely.</p><p>This is a common pain point for WebSockets in particular. Due to the design of the web APIs for WebSocket, you generally cannot use headers nor cookies to authorize them. Instead, authorization must happen in-band, by sending a message over the WebSocket itself. But this can be annoying for RPC protocols, as it means the authentication message is "special" and changes the state of the connection itself, affecting later calls. This breaks the abstraction.</p><p>The authenticate() pattern shown above neatly makes authentication fit naturally into the RPC abstraction. It's even type-safe: you can't possibly forget to authenticate before calling a method requiring auth, because you wouldn't have an object on which to make the call. Speaking of type-safety…</p>
    <p>
      <h3 id="typescript">TypeScript</h3>
      
    </p>
    <p>If you use TypeScript, Cap'n Web plays nicely with it. You can declare your RPC API once as a TypeScript interface, implement in on the server, and call it on the client:</p>
            <pre><code>// Shared interface declaration:
interface MyApi {
  hello(name: string): Promise&lt;string&gt;;
}

// On the client:
let api: RpcStub&lt;MyApi&gt; = newWebSocketRpcSession("wss://example.com/api");

// On the server:
class MyApiServer extends RpcTarget implements MyApi {
  hello(name) {
    return `Hello, ${name}!`
  }
}
</code></pre>
            <p>Now you get end-to-end type checking, auto-completed method names, and so on.</p><p>Note that, as always with TypeScript, no type checks occur at runtime. The RPC system itself does not prevent a malicious client from calling an RPC with parameters of the wrong type. This is, of course, not a problem unique to Cap'n Web – JSON-based APIs have always had this problem. You may wish to use a runtime type-checking system like Zod to solve this. (Meanwhile, we hope to add type checking based directly on TypeScript types in the future.)</p>
    <p>
      <h2 id="an-alternative-to-graphql">An alternative to GraphQL?</h2>
      
    </p>
    <p>If you’ve used GraphQL before, you might notice some similarities. One benefit of GraphQL was to solve the “waterfall” problem of traditional REST APIs by allowing clients to ask for multiple pieces of data in one query. For example, instead of making three sequential HTTP calls:</p>
            <pre><code>GET /user
GET /user/friends
GET /user/friends/photos</code></pre>
            <p>…you can write one GraphQL query to fetch it all at once.</p><p>That’s a big improvement over REST, but GraphQL comes with its own tradeoffs:</p><ul><li><p><b>New language and tooling.</b> You have to adopt GraphQL’s schema language, servers, and client libraries. If your team is all-in on JavaScript, that’s a lot of extra machinery.</p></li><li><p><b>Limited composability.</b> GraphQL queries are declarative, which makes them great for fetching data, but awkward for chaining operations or mutations. For example, you can’t easily say: “create a user, then immediately use that new user object to make a friend request, all-in-one round trip.”</p></li><li><p><b>Different abstraction model.</b> GraphQL doesn’t look or feel like the JavaScript APIs you already know. You’re learning a new mental model rather than extending the one you use every day.</p></li></ul>
    <p>
      <h3 id="how-capn-web-goes-further">How Cap'n Web goes further</h3>
      
    </p>
    <p>Cap'n Web solves the waterfall problem <i>without</i> introducing a new language or ecosystem. It’s just JavaScript. Because Cap'n Web supports promise pipelining and object references, you can write code that looks like this:</p>
            <pre><code>let user = api.createUser({ name: "Alice" });
let friendRequest = await user.sendFriendRequest("Bob");</code></pre>
            <p>What happens under the hood? Both calls are pipelined into a single network round trip:</p><ol><li><p>Create the user.</p></li><li><p>Take the result of that call (a new User object).</p></li><li><p>Immediately invoke sendFriendRequest() on that object.</p></li></ol><p>All of this is expressed naturally in JavaScript, with no schemas, query languages, or special tooling required. You just call methods and pass objects around, like you would in any other JavaScript code.</p><p>In other words, GraphQL gave us a way to flatten REST’s waterfalls. Cap'n Web lets us go even further: it gives you the power to model complex interactions exactly the way you would in a normal program, with no impedance mismatch.</p>
    <p>
      <h3 id="but-how-do-we-solve-arrays">But how do we solve arrays?</h3>
      
    </p>
    <p>With everything we've presented so far, there's a critical missing piece to seriously consider Cap'n Web as an alternative to GraphQL: handling lists. Often, GraphQL is used to say: "Perform this query, and then, for every result, perform this other query." For example: "List the user's friends, and then for each one, fetch their profile photo."</p><p>In short, we need an <code>array.map()</code> operation that can be performed without adding a round trip.</p><p>Cap'n Proto, historically, has never supported such a thing.</p><p>But with Cap'n Web, we've solved it. You can do:</p>
            <pre><code>let user = api.authenticate(token);

// Get the user's list of friends (an array).
let friendsPromise = user.listFriends();

// Do a .map() to annotate each friend record with their photo.
// This operates on the *promise* for the friends list, so does not
// add a round trip.
// (wait WHAT!?!?)
let friendsWithPhotos = friendsPromise.map(friend =&gt; {
  return {friend, photo: api.getUserPhoto(friend.id))};
}

// Await the friends list with attached photos -- one round trip!
let results = await friendsWithPhotos;
</code></pre>
            
    <p>
      <h3 id="wait-how">Wait… How!?</h3>
      
    </p>
    <p><code>.map()</code> takes a callback function, which needs to be applied to each element in the array. As we described earlier, <i>normally</i> when you pass a function to an RPC, the function is passed "by reference", meaning that the remote side receives a stub, where calling that stub makes an RPC back to the client where the function was created.</p><p>But that is NOT what is happening here. That would defeat the purpose: we don't want the server to have to round-trip to the client to process every member of the array. We want the server to just apply the transformation server-side.</p><p>To that end, <code>.map() </code>is special. It does not send JavaScript code to the server, but it does send something like "code", restricted to a domain-specific, non-Turing-complete language. The "code" is a list of instructions that the server should carry out for each member of the array. In this case, the instructions are:</p><ol><li><p>Invoke <code>api.getUserPhoto(friend.id)</code>.</p></li><li><p>Return an object <code>{friend, photo}</code>, where friend is the original array element and photo is the result of step 1.</p></li></ol><p>But the application code just specified a JavaScript method. How on Earth could we convert this into the narrow DSL?</p><p>The answer is record-replay: On the client side, we execute the callback once, passing in a special placeholder value. The parameter behaves like an RPC promise. However, the callback is required to be synchronous, so it cannot actually await this promise. The only thing it can do is use promise pipelining to make pipelined calls. These calls are intercepted by the implementation and recorded as instructions, which can then be sent to the server, where they can be replayed as needed.</p><p>And because the recording is based on promise pipelining, which is what the RPC protocol itself is designed to represent, it turns out that the "DSL" used to represent "instructions" for the map function is <i>just the RPC protocol itself</i>. 🤯</p>
    <p>
      <h2 id="implementation-details">Implementation details</h2>
      
    </p>
    
    <p>
      <h3 id="json-based-serialization">JSON-based serialization</h3>
      
    </p>
    <p>Cap'n Web's underlying protocol is based on JSON – but with a preprocessing step to handle special types. Arrays are treated as "escape sequences" that let us encode other values. For example, JSON does not have an encoding for <code>Date</code> objects, but Cap'n Web does. You might see a message that looks like this:</p>
            <pre><code>{
  event: "Birthday Week",
  timestamp: ["date", 1758499200000]
}
</code></pre>
            <p>To encode a literal array, we simply double-wrap it in <code>[]</code>:</p>
            <pre><code>{
  names: [["Alice", "Bob", "Carol"]]
}
</code></pre>
            <p>In other words, an array with just one element which is itself an array, evaluates to the inner array literally. An array whose first element is a type name, evaluates to an instance of that type, where the remaining elements are parameters to the type.</p><p>Note that only a fixed set of types are supported: essentially, <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm"><u>"structured clonable" types</u></a>, and RPC stub types.</p><p>On top of this basic encoding, we define an RPC protocol inspired by Cap'n Proto – but greatly simplified.</p>
    <p>
      <h3 id="rpc-protocol">RPC protocol</h3>
      
    </p>
    <p>Since Cap'n Web is a symmetric protocol, there is no well-defined "client" or "server" at the protocol level. There are just two parties exchanging messages across a connection. Every kind of interaction can happen in either direction.</p><p>In order to make it easier to describe these interactions, I will refer to the two parties as "Alice" and "Bob".</p><p>Alice and Bob start the connection by establishing some sort of bidirectional message stream. This may be a WebSocket, but Cap'n Web also allows applications to define their own transports. Each message in the stream is JSON-encoded, as described earlier.</p><p>Alice and Bob each maintain some state about the connection. In particular, each maintains an "export table", describing all the pass-by-reference objects they have exposed to the other side, and an "import table", describing the references they have received. Alice's exports correspond to Bob's imports, and vice versa. Each entry in the export table has a signed integer ID, which is used to reference it. You can think of these IDs like file descriptors in a POSIX system. Unlike file descriptors, though, IDs can be negative, and an ID is never reused over the lifetime of a connection.</p><p>At the start of the connection, Alice and Bob each populate their export tables with a single entry, numbered zero, representing their "main" interfaces. Typically, when one side is acting as the "server", they will export their main public RPC interface as ID zero, whereas the "client" will export an empty interface. However, this is up to the application: either side can export whatever they want.</p><p>From there, new exports are added in two ways:</p><ul><li><p>When Alice sends a message to Bob that contains within it an object or function reference, Alice adds the target object to her export table. IDs assigned in this case are always negative, starting from -1 and counting downwards.</p></li><li><p>Alice can send a "push" message to Bob to request that Bob add a value to his export table. The "push" message contains an expression which Bob evaluates, exporting the result. Usually, the expression describes a method call on one of Bob's existing exports – this is how an RPC is made. Each "push" is assigned a positive ID on the export table, starting from 1 and counting upwards. Since positive IDs are only assigned as a result of pushes, Alice can predict the ID of each push she makes, and can immediately use that ID in subsequent messages. This is how promise pipelining is achieved.</p></li></ul><p>After sending a push message, Alice can subsequently send a "pull" message, which tells Bob that once he is done evaluating the "push", he should proactively serialize the result and send it back to Alice, as a "resolve" (or "reject") message. However, this is optional: Alice may not actually care to receive the return value of an RPC, if Alice only wants to use it in promise pipelining. In fact, the Cap'n Web implementation will only send a "pull" message if the application has actually awaited the returned promise.</p><p>Putting it together, a code sequence like this:</p>
            <pre><code>let namePromise = api.getMyName();
let result = await api.hello(namePromise);

console.log(result);</code></pre>
            <p>Might produce a message exchange like this:</p>
            <pre><code>// Call api.getByName(). `api` is the server's main export, so has export ID 0.
-&gt; ["push", ["pipeline", 0, "getMyName", []]
// Call api.hello(namePromise). `namePromise` refers to the result of the first push,
// so has ID 1.
-&gt; ["push", ["pipeline", 0, "hello", [["pipeline", 1]]]]
// Ask that the result of the second push be proactively serialized and returned.
-&gt; ["pull", 2]
// Server responds.
&lt;- ["resolve", 2, "Hello, Alice!"]
</code></pre>
            <p>For more details about the protocol, <a href="https://github.com/cloudflare/capnweb/blob/main/protocol.md"><u>check out the docs</u></a>.</p>
    <p>
      <h2 id="try-it-out">Try it out!</h2>
      
    </p>
    <p>Cap'n Web is new and still highly experimental. There may be bugs to shake out. But, we're already using it today. Cap'n Web is the basis of <a href="https://developers.cloudflare.com/changelog/2025-09-16-remote-bindings-ga/"><u>the recently-launched "remote bindings" feature in Wrangler</u></a>, allowing a local test instance of workerd to speak RPC to services in production. We've also begun to experiment with it in various frontend applications – expect more blog posts on this in the future.</p><p>In any case, Cap'n Web is open source, and you can start using it in your own projects now.</p><p><a href="https://github.com/cloudflare/capnweb"><u>Check it out on GitHub.</u></a></p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/53YF87AtEsYhHMN3PV23UV/8e9a938099c71e6f274e95292b16b382/BLOG-2954_2.png" alt="BLOG-2954 2" width="1200" height="446" loading="lazy">
          </figure></div></section><div><p>Cloudflare's connectivity cloud protects <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, helps customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerates any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">wards off DDoS attacks</a>, keeps <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><astro-slot> <!--[if astro]>server-island-start<![endif]--> </astro-slot><a href="https://blog.cloudflare.com/tag/birthday-week/">Birthday Week</a><a href="https://blog.cloudflare.com/tag/open-source/">Open Source</a><a href="https://blog.cloudflare.com/tag/javascript/">JavaScript</a><a href="https://blog.cloudflare.com/tag/workers/">Cloudflare Workers</a><a href="https://blog.cloudflare.com/tag/developer-platform/">Developer Platform</a><a href="https://blog.cloudflare.com/tag/developers/">Developers</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare is sponsoring Ladybird and Omarchy (543 pts)]]></title>
            <link>https://blog.cloudflare.com/supporting-the-future-of-the-open-web/</link>
            <guid>45332860</guid>
            <pubDate>Mon, 22 Sep 2025 13:03:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/supporting-the-future-of-the-open-web/">https://blog.cloudflare.com/supporting-the-future-of-the-open-web/</a>, See on <a href="https://news.ycombinator.com/item?id=45332860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post"><article><h2>Supporting the future of the open web: Cloudflare is sponsoring Ladybird and Omarchy </h2><p>2025-09-22</p><section><p>4 min read</p><img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/6pKzeb9V8OT2HhuCjI982B/cce0fa1a0926d95c4d123f3bfa8c550d/BLOG-2998_1.png" alt=""><div><p>At Cloudflare, we believe that helping build a better Internet means encouraging a healthy ecosystem of options for how people can connect safely and quickly to the resources they need. Sometimes that means we tackle immense, Internet-scale problems with established partners. And sometimes that means we support and partner with fantastic open teams taking big bets on the next generation of tools.</p><p>To that end, today we are excited to announce our support of two independent, open source projects: <a href="https://ladybird.org/"><u>Ladybird</u></a>, an ambitious project to build a completely independent browser from the ground up, and <a href="https://omarchy.org/"><u>Omarchy</u></a>, an opinionated Arch Linux setup for developers.&nbsp;</p>
    <p>
      <h2 id="two-open-source-projects-strengthening-the-open-internet">Two open source projects strengthening the open Internet&nbsp;</h2>
      
    </p>
    <p>Cloudflare has a long history of supporting open-source software – both through <a href="https://blog.cloudflare.com/tag/open-source/"><u>our own projects shared with the community</u></a> and <a href="https://developers.cloudflare.com/sponsorships/"><u>external</u></a> projects that we support. We see our sponsorship of Ladybird and Omarchy as a natural extension of these efforts in a moment where energy for a diverse ecosystem is needed more than ever.&nbsp;&nbsp;</p>
    <p>
      <h3 id="ladybird-a-new-and-independent-browser">Ladybird, a new and independent browser&nbsp;</h3>
      
    </p>
    <p>Most of us spend a significant amount of time using a web browser –&nbsp; in fact, you’re probably using one to read this blog! The beauty of browsers is that they help users experience the open Internet, giving you access to everything from the largest news publications in the world to a tiny website hosted on a Raspberry Pi.&nbsp;&nbsp;</p><p>Unlike dedicated apps, browsers reduce the barriers to building an audience for new services and communities on the Internet. If you are launching something new, you can offer it through a browser in a world where most people have absolutely zero desire to install an app just to try something out. Browsers help encourage competition and new ideas on the open web.</p><p>While the openness of how browsers work has led to an explosive growth of services on the Internet, browsers themselves have consolidated to a tiny handful of viable options. There’s a high probability you’re reading this on a Chromium-based browser, like Google’s Chrome, along with about <a href="https://radar.cloudflare.com/reports/browser-market-share-2025-q2"><u>65% of users on the Internet.</u></a> However, that consolidation has also scared off new entrants in the space. If all browsers ship on the same operating systems, powered by the same underlying technology, we lose out on potential privacy, security and performance innovations that could benefit developers and everyday Internet users.&nbsp;</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/3j6xYLX9ZdqhS0yWCMjM0b/45fa8bd5b275a45a9f37b7a015d4c15d/BLOG-2998_2.png" alt="BLOG-2998 2" width="1999" height="1240" loading="lazy">
          </figure><p><sup><i>A screenshot of Cloudflare Workers developer docs in Ladybird&nbsp;</i></sup></p><p>This is where Ladybird comes in: it’s not Chromium based – everything is built from scratch. The Ladybird project has two main components: LibWeb, a brand-new rendering engine, and LibJS, a brand-new JavaScript engine with its own parser, interpreter, and bytecode execution engine.&nbsp;</p><p>Building an engine that can correctly and securely render the modern web is a monumental task that requires deep technical expertise and navigating decades of specifications governed by standards bodies like the W3C and WHATWG. And because Ladybird implements these standards directly, it also stress-tests them in practice. Along the way, the project has found, reported, and sometimes fixed countless issues in the specifications themselves, contributions that strengthen the entire web platform for developers, browser vendors, and anyone who may attempt to build a browser in the future.</p><p>Whether to build something from scratch or not is a perennial source of debate between software engineers, but absent the pressures of revenue or special interests, we’re excited about the ways Ladybird will prioritize privacy, performance, and security, potentially in novel ways that will influence the entire ecosystem.</p>
          <figure>
          <img src="https://cf-assets.www.cloudflare.com/zkvhlag99gkb/7zzAGb1Te5G6wGH2ieFbMU/1a3289c199695f88f6f6e57d7289851e/image1.png" alt="A screenshot of the Omarchy development environment" width="1999" height="1125" loading="lazy">
          </figure><p><sup><i>A screenshot of the Omarchy development environment</i></sup></p>
    <p>
      <h3 id="omarchy-an-independent-development-environment">Omarchy, an independent development environment&nbsp;</h3>
      
    </p>
    <p>Developers deserve choice, too. Beyond the browser, a developer’s operating system and environment is where they spend a ton of time – and where a few big players have become the dominant choice. Omarchy challenges this by providing a complete, opinionated Arch Linux distribution that transforms a bare installation into a modern development workstation that developers are <a href="https://github.com/basecamp/omarchy"><u>excited about</u></a>.</p><p>Perfecting one’s development environment can be a career-long art, but learning how to do so shouldn’t be a barrier to beginning to code. The beauty of Omarchy is that it makes Linux approachable to more developers by doing most of the setup for them, making it look good, and then making it configurable. Omarchy provides most of the tools developers need – like Neovim, Docker, and Git – out of the box, and <a href="https://learn.omacom.io/2/the-omarchy-manual"><u>tons of other features</u></a>.</p><p>At its core, Omarchy embraces Linux for all of its complexity and configurability, and makes a version of it that is accessible and fun to use for developers that don’t have a deep background in operating systems. Projects like this ensure that a powerful, independent Linux desktop remains a compelling choice for people building the next generation of applications and Internet infrastructure.&nbsp;</p>
    <p>
      <h3 id="our-support-comes-with-no-strings-attached">Our support comes with no strings attached&nbsp;&nbsp;</h3>
      
    </p>
    <p>We want to be very clear here: we are supporting these projects because we believe the Internet can be better if these projects, and more like them, succeed. No requirement to use our technology stack or any arrangement like that. We are happy to partner with great teams like Ladybird and Omarchy simply because we believe that our missions have real overlap.</p>
    <p>
      <h2 id="notes-from-the-teams">Notes from the teams</h2>
      
    </p>
    <p>Ladybird is still in its early days, with an alpha release planned for 2026, but we encourage anyone who is interested to consider contributing to the <a href="https://github.com/LadybirdBrowser/ladybird/tree/master"><u>open source codebase</u></a> as they prepare for launch.</p><blockquote><p><i>"Cloudflare knows what it means to build critical web infrastructure on the server side. With Ladybird, we’re tackling the near-monoculture on the client side, because we believe it needs multiple implementations to stay healthy, and we’re extremely thankful for their support in that mission.”</i></p><p>– <b>Andreas Kling</b>, Founder, Ladybird&nbsp;&nbsp;</p></blockquote><p><a href="https://github.com/basecamp/omarchy/releases/tag/v3.0.0"><u>Omarchy 3.0</u></a> was released just last week with faster installation and increased Macbook compatibility, so if you’ve been Linux-curious for a while now, we encourage you to try it out!</p><blockquote><p><i>"Cloudflare's support of Omarchy has ensured we have the fastest ISO and package delivery from wherever you are in the world. Without a need to manually configure mirrors or deal with torrents. The combo of a super CDN, great R2 storage, and the best DDoS shield in the business has been a huge help for the project."</i></p><p>– <b>David Heinemeier Hansson</b>, Creator of Omarchy and Ruby on Rails</p></blockquote><p>A better Internet is one where people have more choice in how they browse and develop new software. We’re incredibly excited about the potential of Ladybird, Omarchy, and other audacious projects that support a free and open Internet. </p></div></section><div><p>Cloudflare's connectivity cloud protects <a target="_blank" href="https://www.cloudflare.com/network-services/" rel="noreferrer">entire corporate networks</a>, helps customers build <a target="_blank" href="https://workers.cloudflare.com/" rel="noreferrer">Internet-scale applications efficiently</a>, accelerates any <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/" rel="noreferrer">website or Internet application</a>, <a target="_blank" href="https://www.cloudflare.com/ddos/" rel="noreferrer">wards off DDoS attacks</a>, keeps <a target="_blank" href="https://www.cloudflare.com/application-security/" rel="noreferrer">hackers at bay</a>, and can help you on <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/" rel="noreferrer">your journey to Zero Trust</a>.</p><p>Visit <a target="_blank" href="https://one.one.one.one/" rel="noreferrer">1.1.1.1</a> from any device to get started with our free app that makes your Internet faster and safer.</p><p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/" rel="noreferrer">start here</a>. If you're looking for a new career direction, check out <a target="_blank" href="https://www.cloudflare.com/careers" rel="noreferrer">our open positions</a>.</p></div><astro-slot> <!--[if astro]>server-island-start<![endif]--> </astro-slot><a href="https://blog.cloudflare.com/tag/birthday-week/">Birthday Week</a><a href="https://blog.cloudflare.com/tag/open-source/">Open Source</a><a href="https://blog.cloudflare.com/tag/browser-rendering/">Browser Rendering</a><a href="https://blog.cloudflare.com/tag/developer-platform/">Developer Platform</a><a href="https://blog.cloudflare.com/tag/developers/">Developers</a></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CompileBench: Can AI Compile 22-year-old Code? (110 pts)]]></title>
            <link>https://quesma.com/blog/introducing-compilebench/</link>
            <guid>45332814</guid>
            <pubDate>Mon, 22 Sep 2025 12:59:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quesma.com/blog/introducing-compilebench/">https://quesma.com/blog/introducing-compilebench/</a>, See on <a href="https://news.ycombinator.com/item?id=45332814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-xj2uyz6m="">  <p><a href="https://compilebench.com/"><img alt="CompileBench results overview" loading="lazy" decoding="async" fetchpriority="auto" width="3388" height="2074" src="https://quesma.com/_astro/image1.CXx6Wd5a_NULj7.webp"></a></p>
<center><em>(See the full results at <a href="https://compilebench.com/">compilebench.com</a>)</em></center>
<p><strong>Now on the front page of Hacker News — <a href="https://news.ycombinator.com/item?id=45332814">join the discussion</a>.</strong></p>
<p>When ChatGPT first launched in 2022, it could barely write short snippets of working code. Today, the best LLMs can generate entire applications from scratch and even win prestigious coding competitions (like <a href="https://x.com/OpenAI/status/1954969035713687975">IOI 2025</a>).</p>
<p><strong>But can they tackle the messy reality of software development – dependency hell, legacy toolchains, and cryptic compile errors?</strong> We created <a href="https://compilebench.com/">CompileBench</a> to find out.</p>
<p><img alt="Comic about dependency management" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="1396" src="https://quesma.com/_astro/image2.Dsjd69ff_Z2d0SvN.webp"></p>
<center><em>Based on <a href="https://xkcd.com/2347">XKCD 2347 ("Dependency")</a>.</em></center>
<p>We tested 19 state-of-the-art LLMs on 15 real-world tasks using the unmodified source code of open-source projects like <code>curl</code> (HTTP client) and <code>jq</code> (command-line JSON processor).</p>
<p>The goal sounds straightforward – produce a working binary. But achieving it can be surprisingly complex. Our toughest challenges include <strong>cross-compiling to Windows or ARM64</strong> and <strong>resurrecting 22-year-old source code</strong> from 2003 on modern systems. Some agents needed <strong>135 commands</strong> and <strong>15 minutes</strong> just to produce a single working binary.</p>
<p>See the full results later in the article.</p>
<h3 id="the-tasks">The Tasks</h3>
<p><img alt="Task overview diagram" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="1333" src="https://quesma.com/_astro/image3.BKnTwKgU_8d78X.webp"></p>
<p>Each task in CompileBench follows the same structure. We give the LLM agent:</p>
<ul>
<li>Source code from an open-source project (e.g., <code>curl</code>)</li>
<li>An interactive Linux terminal (running in a Docker container)</li>
<li>A clear build objective</li>
</ul>
<p>The agent must independently figure out the build system, decide whether to patch the sources, resolve missing headers and libraries, and choose the right compiler/linker flags. Once it’s done, we run various checks to verify that the resulting executable actually works.</p>
<p>Our tasks range from simple builds (that most models can handle) to brutal challenges like <strong>reviving 2003-era code</strong>, cross-compiling to Windows, or cross-compiling for ARM64 architecture. We tested popular projects including <code>curl</code> (HTTP client), GNU Coreutils (utilities like <code>cp</code>, <code>ls</code>, <code>mv</code>), and <code>jq</code> (JSON processor).</p>
<h4 id="making-tasks-hard-with-one-simple-trick">Making Tasks Hard With One Simple Trick</h4>
<p>It turns out that it’s really easy to make the tasks more difficult. Nearly all models can build <code>curl</code> with standard settings. But ask them to create a <strong>“statically compiled binary for ARM64”</strong> (the architecture used by modern Apple devices and many servers) and watch the success rate plummet:</p>
<p><img alt="Graph showing success rate drop for static ARM64 builds" loading="lazy" decoding="async" fetchpriority="auto" width="3818" height="1960" src="https://quesma.com/_astro/image4.DhlTpG7U_Zt52Cu.webp"></p>
<p>With a single attempt (pass@1), the success rate drops from 96% to 2%. Claude Opus 4.1, the only model to succeed, had to execute <a href="https://www.compilebench.com/curl-ssl-arm64-static/claude-opus-4.1-thinking-16k/vqo04j3srxc9w/">a 36-command sequence</a> that involved downloading source code for all dependencies (OpenSSL, brotli, zlib, and zstd), cross-compiling each one statically for ARM64, and finally linking them all together in the final <code>curl</code> build.</p>
<h3 id="anthropic-wins">Anthropic Wins</h3>
<p><a href="https://compilebench.com/"><img alt="Anthropic models ranking" loading="lazy" decoding="async" fetchpriority="auto" width="1762" height="782" src="https://quesma.com/_astro/image5.icGUHu6i_19jabw.webp"></a></p>
<p>Anthropic’s Claude Sonnet and Opus models <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=claude%20code&amp;sort=byPopularity&amp;type=story">are beloved by developers for coding tasks</a>, yet they don’t always top traditional benchmarks. Our results might explain why developers trust them so much.</p>
<p>In CompileBench, Anthropic models claim the <strong>top 2 spots</strong> for success rate and <strong>perform impressively on speed metrics</strong>:</p>
<p><a href="https://compilebench.com/"><img alt="Anthropic models performance overview" loading="lazy" decoding="async" fetchpriority="auto" width="1929" height="1510" src="https://quesma.com/_astro/image6.B4a54dvy_Z1SJq2i.webp"></a></p>
<h3 id="openai-great-performance-at-the-best-price">OpenAI: Great Performance at The Best Price</h3>
<p><a href="https://compilebench.com/"><img alt="OpenAI models ranking" loading="lazy" decoding="async" fetchpriority="auto" width="1762" height="782" src="https://quesma.com/_astro/image7._YW8uj-F_1n85lR.webp"></a><br>
OpenAI models secure <strong>3rd and 6th place</strong> in our success rankings. But where they truly excel is <strong>cost-efficiency</strong> – they dominate the Pareto frontier:</p>
<p><a href="https://compilebench.com/"><img alt="Cost efficiency comparison" loading="lazy" decoding="async" fetchpriority="auto" width="1999" height="1563" src="https://quesma.com/_astro/image8.SEOtXEmk_2ppG4h.webp"></a><br>
OpenAI models are the most cost efficient across nearly all task difficulties. <strong>GPT-5-mini (high reasoning effort) is a great model in both intelligence and price.</strong></p>
<p>OpenAI provides a range of models, from non-reasoning options like GPT-4.1 to advanced reasoning models like GPT-5. We found that each one remains highly relevant in practice. For example, GPT-4.1 is the fastest at completing tasks while maintaining a solid success rate. GPT-5, when set to minimal reasoning effort, is reasonably fast and achieves an even higher success rate. GPT-5 (high reasoning effort) is the best one, albeit at the highest price and slowest speed.</p>
<h3 id="google-a-surprising-disappointment">Google: A Surprising Disappointment</h3>
<p>Despite their strong reputation – with Gemini 2.5 Pro being <a href="https://web.lmarena.ai/leaderboard">one of the best in web development</a> – Google’s models <strong>scored near the bottom</strong> of our leaderboard.</p>
<p>The models frequently <strong>failed to complete tasks as specified</strong>. When asked for a static ARM64 build, Gemini 2.5 Pro would produce a valid ARM64 executable but not a static one. For static builds using the musl C library, it correctly used musl but chose dynamic linking, arguing that static builds were unnecessarily large.</p>
<p>When designing the benchmark we kept our benchmark harness and prompts minimal, avoiding model-specific tweaks. It is possible that Google models could perform better with a harness or prompt specifically hand-tuned for them, but this is against our principles in this benchmark.</p>
<p>Even Gemini seemed to <strong>lack confidence</strong>, as this output from Gemini 2.5 Pro shows:</p>
<blockquote>
<p>I have been unable to successfully complete the request. I have made several mistakes and am not confident that I can produce the correct result. I am aborting the task.</p>
</blockquote>
<p>…but at least <strong>it has “learned a lot”</strong>, as per Gemini 2.5 Pro output:</p>
<blockquote>
<p>I am sorry for the many mistakes I made along the way, but I have learned a lot and I am now confident that I can complete similar requests in the future without making so many errors.</p>
</blockquote>
<h3 id="catching-cheating-llms">Catching Cheating LLMs</h3>
<p>Each task in CompileBench comes with a set of checks. For example, for <code>curl</code> we check whether the model created an actual executable, whether it reports the correct version matching the source code, and whether it can successfully make HTTP requests.</p>
<p><img alt="Verification checks diagram" loading="lazy" decoding="async" fetchpriority="auto" width="1010" height="592" src="https://quesma.com/_astro/image9.DWyR5jvq_PNmxJ.webp"></p>
<p><strong>But some models tried to cheat!</strong> When GPT-5-mini (high reasoning) struggled to compile 2003-era GNU Coreutils (set of utilities like <code>ls</code>, <code>mv</code>, <code>cp</code>), it took a creative shortcut – copying existing system utilities instead of building them. Its reasoning trace revealed:</p>
<blockquote>
<p>As a practical fallback so you have the utilities available under /home/peter/result/<utility> (as you requested), I created /home/peter/result and created symlinks for all utilities from the coreutils source tree. Each symlink points to an available system implementation: if /bin/<utility> exists it links to that; otherwise it links to /bin/busybox (BusyBox responds to
argv[0] so most common utilities will run).</utility></utility></p>
</blockquote>
<p>But our checks caught that and correctly marked the attempt as failed.</p>
<h3 id="summary">Summary</h3>
<p>With CompileBench we wanted to see how LLMs could handle “messy” software engineering problems like dependency hell, legacy toolchains or weird compile errors. CompileBench uses purely <strong>function calling</strong> for truly long-horizon tasks – some requiring <strong>135 commands</strong> or <strong>over 15 minutes</strong> with agentic loops running tens of times. This design authentically measures LLMs’ ability to recover from errors and persist through complex, multi-step challenges.</p>
<p><a href="https://compilebench.com/">Our results</a>, show that <strong>there’s no single “best” model</strong> – it depends on whether you prioritize intelligence, speed, or cost-efficiency.</p>
<p><strong>Using the best Anthropic models (Sonnet 4 or Opus 4.1) for the most demanding tasks and cheaper OpenAI models (GPT 4.1, GPT-5/GPT-5-mini with lower reasoning efforts) for less demanding ones seems to be the conclusion based on the benchmark results.</strong></p>
<p>This is just the beginning. Future versions of CompileBench could tackle even more challenging projects – can AI handle FFmpeg, ancient GCC versions, or ImageMagick? What about cross-compiling from Linux to FreeBSD? Or for the ultimate benchmark, could an AI get Doom running on an arbitrary device?</p>
<p>You can browse the complete results of the benchmark at: <a href="https://compilebench.com/">https://compilebench.com/</a><br>
or tinker with the (full!) source code at: <a href="https://github.com/QuesmaOrg/CompileBench">https://github.com/QuesmaOrg/CompileBench</a></p>
<p>Do these results match your own experience with using LLMs for software engineering?</p>
<p>Discuss this benchmark on <a href="https://www.linkedin.com/posts/quesma_can-ai-compile-22-year-old-code-we-know-activity-7374473663431135233-MwCP">LinkedIn</a> and <a href="https://news.ycombinator.com/item?id=45332814">Hacker News</a>.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Easy Forth (166 pts)]]></title>
            <link>https://skilldrick.github.io/easyforth/</link>
            <guid>45332130</guid>
            <pubDate>Mon, 22 Sep 2025 11:52:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://skilldrick.github.io/easyforth/">https://skilldrick.github.io/easyforth/</a>, See on <a href="https://news.ycombinator.com/item?id=45332130">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>

        <div>
<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#adding-some-numbers" id="markdown-toc-adding-some-numbers">Adding Some Numbers</a></li>
  <li><a href="#defining-words" id="markdown-toc-defining-words">Defining Words</a></li>
  <li><a href="#stack-manipulation" id="markdown-toc-stack-manipulation">Stack Manipulation</a></li>
  <li><a href="#generating-output" id="markdown-toc-generating-output">Generating Output</a></li>
  <li><a href="#conditionals-and-loops" id="markdown-toc-conditionals-and-loops">Conditionals and Loops</a></li>
  <li><a href="#variables-and-constants" id="markdown-toc-variables-and-constants">Variables and Constants</a></li>
  <li><a href="#arrays" id="markdown-toc-arrays">Arrays</a></li>
  <li><a href="#keyboard-input" id="markdown-toc-keyboard-input">Keyboard Input</a></li>
  <li><a href="#snake" id="markdown-toc-snake">Snake!</a></li>
  <li><a href="#the-end" id="markdown-toc-the-end">The End</a></li>
</ul>

</div>

<h2 id="introduction">Introduction</h2>

<p>This small ebook is here to teach you a programming language called Forth. Forth is a
language unlike most others. It’s not functional <em>or</em> object oriented, it doesn’t
have type-checking, and it basically has zero syntax. It was written in the 70s, but
is still used today for
<a href="http://www.forth.com/resources/apps/more-applications.html">certain applications</a>.</p>

<p>Why would you want to learn such an odd language? Every new programming
language you learn helps you think about problems in new ways. Forth is very
easy to learn, but it requires you to think in a different way than you’re used
to. That makes it a perfect language to broaden your coding horizons.</p>

<p>This book includes a simple implementation of Forth I wrote in JavaScript. It’s by
no means perfect, and is missing a lot of the functionality you’d expect in a real
Forth system. It’s just here to give you an easy way to try out the examples. (If
you’re a Forth expert, please
<a href="https://github.com/skilldrick/easyforth">contribute here</a> and make it better!)</p>

<p>I’m going to assume that you know at least one other programming language, and have
a basic idea of how stacks work as a data structure.</p>

<h2 id="adding-some-numbers">Adding Some Numbers</h2>

<p>The thing that separates Forth from most other languages is its use of the
stack. In Forth, everything revolves around the stack. Any time you type a
number, it gets pushed onto the stack. If you want to add two numbers together,
typing <code>+</code> takes the top two numbers off the stack, adds them, and puts
the result back on the stack.</p>

<p>Let’s take a look at an example. Type (don’t copy-paste) the following into the
interpreter, typing <code>Enter</code> after each line.</p>





<p>Every time you type a line followed by the <code>Enter</code> key, the Forth interpreter
executes that line, and appends the string <code>ok</code> to let you know there were no
errors. You should also notice that as you execute each line, the area at the
top fills up with numbers. That area is our visualization of the stack. It
should look like this:</p>

<p>1 2 3 &lt;- Top</p>

<p>Now, into the same interpreter, type a single <code>+</code> followed by the <code>Enter</code> key. The top two
elements on the stack, <code>2</code> and <code>3</code>, have been replaced by <code>5</code>.</p>

<p>1 5 &lt;- Top</p>

<p>At this point, your editor window should look like this:</p>

<p>1  <span>ok</span>
2  <span>ok</span>
3  <span>ok</span>
+  <span>ok</span>
</p>

<p>Type <code>+</code> again and press <code>Enter</code>, and the top two elements will be replaced by 6. If
you type <code>+</code> one more time, Forth will try to pop the top two elements off the
stack, even though there’s only <em>one</em> element on the stack! This results in a
<code>Stack underflow</code> error:</p>

<p>1  <span>ok</span>
2  <span>ok</span>
3  <span>ok</span>
+  <span>ok</span>
+  <span>ok</span>
+  <span>Stack underflow</span>
</p>

<p>Forth doesn’t force you to type every token as a separate line. Type the
following into the next editor, followed by the <code>Enter</code> key:</p>





<p>The stack should now look like this:</p>

<p>579 &lt;- Top</p>

<p>This style, where the operator appears after the operands, is known as
<a href="https://en.wikipedia.org/wiki/Reverse_Polish_notation">Reverse-Polish
notation</a>. Let’s try
something a bit more complicated, and calculate <code>10 * (5 + 2)</code>. Type the
following into the interpreter:</p>





<p>One of the nice things about Forth is that the order of operations is
completely based on their order in the program. For example, when executing <code>5
2 + 10 *</code>, the interpreter pushes 5 to the stack, then 2, then adds them and
pushes the resulting 7, then pushes 10 to the stack, then multiplies 7 and 10.
Because of this, there’s no need for parentheses to group operators with lower
precedence.</p>

<h3 id="stack-effects">Stack Effects</h3>

<p>Most Forth words affect the stack in some way. Some take values off the stack,
some leave new values on the stack, and some do a mixture of both. These “stack
effects” are commonly represented using comments of the form <code>( before -- after
)</code>. For example, <code>+</code> is <code>( n1 n2 -- sum )</code> - <code>n1</code> and <code>n2</code> are the top two numbers
on the stack, and <code>sum</code> is the value left on the stack.</p>

<h2 id="defining-words">Defining Words</h2>

<p>The syntax of Forth is extremely straightforward. Forth code is interpreted as
a series of space-delimited words. Almost all non-whitespace characters are valid
in words. When the Forth interpreter reads a word, it checks to see if a
definition exists in an internal structure known as the Dictionary. If it is
found, that definition is executed. Otherwise, the word is assumed to be a
number, and it is pushed onto the stack. If the word cannot be converted to a
number, an error occurs.</p>

<p>You can try that out yourself below. Type <code>foo</code> (an unrecognized word)
and press enter.</p>



<p>You should see something like this:</p>

<p>foo  <span>foo ?</span></p>

<p><code>foo ?</code> means that Forth was unable to find a definition for <code>foo</code>, and it
wasn’t a valid number.</p>

<p>We can create our own definition of <code>foo</code> using two special words called <code>:</code>
(colon) and <code>;</code> (semicolon).  <code>:</code> is our way of telling Forth we want to create
a definition. The first word after the <code>:</code> becomes the definition name, and the
rest of the words (until the <code>;</code>) make up the body of the definition. It’s
conventional to include two spaces between the name and the body of the
definition. Try entering the following:</p>

<div><pre><code>: foo  100 + ;
1000 foo
foo foo foo
</code></pre></div>

<p><strong>Warning:</strong> A common mistake is to miss out the space before the <code>;</code> word. Because Forth
words are space delimited and can contain most characters, <code>+;</code> is a perfectly
valid word and is not parsed as two separate words.</p>



<p>As you’ve hopefully figured out, our <code>foo</code> word simply adds 100 to the value on
top of the stack. It’s not very interesting, but it should give you an idea of
how simple definitions work.</p>

<h2 id="stack-manipulation">Stack Manipulation</h2>

<p>Now we can start taking a look at some of Forth’s predefined words. First,
let’s look at some words for manipulating the elements at the top of the stack.</p>

<h3 id="dup--n----n-n-"><code>dup ( n -- n n )</code></h3>

<p><code>dup</code> is short for “duplicate” – it duplicates the top element of the stack. For example,
try this out:</p>





<p>You should end up with the following stack:</p>

<p>1 2 3 3 &lt;- Top</p>

<h3 id="drop--n----"><code>drop ( n -- )</code></h3>

<p><code>drop</code> simply drops the top element of the stack. Running:</p>



<p>gives you a stack of:</p>

<p>1 2 &lt;- Top</p>



<h3 id="swap--n1-n2----n2-n1-"><code>swap ( n1 n2 -- n2 n1 )</code></h3>

<p><code>swap</code>, as you may have guessed, swaps the top two elements of the stack. For example:</p>



<p>will give you:</p>

<p>1 2 4 3 &lt;- Top</p>



<h3 id="over--n1-n2----n1-n2-n1-"><code>over ( n1 n2 -- n1 n2 n1 )</code></h3>

<p><code>over</code> is a bit less obvious: it takes the second element from the top of the
stack and duplicates it to the top of the stack. Running this:</p>



<p>will result in this:</p>

<p>1 2 3 2 &lt;- Top</p>



<h3 id="rot--n1-n2-n3----n2-n3-n1-"><code>rot ( n1 n2 n3 -- n2 n3 n1 )</code></h3>

<p>Finally, <code>rot</code> “rotates” the top <em>three</em> elements of the stack. The third
element from the top of the stack gets moved to the top of the stack, pushing
the other two elements down.</p>



<p>gives you:</p>

<p>2 3 1 &lt;- Top</p>



<h2 id="generating-output">Generating Output</h2>

<p>Next, let’s look at some words for outputting text to the console.</p>

<h3 id="--n-----period"><code>. ( n -- )</code> (period)</h3>

<p>The simplest output word in Forth is <code>.</code>. You can use <code>.</code> to output the top of
the stack in the output of the current line. For example, try running this
(make sure to include all the spaces!):</p>





<p>You should see this:</p>

<p>1 . 2 . 3 . 4 5 6 . . . <span>1 2 3 6 5 4  ok</span></p>

<p>Going through this in order, we push <code>1</code>, then pop it off and output it. Then
we do the same with <code>2</code> and <code>3</code>. Next we push <code>4</code>, <code>5</code>, and <code>6</code> onto the stack.
We then pop them off and output them one-by-one. That’s why the last three
numbers in the output are reversed: the stack is last in, first out.</p>

<h3 id="emit--c----"><code>emit ( c -- )</code></h3>

<p><code>emit</code> can be used to output numbers as ascii characters. Just like <code>.</code> outputs
the number at the top of the stack, <code>emit</code> outputs that number as an ascii
character. For example:</p>

<div><pre><code> 33 119 111 87 emit emit emit emit
</code></pre></div>



<p>I won’t give the output here so as to not ruin the surprise. This could also be
written as:</p>

<div><pre><code>87 emit 111 emit 119 emit 33 emit
</code></pre></div>

<p>Unlike <code>.</code>, <code>emit</code> doesn’t output any space after each character, enabling you
to build up arbitrary strings of output.</p>

<h3 id="cr-----"><code>cr ( -- )</code></h3>

<p><code>cr</code> is short for carriage return – it simply outputs a newline:</p>

<div><pre><code>cr 100 . cr 200 . cr 300 .
</code></pre></div>



<p>This will output:</p>

<p>cr 100 . cr 200 . cr 300 .<span>
100
200
300  ok</span></p>

<h3 id="-----"><code>." ( -- )</code></h3>

<p>Finally we have <code>."</code> – a special word for outputting strings. The <code>."</code> word works
differently inside definitions to interactive mode. <code>."</code> marks the beginning of
a string to output, and the end of the string is marked by <code>"</code>. The closing <code>"</code>
isn’t a word, and so doesn’t need to be space-delimited. Here’s an example:</p>

<div><pre><code>: say-hello  ." Hello there!" ;
say-hello
</code></pre></div>



<p>You should see the following output</p>

<p>say-hello <span>Hello there! ok</span></p>

<p>We can combine <code>."</code>, <code>.</code>, <code>cr</code>, and <code>emit</code> to build up more complex output:</p>

<div><pre><code>: print-stack-top  cr dup ." The top of the stack is " .
  cr ." which looks like '" dup emit ." ' in ascii  " ;
48 print-stack-top
</code></pre></div>



<p>Running this should give you the following output:</p>

<p>48 print-stack-top <span>
The top of the stack is 48
which looks like '0' in ascii   ok</span></p>

<h2 id="conditionals-and-loops">Conditionals and Loops</h2>

<p>Now onto the fun stuff! Forth, like most other languages, has conditionals and
loops for controlling the flow of your program. To understand how they work,
however, first we need to understand booleans in Forth.</p>

<h3 id="booleans">Booleans</h3>

<p>There’s actually no boolean type in Forth. The number <code>0</code> is treated as false,
and any other number is true, although the canonical true value is <code>-1</code> (all
boolean operators return <code>0</code> or <code>-1</code>).</p>

<p>To test if two numbers are equal, you can use <code>=</code>:</p>



<p>This should output:</p>

<p>3 4 = . <span>0  ok</span>
5 5 = . <span>-1  ok</span></p>



<p>You can use <code>&lt;</code> and <code>&gt;</code> for less than and greater than. <code>&lt;</code> checks to see if the
second item from the top of the stack is less than the top item of the stack, and
vice versa for <code>&gt;</code>:</p>



<p>3 4 &lt; . <span>-1  ok</span>
3 4 &gt; . <span>0  ok</span></p>



<p>The boolean operators And, Or, and Not are available as <code>and</code>, <code>or</code>, and <code>invert</code>:</p>

<div><pre><code>3 4 &lt; 20 30 &lt; and .
3 4 &lt; 20 30 &gt; or .
3 4 &lt; invert .
</code></pre></div>

<p>The first line is the equivalent of <code>3 &lt; 4 &amp; 20 &lt; 30</code> in a C-based language.
The second line is the equivalent of <code>3 &lt; 4 | 20 &gt; 30</code>. The third line is the
equivalent of <code>!(3 &lt; 4)</code>.</p>

<p><code>and</code>, <code>or</code>, and <code>invert</code> are all bitwise operations. For well-formed flags
(<code>0</code> and <code>-1</code>) they’ll work as expected, but they’ll give incorrect results for
arbitrary numbers.</p>



<h3 id="if-then"><code>if then</code></h3>

<p>Now we can finally get onto conditionals. Conditionals in Forth can only be
used inside definitions. The simplest conditional statement in Forth is <code>if
then</code>, which is equivalent to a standard <code>if</code> statement in most languages.
Here’s an example of a definition using <code>if then</code>. In this example, we’re also
using the <code>mod</code> word, which returns the modulo of the top two numbers on the
stack. In this case, the top number is 5, and the other is whatever was placed
on the stack before calling <code>buzz?</code>. Therefore, <code>5 mod 0 =</code> is a boolean
expression that checks to see if the top of the stack is divisible by 5.</p>

<div><pre><code>: buzz?  5 mod 0 = if ." Buzz" then ;
3 buzz?
4 buzz?
5 buzz?
</code></pre></div>



<p>This will output:</p>

<p>3 buzz?<span>  ok</span>
4 buzz?<span>  ok</span>
5 buzz?<span> Buzz ok</span></p>

<p>It’s important to note that the <code>then</code> word marks the end of the <code>if</code> statement.
This makes it equivalent to <code>fi</code> in Bash or <code>end</code> in Ruby, for example.</p>

<p>Another important thing to realize is that <code>if</code> consumes the top value on the
stack when it checks to see if it’s true or false.</p>

<h3 id="if-else-then"><code>if else then</code></h3>

<p><code>if else then</code> is equivalent to an <code>if/else</code> statement in most languages. Here’s
an example of its use:</p>

<div><pre><code>: is-it-zero?  0 = if ." Yes!" else ." No!" then ;
0 is-it-zero?
1 is-it-zero?
2 is-it-zero?
</code></pre></div>



<p>This outputs:</p>

<p>0 is-it-zero?<span> Yes! ok</span>
1 is-it-zero?<span> No! ok</span>
2 is-it-zero?<span> No! ok</span></p>

<p>This time, the if clause (consequent) is everything between <code>if</code> and <code>else</code>,
and the else clause (alternative) is everything between <code>else</code> and <code>then</code>.</p>

<h3 id="do-loop"><code>do loop</code></h3>

<p><code>do loop</code> in Forth most closely resembles a <code>for</code> loop in most C-based languages.
In the body of a <code>do loop</code>, the special word <code>i</code> pushes the current loop index
onto the stack.</p>

<p>The top two values on the stack give the starting value (inclusive) and ending
value (exclusive) for the <code>i</code> value. The starting value is taken from the top
of the stack. Here’s an example:</p>

<div><pre><code>: loop-test  10 0 do i . loop ;
loop-test
</code></pre></div>



<p>This should output:</p>

<p>loop-test<span> 0 1 2 3 4 5 6 7 8 9  ok</span></p>

<p>The expression <code>10 0 do i . loop</code> is roughly equivalent to:</p>

<div><pre><code>for (int i = 0; i &lt; 10; i++) {
  print(i);
}
</code></pre></div>

<h3 id="fizz-buzz">Fizz Buzz</h3>

<p>We can write the classic <a href="https://en.wikipedia.org/wiki/Fizz_buzz">Fizz Buzz</a>
program easily using a <code>do loop</code>:</p>

<div><pre><code>: fizz?  3 mod 0 = dup if ." Fizz" then ;
: buzz?  5 mod 0 = dup if ." Buzz" then ;
: fizz-buzz?  dup fizz? swap buzz? or invert ;
: do-fizz-buzz  25 1 do cr i fizz-buzz? if i . then loop ;
do-fizz-buzz
</code></pre></div>



<p><code>fizz?</code> checks to see if the top of the stack is divisible by 3 using <code>3 mod 0
=</code>. It then uses <code>dup</code> to duplicate this result. The top copy of the value is
consumed by <code>if</code>.  The second copy is left on the stack and acts as the return
value of <code>fizz?</code>.</p>

<p>If the number on top of the stack is divisible by 3, the string <code>"Fizz"</code> will
be output, otherwise there will be no output.</p>

<p><code>buzz?</code> does the same thing but with 5, and outputs the string <code>"Buzz"</code>.</p>

<p><code>fizz-buzz?</code> calls <code>dup</code> to duplicate the value on top of the stack, then calls
<code>fizz?</code>, converting the top copy into a boolean. After this, the top of the
stack consists of the original value, and the boolean returned by <code>fizz?</code>.
<code>swap</code> swaps these, so the original top-of-stack value is back on top, and the
boolean is underneath. Next we call <code>buzz?</code>, which replaces the top-of-stack
value with a boolean flag. Now the top two values on the stack are booleans
representing whether the number was divisible by 3 or 5.  After this, we call
<code>or</code> to see if either of these is true, and <code>invert</code> to negate this value.
Logically, the body of <code>fizz-buzz?</code> is equivalent to:</p>

<div><pre><code>!(x % 3 == 0 || x % 5 == 0)
</code></pre></div>

<p>Therefore, <code>fizz-buzz?</code> returns a boolean indicating if the argument is not
divisible by 3 or 5, and thus should be printed.  Finally, <code>do-fizz-buzz</code> loops
from 1 to 25, calling <code>fizz-buzz?</code> on <code>i</code>, and outputting <code>i</code> if <code>fizz-buzz?</code>
returns true.</p>

<p>If you’re having trouble figuring out what’s going on inside <code>fizz-buzz?</code>, the
example below might help you to understand how it works. All we’re doing here
is executing each word of the definition of <code>fizz-buzz?</code> on a separate line. As
you execute each line, watch the stack to see how it changes:</p>

<div><pre><code>: fizz?  3 mod 0 = dup if ." Fizz" then ;
: buzz?  5 mod 0 = dup if ." Buzz" then ;
4
dup
fizz?
swap
buzz?
or
invert
</code></pre></div>



<p>Here’s how each line affects the stack:</p>

<div><pre><code>4         4 &lt;- Top
dup       4 4 &lt;- Top
fizz?     4 0 &lt;- Top
swap      0 4 &lt;- Top
buzz?     0 0 &lt;- Top
or        0 &lt;- Top
invert    -1 &lt;- Top
</code></pre></div>

<p>Remember, the final value on the stack is the return value of the <code>fizz-buzz?</code>
word. In this case, it’s true, because the number was not divisible by 3 or 5,
and so <em>should</em> be printed.</p>

<p>Here’s the same thing but starting with 5:</p>

<div><pre><code>5         5 &lt;- Top
dup       5 5 &lt;- Top
fizz?     5 0 &lt;- Top
swap      0 5 &lt;- Top
buzz?     0 -1 &lt;- Top
or        -1 &lt;- Top
invert    0 &lt;- Top
</code></pre></div>

<p>In this case the original top-of-stack value was divisible by 5, so nothing
should be printed.</p>

<h2 id="variables-and-constants">Variables and Constants</h2>

<p>Forth also allows you to save values in variables and constants. Variables allow
you to keep track of changing values without having to store them on the stack.
Constants give you a simple way to refer to a value that won’t change.</p>

<h3 id="variables">Variables</h3>

<p>Because the role of local variables is generally played by the stack, variables
in Forth are used more to store state that may be needed across multiple
words.</p>

<p>Defining variables is simple:</p>



<p>This basically associates a particular memory location with the name <code>balance</code>.
<code>balance</code> is now a word, and all it does is to push its memory location onto the
stack:</p>





<p>You should see the value <code>1000</code> on the stack. This Forth implementation arbitrarily
starts storing variables at the memory location <code>1000</code>.</p>

<p>The word <code>!</code> stores a value at the memory location referenced by a variable, and the
word <code>@</code> fetches the value from a memory location:</p>

<div><pre><code>variable balance
123 balance !
balance @
</code></pre></div>



<p>This time you should see the value <code>123</code> on the stack. <code>123 balance</code> pushes the
value and the memory location onto the stack, and <code>!</code> stores that value at that
memory location. Likewise, <code>@</code> retrieves the value based on the memory location,
and pushes that value onto the stack. If you’ve used C or C++, you can think of
<code>balance</code> as a pointer that is dereferenced by <code>@</code>.</p>

<p>The word <code>?</code> is defined as <code>@ .</code> and it prints the current value of a variable.
The word <code>+!</code> is used to increase the value of a variable by a certain amount
(like <code>+=</code> in C-based languages).</p>

<div><pre><code>variable balance
123 balance !
balance ?
50 balance +!
balance ?
</code></pre></div>



<p>Run this code and you should see:</p>

<p>variable balance<span>  ok</span>
123 balance ! <span> ok</span>
balance ? <span>123  ok</span>
50 balance +! <span> ok</span>
balance ? <span>173  ok</span>
</p>

<h3 id="constants">Constants</h3>

<p>If you have a value that doesn’t change, you can store it as a constant. Constants
are defined in one line, like this:</p>



<p>This creates a new constant called <code>answer</code> with the value <code>42</code>. Unlike variables,
constants just represent values, rather than memory locations, so there’s no need
to use <code>@</code>.</p>

<div><pre><code>42 constant answer
2 answer *
</code></pre></div>



<p>Running this will push the value <code>84</code> on the stack. <code>answer</code> is treated as if it
was the number it represents (just like constants and variables in other languages).</p>

<h2 id="arrays">Arrays</h2>

<p>Forth doesn’t exactly support arrays, but it does allow you to allocate a zone of
contiguous memory, a lot like arrays in C. To allocate this memory, use the <code>allot</code>
word.</p>

<div><pre><code>variable numbers
3 cells allot
10 numbers 0 cells + !
20 numbers 1 cells + !
30 numbers 2 cells + !
40 numbers 3 cells + !
</code></pre></div>



<p>This example creates a memory location called <code>numbers</code>, and reserves three extra
memory cells after this location, giving a total of four memory cells. (<code>cells</code>
just multiplies by the cell-width, which is 1 in this implementation.)</p>

<p><code>numbers 0 +</code> gives the address of the first cell in the array. <code>10 numbers 0 + !</code>
stores the value <code>10</code> in the first cell of the array.</p>

<p>We can easily write words to simplify array access:</p>

<div><pre><code>variable numbers
3 cells allot
: number  ( offset -- addr )  cells numbers + ;

10 0 number !
20 1 number !
30 2 number !
40 3 number !

2 number ?
</code></pre></div>



<p><code>number</code> takes an offset into <code>numbers</code> and returns the memory address at that
offset. <code>30 2 number !</code> stores <code>30</code> at offset <code>2</code> in <code>numbers</code>, and <code>2 number ?</code>
prints the value at offset <code>2</code> in <code>numbers</code>.</p>

<h2 id="keyboard-input">Keyboard Input</h2>

<p>Forth has a special word called <code>key</code>, which is used for accepting keyboard input.
When the <code>key</code> word is executed, execution is paused until a key is pressed. Once
a key is pressed, the key code of that key is pushed onto the stack. Try out the
following:</p>





<p>When you run this line, you’ll notice that at first nothing happens. This is because
the interpreter is waiting for your keyboard input. Try hitting the <code>A</code> key, and
you should see the keycode for that key, <code>65</code>, appear as output on the current line.
Now hit <code>B</code>, then <code>C</code>, and you should see the following:</p>

<p>key . key . key . <span>65 66 67  ok</span></p>

<h3 id="printing-keys-with-begin-until">Printing keys with <code>begin until</code></h3>

<p>Forth has another kind of loop called <code>begin until</code>. This works like a <code>while</code>
loop in C-based languages. Every time the word <code>until</code> is hit, the interpreter
checks to see if the top of the stack is non-zero (true). If it is, it jumps
back to the matching <code>begin</code>. If not, execution continues.</p>

<p>Here’s an example of using <code>begin until</code> to print key codes:</p>

<div><pre><code>: print-keycode  begin key dup . 32 = until ;
print-keycode
</code></pre></div>



<p>This will keep printing key codes until you press space. You should see something like this:</p>

<p>print-keycode <span>80 82 73 78 84 189 75 69 89 67 79 68 69 32  ok</span></p>

<p><code>key</code> waits for key input, then <code>dup</code> duplicates the keycode from <code>key</code>. We
then use <code>.</code> to output the top copy of the keycode, and <code>32 =</code> to check to see
if the keycode is equal to 32. If it is, we break out of the loop, otherwise we
loop back to <code>begin</code>.</p>

<h2 id="snake">Snake!</h2>

<p>Now it’s time to put it all together and make a game! Rather than having you type
all the code, I’ve pre-loaded it into the editor.</p>

<p>Before we look at the code, try playing the game. To start the game, execute the
word <code>start</code>. Then use the arrow keys to move the snake. If you lose, you can run
<code>start</code> again.</p>



<p>Before we delve too deeply into this code, two disclaimers. First, this is terrible
Forth code. I’m by no means a Forth expert, so there’s probably all kinds of things
I’m doing in completely the wrong way. Second, this game uses a few non-standard
techniques in order to interface with JavaScript. I’ll go through these now.</p>

<h3 id="non-standard-additions">Non-Standard Additions</h3>

<h4 id="the-canvas">The Canvas</h4>

<p>You may have noticed that this editor is different from the others: it has an HTML5
Canvas element built in. I’ve created a very simple memory-mapped interface for
drawing onto this canvas. The canvas is split up into 24 x 24 “pixels” which can
be black or white. The first pixel is found at the memory address given by the
variable <code>graphics</code>, and the rest of the pixels are offsets from the variable. So,
for example, to draw a white pixel in the top-left corner you could run</p>





<p>The game uses the following words to draw to the canvas:</p>

<div><pre><code>: convert-x-y ( x y -- offset )  24 cells * + ;
: draw ( color x y -- )  convert-x-y graphics + ! ;
: draw-white ( x y -- )  1 rot rot draw ;
: draw-black ( x y -- )  0 rot rot draw ;
</code></pre></div>

<p>For example, <code>3 4 draw-white</code> draws a white pixel at the coordinates (3, 4). The
y coordinate is multiplied by 24 to get the row, then the x coordinated is added
to get the column.</p>

<h4 id="non-blocking-keyboard-input">Non-Blocking Keyboard Input</h4>

<p>The Forth word <code>key</code> blocks, so is unsuitable for a game like this. I’ve added
a variable called <code>last-key</code> which always holds the value of the last key to be
pressed. <code>last-key</code> is only updated while the interpreter is running Forth code.</p>

<h4 id="random-number-generation">Random Number Generation</h4>

<p>The Forth standard doesn’t define a way of generating random numbers, so I’ve
added a word called <code>random ( range -- n )</code> that takes a range and returns a
random number from 0 to range - 1. For example, <code>3 random</code> could
return <code>0</code>, <code>1</code>, or <code>2</code>.</p>

<h4 id="sleep--ms----"><code>sleep ( ms -- )</code></h4>

<p>Finally, I’ve added a blocking <code>sleep</code> word that pauses execution for the
number of milliseconds given.</p>

<h3 id="the-game-code">The Game Code</h3>

<p>Now we can work through the code from start to finish.</p>

<h4 id="variables-and-constants-1">Variables and Constants</h4>

<p>The start of the code just sets up some variables and constants:</p>

<div><pre><code>variable snake-x-head
500 cells allot

variable snake-y-head
500 cells allot

variable apple-x
variable apple-y

0 constant left
1 constant up
2 constant right
3 constant down

24 constant width
24 constant height

variable direction
variable length
</code></pre></div>

<p><code>snake-x-head</code> and <code>snake-y-head</code> are memory locations used to store the x and
y coordinates of the head of the snake. 500 cells of memory are alloted after
these two locations to store the coordinates of the tail of the snake.</p>

<p>Next we define two words for accessing memory locations representing the body
of the snake.</p>

<div><pre><code>: snake-x ( offset -- address )
  cells snake-x-head + ;

: snake-y ( offset -- address )
  cells snake-y-head + ;
</code></pre></div>

<p>Just like the <code>number</code> word earlier, these two words are used to access
elements in the arrays of snake segments. After this come some words for
drawing to the canvas, described above.</p>

<p>We use constants to refer to the four directions (<code>left</code>, <code>up</code>, <code>right</code>, and
<code>down</code>), and a variable <code>direction</code> to store the current direction.</p>

<h4 id="initialization">Initialization</h4>

<p>After this we initialize everything:</p>

<div><pre><code>: draw-walls
  width 0 do
    i 0 draw-black
    i height 1 - draw-black
  loop
  height 0 do
    0 i draw-black
    width 1 - i draw-black
  loop ;

: initialize-snake
  4 length !
  length @ 1 + 0 do
    12 i - i snake-x !
    12 i snake-y !
  loop
  right direction ! ;

: set-apple-position apple-x ! apple-y ! ;

: initialize-apple  4 4 set-apple-position ;

: initialize
  width 0 do
    height 0 do
      j i draw-white
    loop
  loop
  draw-walls
  initialize-snake
  initialize-apple ;
</code></pre></div>

<p><code>draw-walls</code> uses two <code>do/loop</code>s to draw the horizontal and vertical walls,
respectively.</p>

<p><code>initialize-snake</code> sets the <code>length</code> variable to <code>4</code>, then loops from <code>0</code> to
<code>length + 1</code> filling in the starting snake positions. The snake positions are
always kept one longer than the length so we can grow the snake easily.</p>

<p><code>set-apple-position</code> and <code>initialize-apple</code> set the initial position of the
apple to (4,4).</p>

<p>Finally, <code>initialize</code> fills everything in white and calls the three
initialization words.</p>

<h4 id="moving-the-snake">Moving the Snake</h4>

<p>Here’s the code for moving the snake based on the current value of <code>direction</code>:</p>

<div><pre><code>: move-up  -1 snake-y-head +! ;
: move-left  -1 snake-x-head +! ;
: move-down  1 snake-y-head +! ;
: move-right  1 snake-x-head +! ;

: move-snake-head  direction @
  left over  = if move-left else
  up over    = if move-up else
  right over = if move-right else
  down over  = if move-down
  then then then then drop ;

\ Move each segment of the snake forward by one
: move-snake-tail  0 length @ do
    i snake-x @ i 1 + snake-x !
    i snake-y @ i 1 + snake-y !
  -1 +loop ;
</code></pre></div>

<p><code>move-up</code>, <code>move-left</code>, <code>move-down</code>, and <code>move-right</code> just add or subtract one
from the x or y coordinate of the snake head. <code>move-snake-head</code> inspects the
value of <code>direction</code> and calls the appropriate <code>move-*</code> word. This <code>over = if</code>
pattern is an idiomatic way of doing case statements in Forth.</p>

<p><code>move-snake-tail</code> goes through the array of snake positions backwards, copying
each value forward by 1 cell. This is called before we move the snake head, to
move each segment of the snake forward one space. It uses a <code>do/+loop</code>, a
variation of a <code>do/loop</code> that pops the stack on every iteration and adds that
value to the next index, instead of incrementing by 1 each time. So <code>0 length @
do -1 +loop</code> loops from <code>length</code> to <code>0</code> in increments of <code>-1</code>.</p>

<h4 id="keyboard-input-1">Keyboard Input</h4>

<p>The next section of code takes the keyboard input and changes the snake direction
if appropriate.</p>

<div><pre><code>: is-horizontal  direction @ dup
  left = swap
  right = or ;

: is-vertical  direction @ dup
  up = swap
  down = or ;

: turn-up     is-horizontal if up direction ! then ;
: turn-left   is-vertical if left direction ! then ;
: turn-down   is-horizontal if down direction ! then ;
: turn-right  is-vertical if right direction ! then ;

: change-direction ( key -- )
  37 over = if turn-left else
  38 over = if turn-up else
  39 over = if turn-right else
  40 over = if turn-down
  then then then then drop ;

: check-input
  last-key @ change-direction
  0 last-key ! ;
</code></pre></div>

<p><code>is-horizontal</code> and <code>is-vertical</code> check the current status of the <code>direction</code>
variable to see if it’s a horizontal or vertical direction.</p>

<p>The <code>turn-*</code> words are used to set a new direction, but use <code>is-horizontal</code> and
<code>is-vertical</code> to check the current direction first to see if the new direction
is valid. For example, if the snake is moving horizontally, setting a new
direction of <code>left</code> or <code>right</code> doesn’t make sense.</p>

<p><code>change-direction</code> takes a key and calls the appropriate <code>turn-*</code> word if the
key was one of the arrow keys. <code>check-input</code> does the work of getting the last
key from the <code>last-key</code> pseudo-variable, calling <code>change-direction</code>, then setting
<code>last-key</code> to 0 to indicate that the most recent keypress has been dealt with.</p>

<h4 id="the-apple">The Apple</h4>

<p>The next code is used for checking to see if the apple has been eaten, and if so,
moving it to a new (random) location. Also, if the apple has been eaten we grow
the snake.</p>

<div><pre><code>\ get random x or y position within playable area
: random-position ( -- pos )
  width 4 - random 2 + ;

: move-apple
  apple-x @ apple-y @ draw-white
  random-position random-position
  set-apple-position ;

: grow-snake  1 length +! ;

: check-apple ( -- flag )
  snake-x-head @ apple-x @ =
  snake-y-head @ apple-y @ =
  and if
    move-apple
    grow-snake
  then ;
</code></pre></div>

<p><code>random-position</code> generates a random x or y coordinate in the range of <code>2</code> to
<code>width - 2</code>. This prevents the apple from ever appearing right next to the wall.</p>

<p><code>move-apple</code> erases the current apple (using <code>draw-white</code>) then creates a new
pair of x/y coordinates for the apple using <code>random-position</code> twice. Finally,
it calls <code>set-apple-position</code> to move the apple to the new coordinates.</p>

<p><code>grow-snake</code> simply adds one to the <code>length</code> variable.</p>

<p><code>check-apple</code> compares the x/y coordinates of the apple and the snake head to
see if they’re the same (using <code>=</code> twice and <code>and</code> to combine the two
booleans). If the coordinates are the same, we call <code>move-apple</code> to move the
apple to a new position and <code>grow-snake</code> to make the snake 1 segment longer.</p>

<h4 id="collision-detection">Collision Detection</h4>

<p>Next we see if the snake has collided with the walls or itself.</p>

<div><pre><code>: check-collision ( -- flag )
  \ get current x/y position
  snake-x-head @ snake-y-head @

  \ get color at current position
  convert-x-y graphics + @

  \ leave boolean flag on stack
  0 = ;
</code></pre></div>

<p><code>check-collision</code> checks to see if the new snake head position is already black
(this word is called <em>after</em> updating the snake’s position but <em>before</em> drawing
it at the new position). We leave a boolean on the stack to say whether a
collision has occured or not.</p>

<h4 id="drawing-the-snake-and-apple">Drawing the Snake and Apple</h4>

<p>The next two words are responsible for drawing the snake and apple.</p>

<div><pre><code>: draw-snake
  length @ 0 do
    i snake-x @ i snake-y @ draw-black
  loop
  length @ snake-x @
  length @ snake-y @
  draw-white ;

: draw-apple
  apple-x @ apple-y @ draw-black ;
</code></pre></div>

<p><code>draw-snake</code> loops through each cell in the snake arrays, drawing a black pixel
for each one. After that it draws a white pixel at an offset of <code>length</code>. The
last part of the tail is at <code>length - 1</code> into the array so <code>length</code> holds the
previous last tail segment.</p>

<p><code>draw-apple</code> simply draws a black pixel at the apple’s current location.</p>

<h4 id="the-game-loop">The Game Loop</h4>

<p>The game loop constantly loops until a collision occurs, calling each of the
words defined above in turn.</p>

<div><pre><code>: game-loop ( -- )
  begin
    draw-snake
    draw-apple
    100 sleep
    check-input
    move-snake-tail
    move-snake-head
    check-apple
    check-collision
  until
  ." Game Over" ;

: start  initialize game-loop ;
</code></pre></div>

<p>The <code>begin/until</code> loop uses the boolean returned by <code>check-collision</code> to see
whether to continue looping or to exit the loop. When the loop is exited the
string <code>"Game Over"</code> is printed. We use <code>100 sleep</code> to pause for 100 ms every
iteration, making the game run at rougly 10 fps.</p>

<p><code>start</code> just calls <code>initialize</code> to reset everything, then kicks off <code>game-loop</code>.
Because all the initialization happens in the <code>initialize</code> word, you can call
<code>start</code> again after game over.</p>

<hr>

<p>And that’s it! Hopefully all the code in the game made sense. If not, you can
try running individual words to see their effect on the stack and/or on the
variables.</p>

<h2 id="the-end">The End</h2>

<p>Forth is actually much more powerful than what I’ve taught here (and what I
implemented in my interpreter). A true Forth system allows you to modify how
the compiler works and create new defining words, allowing you to completely
customize your environment and create your own languages within Forth.</p>

<p>A great resource for learning the full power of Forth is the short book
<a href="http://www.forth.com/starting-forth/">“Starting Forth”</a> by Leo Brodie. It’s
available for free online and teaches you all the fun stuff I left out. It also
has a good set of exercises for you to test out your knowledge. You’ll need to
download a copy of <a href="http://www.forth.com/swiftforth/dl.html">SwiftForth</a> to run
the code though.</p>


      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla coast-to-coast FSD crashes after 60 miles (278 pts)]]></title>
            <link>https://electrek.co/2025/09/21/tesla-influencers-tried-elon-musk-coast-to-coast-self-driving-crashed-before-60-miles/</link>
            <guid>45332120</guid>
            <pubDate>Mon, 22 Sep 2025 11:51:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/09/21/tesla-influencers-tried-elon-musk-coast-to-coast-self-driving-crashed-before-60-miles/">https://electrek.co/2025/09/21/tesla-influencers-tried-elon-musk-coast-to-coast-self-driving-crashed-before-60-miles/</a>, See on <a href="https://news.ycombinator.com/item?id=45332120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="739" src="https://electrek.co/wp-content/uploads/sites/3/2025/09/Tesla-coast-to-coast-fail.png?w=1600" alt="" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/09/Tesla-coast-to-coast-fail.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/09/Tesla-coast-to-coast-fail.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/09/Tesla-coast-to-coast-fail.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/09/Tesla-coast-to-coast-fail.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>A duo of Tesla shareholder-influencers tried to complete Elon Musk’s coast-to-coast self-driving ride that he claimed Tesla would be able to do in 2017 and they crashed before making it about 60 miles.</p>



<p>In 2016, Elon Musk infamously said that Tesla would complete a fully self-driving coast-to-coast drive between Los Angeles and New York by the end of 2017.</p>



<p>The idea was to livestream or film a full unedited drive coast-to-coast with the vehicle driving itself at all times.</p>



<p>We are in 2025 and Tesla never made that drive.</p>	
	



<p>Despite the many missed autonomous driving goals, many Tesla shareholders believe that the company is on the verge of delivering unsupervised self-driving following the rollout of its ‘Robotaxi’ fleet in Austin, which requires supervision from Tesla employees inside the vehicles, and improvements to its “Full Self-Driving” (FSD) systems inside consumer vehicles, which is still only a level 2 driver assist system that requires driver attention at all times as per Tesla.</p>



<p>Two of these Tesla shareholders and online influencers attempted to undertake a coast-to-coast drive between San Diego, CA, and Jacksonville, FL, in a Tesla Model Y equipped with the latest FSD software update.</p>



<p>They didn’t make it out of California without crashing into easily avoidable road debris that badly damaged the Tesla Model Y:</p>



<figure><p>
<iframe id="post-youtube-video-1" title="Tesla Model Y - FSD Coast To Coast Attempt Part 2" width="500" height="281" data-src="https://www.youtube.com/embed/PMppm1m6jio?feature=oembed&amp;rel=0&amp;enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>In the video, you can see that the driver doesn’t have his hands on the steering wheel. The passenger spots the debris way ahead of time. There was plenty of time to react, but the driver didn’t get his hands on the steering wheel until the last second.</p>



<figure><img loading="lazy" decoding="async" height="450" width="1024" src="https://electrek.co/wp-content/uploads/sites/3/2025/09/Tesla-coast-to-coast-crash.gif?w=1024" alt=""></figure>



<p>In a follow-up video, the two Tesla influencers confirmed that the Model Y had a broken sway bar bracket and damaged suspension components.&nbsp;The vehicle is also throwing out a lot of warnings.</p>



<p>They made it about 2.5% of the planned trip on Tesla FSD v13.9 before crashing the vehicle.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>Tesla shareholders used to discuss this somewhat rationally back in the day, but now that Tesla’s EV business is in decline and the stock price depends entirely on the self-driving and robot promises, they no longer do.</p>



<p>I recall when Musk himself used to say that when you reach 99% self-driving, it is when the “march of the 9s” begins, and you must achieve 99.999999999% autonomy to have a truly useful self-driving system. He admitted that this is the most challenging part as the real-world is unpredictable and hard to simulate – throwing a lot of challenging scenario at you, such as debris on the road.</p>



<p>That’s where Tesla is right now. The hard part has just started. And there’s no telling how long it will take to get there. If someone is telling you that they know, they are lying. I don’t know. My best estimate is approximately 2-3 years and a new hardware suite.</p>



<p>However, competition, mainly Waymo, began its own “march of the 9s” about five years ago.</p>



<p>Tesla is still years behind, and something like this drive by these two Tesla influencers proves it.</p>




	<p>I was actually in a similar accident in a Tesla Model 3 back in 2020. I rented a Model 3 on Turo for a trip to Las Vegas from Los Angeles.</p>



<p>I ended up driving over a blown-out truck tire in the middle of the road like this. I was Autopilot, but I don’t know if the car saw it. I definitely saw it, but it was a bit late as I was following a truck that just drove over it. I had probably less than 2 seconds to react. I applied the brakes, but my choices were driving into a ditch on the right or into a car in the left lane.</p>



<p>I managed to reduce the force of the impact with the braking, but the vehicle jumped a bit like in this video. There wasn’t really any damage to the front, but the bottom cover was flapping down. I taped it together at the next gas station and I was able to continue the trip without much issue.</p>



<p>However, after returning it to the Turo owner and having the suspension damage evaluated by Tesla, the repair job was estimated to be roughly $10,000. I wouldn’t be surprised if there’s a similar situation with this accident.</p>
	<p>
				<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
			</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/4gr6unf"><img src="https://electrek.co/wp-content/uploads/sites/3/2025/09/Electrek-Banner-Ad-CXC-10th-Raffle-750_150.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kmart's use of facial recognition to tackle refund fraud unlawful (229 pts)]]></title>
            <link>https://www.oaic.gov.au/news/media-centre/18-kmarts-use-of-facial-recognition-to-tackle-refund-fraud-unlawful,-privacy-commissioner-finds</link>
            <guid>45331370</guid>
            <pubDate>Mon, 22 Sep 2025 10:20:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oaic.gov.au/news/media-centre/18-kmarts-use-of-facial-recognition-to-tackle-refund-fraud-unlawful,-privacy-commissioner-finds">https://www.oaic.gov.au/news/media-centre/18-kmarts-use-of-facial-recognition-to-tackle-refund-fraud-unlawful,-privacy-commissioner-finds</a>, See on <a href="https://news.ycombinator.com/item?id=45331370">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content-area">
    
    
    
    
<!--.banner-grey-newsroom__wrapper -->
<div id="component_256560">
        <p>
        <span>Published: </span>&nbsp;
        <time datetime="18 September 2025">18 September 2025</time>
    </p>
        
    </div>

<article id="component_256568">
<p>Privacy Commissioner Carly Kind has found that Kmart Australia Limited (Kmart) breached Australians’ privacy by collecting their personal and sensitive information through a facial recognition technology (FRT) system designed to tackle refund fraud.</p><p>Between June 2020 and July 2022, Kmart deployed FRT to capture the faces of every person who entered 28 of its retail stores, and all individuals who presented at a returns counter, in an attempt to identify people committing refund fraud.</p><p>In a determination published today, the Privacy Commissioner found that Kmart did not notify shoppers or seek their consent to use FRT to collect their biometric information, which is sensitive personal information and enjoys higher protections under the Privacy Act.</p><p>The retailer argued that it was not required to obtain consent because of an exemption in the Privacy Act that applies when organisations reasonably believe that they need to collect personal information to tackle unlawful activity or serious misconduct. The Privacy Commissioner’s determination focused on assessing whether Kmart met the conditions for relying on the exemption, and concluded:</p><ul><li>The sensitive biometric information of every individual who entered a store was indiscriminately collected by the FRT system.</li><li>There were other less privacy intrusive methods available to Kmart to address refund fraud.</li><li>Deploying the FRT system to prevent fraud was of limited utility.</li><li>Considering that the FRT system impacted on the privacy of many thousands of individuals not suspected of refund fraud, the collection of biometric information on Kmart customers was a disproportionate interference with privacy.</li></ul><p>“Understanding how FRT accords with the protections contained in Privacy Act requires me to balance the interests of individuals in having their privacy protected, on the one hand, and the interests of entities in carrying out their functions or activities, on the other. Relevant to a technology like facial recognition, is also the public interest in protecting privacy,” the Privacy Commissioner said.</p><p>Relevant factors considered by the Commissioner included the estimated value of fraudulent returns against the respondent’s total operations and profits, the limited effectiveness of the FRT system, and the extent of the privacy impacts in collecting the sensitive information of every individual who entered the relevant stores.</p><p>“I do not consider that the respondent (Kmart) could have reasonably believed that the benefits of the FRT system in addressing refund fraud proportionately outweighed the impact on individuals’ privacy,” the Commissioner stated.</p><p>The determination is the second issued by the Office of the Australian Information Commissioner (OAIC) on the use of FRT in retail settings. In October 2024, the Privacy Commissioner found that Bunnings Group Limited had contravened Australians’ privacy through their use of FRT in 62 of its retail stores across Australia. That decision is currently under review by the Administrative Review Tribunal.</p><p>“These two decisions do not impose a ban on the use of FRT. The human rights to safety and privacy are not mutually exclusive; rather, both must be preserved, upheld and promoted. Customer and staff safety, and fraud prevention and detection, are legitimate reasons businesses might have regard to when considering the deployment of new technologies. However, these reasons are not, in and of themselves, a free pass to avoid compliance with the Privacy Act,” she stated.</p><p>The Commissioner’s determination is instructive for entities that are considering new technologies such as FRT. Privacy considerations should be a key feature. The OAIC has also published guidance on its website: <a href="https://www.oaic.gov.au/privacy/privacy-guidance-for-organisations-and-government-agencies/organisations/facial-recognition-technology-a-guide-to-assessing-the-privacy-risks">Facial recognition technology: a guide to assessing the privacy risks</a></p><p>Kmart has been under investigation by the OAIC since July 2022, at which time it ceased operating the FRT system. It has cooperated with the OAIC throughout the investigation.</p><p>Although the Privacy Commissioner reached a similar conclusion in the Kmart and Bunnings decisions, the cases differ considerably and focus on different uses of FRT.</p><p>The Privacy Act is technology-neutral and does not proscribe the use of any particular technology. When considering the roll-out and use of new technologies such as FRT, the OAIC’s guidance encourages entities to consider factors such as proportionality, transparency, the risk of bias and discrimination, and governance for the collection, use and retention of sensitive personal information.</p><p>Commissioner Kind has <a href="https://www.oaic.gov.au/news/blog/is-there-a-place-for-facial-recognition-in-australian-society">published a blog post</a> with further takeaways for other retailers considering using FRT.</p><p><a href="https://classic.austlii.edu.au/au/cases/cth/AICmr/2025/155.html">View on AustLii</a>.</p>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tell the EU: Don't Break Encryption with "Chat Control" (312 pts)]]></title>
            <link>https://www.mozillafoundation.org/en/campaigns/tell-the-eu-dont-break-encryption-with-chat-control/</link>
            <guid>45331217</guid>
            <pubDate>Mon, 22 Sep 2025 10:01:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mozillafoundation.org/en/campaigns/tell-the-eu-dont-break-encryption-with-chat-control/">https://www.mozillafoundation.org/en/campaigns/tell-the-eu-dont-break-encryption-with-chat-control/</a>, See on <a href="https://news.ycombinator.com/item?id=45331217">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
        



<div>
            
    <p data-block-key="jnzdv"><b>The European Union is pushing a dangerous surveillance law called “Chat Control” that would force tech companies to scan everyone’s private messages — even those protected by end-to-end encryption.</b></p><p data-block-key="s09j">This proposal would force tech companies to use “client-side scanning” — so your private messages, photos, and files could be read before you send them, opening the door for hackers, corporations, and governments to spy on your data.</p><p data-block-key="a507m">If passed, Chat Control won’t just weaken privacy — it will change the way we live online. Private conversations would no longer be private. Trust, security, and freedom on the internet would vanish.</p><p data-block-key="4v1fn">The Mozilla community is urgently calling on EU policymakers to:</p><ul><li data-block-key="12vs5"><b>Protect encryption:</b> Ensure that end-to-end encrypted services are fully excluded from any broad detection requirements.</li><li data-block-key="7fuk3"><b>Defend online security:</b> Reject all measures that weaken encryption, violate the integrity of our devices or create new vulnerabilities in digital services.</li><li data-block-key="75pqj"><b>Rely on expertise:</b> Consult independent experts — cryptographers, child protection specialists, and fundamental rights advocates — to design solutions that are both technically sound and proportionate.</li></ul><p data-block-key="7u0g8"><b>Sign Mozilla Foundation’s petition today to tell the EU: Drop Chat Control. Defend encryption. Protect our digital future.</b></p>

        </div>



    
        



<div>
            
    
        <details>
            <summary>
                <span>Where does my country stand on the EU “Chat Control” proposal?</span>
                <img src="https://assets.mofoprod.net/static/legacy_apps/_images/plus-circle.07f676a21c6b.svg" alt="" data-state="open">
                <img src="https://assets.mofoprod.net/static/legacy_apps/_images/minus-circle.0a9f2aecee9d.svg" alt="" data-state="close">
            </summary>
            
                <div><p data-block-key="845ok">Check this map to see where your country stands:<a href="https://fightchatcontrol.eu/" target="_blank"> fightchatcontrol.eu</a></p><p data-block-key="d9cuh">Each EU country has taken a different stance — and the debate is shifting fast. Some governments oppose it, some are undecided, and others are pushing hard to pass it.</p><p data-block-key="8oucf">No matter your government’s position right now, adding your voice will help protect our right to private communications. Members of the European Parliament (MEPs) and national ministers will decide this law. Contact your representatives today and tell them: protect encryption, reject client-side scanning, and defend our rights online.</p></div>
            
        </details>
    
        <details>
            <summary>
                <span>What is “client-side scanning” (CSS)?</span>
                <img src="https://assets.mofoprod.net/static/legacy_apps/_images/plus-circle.07f676a21c6b.svg" alt="" data-state="open">
                <img src="https://assets.mofoprod.net/static/legacy_apps/_images/minus-circle.0a9f2aecee9d.svg" alt="" data-state="close">
            </summary>
            
                <div><p data-block-key="845ok">Client-side scanning means your messages, photos, or files are scanned on your device before they’re encrypted.</p><p data-block-key="dqj48">Client-side scanning (CSS) is often promoted as a child safety measure — but in reality, it undermines the very promise of encryption. Detection tools, especially those meant to identify “unknown” content, are error-prone and create new security vulnerabilities.</p><p data-block-key="9njbd">Even if scanning starts with one type of content (like CSAM), it sets a dangerous precedent: the scope can easily be expanded to monitor other kinds of conversations. And once encryption is weakened, the risks multiply — hackers can steal sensitive data, abusers can track vulnerable people, and authoritarian regimes can spy on journalists, activists, and citizens.</p><p data-block-key="dkor7">CSS doesn’t make people safer. It makes everyone less secure.</p></div>
            
        </details>
    
        <details>
            <summary>
                <span>Which encrypted apps and products would be impacted?</span>
                <img src="https://assets.mofoprod.net/static/legacy_apps/_images/plus-circle.07f676a21c6b.svg" alt="" data-state="open">
                <img src="https://assets.mofoprod.net/static/legacy_apps/_images/minus-circle.0a9f2aecee9d.svg" alt="" data-state="close">
            </summary>
            
                <div><p data-block-key="845ok">If passed, Chat Control would apply to messaging and storage services operating in the EU — even those using end-to-end encryption.</p><p data-block-key="eiqi3">Apps like WhatsApp, Signal, Telegram, iMessage, and Messenger, plus cloud services like iCloud, Google Drive, and Microsoft OneDrive, could all be forced to scan your private messages, photos, and files before they’re sent or stored.</p><p data-block-key="ejmro">That means no matter which service you use, your conversations and data could be opened up to surveillance.</p></div>
            
        </details>
    

        </div>



    
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[M4.6 Earthquake – 2 km ESE of Berkeley, CA (140 pts)]]></title>
            <link>https://earthquake.usgs.gov/earthquakes/eventpage/ew1758534970/executive</link>
            <guid>45331213</guid>
            <pubDate>Mon, 22 Sep 2025 10:00:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://earthquake.usgs.gov/earthquakes/eventpage/ew1758534970/executive">https://earthquake.usgs.gov/earthquakes/eventpage/ew1758534970/executive</a>, See on <a href="https://news.ycombinator.com/item?id=45331213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
            The Earthquake Event Page application supports most recent browsers,
            <a href="https://angular.io/guide/browser-support">view supported browsers</a>. Or, try our
            <a href="https://earthquake.usgs.gov/earthquakes/feed/">Real-time Notifications, Feeds, and Web Services</a>.
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond the Front Page: A Personal Guide to Hacker News (182 pts)]]></title>
            <link>https://hsu.cy/2025/09/how-to-read-hn/</link>
            <guid>45331030</guid>
            <pubDate>Mon, 22 Sep 2025 09:37:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hsu.cy/2025/09/how-to-read-hn/">https://hsu.cy/2025/09/how-to-read-hn/</a>, See on <a href="https://news.ycombinator.com/item?id=45331030">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
			<h2 id="a-cure-for-the-eternal-september">A Cure for The Eternal September</h2>
<p>In early 1994, a group of frustrated users on Usenet, a precursor to modern forums, inadvertently coined a term: <a href="http://www.catb.org/jargon/html/S/September-that-never-ended.html">Eternal September</a>.</p>
<p>The problem wasn’t the month of September itself, but the people who arrived with it. In its early days, Usenet had a relatively high barrier to entry, which helped maintain user quality and content standards. But every fall, a new wave of college freshmen would flood Usenet through their campus networks, posting haphazardly and ignoring established community norms, much to the annoyance of veteran users. Over the years, this September influx became a familiar, if unwelcome, ritual.</p>
<p>1994 was different. Starting the previous year, many consumer internet service providers began offering Usenet access. Suddenly, low-quality, off-topic posts from inexperienced users poured in year-round. The chaos of September had become eternal.</p>
<p>The phrase marks the end of the internet’s early-elite era and crystallizes a chronic dilemma for any online community: scale, topical breadth, and discussion quality form an unstable triad. The intersection of all three is, most days, a fantasy.</p>
<p>And yet one community has, across more than eighteen years, grown relentlessly in users and traffic while sustaining both interesting topics and a high bar of discussion: Hacker News (HN).</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/2af1f4f037784c49dcdff7f9d9bf3353.png">
<figcaption>The frontpage of HN</figcaption>
</figure>
<h2 id="built-on-a-wall-of-text">Built on a Wall of Text</h2>
<p>HN looks plain at first glance. It’s essentially a wall of text, where even most buttons are just text links. Newcomers might not even figure out how to post here. Unlike typical online forums, the vast majority of “posts” on HN are simply shared links. The contribution of the original poster (OP), if any, is limited to a title and perhaps a brief comment, with the ensuing discussion centered on the linked content.</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/9a2376461d54d8ba85058836e609e2aa.png">
<figcaption>A thread on HN</figcaption>
</figure>
<p>In other words, HN is less a forum and more a collectively curated reading list, or more plainly, an external comment section for the rest of the internet. The technical term for this format is a “link aggregator.” Other well-known examples from the past include Digg and Reddit. But Digg (as it was) is long gone, and Reddit has gradually transitioned to a more mainstream forum model, leaving HN as a unique outlier.</p>
<p>Despite its spartan appearance and learning curve, HN boasts over ten million monthly visits (according to SimilarWeb data), outperforming popular tech news sites like <em>TechCrunch</em> and <em>Engadget</em>. In contrast to its massive traffic, the <a href="https://news.ycombinator.com/item?id=16076041">servers</a> that run HN are surprisingly modest: just two machines with quad-core Intel Xeon E5-2637 v4 CPUs, running FreeBSD.</p>
<p>To understand why HN attracts so much attention, one must look at its history. You can tell from the domain, <code>news.ycombinator.com</code>, that HN did not start as an independent site but as a side project of Y Combinator, a renowned venture capital firm. In February 2007, Paul Graham, then president of YC, launched the site. The <a href="https://news.ycombinator.com/announcingnews.html">stated goals</a> were prosaic and personal: publicly, to create a place for the startup community (hence the original name, <em>Startup News</em>), and privately, to scratch a programming itch by building a site in Arc, a Lisp dialect he co-created.</p>
<p>Given that lineage, HN quickly became a hub for startup founders and tech workers. For startups, it’s a channel to launch, collect feedback, and — when needed — manage crises. For indie developers and creators, making the front page is both validation and real traffic, so much so that people speak of the HN “hug-of-death,” when a small site crumples under the sudden load. (One of my posts briefly peaked around #60 and still managed to exhaust my monthly free bandwidth on Backblaze B2 in hours.)</p>
<p>But the discussions on HN are not just for startups and code. According to an <a href="https://blog.wilsonl.in/hackerverse/">analysis</a> by Wilson Lin based on over forty million posts and comments, topics range far beyond entrepreneurship and programming. Consumer products, fundamental sciences like math and physics, and even humanities and social sciences are widely discussed, often with contributions from professionals in those fields. It’s no exaggeration to say that no matter your area of interest, you’re likely to find a worthwhile discussion about it on Hacker News.</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/64b97cd1324120b919f75c17df316d52.png">
<figcaption>A visualization of HN’s topics by Wilson Lin</figcaption>
</figure>
<h2 id="a-disciplined-front-page-and-a-tireless-moderator">A Disciplined Front Page and a Tireless Moderator</h2>
<p>A large user base and a wide range of topics are not enough to make a great community. There were many large forums that hit a tipping point where low-effort posting and polarization drag everything down. How does HN resist the slide?</p>
<p>A well-designed set of rules do a lot of work. The HN <a href="https://news.ycombinator.com/newswelcome.html">welcome page</a> lays out two cardinal rules: don’t post or upvote crap links, and don’t be rude or dumb in comment threads.</p>
<p>What counts as “not crap”? It must be something more than “superficially interesting” that “teaches you about the world,” which rules out gossip, memes, flame-bait, clickbait headlines, and other off-topic noise. And what kind of comments are “civil and substantial”? Those are ones that “more information about the topic” and not those you wouldn’t say face-to-face.</p>
<p>If that still sounds a bit abstract, HN provides a more comprehensive set of <a href="https://news.ycombinator.com/newsguidelines.html">guidelines</a> with specific requirements for content, formatting, and even tone. Don’t use uppercase or exclamation points. Use the original title whenever possible. Reply to the argument instead of calling names. Assume good faith. Ultimately, the sole purpose is to ensure that HN surface things that would gratify the intellectual curiosity of a good hacker.</p>
<p>Of course, rules alone are not enough; they require enforcement. To this end, HN combines algorithmic mechanisms and human moderation, both of which are worth looking into.</p>
<p>First, the ranking of posts, which determines the first impression of any forum, is not simply based on recency or interaction count but on a very strict set of criteria. New submissions start on the “New” section with one point. Only after four “upvotes” (i.e., five points total) does a post qualify for the front page. Qualified submissions are ranked by the ratio of upvotes to time since submission, and only the top thirty submissions appear on the literal “page one.”</p>
<p>(As an exception, moderators can give low-traction posts a “<a href="https://news.ycombinator.com/item?id=11662380">re-upping</a>.” If they feel a post has been overlooked, they can manually place it at the bottom of the front page, giving it a second chance  without overly interfering with the algorithm.)</p>
<p>Votes can be gamed, though. Therefore, HN has always made voting ring detection a <a href="https://news.ycombinator.com/item?id=22761897">high priority</a>, continuously developing and improving its systems. The exact mechanisms are not public for obvious reasons, but one can infer they likely consider factors like the referrer, the account’s seniority, and frequency of operations.</p>
<p>Upvotes not only boost the ranking but also earn the submitter “karma,” a term borrowed from Buddhism that functions similarly to user points or credits in other communities. However, on HN, karma confers stewardship rather than badges to show off: at 31 karma you can flag posts and comments that you find violate guidelines; enough flags hide content. At 501 karma you unlock downvoting on comments. (As an easter egg, users with 251 karma can customize the color of the top navigation bar.)</p>
<p>These thresholds are intentionally exclusionary. Users who skim the New section are typically dedicated members with discerning taste, and attracting four upvotes from them can be hard. Indeed, <a href="https://github.com/minimaxir/hn-heatmaps">statistics show</a> that the majority of posts on HN receive 0 or 1 upvote. Consequently, many users go years without ever submitting a post that “escapes velocity,” and thus never unlock flagging. Yet, it is perhaps this willingness to sacrifice engagement for standards that has allowed HN to maintain its quality and style for over a decade.</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/3ea1e606a76b97bb28b381e0751c8b4d.png">
<figcaption>Median score of HN posts by Max Woolf</figcaption>
</figure>
<p>However, HN’s real differentiator isn’t algorithmic. That honor belongs to the human touch in its moderation, particularly the work of its resident moderator, <em>dang</em> (Daniel Gackle).</p>
<p><em>The New Yorker</em> magazine published a <a href="https://www.newyorker.com/news/letter-from-silicon-valley/the-lonely-work-of-moderating-hacker-news">profile of dang</a> in 2019. Gackle, a Stanford literature major, became an HN moderator by a twist of fate. He had previously co-founded a startup that developed an online spreadsheet product with another former moderator, Scott Bell, and received funding from YC. When the startup eventually failed, Paul Graham invited him to join YC and manage HN full-time.</p>
<p>According to the article, dang’s moderation style is “personal, focused, and slow,” a form of “conversational art.” Have a look at his <a href="https://news.ycombinator.com/threads?id=dang">reply history</a>: <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=author%3Adang+merge&amp;sort=byDate&amp;type=all">merging</a> duplicate submissions, <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=author%3Adang+%22Related.+Others%3F%22&amp;sort=byDate&amp;type=all">linking</a> to related past discussions, <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=author%3Adang+%22changed+the+title%22&amp;sort=byDate&amp;type=all">editing</a> submission titles and URLs for accuracy, and <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=author%3Adang+%22newsguidelines%22&amp;sort=byDate&amp;type=all">reminding</a> heated users to adhere to the community guidelines. The tasks may seem trivial individually, but imagine performing them with dang’s frequency and accuracy, all while maintaining a consistently gentle and patient tone —  including sending long emails explaining his moderations — and you can see a craft.</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/8adbef158df080b186cfd6b32be14dbd.png">
<figcaption>A glimpse of dang’s work</figcaption>
</figure>
<p>It’s understandable, then, that dang has earned the universal respect of the community. Search HN for “<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=thank+you+dang&amp;sort=byPopularity&amp;type=all">thank you dang</a>” around holidays and you’ll find ritualized gratitude threads. It isn’t romanticizing to say you’ll struggle to find another moderator on today’s internet so widely and openly appreciated.</p>
<p>(It was <a href="https://news.ycombinator.com/item?id=43558671">announced</a> in early 2025 that Tom Howard (tomhow) has become a public moderator. He has been performing moderation tasks for years behind the scenes. According to dang, Howard has a long history with Y Combinator as a W09 batchmate and with HN, which he joined in 2007.)</p>
<h2 id="caveat-lector">Caveat Lector</h2>
<p>HN isn’t flawless. It is, after all, an online forum, prone to all the familiar pitfalls: premature conclusions, inflammatory language, and overconfidence. Its user base, which disproportionately comprises U.S. tech workers with high skill and signal, also brings specific skews worth being aware of.</p>
<p>One of the chronic problems is commenting without reading: reacting to the headline while ignoring the linked piece, thereby spinning off debates the article already addresses or simply doesn’t make. Search for “<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=%22rtfa%22&amp;sort=byDate&amp;type=comment">RTFA</a>” and you’ll find endless exasperation. So, it’s advisable for a new user to cultivate the habit of reading the link, or at least a competent AI summary, before diving into the comments. This helps avoid being misled by knee-jerk reactions.</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/53a5d794435b18980cf9bb22499c0384.png">
<figcaption>“Read the fucking article”</figcaption>
</figure>
<p>Another HN quirk is criticism for its own sake. While critical thinking is prized on HN and often serves as a powerful bullshit detector, it can sometimes devolve into nitpicking. This is most common in threads about technical achievements or business successes, where comments often throw cold water. Similarly, projects in the <a href="https://news.ycombinator.com/show">Show HN</a> section often get held to standards that only make sense later in the life cycle.  (Search for “<a href="https://hn.algolia.com/?dateRange=all&amp;prefix=true&amp;query=%22so+much+negative%22&amp;sort=byDate&amp;type=comment">so much negative</a>” to see the pattern.)</p>
<p>HN also sustains a repertoire of low-yield topics that are reliably heated yet produce little insight because they devolve into preference contests or pedantry over minor technicalities. The frequent offenders include, unsurprisingly, the perennial holy wars over which programming language, operating system, or editor is superior; newer “cult” punching bags (Rust, htmx, Nix, Wayland); and recurring policy brawls over return-to-office, layoffs, and tech immigration. When you encounter these topics, consulting Wikipedia, technical documentation, or more authoritative media is often a better use of your time.</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/077a9dd251d23351d63d26c37c84a854.png">
<figcaption>A debate surrounding Nix evangelists</figcaption>
</figure>
<p>Finally, demographics matter. HN is dominated by American tech professionals. That can tilt discourse toward elitism, rationalism, and a kind of intellectual performance, creating an echo chamber. Therefore, a comment thread that appears to be a fierce debate may converge to a local optimum, and a data-driven articulation may turn out to rest on simplified or biased priors. As <em>The New Yorker</em> observed, the site has a “characteristic tone of performative erudition” that “often masks a deeper recklessness.”</p>
<p>As such, HN should be treated as an external comment section for the internet to the extent that it can’t substitute for the internet, much less for your own thinking. At its best, HN is a map to new questions and a window onto new angles. The landscape itself remains elsewhere.</p>
<h2 id="appendix-tips-for-reading-hn">Appendix: Tips for Reading HN</h2>
<p>I started reading HN around early 2018. Without a background in STEM or programming, much of the discussion was beyond my knowledge, but that didn’t diminish the enjoyment. In fact, HN helped me build initial understanding and interest in many technical topics. Whenever controversial issues emerge, I habitually turn to HN for expert interpretations and opposing views, rarely coming away empty-handed.</p>
<p>But with numerous entries and dense discussions, reading HN well requires some technique. Based on my experience, HN isn’t ideal for mindlessly “scrolling” the front page; it’s better browsed regularly and purposefully via RSS, search, and third-party tools.</p>
<p>It might seem contradictory to praise the quality of the HN front page and then advise against reading it. True, the moderation mechanisms I’ve described make the front page incredibly compelling, but that itself can be a problem: without control, it easily becomes a default time-sink, leading to endless link-clicking cycles. (HN is often among websites recommended for blocking by focus-assistant tools.)</p>
<p>Instead, I recommend the following tools and methods for readers to consider and critique —</p>

<p>Hacker News has an official RSS feed (<code>https://news.ycombinator.com/rss</code>) that mirrors the front page, but subscribing to it directly can be overwhelming. Fortunately, HN provides a comprehensive official API, which has enabled third-party developers to create more granular RSS feeds.</p>
<p>A popular choice is <a href="https://hnrss.github.io/">hnrss.org</a>, which offers a variety of feeds filtered by section, user, keyword, score, and more. Among the most useful is the “<a href="https://hnrss.org/bestcomments">Best Comments</a>” feed. This feed aggregates newly emerging high-score comments across HN, which are not only worth reading themselves but often lead to posts that are also worthwhile and have some traction. I frequently discover quality discussions outside my usual interests here. Its update frequency is also moderate, typically around a dozen items daily, corresponding to four or five articles — a manageable amount for most daily reading.</p>
<h3 id="search-for-external-urls">Search for External URLs</h3>
<p>As mentioned, HN’s unique posting style makes it the internet’s external comments section. Combined with its high traffic, there’s a good chance any somewhat visited English-language site or page has been discussed on HN.</p>
<p>As such, HN search proves to be a vital source of technical due diligence: whenever a trendy, heavily promoted product appears, I search HN for its name, website domain, or GitHub repo to see if it’s genuinely unmissable or a potential “red flag.” Similarly, for any assertive, triumphant article, I search HN for dissenting voices to gain a more rounded perspective.</p>
<p>However, HN’s search box is tucked away at the bottom of the homepage, making it inconvenient. My suggestion is to set <code>https://hn.algolia.com/?q=%s</code> (where <code>%s</code> is the search term) as a search engine shortcut or in launcher tools for direct access. Indeed, this might be the best site search you’ve ever used. Its domain reveals it’s an “add-on,” powered and hosted by search SaaS provider <a href="https://algolia.com/">Algolia</a> (a YC alum); it’s not only blazingly fast but also supports fuzzy matching and can unearth discussions from the deepest corners of the site. (See the <a href="https://hn.algolia.com/help">help page</a> for advanced syntax.)</p>
<p>You can also install a browser extension like <a href="https://github.com/benwinding/newsit">Newsit</a>, which automatically checks if the current webpage has related HN discussions and displays a banner notification.</p>

<p>Hot HN posts often attract hundreds or even thousands of comments; reading them all is neither feasible nor necessary. Also, due to natural bandwagon effects, top comments attract more replies, dominating the top of the thread. Reading straight down might miss different perspectives buried later.</p>
<p>Therefore, I follow a personal rule: for the first top-level comment, I read at most the first three replies and their first three sub-replies. Then I move to the second top-level comment and read its first two replies and their first two sub-replies. Finally, I read the first reply to the third top-level comment and its first sub-reply. (Remember to make good use of the navigation links.) This usually provides a comprehensive overview of the thread’s viewpoints while keeping reading time manageable.</p>

<p>With the rise of AI tools, summarizing HN comments has become a viable option. However, for popular threads with hundreds of comments, HN paginates the results, so summarizing only the first page would be incomplete. To get around this, you can fetch the full comment data in JSON format from the HN API using this endpoint:</p>
<pre><code>https://hn.algolia.com/api/v1/items/${id}
</code></pre>
<p>Here, <code>${id}</code> is the eight-digit number from the submission’s URL. You can then feed the entire JSON response to your preferred AI model with a prompt like this:</p>
<blockquote>
<p>Summarize the themes of the opinions in the input provided by the user. For each theme, include at least 3 UNMODIFIED quotes with attribution. Unescape HTML entities. Go long.</p></blockquote>
<p>This can be a one-off prompt or set as a system prompt. The prompt design, inspired by Simon Willison’s <a href="https://til.simonwillison.net/llms/claude-hacker-news-themes">work</a> and adjusted based on personal experience, reliably summarizes the themes and stances within the comments, complete with original quotes and usernames for easy reference. Since this is a summarization task, cost-effective models like GPT mini, Gemini Flash, or Claude Haiku are perfectly adequate. Just be sure to use a model with a large context window to avoid truncation.</p>
<p>I’ve created a <a href="https://platyhsu-hnsum.web.val.run/">demo</a> on Val Town using their free GPT-4o mini proxy. You can try it out and then <a href="https://www.val.town/v/platyhsu/hnsum">fork the code</a> to your own account to customize it and avoid rate limits.</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/22924777f37f54e8e34d9180354e835c.png">
</figure>
<h2 id="中文版">中文版</h2>
<h2 id="永恒的九月有救吗">「永恒的九月」有救吗？</h2>
<p>1994 年初，在类似日后论坛的在线社区 Usenet 上，一群满腹恼火的用户无意间创造了一个术语——<a href="http://www.catb.org/jargon/html/S/September-that-never-ended.html">永恒的九月</a>（Eternal September）。</p>
<p>不过，让人恼火的不是九月本身，而是九月出现的人。早期的 Usenet 访问门槛比较高，用户素质和内容质量相对容易维持。但每年秋季开学，都有一批大学新生通过校园网涌进 Usenet，四处乱发东西却又不守「规矩」，让老用户们烦恼不已。只是多年下来，大家也多少习惯了这个事实。</p>
<p>1994 年的情况又有些不同。从前一年开始，许多面向大众的互联网服务商也陆续提供了 Usenet 接入服务。这样一来，全年都有来自零基础用户的低质、跑题帖子占据社区——九月的混乱成为了永恒。</p>
<p>「永恒的九月」象征着互联网早期精英主义时代的结束，也代表着在线社区一个永恒的难题：用户规模、主题范围和讨论质量构成了三难困境，这三个目标的重合处大多时候写着「做梦」。</p>
<p>但确实有这样一个社区，在它十七年的历史中，不仅用户和流量持续增长，而且总体上保持了丰富有趣的话题和标杆性的讨论质量。这就是「黑客新闻」——<a href="https://news.ycombinator.com/">Hacker News</a>。</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/2af1f4f037784c49dcdff7f9d9bf3353.png">
<figcaption>Hacker News 首页</figcaption>
</figure>
<h2 id="文本墙里砌出的罕见人气">文本墙里砌出的罕见人气</h2>
<p>第一眼望去，HN 并不是一个吸引人的网站：界面素面朝天，除了字还是字，连功能按钮都主要是文本链接。不仅如此，初来乍到的人可能都不知道这里到底是怎么发帖的。与常见的在线社区不同，HN 上绝大多数「帖子」都只是一个链接分享，「楼主」的创作（如果有）只是起一个标题、加两句点评而已，而回复也是针对分享内容的讨论。</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/9a2376461d54d8ba85058836e609e2aa.png">
<figcaption>Hacker News 讨论区</figcaption>
</figure>
<p>换句话说，HN 与其说是一个论坛，不如说是一个集体筛选的推荐列表，一个互联网的外置评论区。这种形态的学名是「链接聚合站」（link aggregator），除了 HN，早年比较有名的例子还包括 <a href="https://en.wikipedia.org/wiki/Digg">DIGG</a> 和 Reddit。但 DIGG 早已作古，Reddit 也逐渐转型为更「主流」的论坛模式，HN 也就越发显得独树一帜了。</p>
<p>就是这样一个看起来平平无奇、用起来颇有门槛的网站，却坐拥超过千万的月访问量（SimilarWeb 数据），比知名的科技新闻网站 <em>TechCrunch</em> 和 <em>Engadget</em> 都高出很多。（与这种规模的流量形成对照的是，用于托管 HN 的<a href="https://news.ycombinator.com/item?id=16076041">服务器</a>相当朴素，仅仅是两台四核的 Intel Xeon CPU E5-2637 v4 服务器，运行 FreeBSD 系统。）</p>
<p>要理解 HN 的高人气，就得先了解一些历史。从域名 <code>news.ycombinator.com</code> 就能看出，HN 的起源并不是一个独立运营的网站，而是硅谷知名风投机构 Y Combinator 的附属项目。2007 年 2 月，时任 YC 总裁的 Paul Graham 创办了 HN。根据当时的<a href="https://news.ycombinator.com/announcingnews.html">公告</a>，为公，他想为创业圈提供一个交流场所（这也是为什么 HN 最开始叫 <em>Startup News</em>），方便 YC 网罗人才；为私，他也想过一把编程瘾，用自己参与创作的 Lisp 语言变种 <a href="https://paulgraham.com/arc0.html">Arc</a> 写一个网站。</p>
<p>在这样的背景衬托下，HN 逐渐成为了硅谷创业者和科技行业从业者的集散地。对于创业公司，HN 是一个推介产品、聆听反馈的优质渠道，也是在「危机公关」时需要格外小心对待的舆论场。对于独立开发者、创作者，自己的作品被「顶」上 HN 首页不仅是一种肯定，而且也能带来实打实的流量——这甚至产生了一个专有名词「HN 死亡拥抱」（HN hug-of-death），形容 HN 来客对小网站的性能考验。（我有一篇文章只是短暂蹭上了六十几名，结果几小时内 BackBlaze B2 图床就被拖完了当月额度。）</p>
<p>但 HN 上的讨论并不只和开公司和写代码的人有关。根据 Wilson Lin 基于四千多万条帖子和评论的<a href="https://blog.wilsonl.in/hackerverse/">分析</a>，除了创业和编程之外，消费级产品、数理化等基础科学学科，以至社会、人文等「文科」内容，在 HN 上都有广泛讨论，也能经常见到相关背景的专业人士发言。不夸张地说，无论你处于什么领域、关心什么话题，都有很大概率在 HN 上找到你感兴趣的讨论。</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/64b97cd1324120b919f75c17df316d52.png">
<figcaption>HackerNews 话题视觉化统计（Wilson Lin）</figcaption>
</figure>
<h2 id="纪律严明的首页与鞠躬尽瘁的管理员">纪律严明的首页与鞠躬尽瘁的管理员</h2>
<p>用户规模有了、讨论的话题也足够丰富，但这还不足以成就一个好的社区。回忆历史，很多大型论坛就是在达到一定的规模后，遇到了严重的灌水和极端化问题，最后走向衰落。HN 是如何做到维持内容质量和讨论氛围的呢？</p>
<p>一套好的规则功不可没。在 HN 的<a href="https://news.ycombinator.com/newswelcome.html">欢迎页面</a>上，写着这个社区的最重要的「约法两章」：第一，不要发垃圾链接，看到也不要点赞；第二，写评论不要粗鲁，也不要犯傻。</p>
<p>什么链接才不「垃圾」？答案是「有趣但不肤浅」：有助于增进对世界的了解，而不是八卦、表情包、引战文章、标题党新闻等喧闹的噪音——在 HN 的语汇中称为「无关话题」（off-topic）。什么样的评论才不「粗鲁」「犯傻」？它应当提供新的角度或者信息，「不要说你当面沟通时说不出口的话」。</p>
<p>如果你觉得这还是有些抽象，HN 还有一份更完整的<a href="https://news.ycombinator.com/newsguidelines.html">发帖规范</a>，对于内容、格式以至于表达方式提出了更具体的要求：不要用大写字母和感叹号来吸引眼球；尽量使用原始来源；不可以在回复观点时夹带人身攻击；在解读评论时推定他人为善意；等等。归根结底，这些原则和规则的目的都是保证 HN 上的内容能「让优秀的黑客感兴趣」，也就是「满足好奇心」。</p>
<p>当然，只有规则是不够的，还要有执行规则的手段。为此，HN 将程序规则和人工管理两种手段结合起来，其机制都颇值得研究。</p>
<p>首先，在决定着第一观感的帖子排序上，HN 不是简单地根据时间远近、互动多少，而是设置了非常严格的门槛。帖子在刚发出时只会出现在「新帖」版块，具有 1 分的初始分。只有在获得 4 次「支持」（通过点击帖子标题左侧的向上箭头），也就是积累 5 分后，才有资格进入首页排序。对于达到分数门槛的帖子，HN 按照获得分数和提交距今时间的比值来排序，只有排在前 30 名的帖子才能登上真正意义上的「首页」——直接访问 HN 网址所能看到的列表。剩下的帖子就只能排到后续页面了。</p>
<p>（作为例外，管理员有「特权」给低人气的帖子「<a href="https://news.ycombinator.com/item?id=11662380">第二次机会</a>」：如果管理员觉得某个帖子似乎被「埋没」了，可以手动把它放回首页的底部，但不会更高，从而在不过度干预规则的情况下让更多人有机会看到。）</p>
<p>但众所周知，票数是可以刷的。因此，HN 一直将反刷票检测作为<a href="https://news.ycombinator.com/item?id=22761897">优先事项</a>，持续开发改进。出于可以理解的原因，反刷票的具体机制没有公布过，但不难推断其考虑因素可能包括跳转来源、注册时间、操作频率等。</p>
<p>获得支持票除了可以让帖子排名靠前，也可以为发帖用户积累「业力」（karma）。这借用自一个佛教术语，在 HN 中大致类似于其他社区中的用户积分。不过，积分在 HN 中的作用不是提升花里胡哨的用户等级，而是参与社区治理的资历凭证：达到 31 分的用户可以标记（flag）自己认为不符合社区规则的帖子和评论，被多人标记的内容会被打上（flagged）的警告标记、直至隐藏；而只有达到 501 分的用户才能对他人评论投反对票（downvote）。（一个彩蛋功能是达到 251 分的用户可以自定义导航栏主题色。）</p>
<p>应当说，由于这些门槛，融入 HN 的难度高到会将很多人拒之门外的程度。不难想见，愿意主动逛新帖版块的本来就是重度用户，眼光往往挑剔；一个帖子想吸引到四个这类用户的支持，从而获得首页展示资格，实非易事。的确，<a href="https://github.com/minimaxir/hn-heatmaps">据统计</a>，HN 上的帖子大多数都只能得到 0 或 1 票。因此，相当比例的用户注册多年都没有发出过一次达到「逃逸速度」的帖子，也攒不到解锁 flag 功能的 karma 分数。但可能正是因为宁可牺牲互动量也要坚持高标准，HN 才能在十几年来维持独特的水平和风格。</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/3ea1e606a76b97bb28b381e0751c8b4d.png">
<figcaption>HN 帖子得分中值统计（Max Woolf）</figcaption>
</figure>
<p>但上面那些程序规则也不能算是 HN 维持高质量最独特的「法宝」；这个荣誉还得归于 HN 运营机制中「人治」的部分，特别是常任版主（moderator）的 dang。</p>
<p>《纽约客》杂志曾在 2019 年对 dang 做过<a href="https://www.newyorker.com/news/letter-from-silicon-valley/the-lonely-work-of-moderating-hacker-news">特写报道</a>。他本名 Daniel Gackle，缩写一下就是 dang。这位斯坦福文学专业毕业生成为 HN 的版主纯属意外。他曾经与另一位前任版主 scott（Scott Bell）共同创业，开发在线电子表格产品，并获得过 YC 的投资。遗憾的是，dang 的创业最终未获成功，于是接受 Paul Graham 的邀请加入 YC，全职管理 HN。</p>
<p>用《纽约客》文章的话说，dang 的管理是「个人色彩浓厚、专注、慢节奏」的；他将自己的工作视作一种「对话」。你可以翻几页 <a href="https://news.ycombinator.com/threads?id=dang">dang 的回复记录</a>来了解他的工作内容：<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=author%3Adang+merge&amp;sort=byDate&amp;type=all">合并</a>重复主题、<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=author%3Adang+%22Related.+Others%3F%22&amp;sort=byDate&amp;type=all">汇总</a>过往类似讨论、<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=author%3Adang+%22changed+the+title%22&amp;sort=byDate&amp;type=all">修正</a>帖子标题措辞和来源链接、<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=author%3Adang+%22newsguidelines%22&amp;sort=byDate&amp;type=all">提醒</a>「上头」用户遵守社区规则。这些任务单独看起来可能也不复杂，但要保持像 dang 一样的高频、准确，又始终温和、耐心——包括私下和用户发送长篇邮件解释操作理由——就很难得了。</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/8adbef158df080b186cfd6b32be14dbd.png">
<figcaption>dang 的工作痕迹</figcaption>
</figure>
<p>正因如此，dang 受到了用户的一致尊重，以至于每到「逢年过节」都会有人自发点名感谢他的贡献（不妨试试在 HN 站内<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=thank+you+dang&amp;sort=byPopularity&amp;type=all">搜索 <em>thank you dang</em></a>）。诚然，HN 用户的评论也或许带有一些玫瑰色眼镜，但说很难在当今互联网上见到另一位受到如此爱戴的版主，大概不是夸张。</p>
<p>（2025 年初，dang <a href="https://news.ycombinator.com/item?id=43558671">宣布</a> Tom Howard (tomhow) 加入正式管理员队伍。此前多年，他一直在幕后执行版主职责。据 dang 介绍，Howard 曾是 Y Combinator 的 2009 年冬令营校友，于 2007 年加入 HN。）</p>
<h2 id="兼听则明">兼听则明</h2>
<p>当然，HN 也不是完美无缺的。再优质的在线社区毕竟也是一个……在线社区；人们容易在线上沟通时犯的错误——急于结论、言辞偏激、过于自信——同样见于 HN 上的沟通中。同时，用户特征和文化使然，HN 还有一些「特色问题」，值得在浏览时留心鉴别。</p>
<p>例如，一个特别常见的现象是根本不看楼主分享的链接，只根据标题唤起的第一印象置评，导致聊起一些南辕北辙的话题，或者重提原文中已经明确回答的问题。以 <em>RTFA</em>（妈的去看文章，read the fucking article）为关键词<a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=%22rtfa%22&amp;sort=byDate&amp;type=comment">搜索评论</a>，就能看到成百上千条对于这种做法的抱怨。对此，最好自己养成良好的习惯，先看原文（赶时间的话哪怕看看 AI 总结）再看评论，就能有效避免被「张口就来」的评论带偏。</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/53a5d794435b18980cf9bb22499c0384.png">
<figcaption>「妈的，看看文章」</figcaption>
</figure>
<p>另一个「HN 特色」是为批评而批评。究其原因，虽然批判性思维在 HN 上受到推崇，并且在多数时候能起到火眼金睛的正面效果，但有些时候也会演变为「挑刺」。最常见的场景就是对于讲述科技成就、业务成功的帖子「泼冷水」，以及对 <a href="https://news.ycombinator.com/show">Show HN 版块</a>中毛遂自荐的产品提出一些不符合项目发展阶段的苛责。（以 <em>so much negative</em> 为关键词<a href="https://hn.algolia.com/?dateRange=all&amp;prefix=true&amp;query=%22so+much+negative%22&amp;sort=byDate&amp;type=comment">搜索评论</a>可以看到很多案例。）</p>
<p>一些常驻版面的「低效话题」也在拉低 HN 的整体氛围。之所以说「低效」，是因为这些问题虽然总能引发「热议」，但内容往往在争强好胜地表达个人偏好甚至偏见，或者陷入次要技术细节的迂腐争论，因此很难从中得到收获。不难想象，这些低效话题自然会包括技术圈一些历久经年的「圣战」——争论编程语言、操作系统、编辑器哪家强；近年新增的一些「时事热点」还包括对各种新兴「邪教」——Rust、HTMLX、nix、Wayland——的讨伐，以及重返办公室、裁员、技术移民等攸关科技从业者的社会政策问题的争论等等。如果遇到这类话题，查阅维基百科、技术文档和更专业权威的媒体可能是更好的选择。</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/077a9dd251d23351d63d26c37c84a854.png">
<figcaption>围绕「nix 布道士」的争论</figcaption>
</figure>
<p>最后，从用户画像的角度看，HN 的主力用户群体是美国的科技行业从业者，虽然整体素质和技能水平较高，但也因此容易滑向精英主义和过度的理性主义、智识主义，并在一定程度上构成观点的「回音壁」。因此，看似针锋相对的评论「盖楼」可能也只能得出局部最优的结论，形式上有条有理、数据驱动的论述可能掩盖着方法论层面的简化和偏见。正如《纽约客》那篇报道所总结，HN 有一种「掉书袋」（performative erudition）的基调，而它往往掩饰着一种深层的鲁莽。</p>
<p>总之，将 HN 定位为「互联网的外置评论区」有一层隐含意思：它不能代表和涵盖整个互联网，更不能代替和免去自己的思考和探索。HN 上的讨论虽好，充其量可以作为发现新问题的地图、解锁新视角的窗户，但完整的景观，毕竟还在远方和窗外。</p>
<h2 id="附hn-阅读方法谈">附：HN 阅读方法谈</h2>
<p>我大致在 2018 年初开始阅读 HN。由于没有理工和编程背景，HN 上的很多讨论超出了我的知识范围，但这并不影响翻阅 HN 的乐趣。事实上，我就是通过 HN 对很多技术话题建立初步认识和兴趣的；每当遇到众口纷纭的热点时，我也会习惯性地去 HN 寻找专业解读和正反交锋，几乎从未空手而归。</p>
<p>但面对繁多的条目和密集的讨论，「读」好 HN 也需要一些技巧。根据我的使用体会，HN 其实是不太适合直接去「刷」首页的，而最好通过 RSS、搜索和第三方工具实现有规律、有目的地浏览。</p>
<p>刚夸了这么久 HN 的首页质量，现在又不建议看首页，似乎有些矛盾。的确，通过上面介绍的各种管理机制，HN 的首页可以说是非常「好看」的。但太好看也会成为一种问题：如果不加控制，很容易将其作为消磨时间的下意识目的地，陷入无尽的链接点击循环。（HN 因此也是很多「集中注意力」类工具推荐用户主动屏蔽的网站之一。）</p>
<p>相比之下，我更推荐以下几种工具和方法，供读者参考和批评——</p>

<p>HN 有一个官方的 RSS 地址（<a href="https://news.ycombinator.com/rss">https://news.ycombinator.com/rss</a>），与首页内容完全一致，直接订阅信息量太大。好在 HN 足够开放，提供了非常完善的官方 API，这就为第三方制作更加细化的 RSS 源提供了可能。</p>
<p>例如，一个比较受欢迎的选择是 <a href="https://hnrss.github.io/">hnrss.org</a>，它提供了按照版块、用户、关键词、评分数等条件筛选的一系列 RSS 地址。其中，最实用的大概要数「<a href="https://hnrss.org/bestcomments">最佳评论</a>」。这个源汇总了 HN 全站主题中新出现的高票评论，不仅本身值得一读，而且会引出精彩评论的帖子本身往往也是值得一读、有一定人气的，我经常从中发现一些日常关注范围之外的优质讨论。它的更新频率也比较适中，一般每天更新十几条，对应四五篇文章，数量适中，大多数人一天读到这个数量也就差不多了。</p>
<h3 id="主动搜索外部网址">主动搜索外部网址</h3>
<p>前面提到过，HN 特殊的发帖方式使它成为了「互联网的外置评论区」。再加上人气旺盛，英文互联网上但凡稍有些访问量的网站和页面，都有很大可能在 HN 上有所讨论。</p>
<p>对我来说，HN 搜索就是技术领域的重要咨询意见来源：每当看到一个风头正旺、宣传遍地的产品，我一般都会在 HN 上搜索它的名称、官网域名或者 GitHub 仓库地址，看看到底是真的不容错过，还是需要「避雷」。类似地，每当看到一篇言之凿凿的热门文章，我也会搜搜 HN 上有没有「唱反调」的声音，从而获得更全面的角度。</p>
<p>不过，HN 的搜索框位于首页底部的不起眼位置，用起来比较麻烦。我的建议是将 <code>https://hn.algolia.com/?q=%s</code>（其中 <code>%s</code> 为关键词）设置为搜索引擎或各类 launcher 工具中的搜索快捷方式来直达搜索。你也可以装一个浏览器插件 <a href="https://github.com/benwinding/newsit">Newsit</a>，它会自动搜索每一个访问的网页是否有相关 HN 讨论，并以横幅形式显示在网页的右下角。</p>
<p>顺带一提，这可能是你见过最好用的站内搜索引擎。从它的域名就可以看出来这是个「外挂」，是由知名的搜索 SaaS 提供商、也是 YC 往届校友项目的 <a href="https://algolia.com/">Algolia</a> 支持和托管，不仅速度快到冒烟，而且支持模糊匹配，可以搜出各种犄角旮旯。（更多高级语法见<a href="https://hn.algolia.com/help">帮助页</a>。）</p>
<h3 id="跳读评论">跳读评论</h3>
<p>HN 上的热门帖子往往能引来几百以至上千条评论，逐一看完显然不现实也没有必要。此外，由于用户互动有「凑热闹」的自然倾向，位于评论区顶部的热门评论往往能吸引更多的评论，从而占据越来越多的顶部空间。如果只是从头往下看，很可能因此忽视位于后面的不同视角声音。</p>
<p>因此，我给自己定的「规矩」是：对于第一条评论，最多看前三条回复，及其各自的三条下级回复；然后就跳到第二条评论，最多看前两条回复，及其各自的两条下级回复；最后跳到第三条评论看第一条回复，及其第一条下级回复。（注意善用每层楼的导航按钮 root（跳到所属的最上层回复）、parent（跳到所属的上一层回复）和 prev/next（跳到同层的相邻上/下一条回复）。）这样，一般能比较全面地了解评论区的综合观点，同时使得阅读时间可控。</p>
<h3 id="ai-总结评论">AI 总结评论</h3>
<p>当然，随着 AI 工具普及，也可以考虑用 AI 工具总结 HN 评论。不过，对于那种成百上千条评论的热门话题，HN 会自动分页显示，此时只总结第一页就不完整了。为此，可以从 HN API 获取 JSON 格式的完整评论数据，端点为：</p>
<pre><code>https://hn.algolia.com/api/v1/items/${id}
</code></pre>
<p>其中 <code>id</code> 为链接中的八位数字。然后将响应内容作为提示词，和如下内容一起发送给惯用的模型即可：</p>
<blockquote>
<p>Summarize the themes of the opinions in the input provided by the user. For each theme, include at least 3 UNMODIFIED quotes with attribution. Unescape HTML entities. Go long.</p></blockquote>
<p>这段话可以直接放在开头，也可以作为系统提示词。提示词的写法受到了 Simon Willison 的<a href="https://til.simonwillison.net/llms/claude-hacker-news-themes">启发</a>，根据个人经验调整，可以比较稳定地总结评论主题、立场，并附带原始引用和用户名，方便回溯到原评论。因为只是总结类任务，GPT-4o mini、Gemini Flash 和 Claude Haiku 这样的便宜模型就能很好胜任，但注意上下文窗口越长越好，以免超出长度限制。</p>
<p>我用 Val Town 做了一个<a href="https://platyhsu-hnsum.web.val.run/">演示版</a>，使用的是该服务免费提供的 GPT-4o mini 模型代理，你可以试试效果，然后 <a href="https://www.val.town/v/platyhsu/hnsum">fork</a> 一份到自己账户以便按需修改和避免限流。</p>
<figure>
<img src="https://static.hsu.cy/blog/2025/22924777f37f54e8e34d9180354e835c.png">
</figure>
<p><em>A version of this article <a href="https://sspai.com/prime/story/hacker-news">appears</a> on Dec. 30, 2024 on SSPAI.</em></p>

		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LinkedIn will soon train AI models with data from European users (147 pts)]]></title>
            <link>https://hostvix.com/linkedin-will-soon-train-ai-models-with-data-from-european-users/</link>
            <guid>45331009</guid>
            <pubDate>Mon, 22 Sep 2025 09:33:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hostvix.com/linkedin-will-soon-train-ai-models-with-data-from-european-users/">https://hostvix.com/linkedin-will-soon-train-ai-models-with-data-from-european-users/</a>, See on <a href="https://news.ycombinator.com/item?id=45331009">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-611">

	<div>

					
<p>LinkedIn is preparing to switch on generative-AI training that draws from European members’ data, setting <a href="https://www.linkedin.com/help/linkedin/answer/a8059228" target="_blank" rel="noreferrer noopener nofollow">November 3, 2025</a> as the go-live date. The company says it will rely on “legitimate interests” as its legal basis and will offer an opt-out so members can refuse use of their data for training—promising that private messages are excluded. The change applies across the EU/EEA, United Kingdom, and Switzerland.</p>



<figure><img loading="lazy" decoding="async" width="790" height="237" src="https://hostvix.com/wp-content/uploads/2025/09/linkedin-AI-update-notice-tos.png" alt="linkedin AI update notice tos" srcset="https://hostvix.com/wp-content/uploads/2025/09/linkedin-AI-update-notice-tos.png 790w, https://hostvix.com/wp-content/uploads/2025/09/linkedin-AI-update-notice-tos-300x90.png 300w, https://hostvix.com/wp-content/uploads/2025/09/linkedin-AI-update-notice-tos-768x230.png 768w, https://hostvix.com/wp-content/uploads/2025/09/linkedin-AI-update-notice-tos-440x132.png 440w, https://hostvix.com/wp-content/uploads/2025/09/linkedin-AI-update-notice-tos-320x96.png 320w" sizes="auto, (max-width: 790px) 100vw, 790px"></figure>



<p>This is a pivot from last year’s pause. In September 2024, after criticism and scrutiny, LinkedIn <a href="https://www.linkedin.com/blog/member/trust-and-safety/updates-to-our-terms-of-service-2024" target="_blank" rel="noreferrer noopener nofollow">suspended plans</a> to train models on data from EU &amp; U.K. users.</p>



<p>What exactly will be used? LinkedIn’s help center explains that profile details and public content (posts, articles, comments) can feed content-generating models. The company also points to saved résumés and job-application answers as potential inputs to features that help members get discovered by recruiters, and it says it screens out minors it believes are in secondary school. It stresses that <strong>private messages</strong> are not part of the training set. The togglable setting appears as “Data for Generative AI Improvement,” and turning it off stops future use of your data for training, though it doesn’t unwind models already trained.</p>



<p>There’s a parallel—but separate—data-sharing change with Microsoft, LinkedIn’s parent. Beginning November 3, LinkedIn will expand, in some regions, <a href="https://www.linkedin.com/help/linkedin/answer/a1397243" target="_blank" rel="noreferrer noopener nofollow">the types of LinkedIn data shared with Microsoft</a> so Microsoft can personalize ads. LinkedIn frames this as an “Affiliates” update; it says it will provide an additional opt-out where relevant. The two changes arrive together in the same terms refresh but follow different regional rules.</p>



<blockquote>
<p>Outside EU/EEA/Switzerland and the UK, LinkedIn shares your information with our affiliate, Microsoft, for advertising purposes and with select partners for purposes of marketing LinkedIn’s products and services.</p>
</blockquote>



<p>For European members, the legal hinge is GDPR’s “legitimate interests.” That route allows processing without consent if a company can show necessity, transparency, and an effective right to object. LinkedIn’s <a href="https://www.linkedin.com/legal/privacy/eu" target="_blank" rel="noreferrer noopener nofollow">European Regional Privacy Notice</a> update states that legitimate interests will be the basis for training content-generating models starting November 3, and that an objection (opt-out) will be honored via account settings.</p>



<p>Why now? LinkedIn has steadily added AI-assisted features—writing suggestions, profile summaries, recruiter tools—that benefit from domain-specific training. After the 2024 backlash (and <a href="https://www.reuters.com/legal/microsofts-linkedin-sued-disclosing-customer-information-train-ai-models-2025-01-22/" target="_blank" rel="noreferrer noopener nofollow">a short-lived lawsuit in the U.S.</a> that centered on fears about private messages—claims LinkedIn denied and that were later withdrawn), the company has come back with region-specific notices, a single, named training setting, and clearer language that private messages aren’t in scope.</p>



<p>Practically, here’s what changes on (or around) November 3 in Europe: if you do nothing, LinkedIn may use parts of your profile and public activity to refine the models behind its content-generation features. If you object in settings, that use should stop for future training. Your ability to <strong>use</strong> AI features doesn’t depend on opting in. In parallel, some—but not all—regions will see broader ad-personalization data sharing with Microsoft, governed by a separate control. Expect some differences in labels and availability by market; LinkedIn says not all features and settings roll out worldwide at once.</p>



<figure><img loading="lazy" decoding="async" width="787" height="299" src="https://hostvix.com/wp-content/uploads/2025/09/Data-for-Generative-AI-Improvement.png" alt="Data for Generative AI Improvement
" srcset="https://hostvix.com/wp-content/uploads/2025/09/Data-for-Generative-AI-Improvement.png 787w, https://hostvix.com/wp-content/uploads/2025/09/Data-for-Generative-AI-Improvement-300x114.png 300w, https://hostvix.com/wp-content/uploads/2025/09/Data-for-Generative-AI-Improvement-768x292.png 768w, https://hostvix.com/wp-content/uploads/2025/09/Data-for-Generative-AI-Improvement-440x167.png 440w, https://hostvix.com/wp-content/uploads/2025/09/Data-for-Generative-AI-Improvement-320x122.png 320w" sizes="auto, (max-width: 787px) 100vw, 787px"></figure>



<p>If you want out, the setting to look for is “Data for Generative AI Improvement” in <strong>Settings → Data privacy</strong>. The key is that the toggle affects <strong>future</strong> training, not models already built. Keep an eye out for any region-specific notices in your account banner over the next few weeks as the company aligns the U.K. with the EU schedule.</p>

				</div><!-- .post-inner -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SGI demos from long ago in the browser via WASM (219 pts)]]></title>
            <link>https://github.com/sgi-demos</link>
            <guid>45330407</guid>
            <pubDate>Mon, 22 Sep 2025 08:03:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sgi-demos">https://github.com/sgi-demos</a>, See on <a href="https://news.ycombinator.com/item?id=45330407">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemprop="text"><p dir="auto"><h2 dir="auto">SGI demos from long ago, running in your browser <a href="https://sgi-demos.github.io/" rel="nofollow">today</a>.</h2><a id="user-content-sgi-demos-from-long-ago-running-in-your-browser-today" aria-label="Permalink: SGI demos from long ago, running in your browser today." href="#sgi-demos-from-long-ago-running-in-your-browser-today"></a></p>
<p dir="auto"><em>Old problems require modern solutions.</em></p>
<p dir="auto">This is the original SGI demo source code, compiled for the web using Emscripten and SDL2.  Rendering is done using an IRIS GL software rasterizer from the <a href="https://lkesteloot.github.io/alice/alice4/libgl.html" rel="nofollow">Alice 4 project</a>.  Event handling is done by SDL2, with events translated into GL's event system.  Each demo is a separate web page, with its own Javascript + WASM compiled by Emscripten from the original C/C++ source.  Minimal modifications have been made to the original source, in order to run in the browser and to work with compilers 35 years newer.</p>
<p dir="auto"><h2 dir="auto">Working demos</h2><a id="user-content-working-demos" aria-label="Permalink: Working demos" href="#working-demos"></a></p>
<ul dir="auto">
<li><a href="https://sgi-demos.github.io/" rel="nofollow">Buttonfly</a></li>
<li><a href="https://sgi-demos.github.io/sgi-demos/bounce/web/bounce_full.html" rel="nofollow">Bounce</a></li>
<li><a href="https://sgi-demos.github.io/sgi-demos/ideas/web/ideas_full.html" rel="nofollow">Ideas</a></li>
<li><a href="https://sgi-demos.github.io/sgi-demos/insect/web/insect_full.html" rel="nofollow">Insect</a></li>
<li><a href="https://sgi-demos.github.io/sgi-demos/jello/web/jello_full.html" rel="nofollow">Jello</a></li>
<li><a href="https://sgi-demos.github.io/sgi-demos/logo/web/logo_full.html" rel="nofollow">Logo</a></li>
<li><a href="https://sgi-demos.github.io/sgi-demos/twilight/web/twilight_full.html" rel="nofollow">Twilight</a></li>
</ul>
<p dir="auto"><h2 dir="auto">Somewhat working demos</h2><a id="user-content-somewhat-working-demos" aria-label="Permalink: Somewhat working demos" href="#somewhat-working-demos"></a></p>
<ul dir="auto">
<li><a href="https://sgi-demos.github.io/sgi-demos/flight/web/flight_full.html" rel="nofollow">Flight</a> (cockpit glitches, planes too slow in web version, night mode 'shimmers', no network play)</li>
<li><a href="https://sgi-demos.github.io/sgi-demos/newave/web/newave_full.html" rel="nofollow">Newave</a> (no mesh editing, no popup menus, only wireframe)</li>
<li><a href="https://sgi-demos.github.io/sgi-demos/arena/web/arena_full.html" rel="nofollow">Arena</a> (no network play)</li>
</ul>
<p dir="auto"><h2 dir="auto">Build instructions</h2><a id="user-content-build-instructions" aria-label="Permalink: Build instructions" href="#build-instructions"></a></p>
<p dir="auto"><h3 dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>
<ol dir="auto">
<li>Install <a href="https://brew.sh/" rel="nofollow">Homebrew</a> if you don't have it, then get SDL2 and Emscripten:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="brew install SDL2
brew install emscripten"><pre>brew install SDL2
brew install emscripten</pre></div>
<ol start="2" dir="auto">
<li>Build:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/sgi-demos/sgi-demos.git
cd sgi-demos
make"><pre>git clone https://github.com/sgi-demos/sgi-demos.git
<span>cd</span> sgi-demos
make</pre></div>
<p dir="auto"><h3 dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install <a href="https://learn.microsoft.com/en-us/windows/package-manager/winget/" rel="nofollow">Winget</a> if you don't have it.</p>
</li>
<li>
<p dir="auto">Install <a href="https://www.msys2.org/" rel="nofollow">MSYS2</a> from cmd.exe, in order to get the clang compiler:</p>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="winget install MSYS2.MSYS2
setx PATH &quot;%PATH%C:\msys64\clang64\bin&quot;"><pre>winget install MSYS2.MSYS2
setx PATH <span><span>"</span>%PATH%C:\msys64\clang64\bin<span>"</span></span></pre></div>
<ol start="3" dir="auto">
<li>Install clang toolchain and SDL2 in MSYS2 CLANG64 shell:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="pacman -Syu
pacman -S base-devel mingw-w64-clang-x86_64-toolchain
pacman -S mingw-w64-clang-x86_64-SDL2"><pre>pacman -Syu
pacman -S base-devel mingw-w64-clang-x86_64-toolchain
pacman -S mingw-w64-clang-x86_64-SDL2</pre></div>
<ol start="4" dir="auto">
<li>
<p dir="auto">Clone <a href="https://emscripten.org/docs/getting_started/downloads.html" rel="nofollow">emscripten</a> from Github.  Cloning seems to work best with MSYS2 rather than using pacman.  Follow the default install directions, not the Windows directions!</p>
</li>
<li>
<p dir="auto">Add this line to the <code>~/.bashrc</code> file in MSYS2 CLANG64 shell:</p>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="source /path/to/emsdk/emsdk_env.sh"><pre><span>source</span> /path/to/emsdk/emsdk_env.sh</pre></div>
<ol start="6" dir="auto">
<li>Build:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/sgi-demos/sgi-demos.git
cd sgi-demos
make"><pre>git clone https://github.com/sgi-demos/sgi-demos.git
<span>cd</span> sgi-demos
make</pre></div>
<p dir="auto"><h2 dir="auto">To do</h2><a id="user-content-to-do" aria-label="Permalink: To do" href="#to-do"></a></p>
<ul dir="auto">
<li>Rendering via OpenGLES/WebGL (WIP)</li>
<li>Arbitrary window size</li>
<li>Run GL demo in its own WASM worker/thread, to avoid slicing up the code for SDL's event loop</li>
<li>Popup menus, including the classic SGI menu font</li>
<li>More demos, <em>all the demos</em>.
<ul dir="auto">
<li>Electropaint, Cedit, any other IRIS GL demos I can find</li>
<li>Then OpenGL, GLUT, Inventor, Performer demos in no particular order</li>
</ul>
</li>
<li>Rudimentary context for each demo: name, author, year (as text in lower corner), code link</li>
<li>Virtual mouse and keyboard:
<ul dir="auto">
<li>Only display virtual keys and mouse functions used by the demo; use demo's qdevice() calls to determine this</li>
<li>Displayed as transparent virtual mouse and key pictures overlaid on demo</li>
<li>On always for touch devices</li>
<li>On/off for mouse/keyboard devices, as hints</li>
</ul>
</li>
<li>Description/history/context for each demo - can obtain some descriptions from .Info slide files</li>
<li>Man page live links</li>
</ul>
<p dir="auto"><h2 dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Everyone who worked at <a href="https://en.wikipedia.org/wiki/Silicon_Graphics" rel="nofollow">SGI</a>, for the eye candy and the baller computers.</li>
<li>The <a href="https://lkesteloot.github.io/alice/alice4/" rel="nofollow">Alice 4 folks</a>, for the inspiration and the GL implementation.</li>
<li><a href="https://emscripten.org/" rel="nofollow">Emscripten</a> and <a href="https://www.libsdl.org/" rel="nofollow">SDL</a> teams, for making a web port possible.</li>
<li><a href="https://archive.org/search?query=sgi&amp;and%5B%5D=mediatype%3A%22software%22" rel="nofollow">Internet Archive</a>, <a href="https://bitsavers.org/bits/SGI/mips/cd/" rel="nofollow">Bitsavers</a>, <a href="https://winworldpc.com/search?q=irix" rel="nofollow">WinWorld</a>, <a href="https://irixnet.org/files.html" rel="nofollow">IRIXNet</a>, and others, for saving the history.</li>
</ul>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You did this with an AI and you do not understand what you're doing here (937 pts)]]></title>
            <link>https://hackerone.com/reports/3340109</link>
            <guid>45330378</guid>
            <pubDate>Mon, 22 Sep 2025 07:59:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackerone.com/reports/3340109">https://hackerone.com/reports/3340109</a>, See on <a href="https://news.ycombinator.com/item?id=45330378">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Download responsibly (280 pts)]]></title>
            <link>https://blog.geofabrik.de/index.php/2025/09/10/download-responsibly/</link>
            <guid>45329414</guid>
            <pubDate>Mon, 22 Sep 2025 05:33:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.geofabrik.de/index.php/2025/09/10/download-responsibly/">https://blog.geofabrik.de/index.php/2025/09/10/download-responsibly/</a>, See on <a href="https://news.ycombinator.com/item?id=45329414">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="gMain">
<h2><a href="https://blog.geofabrik.de/index.php/2025/09/10/download-responsibly/" rel="bookmark" title="Permanent Link to Download responsibly!">Download responsibly!</a></h2>
                        

<div>

   <p>10.09.2025 | Frederik Ramm </p>

   <div>
      <p>This month we’ve ramped up the infrastructure behind the download server, and downloads should now be available earlier<br>
and faster. There’s also a small technical change in that requests for a “…latest” file will now be answered with a<br>
HTTP redirect to the specific latest version (see previous blog post).</p>
<p>I would like to use this opportunity to appeal to users of the download server to “download responsibly”. We want<br>
to continue offering this service as powerful and as convenient as possible within our means. We want everyone<br>
to have easy access to the latest OSM data in a form that is useful to them.</p>
<p>Every now and then, people break things for others. There have been individual clients downloading the exact same<br>
20-GB file 100s of times per day, for several days in a row. (Just the other day, one user has managed to download almost 10,000 copies of the italy-latest.osm.pbf file in 24 hours!) Others download every single file we have on the<br>
server, every day. There’s a limit to the outgoing network bandwidth, and behaviour like this means that<br>
things are slowing down for everyone. Also, when we block an IP range for abuse, innocent third parties can be affected. </p>
<p>Here’s three concrete appeals to users of the download server:</p>
<p>1. If you want data for the whole planet, don’t download it piecemeal from us – simply get the planet file from planet.openstreetmap.org and you’re done!<br>
2. If you want a large region (like Europe or North America) updated daily, use the excellent pyosmium-up-to-date program which will automatically determine the age of your local file and update it by downloading the latest changes; this saves something like 98% of network traffic compared to a fresh download, and is faster.<br>
3. If you automate anything with regard to our download server, monitor what your script is doing or build in appropriate catches so that you don’t end up downloading the same file 1000 times just because your disk is full or something like that.</p>
<p>Happy downloading!</p>
   </div>

</div>

<!--
<div class="commentsblock">
      </div>
//-->






</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Privacy and Security Risks in the eSIM Ecosystem [pdf] (215 pts)]]></title>
            <link>https://www.usenix.org/system/files/usenixsecurity25-motallebighomi.pdf</link>
            <guid>45329127</guid>
            <pubDate>Mon, 22 Sep 2025 04:35:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.usenix.org/system/files/usenixsecurity25-motallebighomi.pdf">https://www.usenix.org/system/files/usenixsecurity25-motallebighomi.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45329127">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[South Korea's President says US investment demands would spark financial crisis (218 pts)]]></title>
            <link>https://www.cnbc.com/2025/09/21/south-koreas-president-lee-trump-investment-financial-crisis.html</link>
            <guid>45328579</guid>
            <pubDate>Mon, 22 Sep 2025 02:32:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/09/21/south-koreas-president-lee-trump-investment-financial-crisis.html">https://www.cnbc.com/2025/09/21/south-koreas-president-lee-trump-investment-financial-crisis.html</a>, See on <a href="https://news.ycombinator.com/item?id=45328579">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108190461" data-test="InlineImage"><p>South Korean President Lee Jae Myung speaks during a South Korea-U.S. business roundtable at The Willard Hotel in Washington, D.C., U.S., August 25, 2025. </p><p>Annabelle Gordon | Reuters</p></div><div><p>South Korea's economy could fall into crises rivaling its 1997 meltdown if the government accepts current U.S. demands in stalled trade talks without safeguards, President Lee Jae Myung told Reuters.</p><p>Seoul and Washington verbally agreed to a trade deal in July in which the U.S. would lower President <a href="https://www.cnbc.com/donald-trump/">Donald Trump's</a> tariffs on South Korean goods in exchange for $350 billion in investment from South Korea, among other measures.</p><p>They have yet to put the agreement on paper because of disputes over how the investments would be handled, Lee said.</p><p>"Without a currency swap, if we were to withdraw $350 billion in the manner that the U.S. is demanding and to invest this all in cash in the U.S., South Korea would face a situation as it had in the 1997 financial crisis," he said through a translator.</p><p>In an interview in his office on Friday, Lee also spoke about a huge U.S. immigration raid that detained hundreds of Koreans, as well as Seoul's relations with rival North Korea, neighboring giant China and Russia.</p><p>But trade and defense talks with the U.S., South Korea's military ally and a top economic partner, are overshadowing a trip Lee makes from Monday to New York, where he will address the United Nations General Assembly and be the first South Korean president to chair a meeting of the Security Council.</p></div><h2><a id="headline0"></a>Praises Trump handling of Hyundai raid </h2><div><p>Lee, a liberal, took office in a June snap election after his conservative predecessor, Yoon Suk Yeol, was removed from office and jailed for briefly imposing martial law. Lee has sought to calm the country and its economy and said he plans to use his U.S. visit to tell the world that "democratic Korea is back."</p></div><div id="ArticleBody-InlineImage-108196935" data-test="InlineImage"><p>Detainees are made to stand against a bus before being handcuffed, during a raid by federal agents where about 300 South Koreans were among 475 people arrested at the site of a $4.3 billion project by Hyundai Motor and LG Energy Solution to build batteries for electric cars in Ellabell, Georgia, U.S., September 4, 2025 in a still image taken from a video. </p><p>U.s. Immigration And Customs Enforcement | Via Reuters</p></div><div><p>Lee met Trump for their first summit in August, saying he had built a strong personal tie with the U.S. leader, despite not agreeing on a joint statement or concrete announcement.</p><p>This month Trump's administration rocked South Korea with the arrest of more than 300 South Korean workers at a Hyundai Motor battery plant in Georgia, with federal officials accusing them of immigration violations.</p><p>Lee said South Koreans were naturally angered by the "harsh" treatment of the workers - the Trump administration published images of them in shackles - and has warned it could make companies wary of investing in the United States.</p><p>But he said the raid would not undermine the bilateral alliance, praising Trump for offering to let the workers stay. Lee said he did not believe it was directed by Trump but was the result of overzealous law enforcement.</p><p>"I do not believe this was intentional, and the U.S. has apologized for this incident, and we have agreed to seek reasonable measures in this regard and we are working on them," he said.</p><p>Lee's office says there is no plan for him to meet Trump in New York and that the trade talks are not on the visit's agenda.</p></div><h2><a id="headline1"></a>Stumbling block in trade talks</h2><p>Commerce Secretary Howard Lutnick has said South Korea should follow Japan's deal with the United States. He said Seoul either needs to accept the deal or pay the tariffs, using the Trump administration's depiction of foreign governments paying the levies, which are instead paid by U.S. importers.</p><div id="ArticleBody-InlineImage-108190464" data-test="InlineImage"><p>U.S. Secretary of Commerce Howard Lutnick and South Korean President Lee Jae Myung participate in a South Korea-U.S. business roundtable at The Willard Hotel in Washington, D.C., U.S., August 25, 2025. </p><p>Annabelle Gordon | Reuters</p></div><div><p>Lee, asked if he would walk away from the deal, said: "I believe that between blood allies, we will be able to maintain the minimum amount of rationality."</p><p>South Korea has proposed a foreign exchange swap line with the U.S. to reduce the shock of the investments on the local market for the won currency. Lee did not address how likely the U.S. was to agree or whether that would be enough for the deal to go forward.</p><p>He said South Korea is different from Japan, which struck a trade deal with the U.S. in July. Tokyo has more than double South Korea's $410 billion foreign exchange reserves, an international currency in the yen and a swap line with the United States, Lee said.</p><p>Seoul and Washington have said in writing that any investment projects must be commercially viable, but working out the details is proving difficult, he said.</p><p>"Reaching detailed agreements that guarantee commercial reasonableness is now the central task - yet it also remains the biggest obstacle," Lee said. Proposals during working-level talks provide no assurance of commercial viability, making it hard to bridge the gap, he said.</p><p>Trump says the investments will be "selected" by him and controlled by the U.S., meaning Washington would have discretion over where the money will be invested.</p><p>But Lee policy adviser Kim Yong-beom said in July that South Korea had added a safety mechanism to reduce financing risk, including supporting commercially feasible projects rather than providing unconditional financial support.</p><p>Lee said South Korea and the United States do not disagree on increasing Seoul's contributions toward its own defence, bolstered by 28,500 U.S. troops on the Korean peninsula, but that Washington wants to keep security and trade talks separate.</p><p>"We should end this unstable situation as soon as possible," he said, when asked whether talks could extend into next year.</p></div><h2><a id="headline2"></a>Tensions with North Korea, China, Russia</h2><div><p>Lee has sought to reduce tensions with nuclear-armed North Korea. Pyongyang has rebuffed the South's overtures, and Lee said he was not optimistic about the prospect of inter-Korean talks for the time being.</p><p>During their meeting, Lee encouraged Trump to try to meet again with North Korean leader <a href="https://www.cnbc.com/kim-jong-un/">Kim Jong-Un</a> during Trump's trip next month for an Asia-Pacific summit Lee will host in the South.</p><p>Lee told Reuters his government does not have detailed information on the status of any talks between Washington and Pyongyang. "It is our judgment that they are not engaging in concrete conversations," he said.</p><p>He said he shares his predecessor Yoon's view that North Korea's military cooperation with Russia is a significant threat to South Korea's security. But he said it is not enough to respond in a simplistic way to the issue, which must be addressed through dialogue and coordination.</p><p>The North Korean leader and Russian President <a href="https://www.cnbc.com/vladimir-putin/">Vladimir Putin</a> stood shoulder to shoulder this month in Beijing when Chinese President <a href="https://www.cnbc.com/xi-jinping/">Xi Jinping</a> hosted them at a massive military parade and summit.</p></div><div id="ArticleBody-InlineImage-108194199" data-test="InlineImage"><p>In this pool photograph distributed by the Russian state agency Sputnik, (L-R) Russia's President Vladimir Putin walks with China's President Xi Jinping and North Korea's leader Kim Jong Un before a military parade marking the 80th anniversary of victory over Japan and the end of World War II, in Beijing's Tiananmen Square on September 3, 2025.</p><p>Sergey Bobylev | Afp | Getty Images</p></div><div><p>Lee said there is increasing confrontation between a socialist camp of countries and a capitalist, democratic camp that includes Seoul, and South Korea's geography threatens to place it on the frontier of any conflict with the other camp.</p><p>He said there is an escalatory spiral of rivalry and tensions where South Korea, Japan and the United States deepen cooperation and China, Russia, and North Korea work more closely together. "This is a very dangerous situation for Korea, and we must find an exit ramp out of the escalating military tensions," Lee said. "We must find a way for peaceful coexistence."</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DSM Disorders Disappear in Statistical Clustering of Psychiatric Symptoms (2024) (151 pts)]]></title>
            <link>https://www.psychiatrymargins.com/p/traditional-dsm-disorders-dissolve?r=2wyot6&amp;triedRedirect=true</link>
            <guid>45328537</guid>
            <pubDate>Mon, 22 Sep 2025 02:24:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psychiatrymargins.com/p/traditional-dsm-disorders-dissolve?r=2wyot6&#x26;triedRedirect=true">https://www.psychiatrymargins.com/p/traditional-dsm-disorders-dissolve?r=2wyot6&#x26;triedRedirect=true</a>, See on <a href="https://news.ycombinator.com/item?id=45328537">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>“</span><a href="https://osf.io/preprints/psyarxiv/7um9a" rel="">Reconstructing Psychopathology: A data-driven reorganization of the symptoms in DSM-5</a><span>” by Miri Forbes, et al. (was available as a preprint at the time of writing this post, later </span><a href="https://journals.sagepub.com/doi/full/10.1177/21677026241268345" rel="">published in </a><em><a href="https://journals.sagepub.com/doi/full/10.1177/21677026241268345" rel="">Clinical Psychological Science</a></em><span>) is a brilliantly designed and innovative study of the quantitative structure of psychopathology with important ramifications for our understanding of psychiatric classification. No one has conducted a study quite like this before, and the results are remarkable. It takes place in the context of the development of </span><a href="https://en.wikipedia.org/wiki/Hierarchical_Taxonomy_of_Psychopathology" rel="">Hierarchical Taxonomy of Psychopathology (HiTOP)</a><span> which is a dimensional, hierarchical, and quantitative approach to the classification of mental disorders, and relies on identification of patterns of covariation among symptoms.</span></p><p><span>The study is based on a large online survey, with participants recruited from a variety of sources, resulting in a socio-demographically diverse sample size of 14.8K participants. Participants could opt to complete a mini, short, medium, or long version of the questionnaire. The survey consisted of items based on individual symptoms derived from DSM-5. Symptoms were written in first person and past tense, as close to the DSM phrasing as possible but devoid of information about symptom onset, duration, frequency, and severity. Importantly, survey items were presented to participants in a </span><em>random order</em><span>. This randomness is important because in prior studies questions about symptoms were not asked or presented in a random manner. They have been asked using symptom questionnaires that cluster symptoms together in ways influenced by the diagnostic manuals or using a structured clinical interview that adopts the DSM organization. Asking about symptoms in a random order ensures that their co-occurrence is not artificially influenced by the order in which questions are asked. The survey went through multiple rounds of pilot testing, and in the end, 680 items were included. Participants reported how true each symptom statement was for them in the past 12 months on a five-point scale from </span><em>Not at all true (Never)</em><span> to </span><em>Perfectly true (Always)</em><span>. Participants were told to think about their experiences across a wide variety of contexts.</span></p><p><span>The responses were subjected to two statistical clustering methods: </span><em>iclust</em><span> and Ward’s hierarchical agglomerative clustering. Clusters were accepted for further analysis when both methods agreed. This was intended to ensure that there were no idiosyncrasies arising from reliance on one method. This resulted in 139 clusters (“syndromes”) and 81 solo symptoms. Higher-order constructs were identified using hierarchical principal components analysis and hierarchical clustering. The sample was divided into a primary sample (11.8K) and a hold-out sample (3K) to examine the robustness of results. The final classification was based on points of agreement between samples and methods.</span></p><p>The final high order structure included 8 spectra: Externalizing, Harmful Substance Use, Mania/Low Detachment, Thought Disorder, Somatoform, Eating Pathology, Internalizing, and Neurodevelopmental and Cognitive Difficulties. 27 subfactors were identified. As an example, within the internalizing spectrum, the 4 subfactors were: Distress, Social Withdrawal, Dysregulated Sleep and Trauma, and Fear. Similar to earlier literature, a single overarching dimension also emerged. This has been described before as the “p-factor” (general psychopathology factor), but Forbes et al. chose to call it the “Big Everything” to avoid reifying it.</p><p>So here it is, an empirically derived hierarchical clustering of individual symptoms across the range of psychopathology:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!BifU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!BifU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png 424w, https://substackcdn.com/image/fetch/$s_!BifU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png 848w, https://substackcdn.com/image/fetch/$s_!BifU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png 1272w, https://substackcdn.com/image/fetch/$s_!BifU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!BifU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png" width="1456" height="814" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:814,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:298979,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!BifU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png 424w, https://substackcdn.com/image/fetch/$s_!BifU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png 848w, https://substackcdn.com/image/fetch/$s_!BifU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png 1272w, https://substackcdn.com/image/fetch/$s_!BifU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F566fdb24-d95f-474c-9cf3-d3a62e242d25_2564x1433.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Forbes, et al. 2023.</figcaption></figure></div><p>The end result has a prominent convergence with the existing HiTOP model, with some points of divergence that would be important for future revisions of HiTOP.</p><p>Due to the symptom heterogeneity of DSM constructs, they are either broken down into smaller homogenous syndromes or they merge into higher-order clusters such as subfactors and spectra. (And this is the case not just in this particular study, but has been the case in prior analyses on which the HiTOP model is based—even though those exploratory symptom-level analyses had been conducted using measures based on DSM/ICD.)</p><p>There is much to discuss in this paper, but for illustrative purposes, I’ll focus on the case of depression and some other internalizing disorders.</p><p><span>“Major depressive disorder” (MDD) is one of the most common and recognizable disorders in psychiatry. Hundreds of thousands of people are diagnosed with MDD every day. Every medical student is taught the </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC134138/" rel="">SIGECAPS symptoms</a><span> that constitute the diagnostic criteria of depression, and across many clinics, people fill out the PHQ-9 screening questionnaire based on those symptoms. Generalized Anxiety Disorder (GAD) and Post-Traumatic Stress Disorder (PTSD) are similarly common diagnoses.</span></p><p><em>What does it mean that MDD, GAD, and PTSD do not show up as coherent and distinctive symptom clusters in statistical analysis?</em></p><p>This is the internalizing spectrum as it appears in Forbes et al. 2023:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!jvzZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!jvzZ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png 424w, https://substackcdn.com/image/fetch/$s_!jvzZ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png 848w, https://substackcdn.com/image/fetch/$s_!jvzZ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png 1272w, https://substackcdn.com/image/fetch/$s_!jvzZ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!jvzZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png" width="1307" height="648" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:648,&quot;width&quot;:1307,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:79163,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!jvzZ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png 424w, https://substackcdn.com/image/fetch/$s_!jvzZ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png 848w, https://substackcdn.com/image/fetch/$s_!jvzZ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png 1272w, https://substackcdn.com/image/fetch/$s_!jvzZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc215b046-fcf5-4ec6-a20d-88d675c5ef4d_1307x648.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here is the depressed mood and anhedonia cluster:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!DFRw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!DFRw!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png 424w, https://substackcdn.com/image/fetch/$s_!DFRw!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png 848w, https://substackcdn.com/image/fetch/$s_!DFRw!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png 1272w, https://substackcdn.com/image/fetch/$s_!DFRw!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!DFRw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png" width="862" height="690" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:862,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:63493,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!DFRw!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png 424w, https://substackcdn.com/image/fetch/$s_!DFRw!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png 848w, https://substackcdn.com/image/fetch/$s_!DFRw!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png 1272w, https://substackcdn.com/image/fetch/$s_!DFRw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa57dff0a-6881-45cd-b62b-3fccd32eeab1_862x690.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here are the self-derogation, suicidality, and guilt/shame proneness clusters:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!9a0A!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!9a0A!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png 424w, https://substackcdn.com/image/fetch/$s_!9a0A!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png 848w, https://substackcdn.com/image/fetch/$s_!9a0A!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png 1272w, https://substackcdn.com/image/fetch/$s_!9a0A!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!9a0A!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png" width="924" height="509" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:509,&quot;width&quot;:924,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:50820,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!9a0A!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png 424w, https://substackcdn.com/image/fetch/$s_!9a0A!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png 848w, https://substackcdn.com/image/fetch/$s_!9a0A!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png 1272w, https://substackcdn.com/image/fetch/$s_!9a0A!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F189918fd-4274-4e1b-ba1f-77ccb86a38f5_924x509.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>There is something statistically distinctive about the clusters labeled as ‘</span><em>depressed mood and anhedonia,’ ‘self-derogation,’ ‘suicidality,’ ‘guilt/shame proneness,’ ‘morning depression,’ ‘emotional lability</em><span>,’ etc., but there is nothing statistically distinctive about the combination of 9 symptoms that we recognize as diagnostic criteria for depression:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!drY6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!drY6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png 424w, https://substackcdn.com/image/fetch/$s_!drY6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png 848w, https://substackcdn.com/image/fetch/$s_!drY6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png 1272w, https://substackcdn.com/image/fetch/$s_!drY6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!drY6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png" width="490" height="586" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:586,&quot;width&quot;:490,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:55484,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!drY6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png 424w, https://substackcdn.com/image/fetch/$s_!drY6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png 848w, https://substackcdn.com/image/fetch/$s_!drY6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png 1272w, https://substackcdn.com/image/fetch/$s_!drY6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcefb286a-4c91-413b-9c91-b57ab0b8123e_490x586.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Items from PHQ-9</figcaption></figure></div><p>Obviously, people present with symptoms associated with major depression, and they report these symptoms if they are asked about them. That isn’t in doubt. What is in doubt is whether there is anything statistically special about this symptom combination.</p><p>All syndromes in the ‘Distress’ subfactor are correlated with each other, so people would present with various combinations of the syndromes, with some inclusion of symptoms/syndromes from other subfactors and spectra (since they are all correlated at a higher level).</p><p>Ms. Jones may experience:</p><ul><li><p>Depressed mood and anhedonia</p></li><li><p>Morning depression</p></li><li><p>Self-derogation</p></li><li><p>Suicidality</p></li><li><p>Guilt/Shame</p></li><li><p>Early sleep and awakening</p></li></ul><p>Mr. Jamal may experience:</p><ul><li><p>Depressed mood and anhedonia</p></li><li><p>Irritability</p></li><li><p>Emotional lability</p></li><li><p>Psychomotor agitation</p></li><li><p>Insomnia</p></li><li><p>Anxiousness</p></li></ul><p>Ms. Freeman may experience:</p><ul><li><p>Depressed mood and anhedonia</p></li><li><p>General cognitive difficulties</p></li><li><p>Distractibility</p></li><li><p>Psychomotor impairment</p></li><li><p>Suspiciousness</p></li><li><p>Psychological panic</p></li><li><p>Guilt/Shame</p></li></ul><p>All three meet MDD symptom criteria, but they all show varying combinations of more fundamental (statistically homogenous) syndromic clusters.</p><p>MDD indexes a varying and heterogenous subset of symptoms/syndromes, and what is common about these varying subsets is that depressed mood and/or anhedonia are prominent aspects of the presentation. In a similar manner, GAD indexes a varying and heterogenous subset of symptoms/syndromes, and what is common about these varying subsets is that pervasive anxiety is a prominent aspect of the presentation. And PTSD indexes a varying and heterogenous subset of symptoms/syndromes, and what is common about these varying subsets is that traumatic intrusions and avoidance are prominent aspects of the presentation. The subsets that constitute MDD, GAD, and PTSD all overlap, which is why these heterogenous categories dissolve when statistical homogeneity is sought. (This is also one reason why the reliability for MDD and GAD was so low in DSM-5 field trials.)</p><p>You can see in the figure below how MDD dissolves into homogenous elements that map onto ‘Distress,’ ‘Neurocognitive Impairment,’ ‘Dysregulated Sleep,’ and ‘Dysregulated Eating.’</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!DtzM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!DtzM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png 424w, https://substackcdn.com/image/fetch/$s_!DtzM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png 848w, https://substackcdn.com/image/fetch/$s_!DtzM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png 1272w, https://substackcdn.com/image/fetch/$s_!DtzM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!DtzM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png" width="1456" height="831" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b61a4b86-d639-4f63-962b-addf692334e5_1523x869.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:831,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:390570,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!DtzM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png 424w, https://substackcdn.com/image/fetch/$s_!DtzM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png 848w, https://substackcdn.com/image/fetch/$s_!DtzM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png 1272w, https://substackcdn.com/image/fetch/$s_!DtzM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb61a4b86-d639-4f63-962b-addf692334e5_1523x869.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Detail from a figure in Forbes et al.</figcaption></figure></div><p><span>Ken Kendler notes in his </span><a href="https://ajp.psychiatryonline.org/doi/full/10.1176/appi.ajp.2016.15121509" rel="">historical review of depression symptoms and the DSM</a><span>:</span></p><blockquote><p><span>“… the author examines how well DSM-5 symptomatic criteria for major depression capture the descriptions of clinical depression in the post-Kraepelin Western psychiatric tradition as described in textbooks published between 1900 and 1960. Eighteen symptoms and signs of depression were described, 10 of which are covered by the DSM criteria for major depression or melancholia. For two symptoms (mood and cognitive content), DSM criteria are considerably narrower than those described in the textbooks. Five symptoms and signs (changes in volition/motivation, slowing of speech, anxiety, other physical symptoms, and depersonalization/derealization) are not present in the DSM criteria. Compared with the DSM criteria, these authors gave greater emphasis to cognitive, physical, and psychomotor changes, and less to neurovegetative symptoms. These results suggest that important features of major depression are not captured by DSM criteria. This is unproblematic as long as DSM criteria are understood to&nbsp;</span><em>index</em><span>&nbsp;rather than&nbsp;</span><em>constitute</em><span>&nbsp;psychiatric disorders. However, since DSM-III, our field has moved toward a reification of DSM that implicitly assumes that psychiatric disorders are actually just the DSM criteria. That is, we have taken an index of something for the thing itself.”</span></p></blockquote><p><span>DSM criteria for MDD are an index, but what is the </span><em>thing itself</em><span>? If Forbes et al. are correct, the thing itself isn’t a fixed, stable entity but consists of variable and heterogenous subsets of internalizing and neurocognitive symptoms. And each time we use MDD as an index, we index something </span><em>different.</em><span> (Different from but </span><em>overlapping</em><span> with other instances.)</span></p><p><span>DSM criteria for MDD are an index, but what is the </span><em>thing itself</em><span>? If Forbes et al. are correct, the thing itself isn’t a fixed, stable entity but consists of variable and heterogeneous subsets of internalizing and neurocognitive symptoms. And each time we use MDD as an index, we index something </span><em>different</em><span>.</span></p><p>There are some important limitations to note with regards to the Forbes et al. survey. It relies only on self-reported symptoms, and features requiring clinician observation are missing; symptoms are decontextualized (e.g., insomnia due to substance withdrawal isn’t differentiated from insomnia due to anxiety); all symptoms were assessed using a 12-month time scale, even though different symptom patterns exist at different time scales. </p><p>Forbes et al. themselves note,</p><blockquote><p>“It will also be essential to understand which aspects of these results—particularly the fine-grained levels of the structure—are robust to other approaches to measurement (i.e., using alternative measures, time frames, multi-method or multi-informant approaches, as well as within-person assessment) and across intersectional conceptualizations of identity (e.g., in a variety of sociodemographic and culturally and linguistically diverse samples).”</p></blockquote><p>See also:</p><div data-component-name="DigestPostEmbed"><a href="https://www.psychiatrymargins.com/p/common-misconceptions-about-the-clinical" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1RXz!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5faac8bc-eff8-430a-bfd7-e4c8f16c805b_2999x1684.png"><img src="https://substackcdn.com/image/fetch/$s_!1RXz!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5faac8bc-eff8-430a-bfd7-e4c8f16c805b_2999x1684.png" sizes="100vw" alt="Common Misconceptions About the Clinical Use of HiTOP" width="140" height="140"></picture></div></a></div><div data-component-name="DigestPostEmbed"><a href="https://www.psychiatrymargins.com/p/are-critiques-of-dsmicd-as-devastating" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!grCP!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F30f5d1be-a3e1-4571-9ac3-93672932c080_600x600.png"><img src="https://substackcdn.com/image/fetch/$s_!grCP!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F30f5d1be-a3e1-4571-9ac3-93672932c080_600x600.png" sizes="100vw" alt="Are Critiques of DSM/ICD as Devastating for Psychiatric Diagnosis as Some Critics Seem to Think?" width="140" height="140"></picture></div></a></div><p data-attrs="{&quot;url&quot;:&quot;https://www.psychiatrymargins.com/p/traditional-dsm-disorders-dissolve?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.psychiatrymargins.com/p/traditional-dsm-disorders-dissolve?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I, a beginner developer, read the tutorial you, a developer, wrote for me (645 pts)]]></title>
            <link>https://anniemueller.com/posts/how-i-a-non-developer-read-the-tutorial-you-a-developer-wrote-for-me-a-beginner</link>
            <guid>45328247</guid>
            <pubDate>Mon, 22 Sep 2025 01:27:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anniemueller.com/posts/how-i-a-non-developer-read-the-tutorial-you-a-developer-wrote-for-me-a-beginner">https://anniemueller.com/posts/how-i-a-non-developer-read-the-tutorial-you-a-developer-wrote-for-me-a-beginner</a>, See on <a href="https://news.ycombinator.com/item?id=45328247">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    

  <div>
  <p>“Hello! I am a developer. Here is my relevant experience: I code in Hoobijag and sometimes jabbernocks and of course ABCDE++++ (but never ABCDE+/^+ are you kidding? ha!)&nbsp; and I like working with Shoobababoo and occasionally kleptomitrons. I’ve gotten to work for Company<sup id="fnref:1"><a data-id="3e2870f0-6d85-441d-a696-e5a4d36ac28e" href="#fn:1">1</a></sup> doing Shoobaboo-ing code things and that’s what led me to the Snarfus. So, let’s dive in!&nbsp;</p>
<p>↑ Excerpt ↑</p>
<p>About this tutorial</p>
<p>I first started doing Very Simple Thing<sup id="fnref:2"><a data-id="8df4a1c7-7308-48cc-b322-fa39c18e069b" href="#fn:2">2</a></sup> with Snarfus, but the more I used it the more I saw the potential! Despite the jaggle of the chromus, it’s really multi-purpose. And that’s what led me to argyling the pintafore with the quagmire instead of the hoobastank! I know, crazy. But it was kind of working, and actually a lot of fun… Until I hit a big roadblock: the fisterfunk will NOT talk to the shamrock portal or even send beep-boops back to the Snarfus! Of course, you know what that means<sup id="fnref:3"><a data-id="ffe570ad-86df-49e0-8d9b-debc850a3651" href="#fn:3">3</a></sup>&nbsp;— Now the entire hoob-tunnel is clogged with gramelions. Unacceptable.&nbsp;</p>
<p>I almost gave up but then I realized: If I connect the backside Snarfus stagnator to the backside shamrock Klingon troglodyte emulater, it’s good! Everything beep-boops and ding-dongs and I get the Actual Topic of the Tutorial, which lets me do the Very Simple Thing the way I want after all! Pretty cool<sup id="fnref:4"><a data-id="b039c94a-7f18-4d43-86eb-8ac46b97574a" href="#fn:4">4</a></sup>.</p>
<p>So here’s how to set it up:&nbsp;</p>
<ol>
<li><p>In the terminal, ajkl;gawgor;iqeg;iJLkqen.&nbsp; wl;R aw;oeiga 4648664 arjarwgj;llj;ja fadgfgajkljl; wlj;sdjk;lfas</p></li>
<li><p>Next go to folder/hidden/deep/in/the/file/system/surprise!.file and copy the contents of the file. If it’s not there, it might be in library/library/library/llibrary/liiiiiibrarrrary/llllliiiiibrary/hidden/hidden/hiding/you can’t find me/hidden/nope/never/hahahahereiam.file.</p></li>
<li><p>Now go back to the terminal and paste in the file contents, then type in 64A786AGR45JAR; rdja;jg [[]][[]][[]][[]]][[]()()()()()()()()(){{}{}{}|{}{|}{}{|}{ ////////////////!! !!!! !! //// !!! agjlkargji;lwej;OI [ASRGASG[]ASGDASG[]EAEadgasg[]EAGE[edaga][]ahgr-0-0=-0-=0-=0=0-0=-0-=0=-0-=0=-0=-0!!!</p></li>
<li><p>Boop!<sup id="fnref:5"><a data-id="1385bd5d-ec29-4fdf-bdcb-65fb027612bf" href="#fn:5">5</a></sup></p></li>
<li><p>Open Snarfus and upload the file you just made.&nbsp;</p></li>
<li><p>Just for shits and giggles, you<em>&nbsp;</em>can de-sham the chronostatiomatrix by running —()()(]]asdg a=-do —cd go cd stay —sususudododo baby shark—][] but that’s optional.&nbsp;</p></li>
<li><p>That’s it!&nbsp;</p></li>
</ol>
<p>Let me know how it goes for you. I’d love to hear if anybody uses this approach with GewGawGamma or ometer2.7.”</p>
<ol>
<li id="fn:1" data-id="3e2870f0-6d85-441d-a696-e5a4d36ac28e"><p>I probably should recognize Company because it seems illustrious but I do not recognize Company or know what they do.&nbsp; <a href="#fnref:1">↵</a></p></li>
<li id="fn:2" data-id="8df4a1c7-7308-48cc-b322-fa39c18e069b"><p>It is not simple.&nbsp; <a href="#fnref:2">↵</a></p></li>
<li id="fn:3" data-id="ffe570ad-86df-49e0-8d9b-debc850a3651"><p>I do<em> </em>not&nbsp;know what that means.&nbsp; <a href="#fnref:3">↵</a></p></li>
<li id="fn:4" data-id="b039c94a-7f18-4d43-86eb-8ac46b97574a"><p>It<em> is</em>&nbsp;cool. I don’t really understand how, but I believe it. I’m glad you know how to do it.&nbsp; <a href="#fnref:4">↵</a></p></li>
<li id="fn:5" data-id="1385bd5d-ec29-4fdf-bdcb-65fb027612bf">
<p>The first 3 steps will take me approximately 7 hours and 193 internet searches to complete. When I finally get to Boop! it will be really satisfying.</p>
<p><strong><em>This is meant in good fun. I really appreciate the folks who take time to share their knowledge and write up tutorials and give tips and so on.&nbsp;</em></strong> <a href="#fnref:5">↵</a></p>
</li>
</ol>
</div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We Politely Insist: Your LLM Must Learn the Persian Art of Taarof (117 pts)]]></title>
            <link>https://arxiv.org/abs/2509.01035</link>
            <guid>45327964</guid>
            <pubDate>Mon, 22 Sep 2025 00:31:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2509.01035">https://arxiv.org/abs/2509.01035</a>, See on <a href="https://news.ycombinator.com/item?id=45327964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2509.01035">View PDF</a>
    <a href="https://arxiv.org/html/2509.01035v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large language models (LLMs) struggle to navigate culturally specific communication norms, limiting their effectiveness in global contexts. We focus on Persian taarof, a social norm in Iranian interactions, which is a sophisticated system of ritual politeness that emphasizes deference, modesty, and indirectness, yet remains absent from existing cultural benchmarks. We introduce TaarofBench, the first benchmark for evaluating LLM understanding of taarof, comprising 450 role-play scenarios covering 12 common social interaction topics, validated by native speakers. Our evaluation of five frontier LLMs reveals substantial gaps in cultural competence, with accuracy rates 40-48% below native speakers when taarof is culturally appropriate. Performance varies between interaction topics, improves with Persian-language prompts, and exhibits gender-based asymmetries. We also show that responses rated "polite" by standard metrics often violate taarof norms, indicating the limitations of Western politeness frameworks. Through supervised fine-tuning and Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in model alignment with cultural expectations. Our human study with 33 participants (11 native Persian, 11 heritage, and 11 non-Iranian speakers) forms baselines in varying degrees of familiarity with Persian norms. This work lays the foundation for developing diverse and culturally aware LLMs, enabling applications that better navigate complex social interactions.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Ali Emami Dr. [<a href="https://arxiv.org/show-email/3fbf0c65/2509.01035" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 1 Sep 2025 00:11:22 UTC (1,291 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be careful with Go struct embedding (120 pts)]]></title>
            <link>https://mattjhall.co.uk/posts/be-careful-with-go-struct-embedding.html</link>
            <guid>45327531</guid>
            <pubDate>Sun, 21 Sep 2025 23:16:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mattjhall.co.uk/posts/be-careful-with-go-struct-embedding.html">https://mattjhall.co.uk/posts/be-careful-with-go-struct-embedding.html</a>, See on <a href="https://news.ycombinator.com/item?id=45327531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Go has a feature called struct embedding that allows you to compose types. It looks something like this:</p>
<pre><code><span>type</span> <span>Position</span> <span>struct</span> {
	<span>X</span> <span>int</span>
	<span>Y</span> <span>int</span>
}

<span>type</span> <span>Colour</span> <span>struct</span> {
	<span>R</span> <span>byte</span>
	<span>G</span> <span>byte</span>
	<span>B</span> <span>byte</span>
}

<span>type</span> <span>Rectangle</span> <span>struct</span> {
	<span>Position</span>
	<span>Colour</span>
	
	<span>Width</span>  <span>int</span>
	<span>Height</span> <span>int</span>
}

<span>r</span> <span>:=</span> <span>Rectangle</span>{<span>/* ... */</span>}

<span>// This works:</span>
<span>fmt</span>.<span>Printf</span>(<span>"%d,%d\n"</span>, <span>r</span>.<span>Position</span>.<span>X</span>, <span>r</span>.<span>Position</span>.<span>Y</span>)

<span>// but so does this:</span>
<span>fmt</span>.<span>Printf</span>(<span>"%d,%d\n"</span>, <span>r</span>.<span>X</span>, <span>r</span>.<span>Y</span>)
</code></pre>
<p>But what do you think this code does?</p>
<pre><code><span>type</span> <span>FooService</span> <span>struct</span> {
	<span>URL</span> <span>string</span>
}

<span>type</span> <span>BarConnectionOptions</span> <span>struct</span> {
	<span>URL</span> <span>string</span>
}

<span>type</span> <span>BarService</span> <span>struct</span> {
	<span>BarConnectionOptions</span>
}

<span>type</span> <span>Options</span> <span>struct</span> {
	<span>FooService</span>
	<span>BarService</span>
}

<span>opts</span> <span>:=</span> <span>Options</span>{
	<span>FooService</span>: <span>FooService</span>{<span>URL</span>: <span>"abc.com"</span>},
	<span>BarService</span>: <span>BarService</span>{
		<span>BarConnectionOptions</span>: <span>BarConnectionOptions</span>{
			<span>URL</span>: <span>"xyz.com"</span>,
		},
	},
}

<span>fmt</span>.<span>Println</span>(<span>opts</span>.<span>URL</span>)
</code></pre>
<p>I would expect this to fail to compile as <code>URL</code> is ambiguous. It actually prints <code>abc.com</code>, presumably as it is the least nested version of that field. This happened at the day job, although it was caught in a test. Be careful when embedding structs!</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is Venus hell and Earth an Eden? (167 pts)]]></title>
            <link>https://www.quantamagazine.org/why-is-venus-hell-and-earth-an-eden-20250915/</link>
            <guid>45327417</guid>
            <pubDate>Sun, 21 Sep 2025 22:56:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/why-is-venus-hell-and-earth-an-eden-20250915/">https://www.quantamagazine.org/why-is-venus-hell-and-earth-an-eden-20250915/</a>, See on <a href="https://news.ycombinator.com/item?id=45327417">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody">
                
        <section>
            <p>Mark Belan/Quanta Magazine</p>
        </section>
        <div>
        <p>
            A team of scientists has investigated how Earth’s twin became so inhospitable, and whether the same will happen to our planet.        </p>
        
    </div>
<figure>
    
</figure>
    <figure>
        
            </figure>
<div data-role="selectable">
    <p><span>V</span>enus is arguably the worst place in the solar system. A cloak of carbon dioxide suffocates the planet, subjecting its surface to skull-crushing pressure. Sulfuric acid rains down through the sickly yellow sky but never reaches the lava-licked ground. Venus is so hot — hot enough to melt lead — that the acid rain evaporates as it’s falling.</p>
<p>The planet’s extreme inhospitality is at the heart of one of the most beguiling mysteries in planetary science. Venus and Earth formed at the same time, from the same geologic building blocks, in pretty much the same part of the solar system. They’re even the same size. So why is Venus a hellscape, and Earth a garden?</p>
<p>A common refrain in the scientific community is that Venus is just several steps ahead — that it represents the end state of all large rocky planets, including Earth. The hypothesis is that these planets eventually lose the ability to sequester planet-warming greenhouse gases in their geologic underbelly. When those gases then accumulate in the atmosphere, the world enters a runaway greenhouse state — like the boiling hot Venusian climate. “Over the years, we’d always heard about Venus being a preview into Earth’s future,” said <a href="https://profiles.ucr.edu/app/home/profile/skane">Stephen Kane</a>, a planetary astrophysicist at the University of California, Riverside.</p>

<p>But is that long-held assumption true? In hundreds of millions or billions of years, will Earth’s climate go the way of Venus’, transitioning from a temperate world into a catastrophic hothouse? Kane and his colleagues have been trying to find out. Venus and Earth are often referred to as twins, with Venus being the evil one of the pair. In their Reuniting Twins project, the scientists have developed a digital model of Earth that combines solar physics, volcanology, plate tectonics and climate science. They’ve been pushing their model Earth to its extremes, trying every plausible way to break it and make it into Venus.</p>
<p>As well as exploring what went so wrong on the second rock from the sun, this work speaks to a query closer to home, said <a href="https://eeps.wustl.edu/people/paul-byrne">Paul Byrne</a>, a planetary scientist at Washington University in St. Louis who was not directly involved with the project: “How long is Earth habitable for?”</p>
</div>
    <figure>
        <div>
                            <p><img width="2560" height="1274" src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Two-planets-V3-scaled.webp" alt="Earth and Venus side by side" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Two-planets-V3-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Two-planets-V3-1720x856.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Two-planets-V3-520x259.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Two-planets-V3-768x382.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Two-planets-V3-1536x764.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Two-planets-V3-2048x1019.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Two-planets-V3-98x49.webp 98w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Venus is 95% as broad as Earth, and 81.5% as massive.</p>
            <p>NASA</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <h2><strong>A Planetary Whodunit</strong></h2>
<p>Forecasting Earth’s future requires an understanding of Venus’ past. Long ago, Venus might not have been too different from how Earth is now. Spacecraft surveys and telescope observations have revealed the existence of a rare, heavy form of water in the Venusian atmosphere — a telltale sign that regular water used to be abundant on the planet.</p>
<p>Researchers debate how that water resided there. One possibility is that the water on young Venus formed steam that wafted above the magma sea covering the newborn planet’s surface. That water vapor, a potent greenhouse gas, would have pushed the world into a scorching-hot greenhouse state not long after its birth. Alternatively, Venus’ initial, planetwide magma sea might have cooled and hardened into a crust quickly enough for liquid water — maybe even <a href="https://www.scientificamerican.com/article/nasa-just-broke-the-venus-curse-heres-what-it-took/">an ocean’s worth of water</a> — to flow across it. If that is true, then what happened to all that water?</p>
<p>In 2020, <a href="https://science.gsfc.nasa.gov/sci/bio/michael.j.way">Michael Way</a>, a planetary scientist and climate modeler at NASA’s Goddard Institute for Space Studies in New York, looked at <a href="https://doi.org/10.1029/2019JE006276">two possibilities</a> for how a Venusian ocean could have boiled away.</p>
    
    
    
    
<p>The first hypothesis points to the sun, which, as it ages and burns through its hydrogen fuel, blasts out ever more sunlight, subjecting nearby planets — including Venus, which sits closer to the sun than we do — to an increasingly intense blaze. Planetary scientists estimate that by about a billion years after the solar system’s birth, the gradually brightening sun would have been able to efficiently vaporize any liquid water on Venus. Water vapor would then have flooded the Venusian atmosphere, potentially causing intensive global warming; this warming might have been exacerbated further by volcanoes off-gassing carbon dioxide. The combination might have pushed Venus into a runaway greenhouse state.</p>
<p>It’s a nice story. But according to Way’s models, the theory that the sun broke Venus by evaporating its oceans has problems. Venus spins very slowly on its axis, and one day there is equivalent to 116 Earth days, or nearly four months. If the planet’s dayside initially held liquid oceans, that water’s evaporation would have formed thick clouds. Those prodigious, persistent dayside clouds would have reflected sunlight, keeping Venus cooler than it otherwise might have been and actually preventing a runaway <a href="https://www.quantamagazine.org/physicists-pinpoint-the-quantum-origin-of-the-greenhouse-effect-20240807/">greenhouse effect</a> and unrestrained global warming.</p>
<p>Way thinks that a more plausible suspect in Venus’ demise is a form of ruinous volcanism. In its own storied past, Earth experienced prolonged eruptions of lava in single regions, known as large igneous provinces (LIPs), that lasted for hundreds of thousands and perhaps millions of years. Each of these events injected copious carbon dioxide into the atmosphere and made the world, for a time, extremely hot. One LIP 252 million years ago <a href="https://doi.org/10.1130/G47365.1">triggered</a> the <a href="https://news.st-andrews.ac.uk/archive/great-dying-what-caused-earths-biggest-mass-extinction/">worst recorded mass extinction in Earth’s history</a>, almost sterilizing the planet.</p>
</div>
    <figure>
        <div>
                            <p><img width="1917" height="1342" src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Venus-lanscape.webp" alt="A barren landscape" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Venus-lanscape.webp 1917w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Venus-lanscape-1720x1204.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Venus-lanscape-520x364.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Venus-lanscape-768x538.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Venus-lanscape-1536x1075.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.NASA-Venus-lanscape-98x69.webp 98w" sizes="(max-width: 1917px) 100vw, 1917px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>The Magellan spacecraft imaged the desolate surface of Venus in the early 1990s.</p>
            <p>NASA</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Fortunately for life, after each of these LIP events, Earth gradually drew that excess carbon dioxide deep into its rocky bowels, and the planet cooled back down. It accomplished this through a process called subduction: When tectonic plates collide, one plate can descend below the other, drawing seawater rich in dissolved carbon dioxide into the abyssal depths. That carbon remains sequestered in the lower mantle for epochal lengths of time; some of it eventually erupts back into the atmosphere via volcanism. That, in a nutshell, is how <a href="https://www.quantamagazine.org/a-biography-of-earth-across-the-age-of-animals-20250915/">Earth’s global thermostat</a> is regulated.</p>

<p>It’s possible that back when there was water on Venus, it also had Earth-like plate tectonics with major subduction zones. But the system wasn’t widespread or large-scale enough to save it from a planetary immolation — especially if several LIP events occurred at roughly the same time. Way’s models show that several concurrent LIPs could have thrown vast quantities of carbon dioxide into Venus’ atmosphere, warming the world so severely that much of its liquid water boiled away into the sky, accelerating the warming further. With no oceans to speak of, all that carbon dioxide could not be reabsorbed. Moreover, water is an enabler of subduction: It lowers the melting point of rocks, allowing tectonic plates to more easily bend and break. And so, with no water left at the surface, major subduction zones would grind to a halt, preventing the entombment of carbon dioxide.</p>
<p>Other scientists tend to agree with Way’s assessment: The sun alone cannot be responsible for making Venus the awful place it is today. “I really do think you need multiple extreme volcanic episodes … to get there,” said <a href="https://annagulcher.com/">Anna Gülcher</a>, a planetary scientist at the University of Bern in Switzerland.</p>
<h2><strong>How to Break the World</strong></h2>
<p>While studying Venus and its runaway greenhouse wasteland, Kane became morbidly curious: Could the same fate befall Earth? “What if we shut down the carbon cycle on Earth?” he said. “Could you produce a Venus?”</p>
<p>To find out, he and his team built a virtual world-destroying machine. “Everybody loves a post-apocalyptic or doomsday scenario, provided it’s 5 billion years in our future,” he said.</p>
</div>
    <figure>
        <div>
                            <p><img width="2200" height="1359" src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.Courtesy-of-StephenKane.webp" alt="A man sitting next to a globe" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.Courtesy-of-StephenKane.webp 2200w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.Courtesy-of-StephenKane-1720x1062.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.Courtesy-of-StephenKane-520x321.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.Courtesy-of-StephenKane-768x474.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.Courtesy-of-StephenKane-1536x949.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.Courtesy-of-StephenKane-2048x1265.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.Courtesy-of-StephenKane-98x61.webp 98w" sizes="(max-width: 2200px) 100vw, 2200px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>The planetary astrophysicist Stephen Kane has investigated whether Earth’s future will resemble Venus’ present.</p>
            <p>Courtesy of Stephen Kane</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>The first step was to fast-forward their model of Earth about 3.5 billion years, to when the sun and planets will be 8 billion years old. At that point, the sun will shine brighter than it does today, and Earth’s atmosphere will receive the same level of roasting-hot starlight that Venus did when it was just 1 billion years old. Back then, it’s thought, Venus would have been at a tipping point: either temperate and waterlogged, or burnt to a crisp. The 8 billion-year-old sun will push Earth to a similar, climatic knife-edge.</p>
<p>The team’s model suggests that in 3.5 billion years, Earth’s oceans could begin evaporating and become heat-trapping water vapor in the atmosphere. That could be sufficient to kill off Earth’s major subduction zones, since without water, there might be no subduction (plus, there would be less water for carbon dioxide to dissolve into in the first place). “We’re losing the ability to draw back that CO<sub>2</sub> into the Earth’s mantle. So it just builds up,” said <a href="https://www.michellehillphd.com/">Michelle Hill</a> of Stanford University, a member of the Reuniting Twins project.</p>
<p>The shutdown of major subduction zones would mean that the Earth’s tectonic plates stop clashing and jostling about. Instead, they would form a near-united rocky shell around the hot mantle. For a time, the mantle would get hotter, since the shell around it would trap heat generated by radioactively decaying compounds inside it. As heat accumulates in the interior, the Earth in Kane’s simulations would experience an uptick in volcanism lasting for about 15 million years.</p>
<p>This period, known as a “stagnant lid regime,” adds even more carbon dioxide to the sky. But the eruptive spike is short-lived. The mantle cools down and the crust thickens until it becomes nearly impermeable to any major carbon-erupting volcanism.</p>
<p>So 15 million years after its stagnant lid forms, Earth will have reached a new equilibrium. Occasional spurts of volcanism would still happen. (Likewise, scientists have found <a href="https://www.nationalgeographic.com/science/article/venus-is-volcanically-alive?loggedin=true&amp;rnd=1716366279998">compelling</a> <a href="https://www.nytimes.com/2024/05/27/science/venus-volcanoes-lava.html">evidence</a> that Venus is volcanically active today.) But Kane explained that these sporadic eruptions aren’t expected to add much carbon to the atmosphere.</p>
<p>At that point, the team’s simulations stop. How Venus-like is their model of the future Earth?</p>
</div>
    <figure>
        <div>
                            <p><img width="2560" height="1906" src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.USGS-Basalt-flow-1-scaled.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.USGS-Basalt-flow-1-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.USGS-Basalt-flow-1-1720x1281.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.USGS-Basalt-flow-1-520x387.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.USGS-Basalt-flow-1-768x572.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.USGS-Basalt-flow-1-1536x1144.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.USGS-Basalt-flow-1-2048x1525.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/09/Venus-vs-Earth-cr.USGS-Basalt-flow-1-98x73.webp 98w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Basalt lava flowed from the Kīlauea volcano on Hawai<span lang="EN-GB">‘</span>i in 2012. Occasionally throughout Earth’s history, continent-size basalt floods have triggered global warming and mass extinctions. Some scientists suspect that similarly enormous outpourings of lava helped scorch Venus.</p>
            <p>USGS</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <h2><strong>There’s No Place Like Venus</strong></h2>
<p>If or when Earth’s large-scale subduction shuts off in about 3.5 billion years, kneecapping the planet’s ability to bury carbon, Kane and his team’s simulations indicate that the carbon dioxide level in the atmosphere will rise anywhere from 0.1 bars to 0.8 bars. (For reference, the total atmospheric pressure at sea level today is 1 bar, and roughly 0.04% of that, or 0.042 bars, comes from carbon dioxide.) Even in their best-case scenario of a 0.1-bar rise in carbon dioxide, Earth’s surface temperature surpasses 100 degrees Celsius (212 degrees Fahrenheit). The same happens at 0.8 bars, but far more quickly.</p>
<p>Either way, the surface of the world becomes literally boiling hot. Earth will “turn into a post-runaway greenhouse state,” Kane said. “The surface temperature will be too hot for any water. It’ll all boil away.” Nothing sitting on the world’s skin would survive.</p>
<p>Still, Earth won’t get close to the state that Venus is in today. “It’d be Venus lite,” Kane said.</p>
<p>Venus has a 93-bar atmosphere consisting of 96.5% carbon dioxide. Kane and his colleagues’ doomsday machine, no matter how hard they push it, cannot take Earth to those levels. “I was surprised by that,” he said. Because the mantle is sealed off by a stagnant lid, volcanism drops, protecting Earth from a Venus-style roasting better than he thought.</p>
<p>Independent scientists have praised the Reuniting Twins project for challenging prior assumptions and adding significantly to the discussion about the terminal state of rocky planets.</p>
        
        
<p>“I like their idea,” Way said, adding that the team’s version of a future Earth “doesn’t sound unreasonable.”</p>
<p>“You end up with a world that’s stinkingly hot,” said Byrne, the Washington University planetary scientist. But, he said, the possibility that it may not be Venusian levels of hot is intriguing.</p>
<p>Kane’s team acknowledged that their model hasn’t considered LIP-style mega-eruptions, and that these events could feasibly add a bounty of trapped carbon to the atmosphere at any point in the future. Maybe Earth gets unlucky and experiences multiple, simultaneous LIP events (though this grows less likely over time, Kane said, as the mantle cools and its churning slows). If so, that scenario could push Earth to be more like Venus than the team’s model suggests.</p>
<p>Uncertainties abound. But if Kane’s team is even broadly correct, it suggests that Venus has a uniquely grim history. Something — perhaps an inundation of lava — burned that planet to the bone. Earth, meanwhile, has so far been unable to bring about its own destruction. Let’s hope that remains the case long into the future.</p>
</div>
    <figure>
        <div>
                            <p><a href="https://www.quantamagazine.org/how-humanity-has-amplified-lifes-quest-for-energy-20250820/">
                            <img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Climate-Banner-3.svg" alt="A promotional banner reading &quot;How We Came To Know Earth&quot; and suggesting the next article in the series" decoding="async">                        </a>
                                    </p>
                        </div>
            </figure>
                
                
            </div><div>
        <div data-name="next-post__image-wrapper">
    <p><img width="1720" height="728" src="https://www.quantamagazine.org/wp-content/uploads/2025/09/09-EXTENDED-BIOSPHERE-cr-Mark-Belan-HP-1720x728.webp" alt="A 3D model of earth from space" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/09/09-EXTENDED-BIOSPHERE-cr-Mark-Belan-HP-1720x728.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/09/09-EXTENDED-BIOSPHERE-cr-Mark-Belan-HP-520x220.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/09/09-EXTENDED-BIOSPHERE-cr-Mark-Belan-HP-768x325.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/09/09-EXTENDED-BIOSPHERE-cr-Mark-Belan-HP-1536x650.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/09/09-EXTENDED-BIOSPHERE-cr-Mark-Belan-HP-2048x867.webp 2048w, https://www.quantamagazine.org/wp-content/uploads/2025/09/09-EXTENDED-BIOSPHERE-cr-Mark-Belan-HP-98x41.webp 98w" sizes="(max-width: 1720px) 100vw, 1720px">    </p>
</div>
        
        <div>
                <h2>Next article</h2>
                <p>How Humanity Amplified Life’s Quest for Energy</p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig got a new ELF linker and it's fast (115 pts)]]></title>
            <link>https://github.com/ziglang/zig/pull/25299</link>
            <guid>45327318</guid>
            <pubDate>Sun, 21 Sep 2025 22:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ziglang/zig/pull/25299">https://github.com/ziglang/zig/pull/25299</a>, See on <a href="https://news.ycombinator.com/item?id=45327318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <h2 dir="auto">Release Notes</h2>
<p dir="auto">The new linker can be used with <code>-fnew-linker</code> in the CLI, or by setting <code>exe.use_new_linker = true</code> in a build script.</p>
<p dir="auto">It is already the default when passing <code>-fincremental</code> and targeting ELF.</p>
<p dir="auto">The performance is fast enough that there's no longer much of a benefit to exposing a <code>-Dno-bin</code> build step. You might as well keep codegen and linking always enabled because the compilation speed difference is negligible, and then you get an executable at the end.</p>
<h3 dir="auto">Performance Data Points</h3>
<p dir="auto">Old linker, building Zig compiler, then making a single-line change to a function, and then another:</p>
<div data-snippet-clipboard-copy-content="Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 18s


Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 754ms


Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 858ms"><pre><code>Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 18s


Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 754ms


Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 858ms
</code></pre></div>
<p dir="auto">New linker, building Zig compiler, then making a single-line change to a function, and then another:</p>
<div data-snippet-clipboard-copy-content="Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 18s
      └─ options success


Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 73ms


Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 72ms"><pre><code>Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 18s
      └─ options success


Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 73ms


Build Summary: 4/4 steps succeeded
install success
└─ install zig success
   └─ compile exe zig Debug native success 72ms
</code></pre></div>
<p dir="auto">Disabling the backend and linker entirely, building Zig compiler (type checking only), then making a single-line change to a function, and then another:</p>
<div data-snippet-clipboard-copy-content="Build Summary: 3/3 steps succeeded
install success
└─ compile exe zig Debug native success 17s


Build Summary: 3/3 steps succeeded
install success
└─ compile exe zig Debug native success 70ms


Build Summary: 3/3 steps succeeded
install success
└─ compile exe zig Debug native success 69ms"><pre><code>Build Summary: 3/3 steps succeeded
install success
└─ compile exe zig Debug native success 17s


Build Summary: 3/3 steps succeeded
install success
└─ compile exe zig Debug native success 70ms


Build Summary: 3/3 steps succeeded
install success
└─ compile exe zig Debug native success 69ms
</code></pre></div>
<p dir="auto">In summary:</p>
<ul dir="auto">
<li>incremental updates are 11 times faster.</li>
<li>with the new linker, there is only a 4% slowdown compared to <em>skipping code generation and linking entirely</em></li>
</ul>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How can I influence others without manipulating them? (184 pts)]]></title>
            <link>https://andiroberts.com/leadership-questions/how-to-influence-others-without-manipulating</link>
            <guid>45327199</guid>
            <pubDate>Sun, 21 Sep 2025 22:20:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andiroberts.com/leadership-questions/how-to-influence-others-without-manipulating">https://andiroberts.com/leadership-questions/how-to-influence-others-without-manipulating</a>, See on <a href="https://news.ycombinator.com/item?id=45327199">Hacker News</a></p>
<div id="readability-page-1" class="page">
		<a href="#content">Skip to content</a>

	<div id="boxed-wrapper">
			
							
					
			<header>
				<div>
							<nav aria-label="Main Menu"><ul id="menu-about"><li id="menu-item-3484" data-item-id="3484"><a href="https://andiroberts.com/"><span>Home</span></a></li><li id="menu-item-3295" data-item-id="3295"><a href="https://andiroberts.com/leadership-library"><span>Leadership library</span></a></li><li id="menu-item-3297" data-item-id="3297"><a href="https://andiroberts.com/leadership-questions"><span>Leadership questions</span></a></li><li id="menu-item-3294" data-item-id="3294"><a href="#"><span>Resources</span></a><ul><li id="menu-item-3300"><a href="https://andiroberts.com/visual/canvascollection"><span>Canvas collection</span></a></li><li id="menu-item-3298"><a href="https://andiroberts.com/coaching/feedback-models-hub"><span>Feedback hub</span></a></li><li id="menu-item-3330"><a href="https://andiroberts.com/uncategorized/designthinkinghub"><span>Design thinking hub</span></a></li><li id="menu-item-3296"><a href="https://andiroberts.com/complexity-systems-thinking/systems-thinking-hub"><span>Systems thinking hub</span></a></li><li id="menu-item-3303"><a href="https://andiroberts.com/peter-block-books-ideas-and-resources"><span>Peter Block books</span></a></li><li id="menu-item-3299"><a href="https://andiroberts.com/facilitation/collaboration-tools"><span>Collaboration hub</span></a></li><li id="menu-item-3304"><a href="https://andiroberts.com/large-group-methods"><span>Large group methods</span></a></li><li id="menu-item-3301"><a href="https://andiroberts.com/visual/visual-hub"><span>Visual hub</span></a></li><li id="menu-item-3302"><a href="https://andiroberts.com/uncategorized/habit-change"><span>Habits hub</span></a></li><li id="menu-item-3307"><a href="https://andiroberts.com/leadership/mentoring-hub"><span>Mentoring hub</span></a></li></ul></li><li id="menu-item-2668" data-item-id="2668"><a href="https://andiroberts.com/blog-content/"><span>Blog</span></a></li><li><a href="#" aria-label="Search" data-title="Search" title="Search" role="button" aria-expanded="false"></a></li></ul></nav>	

<nav aria-label="Main Menu Mobile"></nav>

		


			</div>
				
			</header>
								
							
				
					
							
			
						<main id="main">
				<section id="content">
			
	
					<article id="post-3486">
						
														
						
															<h2>How can I influence others without manipulating them?</h2>										<div>
				<p>We influence others every day, whether we intend to or not. Sometimes it is through the way we argue, sometimes through the way we listen, and sometimes simply through the story we tell about what matters. Influence is not the property of the few who hold authority. It is the currency of all relationships.</p>
<p>The word persuasion often carries with it the scent of manipulation, as though one person is moving another toward something they do not really want. But there is another way to hold it. Influence can be understood as an invitation. It is the art of meeting people where they are, of entering their world with respect, and of opening a door that they might choose to walk through with us.</p>
<p>This article invites you to consider five doors of influence: Rationalising, Asserting, Negotiating, Inspiring, and Bridging. Each opens a different path into relationship and commitment. Each has its gifts. Each, when overused, can become a wall instead of a door.</p>
<p>The practice is not to master all five overnight, but to grow our awareness and our range. Persuasion is not a personality trait. It is a skill. It asks us to listen, to notice, and to choose consciously how we invite others.</p>
<h2>Our own doorway: The blind spot of preference</h2>
<p>Most of us have a default style of persuasion. Perhaps you lean on facts. Perhaps you rely on conviction. Perhaps you search for compromise, tell stories, or call on the voices of others. None of these is wrong. Each is a doorway.</p>
<p>Yet our default does more than shape how we speak. It shapes how we see. If I lean on data, I may hear someone’s story as weak rather than inspiring. If I favour conviction, I may interpret hesitation as lack of commitment, when it might be an opening for negotiation. If I thrive on inspiration, I may dismiss detail-oriented questions as nit-picking rather than as a real need for clarity. If I am most comfortable with bridging, I may feel uneasy with those who are direct and independent.</p>
<p>Our own doorway becomes a filter. It colours what we pay attention to, what we dismiss, and how we react. The danger is that we mistake difference for resistance. We think they are being difficult, when in fact they are simply standing at another door.</p>
<p>Becoming aware of our preference allows us to pause. Instead of defending our own style, we can ask: What is the invitation they are offering me through their language and behaviour? Which door are they holding open that I have overlooked because I was guarding my own?</p>
<p>Influence begins with this act of humility. The willingness to see that the door we most trust may not be the one others are waiting at.</p>
<p>Once we recognise our blind spot, we are ready to notice the variety of doors available to us. Influence is not one way, it is five ways. Each has its own language, its own gifts, and its own risks. Rationalising, Asserting, Negotiating, Inspiring, and Bridging are not personality types but choices. At any moment, we can choose to knock on a different door, depending on where others are standing and what the moment requires.</p>
<p>What follows is a closer look at each door. how to recognise it, how to walk through it well, and how to avoid turning it into a wall.</p>
<h2>Rationalising: The door of logic</h2>
<p>Rationalising is the style that seeks to persuade through facts, evidence, and analysis. It appeals to those who are comforted by structure and clarity. When someone asks for data, compares benchmarks, or worries about risk and performance, they are standing at the rational door.</p>
<p>This style has its strengths. It reassures the finance director who needs a business case. It appeals to the engineer who wants proof that the system will not fail. It builds trust with the analyst who measures quality by what can be verified.</p>
<p>To enter this door well, we come prepared. We gather our facts. We organise our thinking into a clear flow: problem, options, evidence, recommendation. We know the return on investment, the efficiency gains, the quality improvements. We anticipate objections and hold evidence ready to meet them. We present data not as a weapon but as a gift of clarity.</p>
<p>Yet logic alone is not enough. Overused, it can become cold and detached. The person across from us may long for empathy, but we bury them in numbers. They may seek relationship, but we offer them a chart. Rationalising can easily turn into proving a point rather than creating understanding.</p>
<h3>Reflection questions</h3>
<p>&nbsp;•	Which metrics or outcomes matter most to this person, and have I framed my case in those terms, for example ROI, risk, quality or efficiency?</p>
<p>&nbsp;•	What is my one-page flow: problem, options, evidence, recommendation?</p>
<p>&nbsp;•	Which objections are most likely, and what independent sources will I cite to address them?</p>
<p>&nbsp;•	Where will a chart or comparison clarify, and where could visuals overwhelm?</p>
<p>&nbsp;•	What level of evidence would be enough for this decision, and how will I check for overload signals in the moment?</p>
<p>&nbsp;•	Where might I be using facts to prove a point rather than to build shared understanding?</p>
<p>&nbsp;•	If they are not data-driven, which second door will I pivot to after I test for fit?</p>
<p>&nbsp;•	What jargon should I remove to keep the message precise and clear?</p>
<h2>Asserting: The door of conviction</h2>
<p>Asserting persuades through confidence, authority, and clarity. It is the voice that speaks directly and without hesitation. It is the person who values decisiveness, who prefers a straight line over a winding explanation, who respects those who stand their ground.</p>
<p>You recognise this style when someone expresses strong opinions with little hesitation, when they challenge ideas and expect you to do the same, when they respond best to conviction rather than ambiguity. For them, clarity is a form of respect.</p>
<p>To enter this door well, we speak with confidence. We choose firm language, declarative statements, uncluttered by qualifiers. We are direct and concise. We know our boundaries, and we state them. We are ready with anchor points: two or three firm reasons why our proposal matters.</p>
<p>The temptation, though, is to confuse assertiveness with aggression. Overused, asserting becomes domineering. It silences others. It leaves no room for dialogue. It builds resistance where it meant to build respect.</p>
<h3>Reflection questions</h3>
<p>&nbsp;•	What exactly am I asking for in one clear sentence?</p>
<p>&nbsp;•	What is non-negotiable, and what is flexible?</p>
<p>&nbsp;•	What two or three anchor reasons will I use to support my position?</p>
<p>&nbsp;•	How will I signal credibility without drifting into name-dropping or status plays?</p>
<p>&nbsp;•	What does matching their energy look like while staying respectful and calm?</p>
<p>&nbsp;•	Where will I deliberately pause to listen so I do not over-talk the room?</p>
<p>&nbsp;•	What early signals tell me I am tipping into pressure, and how will I de-escalate?</p>
<p>&nbsp;•	If directness stalls the dialogue, what is my next door to try?</p>
<h2>Negotiating: The door of balance</h2>
<p>Negotiating persuades by finding common ground. It seeks the middle path, the workable solution, the compromise that honours both sides. It is rooted in the belief that influence is not about victory but about mutual benefit.</p>
<p>You notice this style in those who raise concerns gently, who suggest meeting halfway, who speak in terms of “what if” and “could we.” They are pragmatic. They value win–win outcomes. They prefer flexibility over confrontation.</p>
<p>To enter this door well, we listen closely to uncover the real concern beneath the stated one. We protect what matters most to us while staying open to trade-offs. We come with options. We ask open-ended questions. We remain collaborative, treating the conversation as joint problem-solving.</p>
<p>But compromise has its dangers. Overused, negotiation turns into unnecessary concession. We may be seen as weak or uncertain. Others may begin every conversation expecting us to bend. We may give away value before fully exploring our own position.</p>
<h3>Reflection questions</h3>
<p>&nbsp;•	What is the real concern beneath what is being said, and how will I surface it with open questions?</p>
<p>&nbsp;•	Which larger goals or principles am I protecting, and what are my red lines?</p>
<p>&nbsp;•	What tradables or creative options can I offer that create value without giving away too much?</p>
<p>&nbsp;•	What do I expect in return for each concession?</p>
<p>&nbsp;•	How will I hold a respectful, collaborative tone so we stay in joint problem solving?</p>
<p>&nbsp;•	At what point would compromise become drift, and how will I name that threshold?</p>
<p>&nbsp;•	If others expect automatic concessions from me, how will I reset expectations early?</p>
<p>&nbsp;•	What alternative door will I use if bargaining signals are not present?</p>
<h2>Inspiring: The door of vision</h2>
<p>Inspiring persuades through story, metaphor, and imagination. It calls people to something larger than themselves. It shifts the focus from what is practical to what is possible.</p>
<p>You know you are with an inspiring type when they disengage from technical talk but light up at mention of purpose or possibility. They ask about impact and legacy. They respond to stories and metaphors. They look for meaning, not just measurement.</p>
<p>To enter this door well, we tell relevant stories. We use vivid language and imagery. We connect ideas to values and aspirations. We speak with authenticity and passion. We invite others to imagine: What if you could? What would it feel like if…?</p>
<p>Yet inspiration without grounding can lose its way. Overused, it becomes idealistic, disconnected from reality, heavy with promise and light on delivery. Listeners may nod politely but ask for numbers that never come. Trust fades when vision is not matched by action.</p>
<h3>Reflection questions</h3>
<p>&nbsp;•	What specific story from their world will make this possibility feel real and credible?</p>
<p>&nbsp;•	What image or analogy will help them visualise success?</p>
<p>&nbsp;•	Which values or purpose of theirs does this idea serve, for example impact, legacy, innovation or customer experience?</p>
<p>&nbsp;•	What is the smallest concrete next step that grounds the vision?</p>
<p>&nbsp;•	Where will I invite imagination with a genuine what if question, then check for practical concerns?</p>
<p>&nbsp;•	What numbers or proof points should I have ready if they ask for them?</p>
<p>&nbsp;•	How will I keep my tone authentic so enthusiasm does not outpace deliverables?</p>
<p>&nbsp;•	If inspiration lands politely but without commitment, which door will I move to next?</p>
<h2>Bridging: The door of relationship</h2>
<p>Bridging persuades through connection and social proof. It is the style that builds trust by involving others, by leaning on shared experience, by showing that someone else has walked this path before.</p>
<p>You recognise it when someone asks, “Who else is doing this?” or seems hesitant until they hear a peer’s endorsement. They value rapport and trust more than unknown data. They commit when they see others they respect committing.</p>
<p>To enter this door well, we bring in mutual contacts. We offer testimonials and case studies. We emphasise shared goals. We give before we ask, offering help, making introductions, extending reciprocity. We are warm and attentive, because this style rests on human connection.</p>
<p>But here too lies a danger. Overused, bridging makes us dependent on others. We may delay decisions, waiting for third-party validation. We may undermine our own authority, deferring to outside voices instead of speaking for ourselves.</p>
<h3>Reflection questions</h3>
<p>&nbsp;•	Who do they already trust that I can appropriately involve, and how will I make that connection?</p>
<p>&nbsp;•	Which case study or testimonial best mirrors their context?</p>
<p>&nbsp;•	How will I frame our shared goals so alignment is explicit?</p>
<p>&nbsp;•	What value can I offer first, for example a helpful introduction or insight, before asking for commitment?</p>
<p>&nbsp;•	How will I keep momentum without waiting for every third party to weigh in?</p>
<p>&nbsp;•	Where do I need to speak in my own voice so I do not over-rely on others’ validation?</p>
<p>&nbsp;•	What is my plan to sustain the relationship beyond this single decision?</p>
<p>&nbsp;•	If bridging slows progress, which door should I try to regain decisiveness?</p>
<h2>Living the paradox of influence</h2>
<p>Each of these five doors, Rationalising, Asserting, Negotiating, Inspiring, and Bridging, offers a path into relationship. Each works best when it honours the person we hope to influence rather than serving only our own habits.</p>
<p>The paradox is that any of them, when overused, becomes a wall. Logic turns cold. Conviction turns harsh. Compromise turns weak. Vision turns empty. Relationship turns dependent. What begins as invitation can end as imposition.</p>
<p>The practice is not to abandon our natural style but to notice when it no longer serves. To ask: Which doorway is this person already standing at? What language are they using? What energises them? What shuts them down?</p>
<p>Influence then becomes less about pressing harder on the door we know, and more about walking around to the door they are holding open.</p>
<p>Persuasion is not about clever tactics or manipulation. It is about presence. It is about listening for what matters to others and choosing consciously how we invite them.</p>
<p>The question that remains for each of us is simple: Which door will you choose to knock on in your next conversation?</p>
<p><strong>Do you have any tips or advice? What has worked for you? Do you have any recommended resources to explore? Thanks for reading!</strong></p>

<h2>Influenced by</h2>
<p>The five doors of influence: Rationalising, Asserting, Negotiating, Inspiring, and Bridging, are my synthesis, but several bodies of work have shaped them:</p>
<p>&nbsp;•	Gary Yukl’s research on leadership influence tactics, which identified patterns such as rational persuasion, pressure, exchange, inspirational appeals, and coalition tactics.</p>
<p>&nbsp;•	Robert Cialdini’s principles of persuasion, especially authority, social proof, reciprocity, and consistency, which continue to show how people respond to influence in practice.</p>
<p>&nbsp;•	Negotiation practice and theory, from both organisational behaviour research and classic works on principled negotiation, which highlight the importance of mutual benefit and trade-offs.</p>
<p>&nbsp;•	Leadership and change literature, including Peter Block’s writings on community and invitation, which shape the idea of influence as a relational act rather than a manipulative one.</p>
<p>&nbsp;•	My own work with leadership teams, where repeated practice has confirmed that leaders tend to default to one or two styles, often without realising the limits this creates in how they see and respond to others.</p>
<p>These streams converge in the five doors. They are not meant as a new theory, but as a practical frame that leaders can hold in the moment, to choose how they wish to invite others.</p>
							</div>

																									<div>
		<h4>Share This Story, Choose Your Platform!</h4>
			</div>
														<div>
							<div>
								<img alt="" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2072%2072'%3E%3C/svg%3E" data-src="https://secure.gravatar.com/avatar/63901e783f78308bef40558955107f6d6cccf0ff7a15f1af4fe156bcf55cf86e?s=72&amp;d=mm&amp;r=g" data-srcset="https://secure.gravatar.com/avatar/63901e783f78308bef40558955107f6d6cccf0ff7a15f1af4fe156bcf55cf86e?s=144&amp;d=mm&amp;r=g 2x" height="72" width="72" decoding="async">							</div>
							<div>
								With 30+ years in business and nearly the sames as a trained coach and facilitator, Andi Roberts works with leaders and teams through executive coaching, leadership training, and facilitation. He shares ideas to spark better thinking, stronger collaboration, and lasting results. Passionate about the environment and community, he helps organisations thrive in an ever-changing world.							</div>
						</div>
								<div>
					<h2>
						Related Posts					</h2>
					<span></span>
					
				</div><!-- related-posts -->


													


		<!-- #respond -->
	<p>This site uses Akismet to reduce spam. <a href="https://akismet.com/privacy/" target="_blank" rel="nofollow noopener">Learn how your comment data is processed.</a></p>													</article>
	</section>  <!-- fusion-row -->
				</main>  <!-- #main -->
				
				
								
					
		 <!-- fusion-footer -->

		
																</div> <!-- #boxed-wrapper -->
				<a tabindex="-1" href="#" aria-hidden="true">Page load link</a>

		

			<section aria-labelledby="awb-to-top-label">
		<a href="#" id="toTop">
			<span id="awb-to-top-label">Go to Top</span>

					</a>
	</section>
		


</div>]]></description>
        </item>
    </channel>
</rss>