<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 16 Dec 2023 09:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Advancements in machine learning for machine learning (165 pts)]]></title>
            <link>https://blog.research.google/2023/12/advancements-in-machine-learning-for.html</link>
            <guid>38661296</guid>
            <pubDate>Sat, 16 Dec 2023 02:50:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2023/12/advancements-in-machine-learning-for.html">https://blog.research.google/2023/12/advancements-in-machine-learning-for.html</a>, See on <a href="https://news.ycombinator.com/item?id=38661296">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-6088118107306075362">
<p><span>Posted by Phitchaya Mangpo Phothilimthana, Staff Research Scientist, Google DeepMind, and Bryan Perozzi, Senior Staff Research Scientist, Google Research</span>

</p><p>
With the recent and accelerated advances in machine learning (ML), machines can <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">understand natural language</a>, <a href="https://blog.google/technology/ai/lamda/">engage in conversations</a>, <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview">draw images</a>, <a href="https://arxiv.org/abs/2210.02303">create videos</a> and more. Modern ML models are programmed and trained using ML programming frameworks, such as <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://github.com/google/jax">JAX</a>, <a href="https://pytorch.org/">PyTorch</a>, among many others. These libraries provide high-level instructions to ML practitioners, such as linear algebra operations (e.g., matrix multiplication, convolution, etc.) and neural network layers (e.g., <a href="https://keras.io/api/layers/convolution_layers/convolution2d/">2D convolution layers</a>, <a href="https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/">transformer layers</a>). Importantly, practitioners need not worry about how to make their models run efficiently on hardware because an ML framework will automatically optimize the user's model through an underlying <em>compiler</em>. The efficiency of the ML workload, thus, depends on how good the compiler is. A compiler typically relies on heuristics to solve complex optimization problems, often resulting in suboptimal performance.
</p> <p>
In this blog post, we present exciting advancements in ML for ML. In particular, we show how we use ML to improve efficiency of ML workloads! Prior works, both internal and external, have shown that we can use ML to improve performance of ML programs by selecting better ML compiler decisions. Although there exist a few datasets for program performance prediction, they target small sub-programs, such as basic blocks or kernels. We introduce “<a href="https://arxiv.org/abs/2308.13490">TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs</a>” (presented at <a href="https://nips.cc/Conferences/2023">NeurIPS 2023</a>), which we recently released to fuel more research in ML for program optimization. We hosted a <a href="https://www.kaggle.com/competitions/predict-ai-model-runtime/overview">Kaggle competition</a> on the dataset, which recently completed with 792 participants on 616 teams from 66 countries. Furthermore, in “<a href="https://arxiv.org/abs/2305.12322">Learning Large Graph Property Prediction via Graph Segment Training</a>”, we cover a novel method to scale <a href="https://arxiv.org/abs/2005.03675">graph neural network</a> (GNN) training to handle large programs represented as graphs. The technique both enables training arbitrarily large graphs on a device with limited memory capacity and improves generalization of the model.
</p>





<h2>ML compilers</h2>


<p>
ML compilers are software routines that convert user-written programs (here, mathematical instructions provided by libraries such as TensorFlow) to executables (instructions to execute on the actual hardware). An ML program can be represented as a computation graph, where a node represents a tensor operation (such as <a href="https://en.wikipedia.org/wiki/Matrix_multiplication">matrix multiplication</a>), and an edge represents a tensor flowing from one node to another. ML compilers have to solve many complex optimization problems, including <em>graph-level </em>and <em>kernel-level</em> optimizations. A graph-level optimization requires the context of the entire graph to make optimal decisions and transforms the entire graph accordingly. A kernel-level optimization transforms one kernel (a fused subgraph) at a time, independently of other kernels.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s1999/image6.png"><img data-original-height="465" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKHc-UWtoIPsAxdaadumdb1n3K8jz02HYoAbz3CG1BChwyvcNzLUSniYSdRi7HJdPoc2ObwFrs3vSQVN43pVmpUtcnCsSyYU3aypLI-Qyg5AVQRf869uwnP-lNOT2HR3LASYeBGDpWx727a5_mvYe2Oe5F_039pysTR4nKfCDGW9YpklkmYS_lOD_D7k8w/s16000/image6.png"></a></td></tr><tr><td>Important optimizations in ML compilers include graph-level and kernel-level optimizations.</td></tr></tbody></table>


<p>
To provide a concrete example, imagine a <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">matrix</a> (2D tensor):
</p>





<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s1999/image4.png"><img data-original-height="290" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjCUHoSLc08Cq3T475pnipGS1x1rFt0c8cuRCAIiTLWM70FeQMjHkYprNnnroiH0V4PdhtCM2lU7quJ7vJMUX_113S1RAvKZdSQAFiIvdpxrcr8HvnjQti97F3JE5Sno8UKDyjjirTOB5JrhG4kzz2uUiu0GwZ0p5lG5nmhamEPKgaGq1j5cQ9OU9cXj348/s16000/image4.png"></a></p>



<p>
It can be stored in computer memory as [A B C a b c] or [A a B b C c], known as <a href="https://en.wikipedia.org/wiki/Row-_and_column-major_order">row- and column-major memory layout</a>, respectively. One important ML compiler optimization is to assign memory layouts to all intermediate tensors in the program. The figure below shows two different layout configurations for the same program. Let’s assume that on the left-hand side, the assigned layouts (in red) are the most efficient option for each individual operator. However, this layout configuration requires the compiler to insert a <em>copy</em> operation to transform the memory layout between the <em>add</em> and <em>convolution</em> operations. On the other hand, the right-hand side configuration might be less efficient for each individual operator, but it doesn’t require the additional memory transformation. The layout assignment optimization has to trade off between local computation efficiency and layout transformation overhead.
</p>





<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s1999/image3.png"><img data-original-height="343" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_I4RC-UA10yE2aMURcmyfHSZ5_srJ0mxKGxjrUrS6nvEQVNK2CMAp_k__fjCQW8Y20z6IXqA7Fww44Wz5uUAuo6y9njqYxXUPBHu0HGOWfVHFkZ785QZg2mBslF1mrlEmQVt4L60ICPRwk_cbb293MLIx50FT6EsF75xfWi6oa0bCvPAurBB3Y8Mvp1uo/s16000/image3.png"></a></td></tr><tr><td>A node represents a tensor operator, annotated with its output tensor shape [<em>n<sub>0</sub></em>, <em>n<sub>1</sub></em>, ...], where <em>n<sub>i </sub></em>is the size of dimension <em>i</em>. Layout {<em>d<sub>0</sub></em>, <em>d<sub>1</sub></em>, ...} represents minor-to-major ordering in memory. Applied configurations are highlighted in red, and other valid configurations are highlighted in blue. A layout configuration specifies the layouts of inputs and outputs of influential operators (i.e., convolution and reshape). A copy operator is inserted when there is a layout mismatch.</td></tr></tbody></table>



<p>
If the compiler makes optimal choices, significant speedups can be made. For example, we have seen <a href="https://ieeexplore.ieee.org/document/9563030">up to a 32% speedup</a> when choosing an optimal layout configuration over the default compiler’s configuration in the <a href="https://www.tensorflow.org/xla">XLA</a> benchmark suite.
</p>




<h2>TpuGraphs dataset</h2>


<p>
Given the above, we aim to improve ML model efficiency by improving the ML compiler. Specifically, it can be very effective to equip the compiler<strong> </strong>with a <a href="https://arxiv.org/abs/2008.01040">learned cost model</a><strong> </strong>that takes in an input program and compiler configuration and then outputs the predicted runtime of the program. 
</p>
<p>
With this motivation, we <a href="https://arxiv.org/abs/2308.13490">release TpuGraphs</a>, a dataset for learning cost models for programs running on Google’s custom <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Tensor Processing Units</a> (TPUs). The dataset targets two XLA compiler configurations: <em>layout</em> (generalization of row- and column-major ordering, from matrices, to higher dimension tensors) and <em>tiling</em> (configurations of tile sizes). We provide download instructions and starter code on the <a href="https://github.com/google-research-datasets/tpu_graphs">TpuGraphs GitHub</a>. Each example in the dataset contains a computational graph of an ML workload, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source ML programs, featuring popular model architectures, e.g., <a href="https://arxiv.org/abs/1512.03385">ResNet</a>, <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a>, <a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a>, and <a href="https://arxiv.org/abs/1706.03762">Transformer</a>. The dataset provides 25× more graphs than the largest (earlier) graph property prediction dataset (with comparable graph sizes), and graph size is 770× larger on average compared to existing performance prediction datasets on ML programs. With this greatly expanded scale, for the first time we can explore the graph-level prediction task on large graphs, which is subject to challenges such as scalability, training efficiency, and model quality.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s2868/image18.png"><img data-original-height="1546" data-original-width="2868" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtmmh8CXbMXvAmeBpyjMGMrfq2MEXlxeczYqEOUJvps1oca9G_Xlc34D8-vnwUHTDZDlgIZDYqTfPUVF_qi7AJlPw5fV4Whwz8BOEQs-l1S-7TDLqhIsiCXbQN78SPa8yoTE86438CChMyQVhUFEy3vQRiWhQOFhOKfwH3IbIVbhl5Fe1gDTjBFL2AsDBX/s16000/image18.png"></a></td></tr><tr><td>Scale of TpuGraphs compared to other graph property prediction datasets.</td></tr></tbody></table>




<p>
We provide baseline learned cost models with our dataset (architecture shown below). Our baseline models are based on a GNN since the input program is represented as a graph. Node features, shown in blue below, consist of two parts. The first part is an <em>opcode id</em>, the most important information of a node, which indicates the type of tensor operation. Our baseline models, thus, map an opcode id to an <em>opcode embedding</em> via an embedding lookup table. The opcode embedding is then concatenated with the second part, the rest of the node features, as inputs to a GNN. We combine the node embeddings produced by the GNN to create the fixed-size embedding of the graph using a simple graph pooling reduction (i.e., sum and mean). The resulting graph embedding is then linearly transformed into the final scalar output by a feedforward layer.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s2284/image20.png" imageanchor="1"><img data-original-height="1106" data-original-width="2284" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKH2VD6leBGzfWhjN5LA17iDfKlYLIglznzDDI_A73_Jfs4D-tpZR2gPRe2wpmfrdYv3raxf1Dl2m8YNkW3NcUlUWywpCi8GnJGSnEth8ITn0m387T6Z1Ye-f1SJX81fw_pTfAhgAfQ06NDFK6ahEMPEA6g7vc_jzAb4pHIZHFEUp1rEIH4MrKaGIVhdw3/s16000/image20.png"></a></td></tr><tr><td>Our baseline learned cost model employs a GNN since programs can be naturally represented as graphs.</td></tr></tbody></table>




<p>
Furthermore we present <a href="https://arxiv.org/abs/2305.12322">Graph Segment Training</a> (GST), a method for scaling GNN training to handle large graphs on a device with limited memory capacity in cases where the prediction task is on the entire-graph (i.e., graph-level prediction). Unlike scaling training for node- or edge-level prediction, scaling for graph-level prediction is understudied but crucial to our domain, as computation graphs can contain hundreds of thousands of nodes. In a typical GNN training (“Full Graph Training”, on the left below), a GNN model is trained using an entire graph, meaning all nodes and edges of the graph are used to compute gradients. For large graphs, this might be computationally infeasible. In GST, each large graph is partitioned into smaller segments, and a random subset of segments is selected to update the model; embeddings for the remaining segments are produced without saving their intermediate activations (to avoid consuming memory). The embeddings of all segments are then combined to generate an embedding for the original large graph, which is then used for prediction. In addition, we introduce the historical embedding table to efficiently obtain graph segments’ embeddings and segment dropout to mitigate the staleness from historical embeddings. Together, our complete method speeds up the end-to-end training time by 3×.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s790/image2.png"><img data-original-height="434" data-original-width="790" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgX7cF7Sl6XY3xYwmUp7lqPoXGsXaV3RqVC0YgUG_AQy4XAv7B75eaWpZy_gmbq0UG6VVX0wEjvuOyg2Ce7ALeuVLaUBgKUvEWvAi0RnYsN61d7z3YAm9NEeCpEKQu_YU7ZTAgS71h6mq-bpX6H2HFv73TIHd_W1DsZyastPfjG4hogWf3cy8uf_5DGJjDV/s16000/image2.png"></a></td></tr><tr><td>Comparing Full Graph Training (typical method) vs Graph Segment Training (our proposed method).</td></tr></tbody></table>




<h2>Kaggle competition</h2>


<p>
Finally, we ran the “<a href="https://kaggle.com/competitions/predict-ai-model-runtime">Fast or Slow? Predict AI Model Runtime</a>” competition over the TpuGraph dataset. This competition ended with 792 participants on 616 teams. We had 10507 submissions from 66 countries. For 153 users (including 47 in the top 100), this was their first competition. We learned many interesting new techniques employed by the participating teams, such as:
</p>
<ul>

<li><em>Graph pruning / compression</em>: Instead of using the GST method, many teams experimented with different ways to compress large graphs (e.g., keeping only subgraphs that include the configurable nodes and their immediate neighbors).

</li><li><em>Feature padding value</em>: Some teams observed that the default padding value of 0 is problematic because 0 clashes with a valid feature value, so using a padding value of -1 can improve the model accuracy significantly.

</li><li><em>Node features</em>: Some teams observed that additional node features (such as <a href="https://www.tensorflow.org/xla/operation_semantics#dot">dot general’s contracting dimensions</a>) are important. A few teams found that different encodings of node features also matter.

</li><li><em>Cross-configuration attention</em>: A winning team designed a simple layer that allows the model to explicitly "compare" configs against each other. This technique is shown to be much better than letting the model infer for each config individually. 
</li>
</ul>
<p>
We will debrief the competition and preview the winning solutions at the competition session at the <a href="https://mlforsystems.org/">ML for Systems workshop</a> at NeurIPS on December 16, 2023. Finally, congratulations to all the winners and thank you for your contributions to advancing research in ML for systems!
</p>





<h2>NeurIPS expo</h2>


<p>
If you are interested in more research about structured data and artificial intelligence, we hosted the NeurIPS Expo panel <a href="https://nips.cc/Expo/Conferences/2023/talk%20panel/78252">Graph Learning Meets Artificial Intelligence</a> on December 9, which covered advancing learned cost models and more! 
</p>




<h2>Acknowledgements</h2>


<p>
<em>Sami Abu-el-Haija (Google Research) contributed significantly to this work and write-up.   The research in this post describes joint work with many additional collaborators including Mike Burrows, Kaidi Cao, Bahare Fatemi, Jure Leskovec, Charith Mendis, Dustin Zelle, and Yanqi Zhou.</em>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bluetooth keystroke-injection in Android, Linux, macOS and iOS (148 pts)]]></title>
            <link>https://github.com/skysafe/reblog/tree/main/cve-2023-45866</link>
            <guid>38661182</guid>
            <pubDate>Sat, 16 Dec 2023 02:26:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/skysafe/reblog/tree/main/cve-2023-45866">https://github.com/skysafe/reblog/tree/main/cve-2023-45866</a>, See on <a href="https://news.ycombinator.com/item?id=38661182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:skysafe/reblog" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="IGY1jHWvj45J7PAhwmSm3IcdAgcoSkzoDHuOMXnnxEauLpqeRxlktc30r9E6XkyKEnfEhMXshXrYQTnMOd0Ehg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="skysafe/reblog" data-current-org="skysafe" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=toL9V8ml0zAn3gyLixuwanlGPvABX29m4y6axKT6ADVEjH%2BuLJR8xyvWLnEkNA4snWBUWa6l8P%2FpF%2FqMgUQABQ%2B3BHZBAA9lqAkIEbLtsrZLoxJPFFyV6uU4%2FiDs1h9p6S7O7W5ZDMdAnZc93EOuvBZcMn9YQoJJ0FGvBofnKfScuEUcupBsn3Q%2FIlxhy%2F4uPxsPpC5fFTA%2FhNXYWoWKslM3bsJq0dXRv5eAS9Yo%2FpF30ZqlMkC3%2FXeFZ035pO5xWQlmoDd3Kpm83zRwBGqh8v%2FBJaR1lbtCSlaO1pSho2Ap9oml%2BQazM0LtupFlArIPbYFZxvGaMwTBvdwE51c6mOt232n73aZh38jQ8NpXYTEw7SChMz0gdFR%2B8gWUGbRd%2FHkNlmXRw9Gf4OkrzJRIwVGbe6IKanPl8gTJITm3S5U1dIkH8j2pRVCbqupbP0HLQa4E5rJ%2F8grpMc1iPmuRHo%2FpDk8HXdm%2BXbaYxGhFBPy%2Fmbd%2Bg2lTlJhwMTkb6zpxwcUwWhSjy%2B3q6sZSgX%2F7VOTkR6ZxV0Qjqys1aCEf3ruShkX1MnH1ZubU--mPjgXi7ZKrurNwYN--QRIQ1I18v%2BMEZ6XhBYKZHA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Ffiles%2Fdisambiguate&amp;source=header-repo&amp;source_repo=skysafe%2Freblog" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/skysafe/reblog/tree/main/cve-2023-45866&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="2a181ee65596245633167bff86efb2eedc190dbf33fef3e419c987eda2894bcf" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/files/disambiguate;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Location history data in Google Maps will soon be stored on user devices (106 pts)]]></title>
            <link>https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12</link>
            <guid>38660646</guid>
            <pubDate>Sat, 16 Dec 2023 00:39:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12">https://www.businessinsider.com/google-maps-location-data-history-stored-locally-2023-12</a>, See on <a href="https://news.ycombinator.com/item?id=38660646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="piano-inline-content-wrapper" data-piano-inline-content-wrapper=""> 
                    
                    
                    
                          
                          
                          <section data-offer-key="pre-churn-offer" data-component-type="inline-offer" data-place-after-element-selector=".post-content .content-lock-content > p">
                            <article>
                              <img src="https://www.businessinsider.com/public/assets/subscription/marketing/banner-overlay/top-left.svg" alt="">
                              <img src="https://www.businessinsider.com/public/assets/subscription/marketing/banner-overlay/bottom-right.svg" alt="">
                          
                                        </article>
                          </section>
                    
                    <div data-component-type="content-lock" data-load-strategy="exclude">
                                  <ul><li>Location history data in <a target="_blank" href="https://www.businessinsider.com/google-maps-best-features-tips-tricks-2019-5" data-analytics-product-module="summary_bullets" rel="">Google Maps</a> will soon be stored directly on user devices.</li><li>Google itself will no longer have access to the data.</li><li>This also means law enforcement won't be able to request it from Google anymore.</li></ul><!-- Excluded mobile ad on desktop --><div id="formContainer" data-component-type="inline-newsletter-module" data-event-label="insider_today" data-newsletter-id="1" data-newsletter-title="Insider Today" data-acq-source="techinlinesignup">
                        
                        
                          <div>
                              <p>Thanks for signing up!</p>
                              
                              <p>
                              Access your favorite topics in a personalized feed while you're on the go.
                                    </p>
                            </div>
                        
                            <div>
                                <p><img src="https://www.businessinsider.com/public/assets/rebrand/newsletter-bull.png" data-old-src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 1 1'%3E%3C/svg%3E" data-src="/public/assets/rebrand/newsletter-bull.png">
                              
                              
                              
                              </p>    </div>
                        
                          
                        </div><p>Google is making some changes in Google Maps that will increase user privacy.</p><p>Data from the Timeline feature in Google Maps, which is controlled by the Location History setting and keeps a record of routes and trips users have taken, will soon be stored directly on users' devices instead of by Google.</p><p>That means Google itself will no longer have access to user location history data. And by extension, neither will law enforcement, which has often <a target="_blank" href="https://www.businessinsider.com/police-getting-help-social-media-to-prosecute-people-seeking-abortions-2023-2" data-analytics-product-module="body_link" rel="">requested user location data from Google</a> — for example, through "geofence" orders, which request data about every user who was near a specific place at a specific time.</p><p><span></span><span><p>Google has come under increasing pressure to stop collecting user location data, especially since <a target="_blank" href="https://www.businessinsider.com/supreme-court-abortion-decision-final-ruling-scotus-roe-v-wade-2022-6#:~:text=The%20Supreme%20Court%20overturned%20the,Friday%2C%20while%20Republicans%20celebrated%20it." data-analytics-product-module="body_link" rel="">Roe v. Wade was overturned</a>. Location data, along with internet search history and even <a target="_blank" href="https://www.businessinsider.com/roe-abortion-surveillance-location-data-scotus-computer-search-history-2022-6" data-analytics-product-module="body_link" rel="">messaging history can be used as criminal evidence</a> against individuals who get an abortion in states where abortion is illegal.</p><p>42 Democrats from the US House and Senate signed a letter last May addressed to Google CEO Sundar Pichai urging the company to <a target="_blank" href="https://www.businessinsider.com/democrats-demand-google-stop-collecting-location-data-abortion-rights-2022-5" data-analytics-product-module="body_link" rel="">stop collecting and retaining user location information</a>.</p><p>"Google's current practice of collecting and retaining extensive records of cell phone location data will allow it to become a tool for far-right extremists looking to crack down on people seeking reproductive health care," the letter read.</p><p>Last July, Google announced it would <a target="_blank" href="https://www.businessinsider.com/google-says-delete-location-data-of-users-visiting-abortion-clinics-2022-7" data-analytics-product-module="body_link" rel="">delete the location history data of users who visited abortion clinics</a>, drug treatment centers, domestic violence shelters, weight loss clinics, and other sensitive health-related locations. The company said that if its systems identified that a user had visited one of these sensitive locations, it would then delete the entry from that user's location history "soon after they visit."</p><p>Now this control is back in the hands of individual users.</p><p>Google told Business Insider that the update is part of a larger effort by the company to increase user privacy and give individuals more control over their data, pointing to other tools like auto-delete and Incognito Mode. It says the response to the Location History update has been positive.</p><p>The Location History setting is turned off by default in Google Maps, but here's how to find it in the app, toggle it on or off, and delete specific entry:</p><p><strong>1) Click on the icon in the top right corner of the screen.</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c875f0ec98e92f75000a0&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google Maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Click on your user icon.
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p><strong>2) Click on "Your data in Maps."</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c89237a3c8094d5dd8cf4&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google Maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Find "Your data in Maps."
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p><strong>3) Scroll down to "Location History."</strong></p><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/png" data-srcs="{&quot;https://i.insider.com/657c88c950edbc52a864cb77&quot;:{&quot;contentType&quot;:&quot;image/png&quot;,&quot;aspectRatioW&quot;:1179,&quot;aspectRatioH&quot;:2556}}" alt="Google maps" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Find "Location History."
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Kylie Kirschner
                          
                          </span>
                              </span>
                          </figure><p>Here you can turn the Timeline feature and your location history on or off, and change your backup and auto-delete settings.</p><p>Clicking on "See &amp; delete activity" will allow you to see any location history that's already been saved in Google Maps and give you the option to delete specific entries.</p></span></p><!-- Excluded mobile ad on desktop --><!-- Excluded mobile ad on desktop -->
                          
                        <!-- Excluded mobile ad on desktop -->
                          
                        
                          
                        
                      </div>
                    
                    
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Smallest USB-C MIDI Synth (192 pts)]]></title>
            <link>https://mitxela.com/projects/smsc</link>
            <guid>38658497</guid>
            <pubDate>Fri, 15 Dec 2023 20:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitxela.com/projects/smsc">https://mitxela.com/projects/smsc</a>, See on <a href="https://news.ycombinator.com/item?id=38658497">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mxmain"><p><a href="https://mitxela.com/projects/hardware"><img onload="this.style.opacity=1;" src="https://mitxela.com/img/titles/mitxela_dot_com-65.png" title="Back to Hardware" alt="Back to Hardware"></a></p><p>15 Dec 2023<br><b>Progress: Complete</b></p><p>
A new entrant in my series of "smallest and worst" MIDI synthesizers. (I'm not including the <a href="https://mitxela.com/projects/flash_synth">flash synth</a> in that list, which isn't supposed to be the worst!)</p><p>

Here's a video of the creation:</p><p>

<iframe width="704" height="396" src="https://www.youtube.com/embed/bmFmsn6VZSM" allowfullscreen=""></iframe></p><h3>Story</h3><p>
The last few weeks I've been dabbling around with the CH32V003, a 32-bit RISC-V microcontroller that's unbelievably cheap.</p><p>

One of the first things that occurred to me, when I noticed it didn't have hardware USB but the processor is clocked at 48MHz, is that it would be awesome to write a software USB stack for it. I have wanted, for a long time, to dig deep and write a bit-banged USB library, just because it's the best way to learn. I greatly enjoyed writing my <a href="https://mitxela.com/projects/kiloboot">ethernet bootloader in assembly</a>. It's hard to justify writing a USB stack from scratch when one already exists, however, so when I saw the CH32V003 I thought this was the perfect time to do something both educational and <i>useful</i>.</p><p>

Picture my surprise to find that CNLohr has <a href="https://github.com/cnlohr/rv003usb">already done it</a>! I can't exactly complain, that's a fantastic achievement and makes the chip even more useful and impressive than it already was.</p><p>

The very least I can do is get some USB-MIDI working on the chip. At the time of writing, the USB-MIDI demo was unfinished, so I tried it out by soldering a dev board together. It started out a little smaller, but by the end of the experimentation my board looked like this:</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb1.jpg" alt="Messy dev board creation"></p><p>

That's a TSOP20 breakout and a Micro-USB breakout, superglued together with some other scrap circuit board. A regulator, capacitors and a few resistors complete the circuit. The two header pins are my programming header (though with the USB plugged in, we don't even need to connect the ground pin, and could get away with just one pin to program).</p><p>

On the right is a set of buttons. I configured the USB-MIDI device to play notes when these buttons are pressed.</p><p>

At the bottom is a piezo buzzer. Naturally, when MIDI data arrives at the chip, it produces a square wave. I did this with one of the hardware timers of the chip, outputting in differential mode to maximise volume.</p><p>

USB MIDI messages are four bytes, and our low-speed USB endpoint can be eight bytes, so we could (and normally would) send two MIDI messages per packet. However for this simple demo I just blocked until the next USB interrupt for each message.</p><p>

A bit of MIDI loopback on the host side, and we have a really terrible toy keyboard!</p><h3>USB dev board</h3><p>
There are a few dev boards available for the CH32V003, but it doesn't look like any of them wire up the USB pins, probably because there's no hardware USB. I doubt this is the last USB project I'll do with it, so to avoid having to repeatedly wire up that mess above, a simple dev board is in order. I tried to make it as small as possible.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb3.png" alt="KiCad screenshot of dev board"></p><p>

All the necessary pins are broken out, all pins are labelled on both sides. The 1.5K resistor can be soldered in one of two positions, either to D5, or direct to VDD if you don't need software reconnection of USB and want to use D5 for something else. On the underside, the USB data lines can be cut and series resistors can be added, if needed.</p><p>

The three pins in the top right corner are 3V3, GND and D1, which is what I've been using as a programming header. You can either connect all three (to program it with USB unplugged), or D1 and ground, or just D1 if the programmer is on the same machine that the USB is plugged into.</p><p>

The pins around the edge are .1 inch pitch, and the board is 15.2mm by 20.3mm total.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/ch32v003-usb2.jpg" alt="Clean dev board creation"></p><p>

Here's the obligatory 3D model of the board:</p><p>


<model-viewer src="/img/uploads/sw/model-viewer/ch32v003usb.wrl" poster="/img/uploads/sw/model-viewer/ch32v003usb.jpg" loading="onclick">
  Sorry, your browser doesn't support embedded 3D models.
</model-viewer></p><p>

The KiCad design files for this board are published <a href="https://github.com/mitxela/ch32v003usb-hw">here</a> and <a href="https://git.mitxela.com/ch32v003usb-hw">here</a>.</p><h3>More USB, all the USB</h3><p>
Given how cheap the part is and how few supporting components it needs (the USB stack even does the same oscillator calibration frame timing trick that V-USB does), I thought it might be fun to recreate some of my USB ATtiny projects.</p><p>

The <a href="https://mitxela.com/projects/stylocard">stylocard</a> comes to mind first. I've done a few unpublished redesigns of that board in the past, and the best improvement was getting rid of the analog input method which was a little unreliable once the thing got dirty, and switching to direct readout, which means a microcontroller with at least 22 GPIO. One day I should publish all that. Unfortunately our CH32V003 doesn't have enough pins to read the keyboard and do USB together.</p><p>

We could just go with the same as before, a bunch of resistors and make the reading analog, or we could try and do something clever, or even just add a shift register, but it then occurred to me that since the CH32V003 is <i>so</i> cheap, why not just stick two of them on the board? It would be hilarious. One of them could read half the keyboard, the other would read the rest and also do USB.</p><p>

The possibilities are not unlimitless!</p><p>

Recreating my <a href="https://mitxela.com/projects/silly_synth">smallest USB MIDI synth</a> was the next thought. Something I've wanted to do for a while is produce a thin circuit board as they do for some dongles, sliding the thin circuit board inside of the USB-A plug, essentially creating the circuit I did before but in a way that can be mass-produced and easily assembled.</p><p>

But a more interesting idea was to bring the synth forwards through time into the age of USB-C. Electronically this just means adding a couple of resistors and fitting the right connector. You can't mount electronics inside a USB-C plug so easily though. I did find some mid-mount USB-C plugs which may have worked, but after a bit more searching I settled on this vertical mount type, intended for building docks.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/vertical-usb-c.jpg" alt="Vertical mount USB-C plug"></p><p>

The part number is USB4151 although there are a few similar parts from different suppliers.</p><p>

When USB-C was introduced, a lot of engineers complained about the difficulty of fanning out the connections. It seems the designers of the connector assumed that everyone would be using high density boards with microvias. The footprint alone of this connector is technically beyond the spec of a standard 6/6mil process, and that's before we've added any traces.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/usb-footprint0.jpg" alt="Footprint for the USB-C connector from the datasheet"></p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/usb-footprint.png" alt="Footprint for the USB-C connector in KiCad"></p><p>

The plastic studs require a non-plated through hole in very close proximity of a plated hole. For this joke project I'm not going to pay for tighter tolerances, so I decided to just ignore the DRC violation and if they aren't able to manufacture the holes I can trim off the plastic studs with a knife.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/vertical-usb-c2.jpg" alt="Vertical mount USB-C plug underside"></p><p>

The difficulty of fanout on a two layer board is such that special USB-C connectors are available, that don't break out all of the pins, if all you need is USB 2.0 or just power. However, they're not available in this vertical format, and besides I eat tricky routing problems for breakfast.</p><h3>Routing</h3><p>
With the vertical-mount USB-C plug, our ambition is to make the smallest possible circuit board underneath it, that can fit within the diameter of an ordinary piezo buzzer. The buzzer has a pin pitch of 7.62mm, or 0.3 inches, and the outer diameter is 13.8mm, but we want our circuit to fit inside the depression, that meniscus of the potting compound, which means a maximum diameter of about 12mm.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/piezo.jpg" alt="Piezo buzzer"></p><p>

There's no possible orientation where the piezo's pins don't foul the USB support pins. To deal with this, I widened the footprint spacing. It should be fine to bend the pins outwards a little, but if it doesn't fit we can file them down too. As the design iterated, I reshuffled this a few times, eventually I got them down to just 8mm apart.</p><p>

We don't need to connect the USB 3 pins, that's the four superspeed pairs and the two SBU pins, but we do need to wire up CC1 and CC2, which totals 14 pins to connect. Keeping the copper annulus around each plated hole as small as possible, it's <i>just</i> possible to have all the necessary tracks escape.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing1.png" alt="KiCad screenshot"></p><p>

Naturally the tracks are rounded with teardrops, because I have standards to live up to.</p><p>

As the shielding pins are all connected, we could cheat and not connect the grounds together on the board, but in the end it was fine to route these all together too.</p><p>

On the underside, routing is just as tight, with the QFN part shoved off-centre to make enough room for the tracks. Thankfully it doesn't hurt if we connect unused GPIO to ground (or to other signals really), so we can conveniently route ground right through the middle of the chip instead of going around.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing2.png" alt="KiCad screenshot"></p><p>

The regulator is an SC-70 package, that's like the miniature version of SOT-23. You can get even smaller regulators but it didn't seem like it would be an issue. Similarly, around the periphery I've used resistors and capacitors in 0603 format, just because there's no real pressure for space once we're outside of the piezo/USB/QFN footprint mess.</p><p>

On the front side I put three test pads, for power, ground and D1 (SWIO) for programming. In reality only one pad is needed, I'm just going to plug the USB-C in with an extension cable and touch a single wire to the programming pad.</p><h3>Panelization</h3><p>
I wanted to do the panel myself for three reasons.</p><p>

When you get boards made that are this small, and you plan to stencil solder paste onto them, it's extremely fiddly to hold things if you don't have a frame. A 12mm circle would be very tedious to hold at the best of times, but here we have components on both sides so after one side is soldered it'll be almost impossible to stencil the other.</p><p>

Secondly, I specifically wanted the panel to have explicit symmetry. We use the frame and the mounting holes to align the stencil. To save on the tedium of doing this twice, I put down two copies of the design, with the second flipped over. The whole panel is symmetric, so we can stencil one side, flip the board and stencil the other.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/routing3.jpg" alt="KiCad screenshot of panel"></p><p>

The third reason to do the panel myself is that I wanted to also use the frame as a jig. There's an oval hole in the middle designed to be a tight fit around a USB-C plug. Once the board is part-soldered and broken out of the panel, it's going to be a real pain to do anything to it, so this at least should give us a basic grip on the thing. We could have made a jig ourselves out of something else, but there aren't many materials that can be laser-cut and would survive the reflow oven. In a sense, FR4 is the perfect support material.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/render2.jpg" alt="Render of panel with piezos hidden"></p><p>

I should take this moment to praise just how useful the 3D model viewer is. I can remember once having a bizarre argument with a contractor who was a little old-school and failed to see the benefit of it. He'd grown up with OrCAD in the 90s and insisted that setting up 3D models for part footprints was a waste of time, or at the very least, not his job. 3D modelling is for the mechanical engineer, he kept saying.</p><p>

But being able to look at a realistic render provides such a huge safety buffer against silly mistakes. In the old days, we used to spend hours poring over gerber files looking for common mistakes because if you made one, it could set everything back by weeks. And they happened all the time! Things like missing the soldermask aperture on a footprint, or exporting shapes onto the wrong layers. Ever since we shifted to KiCad and made full use of the 3D viewer, I don't think I've ever made one of those mistakes. I still check the gerber files religiously, but the 3D viewer is a second layer of defence against mistakes.</p><p>

In the render below, I've highlighted the piezo, and you can immediately see that the pins don't quite line up with the footprint. This is because I've intentionally altered the footprint in the hope that we can bend those pins outward a little, as mentioned above. But it's exactly the kind of thing that the 3D viewer can help you with, to get a visual on the interference and whether it looks like it's going to work. Or at the very least, it might make you go back and check the 3D model is correct.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/render1.jpg" alt="Render of panel, with piezo highlighted in green"></p><p>

Here's an interactive 3D model if you're especially keen:</p><p>

<model-viewer src="/img/uploads/sillysynth/c/sillysynth2.wrl" poster="/img/uploads/sillysynth/c/render3.jpg" loading="onclick">
  Sorry, your browser doesn't support embedded 3D models.
</model-viewer></p><h3>Assembly</h3><p>
If you get your boards made at the lowest tolerances and they're below a certain size, manufacturers will subsidise the price. It's essentially a promotional deal, they charge you almost nothing because it costs them almost nothing to chuck tiny boards into the corners and crooks of other panels. I wonder how many other people have tried to produce a board with a (nominally) USB 3.2 Gen 2 connector on a two layer, 6/6mil tolerance board.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/bareboard.jpg" alt="Bare board in hand"></p><p>

They didn't question my footprint at all, and it seems to have been produced without problems.</p><p>

The correct order of assembly is to do the tiny parts first, and the USB connector last. The through-mount aspect of the USB connector means it's not possible to stencil the other side once it's fitted.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly1.jpg" alt="One board assembled still in the panel"></p><p>

The USB connector comes with a plastic cap, to allow you to pick it up with a vacuum nozzle.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly2.jpg" alt="Close up of USB-C connector on board"></p><p>

I had planned to reflow a bunch of these, but it's so small I ended up doing all of them with the heat gun. It's possible (and not that unusual) to reflow a board with components underneath already soldered. Even if the solder melts, surface tension holds them in place. It's also possible to use two different alloys of solder with different melting points if it's a concern. But hitting it with the heatgun it's easy enough to direct the heat only to where needed.</p><p>

If I had reflowed it though, the plan was to stencil and place components on both sides, then reflow the whole thing at once. Something like the following picture:</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly3.jpg" alt="Mockup of how the panel could be used to support the board"></p><p>

The connector itself would poke through the grating that makes up the bed of the reflow oven.</p><p>

Anyway I didn't do that, I just soldered them all in place as it was way less tricky than I'd imagined.
 
<img src="https://mitxela.com/img/uploads/sillysynth/c/production-line.jpg" alt="Two boards being assembled"></p><p>

Carefully snap them out of the panel and file the rough edges a little.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly4.jpg" alt="Three assembled boards"></p><p>

After assembling them, I did wonder if perhaps I should have gone with smaller capacitors after all. They're the tallest single components, and smaller caps are easily available. Oops, too late now.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/assembly5.jpg" alt="Close up of the capacitors, board held between fingers"></p><p>

As expected, the buzzer pins were a tight fit, but there was enough play to jam them into place and get the board flush. I then trimmed them to length and delicately soldered the stubs.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/finished1.jpg" alt="Fully assembled"></p><p>

Comically I waited until this moment to realise I didn't have enough piezo buzzers in stock. I ordered some more and the new ones are a minutely different design, which was inevitable. Never mind.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/finished2.jpg" alt="Fully assembled"></p><p>

That vertical-mount USB-C connector was designed to go inside a dock for a phone or tablet. It's supposed to poke through a moulded plastic case, which means it's a little longer than it needed to be. I did have a think about 3D printing a little plastic cover to go over the circuit board and the lowest part of the connector, but I doubt it would look very good. We wouldn't want to cover that mitxela logo anyway.</p><p>

USB-C extension cables are technically against the spec, but that doesn't mean you can't buy them and all kinds of other nonsensical cables and connectors. I have one that only works in certain orientations, which is just so distressing and the opposite of what USB-C was supposed to be, but it'll do to give us power while I poke that SWIO pin with a probe. I flashed the four different synths with different device names, which helps us differentiate them in a DAW. And by DAW I mean the 1998 edition of Cakewalk running in wine.</p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/foursynths1.jpg" alt="Four silly USB-C synths"></p><p>

I then went out of my way to buy a four-port USB-C hub. Surprisingly difficult to find, most of them turn USB-C into various more helpful connectors like USB-A, HDMI, SD card, and so on. </p><p>

<img src="https://mitxela.com/img/uploads/sillysynth/c/foursynths2.jpg" alt="Four silly USB-C synths"></p><p>

I have a bunch of the PCBs left, maybe I should make another handful and get even more hubs?</p><p>

Interestingly the design works with the hub, and it works if I plug it into my phone, but it doesn't enumerate if I plug it straight into my laptop. But it does enumerate on the end of that noncompliant USB-C extension cable I've concocted. It's entirely possible I've wired up the USB-C port marginally wrong, or perhaps the resistors are not exactly the right value – the type of USB connection is determined by the strength of some of the pull resistors. Either way I don't think I care enough about this comedy synth to look into it much further. It's just something to keep in mind for the next USB-C device.</p><div><p>

I have put the source code for this project in the <a href="https://github.com/mitxela/smsc">usual</a> <a href="https://git.mitxela.com/smsc">places</a>.</p></div><nav>
<a href="https://mitxela.com/projects/random" title="random project">~</a>
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/">mitxela.com</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects">Projects</a></span> » 
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/hardware">Hardware</a></span> »
<span typeof="v:Breadcrumb"><a rel="v:url" property="v:title" href="https://mitxela.com/projects/smsc">Smallest USB-C MIDI Synth</a></span>
<p>Questions? Comments? Check out the <a href="https://mitxela.com/forum">Forum</a>
</p><p><a href="https://mitxela.com/support">Support mitxela.com</a>
</p></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leave work slightly unfinished for easier flow the next day (203 pts)]]></title>
            <link>https://read.engineerscodex.com/p/simple-software-engineering-habits</link>
            <guid>38658262</guid>
            <pubDate>Fri, 15 Dec 2023 20:13:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://read.engineerscodex.com/p/simple-software-engineering-habits">https://read.engineerscodex.com/p/simple-software-engineering-habits</a>, See on <a href="https://news.ycombinator.com/item?id=38658262">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>“Your outcomes are a lagging indicator of your habits.” - James Clear</em></p><p>As I became a better software engineer, I noticed 4 key habits in my daily workflow that had made me much more productive.</p><blockquote><p><em><span>Friendly plug: </span><strong><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> is a compilation of 450+ software engineering and system design questions covering databases, authentication, caching, etc. They’ve been vetted by engineers from Google, Meta, Apple, and more.</span></strong></em></p><p><em><span>I’m a core contributor to </span><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> and it’s helped many of my peers (including myself) pass the “software trivia questions” during interviews and feel more confident at work.</span></em></p></blockquote><p data-attrs="{&quot;url&quot;:&quot;https://swequiz.com&quot;,&quot;text&quot;:&quot;Check it out&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://swequiz.com/" rel=""><span>Check it out</span></a></p><p>“Flow” is the root of productivity when programming.&nbsp;</p><p><span>Since software engineering is a </span><a href="http://www.paulgraham.com/makersschedule.html" rel="">“maker” activity</a><span> where I’m producing something, I generally perform best when I have a large block of uninterrupted “flow” time to work on a project.</span></p><p>However, it can often be really hard to get into flow if you’re stuck scrambling on what tasks your project goals entail. Ambiguity is difficult to deal with. Not even knowing where to start can make reaching that “flow state” much harder.</p><p>Each successful action snowballs into more.</p><p>There are a few techniques I use to do this:</p><ul><li><p><strong>Stop right before a “sticking point.”</strong><span> A sticking point is a task that’s part of a project where I know the steps to do to complete it, but I don’t know if there are hidden costs.&nbsp;</span></p><ul><li><p>For example, if my sticking point is deploying my ML model and HTTP server to a dev instance and verifying that it processes requests properly, then the hidden costs are deployment errors, authentication errors, resource constraints, etc.</p></li></ul></li><li><p><strong>Write down the next steps extremely clearly.</strong><span> Writing down steps makes regaining context and the state of mind from the day before easier.</span></p><ul><li><p><strong>Make them actionable and unambiguous.</strong></p></li></ul></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png" width="1456" height="572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:572,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:65283,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a06784b-b277-40f4-832e-699ed2eb42a6_1612x633.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>My first experience with a real “shortcut ninja” was actually not with a software engineer. It was with my investment banker friend, who sped around his Excel sheets without ever touching the mouse.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png" width="516" height="516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:516,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22b7867e-304a-4cea-8e5e-1d24b8a11625_2000x2000.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>This, except I finally did appreciate it years later. Source: </span><a href="https://workchronicles.com/keyboard-shortcuts/" rel="">Work Chronicles</a></figcaption></figure></div><p>After that, I took the time to learn keyboard shortcuts, to the point where I grab my mouse ~60% less than I used to. (Yes, I tracked this.)</p><p>Every editor and tool in my workflow has keyboard shortcuts for pretty much any action you can think of. This doesn’t just apply to your IDE, but also your version control systems, your web browser, and your docs.</p><blockquote><p>For example, my IDE has a linter/formatter/cleaner all in one shortcut, which I use often as I write code to make sure lines stay neat.</p><p>I commonly use Command/Ctrl + Shift + V to paste in text without formatting in docs and chats.</p><p><span>Pressing </span><code>.</code><span> (period) on a GitHub repo page will automatically open up the repo in a VSCode Web instance.</span></p></blockquote><p>When I do need to touch my mouse, it’s configured with shortcuts also. I’m lucky enough to have a mouse with buttons on the sides. I’ve programmed these buttons to switch between Spaces on my Mac, though you can program them for whatever feels intuitive for you. (You can even program them to be different per program.)</p><p><span>The best way to learn shortcuts? </span><strong>Introduce the most common parts of a program that you use, one a time.</strong><span> </span></p><p>For example, if you find yourself right-clicking to format your code often, that can be the first one you “practice.” Every so often, add a new shortcut to your repertoire and use it naturally as you code throughout the weeks. Over time, the shortcuts will be muscle memory.</p><p>I commonly have to run a set of common commands on my terminal.</p><p>I have certain pages that I always visit and some notes about various languages that I always come back to. For example, I simultaneously use templates too rarely and yet too often when writing C++, meaning I usually need to reference the docs when using them.</p><p><span>Instead of digging around documentation pages or constantly looking through my terminal history, I keep commands and common doc lookups in a giant doc with one word describing the command. I call it my </span><em>Big Book of Commands</em><span>, which is around ~10 pages long now. I’m easily able to find any command I need with a quick Ctrl+F. Then, a Shift + Command + ➡️ is a full line-select for an easy copy-paste.</span></p><p>I also have a few common macros programmed into my keyboard for the commands and terms, like hard-to-remember ACL groups. Sometimes, I utilize Terminal aliases.</p><p><span>My friend </span></p><p><span> wrote a great article that dives into his own workflow tips, which starts off with a great primer into aliases, keyboard shortcuts, and tools: </span><a href="https://careercutler.substack.com/p/the-top-7-software-engineering-workflow" rel="">The top 7 software engineering workflow tips I wish I knew earlier 🧰</a></p><p>This is less directly programming related, but I learned to say “no” to things. </p><blockquote><p>I said no to novel technology when boring technology would do the job. </p><p>I said no to automating something when it only needed to be done once manually. </p><p>I said no to more tasks when I knew I was already overloaded with work (even though my people-pleasing mind pleaded to take them on). </p><p>I said no to scope creep suggested by our designers. </p><p>I said no to low-impact tasks.</p></blockquote><p>Learning to say no was harder than I expected, yet one of the most valuable skills I’ve applied in both the workplace and in my personal life.</p><p>Sometimes, it’s painful to say no to things. In both my career, hobbies, and personal life, there are times I say no to things I really want to do. But I don’t because I know my time and energy is better spent on what I’m currently focusing on.</p><p>It’s a cliche to quote Steve Jobs, but I remind myself of his famous quote “focus is about saying no” often.</p><p><span>My friend </span></p><p><span> also has a fantastic article about saying no, which I highly recommend: </span><a href="https://www.thecaringtechie.com/p/software-eng-guide-to-saying-no" rel="">The Software Engineer's guide to saying "no"</a></p><p><em>When you’re ready, here's how I can help:</em></p><p><em><strong><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> is a compilation of 450+ software engineering and system design questions covering databases, authentication, caching, etc. They’ve been vetted by engineers from Google, Meta, Apple, and more.</span></strong></em></p><p><em><span>I’m a core contributor to </span><a href="https://swequiz.com/?utm_source=codex" rel="">SWE Quiz</a><span> and it’s helped many of my peers (including myself) pass the “software trivia questions” during interviews and feel 10x more confident at work.</span></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suspects can refuse to provide phone passcodes to police, court rules (482 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/</link>
            <guid>38657577</guid>
            <pubDate>Fri, 15 Dec 2023 19:16:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/">https://arstechnica.com/tech-policy/2023/12/suspects-can-refuse-to-provide-phone-passcodes-to-police-court-rules/</a>, See on <a href="https://news.ycombinator.com/item?id=38657577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/phone-passcode-800x533.jpeg" alt="A person's hand holding a smartphone while entering the screen-lock passcode.">
      <figcaption><p>Getty Images | releon8211</p></figcaption>  </figure>

  




<!-- cache miss 88:single/related:5898e440fb979d41242bf9b8a83f0ba0 --><!-- empty -->
<p>Criminal suspects can refuse to provide phone passcodes to police under the US Constitution's Fifth Amendment privilege against self-incrimination, according to a unanimous <a href="https://legacy.utcourts.gov/opinions/supopin/State%20v.%20Valdez20231214.pdf">ruling issued today</a> by Utah's state Supreme Court. The questions addressed in the ruling could eventually be taken up by the US Supreme Court, whether through review of this case or a similar one.</p>
<p>The case involves Alfonso Valdez, who was arrested for kidnapping and assaulting his ex-girlfriend. Police officers obtained a search warrant for the contents of Valdez's phone but couldn't crack his passcode.</p>
<p>Valdez refused to provide his passcode to a police detective. At his trial, the state "elicited testimony from the detective about Valdez's refusal to provide his passcode when asked," today's ruling said. "And during closing arguments, the State argued in rebuttal that Valdez's refusal and the resulting lack of evidence from his cell phone undermined the veracity of one of his defenses. The jury convicted Valdez."</p>
<p>A court of appeals reversed the conviction, agreeing "with Valdez that he had a right under the Fifth Amendment to the United States Constitution to refuse to provide his passcode, and that the State violated that right when it used his refusal against him at trial." The Utah Supreme Court affirmed the court of appeals ruling.<br>
                                            </p>
                                                        
<h2>Case possibly ripe for Supreme Court review</h2>
<p>The ruling offered some commentary on the developing legal questions about device passcodes:</p>
<blockquote><p>The prevalence of passcodes that encrypt the information on electronic devices—which are often seized by law enforcement while investigating criminal conduct—has raised important questions about how the Fifth Amendment extends to law enforcement's efforts to unlock these devices and decrypt the contents inside. These questions have proven to be especially complex where law enforcement attempts to access the contents of a seized device by means that do not require the suspect to disclose the actual passcode—like, for example, obtaining an order to compel the suspect to provide an unlocked device.</p></blockquote>
<p>The Valdez case does not involve an order to compel a suspect to unlock a device. Instead, "law enforcement asked Valdez to verbally provide his passcode," Utah justices wrote. "While these circumstances involve modern technology in a scenario that the Supreme Court has not yet addressed, we conclude that these facts present a more straightforward question that is answered by settled Fifth Amendment principles."</p>
<p>Ruling against the state, the Utah Supreme Court said it "agree[s] with the court of appeals that verbally providing a cell phone passcode is a testimonial communication under the Fifth Amendment."</p>
<p>Berkeley Law Professor Orin Kerr <a href="https://reason.com/volokh/2023/12/14/is-compelled-decryption-heading-to-the-supreme-court/">wrote today</a> that the case could head to the US Supreme Court. "One of the major issues in the law of digital evidence investigations is how the Fifth Amendment privilege against self-incrimination applies to unlocking phones," Kerr wrote.</p>
<p>So far, "the lower court case law is a total mess," according to Kerr. "No one can say what the law is. And I've been waiting for a case to come down that might be a good candidate for US Supreme Court review to clear up the mess."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[McDonald's ice cream machine hackers say they found 'smoking gun' (191 pts)]]></title>
            <link>https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/</link>
            <guid>38657192</guid>
            <pubDate>Fri, 15 Dec 2023 18:47:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/">https://www.wired.com/story/kytch-taylor-mcdonalds-ice-cream-machine-smoking-gun/</a>, See on <a href="https://news.ycombinator.com/item?id=38657192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>A little over three years have passed since McDonald's sent out an email to thousands of its restaurant owners around the world that abruptly cut short the future of <a href="https://www.wired.com/story/they-hacked-mcdonalds-ice-cream-makers-started-cold-war/">a three-person startup called Kytch</a>—and with it, perhaps one of McDonald's best chances for fixing its famously out-of-order ice cream machines.</p><p>Until then, Kytch had been selling McDonald's restaurant owners a popular internet-connected gadget designed to attach to their notoriously fragile and often broken soft-serve McFlurry dispensers, manufactured by McDonalds equipment partner Taylor. The Kytch device would essentially hack into the ice cream machine's internals, monitor its operations, and send diagnostic data over the internet to an owner or manager to help keep it running. But despite Kytch's efforts to solve the Golden Arches’ intractable ice cream problems, a McDonald’s email in November 2020 warned its franchisees not to use Kytch, stating that it represented a safety hazard for staff. Kytch says its sales dried up practically overnight.</p><p>Now, after years of litigation, the ice-cream-hacking entrepreneurs have unearthed evidence that they say shows that Taylor, the soft-serve machine maker, helped engineer McDonald's Kytch-killing email—kneecapping the startup not because of any safety concern, but in a coordinated effort to undermine a potential competitor. And Taylor's alleged order, as Kytch now describes it, came all the way from the top.</p><div data-testid="GenericCallout"><p><span><picture><img alt="Image may contain: Food, Creme, Dessert, and Cream" src="https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_775%2Cc_limit/web_hp_IceCream_15046_2.jpg" srcset="https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_120,c_limit/web_hp_IceCream_15046_2.jpg 120w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_240,c_limit/web_hp_IceCream_15046_2.jpg 240w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_320,c_limit/web_hp_IceCream_15046_2.jpg 320w, https://media.wired.com/photos/6077243240d7cfe5e259427c/master/w_640,c_limit/web_hp_IceCream_15046_2.jpg 640w" sizes="100vw"></picture></span></p><div><p>Secret codes. Legal threats. Betrayal. How one couple built a device to fix McDonald’s notoriously broken soft-serve machines—and how the fast-food giant froze them out.</p></div></div><p>On Wednesday, Kytch filed a newly unredacted motion for summary adjudication in its lawsuit against Taylor for alleged trade libel, tortious interference, and other claims. The new motion, which replaces a redacted version from August, refers to internal emails Taylor released in the discovery phase of the lawsuit, which were quietly unsealed over the summer. The motion focuses in particular on one email from Timothy FitzGerald, the CEO of Taylor parent company Middleby, that appears to suggest that either Middleby or McDonald's send a communication to McDonald's franchise owners to dissuade them from using Kytch's device.</p><p>“Not sure if there is anything we can do to slow up the franchise community on the other solution,” FitzGerald wrote on October 17, 2020. “Not sure what communication from either McD or Midd can or will go out.”</p><p>In their legal filing, the Kytch cofounders, of course, interpret “the other solution” to mean their product. In fact, FitzGerald's message was sent in an email thread that included Middleby's then COO, David Brewer, who had wondered earlier whether Middleby could instead acquire Kytch. Another Middleby executive responded to FitzGerald on October 17 to write that Taylor and McDonald’s had already met the previous day to discuss sending out a message to franchisees about McDonald’s lack of support for Kytch.</p><p>But Jeremy O'Sullivan, a Kytch cofounder, claims—and Kytch argues in its legal motion—that FitzGerald’s email nonetheless proves Taylor's intent to hamstring a potential rival. “It's the smoking gun,” O'Sullivan says of the email. “He's plotting our demise.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Although FitzGerald's email doesn't actually order anyone to act against Kytch, the company’s motion argues that Taylor played a key role in what happened next. It's an “ambiguous yet direct message to his underlings,” argues Melissa Nelson, Kytch's other cofounder. “It's just like a mafia boss giving coded instructions to his team to whack someone."</p><p>On November 2, 2020, a little over two weeks after FitzGerald's open-ended suggestion that perhaps a “communication” from McDonald's or Middleby to franchisees could “slow up” adoption of “the other solution,” McDonald's <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">sent out its email blast</a> cautioning restaurant owners not to use Kytch's product.</p><p>The email stated that the Kytch gadget “allows complete access to all aspects of the equipment’s controller and confidential data”—meaning Taylor’s and McDonald’s data, not the restaurant owners’ data; that it “creates a potential very serious safety risk for the crew or technician attempting to clean or repair the machine"; and finally, that it could cause “serious human injury.” The email concluded with a warning in italics and bold: “McDonald’s strongly recommends that you remove the Kytch device from all machines and discontinue use.”</p><p>Kytch has long argued that McDonald’s safety warning was bogus: In its legal complaint, it noted that its devices received certification from Underwriters Laboratory, an independent product safety nonprofit, including meeting its safety standards. It also countered in the complaint any claim that a Kytch device's remote connection to an ice cream machine could result in the machine turning on while a person's hand was inside—in fact, Taylor's own manual advises unplugging the machine before servicing it, and removing the door of the machine to access its rotating barrels automatically disables its motor.</p><p>Kytch's legal motion now argues that FitzGerald's email reveals that the McDonald's warning to restaurant owners was never really about safety, so much as protecting its equipment partner from a startup that might represent competition. The CEO's email “essentially put into place their plan to defame us," Nelson says.</p><p>She and O’Sullivan also argue that the internal email directly contradicts FitzGerald’s public statements that Middleby hadn’t sought to kill Kytch. “We’re not in business to put other companies out of business,” FitzGerald <a href="https://www.nytimes.com/2022/03/12/business/mcdonald-kytch-ice-cream-lawsuit.html">told <em>The New York Times</em></a> early last year.</p><p>When WIRED reached out to Middleby, Taylor’s parent company, for comment, a spokesperson responded in a statement disputing Kytch’s interpretation of its internal emails. “McDonald’s decided to issue the November 2020 field brief on its own accord, not at Middleby or Taylor’s direction,” the statement reads. “Taylor stood, and continues to stand, by the accuracy of statements made in the field brief.” The spokesperson also notes that Taylor won an early ruling in the lawsuit against Kytch’s request for a preliminary injunction—which would have prevented Taylor from developing a device that Kytch claims was <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">copied from its product</a>—and promises an upcoming filing responding to Kytch’s argument, which court documents say will happen in early 2024.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>At the time of McDonald's warning email to franchisees about Kytch, Taylor was developing its own internet-connected ice cream machine, what it referred to as Taylor Shake/Sundae Connectivity, which McDonald's recommended in the same email. But, even now, more than two years after it was promised for delivery, that device has yet to arrive in restaurants—and the <a href="https://arstechnica.com/gadgets/2023/08/mcdonalds-ice-cream-machine-teardown-shows-error-codes-dmca-keep-it-broken/">publicly documented ice cream headaches</a> at McDonald’s appear to have continued. According to the website <a data-offer-url="https://mcbroken.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://mcbroken.com/&quot;}" href="https://mcbroken.com/" rel="nofollow noopener" target="_blank">McBroken</a>, which tracks ice cream machine downtime at McDonald's restaurants across the US, between 13 percent and 17 percent of McDonald's restaurants have had broken ice cream machines at any given time just this month. That percentage has recently been as high as 35 percent in New York City and 28 percent in Washington, DC.</p><p>Taylor declined to comment on any upcoming internet-connected ice cream machine model. But that long-touted solution to the problem has still not been made available to franchisees, according to one McDonald's restaurant owner who goes by the handle McFranchisee (and previously used the handle McD Truth) on X. But McFranchisee says that Taylor has integrated those new features into its next model, which is expected to be available in four to six months. (McFranchisee has also criticized Kytch, claiming that the startup's failure was due to its own reliability problems and an increase in its prices, not a Taylor or McDonald's conspiracy against them.)</p><p>Despite the email from Middleby's CEO that Kytch claims suggests dissuading franchisees from using Kytch's product, Kytch argues that <a href="https://www.wired.com/story/mcdonalds-ice-cream-machine-hacking-kytch-taylor-internal-emails/">other documents released in the lawsuit’s discovery phase</a> show McDonald's itself was also eager to stymie Kytch from the beginning. In February 2020, Taylor president Jeremy Dobrowolski wrote in another email that “McDonald's is all hot and heavy about” Kytch's growing use in restaurants. Before the company sent out its November 2 email warning franchisees about Kytch, Taylor and McDonald’s executives had a meeting to discuss the message, and a McDonald's exec also sent a draft to Taylor for its approval. A Taylor executive wrote to others within the ice cream machine company, “I am a bit in shock they are willing to take such a strong position.”</p><p>When WIRED reached out to McDonald’s for comment on Kytch’s new argument about the “smoking gun” email from Taylor’s CEO, a spokesperson responded with a statement: “McDonald’s won’t speculate about the intent behind this email discussion that we weren't a part of. The intent of our Nov. 2020 communication was to bring awareness to potential safety concerns regarding the unapproved Kytch device.”</p><p>In addition to its lawsuit against Taylor, Kytch is still pursuing a <a href="https://www.wired.com/story/kytch-ice-cream-machine-hackers-sue-mcdonalds-900-million/">bigger lawsuit against McDonald's itself</a>, asking for $900 million in damages for what it describes in its legal complaint as McDonald’s effort to “<a data-offer-url="https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/&quot;}" href="https://www.courtlistener.com/docket/63127533/1/kytch-inc-v-mcdonalds-corporation/" rel="nofollow noopener" target="_blank">drive Kytch out of the marketplace</a>.” That lawsuit against McDonald's, if it moves forward, may soon produce more answers explaining Kytch’s legal claims that McDonald's appears to have cooperated with Taylor in telling its customers not to use Kytch—even as many of its restaurants took a significant hit from lost ice cream sales.</p><p>In the meantime, Kytch says it plans, if necessary, to take the lawsuit against Taylor to trial, currently set to take place in May at Alameda County Superior Court in Oakland, California. “The conspiracy described in Kytch’s complaint involved folks at the highest levels of leadership, not just at Taylor but also at Middleby and at McDonald’s,” says Daniel Watkins, Kytch’s attorney. “We’re really looking forward to the opportunity to present it to an Oakland jury trial.”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I bricked my Christmas lights (407 pts)]]></title>
            <link>https://www.whizzy.org/2023-12-14-bricked-xmas/</link>
            <guid>38657126</guid>
            <pubDate>Fri, 15 Dec 2023 18:41:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.whizzy.org/2023-12-14-bricked-xmas/">https://www.whizzy.org/2023-12-14-bricked-xmas/</a>, See on <a href="https://news.ycombinator.com/item?id=38657126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      

      

      <article role="main">
        <h2 id="reverse-engineering-bluetooth-le-led-light-controllers-or-how-i-bricked-my-christmas-lights">Reverse engineering Bluetooth LE LED light controllers, or How I Bricked My Christmas Lights</h2>

<p>If a device communicates via Bluetooth LE and has an app, it deserves to be integrated into my home automation system.</p>

<p>I’ve spent a significant amount of time reverse engineering various budget-friendly LED light strips to automate them. The process is generally repetitive, but I find it enjoyable. Recently, I successfully connected the cheapest lights I’ve ever come across — a £2.38 Bluetooth LE-controlled 5M non-addressable strip — to Home Assistant in just a few hours. You can buy some <a href="https://www.aliexpress.com/item/1005005485885067.html">here</a> and the code is <a href="https://github.com/8none1/bj_led">here</a>.</p>

<p>There is also the LEDnetWF controller I did the reverse engineering for <a href="https://github.com/raulgbcr/lednetwf_ble">here</a>.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/ha.jpg" alt="AliExpress Lights"></p>

<p>I also had another set of addressable lights on my desk. While decorating my office for Christmas, I decided to invest some time in connecting them to Home Assistant using the BJ_LED code as a template. It should have been straightforward, right? Well, yes, but also no.</p>

<p>These lights consist of a 10M long string of addressable LEDs controlled by the “iDeal LED” app. The app is feature-rich and works reasonably well. The LEDs are likely WS2812 or similar. I was quite pleased with these lights, which you can <a href="https://www.aliexpress.com/item/1005004829475855.html">find on AliExpress</a>.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/aliex_lights.jpg" alt="AliExpress Lights"></p>

<p>Now, let me share a cautionary tale. While I’m omitting some details for brevity, there are no secrets here, and additional instructions are readily available online. I understand this might feel a bit like <a href="https://knowyourmeme.com/memes/how-to-draw-an-owl">drawing the rest of the owl</a> but the provided links should serve as a starting point for anyone interested in reverse engineering their own LED lights.</p>

<h2 id="step-1-the-bytes-over-the-wire">Step 1. The bytes over the wire</h2>

<p>To control devices from your own software, the first step is to examine the bytes sent over Bluetooth to the device from the app. Typically, lights use a simple protocol with a header, command bytes (for actions like turning on/off, changing color), and a footer, which might be a checksum.</p>

<p>Android makes this process easy. Enable developer mode on your Android device, install the app for your lights, and enable <code>Bluetooth HCI snoop</code> in the developer settings. This logs Bluetooth bytes to a file readable by <a href="https://www.wireshark.org/">Wireshark</a>. Perform actions in the app, such as turning the lights on and off, and use <code>adb</code> to copy the logs to your computer.</p>

<p>For example:</p>

<pre><code>adb pull sdcard/btsnoop_hci.log .
</code></pre>

<p>Open the log in Wireshark to see the exact bytes sent to the device. Look for patterns in the values, and you’ll likely identify a series of bytes for each action, with one byte alternating between two values (e.g. <code>1</code> and <code>0</code> for <code>on</code> and <code>off</code>). Here’s a useful Wireshark filter:</p>

<pre><code>bluetooth.dst == ff:ff:ff:ff:ff:ff &amp;&amp; btatt.opcode.method==0x12
</code></pre>

<p>Change MAC address to be the MAC of your lights.  <code>btatt.opcode.method==0x12</code> is a write from the Android device to the lights.</p>

<p>Congratulations, you are now a reverse engineer!</p>

<p>Pro-tip:  You can speed things up a bit by using <a href="https://tshark.dev/">tshark</a> instead of Wireshark.  What you really care about is the values being written to the LED controller.  <code>tshark -r &lt;filename&gt; -T fields -e btatt.value</code> will dump the payload to the terminal for easy interrogation.</p>

<p>Sometimes your bytes will look like this:</p>

<pre><code>69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
69 96 02 01 01
69 96 02 01 00
</code></pre>

<p>On, off, on, off, on, off, on, off.</p>

<p>Sometimes your bytes will look like this:</p>

<pre><code>84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
79 d1 db a4 09 19 c2 46 a8 58 0a e7 d1 1b 78 84
84 dd 50 42 37 41 50 89 7a c8 2f 39 11 09 68 a8
</code></pre>

<p>There is still a repeating pattern here.  There are two distinct sets of bytes, one for on &amp; one for off, but… what?  Why is it so noisy?
Who designs their protocol like this?
The answer is: someone who is trying to hide something.</p>

<h2 id="step-2--replay-attacks">Step 2.  Replay attacks</h2>

<p>If your goal is simply turning the lights on and off, the repeating series of bytes you observed might be sufficient for power control. Test this with <code>gatttool</code>, which lets you connect to a BLE device and send bytes. You’ll need to know the handle to send bytes to, which you can find using Wireshark.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/wireshark.jpg" alt=""></p>

<p>For more control, understanding all those bytes is essential. Let’s go to the source…</p>

<h2 id="step-3-decompile-the-android-app">Step 3. Decompile the Android app</h2>

<p>Download the app’s APK and open it in <a href="https://github.com/skylot/jadx">jadx</a>. Witness the secrets within!</p>

<p>In my case, I noticed references to AES in the source, indicating a potentially encrypted protocol. If the data is encrypted, some assumptions can be made:</p>

<ul>
  <li>The encrypted data doesn’t change every time, suggesting a consistent key.</li>
  <li>The data needs quick decryption on a low-power MCU, favouring shorter keys.</li>
  <li>The key is likely not unique to each device, making a fixed key plausible.</li>
</ul>

<p>The source code contained a compiled AES library <code>libAES.so</code>, which <code>jadx</code> can’t help me with.</p>

<p><img src="https://www.whizzy.org/wp-content/uploads/2023/12/jadx.jpg" alt="">
<img src="https://www.whizzy.org/wp-content/uploads/2023/12/jadx2.jpg" alt=""></p>

<p>This is where I got stuck.  For about 5 minutes.</p>

<p>I asked <a href="https://ubuntu.social/@popey">@popey</a> and <a href="https://mastodon.social/@sil">@sil</a> for some ideas.  @sil Googled some of the decompiled app code and found <a href="https://habr-com.translate.goog/ru/articles/722412/?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp&amp;_x_tr_hist=true">this</a> page.  On closer examination the code looks identical.  This chap used <a href="https://hex-rays.com/ida-free/">ida free</a> to decompile the AES library and found the key embedded in it.  Let’s try that key.</p>

<pre><code>from Crypto.Cipher import AES

key = [
    0x34, 0x52, 0x2A, 0x5B, 0x7A, 0x6E, 0x49, 0x2C,
    0x08, 0x09, 0x0A, 0x9D, 0x8D, 0x2A, 0x23, 0xF8
]

def decrypt_aes_ecb(ciphertext, key):
    cipher = AES.new(key, AES.MODE_ECB)
    plaintext = cipher.decrypt(ciphertext)
    return plaintext
</code></pre>

<p>When we try and decrypt the <code>on</code> and <code>off</code> packets we get:</p>

<pre><code>05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 01 00 00 00 00 00 00 00 00 00 00
05 54 55 52 4E 00 00 00 00 00 00 00 00 00 00 00
</code></pre>

<p>Success!  This is a lot more sensible.  A fixed header, byte 5 switching between a <code>1</code> and a <code>0</code> for on and off, and a bunch of zeros.</p>

<p>We can now decrypt all the packets being sent to the device and we can encrypt our own bytes so that we can duplicate the controls from the Android app in our own code.  It’s pretty much mission accomplished at this point.</p>

<h2 id="step-4-all-the-functions">Step 4. All the functions</h2>

<p>Now, work through each app function, recording the bytes sent. Write down each action, do it multiple times, and use separators like turning the lights on and off. This helps spot patterns and correlate notes with captured bytes.</p>

<p>For example, your process might be:</p>

<pre><code>turn off, turn on - [start of function]
set to red
set to green
set to blue
set to red
set to green
set to blue
set to red
set to green
set to blue
turn off, turn on - [end of colour changing]
set brightness to 100%
set brightness to 50%
set brightness to 10%
set brightness to 50%
set brightness to 100%
turn off, turn on - [end of brightness]
</code></pre>

<p>This will help you to spot patterns in the data and see which bytes change depending on what you are doing.</p>

<h2 id="step-5--automated-e-waste-generator">Step 5.  Automated e-waste generator</h2>

<p>While exploring color changes, I observed that the app never sent a value higher than 0x1F (5 bits) for red, green, or blue. Curious, I tried sending 8-bit values, and it worked remarkably well — brighter colors!</p>

<p>Great success!</p>

<p>Excited by my discovery I got to wondering what other secrets this light controller was hiding from me.  I wonder if there are any additional effects beyond the 10 that the app uses?
A good way to try this out would be a simple loop.</p>

<pre><code>    for n in range(20):
        print(f"Setting effect {n}")
        set_effect(n)
        time.sleep(20)
</code></pre>

<p>I ran this and watched 1 to 10.  So far so good, then it ticked over to 11 and AH HA!  I have found a secret mode!
Then it ticked over to 12 and… darkness.</p>

<p>Oh well, I guess there are only 11 effects, that’s fine.  I’ll reboot it and finish off the rest of the code.</p>

<p>And that was then end of my fun.</p>

<p>The lights never came back.</p>

<p>They don’t advertise on Bluetooth any more and I can’t connect to them.  I’ve tried holding down the button when turning them on.  I’ve left them unplugged over night to see if that helps, but no.</p>

<p>They are dead.</p>

<p>I guess I overflowed some buffer and I’ve corrupted the firmware.</p>

<p>All is not lost however.  The LEDs themselves are standard addressable LEDs so I can at least hook the string up to a different microcontroller and use them.</p>

<h2 id="tell-me-how-i-can-break-my-own-lights">Tell me how I can break my own lights</h2>

<p>Despite the setback, I documented most of the protocol and created a Github project with a Home Assistant custom component. It works, but proceed at your own risk.</p>

<p><a href="https://github.com/8none1/idealLED">Github: 8none1/idealLED</a></p>

      </article>

      

      

      
        <!-- Check if any share-links are active -->








      

      
      
  
  
  

  


  

  



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI: Prompt Engineering (250 pts)]]></title>
            <link>https://platform.openai.com/docs/guides/prompt-engineering</link>
            <guid>38657029</guid>
            <pubDate>Fri, 15 Dec 2023 18:30:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://platform.openai.com/docs/guides/prompt-engineering">https://platform.openai.com/docs/guides/prompt-engineering</a>, See on <a href="https://news.ycombinator.com/item?id=38657029">Hacker News</a></p>
Couldn't get https://platform.openai.com/docs/guides/prompt-engineering: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fastmail Employees Form a Union (186 pts)]]></title>
            <link>https://union.place/@fastmailunited/111563614375789166</link>
            <guid>38656727</guid>
            <pubDate>Fri, 15 Dec 2023 18:01:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://union.place/@fastmailunited/111563614375789166">https://union.place/@fastmailunited/111563614375789166</a>, See on <a href="https://news.ycombinator.com/item?id=38656727">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Do large language models need all those layers? (166 pts)]]></title>
            <link>https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers</link>
            <guid>38656039</guid>
            <pubDate>Fri, 15 Dec 2023 17:00:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers">https://www.amazon.science/blog/do-large-language-models-really-need-all-those-layers</a>, See on <a href="https://news.ycombinator.com/item?id=38656039">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Large language models (<a href="https://www.amazon.science/tag/large-language-models" data-cms-ai="0">LLMs</a>) have been around for a while but have really captured the attention of the public this year, with the advent of ChatGPT. LLMs are typically pretrained on massive volumes of data; recent variants are additionally tuned to follow instructions and incorporate human feedback using <a href="https://www.amazon.science/tag/reinforcement-learning" data-cms-ai="0">reinforcement learning</a>.</p><p>A fascinating ability that these LLMs demonstrate is in-context learning, where a model can learn to perform a task just by following a few (or sometimes even zero) good examples provided along with a new input. Following this paradigm of learning, larger LLMs also proved more capable of performing a wide variety of tasks than smaller ones, when the amount of pretraining data was fixed.</p><p>In a <a href="https://www.amazon.science/publications/rethinking-the-role-of-scale-for-in-context-learning-an-interpretability-based-case-study-at-66-billion-scale" data-cms-ai="0">paper</a> we’re presenting at this year’s meeting of the Association for Computational Linguistics (<a href="https://www.amazon.science/conferences-and-events/acl-2023" data-cms-ai="0">ACL</a>), we investigate the importance of model scale for in-context learning, from the perspective of architectural interpretability. We specifically ask the question <i>Are all LLM components really needed to perform in-context learning?</i></p><div data-align-right-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="20B-parameter Alexa model sets new marks in few-shot learning" href="https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/42f31a5/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/f27a5d6/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="20B-encoder-decoder.gif" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/f27a5d6/2147483647/strip/true/crop/865x489+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F8d%2Fbf%2Fbb7071ba45ba92c7197cdd81285c%2F20b-encoder-decoder.gif" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>With an encoder-decoder architecture — rather than decoder only — the Alexa Teacher Model excels other large language models on few-shot tasks such as summarization and machine translation.</p>


          </div>
    </div>
</ps-related-content>
</div><p>We conducted our investigation as a case study of the OPT-66B model, a 66-billion-parameter LLM that was open-sourced by Meta last year to serve as an open replica of GPT-3 (and was the largest publicly available decoder-only LLM at the time of our study). We found that a significant portion of the model could be discarded without affecting performance, indicating that OPT-66B and quite likely other prominent LLMs are undertrained.</p><p>We believe our findings are useful in helping build more powerful LLMs by identifying (or more generally providing methods to identify) architectural elements that may need to be trained better.</p><h2><p>LLM building blocks</p></h2><p>Modern LLMs use the Transformer architecture, which depends on an attention mechanism: the model learns to predict which prior tokens in the sequence it should <i>attend</i> to when predicting the current token.</p><div data-align-left-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="ACL: Computational linguistics in the age of large language models" href="https://www.amazon.science/blog/acl-computational-linguistics-in-the-age-of-large-language-models" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/754777b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/1035f0b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="Yang.16x9.png" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/1035f0b/2147483647/strip/true/crop/1117x631+2+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fc6%2Fcd%2F7eb8d5be4213b78e95877b929f65%2Fyang.16x9.png" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>Amazon’s Yang Liu, general chair of this year’s meeting of the Association for Computational Linguistics, on the road ahead for LLMs.</p>


          </div>
    </div>
</ps-related-content>
</div><p>Specifically, LLMs use multihead attention, meaning that they apply multiple attention mechanisms, or heads, in parallel. OPT-66B has 64 layers with 72 attention heads in each layer. The output of the multihead attention passes through a separate feed-forward network (FFN) at each layer.</p><p>Our first method for analyzing OPT-66B was to assign a score to each attention head and FFN indicating how important they were to a given task. On the basis of those scores, we then pruned the model.</p><p>We found that important attention heads are primarily clustered in the model’s intermediate layers, and important FFNs are primarily in later layers. The ability to perform zero-/few-shot in-context learning on 14 different natural-language-processing (NLP) datasets/tasks stayed nearly intact when up to 70% (~15.7B parameters in OPT-66B) of the attention heads are removed.</p><div data-align-center=""><figure>
    

    
        <p><figcaption>A heat map representing attention heads’ aggregate importance scores for five-shot in-context learning across 14 NLP tasks, at each layer of the OPT-66B model.</figcaption></p>
    
</figure></div><p>The attention heads that are important (and unimportant) for in-context learning also seemed to overlap across tasks and shots. This indicates that a common task-agnostic subset of the attention heads is responsible for in-context learning. We also found that up to 20% of the FFNs (~8.5B parameters) can be removed with minimal decline in performance on zero-/few-shot in-context learning.</p><p>Our second analytic technique was to quantify the capacity of all attention heads in OPT-66B to perform a pair of task-agnostic primitive operations associated with in-context learning. Those primitives are <i>prefix matching</i> and <i>copying</i>: explicitly searching for a prior occurrence of the current token in context and copying over the token that succeeded it (its <i>suffix</i>).</p><div data-align-center=""><figure>
    

    
        <p><figcaption>Prefix matching and copying.</figcaption></p>
    
</figure></div><p>Heads specialized for these two operations were first discovered by the machine learning research company Anthropic and termed induction heads. We found that a small set of heads in OPT-66B have nontrivial scores for both primitives. We also found that these heads overlap (to varying degrees) with the heads important for specific tasks identified earlier. This indicates that induction heads are capable of more sophisticated behaviors associated with in-context learning, such as latent concept matching, but are not the only heads with such capabilities.</p><div data-align-right-rail=""><ps-related-content>
  <div data-media="" data-content-type="blog post">
        
            <div>
                
                    <a aria-label="Responsible AI in the generative era" href="https://www.amazon.science/blog/responsible-ai-in-the-generative-era" data-cms-ai="0"><picture><source type="image/webp" width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/d247ab4/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4="><source width="400" height="226" data-image-size="relatedContent" data-srcset="https://assets.amazon.science/dims4/default/7337ef8/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" srcset="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    
        <img data-image-size="relatedContent" alt="LLM watermarking.AI.gif" width="400" height="226" data-src="https://assets.amazon.science/dims4/default/7337ef8/2147483647/strip/true/crop/1912x1080+4+0/resize/400x226!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Fe5%2F96%2Fab6a9ad44e6fa1758b3493ada7b6%2Fllm-watermarking.AI.gif" data-lazy-load="true" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgaGVpZ2h0PSIyMjZweCIgd2lkdGg9IjQwMHB4Ij48L3N2Zz4=">
    </picture>
</a>
                
            </div>
        

        <div>
            
              <p>
                  Related content
              </p>
            
            
                
            

            
    <p>Generative AI raises new challenges in defining, measuring, and mitigating concerns about fairness, toxicity, and intellectual property, among other things. But work has started on the solutions.</p>


          </div>
    </div>
</ps-related-content>
</div><p>Our overarching observation that only a core nucleus of attention heads and FFNs seem to be important for in-context learning indicates that OPT-66B and quite likely other prominent LLMs are undertrained. This also reinforces recent research that questions the efficacy of keeping the amount of pretraining data fixed when scaling models up, suggesting that the amount of pretraining data seen must be scaled hand-in-hand with the models themselves to attain optimal performance. It would be interesting to see how newer variants of LLMs released since the publication of our study, such as those tuned to follow instructions, fare in such analyses.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Not even LinkedIn is that keen on Microsoft's cloud: Shift to Azure abandoned (130 pts)]]></title>
            <link>https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/</link>
            <guid>38656038</guid>
            <pubDate>Fri, 15 Dec 2023 17:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/">https://www.theregister.com/2023/12/14/linkedin_abandons_migration_to_microsoft/</a>, See on <a href="https://news.ycombinator.com/item?id=38656038">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>LinkedIn has abandoned its efforts to migrate its datacenter infrastructure to Microsoft Azure four years after announcing the planned move.</p>
<p>Citing sources familiar with the matter, CNBC <a target="_blank" rel="nofollow" href="https://www.cnbc.com/2023/12/14/linkedin-shelved-plan-to-migrate-to-microsoft-azure-cloud.html">reports</a> the effort, codenamed "Blueshift," had run up against numerous challenges in the years since Microsoft acquired the professional networking site in 2016 for $27 billion.</p>
<p>In a statement to <em>The Register</em>, LinkedIn confirmed its plans to invest in its own datacenters while using Azure services where appropriate.</p>

    

<p>"This includes our running 100 employee-facing applications on Azure, leveraging Azure FrontDoor and ongoing work to consolidate our datacenter locations that are currently spread across multiple buildings under a single roof," the spokesperson said. "Azure has been crucial to support and scale collaboration and productivity for our teams to deliver value to our members."</p>

        


        

<p>The decision marks a reversal of LinkedIn's plans, <a target="_blank" href="https://engineering.linkedin.com/blog/2019/building-next-infra">announced</a> in a 2019 blog post, to migrate its workloads to a public cloud. At the time Mohak Shroff, the social network's SVP of engineering, touted the move as an opportunity to better support the site's growing membership.</p>
<p>"With the incredible member and business growth we're seeing, we've decided to begin a multi-year migration of all LinkedIn workloads to the public cloud," he wrote. "Moving to Azure will give us access to a wide array of hardware and software innovations, and unprecedented global scale."</p>
<ul>

<li><a href="https://www.theregister.com/2023/12/13/aws_sees_direct_uk_government/">In just one year, UK.gov's direct spend on AWS rises 76 percent</a></li>

<li><a href="https://www.theregister.com/2023/12/13/google_gemini_duet_ai/">Like Microsoft, Google can't stop its cloud from pouring AI all over your heads</a></li>

<li><a href="https://www.theregister.com/2023/12/07/aws_says_only_microsoft_has/">AWS accuses Microsoft of clipping customers' cloud freedoms</a></li>

<li><a href="https://www.theregister.com/2023/12/11/microsoft_union_ai_partnership/">Microsoft partners with labor unions to shape and regulate AI</a></li>
</ul>
<p>Over the past few years LinkedIn has deployed some services in Azure. In early 2022, the social network <a target="_blank" rel="nofollow" href="https://engineering.linkedin.com/blog/2022/accelerating-the-linkedin-experience-with-azure-front-door">tapped up</a> Azure FrontDoor, Microsoft's content delivery network, which caches commonly accessed content across a global network of edge datacenters reducing the bandwidth and access latencies required to serve users.</p>
<p>However, by mid-2022, CNBC reports, the cracks in LinkedIn's migration strategy were beginning to show. In a memo last summer, LinkedIn CTO Raghu Hiremagalur reportedly told employees LinkedIn was moving to a hybrid-cloud model with some services running in the cloud and others in the company's dedicated datacenters.</p>

        

<p>As it turned out, while Azure's scale may have presented a tantalizing opportunity at first blush, LinkedIn was having a hard time taking advantage of the cloud provider's software. Sources told CNBC that issues arose when LinkedIn attempted to lift and shift its existing software tools to Azure rather than refactor them to run on the cloud provider's ready made tools. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Threestudio – Framework for 3D content generation (104 pts)]]></title>
            <link>https://github.com/threestudio-project/threestudio</link>
            <guid>38655536</guid>
            <pubDate>Fri, 15 Dec 2023 16:11:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/threestudio-project/threestudio">https://github.com/threestudio-project/threestudio</a>, See on <a href="https://news.ycombinator.com/item?id=38655536">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
    <themed-picture data-catalyst-inline="true"><picture>
    <img alt="threestudio" src="https://user-images.githubusercontent.com/19284678/236847132-219999d0-4ffa-4240-a262-c2c025d15d9e.png" width="50%">
    </picture></themed-picture>
</p>
<p dir="auto"><b>
threestudio is a unified framework for 3D content creation from text prompts, single images, and few-shot images, by lifting 2D text-to-image generation models.
</b></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/245695565-f48eca9f-45a7-4092-a519-6bb99f4939e4.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1NjUtZjQ4ZWNhOWYtNDVhNy00MDkyLWE1MTktNmJiOTlmNDkzOWU0LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMGI2NDA2Y2VmZDYzZWE1MzM2YjY3YTVlODk5NTlmZTI4N2U5MDc5YmY2YWYxMjEwMGM1M2ZhODhmMWNkYzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8JamM8fetrNTI8koVeCEwpkvtKlCINhFKAbJfHt3FIM"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/245695565-f48eca9f-45a7-4092-a519-6bb99f4939e4.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1NjUtZjQ4ZWNhOWYtNDVhNy00MDkyLWE1MTktNmJiOTlmNDkzOWU0LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmMGI2NDA2Y2VmZDYzZWE1MzM2YjY3YTVlODk5NTlmZTI4N2U5MDc5YmY2YWYxMjEwMGM1M2ZhODhmMWNkYzUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8JamM8fetrNTI8koVeCEwpkvtKlCINhFKAbJfHt3FIM" width="100%" content-type-secured-asset="image/gif" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/245695584-01a00207-3240-4a8e-aa6f-d48436370fe7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1ODQtMDFhMDAyMDctMzI0MC00YThlLWFhNmYtZDQ4NDM2MzcwZmU3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY0OWZlMTczNGFiYmZhMDBhMDY4NTQwYmY1YzJiMjU4OGFkMmNlNDNhMjM0ZThjNTk1N2U3N2IzOGVlMWE2MWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ExiA4qTu-NdNyREC3APhNUegdwYkr-ZXtxRSuX5S3VE"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/245695584-01a00207-3240-4a8e-aa6f-d48436370fe7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTU1ODQtMDFhMDAyMDctMzI0MC00YThlLWFhNmYtZDQ4NDM2MzcwZmU3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY0OWZlMTczNGFiYmZhMDBhMDY4NTQwYmY1YzJiMjU4OGFkMmNlNDNhMjM0ZThjNTk1N2U3N2IzOGVlMWE2MWEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ExiA4qTu-NdNyREC3APhNUegdwYkr-ZXtxRSuX5S3VE" width="100%"></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047444-1dbdebab-43d5-4830-872c-66b38d9fda92.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0NDQtMWRiZGViYWItNDNkNS00ODMwLTg3MmMtNjZiMzhkOWZkYTkyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJmZmFkZmQ5Y2EwZDE1NmQxZWQ5M2Y1OWIyZjFkYjA2YWY5ZGIxNTUwN2ZiMTFlMjhjZDYxNWI4ZTQzZGIzNGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.YEGdWJPEFq_I3_dXZs8VrpBWQL15OTSvEJSBQx8VmLw"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047444-1dbdebab-43d5-4830-872c-66b38d9fda92.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0NDQtMWRiZGViYWItNDNkNS00ODMwLTg3MmMtNjZiMzhkOWZkYTkyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJmZmFkZmQ5Y2EwZDE1NmQxZWQ5M2Y1OWIyZjFkYjA2YWY5ZGIxNTUwN2ZiMTFlMjhjZDYxNWI4ZTQzZGIzNGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.YEGdWJPEFq_I3_dXZs8VrpBWQL15OTSvEJSBQx8VmLw" width="60%" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047384-437b4044-142c-4e5d-a406-4d9bad0205e1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDczODQtNDM3YjQwNDQtMTQyYy00ZTVkLWE0MDYtNGQ5YmFkMDIwNWUxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUwMWZlY2YxZGVhZTAyNWI3YmNmYWRkMDA5MTIxYmQ4ZjZjYmNlNzI5YTAwYjNkYTg1NTMyNjU2ODM2YTU5NTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.or4eWogG-TmYCRFnSpMrLxDbVjwhnPP8E4dJAgkzmpE"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047384-437b4044-142c-4e5d-a406-4d9bad0205e1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDczODQtNDM3YjQwNDQtMTQyYy00ZTVkLWE0MDYtNGQ5YmFkMDIwNWUxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUwMWZlY2YxZGVhZTAyNWI3YmNmYWRkMDA5MTIxYmQ4ZjZjYmNlNzI5YTAwYjNkYTg1NTMyNjU2ODM2YTU5NTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.or4eWogG-TmYCRFnSpMrLxDbVjwhnPP8E4dJAgkzmpE" width="60%"></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047426-4f4d62c5-2304-4e20-b632-afe6d144a203.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0MjYtNGY0ZDYyYzUtMjMwNC00ZTIwLWI2MzItYWZlNmQxNDRhMjAzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExZTkyMmI3NzFjMjlkNTEyMjc2NmMwOGFmYmU1OWNlYWNmOWYzZGU0NzlkNDAxMzYxYmJmZWM0NmY0ZDUyMTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VWK9TDoRp7g-3L3TCbGjkoTDZFD--DA8GZNOLBEe2h4"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047426-4f4d62c5-2304-4e20-b632-afe6d144a203.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDc0MjYtNGY0ZDYyYzUtMjMwNC00ZTIwLWI2MzItYWZlNmQxNDRhMjAzLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExZTkyMmI3NzFjMjlkNTEyMjc2NmMwOGFmYmU1OWNlYWNmOWYzZGU0NzlkNDAxMzYxYmJmZWM0NmY0ZDUyMTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VWK9TDoRp7g-3L3TCbGjkoTDZFD--DA8GZNOLBEe2h4" width="68%" data-animated-image=""></a>
<br>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/263047241-2f36ddbd-e3cf-4431-b269-47a9cb3d6e6e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDcyNDEtMmYzNmRkYmQtZTNjZi00NDMxLWIyNjktNDdhOWNiM2Q2ZTZlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmODlmY2I0MmFhYWYwODk0YTgwMjNhMDZjZjQxYWM1ZWRjMmJlNjgxOTM1ZDI1YTE0ZTI3OGUyN2I4NDA2OTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.diZG2o4ut4OpZoM3INnDyk2Vkq5U0W4MVr7IMzGOyQc"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/263047241-2f36ddbd-e3cf-4431-b269-47a9cb3d6e6e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMwNDcyNDEtMmYzNmRkYmQtZTNjZi00NDMxLWIyNjktNDdhOWNiM2Q2ZTZlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFmODlmY2I0MmFhYWYwODk0YTgwMjNhMDZjZjQxYWM1ZWRjMmJlNjgxOTM1ZDI1YTE0ZTI3OGUyN2I4NDA2OTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.diZG2o4ut4OpZoM3INnDyk2Vkq5U0W4MVr7IMzGOyQc" width="68%"></a>
</p>
<p dir="auto"><b>
👆 Results obtained from methods implemented by threestudio 👆 <br>
| <a href="https://ml.cs.tsinghua.edu.cn/prolificdreamer/" rel="nofollow">ProlificDreamer</a> | <a href="https://dreamfusion3d.github.io/" rel="nofollow">DreamFusion</a> | <a href="https://research.nvidia.com/labs/dir/magic3d/" rel="nofollow">Magic3D</a> | <a href="https://pals.ttic.edu/p/score-jacobian-chaining" rel="nofollow">SJC</a> | <a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a> | <a href="https://fantasia3d.github.io/" rel="nofollow">Fantasia3D</a> | <a href="https://fabi92.github.io/textmesh/" rel="nofollow">TextMesh</a> |
<br>
| <a href="https://zero123.cs.columbia.edu/" rel="nofollow">Zero-1-to-3</a> | <a href="https://guochengqian.github.io/project/magic123/" rel="nofollow">Magic123</a> |
<br>
| <a href="https://instruct-nerf2nerf.github.io/" rel="nofollow">InstructNeRF2NeRF</a> | <a href="https://control4darxiv.github.io/" rel="nofollow">Control4D</a> |
</b>
</p><p dir="auto">
  <a href="https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb" rel="nofollow">
  <img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg">
  </a>
  <a href="https://huggingface.co/spaces/bennyguo/threestudio" rel="nofollow"><img src="https://camo.githubusercontent.com/d62c84d474b9ca5604efd7987fe4a377b12835d0274154b3c6addfa140ec4809/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323047726164696f25323044656d6f2d48756767696e67666163652d6f72616e6765" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Gradio%20Demo-Huggingface-orange"></a>
  <a href="http://t23-g-01.threestudio.ai/" rel="nofollow"><img src="https://camo.githubusercontent.com/95c56fd6e110f90d44c0a85ef004d02a627e0db551b8c5ff88b2bdd24ca01a5f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47726164696f25323044656d6f2d54656e63656e742d626c75653f6c6f676f3d74656e63656e747171266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Gradio%20Demo-Tencent-blue?logo=tencentqq&amp;logoColor=white"></a>
  <a href="https://discord.gg/ejer2MAB8N" rel="nofollow"><img src="https://camo.githubusercontent.com/4d4aaf8201525ce15823a9d09c37ecbd84dfa70300a9b42c247dbe0a00d78388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d3538363546323f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;logoColor=white"></a>
</p>
<p dir="auto">
    Did not find what you want? Checkout <a href="https://threestudio-project.github.io/threestudio-extensions/" rel="nofollow"><b>threestudio-extension</b></a> or submit a feature request <a href="https://github.com/threestudio-project/threestudio/discussions/46" data-hovercard-type="discussion" data-hovercard-url="/threestudio-project/threestudio/discussions/46/hovercard">here</a>!
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/24589363/287081695-ac6089a7-d88f-414c-96d6-a5e75616115a.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODE2OTUtYWM2MDg5YTctZDg4Zi00MTRjLTk2ZDYtYTVlNzU2MTYxMTVhLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc3NmI1YmJlNzhjZGE1ZmZkMDUwZjZhZWFhMTg3ZGNlYjliZTQ4OTY3N2UwZWJlODQzNThmZTk1ZDI0YjUwMjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nF3tdOu7fiGcX4EYiCyQiEbpIOw9ZFc8tbNdz34cnL4"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/24589363/287081695-ac6089a7-d88f-414c-96d6-a5e75616115a.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODE2OTUtYWM2MDg5YTctZDg4Zi00MTRjLTk2ZDYtYTVlNzU2MTYxMTVhLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc3NmI1YmJlNzhjZGE1ZmZkMDUwZjZhZWFhMTg3ZGNlYjliZTQ4OTY3N2UwZWJlODQzNThmZTk1ZDI0YjUwMjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nF3tdOu7fiGcX4EYiCyQiEbpIOw9ZFc8tbNdz34cnL4" width="68%"></a>
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/24589363/287080628-8892898f-8bd8-43dc-a4ec-dd8d078af860.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODA2MjgtODg5Mjg5OGYtOGJkOC00M2RjLWE0ZWMtZGQ4ZDA3OGFmODYwLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI2YTQ0YjNmMDQ5ZWMzNjMxYTg2Y2I5ZjgzYjY2M2Y4NzViMzA4OTM5Nzc5MDQ0NTU1YzkzYjllMmMzM2RiM2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.0HWYlYXHYbpuyoTxGxvKmeGyub_4kUOM_s9J6g7PCus"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/24589363/287080628-8892898f-8bd8-43dc-a4ec-dd8d078af860.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yODcwODA2MjgtODg5Mjg5OGYtOGJkOC00M2RjLWE0ZWMtZGQ4ZDA3OGFmODYwLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI2YTQ0YjNmMDQ5ZWMzNjMxYTg2Y2I5ZjgzYjY2M2Y4NzViMzA4OTM5Nzc5MDQ0NTU1YzkzYjllMmMzM2RiM2MmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.0HWYlYXHYbpuyoTxGxvKmeGyub_4kUOM_s9J6g7PCus" width="50%" data-animated-image=""></a>
</p>
<h2 tabindex="-1" dir="auto">News</h2>
<ul dir="auto">
<li>11/30/2023 Implementation of <a href="https://github.com/DSaurus/threestudio-mvdream">MVDream</a>, <a href="https://github.com/DSaurus/threestudio-3dgs">Gaussian Splatting</a> as the custom extensions. You can also use neural representation to fit a mesh by <a href="https://github.com/DSaurus/threestudio-meshfitting">Mesh-Fitting</a>.</li>
<li>11/30/2023: Implementation of <a href="https://threestudio-project.github.io/threestudio-extensions/" rel="nofollow">custom extension system</a> and you can add your extensions in <a href="https://github.com/threestudio-project/threestudio-extensions">this project</a>.</li>
<li>08/25/2023: Implementation of <a href="https://guochengqian.github.io/project/magic123/" rel="nofollow">Magic123</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#magic123-">here</a> to give it a try.</li>
<li>07/06/2023: Join our <a href="https://discord.gg/ejer2MAB8N" rel="nofollow">Discord server</a> for lively discussions!</li>
<li>07/03/2023: Try text-to-3D online in <a href="https://huggingface.co/spaces/bennyguo/threestudio" rel="nofollow">HuggingFace Spaces</a> or using our <a href="http://t23-g-01.threestudio.ai/" rel="nofollow">self-hosted service</a> (GPU support from Tencent). To host the web interface locally, see <a href="https://github.com/threestudio-project/threestudio#gradio-web-interface">here</a>.</li>
<li>06/20/2023: Implementations of Instruct-NeRF2NeRF and Control4D for high-fidelity 3D editing! Follow the instructions for <a href="https://github.com/threestudio-project/threestudio#control4d-">Control4D</a> and <a href="https://github.com/threestudio-project/threestudio#instructnerf2nerf-">Instruct-NeRF2NeRF</a> to give it a try.</li>
<li>06/14/2023: Implementation of TextMesh! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#textmesh-">here</a> to give it a try.</li>
<li>06/14/2023: Implementation of <a href="https://arxiv.org/abs/2303.15413" rel="nofollow">prompt debiasing</a> and <a href="https://perp-neg.github.io/" rel="nofollow">Perp-Neg</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a> to give it a try.</li>
<li>05/29/2023: An experimental implementation of using <a href="https://zero123.cs.columbia.edu/" rel="nofollow">Zero-1-to-3</a> for 3D generation from a single image! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#zero-1-to-3-">here</a> to give it a try.</li>
<li>05/26/2023: Implementation of <a href="https://ml.cs.tsinghua.edu.cn/prolificdreamer/" rel="nofollow">ProlificDreamer</a>! Follow the instructions <a href="https://github.com/threestudio-project/threestudio#prolificdreamer-">here</a> to give it a try.</li>
<li>05/14/2023: You can experiment with the SDS loss on 2D images using our <a href="https://github.com/threestudio-project/threestudio/blob/main/2dplayground.ipynb">2dplayground</a>.</li>
<li>05/13/2023: You can now try threestudio on <a href="https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb" rel="nofollow">Google Colab</a>!</li>
<li>05/11/2023: We now support exporting textured meshes! See <a href="https://github.com/threestudio-project/threestudio#export-meshes">here</a> for instructions.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/237616977-ccae2820-e702-484c-a43f-81678a365427.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzc2MTY5NzctY2NhZTI4MjAtZTcwMi00ODRjLWE0M2YtODE2NzhhMzY1NDI3LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdlNWQ1YzhlMTMzNTIxMzgxMDQ4NDk3YWMzNmUwYWM4OWEzY2RhOGExZjU2ZmFkNzBjM2Q1MWI2N2U1ZTVhMTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Z5ohn3i68p0q-RmIKMwOUq-vSFejlKGL7eJd32OUXKI"><img src="https://private-user-images.githubusercontent.com/19284678/237616977-ccae2820-e702-484c-a43f-81678a365427.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzc2MTY5NzctY2NhZTI4MjAtZTcwMi00ODRjLWE0M2YtODE2NzhhMzY1NDI3LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdlNWQ1YzhlMTMzNTIxMzgxMDQ4NDk3YWMzNmUwYWM4OWEzY2RhOGExZjU2ZmFkNzBjM2Q1MWI2N2U1ZTVhMTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Z5ohn3i68p0q-RmIKMwOUq-vSFejlKGL7eJd32OUXKI" alt="export-blender"></a></p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto">See <a href="https://github.com/threestudio-project/threestudio/blob/main/docs/installation.md">installation.md</a> for additional information, including installation via Docker.</p>
<p dir="auto">The following steps have been tested on Ubuntu20.04.</p>
<ul dir="auto">
<li>You must have an NVIDIA graphics card with at least 6GB VRAM and have <a href="https://developer.nvidia.com/cuda-downloads" rel="nofollow">CUDA</a> installed.</li>
<li>Install <code>Python &gt;= 3.8</code>.</li>
<li>(Optional, Recommended) Create a virtual environment:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m virtualenv venv
. venv/bin/activate

# Newer pip versions, e.g. pip-23.x, can be much faster than old versions, e.g. pip-20.x.
# For instance, it caches the wheels of git packages to avoid unnecessarily rebuilding them later.
python3 -m pip install --upgrade pip"><pre>python3 -m virtualenv venv
<span>.</span> venv/bin/activate

<span><span>#</span> Newer pip versions, e.g. pip-23.x, can be much faster than old versions, e.g. pip-20.x.</span>
<span><span>#</span> For instance, it caches the wheels of git packages to avoid unnecessarily rebuilding them later.</span>
python3 -m pip install --upgrade pip</pre></div>
<ul dir="auto">
<li>Install <code>PyTorch &gt;= 1.12</code>. We have tested on <code>torch1.12.1+cu113</code> and <code>torch2.0.0+cu118</code>, but other versions should also work fine.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# torch1.12.1+cu113
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
# or torch2.0.0+cu118
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"><pre><span><span>#</span> torch1.12.1+cu113</span>
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
<span><span>#</span> or torch2.0.0+cu118</span>
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118</pre></div>
<ul dir="auto">
<li>(Optional, Recommended) Install ninja to speed up the compilation of CUDA extensions:</li>
</ul>

<ul dir="auto">
<li>Install dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<ul dir="auto">
<li>
<p dir="auto">(Optional) <code>tiny-cuda-nn</code> installation might require downgrading pip to 23.0.1</p>
</li>
<li>
<p dir="auto">(Optional, Recommended) The best-performing models in threestudio use the newly-released T2I model <a href="https://github.com/deep-floyd/IF">DeepFloyd IF</a>, which currently requires signing a license agreement. If you would like to use these models, you need to <a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0" rel="nofollow">accept the license on the model card of DeepFloyd IF</a>, and login into the Hugging Face hub in the terminal by <code>huggingface-cli login</code>.</p>
</li>
<li>
<p dir="auto">For contributors, see <a href="https://github.com/threestudio-project/threestudio#contributing-to-threestudio">here</a>.</p>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Quickstart</h2>
<p dir="auto">Here we show some basic usage of threestudio. First let's train a DreamFusion model to create a classic pancake bunny.</p>
<p dir="auto"><strong>If you are experiencing unstable connections with Hugging Face, we suggest you either (1) setting environment variable <code>TRANSFORMERS_OFFLINE=1 DIFFUSERS_OFFLINE=1 HF_HUB_OFFLINE=1</code> before your running command after all needed files have been fetched on the first run, to prevent from connecting to Hugging Face each time you run, or (2) downloading the guidance model you used to a local folder following <a href="https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-an-entire-repository" rel="nofollow">here</a> and <a href="https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-files-to-local-folder" rel="nofollow">here</a>, and set <code>pretrained_model_name_or_path</code> of the guidance and the prompt processor to the local path.</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# if you have agreed the license of DeepFloyd IF and have >20GB VRAM
# please try this configuration for higher quality
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;
# otherwise you could try with the Stable Diffusion model, which fits in 6GB VRAM
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;"><pre><span><span>#</span> if you have agreed the license of DeepFloyd IF and have &gt;20GB VRAM</span>
<span><span>#</span> please try this configuration for higher quality</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span>
<span><span>#</span> otherwise you could try with the Stable Diffusion model, which fits in 6GB VRAM</span>
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span></pre></div>
<p dir="auto">threestudio uses <a href="https://github.com/omry/omegaconf">OmegaConf</a> for flexible configurations. You can easily change any configuration in the YAML file by specifying arguments without <code>--</code>, for example the specified prompt in the above cases. For all supported configurations, please see our <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md">documentation</a>.</p>
<p dir="auto">The training lasts for 10,000 iterations. You can find visualizations of the current status in the trial directory which defaults to <code>[exp_root_dir]/[name]/[tag]@[timestamp]</code>, where <code>exp_root_dir</code> (<code>outputs/</code> by default), <code>name</code> and <code>tag</code> can be set in the configuration file. A 360-degree video will be generated after the training is completed. In training, press <code>ctrl+c</code> one time will stop training and head directly to the test stage which generates the video. Press <code>ctrl+c</code> the second time to fully quit the program.</p>
<h3 tabindex="-1" dir="auto">Multi-GPU training</h3>
<p dir="auto">Multi-GPU training is supported, but may still be <a href="https://github.com/threestudio-project/threestudio/issues/195" data-hovercard-type="issue" data-hovercard-url="/threestudio-project/threestudio/issues/195/hovercard">buggy</a>. Note that <code>data.batch_size</code> is the batch size <strong>per rank (device)</strong>. Also remember to</p>
<ul dir="auto">
<li>Set <code>data.n_val_views</code> to be a multiple of the number of GPUs.</li>
<li>Set a unique <code>tag</code> as timestamp is disabled in multi-GPU training and will not be appended after the tag. If you the same tag as previous trials, saved config files, code and visualizations will be overridden.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# this results in an effective batch size of 4 (number of GPUs) * 2 (data.batch_size) = 8
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0,1,2,3 system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot; data.batch_size=2 data.n_val_views=4"><pre><span><span>#</span> this results in an effective batch size of 4 (number of GPUs) * 2 (data.batch_size) = 8</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0,1,2,3 system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span> data.batch_size=2 data.n_val_views=4</pre></div>
<p dir="auto">If you define the <code>CUDA_VISIBLE_DEVICES</code> environment variable before you call <code>launch.py</code>, you don't need to specify <code>--gpu</code> - this will use all available GPUs from <code>CUDA_VISIBLE_DEVICES</code>. For instance, the following command will automatically use GPUs 3 and 4:</p>
<p dir="auto"><code>CUDA_VISIBLE_DEVICES=3,4 python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt="a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes"</code></p>
<p dir="auto">This is particularly useful if you run <code>launch.py</code> in a cluster using a command that automatically picks GPU(s) and exports their IDs through CUDA_VISIBLE_DEVICES, e.g. through SLURM:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd git/threestudio
. venv/bin/activate
srun --account mod3d --partition=g40 --gpus=1 --job-name=3s_bunny python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt=&quot;a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes&quot;"><pre><span>cd</span> git/threestudio
<span>.</span> venv/bin/activate
srun --account mod3d --partition=g40 --gpus=1 --job-name=3s_bunny python launch.py --config configs/dreamfusion-if.yaml --train system.prompt_processor.prompt=<span><span>"</span>a zoomed out DSLR photo of a baby bunny sitting on top of a stack of pancakes<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">Resume from checkpoints</h3>
<p dir="auto">If you want to resume from a checkpoint, do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# resume training from the last checkpoint, you may replace last.ckpt with any other checkpoints
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
# if the training has completed, you can still continue training for a longer time by setting trainer.max_steps
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt trainer.max_steps=20000
# you can also perform testing using resumed checkpoints
python launch.py --config path/to/trial/dir/configs/parsed.yaml --test --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
# note that the above commands use parsed configuration files from previous trials
# which will continue using the same trial directory
# if you want to save to a new trial directory, replace parsed.yaml with raw.yaml in the command

# only load weights from saved checkpoint but dont resume training (i.e. dont load optimizer state):
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 system.weights=path/to/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> resume training from the last checkpoint, you may replace last.ckpt with any other checkpoints</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
<span><span>#</span> if the training has completed, you can still continue training for a longer time by setting trainer.max_steps</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt trainer.max_steps=20000
<span><span>#</span> you can also perform testing using resumed checkpoints</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --test --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt
<span><span>#</span> note that the above commands use parsed configuration files from previous trials</span>
<span><span>#</span> which will continue using the same trial directory</span>
<span><span>#</span> if you want to save to a new trial directory, replace parsed.yaml with raw.yaml in the command</span>

<span><span>#</span> only load weights from saved checkpoint but dont resume training (i.e. dont load optimizer state):</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --train --gpu 0 system.weights=path/to/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">Export Meshes</h3>
<p dir="auto">To export the scene to texture meshes, use the <code>--export</code> option. We currently support exporting to obj+mtl, or obj with vertex colors.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# this uses default mesh-exporter configurations which exports obj+mtl
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter
# specify system.exporter.fmt=obj to get obj with vertex colors
# you may also add system.exporter.save_uv=false to accelerate the process, suitable for a quick peek of the result
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.exporter.fmt=obj
# for NeRF-based methods (DreamFusion, Magic3D coarse, Latent-NeRF, SJC)
# you may need to adjust the isosurface threshold (25 by default) to get satisfying outputs
# decrease the threshold if the extracted model is incomplete, increase if it is extruded
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_threshold=10.
# use marching cubes of higher resolutions to get more detailed models
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_method=mc-cpu system.geometry.isosurface_resolution=256"><pre><span><span>#</span> this uses default mesh-exporter configurations which exports obj+mtl</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter
<span><span>#</span> specify system.exporter.fmt=obj to get obj with vertex colors</span>
<span><span>#</span> you may also add system.exporter.save_uv=false to accelerate the process, suitable for a quick peek of the result</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.exporter.fmt=obj
<span><span>#</span> for NeRF-based methods (DreamFusion, Magic3D coarse, Latent-NeRF, SJC)</span>
<span><span>#</span> you may need to adjust the isosurface threshold (25 by default) to get satisfying outputs</span>
<span><span>#</span> decrease the threshold if the extracted model is incomplete, increase if it is extruded</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_threshold=10.
<span><span>#</span> use marching cubes of higher resolutions to get more detailed models</span>
python launch.py --config path/to/trial/dir/configs/parsed.yaml --export --gpu 0 resume=path/to/trial/dir/ckpts/last.ckpt system.exporter_type=mesh-exporter system.geometry.isosurface_method=mc-cpu system.geometry.isosurface_resolution=256</pre></div>
<p dir="auto">For all the options you can specify when exporting, see <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md#exporters">the documentation</a>.</p>
<p dir="auto">See <a href="https://github.com/threestudio-project/threestudio#supported-models">here</a> for example running commands of all our supported models. Please refer to <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a> for tips on getting higher-quality results, and <a href="https://github.com/threestudio-project/threestudio#vram-optimization">here</a> for reducing VRAM usage.</p>
<h3 tabindex="-1" dir="auto">Gradio Web Interface</h3>
<p dir="auto">Launch the Gradio web interface by</p>
<div data-snippet-clipboard-copy-content="python gradio_app.py launch"><pre><code>python gradio_app.py launch
</code></pre></div>
<p dir="auto">Parameters:</p>
<ul dir="auto">
<li><code>--listen</code>: listens to all addresses by setting <code>server_name="0.0.0.0"</code> when launching the Gradio app.</li>
<li><code>--self-deploy</code>: enables changing arbitrary configurations directly from the web.</li>
<li><code>--save</code>: enables checkpoint saving.</li>
</ul>
<p dir="auto">For feature requests, bug reports, or discussions about technical problems, please <a href="https://github.com/threestudio-project/threestudio/issues/new">file an issue</a>. In case you want to discuss the generation quality or showcase your generation results, please feel free to participate in the <a href="https://github.com/threestudio-project/threestudio/discussions">discussion panel</a>.</p>
<h2 tabindex="-1" dir="auto">Supported Models</h2>
<h3 tabindex="-1" dir="auto">ProlificDreamer <a href="https://arxiv.org/abs/2305.16213" rel="nofollow"><img src="https://camo.githubusercontent.com/c50bc699e2a0a94d97f8594bf640ecd1f2b1732cd84367106970fcef26f3a61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330352e31363231332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2305.16213-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>This is an unofficial experimental implementation! Please refer to <a href="https://github.com/thu-ml/prolificdreamer">https://github.com/thu-ml/prolificdreamer</a> for official code release.</strong></p>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, 256x256 Stage1)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer.mp4">prolificdreamer.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357000-27b42d8f-4aa4-4b47-8ea0-0f77db90fd1e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMDAtMjdiNDJkOGYtNGFhNC00YjQ3LThlYTAtMGY3N2RiOTBmZDFlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBiYjUxMDdlZTZhMDJmNzlhYWY2ZmM4MTU3MDYwMWI3MWYxYjI1YmVjMjgxN2FiMTNmZjRhM2I4ZTAyMGExY2UmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Q-2NK5juuNSn7NG22DuLEh1tlNpOOqDFbI7rV_izums" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357000-27b42d8f-4aa4-4b47-8ea0-0f77db90fd1e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMDAtMjdiNDJkOGYtNGFhNC00YjQ3LThlYTAtMGY3N2RiOTBmZDFlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBiYjUxMDdlZTZhMDJmNzlhYWY2ZmM4MTU3MDYwMWI3MWYxYjI1YmVjMjgxN2FiMTNmZjRhM2I4ZTAyMGExY2UmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Q-2NK5juuNSn7NG22DuLEh1tlNpOOqDFbI7rV_izums" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer-scene.mp4">prolificdreamer-scene.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357021-ffcbbb01-3817-4663-a2bf-5e21a076bc3d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMjEtZmZjYmJiMDEtMzgxNy00NjYzLWEyYmYtNWUyMWEwNzZiYzNkLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY2NWRhYWNmZTc4ZjAyOTg5OWQ1ODMyYTEzODJiNjc3NDgxZDkxNTNiYmU1YTgyNzViYjFmMzE1ZGM3NWQ4YzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.u1mDjx6qaWce6cvEFYtL_QiLcgJMZrJX5B11yW0FSik" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357021-ffcbbb01-3817-4663-a2bf-5e21a076bc3d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwMjEtZmZjYmJiMDEtMzgxNy00NjYzLWEyYmYtNWUyMWEwNzZiYzNkLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWY2NWRhYWNmZTc4ZjAyOTg5OWQ1ODMyYTEzODJiNjc3NDgxZDkxNTNiYmU1YTgyNzViYjFmMzE1ZGM3NWQ4YzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.u1mDjx6qaWce6cvEFYtL_QiLcgJMZrJX5B11yW0FSik" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, 256x256 Stage1, 512x512 Stage2+3)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description prolificdreamer-full.mp4">prolificdreamer-full.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/243357051-cfab881e-18dc-45fc-8384-7476f835b36e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwNTEtY2ZhYjg4MWUtMThkYy00NWZjLTgzODQtNzQ3NmY4MzViMzZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZiZjQ4YTRlZDAzNzg2NzE5MjBjZWMxMGY0MzU5ZmNjNzZhMzFkNTQ4ZjM5ZDBiODlhMTRkYWJjZTllZWExZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.uDVbLmY5OiDwVhzA-Lv5Z7V3vdq6u38vIO5AnXxOWfI" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/243357051-cfab881e-18dc-45fc-8384-7476f835b36e.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDMzNTcwNTEtY2ZhYjg4MWUtMThkYy00NWZjLTgzODQtNzQ3NmY4MzViMzZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZiZjQ4YTRlZDAzNzg2NzE5MjBjZWMxMGY0MzU5ZmNjNzZhMzFkNTQ4ZjM5ZDBiODlhMTRkYWJjZTllZWExZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.uDVbLmY5OiDwVhzA-Lv5Z7V3vdq6u38vIO5AnXxOWfI" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper:</p>
<ul dir="auto">
<li>ProlificDreamer adopts a two-stage sampling strategy with 64 coarse samples and 32 fine samples, while we only use 512 coarse samples.</li>
<li>In the first stage, we only render 64x64 images at the first 5000 iterations. After that, as the empty space has been effectively pruned, rendering 512x512 images wouldn't cost too much VRAM.</li>
<li>We currently don't support multiple particles.</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Stage 1 (NeRF) --------- #
# object generation with 512x512 NeRF rendering, ~30GB VRAM
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot;
# if you don't have enough VRAM, try training with 64x64 NeRF rendering, ~15GB VRAM
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; data.width=64 data.height=64 data.batch_size=1
# using the same model for pretrained and LoRA enables 64x64 training with <10GB VRAM
# but the quality is worse due to the use of an epsilon prediction model for LoRA training
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; data.width=64 data.height=64 data.batch_size=1 system.guidance.pretrained_model_name_or_path_lora=&quot;stabilityai/stable-diffusion-2-1-base&quot;
# Using patch-based renderer to reduce memory consume, 512x512 resolution, ~20GB VRAM
python launch.py --config configs/prolificdreamer-patch.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot;
# scene generation with 512x512 NeRF rendering, ~30GB VRAM
python launch.py --config configs/prolificdreamer-scene.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;Inside of a smart home, realistic detailed photo, 4k&quot;

# --------- Stage 2 (Geometry Refinement) --------- #
# refine geometry with 512x512 rasterization, Stable Diffusion SDS guidance
python launch.py --config configs/prolificdreamer-geometry.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; system.geometry_convert_from=path/to/stage1/trial/dir/ckpts/last.ckpt

# --------- Stage 3 (Texturing) --------- #
# texturing with 512x512 rasterization, Stable Difusion VSD guidance
python launch.py --config configs/prolificdreamer-texture.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a pineapple&quot; system.geometry_convert_from=path/to/stage2/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> --------- Stage 1 (NeRF) --------- #</span>
<span><span>#</span> object generation with 512x512 NeRF rendering, ~30GB VRAM</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span>
<span><span>#</span> if you don't have enough VRAM, try training with 64x64 NeRF rendering, ~15GB VRAM</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> data.width=64 data.height=64 data.batch_size=1
<span><span>#</span> using the same model for pretrained and LoRA enables 64x64 training with &lt;10GB VRAM</span>
<span><span>#</span> but the quality is worse due to the use of an epsilon prediction model for LoRA training</span>
python launch.py --config configs/prolificdreamer.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> data.width=64 data.height=64 data.batch_size=1 system.guidance.pretrained_model_name_or_path_lora=<span><span>"</span>stabilityai/stable-diffusion-2-1-base<span>"</span></span>
<span><span>#</span> Using patch-based renderer to reduce memory consume, 512x512 resolution, ~20GB VRAM</span>
python launch.py --config configs/prolificdreamer-patch.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span>
<span><span>#</span> scene generation with 512x512 NeRF rendering, ~30GB VRAM</span>
python launch.py --config configs/prolificdreamer-scene.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>Inside of a smart home, realistic detailed photo, 4k<span>"</span></span>

<span><span>#</span> --------- Stage 2 (Geometry Refinement) --------- #</span>
<span><span>#</span> refine geometry with 512x512 rasterization, Stable Diffusion SDS guidance</span>
python launch.py --config configs/prolificdreamer-geometry.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> system.geometry_convert_from=path/to/stage1/trial/dir/ckpts/last.ckpt

<span><span>#</span> --------- Stage 3 (Texturing) --------- #</span>
<span><span>#</span> texturing with 512x512 rasterization, Stable Difusion VSD guidance</span>
python launch.py --config configs/prolificdreamer-texture.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a pineapple<span>"</span></span> system.geometry_convert_from=path/to/stage2/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">DreamFusion <a href="https://arxiv.org/abs/2209.14988" rel="nofollow"><img src="https://camo.githubusercontent.com/0a00143f284574687e6b04ef66124417c8da00ce27e46ee6dfc3dc8fe7e2465d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323230392e31343938382d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2209.14988-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 8)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description dreamfusion-if.mp4">dreamfusion-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694848-38ae4ea4-554b-4c9d-b4c7-fba5bee3acb3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NDgtMzhhZTRlYTQtNTU0Yi00YzlkLWI0YzctZmJhNWJlZTNhY2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWViNjEzM2NkOTA4NDM4OGZjZjJjY2I1YTFiZGFiMjBiOGNhMGNiMGYzMTg5NjYxMmU0MTI0YjcxMjI5Mjk0MTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.5Ifmg-F54IVJFg-Uc9oKpcXsT3h9VjGy2wFfjz2kTP4" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694848-38ae4ea4-554b-4c9d-b4c7-fba5bee3acb3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NDgtMzhhZTRlYTQtNTU0Yi00YzlkLWI0YzctZmJhNWJlZTNhY2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWViNjEzM2NkOTA4NDM4OGZjZjJjY2I1YTFiZGFiMjBiOGNhMGNiMGYzMTg5NjYxMmU0MTI0YjcxMjI5Mjk0MTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.5Ifmg-F54IVJFg-Uc9oKpcXsT3h9VjGy2wFfjz2kTP4" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>We use open-source T2I models (StableDiffusion, DeepFloyd IF), while the paper uses Imagen.</li>
<li>We use a guidance scale of 20 for DeepFloyd IF, while the paper uses 100 for Imagen.</li>
<li>We do not use sigmoid to normalize the albedo color but simply scale the color from <code>[-1,1]</code> to <code>[0,1]</code>, as we find this help convergence.</li>
<li>We use HashGrid encoding and uniformly sample points along rays, while the paper uses Integrated Positional Encoding and sampling strategy from MipNeRF360.</li>
<li>We adopt camera settings and density initialization strategy from Magic3D, which is slightly different from the DreamFusion paper.</li>
<li>Some hyperparameters are different, such as the weighting of loss terms.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training
# here we adopt random background augmentation to improve geometry quality
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.background.random_aug=true
# uses StableDiffusion, requires ~6GB VRAM in training
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training</span>
<span><span>#</span> here we adopt random background augmentation to improve geometry quality</span>
python launch.py --config configs/dreamfusion-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.background.random_aug=true
<span><span>#</span> uses StableDiffusion, requires ~6GB VRAM in training</span>
python launch.py --config configs/dreamfusion-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>DeepFloyd IF performs <strong>way better than</strong> StableDiffusion.</li>
<li>Validation shows albedo color before <code>system.material.ambient_only_steps</code> and shaded color after that.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_sparsity</code> if your scene is stuffed with floaters/becoming empty.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_orient</code> if you object is foggy/over-smoothed.</li>
<li>Try replacing the background to random colors with a probability 0.5 by setting <code>system.background.random_aug=true</code> if you find the model incorrectly treats the background as part of the object.</li>
<li>DeepFloyd IF uses T5-XXL as its text encoder, which consumes ~15GB VRAM even when using 8-bit quantization. This is currently the bottleneck for training with less VRAM. If anyone knows how to run the text encoder with less VRAM, please file an issue. We're also trying to push the text encoder to <a href="https://replicate.com/" rel="nofollow">Replicate</a> to enable extracting text embeddings via API, but are having some network connection issues. Please <a href="mailto:imbennyguo@gmail.com">contact bennyguo</a> if you would like to help out.</li>
</ul>
<h3 tabindex="-1" dir="auto">Magic3D <a href="https://arxiv.org/abs/2211.10440" rel="nofollow"><img src="https://camo.githubusercontent.com/92872accb7b8db7a3702adf6bebcf7b02c66650ec96f1d3aabf1843d39b2d171/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231312e31303434302d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2211.10440-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 8; first row: coarse, second row: refine)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description magic3d-if.mp4">magic3d-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694858-0ed6939e-cd7a-408f-a94b-406709ae90c0.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NTgtMGVkNjkzOWUtY2Q3YS00MDhmLWE5NGItNDA2NzA5YWU5MGMwLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA5ZTAyMGU3MmYyZDNiNTY5ZTA5M2RiNTUwMDk4ODEzNTZlZjNhMDA4Y2U4MzJhZTU3MDQ4YzhlMjFhZWZmZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VCyXtsj2LL4k6vgdUHSi8ltI1yIeKuq-3lXquWeW7VE" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694858-0ed6939e-cd7a-408f-a94b-406709ae90c0.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NTgtMGVkNjkzOWUtY2Q3YS00MDhmLWE5NGItNDA2NzA5YWU5MGMwLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA5ZTAyMGU3MmYyZDNiNTY5ZTA5M2RiNTUwMDk4ODEzNTZlZjNhMDA4Y2U4MzJhZTU3MDQ4YzhlMjFhZWZmZTImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.VCyXtsj2LL4k6vgdUHSi8ltI1yIeKuq-3lXquWeW7VE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>We use open-source T2I models (StableDiffusion, DeepFloyd IF) for the coarse stage, while the paper uses eDiff-I.</li>
<li>In the coarse stage, we use a guidance scale of 20 for DeepFloyd IF, while the paper uses 100 for eDiff-I.</li>
<li>In the coarse stage, we use analytic normal, while the paper uses predicted normal.</li>
<li>In the coarse stage, we use orientation loss as in DreamFusion, while the paper does not.</li>
<li>There are many things that are omitted from the paper such as the weighting of loss terms and the DMTet grid resolution, which could be different.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<p dir="auto">First train the coarse stage NeRF:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training
python launch.py --config configs/magic3d-coarse-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;
# uses StableDiffusion, requires ~6GB VRAM in training
python launch.py --config configs/magic3d-coarse-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM to extract text embeddings and ~10GB VRAM in training</span>
python launch.py --config configs/magic3d-coarse-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span>
<span><span>#</span> uses StableDiffusion, requires ~6GB VRAM in training</span>
python launch.py --config configs/magic3d-coarse-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto">Then convert the NeRF from the coarse stage to DMTet and train with differentiable rasterization:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# the refinement stage uses StableDiffusion, and requires ~5GB VRAM in training
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
# if you're unsatisfied with the surface extracted using the default threshold (25)
# you can specify a threshold value using `system.geometry_convert_override`
# decrease the value if the extracted surface is incomplete, increase if it is extruded
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10."><pre><span><span>#</span> the refinement stage uses StableDiffusion, and requires ~5GB VRAM in training</span>
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
<span><span>#</span> if you're unsatisfied with the surface extracted using the default threshold (25)</span>
<span><span>#</span> you can specify a threshold value using `system.geometry_convert_override`</span>
<span><span>#</span> decrease the value if the extracted surface is incomplete, increase if it is extruded</span>
python launch.py --config configs/magic3d-refine-sd.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10.</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>For the coarse stage, DeepFloyd IF performs <strong>way better than</strong> StableDiffusion.</li>
<li>Magic3D uses a neural network to predict the surface normal, which may not resemble the true geometric normal and degrade geometry quality, so we use analytic normal instead.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_sparsity</code> if your scene is stuffed with floaters/becoming empty.</li>
<li>Try increasing/decreasing <code>system.loss.lambda_orient</code> if you object is foggy/over-smoothed.</li>
<li>Try replacing the background with random colors with a probability 0.5 by setting <code>system.background.random_aug=true</code> if you find the model incorrectly treats the background as part of the object.</li>
</ul>
<h3 tabindex="-1" dir="auto">Score Jacobian Chaining <a href="https://arxiv.org/abs/2212.00774" rel="nofollow"><img src="https://camo.githubusercontent.com/61e153afb36cca33506ce0f5fd1560c6fe94155476c855965212182a1808e6f5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231322e30303737342d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2212.00774-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description sjc.mp4">sjc.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694871-87a247c1-2d3d-4cbf-89df-450bfeac3aca.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzEtODdhMjQ3YzEtMmQzZC00Y2JmLTg5ZGYtNDUwYmZlYWMzYWNhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjNzlkOWExZWI5MGRjYTY2NDYzODhiMDA5ZjVmMDM5MDQxZDMwMTc1ODRjZGVlYWVjOWY2YzVkYzY1ZGNhODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.JK95IOQiZYkdb49L26wWkn-UarGBmxQDgMB2Dbh7ops" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694871-87a247c1-2d3d-4cbf-89df-450bfeac3aca.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzEtODdhMjQ3YzEtMmQzZC00Y2JmLTg5ZGYtNDUwYmZlYWMzYWNhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjNzlkOWExZWI5MGRjYTY2NDYzODhiMDA5ZjVmMDM5MDQxZDMwMTc1ODRjZGVlYWVjOWY2YzVkYzY1ZGNhODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.JK95IOQiZYkdb49L26wWkn-UarGBmxQDgMB2Dbh7ops" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper: N/A.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# train with sjc guidance in latent space
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;A high quality photo of a delicious burger&quot;
# train with sjc guidance in latent space, trump figure
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;Trump figure&quot; trainer.max_steps=30000 system.loss.lambda_emptiness=&quot;[15000,10000.0,200000.0,15001]&quot; system.optimizer.params.background.lr=0.05 seed=42"><pre><span><span>#</span> train with sjc guidance in latent space</span>
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>A high quality photo of a delicious burger<span>"</span></span>
<span><span>#</span> train with sjc guidance in latent space, trump figure</span>
python launch.py --config configs/sjc.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>Trump figure<span>"</span></span> trainer.max_steps=30000 system.loss.lambda_emptiness=<span><span>"</span>[15000,10000.0,200000.0,15001]<span>"</span></span> system.optimizer.params.background.lr=0.05 seed=42</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>SJC uses subpixel rendering which decodes a <code>128x128</code> latent feature map for better visualization quality. You can turn off this feature by <code>system.subpixel_rendering=false</code> to save VRAM in validation/testing.</li>
</ul>
<h3 tabindex="-1" dir="auto">Latent-NeRF <a href="https://arxiv.org/abs/2211.07600" rel="nofollow"><img src="https://camo.githubusercontent.com/2cc556c25ad825d7078271e0eb352715d1a528320b10d315bc1f1866c129a96c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323231312e30373630302d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2211.07600-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description latent-nerf.mp4">latent-nerf.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694876-5a270347-6a41-4429-8909-44c90c554e06.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzYtNWEyNzAzNDctNmE0MS00NDI5LTg5MDktNDRjOTBjNTU0ZTA2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkyYWVjN2VlZjY1NzMwOTlhMTQ2NWE5MDc3Nzc1MGRhNmFkMThlZDFlZjMxOTdjMTNhMGJmZGU5MjkzMzZkMjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.yZzVykUB51Io13qb0I6VjWOYM91vw1uk0Kzt917FVqg" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694876-5a270347-6a41-4429-8909-44c90c554e06.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4NzYtNWEyNzAzNDctNmE0MS00NDI5LTg5MDktNDRjOTBjNTU0ZTA2Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTkyYWVjN2VlZjY1NzMwOTlhMTQ2NWE5MDc3Nzc1MGRhNmFkMThlZDFlZjMxOTdjMTNhMGJmZGU5MjkzMzZkMjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.yZzVykUB51Io13qb0I6VjWOYM91vw1uk0Kzt917FVqg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Notable differences from the paper: N/A.</p>
<p dir="auto">We currently only implement Latent-NeRF for text-guided and Sketch-Shape for (text,shape)-guided 3D generation. Latent-Paint is not implemented yet.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# train Latent-NeRF in Stable Diffusion latent space
python launch.py --config configs/latentnerf.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot;
# refine Latent-NeRF in RGB space
python launch.py --config configs/latentnerf-refine.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt

# train Sketch-Shape in Stable Diffusion latent space
python launch.py --config configs/sketchshape.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=&quot;a teddy bear in a tuxedo&quot;
# refine Sketch-Shape in RGB space
python launch.py --config configs/sketchshape-refine.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=&quot;a teddy bear in a tuxedo&quot; system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> train Latent-NeRF in Stable Diffusion latent space</span>
python launch.py --config configs/latentnerf.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span>
<span><span>#</span> refine Latent-NeRF in RGB space</span>
python launch.py --config configs/latentnerf-refine.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt

<span><span>#</span> train Sketch-Shape in Stable Diffusion latent space</span>
python launch.py --config configs/sketchshape.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=<span><span>"</span>a teddy bear in a tuxedo<span>"</span></span>
<span><span>#</span> refine Sketch-Shape in RGB space</span>
python launch.py --config configs/sketchshape-refine.yaml --train --gpu 0 system.guide_shape=load/shapes/teddy.obj system.prompt_processor.prompt=<span><span>"</span>a teddy bear in a tuxedo<span>"</span></span> system.weights=path/to/latent/stage/trial/dir/ckpts/last.ckpt</pre></div>
<h3 tabindex="-1" dir="auto">Fantasia3D <a href="https://arxiv.org/abs/2303.13873" rel="nofollow"><img src="https://camo.githubusercontent.com/98f846baf085f2ee0b2b0e21e7522d380f3b53e7b71174e2ba574f1b00bc1858/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31333837332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.13873-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description fantasia-3d.mp4">fantasia-3d.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/236694880-33b0db21-4530-47f1-9c3b-c70357bc84b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4ODAtMzNiMGRiMjEtNDUzMC00N2YxLTljM2ItYzcwMzU3YmM4NGIzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk1MWNkODMxZjM2NGM4ODEzZmRlMTJhYzZjNDA2YjU3NTkzNDM0ZDhhNDA1NmE2OGMzM2RiN2Y1MzU5OGQzMWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.RXvCegjzK4rbcg-xAUfin2BznxNnh9T1xJ1DMLN_okE" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/236694880-33b0db21-4530-47f1-9c3b-c70357bc84b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yMzY2OTQ4ODAtMzNiMGRiMjEtNDUzMC00N2YxLTljM2ItYzcwMzU3YmM4NGIzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk1MWNkODMxZjM2NGM4ODEzZmRlMTJhYzZjNDA2YjU3NTkzNDM0ZDhhNDA1NmE2OGMzM2RiN2Y1MzU5OGQzMWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.RXvCegjzK4rbcg-xAUfin2BznxNnh9T1xJ1DMLN_okE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Results obtained by threestudio (Stable Diffusion, mesh initialization)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description fantasia3d-mesh.mp4">fantasia3d-mesh.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/242636697-762903c1-665b-47b5-a2c2-bd7021a9e548.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzY2OTctNzYyOTAzYzEtNjY1Yi00N2I1LWEyYzItYmQ3MDIxYTllNTQ4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkZjA0NTYwNjk4ZDA3MDY3OGJjZmE3OTU2OTE2M2YyMGYwZGM4YWE1OWFlMjI4OTljZjdkZmY1OTEyMWQyZDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.G78odD1DOTH_iZiw492CGCEmIDvF-KSG1lvUVSKLSdU" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/242636697-762903c1-665b-47b5-a2c2-bd7021a9e548.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzY2OTctNzYyOTAzYzEtNjY1Yi00N2I1LWEyYzItYmQ3MDIxYTllNTQ4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkZjA0NTYwNjk4ZDA3MDY3OGJjZmE3OTU2OTE2M2YyMGYwZGM4YWE1OWFlMjI4OTljZjdkZmY1OTEyMWQyZDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.G78odD1DOTH_iZiw492CGCEmIDvF-KSG1lvUVSKLSdU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/19284678/242637322-2d22e30f-4a32-454a-a06e-d6e6bd2a1b96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzczMjItMmQyMmUzMGYtNGEzMi00NTRhLWEwNmUtZDZlNmJkMmExYjk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjMDBkOWZmYTM4MjZjMWY4YTcwODdkNTVkN2RkYjEwMDg4ZmRjNmFlMmE3NDU4ZmExMDMzODE0M2YzNWQyOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.mIXWeTV9bsJ7NdDyI4ZhZ4S3xK1aVy8PX89j-oGne9w"><img alt="threestudio" src="https://private-user-images.githubusercontent.com/19284678/242637322-2d22e30f-4a32-454a-a06e-d6e6bd2a1b96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDI2MzczMjItMmQyMmUzMGYtNGEzMi00NTRhLWEwNmUtZDZlNmJkMmExYjk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjMDBkOWZmYTM4MjZjMWY4YTcwODdkNTVkN2RkYjEwMDg4ZmRjNmFlMmE3NDU4ZmExMDMzODE0M2YzNWQyOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.mIXWeTV9bsJ7NdDyI4ZhZ4S3xK1aVy8PX89j-oGne9w" width="100%"></a>
</p>
<p dir="auto">Notable differences from the paper:</p>
<ul dir="auto">
<li>We enable tangent-space normal perturbation by default, which can be turned off by appending <code>system.material.use_bump=false</code>.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Geometry --------- #
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a DSLR photo of an ice cream sundae&quot;
# Fantasia3D highly relies on the initialized SDF shape
# the default shape is a sphere with radius 0.5
# change the shape initialization to match your input prompt
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;The leaning tower of Pisa&quot; system.geometry.shape_init=ellipsoid system.geometry.shape_init_params=&quot;[0.3,0.3,0.8]&quot;
# or you can initialize from a mesh
# here shape_init_params is the scale of the shape
# also make sure to input the correct up and front axis (in +x, +y, +z, -x, -y, -z)
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;hulk&quot; system.geometry.shape_init=mesh:load/shapes/human.obj system.geometry.shape_init_params=0.9 system.geometry.shape_init_mesh_up=+y system.geometry.shape_init_mesh_front=+z
# --------- Texture --------- #
# to train PBR texture continued from a geometry checkpoint:
python launch.py --config configs/fantasia3d-texture.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;a DSLR photo of an ice cream sundae&quot; system.geometry_convert_from=path/to/geometry/stage/trial/dir/ckpts/last.ckpt"><pre><span><span>#</span> --------- Geometry --------- #</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a DSLR photo of an ice cream sundae<span>"</span></span>
<span><span>#</span> Fantasia3D highly relies on the initialized SDF shape</span>
<span><span>#</span> the default shape is a sphere with radius 0.5</span>
<span><span>#</span> change the shape initialization to match your input prompt</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>The leaning tower of Pisa<span>"</span></span> system.geometry.shape_init=ellipsoid system.geometry.shape_init_params=<span><span>"</span>[0.3,0.3,0.8]<span>"</span></span>
<span><span>#</span> or you can initialize from a mesh</span>
<span><span>#</span> here shape_init_params is the scale of the shape</span>
<span><span>#</span> also make sure to input the correct up and front axis (in +x, +y, +z, -x, -y, -z)</span>
python launch.py --config configs/fantasia3d.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>hulk<span>"</span></span> system.geometry.shape_init=mesh:load/shapes/human.obj system.geometry.shape_init_params=0.9 system.geometry.shape_init_mesh_up=+y system.geometry.shape_init_mesh_front=+z
<span><span>#</span> --------- Texture --------- #</span>
<span><span>#</span> to train PBR texture continued from a geometry checkpoint:</span>
python launch.py --config configs/fantasia3d-texture.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>a DSLR photo of an ice cream sundae<span>"</span></span> system.geometry_convert_from=path/to/geometry/stage/trial/dir/ckpts/last.ckpt</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>If you find the shape easily diverge in early training stages, you may use a lower guidance scale by setting <code>system.guidance.guidance_scale=30.</code>.</li>
</ul>
<h3 tabindex="-1" dir="auto">TextMesh <a href="https://arxiv.org/abs/2304.12439" rel="nofollow"><img src="https://camo.githubusercontent.com/1ef5ad9a7578c2b914416b439572e33f7443a2746362edf2a0b232e59a085351/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330342e31323433392d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2304.12439-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (DeepFloyd IF, batch size 4)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description textmesh-if.mp4">textmesh-if.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/245696160-72217cdd-765a-475b-92d0-4ab62bf0f57a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTYxNjAtNzIyMTdjZGQtNzY1YS00NzViLTkyZDAtNGFiNjJiZjBmNTdhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE0ZjIxZTc0MDQxNGM3ODBlNzk0MWViNDdkM2EwYTdhNDQwZjdiZDYwMTU5ZTQyOTk3MTk3ZmMyZjliMTc4YWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.IxiDrL4A-nvPYk0JfPFjyb7mGPR0uuQR2SsQW5jSIo0" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/245696160-72217cdd-765a-475b-92d0-4ab62bf0f57a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNDU2OTYxNjAtNzIyMTdjZGQtNzY1YS00NzViLTkyZDAtNGFiNjJiZjBmNTdhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE0ZjIxZTc0MDQxNGM3ODBlNzk0MWViNDdkM2EwYTdhNDQwZjdiZDYwMTU5ZTQyOTk3MTk3ZmMyZjliMTc4YWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.IxiDrL4A-nvPYk0JfPFjyb7mGPR0uuQR2SsQW5jSIo0" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>Most of the settings are the same as the DreamFusion model. Please refer to the notable differences of the DreamFusion model.</li>
<li>We use NeuS as the geometry representation while the original paper uses VolSDF.</li>
<li>We adopt techniques from <a href="https://arxiv.org/abs/2306.03092" rel="nofollow">Neuralangelo</a> to stablize normal computation when using hash grids.</li>
<li>We currently only implemented the coarse stage of TextMesh.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# uses DeepFloyd IF, requires ~15GB VRAM
python launch.py --config configs/textmesh-if.yaml --train --gpu 0 system.prompt_processor.prompt=&quot;lib:cowboy_boots&quot;"><pre><span><span>#</span> uses DeepFloyd IF, requires ~15GB VRAM</span>
python launch.py --config configs/textmesh-if.yaml --train --gpu 0 system.prompt_processor.prompt=<span><span>"</span>lib:cowboy_boots<span>"</span></span></pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>TextMesh uses a surface-based geometry representation, so you don't need to manually tune the isosurface threshold when exporting meshes!</li>
</ul>
<h3 tabindex="-1" dir="auto">Control4D <a href="https://arxiv.org/abs/2305.20082" rel="nofollow"><img src="https://camo.githubusercontent.com/fd4b0abaf42e9ee37229eb60b5910b3e4148c1c96d4bb7ff7ac204e6426221d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330352e32303038322d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2305.20082-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>This is an experimental implementation of Control4D using threestudio! Control4D will release the full code including static and dynamic editing after paper acceptance.</strong></p>
<p dir="auto"><strong>Results obtained by threestudio (512x512)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description origin_1.mp4">origin_1.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/24589363/247185185-97d9aadd-32c7-488f-9543-6951b285d588.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODUxODUtOTdkOWFhZGQtMzJjNy00ODhmLTk1NDMtNjk1MWIyODVkNTg4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyZWQ5MTM4ODVlMGNjNTcxMmJkNDkxYjcxYTYzNDVmNjZkZWU0OTZkZjMyMjA4MDA0YzIzNmZhMTY5MTE3ODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.x-8BtwUBdXv9YKKpy3Pf0KxOPYTX3DSTWgFOeETn16o" data-canonical-src="https://private-user-images.githubusercontent.com/24589363/247185185-97d9aadd-32c7-488f-9543-6951b285d588.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODUxODUtOTdkOWFhZGQtMzJjNy00ODhmLTk1NDMtNjk1MWIyODVkNTg4Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyZWQ5MTM4ODVlMGNjNTcxMmJkNDkxYjcxYTYzNDVmNjZkZWU0OTZkZjMyMjA4MDA0YzIzNmZhMTY5MTE3ODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.x-8BtwUBdXv9YKKpy3Pf0KxOPYTX3DSTWgFOeETn16o" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">We currently don't support dynamic editing.</p>
<p dir="auto">Download the data sample of control4D using this <a href="https://mailstsinghuaeducn-my.sharepoint.com/:u:/g/personal/shaorz20_mails_tsinghua_edu_cn/EcqOaEuNwH1KpR0JTzL4Ur0BO_iJr8RiY2rNAGVC7h3fng?e=Dyr2gu" rel="nofollow">link</a>.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- Control4D --------- #
# static editing with 128x128 NeRF + 512x512 GAN rendering, ~20GB VRAM
python launch.py --config configs/control4d-static.yaml --train --gpu 0 data.dataroot=&quot;YOUR_DATAROOT/twindom&quot; system.prompt_processor.prompt=&quot;Elon Musk wearing red shirt, RAW photo, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3&quot;"><pre><span><span>#</span> --------- Control4D --------- #</span>
<span><span>#</span> static editing with 128x128 NeRF + 512x512 GAN rendering, ~20GB VRAM</span>
python launch.py --config configs/control4d-static.yaml --train --gpu 0 data.dataroot=<span><span>"</span>YOUR_DATAROOT/twindom<span>"</span></span> system.prompt_processor.prompt=<span><span>"</span>Elon Musk wearing red shirt, RAW photo, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">InstructNeRF2NeRF <a href="https://arxiv.org/abs/2303.12789" rel="nofollow"><img src="https://camo.githubusercontent.com/9187e8930897819d323fd5d972e6473d03185795beeec7cc9091f417dc0cc8d2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31323738392d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.12789-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description in2n.mp4">in2n.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/24589363/247187180-7aa43a2d-87d7-4ef5-94b6-f778ddb041b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODcxODAtN2FhNDNhMmQtODdkNy00ZWY1LTk0YjYtZjc3OGRkYjA0MWI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzM2YzZTgwNDY1YjY0MGU0NGJlYzkzMDJiZTQ3MDBkYTE2N2IyYzc5ZTdjNGVmMzhiODRjNDNiYzdjZjQ3ZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lwgDP0NkmZkjvvfzEJoM54FEZ3NgSHeWoLBNaC3Z0RQ" data-canonical-src="https://private-user-images.githubusercontent.com/24589363/247187180-7aa43a2d-87d7-4ef5-94b6-f778ddb041b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yNDU4OTM2My8yNDcxODcxODAtN2FhNDNhMmQtODdkNy00ZWY1LTk0YjYtZjc3OGRkYjA0MWI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzM2YzZTgwNDY1YjY0MGU0NGJlYzkzMDJiZTQ3MDBkYTE2N2IyYzc5ZTdjNGVmMzhiODRjNDNiYzdjZjQ3ZDUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.lwgDP0NkmZkjvvfzEJoM54FEZ3NgSHeWoLBNaC3Z0RQ" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Download the data sample of InstructNeRF2NeRF using this <a href="https://mailstsinghuaeducn-my.sharepoint.com/:u:/g/personal/shaorz20_mails_tsinghua_edu_cn/EbNazeNAYsBIvxGeXuCmOXgBiLv8KM-hfRNbNS7DtTvSvA?e=C1k4bM" rel="nofollow">link</a>.</p>
<p dir="auto"><strong>Example running commands</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# --------- InstructNeRF2NeRF --------- #
# 3D editing with NeRF patch-based rendering, ~20GB VRAM
python launch.py --config configs/instructnerf2nerf.yaml --train --gpu 0 data.dataroot=&quot;YOUR_DATAROOT/face&quot; data.camera_layout=&quot;front&quot; data.camera_distance=1 data.eval_interpolation=[1,3,50] system.prompt_processor.prompt=&quot;Turn him into Albert Einstein&quot;"><pre><span><span>#</span> --------- InstructNeRF2NeRF --------- #</span>
<span><span>#</span> 3D editing with NeRF patch-based rendering, ~20GB VRAM</span>
python launch.py --config configs/instructnerf2nerf.yaml --train --gpu 0 data.dataroot=<span><span>"</span>YOUR_DATAROOT/face<span>"</span></span> data.camera_layout=<span><span>"</span>front<span>"</span></span> data.camera_distance=1 data.eval_interpolation=[1,3,50] system.prompt_processor.prompt=<span><span>"</span>Turn him into Albert Einstein<span>"</span></span></pre></div>
<h3 tabindex="-1" dir="auto">Magic123 <a href="https://arxiv.org/abs/2306.17843" rel="nofollow"><img src="https://camo.githubusercontent.com/fc721c573072ceed6e5ffadac512640054425b25449d462163de96d1e99800b8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330362e31373834332d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2306.17843-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Results obtained by threestudio (Zero123 + Stable Diffusion)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description magic123.mp4">magic123.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19284678/263146263-335a58a8-8fee-485b-ac27-c55a16f4a673.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMxNDYyNjMtMzM1YTU4YTgtOGZlZS00ODViLWFjMjctYzU1YTE2ZjRhNjczLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJiNTM0NTczZWE1ZjFiNmExOTA2YWNkNjE3ZThjNTI4M2JmMmFiZjE5OTNkMDlmYWI4MjVkNjVhZDI4MTc5NjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nztBsQhofQ6SkQg_7wiJjoW5bxuAZGm9qEACjFU8d14" data-canonical-src="https://private-user-images.githubusercontent.com/19284678/263146263-335a58a8-8fee-485b-ac27-c55a16f4a673.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8xOTI4NDY3OC8yNjMxNDYyNjMtMzM1YTU4YTgtOGZlZS00ODViLWFjMjctYzU1YTE2ZjRhNjczLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWJiNTM0NTczZWE1ZjFiNmExOTA2YWNkNjE3ZThjNTI4M2JmMmFiZjE5OTNkMDlmYWI4MjVkNjVhZDI4MTc5NjQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.nztBsQhofQ6SkQg_7wiJjoW5bxuAZGm9qEACjFU8d14" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Notable differences from the paper</strong></p>
<ul dir="auto">
<li>This is an unofficial re-implementation which shares the same overall idea with the <a href="https://github.com/guochengqian/Magic123">official implementation</a> but differs in some aspects like hyperparameters.</li>
<li>Textual Inversion is not supported, which means a text prompt is needed for training.</li>
</ul>
<p dir="auto"><strong>Example running commands</strong></p>
<p dir="auto">First train the coarse stage NeRF:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Zero123 + Stable Diffusion, ~12GB VRAM
# data.image_path must point to a 4-channel RGBA image
# system.prompt_proessor.prompt must be specified
python launch.py --config configs/magic123-coarse-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot;"><pre><span><span>#</span> Zero123 + Stable Diffusion, ~12GB VRAM</span>
<span><span>#</span> data.image_path must point to a 4-channel RGBA image</span>
<span><span>#</span> system.prompt_proessor.prompt must be specified</span>
python launch.py --config configs/magic123-coarse-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span></pre></div>
<p dir="auto">Then convert the NeRF from the coarse stage to DMTet and train with differentiable rasterization:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Zero123 + Stable Diffusion, ~10GB VRAM
# data.image_path must point to a 4-channel RGBA image
# system.prompt_proessor.prompt must be specified
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
# if you're unsatisfied with the surface extracted using the default threshold (25)
# you can specify a threshold value using `system.geometry_convert_override`
# decrease the value if the extracted surface is incomplete, increase if it is extruded
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=&quot;a delicious hamburger&quot; system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10."><pre><span><span>#</span> Zero123 + Stable Diffusion, ~10GB VRAM</span>
<span><span>#</span> data.image_path must point to a 4-channel RGBA image</span>
<span><span>#</span> system.prompt_proessor.prompt must be specified</span>
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt
<span><span>#</span> if you're unsatisfied with the surface extracted using the default threshold (25)</span>
<span><span>#</span> you can specify a threshold value using `system.geometry_convert_override`</span>
<span><span>#</span> decrease the value if the extracted surface is incomplete, increase if it is extruded</span>
python launch.py --config configs/magic123-refine-sd.yaml --train --gpu 0 data.image_path=load/images/hamburger_rgba.png system.prompt_processor.prompt=<span><span>"</span>a delicious hamburger<span>"</span></span> system.geometry_convert_from=path/to/coarse/stage/trial/dir/ckpts/last.ckpt system.geometry_convert_override.isosurface_threshold=10.</pre></div>
<p dir="auto"><strong>Tips</strong></p>
<ul dir="auto">
<li>If the image contains non-front-facing objects, specifying the approximate elevation and azimuth angle by setting <code>data.default_elevation_deg</code> and <code>data.default_azimuth_deg</code> can be helpful. In threestudio, top is elevation +90 and bottom is elevation -90; left is azimuth -90 and right is azimuth +90.</li>
</ul>
<h3 tabindex="-1" dir="auto">Stable Zero123</h3>
<p dir="auto"><strong>Installation</strong></p>
<p dir="auto">Download pretrained Stable Zero123 checkpoint <code>stable-zero123.ckpt</code> into <code>load/zero123</code> from <a href="https://huggingface.co/stabilityai/stable-zero123" rel="nofollow">https://huggingface.co/stabilityai/stable-zero123</a></p>
<p dir="auto"><strong>Results obtained by threestudio (Stable Zero123 vs Zero123-XL)</strong>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/22424247/289674233-bf2d2213-5027-489c-a6ba-1c56c14ee8b7.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yODk2NzQyMzMtYmYyZDIyMTMtNTAyNy00ODljLWE2YmEtMWM1NmMxNGVlOGI3LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwOTY5NDY4ZTM2OTk4Mzg0YmE2YTFkMmUzNWQ0ODY4M2FlYmM0MDEwZjk2ZjJjYjQ1MzMyYjkzODI2NTg3ODQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.v6OV91vhNJ8WxaWudRqGQ8M0JDSnTzK09dXnDhhnlVg"><img src="https://private-user-images.githubusercontent.com/22424247/289674233-bf2d2213-5027-489c-a6ba-1c56c14ee8b7.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yODk2NzQyMzMtYmYyZDIyMTMtNTAyNy00ODljLWE2YmEtMWM1NmMxNGVlOGI3LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTAwOTY5NDY4ZTM2OTk4Mzg0YmE2YTFkMmUzNWQ0ODY4M2FlYmM0MDEwZjk2ZjJjYjQ1MzMyYjkzODI2NTg3ODQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.v6OV91vhNJ8WxaWudRqGQ8M0JDSnTzK09dXnDhhnlVg" alt="Final_video_v01" data-animated-image=""></a></p>
<p dir="auto"><strong>Example running commands</strong></p>
<ol dir="auto">
<li>Take an image of your choice, or generate it from text using your favourite AI image generator such as SDXL Turbo (<a href="https://clipdrop.co/stable-diffusion-turbo" rel="nofollow">https://clipdrop.co/stable-diffusion-turbo</a>) E.g. "A simple 3D render of a friendly dog"</li>
<li>Remove its background using Clipdrop (<a href="https://clipdrop.co/remove-background" rel="nofollow">https://clipdrop.co/remove-background</a>)</li>
<li>Save to <code>load/images/</code>, preferably with <code>_rgba.png</code> as the suffix</li>
<li>Run Zero-1-to-3 with the Stable Zero123 ckpt:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/stable-zero123.yaml --train --gpu 0 data.image_path=./load/images/hamburger_rgba.png"><pre>python launch.py --config configs/stable-zero123.yaml --train --gpu 0 data.image_path=./load/images/hamburger_rgba.png</pre></div>
<p dir="auto"><strong>IMPORTANT NOTE: This is an experimental implementation and we're constantly improving the quality.</strong></p>
<p dir="auto"><strong>IMPORTANT NOTE: This implementation extends the Zero-1-to-3 implementation below, and is heavily inspired from the Zero-1-to-3 implementation in <a href="https://github.com/threestudio-project/threestudio/blob/main/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a>! <code>extern/ldm_zero123</code> is borrowed from <code>stable-dreamfusion/ldm</code>.</strong></p>
<h3 tabindex="-1" dir="auto">Zero-1-to-3 <a href="https://arxiv.org/abs/2303.11328" rel="nofollow"><img src="https://camo.githubusercontent.com/cff3bb989636a7adcd8dfd9b30f2be23d690ed419b7e6b0a5fd65f147731cbc1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330332e31313332382d6233316231622e7376673f7374796c653d666c61742d737175617265" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2303.11328-b31b1b.svg?style=flat-square"></a></h3>
<p dir="auto"><strong>Installation</strong></p>
<p dir="auto">Download pretrained Zero123XL weights into <code>load/zero123</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd load/zero123
wget https://zero123.cs.columbia.edu/assets/zero123-xl.ckpt"><pre><span>cd</span> load/zero123
wget https://zero123.cs.columbia.edu/assets/zero123-xl.ckpt</pre></div>
<p dir="auto"><strong>Results obtained by threestudio (Zero-1-to-3)</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description ezgif-3-355a192487.mp4">ezgif-3-355a192487.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/22424247/252868915-f4e7b66f-7a46-4f9f-8fcd-750300cef651.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNTI4Njg5MTUtZjRlN2I2NmYtN2E0Ni00ZjlmLThmY2QtNzUwMzAwY2VmNjUxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5YmI2MGRhZGY0Zjc5MWMzZDUwNTNhMjgyNDk0ODhkZWQ5NDE1ZjJmYzhkOThhN2YxNDEyNTJmMDg0NzY0NGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.NZxBwlaLEPYsqbRhn4QJwiV6iF5_0AnZaSGKlg-cF2c" data-canonical-src="https://private-user-images.githubusercontent.com/22424247/252868915-f4e7b66f-7a46-4f9f-8fcd-750300cef651.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNTI4Njg5MTUtZjRlN2I2NmYtN2E0Ni00ZjlmLThmY2QtNzUwMzAwY2VmNjUxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5YmI2MGRhZGY0Zjc5MWMzZDUwNTNhMjgyNDk0ODhkZWQ5NDE1ZjJmYzhkOThhN2YxNDEyNTJmMDg0NzY0NGYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.NZxBwlaLEPYsqbRhn4QJwiV6iF5_0AnZaSGKlg-cF2c" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>IMPORTANT NOTE: This is an experimental implementation and we're constantly improving the quality.</strong></p>
<p dir="auto"><strong>IMPORTANT NOTE: This implementation is heavily inspired from the Zero-1-to-3 implementation in <a href="https://github.com/threestudio-project/threestudio/blob/main/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a>! <code>extern/ldm_zero123</code> is borrowed from <code>stable-dreamfusion/ldm</code>.</strong></p>
<p dir="auto"><strong>Example running commands</strong></p>
<ol dir="auto">
<li>Take an image of your choice, or generate it from text using your favourite AI image generator such as Stable Diffusion XL (<a href="https://clipdrop.co/stable-diffusion" rel="nofollow">https://clipdrop.co/stable-diffusion</a>) E.g. "A simple 3D render of a friendly dog"</li>
<li>Remove its background using Clipdrop (<a href="https://clipdrop.co/remove-background" rel="nofollow">https://clipdrop.co/remove-background</a>)</li>
<li>Save to <code>load/images/</code>, preferably with <code>_rgba.png</code> as the suffix</li>
<li>Run Zero-1-to-3:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/zero123.yaml --train --gpu 0 data.image_path=./load/images/dog1_rgba.png"><pre>python launch.py --config configs/zero123.yaml --train --gpu 0 data.image_path=./load/images/dog1_rgba.png</pre></div>
<p dir="auto">For more scripts for Zero-1-to-3, please check <code>threestudio/scripts/run_zero123.sh</code>.</p>
<p dir="auto">Previous Zero-1-to-3 weights are available at <code>https://huggingface.co/cvlab/zero123-weights/</code>. You can download them to <code>load/zero123</code> as above, and replace the path at <code>system.guidance.pretrained_model_name_or_path</code>.</p>
<p dir="auto"><strong>Guidance evaluation</strong></p>
<p dir="auto">Also includes evaluation of the guidance during training. If <code>system.freq.guidance_eval</code> is set to a value &gt; 0, this will save rendered image, noisy image (noise added mentioned at top left), 1-step-denoised image, 1-step prediction of original image, fully denoised image. For example:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/22424247/242720649-c8e7d835-4937-4852-bfb0-3e906e6b66b7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNDI3MjA2NDktYzhlN2Q4MzUtNDkzNy00ODUyLWJmYjAtM2U5MDZlNmI2NmI3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIzMzIxNzM1ZmQwODgzODJkYTgxMmVjNDE1OTUzODU5OWY2MTFmNWMwZmQzZjEwOWI5NmFmMzJhODdhMmY3OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xBAJWPzi1QbuZgb3ujJScfy8tyKgNhkQMXyRw-vp4LU"><img src="https://private-user-images.githubusercontent.com/22424247/242720649-c8e7d835-4937-4852-bfb0-3e906e6b66b7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI3MDY3MDQsIm5iZiI6MTcwMjcwNjQwNCwicGF0aCI6Ii8yMjQyNDI0Ny8yNDI3MjA2NDktYzhlN2Q4MzUtNDkzNy00ODUyLWJmYjAtM2U5MDZlNmI2NmI3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjE2VDA2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIzMzIxNzM1ZmQwODgzODJkYTgxMmVjNDE1OTUzODU5OWY2MTFmNWMwZmQzZjEwOWI5NmFmMzJhODdhMmY3OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.xBAJWPzi1QbuZgb3ujJScfy8tyKgNhkQMXyRw-vp4LU" alt="it143-train"></a></p>
<h3 tabindex="-1" dir="auto">More to come, please stay tuned.</h3>

<p dir="auto"><strong>If you would like to contribute a new method to threestudio, see <a href="https://github.com/threestudio-project/threestudio#contributing-to-threestudio">here</a>.</strong></p>
<h2 tabindex="-1" dir="auto">Prompt Library</h2>
<p dir="auto">For easier comparison, we collect the 397 preset prompts from the website of <a href="https://dreamfusion3d.github.io/gallery.html" rel="nofollow">DreamFusion</a> in <a href="https://github.com/threestudio-project/threestudio/blob/main/load/prompt_library.json">this file</a>. You can use these prompts by setting <code>system.prompt_processor.prompt=lib:keyword1_keyword2_..._keywordN</code>. Note that the prompt should starts with <code>lib:</code> and all the keywords are separated by <code>_</code>. The prompt processor will match the keywords to all the prompts in the library, and will only succeed if there's <strong>exactly one match</strong>. The used prompt will be printed to the console. Also note that you can't use this syntax to point to every prompt in the library, as there are prompts that are subset of other prompts lmao. We will enhance the use of this feature.</p>
<h2 tabindex="-1" dir="auto">Tips on Improving Quality</h2>
<p dir="auto">It's important to note that existing techniques that lift 2D T2I models to 3D cannot consistently produce satisfying results. Results from great papers like DreamFusion and Magic3D are (to some extent) cherry-pickled, so don't be frustrated if you do not get what you expected on your first trial. Here are some tips that may help you improve the generation quality:</p>
<ul dir="auto">
<li><strong>Increase batch size</strong>. Large batch sizes help convergence and improve the 3D consistency of the geometry. State-of-the-art methods claim using large batch sizes: DreamFusion uses a batch size of 4; Magic3D uses a batch size of 32; Fantasia3D uses a batch size of 24; some results shown above use a batch size of 8. You can easily change the batch size by setting <code>data.batch_size=N</code>. Increasing the batch size requires more VRAM. If you have limited VRAM but still want the benefit of large batch sizes, you may use <a href="https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#accumulate-gradients" rel="nofollow">gradient accumulation provided by PyTorch Lightning</a> by setting <code>trainer.accumulate_grad_batches=N</code>. This will accumulate the gradient of several batches and achieve a large effective batch size. Note that if you use gradient accumulation, you may need to multiply all step values by N times in your config, such as values that have the name <code>X_steps</code> and <code>trainer.val_check_interval</code>, since now N batches equal to a large batch.</li>
<li><strong>Train longer.</strong> This helps if you can already obtain reasonable results and would like to enhance the details. If the result is still a mess after several thousand steps, training for a longer time often won't help. You can set the total training iterations by <code>trainer.max_steps=N</code>.</li>
<li><strong>Try different seeds.</strong> This is a simple solution if your results have correct overall geometry but suffer from the multi-face Janus problem. You can change the seed by setting <code>seed=N</code>. Good luck!</li>
<li><strong>Tuning regularization weights.</strong> Some methods have regularization terms which can be essential to obtaining good geometry. Try tuning the weights of these regularizations by setting <code>system.loss.lambda_X=value</code>. The specific values depend on your situation, you may refer to <a href="https://github.com/threestudio-project/threestudio#supported-models">tips for each supported model</a> for more detailed instructions.</li>
<li><strong>Try debiasing methods.</strong> When conventional SDS techniques like DreamFusion, Magic3D, SJC, and others fail to produce the desired 3D results, Debiased Score Distillation Sampling (D-SDS) can be a solution. D-SDS is devised to tackle challenges such as artifacts or the Janus problem, employing two strategies: score debiasing and prompt debiasing. You can activate score debiasing by just setting <code>system.guidance.grad_clip=[0,0.5,2.0,10000]</code>, where the order is <code>start_step, start_value, end_value, end_step</code>. You can enable prompt debiasing by setting <code>system.prompt_processor.use_prompt_debiasing=true</code>. When using prompt debiasing, it's recommended to set a list of indices for words that should potentially be removed by <code>system.prompt_processor.prompt_debiasing_mask_ids=[i1,i2,...]</code>. For example, if the prompt is <code>a smiling dog</code> and you only want to remove the word <code>smiling</code> for certain views, you should set it to <code>[1]</code>. You could also manually specify the prompt for each view by setting <code>system.prompt_processor.prompt_side</code>, <code>system.prompt_processor.prompt_back</code> and <code>system.prompt_processor.prompt_overhead</code>. For a detailed explanation of these techniques, refer to <a href="https://arxiv.org/abs/2303.15413" rel="nofollow">the D-SDS paper</a> or check out <a href="https://susunghong.github.io/Debiased-Score-Distillation-Sampling/" rel="nofollow">the project page</a>.</li>
<li><strong>Try Perp-Neg.</strong> The <a href="https://perp-neg.github.io/" rel="nofollow">Perp-Neg algorithm</a> can potentially alleviate the multi-face Janus problem. We now support Perp-Neg for <code>stable-diffusion-guidance</code> and <code>deep-floyd-guidance</code> by setting <code>system.prompt_processor.use_perp_neg=true</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto">VRAM Optimization</h2>
<p dir="auto">If you encounter CUDA OOM error, try the following in order (roughly sorted by recommendation) to meet your VRAM requirement.</p>
<ul dir="auto">
<li>If you only encounter OOM at validation/test time, you can set <code>system.cleanup_after_validation_step=true</code> and <code>system.cleanup_after_test_step=true</code> to free memory after each validation/test step. This will slow down validation/testing.</li>
<li>Use a smaller batch size or use gradient accumulation as demonstrated <a href="https://github.com/threestudio-project/threestudio#tips-on-improving-quality">here</a>.</li>
<li>If you are using PyTorch1.x, enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#memory-efficient-attention" rel="nofollow">memory efficient attention</a> by setting <code>system.guidance.enable_memory_efficient_attention=true</code>. PyTorch2.0 has built-in support for this optimization and is enabled by default.</li>
<li>Enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#sliced-attention-for-additional-memory-savings" rel="nofollow">attention slicing</a> by setting <code>system.guidance.enable_attention_slicing=true</code>. This will slow down training by ~20%.</li>
<li>If you are using StableDiffusionGuidance, you can use <a href="https://github.com/dbolya/tomesd">Token Merging</a> to <strong>drastically</strong> speed up computation and save memory. You can easily enable Token Merging by setting <code>system.guidance.token_merging=true</code>. You can also customize the Token Merging behavior by setting the parameters <a href="https://github.com/dbolya/tomesd/blob/main/tomesd/patch.py#L183-L213">here</a> to <code>system.guidance.token_merging_params</code>. Note that Token Merging may degrade generation quality.</li>
<li>Enable <a href="https://huggingface.co/docs/diffusers/optimization/fp16#offloading-to-cpu-with-accelerate-for-memory-savings" rel="nofollow">sequential CPU offload</a> by setting <code>system.guidance.enable_sequential_cpu_offload=true</code>. This could save a lot of VRAM but will make the training <strong>extremely slow</strong>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">threestudio use <a href="https://github.com/omry/omegaconf">OmegaConf</a> to manage configurations. You can literally change anything inside the yaml configuration file or by adding command line arguments without <code>--</code>. We list all arguments that you can change in the configuration in our <a href="https://github.com/threestudio-project/threestudio/blob/main/DOCUMENTATION.md">documentation</a>. Happy experimenting!</p>
<h2 tabindex="-1" dir="auto">wandb (Weights &amp; Biases) logging</h2>
<p dir="auto">To enable the (experimental) wandb support, set <code>system.loggers.wandb.enable=true</code>, e.g.:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true`"><pre>python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true<span><span>`</span></span></pre></div>
<p dir="auto">If you're using a corporate wandb server, you may first need to login to your wandb instance, e.g.:
<code>wandb login --host=https://COMPANY_XYZ.wandb.io --relogin</code></p>
<p dir="auto">By default the runs will have a random name, recorded in the <code>threestudio</code> project. You can override them to give a more descriptive name, e.g.:</p>
<p dir="auto"><code>python launch.py --config configs/zero123.yaml --train --gpu 0 system.loggers.wandb.enable=true system.loggers.wandb.name="zero123xl_accum;bs=4;lr=0.05"</code></p>
<h2 tabindex="-1" dir="auto">Contributing to threestudio</h2>
<ul dir="auto">
<li>Fork the repository and create your branch from <code>main</code>.</li>
<li>Install development dependencies:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements-dev.txt"><pre>pip install -r requirements-dev.txt</pre></div>
<ul dir="auto">
<li>
<p dir="auto">If you are using VSCode as the text editor: (1) Install <code>editorconfig</code> extension. (2) Set the default linter to mypy to enable static type checking. (3) Set the default formatter to black. You could either manually format the document or let the editor format the document each time it is saved by setting <code>"editor.formatOnSave": true</code>.</p>
</li>
<li>
<p dir="auto">Run <code>pre-commit install</code> to install pre-commit hooks which will automatically format the files before commit.</p>
</li>
<li>
<p dir="auto">Make changes to the code, update README and DOCUMENTATION if needed, and open a pull request.</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Code Structure</h3>
<p dir="auto">Here we just briefly introduce the code structure of this project. We will make more detailed documentation about this in the future.</p>
<ul dir="auto">
<li>All methods are implemented as a subclass of <code>BaseSystem</code> (in <code>systems/base.py</code>). There typically are six modules inside a system: geometry, material, background, renderer, guidance, and prompt_processor. All modules are subclass of <code>BaseModule</code> (in <code>utils/base.py</code>) except for guidance, and prompt_processor, which are subclass of <code>BaseObject</code> to prevent them from being treated as model parameters and better control their behavior in multi-GPU settings.</li>
<li>All systems, modules, and data modules have their configurations in their own dataclasses.</li>
<li>Base configurations for the whole project can be found in <code>utils/config.py</code>. In the <code>ExperimentConfig</code> dataclass, <code>data</code>, <code>system</code>, and module configurations under <code>system</code> are parsed to configurations of each class mentioned above. These configurations are strictly typed, which means you can only use defined properties in the dataclass and stick to the defined type of each property. This configuration paradigm (1) naturally supports default values for properties; (2) effectively prevents wrong assignments of these properties (say typos in the yaml file) or inappropriate usage at runtime.</li>
<li>This projects use both static and runtime type checking. For more details, see <code>utils/typing.py</code>.</li>
<li>To update anything of a module at each training step, simply make it inherit to <code>Updateable</code> (see <code>utils/base.py</code>). At the beginning of each iteration, an <code>Updateable</code> will update itself, and update all its attributes that are also <code>Updateable</code>. Note that subclasses of <code>BaseSystem</code>, <code>BaseModule</code> and <code>BaseObject</code> are by default inherited to <code>Updateable</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Known Problems</h2>
<ul dir="auto">
<li>Gradients of Vanilla MLP parameters are empty in AMP (temporarily fixed by disabling autocast).</li>
<li>FullyFused MLP may cause NaNs in 32 precision.</li>
</ul>
<h2 tabindex="-1" dir="auto">Credits</h2>
<p dir="auto">threestudio is built on the following amazing open-source projects:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/Lightning-AI/lightning">Lightning</a></strong> Framework for creating highly organized PyTorch code.</li>
<li><strong><a href="https://github.com/omry/omegaconf">OmegaConf</a></strong> Flexible Python configuration system.</li>
<li><strong><a href="https://github.com/KAIR-BAIR/nerfacc">NerfAcc</a></strong> Plug-and-play NeRF acceleration.</li>
</ul>
<p dir="auto">The following repositories greatly inspire threestudio:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/ashawkey/stable-dreamfusion">Stable-DreamFusion</a></strong></li>
<li><strong><a href="https://github.com/eladrich/latent-nerf">Latent-NeRF</a></strong></li>
<li><strong><a href="https://github.com/pals-ttic/sjc">Score Jacobian Chaining</a></strong></li>
<li><strong><a href="https://github.com/ashawkey/fantasia3d.unofficial">Fantasia3D.unofficial</a></strong></li>
</ul>
<p dir="auto">Thanks to the maintainers of these projects for their contribution to the community!</p>
<h2 tabindex="-1" dir="auto">Citing threestudio</h2>
<p dir="auto">If you find threestudio helpful, please consider citing:</p>
<div data-snippet-clipboard-copy-content="@Misc{threestudio2023,
  author =       {Yuan-Chen Guo and Ying-Tian Liu and Ruizhi Shao and Christian Laforte and Vikram Voleti and Guan Luo and Chia-Hao Chen and Zi-Xin Zou and Chen Wang and Yan-Pei Cao and Song-Hai Zhang},
  title =        {threestudio: A unified framework for 3D content generation},
  howpublished = {\url{https://github.com/threestudio-project/threestudio}},
  year =         {2023}
}"><pre><code>@Misc{threestudio2023,
  author =       {Yuan-Chen Guo and Ying-Tian Liu and Ruizhi Shao and Christian Laforte and Vikram Voleti and Guan Luo and Chia-Hao Chen and Zi-Xin Zou and Chen Wang and Yan-Pei Cao and Song-Hai Zhang},
  title =        {threestudio: A unified framework for 3D content generation},
  howpublished = {\url{https://github.com/threestudio-project/threestudio}},
  year =         {2023}
}
</code></pre></div>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Database Fundamentals (664 pts)]]></title>
            <link>https://tontinton.com/posts/database-fundementals/</link>
            <guid>38655066</guid>
            <pubDate>Fri, 15 Dec 2023 15:28:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tontinton.com/posts/database-fundementals/">https://tontinton.com/posts/database-fundementals/</a>, See on <a href="https://news.ycombinator.com/item?id=38655066">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>About a year ago, I tried thinking which database I should choose for my next project, and came to the realization that I don't really know the differences of databases enough. I went to different database websites and saw mostly marketing and words I don't understand.</p><p>This is when I decided to read the excellent books <code>Database Internals</code> by Alex Petrov and <code>Designing Data-Intensive Applications</code> by Martin Kleppmann.</p><p>The books piqued my curiosity enough to write my own little database I called <a href="https://github.com/tontinton/dbeel">dbeel</a>.</p><p>This post is basically a short summary of these books, with a focus on the fundamental problems a database engineer thinks about in the shower.</p><h2 id="bashdb">bashdb</h2><p>Let's start with the simplest database program ever written, just 2 bash functions (we'll call it <code>bashdb</code>):</p><pre data-lang="bash"><code data-lang="bash"><span>#!/bin/bash
</span><span>
</span><span>db_set</span><span>() {
</span><span>    </span><span>echo </span><span>"$</span><span>1</span><span>,$</span><span>2</span><span>" </span><span>&gt;&gt;</span><span> database
</span><span>}
</span><span>
</span><span>db_get</span><span>() {
</span><span>    </span><span>grep </span><span>"^$</span><span>1</span><span>,"</span><span> database </span><span>| </span><span>sed</span><span> -e </span><span>"s/^$</span><span>1</span><span>,//" </span><span>| </span><span>tail</span><span> -n</span><span> 1
</span><span>}
</span></code></pre><p>Try it out:</p><pre data-lang="sh"><code data-lang="sh"><span>$</span><span> db_set 500 </span><span>'{"movie": "Airplane!", "rating": 9}'
</span><span>
</span><span>$</span><span> db_set 111 </span><span>'{"movie": "Tokio Drift", "rating": 6}'
</span><span>
</span><span>$</span><span> db_get 500
</span><span>{</span><span>"movie"</span><span>: </span><span>"Airplane!"</span><span>, </span><span>"rating"</span><span>: 9}
</span></code></pre><p>Before you continue reading, I want you to pause and think about why you wouldn't use <code>bashdb</code> in production.</p><pre><code><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>Some space for you to think :)
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span><span>
</span></code></pre><p>You probably came up with at least a dozen issues in <code>bashdb</code>. Now I won't go over <em>all</em> of the possible issues, for this post I will focus on the following ones:</p><ul><li><strong>Durability</strong> - If the machine crashes after a successful <code>db_set</code>, the data might be lost, as it was not flushed to disk.</li><li><strong>Atomicity</strong> - If the machine crashes while you call <code>db_set</code>, data might be written partially, corrupting our data.</li><li><strong>Isolation</strong> - If one process calls <code>db_get</code>, while another calls <code>db_set</code> concurrently on the same item, the first process might read only part of the data, leading to a corrupt result.</li><li><strong>Performance</strong> - <code>db_get</code> uses <code>grep</code>, so search goes line by line and is <code>O(n)</code>, <code>n</code> = all items saved.</li></ul><p>Could you figure out these problems yourself? If you could, well done, you don't need me, you already understand databases 😀</p><p>In the next section, we'll try get rid of these problems, to make <code>bashdb</code> a <em>real</em> database we might use in production (not really, please don't, just use <code>PostgreSQL</code>).</p><h2 id="improving-bashdb-to-be-acid">Improving bashdb to be ACID</h2><p>Before we begin, know that I did not come up with most of these problems on my own, they are part of an acronym named <code>ACID</code>, which almost all databases strive to guarantee:</p><ul><li><strong>Atomicity</strong> - Not to be confused with multi-threading's definition of atomicity (which is more similar to isolation), a transaction is considered atomic when a fault happens in the middle of a write, and the database either undos or aborts it completely, as if the write never started, leaving no partially written data.</li><li><strong>Consistency</strong> - This one doesn't really belong on ACID as a property of databases, as it is a property of the application.</li><li><strong>Isolation</strong> - No race conditions in concurrent accesses to the same data. There are multiple isolation levels, and we will discuss some of them later.</li><li><strong>Durability</strong> - The first thing that comes to mind when talking about a database. It should store data you wrote to it, forever, even in the event of monkeys pulling the power plug out.</li></ul><blockquote><p>Not all databases need to guarantee ACID, for some use cases, it is fine to drop guarantees for performance reasons.</p></blockquote><p>But <em>how</em> can we make <code>bashdb</code> ACID?</p><p>We can start with durability, as it's pretty easy to make <code>bashdb</code> durable by running <code>sync</code> right after writing in <code>db_set</code>:</p><pre data-lang="bash"><code data-lang="bash"><span>db_set</span><span>() {
</span><span>    </span><span>echo </span><span>"$</span><span>1</span><span>,$</span><span>2</span><span>" </span><span>&gt;&gt;</span><span> database </span><span>&amp;&amp; </span><span>sync</span><span> -d</span><span> database
</span><span>}
</span></code></pre><p>But wait a minute, what is going on, what is <code>sync</code> really doing? And what is that <code>-d</code>?</p><h3 id="durability">Durability</h3><p>The <code>write</code> syscall writes a buffer to a file, but who said it writes to disk?</p><p>The buffer you write could end up in any cache along the way to the non volatile memory. For example, the kernel stores the buffer in the page cache with each page marked as dirty, meaning it will flush it to disk sometime in the future.</p><p>To make matters worse, the disk device, or something managing your disks (for example a RAID system), might have a write cache as well.</p><p>So how do you tell all the systems in the middle to flush all dirty pages to the disk? For that we have <code>fsync</code> / <code>fdatasync</code>, let's see what <code>man</code> has to say:</p><pre><code><span>$ man 2 fsync
</span><span>
</span><span>...
</span><span>
</span><span>fsync() transfers ("flushes") all modified in-core data of (i.e., modified buffer cache pages for)
</span><span>the file referred to by the file descriptor fd to the disk device (or other permanent storage
</span><span>device) so that all changed information can be retrieved even if the system crashes or is rebooted.
</span><span>This includes writing through or flushing a disk cache if present.
</span><span>The call blocks until the device reports that the transfer has completed.
</span><span>
</span><span>...
</span><span>
</span><span>fdatasync() is similar to fsync(), but does not flush modified metadata unless that metadata itself
</span><span>in order to allow a subsequent data  retrieval to be correctly handled.
</span><span>
</span><span>...
</span></code></pre><p>In short, <code>fdatasync</code> flushes the dirty raw buffers we gave <code>write</code>. <code>fsync</code> also flushes the file's metadata like <code>mtime</code>, which we don't really care about.</p><p>The <code>sync</code> program is basically like running <code>fsync</code> on all dirty pages, unless a specific file is specified as one of the arguments. It has the <code>-d</code> flag for us to call <code>fdatasync</code> instead of <code>fsync</code>.</p><p>The biggest drawback in adding <code>sync</code> is that we get worse performance. Usually sync is slower than even the write itself. But hey, at least we are now <em>durable</em>.</p><blockquote><p>A short but important note about fsync. When fsync() returns success it means "all writes since the last fsync have hit disk" when you might have assumed it means "all writes since the last SUCCESSFUL fsync have hit disk". PostgreSQL learned about this only recently (2018), which led to them modifying the behavior of syncing from retrying fsync until a success is returned, to simply panic on fsync failure. This incident got famous and was named fsyncgate. You can learn a lot more about fsync failures <a href="https://www.usenix.org/system/files/atc20-rebello.pdf">here</a>.</p></blockquote><blockquote><p>Dear <code>MongoDB</code> users, know that by default writes are <a href="https://www.mongodb.com/docs/manual/core/journaling/writes">synced every 100ms</a>, meaning it is not 100% durable.</p></blockquote><h3 id="isolation">Isolation</h3><p>The simplest way to have multiprocess isolation in <code>bashdb</code> is to add a lock before we read / write to the storage file.</p><p>There's a program in linux called <code>flock</code>, which locks a file, and you can even provide it with the <code>-s</code> flag, to specify that you will not modify the file, meaning all callers who specify <code>-s</code> are allowed to read the file concurrently. <code>flock</code> blocks until it has taken the lock.</p><blockquote><p>flock simply calls the flock syscall</p></blockquote><p>With such an awesome program, <code>bashdb</code> can guarantee <em>isolation</em>, here's the code:</p><pre data-lang="bash"><code data-lang="bash"><span>db_set</span><span>() {
</span><span>    (
</span><span>        </span><span>flock</span><span> 9 </span><span>&amp;&amp; </span><span>echo </span><span>"$</span><span>1</span><span>,$</span><span>2</span><span>" </span><span>&gt;&gt;</span><span> database
</span><span>    ) </span><span>9</span><span>&gt;</span><span>database.lock
</span><span>}
</span><span>
</span><span>db_get</span><span>() {
</span><span>    (
</span><span>        </span><span>flock</span><span> -s</span><span> 9 </span><span>&amp;&amp; </span><span>grep </span><span>"^$</span><span>1</span><span>,"</span><span> database </span><span>| </span><span>sed</span><span> -e </span><span>"s/^$</span><span>1</span><span>,//" </span><span>| </span><span>tail</span><span> -n</span><span> 1
</span><span>    ) </span><span>9</span><span>&gt;</span><span>database.lock
</span><span>}
</span></code></pre><p>The biggest drawback is that we are now locking the entire database whenever we write to it.</p><p>The only things left are atomicity and improving the algorithm to not be <code>O(n)</code>.</p><h2 id="bad-news">Bad News</h2><p>I'm sorry, this is as far as I could get with <code>bashdb</code>, I could not find a simple way to ensure atomicity in bash ☹️</p><p>And even if it was possible, we still need to fix the <code>O(n)</code> situation.</p><p>Before beginning the <code>bashdb</code> adventure, I knew that we won't be able to easily solve all these problems in less than 10 lines of bash, but by trying to, you've hopefully started to get a feel for the problems database engineers face.</p><h2 id="storage-engine">Storage Engine</h2><p>Let's start with the first big component of a database, the <code>Storage Engine</code>.</p><p>The purpose of the storage engine is to provide an abstraction over reading and writing data to persistent storage, with the main goal to be <strong>fast</strong>, i.e. have <strong>high throughput</strong> and <strong>low latency</strong> on requests.</p><p>But what makes software slow?</p><pre><code><span>Latency Comparison Numbers (~2012)
</span><span>----------------------------------
</span><span>L1 cache reference                           0.5 ns
</span><span>Branch mispredict                            5   ns
</span><span>L2 cache reference                           7   ns                      14x L1 cache
</span><span>Mutex lock/unlock                           25   ns
</span><span>Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
</span><span>Compress 1K bytes with Zippy             3,000   ns        3 us
</span><span>Send 1K bytes over 1 Gbps network       10,000   ns       10 us
</span><span>Read 4K randomly from SSD              150,000   ns      150 us          ~1GB/sec SSD
</span><span>Read 1 MB sequentially from memory     250,000   ns      250 us
</span><span>Round trip within same datacenter      500,000   ns      500 us
</span><span>Read 1 MB sequentially from SSD      1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
</span><span>Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
</span><span>Read 1 MB sequentially from disk    20,000,000   ns   20,000 us   20 ms  80x memory, 20X SSD
</span><span>Send packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms
</span></code></pre><p>If L1 cache reference took as long as a heart beat (around half a second), reading 1 MB sequentially from SSD would take ~12 days and reading 1 MB sequentially from disk would take ~8 months.</p><p>This is why the main limitation of storage engines is the disk itself, and thus all designs try to minimize disk I/O and disk seeks as much as possible. Some designs even get rid of disks in favor of SSDs (although they are much more expensive).</p><p>A storage engine design usually consists of:</p><ul><li>The underlying data structure to store items on disk.</li><li>ACID transactions. <ul><li>Some may skip this to achieve better performance for specific use cases where ACID is not important.</li></ul></li><li>Some cache - to not read from disk <em>every</em> time. <ul><li>Most use buffered I/O to let the OS cache for us.</li></ul></li><li>API layer - SQL / document / graph / ...</li></ul><p>Storage engine data structures come in all shapes and sizes, I'm going to focus on the 2 categories you will most likely find in the wild - mutable and immutable data structures.</p><p>Mutable means that after writing data to a file, the data can be overwritten later in the future, while immutable means that after writing data to a file, it can only be read again.</p><h2 id="mutable-b-trees">Mutable B-Trees</h2><p>To achieve the goal of maintaining good performance as the amount of data scales up, the data structure we use should be able to search an item in at most logarithmic time, and not linear time like in <code>bashdb</code>.</p><p>A simple data structure you are probably familiar with is the BST (binary search tree), where lookups are made in <code>O(log n)</code> time.</p><p>The problem with BSTs is nodes are placed randomly apart from each other, which means that after reading a node while traversing the tree, the next node is most likely going to be somewhere far away on disk. To minimize disk I/O &amp; seeks, each page read from disk should be read as much as possible from memory again, without reaching to disk.</p><p>The property we're looking for is called "spatial locality", and one of the most famous "spatially local" variations of BSTs are B-trees.</p><p>B-tree generalizes BST, allowing for nodes with more than two children. Here's what they look like:</p><pre><code><span>                  ------------------------------------
</span><span>                  |     7     |     16     |    |    |
</span><span>                  ------------------------------------
</span><span>                 /            |             \
</span><span>-----------------     ----------------       -----------------
</span><span>| 1 | 2 | 5 | 6 |     | 9 | 12 |  |  |       | 18 | 21 |  |  |
</span><span>-----------------     ----------------       -----------------
</span></code></pre><p>With the search algorithm in pseudo python code:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>get</span><span>(</span><span>node</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>for </span><span>i, child </span><span>in </span><span>enumerate</span><span>(node</span><span>.</span><span>children):
</span><span>        </span><span>if not </span><span>child:
</span><span>            </span><span>return </span><span>None
</span><span>
</span><span>        </span><span>if </span><span>child</span><span>.</span><span>key </span><span>== </span><span>key:
</span><span>            </span><span># Found it!
</span><span>            </span><span>return </span><span>child</span><span>.</span><span>value
</span><span>
</span><span>        </span><span>if </span><span>child</span><span>.</span><span>key </span><span>&gt; </span><span>key:
</span><span>            </span><span>return </span><span>get</span><span>(node</span><span>.</span><span>nodes[i], key)
</span><span>
</span><span>    </span><span>return </span><span>get</span><span>(node</span><span>.</span><span>nodes[</span><span>-</span><span>1</span><span>], key)
</span></code></pre><p>On each read of a page from disk (usually 4KB or 8KB), we iterate over multiple nodes sequentially from memory and the various CPU caches, trying to keep the least amount of bytes read go to waste.</p><p>Remember, reading from memory and the CPU caches is a few order of magnitudes faster than disk, so much faster in fact, that it can be considered to be basically free in comparison.</p><p>I know some of you reading this right now think to themselves <em>"Why not binary search instead of doing it linearly?"</em>, to you I say, please look at the L1 / L2 cache reference times in the latency comparison numbers table again. Also, modern CPUs execute multiple operations in parallel when it operates on sequential memory thanks to <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a>, <a href="https://en.wikipedia.org/wiki/Instruction_pipelining">instruction pipelining</a> and <a href="https://en.wikipedia.org/wiki/Cache_prefetching">prefetching</a>. You would be surprised just how far reading sequential memory can take you in terms of performance.</p><p>There's a variation of the B-tree that takes this model even further, called a B+ tree, where the final leaf nodes hold a value and all other nodes hold only keys, thus fetching a page from disk results in a lot more keys to compare.</p><p>B-trees, to be space optimized, need to sometimes reclaim space as a consequence of data fragmentation created by operations on the tree like:</p><ul><li>Big value updates - updating a value into a larger value might overwrite data of the next node, so the tree relocates the item to a different location, leaving a "hole" in the original page.</li><li>Small value updates - updating a value to a smaller value leaves a "hole" at the end.</li><li>Deletes - deletion causes a "hole" right where the deleted value used to reside.</li></ul><p>The process that takes care of space reclamation and page rewrites can sometimes be called vacuum, compaction, page defragmentation, and maintenance. It is usually done in the background to not interfere and cause latency spikes to user requests.</p><blockquote><p>See for example how in <code>PostgreSQL</code> you can configure an <a href="https://www.postgresql.org/docs/current/routine-vacuuming.html">auto vacuum daemon</a>.</p></blockquote><p>B-trees are most commonly used as the underlying data structure of an index (<code>PostgreSQL</code> creates B-tree indexes by default), or all data (I've seen <code>DynamoDB</code> once jokingly called <em>"a distributed B-tree"</em>).</p><h2 id="immutable-lsm-tree">Immutable LSM Tree</h2><p>As we have already seen in the latency comparison numbers table, disk seeks are really expensive, which is why the idea of sequentially written immutable data structures got so popular.</p><p>The idea is that if you only append data to a file, the disk needle doesn't need to move as much to the next position where data will be written. On write heavy workloads it has been proven very beneficial.</p><p>One such append only data structure is called the <code>Log Structured Merge tree</code> or <code>LSM tree</code> in short, and is what powers <em>a lot</em> of modern database storage engines, such as <code>RocksDB</code>, <code>Cassandra</code> and my personal favorite <code>ScyllaDB</code>.</p><p>LSM trees' general concept is to buffer writes to a data structure in memory, preferably one that is easy to iterate in a sorted fashion (for example <code>AVL tree</code> / <code>Red Black tree</code> / <code>Skip List</code>), and once it reaches some capacity, flush it sorted to a new file called a <code>Sorted String Table</code> or <code>SSTable</code>. An SSTable stores sorted data, letting us leverage binary search and sparse indexes to lower the amount of disk I/O.</p><img src="https://tontinton.com/lsm_tree_write.svg"><p>To maintain durability, when data is written to memory, the action is stored in a <code>Write-Ahead Log</code> or <code>WAL</code>, which is read on program's startup to reset state to as it was before shutting down / crashing.</p><p>Deletions are also appended the same way a write would, it simply holds a tombstone instead of a value. The tombstones get deleted in the compaction process detailed later.</p><p>The read path is where it a bit wonky, reading from an LSM tree is done by first searching for the item of the provided key in the data structure in memory, if not found, it then searches for the item by iterating over all SSTables on disk, from the newest one to the oldest.</p><img src="https://tontinton.com/lsm_tree_read.svg"><p>You can probably already tell that as more and more data is written, there will be more SSTables to go through to find an item of a specific key, and even though each file is sorted, going over a lot of small files is slower than going over one big file with all items (lookup time complexity: <code>log(num_files * table_size) &lt; num_files * log(table_size)</code>). This is another reason why LSM trees require compaction, in addition to removing tombstones.</p><p>In other words: compaction combines a few small SSTables into one big SSTable, removing all tombstones in the process, and is usually run as a background process.</p><img src="https://tontinton.com/lsm_tree_compact.svg"><p>Compaction can be implemented using a binary heap / priority queue, something like:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>compact</span><span>(</span><span>sstables</span><span>, </span><span>output_sstable</span><span>): 
</span><span>    </span><span># Ordered by ascending key. pop() results in the item of the smallest key.
</span><span>    heap </span><span>= </span><span>heapq</span><span>.</span><span>heapify</span><span>([(sstable</span><span>.</span><span>next</span><span>(), sstable) </span><span>for </span><span>sstable </span><span>in </span><span>sstables])
</span><span>
</span><span>    </span><span>while </span><span>(item, sstable) </span><span>:= </span><span>heap</span><span>.</span><span>pop</span><span>()
</span><span>        </span><span>if not </span><span>item</span><span>.</span><span>is_tombstone</span><span>():
</span><span>            output_sstable</span><span>.</span><span>write</span><span>(item)
</span><span>
</span><span>        </span><span>if </span><span>item </span><span>:= </span><span>sstable</span><span>.</span><span>next</span><span>():
</span><span>            </span><span># For code brevity, imagine pushing an item with a key that exists
</span><span>            </span><span># in the heap removes the item with the smaller timestamp,
</span><span>            </span><span># resulting in last write wins.
</span><span>            heap</span><span>.</span><span>push</span><span>((item, sstable))
</span></code></pre><blockquote><p>For a real working example in rust 🦀, <a href="https://github.com/tontinton/dbeel/blob/ee3de152a5/src/storage_engine/lsm_tree.rs#L1038">click here</a>.</p></blockquote><p>To optimize an LSM tree, you should decide <em>when</em> to compact and on <em>which</em> sstable files. <code>RocksDB</code> for example implements <a href="https://github.com/facebook/rocksdb/wiki/Leveled-Compaction">Leveled Compaction</a>, where the newly flushed sstables are said to reside in level 0, and once a configured N number of files are created in a level, they are compacted and the new file is promoted to the next level.</p><p>It's important to handle removal of tombstones with care to not cause data resurrection. An item might be removed and then resurrected on compaction with another file that holds that item, even if the write happened before the deletion, there is no way to know once deleted in a previous compaction. <code>RocksDB</code> keeps tombstones around until a compaction of files that result in a promotion to the last level.</p><h3 id="bloom-filters">Bloom Filters</h3><p>LSM trees can be further optimized by something called a bloom filter.</p><p>A bloom filter is a probabilistic set data structure that lets you to efficiently check whether an item doesn't exist in a set. Checking whether an item exists in the set results in either <code>false</code>, which means the item is definitely not in the set, or in <code>true</code>, which means the item is <strong>maybe</strong> in the set, and that's why it's called a <em>probabilistic</em> data structure.</p><p>The beauty is that the space complexity of a bloom filter set of <code>n</code> items is <code>O(log n)</code>, while a regular set with <code>n</code> items is <code>O(n)</code>.</p><p>How do they work? The answer is hash functions! On insertion, they run multiple different hash functions on the inserted key, then take the results and store 1 in the corresponding bit (<code>result % number_of_bits</code>).</p><pre data-lang="python"><code data-lang="python"><span># A bloom filter's bitmap of size 8 (bits).
</span><span>bloom </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>]
</span><span>
</span><span># Inserting key - first run 2 hash functions.
</span><span>Hash1</span><span>(key1) </span><span>= </span><span>100
</span><span>Hash2</span><span>(key1) </span><span>= </span><span>55
</span><span>
</span><span># Then calculate corresponding bits.
</span><span>bits </span><span>= </span><span>[</span><span>100 </span><span>% </span><span>8</span><span>, </span><span>55 </span><span>% </span><span>8</span><span>] </span><span>= </span><span>[</span><span>4</span><span>, </span><span>7</span><span>]
</span><span>
</span><span># Set 1 to corresponding bits.
</span><span>bloom[</span><span>4</span><span>] </span><span>= </span><span>1
</span><span>bloom[</span><span>7</span><span>] </span><span>= </span><span>1
</span><span>
</span><span># After insertion it should look like:
</span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>]
</span></code></pre><p>Now comes the exciting part - checking!</p><pre data-lang="python"><code data-lang="python"><span>bloom </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>, </span><span>0</span><span>, </span><span>0</span><span>, </span><span>1</span><span>]
</span><span>
</span><span># To check a key, simply run the 2 hash functions and find the corresponding
</span><span># bits, exactly like you would on insertion:
</span><span>Hash1</span><span>(key2) </span><span>= </span><span>34
</span><span>Hash2</span><span>(key2) </span><span>= </span><span>35
</span><span>
</span><span>bits </span><span>= </span><span>[</span><span>34 </span><span>% </span><span>8</span><span>, </span><span>35 </span><span>% </span><span>8</span><span>] </span><span>= </span><span>[</span><span>2</span><span>, </span><span>3</span><span>]
</span><span>
</span><span># And then check whether all the corresponding bits hold 1, if true, the item
</span><span># maybe exists in the set, otherwise it definitely isn't.
</span><span>result </span><span>= </span><span>[bloom[</span><span>2</span><span>], bloom[</span><span>3</span><span>]] </span><span>= </span><span>[</span><span>0</span><span>, </span><span>0</span><span>] </span><span>= </span><span>false
</span><span>
</span><span># false. key2 was never inserted in the set, otherwise those exact same bits
</span><span># would have all been set to 1.
</span></code></pre><blockquote><p>Think about why it is that even when all checked bits are 1, it doesn't guarantee that the same exact key was inserted before.</p></blockquote><p>A nice benefit of bloom filters is that you can control the chance of being certain that the item doesn't exist in the set, by allocating more memory for the bitmap and by adding more hash functions. There's even <a href="https://hur.st/bloomfilter/">calculators</a> for it.</p><p>LSM trees can store a bloom filter for each SSTable, to skip searching in SSTables if their bloom filter validates that an item doesn't exist in it. Otherwise, we search the SSTable normally, even if the item doesn't necessarily exist in it.</p><h2 id="write-ahead-log">Write Ahead Log</h2><p>Remember ACID? Let's talk briefly about how storage engines achieve ACID transactions.</p><p>Atomicity and durability are properties of whether data is correct at all times, even when power shuts down the machine.</p><p>The most popular method to survive sudden crashes is to log all transaction actions into a special file called a <code>Write-Ahead Log</code> / <code>WAL</code> (we touched on this briefly in the <code>LSM tree</code> section).</p><p>When the database process starts, it reads the <code>WAL</code> file, and reconstructs the state of the data, skipping all transactions that don't have a commit log, thus achieving atomicity.</p><p>Also, as long a write request's data is written + flushed to the <code>WAL</code> file before the user receives the response, the data is going to be 100% read at startup, meaning you also achieve durability.</p><p>WALs are basically a sort of <a href="https://martinfowler.com/eaaDev/EventSourcing.html">event sourcing</a> of the transactional events.</p><h2 id="isolation-1">Isolation</h2><p>To achieve isolation, you can either:</p><ul><li>Use pessimistic locks - Block access to data that is currently being written to.</li><li>Use optimistic locks - Update a copy of the data and then commit it only whether the data was not modified during the transaction, if it did, retry on the new data. Also known as optimistic concurrency control.</li><li>Read a copy of the data - MVCC (Multiversion concurrency control) is a common method used to avoid blocking user requests. In MVCC when data is mutated, instead of locking + overwriting it, you create a new version of the data that new requests read from. Once no readers remain that are reading the old data it can be safely removed. With MVCC, each user sees a <em>snapshot</em> of the database at a specific instant in time.</li></ul><p>Some applications don't require perfect isolation (or <code>Serializable Isolation</code>), and can relax their read isolation levels.</p><p>The ANSI/ISO standard SQL 92 includes 3 different possible outcomes from reading data in a transaction, while another transaction might have updated that data:</p><ul><li><strong>Dirty reads</strong> - A dirty read occurs when a transaction retrieves a row that has been updated by another transaction that is not yet committed.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 20
</span><span>
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>UPDATE</span><span> users </span><span>SET</span><span> age </span><span>= </span><span>21 </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>                                        </span><span>-- no commit here
</span><span>
</span><span>
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves in 21
</span><span>COMMIT</span><span>;
</span></code></pre><ul><li><strong>Non-repeatable reads</strong> - A non-repeatable read occurs when a transaction retrieves a row twice and that row is updated by another transaction that is committed in between.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 20
</span><span>
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>UPDATE</span><span> users </span><span>SET</span><span> age </span><span>= </span><span>21 </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>                                        </span><span>COMMIT</span><span>;
</span><span>
</span><span>
</span><span>SELECT</span><span> age </span><span>FROM</span><span> users </span><span>WHERE</span><span> id </span><span>= </span><span>1</span><span>;
</span><span>-- retrieves 21
</span><span>COMMIT</span><span>;
</span></code></pre><ul><li><strong>Phantom reads</strong> - A phantom read occurs when a transaction retrieves a set of rows twice and new rows are inserted into or removed from that set by another transaction that is committed in between.</li></ul><pre data-lang="sql"><code data-lang="sql"><span>BEGIN</span><span>;
</span><span>SELECT</span><span> name </span><span>FROM</span><span> users </span><span>WHERE</span><span> age </span><span>&gt; </span><span>17</span><span>;
</span><span>-- retrieves Alice and Bob
</span><span>	
</span><span>
</span><span>                                        </span><span>BEGIN</span><span>;
</span><span>                                        </span><span>INSERT INTO</span><span> users </span><span>VALUES</span><span> (</span><span>3</span><span>, </span><span>'Carol'</span><span>, </span><span>26</span><span>);
</span><span>                                        </span><span>COMMIT</span><span>;
</span><span>
</span><span>
</span><span>SELECT</span><span> name </span><span>FROM</span><span> users </span><span>WHERE</span><span> age </span><span>&gt; </span><span>17</span><span>;
</span><span>-- retrieves Alice, Bob and Carol
</span><span>COMMIT</span><span>;
</span></code></pre><p>Your application might not need a guarantee of no dirty reads for example in a specific transaction, so it can choose a different isolation level to allow greater performance, as to achieve higher isolation levels, you usually sacrifice performance.</p><p>Here are isolation levels defined by the ANSI/SQL 92 standard from highest to lowest (higher levels guarantee at least everything lower levels guarantee):</p><ul><li><strong>Serializable</strong> - The highest isolation level. Reads always return data that is committed, including range based writes on multiple rows (avoiding phantom reads).</li><li><strong>Repeatable reads</strong> - Phantom reads are acceptable.</li><li><strong>Read committed</strong> - Non-repeatable reads are acceptable.</li><li><strong>Read uncommitted</strong> - The lowest isolation level. Dirty reads are acceptable.</li></ul><blockquote><p>The ANSI/SQL 92 standard isolation levels are often criticized for not being complete. For example, many MVCC implementations offer <a href="https://en.wikipedia.org/wiki/Snapshot_isolation">snapshot isolation</a> and not serializable isolation (for the differences, read the provided wikipedia link). If you want to learn more about MVCC, I recommend reading about <a href="https://db.in.tum.de/~muehlbau/papers/mvcc.pdf">HyPer</a>, a fast serializable MVCC algorithm.</p></blockquote><p>So to conclude the storage engine part of this post, the fundamental problem you solve writing a storage engine are: how to store / retrieve data while trying to guarantee some ACID transactions in the most performant way.</p><blockquote><p>One topic I left out is the API to choose when writing a database / storage engine, but I'll leave a post called <a href="https://www.scattered-thoughts.net/writing/against-sql/">"Against SQL"</a> for you to start exploring the topic yourself.</p></blockquote><h2 id="distributed-systems">Distributed Systems</h2><p>Going distributed should be a last mile resort, introducing it to a system adds a <strong>ton</strong> of complexity, as we will soon learn. Please avoid using distributed systems when non distributed solutions suffice.</p><blockquote><p>A distributed system is one in which the failure of a computer you didn’t even know existed can render your own computer unusable. ~Leslie Lamport</p></blockquote><p>The common use cases of needing to distribute data across multiple machines are:</p><ul><li><strong>Availability</strong> - If for some reason the machine running the database crashes / disconnects from our users, we might still want to let users use the application. By distributing data, when one machine fails, you can simply point requests to another machine holding the "redundant" data.</li><li><strong>Horizontal Scaling</strong> - Conventionally, when an application needed to serve more user requests than it can handle, we would have upgraded the machine's resources (faster / more disk, RAM, CPUs). This is called <code>Vertical Scaling</code>. It can get very expensive and for some workloads there just doesn't exist hardware to match the amount of resources needed. Also, most of the time you don't need all those resources, except in peaks of traffic (imagine shopify on black Friday). Another strategy called <code>Horizontal Scaling</code>, is to operate on multiple separate machines connected over a network, seemingly working as a single machine.</li></ul><p>Sounds like a dream, right? What can go wrong with going distributed?</p><p>Well, you have now introduced operational complexity (deployments / etc...) and more importantly partitioning / network partitioning, infamous for being the P in something called the CAP theorem.</p><p>The CAP theorem states that a system can guarantee only 2 of the following 3:</p><ul><li><strong>Consistency</strong> - Reads receive the most recent write.</li><li><strong>Availability</strong> - All requests succeed, no matter the failures.</li><li><strong>Partition Tolerance</strong> - The system continues to operate despite dropped / delayed messages between nodes.</li></ul><p>To understand why this is, imagine a database operating on a single machine. It is definitely <em>partition tolerant</em>, as messages in the system are not sent through something like a network, but through function calls operating on the same hardware (CPU / memory). It is also <em>consistent</em>, as the state of the data is saved on the same hardware (memory / disk) that all other read / write requests operate on. Once the machine fails (be it software failures like SIGSEGV or hardware failures like the disk overheating) all new requests to it fail, violating <em>availability</em>.</p><p>Now imagine a database operating on 2 machines with separate CPUs, memory and disks, connected through some cable. When a request to one of the machines fails, for whatever reason, the system can choose to do one of the following:</p><ul><li>Cancel the request, thus sacrificing <em>availability</em> for <em>consistency</em>.</li><li>Allow the request to continue only on the working machine, meaning once the other machine will now have inconsistent data (reads from it will not return the most recent write), thus sacrificing <em>consistency</em> for <em>availability</em>. When a system does this, it is called eventually consistent.</li></ul><p>The original <a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">dynamo paper</a> is famous for many things, one of them being amazon stating that amazon.com's shopping cart should be highly available, and that it's more important to them than consistency. In the unlikely scenario a user sees 2 of the same item in the shopping cart, they will simply remove one of them, which is a better situation then them not being able to purchase and pay money!</p><blockquote><p>I really enjoy out of the box thinking of sacrificing something that adds software complexity (like consistency in amazon's shopping cart) for a simpler human solution like the user getting a refund. Software complexity can get more expensive to operate than having a refund budget for example.</p></blockquote><p>To achieve <em>availability</em> it's not enough to have multiple nodes together combining all the data, there must also be data redundancy, or in other words, for each item a node stores there must be at least 1 other node to store a copy of that item. These nodes are usually called <strong>replicas</strong>, and the process of copying the data is called <strong>replication</strong>.</p><p>Assigning more replica nodes means that the system will be more <em>available</em>, with the obvious drawback of needing more resources to store all these copies.</p><blockquote><p>Copies of data don't need to be stored "whole", they can be split and scattered across multiple nodes using a technique called erasure coding, which also has some interesting <a href="https://brooker.co.za/blog/2023/01/06/erasure.html">latency characteristics</a> (by the way brooker's blog is simply amazing for learning distributed systems).</p></blockquote><h2 id="consistent-hashing">Consistent Hashing</h2><p>Now that you have multiple nodes, you need some kind of load balancing / data partitioning method. When a request to store some data comes in, how do you determine which node receives the request?</p><p>You could go for the simplest solution, which is to simply always take a primary key (some id) in addition to the data, hash the key and modulo the result by the number of available nodes, something like:</p><pre data-lang="python"><code data-lang="python"><span>def </span><span>get_owning_node</span><span>(</span><span>nodes</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>return </span><span>nodes[</span><span>hash</span><span>(key) </span><span>% </span><span>len</span><span>(nodes)] 
</span></code></pre><p>This modulo method works fine, until a node is either added or removed from the cluster. Once that happens, the calculation returns a different result because the number of available nodes changed, meaning a different node will be selected for the same key. To accommodate, each node can migrate keys that should now live on different nodes, but then almost all items are migrated, which is really expensive.</p><p>One method to lower the amount of items to be migrated on node addition / removal that is used by some databases (e.g. <code>Dynamo</code> and <code>Cassandra</code>) is <code>Consistent Hashing</code>.</p><p>Consistent hashing creates a ring of nodes instead of an array, placing each node's name hash on the ring. Then each request's key is hashed just like before, but instead of doing the modulo operation, we get the first node in the ring whose name's hash is greater or equal to the request key hash:</p><pre data-lang="python"><code data-lang="python"><span># Assume nodes are sorted, with the first node having the smallest hash value.
</span><span>def </span><span>get_owning_node</span><span>(</span><span>nodes</span><span>, </span><span>key</span><span>):
</span><span>    </span><span>if </span><span>len</span><span>(nodes) </span><span>== </span><span>0</span><span>:
</span><span>        </span><span>return </span><span>None
</span><span>
</span><span>    key_hash </span><span>= </span><span>hash</span><span>(key)
</span><span>
</span><span>    </span><span>for </span><span>node </span><span>in </span><span>nodes:
</span><span>        </span><span>if </span><span>node</span><span>.</span><span>hash </span><span>&gt;= </span><span>key_hash:
</span><span>            </span><span>return </span><span>node
</span><span>
</span><span>    </span><span>return </span><span>nodes[</span><span>0</span><span>]
</span></code></pre><p>For a visual explanation, imagine a ring that goes from 0 -&gt; 99, holding nodes with the names "half", "quarter" and "zero" whose hashes are 50, 25 and 0 respectively:</p><pre><code><span>   zero
</span><span> /      \
</span><span>|     quarter 
</span><span> \      /
</span><span>   half
</span></code></pre><p>Let's say a user now wants to set an item with the key "four-fifths", with a hash value of 80. The first node with a name hash greater or equal to 80 is "half" (with hash value of 50), so that's the node to receive the request!</p><p>Choosing replicas is very simple, when an item is set to be stored on a specific node, go around the ring counter-clockwise, the next node will store a copy of that item. In our example, "zero" is the replica node for all items "half" owns, so when "half" dies and requests will now be routed to "zero", it can serve these requests, keeping our system <em>available</em>. This method is sometimes called <code>Leaderless Replication</code> and is used by "Dynamo" style databases like <code>Cassandra</code>.</p><blockquote><p>Another method is to choose a leader node and replica nodes is <code>Leader Election</code>, which is a huge topic on its own that I won't get into in this post.</p></blockquote><p>Now, what happens when a node is added to the cluster? Let's add a node named "three-quarters" with a hash value of 75, the item "four-fifths" should be migrated to the new "three-quarters" node, as new requests to it will now point to it.</p><p>This migration process is a lot less expensive than what we previously had in the modulo solution. The number of keys that need to be migrated is equal to <code>num_keys / num_nodes</code> on average.</p><p>A cool trick is to introduce the concept of virtual nodes, where you add multiple instances of a node to the ring, to lower the chances of some nodes owning more items than other nodes (in our example "half" will store twice as many items on average than the other nodes). You can generate virtual node names by for example adding an index as a suffix to the node name ("half-0", "half-1", etc...) and then the hash will result in a completely different location on the ring.</p><p>Here's a more detailed example of a migration in a cluster with a replication factor of 3:</p><img src="https://tontinton.com/migration.svg"><blockquote><p>Same colored nodes are virtual nodes of the same node, green arrows show to which node an item is being migrated to, red arrows show item deletions from nodes and the brown diamonds are items.</p></blockquote><h2 id="leaderless-replication">Leaderless Replication</h2><p>In a leaderless setup, you get amazing <em>availability</em>, while sacrificing <em>consistency</em>. If the owning node is down on a write request, it will be written to the replica, and once the owning node is up and running again, a read request will read stale data.</p><p>When <em>consistency</em> is needed for a specific request, read requests can be sent in parallel to several replica nodes as well as to the owning node. The client will pick the most up to date data. Write requests are usually sent in parallel to all replica nodes but wait for an acknowledgement from only some of them. By choosing the number of read requests and number of write requests acknowledge, you can tune the <em>consistency</em> level on a request level.</p><p>To know whether a request is <em>consistent</em>, you just need to validate that <code>R + W &gt; N/2 + 1</code>, where:</p><ul><li><strong>N</strong> - Number of nodes holding a copy of the data.</li><li><strong>W</strong> - Number of nodes that will acknowledge a write for it to succeed.</li><li><strong>R</strong> - Number of nodes that have to respond to a read operation for it to succeed.</li></ul><blockquote><p>Sending a request to a majority of nodes (where <code>W</code> or <code>R</code> is equal to <code>N/2 + 1</code>) is called a quorum.</p></blockquote><p>Picking the correct read as the latest written one is called <code>Conflict Resolution</code> and it is not a simple task, you might think that simply comparing timestamps and choosing the biggest one is enough, but using times in a distributed system are unreliable.</p><blockquote><p>This didn't stop <a href="https://cassandra.apache.org/doc/latest/cassandra/architecture/dynamo.html#data-versioning">Cassandra from using timestamps</a> though.</p></blockquote><p>Each machine has its own hardware clock, and the clocks <em>drift</em> apart as they are not perfectly accurate (usually a quartz crystal oscillator). Synchronizing clocks using NTP (Network Time Protocol), where a server returns the time from a more accurate time source such as a GPS receiver, is not enough to provide accurate results, as the NTP request is over the network (another distributed system) and we can't know exactly how much time will pass before receiving a response.</p><blockquote><p>Google's <code>Spanner</code> actually did achieve consistency with clocks, by uses special high precision time hardware and its API exposes the time range uncertainty of each timestamp. You can read more about it <a href="https://research.google/pubs/pub39966.pdf">here</a>.</p></blockquote><p>But if clocks are so unreliable, how else are we supposed to know which value is correct?</p><p>Some systems (for example <code>Dynamo</code>) try to solve this partially using <code>Version Vectors</code>, where you attach a (node, counter) pair for each version of an item, which gives you the ability to find causality between the different versions. By finding versions of values that are definitely newer (have a higher counter) you can remove some versions of a value, which makes the problem easier.</p><img src="https://tontinton.com/version_vector.svg"><blockquote><p>An example showing how easily conflicts arise. At the end we are left with {v2, v3} as the conflicting values for the same key. The reason I removed v1 is to show that by using something like <code>Version Vectors</code>, versions of values can be safely removed to minimize the amount of conflicts. To learn more on <code>Version Vectors</code> and their implementations, I recommend reading <a href="https://github.com/ricardobcl/Dotted-Version-Vectors">Dotted Version Vectors</a>.</p></blockquote><p>We could also decide to simply let the application decide how to deal with conflicts, by returning all conflicting values for the requested item. The application might know a lot more on the data than the database, so why not let it resolve conflicts? This is what <code>Riak KV</code> does for example.</p><blockquote><p>An idea I think about often is that you could even allow users to compile conflict resolution logic as a WASM module, and upload it to the database, so that when conflicts occur, the database resolves them, never relying on the application.</p></blockquote><p>There are lots of different ideas to reduce conflicts in an eventually consistent system, they usually fall under the umbrella term <code>Anti Entropy</code>.</p><h2 id="anti-entropy">Anti Entropy</h2><p>Here are examples of some of the most popular <code>Anti Entropy</code> mechanisms:</p><p><strong>Read Repair</strong> - After a client chooses the "latest" value from a read request that went to multiple nodes (by conflict resolution), it sends that value back to all the nodes that don't currently store that value, thus <em>repairing</em> them.</p><p><strong>Hinted Handoff</strong> - When a write request can't reach one of the target nodes, send it instead as a "hint" to some other node. As soon as that target node is available again, send it the saved "hint". On a quorum write, this mechanism is also called <code>Sloppy Quorum</code>, which provides even better <em>availability</em> for quorum requests.</p><p><strong>Merkle Trees</strong> - Because read repair only fixes queried data, a lot of data can still become inconsistent for a long time. Nodes can choose to start a synchronization process by talking to each other and see the differences in data. This is really expensive when there is a lot of data (<code>O(n)</code>). To make the sync algorithm faster (<code>O(log n)</code>) we can introduce <a href="https://en.wikipedia.org/wiki/Merkle_tree">merkle trees</a>. A merkle tree stores the hash of a range of the data in lowest leaf nodes, with the parent leaf nodes being a combined hash of the 2 of its children, thus creating a hierarchy of hashes up to the root of the tree. The sync process now starts by one node comparing the root of the merkle tree to another node's merkle tree, if the hashes are the same, it means they have exactly the same data. If the hashes differ, the leaf hashes are checked the same way, recursively until the inconsistent data is found.</p><p><strong>Gossip Dissemination</strong> - Send broadcast events to all nodes in the cluster in a simple and reliable way, by imitating how humans spread rumors or a disease. You send the event message to a configured number of randomly chosen nodes (called the "fanout"), then when they receive the message they repeat the process and send the message to another set of randomly chosen <code>N</code> nodes. To not repeat the message forever in the cluster, a node stops broadcasting a gossip message when it sees it a configured number of times. To get a feel for how data converges using gossip, head over to the <a href="https://www.serf.io/docs/internals/simulator.html">simulator</a>! As an optimization, gossip messages are usually sent using UDP, as the mechanism is just that reliable.</p><h2 id="conclusion">Conclusion</h2><p>There is a lot more to talk about databases, be it the use of <a href="https://yarchive.net/comp/linux/o_direct.html">O_DIRECT</a> in linux and implementing your own page cache, failure detection in distributed systems, consensus algorithms like <a href="https://raft.github.io/">raft</a>, distributed transactions, leader election, and an almost infinite amount more.</p><p>I hope I have piqued your curiosity enough to explore the world of databases further, or provided the tools for you to better understand which database to pick in your next project 😀</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scheme in Scheme on WASM in the browser (165 pts)]]></title>
            <link>https://spritely.institute/news/scheme-in-scheme-on-wasm-in-the-browser.html</link>
            <guid>38655047</guid>
            <pubDate>Fri, 15 Dec 2023 15:26:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spritely.institute/news/scheme-in-scheme-on-wasm-in-the-browser.html">https://spritely.institute/news/scheme-in-scheme-on-wasm-in-the-browser.html</a>, See on <a href="https://news.ycombinator.com/item?id=38655047">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://spritely.institute/static/images/blog/hoot-meta-repl.gif" alt="Hoot metacircular evaluator demo recording"></p><p>Hey, folks!  Today we want to talk about the wonderful
<a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">read-eval-print-loop
(REPL)</a>.
Thanks to WebAssembly (Wasm), it's becoming increasingly common for
programming language websites to embed a REPL in which passersby can
easily evaluate code and get a feel for the language without having to
install anything on their computer.  We'd like to do the same thing
for our language of choice, <a href="https://gnu.org/software/guile">Guile
Scheme</a>, using <a href="https://spritely.institute/hoot">Guile Hoot</a>.</p><p>Guile Hoot is a Scheme-to-Wasm compiler that leverages the <a href="https://developer.chrome.com/blog/wasmgc">Wasm
garbage collection (GC)
extension</a> that has been
rolling out to major browsers recently.  Wasm GC has finally made it
possible to use <a href="https://en.wikipedia.org/wiki/Dynamic_programming_language">dynamic
languages</a>
<em>besides JavaScript</em> on the client-side web, but it will still take
some additional effort to bring our favorite tools along for the ride.
In this post, we'll walk through building a tiny Scheme interpreter
with a simple user interface and explain what's in store for the
future.</p><p>To learn more about Hoot, check out the <a href="https://spritely.institute/news/guile-hoot-v020-released.html">0.2.0 release
announcement</a> from a couple of
weeks ago!</p><h3>The case of the missing REPL</h3><p>For Scheme programmers (and Lisp programmers in general), the REPL is
at the core of our development workflow.  We modify some code in our
text editor, evaluate it in the REPL, inspect the output or enter a
debugger, and repeat the process until the program behaves the way we
want.</p><p>We're used to having a native REPL process running directly on our
operating system, but since Hoot is putting Scheme on the web, we'd
really like a REPL available in the browser, too.  A web REPL would be
convenient for seasoned Schemers and newcomers alike.  However, at
this early stage in Hoot's development, a fully-featured Scheme REPL
is not yet possible.  We don't have access to the macro expander,
interpreter, etc. from the comfort of our Wasm runtime... yet.</p><p>We'll have these things eventually, but what can we have now?</p><p>Well, we could implement an interpreter for a small subset of
Scheme... in Scheme!  Sounds pretty recursive, and if you know
anything about Schemers you know we love recursion (and also, that we
love recursion), so let's give it a shot!</p><h3>Scheme in Scheme</h3><p>In Chapter 4 of the classic computer science textbook <a href="https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/6515/sicp.zip/full-text/book/book-Z-H-25.html#%_chap_4">Structure and
Interpretation of Computer
Programs</a>
(SICP), Gerald Sussman and Hal Abelson walk through building a small
Scheme interpreter embedded in Scheme.  They called this the
"metacircular evaluator", meaning that it's an interpreter written in
the language that it is interpreting.</p><p>This sounds pretty fancy — the kind of task reserved for MIT
professors that write iconic textbooks — but implementing a Scheme
interpreter is simpler than one might think!  For one thing, since
Scheme can manipulate its own syntax by design, we don't need to spend
any time parsing, a notoriously complex subject in its own right.  We
can skip directly to the fun part instead: evaluation!</p><p>Spritely's own <a href="https://spritely.institute/static/papers/scheme-primer.html">Scheme
Primer</a>
distills SICP's interpreter into a much simpler form intended for
people who are brand new to Scheme.  The code examples in the rest of
this post are derived from the "Scheme in Scheme" section of the
Primer.  If you're completely new to Scheme and want to better
understand the code in this post, consider working through our Primer
sometime!</p><p><img src="https://spritely.institute/static/images/blog/eval-apply.png" alt="eval/apply diagram from Andres Raba's unofficial SICP, licensedunder Creative Commons Attribution-ShareAlike 4.0 InternationalLicense"></p><p>The metacircular evaluator is composed of two main components: <code>eval</code>
and <code>apply</code>:</p><ul><li><code>eval</code> processes an expression in the context of an environment.</li><li><code>apply</code> calls a procedure (function) with a list of arguments.</li></ul><p>The output of <code>eval</code> is fed to <code>apply</code>, which may feed more input to
<code>eval</code>, and this mutually recursive cycle continues until the program
runs out of expressions to evaluate.</p><p>In order to implement <code>eval</code>, we first need a way to represent
environments.  An environment maps variable names to their bound
values.  We'll use <a href="https://en.wikipedia.org/wiki/Association_list">association
lists</a> for our
environments.</p><p>An empty environment with no variable bindings is represented by the
empty list <code>'()</code>.</p><p>The first bit of code from the metacircular evaluator we'll cover is
the <code>extend-env</code> procedure, which creates a new environment by
extending an existing one:</p><pre><code><span>(</span><span>define</span> <span>(</span><span>extend-env</span> <span>env</span> <span>names</span> <span>vals</span><span>)</span>
  <span>;; If there are no more variables to add to env, return env.
</span>  <span>(</span><span>if</span> <span>(</span><span>null?</span> <span>names</span><span>)</span>
      <span>env</span>
      <span>;; Otherwise, add the first binding to the recursive
</span>      <span>;; extension of env with the rest of the bindings.
</span>      <span>(</span><span>cons</span> <span>(</span><span>cons</span> <span>(</span><span>car</span> <span>names</span><span>)</span> <span>(</span><span>car</span> <span>vals</span><span>)</span><span>)</span>
            <span>(</span><span>extend-env</span> <span>env</span> <span>(</span><span>cdr</span> <span>names</span><span>)</span> <span>(</span><span>cdr</span> <span>vals</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>We can extend the empty environment to make a new environment that
includes the variable <code>foo</code>:</p><pre><code><span>(</span><span>extend-env</span> <span>'</span><span>(</span><span>)</span> <span>'</span><span>(</span><span>foo</span><span>)</span> <span>'</span><span>(</span><span>1</span><span>)</span><span>)</span> <span>; =&gt; ((foo . 1))</span></code></pre><p>We can further extend that environment to make a new environment that
includes the variable <code>bar</code>:</p><pre><code><span>(</span><span>extend-env</span> <span>'</span><span>(</span><span>(</span><span>foo</span> <span>.</span> <span>1</span><span>)</span><span>)</span> <span>'</span><span>(</span><span>bar</span><span>)</span> <span>'</span><span>(</span><span>2</span><span>)</span><span>)</span> <span>; =&gt; ((bar . 2) (foo . 1))</span></code></pre><p>Note that environment manipulation is implemented in a
<a href="https://en.wikipedia.org/wiki/Functional_programming">functional</a>
style.  Extending an environment does not mutate the original one.
This is because mutating the original environment would propagate the
new bindings to other evaluation contexts, potentially breaking
Scheme's <a href="https://en.wikipedia.org/wiki/Scope_(computer_science)">lexical scoping
rules</a>.</p><p>We can look up a variable's value using the <code>env-lookup</code> procedure:</p><pre><code><span>(</span><span>define</span> <span>(</span><span>env-lookup</span> <span>env</span> <span>name</span><span>)</span>
  <span>;; Lookup name in association list.
</span>  <span>(</span><span>match</span> <span>(</span><span>assq</span> <span>name</span> <span>env</span><span>)</span>
    <span>;; Success: return the bound value.
</span>    <span>(</span><span>(</span><span>_</span> <span>.</span> <span>val</span><span>)</span> <span>val</span><span>)</span>
    <span>;; Failure: throw an error.
</span>    <span>(</span><span>#f</span> <span>(</span><span>error</span> <span>"Variable unbound:"</span> <span>name</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>Example:</p><pre><code><span>(</span><span>env-lookup</span> <span>'</span><span>(</span><span>(</span><span>foo</span> <span>.</span> <span>1</span><span>)</span><span>)</span> <span>'foo</span><span>)</span> <span>; =&gt; 1</span></code></pre><p>And now for the rest of the owl.  Our toy <code>eval</code> supports:</p><ul><li>booleans</li><li>numbers</li><li>strings</li><li>quoted expressions</li><li>conditionals</li><li>procedures as values</li><li>procedure calls</li></ul><pre><code><span>(</span><span>define</span> <span>(</span><span>eval</span> <span>expr</span> <span>env</span><span>)</span>
  <span>(</span><span>match</span> <span>expr</span>
    <span>;; Booleans, numbers, strings
</span>    <span>(</span><span>(</span><span>or</span> <span>(</span><span>?</span> <span>boolean?</span><span>)</span> <span>(</span><span>?</span> <span>number?</span><span>)</span> <span>(</span><span>?</span> <span>string?</span><span>)</span><span>)</span>
     <span>expr</span><span>)</span>
    <span>;; Quoted expressions
</span>    <span>(</span><span>(</span><span>'quote</span> <span>quoted-expr</span><span>)</span>
     <span>quoted-expr</span><span>)</span>
    <span>;; Variable lookup
</span>    <span>(</span><span>(</span><span>?</span> <span>symbol?</span> <span>name</span><span>)</span>
     <span>(</span><span>env-lookup</span> <span>env</span> <span>name</span><span>)</span><span>)</span>
    <span>;; Conditionals
</span>    <span>(</span><span>(</span><span>'if</span> <span>test</span> <span>consequent</span> <span>alternate</span><span>)</span>
     <span>(</span><span>if</span> <span>(</span><span>eval</span> <span>test</span> <span>env</span><span>)</span>
         <span>(</span><span>eval</span> <span>consequent</span> <span>env</span><span>)</span>
         <span>(</span><span>eval</span> <span>alternate</span> <span>env</span><span>)</span><span>)</span><span>)</span>
    <span>;; Procedures
</span>    <span>(</span><span>(</span><span>'lambda</span> <span>args</span> <span>body</span><span>)</span>
     <span>(</span><span>lambda</span> <span>vals</span>
       <span>(</span><span>eval</span> <span>body</span> <span>(</span><span>extend-env</span> <span>env</span> <span>args</span> <span>vals</span><span>)</span><span>)</span><span>)</span><span>)</span>
    <span>;; Procedure application
</span>    <span>(</span><span>(</span><span>proc-expr</span> <span>.</span> <span>arg-exprs</span><span>)</span>
     <span>;; Recursively evaluate the procedure expression and all
</span>     <span>;; argument expressions, then apply the procedure with the
</span>     <span>;; arguments.
</span>     <span>(</span><span>apply</span> <span>(</span><span>eval</span> <span>proc-expr</span> <span>env</span><span>)</span>
            <span>;; Recursively evaluate the arguments.
</span>            <span>(</span><span>map</span> <span>(</span><span>lambda</span> <span>(</span><span>arg-expr</span><span>)</span>
                   <span>(</span><span>eval</span> <span>arg-expr</span> <span>env</span><span>)</span><span>)</span>
                 <span>arg-exprs</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>Fortunately, we don't need to implement <code>apply</code> ourselves.  Since
<code>lambda</code> forms in our interpreter evaluate to regular Scheme
procedures, we can just use Scheme's built-in <code>apply</code> to pass
arguments to them.  Calling a procedure will recursively call <code>eval</code>
on the procedure's body expression.</p><p>Example:</p><pre><code><span>(</span><span>eval</span> <span>'</span><span>(</span><span>+</span> <span>1</span> <span>2</span> <span>3</span><span>)</span> <span>`</span><span>(</span><span>(</span><span>+</span> <span>.</span> <span>,+</span><span>)</span><span>)</span><span>)</span> <span>; =&gt; 6</span></code></pre><p>Incidentally, we've created a
<a href="https://en.wikipedia.org/wiki/Capability-based_security">capability-secure</a>
system reminiscent of <a href="https://webassembly.github.io/spec/core/intro/introduction.html#security-considerations">Wasm's security
model</a>!
The program <code>(+ 1 2 3)</code> only has access to the <code>+</code> procedure from our
Scheme runtime.  Aside from creating unbounded loops, a devious
developer can't do any harm if their code is evaluated in this
restricted environment.  Neat!</p><h3>Scheme in Scheme on Wasm</h3><p>Now that we have a simple evaluator, we can compile it with Hoot to
run it inside a web browser.  But to actually use it, we need a user
interface.  To make one, we can borrow the React-like rendering code
we walked through in our previous tutorial on rendering web pages with
Wasm, <a href="https://spritely.institute/news/building-interactive-web-pages-with-guile-hoot.html">"Building interactive web pages with Guile
Hoot"</a>!</p><p>For simplicity, we'll go for a minimalist design reminiscent of a
terminal.  As for the REPL output log, we can store it as a list of
strings.  Let's pre-populate it with a friendly welcome message:</p><pre><code><span>(</span><span>define</span> <span>*log*</span>
  <span>'</span><span>(</span><span>"Welcome to the Hoot metacircular evaluator demo!

This is a miniature Scheme interpreter written in Scheme that's been
compiled to WebAssembly, and is running directly in the browser!  Its
UI is also written in Scheme, and uses the Hoot FFI to render itself
to the DOM.

"</span><span>)</span><span>)</span>

<span>(</span><span>define</span> <span>(</span><span>log-append!</span> <span>.</span> <span>lines</span><span>)</span>
  <span>(</span><span>set!</span> <span>*log*</span> <span>(</span><span>append</span> <span>*log*</span> <span>lines</span><span>)</span><span>)</span><span>)</span></code></pre><p>Here's the rendering template, which prints all the log lines, and
appends the classic <code>&gt;</code> prompt and a <code>textarea</code> to the bottom so the
user can write some Scheme code:</p><pre><code><span>(</span><span>define</span> <span>prompt</span> <span>"&gt; "</span><span>)</span>

<span>(</span><span>define</span> <span>(</span><span>render</span><span>)</span>
  <span>`</span><span>(</span><span>div</span> <span>(</span><span>@</span> <span>(</span><span>class</span> <span>"container"</span><span>)</span><span>)</span>
        <span>(</span><span>h1</span> <span>"Hoot REPL"</span><span>)</span>
        <span>(</span><span>div</span> <span>(</span><span>@</span> <span>(</span><span>id</span> <span>"repl"</span><span>)</span>
                <span>(</span><span>class</span> <span>"repl repl-text"</span><span>)</span><span>)</span>
             <span>(</span><span>div</span> <span>(</span><span>@</span> <span>(</span><span>class</span> <span>"log"</span><span>)</span><span>)</span> <span>,@*log*</span><span>)</span>
             <span>(</span><span>div</span> <span>(</span><span>@</span> <span>(</span><span>class</span> <span>"prompt"</span><span>)</span><span>)</span>
                  <span>,prompt</span>
                  <span>(</span><span>textarea</span> <span>(</span><span>@</span> <span>(</span><span>id</span> <span>"expression"</span><span>)</span>
                               <span>(</span><span>class</span> <span>"repl-text"</span><span>)</span>
                               <span>(</span><span>placeholder</span> <span>"(+ 1 2 3)"</span><span>)</span>
                               <span>(</span><span>keyup</span> <span>,maybe-eval</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>Every time the user presses a key while focused on the <code>textarea</code>, we
call the <code>maybe-eval</code> procedure shown below.  When the user presses
the Enter key, the REPL</p><ul><li>evaluates their code</li><li>clears the <code>textarea</code></li><li>appends any new output</li><li>scrolls to the bottom of the log to keep the terminal screen up to
date</li></ul><pre><code><span>(</span><span>define</span> <span>(</span><span>maybe-eval</span> <span>event</span><span>)</span>
  <span>;; Get the event's key.
</span>  <span>(</span><span>let</span> <span>(</span><span>(</span><span>key</span> <span>(</span><span>keyboard-event-key</span> <span>event</span><span>)</span><span>)</span><span>)</span>
    <span>;; Evaluate user code when Enter is pressed, but not when
</span>    <span>;; Shift is being held so the user can edit across multiple
</span>    <span>;; lines.
</span>    <span>(</span><span>when</span> <span>(</span><span>and</span> <span>(</span><span>string-=?</span> <span>key</span> <span>"Enter"</span><span>)</span>
               <span>(</span><span>not</span> <span>(</span><span>keyboard-event-shift?</span> <span>event</span><span>)</span><span>)</span><span>)</span>
      <span>;; Get the text within the expression textarea.
</span>      <span>(</span><span>let*</span> <span>(</span><span>(</span><span>input</span> <span>(</span><span>get-element-by-id</span> <span>"expression"</span><span>)</span><span>)</span>
             <span>(</span><span>exp</span> <span>(</span><span>element-value</span> <span>input</span><span>)</span><span>)</span><span>)</span>
        <span>;; If the textarea is empty, do nothing.
</span>        <span>(</span><span>unless</span> <span>(</span><span>string-=?</span> <span>exp</span> <span>""</span><span>)</span>
          <span>;; Clear the textarea.
</span>          <span>(</span><span>set-element-value!</span> <span>input</span> <span>""</span><span>)</span>
          <span>;; Evaluate and append output to log.
</span>          <span>(</span><span>eval!</span> <span>exp</span><span>)</span>
          <span>;; Update UI.
</span>          <span>(</span><span>refresh!</span><span>)</span>
          <span>;; Scroll the log to show the next output.
</span>          <span>(</span><span>scroll-to-bottom!</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>At the bottom, you'll notice <code>maybe-eval</code> uses a new procedure we've
called <code>eval!</code> (how exciting).  <code>eval!</code> is <code>eval</code> + <code>read</code> — it parses
the user's text using Scheme's built-in <code>read</code> procedure, calls
<code>eval</code>, and prints the output to the log:</p><pre><code><span>(</span><span>define</span> <span>(</span><span>eval!</span> <span>str</span><span>)</span>
  <span>;; Parse user input.
</span>  <span>(</span><span>let</span> <span>(</span><span>(</span><span>exp</span> <span>(</span><span>read</span> <span>(</span><span>open-input-string</span> <span>str</span><span>)</span><span>)</span><span>)</span>
        <span>;; Open output port.
</span>        <span>(</span><span>output</span> <span>(</span><span>open-output-string</span><span>)</span><span>)</span><span>)</span>
    <span>;; Redirect all output to our output port.
</span>    <span>(</span><span>parameterize</span> <span>(</span><span>(</span><span>current-output-port</span> <span>output</span><span>)</span><span>)</span>
      <span>;; Echo the prompt and user code.
</span>      <span>(</span><span>display</span> <span>prompt</span><span>)</span>
      <span>(</span><span>display</span> <span>str</span><span>)</span>
      <span>;; Invoke the interpreter.
</span>      <span>(</span><span>call-with-values</span> <span>(</span><span>lambda</span> <span>(</span><span>)</span> <span>(</span><span>eval</span> <span>exp</span> <span>init-env</span><span>)</span><span>)</span>
        <span>;; Display each returned value on its own line.
</span>        <span>(</span><span>lambda</span> <span>vals</span>
          <span>(</span><span>if</span> <span>(</span><span>null?</span> <span>vals</span><span>)</span>
              <span>(</span><span>display</span> <span>"\n"</span><span>)</span>
              <span>(</span><span>for-each</span> <span>(</span><span>lambda</span> <span>(</span><span>val</span><span>)</span>
                          <span>(</span><span>unless</span> <span>(</span><span>unspecified?</span> <span>val</span><span>)</span>
                            <span>(</span><span>display</span> <span>"=&gt; "</span><span>)</span>
                            <span>(</span><span>write</span> <span>val</span><span>)</span><span>)</span>
                          <span>(</span><span>newline</span><span>)</span><span>)</span>
                        <span>vals</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
    <span>;; Append output to log.
</span>    <span>(</span><span>log-append!</span> <span>(</span><span>get-output-string</span> <span>output</span><span>)</span><span>)</span><span>)</span><span>)</span></code></pre><p>(Note how, thanks to <code>read</code>, we've elided the messy work of parsing.)</p><p>Expressions are evaluated within the context of the following
environment that provides capabilities for basic arithmetic, lists,
multiple value return, and printing:</p><pre><code><span>(</span><span>define</span> <span>init-env</span>
  <span>`</span><span>(</span><span>(</span><span>+</span> <span>.</span> <span>,+</span><span>)</span>
    <span>(</span><span>-</span> <span>.</span> <span>,-</span><span>)</span>
    <span>(</span><span>*</span> <span>.</span> <span>,*</span><span>)</span>
    <span>(</span><span>/</span> <span>.</span> <span>,/</span><span>)</span>
    <span>(</span><span>=</span> <span>.</span> <span>,=</span><span>)</span>
    <span>(</span><span>cons</span> <span>.</span> <span>,cons</span><span>)</span>
    <span>(</span><span>car</span> <span>.</span> <span>,car</span><span>)</span>
    <span>(</span><span>cdr</span> <span>.</span> <span>,cdr</span><span>)</span>
    <span>(</span><span>list</span> <span>.</span> <span>,list</span><span>)</span>
    <span>(</span><span>pair?</span> <span>.</span> <span>,pair?</span><span>)</span>
    <span>(</span><span>null?</span> <span>.</span> <span>,null?</span><span>)</span>
    <span>(</span><span>values</span> <span>.</span> <span>,values</span><span>)</span>
    <span>(</span><span>display</span> <span>.</span> <span>,display</span><span>)</span>
    <span>(</span><span>newline</span> <span>.</span> <span>,newline</span><span>)</span><span>)</span><span>)</span></code></pre><p>The initial environment describes the primitive functionality of our
interpreter.  Primitives are simple things that the programmer can
take for granted.  This environment doesn't provide much, but it's
enough to have a bit of fun.</p><p>And...</p><p>🥁</p><p>🥁</p><p>🥁</p><p>...here's the finished program!</p><p>Some expressions you could try:</p><p>Greet the world:</p><pre><code><span>(</span><span>display</span> <span>"Hello, world!"</span><span>)</span></code></pre><p>Make a list of the veggies you want on your sandwich:</p><pre><code><span>'</span><span>(</span><span>lettuce</span> <span>tomato</span> <span>onion</span> <span>pepper</span> <span>pickles</span><span>)</span></code></pre><p>Display all your favorite pets as multiple return values:</p><pre><code><span>(</span><span>values</span> <span>'cat</span> <span>'dog</span> <span>'chicken</span><span>)</span></code></pre><p>Apply a procedure that squares numbers:</p><pre><code><span>(</span><span>(</span><span>lambda</span> <span>(</span><span>x</span><span>)</span> <span>(</span><span>*</span> <span>x</span> <span>x</span><span>)</span><span>)</span> <span>4</span><span>)</span></code></pre><p>Recursively apply a procedure to compute the nth Fibonacci number:</p><pre><code><span>(</span><span>(</span><span>lambda</span> <span>(</span><span>f</span> <span>x</span><span>)</span> <span>(</span><span>f</span> <span>f</span> <span>x</span><span>)</span><span>)</span>
 <span>(</span><span>lambda</span> <span>(</span><span>fib</span> <span>n</span><span>)</span>
   <span>(</span><span>if</span> <span>(</span><span>=</span> <span>n</span> <span>0</span><span>)</span>
       <span>0</span>
       <span>(</span><span>if</span> <span>(</span><span>=</span> <span>n</span> <span>1</span><span>)</span>
           <span>1</span>
           <span>(</span><span>+</span> <span>(</span><span>fib</span> <span>fib</span> <span>(</span><span>-</span> <span>n</span> <span>1</span><span>)</span><span>)</span>
              <span>(</span><span>fib</span> <span>fib</span> <span>(</span><span>-</span> <span>n</span> <span>2</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span><span>)</span>
 <span>10</span><span>)</span></code></pre><p>Whoa, that last one was kinda wild, huh?  That's because our little
interpreter lacks even the ability to bind variables outside of
<code>lambda</code>.  There is no <code>let</code> nor <code>define</code>, so recursive code gets
weird fast!  Talk about minimalism...</p><h3>Scheming ahead</h3><p>The interpreter we've shown here is just a toy, but in the coming
months we expect to add support for the <a href="https://small.r7rs.org/attachment/r7rs.pdf">R7RS-small
standard</a>'s <code>eval</code>, taking
our REPL from toy to full Scheme interpreter.</p><p>Once we do so, we could embed Scheme REPLs directly into documents
like the <em>Scheme Primer</em>, further reducing the activation energy
required to get started with Scheme.  And who knows what exciting
integrations our community will come up with?</p><p>In the meantime, if you'd like to start immediately hacking on the
REPL demo, you can find the complete source code on
<a href="https://gitlab.com/spritely/guile-hoot-meta-repl">GitLab</a>.  If you're
into homework assignments, try adding support for more Scheme syntax
such as <code>let</code>, or adding more procedures to the environment to grant
more power to the interpreter.</p><p>And be sure to show off what you build with Hoot in our <a href="https://community.spritely.institute/">community
forum</a>!  (Use OCAPN2023 for the
invite code when you join.)</p><p>Happy hooting! 🦉</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cognitive distortions that undermine clear thinking (103 pts)]]></title>
            <link>https://www.leadingsapiens.com/cognitive-distortions-leaders/</link>
            <guid>38655010</guid>
            <pubDate>Fri, 15 Dec 2023 15:22:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.leadingsapiens.com/cognitive-distortions-leaders/">https://www.leadingsapiens.com/cognitive-distortions-leaders/</a>, See on <a href="https://news.ycombinator.com/item?id=38655010">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <div>

      <p><em>Leaders are paid to think clearly. And a big impediment to clear thinking is what CBT/REBT calls <strong>Cognitive Distortions: errors in thinking or thinking traps we commonly fall for</strong>. Most high-performers are not pathological, but instead fall for them when upset or in high-stress situations.</em></p><p><em> In this post, I share <strong>17 of the most common cognitive distortions</strong>. I highlight the <strong>use-cases of leaders and job-hunters</strong>, what causes them, and strategies for countering them.</em></p><hr><h2 id="what-are-cognitive-distortions">What are cognitive distortions?</h2><p>Also known as thinking errors or thought traps, cognitive distortions are defined as "<em>errors of processing in which the person cognitively focuses on insufficient or inappropriate data and draws illogical conclusions, makes inaccurate inferences or bases predicted outcomes upon little or no empirical evidence</em>." [3]</p><p>Put simply, these are habitual but unhelpful ways of thinking. Why are they important? </p><p>Because the way we feel, and how well we operate, are directly dependent on the quality of our thinking. It's not external events or others that make us feel a certain way, but instead our thoughts about them. But this process is so fast and automatic that <strong>we forget our own role in creating these problems</strong>.  </p><blockquote>Your emotions result entirely from the way you look at things. It is an obvious neurological fact that before you can experience any event, you must process it with your mind and give it meaning. You must understand what is happening to you before you can feel it. <br>...<br>It is not the actual events but your perceptions that result in changes in mood. When you are sad, your thoughts will represent a realistic interpretation of negative events. When you are depressed or anxious, your thoughts will always be illogical, distorted, unrealistic, or just plain wrong.<p>— David Burns [1]</p></blockquote><h3 id="are-cognitive-distortions-applicable-to-leadership-coaching"><strong>Are cognitive distortions applicable to leadership coaching</strong></h3><p>In leadership, staying alert to cognitive distortions is important from two key perspectives: the leader's performance, and more importantly, how it affects others.</p><p>Elite athletes have to train constantly to maintain peak physical and mental conditioning. Their training routines focus on maintaining correct form. The equivalent of good form in leadership is clear thinking. Most high-functioning folks are not necessarily pathological, but nevertheless fall for these subtle but hidden patterns. </p><p>Additionally, leadership behavior has ripple-effects throughout the organization. <a href="https://www.leadingsapiens.com/why-people-copy-leaders-complexity-view/" rel="noreferrer">People not only copy leaders</a>, but also seek answers from them, both direct and implied. Whether by design or accident, <a href="https://www.leadingsapiens.com/mastery-of-context-in-leadership/" rel="noreferrer">leaders are always setting the context</a> for their teams. It's why they simply cannot afford to get derailed by thought traps.</p><div><p>💡</p><p><b><strong>A caveat: </strong></b>When going through this list and the examples, it's easy to dismiss them as simplistic and extreme. And they mostly are. However, most of us fall for them in subtle ways that often go undetected. </p></div><h2 id="common-cognitive-distortions">Common cognitive distortions</h2><p>Just as exercise improves physical strength, there are practices to improve mental conditioning — aka clear thinking. One such practice is to learn, identify, and interrupt common but unhelpful thinking patterns that we fall for.</p><p>Coaching borrows from a range of disciplines including cognitive behavioral methods. What follows are the most common cognitive distortions that CBT/REBT recognizes.</p><p>Each cognitive distortion is followed by examples from the perspective of a job-hunter and a leader. Why? Thinking errors are most likely when operating in high-stress situations which are rich territory for thought traps.</p><figure><img src="https://www.leadingsapiens.com/content/images/2023/12/Cognitive-Distortions--lr-2.png" alt="17 Cognitive Distortions from CBT and REBT" loading="lazy" width="1080" height="1950" srcset="https://www.leadingsapiens.com/content/images/size/w600/2023/12/Cognitive-Distortions--lr-2.png 600w, https://www.leadingsapiens.com/content/images/size/w1000/2023/12/Cognitive-Distortions--lr-2.png 1000w, https://www.leadingsapiens.com/content/images/2023/12/Cognitive-Distortions--lr-2.png 1080w" sizes="(min-width: 720px) 720px"></figure><h3 id="1-all-or-nothing-thinking">(1) All-or-Nothing Thinking</h3><p><strong>Definition</strong><br>We see the world in dichotomies — it's either black or white with no shades of grey in between. Anything short of perfect is a failure. Characterized by thinking in polarities, extremes, and absolutes. It's either excellent or awful, and forms the basis of perfectionism.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"If my team doesn't achieve 100% of the goals, it's a complete disaster."</em><br>⦿ Job-Hunter:&nbsp;<em>"If I don't get this job, I'll never find a good career opportunity."</em></p><p><strong>How to counter</strong><br>Cultivate multi-category thinking and a richer understanding to see the shades of grey instead of falling for simplistic explanations. Learn to discern between facts vs hunches or interpretations. Resolve to examine your negative interpretations by testing them out. </p><h3 id="2-overgeneralization">(2) Overgeneralization</h3><p><strong>Definition</strong><br>Drawing broad conclusions based on isolated negative events. Characterized by drawing global beliefs and never-ending patterns from a single situation.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"My proposal got rejected ; I'm terrible at pitching ideas."</em><br>⦿ Job-Hunter:&nbsp;<em>"I failed this interview; I'll never get hired anywhere."</em></p><p><strong>How to counter</strong><br>Avoid extrapolation of both good and bad. And when doing so, check if the data actually supports your stance.</p><h3 id="3-mental-filtering">(3) Mental filtering</h3><p><strong>Definition</strong><br>Picking out and focusing exclusively on a single negative aspect while ignoring any positives.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"Despite other successful projects, I can't stop thinking about that one recent failure."&nbsp;</em><br>⦿ Job-Hunter:&nbsp;<em>"I had an excellent interview, but I stumbled on one question, and now I feel like I ruined it all."</em></p><p><strong>How to counter</strong><br>Recognize the complexity of situations and acknowledge positive aspects as well that balance out the negative.</p><h3 id="4-disqualifying-the-positive">(4) Disqualifying the positive</h3><p><strong>Definition</strong><br>Rejecting positive experiences as if they don't count for one reason or another. A simple example is dismissing compliments.<strong> </strong>Success is seen as a fluke. This make it difficult to build on anything because everything is happenstance.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"The team's success was due to external factors, not my leadership."</em><br>⦿ Job-Hunter:&nbsp;<em>"The interviewer praised my skills, but I think they were just being polite."</em></p><p><strong>How to counter</strong><br>Learn to accept and acknowledge positive feedback and explanations, in addition to negative and neutral ones.</p><h3 id="5-jumping-to-conclusions">(5) Jumping to Conclusions</h3><p><strong>Definition</strong><br>Assuming negative outcomes, or negative interpretation without facts or supporting evidence. This is characterized by certainty in interpreting situations despite little evidence.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"He's late today; he must not care about work."</em><br>⦿ Job-Hunter:&nbsp;<em>"The interviewer looked uninterested; they probably think I'm unqualified."</em></p><p><strong>How to counter</strong><br>Notice your inferences and how far removed they might be from observable facts. Learn to <a href="https://www.leadingsapiens.com/ground-rules-effective-meetings-groups/#1-test-assumptions-inferences-and-attributions" rel="noreferrer">test your inferences and hunches</a>.</p><h3 id="6-magnification-catastrophizing-or-minimization">(6) Magnification (Catastrophizing) Or Minimization</h3><p><strong>Definition</strong><br>Exaggerating or downplaying the importance of events. Also known as <strong>the&nbsp;"binocular trick"</strong>.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"Missing this deadline will ruin my career."</em><br>⦿ Job-Hunter:<em>"If I don't get this job, I'll never find any job, and my life will be ruined."</em></p><p><strong>How to counter</strong><br>Use a balanced perspective instead of a skewed one that catastrophes or  minimizes. How would an outside observer view your situation? What are your strengths?</p><h3 id="7-emotional-reasoning">(7) Emotional Reasoning</h3><p><strong>Definition</strong><br>Believing feelings and negative emotions accurately define or reflect reality. The logic goes: "I feel it, so it must be true."</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I feel like a failure, so I must be incompetent."</em><br>⦿ Job-Hunter:&nbsp;<em>"I'm nervous about the interview, which means I'll perform poorly."</em></p><p><strong>How to counter</strong><br>Get separation between facts and how you feel about something. Discern between emotionally laden assessments vs. factual takes. </p><h3 id="8-should-statements">(8) Should Statements</h3><p><strong>Definition</strong><br>Trying to motivate yourself by holding rigid or unrealistic expectations on oneself or others. Characterized by should(s), shouldn't(s), musts, oughts and "have to's",  as if one has to be punished and whipped before correct behavior. Albert Ellis (REBT founder) called this&nbsp;MUSTerbation.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I should always have the solution to every problem."</em><br>⦿ Job-Hunter:&nbsp;<em>"Employers should always provide feedback after interviews."</em></p><p><strong>How to counter</strong><br>Stay alert to your use of language and how that influences your interpretations, choice of actions, and behavior.</p><h3 id="9-labeling-and-mislabeling">(9) Labeling and Mislabeling</h3><p><strong>Definition</strong><br>This is an extreme form of overgeneralization, where we assign global, negative labels to oneself or others. Instead of describing an error, we attach negative labels: "I'm a loser", or: "what a louse". It's often characterized by highly colored and emotionally laden language. Over time, this can also lead to a <strong>lack of separation between the label and </strong><a href="https://www.leadingsapiens.com/leadership-identity-development-choices-actions/#identity-as-an-effect-rather-than-a-cause" rel="noreferrer"><strong>identity</strong></a>. Eg. "I'm a nervous nelly". </p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I'm a total failure as a manager."&nbsp;</em><br>⦿ Job-Hunter:&nbsp;<em>"I'm a slouch when it comes to interviews."</em></p><p><strong>How to counter</strong><br>Avoid using labels or rating yourself/others globally. Almost no-one is a "total idiot" or a "complete failure". Question the validity of your labels and <a href="https://www.leadingsapiens.com/effective-constructive-feedback/#principle-6-sharing-emotions-owning-opinions-and-avoiding-generalities" rel="noreferrer">separate the behavior from the person</a>.</p><h3 id="10-personalization">(10) Personalization</h3><p><strong>Definition</strong><br>Also known as self-blame: you assume undue responsibility for negative external events for which you were not responsible. This is an easy trap for conscientious leaders – <strong>they've confused influence with control</strong>. </p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"The project's failure is entirely my fault."&nbsp;</em><br>⦿ Job-Hunter:&nbsp;<em>"I didn't get the job; I must have said something wrong during the interview."</em></p><p><strong>How to counter</strong><br>Try to understand the whole picture instead of one particular aspect that you might have contributed to. Look for alternative explanations that might highlight <a href="https://www.leadingsapiens.com/linear-causality-vs-circular-causality-in-decision-making/" rel="noreferrer">different causal factors</a>.</p><h3 id="11-mind-reading">(11) Mind-reading</h3><p><strong>Definition</strong><br>Believing you know what others are thinking about you without evidence. It also means assuming that others know what you are thinking. It's characterized by a lack of direct communication (they should know), and expecting others to read your body language.</p><p><strong>Examples</strong><br>⦿ Leader:<em>&nbsp;"My team thinks I'm incompetent."&nbsp;</em><br>⦿ Job-Hunter:&nbsp;<em>"The interviewer didn't like my answer."</em></p><p><strong>How to counter</strong><br>Check your interpretations of others' reactions instead of assuming it's obvious. Look for alternative explanations that are more generous.  </p><h3 id="12-fortune-teller-error">(12) Fortune-teller error</h3><p><strong>Definition</strong><br>Predicting future events, usually negative, without evidence. And acting as if your prediction is a fact. Often characterized by anticipation of bad events and assuming outcomes as foregone conclusions. There's a false sense of hopelessness and defeatism.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I know the client will reject our proposal." </em><br>⦿ Job-Hunter:&nbsp;<em>"I'm certain they won't hire me, so why apply."</em></p><p><strong>How to counter</strong><br>Discern between what's in your control and what's not. Avoid making predictions that are not actually in your control. <strong>Let the decision-makers do their job while you do yours</strong>.</p><h3 id="13-alwaysnever-thinking">(13) Always/Never Thinking</h3><p><strong>Definition</strong><br>Believing that negative events will always happen, or that good events will never happen.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"My team always makes mistakes." </em><br>⦿ Job-Hunter:<em>&nbsp;"I'll never get hired."</em></p><p><strong>How to counter</strong><br>Take a balanced view of the past, present, and future. Recognize that while bad events are always possible, they're not inevitable and can be mitigated. Meanwhile, positive events are equally likely.</p><h3 id="14-entitlement">(14) Entitlement</h3><p><strong>Definition</strong><br>Believing one deserves special treatment or privileges, and characterized by unrealistic expectations and demanding behavior based on status.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I should be promoted without having to prove myself."</em><br>⦿ Job-Hunter:<em>&nbsp;"The employer should hire me because of my qualifications."</em></p><p><strong>How to counter</strong><br>Watch out for status expectations and wanting to skirt effort in getting to your goals.</p><h3 id="15-outsourcing-happiness">(15) <strong>Outsourcing Happiness</strong></h3><p><strong>Definition</strong><br>Emotional dependence and reliance on external circumstances for happiness instead of internal control. Characterized by the logic: "I'll be happy when/if..."</p><p><strong>Examples</strong><br>⦿ Leader:<em>&nbsp;"I'll be happy when my team performs well."</em><br>⦿ Job-Hunter:&nbsp;<em>"Getting this job will make me happy."</em></p><p><strong>How to counter</strong><br>Watch out for when you make your happiness contingent on something outside your control.</p><h3 id="16-control-fallacy">(16) <strong>Control Fallacy</strong></h3><p><strong>Definition</strong><br>Believing external forces have complete control over one's life (helplessness). Or the opposite of everything being in our control.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"I'm powerless against the company's decisions."</em><br>⦿ Job-Hunter:&nbsp;<em>"The job market controls my job prospects."</em></p><p><strong>How to counter</strong><br>Most of the time, <a href="https://www.leadingsapiens.com/ask-before-setting-goals/#the-nature-of-choice" rel="noreferrer">we have more choice than we realize</a>. But often this choice is hidden behind consequences that we might not be ok with.</p><h3 id="17-fairness-fallacy">(17) <strong>Fairness Fallacy</strong></h3><p><strong>Definition</strong><br>Believing that life should always be fair and just. Characterized by resentment and frustration.</p><p><strong>Examples</strong><br>⦿ Leader:&nbsp;<em>"It's not fair that my colleague got promoted, and I didn't."</em><br>⦿ Job-Hunter:&nbsp;<em>"I deserved that job more than the other candidates."</em></p><p><strong>How to counter</strong><br>Culture programs us to expect fairness, but the world is far from being fair, let alone predictable. Don't allow expectations of fairness to scuttle your level of commitment and effort.</p>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<h2 id="what-causes-cognitive-distortions"><strong>What causes Cognitive Distortions?</strong></h2><p>While recognizing and countering cognitive distortions is useful, it's also valuable to go further upstream and understand what causes them to begin with. REBT has two constructs that explain what causes distortions: levels of cognitions and irrational beliefs. </p><h3 id="a-levels-of-cognitions">(a) Levels of cognitions</h3><p>REBT postulates that our disturbed mental states and unhelpful emotions follow from thinking that happens at three levels [6].</p><ol><li><strong>First level</strong>: perceptions, automatic thoughts, and negative attributions (inferential)<ol><li>This is the level at which cognitive distortions occur. It's what we're aware of and that forms our stream of consciousness. These thoughts are inferential in nature. Eg. "He didn't like my work"/"I fail at everything"/"I am stupid". Because they're inferences, they can be tested. </li></ol></li><li><strong>Second level</strong>: awfulizing, global evaluations, and frustration intolerance (evaluative), that are derived from the third level.<ol><li>They evaluate the importance of the inferences: the badness of the inference(awfulizing), evaluation of your ability to tolerate the situation (frustration intolerance), or worth of person involved (global evaluations).</li></ol></li><li><strong>Third level</strong>: the core irrational belief of "demandingness" from which everything else follows. (imperative/schematic)<ol><li>Also known as "imperative demands" , these are thoughts about the way reality "should" be. They are core beliefs that follow the construct of " I must", "you must", or "the world must". Albert Ellis calls this <strong>MUSTerbation — expectations about the way the world is, ought to be, and what is good or bad about what is/ought to be</strong>. Words like "should", "ought", "must" and "have to" represent demandingness.</li></ol></li></ol><h3 id="b-types-of-irrational-beliefs">(b) Types of irrational beliefs</h3><p>REBT recognizes 5 basic types of irrational beliefs that underlie cognitive distortions: <em>demandingness, awfulizing, frustration intolerance, self-worth ratings, and other-worth ratings</em>. </p><blockquote><em><strong>Demandingness</strong> is an unrealistic and absolute expectation of events or individuals being the way a person desires them to be.&nbsp;<p><strong>Awfulizing</strong> is an exaggeration of the negative consequences of a situation to an extreme degree, so that an unfortunate occurrence becomes “terrible".</p><p><strong>Frustration Intolerance (FI)</strong> stems from demands for ease and comfort, and reflects an intolerance of discomfort.&nbsp;</p><p><strong>Global evaluations of human worth</strong>, either of the self or others, imply that human beings can be rated, and that some people are worthless, or at least less valuable than others.&nbsp;</p><p>[6]</p></em></blockquote><p>The most common situations where we use these irrational belief processes happen in these contexts: <strong><em>social relationships, achievement, comfort, and fairness</em></strong>.</p><h2 id="how-to-use-this-list-of-cognitive-distortions">How to use this list of Cognitive Distortions</h2><p>The key is to understand that thought patterns are akin to muscle movements.</p><p>Habitual ways of thinking have an evolutionary purpose — they help our brains use less energy, draw conclusions faster, and move on with our lives. We get used to a certain way of doing (in this case thinking), and the only way to change/improve is to do it differently, and getting the new “movement” into muscle memory. In this case, your neural pathways.</p><p>But to do that, you have to first recognize them. We cannot improve something we don’t notice. Identifying the usual suspects beforehand helps raise our level of awareness, which in turn helps in disrupting habitual patterns.</p><p>⦿ Familiarize yourself with the common ones.<br>⦿ Identify 2-3 patterns you regularly fall for.<br>⦿ Target those in the coming weeks.<br>⦿ Practice identifying them.<br>⦿ Replace those recurring patterns with counter narratives.<br>⦿ Practice just as you would practice scales in music, or weights for building muscle. </p><p>Reading about them raises awareness, but changes are not sustained without practice.</p><p>In addition to the specific countering of cognitive distortions, below are some general approaches you can use to improve your thinking and avoid these thought traps.</p><h3 id="thought-record-journals">Thought record journals</h3><p>Writing down your thoughts and identifying what distortion you might be falling for is super helpful. Done consistently, this practice in itself can literally rewire your neural system.</p><h3 id="self-compassion">Self-Compassion</h3><p>If you were advising your friend in a similar situation, what might you tell them? We're often much harsher on ourselves than with others.</p><h3 id="relative-thinking">Relative thinking</h3><p>Learn to recognize the complexity of problems and situations and how your take might be too simplistic. Extreme interpretations require extreme simplification.</p><h3 id="scientific-thinking">Scientific thinking</h3><p>Always be searching for evidence. Test out your interpretations and get feedback from multiple sources. A useful tool for seeking critical feedback and identifying potential blindspots is <a href="https://www.leadingsapiens.com/johari-window-complete-guide-for-leaders/" rel="noreferrer">the Johari Window</a>. </p><h3 id="watch-out-for-emotive-language">Watch out for emotive language</h3><p>Be extra alert to “musts”, “should”, “oughts”, “have to’s”, “it’s awful”, “I can’t stand it”, “never”, and so on.</p><h3 id="identify-your-level-of-thinking">Identify your level of thinking</h3><p>Follow the chain of inferences down the levels of thinking and identify what irrational belief  might be driving your distortion. Another useful tool is <a href="https://www.leadingsapiens.com/ladder-of-inference-decision-making/" rel="noreferrer">the ladder of inference</a>.</p><h3 id="cost-benefit-analysis">Cost-benefit analysis</h3><p>Is your particular approach or way of thinking helpful or harmful? Are there alternate explanations that might be more beneficial?</p><hr><div><p>💡</p><div><p>Liked this article? <b><strong>Try</strong></b> <b><strong>my free newsletter</strong></b>. Every edition covers essential frameworks on <b><strong>leadership, careers, and organizations </strong></b>in bite-sized form.</p><p>📚 <b><strong>HBR 100 Best Reads:</strong></b> You also get a curated spreadsheet of the best articles Harvard Business Review has ever published. Spans 70 years, comes complete with categories and short summaries.</p></div></div>
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<hr><h2 id="further-reading">Further reading</h2><p>Below are some related articles that build and add to the concepts discussed above.</p><ul><li>Instead of learning new mental models, it's often more productive to identify and dismantle <a href="https://www.leadingsapiens.com/mental-models-subtractive-approach/" rel="noreferrer">hidden but unhelpful mental models</a>:</li></ul><figure><a href="https://www.leadingsapiens.com/mental-models-subtractive-approach/"><div><p>Mental Models - A Subtractive Approach</p><p>Most common discourse on mental models takes an additive approach. But this tends to be half-baked, often useless in practice. There is equal value, even more so, in a subtractive approach to mental models. The key is in understanding the difference between hard and soft mental models. How is the</p><p><img src="https://www.leadingsapiens.com/content/images/size/w256h256/2022/05/LS-logo.png" alt=""><span>Leading Sapiens</span><span>Sheril Mathews</span></p></div><p><img src="https://www.leadingsapiens.com/content/images/2023/02/Mental-Models---Hard-vs-Soft-2.png" alt=""></p></a></figure><ul><li><a href="https://www.leadingsapiens.com/double-loop-learning-leadership-performance/" rel="noreferrer">Double-loop learning: a simple model of performance</a> and what lies upstream</li></ul><figure><a href="https://www.leadingsapiens.com/double-loop-learning-leadership-performance/"><div><p>How Double-Loop Learning Improves Performance</p><p>Our actions, and by extension performance, stem from thinking that is based on a set of hidden mental models. How do you uncover these mental models and change them? One way is to understand and practice the concepts of single-loop and double-loop learning. Professional sports teams use postgame films and</p><p><img src="https://www.leadingsapiens.com/content/images/size/w256h256/2022/05/LS-logo.png" alt=""><span>Leading Sapiens</span><span>Sheril Mathews</span></p></div><p><img src="https://www.leadingsapiens.com/content/images/size/w1200/2023/03/Double-loop-learning---Argyris-Schon-1.png" alt=""></p></a></figure><ul><li>The <a href="https://www.leadingsapiens.com/ladder-of-inference-decision-making/" rel="noreferrer">ladder of inference</a> is a useful framework that shows how we go from facts to inferences to actions:</li></ul><figure><a href="https://www.leadingsapiens.com/ladder-of-inference-decision-making/"><div><p>Using the ladder of inference to make better decisions</p><p>The ladder of inference is a powerful tool to make better decisions by uncovering hidden mental models and understanding how we reach conclusions.</p><p><img src="https://www.leadingsapiens.com/content/images/size/w256h256/2022/05/LS-logo.png" alt=""><span>Leading Sapiens</span><span>Sheril Mathews</span></p></div><p><img src="https://www.leadingsapiens.com/content/images/2022/12/Ladder-of-Inference.png" alt=""></p></a></figure><ul><li><a href="https://www.leadingsapiens.com/dont-overcome-self-doubt/#dunning-kruger-effect-and-impostor-syndrome" rel="noreferrer">Imposter syndrome</a> is one kind of cognitive distortion/thought trap:</li></ul><figure><a href="https://www.leadingsapiens.com/dont-overcome-self-doubt/"><div><p>There’s no need to eliminate or even overcome self-doubt.</p><p>Self-doubt is not an impediment to be eliminated as is commonly thought of. It’s a condition of the game and can even be a positive indicator.</p><p><img src="https://www.leadingsapiens.com/content/images/size/w256h256/2022/05/LS-logo.png" alt=""><span>Leading Sapiens</span><span>Sheril Mathews</span></p></div><p><img src="https://www.leadingsapiens.com/content/images/2022/12/Doubt--Dunning-Kruger-Imposter-Syndrome.png" alt=""></p></a></figure><h2 id="sources">Sources</h2><ol><li><a href="https://amzn.to/3TjOMss?ref=leadingsapiens.com" rel="noreferrer">Feeling Good</a> by David Burns</li><li> <a href="https://amzn.to/473A3oY?ref=leadingsapiens.com" rel="noreferrer">The Anxious Achiever</a> by Morra Aarons-Mele</li><li><a href="https://amzn.to/48eSasR?ref=leadingsapiens.com" rel="noreferrer">Handbook of Coaching Psychology </a> by Stephen Palmer, Alison Whybrow</li><li><a href="https://amzn.to/3uWpZ3x?ref=leadingsapiens.com" rel="noreferrer">Dealing with Emotional Problems&nbsp;Using RECBT</a> by Windy Dryden</li><li><a href="https://amzn.to/4afpuBP?ref=leadingsapiens.com" rel="noreferrer">Evidence Based Coaching Handbook</a> by Dianne Stober, Anthony Grant</li><li><a href="https://amzn.to/3TtpdVM?ref=leadingsapiens.com" rel="noreferrer">A Practitioner's Guide to REBT</a> by Raymond DiGiuseppe, Kristen Doyle, Windy Dryden, Wouter Backx</li></ol>
    </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Double-mirror illusion (113 pts)]]></title>
            <link>https://journalofillusion.net/index.php/joi/article/view/9839/16407</link>
            <guid>38654968</guid>
            <pubDate>Fri, 15 Dec 2023 15:17:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journalofillusion.net/index.php/joi/article/view/9839/16407">https://journalofillusion.net/index.php/joi/article/view/9839/16407</a>, See on <a href="https://news.ycombinator.com/item?id=38654968">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="customblock-about">
		<p><span>About the journal</span></p>
<p>Journal of Illusion (ISSN: 2436-4045) is an open-access journal that aims at gathering resources to promote the study of illusion. JOI defines an illusion as the perception of an object or phenomenon that is considered to be inconsistent with individual’s or group’s prior knowledge, recognition, or belief as to what the object or phenomenon should be in perception, cognition, and/or physics.&nbsp; Therefore, JOI focuses on perceptual illusions, cognitive illusions (e.g. magic or misunderstanding) or physical illusions (e.g. mirage or the Doppler effect). For perceptual illusions, not only visual illusions but also illusions at various sensory modalities are welcome. Trompe l’oeil as well as illusion artworks are also welcome. <strong><a href="https://journals.openacademia.net/index.php/joi/about">Learn more &gt;&gt;</a></strong></p>
	</div><div id="customblock-whypublish">
		<p><span>Why publish with <strong><em>Journal of Illusion</em></strong>?</span></p>
<p><span>Open Access</span>&nbsp;–&nbsp;<em>Journal of Illusion</em>&nbsp;is free from all access barriers, allowing for the widest possible dissemination of your work.</p>
<p><span>Retain copyright&nbsp;</span>– you are free to disseminate your work, make unlimited copies, and deposit it in any repository.&nbsp;</p>
<p><span>Personal service</span>&nbsp;–&nbsp;<em>Journal of Illusion</em>&nbsp;is published in partnership with <a href="https://openacademia.net/index.html">Open Academia</a>, a Publishing Partner dedicated to giving you excellent service.&nbsp;</p>
<p><span>Self-archiving</span>&nbsp;– you can deposit&nbsp;<em>any</em>&nbsp;version of your manuscript in any required repository or archive, or post it to your personal or institutional website.&nbsp;</p>
<p><strong>Post-publication statistics</strong> – metrics shown with each article make it easy to check how often your paper is being downloaded via the JOI website.</p>
<p><strong>Add supplementary material</strong> – you can make data sets, protocols, very large illustrations, videos, questionnaires etc. available to readers alongside your article, free of charge.&nbsp;</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Delta Dental says data breach exposed info of 7M people (233 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/delta-dental-says-data-breach-exposed-info-of-7-million-people/</link>
            <guid>38654805</guid>
            <pubDate>Fri, 15 Dec 2023 14:59:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/delta-dental-says-data-breach-exposed-info-of-7-million-people/">https://www.bleepingcomputer.com/news/security/delta-dental-says-data-breach-exposed-info-of-7-million-people/</a>, See on <a href="https://news.ycombinator.com/item?id=38654805">Hacker News</a></p>
Couldn't get https://www.bleepingcomputer.com/news/security/delta-dental-says-data-breach-exposed-info-of-7-million-people/: Error: Request failed with status code 503]]></description>
        </item>
        <item>
            <title><![CDATA[September 11th That You Have Never Seen (133 pts)]]></title>
            <link>https://medium.com/@jeremiahjw/31-photos-from-september-11th-that-you-have-never-seen-9514aaac2d54</link>
            <guid>38654567</guid>
            <pubDate>Fri, 15 Dec 2023 14:34:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@jeremiahjw/31-photos-from-september-11th-that-you-have-never-seen-9514aaac2d54">https://medium.com/@jeremiahjw/31-photos-from-september-11th-that-you-have-never-seen-9514aaac2d54</a>, See on <a href="https://news.ycombinator.com/item?id=38654567">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://medium.com/@jeremiahjw?source=post_page-----9514aaac2d54--------------------------------"><div aria-hidden="false"><p><img alt="Jeremiah Warren" src="https://miro.medium.com/v2/resize:fill:88:88/1*cBiTRH-yRdfE6I0-iClrkg.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="3f35">In 2001, digital cameras were a rare commodity. They were expensive, bulky and captured images that were inferior to the organic look of film. After you downloaded and edited those whopping 3.1 megapixel images, you had very few options of where you could publish them online. Remember when Shutterfly and Snapfish were a thing?</p><p id="d82a">Contrast that to today where you have people shooting magazine covers on cell phones and uploading over 500,000 images to Instagram, Snapchat, Twitter, and Facebook every minute. We don’t hear about national tragedies on the news anymore, we read about them in our Twitter and Facebook feeds. Seconds after they happen.</p><p id="69e2">When the events of 9/11 took place there were thousands of photographs taken by professional photographers and members of the press. These images were shown on the news and published in magazines and newspapers all across the country. Yet, few of these featured photographs were taken by everyday people.</p><p id="0dc5">I wanted to set about curating a selection of photographs that most of you haven’t seen. Photographs captured by everyday people. Thanks to the internet these individuals have been able to publish their photos to Flickr, but most of them have less than a thousand, or even less than a hundred views. As I searched for these images it was like I was witnessing history again, but from an angle that no one had ever been shown. I decided to share these photographs with all of you.</p><p id="0b95">For the images that were captured on a digital camera, I’ve made a note of the model of the camera. All images are hosted on the account of the person that owns the photographs, none of them were taken down and re-hosted.</p><figure></figure><p id="968f">Seconds after flight 175 struck the South Tower. Taken with a Canon PowerShot S100 by <a href="http://www.flickr.com/people/georgeweld/" rel="noopener ugc nofollow" target="_blank">George Weld</a>.</p><figure></figure><p id="b5e1"><a href="http://www.flickr.com/people/since1968/" rel="noopener ugc nofollow" target="_blank">Marc Garrett</a>, about this image he captured. “The second plane flew directly over my head and slammed into the south tower. It took me a few seconds to get my head together, and this was the shot I took. I’m not a professional photojournalist, but I believe having a camera in my hand and feeling like a I had a “job” to do helped me keep my head.”</p><figure></figure><p id="faff"><a href="http://www.flickr.com/people/hiro_oshima/" rel="noopener ugc nofollow" target="_blank">The photographer</a> who took this photo mentions that at the time it didn’t occur to him how bad of an idea it was to walk so close to the tower right after it was struck. Later he discovered that he had been hit in the leg by a piece of falling metal, but didn’t notice it until hours later after he had settled down. If you read the comments you’ll find one by the owner of the open delivery truck you see in this image. He mentioned that the driver of the truck, seen in the blue shirt and pants survived the ordeal. The truck, however, was crushed. This image and the following were taken on an Olympus E-10.</p><figure></figure><p id="2f29">This image struck me on a deep emotion level. In the midst of the chaos and destruction there were still people willing to show their selflessness and cover the remains of the victims.</p><figure></figure><p id="7cb7">Taken a few moments after the second tower was hit, you can see the cloud of paper floating through the air. Photograph by <a href="http://www.flickr.com/people/79579905@N00/" rel="noopener ugc nofollow" target="_blank">Ronald Smits</a>.</p><figure></figure><p id="a71d">You can see the outline of the plane’s wing span. Photograph by <a href="http://www.flickr.com/people/hiro_oshima/" rel="noopener ugc nofollow" target="_blank">Hiro</a>.</p><figure></figure><p id="7135">I think this image speaks for itself. Photograph by <a href="http://www.flickr.com/people/lukekurtis/" rel="noopener ugc nofollow" target="_blank">Luke Kurtis</a>.</p><figure></figure><p id="ea71">Photographer <a href="http://www.flickr.com/people/strippednuts/" rel="noopener ugc nofollow" target="_blank">Jay Boucher says</a>: “My wife had called me that morning to let me know she was safe. “Huh?” I said. She told me to turn on the TV and there was the Trade Center, burning. I grabbed my cameras and ran out to Hoboken’s Pier A. This is what I saw”.</p><figure></figure><p id="8a49">Photograph by <a href="http://www.flickr.com/people/georgeweld/" rel="noopener ugc nofollow" target="_blank">George Weld</a>, taken on a Canon PowerShot S100.</p><figure></figure><p id="df48">Photograph by <a href="http://www.flickr.com/people/pixorama/" rel="noopener ugc nofollow" target="_blank">Michael Foran</a>, taken on an Olympus C2000Z.</p><figure></figure><p id="a850">Photographed by <a href="http://www.flickr.com/people/hbomb/" rel="noopener ugc nofollow" target="_blank">Harvey Silikovitz</a> on Houston Street in Greenwich Village. “An out-of-town TV reporter who is covering the 9/11 tragedy looks at the smoke emanating from the wreckage of the World Trade Center, a couple of miles to the south. Taken during my pre-digital days, this picture happened to be on a roll that for some reason I had gotten burned onto a CD when I got it developed.”</p><figure></figure><p id="696f">A rescue team taking off to attempt a rooftop rescue. They never made it. Photograph by <a href="http://www.flickr.com/people/bryanthatcher/" rel="noopener ugc nofollow" target="_blank">Bryan Thatcher</a>, taken with a Sony Cybershot.</p><figure></figure><p id="ecaf">Photographer <a href="http://www.flickr.com/people/pixorama/" rel="noopener ugc nofollow" target="_blank">Michael Foran</a> says “This man was overcome with emotion as we listened to the calls of the Firemen and Police trapped in the rubble of the collapsed towers on his police scanner radio.”</p><figure></figure><p id="dd24">Photograph by <a href="http://www.flickr.com/people/13189502@N02/" rel="noopener ugc nofollow" target="_blank">Eddy</a>, taken on an Olympus C3000Z</p><figure></figure><p id="3be9">It looks like this woman is shooting with an Olympus film camera. I think I still have the same lens and camera. Photograph by <a href="http://www.flickr.com/people/theactionitems/" rel="noopener ugc nofollow" target="_blank">Marc AuMarc</a>.</p><figure></figure><p id="2aa1">Photograph by <a href="http://www.flickr.com/photos/georgeweld/" rel="noopener ugc nofollow" target="_blank">George Weld</a>, taken on a Canon PowerShot S100.</p><figure></figure><p id="be8f">There are a lot of photographs of messages scrawled into the dust covering the cars. I can’t make out what the note says. Photograph by <a href="http://www.flickr.com/people/theactionitems/" rel="noopener ugc nofollow" target="_blank">Marc AuMarc</a>.</p><figure></figure><p id="dc1b">Photographer <a href="http://www.flickr.com/people/hiro_oshima/" rel="noopener ugc nofollow" target="_blank">Hiro</a> says “The firemen were utterly covered by the debris. We all could tell that a lot of it was asbestos, though no one said it outloud. It crossed my mind that this could be the real terror, if all the people around became ill after the fact.”</p><figure></figure><p id="ac30">Taken with a Nikon E990 by <a href="http://www.flickr.com/people/georgehjr/" rel="noopener ugc nofollow" target="_blank">George Hackett</a>.</p><figure></figure><p id="ace7">Photograph by <a href="http://www.flickr.com/people/malarchie/" rel="noopener ugc nofollow" target="_blank">Shayna Marchese</a>. Her father posted this image on his Flickr account, he says “This is 6th Avenue and there was no traffic on it at all. Just pedestrians beginning to realize that the first tower had fallen.”</p><figure></figure><p id="dd43">Photographer <a href="http://www.flickr.com/people/boyds/" rel="noopener ugc nofollow" target="_blank">Brian Boyd</a> says “I’m running North on West Side Highway, just one block from Chambers street. The tower just collapsed seconds before this photo.”</p><figure></figure><p id="699b">Photograph by <a href="http://www.flickr.com/people/georgehjr/" rel="noopener ugc nofollow" target="_blank">George Hackett</a></p><figure></figure><p id="a5e1">Photograph by <a href="http://www.flickr.com/people/georgehjr/" rel="noopener ugc nofollow" target="_blank">George Hackett</a>.</p><figure></figure><p id="e60f">Photograph by <a href="http://www.flickr.com/people/bryanthatcher/" rel="noopener ugc nofollow" target="_blank">Bryan Thatcher</a>, taken on a Sony Cybershot.</p><figure></figure><p id="4890">Photographer <a href="http://www.flickr.com/people/santijose/" rel="noopener ugc nofollow" target="_blank">Santi-Jose</a> says “I never go down to that area of the city during the week, but there I was on that morning. chance or fate? I was to witness this moment in history. ever since that day seven years ago I almost never leave the house without my camera.”</p><figure></figure><p id="5964">Brooklyn onlookers. Photograph by <a href="http://www.flickr.com/people/x-hibition/" rel="noopener ugc nofollow" target="_blank">Hans</a>.</p><figure></figure><p id="43af">Photograph by <a href="http://www.flickr.com/people/kenengimaging/" rel="noopener ugc nofollow" target="_blank">Ken Eng</a>. Taken on a Fujifilm FinePixS1 Pro.</p><figure></figure><p id="5936">Photograph by <a href="http://www.flickr.com/people/kenengimaging/" rel="noopener ugc nofollow" target="_blank">Ken Eng</a>.</p><figure></figure><p id="374e">Photograph by <a href="http://www.flickr.com/people/kenengimaging/" rel="noopener ugc nofollow" target="_blank">Ken Eng</a>.</p><figure></figure><p id="b14f">Photographed by <a href="http://www.flickr.com/people/demonbaby/" rel="noopener ugc nofollow" target="_blank">Rob Sheridan</a> from his Brooklyn apartment, on a Canon EOS D30.</p><figure></figure><p id="4ec1">This was taken the day after 9/11, on September 12th, by <a href="http://www.flickr.com/people/13189502@N02/" rel="noopener ugc nofollow" target="_blank">Eddy</a>.</p></div><div><p id="8b70"><strong>If you enjoyed this piece, please consider clicking the little clapping hands icon below, so someone else will see and read it too. Thank you! :)</strong></p><p id="c2ff"><em>Follow me on Twitter </em><a href="http://twitter.com/jeremiahjw" rel="noopener ugc nofollow" target="_blank"><em>@JeremiahJW </em></a><em>.</em></p><p id="565c"><em>See more of my work at </em><a href="http://jeremiahwarren.com/" rel="noopener ugc nofollow" target="_blank"><em>JeremiahWarren.com</em></a><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Data exfiltration from Writer.com with indirect prompt injection (201 pts)]]></title>
            <link>https://promptarmor.substack.com/p/data-exfiltration-from-writercom</link>
            <guid>38654533</guid>
            <pubDate>Fri, 15 Dec 2023 14:31:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://promptarmor.substack.com/p/data-exfiltration-from-writercom">https://promptarmor.substack.com/p/data-exfiltration-from-writercom</a>, See on <a href="https://news.ycombinator.com/item?id=38654533">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>This vulnerability can allow attackers to steal a user’s private documents by manipulating the language model used for content generation. As of now, it has not been fixed as it was not triaged as a security vulnerability by Writer.com after disclosure (more details in Responsible Disclosure section at the end). </em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png" width="480" height="289.1208791208791" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:877,&quot;width&quot;:1456,&quot;resizeWidth&quot;:480,&quot;bytes&quot;:1399454,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb633d5a-2e58-448c-988a-e9e745288dd7_3966x2390.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>(the attack, disguised in white text on an attacker controlled website)</figcaption></figure></div><p><span>Writer.com is an application that can be used by enterprises and consumers alike. Users can upload data files, share links, and ask questions in order to generate tailored content for their business needs. It has access to your brand and knowledge base and as such can maintain consistency when writing articles for you. They emphasize its data security given the sensitivity of information its clients upload throughout its website: </span><a href="https://writer.com/product/data-security-privacy/" rel="nofollow ugc noopener">https://writer.com/product/data-security-privacy/</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png" width="480" height="255.14950166112956" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1204,&quot;resizeWidth&quot;:480,&quot;bytes&quot;:67733,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd37f09c5-5444-4973-835d-ddbb2b28d6b1_1204x640.png 1456w" sizes="100vw"></picture></div></a><figcaption>(screenshot from writer.com/security on Dec 13)</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png" width="470" height="129.12087912087912" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:1456,&quot;resizeWidth&quot;:470,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8504d66f-35a8-4567-a887-3f445e34aa79_1514x416.png 1456w" sizes="100vw"></picture></div></a><figcaption>(screenshot from writer.com on Dec 5)</figcaption></figure></div><p>In Writer, users can enter a ChatGPT-like session to edit or create their documents. In this chat session, the LLM can retrieve information from sources on the web to assist users in creation of their documents. We show that attackers can prepare websites that, when a user adds them as a source, manipulate the LLM into sending private information to the attacker or perform other malicious activities.&nbsp;</p><p>The data theft can include documents the user has uploaded, their chat history or potentially specific private information the chat model can convince the user to divulge at the attacker's behest.</p><p><span>This type of attack is called </span><a href="https://arxiv.org/abs/2302.12173" rel="nofollow ugc noopener">indirect prompt injection</a><span>, initially coined by Kai Greshake.</span></p><p>To prove the feasibility of such an attack, we uploaded a file that contains mocked sensitive information (SSN numbers, revenue figures, salary information), and were able to exfiltrate all of it:&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png" width="1456" height="84" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:84,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:96853,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8bf00bc-a870-4ece-827b-1fbd5e9e85e8_1600x92.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from our pentesting exfiltration server)</figcaption></figure></div><p>The website that hosts the payload looks like any other website, and the payload is hidden from any user visiting it. In the following screenshot, the hidden text of the payload is highlighted (it has white font, but there are other methods to hide it or embed payloads on other websites such as social media platforms).:&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png" width="1456" height="363" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:363,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9d4a38c-c276-4585-8814-069dbdd16125_1600x399.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from the website with the injection)</figcaption></figure></div><p>Note that *.cloudfront.net is one of the locations allowed by the CSP. </p><p>A typical user use case would be the following:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png" width="1456" height="258" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:258,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1083806,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e65223a-e808-496c-ab68-e2885413f81d_4760x842.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>However, here’s what actually happens in the background with the injection</p><p>A) They ask Writer to write a report for them based on some sources and some data they upload.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png" width="1456" height="1147" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1147,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c3f8a54-fe97-4878-8421-48a1a63b4785_1600x1260.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from our chat session at writer.com)</figcaption></figure></div><p>B) They find a nice source on the web which has the information they need</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png" width="1456" height="1398" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1398,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3476bfd3-f19c-41ae-b7ff-b205518200ad_1600x1536.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>C) They upload some sensitive data</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png" width="1360" height="832" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b856d97d-56ff-4027-9307-06cc92003da6_1360x832.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:832,&quot;width&quot;:1360,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb856d97d-56ff-4027-9307-06cc92003da6_1360x832.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>D) They get Writer to write the report</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png" width="1456" height="428" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:428,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff02b01b2-526c-415a-a59b-904e4c1127fc_1600x470.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from our chat window at writer.com)</figcaption></figure></div><p>E) Writer reads the webpage, but it contains a hidden injection in small white text:&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png" width="1456" height="877" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:877,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6622f249-a11d-4265-8b32-a1737ee68806_1600x964.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>F) Writer follows the instructions, and overrides the initial instructions of the user and any security filters Writer.com has enforced. The user never asked for this image and it was not on the webpage that they initially asked for. Nevertheless, Writer.com has automatically rendered the attacker-controlled image in markdown and you can see in the network activity that it has appended the contents of the uploaded client data file to the HTTP parameters, just as instructed: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png" width="1456" height="758" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:758,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4418822,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ab5f8a6-6b28-4db2-8d02-59c6fcf3e34a_14198x7389.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot of our chat session at writer.com with network activity expanded)</figcaption></figure></div><p>Here’s a side by side comparison between the uploaded client data file, and a zoomed in image of the HTTP parameters from the above screenshot: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png" width="1456" height="255" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:255,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1406652,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9089e37b-2eb5-46b6-855c-200708012b0e_17604x3087.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>G) Without their knowledge, their data has now been exfiltrated to the attacker’s server. Rendering the image in markdown automatically created a GET request with the HTTP parameters including the content of the file. The attacker can read their logs to extract the sensitive client data from the file.&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png" width="1456" height="1634" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1634,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2070420,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2068bb49-7d45-4b67-88f0-e966db968cbf_4248x4768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>(screenshot from pentesting exfiltration server logs)</figcaption></figure></div><p><span>Rendering an image to exfiltrate data is only one method of exfiltrating data. Note that while, to our knowledge, Writer does not use OpenAI for text generation, </span><a href="https://embracethered.com/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/#:~:text=Responsible%20Disclosure" rel="nofollow ugc noopener">OpenAI has said the same issue in their system is a “won’t fix.”</a></p><p>Please see below for other example attacks which use other mediums (like links) to exfiltrate data.&nbsp;</p><h5>Example 1: Exfiltration of uploaded files</h5><p>In this example, an attacker is able to exfiltrate a confidential file that the user uploads via&nbsp;a link, using this injection: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png" width="1456" height="284" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:284,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:121493,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa07c38f3-f4f1-47e2-ade0-b594babbe669_1540x300.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://www.loom.com/share/d8ffe66d9dfb44429be8a60182372f38" rel="nofollow ugc noopener">Video explanation (sent with disclosure)</a></p><h5>Example 2: Exfiltration of chat history</h5><p>In this example, an attacker is able to exfiltrate the chat history from a user via a link, using this injection: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png" width="1456" height="309" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:309,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F211bd372-8a83-4fb3-86d0-ed1399304db7_1600x340.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://www.loom.com/share/678d76f03c5244a1aee592ba41e744dd" rel="nofollow ugc noopener">Video explanation (sent with disclosure)</a></p><p><span>These type of attacks have been done in other LLM surfaces, such as the </span><a href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/" rel="nofollow ugc noopener">Bard attack by Thacker, Rehberger and Greshake </a><span>, which was resolved promptly by the Google Security and Bard team.&nbsp;</span></p><p>For more information on these attacks and relevant information here are some great sources:&nbsp;</p><ul><li><p><a href="https://kai-greshake.de/" rel="nofollow ugc noopener">https://kai-greshake.de/</a><span> (twitter: @KGreshake)</span></p></li><li><p><a href="https://embracethered.com/blog/index.html" rel="nofollow ugc noopener">https://embracethered.com/blog/index.html</a><span> (twitter: @wunderwuzzi23)</span></p></li><li><p><a href="https://josephthacker.com/" rel="nofollow ugc noopener">https://josephthacker.com/</a><span> (twitter: @rez0_) </span></p></li><li><p><a href="https://promptarmor.com/" rel="nofollow ugc noopener">https://promptarmor.com/ </a><span>(twitter: @promptarmor)</span></p></li></ul><p>And to learn more about LLM security risks feel free to check out:&nbsp;</p><ul><li><p><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/" rel="nofollow ugc noopener">https://owasp.org/www-project-top-10-for-large-language-model-applications/</a></p></li><li><p><a href="https://atlas.mitre.org/" rel="nofollow ugc noopener">https://atlas.mitre.org/</a></p></li></ul><p>Responsible Disclosure Timeline</p><ul><li><p>Nov 29: We disclose issue to CTO &amp; Security team with video examples</p></li><li><p>Nov 29: Writer responds, asking for more details</p></li><li><p>Nov 29: We respond describing the exploit in more detail with screenshots</p></li><li><p>Dec 1: We follow up</p></li><li><p>Dec 4: We follow up with re-recorded video with voiceover asking about their responsible disclosure policy</p></li><li><p>Dec 5: Writer responds “We do not consider this to be a security issue since the real customer accounts do not have access to any website.”</p></li><li><p>Dec 5: We explain that paid customer accounts have the same vulnerability, and inform them that we are writing a post about the vulnerability so consumers are aware. No response from the Writer team after this point in time. </p></li></ul><p>Feel free to reach out to us at founders@promptarmor.com or at https://kai-greshake.de/about</p><h6>Disclaimer: The content of this blog is intended solely for research and educational use, aimed at enhancing knowledge, understanding, and awareness regarding attacks and their countermeasures to bolster the security of Large Language Models (LLMs)</h6></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Happens in the Brain While Daydreaming? (179 pts)]]></title>
            <link>https://hms.harvard.edu/news/what-happens-brain-while-daydreaming</link>
            <guid>38654388</guid>
            <pubDate>Fri, 15 Dec 2023 14:18:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hms.harvard.edu/news/what-happens-brain-while-daydreaming">https://hms.harvard.edu/news/what-happens-brain-while-daydreaming</a>, See on <a href="https://news.ycombinator.com/item?id=38654388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span lang="EN" xml:lang="EN"><strong>At a glance:</strong></span></p><ul><li><span lang="EN" xml:lang="EN"></span><span lang="EN" xml:lang="EN"><strong>During quiet waking, brain activity in mice suggests the animals are daydreaming about a recent image.</strong></span></li><li><span lang="EN" xml:lang="EN"></span><span lang="EN" xml:lang="EN"><strong>Having daydreams about a recently viewed image predicted how the brain would respond to the image in the future.</strong></span></li><li><p><span lang="EN" xml:lang="EN"></span><span lang="EN" xml:lang="EN"><strong>The findings provide a clue that daydreams may play a role in brain plasticity. </strong></span></p><hr></li></ul><p><span lang="EN" xml:lang="EN"></span><span lang="EN" xml:lang="EN">You are sitting quietly, and suddenly your brain tunes out the world and wanders to something else entirely — perhaps a recent experience, or an old memory. You just had a daydream. </span></p><p><span lang="EN" xml:lang="EN">Yet despite the ubiquity of this experience, what is happening in the brain while daydreaming is a question that has largely eluded neuroscientists. </span></p><p><span lang="EN" xml:lang="EN">Now, a study in mice, </span><a href="https://www.nature.com/articles/s41586-023-06810-1"><span lang="EN" xml:lang="EN">published Dec. 13 in </span><em><span lang="EN" xml:lang="EN">Nature</span></em></a><em><span lang="EN" xml:lang="EN">,</span></em><span lang="EN" xml:lang="EN"> has brought a team led by researchers at Harvard Medical School one step closer to figuring it out. </span></p><p><a href="https://hms.harvard.edu/news-events/sign-email-communications"><span lang="EN" xml:lang="EN"><strong>Get more HMS news here</strong></span></a><span lang="EN" xml:lang="EN"></span></p><p><span lang="EN" xml:lang="EN">The researchers tracked the activity of neurons in the visual cortex of the brains of mice while the animals remained in a quiet waking state. They found that occasionally these neurons fired in a pattern similar to one that occurred when a mouse looked at an actual image, suggesting that the mouse was thinking — or daydreaming — about the image. Moreover, the patterns of activity during a mouse’s first few daydreams of the day predicted how the brain’s response to the image would change over time. </span></p><p><span lang="EN" xml:lang="EN">The research provides tantalizing, if preliminary, evidence that daydreams can shape the brain’s future response to what it sees. This causal relationship needs to be confirmed in further research, the team cautioned, but the results offer an intriguing clue that daydreams during quiet waking may play a role in brain plasticity — the brain’s ability to remodel itself in response to new experiences.</span></p><p><span lang="EN" xml:lang="EN">“We wanted to know how this daydreaming process occurred on a neurobiological level, and whether these moments of quiet reflection could be important for learning and memory,” said lead author Nghia Nguyen, a PhD student in neurobiology in the Blavatnik Institute at HMS. </span></p><h3><span lang="EN" xml:lang="EN"><strong>An overlooked brain region</strong></span></h3><div><p><span lang="EN" xml:lang="EN">Scientists have spent considerable time studying how neurons replay past events to form memories and map the physical environment in the hippocampus, a seahorse-shaped brain region that plays a key role in memory and spatial navigation.</span></p><p><span lang="EN" xml:lang="EN">By contrast, there has been little research on the replay of neurons in other brain regions, including the visual cortex. Such efforts would provide valuable insights about how visual memories are formed. </span></p></div><p><span lang="EN" xml:lang="EN">“My lab became interested in whether we could record from enough neurons in the visual cortex to understand what exactly the mouse is remembering — and then connect that information to brain plasticity,” said senior author </span><a href="https://www.andermannlab.com/"><span lang="EN" xml:lang="EN">Mark Andermann</span></a><span lang="EN" xml:lang="EN">, professor of medicine at Beth Israel Deaconess Medical Center, and professor of neurobiology at HMS. </span></p><figure><article><img loading="lazy" src="https://hms.harvard.edu/sites/default/files/2023-12/MouseMovie-1-and-2-400x320.gif" width="400" height="320" alt="A checkerboard pattern of gray and black and white squares that morphs into another, similar pattern"></article><figcaption>During the experiments, mice repeatedly looked at one of two images, shown here, with one-minute breaks in between. The images were selected based on their ability to elicit a strong response from neurons in the visual cortex. Video: Andermann lab</figcaption></figure><p><span lang="EN" xml:lang="EN">In the new study, the researchers repeatedly showed mice one of two images, each consisting of a different checkerboard pattern of gray and dappled black and white squares. Between images, the mice spent a minute looking at a gray screen. The team simultaneously recorded activity from around 7,000 neurons in the visual cortex. </span></p><p><span lang="EN" xml:lang="EN">The researchers found that when a mouse looked at an image, the neurons fired in a specific pattern, and the patterns were different enough to discern image one from image two. More important, when a mouse looked at the gray screen between images, the neurons sometimes fired in a similar, but not identical, pattern, as when the mouse looked at the image, a sign that it was daydreaming about the image. These daydreams occurred only when mice were relaxed, characterized by calm behavior and small pupils. </span></p><p><span lang="EN" xml:lang="EN">Unsurprisingly, mice daydreamed more about the most recent image — and they had more daydreams at the beginning of the day than at the end, when they had already seen each image dozens of times. </span></p><figure><article><img loading="lazy" src="https://hms.harvard.edu/sites/default/files/2023-12/FiringMouseNeurons-400x274.gif" width="400" height="274" alt="A grayscale image showing a cloud-like group of neurons lighting up"></article><figcaption>Between images, mice spent a minute looking at a gray screen. During this time, neurons in the visual cortex of the brain, shown here, occasionally fired in a pattern similar to one seen when the mice were looking at an image, suggesting that mice were daydreaming about the image. Video: Andermann lab</figcaption></figure><p><span lang="EN" xml:lang="EN">But what the researchers found next was completely unexpected. </span></p><p><span lang="EN" xml:lang="EN">Throughout the day, and across days, the activity patterns seen when the mice looked at the images changed — what neuroscientists call “representational drift.” Yet this drift wasn’t random. Over time, the patterns associated with the images became even more different from each other, until each involved an almost entirely separate set of neurons. Notably, the pattern seen during a mouse’s first few daydreams about an image predicted what the pattern would become when the mouse looked at the image later. </span></p><p><span lang="EN" xml:lang="EN">“There’s drift in how the brain responds to the same image over time, and these early daydreams can predict where the drift is going,” Andermann said. </span></p><p><span lang="EN" xml:lang="EN">Finally, the researchers found that the visual cortex daydreams occurred at the same time as replay activity occurred in the hippocampus, suggesting that the two brain regions were communicating during these daydreams. </span></p><h3><span lang="EN" xml:lang="EN"><strong>To sit, perchance to daydream</strong></span></h3><p><span lang="EN" xml:lang="EN">Based on the results of the study, the researches suspect that these daydreams may be actively involved in brain plasticity. </span></p><p><span lang="EN" xml:lang="EN">“When you see two different images many times, it becomes important to discriminate between them. Our findings suggest that daydreaming may guide this process by steering the neural patterns associated with the two images away from each other,” Nguyen said, while noting that this relationship needs to be confirmed. </span></p><p><span lang="EN" xml:lang="EN">Nguyen added that learning to differentiate between the images should help the mouse respond to each image with more specificity in the future. </span></p><p><span lang="EN" xml:lang="EN">These observations align with a growing body of </span><a href="https://www.sciencedirect.com/science/article/pii/S0149763422002883?casa_token=MBHEs_LeUGwAAAAA:KWgpbzTsGl7jv55nOZFGPssjIi-0j8-H1Th5rZ52Jxt71QglVXVsmU4BpcIhUAjtDw4QPyhXYHQ"><span lang="EN" xml:lang="EN">evidence in rodents and humans</span></a><span lang="EN" xml:lang="EN"> that entering a state of quiet wakefulness after an experience can improve learning and memory. </span></p><p><span lang="EN" xml:lang="EN">Next, the researchers plan to use their imaging tools to visualize the connections between individual neurons in the visual cortex and to examine how these connections change when the brain “sees” an image. </span></p><p><span lang="EN" xml:lang="EN">“We were chasing this 99 percent of unexplored brain activity and discovered that there’s so much richness in the visual cortex that nobody knew anything about,” Andermann said. </span></p><p><span lang="EN" xml:lang="EN">Whether daydreams in people involve similar activity patterns in the visual cortex is an open question, and the answer will require additional experiments. However, there is preliminary evidence that an analogous process occurs in humans when they recall visual imagery.</span></p><p><a href="https://bucknerlab.fas.harvard.edu/"><span lang="EN" xml:lang="EN">Randy Buckner</span></a><span lang="EN" xml:lang="EN">, the Sosland Family Professor of Psychology and of Neuroscience at Harvard University, has shown that </span><a href="https://www.pnas.org/doi/10.1073/pnas.97.20.11125"><span lang="EN" xml:lang="EN">brain activity in the visual cortex increases</span></a><span lang="EN" xml:lang="EN"> when people are asked to recall an image in detail. Other studies have recorded </span><a href="https://www.science.org/doi/10.1126/science.aax1030"><span lang="EN" xml:lang="EN">flurries of electrical activity</span></a><span lang="EN" xml:lang="EN"> in the visual cortex and the hippocampus during such recall. </span></p><p><span lang="EN" xml:lang="EN">For the researchers, the results of their study and others suggest that it may be important to make space for moments of quiet waking that lead to daydreams. For a mouse, this may mean taking a pause from looking at a series of images and, for a human, this could mean taking a break from scrolling on a smartphone. </span></p><p><span lang="EN" xml:lang="EN">“We feel pretty confident that if you never give yourself any awake downtime, you’re not going to have as many of these daydream events, which may be important for brain plasticity,” Andermann said. </span></p><p><span lang="EN" xml:lang="EN"><strong>Authorship, funding, disclosures</strong></span></p><p><span lang="EN" xml:lang="EN">Additional authors on the paper include Andrew Lutas, Oren Amsalem, Jesseba Fernando, Andy Young-Eon Ahn, Richard Hakim, Josselyn Vergara, Justin McMahon, Jordane Dimidschstein, and Bernardo Sabatini.</span></p><p><span lang="EN" xml:lang="EN">The research was supported by a National Defense Science and Engineering Fellowship, a Howard Hughes Medical Institute Gilliam Fellowship, the National Institutes of Health (F32 DK112589; DP2 DK105570; DP1 AT010971-02S1; R01 MH12343), a Davis Family Foundation award, a McKnight Scholar Award, a Harvard Mind Brain Behavior Interfaculty Initiative Faculty Research Award, the Harvard Brain Science Initiative Bipolar Disorder Seed Grant, and by Kent and Liz Dauten. </span></p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WWII code breaker Mary Ratcliffe has died (146 pts)]]></title>
            <link>https://www.dailymail.co.uk/news/article-12867531/code-breaker-Alan-Turing-Nazi-secrets-dies-aged-98.html</link>
            <guid>38654375</guid>
            <pubDate>Fri, 15 Dec 2023 14:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dailymail.co.uk/news/article-12867531/code-breaker-Alan-Turing-Nazi-secrets-dies-aged-98.html">https://www.dailymail.co.uk/news/article-12867531/code-breaker-Alan-Turing-Nazi-secrets-dies-aged-98.html</a>, See on <a href="https://news.ycombinator.com/item?id=38654375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>Tributes have been paid to a female code breaker who helped Alan Turing reveal the Nazi war machine's encrypted secrets during the&nbsp;<a target="_self" href="https://www.dailymail.co.uk/news/world-war-ii/index.html" id="mol-41a53fd0-9b28-11ee-b875-4dff807a2589">Second World War</a> after she died aged 98.</p><p>Mary Ratcliffe worked at a secret code breaking base in Middlesex, helping to decipher coded messages intercepted from the Nazis.</p><p>She&nbsp;decoded messages which were encrypted by German Enigma machines using Bombe machines invented by <a target="_self" href="https://www.dailymail.co.uk/news/article-12435889/Alan-Turing-OBE-King-George-VI-returned.html">Alan Turing</a> at Bletchley Park.</p><p>Many historians have credited Turing's work with shortening the war and say that he and the people who operated his machines saved millions of lives.</p><p>The main base for codebreaking was at Bletchley Park in Milton Keynes, but Mary's site in Eastcote was one of several others established to ensure that if one was bombed or sabotaged, the rest would still be operational.</p><div>  <p>Tributes have been paid to a code breaker&nbsp;Mary Ratcliffe who helped Alan Turing reveal the Nazi war machine's encrypted secrets during the Second World War after she died aged 98</p></div><div>  <p>Mrs Ratcliffe worked at a secret code breaking base in Middlesex, helping to decipher&nbsp;coded messages intercepted from the Nazis</p></div><p>Throughout the decades that followed, Mrs Ratcliffe become a familiar face in her home town of Swindon, Wiltshire, thanks to her willingness to support good causes and her years of public appearances dressed as <a target="_self" href="https://www.dailymail.co.uk/news/queen-victoria/index.html" id="mol-41727000-9b28-11ee-b875-4dff807a2589">Queen Victoria</a>.</p><p>According to her family, she took great pride in her work, but spoke very little about it due to the secrecy that surrounded the profession.</p><p>She later worked as an acupuncturist and was most famed for her portrayal of Queen Victoria at local events, which she did for 30 years.</p><p>She opened fetes, appeared in parades and graced Swindon with her presence as Queen Victoria, free of charge, at the request of event organisers.</p><p>When word of her handwritten royal tributes and Queen Victoria portrayal reached Buckingham Palace, she was invited to meet the living royals herself at Queen Elizabeth II's Garden Party.</p><p>In 2008, Mrs Ratcliffe then became one of the first-ever recipients of the Pride of Swindon award for her work doing soup runs for the homeless with the Simon Community, and her campaigns for various social causes.</p><p>Paying tribute, her family said: 'Whether as Mary or Queen Victoria, she championed underdogs with eloquent ferocity and actively supported humanitarian causes ranging from elder abuse to homelessness.</p><p>'She tackled grave issues, where others feared to tread and as such was always true to herself.'</p><p>Mrs Ratcliffe moved to Kings Court Care Centre in her 90s after an accident left her in need of care, and it was there that she died on November 29, aged 98.</p><p>She now leaves behind three adult children and many grandchildren who say they will sorely miss her warm presence and appetite for life.</p><p>'She was fiercely independent and climbed the stairs to bed until the very last of her life,' her family have said.</p><p>Her family would like to say a particular heartfelt thanks to the staff at Kings Court Care Centre who cared for Mrs Ratcliffe right up until her last day.</p><p>In a previous interview, Mrs Ratcliffe told her local paper the Swindon Advertiser about her wartime exploits.</p><div>  <p>Mrs Ratcliffe used the Bombe machines invented by Alan Turing (pictured), credited with shortening the war by helping decipher messages produced by German Enigma enciphering devices.</p></div><div>  <p>The registration room in hut 6 at Bletchley Park, Buckinghamshire where codebreakers used Bombe machines to break the German enigma</p></div><p>She said: 'Joining the Women's Royal Naval Service at 19 was a defining moment for me.</p><p>'At the Mill Hill recruiting station in North London I was interviewed and assigned to a base.</p><p>'I was not told where I was going, or the nature of the work I would be doing.</p><p>'We were bundled into an Army lorry. The flap was pulled down. Our 'secret' destination was Eastcote, in Middlesex.</p><p>'We were immediately taken into a room where we were instructed to take the Oath of Allegiance to our God, King and country.</p><p>'Our vow of silence was absolute. We were not allowed to discuss our work with anybody. We were not allowed to wear a category badge; if asked, we were told to say we were recruits, which, of course, would not stimulate any further interest.</p><p>'The 30 years vow of silence was sacrosanct, even after the end of the war.'</p><p>Bletchley Park is now a major heritage attraction which houses a refurbished Bombe, but speaking previously Mrs Ratcliffe said she had clear memories of operating their banks of drums in earnest.</p><p>The work was constant and done in rolling eight-hour shifts.</p><p>She added: 'Our task was to follow a menu that instructed the setting of each drum on which the letters of the alphabet were displayed.</p><p>'There were nine rows of coloured drums on every Bombe machine. Each time it stopped, the position of the drums was recorded on the checking machine before restarting the Bombe machine.</p><p>'A team of technicians was assigned to every bay. The daylight lighting was sometimes a strain.</p><p>'Many colleagues found the work boring, but for me, the rhythm of the drums stimulated my creative thoughts. Many amongst us were mavericks or eccentrics. Both apply to me!</p><p>'We were not told what we had achieved. All our successful decoding was immediately wired back to Bletchley Park.'</p><div>  <p>A working replica of one of Turing's Bombe machines which was used to break the Nazi enigma</p></div><p>She also had vivid memories of VE Day: 'The atmosphere was euphoric. We made our way towards the Mall.'</p><p>The group were offered a lift by some young men who had a horse-dawn cart.</p><p>Mrs Ratcliffe added: 'So, in style, we made our way towards Buckingham Palace where the Royal Family were on the balcony with Winston Churchill, who was then left alone so that we could loudly applaud him for his unique, inspiring leadership in defence of our precious core freedoms throughout six years of conflict, that had claimed so many lives who were the creme de la creme of our nation.'</p><p>Mrs Ratcliffe visited Bletchley Park and wrote a tribute to Alan Turing in the form of a poem. Copies were sent to Bletchley Park, GCHQ and the author of a book about Turing's work.</p><mol-permabox id="mol-a4b6a820-9b5a-11ee-bf66-c75e34c59e82"><div data-version="2" id="mol-a7d0d1e0-e2ad-11e8-a337-f9af42da1b43" data-permabox-url="/sciencetech/fb-6364119/WHO-ALAN-TURING.html"><h3><a href="https://www.dailymail.co.uk/sciencetech/fb-6364119/WHO-ALAN-TURING.html">Who was Alan Turing? Pioneering scientist who helped crack Hitler's enigma machine only to be convicted for homosexuality after WWII</a></h3><div><div>  <p>Alan Turing (pictured) was a British mathematician best known for his work cracking the enigma code during the Second World War</p></div><p>Alan Turing was a British mathematician born on June 23, 1912 In Maida Vale, London, to father Julius, a civil servant, and mother Ethel, the daughter of a railway engineer.&nbsp;</p><p>His talents were recognised early on at school but he struggled with his teachers when he began boarding at Sherborne School aged 13 because he was too fixated on science.&nbsp;</p><p>Turing continued to excel at maths but his time at Sherborne was also rocked by the death of his close friend Christopher Morcom from tuberculosis. Morcom was described as Turing's 'first love' and he remained close with his mother following his death, writing to her on Morcom's birthday each year.&nbsp;</p><p>He then moved on to Cambridge where he studied at King's College, graduating with a first class degree in mathematics.&nbsp;&nbsp;</p><p>During the Second World War, Turing was pivotal in cracking the Enigma codes used by the German military to encrypt their messages.</p><p>His work gave Allied leaders vital information about the movement and intentions of Hitler’s forces.</p><p>Historians credit the work of Turing and his fellow codebreakers at Bletchley Park in Buckinghamshire with shortening the war by up to two years, saving countless lives, and he was awarded an OBE in 1946 for his services.&nbsp;</p><p>Turing is also widely seen as the father of computer science and artificial intelligence due to his groundbreaking work in mathematics in the 1930s.</p><p>He was able to prove a 'universal computing machine' would be able to perform equations if they were presented as an algorithm - and had a paper published on the subject in 1936 in the&nbsp;Proceedings of the London Mathematical Society Journal when he was aged just 23.&nbsp;</p><p>But he was disgraced in 1952 when he was convicted for homosexual activity, which was illegal at the time and would not be decriminalised until 1967.</p><p>To avoid prison, Turing agreed to ‘chemical castration’ – hormonal treatment designed to reduce libido.</p><p>As well as physical and emotional damage, his conviction had led to the removal of his security clearance and meant he was no longer able to work for GCHQ, the successor to the Government Code and Cypher School, based at Bletchley Park.&nbsp;</p><div>  <p>Turing was awarded an OBE in 1946 for his codebreaking work at Bletchley Park, pictured, which is credited with ending World War II two years early</p></div><p>Then In 1954, aged 41, he died of cyanide poisoning. An inquest recorded a verdict of suicide, although his mother and others maintained that his death was accidental.&nbsp;</p><p>When his body was discovered, an apple laid half-eaten next to his bed. It was never tested for cyanide but it is speculated it was the source of the fatal dose.&nbsp;</p><p>Some more peculiar theories suggest Turing was 'obsessed' with fairytale Snow White and the Seven Dwarfs and his death was inspired by the poisoned apple in the story.&nbsp;</p><p>Following a public outcry over his treatment and conviction, the then Prime Minister Gordon Brown issued a public apology in 2009.&nbsp;</p><p>He then received a posthumous Royal pardon in 2014, only the fourth to be issued since the end of the Second World War.</p><p>It was requested by Justice Secretary Chris Grayling, who described Turing as a national hero who fell foul of the law because of his sexuality.</p><p>An e-petition demanding a pardon for Turing had previously received 37,404 signatures.&nbsp;</p><p>A 2017 law, that retroactively pardoned all men cautioned or convicted for homosexual acts under historical legislation, was named in his honour.&nbsp;</p> </div></div></mol-permabox></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Lego builds a new Lego set (356 pts)]]></title>
            <link>https://www.theverge.com/c/23991049/lego-ideas-polaroid-onestep-behind-the-scenes-price</link>
            <guid>38653456</guid>
            <pubDate>Fri, 15 Dec 2023 12:25:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/c/23991049/lego-ideas-polaroid-onestep-behind-the-scenes-price">https://www.theverge.com/c/23991049/lego-ideas-polaroid-onestep-behind-the-scenes-price</a>, See on <a href="https://news.ycombinator.com/item?id=38653456">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
		
			<div>
					<p>
		Marc Corfmat was a teenager when he began to compete for Lego’s ultimate prize: the chance to design an official set. He and his brother Nick had been building custom Lego creations ever since they were kids, sometimes in California, sometimes during vacations at their grandparents’ home in La Rochelle, France. They shared their models <a href="https://www.youtube.com/c/MiniBrickProductions/videos">on YouTube</a> and posted their creations to Lego’s website, but interest from the Lego world came slowly, if it came at all.
</p>


					<p>
		Then, in 2020, the brothers started having some luck. <a href="https://ideas.lego.com/">The Lego Ideas program</a> gives fans the chance to turn their designs into reality, offering both fame and a small fortune —&nbsp;1 percent of net sales — to anyone who can convince 10,000 peers and The Lego Group that their set deserves to exist. After three years and 18 submissions, Marc finally cleared the 10,000-vote hurdle with a design based on <a href="https://ideas.lego.com/projects/d0772ba6-298a-44df-ba84-f26faa8d7216"><em>Avatar: The Last Airbender</em></a>. A month later, <a href="https://ideas.lego.com/projects/f4dead4b-7900-451a-9cfe-96c4ab3e756c">his <em>Tintin</em> idea</a> was chosen as a staff pick. <a href="https://ideas.lego.com/projects/0b9f43ac-67c0-4108-9e82-a44d6574b8ab/official_comments#content_nav_tabs">Another design</a> based on <em>The Polar Express </em>hit 10,000 votes the next year.
</p>


					<p>
		And then… nothing. The <em>Tintin</em> votes dried up, and Lego rejected both his fan-favorite <em>Avatar</em> and <em>Polar Express </em>ideas<em>. </em>The company never says why it rejects an Ideas submission, only that deciding factors include everything from “playability” and “brand fit” to the difficulties in licensing another company’s IP.
</p>


					<p>
		“We knew it was almost impossible to get products on the shelves. You see maybe a few selected a year out of thousands of submissions — but even that slight glimmer of hope was enough to really keep us going,” says Marc, now a graduate student in mechanical engineering at the University of California, Davis.
</p>


					<p>
		Then, he decided to try an idea that had been noodling about his brain: a Polaroid, like one of the instant cameras his sister Mia liked using. Marc wasn’t a Polaroid devotee himself, but he’d liked the iconic look of the original 1977 Polaroid OneStep. The rainbow stripe camera had lived on his internal mood board for “quite some time,” but when he saw that a 2020 Lego <em>Minions</em> set had introduced <a href="https://www.bricklink.com/catalogItemIn.asp?P=68327&amp;in=S">the perfect size lens ring</a> for his purposes, he decided to begin building.
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/FpaAR448zYP_rwB0naZjb3AzIi0=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161196/MarcCorfmatPicture_4_3.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/OLne4uI2UUgGwYUxQ16dyP0-Ujw=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161196/MarcCorfmatPicture_4_3.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/_GSzwxYPek3-4dbwRPRmZZ3ejes=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161196/MarcCorfmatPicture_4_3.jpg" alt="Marc Corfmat is the fan designer behind this set — here, he’s holding the original Polaroid OneStep SX-70 instant camera." src="https://cdn3.vox-cdn.com/thumbor/_GSzwxYPek3-4dbwRPRmZZ3ejes=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161196/MarcCorfmatPicture_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Marc Corfmat is the fan designer behind this set — here, he’s holding the original Polaroid OneStep SX-70 instant camera.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/SG-jj-rN8oWc6JRpGCeQ_6ZGB_0=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161292/Progress3.jpg" data-sourcetablet="https://cdn0.vox-cdn.com/thumbor/WiTH--UfUDK12esDYQPOqfFRw9Q=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161292/Progress3.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/sYOCgbZ0H0Dx5cYJAT-ThsPrJes=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161292/Progress3.jpg" alt="Marc’s original digital designs included a truck tire around the lens." src="https://cdn0.vox-cdn.com/thumbor/sYOCgbZ0H0Dx5cYJAT-ThsPrJes=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161292/Progress3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Marc’s original digital designs included a truck tire around the lens.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/YQojalpfimOHBOID89F6xdoURik=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161283/PolaroidSideViewOpen.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/V7fyMu8eU_H9xgMxg-WTl_RQRfI=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161283/PolaroidSideViewOpen.jpg" data-sourcemobile="https://cdn2.vox-cdn.com/thumbor/3Y7ZZ3SpmulOaOaakgsftepFx_s=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161283/PolaroidSideViewOpen.jpg" alt="Marc’s final fan renders show an opening film bay on the front of the camera and a prominent dial on the side so you can spit out photos." src="https://cdn2.vox-cdn.com/thumbor/3Y7ZZ3SpmulOaOaakgsftepFx_s=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161283/PolaroidSideViewOpen.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Marc’s final fan renders show an opening film bay on the front of the camera and a prominent dial on the side so you can spit out photos.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/DAActsXdL2Y3kZWJPz0xjtFwwe8=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161072/marc_lego_polaroid_prototype.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/dkeDEmdejlVmTfdA6ER_kTJTT0Y=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161072/marc_lego_polaroid_prototype.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/6Rj9MOT1DlQJaUjwuirKouh7Qgw=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161072/marc_lego_polaroid_prototype.jpg" alt="One of Marc’s physical prototypes, with a different lens idea." src="https://cdn0.vox-cdn.com/thumbor/6Rj9MOT1DlQJaUjwuirKouh7Qgw=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161072/marc_lego_polaroid_prototype.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>One of Marc’s physical prototypes, with a different lens idea.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/bRDRDH68c8kVIE3R1N4FWIfBP00=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161222/polaroid_marc_corfmat_lego_4_3.gif.gif" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/4oO45CQm27ZSgM20DHend-XErxY=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161222/polaroid_marc_corfmat_lego_4_3.gif.gif" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/m433V4pGlapbuRfi5hHvO0vCYOc=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161222/polaroid_marc_corfmat_lego_4_3.gif.gif" alt="Marc’s animation of his Lego Polaroid fan design shows it pushing a photo brick in and out." src="https://cdn.vox-cdn.com/csk/bc589934-607c-4e08-aa73-839ad29e1951/2f7ff42a-850b-48b6-a4a9-9a9d63fecacb/images/placeholder.png">
    	<label tabindex="0">
			<div>
				
				<p><span>Marc’s animation of his Lego Polaroid fan design shows it pushing a photo brick in and out.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		Everything just clicked. “All the angles were lining up perfectly, everything was working,” he says. “It very quickly became apparent to me that I was falling in love with this thing.”&nbsp;
</p>


					<p>
		And realizing that, Marc decided to do something differently with his Lego Ideas submission in January 2022: he made it <em>move</em>. His model let you “load film” by opening the iconic hinged door, then “eject” a photo by turning a dial or sliding a hidden lever underneath. For the first time, he <a href="https://ideas.lego.com/projects/200dd32e-8ec8-44aa-8f7d-e4dcc6f74e5c">showed off motion on the web</a> in crisp, clean animations that made the gadgety design look irresistible. It got the “staff pick” nod in under two weeks and hit 10,000 supporters in under two months. And this time, Lego finally got in touch.
</p>


					<p>
		Today, Lego is opening preorders for its replica of the classic rainbow stripe Polaroid OneStep SX-70 instant camera, based on Marc’s homegrown build. Lego sent one to <em>The Verge</em> to build and toy with, and as I’ll explain later, the $80 / €80 / £70 set is a delight. Lego also granted us multiple interviews to discuss <em>how</em> a Lego dream comes to life —&nbsp;and the challenges that come with turning a fan-made design into a ready-to-sell product. 
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/FNPiZ6zFEYRlDvwY842AtRGlrfU=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161538/1vpavic_20231012__0030.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/COvBNc4ZVL4mAlFCtuFJ0JiVGf8=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161538/1vpavic_20231012__0030.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/B5WMvdioR1zcj1H6RYRRQQvY-2w=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161538/1vpavic_20231012__0030.jpg" alt="I hold up the final prototype of Lego’s Polaroid as if to take a photo, with its iconic “OneStep” and “Polaroid Land Camera” badges on display." src="https://cdn1.vox-cdn.com/thumbor/B5WMvdioR1zcj1H6RYRRQQvY-2w=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161538/1vpavic_20231012__0030.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>I hold up the final prototype of Lego’s Polaroid as if to take a photo, with its iconic “OneStep” and “Polaroid Land Camera” badges on display.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/b_pKsGIA-xY-ieAh3yIb-ScSkyM=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161540/1vpavic_20231012__0016.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/w4xpF4R4stZi3RAd4AjxNMHW4hM=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161540/1vpavic_20231012__0016.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/huWMQcOyqJiQ7DUX2Mcghfv8vWU=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161540/1vpavic_20231012__0016.jpg" alt="The Lego Ideas Polaroid OneStep in final prototype form, with its film bay open, sitting alongside its brick-built film box and photocards." src="https://cdn3.vox-cdn.com/thumbor/huWMQcOyqJiQ7DUX2Mcghfv8vWU=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161540/1vpavic_20231012__0016.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The Lego Ideas Polaroid OneStep in final prototype form, with its film bay open, sitting alongside its brick-built film box and photocards.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/-tX4zBc8N3fkd9B5dFlu74RP_ao=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161746/lego_polaroid_comparo.gif" data-sourcetablet="https://cdn0.vox-cdn.com/thumbor/4SowmjtScUNFi4kufnXxpa0mzqo=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161746/lego_polaroid_comparo.gif" data-sourcemobile="https://cdn2.vox-cdn.com/thumbor/9OLgumhKvRbdtJ94ITnskIEIUT0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161746/lego_polaroid_comparo.gif" alt="Here’s a GIF we made that flips back and forth between the Lego Polaroid and the one that takes film. You can catch some of the minute visual differences, like the deeper, wider film bay and wide-angle viewfinder on the original." src="https://cdn.vox-cdn.com/csk/bc589934-607c-4e08-aa73-839ad29e1951/2f7ff42a-850b-48b6-a4a9-9a9d63fecacb/images/placeholder.png">
    	<label tabindex="0">
			<div>
				
				<p><span>Here’s a GIF we made that flips back and forth between the Lego Polaroid and the one that takes film. You can catch some of the minute visual differences, like the deeper, wider film bay and wide-angle viewfinder on the original.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/PCFj3M-VFo4EjBLOiIBr5GTUcCE=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163112/1vpavic_20231012__0055.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/FrGds2ZAV_GtJzvxuw_zrNqG910=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163112/1vpavic_20231012__0055.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/KaSnlw0T_UmMrkKWSUdnIiYvS-M=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163112/1vpavic_20231012__0055.jpg" alt="The set’s brick-built photo box has printed parts — no stickers — and comes with “photos” of Polaroid founder Edwin Land, Marc’s sister Mia at a cafe in France, and the Lego House." src="https://cdn1.vox-cdn.com/thumbor/KaSnlw0T_UmMrkKWSUdnIiYvS-M=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163112/1vpavic_20231012__0055.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The set’s brick-built photo box has printed parts — no stickers — and comes with “photos” of Polaroid founder Edwin Land, Marc’s sister Mia at a cafe in France, and the Lego House.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn3.vox-cdn.com/thumbor/u_4A4uu5k0v2ZoL3B-_Rg2IKUoM=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161548/1vpavic_20231012__0037.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/jsfsH_7YxVwrIO-o6sptSuDX0FQ=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161548/1vpavic_20231012__0037.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/19S4bHku4br2VxeNZCCOmzYkWFQ=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161548/1vpavic_20231012__0037.jpg" alt="Looking through the Lego viewfinder, we get a narrow look at the Lego-ified face of the fan designer’s sister." src="https://cdn0.vox-cdn.com/thumbor/19S4bHku4br2VxeNZCCOmzYkWFQ=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161548/1vpavic_20231012__0037.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Looking through the Lego viewfinder, we get a narrow look at the Lego-ified face of the fan designer’s sister.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		Each project moving through the Lego Ideas program starts the same way: a Lego designer tries to replicate the original fan creation in the real world to see what works and what doesn’t. In Lego’s Billund, Denmark, headquarters, designers walk through a room called the Brick Library that’s filled with veritable supermarket aisles of parts sorted by color and shape. They can take whatever they need.
</p>


					<p>
		<a href="https://brickset.com/sets/designer-Jordan-Scott/">Jordan David Scott</a>, a creative lead in the Lego Ideas program, says that creating a true Lego set <em>isn’t </em>a straightforward series of steps. Though Marc’s Polaroid set was well built, every set must go through stringent quality control that inevitably leads to changes. To pass, even Lego’s seasoned designers head back to the drawing board to swap out parts again and again.&nbsp;
</p>


					<p>
		In addition to production, packing, packaging, and marketing, Lego has a host of teams that work directly with designers, including a function testing department, a safety department, an engineering department, and a textile department. There’s even a dedicated “building instructions” department and a “model quality” team, each of which sits with designers and watches them build. They make sure the build process stays fun, the instructions make sense, and the model stays stable enough that there’s little chance it breaks while you build it. “It’s like the final exam of the design process,” says Lego designer <a href="https://brickset.com/sets/designer-James-May">James May</a>.&nbsp;
</p>


					<p>
		While some designers think in bricks, May tells me he thinks in Lego’s internal design tool. While it’s similar to fan-facing tools like <a href="https://www.bricklink.com/v3/studio/download.page">BrickLink Studio</a>, which lets designers automatically snap together digital bricks, the internal Lego version is linked to the company’s other projects and systems. That means he can collaborate with fellow designers, see which new Lego elements are becoming available, and even budget how much pieces will cost and how many bags of parts will be created and boxed in the final set.&nbsp;
</p>


					<p>
		May is the primary builder on the Polaroid set, and that means building the Lego camera many, many times over a matter of months — some digitally, some picture-perfect physical sets, and some physical models in random colors just for stress testing. One gets baked in an oven to simulate the set sitting out in a particularly hot country; another gets poked by a robot arm to test its moving parts. May says he doesn’t keep track of “drafts” because each set is a <a href="https://en.wikipedia.org/wiki/Ship_of_Theseus">Ship of Theseus</a>, the same design constantly evolving as pieces are swapped out to satisfy Lego’s standards.
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/GPqRjxqEOGLh5YO2VUZ422drm9U=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163619/lego_polaroid_random_colors_4_3.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/igmrjWgz-m4JtKBtD66c7tiaYOY=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163619/lego_polaroid_random_colors_4_3.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/RuSRoen9jHazIki2vNQ3GxNDqrg=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163619/lego_polaroid_random_colors_4_3.jpg" alt="This particular red, gray, black, white, and yellow Lego Polaroid prototype was made from the colors that were on hand." src="https://cdn0.vox-cdn.com/thumbor/RuSRoen9jHazIki2vNQ3GxNDqrg=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163619/lego_polaroid_random_colors_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>This particular red, gray, black, white, and yellow Lego Polaroid prototype was made from the colors that were on hand.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/ulUK4U6FxPTdwJouMBsyHrP-lTc=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163629/lego_polaroid_internal_prototypes_4_3.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/-zUgm7_ngfElCar1L-dSP7e5a0E=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163629/lego_polaroid_internal_prototypes_4_3.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/_wQDM8w8SeHDxdrlFceKot7aMgM=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163629/lego_polaroid_internal_prototypes_4_3.jpg" alt="Lego went through many iterations of the Polaroid set’s internal mechanism, seen here without any housing — there are a few different ways to let the shutter button release a spring-loaded lever." src="https://cdn3.vox-cdn.com/thumbor/_wQDM8w8SeHDxdrlFceKot7aMgM=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163629/lego_polaroid_internal_prototypes_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Lego went through many iterations of the Polaroid set’s internal mechanism, seen here without any housing — there are a few different ways to let the shutter button release a spring-loaded lever.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/OG7gM8izSOUXqP71oT-D8EqX9vs=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161562/1vpavic_20231012__0049_4_3.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/kDSKxe15J5qhvcc6kdMqnguXCXQ=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161562/1vpavic_20231012__0049_4_3.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/hT3y-1yspYivGqQSVi8RrFZtx7M=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161562/1vpavic_20231012__0049_4_3.jpg" alt="The final mechanism uses a simple orange and green linkage to raise a blue tooth, which lets the arm shoot forward. Here, you can see it encased in the Polaroid’s pyramid-like rear shell." src="https://cdn0.vox-cdn.com/thumbor/hT3y-1yspYivGqQSVi8RrFZtx7M=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161562/1vpavic_20231012__0049_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The final mechanism uses a simple orange and green linkage to raise a blue tooth, which lets the arm shoot forward. Here, you can see it encased in the Polaroid’s pyramid-like rear shell.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/qGhLuJKSeMtEaz_xXpEQFsUSF3c=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163123/1vpavic_20231012__0059_4_3.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/ueq9BDoqDPQphTZmUc9zUrdUzW8=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163123/1vpavic_20231012__0059_4_3.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/SdGVn-7LEr59IoKaITwra5UTwKM=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163123/1vpavic_20231012__0059_4_3.jpg" alt="The result: when you press the shutter button, the Lego Polaroid shoots a photo out of its slot." src="https://cdn1.vox-cdn.com/thumbor/SdGVn-7LEr59IoKaITwra5UTwKM=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163123/1vpavic_20231012__0059_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The result: when you press the shutter button, the Lego Polaroid shoots a photo out of its slot.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		In the case of the Lego Polaroid, one particular challenge kept May and his colleagues swapping out parts: a request from the CEO of Polaroid himself.&nbsp;
</p>


					<p>
		When Lego came calling, Polaroid CEO Oskar Smolokowski didn’t hesitate. “I’m a (casual) Lego fan building a few sets a year so it wasn’t really a decision I had to think about!” he tells me via email. He accepted Lego’s offer almost on the spot, he says, while dodging my question about how much Lego did or didn’t pay for the license. “We didn’t feel the need to negotiate anything <span> it felt fair and win-win to us,” he writes.</span>
</p>


					<p>
		But Polaroid’s CEO did have one ask: he wanted the Lego Polaroid’s big red shutter button to <em>do</em> <em>something</em>. “I really wanted the camera to be as much of a camera as possible,” he recalls, and the CEO brought up this idea in the very first Lego / Polaroid kickoff meeting, remembers Scott.
</p>


					<p>
		Lego wasn’t quite ready to commit to that. “I said yeah… we can look into it?” Scott recalls. Marc’s design could already eject a photo by turning a dial, and Lego had already successfully replicated that. The dial would definitely be Plan B.
</p>


					<p>
		But Scott decided to challenge May, who had previously worked on <a href="https://youtu.be/HRrHf57TMa8?si=Jga7xnV53wVzDuIi&amp;t=134">the moving Lego Typewriter</a>, to make the button work. With help from other teams that specialize in Lego’s mechanism-friendly Technic bricks, they landed on using a pair of tiny rubber bands connected to a sliding arm to eject the photo.&nbsp;
</p>


						</div>
						
						<div>
					<p>
		“It definitely didn’t work the first time,” says Scott. “I don’t know how many versions James went through.” They had to tinker with tiny details to make the mechanism work — making the contraption half a Lego plate thicker here or moving it over by one brick’s width. “A lot of it came down to nuances,” says Scott, “and all these subtleties you wouldn’t necessarily think of like which bricks are better at <em>stopping</em> it from firing out.”
</p>


					<p>
		In the end, the team attached the shutter button to an internal lever that, when pushed, raises an internal tooth, which releases a spring-loaded carriage that pushes the photo out with a satisfying <em>chonk</em> each time.
</p>


					<p>
		“Everyone came together to make this happen, and it’s so much better,” says Scott, adding that colleagues were wowed by the action (and sound) when they came by.
</p>


					<p>
		They also had to make sure the button worked no matter how many times someone pressed it. “A lot of the feedback we got was that the function just isn’t triggering after several hundred or several thousand times, it’s failing,” he adds. The function department even rigged up a robot to simulate pushing the shutter button tens of thousands of times — one which, I’m unreasonably pleased to say, uses Lego to test Lego: 
</p>


					<div><p><iframe src="https://www.youtube.com/embed/UR1Ul9Yn7Rw?rel=0" allowfullscreen="" scrolling="no" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;"></iframe></p></div>

			</div>
		
			<div>
					<p>
		The other half of the Polaroid button challenge: figuring out how to create a Polaroid-like “picture” worthy of being ejected from the model. Originally, they tried a flat tile like Marc did but decided it wasn’t right.&nbsp;
</p>


					<p>
		“It looked Lego, it felt Lego, but it didn’t feel like a Polaroid photo because you want it to be thin; it also meant we couldn’t print on the back because you need the tube side; it caused a lot of issues in production because of warping,” says Scott.&nbsp;
</p>


					<p>
		But Lego’s textile department came to the rescue: “We found this card, could we use this for anything?” Scott remembers them asking. It was a thin sheet of matte polypropylene plastic — a “foil” — that had only been used a couple of times before in Lego sets, most prominently <a href="https://jaysbrickblog.com/reviews/review-lego-80109-lunar-new-year-ice-festival/#:~:text=this%20photobooth%20for%20minifigures">in this Chinese Lunar New Year Ice Festival photobooth</a> where minifigures can pop their heads through. It was flexible (though you can’t <em>quite</em> “<a href="https://www.cnn.com/2004/TECH/ptech/02/17/polaroid.warns.reut/">shake it like a Polaroid picture</a>”), and it could be easily printed on both sides.&nbsp;
</p>


					<p>
		So, Lego graphics designer Matthew Parsons, who typically works for the Lego City team, embedded himself in the company’s textile department to help figure out the foils. A photographer himself, he jumped at the chance to be part of the Polaroid project, and he designed the three Easter egg photocards that come in every box.
</p>


					<p>
		Lego got one of the images, choosing to depict the <a href="https://en.wikipedia.org/wiki/Lego_House_(Billund)">Lego House</a>; Polaroid chose an iconic photo of its founder, Edwin Land; and Marc decided to thank his inspirations: the city of La Rochelle, France, where he cultivated his love of Lego and first prototyped the set, and his sister Mia, whose instant photography hobby brought him the idea. You can see some of Parsons’ sketches in our embedded gallery.
</p>


					<p>
		One of the last challenges was safety. Unlike actual Polaroids, the foils have rounded corners rather than sharp points. But even then, Lego’s safety department had to continually test the launcher during the monthslong project to ensure <em>other </em>unspecified objects couldn’t be dangerously blasted. With just a few weeks left in the schedule, they told the team they’d found one more undesirable object that someone could potentially launch out of the camera. “So that was another week of testing and building,” Scott says.
</p>


					<p>
		The final design ensures four Lego studs barely brush against the photo every time it ejects thanks to two sets of locking hinges that hold them at just the right angle. Inclined slopes on the edge of the film slot make the photo curve slightly upward as it ejects, too. Put it all together, press the button, and — <em>chonk</em> — the photo extends just far enough for you to easily grab, almost exactly an inch, instead of shooting all the way out. 
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/bbyE4HA2epXCiwAsfBHBNd9HKeY=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163655/first_digital_sketch_lego_polaroid.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/5a7nifd85hGX-fmL4bFVWXPqJCs=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163655/first_digital_sketch_lego_polaroid.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/W96THeP6-UmoV2xhEU-3ln2n9jw=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163655/first_digital_sketch_lego_polaroid.jpg" alt="The first digital mockup of the Lego Polaroid shows how it resembled Marc’s design. You see colorful highlights like the red shutter button, gold film ejector, and rainbow stripe on the film box — before any stickers or printing are added." src="https://cdn3.vox-cdn.com/thumbor/W96THeP6-UmoV2xhEU-3ln2n9jw=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163655/first_digital_sketch_lego_polaroid.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The first digital mockup of the Lego Polaroid shows how it resembled Marc’s design. You see colorful highlights like the red shutter button, gold film ejector, and rainbow stripe on the film box — before any stickers or printing are added.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn3.vox-cdn.com/thumbor/DYPK4cfN7OlFQ2KUcVpH5-27TeA=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163646/lego_polaroid_sketches_final.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/rcrOMGK4JxGGOQoF0ygORsv8aiI=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163646/lego_polaroid_sketches_final.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/kvuTFpVCrlN2mwU2Ur1xVhGWG50=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163646/lego_polaroid_sketches_final.jpg" alt="Lego graphics designer Matthew Parsons’ sketches and art for one included photocard, starting in black and white, then color, then refined." src="https://cdn0.vox-cdn.com/thumbor/kvuTFpVCrlN2mwU2Ur1xVhGWG50=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163646/lego_polaroid_sketches_final.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Lego graphics designer Matthew Parsons’ sketches and art for one included photocard, starting in black and white, then color, then refined.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/BjWJv8j40H6Xyt4KaCj53aApw2A=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163669/lego_polaroid_final_look.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/0SzaxksBnfQ_1kizZEJmtcbZAmg=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163669/lego_polaroid_final_look.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/JoZYJFvdaDWOJNkVedePGKUzSI0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163669/lego_polaroid_final_look.jpg" alt="A final image of the set, provided by Lego, with all the decorations and printing complete." src="https://cdn3.vox-cdn.com/thumbor/JoZYJFvdaDWOJNkVedePGKUzSI0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163669/lego_polaroid_final_look.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>A final image of the set, provided by Lego, with all the decorations and printing complete.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/S0vPpsOvNhuhjuENT9IMQWYrzrY=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163110/1vpavic_20231012__0058_4_3.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/Ddc5ph2rO4A3YgMsMY9z2KarjCE=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163110/1vpavic_20231012__0058_4_3.jpg" data-sourcemobile="https://cdn2.vox-cdn.com/thumbor/-AEXzxXNpSvVCh-pqSyRFRv5pDE=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163110/1vpavic_20231012__0058_4_3.jpg" alt="The final prototype photocards in real life, sitting against the Lego camera." src="https://cdn2.vox-cdn.com/thumbor/-AEXzxXNpSvVCh-pqSyRFRv5pDE=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163110/1vpavic_20231012__0058_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The final prototype photocards in real life, sitting against the Lego camera.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/Lt7xq_y74pFU9jJw_DXiZoyiwGw=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163676/lego_polaroid_studded_vs_semi.jpg" data-sourcetablet="https://cdn0.vox-cdn.com/thumbor/sC5nHTD-PwpOKsrOyPRDARifMv4=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163676/lego_polaroid_studded_vs_semi.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/lif7CWHndiHFYnXIxLRvqtCOlUo=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163676/lego_polaroid_studded_vs_semi.jpg" alt="Lego tried both smooth and partially studded backs for the Ideas Polaroid OneStep but settled on fully studded in the end.  " src="https://cdn3.vox-cdn.com/thumbor/lif7CWHndiHFYnXIxLRvqtCOlUo=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163676/lego_polaroid_studded_vs_semi.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Lego tried both smooth and partially studded backs for the Ideas Polaroid OneStep but settled on fully studded in the end.  </span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/I2_WN1WfUd8qLoRD8E1ZeCjSxSw=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161773/1vpavic_20231012__0022.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/r4xiZN9k5NuBwsgkCqIm_Nj3_5k=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161773/1vpavic_20231012__0022.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/ZI9aWeQoxgVSu-ggUrHZGsbCsP8=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161773/1vpavic_20231012__0022.jpg" alt="And here’s the real Polaroid’s textured back and the Lego Polaroid’s all-studded back, for comparison." src="https://cdn0.vox-cdn.com/thumbor/ZI9aWeQoxgVSu-ggUrHZGsbCsP8=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161773/1vpavic_20231012__0022.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>And here’s the real Polaroid’s textured back and the Lego Polaroid’s all-studded back, for comparison.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		If you’ve ever used an old Polaroid camera, you know that photos tend to pop out quite a bit more than an inch, accompanied by a stretchy black film to slow their roll. It’s not the only way the Lego Polaroid diverges from the real thing, of course. You won’t find the OneStep SX-70’s trademark camera strap, or the film bay’s stickers with the manufacturer’s warranty support telephone number, or an optional green button that shipped in some markets, things Marc says he asked for when they solicited his input but says understandably didn’t make the cut.&nbsp;
</p>


					<p>
		(He also says he would have preferred a smooth, tiled back instead of studs — but Lego did try that, and both Polaroid and Lego agreed they preferred the studded look. And you can swap the “OneStep” sticker for a “1000” sticker, which is how some versions looked.)
</p>


					<p>
		Overall, I’m wildly impressed by the result. I bought the actual 1977 camera over a year ago just because <a href="https://www.theverge.com/2022/10/25/23423120/lego-polaroid-land-camera-sx-70-replica-film-ideas">I knew this set was coming</a>, and I sometimes mistake one for the other on my office shelf. The size, shapes, and weight are incredibly close — both weigh approximately one pound, with the Lego set’s nose (and lens) mostly just protruding a little bit more than the actual camera. The body is also a tad narrower.&nbsp;
</p>


			</div>
		
				<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/pPhGNxj4t0-JK1Ekg4cLovKU3ro=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161494/polaroid_of_polaroid_1.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/VSksGZs_sb6OsvH5i5awTKB5wkg=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161494/polaroid_of_polaroid_1.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/7RzM-MGnHz5jzsMlaSvXIyc3ZXE=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161494/polaroid_of_polaroid_1.jpg" alt="We shot some actual Polaroids of the Lego Polaroid with the Polaroid camera that it’s based on. Here it is in front of the Golden Gate Bridge in San Francisco and the historic Dutch Windmill." src="https://cdn1.vox-cdn.com/thumbor/7RzM-MGnHz5jzsMlaSvXIyc3ZXE=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161494/polaroid_of_polaroid_1.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>We shot some actual Polaroids of the Lego Polaroid with the Polaroid camera that it’s based on. Here it is in front of the Golden Gate Bridge in San Francisco and the historic Dutch Windmill.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
			<div>
					<p>
		There actually is a substantial nod to the missing camera strap on the back of the set, too, with openings for a strap cleverly sculpted by the gap in <a href="https://www.bricklink.com/v2/catalog/catalogitem.page?P=39613">heart-shaped Lego plates</a> — ones that meld into the camera’s smooth corners thanks to a semi-advanced build technique. (If you’re a big Lego fan, you’ll be familiar with the phrase “Studs Not On Top.”) The film bay eject lever, film counter, and flash hot shoe are all represented with gaps or bulges, too, and the mechanism inside the black-and-white shell is a hidden rainbow of color, using all the same hues as the rainbow stripe up front. 
</p>


					<p>
		(It inspired me to hunt down a copy of the classic rainbow stripe for the right rail of this <em>Verge</em> story, in fact — Polaroid doesn’t really use the deep pink color anymore, and they had to dig it up at my request.)
</p>


					<p>
		The Lego team even splurged on a custom red plate with a white edge to represent Polaroid’s shutter button, plus two printed tiles for the brick-built film pack that reads “Polaroid” and “Time-Zero Supercolor SX-70 Land Film.”
</p>


					<p>
		I haven’t yet gotten to the single most satisfying step in the build, the one Lego saves for last: the iconic Polaroid rainbow stripe on this camera isn’t a sticker. It’s a sideways stack of 1x6 plates and <a href="https://www.newelementary.com/2018/12/lego-35459-1x3-inverted-tile-hole.html">1x3 inverted hole tiles</a> in colors that match up almost perfectly to Polaroid’s original hues, held together by thin Lego pipes. It’s great — but it made me wonder why Lego still does use some other stickers in this design.
</p>


					<p>
		Many Lego fans are vocal about their preference for printed parts over stickers, and there’s always annoyance when a set aimed at adults uses any stickers at all. Here, your “Polaroid Land Camera,” “OneStep” or “1000,” and the exposure dial’s white and black EV marks are all sticky labels, not printed tiles.
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/ox0n4tHzoSMw_fKMDPpjWPCBa4Q=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163120/1vpavic_20231012__0060.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/OoTfdVle2HB2d7RV2wnyZAKCCrU=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163120/1vpavic_20231012__0060.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/dliybiQSMaoglbsFQxFUeGgDiZ0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163120/1vpavic_20231012__0060.jpg" alt="The brick-built rainbow stripe that adorns the front of the Lego camera. Here it is all together, close enough so you can see the gaps that show how it was made." src="https://cdn1.vox-cdn.com/thumbor/dliybiQSMaoglbsFQxFUeGgDiZ0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163120/1vpavic_20231012__0060.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The brick-built rainbow stripe that adorns the front of the Lego camera. Here it is all together, close enough so you can see the gaps that show how it was made.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn3.vox-cdn.com/thumbor/KwlV5S4eJ0Va8k_W8HV4Y73WQbc=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163121/1vpavic_20231012__0057.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/9Mvj_ylUu05URHiE5Ne0cLx8Rv0=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163121/1vpavic_20231012__0057.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/TWlymZTAfCqnF65_OwfyJyRrONI=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163121/1vpavic_20231012__0057.jpg" alt="Here’s a look at the rainbow stripe broken apart so you can see the Lego studs on the green plates inside." src="https://cdn1.vox-cdn.com/thumbor/TWlymZTAfCqnF65_OwfyJyRrONI=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163121/1vpavic_20231012__0057.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Here’s a look at the rainbow stripe broken apart so you can see the Lego studs on the green plates inside.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/_7Vu9TQFMz_rsBJLnaOmdaESw_o=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161577/zoom_with_lego_director_2.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/nfNyxoTtYbkrSoo7z2EpU350kQ8=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161577/zoom_with_lego_director_2.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/J7Lqp-6VThj_ipL5ngb_ftbEhOg=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161577/zoom_with_lego_director_2.jpg" alt="On a video call with&nbsp;The Verge, Lego Ideas creative lead Jordan David Scott holds up the key to the rainbow stripe in front of his eyes — a pair of inverted blue tiles with holes inside." src="https://cdn3.vox-cdn.com/thumbor/J7Lqp-6VThj_ipL5ngb_ftbEhOg=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161577/zoom_with_lego_director_2.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>On a video call with&nbsp;The Verge, Lego Ideas creative lead Jordan David Scott holds up the key to the rainbow stripe in front of his eyes — a pair of inverted blue tiles with holes inside.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		To my great surprise, Scott was willing to explain how Lego makes those kinds of choices.
</p>


					<p>
		Lego’s picker system requires each printed piece to have its own unique storage bin, so rather than continually opening more warehouses, Lego limits how many custom parts designers can introduce each year.&nbsp;
</p>


					<p>
		“We can’t make everything decorated. We can’t change every brick into every color,” Scott says. “Otherwise the portfolio would just explode in complexity, so we have teams that manage the complexity level.”&nbsp;
</p>


					<p>
		And those teams came up with one simple idea to stem the tide of complexity: “frames.”&nbsp;
</p>


					<p>
		Want a part in a different color? That costs designers a frame. A new piece? Spend some frames. Bring back an old out-of-print piece? That’s a frame, too. Every year, design leads like Scott are given a limited number of frames that they can spend on their entire portfolio for physical pieces that aren’t readily at hand. “If I have five products or 10 products coming out, I need to allocate where those frames go,” says Scott.&nbsp;
</p>


						</div>
						
						<div>
					<p>
		Doing so is “a bit of a puzzle” to figure out which sets will need lots of frames — the <a href="https://www.theverge.com/23909975/lego-animal-crossing-pictures-price-release-date-sets">new <em>Animal Crossing</em> sets</a> with their custom minifigures probably ate a few — and which ones can be built mostly out of preexisting parts.&nbsp;
</p>


					<p>
		Designers also try to save frames by sharing brand-new bricks with other teams, giving them a heads-up that they might come in handy for other sets, too. Some of that happens automatically: “When someone puts in an order for a particular color change, we can see it showing up in the library of digital bricks,” says Scott.
</p>


					<p>
		Some of it is designers intentionally pooling their resources: “If Ninjago are making something we could use, we kind of have a dialogue and say, ‘Oh, we can use this as well, that would be great, so maybe we need to get you a frame or something to share it.’” 
</p>


					<p>
		Designers always want more frames for their sets, May says. But he explains those constraints are just part of the process. When designers don’t have as many frames as they’d like, they have to get creative — just like any other Lego fan.&nbsp;
</p>


					<p>
		For the Lego Polaroid, the team spent a frame on the red and white shutter button — which could now appear in any number of other sets — and two frames for the decorations on the film pack, which are obviously exclusive to Polaroid. Scott planned to spend frames on ejecting photos, too: internally, he and his fellow designers were excited about making a new 8x6 printed photo tile, until the foils came along.
</p>


					<p>
		Polaroid’s CEO remembers one more thing that didn’t make the cut: “I think the only other thing I may have mentioned was a little Edwin Land figure,” he says, referencing the founder of Polaroid. “That would’ve been awesome.” Instead, Land is on one of the three photocards that come with the set.
</p>


			</div>
		
				<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/R2zLBoEnl9-uCdFDS1D49siHaUQ=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161722/real_polaroids_polaroid_2.jpg" data-sourcetablet="https://cdn0.vox-cdn.com/thumbor/R6i5QHekqIE_nNEXFtDTL3hSS-A=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161722/real_polaroids_polaroid_2.jpg" data-sourcemobile="https://cdn2.vox-cdn.com/thumbor/gJeDDOgBIl74dD2FUlIjgqERtP0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161722/real_polaroids_polaroid_2.jpg" alt="Two more real, unedited Polaroids we shot of the Lego Polaroid with a Polaroid OneStep SX-70 — the camera it’s based on. One is me, holding the Lego Polaroid up to my eye facing the camera. The other is the Polaroid in its native habitat (on a railing next to the Camera Obscura near San Francisco’s Cliff House, with the ocean in the background)." src="https://cdn2.vox-cdn.com/thumbor/gJeDDOgBIl74dD2FUlIjgqERtP0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161722/real_polaroids_polaroid_2.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Two more real, unedited Polaroids we shot of the Lego Polaroid with a Polaroid OneStep SX-70 — the camera it’s based on. One is me, holding the Lego Polaroid up to my eye facing the camera. The other is the Polaroid in its native habitat (on a railing next to the Camera Obscura near San Francisco’s Cliff House, with the ocean in the background).</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
			<div>
					<p>
		“Just thinking about the fact that because I submitted an idea like a year and a half ago, that now so many people in the community are going to have a Lego Polaroid set — it’s just insane,” says Marc.
</p>


					<p>
		I get the sense, though, that the process wasn’t <em>entirely</em> a dream come true. Lego mostly took his idea and ran with it. It never flew him to Denmark to meet the designers in person, something he says he would have loved, nor did it ship him prototypes during the process; he got to see it on a video call. He assured me it wasn’t a big deal — he’ll get 10 free copies after all.&nbsp;
</p>


					<p>
		Lego demands a high level of secrecy, too: he felt he couldn’t tell his own Lego-loving brother for <em>months</em>. Or his mom. Or his sister Mia, who may not quite know what she’s gotten into. “Like, I don’t think she understands that she’s going to be in the Lego set, you know, mass-produced,” says Marc. (He says he did ask permission to “steal her likeness,” and she was “totally cool” with it hypothetically being in Lego.)
</p>


					<p>
		But judging by their Lego Ideas page, Marc and his brother Nick don’t seem to have been put off one bit. In September, their “Minibrick Productions” submitted a brick-built version of <a href="https://ideas.lego.com/projects/7a6d8d85-093d-4f6c-ab7f-48b405115d60/official_comments#content_nav_tabs">the <em>Interstellar</em> space shuttle</a> that took just weeks to become a Lego staff pick and has crossed 6,000 votes. <a href="https://ideas.lego.com/projects/53056645-ba8d-4a35-a219-77d30aa6f733">A set based on Blackpink’s music video for “Lovesick Girls”</a> hit 5,000 votes in August.
</p>


					<p>
		If you’re looking to follow in their footsteps with a Lego set of your own, here’s Marc’s advice: design it like a product you’d want to sell. “Showcase its play features like you’d showcase a final product.” And — though this could be <a href="https://en.wikipedia.org/wiki/Survivorship_bias">survivorship bias</a> — he says you have to keep trying, pointing to his many previous rejections as evidence.&nbsp;
</p>


					<p>
		“I think you really just have to keep going and continue with that spark of hope, that maybe one of your future projects will become an actual set.”
</p>


			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fly Postgres, Managed by Supabase (199 pts)]]></title>
            <link>https://supabase.com/blog/postgres-on-fly-by-supabase</link>
            <guid>38653212</guid>
            <pubDate>Fri, 15 Dec 2023 11:52:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://supabase.com/blog/postgres-on-fly-by-supabase">https://supabase.com/blog/postgres-on-fly-by-supabase</a>, See on <a href="https://news.ycombinator.com/item?id=38653212">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p><img alt="Fly Postgres, managed by Supabase" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=640&amp;q=75 640w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=750&amp;q=75 750w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=828&amp;q=75 828w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=1080&amp;q=75 1080w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=1200&amp;q=75 1200w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=1920&amp;q=75 1920w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=2048&amp;q=75 2048w, https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=3840&amp;q=75 3840w" src="https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-supafly%2Ffly-postgres-thumb.png&amp;w=3840&amp;q=75"></p>
<p>We're launching Fly Postgres, a managed Postgres offering by Supabase and <a href="http://fly.io/">Fly.io</a>.</p>
<p>Fly Postgres databases launch on Fly.io's edge computing platform from any of their 37+ locations. You get everything you expect from a Supabase managed database:</p>
<ul>
<li>a full-featured Postgres database with <a href="https://github.com/supabase/postgres">over 40+ extensions</a></li>
<li><a href="https://github.com/pgvector/pgvector/">pgvector</a> support for <a href="https://supabase.com/docs/guides/ai">Vector/AI workloads</a></li>
<li><a href="https://supabase.com/blog/supavisor-postgres-connection-pooler">Supavisor</a>, our Postgres connection pooler</li>
<li>Daily backups and point-in-time recovery</li>
<li><a href="https://supabase.com/docs/guides/platform/branching">Branching</a>, <a href="https://github.com/supabase/supabase-grafana">observability</a>, and migrations</li>
<li>A <a href="https://supabase.com/blog/studio-introducing-assistant">dashboard</a> for managing your database</li>
<li>Auto-generated Data APIs:<!-- -->
<ul>
<li><a href="https://supabase.com/docs/guides/api">REST</a> (using <a href="https://postgrest.org/">PostgREST</a>)</li>
<li><a href="https://supabase.com/docs/guides/graphql">GraphQL</a> (using <a href="https://github.com/supabase/pg_graphql/">pg_graphql</a>)</li>
</ul>
</li>
</ul>
<p>This is deployed within the Fly infrastructure, making it the fastest Postgres database for your data intensive applications deployed on Fly.</p>
<h2 id="managing-expectations">Managing expectations</h2>
<p>Before you get too excited, this will be a progressive rollout. It turns out that building inter-company integrations is a lot of work when you factor in billing, support handoff, and educating Supabase staff on how to understand <a href="https://fly.io/blog/carving-the-scheduler-out-of-our-orchestrator/">sandwich analogies</a>.</p>
<p>We've been working with a few early testers and we have some bugs to iron out. You can <a href="https://forms.supabase.com/fly-postgres">sign up for the waitlist</a> if you want to help with testing. We'll accept more testers next month, and we'll communicate more release timelines as soon as we're confident that your data is safe.</p>
<h2 id="supabase--fly--supafly">Supabase + Fly = SupaFly?</h2>
<p>We're excited about what this partnership means for 2024. Namely, distributing Postgres across the planet. The Firecracker VM gives us some neat ideas for Postgres. Integrating with Fly also puts a bunch of easy-to-spin-up compute resources right next to the database. That sounds like fun.</p>
<h2 id="managed-vs-unmanaged-postgres">Managed vs unmanaged Postgres</h2>
<p>Fly's current Postgres offering is <a href="https://fly.io/docs/postgres/getting-started/what-you-should-know">unmanaged</a>. This means that you're responsible for handling scaling, point-in-time backups, replication, major version upgrades, etc. We'll run Fly's <em>managed</em> Postgres, which means that we do all that for you, and you can concentrate on building.</p>
<p>The managed service is built with the <a href="https://fly.io/docs/reference/extensions_api/">Fly extension API</a> (also used by <a href="https://fly.io/docs/reference/redis/">Fly Redis</a>).</p>
<p>Testers can launch a Postgres database using the <code>fly extensions</code> command:</p>
<div><p><code><br><div><p><span>_<!-- -->10</span></p><p><span>fly extensions supabase create</span></p></div><br></code></p></div>
<p>Once the service is stable, it will be swapped for the <code>postgres</code> namespace:</p>

<p>With Fly Postgres, the database is deployed within Fly infrastructure leading to a much lower latency for data heavy applications.</p>
<h2 id="under-the-hood">Under the hood</h2>
<p>Let's dig into the implementation.</p>
<h3 id="working-with-fly-machines"><strong>Working with Fly machines</strong></h3>
<p>Fly Postgres is built on top of <a href="https://fly.io/docs/machines/">Fly machines</a>. Machines are light-weight Firecracker VMs. The Machines API offers substantial control over an application's lifecycle. They can be suspended during inactivity and resumed within a couple of seconds whenever a new request arrives.</p>
<p>We built <a href="https://github.com/supabase/fly-admin">fly-admin</a>, a Typescript wrapper to simplify our interaction with the Fly API. Supabase bundles a few extra services into Postgres, so we prepared a single Docker image which we can pass to the Fly Machines API. Our current build process outputs an AMI for AWS using <a href="https://www.packer.io/">Packer</a>. We re-use parts of that pipeline to build an <a href="https://github.com/supabase/postgres/tree/develop/docker/all-in-one">All In One Image</a>. This image has all the services to run a Supabase project within a single Docker container.</p>
<h3 id="move-to-multi-cloud"><strong>Move to multi-cloud</strong></h3>
<p>With this launch, Supabase is officially multi-cloud. We deliberately avoided using AWS's managed services when building Supabase to simplify our multi-cloud transition. These transitions are never simple - even the base primitives offered between cloud providers can vary significantly.</p>
<p>For example, Fly Machines offer a simple method for suspending a VM when it's not in use, transparently resuming it within seconds. This simplifies the process of pausing inactive databases. There is no direct primitive on AWS to achieve this.</p>
<p>On the other hand, we had to work around a few AWS primitives that Fly doesn't provide. Fly machines don't have network-attached storage, so we treat any data in Fly volumes as ephemeral. We run physical backups for all projects running on Fly using WAL-G. Database changes are continuously streamed to S3. When there is a host or volume corruption, we restore the project to a new Fly host using the latest data in S3.</p>
<p>To capture host issues on AWS, we listen to <a href="https://docs.aws.amazon.com/health/latest/ug/aws-health-concepts-and-terms.html">AWS Health events</a>. For Fly, we send the Machine logs to <a href="https://logflare.com/">Logflare</a> using the <a href="https://github.com/superfly/fly-log-shipper">fly-log-shipper</a>.</p>
<p>In addition to publishing images in AWS's container registry, we publish the All In One image to Fly's Docker registry. This improved the reliability and performance of project launches on Fly.</p>
<h3 id="building-the-fly-extension"><strong>Building the Fly extension</strong></h3>
<p>Fly has an <a href="https://fly.io/docs/reference/extensions_api/">excellent approach</a> for extending their platform. We added a few routes to our API to provision users and projects and we were on our way.</p>
<p>Fly users can access the Supabase dashboard using their existing Fly credentials. The Supabase API initiates an OAuth flow with Fly to authenticate the user. Our Auth team created a <a href="https://github.com/supabase/gotrue/pull/1261">Fly OAuth provider</a> to make the integration with our API easier.</p>
<h2 id="challenges">Challenges</h2>
<p>We're still working through a few challenges with the Fly team.</p>
<h3 id="support-for-network-restrictions">Support for Network Restrictions</h3>
<p>The <a href="https://supabase.com/docs/guides/platform/network-restrictions">network restrictions</a> feature relies on the container receiving the correct IP of the client connecting to it. With our current setup, the container sees the Fly proxy IP instead. Connections run through the Fly proxy, which exposes the Proxy protocol. Postgres can't use this information directly, but we're looking at <a href="https://github.com/supabase/supavisor">making Supavisor proxy-protocol aware</a>.</p>
<h3 id="backups-within-fly">Backups within Fly</h3>
<p>Fly projects are backed up to AWS S3 as Fly doesn't provide managed Blob storage (yet). This incurs inter-cloud bandwidth fees. Luckily, Fly are working on <a href="https://fly.io/docs/flyctl/extensions-tigris-list/">Blob Storage</a>, watch this space.</p>
<h2 id="getting-started">Getting started</h2>
<p>Sign up for the preview <a href="https://forms.supabase.com/fly-postgres">here</a>, wait till we allowlist your org, and get started with the <a href="https://supabase.com/docs/guides/platform/fly-postgres#quickstart">Quickstart</a> in our docs.</p>
<p>Fly organizations will get one free project. We're still working through some of the finer details on billing, but the pricing will remain relatively unchanged from our current <a href="https://supabase.com/pricing">pricing</a>.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebP is so great except it's not (261 pts)]]></title>
            <link>https://eng.aurelienpierre.com/2021/10/webp-is-so-great-except-its-not/</link>
            <guid>38653110</guid>
            <pubDate>Fri, 15 Dec 2023 11:32:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eng.aurelienpierre.com/2021/10/webp-is-so-great-except-its-not/">https://eng.aurelienpierre.com/2021/10/webp-is-so-great-except-its-not/</a>, See on <a href="https://news.ycombinator.com/item?id=38653110">Hacker News</a></p>
<div id="readability-page-1" class="page">
		<a href="#content">Skip to content</a>

	<div id="boxed-wrapper">
			
							
					
			<header>
				<div>
					<div data-margin-top="0px" data-margin-bottom="0px" data-margin-left="0px" data-margin-right="0px">
			<a href="https://eng.aurelienpierre.com/">

						<!-- standard logo -->
			<img src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/03/signature-logo-petit.png" srcset="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/03/signature-logo-petit.png 1x, https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2020/10/signature-logo-retina.png 2x" width="" height="" alt="Aurélien PIERRE Engineering Logo" data-retina_logo_url="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2020/10/signature-logo-retina.png">

			
					</a>
		</div>		<nav aria-label="Main Menu"></nav>	

<nav aria-label="Main Menu Mobile"></nav>

					</div>
				
			</header>
								
							
				
					
							
			<div aria-label="Page Title Bar">

																							<h2>WebP is so great… except it’s not</h2>

											
																
				</div>

						<main id="main">
				<div id="content">
					<!-- maths detected --><article><div><p>I’m a responsible web designer, and as such, since WordPress (finally) accepts media uploads of <code>image/webp</code> MIME type and since <strong>all</strong> <a href="https://caniuse.com/webp">web browsers</a> newer than september 2020 (even Apple Safari \o/) can display it, I have been moving my <a href="https://photo.aurelienpierre.com/portfolio">photos library</a> to <a href="https://en.wikipedia.org/wiki/WebP">WebP</a>. After all, when you create content, the least you can do is to also provide the smoothest user experience around it.</p>

<p>WebP falls close to magical : lookie those file weights ! <a href="https://www.industrialempathy.com/posts/avif-webp-quality-settings/">15% savings</a> compared to JPEG at same quality ! What are we waiting for ? Google even claims <a href="https://developers.google.com/speed/webp/docs/webp_study">25-34% smaller</a>.</p>

<p>There are dozens of WordPress plugins allowing you to convert your old media library on-the-fly, most of them operating as SaaS (Shit as a Software) and doing the conversion on their own servers, which entitles them to make you pay a ridiculous amount for it, <a href="https://imagify.io/">one of them</a> I’m very unhappy to have actually paid (something about sparing time, which actually led to losing time AND money). All of them claim that their “aggressive” compression factor is safe for 99 % of your pictures.</p>

<p>The most technical ones will go as far as telling you that WebP quality greater than <code>lossy 80</code> is useless for most pictures, sustaining their claim with a glorious <a href="https://groups.google.com/a/webmproject.org/forum/#!topic/webp-discuss/0GmxDmlexek">Google logo</a> encoded at various rates. Because everyone knows shooting logos is the bread and butter of every photographer, especially the Adobe Stock ones. Also, logos have their gradients following an hyper-laplacian distribution like any other natural image<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">[1]</a></sup>. Or maybe not. Who cares about gradients stats anyway ? We are only talking about 2D compression heuristics with entropy and high-frequencies thresholding, after all.</p>

<p>So, while I may have lost all respect for coding monkeys turned into image dudes just because a position opened (and everyone loves pics, right ? They are fun and much easier on the brain than words), especially the internet image dudes, I still fall every time for that silly assumption:&nbsp;people who are supposed to know, actually know. Years pass, I&nbsp;don’t learn : I&nbsp;read docs, I&nbsp;do what they say, I discover it doesn’t work as promised, only then I remember those guys don’t know shit about images. And here I am, loosing faith in humanity one wrong expert at a time.</p>

<p>In my great silliness, I set the <a href="https://wordpress.org/plugins/webp-express/">third and last plugin I&nbsp;tried</a> to the advised <code>lossy 80</code> quality and trigger the batch conversion. I&nbsp;have relative faith in it since it uses server-side <a href="https://en.wikipedia.org/wiki/GraphicsMagick">GraphicsMagic</a> instead of the unfortunate PHP shitstack (GD, Gmagick and the likes) or the laggy HTTP-error connection-timed-out DNS-said-not-today please-retry-later SaaS nonsense.</p>

<p>Everything goes well, until this happens…</p>

<p><a href="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.webp"><img decoding="async" data-orig-src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.webp" alt="" src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.webp"></a></p>

<p>To the non-educated eye, this might look ok, but for a photographer it’s not, and for several reasons. See the posterized ring in the background ? First of all, it’s not graceful, but then it has nothing to do there. This comes from a 16 bits scan of an Ilford Delta 400 film shot with a Mamiya RB 67, that is old school analog medium format at 6×7 cm. The silver halide crystals of the Delta 400 emulsion act as a natural dithering which makes high-frequency compressions more difficult and therefore prevent posterization in smooth areas. So, for any compression algo, managing to posterize a Delta 400 scan is a feat of the wrong kind.</p>

<p>Look at the original JPEG at quality 85 :</p>

<p><a href="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.jpg"><img decoding="async" data-orig-src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.jpg" alt="" src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.jpg"></a></p>

<p>It’s not 100 % clean either, but much better. Granted, this is WebP re-encoding of an already lossy compressed JPEG, so we stack 2 steps of destructive compression. <strong>But</strong> this is what <a href="https://developers.google.com/speed/pagespeed/insights/">Google Page Speed insights</a> encourage you to do and what a shitload of plugins enable you to do, while pretending it’s completely safe. <strong>It’s not.</strong></p>

<p>I have seen a similar effect in other similar pictures : always pictures with large, smooth, gradients in the background, which happens a lot when some punctual-ish light falls off a wall. That’s not something accidental, smooth fall-off are actively built by photographers to create organic-looking backgrounds with just enough of texture to not get boring, yet discrete enough to not draw attention off the foreground/subject.</p>

<p>So, I wondered how bad it was for actual raw photos encoded straight in darktable. Meaning just one step of encoding. Meaning real WebP quality comparison for real-life studio head-shots, which are one of the last things customers are willing to pay actual photographers to do (instead of snapping their own iPhone). Meaning real money for real professionals. Meaning something the image coding douchebags may have not foreseen, because it doesn’t happen in VS Code (or Vim, for that matter).</p>

<p>Let’s start. The following 2 images use <a href="https://en.wikipedia.org/wiki/Floyd%E2%80%93Steinberg_dithering">Floyd-Steinberg dithering</a> in 8 bits, with lossy compression set at 90 for both JPEG and WebP (remember, the experts recommend 80 for WebP). All images below, saved in WebP, use the “photo” image hint of Jeroen Oom’s <a href="https://cran.r-project.org/web/packages/webp/index.html">libwebp</a> 1.2.1. <em>Click on images to open the full-size version, or better : right-click and open them in a new tab</em></p>

<p>JPEG, lossy, 90 : 227 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>JPEG, lossy, 85 : 184 kiB 
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-200x113.webp 200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-300x169.jpg 300w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-400x225.webp 400w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-600x338.webp 600w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-768x432.jpg 768w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-800x450.webp 800w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1024x576.jpg 1024w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1200x675.webp 1200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1536x864.webp 1536w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-200x113.webp 200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-300x169.jpg 300w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-400x225.webp 400w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-600x338.webp 600w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-768x432.jpg 768w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-800x450.webp 800w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1024x576.jpg 1024w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1200x675.webp 1200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1536x864.webp 1536w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>WebP, lossy, 90 : 140 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>JPEG 85 and WebP 90 both fail the test, looking like shit. But WebP looks more like shit : the contrast in posterized rings is higher. <strong>And</strong> we are already 10 % above the recommended quality that “should fit 99 % of pictures”. JPEG 90 looks ok though, but it’s a lot heavier.</p>

<p>So, let’s try something else, now : going lossless WebP. That should be our ground truth of WebP supremacy.</p>

<p>WebP, lossless : 660 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>JPEG, quality 100 : 759 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>JPEG, quality 95 : 363 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>So, the WebP quality is now clean, but I’m not impressed with the weight, especially since you need a really good look to distinguish it from JPEG 90, which weighs about a third of that, and it’s forensically similar to JPEG 95, which is a bit more than half. Ooops.</p>

<p>Let’s try something else : redo it, but instead of the light Floyd-Steinberg dithering, use heavier random noise at -48 dB PSNR. That’s a very high PSNR, meaning it should be almost unnoticeable to human eyes but should give an harder time to the high-frequency filtering which is most of the trick behind image compressing.</p>

<p>JPEG, lossy, 85 : 211 KiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-200x113.webp 200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-300x169.jpg 300w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-400x225.webp 400w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-600x338.webp 600w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-768x432.jpg 768w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-800x450.webp 800w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1024x576.jpg 1024w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1200x675.webp 1200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1536x864.webp 1536w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-200x113.webp 200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-300x169.jpg 300w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-400x225.webp 400w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-600x338.webp 600w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-768x432.jpg 768w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-800x450.webp 800w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1024x576.jpg 1024w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1200x675.webp 1200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1536x864.webp 1536w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>WebP, lossy, 90 : 146 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>The WebP is still more prone to posterization. So, I wondered what the WebP quality was that would be as smooth as the JPEG 85 with -48 dB of noise (which was pretty damn smooth). The answer is somewhere between 95 and 96, even though it’s hard to make an equivalence since the quality and texture of the artifacts differ.</p>

<p>WebP, lossy, 96 : 294 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>Yeah, you read it. WebP is actually 39 % heavier than JPEG 85 plus noise for a similar-ish look on this difficult picture, and still not totally as smooth as the JPEG (there is still a tiny bit of ringing). It’s also 30 % heavier than JPEG 90 with simple Floyd-Steinberg dithering.</p>

<p>So, what do we take from that ?</p>

<p>First, at similar visual quality and for photographs, WebP is not lighter than JPEG, it’s actually the opposite. All the Google claims rely on measuring the average SSIM and average bit weight over a dataset of images. Call me crazy, but I don’t give a shit about averages. For a <a href="https://en.wikipedia.org/wiki/Normal_distribution">gaussian "normal" process</a>, probabilities say half of your sample will be above and half will be below the average (which is also the median in a gaussian distribution). If we designed cars for the average load they would have to sustain, it means we would kill about half of the customers. Instead, we design cars for the worst foreseeable scenario, add a safety factor on top, and they still kill a fair amount of them, but a lot fewer than in the past. Most probabilistic distributions are close to gaussian, so the assumption that average = median ± a little something is fair. Also the <a href="https://fr.wikipedia.org/wiki/Structural_Similarity">SSIM</a> metric is an incomplete, biased, controverted metric of image similarity that takes no actual perceptual metric into account<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">[2]</a></sup>, it’s just averages, variances and covariances, meaning it is barely a pattern recognition scheme from a machine perspective.</p>

<p>As a photographer, I&nbsp;care about robustness of the visual output. Which means, as a designer, designing for the worst possible image and taking numerical metrics with a grain of salt. And that whole WebP hype is unjustified, in this regard. It surely performs well in well chosen examples, no doubt. The question is : what happens when it doesn’t ? I can’t fine-tune the WebP quality for each individual image on my website, that’s time consuming and WordPress doesn’t even allow that. I can’t have a portfolio of pictures with even 25 % posterized backgrounds either, the whole point of a portfolio is to showcase your skills and results, not to take a wild guess on the compression performance of your image backend. Average won’t do, it’s simply not good enough. And in setting the weight vs. quality ratio, the nature of the induced artifacts matters perhaps more than the norm of the deviation. We can tolerate higher variance in random noise than in patterned blotches.</p>

<p>Second, I don’t know why all the techies around have a huge kink over sharpness, but the most challenging situations I&nbsp;have faced as a photographer were with smooth gradients. Or more accurately, gradients that should have been smooth and weren’t in the output. So there is a real issue with the design priorities of image algos from tech guys who clearly lack historical and artistic background, and don’t talk to artists, who anyway have largely decided that they were above science, maths and other menial materialistic concerns. Most test pictures for WebP compression showcase sharp scenes with large depth of field, so lots of details, aka high-frequencies, which have zero chance of posterization and are not the pain point of such algorithms. Lack of sharpness has never destroyed a picture, on the contrary. Painters took as much trouble to render atmospheric haze and <em><a href="https://en.wikipedia.org/wiki/Sfumato">sfumato</a></em> as photographers take now to revert them. But having a staircase in place of a smooth vignette surely <strong>is</strong> damaging to the picture in an unacceptable way.</p>

<p>Third, big shout-out to all the morons, idiots, douchebags and monkeys who make big claims all around on matters they don’t nearly understand. Why the big words ? I have been told on my previous article that I&nbsp;was too heavy on insults… Well, we live in a time where time is the ultimate luxury, and the idiots-who-should-know-but-didn’t are not only causing damages, they also cost money and time, and I really think they should be punished for this. You can refund money, you can’t refund time. Thing is, as technologies are “improving”, people don’t get more free time because the work doesn’t get any easier. Instead, the tools become more complex and the customer expect more as the tools get faster, meaning workers have as much work as before, only with more complex toolkits. So, actually, better tech doesn’t mean less or easier work for the actual workers, it may just mean better result if it is actually better tech, which, in this case, it is not. The proof has been made here that WebP is simply not robust enough for image makers, regardless of its average performance, if lesser (or even similar) data bandwidth is the ultimate goal. The test done here is simple enough to have been done by anyone much earlier, provided they used image datasets from actual photographers.</p>

<p>Image-making is not just a vocational part-time activity for bored upper-class or retired citizen with enough money to buy 10 k€ camera systems and do mostly nothing out of them. Some people rely on that to make a living. And they are already in a precarious enough situation (even before COVID… how many newspapers still had a photo staff in 2015 ?) to not take more shit from the people who pretend to help them, when they do the opposite. I have the ability to double-check the stupid shit I read here and there, but the large majority of visual artists don’t and will take the word of “experts” for truth even though it contradicts facts they have witnessed themselves for years.</p>

<p>The Google monkeys at Page Speed are idiots when they advise you to move all your content to WebP. Also they are dishonest since they commited it themselves, so they are judge and party. The Google monkeys who said WebP has lower weight at similar average SSIM say nothing because neither the SSIM nor the average are meaningful : none is robust enough, at best it’s a 50⁄50 % of satisfying/unacceptable outcome. The WordPress plugins monkeys are idiots when they advise and tool you up to convert already lossy JPEGs to WebP. Oh, they probably make all their claims in good faith, the problem is they didn’t see the problems, precisely. And it’s super difficult to argue with people who don’t — literally — see the problems because it’s their bad eyes against your experience, and since people believe only what they see, you are screwed. But then, a lot of lower-tier websites and blog will repeat everything coming from these “trustable” sources, doing even more damage. I have personally lost about a full working week in the past 6 months over that whole WebP migration madness and thanks to all these fake news, to make it work across URL rewriting and CDN redirections, and then to understand why it looked so bad at the end.</p>

<p>Finally, WebP is badly designed. Being necessarily RGB or RGBalpha, there is no way to save a monochrome grey image on single channel. We see that all the posterization here is made worse by magenta and green rings which come solely from the chroma subsampling. With a purely monochrome format saved on a single channel, you don’t introduce any additional chroma shift. It’s as bad as JPEG, but it could have been fixed. That’s what AVIF did, at least, but it won’t be a technical reality for at least another decade.</p>

<p>How do we solve that ?</p>

<ol>
<li>Stick to JPEG at 90 quality (or at least 85) if images matter to you, e.g. if you are a visual artist. If images are pretty decorations for your textual content, it doesn’t matter.</li>
<li>Always add dithering and/or a tiny bit of noise in your images, just to be sure smooth gradients will stay smooth no matter the amount of damage they will take from stupid websites recompressions.</li>
<li>Don’t convert your old JPEG to WebP even if every idiot around tells you to, unless you find the images shown above remotely acceptable.</li>
<li>Serve your images from a fast <a href="https://developer.mozilla.org/en-US/docs/Glossary/CDN">CDN</a>, use <a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images">responsive image sizes</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/Performance/Lazy_loading">image lazy loading</a> to improve loading speed and perceived responsiveness from the user/client side, but there is not much more you can do without damaging the quality of your images.</li>
<li>Avoid all the SaaS ways of converting your images on another server. On paper, they sound great because they relieve your own server from the conversion load, which is good on mutualized hostings. Except they cost, don’t disclose the actual quality factor they use, and don’t work in lots of cases (HTTP connections errors everywhere, especially if you have hardened WordPress with a security plugin). You would be better off with a better hosting and running conversions on your server straight with Image Magick/Graphics Magick (not the PHP&nbsp;interfaces, but directly the server program). There is a <a href="https://wordpress.org/plugins/imagemagick-engine/">WordPress plugin</a> that does just that.</li>
<li>Devs and techs really need to pull their head out of their arses and start discussing with actual artists to understand their challenges and priorities.</li>
<li>Devs and techs really need to get a grasp at basic probabilities because… average, really ?</li>
<li>We really need people able to have one foot in the tech world and the other in the art world, and being able to discuss with both worlds, because having them in two separate bubbles is damaging on a large scale right now, and I don’t see it improving.</li>
</ol>


</div></article>
				</div>  <!-- fusion-row -->
				</main>  <!-- #main -->
				
				
								
					
		 <!-- fusion-footer -->

		
					
												</div> <!-- #boxed-wrapper -->
				<a tabindex="-1" href="#" aria-hidden="true">Page load link</a>

		

			<section aria-labelledby="awb-to-top-label">
		<a href="#" id="toTop">
			<span id="awb-to-top-label">Go to Top</span>
		</a>
	</section>
		


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU capitals want media law carve-out to spy on reporters (101 pts)]]></title>
            <link>https://www.politico.eu/article/eu-capitals-want-media-law-carve-out-to-spy-on-reporters/</link>
            <guid>38652905</guid>
            <pubDate>Fri, 15 Dec 2023 10:51:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.eu/article/eu-capitals-want-media-law-carve-out-to-spy-on-reporters/">https://www.politico.eu/article/eu-capitals-want-media-law-carve-out-to-spy-on-reporters/</a>, See on <a href="https://news.ycombinator.com/item?id=38652905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

									

									
							<div id="amazon-polly-audio-table">
				<p>Press play to listen to this article</p>
				<div id="amazon-polly-audio-tab">
					
					<p>Voiced by artificial intelligence.</p>
				</div>
		</div>
<p>European Union governments want to be able to spy on reporters in the name of national security, even as lawmakers <a href="https://www.politico.eu/?p=3203142">urge</a> them to crack down on spyware.</p>



<p>Governments' deputy ambassadors<a href="https://www.consilium.europa.eu/en/documents-publications/public-register/public-register-search/results/?AllLanguagesSearch=False&amp;OnlyPublicDocuments=False&amp;DocumentNumber=10570%2F23&amp;DocumentLanguage=EN" target="_blank"> are set</a> to give their blessing at a Council meeting on Wednesday to a national security exemption in a new media regulation whose original purpose was to safeguard media independence and pluralism.</p>



<p>Privacy advocates and journalists’ organizations argue the new clause would give countries a free pass to snoop on reporters.&nbsp;</p>



<p>This<a href="https://oeil.secure.europarl.europa.eu/oeil/popups/ficheprocedure.do?reference=2022/0277(COD)&amp;l=en" target="_blank"> first-of-a-kind rulebook for media</a>, proposed by the European Commission in September, touches upon a broad range of areas, including surveillance, media concentration, public broadcasters and online platforms.</p>	<div>
									<h3>
					
					You may like
					
									</h3>
			
			
						
		
		
		
			</div>
	



<p>The original proposal sought to ensure governments could not "detain, sanction, intercept, subject to surveillance or search and seizure" journalists in order to uncover their sources, unless "justified by an overriding requirement in the public interest."</p>



<p>They shouldn't deploy spyware on journalists’ devices either, unless — again — it’s for national security purposes, "on a case-by-case basis," or such surveillance is needed to investigate "serious crimes," which the Commission listed as terrorism, human or weapons trafficking, exploitation of children, murder or rape, for example.</p>



<p>But EU capitals, led by France, want a bigger carve-out.</p>



<h3>Hands off!</h3>



<p>Last month, in a document first reported by POLITICO, Paris<a href="https://www.politico.eu/?p=3065400"> called</a> for an “explicit and unconditional” clause in the text to safeguard member countries’ prerogatives on security and defense and for narrower immunity for journalists under the new EU-wide media rules.</p>



<p>"The particularly exorbitant nature of this immunity raises questions — both in terms of its necessity and its proportionality," the document reads, while stressing France's "attachment" to the confidentiality of journalists’ sources and acknowledging "the emotion triggered by the Pegasus [spyware] affair."</p>



<p>It is "essential to strike a fair balance between the need to protect the confidentiality of journalists' sources and the need to protect citizens and the state against serious threats [...] whoever the perpetrators may be," France added in the document.&nbsp;</p>



<p>Sweden, at the helm of the Council until the end of the month and responsible for steering the negotiations, has accepted France’s wish list. </p>



<p>It pitched an article stating the provisions protecting journalists from interference and surveillance shall be "without prejudice to the member states’ responsibility for safeguarding national security."</p>



<figure><img decoding="async" fetchpriority="high" width="1024" height="683" src="https://www.politico.eu/wp-content/uploads/2023/05/09/GettyImages-623803568-1024x683.jpg" alt="" srcset="https://www.politico.eu/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2023/05/09/GettyImages-623803568.jpg 1024w, https://www.politico.eu/cdn-cgi/image/width=300,quality=80,onerror=redirect,format=auto/wp-content/uploads/2023/05/09/GettyImages-623803568.jpg 300w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Spyware tools allow security services, governments, companies and hackers to gain access to mobile phones and other devices and snoop on data | Damien Meyer/AFP via Getty Images</figcaption></figure>



<p>The Commission’s list of "serious crimes" has also been removed from the original text. Spyware can instead be deployed to investigate the crimes referred to in 2002's<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32002F0584" target="_blank"> Council Framework Decision of the European arrest warrant</a> — broadening the number of offenses allowing such surveillance from 10 to 32. Alternatively, it can be used to investigate crimes punishable by at least five years’ imprisonment.</p>



<p>In the <a rel="noreferrer noopener" href="https://api.politico.eu/editorial_documents/d5d4af34-a78d-42e9-be54-c593a309841a" target="_blank">latest compromise text</a>, dated June 16 and obtained by POLITICO, and set to become the Council's official position ahead of the three-way negotiations with the Commission and Parliament, the wording "spyware" was replaced by "intrusive surveillance software." The Swedes also outlined that "one delegation" is still pushing for a reference to member countries' sole responsibility for national security in the first article. </p>



<p>The Swedish presidency and the French permanent representation to the EU declined to comment because talks haven't finished. </p>



<h3>Pandora's box</h3>



<p>Unsurprisingly, journalists and privacy advocates are not happy with the Council’s suggested tweaks.</p>



<p>"National security is a classic exception. It opens the door to all kinds of abuse," Julie Majerczak, head of the Brussels bureau of Reporters Without Borders (RSF), told POLITICO, calling it a "Pandora's box."</p>



<p>With 59 other civil society organizations as signatories, including RSF, EDRi wrote an <a href="https://edri.org/our-work/open-letter-council-of-european-union-journalists-spyware-and-surveillance-european-media-freedom-act/" target="_blank">open letter</a> to EU deputy ambassadors on Monday, urging them to reconsider their position. The current compromise "is not only weakening safeguards against the deployment of spyware but also strongly incentivizes their use based solely on member states’ discretion," the signatories said.</p>



<p>Majerczak is calling for strong legal safeguards — failing the withdrawal of the national security clause — like involving judicial authorities, should a government decide to spy on a reporter.</p>



<p>Nothing is set in stone. Once the Council has agreed on its mandate, it will have to negotiate with the Parliament — which has yet to reach its position — and the Commission before the new rules can enter into force.</p>



<p>In Parliament, the provisions for the surveillance of journalists are under the sole jurisdiction of the civil liberties and justice committee — led by the rapporteur, liberal Romanian lawmaker Ramona Strugariu of Renew Europe.</p>



<p>"Member states who are champions in providing guarantees for the freedom of speech should give a very serious thought to what kind of precedent they are setting and what standards they commit to," she told POLITICO, pledging "to strengthen the Commission proposal and to have a balanced deal with the Council."</p>



<p>In her<a href="https://www.europarl.europa.eu/doceo/document/LIBE-PA-746757_EN.pdf" target="_blank"> draft report</a>, Strugariu suggested that the deployment of spyware should be ordered by a court of law or by a judge — which promises lively discussions between the Parliament and the Council as they hammer out the final text.</p>



<p><em>CLARIFICATION: This article has been updated to specify that EDRi wrote the open letter to EU deputy ambassadors, while RSF was a signatory. </em></p>
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oxlint – written in Rust – 50-100 Times Faster than ESLint (273 pts)]]></title>
            <link>https://oxc-project.github.io/blog/2023-12-12-announcing-oxlint.html</link>
            <guid>38652887</guid>
            <pubDate>Fri, 15 Dec 2023 10:48:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oxc-project.github.io/blog/2023-12-12-announcing-oxlint.html">https://oxc-project.github.io/blog/2023-12-12-announcing-oxlint.html</a>, See on <a href="https://news.ycombinator.com/item?id=38652887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-d1cebd69=""><header data-v-6584a84b=""><h2 data-v-6584a84b="">Oxlint General Availability</h2></header><p>We're thrilled to announce that oxlint is now generally available! This milestone signifies our team's ability to promptly address and triage issues.</p><p>Oxlint is a JavaScript linter designed to catch erroneous or useless code without requiring any configurations by default.</p><h2 id="how-to-use" tabindex="-1">How to Use <a href="#how-to-use" aria-label="Permalink to &quot;How to Use&quot;">​</a></h2><p>At this stage, oxlint is <strong>not intended to fully replace ESLint</strong>; it serves as an enhancement when ESLint's slowness becomes a bottleneck in your workflow.</p><p>For faster feedback loops, we recommend running oxlint before ESLint in your lint-staged or CI setup, considering it only takes a few seconds to run on large codebases.</p><p>To test oxlint in your JavaScript / TypeScript codebase, simply execute the following command at the root directory of your repository:</p><div><p><label for="tab-zlT1HlT">npm</label><label for="tab-bS3cme4">pnpm</label><label for="tab-eHmabbt">yarn</label><label for="tab-qvH706H">bun</label><label for="tab-NTSJzp6">deno</label></p><div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>npx</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>npx</span><span> </span><span>oxlint@latest</span></span></code></pre></div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>pnpm</span><span> </span><span>dlx</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>pnpm</span><span> </span><span>dlx</span><span> </span><span>oxlint@latest</span></span></code></pre></div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>yarn</span><span> </span><span>dlx</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>yarn</span><span> </span><span>dlx</span><span> </span><span>oxlint@latest</span></span></code></pre></div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>bunx</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>bunx</span><span> </span><span>oxlint@latest</span></span></code></pre></div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>deno</span><span> </span><span>run</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>deno</span><span> </span><span>run</span><span> </span><span>oxlint@latest</span></span></code></pre></div></div></div><p>Alternatively, refer to the <a href="https://oxc-project.github.io/docs/guide/usage/linter.html">installation guide</a> for detailed instructions.</p><h2 id="design" tabindex="-1">Design <a href="#design" aria-label="Permalink to &quot;Design&quot;">​</a></h2><h3 id="_50-100-times-faster-than-eslint" tabindex="-1">50-100 Times Faster than ESLint <a href="#_50-100-times-faster-than-eslint" aria-label="Permalink to &quot;50-100 Times Faster than ESLint&quot;">​</a></h3><p>In real-world scenarios, Shopify reported that their 75 CI minutes ESLint run is now only 10 seconds.</p><p>From Jason Miller, Shopify DX and creator of Preact:</p><blockquote><p>oxlint has been a massive win for us at Shopify. Our previous linting setup took 75 minutes to run, so we were fanning it out across 40+ workers in CI.</p><p>By comparison, oxlint takes around 10 seconds to lint the same codebase on a single worker, and the output is easier to interpret.</p><p>We even caught a few bugs that were hidden or skipped by our old setup when we migrated!</p></blockquote><p>The majority of the performance gains stem from Oxlint being purposefully designed for performance, utilizing Rust and parallel processing as key factors.</p><h3 id="lint-for-correctness" tabindex="-1">Lint for Correctness <a href="#lint-for-correctness" aria-label="Permalink to &quot;Lint for Correctness&quot;">​</a></h3><p>Oxlint defaults to identifying erroneous, redundant, or confusing code — prioritizing correctness over unnecessary nitpicking rules (categorized as <code>perf</code>, <code>suspicious</code>, <code>pedantic</code>, or <code>style</code>), which are disabled by default.</p><h3 id="ease-of-use" tabindex="-1">Ease of Use <a href="#ease-of-use" aria-label="Permalink to &quot;Ease of Use&quot;">​</a></h3><p>Setting up new JavaScript / TypeScript codebases is becoming increasingly complex. There's a high likelihood of encountering compatibility issues among your tools, potentially resulting in hours of wasted time.</p><p>That's why we designed oxlint to be zero-config out of the box; even Node.js is not a requirement. Most adjustments can be made through the command-line, and reading from ESLint configuration file is currently work in progress.</p><h3 id="enhanced-diagnostics" tabindex="-1">Enhanced Diagnostics <a href="#enhanced-diagnostics" aria-label="Permalink to &quot;Enhanced Diagnostics&quot;">​</a></h3><p>Understanding linter messages can be challenging. Oxlint aims to simplify this by pinpointing root causes and providing helpful messages — eliminating the need for lengthy rule documentation reading, saving valuable time.</p><p>Running <code>oxlint -D perf</code> in the <a href="https://github.com/microsoft/vscode" target="_blank" rel="noreferrer">vscode repository</a>:</p><p><img width="100%" src="https://github.com/oxc-project/oxc/assets/1430279/094a3b24-0433-42ae-aad2-48a7dec2b985"></p><h3 id="consolidated-rules" tabindex="-1">Consolidated Rules <a href="#consolidated-rules" aria-label="Permalink to &quot;Consolidated Rules&quot;">​</a></h3><p>Oxlint does not provide a plugin system yet, but we are actively consolidating rules from popular plugins like TypeScript, React, Jest, Unicorn, JSX-a11y and Import.</p><p>We recognize the importance of plugins in the JavaScript ecosystem and are also investigating a DSL-based plugin system.</p><p>However, you might appreciate a standalone linter — no need to manage a list of plugin dependencies, navigate through <a href="https://github.com/antfu/eslint-ts-patch" target="_blank" rel="noreferrer">compatibility issues</a>, or <a href="https://github.com/import-js/eslint-plugin-import/pull/2504#issuecomment-1191057877" target="_blank" rel="noreferrer">resort to forked plugins due to version constraints</a>.</p><hr><p>Happy linting and have a joyful holiday season!</p><p>To get started, follow the <a href="https://oxc-project.github.io/docs/guide/usage/linter.html">installation guide</a>, or <a href="https://oxc-project.github.io/docs/guide/introduction.html">learn more about the oxc project</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Interactive Guide to the Fourier Transform (180 pts)]]></title>
            <link>https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/</link>
            <guid>38652794</guid>
            <pubDate>Fri, 15 Dec 2023 10:29:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/">https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/</a>, See on <a href="https://news.ycombinator.com/item?id=38652794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The Fourier Transform is one of deepest insights ever made. Unfortunately, the meaning is buried within dense equations:</p>
<p><img src="https://betterexplained.com/wp-content/plugins/wp-latexrender/pictures/45c088dbb767150fc0bacfeb49dd49e5.png" width="189" height="54" alt="\displaystyle{X_k = \sum_{n=0}^{N-1} x_n \cdot e^{-i 2 \pi k n / N}}"></p>
<p><img src="https://betterexplained.com/wp-content/plugins/wp-latexrender/pictures/faeb9c5bf2e60add63ae4a70b293c7b4.png" width="202" height="54" alt="\displaystyle{x_n = \frac{1}{N} \sum_{k=0}^{N-1} X_k \cdot e^{i 2 \pi k n / N}}"></p>
<p>Yikes. Rather than jumping into the symbols, let's experience the key idea firsthand. Here's a plain-English metaphor:</p>
<ul>
<li><strong>What does the Fourier Transform do?</strong> Given a smoothie, it finds the recipe.</li>
<li><strong>How?</strong> Run the smoothie through filters to extract each ingredient.</li>
<li><strong>Why?</strong> Recipes are easier to analyze, compare, and modify than the smoothie itself.</li>
<li><strong>How do we get the smoothie back?</strong> Blend the ingredients.</li>
</ul>
<p>Here's the "math English" version of the above:</p>
<ul>
<li>The Fourier Transform takes a time-based pattern, measures every possible cycle, and returns the overall "cycle recipe" (the amplitude, offset, &amp; rotation speed for every cycle that was found).</li>
</ul>
<p>Time for the equations? No! Let's get our hands dirty and <em>experience</em> how any pattern can be built with cycles, with live simulations.</p>
<p>If all goes well, we'll have an aha! moment and intuitively realize why the Fourier Transform is possible. We'll save the detailed math analysis for the follow-up.</p>
<p>This isn't a force-march through the equations, it's the casual stroll I wish I had. Onward!</p>
<p><iframe src="https://www.youtube.com/embed/iN0VG9N2q0U?ecver=2" width="640" height="360" frameborder="0" allowfullscreen=""></iframe></p>
<h2>From Smoothie to Recipe</h2>
<p>A math transformation is a change of perspective. We change our notion of quantity from "single items" (lines in the sand, tally system) to "groups of 10" (decimal) depending on what we're counting. Scoring a game? Tally it up. Multiplying? Decimals, please.</p>
<p>The Fourier Transform changes our perspective from consumer to producer, turning <em>What do I have?</em> into <em>How was it made?</em></p>
<p>In other words: given a smoothie, let's find the recipe.</p>
<p>Why? Well, recipes are great descriptions of drinks. You wouldn't share a drop-by-drop analysis, you'd say "I had an orange/banana smoothie". A recipe is more easily categorized, compared, and modified than the object itself.</p>
<p>So... given a smoothie, how do we find the recipe?</p>
<p><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/images/smoothie-to-recipe-20121030-223058.png.webp" type="image/webp"><img decoding="async" alt="fourier transform analogy smoothie to recipe" src="https://betterexplained.com/wp-content/uploads/images/smoothie-to-recipe-20121030-223058.png" height="300"></picture></p>
<p>Well, imagine you had a few filters lying around:</p>
<ul>
<li>Pour through the "banana" filter. 1 oz of bananas are extracted.</li>
<li>Pour through the "orange" filter. 2 oz of oranges.</li>
<li>Pour through the "milk" filter. 3 oz of milk.</li>
<li>Pour through the "water" filter. 3 oz of water.</li>
</ul>
<p>We can reverse-engineer the recipe by filtering each ingredient. The catch?</p>
<ul>
<li><p><strong>Filters must be independent</strong>. The banana filter needs to capture bananas, and nothing else. Adding more oranges should never affect the banana reading.</p></li>
<li><p><strong>Filters must be complete</strong>. We won't get the real recipe if we leave out a filter ("There were mangoes too!"). Our collection of filters must catch every possible ingredient.</p></li>
<li><p><strong>Ingredients must be combine-able</strong>. Smoothies can be separated and re-combined without issue (A cookie? Not so much. Who wants crumbs?). The ingredients, when separated and combined in any order, must make the same result.</p></li>
</ul>
<h2>See The World As Cycles</h2>
<p>The Fourier Transform takes a specific viewpoint: <strong>What if any signal could be filtered into a bunch of circular paths?</strong></p>
<p>Whoa. This concept is mind-blowing, and poor Joseph Fourier had his idea rejected at first. (<em>Really Joe, even a staircase pattern can be made from circles?</em>)</p>
<p>And despite <a href="http://wiki.answers.com/Q/What_is_the_history_of_fourier_series">decades of debate</a> in the math community, we expect students to internalize the idea without issue. Ugh. Let's walk through the intuition.</p>
<p>The Fourier Transform finds the recipe for a signal, like our smoothie process:</p>
<ul>
<li>Start with a time-based signal</li>
<li>Apply filters to measure each possible "circular ingredient"</li>
<li>Collect the full recipe, listing the amount of each "circular ingredient"</li>
</ul>
<p>Stop. Here's where most tutorials excitedly throw engineering applications at your face. Don't get scared; think of the examples as "Wow, we're finally seeing the source code (DNA) behind previously confusing ideas".</p>
<ul>
<li><p>If earthquake vibrations can be separated into "ingredients" (vibrations of different speeds &amp; amplitudes), buildings can be designed to avoid interacting with the strongest ones.</p></li>
<li><p>If sound waves can be separated into ingredients (bass and treble frequencies), we can boost the parts we care about, and hide the ones we don't. The crackle of random noise can be removed. Maybe similar "sound recipes" can be compared (music recognition services compare recipes, not the raw audio clips).</p></li>
<li><p>If computer data can be represented with oscillating patterns, perhaps the least-important ones can be ignored. This "lossy compression" can drastically shrink file sizes (and why JPEG and MP3 files are much smaller than raw .bmp or .wav files).</p></li>
<li><p>If a radio wave is our signal, we can use filters to listen to a particular channel. In the smoothie world, imagine each person paid attention to a different ingredient: Adam looks for apples, Bob looks for bananas, and Charlie gets cauliflower (sorry bud).</p></li>
</ul>
<p>The Fourier Transform is useful in engineering, sure, but it's a metaphor about finding the root causes behind an observed effect.</p>
<h2>Think With Circles, Not Just Sinusoids</h2>
<p>One of my giant confusions was separating the definitions of "sinusoid" and "circle".</p>
<ul>
<li>A "sinusoid" is a specific back-and-forth pattern (a <a href="https://betterexplained.com/articles/intuitive-understanding-of-sine-waves/">sine</a> or cosine wave), and 99% of the time, it refers to motion in one dimension.</li>
<li>A "circle" is a round, 2d pattern you probably know. If you enjoy using 10-dollar words to describe 10-cent ideas, you might call a circular path a "complex sinusoid".</li>
</ul>
<p>Labeling a circular path as a "complex sinusoid" is like describing a word as a "multi-letter". You zoomed into the wrong level of detail. Words are about concepts, not the letters they can be split into!</p>
<p>The Fourier Transform is about circular paths (not 1-d sinusoids) and <a href="https://betterexplained.com/articles/intuitive-understanding-of-eulers-formula/">Euler's formula</a> is a clever way to generate one:</p>
<p><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/euler/equal_paths.png.webp" type="image/webp"><img decoding="async" src="https://betterexplained.com/wp-content/uploads/euler/equal_paths.png" alt="euler path"></picture></p>
<p>Must we use imaginary exponents to move in a circle? Nope. But it's convenient and compact. And sure, we can describe our path as coordinated motion in two dimensions (real and imaginary), but don't forget the big picture: we're just moving in a circle.</p>
<h2>Following Circular Paths</h2>
<p>Let's say we're chatting on the phone and, like usual, I want us to draw the same circle simultaneously. (<em>You promised!</em>) What should I say?</p>
<ul>
<li>How big is the circle? (Amplitude, i.e. size of radius)</li>
<li>How fast do we draw it? (Frequency. 1 circle/second is a frequency of 1 Hertz (Hz) or 2*pi radians/sec)</li>
<li>Where do we start? (Phase angle, where 0 degrees is the x-axis)</li>
</ul>
<p>I could say "2-inch radius, start at 45 degrees, 1 circle per second, go!". After half a second, we should each be pointing to: starting point + amount traveled = 45 + 180 = 225 degrees (on a 2-inch circle).</p>
<p><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/images/circular-path-parameters-20121201-203317.png.webp" type="image/webp"><img decoding="async" alt="circular path with parameters" src="https://betterexplained.com/wp-content/uploads/images/circular-path-parameters-20121201-203317.png" width="500"></picture></p>
<p>Every circular path needs a size, speed, and starting angle (amplitude/frequency/phase). We can even combine paths: imagine tiny motorcars, driving in circles at different speeds.</p>
<p>The combined position of <em>all the cycles</em> is our signal, just like the combined flavor of <em>all the ingredients</em> is our smoothie.</p>
<p>Here's a simulation of a basic circular path:</p>

<p>(Based on <a href="http://treeblurb.com/dev_math/sin_canv00.html">this animation</a>, here's <a href="https://gist.github.com/kazad/8bb682da198db597558c">the source code</a>. Modern browser required. Click the graph to pause/unpause.)</p>
<p>The magnitude of each cycle is listed in order, starting at 0Hz. Cycles <code>[0 1]</code> means</p>
<ul>
<li>0 amplitude for the 0Hz cycle (0Hz = a constant cycle, stuck on the x-axis at zero degrees)</li>
<li>1 amplitude for the 1Hz cycle (completes 1 cycle per time interval)</li>
</ul>
<p>Now the tricky part:</p>
<ul>
<li><strong>The blue graph measures the <em>real part</em> of the cycle</strong>. Another lovely math confusion: the real axis of the circle, which is usually horizontal, has its magnitude shown on the vertical axis. You can mentally rotate the circle 90 degrees if you like.</li>
<li><strong>The time points are spaced at the fastest frequency</strong>. A 1Hz signal needs 2 time points for a start and stop (a single data point doesn't have a frequency). The time values <code>[1 -1]</code> shows the amplitude at these equally-spaced intervals.</li>
</ul>
<p>With me? <code>[0 1]</code> is a pure 1Hz cycle.</p>
<p>Now let's add a 2Hz cycle to the mix. <code>[0 1 1]</code> means "Nothing at 0Hz, 1Hz of amplitude 1, 2Hz of amplitude 1":</p>

<p>Whoa. The little motorcars are getting wild: the green lines are the 1Hz and 2Hz cycles, and the blue line is the combined result. Try toggling the green checkbox to see the final result clearly. The combined "flavor" is a sway that starts at the max and dips low for the rest of the interval.</p>
<p>The yellow dots are when we actually measure the signal. With 3 cycles defined (0Hz, 1Hz, 2Hz), each dot is 1/3 of the way through the signal. In this case, cycles <code>[0 1 1]</code> generate the time values <code>[2 -1 -1]</code>, which starts at the max (2) and dips low (-1).</p>
<p>Oh! We can't forget phase, the starting angle! Use <code>magnitude:angle</code> to set the phase. So <code>[0 1:45]</code> is a 1Hz cycle that starts at 45 degrees:</p>

<p>This is a shifted version of <code>[0 1]</code>. On the time side we get <code>[.7 -.7]</code> instead of <code>[1 -1]</code>, because our cycle isn't exactly lined up with our measuring intervals, which are still at the halfway point (this could be desired!).</p>
<p><strong>The Fourier Transform finds the set of cycle speeds, amplitudes and phases to match any time signal.</strong></p>
<p>Our signal becomes an abstract notion that we consider as "observations in the time domain" or "ingredients in the frequency domain".</p>
<p>Enough talk: try it out! In the simulator, type any time or cycle pattern you'd like to see. If it's time points, you'll get a collection of cycles (that combine into a "wave") that matches your desired points.</p>
<p><a href="https://betterexplained.com/wp-content/uploads/images/time-patterns-20121205-164541.png" target="_blank" rel="noopener">
<picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/images/time-patterns-20121205-164541.png.webp" type="image/webp"><img decoding="async" alt="fourier transform examples" src="https://betterexplained.com/wp-content/uploads/images/time-patterns-20121205-164541.png" width="550px"></picture></a></p>
<p>But… doesn't the combined wave have strange values between the yellow time intervals? Sure. But who's to say whether a signal travels in straight lines, or curves, or zips into other dimensions when we aren't measuring it? It behaves exactly as we need at the equally-spaced moments we asked for.</p>
<h2>Making A Spike In Time</h2>
<p>Can we make a spike in time, like <code>(4 0 0 0)</code>, using cycles? I'll use parentheses <code>()</code> for a sequence of time points, and brackets <code>[]</code> for a sequence of cycles.</p>
<p>Although the spike seems boring to us time-dwellers (<em>one data point, that's it?</em>), think about the complexity in the cycle world. Our cycle ingredients must start aligned (at the max value, 4) and then "explode outwards", each cycle with partners that cancel it in the future. Every remaining point is zero, which is a tricky balance with multiple cycles running around (we can't just "turn them off").</p>
<p>Let's walk through each time point:</p>
<ul>
<li><p>At time 0, the first instant, every cycle ingredient is at its max. Ignoring the other time points, <code>(4 ? ? ?)</code> can be made from 4 cycles (0Hz 1Hz 2Hz 3Hz), each with a magnitude of 1 and phase of 0 (i.e., 1 + 1 + 1 + 1 = 4).</p></li>
<li><p>At every future point (t = 1, 2, 3), the sum of all cycles must cancel.</p></li>
</ul>
<p>Here's the trick: when two cycles are on opposites sides of the circle (North &amp; South, East &amp; West, etc.) their combined position is zero (3 cycles can cancel if they're spread evenly at 0, 120, and 240 degrees).</p>
<p>Imagine a constellation of points moving around the circle. Here's the position of each cycle at every instant:</p>
<pre>Time 0 1 2 3 
------------
0Hz: 0 0 0 0 
1Hz: 0 1 2 3
2Hz: 0 2 0 2
3Hz: 0 3 2 1
</pre>
<p>Notice how the the 3Hz cycle starts at 0, gets to position 3, then position "6" (with only 4 positions, 6 <a href="https://betterexplained.com/articles/fun-with-modular-arithmetic/">modulo</a> 4 = 2), then position "9" (9 modulo 4 = 1).</p>
<p>When our cycle is 4 units long, cycle speeds a half-cycle apart (2 units) will either be lined up (difference of 0, 4, 8…) or on opposite sides (difference of 2, 6, 10…).</p>
<p>OK. Let's drill into each time point:</p>
<ul>
<li>Time 0: All cycles at their max (total of 4)</li>
<li>Time 1: 1Hz and 3Hz cancel (positions 1 &amp; 3 are opposites), 0Hz and 2Hz cancel as well. The net is 0.</li>
<li>Time 2: 0Hz and 2Hz line up at position 0, while 1Hz and 3Hz line up at position 2 (the opposite side). The total is still 0.</li>
<li>Time 3: 0Hz and 2Hz cancel. 1Hz and 3Hz cancel.</li>
<li>Time 4 (repeat of t=0): All cycles line up.</li>
</ul>
<p>The trick is having individual speeds cancel (0Hz vs 2Hz, 1Hz vs 3Hz), or having the lined-up pairs cancel (0Hz + 2Hz vs 1Hz + 3Hz).</p>
<p>When every cycle has equal power and 0 phase, we start aligned and cancel afterwards. (I don't have a nice proof yet -- any takers? -- but you can see it yourself. Try <code>[1 1]</code>, <code>[1 1 1]</code>, <code>[1 1 1 1]</code> and notice the signals we generate: <code>(2 0)</code>, <code>(3 0 0)</code>, <code>(4 0 0 0)</code>).</p>
<p>In my head, I label these signals as "time spikes": they have a value for a single instant, and are zero otherwise (the fancy name is a <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">delta function</a>.)</p>
<p>Here's how I visualize the initial alignment, followed by a net cancellation:</p>
<p><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/images/fourier-interference-20121211-171719.png.webp" type="image/webp"><img decoding="async" alt="fourier transform constructive and destructive interference" src="https://betterexplained.com/wp-content/uploads/images/fourier-interference-20121211-171719.png" width="400px"></picture></p>
<h2>Moving The Time Spike</h2>
<p>Not everything happens at t=0. Can we change our spike to <code>(0 4 0 0)</code>?</p>
<p>It seems the cycle ingredients should be similar to <code>(4 0 0 0)</code>, but the cycles must align at t=1 (one second in the future). Here's where phase comes in.</p>
<p>Imagine a race with 4 runners. Normal races have everyone lined up at the starting line, the <code>(4 0 0 0)</code> time pattern. Boring.</p>
<p>What if we want everyone to <em>finish</em> at the same time? Easy. Just move people forward or backwards by the appropriate distance. Maybe granny can start 2 feet in front of the finish line, Usain Bolt can start 100m back, and they can cross the tape holding hands.</p>
<p>Phase shifts, the starting angle, are delays in the cycle universe. Here's how we adjust the starting position to delay every cycle 1 second:</p>
<ul>
<li>A 0Hz cycle doesn't move, so it's already aligned</li>
<li>A 1Hz cycle goes 1 revolution in the entire 4 seconds, so a 1-second delay is a quarter-turn. Phase shift it 90 degrees backwards (-90) and it gets to phase=0, the max value, at t=1.</li>
<li>A 2Hz cycle is twice as fast, so give it twice the angle to cover (-180 or 180 phase shift -- it's across the circle, either way).</li>
<li>A 3Hz cycle is 3x as fast, so give it 3x the distance to move (-270 or +90 phase shift)</li>
</ul>
<p>If time points <code>(4 0 0 0)</code> are made from cycles <code>[1 1 1 1]</code>, then time points <code>(0 4 0 0)</code> are made from <code>[1 1:-90 1:180 1:90]</code>. (Note: I'm using "1Hz", but I mean "1 cycle over the entire time period").</p>
<p>Whoa -- we're working out the cycles in our head!</p>
<p>The interference visualization is similar, except the alignment is at t=1.</p>
<p><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/images/delayed-spike-20121214-162623.png.webp" type="image/webp"><img decoding="async" alt="fourier transform time spike" src="https://betterexplained.com/wp-content/uploads/images/delayed-spike-20121214-162623.png" width="400px"></picture></p>
<p>Test your intuition: Can you make <code>(0 0 4 0)</code>, i.e. a 2-second delay? 0Hz has no phase. 1Hz has 180 degrees, 2Hz has 360 (aka 0), and 3Hz has 540 (aka 180), so it's <code>[1 1:180 1 1:180]</code>.</p>
<h2>Discovering The Full Transform</h2>
<p>The big insight: our signal is just a bunch of time spikes! If we merge the recipes for each time spike, we should get the recipe for the full signal.</p>
<p>The Fourier Transform builds the recipe frequency-by-frequency:</p>
<ul>
<li>Separate the full signal (a b c d) into "time spikes": (a 0 0 0) (0 b 0 0) (0 0 c 0) (0 0 0 d)</li>
<li>For any frequency (like 2Hz), the <em>tentative</em> recipe is "a/4 + b/4 + c/4 + d/4" (the amplitude of each spike is split among all frequencies)</li>
<li>Wait! We need to offset each spike with a phase delay (the angle for a "1 second delay" depends on the frequency).</li>
<li>Actual recipe for a frequency = a/4 (no offset) + b/4 (1 second offset) + c/4 (2 second offset) + d/4 (3 second offset).</li>
</ul>
<p>We can then loop through every frequency to get the full transform.</p>
<p>Here's the conversion from "math English" to full math:</p>
<p><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/images/fourier-explained-20121219-224649.png.webp" type="image/webp"><img decoding="async" alt="fourier transform plain english" src="https://betterexplained.com/wp-content/uploads/images/fourier-explained-20121219-224649.png" width="500px"></picture></p>
<p>A few notes:</p>
<ul>
<li>N = number of time samples we have</li>
<li>n = current sample we're considering (0 .. N-1)</li>
<li>x<sub>n</sub> = value of the signal at time n</li>
<li>k = current frequency we're considering (0 Hertz up to N-1 Hertz)</li>
<li>X<sub>k</sub> = amount of frequency k in the signal (amplitude and phase, a complex number)</li>
<li>The 1/N factor is usually moved to the <em>reverse transform</em> (going from frequencies back to time). This <a href="http://math.stackexchange.com/questions/58163/dft-why-are-the-definitions-for-inverse-and-forward-commonly-switched">is allowed</a>, though I prefer 1/N in the forward transform since it gives the <em>actual</em> sizes for the time spikes. You can get wild and even use $1/\sqrt{N}$ on both transforms (going forward and back creates the 1/N factor).</li>
<li>n/N is the percent of the time we've gone through. 2 * pi * k is our speed in radians / sec. e^-ix is our backwards-moving circular path. The combination is how far we've moved, for this speed and time.</li>
<li>The raw equations for the Fourier Transform just say "add the complex numbers". Many programming languages cannot handle complex numbers directly, so you convert everything to rectangular coordinates and add those.</li>
</ul>
<h2>Onward</h2>
<p>This was my most challenging article yet. The Fourier Transform has several flavors (discrete/continuous/finite/infinite), covers deep math (Dirac delta functions), and it's easy to get lost in details. I was constantly bumping into the edge of my knowledge.</p>
<p>But there's always simple analogies out there -- I refuse to think otherwise. Whether it's a smoothie or Usain Bolt &amp; Granny crossing the finish line, take a simple understanding and refine it. The analogy is flawed, and that's ok: it's a raft to use, and leave behind once we cross the river.</p>
<p>I realized how feeble my own understanding was when I couldn't work out the transform of <code>(1 0 0 0)</code> in my head. For me, it was like saying I knew addition but, gee whiz, I'm not sure what "1 + 1 + 1 + 1" would be. Why not? Shouldn't we have an intuition for the simplest of operations?</p>
<p>That discomfort led me around the web to build my intuition. In addition to the references in the article, I'd like to thank:</p>
<ul>
<li><a href="http://www.scotthyoung.com/blog/">Scott Young</a>, for the initial impetus for this post</li>
<li><a href="http://shaheengandhi.com/">Shaheen Gandhi</a>, Roger Cheng, and <a href="http://britcruise.com/">Brit Cruise</a> for kicking around ideas &amp; refining the analogy</li>
<li><a href="http://cns-alumni.bu.edu/~slehar/fourier/fourier.html">Steve Lehar</a> for great examples of the Fourier Transform on images</li>
<li><a href="http://www.complextoreal.com/chapters/fft1.pdf">Charan Langton</a> for her detailed walkthrough</li>
<li><a href="https://ccrma.stanford.edu/~jos/log/">Julius Smith</a> for a fantastic walkthrough of the Discrete Fourier Transform (what we covered today)</li>
<li><a href="http://worrydream.com/#!/LearnableProgramming">Bret Victor</a> for his techniques on visualizing learning</li>
</ul>
<p>Today's goal was to <em>experience</em> the Fourier Transform. We'll save the advanced analysis for next time.</p>
<p>Happy math.</p>
<h2>Appendix: Projecting Onto Cycles</h2>
<p>Stuart Riffle has a <a href="https://web.archive.org/web/20120418231513/http://www.altdevblogaday.com/2011/05/17/understanding-the-fourier-transform/">great interpretation</a> of the Fourier Transform:</p>
<p><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/images/DerivedDFT.png.webp" type="image/webp"><img decoding="async" alt="fourier transform colorized" src="https://betterexplained.com/wp-content/uploads/images/DerivedDFT.png"></picture></p>
<p>Imagine spinning your signal in a centrifuge and checking for a bias. I have a correction: we must spin <em>backwards</em> (the exponent in the equation above should be $e^{-i 2 \pi...}$). You already know why: we need a phase <em>delay</em> so spikes appear in the <em>future</em>.</p>
<h2>Appendix: Another Awesome Visualization</h2>
<p><a href="http://toxicdump.org/blog/">Lucas Vieira</a>, author of excellent <a href="https://en.wikipedia.org/wiki/User:Kieff/Gallery">Wikipedia animations</a>, was <a href="http://blog.matthen.com/post/42112703604/the-smooth-motion-of-rotating-circles-can-be-used">inspired</a> to make this interactive animation:</p>
<p><a href="https://web.archive.org/web/20170201145911/http://toxicdump.org/stuff/FourierToy.swf">Fourier Toy - Click to download, requires flash</a></p>
<p><a href="http://toxicdump.org/stuff/FourierToy.swf"><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/2012/12/Fourier_Toy-300x126.png.webp 300w, https://betterexplained.com/wp-content/webp-express/webp-images/uploads/2012/12/Fourier_Toy-768x323.png.webp 768w, https://betterexplained.com/wp-content/webp-express/webp-images/uploads/2012/12/Fourier_Toy-1024x431.png.webp 1024w, https://betterexplained.com/wp-content/webp-express/webp-images/uploads/2012/12/Fourier_Toy-320x135.png.webp 320w, https://betterexplained.com/wp-content/webp-express/webp-images/uploads/2012/12/Fourier_Toy-200x84.png.webp 200w, https://betterexplained.com/wp-content/webp-express/webp-images/uploads/2012/12/Fourier_Toy-400x168.png.webp 400w, https://betterexplained.com/wp-content/webp-express/webp-images/uploads/2012/12/Fourier_Toy.png.webp 1438w" sizes="(max-width: 300px) 100vw, 300px" type="image/webp"><img loading="lazy" decoding="async" src="https://betterexplained.com/wp-content/uploads/2012/12/Fourier_Toy-300x126.png" alt="Fourier_Toy" width="300" height="126" srcset="https://betterexplained.com/wp-content/uploads/2012/12/Fourier_Toy-300x126.png 300w, https://betterexplained.com/wp-content/uploads/2012/12/Fourier_Toy-768x323.png 768w, https://betterexplained.com/wp-content/uploads/2012/12/Fourier_Toy-1024x431.png 1024w, https://betterexplained.com/wp-content/uploads/2012/12/Fourier_Toy-320x135.png 320w, https://betterexplained.com/wp-content/uploads/2012/12/Fourier_Toy-200x84.png 200w, https://betterexplained.com/wp-content/uploads/2012/12/Fourier_Toy-400x168.png 400w, https://betterexplained.com/wp-content/uploads/2012/12/Fourier_Toy.png 1438w" sizes="(max-width: 300px) 100vw, 300px"></picture></a></p>

<p>(<a href="http://www.reddit.com/r/math/comments/17v8cv/visualized_fourier/c89c3pa">Detailed list</a> of control options)</p>
<p>The Fourier Transform is about cycles added to cycles added to cycles. Try making a "time spike" by setting a amplitude of 1 for every component (press Enter after inputting each number). Fun fact: with enough terms, you can draw any shape, even <a href="https://www.youtube.com/watch?v=QVuU2YCwHjw&amp;feature=youtu.be">Homer Simpson</a>.</p>
<iframe loading="lazy" src="https://www.youtube.com/embed/QVuU2YCwHjw" width="420" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe>
<p>Check out <a href="http://www.jezzamon.com/fourier/">http://www.jezzamon.com/fourier/</a> for a great tool to draw any shape using epicycles.</p>
<p><a href="http://www.jezzamon.com/fourier/"><picture><source srcset="https://betterexplained.com/wp-content/webp-express/webp-images/uploads/2019/01/Fourier-Transforms-2019-01-12-15-11-48.png.webp" type="image/webp"><img decoding="async" alt="An Interactive Guide To The Fourier Transform" src="https://betterexplained.com/wp-content/uploads/2019/01/Fourier-Transforms-2019-01-12-15-11-48.png" width="500"></picture>
</a></p>
<h2>Appendix: Article with R code samples</h2>
<p>João Neto made a great writeup, with technical (R) code samples here:</p>
<p><a href="http://www.di.fc.ul.pt/~jpn/r/fourier/fourier.html">http://www.di.fc.ul.pt/~jpn/r/fourier/fourier.html</a></p>
<h2>Appendix: Using the code</h2>
<p>All the code and examples are open source (MIT licensed, do what you like).</p>
<ul>
<li><a href="https://betterexplained.com/examples/fourier/">Interactive example</a> (view source)</li>
<li><a href="https://gist.github.com/129d477ddb1c8025c9ac">Github gist</a></li>
<li><a href="http://www.reddit.com/r/programming/comments/156a9i/an_interactive_guide_to_the_fourier_transform/c7k9b1t?context=3">Reddit discussion</a> on details of the computation, I'm pb_zeppelin</li>
</ul>
<h2>Other Posts In This Series</h2><ol>
<li><a href="https://betterexplained.com/articles/a-visual-intuitive-guide-to-imaginary-numbers/" title="A Visual, Intuitive Guide to Imaginary Numbers">A Visual, Intuitive Guide to Imaginary Numbers</a></li>
<li><a href="https://betterexplained.com/articles/intuitive-arithmetic-with-complex-numbers/" title="Intuitive Arithmetic With Complex Numbers">Intuitive Arithmetic With Complex Numbers</a></li>
<li><a href="https://betterexplained.com/articles/understanding-why-complex-multiplication-works/" title="Understanding Why Complex Multiplication Works">Understanding Why Complex Multiplication Works</a></li>
<li><a href="https://betterexplained.com/articles/intuitive-guide-to-angles-degrees-and-radians/" title="Intuitive Guide to Angles, Degrees and Radians">Intuitive Guide to Angles, Degrees and Radians</a></li>
<li><a href="https://betterexplained.com/articles/intuitive-understanding-of-eulers-formula/" title="Intuitive Understanding Of Euler's Formula">Intuitive Understanding Of Euler's Formula</a></li>
<li><a href="https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/" title="An Interactive Guide To The Fourier Transform">An Interactive Guide To The Fourier Transform</a></li>
<li><a href="https://betterexplained.com/articles/intuitive-convolution/" title="Intuitive Guide to Convolution">Intuitive Guide to Convolution</a></li>
<li><a href="https://betterexplained.com/articles/intuitive-understanding-of-sine-waves/" title="Intuitive Understanding of Sine Waves">Intuitive Understanding of Sine Waves</a></li>
<li><a href="https://betterexplained.com/articles/linear-algebra-guide/" title="An Intuitive Guide to Linear Algebra">An Intuitive Guide to Linear Algebra</a></li>
<li><a href="https://betterexplained.com/articles/matrix-multiplication/" title="A Programmer's Intuition for Matrix Multiplication">A Programmer's Intuition for Matrix Multiplication</a></li>
<li><a href="https://betterexplained.com/articles/imaginary-multiplication-exponents/" title="Imaginary Multiplication vs. Imaginary Exponents">Imaginary Multiplication vs. Imaginary Exponents</a></li>
<li><a href="https://betterexplained.com/articles/hyperbolic-functions/" title="Intuitive Guide to Hyperbolic Functions">Intuitive Guide to Hyperbolic Functions</a></li>
</ol>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: AI/ML papers to catch up with current state of AI? (177 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38652736</link>
            <guid>38652736</guid>
            <pubDate>Fri, 15 Dec 2023 10:19:14 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38652736">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="38654772"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654772" href="https://news.ycombinator.com/vote?id=38654772&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>The good (and some might say bad thing) is that when it comes to fundamental technologies there are only 2 that are relevant:<p>1. Transformers
2. Diffusion</p><p>The benefit is that, focus on understanding them both reeaaalllyy well and you are at the forefront of research;)</p><p>Also, what is the reason you want to do this? If it is about building some kind of AI enabled app, you don't have to read anything. Get an API key and let's go the barrier has never been lower.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38654200"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654200" href="https://news.ycombinator.com/vote?id=38654200&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span><a href="https://trendingpapers.com/" rel="nofollow noreferrer">https://trendingpapers.com/</a><p>This tool can help you find what's new &amp; relevant to read. It's updated every day (based on ArXiv).</p><p>You can filter by category (Computer Vision, Machine Learning, NLP, etc), by release date, but most importantly, you can rank by PageRank (proxy of influence/readership), PageRank growth (to see the fastest growing papers in terms of influence), total # of citations, etc...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38654526"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38654526" href="https://news.ycombinator.com/vote?id=38654526&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>I'd be wary of programmatic lists that claim to track the most important recent papers. There's a ridiculous amount of <i>hype/propaganda</i> and <i>citation hacking</i> surrounding new AI research, making it hard to discern what will truly stand the test of time. Tomas Mikolov just posted about this:<p><a href="https://news.ycombinator.com/item?id=38654038">https://news.ycombinator.com/item?id=38654038</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="38654264"><td></td></tr>
                <tr id="38654737"><td></td></tr>
                  <tr id="38653719"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38653719" href="https://news.ycombinator.com/vote?id=38653719&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>My view is to focus on doing stuff. That's what I did. Pick up some task you want the model to do, try finetuning llama, playing with APIs from OpenAI, etc. Googling and asking GPT along the way.<p>Foundational model training got so expensive that unless you can get hired by "owns nuclear power plant of GPUs" you are not going to get any "research" done. And as the area got white-hot those companies have more available talent than hardware nowadays. So just getting into the practitioner area is the best way to get productive with those models. And you improve as a practitioner by practicing, not by reading papers.</p><p>If you're at the computer, your time is best spent writing code and interacting with those models in my opinion. If you cannot (e.g. commute) I listen to some stuff (e.g. <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g" rel="nofollow noreferrer">https://www.youtube.com/watch?v=zjkBMFhNj_g</a> - Anything from Karpathy on youtube, or <a href="https://www.youtube.com/@YannicKilcher" rel="nofollow noreferrer">https://www.youtube.com/@YannicKilcher</a> channel).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38654704"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654704" href="https://news.ycombinator.com/vote?id=38654704&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>Bear in mind that ML skillset is now bifurcating into two components. On the one side are the people who work at places like OpenAI/DeepMind/Mistral/etc, who have billion dollar compute budgets. They are the ones who will create the foundational models. At this point a lot of this work is very technically narrow, dealing with CUDA, GPU issues, numerical stability, etc. On the other side are people who are using the models through the APIs in various ways. This is much more open-ended and potentially creative, but you don't need to know how QLearning works to do this.<p>It's a bit analogous to the situation with microprocessors. There is a ton of deep technical knowledge about how chips work, but most of this knowledge isn't critical for mainstream programming.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38654486"><td></td></tr>
            <tr id="38653406"><td></td></tr>
            <tr id="38653159"><td></td></tr>
                <tr id="38653205"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38653205" href="https://news.ycombinator.com/vote?id=38653205&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>Thanks! Purchased a copy for myself and a friend.<p>And, Francois could easily report the unauthorized seller to Amazon, or send S&amp;D letter, suing not required.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="38654333"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654333" href="https://news.ycombinator.com/vote?id=38654333&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span><i>Build something</i> of personal interest to you. Start by looking for similar open-source projects online. Look at the online posts of the authors. <i>Then</i> look for the papers that <i>you</i> think will be useful for <i>your</i> project. Before you know it, you'll become an expert in your area of interest.<p>Above all, be wary of programmatic lists that claim to track the most important recent papers. There's a ridiculous amount of <i>hype/propaganda</i> and <i>citation hacking</i> surrounding new AI research, making it hard to discern what will truly stand the test of time. Tomas Mikolov just posted about this.[a]</p><p>---</p><p>[a] <a href="https://news.ycombinator.com/item?id=38654038">https://news.ycombinator.com/item?id=38654038</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38654256"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654256" href="https://news.ycombinator.com/vote?id=38654256&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>I kind of despair of keeping up to date with ML, at least to the extent that I might ever get current enough to be paid to work with it. I did Andrew Ng's Coursera specialisation a few years back - and I've worked through some of the developer-oriented courses, implemented some stuff. read more than a few books, read papers (the ones I might have a hope of understanding), and tried to get a former employer to take it seriously. But its seeming like unless you have a PhD or big-co experience then its very difficult to keep up to date by working in the field.<p>Notwithstanding the above, I'd agree with others here who suggest learning by doing/implementing, not reading papers.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38653228"><td></td></tr>
                <tr id="38654346"><td></td></tr>
                  <tr id="38654362"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654362" href="https://news.ycombinator.com/vote?id=38654362&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>Posted in another thread, but sadly I got no replies...<p>Related question: how can I learn how to read the mathematical notation used in AI/ML papers? Is there a definitive work that describes the basics? I am a post-grad Engineer, so I know the fundamentals, but I'm really struggling with a lot of the Arxiv papers. Any pointers hugely appreciated.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38654475"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38654475" href="https://news.ycombinator.com/vote?id=38654475&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>I particularly enjoyed Kevin Murphy's book [0] for being just rigorous enough to satisfy but not too dry, but also not trying to add humor unnecessarily.  It's not the best introduction text but it's great for someone with a little familiarity in the field who wants to broaden their understanding.  There are proofs to rationalize some approaches, but not to the degree that would satisfy a hardcore mathematicians maybe, but tbh I think that's a good thing for a book of this scope.<p>If you find a sample, it may include the index of symbols in the beginning which is pretty comprehensive and may satisfy your question on its own.</p><p><a href="https://www.goodreads.com/book/show/15857489-machine-learning" rel="nofollow noreferrer">https://www.goodreads.com/book/show/15857489-machine-learnin...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                      <tr id="38654573"><td></td></tr>
                  <tr id="38653497"><td></td></tr>
            <tr id="38654135"><td></td></tr>
            <tr id="38653865"><td></td></tr>
            <tr id="38653235"><td></td></tr>
            <tr id="38653407"><td></td></tr>
            <tr id="38654078"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654078" href="https://news.ycombinator.com/vote?id=38654078&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><br><div>
                  <p><span>can I get some insights on ai and robotics some papers to implement and get my hands dirty</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="38654107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654107" href="https://news.ycombinator.com/vote?id=38654107&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>you don’t need papers, Arxiv are self aggrandizement from some meme in East Asia<p>just join communities on discord or locallama on reddit
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
    </channel>
</rss>