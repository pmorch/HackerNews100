<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 28 May 2025 12:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[AI: Accelerated Incompetence (120 pts)]]></title>
            <link>https://www.slater.dev/accelerated-incompetence/</link>
            <guid>44114631</guid>
            <pubDate>Wed, 28 May 2025 10:50:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.slater.dev/accelerated-incompetence/">https://www.slater.dev/accelerated-incompetence/</a>, See on <a href="https://news.ycombinator.com/item?id=44114631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
<article>
  <!-- Page Start inject -->
  

  <header>
    
    <div>
  
  <p><img src="https://www.slater.dev/images/doug.jpg" alt="Doug Slater">
  </p>

  
  <p><span>Doug</span>
    
    
    <span>·</span>
    <time>2025-05-19</time>
    
    
  </p>
</div>

  </header>

  

  <!-- TOC -->
  <!---->


<!---->

  <!-- Content -->
  <section><p><em>In software engineering, over-reliance on LLMs accelerates incompetence. LLMs can't replace human critical thinking.</em></p>
<p><em>The text in this essay was written without any use of AI.</em></p>
<img src="https://www.slater.dev/llm_dependence.jpg" alt="A chart showing a speculative inverse correlation between LLM dependence and IQ">
<p>
    A speculative inverse correlation between LLM dependence and IQ
  </p>
<p>By now much ink has dried on the wave of AI and LLMs which crashed upon the public consciousness in late 2022. As an experienced software engineer, I'd like to speak to two troubling engineering perspectives I've observed on LLMs.</p>
<h2 id="llms-are-my-friend">"LLMs are my friend"</h2>
<p>I don't think anyone believes that a computer program is literally their companion, so let's address the euphemistic intent of the above phrase: namely that an LLM conveys magnificent benefits upon its user.</p>
<p>Engineers who view LLMs as an ally invariably prioritize or feel pressured to prioritize velocity; for them, production trumps perspicacity. While it's true that LLMs can deliver a lot of code quickly, their use carries a long tail of <em>risks</em>.</p>
<h2 id="risks-of-using-llm">Risks of using LLM</h2>
<ul>
<li><strong>Output Risk</strong>. An LLM can give output that is blatantly incorrect, for example code that won't compile. More likely and dangerously, it can give output that is subtly and undetectably wrong, like logic bugs. The risk is elevated if the prompter is not qualified to evaluate the output, for example project managers prompting for source code.</li>
<li><strong>Input Risk</strong>. An LLM does not challenge a prompt which is leading<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">1</a></sup> or whose assumptions are flawed or context is incomplete. Example: An engineer prompts, "Provide a thread-safe list implementation in C#" and receives 200 lines of flawless, correct code. It's still the wrong answer, because the question should have been, "How can I make this code thread-safe?" and whose answer is "Use <code>System.Collections.Concurrent</code>" and 1 line of code. The LLM is not able to recognize an instance of the XY problem<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">2</a></sup> because it was not asked to.</li>
<li><strong>Future Velocity</strong>. This is your typical "tech debt" argument, but more urgent. AI can degrade the quality of your codebase <em>so fast</em>. Have you ever seen the fruits of hoarding disorder? From the outside, a house or apartment may look fine. But the inside is unsanitary, reprehensible, and nonfunctional. Developers are discovering that without strong guardrails, code produced by an LLM is like such a space.</li>
<li><strong>User Infantilization</strong>. An extinction of talent will occur within individuals and organizations that outsource thinking and problem solving to LLMs:
<ul>
<li>As senior engineers are deprived of the opportunity to learn through productive struggle, their existing problem solving and critical thinking skills atrophy:
<ul>
<li>"Microsoft research on knowledge workers found that AI-driven confidence often comes at the expense of critical thinking"<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">3</a></sup></li>
<li>"In a world pushing for “reflexive AI usage,” I’m advocating for something different: thoughtful, intentional collaboration with AI that preserves the essence of coding as a craft"<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">4</a></sup></li>
<li>"LLMs give me finished thoughts, polished and convincing, but none of the intellectual growth that comes from developing them myself" <sup><a href="https://www.slater.dev/accelerated-incompetence/#references">5</a></sup></li>
</ul>
</li>
<li>Junior engineers never develop such skills to begin with and so can never in turn mentor future junior engineers.</li>
</ul>
</li>
<li><strong>Loss of Joy</strong>. Many developers are reporting that using AI robs them of flow state and the joy of creation.<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">6</a></sup> AI-generated code is miserable to read and change.</li>
</ul>
<p>In a future post, I plan to write about mitigations for each of these risks. Be sure to subscribe below if that sounds interesting.</p>
<h2 id="i-ll-become-redundant">"I'll become redundant"</h2>
<p>Source<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">7</a></sup></p>
<p>No, you won't. That said, there are certainly things you can do to further differentiate yourself from an LLM. To stay on topic, I'll defer that to a future post.</p>
<p>There are two programming competences that LLMs cannot furnish: <em>program theory</em> and <em>program entropy</em>.</p>
<h2 id="program-theory">Program Theory</h2>
<blockquote>
<p>...programming properly should be regarded as an activity by which the programmers form or achieve a certain kind of insight, a theory, of the matters at hand</p>
</blockquote>
<p>-- Peter Naur, <em>Programming as Theory Building</em>, 1985<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">8</a></sup></p>
<p>Naur was one of the greats in computing. He argued, against popular belief at the time, that a program is not its source code. Rather, the program is a shared mental construct: a <em>theory</em> or <em>design</em>. From that, the engineer derives code, but the work product of value is the design, not code.</p>
<p>To help you think about the difference between program theory and program text, consider this thought experiment: Imagine that two engineering teams of equivalent talent, A and B, are locked in separate rooms. Each team is told not to communicate with the other. Team A is tasked to write a program, for example a simple terminal-based Chess game. Team B just waits, plays real Chess, or whatever. When Team A is finished, their source code is handed to Team B. Now each team is asked in parallel to add a feature to the program, for example a virtual chess player so the game can be played solo. (We'll let Team A take a coffee break before they get started).</p>
<p><em>Question</em>: Which team will deliver a better solution?</p>
<p><em>Answer</em>: Team A, because those engineers have a fresh mental model of the program they just created, while Team B has none.</p>
<p>According to Naur, the theory matters because inevitably a program needs to be <em>maintained</em>, i.e. modified after its initial creation. If all you have is the source code and not an internalized understanding of its design, the cost for those modifications will be higher. I think we can each remember a time we were introduced to a big existing codebase. At first our productivity was near zero. As we loaded the program theory into our mind, productivity rose.</p>
<h3 id="llms-and-program-theory">LLMs and Program Theory</h3>
<p>LLMs as they currently exist cannot master a theory, design, or mental construct because they don't remember beyond their context window. Only humans can can gain and retain program theory.</p>
<h2 id="program-entropy">Program Entropy</h2>
<p>Complexity is a fundamental opposing force of programming<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">9</a></sup>, and it correlates with entropy.</p>
<blockquote>
<p>...program building is an entropy-decreasing process...program maintenance is an entropy-increasing process, and even its most skillful execution only delays the subsidence of the system into unfixable obsolescence</p>
</blockquote>
<p>-- Fred Brooks, <em>The Mythical Man-Month</em>, 1975</p>
<p>Brooks, another prominent historical figure in computing, asserted that after initial construction, the changes made to a program can only make the source code more complex. However, changes made in harmony with the design will do so at a slower rate.</p>
<h3 id="llms-and-program-entropy">LLMs and Program Entropy</h3>
<p>An LLM is a token predictor. It works only at the level of text. It is not capable of working at a conceptual level: it doesn't reason about ideas, diagrams, or requirements specifications. Everyone who has prompted an LLM with a large chunk of code has beheld that the LLM tends to apply unnecessary and bizarre changes, and the longer the conversation drags on, the more it diverges. How often have you witnessed an LLM <em>reduce</em> the complexity of a piece of code?</p>
<p>Only humans can decrease or resist complexity.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We found wisdom for the LLM age by remembering what two forerunners of our discipline had to say about software design and complexity.</p>
<p>If you had hoped that AI would launch your engineering career to the next level, be warned that it could do the opposite. <em>LLMs can accelerate incompetence.</em></p>
<p>If you're a skilled, experienced engineer and you fear that AI will make you unemployable, adopt a more nuanced view. <em>LLMs can't replace human engineering.</em></p>
<p>The business allure of AI is reduced costs through commoditized engineering, but just like offshore engineering talent brings forth mixed fruit, LLMs fall short and open risks.</p>
<p>The AI hype cycle will eventually peak<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">10</a></sup>. Companies which overuse AI now will inherit a long tail of costs, and they'll either pivot or go extinct. As such, the long-term value proposition for humans in engineering remains unchanged. The world still needs and will pay for technical skills and deep thinking in engineering.</p>
<p>AI will stick around, though. Use it as a tool, not a crutch, and continue to invest in the same fundamental engineering skills that were deemed valuable in 2019.</p>
<h2 id="next">Next...</h2>
<p>Subscribe to my email list below. I plan to write more.</p>
<h2 id="references">References</h2>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Leading_question">Leading Question</a></li>
<li><a href="https://en.wikipedia.org/wiki/XY_problem">The XY Problem</a></li>
<li><a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/radar/2025/04/tr_technology_radar_vol_32_en.pdf">ThoughtWorks Technology Radar Volume 32</a></li>
<li><a href="https://cekrem.github.io/posts/coding-as-craft-going-back-to-the-old-gym/">Coding as Craft: Going Back to the Old Gym</a></li>
<li><a href="https://dcurt.is/thinking">Thoughts on Thinking</a></li>
<li><a href="https://terriblesoftware.org/2025/04/23/the-hidden-cost-of-ai-coding/">The Hidden Cost of AI Coding</a></li>
<li><a href="https://www.reddit.com/r/ExperiencedDevs/comments/1h3xpke/dont_know_if_the_right_place_how_to_work_on/">"I wonder if I'll become redundant"</a></li>
<li><a href="https://pablo.rauzy.name/dev/naur1985programming.pdf">Programming as Theory Building</a></li>
<li><a href="https://grugbrain.dev/#grug-on-complexity">Grug on Complexity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gartner_hype_cycle">Gartner Hype Cycle</a></li>
</ol>
</section>

  <hr>

              <div>
                <h3>Subscribe for More</h3>
                <h5>I'll tell you about new posts. I take your privacy seriously.</h5>
                
            </div>
 

  <!-- Post Taxonomies -->
  


<!---->

  <!-- Post Nav -->
  
<nav>
  
  <a href="https://www.slater.dev/tech-risk-is-business-risk/"><span>←</span><span>Tech Risk is Business Risk</span></a>
  <!---->
  
</nav>

<!---->

  <!-- Comment -->
  <!---->
  


<!---->
<!---->
  

  <!-- Page End inject -->
  
</article>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cory Doctorow on how we lost the internet (105 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/</link>
            <guid>44113735</guid>
            <pubDate>Wed, 28 May 2025 08:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/">https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/</a>, See on <a href="https://news.ycombinator.com/item?id=44113735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>
Cory Doctorow <a href="https://craphound.com/bio/">wears many hats</a>:
digital activist, science-fiction author, journalist, and more.  He has
also written many books, both fiction and non-fiction, runs the <a href="https://pluralistic.net/">Pluralistic blog</a>, is a visiting
professor, and is an advisor to the <a href="https://www.eff.org/">Electronic
Frontier Foundation</a> (EFF); his <a href="https://chokepointcapitalism.com/"><i>Chokepoint Capitalism</i></a>
co-author, Rebecca Giblin, gave a <a href="https://lwn.net/Articles/927278/">2023 keynote
in Australia</a> that we covered.  Doctorow gave a rousing keynote on
the state of the "enshitternet"—today's internet—to kick
off the recently held <a href="https://us.pycon.org/2025/">PyCon US
2025</a> in Pittsburgh, Pennsylvania.
</p>

<p>
He began by noting that he is known for coining the term
"<a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a>" about the decay of tech platforms, so attendees were
probably expecting to hear about that; instead, he wanted to start by
talking about nursing.  A <a href="https://rooseveltinstitute.org/publications/uber-for-nursing/">recent
study</a> described how nurses are increasingly getting work through one of
three main apps that "<q>bill themselves out as 'Uber for nursing'</q>".
The nurses never know what they will be paid per hour prior to accepting a
shift and the three companies act as a cartel in order to "<q>play all
kinds of games with the way that labor is priced</q>".
</p>

<p>
In particular, the
companies purchase financial information from a data broker before offering
a nurse a shift; if the nurse is carrying a lot of credit-card debt,
especially if some of that is delinquent, the amount offered is
reduced. "<q>Because, the more desperate you are, the less you'll accept to
come into work and do that grunt work of caring for the sick, the elderly,
and the dying.</q>"  That is horrific on many levels, he said, but "<q>it
is emblematic of 'enshittification'</q>", which is one of the reasons he
highlighted it.
</p>

<h4>Platform decay</h4>

<p>
Enshittification is a three-stage process; he used Google to
illustrate the idea.  At first, Google minimized ads and maximized spending
on engineering to produce a great search engine; while it was doing that,
however, it was buying its way to dominance. "<q>They bribed every service,
every product
that had a search box to make sure that that was a Google search box.</q>"
No matter which browser, phone carrier, or operating system you were using,
Google ensured that you were using its search by default; by the early
2020s, it was spending the equivalent of buying a Twitter every 18 months
to do so, he said.  That is the first stage of the process: when the
provider is being good to its users, but is finding ways to lock them in.
</p>

<p><a href="https://lwn.net/Articles/1022663/">
<img src="https://static.lwn.net/images/2025/pycon-doctorow-sm.png" alt="[Cory Doctorow]" title="Cory Doctorow" width="206" height="300">
</a></p><p>
The second phase occurs once the company recognizes that it has users
locked in, so it will be difficult for them to switch away, and it shifts
to making things worse for its users in order to enrich its business
customers.  For Google, those are the publishers and advertisers.  A
growing portion of the search results page is shifted over to ads
"<q>marked off with ever-subtler, ever-smaller, ever-grayer labels
distinguishing them from the organic search results</q>".  While the
platform is getting better for business customers—at the expense of the
users—those customers are also getting locked in.
</p>

<p>
Phase three of enshittification is when the value of the platform is
clawed back until all that is left is kind of a "<q>homeopathic residue—the
least value needed to keep both business customers and end users locked to
the platform</q>".  We have gained a view into this process from the three
monopoly cases that Google has lost over the last 18 months. In 2019, the
company had 90% of the world's search traffic and its users were loyal;
"<q>everyone who searched on Google, searched everything on Google</q>".
</p>

<p>
But that meant that Google's search growth had plateaued, so how was the
company going to be able to grow?  It could "<q>raise a billion humans to
adulthood and make them Google customers, which is <a href="https://classroom.google.com/">Google Classroom</a>, but that's a
slow process</q>".  From the internal memos that came to light from the
court cases, we can see what the company chose to do, he said: "<q>they
made search worse</q>".  
</p>

<p>
The accuracy of the search results was reduced, which meant that users
needed to do two or three queries to the get the results they would have
seen on the first page.  That increased the number of ads that could be
shown, which is obviously bad for searchers, but the company was also
attacking its business customers at the same time.  For example, "<q>Google entered into
an illegal, collusive arrangement with Meta, called <a href="https://en.wikipedia.org/wiki/Jedi_Blue">Jedi Blue</a></q>" that
"<q>gamed the advertising market</q>" so that publishers got paid less and
advertisers had to pay more, he said.
</p>

<p>
So that's how we have ended up at the Google of today, where the top of the
search results page is "<q>a mountain of AI slop</q>", followed by five
paid results "<q>marked with the word 'Ad' in eight point, 90%
gray-on-white type</q>", ending with "<q>ten spammy SEO [search-engine
optimization] links from someone else who's figured out how to game
Google</q>".  The amazing thing is "<q>that we are still using Google
because we're locked into it</q>".  It is a perfect example of the result
of the "<q>tragedy in three acts</q>" that is enshittification.
</p>

<h4>Twiddling</h4>

<p>
The underlying technical means that allows this enshittification is
something he calls "twiddling".  Because the companies run their apps on
computers, they can change a nearly infinite number of knobs to potentially
alter "<q>the prices, the cost, the search rankings, the
recommendations</q>" each time the platform is visited.  Going back to the
nursing example, "<q>that's just twiddling, it's something you can only do
with computers</q>".
</p>

<p>
Legal scholar Veena Dubal coined the term "<a href="https://en.wikipedia.org/wiki/Algorithmic_wage_discrimination">algorithmic
wage discrimination</a>" to describe this kind of twiddling for the "gig
economy", which is "<q>a major locus for enshittification</q>"; the nursing
apps, Uber, and others are examples of that economy. "<q>Gig work is that
place where your shitty boss is a shitty app and you're not allowed to call
yourself an employee.</q>"
</p>

<p>
Uber invented a particular form of algorithmic wage discrimination; if its
drivers are picky about which rides they accept, Uber will slowly raise the
rates to entice those drivers—until they start accepting rides.  Once a
driver does accept a ride, "<q>the wage starts to push down and down at
random intervals in increments that are too small for human beings to
readily notice</q>".  It is not really "<q>boiling the frog</q>", Doctorow
said, so much as it is "<q>slowly poaching it</q>".
</p>

<p>
As anyone with a technical background knows, "<q>any task that is simple,
but time-consuming is a prime candidate for automation</q>".  This
kind of "<q>wage theft</q>" would be tedious and expensive to do by hand,
but it is trivial to play these games using computers.  This kind of thing
is not just bad for nurses, he said, its bad for those who are using their
services.
</p><blockquote>
Do you really think that paying nurses based on how desperate
they are, at a rate calculated to increase their desperation so that
they'll accept ever-lower wages,
is going to result in us getting the best care when we see a nurse?  Do you
really want your catheter inserted by a nurse on food stamps who drove an
Uber until midnight the night before and skipped breakfast this morning so
that they could pay the rent?
</blockquote>


<h4>Paying and products</h4>

<p>
It is misguided to say "<q>if you're not paying for the product, you're the
product</q>", because it makes it seem like we are complicit in sustaining
<a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance
capitalism</a>—and we are not. The thinking goes that if we were only
willing to start paying for things, "<q>we could restore capitalism to its
functional non-surveillance state and companies would treat us better
because we'd be customers and not products</q>".  That thinking elevates
companies like Apple as "<q>virtuous alternatives</q>" because the company
charges money and not attention, so it can focus on improving the
experience for its customers.
</p>

<p>
There is a small sliver of truth there, he said; Apple rolled out a feature
on its phones that allowed users to opt-out of third-party
surveillance—notably Facebook tracking.  96% of users opted out, he said;
the other 4% "<q>were either drunk or Facebook employees or drunk Facebook
employees</q>".  
</p>

<p>
So that makes it seem like Apple will not treat its customers as products,
but at the same time it added the opt-out, the company secretly started gathering
exactly the same information for its "<q>own surveillance
advertising network</q>".  There was no notice given to users and no way to
opt out of that surveillance; when journalists discovered it and published
their findings, Apple "<q>lied about it</q>".  The "<q>$1000 Apple
distraction rectangle in your pocket is something you paid for</q>", but
that does not stop Apple from "<q>treating you like the product</q>".
</p>

<p>
It is not just end users that Apple treats like products; the app vendors
are also treated that way with 30% fees for payment processing in the App
Store. That's what is happening with gig-app nurses: "<q>the nurses are the
product, the patients are the product, the hospitals are the product—in
enshittification, the product is anyone you can productize</q>".
</p>

<p>
While it is tempting to blame tech, Doctorow said, these companies did not
start out enshittified.  He recounted the "<q>magic</q>"  when Google debuted;
"<q>you could <a href="https://en.wikipedia.org/wiki/Ask.com">ask
Jeeves</a> questions for a thousand years and still not get an answer as
crisp, as useful, as helpful as the answer you would get by typing a few
vague keywords</q>" into Google.  Those companies spent decades producing
great products, which is why people switched to Google, bought iPhones, and
joined their friends on Facebook.  They were all born digital, thus could
have enshittified at any time, "<q>but they didn't, until they did, and
then they did it all at once</q>". 
</p>

<p>
He believes that changes to the policy environment is what has led to
enshittification, not changes in technology.  These changes to the rules of
the game were "<q>undertaken in living memory by named parties who were
warned at the time of the likely outcomes</q>"—and did it anyway.
Those people are now extremely rich and respected; they have "<q>faced no
consequences, no accountability for their role in ushering in the
Enshittocene</q>".  We have created a perfect breeding ground for the worst
practices in our society, which allowed them to thrive and dominate
decision-making for companies and governments "<q>leading to a vast
enshittening of everything</q>".
</p>

<p>
That is a dismal outlook, he said, but there is a bit of good news hidden
in there.  This change did not come about because of a new kind of evil
person or the weight of history, but rather because of specific policy
choices that were made—and can be unmade.  We can consign the enshitternet
to the scrap heap as
simply "<q>a transitional state from the old good internet that we used to
have and the new good internet that we could have</q>".
</p>

<p>
All companies want to maximize profits and the equation to do so is simple:
charge as much as you can, pay suppliers and workers as little as you can,
and spend the smallest amount possible on quality and safety.  The
theoretically "perfect" company that charges infinity and spends nothing
fails because no one wants to work for it—or buy anything from it.  That
shows that there are external constraints that tend to tamp down the
"<q>impulse to charge infinity and deliver nothing</q>".
</p>

<h4>Four constraints</h4>

<p>
In technology, there are four constraints that help make companies
better; they help push back against the impulse to enshittify.  The first
is markets; businesses that charge more and deliver less lose customers,
all else being equal.  This is the bedrock idea behind capitalism and it is
also the basis of antitrust law, but the
rules on antitrust have changed since the <a href="https://en.wikipedia.org/wiki/Sherman_Antitrust_Act">Sherman
Antitrust Act</a> was enacted in 1890.  More than forty years ago, during the Reagan
administration in the US, the interpretation of what it means to be a
monopoly was changed, not just in US, but also with its major trading
partners in the UK, EU, and Asia.
</p>

<p>
Under this interpretation, monopolies are assumed to be efficient; if
Google has 90% of the market, it means that it deserves to be there because
no one can possibly do search any better.  No competitor has arisen because
there is no room to improve on what Google is doing. This pro-monopoly
stance did exactly what might be expected, he said, it gave us more
monopolies: "<q>in pharma, in beer, in glass bottles, vitamin C, athletic
shoes, microchips, cars, mattresses, eyeglasses, and, of course,
professional wrestling</q>", he said to laughter.
</p>

<p>
Markets do not constrain technology firms because those firms do not compete
with their rivals—they simply buy their rivals instead. That is confirmed
by a memo from Mark Zuckerberg—"<q>a man who puts all of his dumbest ideas
in writing</q>"—who wrote: "<q>It is better to buy than to compete</q>".
Even though that anti-competitive behavior came to light before Facebook
was allowed to buy Instagram in order to ensure that users switching would
still be part of Facebook the platform, the Obama administration
permitted the sale.  Every government over the past 40 years, of all political stripes, has treated monopolies as efficient,
Doctorow said.
</p>

<p>
Regulation is also a constraint, unless the regulators have already been
captured by the industry they are supposed to oversee.  There are several
examples of <a href="https://en.wikipedia.org/wiki/Regulatory_capture">regulatory
capture</a> in the nursing saga, but the most egregious is that anyone in
the US can obtain financial information on anyone else in the country,
simply by contacting a data broker.  "<q>This is because the US congress
has not passed a new consumer privacy law since 1988.</q>"  The <a href="https://en.wikipedia.org/wiki/Video_Privacy_Protection_Act">Video
Privacy Protection Act</a> was aimed at stopping video-store clerks from
telling newspapers what VHS video titles were purchased or rented, but no
protections have been added since then.
</p>

<p>
The reason congress has not addressed privacy legislation "<q>since <a href="https://en.wikipedia.org/wiki/Die_Hard"><i>Die
Hard</i></a> was in its first run in theaters</q>" is neither a coincidence
nor an oversight, he said.  It is "<q>expensively purchased inaction</q>"
by an industry that has "<q>monetized the abuse of human rights at
unimaginable scale</q>".  The coalition in favor of freezing privacy law
keeps growing because there are so many ways to "<q>transmute the
systematic invasion of our privacy into cash</q>". 
</p>

<p>
Tech companies are not being constrained by either markets or governments,
but there are two other factors that could serve to tamp down "<q>the
reproduction of sociopathic, enshittifying monsters</q>" within these
companies.  The first is interoperability; in the non-digital world, it is
a lot of work to, say, ensure that any light bulb can be used with any
light socket.
In the digital world, all of our programs run on the same
"<q>Turing-complete, universal <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">Von Neumann machine</a></q>", so a program that
breaks interoperability can be undone with a program that restores it.
Every ten-foot fence can be surmounted with an 11-foot ladder; if HP writes
a program to ensure that third-party ink cannot be used with its printers, someone
can write a program to undo that restriction.
</p>

<p>
DoorDash workers generally make their money on tips, but the app hides the
amount of the tip until the driver commits to taking the delivery.  A
company called Para wrote a program that looked inside the JSON that was
exchanged to find the tip, which it then displayed <i>before</i> the driver
had to commit.  DoorDash shut down the Para app, "<q>because in America,
apps like Para are illegal</q>".  The 1998 <a href="https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_Act">Digital
Millennium Copyright Act</a> (DMCA) signed by Bill Clinton "<q>makes it a
felony to 'bypass an access control for a copyrighted work'</q>".  So even
just reverse-engineering the DoorDash app is a potential felony, which is
why companies are so desperate to move their users to apps instead of web
sites.  "<q>An app is just a web site that we have wrapped in a correct
DRM [<a href="https://en.wikipedia.org/wiki/Digital_rights_management">digital
rights management</a>] to make it a felony to protect your privacy while
you use it</q>", he said to widespread applause.
</p>

<p>
At the behest of the US trade representative, Europe and Canada have also
enacted DMCA-like laws.  This happened despite experts warning the leaders
of those countries that "<q>laws that banned tampering with digital locks
would let American tech giants corner digital markets in their
countries</q>".  The laws were a gift to monopolists and allowed companies
like HP to continually raise the price of ink until it "<q>has become the
most expensive substance you, as a civilian, can buy without a permit</q>";
printing a shopping list uses "<q>colored water that costs more than the
semen of a <a href="https://en.wikipedia.org/wiki/Kentucky_Derby">Kentucky-Derby</a>-winning
stallion</q>".
</p>

<p>
The final constraint, which did hold back platform decay for quite some
time, is labor. Tech workers have historically been respected and
well-paid, without unions.  The power of tech workers did not come from
solidarity, but from scarcity, Doctorow said.  The minute bosses ordered
tech workers to enshittify the product they were loyally working on,
perhaps missing various important social and family events  to
ship it on time, those workers could say no—perhaps in a much more coarse
way.  Tech workers could simply walk across the street "<q>and have a new
job by the end of the day</q>" if the boss persisted.
</p>

<p>
So labor held off enshittification after competition, regulation, and
interoperability were all systematically undermined and did so for quite
some time—until the mass tech layoffs.  There have been half a million
tech workers laid off since 2023, more are announced regularly, sometimes
in conjunction with raises for executive salaries and bonuses.  Now,
workers cannot turn their bosses down because there are ten others out
there just waiting to take their job.
</p>

<h4>Reversing course</h4>

<p>
Until we fix the environment we find ourselves in, the contagion will
spread to other companies, he said.  The good news is that after 40 years
of antitrust decline, there has been a lot of worldwide antitrust activity
and it is coming from all over the political spectrum.  The EU, UK,
Australia, Germany, France, Japan, South Korea, "<q>and China, yes,
China</q>" have passed new antitrust laws and launched enforcement actions.
The countries often collaborate, so a UK study on Apple's 30%
payment-processing fee was used by the EU to fine the company for billions
of euros and ban Apple's payment monopoly; those cases then found their way
to Japan and South Korea where Apple was further punished.
</p>

<p>
"<q>There are no billionaires funding the project to make billionaires
obsolete</q>", Doctorow said, so the antitrust work has come from and been
funded by
grassroots efforts.
</p>

<p>
Europe and Canada have passed strong right-to-repair legislation, but those
efforts "<q>have been hamstrung by the anti-circumvention laws</q>" (like
the DMCA).  Those laws can only be used if there are no locks to get
around, but the manufacturers ensure that every car, tractor, appliance,
medical implant, and hospital medical device has locks to prevent repair.
That raises the question of why these countries don't repeal their versions
of the DMCA.
</p>

<p>
The answer is tariffs, it seems.  The US trade representative has long
threatened countries with tariffs if they did not have such a law on their
books. "<q>Happy 'Liberation Day' everyone</q>", he said with a smile,
which resulted in laughter, cheering, and applause.  The response of most
countries when faced with the US tariffs (or threats thereof) has been to
impose retaliatory tariffs, making US products more expensive for their
citizens, which is a weird way to punish Americans.  "<q>It's like punching
yourself in the face really hard and hoping someone else says 'ouch'.</q>"
</p>

<p>
What would be better is for the countries to break the monopolies of the US
tech giants by making it legal to reverse-engineer, jailbreak, and modify
American products and services.  Let companies jailbreak Teslas and deliver
all of the features that ship in the cars, but are disabled by software,
for one price; that is a much better way to hurt Elon Musk, rather than by
expressing outrage at his Nazi salutes, since he loves the
attention. "<q>Kick him in the dongle.</q>"
</p>

<p>
Or, let
a Canadian company set up an App Store that only charges 3% for payment
processing, which will give any content producer an immediate 25% raise, so
publishers will flock to it.   The same could be done for car and tractor
diagnostic devices and more.
"<q>Any country in the world has it right now in their power to become a
tech-export powerhouse.</q>"
Doing so would directly attack the tech giants in their most profitable
lines of business: "<q>it takes the revenues
from those rip-off scams globally from hundreds of billions of dollars to
zero overnight</q>".  And "<q>that is how you win a trade war</q>", he said
to more applause.
</p>

<p>
He finished with a veritable laundry list of all of the ills facing the
world today (the "<q>omni-shambolic poly-crisis</q>"), both on and off the
internet, and noted that the tech giants
would willingly "<q>trade a habitable planet and human rights for a 3% tax
cut</q>".  But it did not have to be this way, "<q>the enshitternet was not
inevitable</q>" and was, in fact, the product of policy choices made by
known people in the last few decades.  "<q>They chose enshittification; we
warned them what would come of it and we don't have to be eternal prisoners
of the catastrophic policy blunders of clueless lawmakers of old.</q>"
</p>

<p>
There once was an "<q>old good internet</q>", Doctorow said, but it was
too difficult for non-technical people to connect up to; web 2.0 changed
that, making it easy for everyone to get online, but that led directly into
hard-to-escape walled gardens.  A new good internet is possible and needed; "<q>we can
build it with all of the technological self-determination of the old good
internet and the ease of web 2.0</q>".  It can be a place to come together
and organize in order to "<q>resist and survive climate collapse, fascism,
genocide, and authoritarianism</q>".  He concluded: "<q>we can build it and
we must</q>".
</p>

<p>
His speech was well-received and was met with a standing ovation.  Some of
his harshest rhetoric (much of which was toned down here) may not have been
popular with everyone, perhaps especially the PyCon sponsors who were named and
shamed in the keynote, but it did seem to resonate within the crowd of
attendees.   Doctorow's perspective is always interesting—and he certainly
pulls no punches.
</p>

<p>
A <a href="https://www.youtube.com/watch?v=ydVmzg_SJLw">YouTube video</a>
of the talk is available.
</p>

<p>
[I would like to thank LWN's travel sponsor, the Linux Foundation, for
supporting my travel to Pittsburgh for PyCon.]
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/ConferenceIndex/">Conference</a></td><td><a href="https://lwn.net/Archives/ConferenceIndex/#PyCon-2025">PyCon/2025</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why are 2025/05/28 and 2025-05-28 different days in JavaScript? (105 pts)]]></title>
            <link>https://brandondong.github.io/blog/javascript_dates/</link>
            <guid>44113397</guid>
            <pubDate>Wed, 28 May 2025 07:09:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brandondong.github.io/blog/javascript_dates/">https://brandondong.github.io/blog/javascript_dates/</a>, See on <a href="https://news.ycombinator.com/item?id=44113397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article><header>  <time datetime="2025-05-28">2025-05-28</time> </header><p>While setting up this site itself, I ran into the following oddity:</p>
<pre><code><span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025/05/28'</span>).<span>toDateString</span>()); <span>// Wed May 28 2025</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28'</span>).<span>toDateString</span>()); <span>// Tue May 27 2025</span>
<span>// Bonus: (omit leading 0)</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-5-28'</span>).<span>toDateString</span>()); <span>// Wed May 28 2025</span></code></pre>
<p>You may get different results on your machine!</p>
<section id="What's_going_on?"> <h2><a href="#What's_going_on?">What's going on?</a></h2> <p>A <code>Date</code> in JavaScript always represents a point in time (i.e. milliseconds since epoch). This is more apparent when printing out the full date string:</p><pre><code><span>const</span> date = <span>new</span> <span>Date</span>(<span>'2025/05/28'</span>);
<span>console</span>.<span>log</span>(date); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(date.<span>toDateString</span>()); <span>// Wed May 28 2025</span></code></pre><p>In this case, the passed-in date string is being interpreted as a timestamp in my local time zone. <code>toDateString()</code> also operates relative to the local time and so we get the same day-of-the-month back out.</p><p>The difference with <code>'2025-05-28'</code> is in parsing behavior; the string is interpreted as UTC and so ends up at a different point in time:</p><pre><code><span>const</span> date = <span>new</span> <span>Date</span>(<span>'2025-05-28'</span>);
<span>console</span>.<span>log</span>(date); <span>// Tue May 27 2025 17:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(date.<span>toDateString</span>()); <span>// Tue May 27 2025</span></code></pre><p>Why the discrepancy?</p> </section>
<section id="The_misadventures_of_browser_date-parsing"> <h2><a href="#The_misadventures_of_browser_date-parsing">The misadventures of browser date-parsing</a></h2> <p>After digging through the code and commit histories of Chrome/Firefox/Safari, I’ve reconstructed a timeline:</p><ol>
<li>In 2009, these browsers supported parsing a mishmash of date-time formats. When time zone offsets are not explicitly specified in the string, they all fall back to using local time, including for a date string like <code>'2025/05/28'</code>.</li>
<li><a href="https://ecma-international.org/wp-content/uploads/ECMA-262_5th_edition_december_2009.pdf" target="_blank">ES5</a>, to be released at the end of the year, includes a requirement for supporting a new standardized date-time format based heavily off of <a href="https://en.wikipedia.org/wiki/ISO_8601" target="_blank">ISO 8601</a>. This format is broken up into date-<em>only</em> forms like <code>'2025-05-28'</code> and date-<em>time</em> forms like <code>'2025-05-27T17:00-07:00'</code> where the ending UTC offset is optional.
<ul>
<li>What does the spec say about time zone interpretation for date-only forms (which never have an offset) or date-time forms missing an offset? Only that <q>The String may be interpreted as a local time, a UTC time, or a time in some other time zone, depending on the contents of the String.</q> (Gee, thanks…)</li>
</ul>
</li>
<li>Firefox is the first to <a href="https://github.com/mozilla-firefox/firefox/commit/b866df4f3680502a8e78e67bd495a96ea3d9c59e" target="_blank">implement this requirement</a>. They choose to interpret date-only forms as UTC and date-time forms missing an offset as local time. Not only is there now a discrepancy between <code>'2025/05/28'</code> and <code>'2025-05-28'</code>, but also surprising behavior like: <pre><code><span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28'</span>)); <span>// Tue May 27 2025 17:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28T00:00'</span>)); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span></code></pre></li>
<li>Chrome is <a href="https://chromium.googlesource.com/v8/v8.git/+/6ceb02e6eb791f837ed84b7ed41332058cd3f1dc" target="_blank">next</a>, choosing to use local time for both.</li>
<li>Safari is <a href="https://github.com/WebKit/WebKit/commit/d9bdbae4126006e130914e5ebe57a761d3ea19bb" target="_blank">next</a>, but its parsing logic incorrectly requires that all date, time, and offset fields be present.</li>
<li><a href="https://262.ecma-international.org/5.1/index.html#sec-15.9.4.2" target="_blank">ES5.1</a> releases in mid-2011 and now additionally mentions that <q>The value of an absent time zone offset is Z.</q></li>
<li>Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/ff9ce1abd4add01bbb3f1917bd72207e5ddd70b5" target="_blank">updates its implementation</a> to use UTC for both cases.</li>
<li>Safari <a href="https://github.com/WebKit/WebKit/commit/a841b97de44dbff4ecb30f62e984b1fc72493ac6" target="_blank">fixes the earlier bug</a> and uses UTC for both cases.</li>
<li>A <a href="https://web.archive.org/web/20141214115940/https://bugs.ecmascript.org/show_bug.cgi?id=112" target="_blank">bug</a> is filed against the spec itself, pointing out that ISO 8601 represents date-times without offsets as local time. <a href="https://262.ecma-international.org/6.0/index.html#sec-date-time-string-format" target="_blank">ES6</a> in 2015 replaces the ES5.1 addition with <q>If the time zone offset is absent, the date-time is interpreted as a local time.</q></li>
<li>Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/f06754a8e1d305a43560705f6c167d85d40e602d" target="_blank">switches back</a> to using local time for both cases.</li>
<li>A <a href="https://issues.chromium.org/issues/40440226" target="_blank">bug</a> is filed against Chrome for breaking backwards compatibility when parsing date-only forms. They <a href="https://chromium.googlesource.com/v8/v8.git/+/dd3f1ecf719afd21b4c695c776b4da2fb494ef92" target="_blank">revert the previous change</a>.</li>
<li>Chrome files an <a href="https://github.com/tc39/ecma262/issues/87" target="_blank">issue</a> against the spec and after discussion, it’s decided to switch date-only forms back to UTC but leave date-time forms without offset as local (i.e. Firefox’s behavior).</li>
<li><a href="https://262.ecma-international.org/7.0/index.html#sec-date.parse" target="_blank">ES7</a> releases with the updated requirement. Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/d31c5410c4fdfc5eb66582892d5e3ecd3706bd58" target="_blank">makes the change</a> and then eventually, <a href="https://github.com/WebKit/WebKit/commit/2148a43f377e67c60b167f5730c7b5c5c21b202d" target="_blank">Safari</a>.</li>
</ol><p>This behavior has been maintained to the present day where every possible string accepted by the <code>Date</code> constructor falls back to local time <em>except</em> valid ISO date-strings like <code>'2025-05-28'</code>.</p><p>What’s interesting looking at the timeline is that despite being designed as a standardized format, from its release in 2009 up until early 2020, there would never exist a point where the major browsers behaved consistently for missing offsets. Meanwhile, Chrome has flipped hilariously from <a href="https://chromium.googlesource.com/v8/v8.git/+/6ceb02e6eb791f837ed84b7ed41332058cd3f1dc" target="_blank">local</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/ff9ce1abd4add01bbb3f1917bd72207e5ddd70b5" target="_blank">UTC</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/f06754a8e1d305a43560705f6c167d85d40e602d" target="_blank">local</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/dd3f1ecf719afd21b4c695c776b4da2fb494ef92" target="_blank">UTC</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/d31c5410c4fdfc5eb66582892d5e3ecd3706bd58" target="_blank">local</a> when parsing <code>'2025-05-28T00:00'</code>. And all this just to settle at Firefox’s 2009 behavior which, in my opinion, is the most unintuitive of them all.</p> </section>
<section id="What_about_Temporal?"> <h2><a href="#What_about_Temporal?">What about Temporal?</a></h2> <p>For the unaware, <a href="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/" target="_blank">JavaScript Temporal is coming</a>: a new set of date and time APIs intended to replace the <code>Date</code> object.</p><p>Our whole original date parsing issue stemmed from time zone ambiguity but in many cases, the desire is to treat date-only strings as exactly that — dates only. For example, when I say that Christmas this year is <code>2025-12-25</code>, I don’t mean the universal instant in time that is <code>2025-12-25T00:00:00.000Z</code>.</p><p>While <code>Date</code> can only ever represent the latter, Temporal offers the option of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/PlainDate" target="_blank">plain dates</a> (i.e. a date without a time zone). <code>'2025-12-25'</code> is just <code>2025-12-25</code>, side-stepping the parsing ambiguity issue entirely.</p><p>But what if one really wants to parse a date-only string into an instant in time? What time zone will Temporal choose when absent in the string itself?</p><p>Answer: It’s a hard error; an <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/Instant#z%C2%B1hhmm" target="_blank">offset</a> or <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/ZonedDateTime#time_zone_id" target="_blank">time zone identifier</a> must be provided. No repeat mistakes here.</p> </section>
<section id="Bonus:_enter_the_cursed_zone"> <h2><a href="#Bonus:_enter_the_cursed_zone">Bonus: enter the cursed zone</a></h2> <p>One thing I never realized until reading browser date-parsing source code is just how lenient it can be.</p><p>Here’s a fun example for Chrome/Firefox: can you spot why this (valid!) date string is being parsed as the month of May?</p><pre><code><span>const</span> date = <span>'it is wednesday, my dudes. 2025, April, maybe...28(?)'</span>;
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(date)); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span></code></pre> </section></article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: AutoThink – Boosts local LLM performance with adaptive reasoning (289 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44112326</link>
            <guid>44112326</guid>
            <pubDate>Wed, 28 May 2025 02:39:11 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44112326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I built AutoThink, a technique that makes local LLMs reason more efficiently by adaptively allocating computational resources based on query complexity.</p><p>The core idea: instead of giving every query the same "thinking time," classify queries as HIGH or LOW complexity and allocate thinking tokens accordingly. Complex reasoning gets 70-90% of tokens, simple queries get 20-40%.</p><p>I also implemented steering vectors derived from Pivotal Token Search (originally from Microsoft's Phi-4 paper) that guide the model's reasoning patterns during generation. These vectors encourage behaviors like numerical accuracy, self-correction, and thorough exploration.</p><p>Results on DeepSeek-R1-Distill-Qwen-1.5B:</p><p>- GPQA-Diamond: 31.06% vs 21.72% baseline (+43% relative improvement)</p><p>- MMLU-Pro: 26.38% vs 25.58% baseline</p><p>- Uses fewer tokens than baseline approaches</p><p>Works with any local reasoning model - DeepSeek, Qwen, custom fine-tuned models. No API dependencies.</p><p>The technique builds on two things I developed: an adaptive classification framework that can learn new complexity categories without retraining, and an open source implementation of Pivotal Token Search.</p><p>Technical paper: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327" rel="nofollow">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327</a></p><p>Code and examples: <a href="https://github.com/codelion/optillm/tree/main/optillm/autothink">https://github.com/codelion/optillm/tree/main/optillm/autoth...</a></p><p>PTS implementation: <a href="https://github.com/codelion/pts">https://github.com/codelion/pts</a></p><p>I'm curious about your thoughts on adaptive resource allocation for AI reasoning. Have you tried similar approaches with your local models?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Look ma, no bubbles designing a low-latency megakernel for Llama-1B (163 pts)]]></title>
            <link>https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles</link>
            <guid>44111673</guid>
            <pubDate>Wed, 28 May 2025 00:01:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles">https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles</a>, See on <a href="https://news.ycombinator.com/item?id=44111673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There are some applications that benefit from running LLMs really, really fast. This low-latency regime encompasses applications like chatbots and human-in-the-loop workflows, where users care a lot about seeing responses come back immediately.</p>
<p>Given the importance of these low-latency workloads, we wanted to explore just how fast we can run open-source models on modern GPUs. To really stress-test existing systems, we consider an aggressive low-latency scenario where we generate a single sequence with Llama-3.2-1B. This workload is strongly memory bound – our performance is dominated by how fast we can load model weights from GPU global memory.</p>
<p>It turns out that popular LLM inference engines – vLLM and SGLang – are only able to use at most 50% of available GPU bandwidth when running this workload on an H100. The root of the problem, which we'll describe more below, is that existing systems break down a model forward pass into around <strong>a hundred separate kernels</strong> that each implement a few operations (e.g. RMS norm, attention, an MLP layer + activation, rotary). Each kernel comes with a setup and teardown period and during this time no useful work gets done – for instance, the all-important task of loading model weights is stalled.</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/result.png" alt="Performance comparison graph"></p><p><em>Figure 1: Speed! Results generated with a 32-token prompt and 128 generated tokens, with no speculation</em></p></div>
<p>In this post, we show how we can bypass this problem by merging the entire Llama-1B forward pass into a single "megakernel" that eliminates kernel boundaries altogether. Doing this achieves brr – on an H100, we use 78% of memory bandwidth and outperform existing systems by over 1.5x. (To our knowledge, this is the lowest-latency forward pass for Llama-1B in bfloat16!) In the rest of this post, we'll walk through how and why one would do this. Specifically:</p>
<ul>
<li>First, we'll talk about how small kernels lead to AI systems that underutilize the GPU's full bandwidth.</li>
<li>Second, we'll describe three important points about how we built our megakernel: how we fused lots of kernels together, how we share hardware resources across them to minimize overhead, and how we synchronize them efficiently.</li>
</ul>
<p>If you're interested in learning more of the details or using these ideas yourself, we're <a href="https://github.com/HazyResearch/Megakernels">open-sourcing all of our code here</a>.</p>
<h2>Separate Kernels Kill the Vibe</h2>
<p>In general, the way one runs code on a GPU is by launching a "kernel" – a small program that does a well-defined operation (e.g. RMS norm, MLP). Today, all AI workloads run as long sequences of relatively small kernels. To get an initial sense, let's look at the operations in the Llama-1B transformer block, and some example kernel boundaries of how they might be divided up (Figure 2).</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/kernel_boundaries.png" alt="Kernel boundaries diagram"></p><p><em>Figure 2: An example set of kernel boundaries for the Llama-1B transformer block. Red boxes delineate the work done by individual kernels.</em></p></div>
<p>As we described earlier, decoding a single sequence with Llama-1B is a purely memory-bound workload: our performance depends on being able to <strong>always</strong> be loading weights from GPU global memory. So, why are existing approaches so far from using the full bandwidth of the GPU?</p>
<p>When we dug into it, we noticed a key problem was that the current kernel-based approach to running models introduces stalls that prevent us from constantly loading memory:</p>
<ul>
<li>First: GPU kernels are launched with a strict ordering, so that a thread block in one kernel can't start until all thread blocks in previous kernels have completely finished. Consequently, every time we start a kernel, we have to wait for all the straggler thread blocks from the prior one to finish. For example, if a kernel runs 512 thread blocks (like our Llama-1B down projection), but we only have 148 streaming multiprocessors (like on a B200), we end up with 80 empty SM's at the end.</li>
<li>Second, as we've <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">previously highlighted</a>, each kernel launch and teardown incurs costs. In principle, NVIDIA's CUDA graphs can help hide costs, but by our measurements they still leave a lot on the table. For a simple dummy kernel (which dumps a start time, sleeps, and dumps an end time) on an H100, we find that running on a CUDA stream incurs a launch cost of about 2.1 microseconds, and with CUDA graphs the launch cost only decreases to around 1.3 microseconds – time spent with the GPU doing no useful work! We'd like to have the GPU spend all of its time doing useful work.</li>
<li>Finally, even after we start the next kernel, we still have to wait to load weights and activations before any compute can start. These latencies leave the GPU sitting idle for thousands of cycles! Ideally, we'd start loading the next weights while the previous computations and stores are happening. NVIDIA has also built a mechanism for this called <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#programmatic-dependent-launch-and-synchronization">Programmatic Dependent Launch</a> (PDL), which allows the next kernel to start preparing while the previous kernel is running, but we found it still introduces unnecessary stalls because the PDL synchronization mechanism (cudaGridDependencySynchronize) is very coarse. For example, it means we have to wait for all queries, keys, and values to complete in order to start attention, as opposed to starting heads as soon as they are ready. We'll later show another specific case of where this is useful in Llama-1B.</li>
</ul>
<p>Taken together, these form the "memory pipeline bubbles" our title references – and they represent a key reason that we're <strong>not always loading from memory</strong>.  For short operations, these pauses add up, wasting a huge chunk of potential bandwidth. In part, this is because Llama-1B (actually 1.24B parameters) in batch size 1 is just so... small: if each operation is really fast, then the time spent in-between them really starts to matter.</p>
<p>To illustrate the magnitude of the problem: for single-sequence generation in 16-bit precision on a single H100, the <strong>memory limit</strong> is 3.35TB/s / 2.48GB = ~1350 forward passes per second. But with 7 kernel launches per layer, and 16 layers, even with an optimistic 5 us of stalling per kernel (counting stragglers, kernel launch, and memory latencies), generation would run at just ~770 forward passes per second. In practice, it's often worse. On low-latency workloads, GPUs spend only a fraction of their time actually doing any useful work!</p>
<p>So while CUDA does provide some existing features (e.g. graphs, streams, PDL) to partially solve these problems, we wanted to see if a different approach could solve all of these problems, where we just fuse the entire model forward pass into a single kernel.</p>
<h2>How to Megakernel</h2>
<p>Next, we'll show you how we fused a whole Llama forward pass into a single kernel, and our methods for resolving three key problems:</p>
<ol>
<li>Fusing dozens of operations is hard to do from scratch. We need a mechanism for executing these operations within the megakernel.</li>
<li>In order to overlap multiple operations on the same hardware, we need to prevent contention over limited resources, such as shared memory.</li>
<li>The GPU synchronizes after each kernel in the traditional kernel model. Without kernels, we have to synchronize the GPU all by ourselves!</li>
</ol>
<p>Let's start with the first issue:</p>
<h4>Issue 1/3: Fusing Lots of Operations</h4>
<p>Traditional kernel fusion generally merges just two or three operations together. In contrast, we need to fuse about a hundred. Consequently, we need to have a sensible abstraction for how we can actually program a megakernel.</p>
<p>Our approach is built on an on-GPU interpreter – essentially a more sophisticated version of our infrastructure underlying <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">ThunderMLA</a>. Our interpreter is designed such that each streaming multiprocessor (SM) within the GPU receives a sequence of <strong>instructions</strong> (each implemented using the same CUDA template) and executes them. We <strong>schedule</strong> each SM's instruction sequence ahead of time on the Python side, and notably we can reuse each schedule for hundreds of forward passes!</p>
<p>For our end-to-end Llama forwards pass megakernel, we define the following set of instructions:</p>
<ul>
<li>A fused RMS norm &amp; QKV &amp; RoPE instruction.</li>
<li>An attention computation instruction.</li>
<li>An attention reduction instruction (for ThunderGQA on long sequences).</li>
<li>An O-projection + residual instruction.</li>
<li>A fused RMS norm &amp; up-gate &amp; SiLU instruction.</li>
<li>A down-projection + residual instruction.</li>
<li>An RMS norm &amp; language modeling head instruction, for computing the final token logits.</li>
</ul>
<p>We implement each of these instructions using a common <a href="https://github.com/HazyResearch/Megakernels/blob/main/util/mk_init/sources/src/%7B%7BPROJECT_NAME_LOWER%7D%7D.cu">CUDA template</a> (with load, store, compute boilerplate functions), facilitating interoperability within our interpreter framework.</p>
<h4>Issue 2/3: <span>S</span><span>h</span><span>a</span><span>r</span><span>i</span><span>n</span><span>g</span> Shared Memory to Eliminate Memory Bubbles</h4>
<p>The instruction-and-interpreter structure lets us cleanly organize our megakernel. However, we haven't yet addressed the key issue: making sure that model weights are always being loaded in order to maximize memory bandwidth utilization.</p>
<p>The reason why a megakernel lets us solve this problem is that we can pipeline memory loads across instructions: our interpreter will start loading the model weights for an instruction as soon as it can, even if a previous instruction is still finishing up (e.g. storing out its results to global memory). It's this tight transitioning between instructions that minimizes the memory bubbles that would otherwise appear if we launched multiple kernels.</p>
<p>However, there's a catch: loading the weights from global memory for the next instruction doesn't do you much good if you have no place to put the data you loaded! More precisely, all of our weight matrices are loaded from GPU global memory into our SM's "shared memory" – NVIDIA's term for the fast memory on each SM. Shared memory is a scarce resource on each SM, and we can't start a load for a new instruction if a previous instruction is using all of it. This necessitates a way to keep track of which instruction is using which piece of shared memory and quickly transition shared memory to the next instruction when the current instruction is done with it.</p>
<p>We accomplish this by <strong>paging</strong> shared memory. We first divide the first 213kB of shared memory on an H100 into 13 16KiB pages, and use remaining shared memory for special purposes, like storing instruction parameters. To use one of these pages, instructions have to explicitly request and release them from the interpreter. The interpreter automatically passes released pages to the next instruction, allowing them to start issuing memory loads as early as shared memory becomes available.</p>
<h4>Issue 3/3: Synchronization</h4>
<p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/thanos.png" alt="Thanos illustration"></p>
<p>While megakernels let us minimize pipeline bubbles, they also introduce a new problem: synchronization. The performance limitation with the normal many-kernel execution model is that no thread blocks in a kernel can start until all thread blocks in previous kernels are finished. However, it's precisely this property that makes it easy to manage data dependencies. When a kernel launches, CUDA guarantees that all of the kernel's input tensors have already been produced and are safe to read from immediately.</p>
<p>With megakernels, we have no such guarantees: when an SM starts to execute a new instruction, its inputs might not be ready! To address this, we explicitly synchronize the instructions inside of our megakernel. We accomplish this with a simple counter system. Before the megakernel launches, we initialize an array of counters (i.e. integers) in GPU global memory with a starting value of zero. Whenever an instruction completes, it increments one of these counters. Similarly, whenever a new instruction starts, it must wait for some of these counters to reach a target value, indicating that all of its dependencies have finished.</p>
<p>One optimization this enables is in the big multi-layer perceptrons (MLPs) in Llama-1B.</p>
<ul>
<li>In a naive implementation using PDL, one must await completing the whole hidden state before beginning the down projection matrix multiply.</li>
<li>We instead produce and consume the intermediate state in four chunks, each with their own counter. This way, an instruction for the down projection only needs to wait for its input chunk to finish.</li>
</ul>
<h2>Putting It All Together</h2>
<p>To our knowledge, our H100 megakernel represents the first time anyone has run the forward pass for a 16-bit 1B+ parameter language model in under one millisecond on a GPU. Our B200 implementation pushes this even further to under 680 microseconds per forward pass!</p>
<p>As shown in Figure 1, our megakernel outperforms vLLM and SGLang baselines (which use CUDA graphs and torch compilation):</p>
<ul>
<li>On an H100, our megakernel runs almost 2.5x faster than vLLM and over 1.5x faster than SGLang.</li>
<li>On a B200, the gap with vLLM rises to over 3.5x, and we remain more than 1.5x faster than SGLang, too.</li>
</ul>
<p>We're still actually quite a ways off from the theoretical limit on a B200, which is around ~3,000 forward passes per second. Part of this gap is because this theoretical limit is based purely on memory bandwidth – but we still have to wait to load activations. And although these activations are small (and don't cost a lot of bandwidth), there are still latencies in loading them that we can't hide. A breakdown of the runtime of our current B200 forward pass (total runtime 600 microseconds):</p>
<ul>
<li>250 microseconds are spent storing activations, awaiting consistency, and loading them. This is about 20% higher than a simple model would suggest: since each instruction has a dependence on the last one, we need to pay two load latencies (check ready, and then load activations) and two store latencies (store activations, then mark ready) per instruction. Using ~500 nanoseconds latency per load / store, this would impose about 200 microseconds of overhead. (We suspect some of the remaining 50 microseconds comes from time spent processing atomics in global memory.)</li>
<li>200 microseconds are spent actually running RMS norm and matrix-vector computations. 95% of this portion is devoted to matrix-vector. On Blackwell, we find that using the tensor cores is marginally helpful for this; on Hopper, we find it better to simply run on the CUDA cores. This difference comes from the fact that both GPUs have relatively similar CUDA core performance, but Blackwell tensor cores are much faster.</li>
<li>30 microseconds are spent awaiting weights from global memory (pipelining works!) Of these, 40% are spent in the LM head, which is the best-pipelined part of the whole megakernel due to its homogeneity and huge size.</li>
<li>40 microseconds are spent on low-level synchronization overhead across warps. A key issue here is that CUDA's asynchronous barriers are relatively slow, even when they're already in the "pass" state, requiring about 60 nanoseconds each time.</li>
<li>80 microseconds are on setup and various other overheads (e.g. passing instruction barriers, marking pages as complete, etc.)</li>
</ul>
<p>We think there's probably more to do on each of these, but that'll have to wait for a future update!</p>
<h2>The Megakernel Cinematic Universe</h2>
<p>In this blog, we focus narrowly on designing a megakernel for low-latency, batch-size one LLM inference. However, we believe that the ability to more precisely control GPU execution with megakernels can more generally be applied to accelerate a much broader set of AI workloads. Stay tuned!</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/sonic.png" alt="Sonic illustration"></p><p><strong>The Main Message of this Blog Post</strong></p></div>
<p>If you'd like to learn more, please reach out to Ben or Jordan! Please include a tribute of at least five pictures of kittens in your email.</p>
<ul>
<li>Ben: <a href="mailto:bfs@stanford.edu">bfs@stanford.edu</a></li>
<li>Jordan: <a href="mailto:jbj@stanford.edu">jbj@stanford.edu</a></li>
</ul>
<p>And many, many thanks to Together AI for generously providing us with B200s and H100s to do this work, which would not have been possible without them!</p>
<p>See also: <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla"><strong>pretty big kernels</strong></a> | <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk"><strong>regular kernels</strong></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A UEFI app that sends LLDP-MED pkt at boot to negotiate PoE+ power before the OS (165 pts)]]></title>
            <link>https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution</link>
            <guid>44111609</guid>
            <pubDate>Tue, 27 May 2025 23:45:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution">https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution</a>, See on <a href="https://news.ycombinator.com/item?id=44111609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Back in 2015, I was working on a project to build PoE-powered embedded x86 computers and digital signage systems. These were full Windows 10 Professional machines running Intel Atom processors, designed to simplify deployment by drawing power directly over Ethernet. Our goal was to eliminate the need to run traditional AC power to these devices, which can be costly and impractical in many deployment scenarios. But unlike typical IoT or low-power devices, these were full-fledged x86 computers that required more power than what the standard PoE (802.3af) could deliver, which maxes out at 15.4W at the PSE (Power Sourcing Equipment), such as a PoE network switch or injector.</p>
<p>Our device required about 23W when fully operational, which pushed us into <strong>802.3at (PoE+)</strong> territory. In most client environments their PoE+ switches provided the power we needed with no problem. But some environments had network switches that would not give us the additional power.</p>
<h3><strong>PoE Standards Overview (IEEE 802.3)</strong></h3>
<table>
<thead>
<tr>
<th>Standard</th>
<th>Max Power at PSE</th>
<th>Max Power at PD</th>
<th>Voltage Range</th>
<th>Pairs Used</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>802.3af (PoE)</strong></td>
<td>15.4 W</td>
<td>12.95 W</td>
<td>44–57 V DC</td>
<td>2 pairs</td>
<td>2003</td>
</tr>
<tr>
<td><strong>802.3at (PoE+)</strong></td>
<td>30 W</td>
<td>25.5 W</td>
<td>50–57 V DC</td>
<td>2 pairs</td>
<td>2009</td>
</tr>
</tbody>
</table>
<p>The problem was that our embedded systems only supported physical‑layer classification which is limited to signaling power requirements through resistive detection and pulsed current signatures during initial PoE handshaking. Only relying on this method can be problematic if the switch is configured to require LLDP for Data Link Layer Classification for devices requiring more than 15.4W. Which is a problem because at minimum our computers required at least 18W in order to boot into the operating system. So our systems would initially start to boot, but then eventually shut off before it got into Windows. We were stuck in a frustrating Catch-22, we needed to send LLDP packets to get more power, but we couldn’t even boot the OS to send them.</p>
<h3><strong>So What Do You Do When the OS Can't Help?</strong></h3>
<p>We did some testing and measured power draw during various phases of the boot cycle. Fortunately, the system's power needs during initial startup (BIOS/UEFI initialization) were low enough to stay under the 15.4W limit. That gave us a brief window to request more power <em>before</em> booting Windows.</p>
<p>So the challenge became: negotiate higher PoE+ power <strong>before</strong> Windows starts. The answer was to handle LLDP negotiation at the BIOS level, or more accurately the UEFI (Unified Extensible Firmware Interface) firmware.  Through our research we discovered that UEFI supports the TCP/IP protocol and has access to the network stack, enabling communication over Ethernet without an OS.</p>
<p>Our first attempt was to work with the motherboard vendor and AMI (the BIOS provider) for a custom firmware build. We signed NDAs and had multiple discussions, but despite our efforts, they ultimately declined to create a custom BIOS for us. After hitting that roadblock and feeling the frustration of stalled progress, I refused to give up. I dug deeper and came across the concept of <strong>UEFI applications</strong>.</p>
<p>A UEFI application is a type of software designed to run in the pre-boot environment of a computer, managed entirely by the UEFI firmware. These applications are different from traditional programs that run once an operating system like Windows or Linux has loaded. Instead, UEFI applications operate with the services and resources provided by the firmware itself, bypassing the need for an OS.</p>
<p>They are typically stored on a dedicated partition called the EFI System Partition (ESP) and launched by the UEFI boot manager during the system's boot process. These apps can access low-level system functionality, including networking, file systems, and input/output devices. In our case, that meant we could build a standalone tool to t ransmit LLDP packets <em>before</em> the OS even initialized. This was the perfect solution, because it required no changes to the BIOS/UEFI firmware itself. I just needed to find someone with the embedded firmware expertise to bring it to life.</p>
<h3><strong>From Warsaw With Code</strong></h3>
<p>After some research, I found <a href="https://www.linkedin.com/in/krolpiotr/">Piotr Król</a>, a former BIOS software engineer at Intel who was doing freelance work out of Poland. He understood the problem immediately. We set up remote serial and IP-KVM access to our development hardware, and Piotr got to work.</p>
<p>There were some challenges along the way including lack of vendor support, incomplete firmware tooling, and remote hardware limitations. Our system didn't include <code>bcfg</code>, which meant we couldn't persistently change the boot order through standard UEFI tools. Piotr identified this early and suggested using <code>startup.nsh</code> as a workaround, a shell script that would automatically run our LLDP application when the EFI shell launched.</p>
<p>Four months later, Piotr delivered <strong>PoePwrNegotiator</strong>: a UEFI application written in C that transmits LLDP-MED (Link Layer Discovery Protocol – Media Endpoint Discovery) packets and requests the higher power levels we needed. No OS required. We deployed this UEFI application on all of our PoE devices in production and it worked flawlessly.</p>
<h3><strong>Sharing the Solution</strong></h3>
<p>This project began as an attempt to solve a very specific challenge we faced nearly a decade ago. I don’t know how many others have tackled this type of problem or taken this approach, but I wanted to share the work in case it helps someone else.</p>
<p>By open-sourcing <strong>PoePwrNegotiator</strong>, my goal is to preserve and document a unique solution to a problem that may still be relevant to those building PoE-powered x86 systems. If someone out there is working on a similar challenge, or even just wants to understand how UEFI applications can be used to control networking behavior at boot, I hope this gives them a useful head start.</p>
<p>PoePwrNegotiator is released under the <strong>MIT License</strong>, one of the most permissive open source licenses available. This means anyone can use, modify, or integrate this code into their own projects, commercial or personal, as long as the original license and copyright notice are included. The goal is to make this as accessible and useful as possible to anyone dealing with power negotiation challenges or looking to learn more about UEFI networking.</p>
<p><strong>GitHub Repo:</strong> <a href="https://github.com/orbitrod/PoePwrNegotiator">https://github.com/orbitrod/PoePwrNegotiator</a></p>
<h3><strong>Special Thanks</strong></h3>
<p><strong>Carlos</strong>, you were instrumental during the testing and the deployment of this application. You were my right hand throughout this project and far beyond it, and your dedication to me and to the work we were doing will never be forgotten. I cannot express enough how much your loyalty and commitment meant to me throughout that entire journey.</p>
<p><strong>Piotr</strong>, thank you for being brilliant, resourceful, and incredibly effective. Your deep expertise in firmware helped us solve a problem others wouldn’t touch. I’m grateful for your expertise and contribution to our project, your work solved the last piece of the puzzle.</p>
<hr>
<blockquote>
<p><em>This project reminded me that innovation often comes from working around limitations, not just within them. PoePwrNegotiator was a solution to a very specific challenge I faced in 2015, but the lessons and approach still feel relevant today. If it sparks ideas, helps someone overcome a similar obstacle, or contributes in any way to future PoE-powered system design, that’s all the reason I need to put it out there.</em></p>
<p><em>— Roderick</em></p>
</blockquote>
<hr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenTPU: Open-Source Reimplementation of Google Tensor Processing Unit (TPU) (117 pts)]]></title>
            <link>https://github.com/UCSBarchlab/OpenTPU</link>
            <guid>44111452</guid>
            <pubDate>Tue, 27 May 2025 23:10:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/UCSBarchlab/OpenTPU">https://github.com/UCSBarchlab/OpenTPU</a>, See on <a href="https://news.ycombinator.com/item?id=44111452">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">UCSB ArchLab OpenTPU Project</h2><a id="user-content-ucsb-archlab-opentpu-project" aria-label="Permalink: UCSB ArchLab OpenTPU Project" href="#ucsb-archlab-opentpu-project"></a></p>
<p dir="auto">OpenTPU is an open-source re-implementation of Google's Tensor Processing Unit (TPU) by the UC Santa Barbara ArchLab.</p>
<p dir="auto">The TPU is Google's custom ASIC for accelerating the inference phase of neural network computations.</p>
<p dir="auto">Our design is based on details from Google's paper titled "In-Datacentre Performance Analysis of a Tensor Processing Unit" (<a href="https://arxiv.org/abs/1704.04760" rel="nofollow">https://arxiv.org/abs/1704.04760</a>), which is to appear at ISCA2017. However, no formal spec, interface, or ISA has yet been published for the TPU.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The OpenTPU is powered by PyRTL (<a href="http://ucsbarchlab.github.io/PyRTL/" rel="nofollow">http://ucsbarchlab.github.io/PyRTL/</a>).</h4><a id="user-content-the-opentpu-is-powered-by-pyrtl-httpucsbarchlabgithubiopyrtl" aria-label="Permalink: The OpenTPU is powered by PyRTL (http://ucsbarchlab.github.io/PyRTL/)." href="#the-opentpu-is-powered-by-pyrtl-httpucsbarchlabgithubiopyrtl"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Python 3</li>
<li>PyRTL version &gt;= 0.8.5</li>
<li>numpy</li>
</ul>
<p dir="auto">Both PyRTL and numpy can be installed with pip; e.g., <code>pip install pyrtl</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to Run" href="#how-to-run"></a></p>
<p dir="auto">To run the simple matrix multiply test in both the hardware and functional simulators:</p>
<p dir="auto">Make sure MATSIZE is set to 8 in config.py, then</p>
<div data-snippet-clipboard-copy-content="python3 assembler.py simplemult.a
python3 runtpu.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
python3 sim.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy"><pre><code>python3 assembler.py simplemult.a
python3 runtpu.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
python3 sim.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
</code></pre></div>
<p dir="auto">To run the Boston housing data regression test in both the hardware and functional simulators:</p>
<p dir="auto">Make sure MATSIZE is set to 16 in config.py, then</p>
<div data-snippet-clipboard-copy-content="python3 assembler.py boston.a
python3 runtpu.py boston.out boston_inputs.npy boston_weights.npy
python3 sim.py boston.out boston_inputs.npy boston_weights.npy"><pre><code>python3 assembler.py boston.a
python3 runtpu.py boston.out boston_inputs.npy boston_weights.npy
python3 sim.py boston.out boston_inputs.npy boston_weights.npy
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Simulation</h3><a id="user-content-hardware-simulation" aria-label="Permalink: Hardware Simulation" href="#hardware-simulation"></a></p>
<p dir="auto">The executable hardware spec can be run using PyRTL's simulation features by running <code>runtpu.py</code>. The simulation expects as inputs a binary program and numpy array files containing the initial host memory and the weights.</p>
<p dir="auto">Be aware that the size of the hardware Matrix Multiply unit is parametrizable --- double check <code>config.py</code> to make sure MATSIZE is what you expect.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Functional Simulation</h3><a id="user-content-functional-simulation" aria-label="Permalink: Functional Simulation" href="#functional-simulation"></a></p>
<p dir="auto">sim.py implements the functional simulator of OpenTPU. It reads in three cmd args: the assembly program, the host memory file, and the weights file. Due to the different quantization mechnisms between high-level applications (written in tensorflow) and OpenTPU, the simulator runs in two modes: 32b float mode and 8b int mode. The downsampling/quantization mechanism is consistent with the HW implementation of OpenTPU. It generates two sets of outputs, one set being 32b-float typed, the other 8b-int typed.</p>
<p dir="auto">Example usage:</p>
<div data-snippet-clipboard-copy-content="python sim.py boston.out boston_input.npy boston_weights.npy"><pre><code>python sim.py boston.out boston_input.npy boston_weights.npy
</code></pre></div>
<p dir="auto">Numpy matrices (.npy files) can be generated by calling <code>numpy.save</code> on a numpy array.</p>
<p dir="auto">checker.py implementes a simple checking function to verify the results from HW, simulator and applications. It checkes the 32b-float application results against 32b-float simulator results and then checks the 8b-int simulator results against 8b-int HW results.</p>
<p dir="auto">Example usage:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">FAQs:</h2><a id="user-content-faqs" aria-label="Permalink: FAQs:" href="#faqs"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How big/efficient/fast is OpenTPU?</h3><a id="user-content-how-bigefficientfast-is-opentpu" aria-label="Permalink: How big/efficient/fast is OpenTPU?" href="#how-bigefficientfast-is-opentpu"></a></p>
<p dir="auto">As of the alpha release, we do not have hard synthesis figures for the full 256x256 OpenTPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What can OpenTPU do?</h3><a id="user-content-what-can-opentpu-do" aria-label="Permalink: What can OpenTPU do?" href="#what-can-opentpu-do"></a></p>
<p dir="auto">The hardware prototype can currently handle matrix multiplies and activations for ReLU and sigmoid --- i.e., the inference phase of many neural network computations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What features are missing?</h3><a id="user-content-what-features-are-missing" aria-label="Permalink: What features are missing?" href="#what-features-are-missing"></a></p>
<p dir="auto">Convolution, pooling, programmable normalization.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does your design follow that of the TPU?</h3><a id="user-content-does-your-design-follow-that-of-the-tpu" aria-label="Permalink: Does your design follow that of the TPU?" href="#does-your-design-follow-that-of-the-tpu"></a></p>
<p dir="auto">We used high-level design details from the TPU paper to guide our design when possible. Thus, the major components of the chip are the same --- matrix multiply unit, unified buffer, activation unit, accumulator, weight FIFO, etc. Beyond that, the implementations may have many differences.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does OpenTPU support all the same instructions as TPU?</h3><a id="user-content-does-opentpu-support-all-the-same-instructions-as-tpu" aria-label="Permalink: Does OpenTPU support all the same instructions as TPU?" href="#does-opentpu-support-all-the-same-instructions-as-tpu"></a></p>
<p dir="auto">No. Currently, OpenTPU supports the RHM, WHM, RW, MMC, ACT, NOP, and HLT instructions (see ISA section for details). The purpose, definition, and specification of other TPU instructions is absent from the published paper. Some instructions will likely be added to OpenTPU as we continue development (such as SYNC), but the final ISA will likely feature many differences without a published spec from Google to work off of.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Is OpenTPU binary compatible with the TPU?</h3><a id="user-content-is-opentpu-binary-compatible-with-the-tpu" aria-label="Permalink: Is OpenTPU binary compatible with the TPU?" href="#is-opentpu-binary-compatible-with-the-tpu"></a></p>
<p dir="auto">No. There is no publicly available interface or spec for TPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I'd like to do some analysis/extensions of OpenTPU, but I need Verilog. Do you have a Verilog version?</h3><a id="user-content-id-like-to-do-some-analysisextensions-of-opentpu-but-i-need-verilog-do-you-have-a-verilog-version" aria-label="Permalink: I'd like to do some analysis/extensions of OpenTPU, but I need Verilog. Do you have a Verilog version?" href="#id-like-to-do-some-analysisextensions-of-opentpu-but-i-need-verilog-do-you-have-a-verilog-version"></a></p>
<p dir="auto">PyRTL can can output structural Verilog for the design, using the <code>OutputToVerilog</code> function.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I have suggestions, criticisms, and/or would like to contribute.</h3><a id="user-content-i-have-suggestions-criticisms-andor-would-like-to-contribute" aria-label="Permalink: I have suggestions, criticisms, and/or would like to contribute." href="#i-have-suggestions-criticisms-andor-would-like-to-contribute"></a></p>
<p dir="auto">That's not a question, but please get in touch! Email Deeksha (<a href="mailto:deeksha@cs.ucsb.edu">deeksha@cs.ucsb.edu</a>) or Joseph (<a href="mailto:jmcmahan@cs.ucsb.edu">jmcmahan@cs.ucsb.edu</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I'm a Distinguished Hardware Engineer at Google and the Lead Architect of the TPU. I see many inefficiencies in your implementation.</h3><a id="user-content-im-a-distinguished-hardware-engineer-at-google-and-the-lead-architect-of-the-tpu-i-see-many-inefficiencies-in-your-implementation" aria-label="Permalink: I'm a Distinguished Hardware Engineer at Google and the Lead Architect of the TPU. I see many inefficiencies in your implementation." href="#im-a-distinguished-hardware-engineer-at-google-and-the-lead-architect-of-the-tpu-i-see-many-inefficiencies-in-your-implementation"></a></p>
<p dir="auto">Hi Norm! Tim welcomes you to Santa Barbara to talk about all things TPU :)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software details</h2><a id="user-content-software-details" aria-label="Permalink: Software details" href="#software-details"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">ISA</h3><a id="user-content-isa" aria-label="Permalink: ISA" href="#isa"></a></p>
<ul dir="auto">
<li>RHM src, dst, N
Read Host Memory.
Read <em>N</em> vectors from host memory beginning at address <em>src</em> and save them in the UB (unified buffer) beginning at address <em>dst</em>.</li>
<li>WHM src, dst, N
Write Host Memory.
Write <em>N</em> vectors from the UB beginning at address <em>src</em> to host memory beginning at address <em>dst</em>.</li>
<li>RW addr
Read Weights.
Load the weights tile from the weights DRAM at address <em>addr</em> into the on-chip FIFO.</li>
<li>MMC.{OS} src, dst, N
Matrix Multiply/Convolution.
Perform a matrix multiply operation on the <em>N</em> vectors beginning at UB address <em>src</em>, storing the result in the accumulator buffers beginning at address <em>dst</em>. If the <em>O</em> (overwrite) flag is specified, overwrite the contents of the accumulator buffers at the destination addresses; default behavior is to add to the value there and store the new sum. If the <em>S</em> (switch) flag is specified, switch to using the next tile of weights, which must have already been pre-loaded. The first <code>MMC</code> instruction in a program should always use the <em>S</em> flag.</li>
<li>ACT.{RQ} src, dst, N
Activate.
Perform activation on <em>N</em> vectors in the accumulator buffers starting at address <em>src</em>, storing the results in the UB beginning at address <em>dst</em>. Activation function is specified with a flag: <em>R</em> for ReLU and <em>Q</em> for sigmoid. With no flag, values are passed through without activation. Normalization is programmable at synthesis-time, but not at run-time; by default, after activation the upper 24 bits are dropped from each value, producing an 8-bit integer.</li>
<li>NOP
No op. Do nothing for one cycle.</li>
<li>HLT
Halt. Stop simulation.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Writing a Program</h3><a id="user-content-writing-a-program" aria-label="Permalink: Writing a Program" href="#writing-a-program"></a></p>
<p dir="auto">OpenTPU uses no dynamic scheduling; all execution is fully determinstic* and the hardware relies on the compiler to correctly schedule operations and pad NOPs to handle delays. This OpenTPU release does <br>
not support "repeat" flags on instructions, so many NOPs are required to ensure correct execution.</p>
<p dir="auto">*DRAM is a source of non-deterministic latency, discussed in the Memory Controller section of Microarchitecture.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generating Data</h3><a id="user-content-generating-data" aria-label="Permalink: Generating Data" href="#generating-data"></a></p>
<p dir="auto"><strong>Application</strong></p>
<ol dir="auto">
<li>Simple one hot 2-layer NN</li>
</ol>
<p dir="auto">gen_one_hot.py generates 8b-int typed random squre matrix as training data and vector as label, example usage:</p>
<div data-snippet-clipboard-copy-content="python gen_one_hot.py --path simple_train --shape 8 8 --range -5 5
python gen_one_hot.py --path simple_train_label --shape 8 1 --range 0 2"><pre><code>python gen_one_hot.py --path simple_train --shape 8 8 --range -5 5
python gen_one_hot.py --path simple_train_label --shape 8 1 --range 0 2
</code></pre></div>
<p dir="auto">simple_nn.py trains a simple 2-layer nn on the given train/label dataset and writes the weights into a file, example usage (run gen_one_hot example first to generate the files):</p>
<div data-snippet-clipboard-copy-content="python simple_nn.py --path simple_train.npy --label simple_train_label.npy"><pre><code>python simple_nn.py --path simple_train.npy --label simple_train_label.npy
</code></pre></div>
<p dir="auto">After running the above command, two files are generated: simple_nn_weight_dram.npy is the 8b-int typed weight dram that the OpenTPU operates on, simple_nn_gt is the pickled ground truth 32b-float resulits and weights. To run with OpenTPU, a test file must also be generated, example usage:</p>
<div data-snippet-clipboard-copy-content="python gen_one_hot.py --path simple_test --shape 100 8 --range 1, 9"><pre><code>python gen_one_hot.py --path simple_test --shape 100 8 --range 1, 9
</code></pre></div>
<p dir="auto">After which simple_test.npy will be generated and it should be used as the host memory by OpenTPU.</p>
<p dir="auto">We also provide simple_nn.a -- the assembly program for this simple nn.</p>
<ol start="2" dir="auto">
<li>Tensorflow DNN regression</li>
</ol>
<p dir="auto">Although applications written in any high-level nn framework can be used, here we use tensorflow as an example.</p>
<p dir="auto">tf_nn.py trains a MLP regressor on the Boston Housing Dataset (<a href="https://archive.ics.uci.edu/ml/datasets/housing" rel="nofollow">https://archive.ics.uci.edu/ml/datasets/housing</a>). Example usage:</p>
<div data-snippet-clipboard-copy-content="python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output
python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output --raw"><pre><code>python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output
python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output --raw
</code></pre></div>
<p dir="auto">After running the above command, four files are generated: gt32.npy holds the ground truth prediction values, boston_input.npy holds the input test cases which is used as the host memeory for OpenTPU, boston_output.npy holds all the intermediate output values, and boston_weights.npy holds the weight matrices which are used as the weight dram for OpenTPU.</p>
<p dir="auto">Adding --raw to the command generates 32b-float typed files instead of 8b ints.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Latencies</h3><a id="user-content-latencies" aria-label="Permalink: Latencies" href="#latencies"></a></p>
<p dir="auto">The following gives the hardware execution latency for each instruction on OpenTPU:</p>
<ul dir="auto">
<li>RHM - <em>M</em> cycles for reading <em>M</em> vectors</li>
<li>WHM - <em>M</em> cycles for writing <em>M</em> vectors</li>
<li>RW - <em>N*N</em>/64 cycles for <em>N_x_N</em> MM Array for DRAM transfer, and up to 3 additional cycles to propagate through the FIFO</li>
<li>MMC - <em>L+2N</em> cycles, for <em>N_x_N</em> MM Array and <em>L</em> vectors multiplied in the instruction</li>
<li>ACT - <em>L+1</em> cycles, for <em>L</em> vectors activated in the instruction</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Microarchitecture</h2><a id="user-content-microarchitecture" aria-label="Permalink: Microarchitecture" href="#microarchitecture"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Matrix Multiply (MM) Unit</h3><a id="user-content-matrix-multiply-mm-unit" aria-label="Permalink: Matrix Multiply (MM) Unit" href="#matrix-multiply-mm-unit"></a></p>
<p dir="auto">The core of the compute of the OpenTPU is the parametrizable array of 8-bit Multiply-Accumulate Units (MACs), each consisting of an 8-bit integer multiplier and an integer adder of between 16 and 32 bits<br>
*. Each MAC has two buffers storing 8-bit weights (the second buffer allows weight programming to happen in parallel). Input vectors enter the array from the left, with values advancing one unit to the r<br>
ight each cycle. Each unit multiplies the input value by the active weight, adds it to the value from the unit above, and passes the result to the unit below. Input vectors are fed diagonally so that val<br>
ues align correctly as partial sums flow down the array.</p>
<p dir="auto">*The multipliers produce 16-bit outputs; as values move down the columns of the array, each add produces 1 extra bit. Width is capped at 32, creating the potential for uncaught overflow.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Accumulator Buffers</h3><a id="user-content-accumulator-buffers" aria-label="Permalink: Accumulator Buffers" href="#accumulator-buffers"></a></p>
<p dir="auto">Result vectors from the MM Array are written to a software-specified address in a set of accumulator buffers. Instructions indicate whether values should be added into the value already at the address or<br>
overwrite it. MM instructions read from the Unified Buffer (UB) and write to the accumulator buffers; activate instructions read from the accumulator buffers and write to the UB.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Weight FIFO</h3><a id="user-content-weight-fifo" aria-label="Permalink: Weight FIFO" href="#weight-fifo"></a></p>
<p dir="auto">At scale (256x256 MACs), a full matrix of weights (a "tile") is 64KB; to avoid stalls while weights are moved from off-chip weight DRAM, a 4-entry FIFO is used to buffer tiles. It is assumed the connecti<br>
on to the weight DRAM is a standard DDR interface moving data in 64-byte chunks (memory controllers are currently emulated with no simulated delay, so one chunk arrives each cycle). When an MM instructio<br>
n carries the "switch" flag, each MAC switches the active weight buffer as first vector of the instruction propagates through the array. Once it reaches the end of the first row, the FIFO begins feeding <br>
new weight values into the free buffers of the array. New weight values are passed down through the array each cycle until each row reaches its destination.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Systolic Setup</h3><a id="user-content-systolic-setup" aria-label="Permalink: Systolic Setup" href="#systolic-setup"></a></p>
<p dir="auto">Vectors are read all at once from the Unified Buffer, but must be fed diagonally into the MM Array. This is accomplished with a set of sequential buffers in a lower triangular configuration. The top valu<br>
e reaches the matrix immediately, the second after one cycle, the third after two, etc., so that each value reaches a MAC at the same time as the corresponding partial sum from the same source vector.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory Controllers</h3><a id="user-content-memory-controllers" aria-label="Permalink: Memory Controllers" href="#memory-controllers"></a></p>
<p dir="auto">Currently, memory controllers are emulated and have no delay. The connection to Host Memory is currently the size of one vector. The connection to the Weight DRAM uses a standard width of 64 bytes.</p>
<p dir="auto">Because the emulated controllers can return a new value each cycle, the OpenTPU hardware simulation currently has no non-detministic delay. With a more accurate DRAM interface that may encounter dynamic <br>
delays, programs would need to either take care to schedule for the worst-case memory delay, or make use of another instruction to ensure memory operations complete before the values are required*.</p>
<p dir="auto">*We note that the TPU "SYNC" instruction may fulfill this purpose, but is currently unimplemented on OpenTPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Unified Buffer size, Accumulator Buffer size, and the size of the MM Array can all be specified in config.py. However, the MM Array must always be square, and vectors/weights are always composed of 8-bit integers.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My LLM CLI tool can run tools now, from Python code or plugins (398 pts)]]></title>
            <link>https://simonwillison.net/2025/May/27/llm-tools/</link>
            <guid>44110584</guid>
            <pubDate>Tue, 27 May 2025 20:53:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/May/27/llm-tools/">https://simonwillison.net/2025/May/27/llm-tools/</a>, See on <a href="https://news.ycombinator.com/item?id=44110584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/May/27/llm-tools/">

<p>27th May 2025</p>



<p><strong><a href="https://llm.datasette.io/en/stable/changelog.html#v0-26">LLM 0.26</a></strong> is out with the biggest new feature since I started the project: <a href="https://llm.datasette.io/en/stable/tools.html"><strong>support for tools</strong></a>. You can now use the LLM <a href="https://llm.datasette.io/en/stable/usage.html">CLI tool</a>—and <a href="https://llm.datasette.io/en/stable/python-api.html">Python library</a>—to grant LLMs from OpenAI, Anthropic, Gemini and local models from Ollama with access to any tool that you can represent as a Python function.</p>
<p>LLM also now has <a href="https://llm.datasette.io/en/stable/plugins/directory.html#tools">tool plugins</a>, so you can install a plugin that adds new capabilities to whatever model you are currently using.</p>
<p>There’s a lot to cover here, but here are the highlights:</p>
<ul>
<li>
<strong>LLM can run tools now</strong>! You can <strong>install tools from plugins</strong> and load them by name with <code>--tool/-T name_of_tool</code>.</li>
<li>You can also <strong>pass in Python function code on the command-line</strong> with the <code>--functions</code> option.</li>
<li>The <strong>Python API supports tools too</strong>: <code>llm.get_model("gpt-4.1").chain("show me the locals", tools=[locals]).text()</code>
</li>
<li>Tools work in <strong>both async and sync contexts</strong>.</li>
</ul>
<p>Here’s what’s covered in this post:</p>
<ul>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#trying-it-out">Trying it out</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#more-interesting-tools-from-plugins">More interesting tools from plugins</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#ad-hoc-command-line-tools-with-functions">Ad-hoc command-line tools with --functions</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#tools-in-the-llm-python-api">Tools in the LLM Python API</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#why-did-this-take-me-so-long-">Why did this take me so long?</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#is-this-agents-then-">Is this agents then?</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#what-s-next-for-tools-in-llm-">What’s next for tools in LLM?</a></li>
</ul>


<h4 id="trying-it-out">Trying it out</h4>
<p>First, <a href="https://llm.datasette.io/en/stable/setup.html">install the latest LLM</a>. It may not be on Homebrew yet so I suggest using <code>pip</code> or <code>pipx</code> or <code>uv</code>:</p>

<p>If you have it already, <a href="https://llm.datasette.io/en/stable/setup.html#upgrading-to-the-latest-version">upgrade it</a>.</p>

<p>Tools work with other vendors, but let’s stick with OpenAI for the moment. Give LLM an OpenAI API key</p>
<div><pre>llm keys <span>set</span> openai
<span><span>#</span> Paste key here</span></pre></div>
<p>Now let’s run our first tool:</p>
<div><pre>llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td</pre></div>
<p>Here’s what I get:</p>
<p><img src="https://static.simonwillison.net/static/2025/llm-tools.gif" alt="Animated demo. I run that command, LLM shows Tool call: llm_version({}) in yellow, then 0.26a1 in green, then streams out the text The installed version is 0.26a1"></p>
<p><code>llm_version</code> is a very simple demo tool that ships with LLM. Running <code>--tool llm_version</code> exposes that tool to the model—you can specify that multiple times to enable multiple tools, and it has a shorter version of <code>-T</code> to save on typing.</p>
<p>The <code>--td</code> option stands for <code>--tools-debug</code>—it causes LLM to output information about tool calls and their responses so you can peek behind the scenes.</p>
<p>This is using the default LLM model, which is usually <code>gpt-4o-mini</code>. I switched it to <code>gpt-4.1-mini</code> (better but fractionally more expensive) by running:</p>
<div><pre>llm models default gpt-4.1-mini</pre></div>
<p>You can try other models using the <code>-m</code> option. Here’s how to run a similar demo of the <code>llm_time</code> built-in tool using <code>o4-mini</code>:</p>
<div><pre>llm --tool llm_time <span><span>"</span>What time is it?<span>"</span></span> --td -m o4-mini</pre></div>
<p>Outputs:</p>
<blockquote>
<p><code>Tool call: llm_time({})</code></p>
<div><pre>  {
    <span>"utc_time"</span>: <span><span>"</span>2025-05-27 19:15:55 UTC<span>"</span></span>,
    <span>"utc_time_iso"</span>: <span><span>"</span>2025-05-27T19:15:55.288632+00:00<span>"</span></span>,
    <span>"local_timezone"</span>: <span><span>"</span>PDT<span>"</span></span>,
    <span>"local_time"</span>: <span><span>"</span>2025-05-27 12:15:55<span>"</span></span>,
    <span>"timezone_offset"</span>: <span><span>"</span>UTC-7:00<span>"</span></span>,
    <span>"is_dst"</span>: <span>true</span>
  }</pre></div>
<p>The current time is 12:15 PM PDT (UTC−7:00) on May 27, 2025, which corresponds to 7:15 PM UTC.</p>
</blockquote>
<p>Models from (tool supporting) plugins work too. Anthropic’s Claude Sonnet 4:</p>
<div><pre>llm install llm-anthropic -U
llm keys <span>set</span> anthropic
<span><span>#</span> Paste Anthropic key here</span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m claude-4-sonnet</pre></div>
<p>Or Google’s Gemini 2.5 Flash:</p>
<div><pre>llm install llm-gemini -U
llm keys <span>set</span> gemini
<span><span>#</span> Paste Gemini key here</span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m gemini-2.5-flash-preview-05-20</pre></div>
<p>You can even run simple tools with Qwen3:4b, a <em>tiny</em> (2.6GB) model that I run using <a href="https://ollama.com/">Ollama</a>:</p>
<div><pre>ollama pull qwen3:4b
llm install <span><span>'</span>llm-ollama&gt;=0.11a0<span>'</span></span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m qwen3:4b</pre></div>
<p>Qwen 3 calls the tool, thinks about it a bit and then prints out a response:
<img src="https://static.simonwillison.net/static/2025/llm-tools-qwen.jpg" alt="Tool call: llm_version({}) 0.26a1<think> Okay, the user asked, &quot;What version?&quot; I need to respond with the version of the LLM. The tool provided is llm_version, which returns the installed version. I called that function and got the response 0.26a1. Now I should present this information clearly. Let me check if there's any additional context needed, but the user just asked for the version, so a straightforward answer should work. I'll state the version number and maybe mention that it's the installed version. Keep it simple and precise. </think> The installed version of the LLM is 0.26a1."></p>
<h4 id="more-interesting-tools-from-plugins">More interesting tools from plugins</h4>
<p>This demo has been pretty weak so far. Let’s do something a whole lot more interesting.</p>
<p>LLMs are notoriously bad at mathematics. This is deeply surprising to many people: supposedly the most sophisticated computer systems we’ve ever built can’t multiply two large numbers together?</p>
<p>We can fix that with tools.</p>
<p>The <a href="https://github.com/simonw/llm-tools-simpleeval">llm-tools-simpleeval</a> plugin exposes the <a href="https://github.com/danthedeckie/simpleeval">simpleeval</a> “Simple Safe Sandboxed Extensible Expression Evaluator for Python” library by Daniel Fairhead. This provides a robust-enough sandbox for executing simple Python expressions.</p>
<p>Here’s how to run a calculation:</p>
<div><pre>llm install llm-tools-simpleeval
llm -T simpleeval </pre></div>
<p>Trying that out:</p>
<div><pre>llm -T simple_eval <span><span>'</span>Calculate 1234 * 4346 / 32414 and square root it<span>'</span></span> --td</pre></div>
<p>I got back this—it tried <code>sqrt()</code> first, then when that didn’t work switched to <code>** 0.5</code> instead:</p>
<pre><code>Tool call: simple_eval({'expression': '1234 * 4346 / 32414'})
  165.45208860368976


Tool call: simple_eval({'expression': 'sqrt(1234 * 4346 / 32414)'})
  Error: Function 'sqrt' not defined, for expression 'sqrt(1234 * 4346 / 32414)'.


Tool call: simple_eval({'expression': '(1234 * 4346 / 32414) ** 0.5'})
  12.862818066181678

The result of (1234 * 4346 / 32414) is approximately
165.45, and the square root of this value is approximately 12.86.
</code></pre>
<p>I’ve released four tool plugins so far:</p>
<ul>
<li>
<strong><a href="https://github.com/simonw/llm-tools-simpleeval">llm-tools-simpleeval</a></strong>—as shown above, simple expression support for things like mathematics.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-quickjs">llm-tools-quickjs</a></strong>—provides access to a sandboxed QuickJS JavaScript interpreter, allowing LLMs to run JavaScript code. The environment persists between calls so the model can set variables and build functions and reuse them later on.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-sqlite">llm-tools-sqlite</a></strong>—read-only SQL query access to a local SQLite database.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-datasette">llm-tools-datasette</a></strong>—run SQL queries against a remote <a href="https://datasette.io/">Datasette</a> instance!</li>
</ul>
<p>Let’s try that Datasette one now:</p>
<div><pre>llm install llm-tools-datasette
llm -T <span><span>'</span>Datasette("https://datasette.io/content")<span>'</span></span> --td <span><span>"</span>What has the most stars?<span>"</span></span></pre></div>
<p>The syntax here is slightly different: the Datasette plugin is what I’m calling a “toolbox”—a plugin that has multiple tools inside it and can be configured with a constructor.</p>
<p>Specifying <code>--tool</code> as <code>Datasette("https://datasette.io/content")</code> provides the plugin with the URL to the Datasette instance it should use—in this case the <a href="https://datasette.io/content">content database</a> that powers the Datasette website.</p>
<p>Here’s the output, with the schema section truncated for brevity:</p>
<p><img src="https://static.simonwillison.net/static/2025/datasette-tool.jpg" alt="I run that command. It first does a Tool call to Datasette_query with SELECT name, stars, FROM repos ORDER BY stars DESC LIMIT 1. This returns an error message because there is no such column stars. It calls the Datasette_schema() function which returns a whole load of CREATE TABLE statements. Then it executes Datasette_query again this time with SELECT name, stargazers_count FROM repos ORDER BY stargazers_count DESC LIMIT 1. This returns name=datasette a count of 10020, so the model replies and says The repository with the most stars is &quot;datasette&quot; with 10,020 stars."></p>
<p>This question triggered three calls. The model started by guessing the query! It tried <code>SELECT name, stars FROM repos ORDER BY stars DESC LIMIT 1</code>, which failed because the <code>stars</code> column doesn’t exist.</p>
<p>The tool call returned an error, so the model had another go—this time calling the <code>Datasette_schema()</code> tool to get the schema of the database.</p>
<p>Based on that schema it assembled and then executed the correct query, and output its interpretation of the result:</p>
<blockquote>
<p>The repository with the most stars is “datasette” with 10,020 stars.</p>
</blockquote>
<p>Getting to this point was a real <a href="https://www.penny-arcade.com/comic/2010/09/17/mine-all-mine-part-one">Penny Arcade Minecraft moment</a> for me. The possibilities here are <em>limitless</em>. If you can write a Python function for it, you can trigger it from an LLM.</p>
<h4 id="ad-hoc-command-line-tools-with-functions">Ad-hoc command-line tools with <code>--functions</code>
</h4>
<p>I’m looking forward to people building more plugins, but there’s also much less structured and more ad-hoc way to use tools with the LLM CLI tool: the <code>--functions</code> option.</p>
<p>This was inspired by a similar feature <a href="https://sqlite-utils.datasette.io/en/stable/cli.html#defining-custom-sql-functions">I added to sqlite-utils</a> a while ago.</p>
<p>You can pass a block of literal Python code directly to the CLI tool using the <code>--functions</code> option, and any functions defined there will be made available to the model as tools.</p>
<p>Here’s an example that adds the ability to search my blog:</p>
<div><pre>llm --functions <span><span>'</span></span>
<span>import httpx</span>
<span>
<span>def search_blog(q):</span>
<span>    "Search Simon Willison blog"</span>
<span>    return httpx.get("https://simonwillison.net/search/", params={"q": q}).content</span>
<span><span>'</span></span> --td <span><span>'</span>Three features of sqlite-utils<span>'</span></span> -s <span><span>'</span>use Simon search<span>'</span></span></span></pre></div>
<p>This is <em>such a hack</em> of an implementation! I’m literally just hitting <a href="https://simonwillison.net/search/?q=pelicans">my search page</a> and dumping the HTML straight back into tho model.</p>
<p>It totally works though—it helps that the GPT-4.1 series all handle a million tokens now, so crufty HTML is no longer a problem for them.</p>
<p>(I had to add “use Simon search” as the system prompt because without it the model would try to answer the question itself, rather than using the search tool I provided. System prompts for tools are clearly a <em>big topic</em>, Anthropic’s own web search tool has <a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#search-instructions">6,471 tokens of instructions</a>!)</p>
<p>Here’s the output I got just now:</p>
<blockquote>
<p>Three features of sqlite-utils are:</p>
<ol>
<li>It is a combined CLI tool and Python library for manipulating SQLite databases.</li>
<li>It can automatically add columns to a database table if you attempt to insert data that doesn’t quite fit (using the alter=True option).</li>
<li>It supports plugins, allowing the extension of its functionality through third-party or custom plugins.</li>
</ol>
</blockquote>
<p>A better search tool would have more detailed instructions and would return relevant snippets of the results, not just the headline and first paragraph for each result. This is pretty great for just four lines of Python though!</p>
<h4 id="tools-in-the-llm-python-api">Tools in the LLM Python API</h4>
<p>LLM is both a CLI tool and a Python library at the same time (similar to my other project <a href="https://sqlite-utils.datasette.io/">sqlite-utils</a>). The LLM Python library <a href="https://llm.datasette.io/en/stable/python-api.html#tools">grew tool support</a> in LLM 0.26 as well.</p>
<p>Here’s a simple example solving one of the previously hardest problems in LLMs: counting the number of Rs in “strawberry”:</p>
<pre><span>import</span> <span>llm</span>

<span>def</span> <span>count_char_in_text</span>(<span>char</span>: <span>str</span>, <span>text</span>: <span>str</span>) <span>-&gt;</span> <span>int</span>:
    <span>"How many times does char appear in text?"</span>
    <span>return</span> <span>text</span>.<span>count</span>(<span>char</span>)

<span>model</span> <span>=</span> <span>llm</span>.<span>get_model</span>(<span>"gpt-4.1-mini"</span>)
<span>chain_response</span> <span>=</span> <span>model</span>.<span>chain</span>(
    <span>"Rs in strawberry?"</span>,
    <span>tools</span><span>=</span>[<span>count_char_in_text</span>],
    <span>after_call</span><span>=</span><span>print</span>
)
<span>for</span> <span>chunk</span> <span>in</span> <span>chain_response</span>:
    <span>print</span>(<span>chunk</span>, <span>end</span><span>=</span><span>""</span>, <span>flush</span><span>=</span><span>True</span>)</pre>
<p>The <code>after_call=print</code> argument is a way to peek at the tool calls, the Python equivalent of the <code>--td</code> option from earlier.</p>
<p>The <code>model.chain()</code> method is new: it’s similar to <code>model.prompt()</code> but knows how to spot returned tool call requests, execute them and then prompt the model again with the results. A <code>model.chain()</code> could potentially execute dozens of responses on the way to giving you a final answer.</p>
<p>You can iterate over the <code>chain_response</code> to output those tokens as they are returned by the model, even across multiple responses.</p>
<p>I got back this:</p>
<blockquote>
<p><code>Tool(name='count_char_in_text', description='How many times does char appear in text?', input_schema={'properties': {'char': {'type': 'string'}, 'text': {'type': 'string'}}, 'required': ['char', 'text'], 'type': 'object'}, implementation=&lt;function count_char_in_text at 0x109dd4f40&gt;, plugin=None) ToolCall(name='count_char_in_text', arguments={'char': 'r', 'text': 'strawberry'}, tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu') ToolResult(name='count_char_in_text', output='3', tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu', instance=None, exception=None)</code><br></p>
<p>There are 3 letter “r”s in the word “strawberry”.</p>
</blockquote>
<p>LLM’s Python library also supports <code>asyncio</code>, and tools can be <code>async def</code> functions <a href="https://llm.datasette.io/en/latest/python-api.html#tool-functions-can-be-sync-or-async">as described here</a>. If a model requests multiple async tools at once the library will run them concurrently with <code>asyncio.gather()</code>.</p>
<p>The Toolbox form of tools is supported too: you can pass <code>tools=[Datasette("https://datasette.io/content")]</code> to that <code>chain()</code> method to achieve the same effect as the <code>--tool 'Datasette(...)</code> option from earlier.</p>
<h4 id="why-did-this-take-me-so-long-">Why did this take me so long?</h4>
<p>I’ve been tracking <a href="https://simonwillison.net/tags/llm-tool-use/">llm-tool-use</a> for a while. I first saw the trick described in <a href="https://arxiv.org/abs/2210.03629">the ReAcT paper</a>, first published in October 2022 (a month before the initial release of ChatGPT). I built <a href="https://til.simonwillison.net/llms/python-react-pattern">a simple implementation of that</a> in a few dozen lines of Python. It was clearly a very neat pattern!</p>
<p>Over the past few years it has become <em>very</em> apparent that tool use is the single most effective way to extend the abilities of language models. It’s such a simple trick: you tell the model that there are tools it can use, and have it output special syntax (JSON or XML or <code>tool_name(arguments)</code>, it doesn’t matter which) requesting a tool action, then stop.</p>
<p>Your code parses that output, runs the requested tools and then starts a new prompt to the model with the results.</p>
<p>This works with almost <strong>every model</strong> now. Most of them are specifically trained for tool usage, and there are leaderboards like the <a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard</a> dedicated to tracking which models do the best job of it.</p>
<p>All of the big model vendors—OpenAI, Anthropic, Google, Mistral, Meta—have a version of this baked into their API, either called tool usage or function calling. It’s all the same underlying pattern.</p>
<p>The models you can run locally are getting good at this too. Ollama <a href="https://ollama.com/blog/tool-support">added tool support</a> last year, and it’s baked into the <a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md">llama.cpp</a> server as well.</p>
<p>It’s been clear for a while that LLM absolutely needed to grow support for tools. I released <a href="https://simonwillison.net/2025/Feb/28/llm-schemas/">LLM schema support</a> back in February as a stepping stone towards this. I’m glad to finally have it over the line.</p>
<p>As always with LLM, the challenge was designing an abstraction layer that could work across as many different models as possible. A year ago I didn’t feel that model tool support was mature enough to figure this out. Today there’s a very definite consensus among vendors about how this should work, which finally gave me the confidence to implement it.</p>
<p>I also presented a workshop at PyCon US two weeks ago about <a href="https://simonwillison.net/2025/May/15/building-on-llms/">Building software on top of Large Language Models</a>, which was exactly the incentive I needed to finally get this working in an alpha! Here’s the <a href="https://building-with-llms-pycon-2025.readthedocs.io/en/latest/tools.html">tools section</a> from that tutorial.</p>
<h4 id="is-this-agents-then-">Is this agents then?</h4>
<p><em>Sigh</em>.</p>
<p>I still <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#-agents-still-haven-t-really-happened-yet">don’t like</a> using the term “agents”. I worry that developers will think <a href="https://simonwillison.net/2025/May/22/tools-in-a-loop/">tools in a loop</a>, regular people will think virtual AI assistants <a href="https://en.m.wikipedia.org/wiki/Her_(2013_film)">voiced by Scarlett Johansson</a> and academics will <a href="https://simonwillison.net/2025/Mar/19/worms-and-dogs-and-countries/">grumble about thermostats</a>. But in the LLM world we appear to be converging on “tools in a loop”, and that’s absolutely what this.</p>
<p>So yes, if you want to build “agents” then LLM 0.26 is a great way to do that.</p>
<h4 id="what-s-next-for-tools-in-llm-">What’s next for tools in LLM?</h4>
<p>I already have a <a href="https://github.com/simonw/llm/milestone/13">LLM tools v2 milestone</a> with 13 issues in it, mainly around improvements to how tool execution logs are displayed but with quite a few minor issues I decided shouldn’t block this release. There’s a bunch more stuff in the <a href="https://github.com/simonw/llm/issues?q=is%3Aissue%20state%3Aopen%20label%3Atools">tools label</a>.</p>
<p>I’m most excited about the potential for plugins.</p>
<p>Writing tool plugins is <em>really fun</em>. I have an <a href="https://github.com/simonw/llm-plugin-tools">llm-plugin-tools</a> cookiecutter template that I’ve been using for my own, and I plan to put together a tutorial around that soon.</p>
<p>There’s more work to be done adding tool support to more model plugins. I added <a href="https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#supporting-tools">details of this</a> to the advanced plugins documentation. This commit <a href="https://github.com/simonw/llm-gemini/commit/a7f1096cfbb733018eb41c29028a8cc6160be298">adding tool support for Gemini</a> is a useful illustratino of what’s involved.</p>

<p>And yes, <strong>Model Context Protocol</strong> support is clearly on the agenda as well. MCP is emerging as the standard way for models to access tools at a frankly bewildering speed. Two weeks ago it wasn’t directly supported by the APIs of any of the major vendors. In just the past eight days <a href="https://simonwillison.net/2025/May/27/mistral-agents-api/">it’s been added</a> by OpenAI, Anthropic <em>and</em> Mistral! It’s feeling like a lot less of a moving target today.</p>
<p>I want LLM to be able to act as an MCP client, so that any of the MCP servers people are writing can be easily accessed as additional sources of tools for LLM.</p>
<p>If you’re interested in talking more about what comes next for LLM, <a href="https://datasette.io/discord-llm">come and chat to us in our Discord</a>.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why the Original Macintosh Had a Screen Resolution of 512×324 (162 pts)]]></title>
            <link>https://512pixels.net/2025/05/original-macintosh-resolution/</link>
            <guid>44110219</guid>
            <pubDate>Tue, 27 May 2025 20:02:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://512pixels.net/2025/05/original-macintosh-resolution/">https://512pixels.net/2025/05/original-macintosh-resolution/</a>, See on <a href="https://news.ycombinator.com/item?id=44110219">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-32349">
	<!-- .entry-header -->

	<div>
		<p>Many classic Macs came with — or supported — displays running at 512×384 pixels, but many compact Macs, ranging from <a href="https://support.apple.com/en-us/112190">the original 1984 machine</a> up through 1991’s <a href="https://support.apple.com/en-us/112201">Macintosh Classic II</a> had built-in CRTs running at 512×342 pixels. That covers all black-and-white compact Macs with a 9-inch screen. The later Color Classic and Color Classic II used a 10-inch CRT at a full 512×384.</p>
<p>This came up when <a href="https://512pixels.net/2025/05/oh-hey-it-me/">I joined John Gruber on The Talk Show</a>. At one point in the show, I rattled off the original Mac’s resolution as being 512×384.</p>
<p>Except… it wasn’t. The original Mac screen ran at 512×342. I remembered the right number and corrected myself a moment later, but given the name of this website, it was pretty embarrassing. This set me off on a journey to understand why Apple made this decision. Why were the displays on early Macs 42 pixels shorter in height than later ones?</p>
<p>After doing <em>a lot</em> of reading, there are several factors to consider when trying to answer this question.</p>
<p><img decoding="async" src="https://512pixels.net/wp-content/uploads/2025/05/apple1984mac-full.png" alt="Original Macintosh"></p>
<h2>Memory</h2>
<p>The original Mac had a mere 128 <em>kilobytes</em> of memory. The photo of the original Macintosh in this blog post is 604 KB in size, some 4.7x larger than the entire memory footprint of the 1984 machine. Of course, Apple would improve this with later models, but many design decisions made to accommodate the original Mac would forward for years.</p>
<p>Over at Folklore.org, <a href="https://www.folklore.org/Five_Different_Macs.html">Andy Hertzfeld wrote a great post</a> walking through several early versions of what would become the Macintosh, including ones with even <em>less</em> memory:</p>
<blockquote><p>
  In the beginning of 1982, the original 68000 design was more than a year old, and the software was nowhere near finished, so Burrell [Smith] was afraid some of the trade-offs of the original design were no longer current. He used the expansive canvas of a custom chip, where additional logic was almost free, to update the architecture.</p>
<p>  The most important decision was admitting that the software would never fit into 64K of memory and going with a full 16-bit memory bus, requiring 16 RAM chips instead of 8. The extra memory bandwidth allowed him to double the display resolution, going to dimensions of 512 by 342 instead of 384 by 256. He also added bells and whistles like a fancy, DMA-fed sound generator with four independent voices. This was the fourth version of the Macintosh design.
</p></blockquote>
<p>Shipping a computer in the 1980s with a resolution of 384×256 wouldn’t have been too wild. 1982’s Commodore 64 ran at a maximum resolution 320×200. The Apple IIe that shipped in 1983 offered several display modes:</p>
<ul>
<li>40 or 80 columns text, white-on-black, with 24 lines</li>
<li>Low-Resolution: 40×48 (16 colors)</li>
<li>High-Resolution: 280×192 (6 colors)</li>
<li>Double-Low-Resolution: 80×48 (16 colors)</li>
<li>Double-High-Resolution: 560×192 (16 colors)</li>
</ul>
<p>Computers like the C64 and Apple II had to pull off a lot of tricks to pull off getting graphics on the screen. The Macintosh was going to be powered by a full GUI, and 384×256 would have been just too chunky, so thinking about 128 kilobytes of RAM as an <em>upgrade</em> is a fun twist on the normal take of “Wow, the original Mac was so held back!” Really, it’s amazing that it could do what it did.</p>
<p>That feeling takes on new life when you realize the Mac used a portion of its memory to drive the display. At 512×342, the memory needed to draw the screen was a total of 21.8 KB. Had Apple opted for a 4:3 display running at 512×384, the system would have needed 24 KB for the display. Every byte was precious back in the day. Again, <a href="https://folklore.org/Monkey_Lives.html">we turn to Andy Hertzfeld</a>:</p>
<blockquote><p>
  The original Macintosh only had 128K bytes of RAM (that’s one eighth of a megabyte), so dealing with memory management was usually the hardest part of writing both the system and applications. We allocated around 16K bytes for system use, and another 22K bytes for the 512 by 342 black and white screen, so applications were left with only 90K bytes or so. The bigger ones like MacWrite or MacPaint seemed to be bursting at the seams.
</p></blockquote>
<p>It seems that two things are true:</p>
<ul>
<li>The original Macintosh shipped with more memory than earlier designs</li>
<li>Even at 128K, things were extremely tight</li>
</ul>
<p>Daniel Knight pointed this out when writing about the original Mac:</p>
<blockquote><p>
  As&nbsp;<a href="http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Scrooge_McDuck.txt&amp;topic=Origins&amp;sortOrder=Sort%20by%20Date">Andy Hertzfeld writes</a>, the Mac was only going to have a 256×256 pixel display (a step up from the 280×192 graphics of the Apple II). It wasn’t until&nbsp;<a href="http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Good_Earth.txt&amp;topic=Origins&amp;sortOrder=Sort%20by%20Date">January 1981</a>&nbsp;that the Mac team decided to give the Motorola 68000 a try. A good thing, too, as the first Mac shipped with a 512×342 pixel display, and that would have consumed over 30% of the 64 KB of memory originally envisioned for the low-cost information appliance.
</p></blockquote>
<h2>CPU Timing</h2>
<p>At the heart of the Macintosh was a Motorola 68000 CPU running at 8 MHz. Just like the 128 kilobytes of RAM, this came with some inherit limitations when paired with the display hardware.</p>
<p>To minimize CRT flicker, Apple worked to achieve a vertical refresh rate of 60 Hz. This meant the CPU spent one-third of its time drawing the display. Just as with the memory, a taller screen would have taken more resources away from running the Mac’s operating system and programs.</p>
<p>If you are familiar with standard TV formats, you probably have already picked up on the fact that this refresh rate/screen size combination meant the Mac was incompatible with NTSC composite video, which the Apple II supported. (It’s also different than PAL systems.) This let Apple balance performance and picture quality in a way the team saw fit, given the hardware that they had, <a href="https://www.folklore.org/Joining_Apple_Computer.html">as Bill Atkinson wrote:</a></p>
<blockquote><p>
  The Apple II displayed white text on a black background. I argued that to do graphics properly we had to switch to a white background like paper. It works fine to invert text when printing, but it would not work for a photo to be printed in negative. The Lisa hardware team complained the screen would flicker too much, and they would need faster refresh with more expensive RAM to prevent smearing when scrolling. Steve listened to all the pros and cons then sided with a white background for the sake of graphics.
</p></blockquote>
<p>Here’s the thing: the original Mac <em>did not</em> run at 8 MHz, but rather 7.83 MHz. This slight tuning meant Apple could time the CPU’s cycles and the CRT’s need for updating more easily.</p>
<h2>Square Pixels</h2>
<p>Running the 9-inch CRT at 512×342 gave the original Mac a pixels density of 72 PPI, but more importantly, the screen size allowed the Mac to have square pixels.</p>
<p>Apple’s first GUI-powered machine, <a href="https://www.macstories.net/mac/the-lisa/">the Lisa</a>, famously had rectangular pixels, <a href="https://folklore.org/Square_Dots.html">as Andy Hertzfeld covered here</a>:</p>
<blockquote><p>
  The Lisa team decided to optimize their display for horizontal resolution, in order to be able to display 80 columns of text in an attractive font. The vertical resolution wasn’t as important, because vertical scrolling works much better for text than horizontal scrolling. The designers decided to endow Lisa with twice as much horizontal resolution as vertical, using a 720 by 360 pixel display, with pixels that were twice as high as they were wide. This was great for text oriented applications like the word processor, but it made things somewhat awkward for the more graphical applications.</p>
<p>  When Burrell redesigned the Macintosh in December 1980 to use the same microprocessor as Lisa, the Motorola 68000, it set off shock waves within Apple. Not only was Burrell’s new design much simpler than Lisa, with less than half the chip count, but it also ran almost twice as fast, using an 8 megahertz clock instead of a 5 megahertz clock. Among other advantages was the fact that the Mac’s 384 by 256 pixel display had the identical horizontal and vertical resolution, a feature that we called “square dots”. Square dots made it easier to write graphical applications, since you didn’t have to worry about the resolution disparity.
</p></blockquote>
<p>Hertzfeld goes on to share that the Mac team tried to get the Lisa team to switch to square pixels, bumping the machine’s resolution to a mind-blowing-for-the-time 768×512 pixels, but it wasn’t in the cards, as the Lisa was well on its way to shipping by the time this meeting took place.</p>
<p><img decoding="async" src="https://512pixels.net/wp-content/uploads/2025/05/apple-lisa.jpg" alt="Apple Lisa"></p>
<p>Of course, the Lisa would end up being a doomed product, and in 1985, Apple rebadged a later revision of the machine as the “Macintosh XL.” It shipped with <a href="https://en.wikipedia.org/wiki/MacWorks_XL">a software shim called “MacWorks XL”</a> that allowed Mac software to run on the Lisa, but the rectangular pixels made the software appear stretched. To solve this, Apple sold a product named the  Macintosh XL Screen Kit, which changed the resolution to 608×432 pixels. This is how the product is described in <a href="https://512pixels.net/wp-content/uploads/2025/05/Lisa-DIY-Guide.pdf">a document outlining DIY upgrades</a> from <a href="https://en.wikipedia.org/wiki/Sun_Remarketing">Sun Remarketing</a>, a company that was focused on keeping Lisa hardware up and running.</p>
<blockquote><p>
  No recently restored Lisa/Mac XL is complete without a Macintosh XL Screen Kit. Unlike the standard 9-inch Macintosh which has square pixels, the stock Lisa/XL has rectangular pixels. With rectangular pixels, circles look like footballs, squares look like spaghetti boxes. The purpose of the Macintosh XL Screen Kit is to square up the pixels. Proportions become exactly the same as on other Macs (1 to 1 ), but the overall display area (608 pixels x 432 pixels) is made roughly the same as a 12-inch Macintosh 11 WYSIWYG monitor (640×480). Standard 9-inch Macs only display 512×342 pixels.</p>
<p>  The complete screen modification kit includes new 3A boot ROMs, a new video ROM and a new yoke coil. (Newer software requires System Update 5.0 and MacWorks Plus as well.) Conscientious installation of the complete screen kit requires one to two hours.
</p></blockquote>
<h2>Mimicking the Real World</h2>
<p>In addition to their square pixels making on-screen graphics look better, the Macintosh team also wanted the computer to be useful for those who needed to print their work. The 72 DPI screen was more than sharp enough for work in applications like MacWrite, MacPaint, and the page layout tools that would follow them. Users could see their work full-sized or at a reduced scale to get a sense of the overall page they were working on. Larger displays would come, but for 1984, 512×342 was plenty.</p>
<h2>Everything in Balance</h2>
<p>In short, there’s no easy answer to explain why early compact Macs ran at a screen resolution of 512×342. Rather, Apple was doing what it does best: designing a product with the right trade-offs for performance, ease of use, and cost. In a few short years, the Mac would grow to support larger displays, but for 1984, the balance was set correctly.</p>
<p>In the very first edition of <em>Macworld</em> magazine, <a href="https://archive.org/details/MacWorld_8404_April_1984_premier">Matthew Douglas wrote</a>:</p>
<blockquote><p>
  Appearances can be deceiving. Most computers display text on one of 24 or 25 “invisible” horizontal lines on the screen. This display is called text mode. To display graphics, the software switches to graphics mode, and the display becomes a field of dots. Each dot, or pixel, is either off (invisible) or on (visible). Of course, a computer may have more than one text mode or two or more graphics modes, or it may be a mixed mode of graphics and text.</p>
<p>  The Mac display has only one mode: graphics. The entire screen is made up of dots: 512 dots horizontally and 342 dots vertically, a total of175,104 dots that combine to display everything you’ll ever see on a Mac screen. (Now you know the secret behind the incredible range of type fonts, attributes, and type sizes.)
</p></blockquote>
<p>In the same edition, David Bunnell interviewed Bill Gates, and he was asked about what made the Mac special. Gates — who had previously said the Mac was a computer he would want his mom to try — replied:</p>
<blockquote><p>
  The Mac was designed as a graphics machine. Apple didn’t put in a ROM character generator or a bunch of video modes. They put in only one video mode, and that’s the pure bit-mapped, 512-by 342-pixel screen. The monitor was designed into the machine so that they could get extremely crisp pictures and have one integrated system. They knew what the aspect ratio was and how the dots would appear. And they also made sure that the mouse would be used and that the 64K ROM would support very rich graphics interaction.</p>
<p>  You can configure a PC with one of the better graphics boards and add a Microsoft mouse and the necessary software, but that’s not the thrust of the machine. The PC is used primarily in its text mode, and to date it’s used mostly without a mouse; you couldn’t get performance or graphics like the Mac’s out of the PC at a comparable price. Although they’re both “turing” machines (that is, they have finite memory), the thrust of the Mac is quite different.</p>
<p>  Of all the personal computers available today, the Mac is unique. It’s the first time somebody said, “We don’t need a lot of the things that other personal computers have, so let’s optimize a few areas and make sure the software is designed around them.”
</p></blockquote>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running GPT-2 in WebGL: Rediscovering the Lost Art of GPU Shader Programming (134 pts)]]></title>
            <link>https://nathan.rs/posts/gpu-shader-programming/</link>
            <guid>44109257</guid>
            <pubDate>Tue, 27 May 2025 18:02:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nathan.rs/posts/gpu-shader-programming/">https://nathan.rs/posts/gpu-shader-programming/</a>, See on <a href="https://news.ycombinator.com/item?id=44109257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="content"><article><p><i>May, 24 2025 •
16 min read •
2461 words</i></p><p>Preface: A few weeks back, I implemented GPT-2 using WebGL and shaders (<a href="https://github.com/nathan-barry/gpt2-webgl/tree/main">Github Repo</a>) which made the front page of Hacker News (<a href="https://news.ycombinator.com/item?id=43870998">discussion</a>). By popular demand, here is a short write-up over the main ideas behind GPU shader programming (for general-purpose computing).</p><div><h2>Table Of Contents</h2><hr><div><nav id="TableOfContents"><ol><li><a href="#the-origins-of-general-purpose-gpu-programming">The Origins of General Purpose GPU Programming</a></li><li><a href="#graphics-api-vs-compute-api">Graphics API vs Compute API</a></li><li><a href="#implementing-gpt-2-with-shaders">Implementing GPT-2 with shaders</a><ol><li><a href="#textures--framebuffers-the-data-bus">Textures &amp; Framebuffers: The Data Bus</a></li><li><a href="#fragment-shaders-as-compute-kernels">Fragment Shaders as Compute Kernels</a></li><li><a href="#chaining-passes">Chaining Passes</a></li><li><a href="#limitations">Limitations</a></li></ol></li></ol></nav></div></div><h2 id="the-origins-of-general-purpose-gpu-programming">The Origins of General Purpose GPU Programming</h2><hr><p>In the early 2000s, NVIDIA introduced programmable shaders with the GeForce 3 (2001) and GeForce FX (2003). Instead of being limited to predetermined transformations and effects of earlier GPUs, developers were now given unprecedented control over the rendering pipeline, enabling much more sophisticated visual effects. These programmable shaders laid the foundation for modern GPU computing.</p><p>Researchers soon discovered that certain computations (like linear algebra involving matrices and vectors) could be accelerated by casting them as graphics operations on the GPU.
However, using shader languages like OpenGL’s GLSL for no-graphics tasks was cumbersome. By the mid-2000s, the need for a more straightforward, non-graphics interface to GPUs had become clear, and NVIDIA saw a new opportunity.</p><p>Inspired by the demand for <strong>general-purpose GPU (GPGPU)</strong> programming, in November 2006, NVIDIA released <strong>CUDA</strong>, the <strong>Compute Unified Device Architecture</strong>. CUDA is a parallel computing platform and programming model that gives developers direct access to the GPU’s computational power without the intermediary of a graphics API. With CUDA, one could write C/C++ code to execute on the CPU using straightforward extensions for parallel kernels and managing GPU memory explicitly. This meant that developers could now ignore graphics-specific concepts and dramatically lowered the barrier for general-purpose GPU computing. Following CUDA came OpenCL which expanded general purpose computing beyond the NVIDIA ecosystem.</p><h2 id="graphics-api-vs-compute-api">Graphics API vs Compute API</h2><hr><p>Traditional graphics APIs like OpenGL are centered around a fixed-function pipeline tailored for rendering images. The pipeline consists of stages like vertex processing, rasterization, fragment processing, etc. Each stage can be programmable with shaders, but the overall flow is fixed.
Using OpenGL for computation required a lot of boilerplate. One had to pack data into texture formats, use off-screen framebuffers to capture the results, and often perform multiple render passes to accomplish multi-stage algorithms.</p><p>In contrast, OpenCL and CUDA expose a direct compute model which lets you treat the GPU as one giant SIMD processor:</p><ul><li><strong>Kernels, not shaders</strong>: You write a function and then launch thousands of copies to run in parallel (no notion of vertices or fragments).</li><li><strong>Raw buffers</strong>: Allocate arrays of floats or integers, read/write them directly, and move them back and forth between host and device with explicit copy calls.</li><li><strong>User-driven pipeline</strong>: You define exactly what happens and when instead of using a predefined fixed sequence of rendering stages.</li></ul><p>The result is a far more natural fit for linear algebra, simulations, physics, ML, and any algorithm where you just want to compute independent calculations in bulk.</p><p>In OpenGL, the output of your computation would ultimately be pixels in a framebuffer or values in a texture; in OpenCL, the output can be data in any form (float arrays, computed lists of numbers, etc.) which you then transfer back to the CPU or use in further computations. This makes OpenCL more suitable for general algorithms where you just want the numerical results.</p><h2 id="implementing-gpt-2-with-shaders">Implementing GPT-2 with shaders</h2><hr><p>Below covers textures, framebuffers, vertex and fragment shaders, and other graphics specific concepts I hijacked to get GPT-2 running on a GPU using shaders.</p><h3 id="textures--framebuffers-the-data-bus">Textures &amp; Framebuffers: The Data Bus</h3><p>In traditional graphics rendering, a <strong>texture</strong> is simply a 2D (or 3D) array of pixel data stored in GPU memory. When you map a texture onto a triangle, the fragment shader “samples” it to look up color values. In our compute‐as‐graphics paradigm, we hijack this same mechanism to store and manipulate numerical data instead of colors:</p><ul><li><p><strong>Textures as tensors</strong>:
Each texture is an array of floating‐point values (we use single‐channel R32F formats), where each pixel encodes one element of a matrix or vector. Just like you’d think of an H×W texture as holding H×W RGB pixels for an image, here it holds H×W scalar values for a weight matrix or activation map.</p></li><li><p><strong>Sampling without filtering</strong>:
We use functions like <code>texelFetch</code> to read texture data by exact integer coordinates, bypassing any interpolation. This gives us deterministic, “random access” reads into our weight and activation arrays, akin to indexing into a CPU array by row and column.</p></li></ul><p>A <strong>Framebuffer Object (FBO)</strong> is a lightweight container that lets us redirect rendering output from the screen into one of our textures:</p><ol><li><p><strong>Attach a texture as the render target</strong>:
By binding a texture to an FBO, any draw call you make, normally destined for your monitor, writes into that texture instead. The fragment shader’s <code>out</code> variable becomes a write port into GPU memory.</p></li><li><p><strong>Offscreen and ping-pong rendering</strong>:
Because we can attach different textures in succession, we “ping-pong” between them: one pass writes into <strong>Texture A</strong>, the next pass reads from <strong>Texture A</strong> while writing into <strong>Texture B</strong>, and so on. This avoids ever copying data back to the CPU until the very end.</p></li><li><p><strong>High‐throughput data bus</strong>:
All of this happens entirely on the GPU’s VRAM bus. Binding textures and framebuffers is just pointer swapping on the GPU. Once set up, your fragment shader passes stream through millions of cores in parallel, reading, computing, and writing without ever touching system memory.</p></li></ol><p>Together, textures and FBOs form the <strong>data bus</strong> of our shader‐based compute engine: textures hold the raw bits of your neural network (weights, intermediate activations, and outputs), and framebuffers let you chain shader passes seamlessly, keeping everything on the high-speed GPU pipeline until you explicitly pull the final logits back to the CPU.</p><h3 id="fragment-shaders-as-compute-kernels">Fragment Shaders as Compute Kernels</h3><p>Fragment shaders are where the magic happens. Instead of using fragment shaders to shade pixels for display, we hijack them as compute kernels; each fragment invocation becomes one “thread” that calculates a single output value. The GPU will launch thousands of these in parallel, giving us massive throughput for neural-network operations.</p><p>Below is an example fragment shader for matrix multiplication:</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// Matrix Multiply (C = A × B)</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>precision</span> <span>highp</span> <span>float</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_A<span>;</span>    <span>// Texture holding matrix A (M x K)</span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_B<span>;</span>    <span>// Texture holding matrix B (K x N)</span>
</span></span><span><span><span>uniform</span> <span>int</span>        u_K<span>;</span>   <span>// Shared inner dimension</span>
</span></span><span><span><span>out</span> <span>vec4</span>           outColor<span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  <span>// Determine the output coordinate (i, j) from the fragment’s pixel position</span>
</span></span><span><span>  <span>ivec2</span> coord <span>=</span> <span>ivec2</span><span>(</span>gl_FragCoord<span>.</span>xy<span>);</span>
</span></span><span><span>  <span>float</span> sum <span>=</span> <span>0.0</span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>// Perform the dot–product along the K dimension</span>
</span></span><span><span>  <span>for</span> <span>(</span><span>int</span> k <span>=</span> <span>0</span><span>;</span> k <span>&lt;</span> u_K<span>;</span> <span>++</span>k<span>)</span> <span>{</span>
</span></span><span><span>    <span>float</span> a <span>=</span> texelFetch<span>(</span>u_A<span>,</span> <span>ivec2</span><span>(</span>k<span>,</span> coord<span>.</span>y<span>),</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>    <span>float</span> b <span>=</span> texelFetch<span>(</span>u_B<span>,</span> <span>ivec2</span><span>(</span>coord<span>.</span>x<span>,</span> k<span>),</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>    sum <span>+=</span> a <span>*</span> b<span>;</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span>
</span></span><span><span>  <span>// Write the result into the single‐channel R component of the output texture</span>
</span></span><span><span>  outColor <span>=</span> <span>vec4</span><span>(</span>sum<span>,</span> <span>0.0</span><span>,</span> <span>0.0</span><span>,</span> <span>1.0</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Here we have:</p><ul><li><strong>Per-pixel work item</strong>: Each fragment corresponds to one matrix element (i, j). The GPU runs this loop for every (i, j) in parallel across its shader cores.</li><li><strong>Exact indexing</strong>: texelFetch reads a single float by its integer coordinate.</li><li><strong>Write-back</strong>: Assigning to outColor.r writes that computed value directly into the bound FBO’s texture at (i, j).</li></ul><p>Here is another fragment shader but for the GELU activation function:</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// GELU Activation</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>precision</span> <span>highp</span> <span>float</span><span>;</span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_X<span>;</span>
</span></span><span><span><span>out</span> <span>vec4</span> o<span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  <span>ivec2</span> c <span>=</span> <span>ivec2</span><span>(</span>gl_FragCoord<span>.</span>xy<span>);</span>
</span></span><span><span>  <span>float</span> x <span>=</span> texelFetch<span>(</span>u_X<span>,</span> c<span>,</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>  <span>float</span> t <span>=</span> <span>0.5</span> <span>*</span> <span>(</span><span>1.0</span> <span>+</span> tanh<span>(</span><span>0.79788456</span> <span>*</span> <span>(</span>x <span>+</span> <span>0.044715</span> <span>*</span> x<span>*</span>x<span>*</span>x<span>)));</span>
</span></span><span><span>  o <span>=</span> <span>vec4</span><span>(</span>x <span>*</span> t<span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h4 id="shared-vertex-shader">Shared Vertex Shader</h4><p>Every operation gets its own fragment shader since that’s where the math for the operation happens. The vertex shader, on the other hand, is simple and the same for each. All it does is draw two triangles which covers the entire view port.</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// Shared Vertex Shader</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>in</span> <span>vec2</span> a_position<span>;</span>
</span></span><span><span><span>out</span> <span>vec2</span> v_uv<span>;</span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  v_uv <span>=</span> a_position <span>*</span> <span>0.5</span> <span>+</span> <span>0.5</span><span>;</span>  <span>// map [-1,+1] to [0,1]</span>
</span></span><span><span>  gl_Position <span>=</span> <span>vec4</span><span>(</span>a_position<span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><ul><li><strong>Full-screen quad</strong>: Two triangles cover the viewport. Every pixel in the fragment stage maps to one tensor element.</li><li><strong>Reusable</strong>: Because the vertex work is identical for all operations, we compile it once and reuse it across every matrix multiply, activation, and bias-add pass.</li></ul><p>With this structure in mind, every “shader pass” is really just:</p><ol><li><strong>Vertex shader</strong>: map two triangles to the viewport</li><li><strong>Fragment shader</strong>: perform one tiny piece of your transformer math per pixel</li></ol><h3 id="chaining-passes">Chaining Passes</h3><p>Under the hood, every neural‐network operation, whether it’s a matrix multiply, an activation function, or a bias addition, boils down to four simple GPU steps:</p><ol><li>Bind inputs as textures (weights, activations, or intermediate results).</li><li>Attach a fresh output texture to an offscreen framebuffer (FBO).</li><li>Draw a full‐screen quad using the shared vertex shader.</li><li>Execute the fragment shader, which performs the actual computation per pixel.</li></ol><p>All of the WebGL boilerplate lives in our <code>_runPass</code> helper, so each pass in the GPT-2 forward loop feels like a single, declarative call:</p><div><pre tabindex="0"><code data-lang="typescript"><span><span><span>private</span> <span>_runPass</span><span>(</span>
</span></span><span><span>  <span>name</span>: <span>string</span><span>,</span>
</span></span><span><span>  <span>inputs</span><span>:</span> <span>{</span> <span>[</span><span>u</span>: <span>string</span><span>]</span><span>:</span> <span>WebGLTexture</span> <span>},</span>
</span></span><span><span>  <span>ints</span><span>:</span> <span>{</span> <span>[</span><span>u</span>: <span>string</span><span>]</span><span>:</span> <span>number</span> <span>},</span>
</span></span><span><span>  <span>outTex</span>: <span>WebGLTexture</span><span>,</span>
</span></span><span><span>  <span>W</span>: <span>number</span><span>,</span>
</span></span><span><span>  <span>H</span>: <span>number</span>
</span></span><span><span><span>)</span> <span>{</span>
</span></span><span><span>  <span>// Grab the WebGL2 context and compiled shader program for this pass
</span></span></span><span><span><span></span>  <span>const</span> <span>gl</span> <span>=</span> <span>this</span><span>.</span><span>gl</span><span>;</span>
</span></span><span><span>  <span>const</span> <span>prog</span> <span>=</span> <span>this</span><span>.</span><span>programs</span><span>[</span><span>name</span><span>];</span> <span>// This has our vertex and frag shaders
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>useProgram</span><span>(</span><span>prog</span><span>);</span>
</span></span><span><span>
</span></span><span><span>  <span>// BOILERPLATE: Bind all input textures as uniforms
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// BOILERPLATE: Bind an FBO and attach our empty texture to it.
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// BOILERPLATE: Set up the full-screen quad geometry
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Draw into our texture
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>viewport</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>W</span><span>,</span> <span>H</span><span>);</span>            <span>// Ensure viewport matches texture size
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>drawArrays</span><span>(</span><span>gl</span><span>.</span><span>TRIANGLES</span><span>,</span> <span>0</span><span>,</span> <span>6</span><span>);</span>  <span>// Runs our shaders
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Clean up: Unbind FBO
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>bindFramebuffer</span><span>(</span><span>gl</span><span>.</span><span>FRAMEBUFFER</span><span>,</span> <span>null</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h4 id="forward-pass-layer-by-layer">Forward Pass: Layer by Layer</h4><p>Because each pass leaves its results in VRAM, we never pay the cost of round-trips back to the CPU until the very end. Below is a high-level description of the entire forward pass:</p><ol><li><strong>Upload Embeddings</strong>: Compute the token+position embeddings on the CPU and send them to the GPU as one texture.</li><li><strong>Transformer Layers (12 in total)</strong>:<ul><li><em>Normalize &amp; Project</em>: Apply layer normalization, then run the attention and feed-forward sublayers entirely on the GPU.</li><li><em>Attention</em>: Compute queries, keys, values; calculate attention weights; combine values.</li><li><em>Feed-Forward</em>: Two matrix multiplies with a GELU activation in between.</li><li><em>Residuals</em>: Add the layer’s input back in at each substep.</li></ul></li><li><strong>Final Normalization &amp; Output</strong>: Do one last layer normalization, multiply by the output weight matrix, then read the resulting logits back to the CPU.</li></ol><p>Once logits are back on the CPU, we apply softmax and sample (top-k or top-p) to pick the next token. Then the process starts over again with the new token being appended to the context.</p><p>By chaining these operation passes together, we keep the entire GPT-2 pipeline on the GPU until the final logits. This is how programmable shaders let us treat the graphics pipeline as a general-purpose parallel engine.</p><h3 id="limitations">Limitations</h3><p>While hijacking WebGL allows us to run machine learning models on the GPU, it carries several key limitations:</p><ul><li><strong>No shared/local memory</strong>: Fragment shaders can only read/write global textures. There’s no on-chip scratchpad for blocking or data reuse, so you’re limited to element-wise passes.</li><li><strong>Texture size limits</strong>: GPUs enforce a maximum 2D texture dimension (e.g. 16 K×16 K). Anything larger must be manually split into tiles, adding bookkeeping and extra draw calls.</li><li><strong>No synchronization or atomics</strong>: You can’t barrier or coordinate between fragments in a pass, making reductions, scatter/gather, and other data-dependent patterns difficult or impossible.</li><li><strong>Draw-call and precision overhead</strong>: Every neural-net operation requires binding an FBO, swapping textures, and issuing a draw call (dozens per layer) which incurs CPU overhead. Plus, you’re bound to 16- or 32-bit floats (via <code>EXT_color_buffer_float</code>), with no double precision or integer textures.</li></ul><p>Taken together, these constraints make shader-based compute an interesting educational project but a only a historical novelty for real world use. Compute APIs like CUDA or OpenCL give easier and better tools to achieve the same thing.</p><p>Thanks for reading! You can view the code and run the demo locally at the repo <a href="https://github.com/nathan-barry/gpt2-webgl/tree/main">here</a>. Contact me on <a href="https://x.com/nathanbarrydev">X</a> if you have any suggestions.</p><br></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US pauses new student visa interviews as it mulls expanding social media vetting (124 pts)]]></title>
            <link>https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501</link>
            <guid>44109253</guid>
            <pubDate>Tue, 27 May 2025 18:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501">https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501</a>, See on <a href="https://news.ycombinator.com/item?id=44109253">Hacker News</a></p>
Couldn't get https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I salvaged $6k of luxury items discarded by Duke students (245 pts)]]></title>
            <link>https://indyweek.com/culture/duke-students-dumpster-diving/</link>
            <guid>44108207</guid>
            <pubDate>Tue, 27 May 2025 15:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://indyweek.com/culture/duke-students-dumpster-diving/">https://indyweek.com/culture/duke-students-dumpster-diving/</a>, See on <a href="https://news.ycombinator.com/item?id=44108207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					

<article id="post-278832">
	<div>

		
		<p><strong>I</strong> live in an apartment building in downtown Durham that houses more Duke University undergrads than any other category of person—a friend once characterized it as an “adult dorm”—so it wasn’t all that surprising when, last week, I found a cute little table in the trash room on my floor. At the end of the school year, a lot gets thrown away.</p><p>The table was in great condition, amid stacks of linens and unopened boxes of date-nut energy bites. Made from clear acrylic, its edges were tinged a neon lemon-lime color that changed with the light—sometimes appearing to be part of the acrylic itself, other times a reflection dancing along its curves.&nbsp;</p><p>I took it home. When I looked it up online, I discovered it costs $900. (Shipping cost: $199.)</p><p>That was retrieved from the trash room at the end of my hall, where you put things down the chute. The real gold mine is the ground-floor room that the chute empties into, accessible by one of the elevators.</p><p>This is where, around graduation each year, you can find dozens of vacuums, Keurigs, stainless steel trash cans in every size and shape imaginable, mattresses, mirrors, and enough luxury goods to make a reseller weep with joy. The first time I went down there, last week, I noticed something neon in a tote bag and pulled out $395 Balenciaga slides. Nearby were $980 Valentino sneakers—worn, but definitely wearable. More than $1,000 of Lululemon workout clothing tumbled from a bag onto a couch.</p><p>You don’t really have to do any digging—most of the stuff I’ve gotten was sitting on top of discarded furniture. But you do have to rush. After I took the Lululemon haul upstairs, I returned to find city waste workers loading things into a garbage truck, off to a landfill. The volume of discarded clothing seems consistent with generational trends: textile waste in the United States went up by more than 50 percent between 2000 and 2018.&nbsp;</p><div><figure><img decoding="async" width="768" height="1024" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-768x1024.jpeg" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-768x1024.jpeg 768w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-225x300.jpeg 225w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1152x1536.jpeg 1152w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1536x2048.jpeg 1536w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-900x1200.jpeg?crop=1 900w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-600x800.jpeg?crop=1 600w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-450x600.jpeg?crop=1 450w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-300x400.jpeg?crop=1 300w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-150x200.jpeg?crop=1 150w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1200x1600.jpeg 1200w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-2000x2667.jpeg 2000w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-780x1040.jpeg 780w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-400x533.jpeg 400w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-706x941.jpeg 706w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-scaled.jpeg 1920w" sizes="(max-width: 768px) 100vw, 768px"><figcaption>The trash room. Photo by Lena Geller. </figcaption></figure></div><p>Not every treasure is a flashy brand-name item. I also recovered pink satin pajamas that I remember seeing on someone attending a pajama party on my floor, and a ruffled olive-green <em>Top Gun</em> romper from a Halloween event. (Sadly, there was nothing from the risqué Dr. Seuss party. A few months ago, a fire alarm went off, and it became apparent just how much of the building is occupied by Duke students, as nearly everyone except me, my roommate, and a family with two young kids was drunk and dressed in <em>Cat in the Hat</em> costumes.)</p><p>It feels wrong for this much stuff to have been thrown out in the first place, but it also feels mildly wrong to take it. So it was nice to get intermittent reassurance from my building’s maintenance man, Eric. </p><p>The first time, as I was scurrying back to my room, carrying an upholstered kitchen chair that my cats now spend all their time in, I passed Eric in the hallway. He asked me how I was.</p><p>“Just doing some scavenging,” I said. I must have looked guilty, because he said, “That’s OK.”</p><p>A few days later, I was again downstairs in the big trash room when Eric walked in. I moved to leave, feeling awkward about being caught again. “You’re welcome here anytime,” he assured.&nbsp;</p><p>The sheer volume of valuable, usable things being discarded boggles the brain, particularly when it comes to items like clothing with the tags still on and unopened, unexpired food items.&nbsp;</p><p>In trying to make sense of things, I made spreadsheets.</p><p>The first tracks the prices and brands of the items that I kept, donated, or sold. The total value came to around $6,000, not including several items I couldn’t find prices for.</p><div><figure><img decoding="async" width="1024" height="982" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59%E2%80%AFAM-1024x982.png" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-1024x982.png 1024w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-300x288.png 300w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-768x737.png 768w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-1200x1151.png 1200w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-780x748.png 780w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-400x384.png 400w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-706x677.png 706w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM.png 1370w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>A screenshot of one section of the author’s itemized spreadsheet</figcaption></figure></div><p>The second spreadsheet compares Duke’s donation collection data with that at other universities, in an effort to understand whether this college-town phenomenon is universal. I gathered publicly available data from university websites and press releases, supplemented by direct inquiries.</p><p>Duke told me their “Devils Care Donations” initiative collected 32,000 pounds this year through partnerships with TROSA and Goodwill. Ali Harrison, senior associate dean for residence life, says that the university places donation bins in every residence hall on campus, plus off-campus Duke housing like Blue Light and Swift Apartments. Harrison also notes that “Duke students who live off campus in non-Duke housing can schedule a TROSA pickup for large or bulky items and large donations.”</p><p>I emailed six universities, asking about their donation programs and collection data. Most didn’t respond or declined. One directed me to a public web page. Rice University, whose “Give a Hoot! Donate Your Loot!” campaign recently won a statewide award in Texas, sent a detailed response. They reported that they collected around 11,000 pounds of “durable goods” from students this year. (Rice has around 9,000 total students, with roughly half undergrads and half graduate students.)</p><p>Rice’s approach is to implement collections every semester, not just during spring move-out. “By maintaining a consistent presence throughout the academic year,” a spokesperson wrote, “the campaign has become a familiar part of the student experience,” helping students plan ahead to donate rather than discard.</p><p>Looking at the data, Duke’s per-undergraduate donation rate (about 4.9 pounds) is comparable to that at other wealthy private universities like Princeton (7.6 pounds) and Georgetown (6.1 pounds). Duke actually outperforms some schools with similar student demographics like the University of Chicago (0.8 pounds) and Northwestern (0.9 pounds). Most large public universities hover around one pound per student.</p><figure><img decoding="async" width="1024" height="683" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1024x683.jpg" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1024x683.jpg 1024w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-300x200.jpg 300w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-768x512.jpg 768w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1536x1024.jpg 1536w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-2048x1365.jpg 2048w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1200x800.jpg 1200w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-2000x1333.jpg 2000w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-780x520.jpg 780w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-400x267.jpg 400w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-706x471.jpg 706w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>INDY </em>staff writer Lena Geller poses for a picture while wearing Valentino Garavani tennis shoes on Tuesday, May 20, 2025, in Durham. Photo by Angelica Edwards. </figcaption></figure><p><strong>T</strong>he emotional reality of my salvaging week was harder to organize into neat columns. For one, I started feeling like everything I own is shitty. When you’re pulling something out of the trash, it doesn’t feel like it’s going to be a luxury item, so at first, I didn’t think much of a comforter I salvaged and offered it to my boyfriend, who’s always looking for blankets for his dog to lie on. After looking up the cost ($222) and thread count (600), I went back on that offer and replaced my existing comforter with the salvaged one. (The next day, my boyfriend found his own down comforter in the trash.)</p><p>Most items I salvaged were like new, but some needed attention. It felt good to wash, clean, and mend things—removing stains from a blouse, fixing belt loops on black slacks. But then futility would set in. I tried to get the stains out of a pair of muddy Nike high-tops with floral embroidery, using a Solo cup I salvaged as a mixing receptacle to stir together baking soda and hydrogen peroxide into a thick paste, but even after slathering it onto the shoes, the stains persist.&nbsp;</p><p>I also spent some time scrubbing a toaster oven, only to go back to the trash room a few days later and find one that’s cleaner and fancier. Retail value: $400.</p><p>In what would become my final scavenging trip of the year, I tried carrying too many things at once—a handheld vacuum, an air filter, some velvet hangers—and dropped the toaster oven, which splashed water all over me from its steam reservoir.&nbsp;</p><p>Sometimes it’s a spill that does it. I stood there, damp, surrounded by other people’s discards, feeling ridiculous. My apartment was already filled with rescued items. I went home, found that the air filter didn’t fit my unit, and cried.</p><p>The next night, my cat jumped down from the salvaged chair he loves, used his litter box, and then kicked litter everywhere—as per usual. Managing litter has been an ongoing struggle. Various vacuums have proved too weak or too bulky to reach the corners behind the box, so I usually just sweep with a handheld broom and dustpan.</p><p>But as I bent over with my dustpan that night, I remembered the handheld vacuum I’d salvaged just before dropping the toaster oven. I’d found it with its charging cord sitting right next to it, still coiled neatly with a twist tie.</p><p>I grabbed it from my pile of findings and turned it on. It was the most powerful little vacuum I’ve ever seen, its pointed nose perfect for crevices.</p><p><em>Reach Staff Writer Lena Geller at&nbsp;<a href="mailto:lgeller@indyweek.com"><em>lgeller@indyweek.com.</em></a>&nbsp;Comment on this story at&nbsp;<a href="mailto:backtalk@indyweek.com"><em>backtalk@indyweek.com</em></a>.</em><br></p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
</article><!-- #post-${ID} -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Relace (YC W23) – Models for fast and reliable codegen (101 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44108206</link>
            <guid>44108206</guid>
            <pubDate>Tue, 27 May 2025 15:59:20 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44108206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="44108206">
      <td><span></span></td>      <td><center><a id="up_44108206" href="https://news.ycombinator.com/vote?id=44108206&amp;how=up&amp;goto=item%3Fid%3D44108206"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44108206">Launch HN: Relace (YC W23) – Models for fast and reliable codegen</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_44108206">101 points</span> by <a href="https://news.ycombinator.com/user?id=eborgnia">eborgnia</a> <span title="2025-05-27T15:59:20 1748361560"><a href="https://news.ycombinator.com/item?id=44108206">20 hours ago</a></span> <span id="unv_44108206"></span> | <a href="https://news.ycombinator.com/hide?id=44108206&amp;goto=item%3Fid%3D44108206">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Relace%20%28YC%20W23%29%20%E2%80%93%20Models%20for%20fast%20and%20reliable%20codegen&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44108206&amp;auth=222663f39655b321e1eb94a0ff64f886cb3b9237">favorite</a> | <a href="https://news.ycombinator.com/item?id=44108206">47&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN community! We're Preston and Eitan, and we're building Relace (<a href="https://relace.ai/">https://relace.ai</a>). We're trying to make building code agents easy and cheap.</p><p>Here’s an example of our apply model vs. whole file edits:
<a href="https://youtu.be/J0-oYyozUZw" rel="nofollow">https://youtu.be/J0-oYyozUZw</a></p><p>Building reliable code agents is hard. Beyond simple prototypes, any app with code generation in production quickly runs into two problems --  how do you reliably apply diffs, and how do you manage codebase context?</p><p>We're focused on solving these two problems at order-of-magnitude lower price and latency.</p><p>Our first model that we released, in February, is the Fast Apply model -- it merges code snippets with files at 4300 tok/s. It is more reliable (in terms of merge errors) than Sonnet, Qwen, Llama, or any other model at this task. Each file takes ~900ms and gives an instantaneous user experience, as well as saving ~40% on Claude 4 output tokens.</p><p>Our second model focuses on retrieval. For both vibe-coded and enterprise codebases, retrieving only the files relevant to a user request saves both on SoTA input token cost and reduces the number of times code agents need to view files. Our reranker (evals below) can scan a million-line codebase in ~1-2s, and our embedding model outperforms any other embedding model for retrieval as evaluated on a corpus of Typescript/React repositories.</p><p>There are many different ways to build coding agents, but being able to edit code reliably and retrieve the most relevant parts of the codebase is going to be a foundational issue. We're excited to be building ways to make it more accessible to millions of users who don't want to spend $$$ on Claude.</p><p>These models are used in production, millions of times per week. If you've used Lovable, Create.xyz, Magic Patterns, Codebuff, Tempo Labs then you've used us!</p><p>Here's a link to try it out: <a href="https://app.relace.ai/">https://app.relace.ai</a>, and here are our docs: <a href="https://docs.relace.ai/">https://docs.relace.ai</a>.</p><p>We've opened up free access for prototyping on our website to everyone, and the limits should be enough for personal coding use and building small projects (correct us if it’s not). We integrate directly with Open-Source IDE's like Continue.dev. Please try us out, we'd love to hear your feedback!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Square Theory (586 pts)]]></title>
            <link>https://aaronson.org/blog/square-theory</link>
            <guid>44107942</guid>
            <pubDate>Tue, 27 May 2025 15:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aaronson.org/blog/square-theory">https://aaronson.org/blog/square-theory</a>, See on <a href="https://news.ycombinator.com/item?id=44107942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The story starts in <a href="https://discord.com/invite/GPyU97XBGX">Crosscord</a>, the crossword Discord server. Over 5,000 users strong, the server has emerged as a central hub for the online crossword community, a buzzing, sometimes overwhelming, sometimes delightful town square where total noobs, veteran constructors, and champion solvers alike come together to talk about words that cross each other.</p>

<h2 id="square-roots">Square roots</h2>

<p>We direct our attention toward the #etuiposting channel, Crosscord’s designated space for shitposting (so named because ETUI, a sewing case, is a prototypically shitty piece of crosswordese). There, one afternoon in January 2022, crossword constructor and <a href="https://crosswordnexus.com/">Crossword Nexus</a> warden Alex Boisvert posted what seemed at the time to be an innocuous, mildly interesting observation:</p>

<p><img src="https://aaronson.org/assets/images/square-boisvert.png" alt="Alex Boisvert: JET BLACK and JETBLUE have very different meanings, even though they look superficially similar.  Same thing with CATNAP and DOGNAP.  Any other examples of this?"></p>

<p>Suffice to say, the Crosscord hivemind had other examples of this. <a href="http://blog.bewilderinglypuzzles.com/">Will Nediger</a> replied a few minutes later with the clever MULTITOOL and MULTIPLIERS (words with completely unrelated meanings, despite the fact that PLIERS are a TOOL). Several messages later, Alex chimed back in with the elegant PUB QUIZ and BAR EXAM, a pairing that had been used in some form in crosswords by constructors <a href="http://arctanxwords.blogspot.com/2018/04/puzzle-53-i-thought-this-was-speed.html">Christopher Adams</a> (2018) and <a href="https://www.nytimes.com/crosswords/game/daily/2021/01/29">Robyn Weintraub</a> (2021).</p>

<p>Something about this concept—two sets of synonyms (PUB and BAR, QUIZ and EXAM), which when paired together, form phrases that themselves are not synonyms (PUB QUIZ and BAR EXAM)—captured the minds of Crosscord. Suddenly, the floodgates were open.</p>

<p><img src="https://aaronson.org/assets/images/square-discord-posts.png" alt="Will Nediger: UBEREATS / SUPERFOOD; Assorted-Interests: THROW SHADE / PITCH BLACK; Tyler Hinman, Aged Prodigy: With this topic resurrected, it seems nobody posted what I think is the canonical one: BOOTY CALL and BUTT DIAL; jenna lafleur: ROMAN MARS / CLASSICAL RUINS; gppasco: GRAND CANYON / K-HOLE; robinyu: DAD BOD and FATHER FIGURE; kareila: PERMANENT PRESS / FOREVER STAMP; heywhatsupitsbob: FRIENDLY FAVOR / PLATONIC SOLID"></p>

<p>Intermittently over the next <em>year</em>, #etuiposting would be flooded with these pairs of pairs. They became too much even for the shitposting channel, and were ultimately confined to a thread called #double-doubles (a name <a href="https://heywhatsupitsbob.com/">Bob Weisz</a> and I both proposed simultaneously). Today, more than three years after Alex’s original prompt, the thread still remains active, a wordplay oasis of over 3,000 posts.</p>

<p>There’s something going on here. Something more than a shitpost or an ephemeral trend. Double doubles have the proverbial juice, and the juice lies in their structure. Each pair of pairs can be modeled as a square, where the corners are words and the sides are relations between those words:</p>

<p><img src="https://aaronson.org/assets/images/square-booty-call.jpeg" alt="Square showing BOOTY - phrase - CALL connected via synonyms to BUTT - phrase - DIAL"></p>

<p>It’s this square structure that makes each double double feel tight, feel satisfying, feel like a real “find”. This is the essence of what I’ve started calling <strong>square theory</strong>, and it applies to much more than just posts in a Discord server.</p>

<p>Just like it’s satisfying when an essay or a news story comes full circle, or mindblowing when you find an unexpected cycle in your network of friends, it’s inherently compelling when things wrap around and complete the square. Let’s break it down.</p>

<h2 id="the-theory-of-everything">The theory of everything</h2>

<p>Crosscord wasn’t the first to catch onto this kind of formation: Ricki Heicklen has maintained a <a href="https://rickiheicklen.com/unparalleled-misalignments.html">huge list</a> of double doubles (which she calls “Unparalleled Misalignments”—itself a sort of double double) since 2018, and the success of her recent <a href="https://x.com/tradegal_/status/1920189768261828748">Twitter thread</a> about them is another testament to their widespread appeal. Pairs of the same form pop up on a regular basis in the form of crossword clues and Twitter jokes:</p>

<p><img src="https://aaronson.org/assets/images/square-top-gun.png" alt="Crossword clue [Top gun?] for TSHIRTCANNON, with a square showing TOP - phrase - GUN connected via synonyms to TSHIRT - phrase CANNON">
    <img src="https://aaronson.org/assets/images/square-dad-bod.png" alt="Tweet by Steven W Skinner that says 'Why is it called a dad-bod and not a father-figure', with a square showing DAD - phrase - BOD connected via synonyms to FATHER - phrase - FIGURE">
</p>

<p>However, there’s nothing about the square structure that dictates the edges must represent phrases and synonyms. Each edge of the square can be any relation that connects its vertices (but generally, the stronger the relations, the stronger the square). The vertices don’t even necessarily have to be words—they can be any entity or concept.</p>

<p>It evokes the mathematical field of <a href="https://en.wikipedia.org/wiki/Category_theory">category theory</a>, which very abstractly studies mathematical objects and the relations between them. It’s the topology of the square that makes it satisfying, regardless of what the edges and vertices represent.</p>

<p>Members of the #double-doubles thread have already noticed this, consciously or not, with many of the posts interpreting the original prompt more liberally and swapping out the “synonym” relation for something else:</p>

<p><img src="https://aaronson.org/assets/images/square-left-on-read.png" alt="Crosscord post from Joah: LEFT ON READ vs. RIGHT ON RED. Same number of letters too. Maybe I'll make a mini out of it; Square showing LEFT on READ connected via antonym and homophone to RIGHT on RED">
    <img src="https://aaronson.org/assets/images/square-arizona-wildcat.png" alt="Crosscord post from Quiara, Newsletter Economist: ARIZONA WILDCAT / ARIGATO; Square showing ARIZONA phrase WILDCAT connected via abbr. and translation to ARI word GATO">
</p>

<p>Sometimes it feels like the #double-doubles thread has devolved into just #question-mark-clues (crossword clues that are trying to trick you, requiring you to reinterpret them beyond their words’ most likely meaning, or “surface sense”). But that’s no coincidence—abstractly, every question mark clue takes the form of a square.</p>

<p>When brainstorming for question mark clues, crossword constructors experience this on a regular basis. You start with the answer at hand, playing word association with it in search of a combination of words that usually means one thing (the surface sense) but can be interpreted differently (the intended interpretation) to point to the answer, thus completing the square:</p>

<p><img src="https://aaronson.org/assets/images/square-question-mark-clue.png" alt="Square showing word(s) connected to word(s) by surface sense, which are connected by homonyms to word(s) connected to word(s) by intended interpretation, which leads to the answer"></p>

<p>Take <a href="https://www.nytimes.com/2001/04/08/magazine/endpaper-how-to-solve-the-new-york-times-crossword-puzzle.html">Will Shortz’s all-time favorite clue</a> for instance, from a 1995 Martin Ashwood-Smith puzzle: [It turns into a different story] (which deviously didn’t even include the question mark). On the surface, “turns into a different story” typically means something like “develops into another situation.” But the intended interpretation takes the clue’s words to mean “rotates into another floor,” leading to the correct answer of SPIRAL STAIRCASE:</p>

<p><img src="https://aaronson.org/assets/images/square-spiral-staircase.png" alt="Square showing turns (develops) connected to story (situation) by &quot;develops into another situation&quot;, which are connected by homonyms to turns (rotates) connected to story (floor) by &quot;rotates into another floor&quot;, which leads to the answer SPIRAL STAIRCASE"></p>

<p>You might be familiar with this same sort of brainstorming if you’ve ever tried to come up with a clever title for a research paper, or an apt name for a company. There are plenty of names that might make you go “I guess that could work,” but it’s the square-completing ones that make you go “that’s the one,” or “that’s so good!”</p>

<p>One of my favorite examples of this is <a href="https://www.underconsideration.com/brandnew/">Brand New</a>, the blog that catalogues the latest in corporate rebrands. Leave it to a branding blog to have a name this immaculate:</p>

<p><img src="https://aaronson.org/assets/images/square-brand-new.png" alt="Square showing BRAND phrase NEW connected via homonym and synonym to what the blog chronicles, updated brands"></p>

<p>Even a seemingly straightforward brand name like <a href="https://www.grubhub.com/">Grubhub</a> can exemplify the power of square theory. Presumably, Grubhub’s branding team started with a concept (a centralized app for food deliveries) and came up with a name that completes the square. But remove any edge of the square (besides the edge that dictates the app’s purpose), and you’re left with a name that only <em>kinda</em> works:</p>

<p><img src="https://aaronson.org/assets/images/square-grubhub.png" alt="Complete square showing GRUB rhyme HUB connected via synonyms to what the app provides, a central place for food">
    <img src="https://aaronson.org/assets/images/square-grubnexus.png" alt="Incomplete square showing GRUB and NEXUS connected via synonyms to what the app provides, a central place for food">
</p>
<p><img src="https://aaronson.org/assets/images/square-grubcub.png" alt="Incomplete square showing GRUB rhyme CUB connected via only one synonym to what the app provides, a central place for food">
    <img src="https://aaronson.org/assets/images/square-tubhub.png" alt="Incomplete square showing TUB rhyme HUB connected via only one synonym to what the app provides, a central place for food">
</p>

<p>Aside from crossword clues and brand names, squares appear in the wild all the time in the form of jokes. There’s a vast universe of pun-based jokes (often in the form of dad jokes, or Twitter jokes, or <a href="https://www.timeout.com/newyork/clubs/punderdome">Punderdome</a> puns) that can be modeled as a square, where one side of the square is the contrived setup (“what do you call an X with a Y?”) that connects in at least two ways to the punchline (“an algebra problem!”) on the opposite side.</p>

<p>The strength of the joke rests in the strength of the setup, the punchline, and the connections between them—and if every side of the square is strong, you might have created something funny:</p>

<p><img src="https://aaronson.org/assets/images/square-joke-abstract.png" alt="Square showing a setup (contrived) of at least two words, which are connected by synonyms or homonyms, usually, to at least two other words, the punchline (a real word or phrase, or a play on one)">
    <img src="https://aaronson.org/assets/images/square-impasta.png" alt="Square showing FAKE and NOODLE connected by the setup 'What do you call a fake noodle', which connects via synonyms to IMPOSTOR and PASTA, forming the portmanteau punchline 'An impasta!'">
</p>

<h2 id="getting-into-shape">Getting into shape</h2>

<p>You might be thinking: what’s so special about a square? What about triangle theory, or pentagon theory? (Or rectangle theory? Or rhombus theory? Okay, side lengths and angles <a href="https://en.wikipedia.org/wiki/Topology">don’t matter here</a>.)</p>

<p>Well, it’s true that there’s something compelling about any loop-closing property, regardless of side count—a story that comes full circle is still satisfying no matter how many points it hits in between, and it’s still neat to discover a triangle of people who coincidentally know each other.</p>

<p>But here’s what I think makes squares special: a square is the simplest polygon that has non-adjacent sides. In a triangle, each side is adjacent to the other two sides. But in a square, opposite sides have no points in common, which makes any connection between them feel surprising, like a coincidence. In pentagons and beyond, this still holds, but the extra sides add complexity that make them feel slightly less elegant. Nevertheless, other shapes can be interesting too, but I see them as the exception, not the rule.</p>

<p>Remember Alex Boisvert’s original JET BLACK / JETBLUE example? Seems like it could be modeled as a triangle, right? Well, it turns out the “jet” in JET BLACK refers to the gemstone <a href="https://en.wikipedia.org/wiki/Jet_(gemstone)">jet</a>, which is <a href="https://www.etymonline.com/word/jet">etymologically unrelated</a> to JETBLUE’s airplane jet, so it’s actually more properly modeled as a square:</p>

<p><img src="https://aaronson.org/assets/images/square-jet-triangle.png" alt="Triangle showing JET phrase BLACK colors BLUE airline JET">
    <img src="https://aaronson.org/assets/images/square-jet-square.png" alt="Square showing JET homonym JET phrase BLACK colors BLUE airline JET">
</p>

<h2 id="times-square"><em>Times</em> square</h2>

<p>Now that I’ve established that square theory applies to more than just crosswords, it’s time to talk about crosswords again.</p>

<p>It’s typical for American-style crosswords (à la <em>New York Times</em>) to have a theme, which will generally consist of the 4–6 longest Across entries in the grid, often including a “revealer” that ties the theme together. Nowadays, it’s common gospel among crossword constructors that themes should have some sort of wordplay-based connection—that is, a theme like “famous basketball players” or “brands of cereal” is unlikely to elicit a real “aha” moment from solvers, and thus unlikely to be accepted at major crossword outlets.</p>

<p>So what makes for a <em>good</em> crossword theme? Consistency is definitely key, and a notion of “tightness” is important too (the set of possible theme entries shouldn’t be too much bigger than the theme set that appears in the puzzle). But time and time again, I’ve noticed that what makes a theme really pop is—you guessed it—when it completes the square.</p>

<p>Take, for example, the <a href="https://www.xwordinfo.com/Crossword?date=2/17/2025">Monday, February 17, 2025 <em>New York Times</em> crossword</a> by Kate Hawkins and Erica Hsiung Wojcik, which features a great execution of a typical Monday theme type. In this puzzle, the seemingly unrelated theme entries SCRAPBOOK, POPEYES, UNDER PRESSURE, and GIDDY UP are united by the fact that they each end in an affirmative (OK, YES, SURE, YUP).</p>

<p>In a vacuum, this fact wouldn’t be that interesting, but Kate and Erica give the theme a <em>raison d’etre</em> with the revealer YEAH RIGHT, clued as [“Uh-huh, I bet” … or a literal description of what 17-, 24-, 36- and 50-Across all have]—that is, each themer has a synonym for “YEAH” on its “RIGHT” side. The key here is that YEAH RIGHT itself is an idiomatic phrase (meaning “Uh-huh, I bet”), and not just an arbitrary description of the theme mechanic, so it completes the square:</p>

<p><img src="https://aaronson.org/assets/images/square-yeah.png" alt="Square showing what the entries have, an affirmative ending, connected via synonyms to the phrase YEAH RIGHT"></p>

<p>But it doesn’t stop there. Consider the <a href="https://www.xwordinfo.com/Crossword?date=2/18/2019">Monday, February 18, 2019 <em>New York Times</em> crossword</a> by Leslie Young and Andrea Carla Michaels. The theme entries here are NIGHT NIGHT, WHITE WEDDING, and MUSHROOM BALL (you know, like a vegetarian meatball), and the revealer, clued as [Graduation garb … or what the compound answers to 17-, 28- and 44-Across represent?], is CAP AND GOWN. That is, the first part of each themer can precede CAP (e.g. MUSHROOM CAP), and the second part can precede GOWN (e.g. BALL GOWN). This maps pretty squarely onto not one, but three squares, one for each theme entry:</p>

<p><img src="https://aaronson.org/assets/images/square-night-night.png" alt="Three squares, for NIGHT NIGHT, WHITE WEDDING, and MUSHROOM BALL, each showing the phrase connected by two phrases to CAP and GOWN"></p>

<p>And just for fun, we can conjoin the three squares by their CAP AND GOWN edges to form a unified graph that represents the entire theme’s topology:</p>

<p><img src="https://aaronson.org/assets/images/square-cap-and-gown.png" alt="Unified CAP AND GOWN square graph"></p>

<p>The final crossword we’ll look at, and maybe my favorite crossword of all time, is Alina Abidi’s <a href="https://www.xwordinfo.com/Crossword?date=8/18/2021">Wednesday, August 18, 2021 <em>New York Times</em> crossword</a>, with a theme that feels almost impossibly tight.</p>

<p>The puzzle has essentially two theme entries, PIN THE TAIL ON THE DONKEY and WHITE ELEPHANT, with the apt revealer PARTY ANIMAL [Frequent reveler, or a hint to 16-/26- and 36-Across]. That alone is clever, since both themers are party games with animals in their names. But then Alina hits you with the <em>second</em> revealer of THOMAS NAST [Cartoonist suggested by this puzzle’s theme], pointing to the fact that not only are the DONKEY and ELEPHANT animals in party games, but they are also the animals that symbolize the Democratic and Republican <em>parties</em>, as popularized by <a href="https://en.wikipedia.org/wiki/Thomas_Nast">Thomas Nast</a>’s political cartoons.</p>

<p>This is the kind of theme that really sticks with you. Or at least it stuck with me, and I tried for years to understand why it felt so amazing. And then I realized square theory offered an explanation. Squares, as we know, feel tight, satisfying, and clever. But Alina’s theme takes that one step further, creating for each theme entry a square with an <em>extra diagonal</em> through it, reflecting the connection between each animal and a political PARTY:</p>

<p><img src="https://aaronson.org/assets/images/square-democrat.png" alt="Square showing PIN THE TAIL ON THE DONKEY containing DONKEY connected to PARTY ANIMAL by setting and example, with an additional Democrats diagonal connecting PARTY and DONKEY">
    <img src="https://aaronson.org/assets/images/square-republican.png" alt="Square showing WHITE ELEPHANT containing ELEPHANT connected to PARTY ANIMAL by setting and example, with an additional Republican diagonal connecting PARTY and ELEPHANT">
</p>

<p>And again, we can combine these two super-squares into one unified theme graph:</p>

<p><img src="https://aaronson.org/assets/images/square-party-animals.png" alt="Unified PARTY ANIMALS square graph"></p>

<p>Granted, there’s more to a crossword than the structure of its theme, and it can be reductive to distill it into a graph like this. Still, for many puzzles, square theory can serve as an illuminating proxy for the intricacy and tightness of a theme. But that’s not all it can do.</p>

<h2 id="letter-box">Letter box</h2>

<p>Let’s talk about Scrabble, one of the <a href="https://www.nytimes.com/2022/01/25/books/review/seven-games-oliver-roeder.html">seven most important games</a> out there. If you’ve ever played Scrabble (or similar games like Bananagrams), you’d know that every word you play has to intersect another word that’s already on the board.</p>

<p><img src="https://aaronson.org/assets/images/square-scrabble-normal.png" alt="Scrabble play that is boring and the word only intersects one other word"></p>

<p>But occasionally, you’ll think up a play that validly intersects not one, but two words on the board, forming a rectangle of words. Plays like this have a certain panache. They’re satisfying, they make you think, “ooh, nice.” And of course, they can be modeled with square theory:</p>

<p><img src="https://aaronson.org/assets/images/square-scrabble-cool.png" alt="Scrabble play where the word MICE intersects two already-on-the-board words CHASM and SINCE">
    <img src="https://aaronson.org/assets/images/square-scrabble.png" alt="Square showing the Scrabble board words CHASM linking A and M, AVID linking A and I, SINCE linking I and C, and MICE linking M and C">
</p>

<p>You might be thinking that the edge relation here (a word that contains both letters) feels a little flimsy, since not every letter in the word is used. But what if every letter in the word <em>was</em> used? What if we could have a dense network of interlocking squares, where every letter was part of exactly two words? Well, we can, and it’s called an American-style crossword.</p>

<p>In American-style crosswords, every letter is mandatorily “checked” (part of an Across and a Down word), which means <em>every</em> letter is a vertex of a square:</p>

<p><img src="https://aaronson.org/assets/images/square-crossword-grid.png" alt="3x3 crossword grid, and a grid of interconnected squares whose vertices are the letters in the crossword and whose edges are the words that connect those letters"></p>

<p>If you’ve ever tried to construct a crossword, you’ll find that the framing of a crossword grid under square theory <em>feels</em> right. When you’re nearing the end of the grid-filling process, finding valid crossings of words to fill that final corner of a grid, there’s a satisfying “clicking” feeling—a sense of magic—when it all fits together, analogous to the wrapping-around feeling of completing the square.</p>

<p>Taking a step back, that means the clues, the themes, and the very grids of crosswords all share the same abstract fundamental structure, the square:</p>

<p><img src="https://aaronson.org/assets/images/square-crosswords-everything.png" alt="Squares from earlier in the post representing clues, themes, and grids of crosswords"></p>

<p>If you accept the premise that squares are satisfying, square theory offers a unified theory for why crosswords are satisfying too. And if squares are fundamentally compelling, the crossword, in its recursively square structure, starts to look like an equally fundamental art form. Like if you started an English-speaking civilization from scratch, someone, somewhere would inevitably reinvent the crossword. And then someone would start a crossword Discord server, and maybe they’d call it Crosscord.</p>

<p><img src="https://aaronson.org/assets/images/square-crosscord.png" alt="Square showing what the server is, a Discord server for crossword puzzles, connected by keywords to CROSSWORD / DISCORD which are portmanteaued into the server's name, CROSSCORD"></p>

<h2 id="its-hip-to-be-square">It’s hip to be square</h2>

<p>If you’ve read this far, I promise you’ll start to notice squares popping up all over the place in your daily life. I can attest, because I’ve been honing the concept for this post for about two years now, and I often find myself thinking “that’s a square!” whenever I come across a tight joke or title or crossword theme.</p>

<p>If you’re a creative person, square theory is a useful framework to keep in mind. If you’re coming up with a title for a paper or a brand name, try to see if you can think of one that completes the square. If you’re writing puns for a popsicle stick or a Laffy Taffy wrapper, you can use squares to model your setups and punchlines. If you’re constructing a crossword, consider whether your theme or your question mark clues can form squares.</p>

<p>And if you’re writing a story or a news article or a blog post, there’s fundamental value in making it come full circle, or perhaps full square.</p>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pyrefly vs. Ty: Comparing Python's Two New Rust-Based Type Checkers (338 pts)]]></title>
            <link>https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/</link>
            <guid>44107655</guid>
            <pubDate>Tue, 27 May 2025 15:01:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/">https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/</a>, See on <a href="https://news.ycombinator.com/item?id=44107655">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
      <blockquote>
<p>Note: I like using em-dashes while writing! Don’t worry, this is not written by AI. <sup><a href="https://medium.com/@brentcsutoras/the-em-dash-dilemma-how-a-punctuation-mark-became-ais-stubborn-signature-684fbcc9f559">(context)</a></sup></p></blockquote>
<p>Earlier this month, two new Rust-based Python type checkers hit the spotlight: <a href="https://github.com/facebook/pyrefly">pyrefly</a> and <a href="https://github.com/astral-sh/ty">ty</a>. Although neither is <em>officially</em> released, they are a welcome change to the Python type checking world, historically dominated by <a href="https://mypy-lang.org/">mypy</a> and <a href="https://pypi.org/project/pylance/">pylance</a>.</p>
<p>While both are open-source and publicly downloadable for quite some time, there have not been any official announcements by Meta nor Astral on their brand new next-generation Python type checkers — <strong>until last week</strong>.</p>
<p>At <a href="https://us.pycon.org/2025/">PyCon 2025</a>, nestled away in a quiet Room 319 at the <a href="https://us.pycon.org/2025/events/typing-summit/">Typing Summit</a>, we had our first official sneak peek into both of these tools — the team behind them, their goals, visions, and ambitions — and their unique approaches to tackling Python’s typing problems.</p>
<figure>
  <img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-introduction.png" alt="ty introduction presentation at PyCon 2025">
  <figcaption>ty team presenting at the typing summit</figcaption>
</figure>
<blockquote>
<p>This blog is a collection of rough notes scribbled during the event, personal conversations with the team, and not-too-thorough experiments that I’ve run myself. As such, some details might be a little blurry.</p></blockquote>
<blockquote>
<p><strong>Also, both of these tools are still in early alpha!</strong></p>
<p>Please do not use this as a definitive judgment as to which one is better and/or worse. This blog is just for fun to see what state the two tools are at now!</p></blockquote>
<blockquote>
<p>The following tests and experiments were performed on the latest versions of pyrefly, ty, mypy, and pyright as of writing this blog:</p>
<ul>
<li><code>pyrefly 0.17.0</code></li>
<li><code>ty 0.0.1-alpha.7 (afb20f6fe 2025-05-26)</code></li>
<li><code>mypy 1.15.0 (compiled: yes)</code></li>
<li><code>pyright 1.1.401</code></li>
</ul></blockquote>
<h2 id="pyrefly">Pyrefly</h2>
<p>Pyrefly is Meta’s new Rust-based Python type checker, replacing <a href="https://pyre-check.org/">Pyre</a> — Meta’s previous Python type checker written in OCaml. The hopes are that Pyrefly should be faster, more portable, and more capable compared to Pyre.</p>
<p>One key thing the Pyrefly team made very clear this year is that they want to be <em><strong>truly open source</strong></em>. Pyre was also <em>technically</em> open source, but it was more of a “we built this for our needs, but here’s the source code if you want it”. In contrast, one of the foundational goals of Pyrefly is to be more engaged with the needs of the open-source community.</p>
<figure>
  <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ZTSZ1OCUaeQ?si=Rc3-M7a7Yh7SSq-X&amp;start=1405" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>pyrefly introduction presentation</figcaption>
</figure>
<h2 id="ty">ty</h2>
<p>ty is also a Rust-based Python type checker currently under development by <a href="https://astral.sh/">Astral</a>, the team behind <a href="https://docs.astral.sh/uv/">uv</a> and <a href="https://github.com/astral-sh/ruff">ruff</a>. The project was formerly known as Red-Knot, but now has its official name: ty. Compared to Meta, Astral is a lot more quiet on its announcement: just a soft launch on GitHub, a quick 30-minute presentation, and a couple of blog articles as podcasts here and there.</p>
<figure>
  <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/7uixlNTOY4s?si=qMCrwoIekSkoH3xF&amp;start=3558" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>ty introduction presentation</figcaption>
</figure>
<h2 id="similarities">Similarities</h2>
<p>Both pyrefly and ty are written in Rust, both are incremental (albeit implemented slightly differently: see details below), and both are powered under the hood by <a href="https://github.com/astral-sh/ruff">Ruff</a> for AST parsing. Also, both have first-class support for command-line type checking and LSP/IDE integration.</p>
<p>However, other than the fact that they are both fast Python type checkers, that’s where the similarities end. In my opinion, there are four categories in which these two tools differ: <strong>in Speed, Goals, Incrementalization, and Capabilities.</strong> That’s what we’ll explore today.</p>
<h2 id="speed">Speed</h2>
<p>Speed seemed like one of the main focuses of Pyrefly, being mentioned multiple times during the intro presentation. According to the team, it’s 35x faster than Pyre and 14x faster than Mypy/Pyright, with support of up to 1.8 million lines of code per second. Fast enough to “type check on every keystroke”.</p>
<p>In comparison, speed was also one of the main design goals for ty, but it felt like less of a focus during the introduction. The only claim was “1-2 orders of magnitude faster than current generation type checkers”. Naturally, I wanted to test performance out for myself.</p>
<h2 id="benchmarking---pytorch">Benchmarking - PyTorch</h2>
<p>For the first test, I cloned and checked out the latest release of PyTorch (<code>v2.7.0</code>) and compared type check times between pyrefly, ty, mypy, and pyright on a MacBook M4. Two tests were run, one on the entire <code>pytorch</code> repository and another on just the <code>torch</code> subdirectory:</p>
<blockquote>
<p>PyTorch on the latest mypy is not supported. Using <code>mypy 1.14.0</code> instead.</p></blockquote>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-pytorch-benchmarks.svg" alt="pytorch benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'pyrefly check'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'ty check'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'mypy --cache-dir=/dev/null .'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'pyright'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      4.039 s ±  0.234 s    [User: 19.135 s, System: 3.850 s]
  Range (min … max):    3.888 s …  4.455 s    5 runs

pyrefly
  Time (mean ± σ):     13.029 s ±  0.136 s    [User: 60.489 s, System: 6.297 s]
  Range (min … max):   12.916 s … 13.184 s    5 run

mypy
  dnf

pyright
  Time (mean ± σ):     262.742 s ±  4.948 s    [User: 472.717 s, System: 18.898 s]
  Range (min … max):   259.173 s … 270.617 s    5 runs
</code></pre></details>

<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-pytorch-torch-benchmarks.svg" alt="pytorch torch benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyrefly check torch'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'ty check torch'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'mypy --cache-dir=/dev/null torch'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyright torch'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      1.123 s ±  0.022 s    [User: 6.460 s, System: 0.604 s]
  Range (min … max):    1.082 s …  1.167 s    10 runs

pyrefly
  Time (mean ± σ):      2.347 s ±  0.261 s    [User: 15.876 s, System: 0.919 s]
  Range (min … max):    2.089 s …  2.988 s    10 runs
  
mypy
  Time (mean ± σ):     24.731 s ±  0.238 s    [User: 24.144 s, System: 0.519 s]
  Range (min … max):   24.299 s … 25.016 s    10 runs
  
pyright
  Time (mean ± σ):     48.096 s ±  1.705 s    [User: 68.526 s, System: 4.072 s]
  Range (min … max):   46.037 s … 50.488 s    10 runs
</code></pre></details>

<p>Out of the gate, we see that for both <code>pytorch</code> and just <code>torch</code>, ty is about 2-3x faster compared to pyrefly, and both are over 10x-20x faster than mypy and pyright.</p>
<blockquote>
<p>One interesting note is that pyrefly detected more source files than ty: about 8600 for pyrefly and 6500 for ty on <code>pytorch</code> (I’m not sure where the discrepancy comes from).</p></blockquote>
<blockquote>
<p><strong>It’s also important to remember that both pyrefly and ty are still in early alpha, and are not feature complete. This may skew the results!</strong></p></blockquote>
<h2 id="benchmarking---django">Benchmarking - Django</h2>
<p>Next, I ran the same benchmark on Django version 5.2.1.</p>
<blockquote>
<p>Note: mypy errored out during this test.</p></blockquote>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-django-benchmarks.svg" alt="django benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyrefly check'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'ty check'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'mypy --cache-dir=/dev/null .'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyright'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):     578.2 ms ±  27.8 ms    [User: 2980.4 ms, System: 546.9 ms]
  Range (min … max):   557.1 ms … 634.0 ms    10 runs

pyrefly
  Time (mean ± σ):     910.7 ms ±  26.2 ms    [User: 3033.0 ms, System: 565.0 ms]
  Range (min … max):   879.6 ms … 963.1 ms    10 runs
  
mypy
  dnf
  
pyright
  Time (mean ± σ):     16.324 s ±  0.476 s    [User: 24.477 s, System: 1.682 s]
  Range (min … max):   15.845 s … 17.182 s    10 runs
</code></pre></details>

<p>We see the same results across the board with ty being the fastest (2,900 files at 0.6s), pyrefly as a close second (3,200 files at 0.9s), and pyright being the slowest (16s).</p>
<h2 id="benchmarking---mypy">Benchmarking - Mypy</h2>
<p>Finally, I ran the benchmark on the <code>mypy</code> repo itself (more specifically the <code>mypyc</code> subdirectory). Similar results here.</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-mypy-mypyc-benchmarks.svg" alt="mypy mypyc benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'pyrefly check mypyc'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'ty check mypyc'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'mypy --cache-dir=/dev/null mypyc'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'pyright mypyc'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      74.2 ms ±   1.5 ms    [User: 403.4 ms, System: 41.6 ms]
  Range (min … max):    71.9 ms …  78.1 ms    20 runs

pyrefly
  Time (mean ± σ):     136.0 ms ±   1.5 ms    [User: 728.3 ms, System: 54.5 ms]
  Range (min … max):   133.4 ms … 139.6 ms    20 runs
  
mypy
  Time (mean ± σ):      3.544 s ±  0.099 s    [User: 3.442 s, System: 0.093 s]
  Range (min … max):    3.420 s …  3.774 s    20 runs
  
pyright
  Time (mean ± σ):      2.852 s ±  0.103 s    [User: 4.315 s, System: 0.227 s]
  Range (min … max):    2.704 s …  3.105 s    20 runs
</code></pre></details>

<h2 id="goals">Goals</h2>
<p>The primary goals between pyrefly and ty are where I feel the main difference lies. Pyrefly tries to be as aggressive as possible when typing — inferring as much as possible so that even code with absolutely no explicit types can have some amount of typing guarantees.</p>
<p>ty, on the other hand, follows a different mantra: <strong>the gradual guarantee</strong>. The principal idea is that in a well-typed program, removing a type annotation should not cause a type error. In other words: you shouldn’t need to add new types to working code to resolve type errors.</p>
<figure>
  <img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/gradual-guarantee.png" alt="the gradual guarantee slide from ty presentation">
  <figcaption>the gradual guarantee slide from ty presentation</figcaption>
</figure>
<p>This is shown in this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>MyClass</span>:
</span></span><span><span>    attr <span>=</span> <span>None</span>
</span></span><span><span>
</span></span><span><span>foo <span>=</span> MyClass()
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | revealed type: None</span>
</span></span><span><span><span># ✅ ty.     | Revealed type: `Unknown | None`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "None"</span>
</span></span><span><span><span># ➖ pyright | Type of "foo.attr" is "None"</span>
</span></span><span><span>reveal_type(foo<span>.</span>attr)
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | ERROR: Literal[1] is not assignable to attribute attr with type None</span>
</span></span><span><span><span># ✅ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Incompatible types in assignment (expression has type "int", variable has type "None")</span>
</span></span><span><span><span># ➖ pyright | ERROR: Cannot assign to attribute "attr" for class "MyClass"</span>
</span></span><span><span>foo<span>.</span>attr <span>=</span> <span>1</span>
</span></span></code></pre></div><p>In this example, pyrefly, mypy, and pyright eagerly type <code>foo.attr</code> as <code>None</code> and throw an exception when assigned as <code>1</code> — whereas ty understands that <code>foo.attr = 1</code> should not actually cause a syntax error, and instead types <code>foo.attr</code> as <code>Unknown | None</code> to allow the assignment. (<code>Unknown</code> is a new type added by ty to denote between an <em>explicit</em> <code>Any</code> versus an <em>“unknown”</em> <code>Any</code>.)</p>
<p>As a consequence, this also means that pyrefly can catch some errors that other type checkers cannot. Take this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>my_list <span>=</span> [<span>1</span>, <span>"b"</span>, <span>None</span>]
</span></span><span><span>val <span>=</span> my_list<span>.</span>pop(<span>1</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int | str | None</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "builtins.object"</span>
</span></span><span><span><span># ➖ pyright | Type of "val" is "Unknown"</span>
</span></span><span><span>reveal_type(val)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: `*` is not supported between `None` and `Literal[2]`</span>
</span></span><span><span><span># ➖ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Unsupported operand types for * ("object" and "int")</span>
</span></span><span><span><span># ➖ pyright | &lt; No Error &gt;</span>
</span></span><span><span>new_val <span>=</span> val <span>*</span> <span>2</span>
</span></span></code></pre></div><blockquote>
<p>mypy <em>technically</em> did throw an error, but for the wrong reasons. For example, setting <code>my_list = [1, "b"]</code> would fix the program, but mypy still reports a mismatch between <code>object</code> and <code>int</code>.</p></blockquote>
<p>Pyrefly implicitly types <code>val</code> as <code>int | str | None</code>, even though neither <code>val</code> nor <code>my_list</code> was explicitly typed. This correctly catches the <code>val * 2</code> error below.</p>
<p>This is just one of many examples, as more will be shown later in the <strong>Capabilities</strong> section.</p>
<h2 id="incrementalism">Incrementalism</h2>
<p>Both pyrefly and ty claim to be incremental — meaning that changing one file would only cause a re-parse on the affected area, and not the entire program. Pyrefly uses a custom incremental engine behind the scenes for its type checker. In constrast, ty uses <a href="https://github.com/salsa-rs/salsa">Salsa</a>, the same incremental framework that powers <a href="https://rust-analyzer.github.io/">Rust Analyzer</a>.</p>
<p>Interestingly, what that means is that ty has fine-grained incrementalization: changing a single function would only cause a re-parse on that function itself (and nothing else), and its dependent functions, and so on. Pyrefly, on the other hand, uses module-level incrementation: changing a single function would cause a re-parse on the entire file/module, and its dependent files/modules, etc.</p>
<p>The reason why pyrefly chose module-level over fine-grained (at least from what I’ve gathered) is that module-level incrementalization is already fast enough in Rust, and fine-grained incrementalization results in a much more complex and harder to maintain codebase with minimal performance improvements.</p>
<h2 id="capabilities">Capabilities</h2>
<p>Both the pyrefly and ty teams make it VERY CLEAR that they are still unfinished and in early alpha, with known issues, bugs, and incomplete features. Despite that, I think it’s cool to go over what each supports <em>as of now</em> as it showcases what each team has focused on and determined to be important so far for their next-generation Python type checkers.</p>
<h2 id="implicit-type-inference">Implicit Type Inference</h2>
<p>Implicit type inference is one of the showcase features of pyrefly. For example, here is a simple case of inferring return types:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>foo</span>(imp: Any):
</span></span><span><span>    <span>return</span> str(imp)
</span></span><span><span>
</span></span><span><span>a <span>=</span> foo(<span>123</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: str</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "Any"</span>
</span></span><span><span><span># ✅ pyright | Type of "a" is "str"</span>
</span></span><span><span>reveal_type(a)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: `+` is not supported between `str` and `Literal[1]`</span>
</span></span><span><span><span># ➖ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | &lt; No Error &gt;</span>
</span></span><span><span><span># ✅ pyright | ERROR: Operator "+" not supported for types "str" and "Literal[1]"</span>
</span></span><span><span>a <span>+</span> <span>1</span>
</span></span></code></pre></div><p>Here’s another example with inferring types of more complex collection objects (in this case, a <code>dict</code>):</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> typing <span>import</span> reveal_type
</span></span><span><span>
</span></span><span><span>my_dict <span>=</span> {
</span></span><span><span>    key: value <span>*</span> <span>2</span>
</span></span><span><span>    <span>for</span> key, value <span>in</span> {<span>"apple"</span>: <span>2</span>, <span>"banana"</span>: <span>3</span>, <span>"cherry"</span>: <span>1</span>}<span>.</span>items()
</span></span><span><span>    <span>if</span> value <span>&gt;</span> <span>1</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: dict[str, int]</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `@Todo`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.dict[builtins.str, builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "my_dict" is "dict[str, int]"</span>
</span></span><span><span>reveal_type(my_dict)
</span></span></code></pre></div><p><strong>But,</strong> here is where the “gradual guarantee” of ty comes in. Take this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>my_list <span>=</span> [<span>1</span>, <span>2</span>, <span>3</span>]
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: list[int]</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `list[Unknown]`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.list[builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "my_list" is "list[int]"</span>
</span></span><span><span>reveal_type(my_list)
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | ERROR: Argument `Literal['foo']` is not assignable to parameter with type `int` in function `list.append`</span>
</span></span><span><span><span># ✅ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Argument 1 to "append" of "list" has incompatible type "str"; expected "int" </span>
</span></span><span><span><span># ➖ pyright | ERROR: Argument of type "Literal['foo']" cannot be assigned to parameter "object" of type "int" in function "append"</span>
</span></span><span><span>my_list<span>.</span>append(<span>"foo"</span>)
</span></span></code></pre></div><p>pyrefly, mypy, and pyright all assume that <code>my_list.append("foo")</code> is a typing error, even though it is <em>technically</em> allowed (Python collections can have multiple types of objects!) If this is the intended behavior, ty is the only checker that implicitly allows this without requiring additional explicit typing on <code>my_list</code>.</p>
<h2 id="generics">Generics</h2>
<p>Another thing the pyrefly team mentioned during their talk was that while redesigning pyrefly from the ground up, they focused on the “hard problems first”. This means that a lot of the architecture around pyrefly was built around things like generics, overloads, and wildcard imports.</p>
<p>For example, here are some examples where pyrefly and ty both have correct generic resolution:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># === Simple Case ===</span>
</span></span><span><span><span>class</span> <span>Box</span>[T]:
</span></span><span><span>    <span>def</span> <span>__init__</span>(self, val: T) <span>-&gt;</span> <span>None</span>:
</span></span><span><span>        self<span>.</span>val <span>=</span> val
</span></span><span><span>
</span></span><span><span>b: Box[int] <span>=</span> Box(<span>42</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span><span># ✅ ty.     | Revealed type: `Unknown | int`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span><span># ✅ pyright | Type of "b.val" is "int"</span>
</span></span><span><span>reveal_type(b<span>.</span>val)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: Argument `Literal[100]` is not assignable to parameter `val` with type `str` in function `Box.__init__`</span>
</span></span><span><span><span># ✅ ty.     | ERROR: Object of type `Box[int]` is not assignable to `Box[str]`</span>
</span></span><span><span><span># ✅ mypy.   | ERROR: Argument 1 to "Box" has incompatible type "int"; expected "str"</span>
</span></span><span><span><span># ✅ pyright | ERROR: Type "Box[int]" is not assignable to declared type "Box[str]"</span>
</span></span><span><span>b2: Box[str] <span>=</span> Box(<span>100</span>)
</span></span><span><span>
</span></span><span><span><span># === Bounded Types with Attribute ===</span>
</span></span><span><span><span>class</span> <span>A</span>:
</span></span><span><span>    x: int <span>|</span> str
</span></span><span><span>
</span></span><span><span><span>def</span> <span>f</span>[T: A](x: T) <span>-&gt;</span> T:
</span></span><span><span>    <span># ✅ pyrefly | revealed type: int | str</span>
</span></span><span><span>    <span># ✅ ty.     | Revealed type: `int | str`</span>
</span></span><span><span>    <span># ✅ mypy.   | Revealed type is "Union[builtins.int, builtins.str]"</span>
</span></span><span><span>    <span># ✅ pyright | Type of "x.x" is "int | str"</span>
</span></span><span><span>    reveal_type(x<span>.</span>x)
</span></span><span><span>    <span>return</span> x
</span></span></code></pre></div><p>Whereas here are some examples where pyrefly has better generic resolution compared to ty:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> typing <span>import</span> Callable, TypeVar, assert_type, reveal_type
</span></span><span><span>    
</span></span><span><span><span># === Generic Class Without Explicit Type Param ===</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>C</span>[T]:
</span></span><span><span>    x: T
</span></span><span><span>
</span></span><span><span>c: C[int] <span>=</span> C()
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: C[int]</span>
</span></span><span><span><span># ➖ ty.     | `C[Unknown]`</span>
</span></span><span><span><span># ✅ pypy.   | Revealed type is "__main__.C[builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "c" is "C[int]"</span>
</span></span><span><span>reveal_type(c)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ✅ pypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span><span># ✅ pyright | Type of "c.x" is "int"</span>
</span></span><span><span>reveal_type(c<span>.</span>x)
</span></span><span><span>
</span></span><span><span><span># === Bounded Types with Callable Attribute ===</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>func</span>[T: Callable[[int], int]](a: T, b: int) <span>-&gt;</span> T:
</span></span><span><span>    <span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span>    <span># ➖ ty.     | ERROR: &lt;Error: Object of type `T` is not callable&gt;</span>
</span></span><span><span>    <span># ✅ pypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span>    <span># ✅ pyright | Type of "a(b)" is "int"</span>
</span></span><span><span>    reveal_type(a(b))
</span></span><span><span>    <span>return</span> a
</span></span></code></pre></div><p>Interestingly enough, both pyrefly and ty seem to struggle with resolving covariance and contravariance relationships. Example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> __future__ <span>import</span> annotations
</span></span><span><span>
</span></span><span><span><span>class</span> <span>A</span>[X]:
</span></span><span><span>    <span>def</span> <span>f</span>(self) <span>-&gt;</span> B[X]:
</span></span><span><span>        <span>...</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>B</span>[Y]:
</span></span><span><span>    <span>def</span> <span>h</span>(self) <span>-&gt;</span> B[Y]:
</span></span><span><span>        <span>...</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>cast_a</span>(a: A[bool]) <span>-&gt;</span> A[int]:
</span></span><span><span>    <span># ➖ pyrefly | ERROR: Return type does not match returned value: expected `A[int]`, found `A[bool]`</span>
</span></span><span><span>    <span># ➖ ty.     | ERROR: Returned type `A[bool]` is not assignable to declared return type `A[int]`</span>
</span></span><span><span>    <span># ✅ mypy.   | &lt; No Error &gt;</span>
</span></span><span><span>    <span># ✅ pyright | &lt; No Error &gt;</span>
</span></span><span><span>    <span>return</span> a  <span># Allowed</span>
</span></span></code></pre></div><h2 id="informative-error-messages">Informative Error Messages</h2>
<p>One explicit feature of ty is to have clear and concise error messages.</p>
<p>For example, here is a simple example of a function call with mismatched types:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-error-message.png" alt="ty-error-message.png"></p>
<p>Compared to pyrefly, mypy, and pyright:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/pyrefly-error-message.png" alt="pyrefly-error-message.png"></p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/mypy-error-message.png" alt="mypy-error-message.png"></p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/pyright-error-message.png" alt="pyright-error-message.png"></p>
<p>Here is another example with mismatched return types:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-error-message-2.png" alt="ty-error-message-2.png"></p>
<p>In my opinion, much cleaner! It’s exciting to see new and improved error messages coming to Python.</p>
<h2 id="intersection-and-negation-types">Intersection and Negation Types</h2>
<p>Finally, one really cool feature the Astral team showed off was support for intersection and negation types — which they claim is the only Python type checker to implement. To illustrate this, take a look at this example:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>WithX</span>:
</span></span><span><span>  x: int
</span></span><span><span>
</span></span><span><span><span>@final</span>
</span></span><span><span><span>class</span> <span>Other</span>:
</span></span><span><span>  <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>foo</span>(obj: WithX <span>|</span> Other):
</span></span><span><span>  <span>if</span> hasattr(obj, <span>"x"</span>):
</span></span><span><span>    <span># ➖ pyrefly | revealed type: Other | WithX</span>
</span></span><span><span>    <span># ✅ ty.     | Revealed type: `WithX`</span>
</span></span><span><span>    <span># ➖ mypy.   | Revealed type is "Union[__main__.WithX, __main__.Other]"</span>
</span></span><span><span>    <span># ➖ pyright | Type of "obj" is "WithX | Other"</span>
</span></span><span><span>    reveal_type(obj)
</span></span></code></pre></div><blockquote>
<p><code>@final</code> is a new feature in Python 3.12 that prevents a class from being subclassed. This is important for the type checker to know that <code>Other</code> cannot be subclassed with <code>x</code> in the future.</p></blockquote>
<p>Given the constraints that <code>obj</code> is either <code>WithX</code> or final type <code>Other</code>, and <code>obj</code> <em>has</em> to have attribute <code>x</code>, the only resolvable type for <code>obj</code> at <code>reveal_type(obj)</code> is <code>WithX</code>. Breaking down what happens behind the scenes:</p>
<pre tabindex="0"><code>(WithX | Other) &amp; &lt;Protocol with members 'x'&gt;
=&gt; (WithX &amp; &lt;Protocol with members 'x'&gt; | (Other &amp; &lt;Protocol with members 'x'&gt;)
=&gt; WithX | Never
=&gt; WithX
</code></pre><p>Take a look at another example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>MyClass</span>:
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>MySubclass</span>(MyClass):
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>bar</span>(obj: MyClass):
</span></span><span><span>    <span>if</span> <span>not</span> isinstance(obj, MySubclass):
</span></span><span><span>        <span># ➖ pyrefly | revealed type: MyClass</span>
</span></span><span><span>        <span># ✅ ty.     | Revealed type: `MyClass &amp; ~MySubclass`</span>
</span></span><span><span>        <span># ➖ mypy.   | Revealed type is "__main__.MyClass"</span>
</span></span><span><span>        <span># ➖ pyright | Type of "obj" is "MyClass"</span>
</span></span><span><span>        reveal_type(obj)
</span></span></code></pre></div><p>ty is the only type checker to resolve <code>obj</code> at <code>reveal_type(obj)</code> to <code>MyClass &amp; ~MySubclass</code>. This means that ty introduces new paradigms to Python types:</p>
<p><strong>intersections and negations!</strong> Neat!</p>
<p>However, this is still in early alpha! For example, this case here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>bar</span>(obj: HasFoo):
</span></span><span><span>    <span>if</span> <span>not</span> hasattr(obj, <span>"bar"</span>):
</span></span><span><span>        reveal_type(obj)
</span></span><span><span>        reveal_type(obj<span>.</span>foo)
</span></span></code></pre></div><p><code>reveal_type(obj)</code> has the correct type of <code>HasFoo &amp; ~&lt;Protocol with members 'bar'&gt;</code>, but <code>reveal_type(obj.foo)</code> resolves to <code>@Todo</code> even though <code>obj.foo</code> should be resolvable to the function <code>foo</code> given the constraints.</p>
<p>As one final fun party trick, here is ty using intersection and negation types to “solve” <a href="https://en.wikipedia.org/wiki/Diophantine_equation">diophantine equations</a>:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># Simply provide a list of all natural numbers here ...</span>
</span></span><span><span>type Nat <span>=</span> Literal[<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>6</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>10</span>, <span>11</span>, <span>12</span>, <span>13</span>]
</span></span><span><span>
</span></span><span><span><span>def</span> <span>pythagorean_triples</span>(a: Nat, b: Nat, c: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>2</span> <span>+</span> b<span>**</span><span>2</span> <span>==</span> c<span>**</span><span>2</span>)
</span></span><span><span>    <span># reveals 'bool': solutions exist (3² + 4² == 5²)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>fermats_last_theorem</span>(a: Nat, b: Nat, c: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>3</span> <span>+</span> b<span>**</span><span>3</span> <span>==</span> c<span>**</span><span>3</span>)
</span></span><span><span>    <span># reveals 'Literal[False]': no solutions!</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>catalan_conjecture</span>(a: Nat, b: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>2</span> <span>-</span> b<span>**</span><span>3</span> <span>==</span> <span>1</span>)
</span></span><span><span>    <span># reveals 'bool': solutions exist (3² - 2³ == 1)</span>
</span></span></code></pre></div><h2 id="final-thoughts">Final Thoughts</h2>
<p>Overall, it’s exciting to have two new faster type checkers in the Python ecosystem! As of right now, pyrefly and ty seem to follow two different systematic goals. Ty takes a gradual approach to typing - given a program that (theoretically) runs flawlessly, running a type checker should not raise any new typing errors - and if it does, it probably indicates an actual flaw somewhere in the code. Pyrefly takes a different approach, one that is similar to many state-of-the-art Python type checkers today - infer as many types as possible, at the cost of possibly introducing typing errors where it shouldn’t.</p>
<p>As mentioned multiple times, both pyrefly and ty are in early alpha. I strongly suspect the features and capabilities of both tools will converge as time goes on, but nevertheless, it is still cool to see where the two type checkers are at now and how they might come into play in different scenarios sometime in the future.</p>
<p><strong>Go try these out for yourself now!</strong></p>
<p>You can try out pyrefly over at <strong><a href="https://pyrefly.org/sandbox">pyrefly.org/sandbox</a></strong>, and ty over at <strong><a href="https://play.ty.dev/">play.ty.dev</a></strong>. Both also have their respective <code>pip install</code> commands and plugins for your editor (VSCode, Cursor, etc).</p>
<p>In the meantime, I heard rumors that Google is planning on open-sourcing their own Go-based Python type checker, so it’ll be very cool to check that out once it comes out 👀 …</p>
<h2 id="appendix">Appendix</h2>
<p>I just wanted to call out that ty’s tests are written in… <strong>MARKDOWN</strong>! How cool is that?</p>
<blockquote>
<p><strong><a href="https://github.com/astral-sh/ruff/tree/main/crates/ty_python_semantic/resources/mdtest">https://github.com/astral-sh/ruff/tree/main/crates/ty_python_semantic/resources/mdtest</a></strong></p></blockquote>
<hr>
<p><em>Thanks for reading!</em></p>
<p><em>If you notice any mistakes, comments, or feedback, please let me know!</em></p>
<p><em>Contact: <a href="mailto:blog@edward-li.com">blog@edward-li.com</a></em></p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Malai – securely share local TCP services (database/SSH) with others (107 pts)]]></title>
            <link>https://malai.sh/hello-tcp/</link>
            <guid>44107393</guid>
            <pubDate>Tue, 27 May 2025 14:34:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://malai.sh/hello-tcp/">https://malai.sh/hello-tcp/</a>, See on <a href="https://news.ycombinator.com/item?id=44107393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="38"><div data-id="42"><p>Introducing Malai TCP &amp; A Bonus!</p><comment data-id="44"></comment></div><div data-id="45"><p><a href="https://github.com/kulfi-project/kulfi/releases/"><code>malai-0.2.5</code></a> is out now!
It brings a new feature to share your local TCP server with the world!</p><p>
Now you can share any TCP-based service running locally — including your
SSH service, Postgres database, Redis, or even a custom TCP protocol — using the
same seamless workflow that you used with <code>malai http</code>.</p></div><p>Install <code>malai</code> today using:</p><div data-id="47"><comment data-id="48"></comment><pre data-id="57"><code data-id="58">curl -fsSL https://malai.sh/install.sh | sh
</code></pre><comment data-id="59"></comment></div><p>And run:</p><div data-id="67"><comment data-id="68"></comment><pre data-id="77"><code data-id="78">$ malai tcp 5432 --public
Malai: Sharing port 5432
Run malai tcp-bridge &lt;id52&gt; &lt;some-port&gt;
to connect to it from any machine.
</code></pre><comment data-id="79"></comment></div><p>This will share your local TCP server running on port 5432 with the world. You
can connect to it from any machine using the command:</p><div data-id="87"><comment data-id="88"></comment><pre data-id="97"><code data-id="98">$ malai tcp-bridge &lt;id52&gt; 9091
Listening on 127.0.0.1:9091
</code></pre><comment data-id="99"></comment></div><p>Now you can connect to <code>localhost:9091</code> and it'll go through <code>malai</code> and
connect to the exposed service.</p><div data-id="107"><p>Share your SSH server</p><comment data-id="109"></comment><div data-id="110"><p>You can even use <code>malai tcp</code> to expose your local SSH server for remote access — without opening port 22 publicly.</p><p>
First, make sure the OpenSSH server is running:</p></div></div><p>Then, run the following on the machine where the SSH server is running:</p><div data-id="131"><comment data-id="132"></comment><pre data-id="141"><code data-id="142">$ malai tcp 22 --public
Malai: Sharing port 5432
Run malai tcp-bridge &lt;id52&gt; &lt;some-port&gt;
to connect to it from any machine.
</code></pre><comment data-id="143"></comment></div><p>On another machine, use the bridge command:</p><div data-id="151"><comment data-id="152"></comment><pre data-id="161"><code data-id="162">$ malai tcp-bridge &lt;id52&gt; 9090
</code></pre><comment data-id="163"></comment></div><p>Replace <code>&lt;id52&gt;</code> with the ID printed by the <code>malai tcp</code> command. Once the
bridge is running, SSH into your machine like this:</p><div data-id="171"><comment data-id="172"></comment><pre data-id="181"><code data-id="182">ssh -p 9090 user@localhost
</code></pre><comment data-id="183"></comment></div><p>You're connecting to <code>localhost:9090</code>, which is where the <code>tcp-bridge</code> is
listening. It forwards your SSH traffic to the original machine via the Kulfi
network. Make sure to use the correct <code>user</code> that exists on the remote machine.</p><div data-id="191"><p>Use cases</p><comment data-id="193"></comment><div data-id="194"><ul>
<li>Secure your SSH server behind the Kulfi network.</li>
<li>Share a local Postgres or Redis instance with your team.</li>
<li>Demo a multiplayer game server or custom TCP service.</li>
<li>Students can share networked apps or environments with instructors for
real-time help or grading.</li>
</ul></div></div><p>To learn more about <code>malai tcp</code>, check out the <a href="https://malai.sh/tcp/">documentation</a>.</p><div data-id="196"><p>Wait, we have more!</p><comment data-id="198"></comment><p>We've also added a new <code>malai folder</code> command to share a folder with everyone.
This is similar to <code>malai http</code> but it serves your local files and folders.
This is more like a call for testing than launching a new feature. Try it out
and give us feedback!</p></div><div data-id="200"><comment data-id="201"></comment><pre data-id="211"><code data-id="212">$ malai folder ~/projects/fastn/assets/ --public
Serving "/Users/siddhant/projects/fastn/assets" on http://127.0.0.1:59136
Malai: Sharing http://127.0.0.1:59136 at
https://pubqaksutn9im0ncln2bki3i8diekh3sr4vp94o2cg1agjrb8dhg.kulfi.site
To avoid the public proxy, run your own with: malai http-bridge

Or use: malai browse kulfi://pubqaksutn9im0ncln2bki3i8diekh3sr4vp94o2cg1agjrb8dhg
</code></pre><comment data-id="213"></comment></div><p>This spins up a basic HTTP server behind the scenes to serve the provided folder:</p><div data-id="221"><div data-id="222"><p><img data-id="223" src="https://malai.sh/-/malai.sh/assets/malai-folder-browser-view.png"></p><comment data-id="224"></comment><p>Browsing a folder served by <code>malai</code></p></div><comment data-id="226"></comment></div><div data-id="230"><p>We're just getting started, and your support means a lot.</p><p>
If you like what we're building, consider <a href="https://github.com/kulfi-project/kulfi">starring the
repo</a> on GitHub. It helps others
discover the project and keeps us motivated to build more!</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Agents API (146 pts)]]></title>
            <link>https://mistral.ai/news/agents-api</link>
            <guid>44107187</guid>
            <pubDate>Tue, 27 May 2025 14:09:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/agents-api">https://mistral.ai/news/agents-api</a>, See on <a href="https://news.ycombinator.com/item?id=44107187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr"><img src="https://cms.mistral.ai/assets/f2a4b295-ff64-4c16-a42a-14f858c65766.png?width=1080&amp;height=457" alt="Cover"></p>
<p dir="ltr">Today we announce our new Agents API, a major step forward in making AI more capable, useful, and an active problem-solver.</p>
<p dir="ltr">Traditional language models excel at generating text but are limited in their ability to perform actions or maintain context. Our new Agents API addresses these limitations by combining Mistral's powerful language models with:</p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Built-in connectors for code execution, web search, image generation, and MCP tools&nbsp;</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Persistent memory across conversations&nbsp;</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Agentic orchestration capabilities</p>
</li>
</ul>
<p dir="ltr">The Agents API complements our <a href="https://docs.mistral.ai/capabilities/completion/" target="_blank" rel="noopener">Chat Completion API</a> by offering a dedicated framework that simplifies implementing agentic use cases. It serves as the backbone of enterprise-grade agentic platforms.</p>
<p dir="ltr">By providing a reliable framework for AI agents to handle complex tasks, maintain context, and coordinate multiple actions, the Agents API enables enterprises to use AI in more practical and impactful ways.</p>
<h2 dir="ltr">Mistral agents in action.</h2>
<p dir="ltr">Explore the diverse applications of Mistral’s Agents API across various sectors:</p>
<ul>
<li id="demo-github" dir="ltr">
<h3>Coding assistant with Github.</h3>
<p dir="ltr">An agentic workflow built with Mistral's agents API where an agent interacts with Github and oversees a developer agent, powered by DevStral to write code. The agent is granted full authority over Github, showcasing automated software development task management.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/1Tt9Fq1pUPQ?si=j4fIT7TqM1RGsyRG" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/github_agent" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-linear" dir="ltr">
<h3>Linear tickets assistant.</h3>
<p dir="ltr">An intelligent task coordination assistant powered by our Agents API, using multi-server MCP architecture to transform call transcripts to PRDs to actionable Linear issues and track project deliverables.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/4UPP-JEjcKo?si=gMuPof7qCpuHuc2z" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/prd_linear_ticket" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-finance" dir="ltr">
<h3>Financial analyst.</h3>
<p dir="ltr">A financial advisory agent constructed with our Agents API, orchestrating multiple MCP servers to source financial metrics, compile insights, and archive results securely.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/ocxRKz73UJw?si=2xJffa3oIFBViA56" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/financial_analyst" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-travel" dir="ltr">
<h3>Travel assistant.</h3>
<p dir="ltr">A powerful AI travel assistant that helps users plan their trips, book accommodations, and manage travel needs.</p>
<iframe title="YouTube video player" src="https://www.youtube.com/embed/DSYlhtG2UNM?si=ZZH4OSd1u3QzhpwF" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe><a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/travel_assistant" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-nutrition" dir="ltr">
<h3>Nutrition assistant.</h3>
<p dir="ltr">An AI-powered food diet companion designed to help users establish goals, log meals, receive personalized food suggestions, track their daily achievements, and discover dining options that align with their nutritional targets.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/uEG2z2esl14?si=Ca_PY02gfVeWChgJ" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/food_diet_companion" target="_blank" rel="noopener">Read our cookbook</a></li>
</ul>
<h2 dir="ltr">Create an agent with built-in connectors and MCP tools.</h2>
<p dir="ltr">Each agent can be equipped with powerful built-in connectors, which are tools that are deployed and ready for Agents to call on demand, and MCP tools:&nbsp;</p>
<ul>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/code_interpreter/" target="_blank" rel="noopener">Code execution</a></h3>
<p dir="ltr" role="presentation">The Agents API can use the code execution connector, empowering developers to create agents that execute Python code in a secure sandboxed environment. This enables agents to tackle a wide range of tasks, including mathematical calculations and analysis, data visualization and plotting, and scientific computing.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/image_generation/" target="_blank" rel="noopener">Image generation</a></h3>
<p dir="ltr" role="presentation">The image generation connector tool, powered by Black Forest Lab FLUX1.1 [pro] Ultra, enables agents to create images for diverse applications. This feature can be leveraged for various use cases such as generating visual aids for educational content, creating custom graphics for marketing materials, or even producing artistic images.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/document_library/" target="_blank" rel="noopener">Document library</a></h3>
<p dir="ltr" role="presentation">Document Library is a built-in connector tool that enables agents to access documents from Mistral Cloud. It powers the integrated RAG functionality, strengthening agents’ knowledge by leveraging the content of user-uploaded documents.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/websearch/" target="_blank" rel="noopener">Web search</a></h3>
<p dir="ltr" role="presentation">The Agents API offers web search as a connector, enabling developers to combine Mistral models with diverse, up-to-date information from web search, reputable news, and other sources. This integration facilitates the delivery of up-to-date, informed, evidence-supported responses.</p>
<p dir="ltr">Agents with web search capabilities show a significant improvement in performance. In the SimpleQA benchmark, Mistral Large and Mistral Medium with web search achieve scores of 75% and 82.32%, respectively, compared to 23% and 22.08% without web search (see figure below).</p>
<h4>SimpleQA Accuracy (Higher is better)</h4>

</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/mcp/" target="_blank" rel="noopener">MCP tools</a></h3>
<p dir="ltr" role="presentation">The Agents API SDK can also leverage tools built on the Model Context Protocol (MCP)—an open, standardized protocol that enables seamless integration between agents and external systems. MCP tools provide a flexible and extensible interface for agents to access real-world context, including APIs, databases, user data, documents, and other dynamic resources. Check out the <a href="#demo-github" rel="noopener">Github</a>, <a href="#demo-finance" rel="noopener">Financial Analyst</a>, and <a href="#demo-linear" rel="noopener">Linear</a> MCP demos to learn how to use MCP tools with Mistral Agents in action.</p>
<img src="https://cms.mistral.ai/assets/5a0eb67b-819c-4a3f-9cc0-7dba190d58d2.svg?width=null&amp;height=null" alt="Mcp Mistral"></li>
</ul>
<h2 dir="ltr">Memory and context with stateful conversations.</h2>
<p dir="ltr">The Agents API provides robust conversation management through a flexible and stateful conversation system. Each conversation retains its context, allowing for seamless and coherent interactions over time.</p>
<ul>
<li>
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#conversations" target="_blank" rel="noopener">Conversation management</a></h3>
<p dir="ltr">There are two ways to start a conversation:</p>
<ol>
<li dir="ltr">With an Agent: Create a conversation with a specific agent_id to leverage its specialized capabilities.</li>
<li dir="ltr">Direct Access: Start a conversation by directly specifying the model and completion parameters, providing quick access to built-in connectors.</li>
</ol>
<p dir="ltr">Each conversation maintains a structured history through conversation entries, ensuring that the context is preserved across interactions.</p>
</li>
<li>
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#continue-a-conversation-working" target="_blank" rel="noopener">Stateful interactions and conversation branching</a></h3>
<p dir="ltr">Developers are no longer required to monitor conversion history; they have the ability to view past conversations. They can always continue any conversation or initiate new conversation paths from any point.&nbsp;</p>
</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#streaming-output-working" target="_blank" rel="noopener">Streaming output</a></h3>
<p dir="ltr">The API also supports streaming outputs, both when starting a conversation and continuing a previous one. This feature allows for real-time updates and interactions.&nbsp;</p>
</li>
</ul>
<h2 dir="ltr">Agent orchestration.</h2>
<p dir="ltr">The true power of our Agents API lies in its ability to orchestrate multiple agents to solve complex problems. Through dynamic orchestration, agents can be added or removed from a conversation as needed—each one contributing its unique capabilities to tackle different parts of a problem.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/55ca02be-4dfa-4f0e-ba6a-adc7c54dce4c.svg?width=null&amp;height=null" alt="Agents"></p>
<ul>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/handoffs/#create-an-agentic-workflow" target="_blank" rel="noopener">Creating an agentic workflow</a></h3>
<p dir="ltr">To build a workflow with handoffs, start by creating all necessary agents. You can create as many agents as needed, each with specific tools and models, to form a tailored workflow.</p>
</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/https://docs.mistral.ai/agents/handoffs/#create-an-agentic-workflow" target="_blank" rel="noopener">Agent handoffs</a></h3>
<p dir="ltr">Once agents are created, define which agents can hand off tasks to others. For example, a finance agent might delegate tasks to a web search agent or a calculator agent based on the conversation's needs.</p>
<p dir="ltr">Handoffs enable a seamless chain of actions. A single request can trigger tasks across multiple agents, each handling specific parts of the request. This collaborative approach allows for efficient and effective problem-solving, unlocking powerful possibilities for real-world applications.</p>
</li>
</ul>
<h2 dir="ltr">Get started.</h2>
<p dir="ltr">To get started, check out our <a href="https://docs.mistral.ai/agents/agents_introduction" target="_blank" rel="noopener">docs</a>, create your first agent, and start building!&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Cline Doesn't Index Your Codebase (and Why That's a Good Thing) (160 pts)]]></title>
            <link>https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing</link>
            <guid>44106944</guid>
            <pubDate>Tue, 27 May 2025 13:44:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing">https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing</a>, See on <a href="https://news.ycombinator.com/item?id=44106944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div href="/install?utm_source=website&amp;utm_medium=header"><span><svg fill="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M23.15 2.587L18.21.21a1.494 1.494 0 0 0-1.705.29l-9.46 8.63-4.12-3.128a.999.999 0 0 0-1.276.057L.327 7.261A1 1 0 0 0 .326 8.74L3.899 12 .326 15.26a1 1 0 0 0 .001 1.479L1.65 17.94a.999.999 0 0 0 1.276.057l4.12-3.128 9.46 8.63a1.492 1.492 0 0 0 1.704.29l4.942-2.377A1.5 1.5 0 0 0 24 20.06V3.939a1.5 1.5 0 0 0-.85-1.352zm-5.146 14.861L10.826 12l7.178-5.448v10.896z"></path></svg></span><p><span>Install Cline<!-- --> • <!-- -->1.6M<!-- --> <!-- -->installs</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckLake is an integrated data lake and catalog format (244 pts)]]></title>
            <link>https://ducklake.select/</link>
            <guid>44106934</guid>
            <pubDate>Tue, 27 May 2025 13:43:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ducklake.select/">https://ducklake.select/</a>, See on <a href="https://news.ycombinator.com/item?id=44106934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<!-- <div class="searchoverlay">
	<div>
		<form autocomplete="off">
			<div class="autocomplete">
				<div class="empty_input"></div>
				<input id="q" type="text" name="q" placeholder="Search docs & blog">
			</div>
		</form>
		<div id="search_results"></div>
		<div class="shortcuts">
			Search Shortcut <span>cmd</span> + <span>k</span> | <span>ctrl</span> + <span>k</span>
		</div>
	</div>
</div> -->

		

			

<div>
      
    
			<p>DuckLake delivers advanced data&nbsp;lake features without traditional lakehouse complexity by using Parquet files and your SQL database. It's an open, standalone format from the DuckDB team.</p>
			
		</div>



<div>
		<div>
      <h2>
        
        Deployment scenarios
        
      </h2>
    
			<p>DuckLake uses a database system to manage your metadata for the catalog. All you need to run your own data warehouse is a database system and storage for Parquet files.</p>
		</div>
		<div>
					<div>
						<ul>
							
							<li data-tab="arch-tab2" data-iconclass="postgre" data-multi="true">PostgreSQL</li>
							<li data-tab="arch-tab3" data-iconclass="sqlite" data-multi="true">SQLite</li>
							<li data-tab="arch-tab4" data-iconclass="mysql" data-multi="true">MySQL</li>
							<li data-tab="arch-tab1" data-iconclass="duckdb" data-multi="false">DuckDB</li>
						</ul>
						<p>← Choose catalog database</p>
					</div>
					
				
					<div>
						<div>
      <h4>
        
        Client
        
      </h4>
    
							
							
							
							<div>
								<div>
      <h4>
        
        Clients
        
      </h4>
    
									<p>Users can run multiple DuckLake clients and connect concurrently to PostgreSQL, MySQL or SQLite.</p>
								</div>
								<div>
      <h4>
        
        Client
        
      </h4>
    
									<p>DuckDB also works with DuckLake as the catalog database. In this case, you are limited to a single client.</p>
								</div>
							</div>
							
						</div>
						<div>
							<div>
      <h4>
        
        Catalog database
        
      </h4>
    
								
								<div>
									<p><img src="https://ducklake.select/images/deployment_diagram/database.svg" alt="Database Icon"></p>
								</div>
								
								<div>
      <h4>
        
        Catalog database
        
      </h4>
    
										<p>DuckLake can use any SQL system as its catalog database, provided that it supports ACID transactions and primary key constraints.</p>
									</div>
								
								
							</div>
							<div>
      <h4>
        
        Storage
        
      </h4>
    
								
								<div>
									<p><img src="https://ducklake.select/images/deployment_diagram/parquet_folder.svg" alt="Parquet Folder"></p><p>Parquet</p>
								</div>
								
								<div>
      <h4>
        
        Storage
        
      </h4>
    
										<p>DuckLake can store your data on any object storage such as AWS S3.</p>
									</div>
								
								
							</div>
						</div>
					</div>
					
					
				</div>
	</div>


<!--
<section>
	<div class="wrap">
      <h2>
        
        Use cases
        
      </h2>
    
		<div class="cards vertical images">
			<div class="card">
				<div class="image"></div>
				<div class="content">
      <h3>
        
        Multiplayer DuckDB
        
      </h3>
    
					<p>DuckLake unlocks concurrency for multiple DuckDB clients.</p>
					<a href="#" class="textbutton arrow-right">Read more</a>
				</div>
			</div>
			<div class="card">
				<div class="image"></div>
				<div class="content">
      <h3>
        
        Self-hosted data warehouse
        
      </h3>
    
					<p>DuckLake allows you to host your own local data warehouse.</p>
					<a href="#" class="textbutton arrow-right">Read more</a>
				</div>
			</div>
		</div>
	</div>
</section>
-->


<div>
      <h2>
        
        DuckLake’s key features
        
      </h2>
    
		<div>
			<div>
				<p><img src="https://ducklake.select/images/icons/waves.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Data lake operations
        
      </h3>
    
					<p>DuckLake supports snapshots, time travel queries, schema evolution and partitioning.</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/documents.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Lightweight snapshots
        
      </h3>
    
					<p>You can have as many snapshots as you want without frequent compacting steps!</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/pipette.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        ACID transactions
        
      </h3>
    
					<p>DuckLake allows concurrent access with ACID transactional guarantees over multi-table operations.</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/clock.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Performance-oriented
        
      </h3>
    
					<p>DuckLake uses statistics for filter pushdown, enabling fast queries even on large datasets.</p>
				</div>
			</div>
		</div>
	</div>

<div>
		<div>
      <h2>
        
        In Conversation: DuckDB Founders on DuckLake
        
      </h2>
    
			<p>Listen to Hannes Mühleisen and Mark Raasveldt walk through the history of data lakes and introduce DuckLake, a new lakehouse format.</p>
		</div>
		<div data-video-id="zeonmOO9jm4">
				<p><img src="https://ducklake.select/images/thumb_introducting-ducklake.png" alt="Thumbnail: Introducing DuckLake"></p>
			</div>
	</div>

<div id="quickinstall">
		<div>
      <h2>
        
        Create your first DuckLake with DuckDB
        
      </h2>
    
			<p>DuckDB provides first-class support for DuckLake through its highly portable extension, running wherever DuckDB does.</p>
			<!--<a href="/docs/installation/" class="button transparent">More installation options</a>-->
		</div>
		<div>
				<div>
					<ul>
						
						<li data-client="duckdb">DuckDB</li>
						<li data-client="sqlite">SQLite</li>
						<li data-client="postgresql">PostgreSQL</li>
						<li data-client="mysql">MySQL</li>
					</ul>
				</div>
				
				<div>
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:metadata.ducklake'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
				</div>
				
	</div></div>

<div id="quick-installation">

<div data-install="duckdb">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:metadata.ducklake'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="postgresql">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> postgres</span><span>;</span>

<span>-- Make sure that the database `ducklake_catalog` exists in PostgreSQL.</span>
<span>ATTACH</span> <span>'ducklake:postgres:dbname=ducklake_catalog host=your_postgres_host'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="sqlite">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> sqlite</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:sqlite:metadata.sqlite'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="mysql">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> mysql</span><span>;</span>

<span>-- Make sure that the database `ducklake_catalog` exists in MySQL</span>
<span>ATTACH</span> <span>'ducklake:mysql:db=ducklake_catalog host=your_mysql_host'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

</div>


<section>
	<div>
      <h2>
        
        Frequently asked questions
        
      </h2>
    
			<p>Answers to common questions to help you understand and make the most of DuckLake.</p>
		</div>
	
	<div>
		<div>
      <h3>
        
        
				Why should I use DuckLake?
			
        
      </h3>
    
			<div>
				<p>DuckLake provides a lightweight one-stop solution for if you need a data lake and catalog.

				</p><p>You can use DuckLake for a “multiplayer DuckDB” setup with multiple DuckDB instances reading and writing the same dataset –
				a concurrency model <a href="https://duckdb.org/docs/stable/connect/concurrency">not supported by vanilla DuckDB</a>.</p>

				<p>If you only use DuckDB for both your DuckLake entry point and your catalog database, you can still benefit from using DuckLake:
				you can run time travel queries,
				exploit data partitioning,
				and can store your data in multiple files instead of using a single (potentially very large) database file.</p>
			</div>
		</div>

		<div>
      <h3>
        
        
				What is DuckLake?
			
        
      </h3>
    
			<div><p>
				First of all, a catchy name for a DuckDB-originated technology for data lakes and lakehouses.
				More seriously, the term “DuckLake” can refer to three things:

				</p><ol>
					<li>the <i>specification</i> of the DuckLake lakehouse format,</li>
					<li>the <a href="https://duckdb.org/docs/stable/core_extensions/ducklake"><code>ducklake</code> <i>DuckDB extension</i></a>, which supports reading/writing datasets in the DuckLake specification,</li>
					<li>a DuckLake, a <i>dataset</i> stored using the DuckLake lakehouse format.</li>
				</ol>
			</div>
		</div>

		<div>
      <h3>
        
        
				What is the license of DuckLake?
			
        
      </h3>
    
			<p>
				The DuckLake specification and the DuckLake DuckDB extension are released under the MIT license.
			</p>
		</div>

	</div>
	
	
	
</section>


		




	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art of Fugue – Contrapunctus I (2021) (122 pts)]]></title>
            <link>https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/</link>
            <guid>44106764</guid>
            <pubDate>Tue, 27 May 2025 13:25:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/">https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/</a>, See on <a href="https://news.ycombinator.com/item?id=44106764">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-22677">
		
	
	<div>
		
<p>JS Bach’s last set of works, collectively titled <em><a href="https://en.wikipedia.org/wiki/The_Art_of_Fugue">The Art of Fugue</a></em>, was published shortly after his death. It was not a big hit. Dense counterpoint was deeply unfashionable at that time, as Western European aristocratic tastes shifted toward singable melodies over block chords. The first published edition of <em>The Art of Fugue</em> only sold about thirty copies, and it wasn’t performed in its entirety until 1922.</p>
<p>Eventually the classical music audience did come to admire Bach’s final fugue collection, but it took <a href="https://en.wikipedia.org/wiki/Johann_Sebastian_Bach#19th_century">almost 100 years after it was written</a>. The fugues still aren’t the easiest listening experience. They were meant to be didactic, to be played and studied rather than to be listened to–though of course you are free to listen to and enjoy them. I’m finding that my own enjoyment is much enhanced by opening up the structure through visualization, so that’s what I’ve done with <a href="https://www.amazon.com/Bach-J-S-Fugue-Angela-Hewitt/dp/B00MX51FHW">Angela Hewitt’s recording of Contrapunctus I</a> using Ableton Live.</p>
<p><iframe title="Bach - The Art of Fugue Contrapunctus I - Ableton Live visualization" width="640" height="480" src="https://www.youtube.com/embed/-yRqKp2rqPk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>The main thing to listen (and watch) for here is <a href="https://en.wikipedia.org/wiki/The_Art_of_Fugue#Structure">the subject</a>, the little melody that each voice plays as it enters. After the subject, the voices wander off to play other intertwining parts, occasionally returning to the subject as they go. In the subsequent <em>Art of Fugue</em> pieces, Bach does all kinds of twisting and warping of the subject, writing it <a href="https://en.wikipedia.org/wiki/Inversion_(music)#Melodies">upside down</a>, <a href="https://en.wikipedia.org/wiki/Retrograde_(music)">backwards</a>, <a href="https://en.wikipedia.org/wiki/Diminution#Diminution_in_composition">twice as fast</a>, <a href="https://en.wikipedia.org/wiki/Augmentation_(music)">half as fast</a>, <a href="https://en.wikipedia.org/wiki/Stretto">overlaid on top of itself</a>, and so on. In Contrapunctus I, however, he doesn’t do any of these formal games. It sounds more like he’s just riffing around the subject. It’s almost casual, at least by his standards.</p>
<p><span id="more-22677"></span>Here’s Glenn Gould playing Contrapunctus I on organ.</p>
<p><iframe title="Glenn Gould plays Bach &quot;The Art Of Fugue BWV 1080&quot; Organ/Piano" width="640" height="360" src="https://www.youtube.com/embed/GnXHnEz94os?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>And here he is playing it live on piano toward the end of his life, with a lovely slow tempo.</p>
<p><iframe title="Glenn Gould-J.S. Bach-The Art of Fugue (HD)" width="640" height="480" src="https://www.youtube.com/embed/4uX-5HOx2Wc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>Bach published <em>The Art of Fugue</em> in “open score,” meaning that each voice of the counterpoint is on its own line, rather than being grouped together in the usual two-staff notation that we’re used to. Here’s an excerpt of <a href="https://www.youtube.com/watch?v=zQXPoJjfz0I">Contrapunctus VII</a> in open score in Bach’s own handwriting, with some informational color-coding added by Guido Magnano:</p>
<p><a href="https://commons.wikimedia.org/wiki/File:ContrapunctusVII.jpg"><img data-recalc-dims="1" loading="lazy" decoding="async" data-attachment-id="22882" data-permalink="https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/contrapunctusvii/" data-orig-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=1719%2C1720&amp;ssl=1" data-orig-size="1719,1720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Bach – Art of Fugue – Contrapunctus VII – color-coded open score" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=640%2C640&amp;ssl=1" src="https://i0.wp.com/ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII-1024x1024.jpeg?resize=640%2C640&amp;ssl=1" alt="" width="640" height="640" srcset="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=1536%2C1536&amp;ssl=1 1536w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?w=1719&amp;ssl=1 1719w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?w=1280&amp;ssl=1 1280w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></p>
<p>Open score was already considered an old-fashioned way to write keyboard music in Bach’s time, and people stopped using it entirely soon afterward. Since the 19th century Bach revival, musicians have taken the open score format as an invitation to play <em>The Art of Fugue</em> on four separate instruments. For example, there have been lots of string quartet recordings. Here’s a good one:</p>
<p><iframe loading="lazy" title="J.S. Bach: The Art Of Fugue, BWV 1080 - Version For String Quartet - Contrapunctus 1" width="640" height="480" src="https://www.youtube.com/embed/3A8iR7cGHHQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>Contrapunctus I also sounds cool on four viols (cousins of the viola and cello, but with frets like a guitar):</p>
<p><iframe loading="lazy" title="Bach-The Art of Fugue- Contrapunctus 1" width="640" height="480" src="https://www.youtube.com/embed/gU8Vu5YEo48?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>It sounds amazing on four saxophones:</p>
<p><iframe loading="lazy" title="J.S. Bach Contrapunctus 1 - Rascher Saxophone Quartet" width="640" height="360" src="https://www.youtube.com/embed/wEJUOUaGlBY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>And it has a nice organ-like quality on four recorders:</p>
<p><iframe loading="lazy" title="Woodpeckers Recorder Quartet - JS Bach - die Kunst der Fuge BWV 1080 - Contrapunctus I &amp; IX" width="640" height="360" src="https://www.youtube.com/embed/aLEL9WcbGLU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p><a href="https://en.wikipedia.org/wiki/Joseph_Kerman">Joseph Kerman</a> writes about Contrapunctus I in his book <em><a href="https://www.ucpress.edu/book/9780520287631/the-art-of-fugue">Art of Fugue</a></em>:</p>
<blockquote>
<p>[I]n order to set off the technical virtuosity that was the work’s raison d’être, Bach had the extraordinary idea of making its first number a fugue without contrapuntal devices. Contrapunctus 1 has neither strettos, diminutions, and so on, nor even <a href="https://en.wikipedia.org/wiki/Subject_(music)#Countersubject">countersubjects</a> or recurring <a href="https://en.wikipedia.org/wiki/Fugue#Episode">episodes</a>. These devices will be introduced only in the succeeding contrapuncti, one by one. In Contrapunctus 1 <a href="https://www.teoria.com/en/reference/i/invertible-counterpoint.php">invertible counterpoint</a> itself is in very short supply. This elemental fugue never modulates beyond the obligatory dominant and subdominant keys.</p>
<p>In any case, this most basic of fugues is necessarily also one of Bach’s freest and must also be one of his smoothest… The contrapuntal lines, consisting mostly of quarter- and eighth-note patterns, move stepwise or by the smallest leaps, and the expectations of eighteenth-century harmony often go unfulfilled. Strong cadences are shunned. While such generalities only begin to explain the almost mesmeric fluency of Bach’s late style, they may help sensitize us to contrasts where it is abrogated, such as at those episodes featuring larger leaps [bars 29–30, 36–40, 49–53], and at the one really, decisively strong cadence [bar 74].</p>
<p>Eventually the surface does begin to ruffle, when in a new exposition the bass steps in on the heels of its predecessor and enters after three bars rather than four [bar 32]. This entry—it can be heard as a second stab at stretto, after a previous, premature effort in bars 29–30, what is sometimes called a false stretto—moves rather hastily from the dominant around to the subdominant, twisting and turning the subject oddly. Then the tenor entry, as though checked by the low As in the bass, hesitates, accumulating dissonances—sevenths, ninths, and pungent augmented intervals [bars 41, 42, 43].</p>
</blockquote>
<p>These intervals are a lot less “pungent” in <a href="https://www.ethanhein.com/wp/2019/why-cant-you-tune-your-guitar/">12-tone equal temperament</a> than they would have been in <a href="https://www.ethanhein.com/wp/2020/what-does-the-well-tempered-clavier-sound-like-in-actual-well-temperament/">the uneven temperament of Bach’s era</a>.</p>
<blockquote>
<p>The soprano in this group of entries emerges as a sort of ethereal climax, led into by another false stretto. The bass drops out, allowing for heightened activity in the remaining voices, like a beating of wings [bars 48–54].</p>
</blockquote>
<blockquote>
<p>Past the exposition, then, the piece can be seen to grow increasingly complex, though the feeling seems to me not exactly of complexity but of complexities tested out and drifted past, ideas considered and shelved, in a constantly changing improvisational field of a unique kind. Endlessly fertile and quite unstoppable, Bach proceeds spontaneously, almost distractedly, until the piece pulls itself together with one grand gesture, the long dominant pedal in the bass from bar 63 to bar 73.</p>
<p>Literally, of course, the pitch A drops out at bar 66, but in the ear it lasts all the way, so the passage has the effect of a cadenza, an increasingly rhapsodic epilogue during which pitch rises and tension mounts until it is too much to bear—or so we must infer; the buildup is so smooth we had no inkling of impending crisis. This programmatically seamless music literally breaks off, stammers, and finally sinks—truly sinks—to rest.</p>
</blockquote>
<p>Bach doesn’t sound much like jazz, but Kerman identifies qualities in Contrapunctus I that are the things I like about jazz: the not-so-rigid development of themes and interplay of voices, the “complexities tested out and drifted past.” The later fugues are full of complexities that are tested all the way out and then some. There are even a couple of palindrome-like <a href="https://en.wikipedia.org/wiki/Mirror_fugue" target="_blank" rel="noopener">mirror fugues</a>. These are fascinating in that <em><a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach" target="_blank" rel="noopener">Gödel, Escher, Bach</a></em> way, but also exhausting. Sometimes you want to just listen to music without doing a whole Rubik’s cube worth of combinatorial math.</p>
<p>My attention span for this music improves when I hear it quantized over a beat. Here’s Angela Hewitt’s recording over the beat from “<a href="https://www.youtube.com/watch?v=QsZlY0Vz4-o">Empire State of Mind</a>” by Jay-Z and Alicia Keys, inspired by an arrangement by <a href="https://www.notesbyheather.com/">Heather Fortune</a>.</p>

<p>In spite of the jokey title, this remix is not meant to be ironic. (Well, not totally ironic.) The beat helps me stay focused and present, rather than having my mind drift into a, you know, fugue state. That’s what beats are for. This music is supposed to be didactic, right? I learn best when I’m learning to a groove. But I also just like the aesthetic effect, and the suggestion that anything has groove potential.</p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]></description>
        </item>
    </channel>
</rss>