<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 23 Aug 2024 09:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Government report proves that we need to liberate the Postcode Address File (158 pts)]]></title>
            <link>https://takes.jamesomalley.co.uk/p/secret-paf-report</link>
            <guid>41326604</guid>
            <pubDate>Fri, 23 Aug 2024 06:36:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://takes.jamesomalley.co.uk/p/secret-paf-report">https://takes.jamesomalley.co.uk/p/secret-paf-report</a>, See on <a href="https://news.ycombinator.com/item?id=41326604">Hacker News</a></p>
Couldn't get https://takes.jamesomalley.co.uk/p/secret-paf-report: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Python's Preprocessor – Pydong (282 pts)]]></title>
            <link>https://pydong.org/posts/PythonsPreprocessor/</link>
            <guid>41322758</guid>
            <pubDate>Thu, 22 Aug 2024 17:54:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pydong.org/posts/PythonsPreprocessor/">https://pydong.org/posts/PythonsPreprocessor/</a>, See on <a href="https://news.ycombinator.com/item?id=41322758">Hacker News</a></p>
Couldn't get https://pydong.org/posts/PythonsPreprocessor/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: InstantDB – A Modern Firebase (729 pts)]]></title>
            <link>https://github.com/instantdb/instant</link>
            <guid>41322281</guid>
            <pubDate>Thu, 22 Aug 2024 17:08:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/instantdb/instant">https://github.com/instantdb/instant</a>, See on <a href="https://news.ycombinator.com/item?id=41322281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://instantdb.com/" rel="nofollow">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/2af63e8197df473d79408acf76aa06b72d1bd76f0a851f8983a8b487ee7faf6b/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6c6f676f5f776974685f746578745f6461726b5f6d6f64652e737667" data-canonical-src="https://instantdb.com/readmes/logo_with_text_dark_mode.svg">
      <img alt="Shows the Instant logo" src="https://camo.githubusercontent.com/fcbab3cc92cfb8b401f1b4ecbc4d31ac2e0323c368b1425689b3e294f5f24ad6/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6c6f676f5f776974685f746578745f6c696768745f6d6f64652e737667" data-canonical-src="https://instantdb.com/readmes/logo_with_text_light_mode.svg">
    </picture></themed-picture>
  </a>
</p>
<p dir="auto">
  <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">
    <img height="20" src="https://camo.githubusercontent.com/8fc18af31e1f1939c19c29462d1e29a0a9bbacb77c7a3ec147be30efde001827/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f31303331393537343833323433313838323335" data-canonical-src="https://img.shields.io/discord/1031957483243188235">
  </a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/89f509177e307d8b573fe98e36deae42e2498fa84230420f8f26ab09359f0a2c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e7374616e7464622f696e7374616e74"><img src="https://camo.githubusercontent.com/89f509177e307d8b573fe98e36deae42e2498fa84230420f8f26ab09359f0a2c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e7374616e7464622f696e7374616e74" alt="stars" data-canonical-src="https://img.shields.io/github/stars/instantdb/instant"></a>
</p>
<p dir="auto">
   <a href="https://instantdb.com/docs" rel="nofollow">Get Started</a> · 
   <a href="https://instantdb.com/examples" rel="nofollow">Examples</a> · 
   <a href="https://instantdb.com/tutorial" rel="nofollow">Try the Demo</a> · 
   <a href="https://instantdb.com/docs" rel="nofollow">Docs</a> · 
   <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">Discord</a>
</p><p dir="auto">Instant is a client-side database that makes it easy to build real-time and collaborative apps like Notion or Figma.</p>
<p dir="auto">You write <a href="https://www.instantdb.com/docs/instaql" rel="nofollow">relational queries</a> in the shape of the data you want and Instant handles all the data fetching, permission checking, and offline caching. When you <a href="https://www.instantdb.com/docs/instaml" rel="nofollow">change data</a>, optimistic updates and rollbacks are handled for you as well. Plus, every query is multiplayer by default.</p>
<p dir="auto">We also support <a href="https://www.instantdb.com/docs/presence-and-topics" rel="nofollow">ephemeral</a> updates, like cursors, or who's online. Currently we have SDKs for <a href="https://www.instantdb.com/docs/start-vanilla" rel="nofollow">Javascript</a>, <a href="https://www.instantdb.com/docs/" rel="nofollow">React</a>, and <a href="https://www.instantdb.com/docs/start-rn" rel="nofollow">React Native</a>.</p>
<p dir="auto">How does it look? Here's a barebones chat app in about 10 lines:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// ༼ つ ◕_◕ ༽つ Real-time Chat
// ----------------------------------
// * Updates instantly
// * Multiplayer
// * Works offline
function Chat() {
  // 1. Read
  const { isLoading, error, data } = useQuery({
    messages: {},
  });

  // 2. Write
  const addMessage = (message) => {
    transact(tx.messages[id()].update(message));
  }

  // 3. Render!
  return <UI data={data} onAdd={addMessage} />
}"><pre><span>// ༼ つ ◕_◕ ༽つ Real-time Chat</span>
<span>// ----------------------------------</span>
<span>// * Updates instantly</span>
<span>// * Multiplayer</span>
<span>// * Works offline</span>
<span>function</span> <span>Chat</span><span>(</span><span>)</span> <span>{</span>
  <span>// 1. Read</span>
  <span>const</span> <span>{</span> isLoading<span>,</span> error<span>,</span> data <span>}</span> <span>=</span> <span>useQuery</span><span>(</span><span>{</span>
    <span>messages</span>: <span>{</span><span>}</span><span>,</span>
  <span>}</span><span>)</span><span>;</span>

  <span>// 2. Write</span>
  <span>const</span> <span>addMessage</span> <span>=</span> <span>(</span><span>message</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>transact</span><span>(</span><span>tx</span><span>.</span><span>messages</span><span>[</span><span>id</span><span>(</span><span>)</span><span>]</span><span>.</span><span>update</span><span>(</span><span>message</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>

  <span>// 3. Render!</span>
  <span>return</span> <span>&lt;</span><span>UI</span> <span>data</span><span>=</span><span>{</span><span>data</span><span>}</span> <span>onAdd</span><span>=</span><span>{</span><span>addMessage</span><span>}</span> <span>/</span><span>&gt;</span>
<span>}</span></pre></div>
<p dir="auto">Want to see for yourself? <a href="https://instantdb.com/tutorial" rel="nofollow">try a demo in your browser.</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">Writing modern apps are full of schleps. Most of the time you start with the server: stand up databases, caches, ORMs, and endpoints. Then you write client-side code: stores, selectors, mutators. Finally you paint a screen. If you add multiplayer you need to think about stateful servers, and if you support offline mode, you need to think about IndexedDB and transaction queues.</p>
<p dir="auto">To make things worse, whenever you add a new feature, you go through the same song and dance over and over again: add models, write endpoints, stores, selectors, and finally the UI.</p>
<p dir="auto">Could it be better?</p>
<p dir="auto">In 2021, <strong>we realized that most of the schleps we face as UI engineers are actually database problems problems in disguise.</strong> (We got into greater detail <a href="https://instantdb.com/essays/next_firebase" rel="nofollow">in this essay</a>)</p>
<p dir="auto">
  <a href="#">
    <img alt="Shows how Instant compresses schleps" src="https://camo.githubusercontent.com/5767f96f66d7e1d87f07bfeab64f3590a30e11792bdeca79001247f8c70e6412/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f636f6d7072657373696f6e2e737667" data-canonical-src="https://instantdb.com/readmes/compression.svg">
  </a>
</p>
<p dir="auto">If you had a database on the client, you wouldn't need to think about stores, selectors, endpoints, or local caches: just write queries. If these queries were multiplayer by default, you wouldn't have to worry about stateful servers. And if your database supported rollback, you'd get optimistic updates for free.</p>
<p dir="auto">So we built Instant. Instant gives you a database you can use in the client, so you can focus on what’s important: building a great UX for your users, and doing it quickly.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architectural Overview</h2><a id="user-content-architectural-overview" aria-label="Permalink: Architectural Overview" href="#architectural-overview"></a></p>
<p dir="auto">Here's how Instant works at a high level:</p>
<p dir="auto">
  <a href="#">
    <img alt="Shows how Instant compresses schleps" src="https://camo.githubusercontent.com/9f9534e626714562bc04bbfc0d2d2a2295a9375ec44c5467b9ff7474395aba86/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6172636869746563747572652e737667" data-canonical-src="https://instantdb.com/readmes/architecture.svg">
  </a>
</p>
<p dir="auto">Under the hood, we store all user data as triples in one big Postgres database. A multi-tenant setup lets us offer a free tier that never pauses.</p>
<p dir="auto">A sync server written in Clojure talks to Postgres. We wrote a query engine that understands datalog and <a href="https://www.instantdb.com/docs/instaql" rel="nofollow">InstaQL</a>, a relational language that looks a lot like GraphQL:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// give me all users, their posts and comments
{ users: { posts: { comments: {} } } }"><pre><span>// give me all users, their posts and comments</span>
<span>{</span> <span>users</span>: <span>{</span> <span>posts</span>: <span>{</span> <span>comments</span>: <span>{</span><span>}</span> <span>}</span> <span>}</span> <span>}</span></pre></div>
<p dir="auto">Taking inspiration from <a href="https://asana.com/inside-asana/worldstore-distributed-caching-reactivity-part-1" rel="nofollow">Asana’s WorldStore</a> and <a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/#syncing-object-properties" rel="nofollow">Figma’s LiveGraph</a>, we tail postgres’ WAL to detect novelty and invalidate relevant queries.</p>
<p dir="auto">For the frontend, we wrote a client-side triple store. The SDK handles persisting a cache of recent queries to IndexedDB on web, and AsyncStorage in React Native.</p>
<p dir="auto">All data goes through a permission system powered by Google's <a href="https://github.com/google/cel-java">CEL library</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">The easiest way to get started with Instant is by signing up on instantdb.com. <a href="https://instantdb.com/docs" rel="nofollow">You can create a functional app in 5 minute or less.</a>.</p>
<p dir="auto">If you have any questions, you can jump in on our <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">discord</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">You can start by joining our <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">discord</a> and introducing yourself. Even if you don't contribute code, we always love feedback.</p>
<p dir="auto">If you want to make changes, start by reading the <a href="https://github.com/instantdb/instant/blob/main/client"><code>client</code></a> and <a href="https://github.com/instantdb/instant/blob/main/server"><code>server</code></a> READMEs. There you'll find instructions to start Instant locally.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Peloton to charge $95 activation fee for used bikes (111 pts)]]></title>
            <link>https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html</link>
            <guid>41322266</guid>
            <pubDate>Thu, 22 Aug 2024 17:06:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html">https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html</a>, See on <a href="https://news.ycombinator.com/item?id=41322266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/PTON/">Peloton</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Thursday said it will <a href="https://www.cnbc.com/2024/08/22/peloton-pton-earnings-q4-2024.html">start charging new subscribers</a> a one-time $95 activation fee if they bought their hardware on the secondary market as more consumers snag lightly used equipment for a fraction of the typical retail price.</p><p>The used equipment activation fee for subscribers in the U.S. and Canada comes as Peloton starts to see a meaningful increase in new members who bought used Bikes or Treads from peer-to-peer markets such as Facebook Marketplace.&nbsp;</p><p>During its fiscal fourth quarter, which ended June 30, Peloton said it saw a "steady stream of paid connected fitness subscribers" who bought hardware on the secondary market. The company said the segment grew 16% year over year.</p><p>"We believe a meaningful share of these subscribers are incremental, and they exhibit lower net churn rates than rental subscribers," the company said in a letter to shareholders.&nbsp;</p><p>"It's also worth highlighting that this activation fee will be a source of incremental revenue and gross profit for us, helping to support our investments in improving the fitness experience for our members," interim co-CEO Christopher Bruzzo later added on a call with analysts.&nbsp;</p><p>While plenty of Peloton subscribers are avid users of the home workout machines, some have likened them to glorified clothes racks because so many people stop using the equipment. Those people paid Peloton for that hardware originally, but importantly, many of them have canceled their monthly subscription, which is how Peloton <a href="https://www.cnbc.com/2024/07/02/peloton-staves-off-liquidity-crunch-in-global-refinance.html">makes the bulk of its money</a>.&nbsp;</p><p>The ability to attract new, budget-conscious members from the secondary market who are willing to pay for a monthly subscription is a unique opportunity for Peloton to grow revenue without any upfront cost, on top of the revenue from the original sale.&nbsp;</p><p>Ari Kimmelfeld — whose startup Trade My Stuff, formerly known as Trade My Spin, sells used Peloton equipment — estimates there are around a million Bikes collecting dust in homes around the world that could be a source of new revenue for the company.&nbsp;</p><p>He told CNBC he previously met with Peloton executives to discuss ways to collaborate, because every time he sells a used piece of equipment, it could lead to more than $500 in new revenue per year for Peloton. With the new used equipment activation fee, that number could grow to more than $600 for the first year.&nbsp;</p><p>"We save the customer a lot more than $95," Kimmelfeld told CNBC on Thursday after the new activation fee was announced. "I don't think it'll stop or slow down people from buying secondary equipment … because you can get a bike delivered faster and cheaper on the secondary market, even with the $95, let's call it a tax, from Peloton."&nbsp;</p><p>Trade My Stuff sells first-generation Bikes for $499, compared with $1,445 new. It offers the Bike+ for $1,199, compared with $2,495 new. It also sells used Treads for $1,999, compared with $2,995 new.&nbsp;</p><p>Since launching his business, Kimmelfeld has worked with people looking to sell their used Peloton equipment and has since sold a "few thousand" Bikes. In 14 cities around the country, including Los Angeles, Denver and New York City, the company offers same- or next-day delivery. Outside of those locales, it provides delivery within three to five days. That compares with a new Peloton purchase, which can take significantly longer to deliver.&nbsp;</p><p>The used equipment activation fee is designed to ensure that new members "receive the same high-quality onboarding experience Peloton is known for," the company said. Bruzzo said that those who buy a used Bike or Bike+ have access to a virtual custom fitting ahead of their first ride, as well as a history summary that shows how many rides those bikes had before they were resold.&nbsp;</p><p>"We're also offering these new members discounts on accessories such as bike shoes, bike mats and spare parts," said Bruzzo. "We'll continue to lean into this important channel and find additional ways to improve the new member experience, for example, providing early education about the broad range of fitness modalities that we offer and the many series and programs our instructors provide to new members."</p></div><div id="RegularArticle-RelatedContent-1"><h2>Don’t miss these insights from CNBC PRO</h2><div><ul><li><a href="https://www.cnbc.com/2024/08/16/the-60/40-portfolio-shined-during-the-market-turbulence-.html">The 60/40 portfolio excelled during the market storm — and Vanguard sees a strong decade ahead</a></li><li><a href="https://www.cnbc.com/2024/08/20/investor-mark-mobius-names-one-risk-that-could-set-back-us-markets.html">Veteran investor Mark Mobius says this 'historically significant' factor could set back U.S. stocks</a></li><li><a href="https://www.cnbc.com/2024/08/14/jefferies-names-3-chip-stocks-to-buy-after-the-sell-off-giving-all-over-50percent-upside.html">Jefferies names 3 chip stocks to buy after the sell-off, giving all over 50% upside</a></li><li><a href="https://www.cnbc.com/2024/08/20/novo-nordisk-vs-eli-lilly-analysts-weigh-in-as-the-obesity-drug-battle-heats-up.html">Novo Nordisk vs. Eli Lilly: Analysts weigh in as the obesity-drug battle heats up</a><br></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aerc: A well-crafted TUI for email (238 pts)]]></title>
            <link>https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/</link>
            <guid>41321981</guid>
            <pubDate>Thu, 22 Aug 2024 16:34:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/">https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/</a>, See on <a href="https://news.ycombinator.com/item?id=41321981">Hacker News</a></p>
Couldn't get https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Continuous reinvention: A brief history of block storage at AWS (303 pts)]]></title>
            <link>https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html</link>
            <guid>41321063</guid>
            <pubDate>Thu, 22 Aug 2024 14:59:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html">https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html</a>, See on <a href="https://news.ycombinator.com/item?id=41321063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header></header><hr><section><p><time itemprop="datePublished" datetime="2024-08-22">August 22, 2024</time> • 4800 words</p><span itemprop="articleBody"><p><em><a href="https://www.linkedin.com/in/msolson/">Marc Olson</a> has been part of the team shaping Elastic Block Store (EBS) for over a decade. In that time, he’s helped to drive the dramatic evolution of EBS from a simple block storage service relying on shared drives to a massive network storage system that delivers over 140 trillion daily operations.</em></p><p><em>In this post, Marc provides a fascinating insider’s perspective on the journey of EBS. He shares hard-won lessons in areas such as queueing theory, the importance of comprehensive instrumentation, and the value of incrementalism versus radical changes. Most importantly, he emphasizes how constraints can often breed creative solutions. It’s an insightful look at how one of AWS’s foundational services has evolved to meet the needs of our customers (and the pace at which they’re innovating).</em></p><p><em>–W</em></p><hr><center><h2>Continuous reinvention: A brief history of block storage at AWS</h2></center><p>I’ve built system software for most of my career, and before joining AWS it was mostly in the networking and security spaces. When I joined AWS nearly 13 years ago, I entered a new domain—storage—and stepped into a new challenge. Even back then the scale of AWS dwarfed anything I had worked on, but many of the same techniques I had picked up until that point remained applicable—distilling problems down to first principles, and using successive iteration to incrementally solve problems and improve performance.</p><p>If you look around at AWS services today, you’ll find a mature set of core building blocks, but it wasn’t always this way. <a href="https://www.allthingsdistributed.com/2008/08/amazon_ebs_elastic_block_store.html">EBS launched on August 20, 2008</a>, nearly two years after EC2 became available in beta, with a simple idea to provide network attached block storage for EC2 instances. We had one or two storage experts, and a few distributed systems folks, and a solid knowledge of computer systems and networks. How hard could it be? In retrospect, if we knew at the time how much we didn’t know, we may not have even started the project!</p><p>Since I’ve been at EBS, I’ve had the opportunity to be part of the team that’s evolved EBS from a product built using shared hard disk drives (HDDs), to one that is capable of delivering hundreds of thousands of IOPS (IO operations per second) to a single EC2 instance. It’s remarkable to reflect on this because EBS is capable of delivering more IOPS to a single instance today than it could deliver to an entire Availability Zone (AZ) in the early years on top of HDDs. Even more amazingly, today EBS in aggregate delivers over 140 trillion operations daily across a distributed SSD fleet. But we definitely didn’t do it overnight, or in one big bang, or even perfectly. When I started on the EBS team, I initially worked on the EBS client, which is the piece of software responsible for converting instance IO requests into EBS storage operations. Since then I’ve worked on almost every component of EBS and have been delighted to have had the opportunity to participate so directly in the evolution and growth of EBS.</p><p>As a storage system, EBS is a bit unique. It’s unique because our primary workload is system disks for EC2 instances, motivated by the hard disks that used to sit inside physical datacenter servers. A lot of storage services place durability as their primary design goal, and are willing to degrade performance or availability in order to protect bytes. EBS customers care about durability, and we provide the primitives to help them achieve high durability with io2 Block Express volumes and volume snapshots, but they also care a lot about the performance and availability of EBS volumes. EBS is so closely tied as a storage primitive for EC2, that the performance and availability of EBS volumes tends to translate almost directly to the performance and availability of the EC2 experience, and by extension the experience of running applications and services that are built using EC2. The story of EBS is the story of understanding and evolving performance in a very large-scale distributed system that spans layers from guest operating systems at the top, all the way down to custom SSD designs at the bottom. In this post I’d like to tell you about the journey that we’ve taken, including some memorable lessons that may be applicable to your systems. After all, systems performance is a complex and really challenging area, and it’s a complex language across many domains.</p><h2 id="queueing-theory-briefly">Queueing theory, briefly <a href="#queueing-theory-briefly"></a></h2><p>Before we dive too deep, let’s take a step back and look at how computer systems interact with storage. The high-level basics haven’t changed through the years—a storage device is connected to a bus which is connected to the CPU. The CPU queues requests that travel the bus to the device. The storage device either retrieves the data from CPU memory and (eventually) places it onto a durable substrate, or retrieves the data from the durable media, and then transfers it to the CPU’s memory.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-computer-arch.png" alt="Architecture with direct attached disk" loading="lazy"><figcaption>High-level computer architecture with direct attached disk</figcaption></figure><p>You can think of this like a bank. You walk into the bank with a deposit, but first you have to traverse a queue before you can speak with a bank teller who can help you with your transaction. In a perfect world, the number of patrons entering the bank arrive at the exact rate at which their request can be handled, and you never have to stand in a queue. But the real world isn’t perfect. The real world is asynchronous. It’s more likely that a few people enter the bank at the same time. Perhaps they have arrived on the same streetcar or train. When a group of people all walk into the back at the same time, some of them are going to have to wait for the teller to process the transactions ahead of them.</p><p>As we think about the time to complete each transaction, and empty the queue, the average time waiting in line (latency) across all customers may look acceptable, but the first person in the queue had the best experience, while the last had a much longer delay. There are a number of things the bank can do to improve the experience for all customers. The bank could add more tellers to process more requests in parallel, it could rearrange the teller workflows so that each transaction takes less time, lowering both the total time and the average time, or it could create different queues for either latency insensitive customers or consolidating transactions that may be faster to keep the queue low. But each of these options comes at an additional cost—hiring more tellers for a peak that may never occur, or adding more real estate to create separate queues. While imperfect, unless you have infinite resources, queues are necessary to absorb peak load.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-simplified-ec2-ebs-queueing.png" alt="Simple diagram of EC2 and EBS queueing from 2012" loading="lazy"><figcaption>Simplified diagram of EC2 and EBS queueing (c. 2012)</figcaption></figure><p>In network storage systems, we have several queues in the stack, including those between the operating system kernel and the storage adapter, the host storage adapter to the storage fabric, the target storage adapter, and the storage media. In legacy network storage systems, there may be different vendors for each component, and different ways that they think about servicing the queue. You may be using a dedicated, lossless network fabric like fiber channel, or using iSCSI or NFS over TCP, either with the operating system network stack, or a custom driver. In either case, tuning the storage network often takes specialized knowledge, separate from tuning the application or the storage media.</p><p>When we first built EBS in 2008, the storage market was largely HDDs, and the latency of our service was dominated by the latency of this storage media. Last year, Andy Warfield went in-depth about the <a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html#technical-scale-scale-and-the-physics-of-storage">fascinating mechanical engineering behind HDDs</a>. As an engineer, I still marvel at everything that goes into a hard drive, but at the end of the day they are mechanical devices and physics limits their performance. There’s a stack of platters that are spinning at high velocity. These platters have tracks that contain the data. Relative to the size of a track (&lt;100 nanometers), there’s a large arm that swings back and forth to find the right track to read or write your data. Because of the physics involved, the IOPS performance of a hard drive has remained relatively constant for the last few decades at approximately 120-150 operations per second, or 6-8 ms average IO latency. One of the biggest challenges with HDDs is that tail latencies can easily drift into the hundreds of milliseconds with the impact of queueing and command reordering in the drive.</p><p>We didn’t have to worry much about the network getting in the way since end-to-end EBS latency was dominated by HDDs and measured in the 10s of milliseconds. Even our early data center networks were beefy enough to handle our user’s latency and throughput expectations. The addition of 10s of microseconds on the network was a small fraction of overall latency.</p><p>Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior. Early on, we knew that we needed to spread customers across many disks to achieve reasonable performance. This had a benefit, it dropped the peak outlier latency for the hottest workloads, but unfortunately it spread the inconsistent behavior out so that it impacted many customers.</p><p>When one workload impacts another, we call this a “noisy neighbor.” Noisy neighbors turned out to be a critical problem for the business. As AWS evolved, we learned that we had to focus ruthlessly on a high-quality customer experience, and that inevitably meant that we needed to achieve strong performance isolation to avoid noisy neighbors causing interference with other customer workloads.</p><p>At the scale of AWS, we often run into challenges that are hard and complex due to the scale and breadth of our systems, and our focus on maintaining the customer experience. Surprisingly, the fixes are often quite simple once you deeply understand the system, and have enormous impact due to the scaling factors at play. We were able to make some improvements by changing scheduling algorithms to the drives and balancing customer workloads across even more spindles. But all of this only resulted in small incremental gains. We weren’t really hitting the breakthrough that truly eliminated noisy neighbors. Customer workloads were too unpredictable to achieve the consistency we knew they needed. We needed to explore something completely different.</p><h2 id="set-long-term-goals-but-dont-be-afraid-to-improve-incrementally">Set long term goals, but don’t be afraid to improve incrementally <a href="#set-long-term-goals-but-dont-be-afraid-to-improve-incrementally"></a></h2><p>Around the time I started at AWS in 2011, solid state disks (SSDs) became more mainstream, and were available in sizes that started to make them attractive to us. In an SSD, there is no physical arm to move to retrieve data—random requests are nearly as fast as sequential requests—and there are multiple channels between the controller and NAND chips to get to the data. If we revisit the bank example from earlier, replacing an HDD with an SSD is like building a bank the size of a football stadium and staffing it with superhumans that can complete transactions orders of magnitude faster. A year later we started using SSDs, and haven’t looked back.</p><p>We started with a small, but meaningful milestone: we built a new storage server type built on SSDs, and a new EBS volume type called Provisioned IOPS. Launching a new volume type is no small task, and it also limits the workloads that can take advantage of it. For EBS, there was an immediate improvement, but it wasn’t everything we expected.</p><p>We thought that just dropping SSDs in to replace HDDs would solve almost all of our problems, and it certainly did address the problems that came from the mechanics of hard drives. But what surprised us was that the system didn’t improve nearly as much as we had hoped and noisy neighbors weren’t automatically fixed. We had to turn our attention to the rest of our stack—the network and our software—that the improved storage media suddenly put a spotlight on.</p><p>Even though we needed to make these changes, we went ahead and launched in August 2012 with a maximum of 1,000 IOPS, 10x better than existing EBS standard volumes, and ~2-3 ms average latency, a 5-10x improvement with significantly improved outlier control. Our customers were excited for an EBS volume that they could begin to build their mission critical applications on, but we still weren’t satisfied and we realized that the performance engineering work in our system was really just beginning. But to do that, we had to measure our system.</p><h2 id="if-you-cant-measure-it-you-cant-manage-it">If you can’t measure it, you can’t manage it <a href="#if-you-cant-measure-it-you-cant-manage-it"></a></h2><p>At this point in EBS’s history (2012), we only had rudimentary telemetry. To know what to fix, we had to know what was broken, and then prioritize those fixes based on effort and rewards. Our first step was to build a method to instrument every IO at multiple points in every subsystem—in our client initiator, network stack, storage durability engine, and in our operating system. In addition to monitoring customer workloads, we also built a set of canary tests that run continuously and allowed us to monitor impact of changes—both positive and negative—under well-known workloads.</p><p>With our new telemetry we identified a few major areas for initial investment. We knew we needed to reduce the number of queues in the entire system. Additionally, the Xen hypervisor had served us well in EC2, but as a general-purpose hypervisor, it had different design goals and many more features than we needed for EC2. We suspected that with some investment we could reduce complexity of the IO path in the hypervisor, leading to improved performance. Moreover, we needed to optimize the network software, and in our core durability engine we needed to do a lot of work organizationally and in code, including on-disk data layout, cache line optimization, and fully embracing an asynchronous programming model.</p><p>A really consistent lesson at AWS is that system performance issues almost universally span a lot of layers in our hardware and software stack, but even great engineers tend to have jobs that focus their attention on specific narrower areas. While the much celebrated ideal of a “full stack engineer” is valuable, in deep and complex systems it’s often even more valuable to create cohorts of experts who can collaborate and get really creative across the entire stack and all their individual areas of depth.</p><p>By this point, we already had separate teams for the storage server and for the client, so we were able to focus on these two areas in parallel. We also enlisted the help of the EC2 hypervisor engineers and formed a cross-AWS network performance cohort. We started to build a blueprint of both short-term, tactical fixes and longer-term architectural changes.</p><h2 id="divide-and-conquer">Divide and conquer <a href="#divide-and-conquer"></a></h2><figure><img src="https://www.allthingsdistributed.com/images/mo-physalia.png" alt="Whiteboard showing how the team removed the contronl from from the IO path with Physalia" loading="lazy"><figcaption>Removing the control plane from the IO path with Physalia</figcaption></figure><p>When I was an undergraduate student, while I loved most of my classes, there were a couple that I had a love-hate relationship with. “Algorithms” was taught at a graduate level at my university for both undergraduates and graduates. I found the coursework intense, but I eventually fell in love with the topic, and <a href="https://www.amazon.com/dp/026204630X">Introduction to Algorithms</a>, commonly referred to as CLR, is one of the few textbooks I retained, and still occasionally reference. What I didn’t realize until I joined Amazon, and seems obvious in hindsight, is that you can design an organization much the same way you can design a software system. Different algorithms have different benefits and tradeoffs in how your organization functions. Where practical, Amazon chooses a divide and conquer approach, and keeps teams small and focused on a self-contained component with well-defined APIs.</p><p>This works well when applied to components of a retail website and control plane systems, but it’s less intuitive in how you could build a high-performance data plane this way, and at the same time improve performance. In the EBS storage server, we reorganized our monolithic development team into small teams focused on specific areas, such as data replication, durability, and snapshot hydration. Each team focused on their unique challenges, dividing the performance optimization into smaller sized bites. These teams are able to iterate and commit their changes independently—made possible by rigorous testing that we’ve built up over time. It was important for us to make continual progress for our customers, so we started with a blueprint for where we wanted to go, and then began the work of separating out components while deploying incremental changes.</p><p>The best part of incremental delivery is that you can make a change and observe its impact before making the next change. If something doesn’t work like you expected, then it’s easy to unwind it and go in a different direction. In our case, the blueprint that we laid out in 2013 ended up looking nothing like what EBS looks like today, but it gave us a direction to start moving toward. For example, back then we never would have imagined that Amazon would one day <a href="https://aws.amazon.com/blogs/aws/aws-nitro-ssd-high-performance-storage-for-your-i-o-intensive-applications/">build its own SSDs</a>, with a technology stack that could be tailored specifically to the needs of EBS.</p><h2 id="always-question-your-assumptions">Always question your assumptions! <a href="#always-question-your-assumptions"></a></h2><p>Challenging our assumptions led to improvements in every single part of the stack.</p><p>We started with software virtualization. Until late 2017 all EC2 instances ran on the Xen hypervisor. With devices in Xen, there is a ring queue setup that allows guest instances, or domains, to share information with a privileged driver domain (dom0) for the purposes of IO and other emulated devices. The EBS client ran in dom0 as a kernel block device. If we follow an IO request from the instance, just to get off of the EC2 host there are many queues: the instance block device queue, the Xen ring, the dom0 kernel block device queue, and the EBS client network queue. In most systems, performance issues are compounding, and it’s helpful to focus on components in isolation.</p><p>One of the first things that we did was to write several “loopback” devices so that we could isolate each queue to gauge the impact of the Xen ring, the dom0 block device stack, and the network. We were almost immediately surprised that with almost no latency in the dom0 device driver, when multiple instances tried to drive IO, they would interact with each other enough that the goodput of the entire system would slow down. We had found another noisy neighbor! Embarrassingly, we had launched EC2 with the Xen defaults for the number of block device queues and queue entries, which were set many years prior based on the limited storage hardware that was available to the Cambridge lab building Xen. This was very unexpected, especially when we realized that it limited us to only 64 IO outstanding requests for an entire host, not per device—certainly not enough for our most demanding workloads.</p><p>We fixed the main issues with software virtualization, but even that wasn’t enough. In 2013, we were well into the development of our first <a href="https://www.allthingsdistributed.com/2020/09/reinventing-virtualization-with-nitro.html">Nitro offload card</a> dedicated to networking. With this first card, we moved the processing of VPC, our software defined network, from the Xen dom0 kernel, into a dedicated hardware pipeline. By isolating the packet processing data plane from the hypervisor, we no longer needed to steal CPU cycles from customer instances to drive network traffic. Instead, we leveraged Xen’s ability to pass a virtual PCI device directly to the instance.</p><p>This was a fantastic win for latency and efficiency, so we decided to do the same thing for EBS storage. By moving more processing to hardware, we removed several operating system queues in the hypervisor, even if we weren’t ready to pass the device directly to the instance just yet. Even without passthrough, by offloading more of the interrupt driven work, the hypervisor spent less time servicing the requests—the hardware itself had dedicated interrupt processing functions. This second Nitro card also had hardware capability to handle EBS encrypted volumes with no impact to EBS volume performance. Leveraging our hardware for encryption also meant that the encryption key material is kept separate from the hypervisor, which further protects customer data.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-network-tuning.png" alt="Diagram showing experiments in network tuning to improve throughput and reduce latency" loading="lazy"><figcaption>Experimenting with network tuning to improve throughput and reduce latency</figcaption></figure><p>Moving EBS to Nitro was a huge win, but it almost immediately shifted the overhead to the network itself. Here the problem seemed simple on the surface. We just needed to tune our wire protocol with the latest and greatest data center TCP tuning parameters, while choosing the best congestion control algorithm. There were a few shifts that were working against us: AWS was experimenting with different data center cabling topology, and our AZs, once a single data center, were growing beyond those boundaries. Our tuning would be beneficial, as in the example above, where adding a small amount of random latency to requests to storage servers counter-intuitively reduced the average latency and the outliers due to the smoothing effect it has on the network. These changes were ultimately short lived as we continuously increased the performance and scale of our system, and we had to continually measure and monitor to make sure we didn’t regress.</p><p>Knowing that we would need something better than TCP, in 2014 we started laying the foundation for Scalable Relatable Diagram (SRD) with “<a href="https://ieeexplore.ieee.org/document/9167399">A Cloud-Optimized Transport Protocol for Elastic and Scalable HPC</a>”. Early on we set a few requirements, including a protocol that could improve our ability to recover and route around failures, and we wanted something that could be easily offloaded into hardware. As we were investigating, we made two key observations: 1/ we didn’t need to design for the general internet, but we could focus specifically on our data center network designs, and 2/ in storage, the execution of IO requests that are in flight could be reordered. We didn’t need to pay the penalty of TCP’s strict in-order delivery guarantees, but could instead send different requests down different network paths, and execute them upon arrival. Any barriers could be handled at the client before they were sent on the network. What we ended up with is a protocol that’s useful not just for storage, but for networking, too. When used in <a href="https://aws.amazon.com/about-aws/whats-new/2022/11/elastic-network-adapter-ena-express-amazon-ec2-instances/">Elastic Network Adapter (ENA) Express</a>, SRD improves the performance of your TCP stacks in your guest. SRD can drive the network at higher utilization by taking advantage of multiple network paths and reducing the overflow and queues in the intermediate network devices.</p><p>Performance improvements are never about a single focus. It’s a discipline of continuously challenging your assumptions, measuring and understanding, and shifting focus to the most meaningful opportunities.</p><h2 id="constraints-breed-innovation">Constraints breed innovation <a href="#constraints-breed-innovation"></a></h2><p>We weren’t satisfied that only a relatively small number of volumes and customers had better performance. We wanted to bring the benefits of SSDs to everyone. This is an area where scale makes things difficult. We had a large fleet of thousands of storage servers running millions of non-provisioned IOPS customer volumes. Some of those same volumes still exist today. It would be an expensive proposition to throw away all of that hardware and replace it.</p><p>There was empty space in the chassis, but the only location that didn’t cause disruption in the cooling airflow was between the motherboard and the fans. The nice thing about SSDs is that they are typically small and light, but we couldn’t have them flopping around loose in the chassis. After some trial and error—and help from our material scientists—we found heat resistant, industrial strength hook and loop fastening tape, which also let us service these SSDs for the remaining life of the servers.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-manual-ssd.png" alt="An SSD in one of our servers" loading="lazy"><figcaption>Yes, we manually put an SSD into every server!</figcaption></figure><p>Armed with this knowledge, and a lot of human effort, over the course of a few months in 2013, EBS was able to put a single SSD into each and every one of those thousands of servers. We made a small change to our software that staged new writes onto that SSD, allowing us to return completion back to your application, and then flushed the writes to the slower hard disk asynchronously. And we did this with no disruption to customers—we were converting a propeller aircraft to a jet while it was in flight. The thing that made this possible is that we designed our system from the start with non-disruptive maintenance events in mind. We could retarget EBS volumes to new storage servers, and update software or rebuild the empty servers as needed.</p><p>This ability to migrate customer volumes to new storage servers has come in handy several times throughout EBS’s history as we’ve identified new, more efficient data structures for our on-disk format, or brought in new hardware to replace the old hardware. There are volumes still active from the first few months of EBS’s launch in 2008. These volumes have likely been on hundreds of different servers and multiple generations of hardware as we’ve updated and rebuilt our fleet, all without impacting the workloads on those volumes.</p><h2 id="reflecting-on-scaling-performance">Reflecting on scaling performance <a href="#reflecting-on-scaling-performance"></a></h2><p>There’s one more journey over this time that I’d like to share, and that’s a personal one. Most of my career prior to Amazon had been in either early startup or similarly small company cultures. I had built managed services, and even distributed systems out of necessity, but I had never worked on anything close to the scale of EBS, even the EBS of 2011, both in technology and organization size. I was used to solving problems by myself, or maybe with one or two other equally motivated engineers.</p><p>I really enjoy going super deep into problems and attacking them until they’re complete, but there was a pivotal moment when a colleague that I trusted pointed out that I was becoming a performance bottleneck for our organization. As an engineer who had grown to be an expert in the system, but also who cared really, really deeply about all aspects of EBS, I found myself on every escalation and also wanting to review every commit and every proposed design change. If we were going to be successful, then I had to learn how to scale myself–I wasn’t going to solve this with just ownership and bias for action.</p><p>This led to even more experimentation, but not in the code. I knew I was working with other smart folks, but I also needed to take a step back and think about how to make them effective. One of my favorite tools to come out of this was peer debugging. I remember a session with a handful of engineers in one of our lounge rooms, with code and a few terminals projected on a wall. One of the engineers exclaimed, “Uhhhh, there’s no way that’s right!” and we had found something that had been nagging us for a while. We had overlooked where and how we were locking updates to critical data structures. Our design didn’t usually cause issues, but occasionally we would see slow responses to requests, and fixing this removed one source of jitter. We don’t always use this technique, but the neat thing is that we are able to combine our shared systems knowledge when things get really tricky.</p><p>Through all of this, I realized that empowering people, giving them the ability to safely experiment, can often lead to results that are even better than what was expected. I’ve spent a large portion of my career since then focusing on ways to remove roadblocks, but leave the guardrails in place, pushing engineers out of their comfort zone. There’s a bit of psychology to engineering leadership that I hadn’t appreciated. I never expected that one of the most rewarding parts of my career would be encouraging and nurturing others, watching them own and solve problems, and most importantly celebrating the wins with them!</p><h2 id="conclusion">Conclusion <a href="#conclusion"></a></h2><p>Reflecting back on where we started, we knew we could do better, but we weren’t sure how much better. We chose to approach the problem, not as a big monolithic change, but as a series of incremental improvements over time. This allowed us to deliver customer value sooner, and course correct as we learned more about changing customer workloads. We’ve improved the shape of the EBS latency experience from one averaging more than 10 ms per IO operation to consistent sub-millisecond IO operations with our highest performing io2 Block Express volumes. We accomplished all this without taking the service offline to deliver a new architecture.</p><p>We know we’re not done. Our customers will always want more, and that challenge is what keeps us motivated to innovate and iterate.</p><ul><li><a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">Building and operating a pretty big storage system called S3</a></li><li><a href="https://www.allthingsdistributed.com/2023/11/standing-on-the-shoulders-of-giants-colm-on-constant-work.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">Reliability, constant work, and a good cup of coffee</a></li><li><a href="https://www.allthingsdistributed.com/2022/11/amazon-1998-distributed-computing-manifesto.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">The Distributed Computing Manifesto</a></li></ul></span></section><hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beastie Boys dismantled their gold record plaque,it didn't contain their music (115 pts)]]></title>
            <link>https://djmag.com/news/beastie-boys-dismantled-their-pauls-boutique-gold-record-plaque-find-it-didnt-contain-their</link>
            <guid>41319955</guid>
            <pubDate>Thu, 22 Aug 2024 13:20:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://djmag.com/news/beastie-boys-dismantled-their-pauls-boutique-gold-record-plaque-find-it-didnt-contain-their">https://djmag.com/news/beastie-boys-dismantled-their-pauls-boutique-gold-record-plaque-find-it-didnt-contain-their</a>, See on <a href="https://news.ycombinator.com/item?id=41319955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="text">
<p><span><span><span><span><span><span><span>Beastie Boys have shared the story of how they discovered that the gold record plaque they received after the certification of ‘Paul’s Boutique’ didn’t actually contain a pressing of their own music.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Ad-Rock (Adam Horovitz) and Mike D (Michael Diamond) from <a href="https://djmag.com/news/beastie-boys-announce-special-edition-check-your-head-30th-anniversary-reissue" rel="noopener" target="_blank">the New York hip-hop innovators</a> recently revealed the issue with their gold record plaque for the seminal 1989 album during an interview on the Conan O’Brien Needs a Friend podcast.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>The certification came when ‘Paul’s Boutique’ sold 500,000 copies just two months after its release. However, when the bandmates closely examined the plaque years later, they noticed that the record displayed inside did not contain the same amount of tracks as the album’s tracklisting.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>“So we’re at our studio here in California and I was smoking the pot,” Ad-Rock said, recalling the discovery. “This was a long time ago. We had a gold record on the wall, it was our record, ‘</span></span><span><span>Paul’s Boutique’</span></span><span><span>. I was looking at it and I could see it had our label and I could see that it had like nine songs on the one side. But I was looking at the actual gold record and it only had four songs on it.”</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Curious about the discrepancy, the band decided to break the glass and remove the record from the plaque. Upon inspection, they found that instead of ‘</span></span><span><span>Paul’s Boutique’,</span></span><span><span>&nbsp;the vinyl contained piano versions of songs like Barry Manilow’s music and Morris Albert’s ‘Feelings’.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>When asked by O’Brien if this was common for all gold record plaques, Ad-Rock replied, “I don’t know about anybody else...” while Mike D speculated that major stars like Barbra Streisand or Donna Summer probably received plaques with their actual records.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>While the plaque didn’t feature their music, '</span></span><span><span>Paul’s Boutique'</span></span><span><span> continued to gain recognition after its gold certification. The album was certified platinum in 1995 and reached double platinum status in 1999.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Listen to the podcast below.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Revisit Ben Cardew’s </span></span></span></span></span></span></span><span><span><span><span><span><span><span>Solid Gold</span></span><span><span> feature on how Beastie Boys' cult classic second album was a high-water mark <a href="https://djmag.com/features/solid-gold-beastie-boys-pauls-boutique" rel="noopener" target="_blank">for rich sampling and undiluted fun</a>.</span></span></span></span></span></span></span></p>
<p>From May, <a href="https://djmag.com/features/how-beastie-boys-ill-communication-set-benchmark-90s-eclecticism" rel="noopener" target="_blank">read DJ Mag’s feature</a> on how the legendary band's fourth album, ‘<span><span>Ill Communication’,&nbsp;</span></span><span><span>set a benchmark for ’90s eclecticism.</span></span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Async2 – The .NET Runtime Async experiment concludes (137 pts)]]></title>
            <link>https://steven-giesel.com/blogPost/59752c38-9c99-4641-9853-9cfa97bb2d29</link>
            <guid>41319224</guid>
            <pubDate>Thu, 22 Aug 2024 11:52:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://steven-giesel.com/blogPost/59752c38-9c99-4641-9853-9cfa97bb2d29">https://steven-giesel.com/blogPost/59752c38-9c99-4641-9853-9cfa97bb2d29</a>, See on <a href="https://news.ycombinator.com/item?id=41319224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div b-gj8cwnhyng=""><p>The .NET team has been working on a new experiment called async2, which is a new implementation of the async/await pattern that is designed to be more efficient and more flexible than the current implementation. It started with green threads and ended with an experiment that moves <code>async</code> and <code>await</code> to the runtime. This post will cover the journey of <code>async2</code> and the conclusion of the experiment.</p>
<h2 id="where-all-began-green-threads">Where all began - Green threads</h2>
<p>Let's start here: <a href="https://github.com/dotnet/runtimelab/issues/2398"><em>"Green Thread Experiment Results"</em></a>. The team invested in an experiment to evaluate the feasibility of using green threads in the .NET runtime. But wait a second, what are green threads?</p>
<h3 id="green-threads">Green threads</h3>
<p>Green threads are user-space threads that are managed by a runtime library or a virtual machine (VM) instead of the operating system.<sup>Source: <a href="https://en.wikipedia.org/wiki/Green_thread">Wikipedia</a></sup> They are lightweight and can be created and managed more quickly than kernel threads. Green threads are also known as "coroutines" or "fibers" in other programming languages. The idea is that you, as a developer, don't have to worry about threads.</p>
<p>Currently, with threads and to some extend with <code>async</code>/<code>await</code>, a new stack is created. You can easily see that in your favourite IDE, if you debug:</p>
<p><img src="https://linkdotnetblog.azureedge.net/blog/20240812_Async2/stacks.webp" alt="stack"></p>
<p>Green threads are different. <a href="https://stackoverflow.com/a/19098856/1892523">The memory of a green thread is allocated on the heap</a>. But all of this comes with a cost: As they aren't managed by the OS, they can't take advantage of multiple cores inherently. But for I/O-bound operations, they are a good fit.</p>
<h3 id="abandoning-green-threads">Abandoning green threads</h3>
<p>The key-challenges were (which lead to the abandonment of the green threads experiment):</p>
<ul>
<li>Complex interaction between green threads and existing async model</li>
<li>Interop with native code was complex and slower than using regular threads</li>
<li>Compatibility issues with security mitigations like shadow stacks</li>
<li>Uncertainty about whether it would be possible to make green threads faster than async in important scenarios, given the effort required for improvement.</li>
</ul>
<p>This lead to the conclusion that green threads are not the right way to go for the .NET runtime and gave birth to the <code>async2</code> experiment. From here on out, I will keep the term <code>async2</code> for the experiment, as it is the codename for the experiment.</p>
<h2 id="async2-the.net-runtime-async-experiment"><code>async2</code> - The .NET Runtime Async experiment</h2>
<p>Now, <code>async2</code> is obviously only a codename. The goal of the experiment was to move <code>async</code> and <code>await</code> to the runtime. The main motivation behind this was to make <code>async</code> more efficient and more flexible. As <code>async</code> is already used as an identifier in C#, the team decided to use <code>async2</code> as a codename for the experiment. <strong>If</strong> that thing ever makes it into the runtime, it will be called <code>async</code> - so it will be a replacement for the current <code>async</code> implementation. But let's start at the beginning.</p>
<h3 id="async-is-a-compiler-feature"><code>async</code> is a compiler feature</h3>
<p>I talked about this from time to time in my blog posts. For example:</p>
<ul>
<li><a href="https://steven-giesel.com/blogPost/720a48fd-0abe-4c32-83ac-26926d501895/the-state-machine-in-c-with-asyncawait"><em>"The state machine in C# with async/await"</em></a></li>
<li><a href="https://steven-giesel.com/blogPost/69dc05d1-9c8a-4002-9d0a-faf4d2375bce/c-lowering"><em>"C# Lowering"</em></a></li>
</ul>
<p>The current implementation of <code>async</code> and <code>await</code> is a compiler feature. The compiler generates a state machine for the <code>async</code> method. The runtime doesn't know anything about <code>async</code> and <code>await</code>. There is no trace of an <code>async</code>-like keyword in IL or in the JIT-compiled code. And that is where the experiment started.</p>
<p>Starting point is this nice GitHub issue: <a href="https://github.com/dotnet/runtime/issues/94620">"<em>.NET 9 Runtime Async Experiment</em>"</a>, which basically describes the whole experiment in more detail with an ongoing discussion from the community.</p>
<h3 id="async-is-a-runtime-feature"><code>async</code> is a runtime feature</h3>
<p>The goal of the experiment was to move <code>async</code> and <code>await</code> to the runtime. This would allow the runtime to have more control over the pattern itself. With that there would be also some different semantics:</p>
<h3 id="async2-and-executioncontext-and-synchronizationcontext"><code>async2</code> and <code>ExecutionContext</code> and <code>SynchronizationContext</code></h3>
<p><code>async2</code> would not have save or restore of <code>SynchronizationContext</code> and <code>ExecutionContext</code> at function boundaries, instead allowing callers to observe changes. With the <code>ExecutionContext</code>, this would shift a big change in how <code>AsyncLocal</code> behaves.</p>
<p>Today, <code>AsyncLocal</code> is used to store data that flows with the logical call context. It gets copied to the new context. That said, if a function deep down the call stack changes the value of an <code>AsyncLocal</code>, the caller will <strong>not</strong> see the updated value, only functions further down the logical async flow. Here an example:</p>
<pre><code>await new AsyncLocalTest().DoOuter();

public class AsyncLocalTest
{
    private readonly AsyncLocal&lt;string&gt; _asyncLocal = new();

    public async Task DoOuter()
    {
        _asyncLocal.Value = "Outer";
        Console.WriteLine($"DoOuter: {_asyncLocal.Value}");
        await DoInner();
        Console.WriteLine($"DoOuter: {_asyncLocal.Value}");
    }

    private async Task DoInner()
    {
        _asyncLocal.Value = "Inner";
        Console.WriteLine($"DoInner: {_asyncLocal.Value}");
        await Task.Yield();
        Console.WriteLine($"DoInner: {_asyncLocal.Value}");
    }
}
</code></pre>
<p>The output of this code is:</p>
<pre><code>DoOuter: Outer
DoInner: Inner
DoInner: Inner
DoOuter: Outer
</code></pre>
<p>With <code>async2</code> those changes are not "reverted" which would lead to a different output:</p>
<pre><code>DoOuter: Outer
DoInner: Inner
DoInner: Inner
DoOuter: Inner
</code></pre>
<h3 id="comparison-with-the-current-implementation-and-some-results">Comparison with the current implementation and some results</h3>
<p>The whole document, that describes all the details, can be found here: <a href="https://github.com/dotnet/runtimelab/blob/feature/async2-experiment/docs/design/features/runtime-handled-tasks.md">https://github.com/dotnet/runtimelab/blob/feature/async2-experiment/docs/design/features/runtime-handled-tasks.md</a></p>
<p>The team found out that the approach of putting <code>async</code> into the JIT might yield the best results overall. Here a basic overview:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th><code>async</code></th>
<th><code>async2</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Performance</td>
<td>Generally slower than <code>async2</code>, especially for deep call stacks</td>
<td>Generally faster than <code>async</code>, with performance comparable to synchronous code in non-suspended scenarios</td>
</tr>
<tr>
<td>Exception Handling</td>
<td>Slow and inefficient, causing GC pauses and impacting responsive performance of applications</td>
<td>Improved EH handling, reducing the impact on application responsiveness</td>
</tr>
<tr>
<td>Stack Depth Limitation</td>
<td>Limited by stack depth, which can cause issues for deep call stacks</td>
<td>No explicit limitations on stack depth, allowing <code>async2</code> to handle deeper call stacks more efficiently</td>
</tr>
<tr>
<td>Memory Consumption</td>
<td>Generally lower than <code>async2</code>, especially in scenarios with many suspended tasks</td>
<td>Higher memory consumption due to capturing entire stack frames and registers, but still acceptable compared to other factors like pause times</td>
</tr>
</tbody>
</table>
<h2 id="where-do-we-go-from-here">Where do we go from here?</h2>
<p>As the name suggests, this is just an experiment, that may lead to a replacement of <code>async</code> <strong>in some years</strong>. Yes, <strong>years</strong>. It might take a while until this is production-ready. And for the transition phase, there has to be interop for <code>async</code> ↔ <code>async2</code>. Anyway - a very good starting point and I am looking forward to the future of <code>async2</code>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electric Clojure v3: Differential Dataflow for UI [video] (107 pts)]]></title>
            <link>https://hyperfiddle-docs.notion.site/Talk-Electric-Clojure-v3-Differential-Dataflow-for-UI-Getz-2024-2e611cebd73f45dc8cc97c499b3aa8b8</link>
            <guid>41319003</guid>
            <pubDate>Thu, 22 Aug 2024 11:24:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hyperfiddle-docs.notion.site/Talk-Electric-Clojure-v3-Differential-Dataflow-for-UI-Getz-2024-2e611cebd73f45dc8cc97c499b3aa8b8">https://hyperfiddle-docs.notion.site/Talk-Electric-Clojure-v3-Differential-Dataflow-for-UI-Getz-2024-2e611cebd73f45dc8cc97c499b3aa8b8</a>, See on <a href="https://news.ycombinator.com/item?id=41319003">Hacker News</a></p>
Couldn't get https://hyperfiddle-docs.notion.site/Talk-Electric-Clojure-v3-Differential-Dataflow-for-UI-Getz-2024-2e611cebd73f45dc8cc97c499b3aa8b8: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[No "Hello", No "Quick Call", and No Meetings Without an Agenda (222 pts)]]></title>
            <link>https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/</link>
            <guid>41318408</guid>
            <pubDate>Thu, 22 Aug 2024 09:46:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/">https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/</a>, See on <a href="https://news.ycombinator.com/item?id=41318408">Hacker News</a></p>
Couldn't get https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>