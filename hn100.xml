<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 05 Mar 2025 09:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[uBlock Origin Forcefully Removed by Chrome (162 pts)]]></title>
            <link>https://old.reddit.com/r/youtube/comments/1j2ec76/ublock_origin_is_gone/</link>
            <guid>43262531</guid>
            <pubDate>Wed, 05 Mar 2025 03:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/youtube/comments/1j2ec76/ublock_origin_is_gone/">https://old.reddit.com/r/youtube/comments/1j2ec76/ublock_origin_is_gone/</a>, See on <a href="https://news.ycombinator.com/item?id=43262531">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/youtube/comments/1j2ec76/ublock_origin_is_gone/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[We're Charging Our Cars Wrong (138 pts)]]></title>
            <link>https://spectrum.ieee.org/ev-charging-2671242103</link>
            <guid>43262468</guid>
            <pubDate>Wed, 05 Mar 2025 03:45:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/ev-charging-2671242103">https://spectrum.ieee.org/ev-charging-2671242103</a>, See on <a href="https://news.ycombinator.com/item?id=43262468">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>A three-port public electric-vehicle charging station, such as this one operated by Electrify America, in Nebraska, can cost as much as half a million dollars.</p><div data-headline="We’re Charging Our Cars Wrong"><p><strong>If there’s one thing</strong> we could do now to hasten the transition to <a href="https://spectrum.ieee.org/search/?q=electric+vehicles" target="_self">electric vehicles</a>, it’s this: Build a robust public EV-charging infrastructure. While the media has focused on vehicle performance and range, consumers have always been clear that they want <a href="https://spectrum.ieee.org/tag/electric-cars">electric cars</a> to do essentially everything their old vehicles do—including long overnight trips.
</p><p>
	To those who don’t yet own an EV, a robust infrastructure may seem unimportant. Studies, after all, show that in developed markets, as much as 
	<a href="https://iea.blob.core.windows.net/assets/d560f6a6-8d40-4e63-aed2-4fc93139dd5c/PolicybriefonpubliccharginginfrastructurePromotingsuccessfulroll-outstrategiesandbusinessmodels.pdf" rel="noopener noreferrer" target="_blank">90 percent</a> of all charging takes place in the home. It turns out, however, that the remaining percentage of charging is critically important. Drivers of delivery trucks and <a href="https://spectrum.ieee.org/tag/taxis">taxis</a>, residents of apartment buildings, students on their way to college, families on vacation, and countless others have learned that driving an EV can be a struggle where public charging is scarce or unreliable. A 2022 <a href="https://www.forbes.com/wheels/features/ev-range-cost-confidence-survey/" rel="noopener noreferrer" target="_blank">survey</a> by <em><em><a href="https://spectrum.ieee.org/tag/forbes">Forbes</a></em></em>, for example, indicated that 62 percent of EV owners were so anxious about EV range that they had at times curtailed their travel plans.
</p><p>
	This is no secret to policymakers. A 
	<a href="https://iea.blob.core.windows.net/assets/d560f6a6-8d40-4e63-aed2-4fc93139dd5c/PolicybriefonpubliccharginginfrastructurePromotingsuccessfulroll-outstrategiesandbusinessmodels.pdf" rel="noopener noreferrer" target="_blank">recent brief</a> from the <a href="https://spectrum.ieee.org/tag/international-energy-agency">International Energy Agency</a> indicates that in <a href="https://spectrum.ieee.org/tag/China">China</a>, investing in charging infrastructure is considered four times as effective for EV success as providing subsidies to EV buyers.
</p><p>
	These are issues we’ve been grappling with for decades. Back in 1992, we cofounded 
	<a href="https://www.acpropulsion.com/" rel="noopener noreferrer" target="_blank">AC Propulsion</a>, which offered the <a href="https://en.wikipedia.org/wiki/AC_Propulsion_tzero" rel="noopener noreferrer" target="_blank">tZero</a>, a high-performance electric sports car whose basic technologies and design were later incorporated into the original <a href="https://spectrum.ieee.org/tag/tesla">Tesla</a> Roadster. In the years since, we’ve thought a lot about how to make vehicles that people actually want to own and drive.
</p><p data-rm-resized-container="25%"><img alt="An open-top yellow roadster is parked along the side of a bridge roadway." data-rm-shortcode-id="08a84e26ca7fef92ff01b32c98266755" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-open-top-yellow-roadster-is-parked-along-the-side-of-a-bridge-roadway.png?id=56626390&amp;width=980" height="1721" id="ca88d" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-open-top-yellow-roadster-is-parked-along-the-side-of-a-bridge-roadway.png?id=56626390&amp;width=980" width="3279"><small placeholder="Add Photo Caption...">The 1997 AC Propulsion TZero was a groundbreaking <a href="https://spectrum.ieee.org/tag/electric-vehicle">electric vehicle</a> featuring technical innovations that were later incorporated into the Tesla Roadster.</small><small placeholder="Add Photo Credit..."><a href="https://commons.wikimedia.org/wiki/File:Tzero_-_The_first_Tesla_vehicle.jpg" rel="noopener noreferrer" target="_blank">PeteGruber/Wikipedia</a></small></p><p>
	When we’ve asked potential EV owners what’s limiting <a href="https://spectrum.ieee.org/collections/the-ev-transition-explained/">EV adoption</a>, they often point to limited access to charging stations—especially to fast public charging. The operators who own these <a href="https://spectrum.ieee.org/tag/charging-stations">charging stations</a> have said it as well, and they also cite the high cost of equipment—a DC fast-charging station with four ports can cost between 
	<a href="https://spectrum.ieee.org/the-ev-transition-explained-2658463735" target="_self">US $470,000 and $725,000</a>. If equipment costs were lower, they say, they would install more recharging stations. It could be a virtuous circle: The recharge businesses would do better, EV owners would benefit, and more people would consider buying an EV.
</p><p>
	The question is, can <a href="https://spectrum.ieee.org/tag/ev-charging">EV charging</a> be done more economically and efficiently? More specifically, is there a way to reduce recharge station complexity and bring down the high cost of fast-charge stations—and, in so doing, significantly boost EV penetration without sacrificing safety?
</p><p>
	The answer is yes, and here’s why.
</p><h2> How EV charging works</h2><p>
	Before we explain our solution, let’s review some fundamentals, starting with the most basic. A charging station is a physical location that has one or more charging ports, each of which can charge a single EV. Each port may have multiple types of service connectors to support 
	<a href="https://spectrum.ieee.org/ev-charging-adapters" target="_self">different EV standards</a>.
</p><p>
	The function of the port is to convert AC power from the grid into DC, which is then applied to the battery. The recharge current must be controlled so that the following criteria are met at all times: The voltage of the battery cells must not exceed a critical limit; cell temperatures must not exceed a preset threshold; and current drawn from the <a href="https://spectrum.ieee.org/tag/electric-utility">electric utility</a> must remain below a certain value. If the first two are not met, cells may be damaged or catch fire. If the third is not met, the charger or utility may be overloaded, causing a breaker to trip or a fuse to blow.
</p><p><img alt="An illustration showing the steps of a process.  " data-rm-shortcode-id="de29d500af34098dd46136aa72868666" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-illustration-showing-the-steps-of-a-process.png?id=56630537&amp;width=980" height="3469" id="14845" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-illustration-showing-the-steps-of-a-process.png?id=56630537&amp;width=980" width="3039"><small placeholder="Add Photo Caption...">A key safety feature of existing EV chargers is an isolation link [in teal]. Within this circuit, a high-frequency transformer provides physical separation between grid power and the electric vehicle’s battery. The isolation link is inside the vehicle’s onboard charger for Level-2 charging (top). For Level-3, or <a href="https://spectrum.ieee.org/tag/fast-charging">fast, charging</a>, the link is located inside the charging station (bottom). </small><small placeholder="Add Photo Credit...">Chris Philpot</small></p><p>
	In addition to these requirements, the charger must protect users from <a href="https://spectrum.ieee.org/tag/electric-shock">electric shock</a>. That’s not always easy. Chargers operate in rugged environments, usually outdoors, with greatly varying levels of humidity and where contaminated water may be present. Equipment may also be damaged or even sabotaged.
</p><p>
	The time-tested way to prevent electric shock is to use electrical grounding. Grounding is exactly what it sounds like: a direct physical connection to the earth that provides a path for electric current. When such a path is present, stray electrical currents—in a chassis, for example—travel directly to the ground, avoiding any people who might be standing close by. In an <a href="https://spectrum.ieee.org/tag/electric-car">electric car</a> that’s charging, the green ground wire in the charging cable becomes the path to ground. (Because an electric car has rubber tires, the car itself can’t serve as a path.)
</p><p>What happens if such a path is not present? If the ground connection in an electric car charger is broken or compromised, the charge port must have a backup solution. Today, that solution is something called galvanic isolation. In galvanic isolation, no direct conduction path is permitted between certain sections of an electrical system.</p><p><img alt="An series of illustration showing a shock hazard and how to prevent a shock hazard" data-rm-shortcode-id="3d841c9e4b5bc289855335b81e917e84" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/an-series-of-illustration-showing-a-shock-hazard-and-how-to-prevent-a-shock-hazard.png?id=56630539&amp;width=980" height="3491" id="f8928" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/an-series-of-illustration-showing-a-shock-hazard-and-how-to-prevent-a-shock-hazard.png?id=56630539&amp;width=980" width="3144"><small placeholder="Add Photo Caption..."> If an <a href="https://spectrum.ieee.org/tag/ev-charger">EV charger</a> does not have an isolation link, and the ground circuit is broken and if a current path exists between the battery and the vehicle body, a person touching the vehicle could receive a potentially deadly electric shock [top illustration]. However, with the simple and inexpensive “double ground” circuit designed by Wally Rippel [bottom illustration, in teal], a detector circuit confirms that the ground is intact before closing contactors that enable current to flow. </small><small placeholder="Add Photo Credit...">Chris Philpot</small></p><p><span>The hardware for a charger’s galvanic isolation is called an isolation link, and it works by physically and electrically separating two circuits, so that a difference in potential won’t result in current flow from one circuit to the other. In the case of EV charging, the two circuits are the 
	</span><a href="https://spectrum.ieee.org/search/?q=electric+grid" target="_self">electric grid</a><span> on the one hand, and the vehicle battery and its associated circuitry on the other.</span></p><p>
	This isolation can be a literal lifesaver. Suppose an EV’s battery is leaking. The leaked fluid is conductive, and can therefore produce a current path between the battery circuit and the vehicle chassis. If the ground circuit happens to be broken, then, without isolation, the vehicle’s chassis would be at a <a href="https://spectrum.ieee.org/tag/high-voltage">high voltage</a>. So a person touching the car while standing on the ground could receive a potentially lethal electric shock (see illustration, “A shock hazard”). With isolation, there wouldn’t be a shock hazard, because no current path would exist from the electric utility to the car body.
</p><p>
	 Only one component exists that can provide separation between two circuits while transmitting kilowatt levels of power—a transformer. 
	<a href="https://spectrum.ieee.org/transformer-shortage" target="_self">The transformers</a> that connect directly to low-frequency utility power are heavy and bulky. But for EV charging, where weight and size are critical, the <a href="https://spectrum.ieee.org/tag/transformers">transformers</a> are much smaller—they’re not even half the size of a standard building brick. That’s because the charging stations convert DC power to high-frequency AC, using an inverter. The high-frequency AC is then applied to the small transformer, which provides the galvanic isolation. Finally, the output of the transformer is changed back to DC by a high-frequency rectifier circuit, completing the process (as shown in the “isolation link...” illustration).
</p><p>
	We’ll get into the details of this 
	<a href="https://spectrum.ieee.org/silicon-carbide" target="_self">power conversion</a> in the next section, but this gives you an idea of how charging is done safely today, whether at a public charger or in a home garage by means of the car’s onboard charger.
</p><h2>Galvanic isolation costs a lot</h2><p>
	Virtually every EV has an onboard charger (OBC), which performs the AC-to-DC conversion function, like a public fast charger does, when the vehicle is charging at home. As its name suggests, the OBC resides in the vehicle. It’s capable of providing power levels from about 5 to 22 kilowatts to the battery, depending on the vehicle make and model. Such charge rates are low in comparison with fast charging, generally only available at 
	<a href="https://spectrum.ieee.org/porsche-claims-it-can-double-teslas-fastcharging-rate" target="_self">public chargers</a>, which starts at 50 kW and can go up to 350 kW.
</p><p>
	Today, all chargers—onboard and off-board—are galvanically isolated. The galvanic isolation is integrated into the power-conversion hardware, regardless of whether it’s in the car or in a public charger.
</p><p><span>A single 300-kW port in a public charging station includes about US $90,000 of <a href="https://spectrum.ieee.org/tag/power-electronics">power electronics</a>, of which about $54,000 is for the isolation link.</span></p><p>
	The 
	<a href="https://www.embitel.com/blog/embedded-blog/power-converter-topologies-for-electric-charging-stations" rel="noopener noreferrer" target="_blank">hardware of an EV charger</a> is basically a much larger and higher-power version of the <a href="https://spectrum.ieee.org/tag/switching-power-supplies">switching power supplies</a> that charge your smartphone or laptop. Earlier, we gave a basic idea about how power conversion in an EV works, but it’s actually a little more involved than that. For <a href="https://spectrum.ieee.org/tag/evs">EVs</a>, power conversion occurs in four stages (illustration, “A shock hazard”). In the first stage, AC power, either single-phase or three-phase, is converted to DC by an active rectifier. In the second, DC power from the first stage is converted to a high-frequency AC square wave (think of a classic sine wave but with a square shape rather than, well, a sinuous one) by a circuit known as an inverter. The reason for this high frequency is that in the third stage, a transformer converts the AC to a different voltage, and the high frequency allows this transformer to be much smaller and lighter than it would be for a lower frequency, like that of the <a href="https://spectrum.ieee.org/tag/power-grid">power grid</a>. Finally, at the fourth stage, a high-frequency rectifier converts the high-frequency AC back to DC, and then sends it to the vehicle’s battery. Collectively, stages two, three, and four make up the isolation link, which provides the galvanic isolation (see illustration, “The isolation link separates utility power from the EV battery”).
</p><p>
	This isolation link is very expensive. It accounts for roughly 60 percent of the cost of the power electronics in a typical EV, and it’s also responsible for about 50 percent of the charger’s power loss. We estimate that the cost of the bill of materials and assembly of a galvanically isolated charging port is about $300 per kilowatt. So a single 300-kW port in a public charging station includes about $90,000 of power electronics, of which about $54,000 is for the isolation link.
</p><p>
	Do the math: A charging station with four ports includes approximately $360,000 in 
	<a href="https://spectrum.ieee.org/search/?q=power+electronics" target="_self">power electronics</a>, with more than $200,000 of that going for galvanic isolation. To get an idea of the total costs in a country, say the <a href="https://spectrum.ieee.org/tag/united-states">United States</a>, multiply that 60 percent cost reduction of the power electronics per charger by the multiple ports at the more than 61,000 public EV-charging stations in the United States.
</p><p>
	For an EV’s onboard charger, the isolation link adds not just cost but also bulk. The higher the charge capability, the greater the cost and size of the isolation system. That’s why you could never do fast charging with an OBC—the cost and size would be too great to include it inside the vehicle.
</p><p>
	These are among the main reasons why we propose to eliminate galvanic isolation. Billions of dollars of capital and energy expenses could be saved. <a href="https://spectrum.ieee.org/tag/hardware-reliability">Hardware reliability</a> would improve because the chargers would use about half as many components. Eliminating galvanic isolation—that is to say, eliminating stages two, three, and four of the charger hardware—would also greatly reduce the size of onboard chargers and enable them to handle fast charging, also known as Level 3 power. This is the highest charging level, providing 100 kW or more of DC current.
</p><p><img alt="A black sports car is seen cruising by a retaining wall." data-rm-shortcode-id="708449a4fbc64363a63171d006ddfc87" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-black-sports-car-is-seen-cruising-by-a-retaining-wall.png?id=56626397&amp;width=980" height="936" id="98a7f" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-black-sports-car-is-seen-cruising-by-a-retaining-wall.png?id=56626397&amp;width=980" width="2066"><small placeholder="Add Photo Caption..."><a href="https://spectrum.ieee.org/tag/tesla-motors">Tesla Motors</a> unveiled its electric Roadster in Santa Monica in 2006.</small><small placeholder="Add Photo Credit...">Glenn Koenig/Los Angeles Times/Getty Images</small></p><p>
	With the isolation link eliminated, we could then take the next step: having the vehicle’s onboard inverter supply power to the motor for driving and also to the <a href="https://spectrum.ieee.org/tag/batteries">batteries</a> for charging. By having the car’s inverter do double duty, we would cut the remaining costs by half 
	<em><em>again</em></em>.
</p><p>
	None of this is a new idea. The original Tesla Roadster, which reached the market in 2008, and all of the products built by AC Propulsion successfully used non-galvanically isolated, integrated charging, in which the recharge function was carried out by the inverter. In those AC Propulsion vehicles, the nominal battery voltage was approximately 400 volts <a href="https://spectrum.ieee.org/tag/direct-current">direct current</a>, just as it is in most EVs today.
</p><h2>Can galvanic isolation be eliminated?</h2><p>
	The requirements for eliminating the isolation link are not terribly complex or costly. Two issues in particular need to be addressed: the risk of 
	<a href="https://spectrum.ieee.org/ev-safety" target="_self">electric shock</a> and the compatibility between the utility and battery voltages.
</p><p>
	First, let’s look at the shock hazard. <a href="https://spectrum.ieee.org/tag/electrocution">Electrocution</a> can occur if three conditions exist simultaneously: The vehicle isn’t grounded, power is applied to the ungrounded vehicle, and a current-leakage path has formed (see illustration, “A shock hazard”). A leakage path might be created if, for example, the battery’s electrolyte has begun leaking, forming a path between the battery and the car body. Because all EV charging systems include a ground connection, a leakage path is a problem only if the ground connection is broken or compromised.
</p><p>
	All charging systems, both onboard and off-board, include components called 
	<a href="https://www.wolfspeed.com/knowledge-center/article/whats-under-the-hood-contactors-implement-ev-safety/" rel="noopener noreferrer" target="_blank">safety contactors</a>, which apply power to the battery only after various electronic checks have been carried out. These checks include ground verification, which tests whether the ground connection is intact. If the ground connection is missing or faulty, charging power won’t be applied to the battery.
</p><p>
	For Level 2 charging—in a home garage, for example—the safety contactors are located in a module called the 
	<a href="https://www.benderinc.com/solutions/electric-vehicles/charging-stations-evse/" rel="noopener noreferrer" target="_blank">electric vehicle supply equipment</a>. The <a href="https://www.evconnect.com/blog/what-is-evse" rel="noopener noreferrer" target="_blank">EVSE</a> is typically the size of a large shoebox and may be mounted on a wall or a post. In the case of public fast charging, the safety contactors are an integral part of the hardware.
</p><p>
	What this means is that removing galvanic isolation won’t pose a shock hazard. If the vehicle is grounded and leakage causes the vehicle chassis to be at a high voltage, the resulting surge of current to ground will instantly trip breakers in the charger.
</p><p>
	So the question then becomes: Can ground verification be trusted to be absolutely fail-safe? In other words, can we guarantee that power is never applied if the ground circuit is broken or compromised—even if components within the ground verification circuit have failed? Such an absolute guarantee is necessary from both moral and legal standpoints. Removing an existing safety factor, such as galvanic isolation, is unacceptable unless it is replaced by something that provides a net gain in safety.
</p><p>
	We can do that. All it would take would be a relatively simple modification of the charger circuit.
</p><p>
	Such a level of safety can be provided by a double-ground combined with ground-continuity detection (see illustration, “A ‘double-ground’ circuit prevents shock”). This double-ground method is based on—you guessed it—two ground wires. With this scheme, if one ground wire is severed, the other one ensures that the vehicle is still grounded. To further enhance safety, the broken ground would be detected and the power shut down, even if one ground wire was still intact.
</p><p><a href="https://www.saferack.com/product/vehicle-grounding/grounding-verification-monitors/" rel="noopener noreferrer" target="_blank">Detection of ground</a>-wire continuity is neither expensive nor complicated. One of us (Rippel) developed a prototype detection circuit about a year ago. The system uses two small transformers, one to inject a signal into one of the ground wires, and the other to detect the signal in the second ground wire. If the signal is not detected by the second transformer, the contactors—in the EVSE, for example—are opened so they can’t apply power. With this circuit, the overall system remains fail-safe in the event that one or more components fail.
</p><p>
	The arrangement makes charging doubly safe, literally. Moreover, because the two ground circuits are mutually independent, no single failure can cause both grounds to fail. This lowers the probability of a ground failure: If the probability of a single ground failure is 
	<em><em>P</em></em>, the probability of both failing is <em><em>P</em></em><span>2</span>. Safety is further improved with the addition of a circuit that senses that the two grounds form a complete circuit; power is turned off as soon as one of the two grounds is damaged or broken.
</p><p>
	Eliminating the risk of electric shock isn’t the only issue that we must deal with if we are to get rid of galvanic isolation. There’s also the issue of voltage—specifically, the need to prevent mismatches between the utility’s AC line voltage and that of the EV battery.
</p><p>
	A voltage mismatch becomes a problem under one condition—when the input utility voltage exceeds the battery voltage. If this occurs, even for an instant, uncontrolled current can flow into the battery, possibly damaging it or causing a breaker to trip.
</p><p>
	The solution to this problem is a device called a 
	<a href="https://www.digikey.com/en/maker/tutorials/2024/how-do-buck-converters-work" target="_blank">buck regulator</a> (or buck converter). A buck regulator is similar, functionally, to a step-down transformer, except that it handles DC current rather than AC. In the event that the utility’s AC voltage exceeds the battery voltage, the buck regulator operates like a transformer and steps it down. In comparison with an isolation link of the same power rating, a buck regulator would cost less than 10 percent and the power loss would be less than 20 percent.
</p><h2>The future of public EV charging</h2><p>
	At this point, we hope you appreciate why the existing four-stage scheme for both onboard and public EV charging is unnecessarily complicated and expensive. Three of the four stages can be completely eliminated. This would leave a single active-rectifier stage, followed, if necessary, by a low-cost buck regulator. To enhance safety to levels as high as if not higher than existing EV charging gear, we would add a double ground with ground-continuity detection. We call this improved approach direct power conversion.
</p><p>
	Using the DPC approach could cut equipment costs by more than half while improving <a href="https://spectrum.ieee.org/tag/energy-efficiency">energy efficiency</a> by two to three percent. That’s precisely what we need at this stage of the EV revolution, because it would make EV charging stations more affordable for operators, and enable thousands more such sites to be built in just a few years, rather than a decade or more. It would also make EVs more attractive to people who’ve resisted buying an EV because they’re put off by the 
	<a href="https://www.hbs.edu/bigs/the-state-of-ev-charging-in-america" target="_blank">feeble state of the charging infrastructure</a>.
</p><p>
	It’s time to simplify the EV recharging process and make it more cost effective. But that surely won’t happen without a discussion of galvanic isolation in the technical community. So let the discussion begin! We’re convinced that eliminating the isolation link should be the first step toward the robust charging infrastructure that 
	<a href="https://spectrum.ieee.org/collections/the-ev-transition-explained/" target="_self">the EV transition</a> so desperately needs. <span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Demoralization is just Beginning (245 pts)]]></title>
            <link>https://geohot.github.io//blog/jekyll/update/2025/03/03/demoralization-is-just-beginning.html</link>
            <guid>43261941</guid>
            <pubDate>Wed, 05 Mar 2025 02:35:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://geohot.github.io//blog/jekyll/update/2025/03/03/demoralization-is-just-beginning.html">https://geohot.github.io//blog/jekyll/update/2025/03/03/demoralization-is-just-beginning.html</a>, See on <a href="https://news.ycombinator.com/item?id=43261941">Hacker News</a></p>
Couldn't get https://geohot.github.io//blog/jekyll/update/2025/03/03/demoralization-is-just-beginning.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Brother accused of locking down third-party printer ink cartridges (266 pts)]]></title>
            <link>https://www.tomshardware.com/peripherals/printers/brother-accused-of-locking-down-third-party-printer-ink-cartridges-via-firmware-updates-removing-older-firmware-versions-from-support-portals</link>
            <guid>43261933</guid>
            <pubDate>Wed, 05 Mar 2025 02:34:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/peripherals/printers/brother-accused-of-locking-down-third-party-printer-ink-cartridges-via-firmware-updates-removing-older-firmware-versions-from-support-portals">https://www.tomshardware.com/peripherals/printers/brother-accused-of-locking-down-third-party-printer-ink-cartridges-via-firmware-updates-removing-older-firmware-versions-from-support-portals</a>, See on <a href="https://news.ycombinator.com/item?id=43261933">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>Fabled RepairTuber and right to repair crusader Louis Rossmann has shared a new video encapsulating his surprise, and disappointment, that Brother has morphed into an “anti-consumer printer company.” More information about Brother’s embrace of the dark side are shared on Rossmann’s <a data-analytics-id="inline-link" href="https://wiki.rossmanngroup.com/wiki/Brother_ink_lockout_%26_quality_sabotage" data-url="https://wiki.rossmanngroup.com/wiki/Brother_ink_lockout_%26_quality_sabotage" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">wiki</a>, with the major two issues being new firmware disabling third party toner, and preventing (on color devices) color registration functionality.</p><div data-nosnippet="">
<div>
<p><span>Brother turns heel &amp; becomes anti-consumer printer company 😢 😢 😢 - YouTube</span>
<img src="https://img.youtube.com/vi/bpHX_9fHNqE/maxresdefault.jpg" alt="Brother turns heel &amp; becomes anti-consumer printer company 😢 😢 😢 - YouTube" data-aspect-ratio="16/9" loading="lazy">
</p>
</div>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 234.67 165.33"><path fill="red" d="M229.763 25.817c-2.699-10.162-10.65-18.165-20.748-20.881C190.716 0 117.333 0 117.333 0S43.951 0 25.651 4.936C15.553 7.652 7.6 15.655 4.903 25.817 0 44.236 0 82.667 0 82.667s0 38.429 4.903 56.85C7.6 149.68 15.553 157.681 25.65 160.4c18.3 4.934 91.682 4.934 91.682 4.934s73.383 0 91.682-4.934c10.098-2.718 18.049-10.72 20.748-20.882 4.904-18.421 4.904-56.85 4.904-56.85s0-38.431-4.904-56.85"></path><path fill="#fff" d="m93.333 117.559 61.333-34.89-61.333-34.894z"></path></svg>
<a href="https://youtu.be/bpHX_9fHNqE" target="_blank" data-url="https://youtu.be/bpHX_9fHNqE" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Watch On <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 507.9 113.39"><g fill="#fff"><path d="M64.792 80.99V32.396l42.082 24.297zm93.803-63.285a20.285 20.285 0 0 0-14.32-14.32C131.642 0 80.99 0 80.99 0S30.337 0 17.705 3.385a20.286 20.286 0 0 0-14.32 14.32C0 30.338 0 56.693 0 56.693S0 83.049 3.385 95.68A20.285 20.285 0 0 0 17.705 110c12.632 3.386 63.285 3.386 63.285 3.386s50.652 0 63.285-3.386a20.284 20.284 0 0 0 14.32-14.32c3.385-12.632 3.385-38.988 3.385-38.988s0-26.355-3.385-38.988m94.473 74.326c.887-2.314 1.332-6.098 1.332-11.35V58.556c0-5.097-.445-8.822-1.332-11.178-.888-2.355-2.452-3.533-4.69-3.533-2.163 0-3.69 1.178-4.577 3.533-.888 2.356-1.332 6.081-1.332 11.178V80.68c0 5.25.424 9.035 1.275 11.35.848 2.318 2.392 3.475 4.633 3.475 2.239 0 3.803-1.157 4.691-3.475zm-17.953 11.122c-3.207-2.16-5.486-5.52-6.835-10.079-1.352-4.554-2.027-10.617-2.027-18.185v-10.31c0-7.644.771-13.784 2.316-18.417 1.544-4.633 3.956-8.011 7.24-10.135 3.282-2.123 7.587-3.186 12.916-3.186 5.251 0 9.459 1.082 12.626 3.243 3.165 2.162 5.482 5.542 6.95 10.136 1.466 4.595 2.2 10.715 2.2 18.36v10.31c0 7.567-.714 13.65-2.142 18.243-1.43 4.595-3.747 7.955-6.951 10.077-3.205 2.124-7.548 3.186-13.03 3.186-5.64 0-10.06-1.082-13.263-3.243m248.053-57.981c-.81 1.005-1.352 2.646-1.621 4.923-.272 2.278-.404 5.734-.404 10.367v5.097h11.697V60.46c0-4.555-.155-8.011-.463-10.367-.309-2.355-.868-4.014-1.678-4.98-.812-.966-2.067-1.449-3.766-1.449-1.7 0-2.954.503-3.765 1.506zm-2.025 29.886v3.591c0 4.557.132 7.974.404 10.251.269 2.279.828 3.94 1.68 4.982.849 1.041 2.16 1.564 3.938 1.564 2.392 0 4.035-.927 4.923-2.781.887-1.853 1.37-4.942 1.447-9.268l13.785.812c.077.62.116 1.469.116 2.548 0 6.565-1.795 11.47-5.387 14.712-3.589 3.242-8.669 4.865-15.232 4.865-7.876 0-13.398-2.47-16.564-7.414-3.168-4.94-4.75-12.586-4.75-22.935V63.589c0-10.657 1.641-18.436 4.924-23.342 3.281-4.903 8.9-7.355 16.854-7.355 5.482 0 9.691 1.004 12.626 3.012 2.933 2.01 5 5.137 6.197 9.383 1.197 4.247 1.796 10.117 1.796 17.607v12.163h-26.757m-284.953-1.33-18.187-65.68h15.869l6.37 29.77c1.623 7.339 2.82 13.594 3.591 18.766h.464c.54-3.706 1.738-9.922 3.591-18.65l6.603-29.886h15.869l-18.417 65.68v31.51h-15.754v-31.51M322.115 34.23v71.007h-12.511l-1.39-8.688h-.347c-3.399 6.564-8.496 9.845-15.291 9.845-4.71 0-8.185-1.543-10.425-4.633-2.24-3.087-3.359-7.915-3.359-14.48V34.23h15.985v52.126c0 3.168.348 5.426 1.043 6.776.695 1.353 1.853 2.027 3.475 2.027 1.39 0 2.722-.423 3.996-1.275 1.274-.849 2.22-1.928 2.838-3.241V34.229h15.986m81.995.001v71.007h-12.511l-1.391-8.688h-.345c-3.402 6.564-8.498 9.845-15.292 9.845-4.711 0-8.186-1.543-10.426-4.633-2.24-3.087-3.358-7.915-3.358-14.48V34.23h15.985v52.126c0 3.168.347 5.426 1.041 6.776.696 1.353 1.855 2.027 3.476 2.027 1.391 0 2.723-.423 3.996-1.275 1.275-.849 2.22-1.928 2.839-3.241V34.229h15.985"></path><path d="M365.552 20.908h-15.87v84.329h-15.637v-84.33h-15.869V8.05h47.376v12.858m76.811 53.636c0 5.174-.215 9.229-.639 12.162-.424 2.937-1.139 5.021-2.143 6.255-1.004 1.236-2.357 1.854-4.053 1.854a7.404 7.404 0 0 1-3.65-.927c-1.12-.618-2.026-1.544-2.722-2.78V50.796c.54-1.93 1.467-3.513 2.78-4.749 1.313-1.234 2.74-1.853 4.285-1.853 1.623 0 2.876.637 3.766 1.91.886 1.275 1.505 3.418 1.853 6.43.348 3.011.523 7.297.523 12.857zm14.652-28.964c-.967-4.478-2.531-7.721-4.692-9.73-2.163-2.007-5.136-3.011-8.919-3.011-2.935 0-5.676.83-8.224 2.49a16.926 16.926 0 0 0-5.908 6.545h-.117l.001-37.416h-15.405v100.777h13.204l1.622-6.717h.347c1.235 2.393 3.088 4.285 5.56 5.675 2.47 1.39 5.213 2.085 8.225 2.085 5.404 0 9.382-2.491 11.931-7.471 2.548-4.982 3.823-12.76 3.823-23.341V64.23c0-7.953-.484-14.17-1.448-18.65"></path></g></svg></a>
</div><p>Rossmann is clearly perturbed by Brother’s quiet volte-face with regard to aftermarket ink. Above he admits that he used to tell long-suffering HP or Canon printing device owners faces with cartridge DRM issues “Buy a brother laser printer for $100 and all of your woes will be solved.”</p><p>Sadly, “Brother is among the rest of them now,” mused the famous RepairTuber. With that, he admitted he would be stumped if asked to recommend a printer today. However, what he has recently seen of Brother makes him determined to keep his current occasionally used output peripheral off the internet and un-updated.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-320-80.jpg" alt="Brother printers - Lois Rossmann Wiki" srcset="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Veftxt6CsjPjT2cTfizCeL.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Louis Rossmann Wiki)</span></figcaption></figure><p>As mentioned in the intro, Rossmann has seen two big issues emerge for Brother printer users with recent firmware updates. Firstly, models that used to work with aftermarket ink, might refuse to work with the same cartridges in place post-update. Brother doesn’t always warn about such updates, so Rossmann says that it is important to keep your printer offline, if possible. Moreover, he reckons it is best to keep your printers offline, and “I highly suggest that you turn off your updates,” in light of these anti-consumer updates.</p><p>Another anti-consumer problem Rossmann highlights affects color devices. He cites reports from a Brother MFP user who noticed color calibration didn’t work with aftermarket inks post-update. They used to work, and if the update doesn’t allow the printer to calibrate with this aftermarket ink the cheaper carts become basically unusable.</p><p>Making matters worse, and an aspect of this tale which seems particularly dastardly, Rossmann says that older printer firmware is usually removed from websites. This means users can’t roll back when they discover the unwanted new ‘features’ post-update.</p><p>While he admittedly can’t do much about these printer industry machinations, Rossmann says it feels important to document these changes which show that property rights for individuals are disappearing.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-2GJmEWwwXxqxV4o5KwumSW"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trump's 'Crypto Reserve' Is Such Brazen Corruption (184 pts)]]></title>
            <link>https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen</link>
            <guid>43261899</guid>
            <pubDate>Wed, 05 Mar 2025 02:30:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen">https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen</a>, See on <a href="https://news.ycombinator.com/item?id=43261899">Hacker News</a></p>
Couldn't get https://zeteo.com/p/trumps-crypto-reserve-is-such-brazen: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Mox – modern, secure, all-in-one email server (299 pts)]]></title>
            <link>https://www.xmox.nl/</link>
            <guid>43261729</guid>
            <pubDate>Wed, 05 Mar 2025 01:58:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.xmox.nl/">https://www.xmox.nl/</a>, See on <a href="https://news.ycombinator.com/item?id=43261729">Hacker News</a></p>
Couldn't get https://www.xmox.nl/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Writing an LLM from scratch, part 8 – trainable self-attention (167 pts)]]></title>
            <link>https://www.gilesthomas.com/2025/03/llm-from-scratch-8-trainable-self-attention</link>
            <guid>43261650</guid>
            <pubDate>Wed, 05 Mar 2025 01:41:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gilesthomas.com/2025/03/llm-from-scratch-8-trainable-self-attention">https://www.gilesthomas.com/2025/03/llm-from-scratch-8-trainable-self-attention</a>, See on <a href="https://news.ycombinator.com/item?id=43261650">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest('.dropdown')) {
                        let targetId = event.target.closest('.dropdown').dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? '' : targetId;
                        event.stopPropagation();
                    }">
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

            

    

    

    <p>This is the eighth post in my trek through <a href="https://sebastianraschka.com/">Sebastian Raschka</a>'s book
"<a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Build a Large Language Model (from Scratch)</a>".
I'm blogging about bits that grab my interest, and things I had to rack my
brains over, as a way
to get things straight in my own head -- and perhaps to help anyone else that
is working through it too.  It's been almost a month since my
<a href="https://www.gilesthomas.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">last update</a> -- and
if you were suspecting that I was
<a href="https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai">blogging about blogging</a> and spending time
getting <a href="https://www.gilesthomas.com/2025/02/adding-maths-to-the-blog">LaTeX working on this site</a> as
procrastination because this next section was always going to be a hard one, then you
were 100% right!  The good news is that -- as so often happens with these things --
it turned out to not be all that tough when I really got down to it.  Momentum
regained.</p>

<blockquote>
  <p>If you found this blog through the blogging-about-blogging, welcome!  Those
  posts were not all that typical, though, and I hope
  you'll enjoy this return to my normal form.</p>
</blockquote>

<p>This time I'm covering section 3.4, "Implementing self-attention
with trainable weights".  How do we create a system that can learn how to interpret
how much attention to pay to words in a sentence, when looking at other words -- for
example, that learns that in "the fat cat sat on the mat", when you're looking at "cat",
the word "fat" is important, but when you're looking at "mat", "fat" doesn't matter
as much?</p>


    
        <p>Before diving into that, especially given the amount of time since the last post,
let's start with the 1,000-foot view of how the GPT-type
decoder-only transformer-based LLMs (hereafter "LLMs" to save me from RSI) work.
For each step I've linked to the posts where I went throught the details.</p>

<ul>
<li>You start off with a string, presumably of words. (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-2">Part 2</a>)</li>
<li>You split it up into tokens (words like "the", or chunks like "semi"). (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-2">Part 2</a>)</li>
<li>The job of the LLM is to predict the next token, given all of the tokens in the
string so far. (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-1">Part 1</a>)</li>
<li>Step 1: map the tokens to a sequence of
vectors called <em>token embeddings</em>.  A particular token,
say, "the", will have a specific embedding -- these start out random but the LLM
works out useful embeddings as it's trained. (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-3">Part 3</a>)</li>
<li>Step 2: generate another sequence of <em>position embeddings</em> -- vectors of the
same size as the token embeddings, also starting random but trained, that represent
"this is the first token", "this is
the second token", and so on.  (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-3">Part 3</a>. <sup id="fnref-1"><a href="#fn-1">1</a></sup>)</li>
<li>Step 3: add the two sequences to generate a new sequence of <em>input embeddings</em>.
The first input embedding is the first token embedding plus the first position
embedding (added element-wise), the second is the second token embedding plus the second
position embedding, and so on. (<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-3">Part 3</a>)</li>
<li>Step 4: self-attention.  Take the input embeddings
and for each one, generate a list of <em>attention scores</em>.  These
are numbers that represent how much attention to pay to each other token when considering the token
in question.  So (assuming one token per word) in "the fat cat sat on the mat",
the token "cat" would need a list of 7 attention scores -- how much attention to
pay to the first "the", how much to pay to "fat", how much to pay to itself, "cat",
how much to pay to "sat", and so on.  Exactly how it does that is what this section
of the book covers -- up until now we've been using a "toy" example calculation.
(<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://www.gilesthomas.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
<li>Step 5: normalise the attention scores to <em>attention weights</em>.  We
want each token's list of attention weights to add up to one -- we do this by running each list through
the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function.
(<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://www.gilesthomas.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
<li>Step 6: generate a new sequence of <em>context vectors</em>.
In the system that we've built so far, this contains, for each token, the sum of multiplying all of the input embeddings
by their respective attention weights and adding the results together.
So in that example above, the context vector for "cat"
would be the input embedding for the first "the" times "cat"'s attention score for
that "the", plus the input embedding for "fat" times "cat"'s attention score for
"fat", and so on for every other token in the sequence.
(<a href="https://www.gilesthomas.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://www.gilesthomas.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
</ul>

<p>After all of this is done, we have a sequence of context vectors,
each of which should in some way represent the meaning of its respective token in
the input, including those bits of meaning it gets from all of the other tokens.
So the context vector for "cat" will include some hint of its fatness, for example.</p>

<p>What happens with those context vectors that allows the LLM to use them to predict
what the next token might be?  That bit is still to be explained, so
we'll have to wait and see.  But the first thing to learn is how we create a trainable
attention mechanism that can take the input vectors and generate the attention
scores so that we can work out those context vectors in the first place.</p>

<p>The answer Raschka gives in this section is called <em>scaled dot product attention</em>.
He gives a crystal-clear runthrough of the code to do it, but I had to bang my head
against it for a weekend to get to a solid mental model.
So, instead of going through the
section bit-by-bit, I'll present my own explanation of how it works -- to save me
from future head-banging when trying to remember it, and perhaps to save other people's
foreheads from the same fate.</p>

<h3 id="the-summary-ahead-of-time">The summary, ahead of time</h3>

<p>I'm a <a href="https://www.gilesthomas.com/2011/10/teaching-programming">long-time fan</a> of the Pimsleur
style of language course, where they start each tutorial with minute or so of conversation
in the language you're trying to learn, then say "in 30 minutes, you'll hear that again
and you'll understand it".  You go through the lession, they play the conversation again, and you
do indeed understand it.</p>

<p>So here is a compressed summary of how self-attention works,
in my own words, based on Raschka's explanation.  It might look like a wall of jargon now, but
(hopefully) by the time
you've finished reading this blog post, you'll be able to re-read it and it will all make sense.</p>

<p>We have an input sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>, of tokens.  We have converted it to a
sequence of input embeddings,
each of which is a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> -- each of these can be treated as a
point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional
space.  Let's represent that sequence of embeddings with values like this: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math>.  Our goal is to produce a
sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> made up of context vectors, each of which represents the
the meaning of the respective input token in the context of the input as a whole.  These
context vectors will each be of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> (which in practice is often equal to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>,
but could in theory be of any length).</p>

<p>We define three matrices, the <em>query weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, the <em>key weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>,
and the <em>value weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math>. These are made up of trainable
weights; each one of them is sized <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mi>×</mi><mi>c</mi></mrow></math>.  Because of those dimensions, we
can treat them as operations that project a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> -- a point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensonal
space -- to a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> -- a point in
a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional space.  We will call these projected spaces <em>key space</em>,
<em>query space</em> and
<em>value space</em>.  To convert an input vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, for example, we just
multiply it by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, like this <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub><mo>=</mo><msub><mi>x</mi><mi>m</mi></msub><msub><mi>W</mi><mi>q</mi></msub></mrow></math>.</p>

<p>When we are considering input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, we want to work out its <em>attention weights</em> for
every input in the sequence (including itself).  The first step is to work out the <em>attention score</em>,
which, when considering another input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math>, is calculated by taking the dot
product of the projection of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, and the projection of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> into
key space.  Doing this across all inputs provides us with an attention score
for every other token for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  We then divide these by the square root of the
dimensionality of the spaces we are projecting into, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, and run the resulting
list through the softmax function to make them all add up to one.  This list is the
attention weights for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  This process is called <em>scaled dot product attention</em>.</p>

<p>The next step is to generate a context vector for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  This is simply the
sum of the projections of all of the inputs into the value space, each one multiplied
by its associated attention weight.</p>

<p>By performing these operations for each of the input vectors, we can generate a list
of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> made up of context vectors of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, each of which represents the meaning of a input token in the context of
the input as a whole.</p>

<p>Importantly, with clever use of matrix multiplication, all of this can be done for
all inputs in the sequence, producing a context vector for every one of them,
with just five matrix multiplications and a transpose.</p>

<h3 id="now-lets-explain-it">Now let's explain it</h3>

<p>First things first, if there's anyone there that understood all of that without
already knowing how attention mechanisms work, then I salute you!  It was pretty
dense, and I hope it didn't read like my friend Jonathan's
<a href="https://www.tartley.com/posts/a-guide-to-git-using-spatial-analogies/">parody of incomprehensible guides to using git</a>.
For me, it took eight re-reads of Raschka's (emininently clear and readable)
explanation to get to a level where I felt I understood it.  I think it's also worth noting
that it's very much a "mechanistic" explanation -- it says how we do these calculations
without saying why.  I think that the "why" is actually out of scope for this book,
but it's something that fascinates me, and I'll blog about it soon.  But,
in order to understand the "why", I think we need to have a solid grounding in the
"how", so let's dig into that for this post.</p>

<p>Up until this section of the book, we have been working out the attention scores by taking the dot product
of the input embeddings against each other -- that is, when you're looking
at <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, the attention score for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> is just <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub><mi>·</mi><msub><mi>x</mi><mi>p</mi></msub></mrow></math>.  I suspected
earlier that the reason that Raschka was using that specific operation for his
"toy" self-attention was that the real implementation is similar, and that has turned
out right, as we're doing scaled dot products here.  But what we do is adjust them first -- <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, the one that we're considering,
is multiplied by the query weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math> first, and the other one <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> is
multiplied by the key weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>.  Raschka refers to this as a projection,
which for me is a really nice way to look at it.  But his reference is just in passing,
and for me it needed a bit more digging in.</p>

<h3 id="matrices-as-projections-between-spaces">Matrices as projections between spaces</h3>

<blockquote>
  <p>If your matrix maths is a bit rusty -- like mine was -- and you haven't read the
  <a href="https://www.gilesthomas.com/2025/02/basic-neural-network-matrix-maths-part-1">primer I posted the other week</a>, then
  you might want to check it out now.</p>
</blockquote>

<p>From your schooldays, you might remember that matrices can be used to apply geometric
transformations.  For example, if you take a vector representing a point, you can multiply
it by a matrix to rotate that point about the origin.
You can use a matrix like this to rotate things anti-clockwise by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>θ</mi></mrow></math> degrees:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><mi>x</mi></mtd><mtd><mi>y</mi></mtd></mtr></mtable><mo>]</mo><mo>[</mo><mtable><mtr><mtd><mi>cos</mi><mi>θ</mi></mtd><mtd><mo>−</mo><mi>sin</mi><mi>θ</mi></mtd></mtr><mtr><mtd><mi>sin</mi><mi>θ</mi></mtd><mtd><mi>cos</mi><mi>θ</mi></mtd></mtr></mtable><mo>]</mo><mo>=</mo><mo>[</mo><mtable><mtr><mtd><mi>x</mi><mo>.</mo><mi>cos</mi><mi>θ</mi><mo>+</mo><mi>y</mi><mo>.</mo><mi>sin</mi><mi>θ</mi></mtd><mtd><mi>x</mi><mo>.</mo><mo>−</mo><mi>sin</mi><mi>θ</mi><mo>+</mo><mi>y</mi><mo>.</mo><mi>cos</mi><mi>θ</mi></mtd></mtr></mtable><mo>]</mo></mrow></math>

<p>This being matrix multiplication, you could add on more points -- that is, if the
first matrix had more rows, each of which was a point you wanted to rotate, the same
multiplication would rotate them all by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>θ</mi></mrow></math>.  So you can see that matrix as
being a function that maps sets of points to their rotated equivalents.  This
works in higher dimensions, too -- a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mn>2</mn></mrow></math> matrix like this can represent
transformations in 2 dimensions, but, for example, in 3d graphics, people
use <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>×</mi><mn>3</mn></mrow></math> matrices to do similar transformations to the points that make up
3d objects. <sup id="fnref-2"><a href="#fn-2">2</a></sup></p>

<p>An alternative way of looking at this <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mn>2</mn></mrow></math> matrix is that it's a function that
projects points from
one 2-dimensional space to another, the target space being the first space rotated
by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>θ</mi></mrow></math> degrees anti-clockwise.  For a simple 2d example like this, or even the
3d ones, that's not
necessarily a better way of seeing it.  It's a philosophical difference rather
than a practical one.</p>

<p>But imagine if the matrix wasn't square --
that is, it had a different number of rows to the number of columns.
If you had a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>×</mi><mn>2</mn></mrow></math> matrix, it could be used to multiply a matrix of vectors
in 3d space and produce a matrix in 2d space.  Remember the rule for matrix multiplication:
a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mn>3</mn></mrow></math> matrix times a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>×</mi><mn>2</mn></mrow></math> matrix will give you a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mn>2</mn></mrow></math> one.</p>

<p>That is actually super-useful;
if you've done any 3d graphics, you might remember the
<a href="https://en.wikipedia.org/wiki/Viewing_frustum">frustum</a> matrix which is used
to convert the 3d points you're working with to 2d points on a screen.  Without
going into too much detail, it allows you to project those 3d points into a 2d
space with a single matrix multiplication.</p>

<p>So: a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mi>×</mi><mi>c</mi></mrow></math> matrix can be seen as a way to project a vector that represents a
point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional space into one that represents one in a different <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional
space.</p>

<p>What we're doing in self-attention is taking our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional vectors that make
up the input embedding sequence, then projecting them into three different <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional
spaces, and working with the projected versions.  Why do we do this?  That's the
question I want to look into in my future post on the "why", but for now, I think one thing that is fairly
clear is that because these projections are learned as part of the training (remember,
the three matrices we're using for the projections are made up of trainable weights),
it's putting some kind of indirection into the mix that the simple dot product attention
that we were using before didn't have.</p>

<h3 id="how-to-do-the-dot-products-of-the-projected-input-embeddings">How to do the dot products of the projected input embeddings</h3>

<p>Sticking with this mechanistic view -- "how" rather than "why" -- for now,
let's look at the calculations and how matrix multiplication makes them efficient.
I'm going to loosely follow Raschka's
explanation, but using mathematical notation rather than code, as (unusually for me as a career
techie) I found it a bit easier to grasp what's going on that way.</p>

<p>We'll stick with the case where we're
considering token <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> and trying to work out its attention score for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math>.
The first thing we do is project <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, which we do by
multiplying it by the query weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><mo>=</mo><msub><mi>x</mi><mi>m</mi></msub><msub><mi>W</mi><mi>q</mi></msub></mrow></math>

<p>Now, let's project <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> into key space by multiplying it by the key weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>k</mi><mi>p</mi></msub><mo>=</mo><msub><mi>x</mi><mi>p</mi></msub><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<p>Our attention score is defined as being the dot product of these two vectors:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>ω</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>=</mo><msub><mi>q</mi><mi>m</mi></msub><mo>.</mo><msub><mi>k</mi><mi>p</mi></msub></mrow></math>

<p>So we could write a simple loop that iterated over all of the inputs <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math> once,
generating the projections into query space for each one, and then inside that
loop iterated over <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math> a second time, projecting them into key space, doing
the dot products, and storing those as attention scores.</p>

<p>But that would be wasteful!  We're doing matrix multiplications, so we can batch
things up.  Let's consider the projections of the inputs into the key space first;
those will always be the same, each time around our hypothetical loop.  So we can
do them in one shot.  Let's treat our input sequence as a matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi></mrow></math> like this:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><mo>.</mo><mo>.</mo><mo>.</mo></mtd></mtr><mtr><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></math>

<p>We have a row for every input embedding in our input
sequence <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow></math>, and so on, with the row being made up of the elements in that embedding.  So it
has <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> rows, one per element in the input sequence, and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> columns, one for each
dimension in the input embeddings, so it's <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mi>d</mi></mrow></math>.  (I'm using <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>3</mn></mrow></math> as
an example here, like Raschka does in the book.)</p>

<p>That's just like our matrix of points in the rotation matrix example above, so
we can project it into key space in one go, just by multiplying it by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>.  Let's
call the result of that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math>:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<p>It will look like this (again, like Raschka, I'm using a 2-dimensional
key space -- that is, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mo>=</mo><mn>2</mn></mrow></math> -- so that it's easy to see whether a matrix is in the
original 3d input embedding space or a 2d projected one):</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><msubsup><mi>k</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><mo>.</mo><mo>.</mo><mo>.</mo></mtd></mtr><mtr><mtd><msubsup><mi>k</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></math>

<p>...where each of those rows is the projection of the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>n</mi></msub></mrow></math> to key space.
It's just all of the projections stacked on top of each other.</p>

<p>Now, let's think about that dot product -- this bit from earlier:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>ω</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>=</mo><msub><mi>q</mi><mi>m</mi></msub><mo>.</mo><msub><mi>k</mi><mi>p</mi></msub></mrow></math>

<p>We now have a matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> containing all of our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>k</mi><mi>n</mi></msub></mrow></math> values.  When you're doing
a matrix multiplication, the value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math> -- that is, the element at
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math>, column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the output matrix -- is the dot product of
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> in the first matrix, taken as a vector, with column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the second
matrix, also considered as a vector.</p>

<p>It sounds like we can make use of that to do all of our dot products in a batch.
Let's treat <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math>, our projection of the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>th input token into query space, as
a single-row matrix.  Can we multiply the key matrix by it, like this</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><mi>K</mi></mrow></math>

<p>...?</p>

<p>Unfortunately not.  <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math> is a one-row matrix (size <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn><mi>×</mi><mi>c</mi></mrow></math>)
and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> is our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mi>c</mi></mrow></math> key matrix.  With matrix multiplication,
the number of columns in the first matrix -- <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> in this case -- needs to match
the number of rows in the second, which is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>.  But, if we transpose K, essentially
swapping rows for columns:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><msup><mi>K</mi><mi>T</mi></msup></mrow></math>

<p>...then we have a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn><mi>×</mi><mi>c</mi></mrow></math> matrix times a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mi>×</mi><mi>n</mi></mrow></math> one, which does make sense --
and, even better, it's every dot product for every pair of (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>k</mi><mi>p</mi></msub></mrow></math>) for all values
of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>p</mi></mrow></math> -- that is, with two matrix multiplications -- the one to work out <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> and this one,
and a transpose, we've worked out all of the attention scores for element <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> in our input
sequence.</p>

<p>But it gets better!</p>

<p>First, let's do the same thing as we did to project the input sequence
into key space to project it all into query space as well.  We
calculated <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math> to work out the key matrix, so we can work out the query matrix the
same way, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>.  Just like <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> was all of the input vectors projected into
key space, "stacked" on top of each other, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> is all of the input vectors projected
into query space.</p>

<p>Now, what happens if we multiply that by the transposed key matrix?</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math>

<p>Well, our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> matrix is one row per input, one column per dimension in our projected
space, so it's <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mi>c</mi></mrow></math>.  And, as we know, the transposed <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> matrix
is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mi>×</mi><mi>n</mi></mrow></math>.  So our result is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mi>n</mi></mrow></math> -- and because matrix multiplication
is defined in terms of dot products, what it contains is the dot product of every
row in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> -- the inputs transformed into query space -- against every column
in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>K</mi><mi>T</mi></msup></mrow></math> -- the inputs transformed into key space.</p>

<p>The plan was to generate attention scores by working out exactly those dot products!</p>

<p>So with three matrix multiplications, we've done that:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Ω</mi><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math>

<p>...where I'm using the capital <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Ω</mi></mrow></math> to represent a matrix where each row
represents an input in the sequence, and each column within the row represents
an attention weight for that input.  The element <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Ω</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub></mrow></math> represents how much
attention to pay to the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> when you are trying to work out the context
vector for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  And it has done that by working out the dot product of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> projected
into query space and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> projected into key space.</p>

<p>That's the "dot product" part of "scaled dot product attention" done and dusted :-)</p>

<h3 id="normalising-it">Normalising it</h3>

<p>So we've worked out our attention scores.  The next thing we need to do is normalise
them; in the past we used the softmax function.  This function takes a list and adjusts
the values in it so that they all sum up to 1, but gives a boost to higher numbers and
a deboost to smaller ones.  I imagine it's named "soft" "max" because it's like finding
the maximum, but in a sense softer because it's leaving the other smaller numbers
in there deboosted.</p>

<p>Raschka explains that when we're working with large numbers of dimensions -- in
real-world LLMs, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> can easily be in the thousands -- using pure softmax
can lead to small gradients -- he says that it can start acting "like a step function",
which I read as meaning that you wind up with all but the largest number in the list
being scaled to really tiny numbers and the largest one dominating.  So, as a workaround,
we divide the numbers by the square root of the number of dimensions in our projected
space <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, and then only then do we run the result through softmax. <sup id="fnref-3"><a href="#fn-3">3</a></sup></p>

<p>Remember that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Ω</mi></mrow></math> is a
matrix of attention scores, with one row for each input token, so we need to apply
the softmax function to each row separately.  Here's what we wind up with:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><mi>Ω</mi></mrow><mrow><msqrt><mrow><mi>c</mi></mrow></msqrt></mrow></mfrac><mo>,</mo><mtext>&nbsp;axis</mtext><mo>=</mo><mn>1</mn><mo stretchy="true" fence="true" form="postfix">)</mo></mrow></mrow></math>

<p>(The <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi><mi>x</mi><mi>i</mi><mi>s</mi><mo>=</mo><mn>1</mn></mrow></math> isn't really proper mathematical notation, it's just something
I've borrowed from PyTorch to say that we're applying softmax to a matrix on a
per-row basis.)</p>

<p>Once we've done that, we have our normalised attention scores -- that is, the
attention weights.  The next, and final, step, is to use those to work out the context
vectors.</p>

<h3 id="creating-the-context-vectors">Creating the context vectors</h3>

<p>Let's reiterate how we're working out the context vectors.  In the previous toy
example, for each token, we took the input embeddings, multiplied each one by
its attention weight, summed the results element-wise, and that was the result.
Now we're doing the same thing, but projecting the input embeddings into another
space first -- the value space.  So let's start off by doing that projection as
a simple matrix multiplication, just like we did for the other spaces:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>v</mi></msub></mrow></math>

<p>Now, from above we have our attention weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi></mrow></math>, which has in row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math> the attention
weights for every token in the input sequence for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> -- that is,
at <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>A</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub></mrow></math> we have the
attention weight for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>p</mi></mrow></math> when we're working out the context vector for
input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>.  That means that for our input sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>, it's
an <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mi>n</mi></mrow></math> matrix.</p>

<p>In our value matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>V</mi></mrow></math>, we also have one row per input.  The values in row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>, treated
as a vector, are the projection of the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into value space.  So it's
an <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mi>c</mi></mrow></math> matrix.</p>

<p>What happens if we do the matrix multiplication</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mi>V</mi></mrow></math>

<p>...?  We'll get a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mi>c</mi></mrow></math> matrix of some kind, by the rules of matrix multiplication,
but what will it mean?</p>

<p>To reiterate, the rule for matrix multiplication is that the value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math> -- that is, the element at
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math>, column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the output matrix -- is the dot product of
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> in the first matrix, taken as a vector, with column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the second
matrix, also considered as a vector.</p>

<p>So, at position <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math> -- first row, first column, we have the dot product of the first row in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi></mrow></math> -- the
attention weight for every token in the input sequence when we're considering the
first token -- and the first column in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>V</mi></mrow></math>, which is the first element of each
input embedding, projected into the value space.  So, that is the first element
of each input embedding times the attention weights for the first token.  Or,
in other words, it's the first element of the context vector for the first token!</p>

<p>At position <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math> -- first row,
second column -- we'll have the same calculation, but for the second element of each
input embedding.  That is the second element of the context vector for the first
token.</p>

<p>...and so on for the rest of the columns.  By the end of the first row,
we'll have something that (treated as a vector) is the sum of all of the input
embeddings, multiplied by the weights for the first input.  It's our context vector
for that input!</p>

<p>The same, of course, repeats for each row.  The result of that single matrix multiplication
is a matrix where the row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math> is the context vector for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.</p>

<p>We're done!</p>

<h3 id="bringing-it-all-together">Bringing it all together</h3>

<p>Let's put together those steps.  We start with our input matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi></mrow></math>, which is the
input embeddings we generated earlier for our sequence of tokens of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>.  Each
row is an embedding, and there are <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> columns, where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> is the dimensionality of
our embeddings.</p>

<p>We also have our weight matrices to map input embeddings into different
spaces: the <em>query weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, the <em>key weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>,
and the <em>value weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math>.</p>

<p>So, we project our input matrix into those spaces with three matrix multiplications:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>v</mi></msub></mrow></math>

<p>...to get our query matrix, our key matrix, and our value matrix.</p>

<p>We then calculate
our attention scores with one further matrix multiplication and a transpose to work out the dot
products:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Ω</mi><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math>

<p>We normalise those to attention weights by scaling them by the square root of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>
and then applying softmax:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><mi>Ω</mi></mrow><mrow><msqrt><mrow><mi>c</mi></mrow></msqrt></mrow></mfrac><mo>,</mo><mtext>&nbsp;axis</mtext><mo>=</mo><mn>1</mn><mo stretchy="true" fence="true" form="postfix">)</mo></mrow></mrow></math>

<p>...and then we use one final matrix multiplication to use that to work out the
context vectors:</p>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>C</mi><mo>=</mo><mi>A</mi><mi>V</mi></mrow></math>

<p>And that's our self-attention mechanism :-)</p>

<p>Now, if you
<a href="#the-summary-ahead-of-time">go back to the explanation at the start</a>, then
hopefully it will make sense.</p>

<h3 id="back-to-the-book">Back to the book</h3>

<p>Section 3.4 in the book works through the above with PyTorch code, and comes out
with a nice simple <code>nn.Module</code> subclass that does exactly those matrix operations.
This is then improved -- the first version uses generic <code>nn.Parameter</code> objects for
the three weight matrices, and the second uses <code>nn.Linear</code> for more effective training.
That side of it was reasonably easy to understand.  And so, we've wrapped up what I
think is the hardest part of "Build a Large Language Model (from scratch)":
implementing self-attention
with trainable weights.</p>

<h3 id="next-steps">Next steps</h3>

<p>The remainder of chapter 3 is much easier now that we're over
this hump.  We'll be going through two things:</p>

<ul>
<li>Causal self-attention (which means that when we are looking at a given token, we don't pay any attention to later
ones, just like we humans do when reading -- our language is structured so that you
don't normally need to read forward to understand what a word means [except <a href="https://faculty.georgetown.edu/jod/texts/twain.german.html">in German</a> ;-]).</li>
<li>Multi-head attention (which isn't as complex an issue as I thought it was when I first read about it).</li>
</ul>

<p>So I think
I'll probably blog about those first, and then circle back to the "why" of this
form of self-attention.  It's pretty amazing that we can do all of this
-- projecting into differently-dimensioned spaces, taking dot products between
every token's input embeddings in those spaces, and weighting the projected input
tokens by the weights we generate -- with just five matrix multiplications.  But
why do we do that specifically?</p>

<p>The names of the matrices used -- query, key and value -- hint at
the roles they play in a metaphorical way; Raschka says in a sidebar that
it's a nod to information retrieval systems like databases.  However, it's different
enough to how DBs actually work that I can't quite make the connection.  I'm sure
it will come with time, though.</p>

<p>I also want to, probably in a separate post, consider what batches do to all of this.
With <a href="https://www.gilesthomas.com/2025/02/basic-neural-network-matrix-maths-part-1">normal neural networks</a>,
all of our activations when considering a given input are single-row or -column
matrices (depending on the ordering of our equations).  Extending to batches
just means moving to normal multi-row, multi-column matrices.</p>

<p>But ever since
we introduced the matrix of attention scores <a href="https://www.gilesthomas.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">for the first time</a>,
it's been clear that even with a single input sequence going through our LLM, we're
already using full matrices.  How do we handle batches where we're processing
multiple input sequences in parallel?  It seems that we're going to need to use
some kind of higher-order tensors -- if scalars are order zero tensors, vectors are
order one tensors, and matrices are order two tensors, we're going to need to start
considering order three tensors at least.  That will require a bit of thought!</p>

<p>But for now, that's all -- see you next time!  And please do comment below --
any thoughts, questions or suggestions would be very welcome, of course, but even
if you just found this post useful it would be great to know :-)</p>



    

    
        
    

    



            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Best Buy and Target CEOs say prices are about to go up because of tariffs (150 pts)]]></title>
            <link>https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs</link>
            <guid>43261626</guid>
            <pubDate>Wed, 05 Mar 2025 01:36:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs">https://www.theverge.com/news/624254/best-buy-target-raise-prices-tariffs</a>, See on <a href="https://news.ycombinator.com/item?id=43261626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img alt="Emma Roth" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></p><p><a href="https://www.theverge.com/authors/emma-roth">Emma Roth</a> <span>is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.</span></p></div><div id="zephr-anchor"><p>Target and Best Buy say Trump’s tariffs on Mexico, Canada, and China could raise prices in their stores as soon as this week. <a href="https://www.cnbc.com/2025/03/04/trump-mexico-tariffs-will-raise-produce-prices-target-ceo-cornell-says.html">During an interview with CNBC</a>, Target CEO Brian Cornell said consumers will “likely see prices increase over the next couple of days,” while Best Buy CEO Corie Barry <a href="https://www.cnbc.com/2025/03/04/best-buy-bby-q4-2025-earnings.html">similarly told investors</a> that more expensive prices are “highly likely.”</p><p>Cornell told CNBC that half of Target’s goods come from the United States, but the company depends on Mexico for “a significant amount” of fruits and vegetables during winter, potentially leading to more expensive strawberries, bananas, and avocados. “Those are categories where we’ll try to protect pricing, but the consumer will likely see price increases over the next couple of days,” Cornell added.</p><p>Meanwhile, Best Buy’s Barry <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.fool.com%2Fearnings%2Fcall-transcripts%2F2025%2F03%2F04%2Fbest-buy-bby-q4-2025-earnings-call-transcript%2F" rel="sponsored">said during an earnings</a> call that China and Mexico remain the top two countries where the company gets its products. “We expect our vendors across our entire assortment will pass along some level of tariff costs to retailers, making price increases for American consumers highly likely,” Barry said.</p><p>On Tuesday, <a href="https://www.theverge.com/news/623403/trump-imposes-tariffs-mexico-canada-china">Trump followed through on threats</a> to impose 25 percent tariffs on products imported from Canada and Mexico, while imports from China will face an additional 10 percent tax on top of the 10 percent tax previously enacted. However, Commerce Secretary Howard Lutnick told Fox Business that Trump <a href="https://www.bloomberg.com/news/articles/2025-03-04/lutnick-says-trump-considering-some-mexico-canada-tariff-relief?utm_source=website&amp;utm_medium=share&amp;utm_campaign=twitter">might “work something out” with Canada and Mexico</a>, adding that he could announce a potential compromise on Wednesday.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Dead Planet Theory (108 pts)]]></title>
            <link>https://arealsociety.substack.com/p/the-dead-planet-theory</link>
            <guid>43261327</guid>
            <pubDate>Wed, 05 Mar 2025 00:45:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arealsociety.substack.com/p/the-dead-planet-theory">https://arealsociety.substack.com/p/the-dead-planet-theory</a>, See on <a href="https://news.ycombinator.com/item?id=43261327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>Everyone loves to talk about the </span><a href="https://en.wikipedia.org/wiki/Dead_Internet_theory" rel="">Dead Internet Theory</a><span>, but less often discussed is how few people “do things” in any venue or on any platform. This phenomenon is known by several names, including </span><a href="https://dlab.berkeley.edu/news/explaining-80-20-rule-pareto-distribution#:~:text=He%20famously%20observed%20that%2080,alpha%E2%80%9D)%20and%20Xm." rel="">the Power Law, the Pareto Principle, and the 80-20 Rule</a><span>. One example of this phenomenon is that </span><a href="https://thesocialshepherd.com/blog/twitter-statistics" rel="">10% of Twitter users account for 92% of tweets</a><span>. This dynamic can be seen in interpersonal relationships, hobbies, and careers. You can use this to quickly rise to the top, or purely to get a little more enjoyment out of the things you do day to day. In the scope of all of creation it can be hard to see the impact of this principle in action, but by separating things, events, and people by category and interest it quickly becomes apparent.</span></p><div><figure><a target="_blank" href="https://rivalstracker.com/ranks" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png" width="1456" height="887" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:887,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:962323,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://rivalstracker.com/ranks&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://arealsociety.substack.com/i/157095326?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6804788d-0bec-481e-a064-3405d3cb219d_2364x1440.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>A great way to visualize how few people </span><a href="https://arealsociety.substack.com/p/what-it-means-to-play-the-game?r=99bhj" rel="">participate in life</a><span> is by looking at rank distributions of competitive games. In Marvel Rivals, every player is initially Bronze 3 and ranks up from there. Almost everyone who actually plays the competitive mode of the game will rank up no matter how bad they are at the game, so we can see that over 30% of the playerbase has never even played the competitive mode. Simply by playing a competitive match, you are ranked in the top 70% of the playerbase. We can look at other activities and interests, such as film, to further reinforce this point.</span></p><div><figure><a target="_blank" href="https://x.com/michaelcurzi/status/1799748650589208624?t=kGJiVbk8fWMQfKxHKxT1eQ&amp;s=19" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png" width="1176" height="516" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:516,&quot;width&quot;:1176,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:105664,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/michaelcurzi/status/1799748650589208624?t=kGJiVbk8fWMQfKxHKxT1eQ&amp;s=19&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://arealsociety.substack.com/i/157095326?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png&quot;,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e34fed4-221f-4d88-b3d2-c04da643ffa5_1176x516.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>In all fields, </span><a href="https://arealsociety.substack.com/p/how-i-learned-to-stop-worrying-and-love-the-sort?r=99bhj" rel="">the best quickly climb to the top</a><span>, so it doesn’t take that long to identify who they are, and to find out what they are doing. I'm mostly focusing on the start of the journey, but things get truly exceptional if you can show long term consistency. Just doing things in general is admirable, but intentional training is the true starting point.</span></p><div><figure><a target="_blank" href="https://x.com/alz_zyd_/status/1828909458480632028?t=wqslJNqSQ4uAxLet1sx0LA&amp;s=19" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png" width="1186" height="376" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:376,&quot;width&quot;:1186,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:80121,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/alz_zyd_/status/1828909458480632028?t=wqslJNqSQ4uAxLet1sx0LA&amp;s=19&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://arealsociety.substack.com/i/157095326?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde3dedd0-d098-4253-a7c4-e5020266efe4_1186x376.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><a href="https://x.com/alz_zyd_/status/1828909458480632028?t=wqslJNqSQ4uAxLet1sx0LA&amp;s=19" rel="">The thread above</a><span> has several great examples, and a common element is that plenty of these are activities that many people do, but few people train. Chess is a great demonstration. On chess.com, you can learn about move-sets by playing, as the app won’t let you make an invalid move. Many people play chess, but few people train chess. Just by spending an hour learning a single opening, you can be substantially better at chess than someone who has played chess for years without ever training. This leads into my next point: no one does the reading.</span></p><div><figure><a target="_blank" href="https://x.com/patio11/status/1800163389630824726?t=6UHrJNcaMzNGy5cPTwyAvg&amp;s=19" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png" width="1184" height="668" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:668,&quot;width&quot;:1184,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:166961,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://x.com/patio11/status/1800163389630824726?t=6UHrJNcaMzNGy5cPTwyAvg&amp;s=19&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://arealsociety.substack.com/i/157095326?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png&quot;,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4cae6365-7ffb-457a-b4bf-ce93a127bf27_1184x668.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>When I was working in tech, I noticed a particular API parameter would reflect any input, and I wanted to know why it was doing that if it was supposed to be a simple identifier. I asked several coworkers before I found someone who knew, and we went into a meeting room where he walked me through all the different service calls that interacted with that identifier, and the reasoning behind it being dynamic. He told me that it had been that way for years, and was mostly unused these days. He also told me I was the first person that had asked about it in 2 years. I ultimately found a major vulnerability in our logging because of this innocuous identifier most people ignored.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg" width="640" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:640,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;1913: When Hitler, Trotsky, Tito, Freud and Stalin all lived together in  Vienna : r/MapPorn&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="1913: When Hitler, Trotsky, Tito, Freud and Stalin all lived together in  Vienna : r/MapPorn" title="1913: When Hitler, Trotsky, Tito, Freud and Stalin all lived together in  Vienna : r/MapPorn" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11448ac0-f808-4d0a-8f56-f85c78b72c82_640x930.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This brings me to my main point, actual actors are rare and tend to coexist. If I were to write a fictional story, and set this many characters of such magnitude in the same city at the same time, people would think the premise was absurd. The reality is you will see this play out in countless ways with countless groups. A common example is a simple friend group. You generally have core members, who are core not because they are more attractive or popular (though to be honest this is sometimes the case), but because they actually show up. When I was organizing frequent events, as long as I could convince 2 or 3 other people to go to an event, I would often get 10 to 15 people actually showing up. There would be an inner circle of 3 or 4 event planners, an additional 3 or so consistent attendees, and 20+ people that would rotate in and out. From the outside it may look like 5 people were more popular than than the others, but the reality is that without that core, the other 20+ rotators would just never hang out with each other.</p><p>From a career perspective there are plenty of ways to “do the reading” or to get ahead. A boomer classic is to speak to a manager and give a firm handshake, the seed of good advice is still there. By being willing to apply to jobs, or studying and practicing for interviews, or asking for a promotion, or by researching salary and negotiating well, you can quickly and substantially elevate yourself. An important thing to remember is that if you want other people to do things for or with you, think about their perspective or challenges and make it as easy as possible for the other person to help you. </p><p><span>A work example is if your job uses a “</span><a href="https://staffeng.com/guides/promo-packets/" rel="">promotion packet</a><span>”, which is common in tech, you can literally write your own promotion packet and get your manager to sign it for you. Normally after a couple years at a big tech job your manager will ask you if you want to go for a promotion. You’ll then spend the next year increasing your scope and taking on more complex projects. If that year goes well, in your next performance evaluation your manager will say he plans to promote you in the next six months to a year. He will then start reaching out to other managers or individual contributors you worked with over the year. He’ll get feedback from each person, and all of your feedback providers will often have packed schedules and take weeks if not months to provide their feedback. Your manager will then compile said feedback, summaries of your projects and achievements, and your performance reviews into a document. This is your promotion packet. Wanting to push for promotion is already rare, and going so far as to write your own packet makes it way more probable that you get the promotion you want, and it can potentially push your promotion forward by years.</span></p><p><span>An added benefit to doing things, or being in the arena in general, is that by </span><a href="https://arealsociety.substack.com/p/what-it-means-to-play-the-game?r=99bhj" rel="">participating in the game</a><span> you enable luck. If you never leave your apartment you can’t have a serendipitous run-in with your future spouse. By entering the ranks of the doers, things can happen to you as well. If you don’t apply to a job you don’t 100% meet the requirements for, they aren’t going to email you a “sorry we missed your application”, they’ll go on to someone else who isn’t a perfect match, but was willing to apply. Too many things in life reward action for you to live in a state of stupor. </span></p><p>If you start anywhere, start with simply doing something. For socializing, this can just be showing up when people invite you to something. If you’ve started doing things already, consider “training”. This can be doing the reading, hosting events yourself, or even literal training. This will allow you to quickly distinguish yourself, and it will make your life substantially more fulfilling. Go forth and remember that with even minimal effort, you can elevate yourself to the category of agentic people and be in that 20% of doers while others observe.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ARC-AGI without pretraining (281 pts)]]></title>
            <link>https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html</link>
            <guid>43259182</guid>
            <pubDate>Tue, 04 Mar 2025 19:52:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html">https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html</a>, See on <a href="https://news.ycombinator.com/item?id=43259182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <p><a name="topofpage"></a><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/teaser_figure_w_title.png" alt="image">
By <a href="https://iliao2345.github.io/">Isaac Liao</a> and <a href="https://goombalab.github.io/">Albert Gu</a></p>

  
  <p>In this blog post, we aim to answer a simple yet fundamental question:</p>

  <p><strong>Can lossless information compression by itself produce intelligent behavior?</strong></p>

  <p>The idea that efficient compression by itself lies at the heart of intelligence is not new (see, e.g., <a href="https://www.researchgate.net/publication/2472570_A_Formal_Definition_of_Intelligence_Based_on_an_Intensional_Variant_of_Algorithmic_Complexity">Hernández-Orallo &amp; Minaya-Collado, 1998</a>; <a href="https://gwern.net/doc/cs/algorithm/information/compression/1999-mahoney.pdf">Mahoney, 1999</a>; <a href="https://link.springer.com/book/10.1007/b138233">Hutter, 2005</a>; <a href="https://arxiv.org/abs/0712.3329">Legg &amp; Hutter, 2007</a>). Rather than revisiting those theoretical discussions, we make a practical demonstration instead.</p>

  <p>In this work, we give evidence that lossless compression during inference time is sufficient to produce intelligent behavior, by developing a method <strong>purely based on compression</strong> that performs well on the <a href="https://arcprize.org/">ARC-AGI challenge</a>, a dataset of IQ-test-like puzzles about inferring a procedure/rule from limited demonstrations. Crucially, our solution, which we name <em>CompressARC</em>, obeys the following three restrictions:</p>

  <ul>
    <li><strong>No pretraining</strong>; models are randomly initialized and trained during inference time.</li>
    <li><strong>No dataset</strong>; one model trains on just the target ARC-AGI puzzle and outputs one answer.</li>
    <li><strong>No search</strong>, in most senses of the word—just gradient descent.</li>
  </ul>

  <p>Despite these constraints, CompressARC achieves 34.75% on the training set and 20% on the evaluation set—processing each puzzle in roughly 20 minutes on an RTX 4070. To our knowledge, this is the first neural method for solving ARC-AGI where the training data is limited to just the target puzzle. CompressARC’s intelligence emerges not from pretraining, vast datasets, exhaustive search, or massive compute—but from compression. We challenge the conventional reliance on extensive pretraining and data, and propose a future where tailored compressive objectives and efficient inference-time computation work together to extract deep intelligence from minimal input.</p>

  

  <h2 id="what-is-arc-agi">What is ARC-AGI?</h2>

  <p><a href="https://arcprize.org/">ARC-AGI</a>, <a href="https://arxiv.org/abs/1911.01547">introduced in 2019</a>, is an artificial intelligence benchmark designed to test a system’s ability to infer and generalize abstract rules from minimal examples. The dataset consists of IQ-test-like puzzles, where each puzzle provides several example images that demonstrate an underlying rule, along with a test image that requires completing or applying that rule. While some have suggested that solving ARC-AGI might signal the advent of <a href="https://arxiv.org/abs/1911.01547">artificial general intelligence</a> (AGI), its true purpose is to spotlight the current challenges hindering progress toward AGI. Below are three of the 1000 puzzles:</p>

  <table>
    <thead>
      <tr>
        <th>Hidden rule: Shift every object to the right by one pixel, except the bottom/right edges of the object.</th>
        <th>Hidden rule: Shrink the big object and set its color to the scattered dots’ color.</th>
        <th>Hidden rule: Extend the green line to meet the red line by turning when hitting a wall.</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/025d127b_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/5ad4f10b_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2dd70a9a_problem.png" alt="image"></td>
      </tr>
    </tbody>
  </table>

  <p>For every puzzle, there is a hidden rule that maps each input grid to each output grid. You are given some number of examples of input-to-output mappings, and you get <strong>two attempts</strong> to guess the output grid for a given input grid, without being told the hidden rule. If either guess is correct, then you score 1 for that puzzle, else you score 0. You are allowed to change the size of the output grid and pick the color of every pixel. The puzzles are designed so that <strong>humans can reasonably find the answer, but machines should have more difficulty</strong>. <a href="https://arcprize.org/guide">The average human can solve 76.2% of the training set</a>, and <a href="https://arxiv.org/abs/2409.01374">a human expert can solve 98.5%.</a></p>

  <p>The 400 training puzzles are easier than the rest, and are meant to help you learn the following patterns:</p>
  <ul>
    <li><strong>Objectness:</strong> Objects persist and cannot appear or disappear without reason. Objects can interact or not depending on the circumstances.</li>
    <li><strong>Goal-directedness:</strong> Objects can be animate or inanimate. Some objects are “agents” - they have intentions and they pursue goals.</li>
    <li><strong>Numbers &amp; counting:</strong> Objects can be counted or sorted by their shape, appearance, or movement using basic mathematics like addition, subtraction, and comparison.</li>
    <li><strong>Basic geometry &amp; topology:</strong> Objects can be shapes like rectangles, triangles, and circles which can be mirrored, rotated, translated, deformed, combined, repeated, etc. Differences in distances can be detected.</li>
  </ul>

  <p>The ARC Prize team has repeatedly launched competitions for solving ARC-AGI, with monetary rewards. <a href="https://www.kaggle.com/competitions/arc-prize-2024">The most recent competition</a> involved potential prizes and awards of upwards of <strong>$1,000,000</strong>, with the main prize reserved for methods which could achieve 85% on a private test set of 100 puzzles, using 12 hours of compute in a constrained environment.</p>

  

  <h2 id="our-solution-method">Our Solution Method</h2>

  <!--<img align="right" src="./resources/algorithm_environment.JPG" width="50%" style="margin: 20px 0 20px 10px;">-->

  <p><strong>We propose that lossless information compression can serve as an effective framework for solving ARC-AGI puzzles. A more efficient (i.e., lower-bit) compression of a puzzle correlates with a more accurate solution.</strong> To solve ARC-AGI puzzles, we design a system that transforms an incomplete puzzle into a completed one—filling in the answers—by finding a compact representation that, when decompressed, reproduces the puzzle with any solution. The key challenge is to obtain this compact representation without needing the answers as inputs.</p>

  <p>CompressARC uses a neural network as the decoder. However, the encoding algorithm is not another network—instead, encoding is realized by the gradient descent algorithm that performs inference-time training on the decoder while maintaining correct decoded output. In other words, running the encoder means optimizing the decoder’s parameters and input distribution to achieve the most compressed puzzle representation. The resulting optimized parameters (e.g., weights and input distribution settings) themselves serve as the compressed bit representation that encodes the puzzle along with its answer.</p>

  <p>In standard machine learning lingo: (without compression terminology, and with some simplifications)</p>

  <ol>
    <li>We start at inference time, and we are given an ARC-AGI puzzle to solve. (e.g., puzzle in the diagram below.)</li>
    <li>We construct a neural network $f$ (see <a href="#architecture">architecture</a>) designed for the puzzle’s specifics (e.g., number of examples, observed colors). The network takes random normal input $z \sim N(\mu, \Sigma)$, and per-pixel color logit predictions across all the grids, including an answer grid (3 input-output examples, for a total of 6 grids). Importantly, $f_\theta$ is equivariant to common augmentations—such as reordering input-output pairs (including the answer’s pair), color permutations, and spatial rotations/reflections.</li>
    <li>We initialize the network weights $\theta$ and set the parameters $\mu$ and $\Sigma$ for the $z$ distribution.</li>
    <li>We jointly optimize $\theta$, $\mu$, and $\Sigma$ to minimize the sum of cross-entropies over the known grids (5 of them,) ignoring the answer grid. A KL divergence penalty keeps $N(\mu, \Sigma)$ close to $N(0,1)$, as in a VAE.</li>
    <li>Since the generated answer grid is stochastic due to the randomness in $z$, we save the answer grids throughout training and choose the most frequently occuring one as our final prediction.</li>
  </ol>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Method_Overview.png"></p>

  <p>It isn’t obvious why such a method is performing compression. You’ll see later <a href="#how-to-derive-our-solution-method">how we derived it</a> from trying to compress ARC-AGI. First, let’s see it try to solve the puzzle above.</p>

  <h3 id="watching-the-network-learn-color-the-boxes">Watching the Network Learn: Color the Boxes</h3>

  <h4 id="human-solution">Human Solution:</h4>
  <p>We first realize that the input is divided into boxes, and the boxes are still there in the output, but now they’re colored. We then try to figure out which colors go in which boxes. First, we notice that the corners are always black. Then, we notice that the middle is always magenta. And after that, we notice that the color of the side boxes depends on which direction they are in: red for up, blue for down, green for right, and yellow for left. At this point, we copy the input over to the answer grid, then we color the middle box magenta, and then color the rest of the boxes according to their direction.</p>

  <h4 id="compressarc-solution">CompressARC Solution:</h4>
  <table>
  <tbody><tr>
  <td>
  <strong> 50 steps of learning:</strong>
  <p>
  CompressARC's network outputs an answer grid (sample) with light blue rows/columns wherever the input has the same. It has noticed that all the other input-output pairs in the puzzle exhibit this correspondence. It doesn't know how the other output pixels are assigned colors; an exponential moving average of the network output (sample average) shows the network assigning mostly the same average color to non-light-blue pixels.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_50_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 150 steps of learning:</strong>
  <p>
  The network outputs a grid where nearby pixels have similar colors. It has likely noticed that this is common among all the outputs, and is guessing that it applies to the answer too.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_150_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 200 steps of learning:</strong>
  <p>
  The network output now shows larger blobs of colors that are cut off by the light blue borders. It has noticed the common usage of borders to demarcate blobs of colors in other outputs, and applies the same idea here. It has also noticed black corner blobs in other given outputs, which the network imitates.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_200_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 350 steps of learning:</strong>
  <p>
  The network output now shows the correct colors assigned to boxes of the correct direction from the center. It has realized that a single color-to-direction mapping is used to pick the blob colors in the other given outputs, so it imitates this mapping. It is still not the best at coloring within the lines, and it's also confused about the center blob, probably because the middle does not correspond to a direction. Nevertheless, the averate network output does show a tinge of the correct magenta color in the middle, meaning the network is catching on.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_350_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 1500 steps of learning:</strong>
  <p>
  The network is as refined as it will ever be. Sometimes it will still make a mistake in the sample it outputs, but this uncommon and filtered out.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_at_1500_steps.png"></td>
  </tr>
</tbody></table>

  <p>After training, <a href="#solution-analysis-color-the-boxes">we can deconstruct the learned z distribution</a> to find that it codes for a color-direction correspondence table and row/column divider positions!</p>

  

  <h2 id="how-to-derive-our-solution-method">How to Derive Our Solution Method</h2>

  <p>Again, it isn’t obvious how we get from trying to perform compression to the method we ended up using. The derivation of our algorithm takes us on a detour through <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>, <a href="https://arxiv.org/abs/0809.2754">algorithmic information theory</a>, and <a href="https://en.wikipedia.org/wiki/Coding_theory">coding theory</a>, with machine learning only making an appearance near the end.</p>

  <h3 id="a-primer-on-lossless-information-compression">A Primer on Lossless Information Compression</h3>

  <p>In <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>, lossless information compression is about trying to represent some information in as few bits as possible, while still being able to reconstruct that information from the bit representation. This type of problem is abstracted as follows:</p>
  <ul>
    <li>A source produces some symbol $x$ from some process that generates symbols from a probability distribution $p(x)$.</li>
    <li>A compressor/encoder $E$ must map the symbol $x$ to a string of bits $s$.</li>
    <li>A decompressor/decoder $D$ must exactly map $s$ back to the original symbol $x$.</li>
  </ul>

  <p>The goal is to use $p$ to construct functions $(E, D)$ which are bit-efficient, (ie. that minimize the expected length of $s$,) without getting any symbols wrong. In our case, the symbol $x$ is the ARC-AGI dataset (many puzzle + answer pairs), and we want to figure out what answers the best compression system might decompress the answers to be. Except, we don’t have the answers (only the puzzles) to give as input to $E$, and we don’t know $p$, since it’s hard to model the intelligent process of puzzle ideation in humans.</p>

  <h3 id="one-size-fits-all-compression">One-Size-Fits-All Compression</h3>

  <p>To build our compression scheme, you might think we need to know what $p$ is, but we argue that it doesn’t really matter since we can make a one-size-fits-all compressor. It all hinges on the following assumption:</p>
  <blockquote>
    <p>There exists some practically implementable, bit efficient compression system $(E, D)$ for ARC-AGI datasets $x$ sampled from $p$.</p>
  </blockquote>

  <p>If this were false, our whole idea of solving ARC-AGI with compression is doomed even if we knew $p$ anyways, so we might as well make this assumption.</p>

  <p>Our one-size-fits-all compressor $(E’, D’)$ is built without knowing $p$, and it is almost just as bit-efficient as the original $(E, D)$:</p>
  <ul>
    <li>$E’$ observes symbol $x$, picks a program $f$ and input $s$ to minimize $\text{len}(f)+\text{len}(s)$ under the constraint that running the program makes $f(s)=x$, and then sends the pair $(f, s)$.</li>
    <li>$D’$ is just a program executor that executes $f$ on $s$, correctly producing $x$.</li>
  </ul>

  <p>It is possible to prove with <a href="https://arxiv.org/abs/0809.2754">algorithmic information theory</a> that $(E’, D’)$ achieves a bit efficiency at most $\text{len}(f)$ bits worse than the bit efficiency of $(E, D)$, where $f$ is the <em>code for implementing D</em>. But since compression is practically implementable, the code for $D$ should be simple enough for a human engineer to write, so $\text{len}(f)$ must be short, meaning our one-size-fits-all compressor will be close to the best possible bit efficiency.</p>

  <p>Ironically, the only problem with using this to solve ARC-AGI is that implementing $E’$ is not practical, since $E’$ needs to minimize the length of a program-input pair $(f, s)$ under partial fixed output constraint $f(s)_{answers}=x_{answers}$.</p>

  <h3 id="neural-networks-to-the-rescue">Neural Networks to the Rescue</h3>

  <p>To avoid searching through program space, we just pick a program $f$ for a small sacrifice in bit efficiency. We hope the diversity of program space can be delegated to diversity in input $s$ space instead. Specifically, we write a program $f$ that runs the forward pass of a neural network, where $s=(\theta, z, \epsilon)$ are the weights, inputs, and corrections to the outputs of the neural network. Then, we can use gradient descent to “search” over $s$.</p>

  <p>This restricted compression scheme uses <a href="https://arxiv.org/abs/2010.01185">Relative Entropy Coding</a> (REC)<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">1</a></sup> to encode noisy weights $\theta$ and neural network inputs $z$ into bits $s_\theta$ and $s_z$, and <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> to encode output error corrections $\epsilon$ into bits $s_\epsilon$, to make a bit string $s$ consisting of three blocks $(s_\theta, s_z, s_\epsilon)$. The compression scheme runs as follows:</p>
  <ul>
    <li>The decoder runs $\theta = \text{REC-decode}(s_\theta)$, $z = \text{REC-decode}(s_z)$, $\text{logits} = \text{Neural-Net}(\theta, z)$, and $x=\text{Arithmetic-decode}(s_\epsilon, \text{logits})$.</li>
    <li>The encoder trains $\theta$ and $z$ to minimize the total code length $\mathbb{E}[\text{len}(s)]$. $s_\epsilon$ is fixed by arithmetic coding to guarantee correct decoding. To calculate the three components of the loss $\mathbb{E}[\text{len}(s)]$ in a differentiable way, we refer to the properties of REC and arithmetic coding:
      <ul>
        <li>It turns out that the $\epsilon$ code length $\mathbb{E}[\text{len}(s_\epsilon)]$ is equal to the total crossentropy error on all the given grids in the puzzle.</li>
        <li>REC requires us to fix some reference distribution $q_\theta$, and also add noise to $\theta$, turning it into a distribution $p_\theta$. Then, REC allows you to store noisy $\theta$ using a code length of $\mathbb{E}[\text{len}(s_\theta)] = KL(p_\theta|| q_\theta) = \mathbb{E}_{\theta \sim p_\theta} [\log (p_\theta(\theta) / q_\theta(\theta))]$ bits. We will choose to fix $q_\theta = N(0, I/2\lambda)$ for large $\lambda$, such that the loss component $\mathbb{E}[\text{len}(s_\theta)] \approx \lambda | \theta|^2 + \text{const}$ is equivalent to regularizing the decoder.</li>
        <li>We must also do for $z$ what we do for $\theta$, since it’s also represented using REC. We will choose to fix $q_z = N(0,I)$, so the code length of $z$ is $\mathbb{E}[\text{len}(s_z)] = KL(p_z|| q_z) = \mathbb{E}_{z \sim p_z} [\log (p_z(z) / q_z(z))]$.</li>
      </ul>

      <p>We can compute gradients of these code lengths via the <a href="https://arxiv.org/abs/1312.6114">reparameterization trick</a>.</p>
    </li>
  </ul>

  <p>At this point, we observe that the total code length for $s$ that we described is actually the VAE loss with decoder regularization (= KL for $z$ + reconstruction error + regularization). Likewise, if we port the rest of what we described above (plus modifications regarding equivariances and inter-puzzle independence, and ignoring regularization) into typical machine learning lingo, we get the <a href="#our-solution-method">above description of CompressARC</a>.</p>

  

  <h2 id="architecture">Architecture</h2>

  <p>We designed our own neural network architecture for decoding the latents $z$ into ARC-AGI puzzles. The most important feature of our architecture is it’s equivariances, which are symmetry rules dictating that whenever the input $z$ undergoes a transformation, the output ARC-AGI puzzle must also transform the same way. Some examples:</p>

  <ul>
    <li>reordering of input/output pairs</li>
    <li>shuffling colors</li>
    <li>flips, rotations, and reflections of grids</li>
  </ul>

  <p>There are too many equivariances for us to think about at once, so we decided to make a <strong>base architecture that’s fully symmetric</strong>, and break unwanted symmetries one by one by <strong>adding asymmetric layers</strong> to give it <a href="#what-puzzles-can-and-cant-we-solve">specific non-equivariant abilities</a>.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Equivariant_Architecture.png"></p>

  <p>To illustrate what we mean, suppose that both $z$ and an ARC-AGI puzzle take the form of a tensor of shape $[n\_examples, n\_colors, height, width, 2 \text{ for input/output}]$ (This is not actually the format of the data (see <a href="#multitensors">multitensors</a>) but it gets the idea across best.) Then, our network starts out as equivariant to permutations of indices in the $example$, $color$, $height$, and $width$ dimensions. Some extra care must be taken with weight sharing, to force the network to also be equivariant to swapping the $width$ and $height$ dimensions. We may then add a layer involving a roll by one in the $width$ and $height$ dimensions, to let the network distinguish short range spatial interactions but not long-range ones.</p>

  <!-- When $z$ is fully symmetrical, then the outputted puzzle must also be fully symmetrical. But notice that [CompressARC](#our-solution-method) is allowed to learn asymmetrical $z$ in order to obtain asymmetrical outputs. Since the $z$ distribution is penalized for deviating from the fully symmetric $N(0,I)$, asymmetrical outputs are discouraged. So, the network must pay a penalty to use $z$ to distinguish the learned roles of two colors, two rows, two pixels, etc. in a puzzle. This makes $z$ naturally lean towards simpler representations of the puzzle. -->

  <p>The actual data ($z$, hidden activations, and puzzles) passing through our layers comes in a format that we call a “<strong>multitensor</strong>”, which is just a bucket of tensors of various shapes. All the equivariances can be described in terms of how they change a multitensor. <strong>In order to understand any of the layers we list, you must first read the below section on multitensors.</strong></p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Multitensor.png" width="50%"></p>

  <h3 id="multitensors">Multitensors</h3>

  <p>Most common classes of machine learning architectures operate on a single type of tensor with constant rank. LLMs operate on rank 3 tensors of shape $[n\_batch, n\_tokens, n\_channels]$, and CNNs operate on a rank 4 tensors of shape $[n\_batch, n\_channels, height, width]$. Our multitensors are a set of varying-rank tensors of unique type, whose dimensions are a subset of a rank 6 tensor of shape $[n\_examples$, $n\_colors$, $n\_directions$, $height$, $width$, $n\_channels]$. We always keep the $channel$ dimension, so there are at most 32 tensors in every multitensor. We also maintain <a href="#rules-for-legal-multitensors">several rules</a> that determine whether a tensor shape is “legal” or not, which reduces the number of tensors in a multitensor to 18.</p>

  <table>
    <thead>
      <tr>
        <th>Dimension</th>
        <th>Size</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Example</td>
        <td>Number of examples in the ARC-AGI puzzle, including the one with held-out answer</td>
      </tr>
      <tr>
        <td>Color</td>
        <td>Number of unique colors in the ARC-AGI puzzle, <a href="#number-of-colors">not including black</a></td>
      </tr>
      <tr>
        <td>Direction</td>
        <td>8</td>
      </tr>
      <tr>
        <td>Height</td>
        <td><a href="#output-shape-determination">Determined when preprocessing the puzzle</a></td>
      </tr>
      <tr>
        <td>Width</td>
        <td><a href="#output-shape-determination">Determined when preprocessing the puzzle</a></td>
      </tr>
      <tr>
        <td>Channel</td>
        <td>In the residual connections, the size is 8 if the $direction$ dimension is included, else 16. Within layers it is layer-dependent.</td>
      </tr>
    </tbody>
  </table>

  <p>To give an idea of how a multitensor stores data, an ARC-AGI puzzle can be represented by using the $[examples, colors, height, width, channel]$ tensor, by using the $channel$ dimension to select either the input or output grid, and the $width$/$height$ dimensions for pixel location, a one hot vector in the $color$ dimension, specifying what color that pixel is. The $[examples, width, channel]$ and $[examples, height, channel]$ tensors can similarly be used to store masks representing grid shapes for every example for every input/output grid. All those tensors are included in a single multitensor that is computed by the network just before the final <a href="#linear-heads">linear heads</a> layer.</p>

  <p>When we apply an operation on a multitensor, we by default assume that all non-$channel$ dimensions are treated identically as batch dimensions by default. The operation is copied across the indices of dimensions unless specified. This ensures that we keep all our symmetries intact until we use a specific layer meant to break a specific symmetry.</p>

  <p>A final note on the $channel$ dimension: usually when talking about a tensor’s shape, we will not even mention the $channel$ dimension as it is included by default.</p>

  <p><strong>The full architecture consists of the following layers, which are each described in the Appendix:</strong></p>
  <ul>
    <li>Begin with parameters of the $z$ distribution,</li>
    <li><a href="#decoding-layer">Decoding Layer</a></li>
    <li>4x
      <ul>
        <li><a href="#multitensor-communication-layer">Multitensor Communication Layer, Upwards</a></li>
        <li><a href="#softmax-layer">Softmax Layer</a></li>
        <li><a href="#directional-cummaxshift-layer">Directional Cummax Layer</a></li>
        <li><a href="#directional-cummaxshift-layer">Directional Shift Layer</a></li>
        <li><a href="#directional-communication-layer">Directional Communication Layer</a></li>
        <li><a href="#nonlinear-layer">Nonlinear Layer</a></li>
        <li><a href="#multitensor-communication-layer">Multitensor Communication Layer, Downwards</a></li>
        <li><a href="#normalization-layer">Normalization Layer</a></li>
      </ul>
    </li>
    <li><a href="#linear-heads">Linear Heads</a></li>
  </ul>

  

  <h2 id="results">Results</h2>

  <h3 id="training-set-3475">Training set: 34.75%</h3>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/accuracy_curve_at_n_training.png">
</p>

  <div>
  <table>
    <tbody><tr>
      <th>Training Iteration</th>
      <th>Time</th>
      <th>Pass@1</th>
      <th>Pass@2</th>
      <th>Pass@5</th>
      <th>Pass@10</th>
      <th>Pass@100</th>
      <th>Pass@1000</th>
    </tr>
    <tr>
      <td>100</td>
      <td>6 h</td>
      <td>1%</td>
      <td>2.25%</td>
      <td>3.5%</td>
      <td>4.75%</td>
      <td>6.75%</td>
      <td>6.75%</td>
    </tr>
    <tr>
      <td>200</td>
      <td>13 h</td>
      <td>11.5%</td>
      <td>14.25%</td>
      <td>16.5%</td>
      <td>18.25%</td>
      <td>23.25%</td>
      <td>23.5%</td>
    </tr>
    <tr>
      <td>300</td>
      <td>19 h</td>
      <td>18.5%</td>
      <td>21.25%</td>
      <td>23.5%</td>
      <td>26.75%</td>
      <td>31.5%</td>
      <td>32.5%</td>
    </tr>
    <tr>
      <td>400</td>
      <td>26 h</td>
      <td>21%</td>
      <td>25%</td>
      <td>28.75%</td>
      <td>31%</td>
      <td>36%</td>
      <td>37.5%</td>
    </tr>
    <tr>
      <td>500</td>
      <td>32 h</td>
      <td>23%</td>
      <td>27.5%</td>
      <td>31.5%</td>
      <td>33.5%</td>
      <td>39.25%</td>
      <td>40.75%</td>
    </tr>
    <tr>
      <td>750</td>
      <td>49 h</td>
      <td>28%</td>
      <td>30.5%</td>
      <td>34%</td>
      <td>36.25%</td>
      <td>42.75%</td>
      <td>44.5%</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>65 h</td>
      <td>28%</td>
      <td>31.75%</td>
      <td>35.5%</td>
      <td>37.75%</td>
      <td>43.75%</td>
      <td>46.5%</td>
    </tr>
    <tr>
      <td>1250</td>
      <td>81 h</td>
      <td>29%</td>
      <td>32.25%</td>
      <td>37%</td>
      <td>39.25%</td>
      <td>45.5%</td>
      <td>49.25%</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>97 h</td>
      <td>29.5%</td>
      <td>33%</td>
      <td>38.25%</td>
      <td>40.75%</td>
      <td>46.75%</td>
      <td>51.75%</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>130 h</td>
      <td>30.25%</td>
      <td>34.75%</td>
      <td>38.25%</td>
      <td>41.5%</td>
      <td>48.5%</td>
      <td>52.75%</td>
    </tr>
  </tbody></table>
</div>

  <h3 id="evaluation-set-20">Evaluation set: 20%</h3>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/accuracy_curve_at_n_evaluation.png">
</p>

  <div>
  <table>
    <tbody><tr>
      <th>Training Iteration</th>
      <th>Time</th>
      <th>Pass@1</th>
      <th>Pass@2</th>
      <th>Pass@5</th>
      <th>Pass@10</th>
      <th>Pass@100</th>
      <th>Pass@1000</th>
    </tr>
    <tr>
      <td>100</td>
      <td>7 h</td>
      <td>0.75%</td>
      <td>1.25%</td>
      <td>2.25%</td>
      <td>2.5%</td>
      <td>3%</td>
      <td>3%</td>
    </tr>
    <tr>
      <td>200</td>
      <td>14 h</td>
      <td>5%</td>
      <td>6%</td>
      <td>7%</td>
      <td>7.75%</td>
      <td>12%</td>
      <td>12.25%</td>
    </tr>
    <tr>
      <td>300</td>
      <td>21 h</td>
      <td>10%</td>
      <td>10.75%</td>
      <td>12.25%</td>
      <td>13.25%</td>
      <td>15.5%</td>
      <td>16.25%</td>
    </tr>
    <tr>
      <td>400</td>
      <td>28 h</td>
      <td>11.75%</td>
      <td>13.75%</td>
      <td>16%</td>
      <td>17%</td>
      <td>19.75%</td>
      <td>20%</td>
    </tr>
    <tr>
      <td>500</td>
      <td>34 h</td>
      <td>13.5%</td>
      <td>15%</td>
      <td>17.75%</td>
      <td>19.25%</td>
      <td>20.5%</td>
      <td>21.5%</td>
    </tr>
    <tr>
      <td>750</td>
      <td>52 h</td>
      <td>15.5%</td>
      <td>17.75%</td>
      <td>19.75%</td>
      <td>21.5%</td>
      <td>22.75%</td>
      <td>25.5%</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>69 h</td>
      <td>16.75%</td>
      <td>19.25%</td>
      <td>21.75%</td>
      <td>23%</td>
      <td>26%</td>
      <td>28.75%</td>
    </tr>
    <tr>
      <td>1250</td>
      <td>86 h</td>
      <td>17%</td>
      <td>20.75%</td>
      <td>23%</td>
      <td>24.5%</td>
      <td>28.25%</td>
      <td>30.75%</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>103 h</td>
      <td>18.25%</td>
      <td>21.5%</td>
      <td>24.25%</td>
      <td>25.5%</td>
      <td>29.5%</td>
      <td>31.75%</td>
    </tr>
    <tr>
      <td>2000</td>
      <td>138 h</td>
      <td>18.5%</td>
      <td>20%</td>
      <td>24.25%</td>
      <td>26%</td>
      <td>31.25%</td>
      <td>33.75%</td>
    </tr>
  </tbody></table>
</div>

  <h3 id="what-puzzles-can-and-cant-we-solve">What Puzzles Can and Can’t We Solve?</h3>

  <p><strong>CompressARC tries to use its abilities to figure out as much as it can, until it gets bottlenecked by one of it’s inabilities.</strong></p>

  <p>For example, puzzle 28e73c20 in the training set requires extension of a pattern from the edge towards the middle:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_problem.png">
</p>

  <p>Given the layers in it’s network, CompressARC is generally able to extend patterns for short ranges but not long ranges. So, it does the best that it can, and correctly extends the pattern a short distance before guessing at what happens near the center:</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_solutions.png" alt="image"></p>

  <p>A short list of abilities that <strong>can</strong> be performed by CompressARC includes:</p>
  <ul>
    <li>Assigning individual colors to individual procedures (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0ca9ddb6</a>)</li>
    <li>Infilling (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0dfd9992</a>)</li>
    <li>Cropping (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1c786137</a>)</li>
    <li>Connecting dots with lines, including 45 degree diagonal lines (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1f876c06</a>)</li>
    <li>Same color detection (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">1f876c06</a>)</li>
    <li>Identifying pixel adjacencies (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">42a50994</a>)</li>
    <li>Assigning individual colors to individual examples (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">3bd67248</a>)</li>
    <li>Identifying parts of a shape (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">025d127b</a>)</li>
    <li>Translation by short distances (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">025d127b</a>)</li>
  </ul>

  <p>A short list of abilities that <strong>cannot</strong> be performed by CompressARC includes:</p>
  <ul>
    <li>Assigning two colors to each other (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0d3d703e</a>)</li>
    <li>Repeating an operation in series many times (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">0a938d79</a>)</li>
    <li>Counting/numbers (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">ce9e57f2</a>)</li>
    <li>Translation, rotation, reflections, rescaling, image duplication (see puzzles <a href="#list-of-mentioned-arc-agi-puzzles">0e206a2e</a>, <a href="#list-of-mentioned-arc-agi-puzzles">5ad4f10b</a>, and <a href="#list-of-mentioned-arc-agi-puzzles">2bcee788</a>)</li>
    <li>Detecting topological properties such as connectivity (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">7b6016b9</a>)</li>
    <li>Planning, simulating the behavior of an agent (see puzzle <a href="#list-of-mentioned-arc-agi-puzzles">2dd70a9a</a>)</li>
    <li>Long range extensions of patterns (see puzzle 28e73c20 above)</li>
  </ul>

  

  <h2 id="case-study-color-the-boxes">Case Study: Color the Boxes</h2>

  <p>(Additional case studies can be found in the <a href="#additional-case-studies">Appendix</a>.)</p>

  <p>We show the puzzle again for convenience.</p>
  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_problem.png">
</p>

  <p>During training, the reconstruction error fell extremely quickly. It remained low on average, but would spike up every once in a while, causing the KL from $z$ to bump upwards at these moments.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_KL_vs_reconstruction.png">
</p>

  <h3 id="solution-analysis-color-the-boxes">Solution Analysis: Color the Boxes</h3>

  <p>So how does CompressARC learn to solve the puzzle? Let’s look at the representations stored in $z$ to find out.</p>

  <p>Since $z$ is a <a href="#multitensors">multitensor</a>, each of the tensors it contains produces an additive contribution to the total KL for $z$. By looking at the per-tensor contributions, we can determine which tensors in $z$ code for information that is used to represent the puzzle. Below is a plot showing the quantity of information stored in each tensor of $z$, ie. the KL contribution used by the <a href="#decoding-layer">decoding layer</a>.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_KL_components.png">
</p>

  <p>All the tensors fall to zero information content during training, except for four tensors. In some replications of this experiment, we saw one of these four necessary tensors fall to zero information content, and CompressARC typically does not recover the correct answer after that. Here we are showing a lucky run where the $(color, direction, channel)$ tensor almost falls but gets picked up 200 steps in, which is right around when the samples from the model begin to show the correct colors in the correct boxes.</p>

  <p>We can look at the average output of the <a href="#decoding-layer">decoding layer</a> corresponding to individual tensors of $z$, to see what information is stored there. Each tensor contains a vector of dimension $n\_channels$ for various indices of the tensor. Taking the PCA of these vectors reveals some number of activated components, telling us how many pieces of information are coded by the tensor.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, channel) tensor:</strong>
  <p>
  For every example and row, there is a vector of dimension $n\_channels$. This forms a dataset of vectors. Taking the PCA of these vectors, the top principal component vector reformatted back into an $(examples, height)$ matrix (shown on right) can tell us which examples/row combinations are uniquely identified by the stored information. The top principal component (shown on right) is 1485 times stronger than the second principal component, which indicates to us that basically all of the information is in the above tensor. <strong>For every example, the two brightest pixels give the rows where the light blue rows in the grids are.</strong></p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_example_height_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Examples, width, channel) tensor:</strong>
  <p>
  A very similar story here: in the top principal component of this tensor, <strong>the two darkest pixels for every example give the columns where the light blue columns in the grids are.</strong> The top principal component is 1253 times stronger than the next principal component.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_example_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Direction, color, channel) tensor:</strong>
  <p>
  In this tensor, we see that the four brightest pixels identify blue with up, green with left, red with down, and yellow with right. <strong>This tensor seems to tell each direction which color to use for the opposite direction's corresponding box.</strong> The top principal component is 829 times stronger than the next principal component.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_direction_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  Here, we look at the top three principal components, since the first and second principal components are 134 and 87 times stronger than the third component, indicating that they play a role while the third component does not. The <strong>magenta and light blue colors</strong> are uniquely identified, indicating their special usage amongst the rest of the colors as <strong>the center color and the color of the row/column divisions</strong>, respectively.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_component_0.png"><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_color_component_1.png"></td>
  </tr>
</tbody></table>

  

  <h2 id="how-to-improve-our-work">How to Improve Our Work</h2>

  <p>At the time of release of CompressARC, there were several ideas which we thought of trying or attempted at some point, but didn’t manage to get working for one reason or another. Some ideas we still believe in, but didn’t use, are listed below.</p>

  <h4 id="joint-compression-via-weight-sharing-between-puzzles">Joint Compression via Weight Sharing Between Puzzles</h4>

  <p>CompressARC tries to solve each puzzle serially by compressing each puzzle on its own. We believe that joint compression of all the entire ARC-AGI dataset at once should yield better learned inductive biases per-puzzle, since computations learned for one puzzle can be transferred to other puzzles. We do not account for the complexity of $f$ in our <a href="#how-to-derive-our-solution-method">method</a>, allowing for $f$ to be used for memorization/overfitting. By jointly compressing the whole dataset, we only need to have one $f$, whereas when compressing each puzzle individually, we need to have an $f$ for every puzzle, allowing for more memorization/overfitting.</p>

  <p>To implement this, we would most likely explore strategies like:</p>
  <ul>
    <li>Using the same network weights for all puzzles, and training for puzzles in parallel. Each puzzle gets assigned some perturbation to the weights, that is constrained in some way, e.g., <a href="https://arxiv.org/abs/2106.09685">LORA</a>.</li>
    <li>Learning a “puzzle embedding” for every puzzle that is a high dimensional vector (more than 16 dim, less than 256 dim), and learning a linear mapping from puzzle embeddings to weights for our network. This mapping serves as a basic <a href="https://arxiv.org/abs/2306.06955">hypernetwork</a>, ie. a neural network that outputs weights for another neural network.
In a successful case, we might want to also try adding in some form of positional encodings, with the hope that $f$ is now small/simple enough to be incapable of memorization/overfitting using positional encodings.</li>
  </ul>

  <p>The reason we didn’t try this is because it would slow down the research iteration process.</p>

  <h4 id="convolution-like-layers-for-shape-copying-tasks">Convolution-like Layers for Shape Copying Tasks</h4>

  <p>This improvement is more ARC-AGI-specific and may have less to do with AGI in our view. Many ARC-AGI puzzles can be seen to involve copying shapes from one place to another, and our network has no inductive biases for such an operation. An operation which is capable of copying shapes onto multiple locations is the <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a>. With one grid storing the shape and another with pixels activated at locations to copy to, convolving the two grids will produce another grid with the shape copied to the designated locations.</p>

  <p>There are several issues with introducing a convolutional operation for the network to use. Ideally, we would read two grids via projection from the residual stream, convolve them, and write it back in via another projection, with norms in the right places and such. Ignoring the fact that the grid size changes during convolution (can be solved with two parallel networks using different grid sizes), the bigger problem is that convolutions tend to amplify noise in the grids much more than the sparse signals, so their inductive bias is not good for shape copying. We can try to apply a softmax to one or both of the grids to reduce the noise (and to draw an interesting connection to attention), but we didn’t find any success.</p>

  <p>The last idea that we were tried before discarding the idea was to modify the functional form of the convolution:</p><p>

\[(f * g)(x) = \sum_y f(x-y)g(y)\]

  </p><p>to <a href="https://arxiv.org/abs/2103.02096">a tropical convolution</a>, which we found to work well on toy puzzles, but not well enough for ARC-AGI training puzzles (which is why we discarded this idea):</p><p>

\[(f*g)(x) = \max_y f(x-y) + g(y)\]

  </p><p>Convolutions, when repeated with some grids flipped by 180 degrees, tend to create high activations at the center pixel, so sometimes it is important to zero out the center pixel to preserve the signal.</p>

  <h4 id="kl-floor-for-posterior-collapse">KL Floor for Posterior Collapse</h4>

  <p>We noticed during testing that crucial posterior tensors whose <a href="https://arxiv.org/abs/1711.00937">KL fell to zero during learning</a> would never make a recovery and play their role in the encoding. We believe that the KL divergence may upper bound the information content of the gradient training signal for parts of the network that process the encoded information. Thus, when a tensor falls to zero KL, the network stops learning to use its information, so the KL is no longer given encouragement to recover. If we can hold the KL above zero for a while, the network may then learn to use the information, giving the KL a reason to stay above zero when released again.</p>

  <p>We implemented a mechanism to keep the KL above a minimum threshold so that the network always learns to use that information, but we do not believe it learns fast enough for this to be useful, as we have never seen a tensor recover before. Therefore, it might be useful to explore different ways to schedule this KL floor to start high and decay to zero, to allow learning when the KL is forced to be high, and to leave the KL unaffected later on in learning. This might cause training results to be more consistent across runs.</p>

  <h4 id="regularization">Regularization</h4>

  <p>We don’t use it. Maybe it matters, but we don’t know. Regularization measures the complexity of $f$ in our <a href="#how-to-derive-our-solution-method">problem formulation</a>, and is native to our derivation of CompressARC. It is somewhat reckless for us to exclude it in our implementation.</p>

  

  

  <h4 id="equivalence-of-compression-and-intelligence">Equivalence of Compression and Intelligence</h4>

  <p>The original inspiration of this work came from the <a href="http://prize.hutter1.net/">Hutter Prize</a>, which awards a prize for those who can compress a file of Wikipedia text the most, as a motivation for researchers to build intelligent systems. It is premised upon the idea that the ability to compress information is equivalent to intelligence.</p>

  <p>This equivalence between inteeligence and compression has a long history. For example, when talking about intelligent solutions to prediction problems, the ideal predictor implements <a href="https://www.sciencedirect.com/science/article/pii/S0019995864902232">Solomonoff Induction</a>, a theoretically best possible but uncomputable prediction algorithm that works universally for all prediction tasks. This prediction algorithm is then equivalent to a best possible compression algorithm whose compressed code length is the <a href="https://www.sciencedirect.com/science/article/pii/S0304397598000759?via%3Dihub">Kolmogorov Complexity</a> of the data. In our work, we try to approximate this best possible compression algorithm with a neural network. A related measure of complexity is known as the <a href="https://www.sciencedirect.com/science/article/abs/pii/0005109878900055?via%3Dihub">Minimum Description Length</a>.</p>

  <h4 id="information-theory-and-coding-theory">Information Theory and Coding Theory</h4>

  <p>Since we build an information compression system, we make use of many results in information theory and coding theory. The main result required to motivate our model architecture is the existence of <a href="https://arxiv.org/abs/2010.01185">Relative Entropy Coding</a> (REC). The fact that REC exists means that as long as a KL divergence can be bounded, the construction of a compression algorithm is always possible and the issue of realizing the algorithm can be abstracted away. Thus, problems about coding theory and translating information from Gaussians into binary and back can be ignored, since we can figure out the binary code length directly from the Gaussians instead. In other words, we only need to do enough information theory using the Gaussians to get the job done, with no coding theory at all. While the existence of <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic coding</a> would suffice to abstract the problem away when distributions are discrete, neural networks operate in a continuous space so we need REC instead.</p>

  <p>Our architecture sends $z$ information through an additive white Gaussian noise (AWGN) channel, so the <a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem">AWGN channel capacity formula</a> (Gaussian input Gaussian noise) plays a heavy role in the design of our <a href="#decoding-layer">decoding layer</a>.</p>

  <h4 id="variational-autoencoders">Variational Autoencoders</h4>

  <p>The decoder side of the <a href="https://arxiv.org/abs/1312.6114">variational autoencoder</a> serves as our decompression algorithm. While we would use something that has more general capabilities like a <a href="https://arxiv.org/abs/1410.5401">neural Turing machine instead</a>, neural Turing machines are not very amenable to gradient descent-based optimization so we stuck with the VAE.</p>

  <p>VAEs have a long history of developments that are relevant to our work. At one point, we tried using multiple <a href="#decoding-layer">decoding layers</a> to make a <a href="https://arxiv.org/abs/1602.02282">hierarchical VAE</a> decoder instead. This does not affect Relative Entropy Coding with the AWGN channel because <a href="https://ieeexplore.ieee.org/document/1056798">channel capacity with feedback is equal to channel capacity without feedback</a>. But, we found empirically that the first decoding layer would absorb all of the KL contribution, making the later decoding layers useless. Thus, we only used one decoding layer at the beginning.</p>

  <p>The <a href="https://openreview.net/forum?id=Sy2fzU9gl">beta-VAE</a> introduces a reweighting of the reconstruction loss to be stronger than the KL loss, and we found that to work well in our case. The <a href="https://arxiv.org/abs/2007.03898">NVAE</a> applies a non-constant weighting to loss components. A rudimentary form of scheduled loss recombination is used in CompressARC.</p>

  <h4 id="arc-agi-methods">ARC-AGI Methods</h4>

  <p>Current methods for solving ARC-AGI focus primarily on using large language models (LLMs). ARC-AGI puzzles are converted into textual representations which are fed into LLMs as input. The LLM may directly output a textual representation of an answer, or some <a href="https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt">code which tries to convert input grids into output grids</a>. Top methods rely heavily on data augmentation and larger <a href="https://arxiv.org/abs/2411.02272">alternative datasets</a>, and sometimes perform autoregressive training on the target puzzle during inference time. Top solutions (<a href="https://ironbar.github.io/arc24/05_Solution_Summary/">example</a>) in the 2024 Kaggle prize competition frequently used <a href="https://arxiv.org/abs/1909.13231">test-time training</a>. <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">Reasoning models</a> have managed to get up to 87.5% on the semi-private evaluation set, albeit with astronomical amounts of compute.</p>

  <p>An older class of methods consists of hard-coded searches through program spaces in <a href="https://github.com/michaelhodel/arc-dsl">hand-written domain-specific languages designed specifically for ARC</a>. Another example <a href="https://github.com/victorvikram/ARC-icecuber">here</a>.</p>

  <p><a href="https://arxiv.org/html/2411.08706v1">Bonnet and Macfarlane introduced a VAE-based method</a> for searching through a latent space of programs.</p>

  <p>We believe CompressARC is the only method so far that uses deep learning without external pretraining nor any large-scale search.</p>

  <h4 id="deep-learning-architectures">Deep Learning Architectures</h4>

  <p>We designed our own neural network architecture from scratch, but not without borrowing crucial design principles from many others.</p>

  <p>Our architecture is fundamentally structured like a <a href="https://arxiv.org/abs/1706.03762">transformer</a>, consisting of a <a href="https://arxiv.org/abs/1512.03385">residual stream</a> where representations are stored and operated upon, followed by a linear head. <a href="https://arxiv.org/abs/2002.04745">Pre-and post-norms</a> with linear up- and down-projections allow layers to read and write to the residual stream. The <a href="https://arxiv.org/abs/1606.08415">SiLU</a>-based <a href="#nonlinear-layer">nonlinear layer</a> is especially similar to a transformer’s.</p>

  <p>Our equivariance structures are inspired by <a href="https://arxiv.org/abs/1703.06114">permutation-invariant neural networks</a>, which are a type of <a href="https://arxiv.org/abs/1602.07576">equivariant neural network</a>. Equivariance transformations are taken from common augmentations to ARC-AGI puzzles.</p>

  

  <hr>
  

  <h2 id="appendix">Appendix</h2>

  <h3 id="layers-in-the-architecture">Layers in the Architecture</h3>

  <h4 id="decoding-layer">Decoding Layer</h4>

  <p>This layer’s job is to sample a multitensor $z$ and bound its information content, before it is passed to the next layer. This layer and outputs the KL divergence between the learned $z$ distribution and $N(0,I)$. Penalizing the KL prevents CompressARC from learning a distribution for $z$ that memorizes the ARC-AGI puzzle in an uncompressed fashion, and forces it to represent the puzzle more succinctly. Specifically, it forces CompressARC to spend more bits on the KL whenever it uses $z$ to break a symmetry, and the larger the symmetry group broken, the more bits it spends.</p>

  <p>This layer takes as input:</p>
  <ul>
    <li>A learned target multiscalar, called the “target capacity”.<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" rel="footnote">2</a></sup> The decoding layer will output $z$ whose information content per tensor is close to the target capacity,<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" rel="footnote">3</a></sup></li>
    <li>learned per-element means for $z$,<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" rel="footnote">4</a></sup></li>
    <li>learned per-element capacity adjustments for $z$.</li>
  </ul>

  <p>We begin by normalizing the learned per-element means for $z$.<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" rel="footnote">5</a></sup> Then, we figure out how much Gaussian noise we must add into every tensor to make the <a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem">AWGN channel capacity</a> equal to the target capacity for every tensor (including per-element capacity adjustments). We apply the noise to sample $z$, keeping unit variance of $z$ by rescaling.<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" rel="footnote">6</a></sup></p>

  <p>We compute the information content of $z$ as the KL divergence between the distribution of this sample and $N(0,1)$.</p>

  <p>Finally, we postprocess the noisy $z$ by scaling it by the sigmoid of the signal-to-noise ratio.<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" rel="footnote">7</a></sup> This ensures that $z$ is kept as-is when its variance consists mostly of useful information and it is nearly zero when its variance consists mostly of noise. All this is done 4 times to make a $channel$ dimension of 4. Then we apply a projection (with different weights per tensor in the multitensor, ie. per-tensor projections) mapping the $channel$ dimension up to the dimension of the residual stream.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Multitensor_Sharing.png" width="50%"></p>

  <h4 id="multitensor-communication-layer">Multitensor Communication Layer</h4>

  <p>This layer allows different tensors in a multitensor to interact with each other.</p>

  <p>First, the input from the residual stream passes through per-tensor projections to a fixed size (8 for downwards communication and 16 for upwards communication). Then a message is sent to every other tensor that has at least the same dimensions for upwards communication, or at most the same dimensions for downwards communication. This message is created by either taking means along dimensions to remove them, or unsqueezing+broadcasting dimensions to add them. All the messages received by every tensor are summed together and normalization is applied. This result gets up-projected back and then added to the residual stream.</p>

  <h4 id="softmax-layer">Softmax Layer</h4>

  <p>This layer allows the network to work with internal one-hot representations, by giving it the tools to denoise and sharpen noisy one-hot vectors. For every tensor in the input multitensor, this layer lists out all the possible subsets of dimensions of the tensor to take a softmax over<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" rel="footnote">8</a></sup>, takes the softmax over these subsets of dimensions, and concatenates all the softmaxxed results together in the $channel$ dimension. The output dimension varies across different tensors in the multitensor, depending on their tensor rank. A pre-norm is applied, and per-tensor projections map to and from the residual stream. The layer has input $channel$ dimension of 2.</p>

  <h4 id="directional-cummaxshift-layer">Directional Cummax/Shift Layer</h4>

  <p>The directional cummax and shift layers allow the network to perform the non-equivariant cummax and shift operations in an equivariant way, namely by applying the operations once per direction, and only letting the output be influenced by the results once the directions are aggregated back together (by the <a href="#multitensor-communication-layer">multitensor communication layer</a>). These layers are the sole reason we included the $direction$ dimension when defining a multitensor: to store the results of directional layers and operate on each individually. Of course, this means when we apply a spatial equivariance transformation, we must also permute the indices of the $direction$ dimension accordingly, which can get complicated sometimes.</p>

  <p>The directional cummax layer takes the eight indices of the $direction$ dimension, treats each slice as corresponding to one direction (4 cardinal, 4 diagonal), performs a cumulative max in the respective direction for each slice, does it in the opposite direction for half the channels, and stacks the slices back together in the $direction$ dimension. The slices are rescaled to have min $-1$ and max $1$ before applying the cumulative max.</p>

  <p>The directional shift layer does the same thing, but for shifting the grid by one pixel instead of applying the cumulative max, and without the rescaling.</p>

  <p>Some details:</p>
  <ul>
    <li>Per-tensor projections map to and from the residual stream, with pre-norm.</li>
    <li>Input $channel$ dimension is 4</li>
    <li>These layers are only applied to the $[example, color, direction, height, width, channel]$ and $[example, direction, height, width, channel]$ tensors in the input multitensor.</li>
  </ul>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Directional_Shift_Cummax.png" alt="image"></p>

  <h4 id="directional-communication-layer">Directional Communication Layer</h4>

  <p>By default, the network is equivariant to permutations of the eight directions, but we only want symmetry up to rotations and flips. So, this layer provides a way to send information between two slices in the $direction$ dimension, depending on the angular difference in the two directions. This layer defines a separate linear map to be used for each of the 64 possible combinations of angles, but the weights of the linear maps are minimally tied such that the directional communication layer is equivariant to reflections and rotations. This gets complicated really fast, since the $direction$ dimension’s indices also permute when equivariance transformations are applied. Every direction slice in a tensor accumulates it’s 8 messages, and adds the results together.<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" rel="footnote">9</a></sup></p>

  <p>For this layer, there are per-tensor projections to and from the residual stream with pre-norm. The input $channel$ dimension is 2.</p>

  <h4 id="nonlinear-layer">Nonlinear Layer</h4>

  <p>We use a SiLU nonlinearity with $channel$ dimension 16, surrounded by per-tensor projections with pre-norm.</p>

  <h4 id="normalization-layer">Normalization Layer</h4>

  <p>We normalize all the tensors in the multitensor, using means and variances computed across all dimensions except the $channel$ dimension. Normalization as used within other layers also generally operates this way.</p>

  <h4 id="linear-heads">Linear Heads</h4>

  <p>We must take the final multitensor, and convert it to the format of an ARC-AGI puzzle. More specifically, we must convert the multitensor into a distribution over ARC-AGI puzzles, so that we can compute the log-likelihood of the observed grids in the puzzle.</p>

  <p><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/Linear_heads.png" width="50%"></p>

  <p>The colors of every pixel for every example for both input and output, have logits defined by the $[examples, colors, height, width, channel]$ tensor, with the $channel$ dimension linearly mapped down to a size of 2, representing the input and output grids.<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" rel="footnote">10</a></sup> The log-likelihood is given by the crossentropy, with sum reduction across all the grids.</p>

  <p>For grids of non-constant shape, the $[examples, width, channel]$ and $[examples, height, channel]$ tensors are used to create distributions over possible contiguous rectangular slices of each grid of colors. Again, the $channel$ dimension is mapped down to a size of 2 for input and output grids. For every grid, we have a vector of size $[width]$ and a vector of size $[height]$. The log likelihood of every slice of the vector is taken to be the sum of the values within the slice, minus the values outside the slice. The log likelihoods for all the possible slices are then normalized to have total probability one, and the colors for every slice are given by the color logits defined in the previous paragraph.</p>

  <p>With the puzzle distribution now defined, we can now evaluate the log-likelihood of the observed target puzzle, to use as the reconstruction error.<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" rel="footnote">11</a></sup></p>

  <h3 id="other-architectural-details">Other Architectural Details</h3>

  <h4 id="rules-for-legal-multitensors">Rules for legal multitensors</h4>

  <ol>
    <li>At least one non-$example$ dimension must be included. Examples are not special for any reason not having to do with colors, directions, rows, and columns.</li>
    <li>If the $width$ or $height$ dimension is included, the $example$ dimension should also be included. Positions are intrinsic to grids, which are indexed by the $example$ dimension. Without a grid it doesn’t make as much sense to talk about positions.</li>
  </ol>

  <h4 id="weight-tying-for-reflectionrotation-symmetry">Weight Tying for Reflection/Rotation Symmetry</h4>

  <p>When applying a different linear layer to every tensor in a multitensor, we have a linear layer for tensors having a $width$ but not $height$ dimension, and another linear layer for tensors having a $height$ but not $width$ dimension. Whenever this is the case, we tie the weights together in order to preserve the whole network’s equivariance to diagonal reflections and 90 degree rotations, which swap the $width$ and $height$ dimensions.</p>

  <p>The softmax layer is not completely symmetrized because different indices of the output correspond to different combinations of dimension to softmax over. Tying the weights properly would be a bit complicated and time consuming for the performance improvement we expect, so we did not do this.</p>

  <h4 id="training">Training</h4>

  <p>We train for 2000 iterations using Adam, with learning rate 0.01, $\beta_1$ of 0.5, and $\beta_2$ of 0.9.</p>

  <h3 id="preprocessing">Preprocessing</h3>

  <h4 id="output-shape-determination">Output Shape Determination</h4>

  <p>The raw data consists of grids of various shapes, while the neural network operates on grids of constant shape. Most of the preprocessing that we do is aimed towards this shape inconsistency problem.</p>

  <p>Before doing any training, we determine whether the given ARC-AGI puzzle follows three possible shape consistency rules:</p>
  <ol>
    <li>The outputs in a given ARC-AGI puzzle are always the same shape as corresponding inputs.</li>
    <li>All the inputs in the given ARC-AGI puzzle are the same shape.</li>
    <li>All the outputs in the given ARC-AGI puzzle are the same shape.</li>
  </ol>

  <p>Based on rules 1 and 3, we try to predict the shape of held-out outputs, prioritizing rule 1 over rule 3. If either rule holds, we force the postprocessing step to only consider the predicted shape by overwriting the masks produced by the <a href="#linear-heads">linear heads</a>. If neither rule holds, we make a temporary prediction of the largest width and height out of the grids in the given ARC-AGI puzzle, and we allow the masks to predict shapes that are smaller than that.</p>

  <p>The largest width and height that is given or predicted, are used as the size of the <a href="#multitensors">multitensor</a>’s $width$ and $height$ dimensions.</p>

  <p>The predicted shapes are also used as masks when performing the <a href="#multitensor-communication-layer">multitensor communication</a>, <a href="#directional-communication-layer">directional communication</a> and <a href="#directional-cummaxshift-layer">directional cummax/shift</a> layers<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" rel="footnote">12</a></sup>. We did not apply masks for the other layers because of time constraints and because we do not believe it will provide for much of a performance improvement.</p>

  <h4 id="number-of-colors">Number of Colors</h4>

  <p>We notice that in almost all ARC-AGI puzzles, colors that are not present in the puzzle are not present in the true answers. Hence, any colors that do not appear in the puzzle are not given an index in the $color$ dimension of the <a href="#multitensors">multitensor</a>.</p>

  <p>In addition, black is treated as a special color that is never included in the multitensor, since it normally represents the background in many puzzles. When performing color classification, a tensor of zeros is appended to the $color$ dimension after applying the <a href="#linear-heads">linear head</a>, to represent logits for the black color.</p>

  <h3 id="postprocessing">Postprocessing</h3>

  <p>Postprocessing primarily deals with denoising the answers sampled from the network. There are also <a href="#linear-heads">some operations</a> performed to convert the constant-shape grids outputted by the network to the variable shape grids present in some puzzles.</p>

  <p>Generally, when we sample answers from the network by taking the logits of the $[examples, colors, height, width, channels]$ tensor and argmaxxing over the $color$ dimension, we find that the grids are noisy and will often have the wrong colors for several random pixels. We developed several methods for removing this noise:</p>
  <ol>
    <li>Find the most commonly sampled answer.</li>
    <li>Construct an exponential moving average of the output color logits before taking the softmax to produce probabilities. Also construct an exponential moving average of the masks.</li>
    <li>Construct an exponential moving average of the output color probabilities after taking the softmax. Also construct an exponential moving average of the masks.</li>
  </ol>

  <p>When applying these techniques, we always take the slice of highest probability given the mask, and then we take the colors of highest probability afterwards.</p>

  <p>We explored several different rules for when to select which method, and arrived at a combination of 1 and 2 with a few modifications:</p>
  <ul>
    <li>At every iteration, count up the sampled answer, as well as the exponential moving average answer (decay $=0.97$).</li>
    <li>If before 150 iterations of training, then downweight the answer by a factor of $e^{-10}$. (Effectively, don’t count the answer.)</li>
    <li>If the answer is from the exponential moving average as opposed to the sample, then downweight the answer by a factor of $e^{-4}$.</li>
    <li>Downweight the answer by a factor of $e^{-10*uncertainty}$, where $uncertainty$ is the average (across pixels) negative log probability assigned to the top color of every pixel.</li>
  </ul>

  <h3 id="what-happens-to-the-representations-during-learning">What Happens to the Representations during Learning</h3>

  <p>During training, the gradient descent tries to find representations of the puzzle that require less and less information to encode. This information is measured by the KL term for $z$, plus the a heavily penalized reconstruction error.</p>

  <p>Due to the 10x penalization on reconstruction error, and the initial high capacity for $z$, the $z$ distribution (which we call the “posterior”) quickly learns the information that is required to perfectly reconstruct the given input/output pairs in the puzzle, within the first 20 or so steps. The remainder of the training steps are about compressing $z$ information under the constraint of perfect reconstruction, by tuning the representations to be more concise.</p>

  <p>Our mental model of how gradient descent compresses the $z$ information consists of several steps which we list below:</p>
  <ol>
    <li>Suppose the posterior $p$ originally codes for some number $n$ pieces of information $z_1, \dots, z_n$ using thin Gaussians.</li>
    <li>The posterior widens and becomes more noisy to try to get closer to the wide Gaussian “prior” $q=N(0,1)$, but since all $n$ pieces of information are needed to ensure good reconstruction, the noise is limited by the reconstruction loss incurred.</li>
    <li>The ever-widening posteriors push the neurons to become more and more resilient to noise, until some limit is reached.</li>
    <li>Learning remains stagnant for a while, as a stalemate between compression and reconstruction.</li>
    <li>If it turns out that $z_1$ is not reconstructible using $z_2, \dots, z_n$, then stop. Else, proceed to step 6.</li>
    <li>The neurons, pushed by the widening posterior of $z_1$, figure out a procedure to denoise $z_1$ using information from $z_2, \dots, z_n$, in the event that the noise sample for $z_1$ is too extreme.</li>
    <li>The posterior for the last piece keeps pushing wider, producing more extreme values for $z_1$, and the denoising procedure is improved, until the $z_1$ representation consists completely of noise, and its usage in the network is replaced by the output of the denoising procedure.</li>
    <li>The posterior for $z_1$ is now identical to the prior, so nothing is coded in $z_1$ and it no longer contributes to the KL loss.</li>
    <li>The posterior now codes for $n-1$ pieces of information $z_2, \dots, z_n$, and compression has occurred.</li>
  </ol>

  <p>These steps happen repeatedly for different unnecessarily coded pieces of information, until there are no more. More than one piece of information can be compressed away at once, and there is no need for the steps to proceed serially. The process stops when all information coded by the posterior is unique, and no piece is reconstructable using the others.</p>

  

  <h2 id="additional-case-studies">Additional Case Studies</h2>

  <p>Below, we show two additional puzzles and a dissection of CompressARC’s solution to them.</p>

  <h3 id="case-study-bounding-box">Case Study: Bounding Box</h3>

  <p>Puzzle 6d75e8bb is part of the training split.</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_problem.png" width="20%">
</p>

  <h4 id="watching-the-network-learn-bounding-box">Watching the Network Learn: Bounding Box</h4>

  <h5 id="human-solution-1">Human Solution:</h5>
  <p>We first realize that the input is red and black, and the output is also red and black, but some of the black pixels are replaced by light blue pixels. We see that the red shape remains unaffected. We notice that the light blue box surrounds the red shape, and finally that it is the smallest possible surrounding box that contains the red shape. At this point, we copy the input over to the answer grid, then we figure out the horizontal and vertical extent of the red shape, and color all of the non-red pixels within that extent as light blue.</p>

  <h5 id="compressarc-solution-1">CompressARC Solution:</h5>
  <table>
  <tbody><tr>
  <td>
  <strong> 50 steps of learning:</strong>
  <p>
  The average of sampled outputs shows that light blue pixels in the input are generally preserved in the output. However, black pixels in the input are haphazardly and randomly colored light blue and red. CompressARC does not seem to know that the colored input/output pixels lie within some kind of bounding box, or that the bounding box is the same for the input and output grids.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_50_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 100 steps of learning:</strong>
  <p>
  The average of sampled outputs shows red pixels confined to an imaginary rectangle surrounding the light blue pixels. CompressARC seems to have perceived that other examples use a common bounding box for the input and output pixels, but is not completely sure about where the boundary lies and what colors go inside the box in the output. Nevertheless, guess 2 (the second most frequently sampled output) shows that the correct answer is already being sampled quite often now.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_100_steps.png"></td>
  </tr>
  <tr>
  <td>
  <strong> 150 steps of learning:</strong>
  <p>
  The average of sampled outputs shows almost all of the pixels in the imaginary bounding box to be colored red. CompressARC has figured out the answer, and further training only refines the answer.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_at_150_steps.png"></td>
  </tr>
</tbody></table>

  <h4 id="solution-analysis-bounding-box">Solution Analysis: Bounding Box</h4>

  <p>Below is a plot of the amount of contained information in every tensor composing the latent $z$:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_KL_components.png">
</p>

  <p>All the tensors in $z$ fall to zero information content during training, except for three tensors. From 600-1000 steps, we see the $(example, height, width, channel)$ tensor suffer a massive drop in information content, with no change in the outputted answer. We believe it was being used to identify the light blue pixels in the input, but this information then got memorized by the nonlinear portions of the network, using the $(example, height, channel)$ and $(example, width, channel)$ as positional encodings.</p>

  <p>We can look at the average output of the <a href="#decoding-layer">decoding layer</a> for these tensors to see what information is stored there.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, channel) tensor:</strong>
  <p>
  The first principal component is 771 times stronger than the second principal component. <strong>A brighter pixel indicates a row with more light blue pixels.</strong> It is unclear how CompressARC knows where the borders of the bounding box are.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_example_height_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Examples, width, channel) tensor:</strong>
  <p>
  The first principal component is 550 times stronger than the second principal component. <strong>A darker pixel indicates a column with more light blue pixels.</strong> It is unclear how CompressARC knows where the borders of the bounding box are.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_example_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  This tensor serves to distinguish the roles of the two colors apart.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_color_component_0.png" width="50%"></td>
  </tr>
</tbody></table>

  

  <h3 id="case-study-center-cross">Case Study: Center Cross</h3>

  <table>
    <thead>
      <tr>
        <th>Puzzle 41e4d17e from training split</th>
        <th>Our Network’s Answer</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_problem.png" alt="image"></td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_at_1500_steps.png" alt="image"></td>
      </tr>
    </tbody>
  </table>

  <h5 id="human-solution-2">Human Solution:</h5>
  <p>We first notice that the input consists of blue “bubble” shapes (really they are just squares, but the fact that they’re blue reminds us of bubbles) on a light blue background and the output has the same. But in the output, there are now magenta rays emanating from the center of each bubble. We copy the input over to the answer grid, and then draw magenta rays starting from the center of each bubble out to the edge in every cardinal direction. At this point, we submit our answer and find that it is wrong, and we notice that in the given demonstrations, the blue bubble color is drawn on top of the magenta rays, and we have drawn the rays on top of the bubbles instead. So, we pick up the blue color and correct each point where a ray pierces a bubble, back to blue.</p>

  <h5 id="compressarc-solution-2">CompressARC Solution:</h5>
  <p>We don’t show CompressARC’s solution evolving over time because we think it is uninteresting; instead will describe. We don’t see much change in CompressARC’s answer over time during learning. It starts by copying over the input grid, and at some point, magenta rows and columns start to appear, and they slowly settle on the correct positions. At no point does CompressARC mistakenly draw the rays on top of the bubbles; it has always had the order correct.</p>

  <h4 id="solution-analysis-center-cross">Solution Analysis: Center Cross</h4>

  <p>Another plot of the amount of information in every tensor in $z$:</p>

  <p>
  <img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_KL_components.png">
</p>

  <p>The only surviving tensors are the $(color, channel)$ and $(example, height, width, channel)$ tensors.</p>

  <table>
  <tbody><tr>
  <td>
  <strong> (Examples, height, width, channel) tensor:</strong>
  <p>
  The top principal component is 2496 times stronger than the second principal component. <strong>The (examples, height, width, channel) tensor codes for the centers of the bubbles.</strong> In the KL contribution plot, we can see that the information content of this tensor is decreasing over time. Likely, CompressARC is in the process of eliminating the plus shaped representation, and replacing it with a pixel instead, which takes fewer bits.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_example_height_width_component_0.png"></td>
  </tr>
  <tr>
  <td>
  <strong> (Color, channel) tensor:</strong>
  <p>
  The $(color, channel)$ tensor just serves to distinguish the individual roles of the colors in the puzzle.
  </p></td>
  <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_color_component_0.png"><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_color_component_1.png"></td>
  </tr>
</tbody></table>

  

  <h2 id="list-of-mentioned-arc-agi-puzzles">List of Mentioned ARC-AGI Puzzles</h2>

  <p>All the puzzles we mentioned are part of the training split.</p>

  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Puzzle</th>
        <th>Name</th>
        <th>Puzzle</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>025d127b</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/025d127b_problem.png" alt="image"></td>
        <td>0a938d79</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0a938d79_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>0ca9ddb6</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0ca9ddb6_problem.png" alt="image"></td>
        <td>0d3d703e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0d3d703e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>0dfd9992</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0dfd9992_problem.png" alt="image"></td>
        <td>0e206a2e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/0e206a2e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>1c786137</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/1c786137_problem.png" alt="image"></td>
        <td>1f876c06</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/1f876c06_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>28e73c20</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/28e73c20_problem.png" alt="image"></td>
        <td>272f95fa</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/272f95fa_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>2bcee788</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2bcee788_problem.png" alt="image"></td>
        <td>2dd70a9a</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/2dd70a9a_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>3bd67248</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/3bd67248_problem.png" alt="image"></td>
        <td>41e4d17e</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/41e4d17e_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>42a50994</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/42a50994_problem.png" alt="image"></td>
        <td>5ad4f10b</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/5ad4f10b_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>6d75e8bb</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/6d75e8bb_problem.png" alt="image"></td>
        <td>7b6016b9</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/7b6016b9_problem.png" alt="image"></td>
      </tr>
      <tr>
        <td>ce9e57f2</td>
        <td><img src="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/resources/ce9e57f2_problem.png" alt="image"></td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
      </tr>
    </tbody>
  </table>

  

  <h2 id="code">Code</h2>

  <p>Code for this project is available <a href="https://github.com/iliao2345/CompressARC">here</a>.</p>

  <p>If you’d like to cite this blog post, use the following entry:</p>
  <div><pre><code>@online{liao2025arcagiwithoutpretraining,
	author = {Isaac Liao and Albert Gu},
	title = {ARC-AGI Without Pretraining},
	year = {2025},
	url = {https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html},
}
</code></pre></div>

  

  

  

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why FastDoom Is Fast (514 pts)]]></title>
            <link>https://fabiensanglard.net/fastdoom/index.html</link>
            <guid>43258709</guid>
            <pubDate>Tue, 04 Mar 2025 19:05:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fabiensanglard.net/fastdoom/index.html">https://fabiensanglard.net/fastdoom/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43258709">Hacker News</a></p>
<div id="readability-page-1" class="page"><br><center>
    

</center><p>
Mar 04, 2025</p>
<p>Why fastDOOM is fast</p><hr>
 



<p>During the winter of 2024, I restored an IBM PS/1 486-DX2 66Mhz, "Mini-Tower", model 2168. It was the computer I always wanted as a teenager but could never afford. Words cannot do justice
to the joy I felt while working on this machine.</p>

<img loading="lazy" src="https://fabiensanglard.net/fastdoom/2168.png" width="1000" height="598">
 <p>As soon as I got something able to boot, I benchmarked the one software I wanted to run.
</p>


<pre>C:\DOOM&gt;doom.exe -timedemo demo1
timed 1710 gametics in 2783 realtics  
</pre>

<p>Doom doesn't give the fps right away. You have to do a bit of math to get the framerate. In this instance, that's 1710/2783*35 = <span><b>21.5</b></span> fps. An honorable performance for the best machine money could (<a href="https://fabiensanglard.net/fastdoom/pentium_ad_pcmag_nov_1993.jpg">reasonably</a>) buy in Dec 1993 (<a href="https://fabiensanglard.net/fastdoom/ASTRA01.png">specs</a>, <a href="https://fabiensanglard.net/fastdoom/ASTRA02.png">chipset</a>, <a href="https://fabiensanglard.net/fastdoom/ASTRA06.png">video</a>, <a href="https://fabiensanglard.net/fastdoom/ASTRA03.png">disk1</a>, <a href="https://fabiensanglard.net/fastdoom/ASTRA04.png">disk2</a>, <a href="https://fabiensanglard.net/fastdoom/speedsys.png">speedsys</a>).</p>

<p>I was resigned to playing under Ibuprofen until I heard of fastDOOM. I am usually not a fan of ports
  because they tend to add features without cohesion (except for the dreamy Chocolate DOOM) but I gave it a try out of curiosity.
</p>

<pre>C:\DOOM&gt;fdoom.exe -timedemo demo1
Timed 1710 gametics in 1988 realtics. FPS: <span><b>30.1</b></span>
</pre>

<p>30% faster without cutting any features<a name="back_1" href="#footnote_1"><sup>[1]</sup></a>! On a demanding map like doom2's demo1, the gain is even higher, from <span><b>16.8</b></span> fps to <span><b>24.9</b></span>&nbsp;fps. That is 48% faster!</p>

<p>I did not suspect that DOOM had left that much on the table. Obviously shipping within one year left little time to optimize. I had to understand how this magic trick happened.</p>


<p>A byte of history </p><hr><p>Before digging into fastDOOM, let's understand where the code comes from. DOOM was originally developed on NeXT Workstation. The game was structured to be easy to port with most of the code in a core surrounded by small sub-systems performing I/O.</p>


<img loading="lazy" src="https://fabiensanglard.net/fastdoom/doom_arch.svg" width="153.81099mm" height="159.21635mm">
 <center><small>Source: Game Engine Black Book: DOOM</small></center>

<p>During development, DOS I/Os were written by id Software. This became the commercial release of DOOM.
But that version could not be open sourced in 1997 because it relied on a proprietary sound library called DMX.</p>

<p>What ended up being open sourced was the linux version, cleaned up by Bernd Kreimeier when he was working on a book project to explain the engine.</p>

<p>A DOS version of DOOM was reconstructed by using linux's core, Heretic I/O, and APODMX (Apogee Sound wrapper) to emulate DMX. 
  Because Heretic used video mode 13h while DOOM used video mode Y, the graphic I/O (<code>i_ibm.c</code>)
was reverse-engineered from <code>DOOM.EXE</code> disassembly. That is how the community got PCDOOM v2<a name="back_2" href="#footnote_2"><sup>[2]</sup></a>.</p>

<p>fastDOOM starting point was PCDOOM v2.</p>

<pre>                    ┌───────────────┐                              
                    │ NeXTStep DOOM │                              
                    └─────┬────┬────┘                              
                          │    │                                 
                          │    │                                 
                          │    │                                 
          ┌────────────┐  │    │  ┌──────┐      ┌─────────┐      
          │ Linux DOOM │◄─┘    └─►│ DOOM ├─────►│ Heretic │      
          └──────┬─────┘          └──────┘      └────┬────┘      
                 │                    ⁞              │           
                 │                    ▼              │           
                 │              ┌──────────┐         │           
                 └─────────────►│ PCDOOMv2 │◄────────┘           
                                └─────┬────┘                     
                                      ▼                          
                                ┌──────────┐                     
                                │ fastDOOM │                     
fastDoom genealogy              └──────────┘
──────────────────</pre>


<p>The big performance picture</p><hr><p>Victor "Viti95" Nieto, wrote release notes to describe the performance improvement of each version but he seemed more interested in making <code>FDOOM.EXE</code> awesome than detailing how he did it.</p>


<p>To get the big picture of performance evolution over time, I downloaded all 52 releases of fastDOOM, PCDOOMv2, and the original <code>DOOM.EXE</code>, wrote a go program to generate a <code>RUN.BAT</code> running <code>-timedemo demo1</code> on all of them, and mounted it all with mTCP's <code>NETDRIVE</code>.</p>


<p>I chose to timedemo <code>DOOM.WAD</code> with sound on and screen size = 10 (fullscreen with status bar). After several hours of shotguns and imps agony, I had run the whole suite five times and graphed the average fps with <code>chart.js</code>.</p>


<canvas id="bpChart"></canvas>


<p>The first thing this graph allows to rule out is that fastDOOM improvements were mostly due to using a modern compiler. <code>PCDOOMv2</code> is built with OpenWatcom 2 but only gets a marginal improvement over <code>DOOM.EXE</code>.</p> 





<p>git archeology</p><hr><p>On top of releasing often, Viti95 displayed outstanding git discipline where one commit does one thing and each release was tagged. fastDOOM git history is made of 3,042 commits which allows to benchmark each feature.</p>

<p>I wrote another go program to build every single commit. I will pass on the gory details of handling the many build system changes (especially from DOS to Linux). After an hour I had the
  most ugly program I ever wrote and 3,042 <code>DOOM.EXE</code>. I was pleased to see the build was almost never broken.</p>

<canvas id="sizesChart"></canvas>



<p>Graphing the files size shows that the early effort was to be lean by cleaning and deleting code. There are major drops with <a href="https://github.com/viti95/FastDoom/commit/bf0e983ed00f038b65a34f10fa626abb99c87fe6">bf0e983</a> 
  (build 239 where sound recording was removed), <a href="https://github.com/viti95/FastDoom/commit/5f3832310b32c32895688b3112301f98e77119b8" target="_blank">5f38323</a>
  (build 0340 where error code strings were deleted), 
  and <a href="https://github.com/viti95/FastDoom/commit/8b9cac591945fa93af84c0e85845b6a55bc76fe3" target="_blank">8b9cac5</a>
   (build 1105 where TASM was replaced with NASM).
 </p>


<p>Going deep</p><hr><p>Timedemoing all builds would have taken a very long time (3042x1.5/60/24 * 3 passes = 9 days) so I focused
on the release where most of the speed was gained. I wrote yet another go program to generate a <code>.BAT</code> file running timedemo for all commits in <code>v0.1</code>, <code>v0.6</code>, <code>v0.8</code>, <code>v0.9.2</code>, and <code>v0.9.7</code>.
I mounted 1.4 GiB of <code>FDOOM.EXE</code> with mTCP and ran it. It took a while because versions with 200+ commit runtime was 8h/pass.</p>



<p>fastDOOM v0.1</p><hr><p>This release featured <a href="">220 commits</a>.</p>

<pre>$ git log --reverse --oneline "0.1" | wc -l
     220
</pre>



<canvas id="v0Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>



<!-- <p>The very first improvement (in a long series) is build 34 (<a href="https://github.com/viti95/FastDoom/commit/d9940eddddd9dd9debe5809adb1b5bb7e93f4dc6">93f4dc6</a>) 
to remove unnecessary vsync.</p>
 -->

<p>The MPV patch of v0.1 is without a doubt build 36 (<a href="https://github.com/viti95/FastDoom/commit/e16bab87df86baaf2bdec5e17e4844fac9b110be">e16bab8</a>). The "Cripy optimization" turns status bar percentage rendering into a noop if they have not changed. This prevents rendering to a scrap buffer and blitting to the screen for a total
of 2 fps boost. At first I could not believe it. I assume my toolchain had a bug. But cherry-picking this patch on PCDOOMv2 confirmed the tremendous speed gain.</p>

<p>Next is build 167 (<a href="https://github.com/viti95/FastDoom/commit/a9359d599e339d7e8bcd730ea2bed7cc22fa2947">a9359d5</a>)
which inlines FixedDiv via macro.</p>

<div><p>Near the end, we see a series of optimizations granting 0.5 fps.</p><!-- Build 128 (<a href="https://github.com/viti95/FastDoom/commit/cd47382c800e3614a0b454235f0757cb51d0313c">1d0313c</a>) Remove visplane limit!<br/> --><p>
  Build 207 (<a href="https://github.com/viti95/FastDoom/commit/9bd3f207df1837f2b9cb87ca2e67fdc01fcc1a95">9bd3f20</a>): A PSX Doom optimization which optimizes the way the BSP is traversed.<br>

Build 212 (<a href="https://github.com/viti95/FastDoom/commit/dc0f48e22d2804bf025d1b8efe4436311ad1d56d">dc0f48e</a>) "Inlined R_MakeSpans" which renders horizontal surfaces.<br>
</p></div>

<p>Overall this version saw a lot of code being deleted (50% of commits were deletions) which probably helped to cuddle the 486 cachelines of my machine.</p>
<pre>git log --reverse --oneline "0.1" | grep -i -E "remove|delete" | wc -l
     100
</pre>

<p>Somehow, <a href="https://github.com/viti95/FastDoom/commit/609c42df">one of my patches</a> made it to fastDOOM. Probably when I was
writing the Black Book? I have zero recollection of writing this!</p>

<p>fastDOOM v0.6</p><hr><p>This release featured 33 commits.</p>

<pre>$ git log --reverse --oneline "0.5"^.."0.6" | wc -l 
      33
</pre>




<canvas id="v6Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>


<div><p>Among many small optimizations (hello GbaDOOM <a href="https://github.com/viti95/FastDoom/commit/e745a5dc62c21de8926e5845b41fea89cd7f03ad">341</a>)) are MVP ones.</p><p>
  
Build 342 (<a href="https://github.com/viti95/FastDoom/commit/b6aea6ab6c966d1d78edb5da915871e6a22819fd">22819fd</a>) Skips rendering unneeded visplanes.<br>
Build 359 (<a href="https://github.com/viti95/FastDoom/commit/3329d704e081d8a18e7fd3a9c4a239d8840e0d4b">40e0d4b</a>) Removes a level of player pointer indirection.<br>
Build 360 (<a href="https://github.com/viti95/FastDoom/commit/8e5e6355d232d3f4dfd9203dc6aa963adccd296f">ccd296f</a>) Double down on indirection removal.<br>
Build 369 (<a href="https://github.com/viti95/FastDoom/commit/dff38f9fa903f15b24fb13da712feeb09f29e665">f29e665</a>) Inlines the screenspace line splitter.</p></div>


<p>fastDOOM v0.8</p><hr><p>This release featured <a href="">282 commits</a>.</p>

<pre>$ git log --reverse --oneline "0.7"^.."0.8" | wc -l
     282
</pre>

<p>The sound system was a bit unstable so I had to timedemo without sound and then normalize the fps. 
  Moreover the focus of v0.8 seems to have been text-mode renderer so two regressions happened at Build 670 
  (<a href="https://github.com/viti95/FastDoom/commit/50848560577f8d008b5c7f5bb69d7595fa92c67f">a92c67f</a>)
   and Build 730 (<a href="https://github.com/viti95/FastDoom/commit/17095d801f6603f9ab8091fa60453d5b6c3f5f50">c3f5f50</a>) where the Cripy optimization went away.</p>

<canvas id="v8Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>



<p><u><b>MVPs:</b></u></p>
<p>
Build 792 (<a href="https://github.com/viti95/FastDoom/commit/b253754a80045e9809ccfb7bf0f0cf680f279b7d">f279b7d</a>): One executable per renderer (<code>FDOOM.EXE</code>, <code>FDOOM13H.EXE</code>, and so on).<br> 
Build 793 (<a href="https://github.com/viti95/FastDoom/commit/5ddc4e1c967a737c271327cbd4290137a1874ee8">1874ee8</a>): Disable debugging for compiler.<br>
Build 796 (<a href="https://github.com/viti95/FastDoom/commit/85a895802e750360e4251e7fd836906db6aae724">6aae724</a>): Bring back Crispy opt.<br> 
Build 794 (<a href="https://github.com/viti95/FastDoom/commit/eefc599d6c740e890fffa96b5d68b03281366ebf">1366ebf</a>): Compile less code whenever possible.<br>
</p>


<p>fastDOOM v0.9.2</p><hr><p>This release featured <a href="">110 commits</a>.</p>
  
<pre>$ git log --reverse --oneline "0.9.1"^.."0.9.2" | wc -l
     110
</pre>

<canvas id="v92Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>


<p><u><b>MVPs:</b></u></p>
<p>
Build 1639 (<a href="https://github.com/viti95/FastDoom/commit/7d992d86be7e3b3492e7c032d76ea90a2ae2a951">ae2a951</a>): Optimize skyflatnum comparison.<br>
Build 1645 (<a href="https://github.com/viti95/FastDoom/commit/9fb58c2ec8072c5de6e5f4bf8094b51b30730cdc">0730cdc</a>): Optimize R_DrawColumn for Mode Y.<br>
Build 1646 (<a href="https://github.com/viti95/FastDoom/commit/c906b0cba8aac6e44698c7a942b77641217c9e83">17c9e83</a>): Cleanup R_DrawSpan code.<br>
</p>




<p>fastDOOM v0.9.7</p><hr><p>This release featured <a href="">293 commits</a>.</p>

<pre>$ git log --reverse --oneline "0.9.6"^.."0.9.7" | wc -l
     294
</pre>

<p>Despite running the benchmark several times, I was unable to reduce the noise of this release.</p>

<canvas id="v97Chart"></canvas>
<p>Chart is click-able and mouseover-able</p>



<p><u><b>MVPs:</b></u></p>
<p>
Build 1941 (<a href="https://github.com/viti95/FastDoom/commit/c0657d1eb1ca2f6caebbda58c5fb20b150688235">0688235</a>): Testing x86 ASM changes.<br>
Build 1943 (<a href="https://github.com/viti95/FastDoom/commit/cd506d432d63d0e0813b97c8d7c915c02f326e73">f326e73</a>): Add CPU selection + CR2 optimization for 386SX.<br>
Build 1944 (<a href="https://github.com/viti95/FastDoom/commit/7682c40f69bd7db825670758b9a0fb881a836abb">a836abb</a>): Add ESP optimization for R_DrawSpan386SX.<br>
Build 2000 (<a href="https://github.com/viti95/FastDoom/commit/38fbcdbd9ccc4c077daa8bd65138f2b403432590">3432590</a>): Add basecode for rendering fuzz columns in ASM.<br>
Build 2031 (<a href="https://github.com/viti95/FastDoom/commit/95232f3d748978a27d142f1cbf7ef40710edab46">0edab46</a>): Remove a CMP comparison each loop (<a href="https://github.com/viti95/FastDoom/issues/143">ken silverman's optimization</a>?).<br>
</p>

<p>Mode 13h vs Mode Y</p><hr><p>fastDOOM explored many ways to make things faster, for a broad range of CPUs (386, 486, Pentium, Cyrix) and video buses (ISA, VLB, PCI). One
optimization that did not work on my machine was to use video mode 13h instead of mode Y.</p>

<p>In mode 13h dispatch of data toward the four VRAM banks of the VGA is done in hardware. To the CPU, the VRAM appears like a single linear 320x200 framebuffer.
The inconvenience is that you can't double buffer in VRAM so you have to do it in RAM which means bytes are written twice. First into the framebuffer in RAM. And then
a second time when sent to VRAM. Also, the engine must block on VSYNC.</p>

<pre>Mode 13h                                                                                               
────────             RAM                   VRAM (VGA card)                 SCREEN              
             ┌───────────────────┐      ┌───────────────────┐       ┌───────────────────┐      
             │ ┌───────────────┐ │      │                   │       │                   │      
             │ │ framebuffer 1 │ │      │                   │       │                   │      
             │ └───────────────┘ │      │                   │       │                   │      
             │ ┌───────────────┐ │      │ ┌───────────────┐ │       │                   │      
    CPU ────►│ │ framebuffer 2 │ ├────► │ │framebuffer(fb)│ ├──────►│                   │      
             │ └───────────────┘ │      │ └───────────────┘ │       │                   │      
             │ ┌───────────────┐ │      │                   │       │                   │      
             │ │ framebuffer 3 │ │      │                   │       │                   │      
             │ └───────────────┘ │      │                   │       │                   │      
             └───────────────────┘      └───────────────────┘       └───────────────────┘      
</pre>


<p>The mode-Y lets programmers access the VGA banks individually. This allows triple-buffering in VRAM. Moreover, it has the advantage of writing bytes once, directly into
 VRAM. The target bank must be manually selected by the developer via very slow <code>OUT</code> instructions
 but that allows to duplicating pixels horizontally (which gives low-detail mode for free) by writing to two VGA banks at once via latches<a name="back_3" href="#footnote_3"><sup>[3]</sup></a>. Another inconvenience is that it makes drawing invisible Specter much slower since it requires reading back from the VRAM.</p>

<pre>Mode Y                                                                                               
───────                                    VRAM (VGA card)                 SCREEN         
                                        ┌───────────────────┐       ┌───────────────────┐ 
                                        │ ┌───────────────┐ │       │                   │ 
                                        │ │fb1 | fb2 | fb3│ │       │                   │ 
                                        │ └───────────────┘ │       │                   │ 
                                        │ ┌───────────────┐ │       │                   │ 
                                        │ │fb1 | fb2 | fb3│ │       │                   │ 
    CPU ──────────────────────────────► │ └───────────────┘ ├──────►│                   │ 
                                        │ ┌───────────────┐ │       │                   │ 
                                        │ │fb1 | fb2 | fb3│ │       │                   │ 
                                        │ └───────────────┘ │       │                   │ 
                                        │ ┌───────────────┐ │       │                   │ 
                                        │ │fb1 | fb2 | fb3│ │       │                   │ 
                                        │ └───────────────┘ │       │                   │ 
                                        └───────────────────┘       └───────────────────┘    
</pre>

<p>For machines with fast CPUs and bus (100+ Mhz/ Pentium and VLB/PCI) where video cards are less likely to handle <code>OUT</code> instruction well, mode 13h is better. 
For "slow CPUs", it is faster to write data once to VRAM via mode Y.</p>

<p>Anyway, Doom used mode Y.</p>



<blockquote>
       DOOM uses 320*200*256 VGA mode, which is slightly different from MCGA
mode (it would NOT run on an MCGA equiped machine). I access the
frame buffer in an interleaved planar mode similar to Michael
Abrash's "Mode X", but still at 200 scan lines instead of 240 (less
pixels == faster update rate).<p>

DOOM cycles between three display pages. If only two were used, it
would have to sync to the VBL to avoid possible display flicker. If
you look carefully at a HOM effect, you should see three distinct
images being cycled between.</p>
    </blockquote>

<p>Another reason John game me for using Mode-Y back in the days is that the tools used by the graphic team (Deluxe Paint) only supported 320x200 (whereas Mode-X is 320x240).</p>



<blockquote><span>
e...@agora.rdrop.com (Ed Hurtley) wrote:
&gt;Check, please... In case you haven't hit ESC ever, the Options menu
&gt;has a Low/High resolution toggle... Low is 320x200, High is
&gt;640x400, with the border graphics (the score bar, menu, etc...) are
&gt;still 320x200... (Just the same graphics files)</span>
<p>
Low detail is 160*200 in the view screen. This is done by setting
two bits in the mapmask register whenever the texturing functions are
writing to video memory, causing two pixels to be set for each byte
written.
</p><p><span>
ui...@freenet.Victoria.BC.CA (Ben Morris) wrote:
<p>
&gt;John,
</p><p>
&gt;You're using a planar graphics system for a bitmapped game that
&gt;updates the entire screen at a respectable framrate on a 486/66?</p></span></p><p>
Its planar, but not bit planar (THAT would stink). Pixels 0,4,8 are
in plane 0, pixels 1,5,9 are in plane 1, etc.
</p><p><span>
&gt;That's pretty incredible. I would have thought all the over-
&gt;head for programming the VGA registers would kill that
&gt;possibility.</span></p><p>
The registers don't need to be programed all that much. The map mask
register only needs to be set once for each vertical column, and four
times for each horizontal row (I step by four pixels in the inner
loop to stay on the same plane, then increment the start pixel and
move to the next plane).
</p><p>
It is still a lot of grief, and it polutes the program quite a bit,
but texture mapping directly to the video memory gives you a fair
amount of extra speed (10% - 15%) on most video cards because the
video writes are interleaved with main memory accesses and texture
calculations, giving the write time to complete without stalling.
</p><p>
Going to that trouble also gets a perfect page flip, rather than the
tearing you get with main memory buffering.</p>
    </blockquote>



  <p>Heretic was released in 1994. Hardware had evolved to make mode 13h<a name="back_6" href="#footnote_6"><sup>[6]</sup></a><a name="back_7" href="#footnote_7"><sup>[7]</sup></a>  more appealing so Raven modified the DOOM engine to this effect. 
  PCDoom v2 used Heretic I/O but re-implemented the video I/O with mode Y. 
  Finally fastDOOM gives users the choice by providing several executable <code>FDOOM.EXE</code>, <code>FDOOM13H.EXE</code>, and <code>FDOOMVBD.EXE</code>.
  </p>

  <blockquote>
The DOOM press release beta (October '93) used Mode 13h, so I assume they switched to Mode Y to improve performance on slower machines (low-detail). I wonder why they didn’t also implement the so-called "potato mode", which writes four pixels with a single 8-bit write to VRAM.
<p>
In FastDoom, I reintroduced Mode 13h because Heretic/Hexen had better-optimized ASM rendering code for this mode. Later, I was able to partially port this approach to column rendering in Mode Y, which resulted in a 5% to 7% performance improvement.
</p><p>
Based on my testing, the best mode for 486 CPUs is the VESA direct mode (FDOOMVBD.EXE for 320x200). This mode combines the advantages of Mode Y with the optimized rendering code from Heretic while avoiding any OUT instructions—except for one to switch buffers, which executes only once per rendered frame. The only downside is that it requires a VLB or PCI graphics card with LFB enabled and has slower performance in low-detail and potato-detail modes.</p><p>- Conversation with Viti95</p>
    </blockquote>

<p>Viti95 elaborated further on fastDOOM mode 13h during proof-reading.</p>
  <blockquote>
In FastDoom, Mode 13h uses a single framebuffer in RAM, which is copied to VRAM after the entire scene is rendered. Vsync is not enforced, which may result in flickering. There are two methods for copying the backbuffer to VRAM, optimized for different bus speeds. For slow buses (8-bit ISA), a differential copy method is used, transferring only modified pixels.
<p>
This approach involves many branches but is faster overall because branching is less expensive than excessive bus transfers. For faster buses (16-bit ISA, VLB, PCI, etc.), a full backbuffer copy is performed using REP MOVS instructions, which is efficient when the bus bandwidth is sufficient.
</p><p>- Conversation with Viti95</p>
    </blockquote>

<p>More optimization which did not work</p><hr><p>Another venue I appreciated seeing explored is OpenWatcom's processor-specific flags (<code>4r</code>/<code>4s</code> vs <code>3r</code>/<code>3s</code>)<a name="back_8" href="#footnote_8"><sup>[8]</sup></a>. Both wcc386's 386 and 486 flags were attempted but ultimately discontinued
  because the 386 version always seemed faster.</p>


<blockquote>
One of my goals for FastDoom is to switch the compiler from OpenWatcom v2 to DJGPP (GCC), which has been shown to produce faster code with the same source. Alternatively, it would be great if someone could improve OpenWatcom v2 to close the performance gap.
 <p>- Conversation with Viti95</p>
    </blockquote>


<p>Overall impression</p><hr><div><p>What splendid work by Victor Nieto!
 If software can die from a thousand cuts, Viti95 made fastDOOM awesome with three thousand optimizations! 
  Not only he leveraged existing improvements (crispy, psx, gba, Lee Killough), he also came up with many news one
and generated so much hype that even Ken Silverman (author of Duke3D build engine) came <a href="https://github.com/viti95/FastDoom/issues/143">to participate</a><a name="back_9" href="#footnote_9"><sup>[9]</sup></a>.
</p><p> I tip my beret to you Victor!</p></div><p>References</p><hr><p id="paperbox"><table><tbody><tr><td><a name="footnote_1"></a><a href="#back_1">^</a></td><td> [1]</td><td><u><b>Note from Viti95:</b></u> Joystick and network gameplay support have been removed, so it's not a completely feature-intact port ^^ (People are still trying to convince me to bring network gameplay back).</td></tr><tr><td><a name="footnote_2"></a><a href="#back_2">^</a></td><td> [2]</td><td><a href="https://doomwiki.org/wiki/Gamesrc-ver-recreation">DOOM engine: gamesrc-ver-recreation</a></td></tr><tr><td><a name="footnote_3"></a><a href="#back_3">^</a></td><td> [3]</td><td><a href="https://fabiensanglard.net/">Game Engine 
 Black Book: Wolfenstein 3D</a></td></tr><tr><td><a name="footnote_4"></a><a href="#back_4">^</a></td><td> [4]</td><td><a href="https://groups.google.com/g/alt.games.doom/c/3tMB2UmEBK0/m/m1VR6LiJRQMJ">Doom graphics modes usenet</a></td></tr><tr><td><a name="footnote_5"></a><a href="#back_5">^</a></td><td> [5]</td><td><a href="https://groups.google.com/g/alt.games.doom/c/3tMB2UmEBK0/m/m1VR6LiJRQMJ">Doom graphics modes usenet</a></td></tr><tr><td><a name="footnote_6"></a><a href="#back_6">^</a></td><td> [6]</td><td><a href="https://www.vogons.org/viewtopic.php?t=61839">Doom vs Heretic VGA performance difference</a></td></tr><tr><td><a name="footnote_7"></a><a href="#back_7">^</a></td><td> [7]</td><td><a href="https://www.vogons.org/viewtopic.php?f=7&amp;t=40699">Doom in DOS: Original vs Source Ports </a></td></tr><tr><td><a name="footnote_8"></a><a href="#back_8">^</a></td><td> [8]</td><td><a href="https://fabiensanglard.net/fastdoom/openwatcom_doc.pdf">OpenWatcom documentation</a></td></tr><tr><td><a name="footnote_9"></a><a href="#back_9">^</a></td><td> [9]</td><td><u><b>Note from Viti95:</b></u> Some of Ken Silverman’s ideas and code made their way into the rendering functions for UMC Green CPUs, resulting in a significant speed boost on that hardware..</td></tr></tbody></table></p> <hr>
 <center>*</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Solving SICP (119 pts)]]></title>
            <link>https://lockywolf.wordpress.com/2021/02/08/solving-sicp/</link>
            <guid>43257963</guid>
            <pubDate>Tue, 04 Mar 2025 17:58:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lockywolf.wordpress.com/2021/02/08/solving-sicp/">https://lockywolf.wordpress.com/2021/02/08/solving-sicp/</a>, See on <a href="https://news.ycombinator.com/item?id=43257963">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<tr>
<td>1</td>
<td>Exercise 1.1 Interpreter result</td>
<td>1.211</td>
<td>2</td>
<td>459</td>
</tr>
<tr>
<td>2</td>
<td>Exercise 1.2 Prefix form</td>
<td>0.001</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>3</td>
<td>Figure 1.1 Tree representation, showing the value of each su</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>4</td>
<td>Exercise 1.4 Compound expressions</td>
<td>0.003</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>5</td>
<td>Exercise 1.5 Ben’s test</td>
<td>0.008</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>6</td>
<td>Exercise 1.6 If is a special form</td>
<td>0.969</td>
<td>2</td>
<td>118</td>
</tr>
<tr>
<td>7</td>
<td>Exercise 1.7 Good enough?</td>
<td>0.949</td>
<td>3</td>
<td>436</td>
</tr>
<tr>
<td>8</td>
<td>Exercise 1.8 Newton’s method</td>
<td>0.197</td>
<td>2</td>
<td>193</td>
</tr>
<tr>
<td>9</td>
<td>Exercise 1.10 Ackermann’s function</td>
<td>3.038</td>
<td>2</td>
<td>379</td>
</tr>
<tr>
<td>10</td>
<td>Exercise 1.11 Recursive vs iterative</td>
<td>0.037</td>
<td>1</td>
<td>54</td>
</tr>
<tr>
<td>11</td>
<td>Exercise 1.12 Recursive Pascal’s triangle</td>
<td>0.012</td>
<td>1</td>
<td>17</td>
</tr>
<tr>
<td>12</td>
<td>Exercise 1.13 Fibonacci</td>
<td>0.092</td>
<td>1</td>
<td>132</td>
</tr>
<tr>
<td>13</td>
<td>Exercise 1.9 Iterative or recursive?</td>
<td>3.722</td>
<td>2</td>
<td>65</td>
</tr>
<tr>
<td>14</td>
<td>Exercise 1.14 count-change</td>
<td>1.038</td>
<td>2</td>
<td>50</td>
</tr>
<tr>
<td>15</td>
<td>Exercise 1.15 sine</td>
<td>0.267</td>
<td>2</td>
<td>195</td>
</tr>
<tr>
<td>16</td>
<td>Exercise 1.16 Iterative exponentiation</td>
<td>0.032</td>
<td>1</td>
<td>46</td>
</tr>
<tr>
<td>17</td>
<td>Exercise 1.17 Fast multiplication</td>
<td>0.019</td>
<td>1</td>
<td>28</td>
</tr>
<tr>
<td>18</td>
<td>Exercise 1.18 Iterative multiplication</td>
<td>0.497</td>
<td>2</td>
<td>23</td>
</tr>
<tr>
<td>19</td>
<td>Exercise 1.19 Logarithmic Fibonacci</td>
<td>1.374</td>
<td>2</td>
<td>93</td>
</tr>
<tr>
<td>20</td>
<td>Exercise 1.20 GCD applicative vs normal</td>
<td>0.099</td>
<td>1</td>
<td>142</td>
</tr>
<tr>
<td>21</td>
<td>Exercise 1.21 smallest-divisor</td>
<td>0.027</td>
<td>1</td>
<td>39</td>
</tr>
<tr>
<td>22</td>
<td>Exercise 1.22 timed-prime-test</td>
<td>0.042</td>
<td>1</td>
<td>61</td>
</tr>
<tr>
<td>23</td>
<td>Exercise 1.23 (next test-divisor)</td>
<td>0.383</td>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>24</td>
<td>Exercise 1.24 Fermat method</td>
<td>0.067</td>
<td>1</td>
<td>96</td>
</tr>
<tr>
<td>25</td>
<td>Exercise 1.25 expmod</td>
<td>0.051</td>
<td>1</td>
<td>74</td>
</tr>
<tr>
<td>26</td>
<td>Exercise 1.26 square vs mul</td>
<td>0.003</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>27</td>
<td>Exercise 1.27 Carmichael numbers</td>
<td>0.333</td>
<td>2</td>
<td>102</td>
</tr>
<tr>
<td>28</td>
<td>Exercise 1.28 Miller-Rabin</td>
<td>0.110</td>
<td>1</td>
<td>158</td>
</tr>
<tr>
<td>29</td>
<td>Exercise 1.29 Simpson’s integral</td>
<td>0.464</td>
<td>2</td>
<td>68</td>
</tr>
<tr>
<td>30</td>
<td>Exercise 1.30 Iterative sum</td>
<td>0.030</td>
<td>2</td>
<td>10</td>
</tr>
<tr>
<td>31</td>
<td>Exercise 1.31 Product</td>
<td>0.028</td>
<td>1</td>
<td>40</td>
</tr>
<tr>
<td>32</td>
<td>Exercise 1.32 Accumulator</td>
<td>0.017</td>
<td>1</td>
<td>24</td>
</tr>
<tr>
<td>33</td>
<td>Exercise 1.33 filtered-accumulate</td>
<td>0.092</td>
<td>1</td>
<td>133</td>
</tr>
<tr>
<td>34</td>
<td>Exercise 1.34 lambda</td>
<td>0.006</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>35</td>
<td>Exercise 1.35 fixed-point</td>
<td>0.265</td>
<td>2</td>
<td>87</td>
</tr>
<tr>
<td>36</td>
<td>Exercise 1.36 fixed-point-with-dampening</td>
<td>0.035</td>
<td>1</td>
<td>50</td>
</tr>
<tr>
<td>37</td>
<td>Exercise 1.37 cont-frac</td>
<td>0.569</td>
<td>2</td>
<td>348</td>
</tr>
<tr>
<td>38</td>
<td>Exercise 1.38 euler constant</td>
<td>0.000</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>39</td>
<td>Exercise 1.39 tan-cf</td>
<td>0.025</td>
<td>1</td>
<td>36</td>
</tr>
<tr>
<td>40</td>
<td>Exercise 1.40 newtons-method</td>
<td>0.205</td>
<td>2</td>
<td>6</td>
</tr>
<tr>
<td>41</td>
<td>Exercise 1.41 double-double</td>
<td>0.010</td>
<td>1</td>
<td>15</td>
</tr>
<tr>
<td>42</td>
<td>Exercise 1.42 compose</td>
<td>0.004</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>43</td>
<td>Exercise 1.43 repeated</td>
<td>0.019</td>
<td>1</td>
<td>27</td>
</tr>
<tr>
<td>44</td>
<td>Exercise 1.44 smoothing</td>
<td>0.099</td>
<td>2</td>
<td>142</td>
</tr>
<tr>
<td>45</td>
<td>Exercise 1.45 nth-root</td>
<td>0.056</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>46</td>
<td>Exercise 1.46 iterative-improve</td>
<td>0.033</td>
<td>1</td>
<td>48</td>
</tr>
<tr>
<td>47</td>
<td>Exercise 2.1 make-rat</td>
<td>1.608</td>
<td>2</td>
<td>109</td>
</tr>
<tr>
<td>48</td>
<td>Exercise 2.2 make-segment</td>
<td>0.024</td>
<td>1</td>
<td>34</td>
</tr>
<tr>
<td>49</td>
<td>Exercise 2.3 make-rectangle</td>
<td>2.183</td>
<td>2</td>
<td>174</td>
</tr>
<tr>
<td>50</td>
<td>Exercise 2.4 cons-lambda</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>51</td>
<td>Exercise 2.5 cons-pow</td>
<td>0.041</td>
<td>1</td>
<td>59</td>
</tr>
<tr>
<td>52</td>
<td>Exercise 2.6 Church Numerals</td>
<td>0.024</td>
<td>1</td>
<td>34</td>
</tr>
<tr>
<td>53</td>
<td>Exercise 2.7 make-interval</td>
<td>0.019</td>
<td>1</td>
<td>28</td>
</tr>
<tr>
<td>54</td>
<td>Exercise 2.8 sub-interval</td>
<td>0.124</td>
<td>1</td>
<td>58</td>
</tr>
<tr>
<td>55</td>
<td>Exercise 2.9 interval-width</td>
<td>0.006</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>56</td>
<td>Exercise 2.10 div-interval-better</td>
<td>0.010</td>
<td>1</td>
<td>15</td>
</tr>
<tr>
<td>57</td>
<td>Exercise 2.11 mul-interval-nine-cases</td>
<td>0.052</td>
<td>1</td>
<td>75</td>
</tr>
<tr>
<td>58</td>
<td>Exercise 2.12 make-center-percent</td>
<td>0.393</td>
<td>2</td>
<td>43</td>
</tr>
<tr>
<td>59</td>
<td>Exercise 2.13 formula for tolerance</td>
<td>0.003</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>60</td>
<td>Exercise 2.14 parallel-resistors</td>
<td>0.047</td>
<td>1</td>
<td>68</td>
</tr>
<tr>
<td>61</td>
<td>Exercise 2.15 better-intervals</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>62</td>
<td>Exercise 2.16 interval-arithmetic</td>
<td>0.002</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>63</td>
<td>Exercise 2.17 last-pair</td>
<td>0.966</td>
<td>2</td>
<td>89</td>
</tr>
<tr>
<td>64</td>
<td>Exercise 2.18 reverse</td>
<td>0.006</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>65</td>
<td>Exercise 2.19 coin-values</td>
<td>0.021</td>
<td>1</td>
<td>30</td>
</tr>
<tr>
<td>66</td>
<td>Exercise 2.20 dotted-tail notation</td>
<td>0.311</td>
<td>2</td>
<td>156</td>
</tr>
<tr>
<td>67</td>
<td>Exercise 2.21 map-square-list</td>
<td>0.013</td>
<td>1</td>
<td>19</td>
</tr>
<tr>
<td>68</td>
<td>Exercise 2.22 wrong list order</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>69</td>
<td>Exercise 2.23 for-each</td>
<td>0.006</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>70</td>
<td>Exercise 2.24 list-plot-result</td>
<td>0.111</td>
<td>2</td>
<td>75</td>
</tr>
<tr>
<td>71</td>
<td>Exercise 2.25 caddr</td>
<td>0.037</td>
<td>1</td>
<td>54</td>
</tr>
<tr>
<td>72</td>
<td>Exercise 2.26 append cons list</td>
<td>0.011</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>73</td>
<td>Exercise 2.27 deep-reverse</td>
<td>0.433</td>
<td>2</td>
<td>40</td>
</tr>
<tr>
<td>74</td>
<td>Exercise 2.28 fringe</td>
<td>0.026</td>
<td>1</td>
<td>37</td>
</tr>
<tr>
<td>75</td>
<td>Exercise 2.29 mobile</td>
<td>0.058</td>
<td>1</td>
<td>83</td>
</tr>
<tr>
<td>76</td>
<td>Exercise 2.30 square-tree</td>
<td>0.100</td>
<td>2</td>
<td>122</td>
</tr>
<tr>
<td>77</td>
<td>Exercise 2.31 tree-map square tree</td>
<td>0.019</td>
<td>1</td>
<td>27</td>
</tr>
<tr>
<td>78</td>
<td>Exercise 2.32 subsets</td>
<td>0.010</td>
<td>1</td>
<td>15</td>
</tr>
<tr>
<td>79</td>
<td>Exercise 2.33 map-append-length</td>
<td>0.375</td>
<td>2</td>
<td>96</td>
</tr>
<tr>
<td>80</td>
<td>Exercise 2.34 horners-rule</td>
<td>0.006</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>81</td>
<td>Exercise 2.35 count-leaves-accumulate</td>
<td>0.011</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>82</td>
<td>Exercise 2.36 accumulate-n</td>
<td>0.006</td>
<td>1</td>
<td>9</td>
</tr>
<tr>
<td>83</td>
<td>Exercise 2.37 matrix-*-vector</td>
<td>0.017</td>
<td>1</td>
<td>24</td>
</tr>
<tr>
<td>84</td>
<td>Exercise 2.38 fold-left</td>
<td>0.372</td>
<td>2</td>
<td>65</td>
</tr>
<tr>
<td>85</td>
<td>Exercise 2.39 reverse fold-right fold-left</td>
<td>0.005</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>86</td>
<td>Exercise 2.40 unique-pairs</td>
<td>0.029</td>
<td>1</td>
<td>42</td>
</tr>
<tr>
<td>87</td>
<td>Exercise 2.41 triple-sum</td>
<td>2.195</td>
<td>2</td>
<td>57</td>
</tr>
<tr>
<td>88</td>
<td>Figure 2.8 A solution to the eight-queens puzzle.</td>
<td>0.001</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>89</td>
<td>Exercise 2.42 k-queens</td>
<td>3.299</td>
<td>2</td>
<td>122</td>
</tr>
<tr>
<td>90</td>
<td>Exercise 2.43 slow k-queens</td>
<td>0.019</td>
<td>1</td>
<td>28</td>
</tr>
<tr>
<td>91</td>
<td>Exercise 2.46 make-vect</td>
<td>2.578</td>
<td>5</td>
<td>535</td>
</tr>
<tr>
<td>92</td>
<td>Exercise 2.47 make-frame</td>
<td>0.083</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>93</td>
<td>Exercise 2.48 make-segment</td>
<td>0.054</td>
<td>1</td>
<td>78</td>
</tr>
<tr>
<td>94</td>
<td>Exercise 2.49 segments-&gt;painter applications</td>
<td>0.294</td>
<td>2</td>
<td>139</td>
</tr>
<tr>
<td>95</td>
<td>Exercise 2.50 flip-horiz and rotate270 and rotate180</td>
<td>0.019</td>
<td>1</td>
<td>27</td>
</tr>
<tr>
<td>96</td>
<td>Exercise 2.51 below</td>
<td>1.801</td>
<td>4</td>
<td>524</td>
</tr>
<tr>
<td>97</td>
<td>Exercise 2.44 up-split</td>
<td>1.169</td>
<td>2</td>
<td>89</td>
</tr>
<tr>
<td>98</td>
<td>Exercise 2.45 split</td>
<td>0.113</td>
<td>2</td>
<td>23</td>
</tr>
<tr>
<td>99</td>
<td>Exercise 2.52 modify square-limit</td>
<td>0.450</td>
<td>2</td>
<td>58</td>
</tr>
<tr>
<td>100</td>
<td>Exercise 2.53 quote introduction</td>
<td>0.008</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>101</td>
<td>Exercise 2.54 equal? implementation</td>
<td>0.050</td>
<td>1</td>
<td>72</td>
</tr>
<tr>
<td>102</td>
<td>Exercise 2.55 quote quote</td>
<td>0.000</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>103</td>
<td>Exercise 2.56 differentiation-exponentiation</td>
<td>0.393</td>
<td>2</td>
<td>65</td>
</tr>
<tr>
<td>104</td>
<td>Exercise 2.57 differentiate-three-sum</td>
<td>0.560</td>
<td>3</td>
<td>147</td>
</tr>
<tr>
<td>105</td>
<td>Exercise 2.58 infix-notation</td>
<td>0.112</td>
<td>1</td>
<td>161</td>
</tr>
<tr>
<td>106</td>
<td>Exercise 2.59 union-set</td>
<td>0.277</td>
<td>2</td>
<td>6</td>
</tr>
<tr>
<td>107</td>
<td>Exercise 2.60 duplicate-set</td>
<td>0.012</td>
<td>1</td>
<td>17</td>
</tr>
<tr>
<td>108</td>
<td>Exercise 2.62 ordered-union-set (ordered list)</td>
<td>0.973</td>
<td>2</td>
<td>14</td>
</tr>
<tr>
<td>109</td>
<td>Exercise 2.61 sets as ordered lists</td>
<td>0.004</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>110</td>
<td>Exercise 2.63 tree-&gt;list (binary search tree)</td>
<td>0.078</td>
<td>1</td>
<td>113</td>
</tr>
<tr>
<td>111</td>
<td>Exercise 2.64 balanced-tree</td>
<td>2.740</td>
<td>3</td>
<td>106</td>
</tr>
<tr>
<td>112</td>
<td>Exercise 2.65 tree-union-set</td>
<td>9.785</td>
<td>2</td>
<td>47</td>
</tr>
<tr>
<td>113</td>
<td>Exercise 2.66 tree-lookup</td>
<td>0.035</td>
<td>1</td>
<td>50</td>
</tr>
<tr>
<td>114</td>
<td>Exercise 2.67 Huffman decode a simple message</td>
<td>0.303</td>
<td>3</td>
<td>108</td>
</tr>
<tr>
<td>115</td>
<td>Exercise 2.68 Huffman encode a simple message</td>
<td>0.023</td>
<td>1</td>
<td>33</td>
</tr>
<tr>
<td>116</td>
<td>Exercise 2.69 Generate Huffman tree</td>
<td>0.608</td>
<td>2</td>
<td>160</td>
</tr>
<tr>
<td>117</td>
<td>Exercise 2.70 Generate a tree and encode a song</td>
<td>0.072</td>
<td>2</td>
<td>57</td>
</tr>
<tr>
<td>118</td>
<td>Exercise 2.71 Huffman tree for frequencies 5 and 10</td>
<td>0.258</td>
<td>2</td>
<td>202</td>
</tr>
<tr>
<td>119</td>
<td>Exercise 2.72 Huffman order of growth</td>
<td>0.050</td>
<td>2</td>
<td>26</td>
</tr>
<tr>
<td>120</td>
<td>Exercise 2.73 data-driven-deriv</td>
<td>0.605</td>
<td>2</td>
<td>189</td>
</tr>
<tr>
<td>121</td>
<td>Exercise 2.74 Insatiable Enterprises</td>
<td>0.410</td>
<td>4</td>
<td>171</td>
</tr>
<tr>
<td>122</td>
<td>Exercise 2.75 make-from-mag-ang message passing</td>
<td>0.019</td>
<td>1</td>
<td>28</td>
</tr>
<tr>
<td>123</td>
<td>Exercise 2.76 types or functions?</td>
<td>0.003</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>124</td>
<td>Exercise 2.77 generic-algebra-magnitude</td>
<td>0.772</td>
<td>3</td>
<td>190</td>
</tr>
<tr>
<td>125</td>
<td>Exercise 2.78 Ordinary numbers for Scheme</td>
<td>0.212</td>
<td>2</td>
<td>67</td>
</tr>
<tr>
<td>126</td>
<td>Exercise 2.79 generic-equality</td>
<td>1.786</td>
<td>2</td>
<td>28</td>
</tr>
<tr>
<td>127</td>
<td>Exercise 2.80 Generic arithmetic zero?</td>
<td>0.056</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>128</td>
<td>Exercise 2.81 coercion to-itself</td>
<td>0.749</td>
<td>3</td>
<td>330</td>
</tr>
<tr>
<td>129</td>
<td>Exercise 2.82 three-argument-coercion</td>
<td>0.433</td>
<td>2</td>
<td>230</td>
</tr>
<tr>
<td>130</td>
<td>Exercise 2.83 Numeric Tower and (raise)</td>
<td>0.717</td>
<td>3</td>
<td>116</td>
</tr>
<tr>
<td>131</td>
<td>Exercise 2.84 Using <code>raise</code> (<code>raise-type</code>) in <code>apply-generic</code></td>
<td>0.865</td>
<td>2</td>
<td>135</td>
</tr>
<tr>
<td>132</td>
<td>Exercise 2.85 Dropping a type</td>
<td>3.089</td>
<td>5</td>
<td>507</td>
</tr>
<tr>
<td>133</td>
<td>Exercise 2.86 Compound complex numbers</td>
<td>0.274</td>
<td>2</td>
<td>108</td>
</tr>
<tr>
<td>134</td>
<td>Exercise 2.87 Generalized zero?</td>
<td>0.919</td>
<td>4</td>
<td>389</td>
</tr>
<tr>
<td>135</td>
<td>Exercise 2.88 Subtraction of polynomials</td>
<td>0.646</td>
<td>3</td>
<td>50</td>
</tr>
<tr>
<td>136</td>
<td>Exercise 2.89 Dense term-lists</td>
<td>0.083</td>
<td>1</td>
<td>120</td>
</tr>
<tr>
<td>137</td>
<td>Exercise 2.90 Implementing dense polynomials as a separate p</td>
<td>0.400</td>
<td>2</td>
<td>148</td>
</tr>
<tr>
<td>138</td>
<td>Exercise 2.91 Division of polynomials</td>
<td>0.111</td>
<td>2</td>
<td>103</td>
</tr>
<tr>
<td>139</td>
<td>Exercise 2.92 Ordering of variables so that addition and mul</td>
<td>4.556</td>
<td>11</td>
<td>964</td>
</tr>
<tr>
<td>140</td>
<td>Exercise 2.93 Rational polynomials</td>
<td>0.378</td>
<td>3</td>
<td>198</td>
</tr>
<tr>
<td>141</td>
<td>Exercise 2.94 Greatest-common-divisor for polynomials</td>
<td>0.091</td>
<td>1</td>
<td>131</td>
</tr>
<tr>
<td>142</td>
<td>Exercise 2.95 Illustrate the non-integer problem</td>
<td>0.450</td>
<td>2</td>
<td>149</td>
</tr>
<tr>
<td>143</td>
<td>Exercise 2.96 Integerizing factor</td>
<td>0.325</td>
<td>2</td>
<td>275</td>
</tr>
<tr>
<td>144</td>
<td>Exercise 2.97 Reduction of polynomials</td>
<td>0.201</td>
<td>1</td>
<td>140</td>
</tr>
<tr>
<td>145</td>
<td>Exercise 3.1 accumulators</td>
<td>0.425</td>
<td>2</td>
<td>53</td>
</tr>
<tr>
<td>146</td>
<td>Exercise 3.2 make-monitored</td>
<td>0.027</td>
<td>1</td>
<td>39</td>
</tr>
<tr>
<td>147</td>
<td>Exercise 3.3 password protection</td>
<td>0.010</td>
<td>1</td>
<td>14</td>
</tr>
<tr>
<td>148</td>
<td>Exercise 3.4 call-the-cops</td>
<td>0.010</td>
<td>1</td>
<td>15</td>
</tr>
<tr>
<td>149</td>
<td>Exercise 3.5 Monte-Carlo</td>
<td>0.528</td>
<td>2</td>
<td>98</td>
</tr>
<tr>
<td>150</td>
<td>Exercise 3.6 reset a prng</td>
<td>0.479</td>
<td>2</td>
<td>68</td>
</tr>
<tr>
<td>151</td>
<td>Exercise 3.7 Joint accounts</td>
<td>0.059</td>
<td>1</td>
<td>85</td>
</tr>
<tr>
<td>152</td>
<td>Exercise 3.8 Right-to-left vs Left-to-right</td>
<td>0.026</td>
<td>1</td>
<td>38</td>
</tr>
<tr>
<td>153</td>
<td>Exercise 3.9 Environment structures</td>
<td>21.030</td>
<td>10</td>
<td>1100</td>
</tr>
<tr>
<td>154</td>
<td>Exercise 3.10 Using <code>let</code> to create state variables</td>
<td>4.933</td>
<td>2</td>
<td>138</td>
</tr>
<tr>
<td>155</td>
<td>Exercise 3.11 Internal definitions</td>
<td>0.994</td>
<td>2</td>
<td>219</td>
</tr>
<tr>
<td>156</td>
<td>Exercise 3.12 Drawing <code>append!</code></td>
<td>2.966</td>
<td>3</td>
<td>347</td>
</tr>
<tr>
<td>157</td>
<td>Exercise 3.13 <code>make-cycle</code></td>
<td>0.010</td>
<td>1</td>
<td>14</td>
</tr>
<tr>
<td>158</td>
<td>Exercise 3.14 <code>mystery</code></td>
<td>0.385</td>
<td>2</td>
<td>77</td>
</tr>
<tr>
<td>159</td>
<td>Exercise 3.15 <code>set-to-wow!</code></td>
<td>1.942</td>
<td>3</td>
<td>117</td>
</tr>
<tr>
<td>160</td>
<td>Exercise 3.16 <code>count-pairs</code></td>
<td>0.171</td>
<td>1</td>
<td>118</td>
</tr>
<tr>
<td>161</td>
<td>Exercise 3.17 Real <code>count-pairs</code></td>
<td>0.029</td>
<td>1</td>
<td>42</td>
</tr>
<tr>
<td>162</td>
<td>Exercise 3.18 Finding cycles</td>
<td>0.012</td>
<td>1</td>
<td>17</td>
</tr>
<tr>
<td>163</td>
<td>Exercise 3.19 Efficient finding cycles</td>
<td>0.934</td>
<td>2</td>
<td>205</td>
</tr>
<tr>
<td>164</td>
<td>Exercise 3.20 Procedural <code>set-car!</code></td>
<td>0.633</td>
<td>2</td>
<td>121</td>
</tr>
<tr>
<td>165</td>
<td>Exercise 3.21 queues</td>
<td>0.021</td>
<td>1</td>
<td>30</td>
</tr>
<tr>
<td>166</td>
<td>Exercise 3.22 procedural queue</td>
<td>0.294</td>
<td>2</td>
<td>67</td>
</tr>
<tr>
<td>167</td>
<td>Exercise 3.23 dequeue</td>
<td>0.049</td>
<td>2</td>
<td>71</td>
</tr>
<tr>
<td>168</td>
<td>Exercise 3.24 tolerant tables</td>
<td>0.780</td>
<td>3</td>
<td>33</td>
</tr>
<tr>
<td>169</td>
<td>Exercise 3.25 multilevel tables</td>
<td>2.103</td>
<td>2</td>
<td>486</td>
</tr>
<tr>
<td>170</td>
<td>Exercise 3.26 binary tree table</td>
<td>0.013</td>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>171</td>
<td>Exercise 3.27 memoization</td>
<td>0.802</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>172</td>
<td>Exercise 3.28 primitive or-gate</td>
<td>1.316</td>
<td>2</td>
<td>783</td>
</tr>
<tr>
<td>173</td>
<td>Exercise 3.29 Compound or-gate</td>
<td>0.001</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>174</td>
<td>Exercise 3.30 ripple-carry adder</td>
<td>0.009</td>
<td>1</td>
<td>13</td>
</tr>
<tr>
<td>175</td>
<td>Exercise 3.31 Initial propagation</td>
<td>0.013</td>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>176</td>
<td>Exercise 3.32 Order matters</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>177</td>
<td>Exercise 3.33 averager constraint</td>
<td>9.460</td>
<td>3</td>
<td>198</td>
</tr>
<tr>
<td>178</td>
<td>Exercise 3.34 Wrong squarer</td>
<td>0.042</td>
<td>1</td>
<td>61</td>
</tr>
<tr>
<td>179</td>
<td>Exercise 3.35 Correct squarer</td>
<td>0.012</td>
<td>1</td>
<td>17</td>
</tr>
<tr>
<td>180</td>
<td>Exercise 3.36 Connector environment diagram</td>
<td>3.319</td>
<td>3</td>
<td>263</td>
</tr>
<tr>
<td>181</td>
<td>Exercise 3.37 Expression-based constraints</td>
<td>0.037</td>
<td>1</td>
<td>53</td>
</tr>
<tr>
<td>182</td>
<td>Exercise 3.38 Timing</td>
<td>0.061</td>
<td>1</td>
<td>88</td>
</tr>
<tr>
<td>183</td>
<td>Exercise 3.39 Serializer</td>
<td>1.266</td>
<td>4</td>
<td>269</td>
</tr>
<tr>
<td>184</td>
<td>Exercise 3.40 Three parallel multiplications</td>
<td>5.973</td>
<td>3</td>
<td>332</td>
</tr>
<tr>
<td>185</td>
<td>Exercise 3.41 Better protected account</td>
<td>4.229</td>
<td>2</td>
<td>97</td>
</tr>
<tr>
<td>186</td>
<td>Exercise 3.42 Saving on serializers</td>
<td>0.023</td>
<td>1</td>
<td>33</td>
</tr>
<tr>
<td>187</td>
<td>Exercise 3.43 Multiple serializations</td>
<td>0.040</td>
<td>1</td>
<td>58</td>
</tr>
<tr>
<td>188</td>
<td>Exercise 3.44 Transfer money</td>
<td>0.005</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>189</td>
<td>Exercise 3.45 new plus old serializers</td>
<td>0.004</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>190</td>
<td>Exercise 3.46 broken test-and-set!</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>191</td>
<td>Exercise 3.47 semaphores</td>
<td>1.044</td>
<td>2</td>
<td>53</td>
</tr>
<tr>
<td>192</td>
<td>Exercise 3.48 serialized-exchange deadlock</td>
<td>0.022</td>
<td>1</td>
<td>31</td>
</tr>
<tr>
<td>193</td>
<td>Exercise 3.49 When numbering accounts doesn’t work</td>
<td>0.008</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>194</td>
<td>Exercise 3.50 stream-map multiple arguments</td>
<td>0.317</td>
<td>3</td>
<td>96</td>
</tr>
<tr>
<td>195</td>
<td>Exercise 3.51 stream-show</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>196</td>
<td>Exercise 3.52 streams with mind-boggling</td>
<td>0.034</td>
<td>1</td>
<td>49</td>
</tr>
<tr>
<td>197</td>
<td>Exercise 3.53 stream power of two</td>
<td>0.016</td>
<td>1</td>
<td>23</td>
</tr>
<tr>
<td>198</td>
<td>Exercise 3.54 mul-streams</td>
<td>0.005</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>199</td>
<td>Exercise 3.55 streams partial-sums</td>
<td>0.013</td>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>200</td>
<td>Exercise 3.56 Hamming’s streams-merge</td>
<td>0.015</td>
<td>1</td>
<td>21</td>
</tr>
<tr>
<td>201</td>
<td>Exercise 3.57 exponential additions fibs</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>202</td>
<td>Exercise 3.58 Cryptic stream</td>
<td>0.010</td>
<td>1</td>
<td>14</td>
</tr>
<tr>
<td>203</td>
<td>Exercise 3.59 power series</td>
<td>0.422</td>
<td>2</td>
<td>30</td>
</tr>
<tr>
<td>204</td>
<td>Exercise 3.60 mul-series</td>
<td>0.048</td>
<td>1</td>
<td>69</td>
</tr>
<tr>
<td>205</td>
<td>Exercise 3.61 power-series-inversion</td>
<td>0.087</td>
<td>1</td>
<td>126</td>
</tr>
<tr>
<td>206</td>
<td>Exercise 3.62 div-series</td>
<td>0.006</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>207</td>
<td>Exercise 3.63 sqrt-stream</td>
<td>0.299</td>
<td>2</td>
<td>8</td>
</tr>
<tr>
<td>208</td>
<td>Exercise 3.64 stream-limit</td>
<td>1.546</td>
<td>2</td>
<td>55</td>
</tr>
<tr>
<td>209</td>
<td>Exercise 3.65 approximating logarithm</td>
<td>0.039</td>
<td>1</td>
<td>56</td>
</tr>
<tr>
<td>210</td>
<td>Exercise 3.66 lazy pairs</td>
<td>0.515</td>
<td>2</td>
<td>107</td>
</tr>
<tr>
<td>211</td>
<td>Exercise 3.67 all possible pairs</td>
<td>0.010</td>
<td>1</td>
<td>14</td>
</tr>
<tr>
<td>212</td>
<td>Exercise 3.68 pairs-louis</td>
<td>0.012</td>
<td>1</td>
<td>17</td>
</tr>
<tr>
<td>213</td>
<td>Exercise 3.70 merge-weighted</td>
<td>0.522</td>
<td>2</td>
<td>188</td>
</tr>
<tr>
<td>214</td>
<td>Exercise 3.71 Ramanujan numbers</td>
<td>0.035</td>
<td>1</td>
<td>51</td>
</tr>
<tr>
<td>215</td>
<td>Exercise 3.72 Ramanujan 3-numbers</td>
<td>0.901</td>
<td>2</td>
<td>187</td>
</tr>
<tr>
<td>216</td>
<td>Figure 3.32</td>
<td>0.022</td>
<td>1</td>
<td>32</td>
</tr>
<tr>
<td>217</td>
<td>Exercise 3.73 RC-circuit</td>
<td>0.090</td>
<td>1</td>
<td>130</td>
</tr>
<tr>
<td>218</td>
<td>Exercise 3.74 zero-crossings</td>
<td>0.153</td>
<td>1</td>
<td>221</td>
</tr>
<tr>
<td>219</td>
<td>Exercise 3.75 filtering signals</td>
<td>0.056</td>
<td>1</td>
<td>81</td>
</tr>
<tr>
<td>220</td>
<td>Exercise 3.76 stream-smooth</td>
<td>0.073</td>
<td>2</td>
<td>36</td>
</tr>
<tr>
<td>221</td>
<td>Exercise 3.77</td>
<td>0.038</td>
<td>1</td>
<td>55</td>
</tr>
<tr>
<td>222</td>
<td>Exercise 3.78 second order differential equation</td>
<td>0.039</td>
<td>1</td>
<td>56</td>
</tr>
<tr>
<td>223</td>
<td>Exercise 3.79 general second-order ode</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>224</td>
<td>Figure 3.36</td>
<td>0.058</td>
<td>1</td>
<td>84</td>
</tr>
<tr>
<td>225</td>
<td>Exercise 3.80 RLC circuit</td>
<td>0.013</td>
<td>1</td>
<td>19</td>
</tr>
<tr>
<td>226</td>
<td>Exercise 3.81  renerator-in-streams</td>
<td>0.040</td>
<td>1</td>
<td>57</td>
</tr>
<tr>
<td>227</td>
<td>Exercise 3.82 streams Monte-Carlo</td>
<td>0.378</td>
<td>2</td>
<td>57</td>
</tr>
<tr>
<td>228</td>
<td>Exercise 4.1 list-of-values ordered</td>
<td>0.437</td>
<td>2</td>
<td>14</td>
</tr>
<tr>
<td>229</td>
<td>Exercise 4.2 application before assignments</td>
<td>0.021</td>
<td>1</td>
<td>30</td>
</tr>
<tr>
<td>230</td>
<td>Exercise 4.3 data-directed eval</td>
<td>0.030</td>
<td>1</td>
<td>43</td>
</tr>
<tr>
<td>231</td>
<td>Exercise 4.4 eval-and and eval-or</td>
<td>0.035</td>
<td>1</td>
<td>50</td>
</tr>
<tr>
<td>232</td>
<td>Exercise 4.5 cond with arrow</td>
<td>12.765</td>
<td>7</td>
<td>1252</td>
</tr>
<tr>
<td>233</td>
<td>Exercise 4.6 Implementing let</td>
<td>0.019</td>
<td>1</td>
<td>27</td>
</tr>
<tr>
<td>234</td>
<td>Exercise 4.7 Implementing let*</td>
<td>0.046</td>
<td>1</td>
<td>66</td>
</tr>
<tr>
<td>235</td>
<td>Exercise 4.8 Implementing named let</td>
<td>0.070</td>
<td>1</td>
<td>101</td>
</tr>
<tr>
<td>236</td>
<td>Exercise 4.9 Implementing until</td>
<td>0.928</td>
<td>3</td>
<td>102</td>
</tr>
<tr>
<td>237</td>
<td>Exercise 4.10 Modifying syntax</td>
<td>14.168</td>
<td>3</td>
<td>462</td>
</tr>
<tr>
<td>238</td>
<td>Exercise 4.11 Environment as a list of bindings</td>
<td>4.368</td>
<td>2</td>
<td>194</td>
</tr>
<tr>
<td>239</td>
<td>Exercise 4.12 Better abstractions for setting a value</td>
<td>0.529</td>
<td>2</td>
<td>120</td>
</tr>
<tr>
<td>240</td>
<td>Exercise 4.13 Implementing <code>make-unbound!</code></td>
<td>0.550</td>
<td>2</td>
<td>149</td>
</tr>
<tr>
<td>241</td>
<td>Exercise 4.14 meta map versus built-in map</td>
<td>0.004</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>242</td>
<td>Exercise 4.15 The <code>halts?</code> predicate</td>
<td>0.018</td>
<td>1</td>
<td>26</td>
</tr>
<tr>
<td>243</td>
<td>Exercise 4.16 Simultaneous internal definitions</td>
<td>0.162</td>
<td>2</td>
<td>177</td>
</tr>
<tr>
<td>244</td>
<td>Exercise 4.17 Environment with simultaneous definitions</td>
<td>0.036</td>
<td>1</td>
<td>52</td>
</tr>
<tr>
<td>245</td>
<td>Exercise 4.18 Alternative scanning</td>
<td>0.018</td>
<td>1</td>
<td>26</td>
</tr>
<tr>
<td>246</td>
<td>Exercise 4.19 Mutual simultaneous definitions</td>
<td>0.220</td>
<td>2</td>
<td>96</td>
</tr>
<tr>
<td>247</td>
<td>Exercise 4.20 letrec</td>
<td>0.206</td>
<td>2</td>
<td>195</td>
</tr>
<tr>
<td>248</td>
<td>Exercise 4.21 Y-combinator</td>
<td>0.013</td>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>249</td>
<td>Exercise 4.22 Extending evaluator to support <code>let</code></td>
<td>1.768</td>
<td>3</td>
<td>144</td>
</tr>
<tr>
<td>250</td>
<td>Exercise 4.23 Analysing sequences</td>
<td>0.005</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>251</td>
<td>Exercise 4.24 Analysis time test</td>
<td>0.022</td>
<td>1</td>
<td>32</td>
</tr>
<tr>
<td>252</td>
<td>Exercise 4.25 lazy factorial</td>
<td>0.034</td>
<td>1</td>
<td>49</td>
</tr>
<tr>
<td>253</td>
<td>Exercise 4.26 unless as a special form</td>
<td>0.313</td>
<td>1</td>
<td>451</td>
</tr>
<tr>
<td>254</td>
<td>Exercise 4.27 Working with mutation in lazy interpreters</td>
<td>0.515</td>
<td>2</td>
<td>112</td>
</tr>
<tr>
<td>255</td>
<td>Exercise 4.28 Eval before applying</td>
<td>0.005</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>256</td>
<td>Exercise 4.29 Lazy evaluation is slow without memoization</td>
<td>0.035</td>
<td>1</td>
<td>50</td>
</tr>
<tr>
<td>257</td>
<td>Exercise 4.30 Lazy sequences</td>
<td>0.153</td>
<td>2</td>
<td>74</td>
</tr>
<tr>
<td>258</td>
<td>Exercise 4.31 Lazy arguments with syntax extension</td>
<td>0.092</td>
<td>2</td>
<td>112</td>
</tr>
<tr>
<td>259</td>
<td>Exercise 4.32 streams versus lazy lists</td>
<td>0.503</td>
<td>2</td>
<td>87</td>
</tr>
<tr>
<td>260</td>
<td>Exercise 4.33 quoted lazy lists</td>
<td>0.097</td>
<td>2</td>
<td>103</td>
</tr>
<tr>
<td>261</td>
<td>Exercise 4.34 printing lazy lists</td>
<td>0.219</td>
<td>3</td>
<td>205</td>
</tr>
<tr>
<td>262</td>
<td>Exercise 4.50 The <code>ramb</code> operator</td>
<td>0.813</td>
<td>4</td>
<td>266</td>
</tr>
<tr>
<td>263</td>
<td>Exercise 4.35 <code>an-integer-between</code> and Pythagorean triples</td>
<td>0.103</td>
<td>2</td>
<td>138</td>
</tr>
<tr>
<td>264</td>
<td>Exercise 3.69 triples</td>
<td>0.115</td>
<td>2</td>
<td>85</td>
</tr>
<tr>
<td>265</td>
<td>Exercise 4.36 infinite search for Pythagorean triples</td>
<td>0.011</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>266</td>
<td>Exercise 4.37 another method for triples</td>
<td>0.035</td>
<td>1</td>
<td>51</td>
</tr>
<tr>
<td>267</td>
<td>Exercise 4.38 Logical puzzle – Not same floor</td>
<td>0.027</td>
<td>1</td>
<td>39</td>
</tr>
<tr>
<td>268</td>
<td>Exercise 4.39 Order of restrictions</td>
<td>0.003</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>269</td>
<td>Exercise 4.40 People to floor assignment</td>
<td>0.019</td>
<td>1</td>
<td>28</td>
</tr>
<tr>
<td>270</td>
<td>Exercise 4.41 Ordinary Scheme to solve the problem</td>
<td>0.072</td>
<td>1</td>
<td>103</td>
</tr>
<tr>
<td>271</td>
<td>Exercise 4.42 The liars puzzle</td>
<td>0.503</td>
<td>1</td>
<td>81</td>
</tr>
<tr>
<td>272</td>
<td>Exercise 4.43 Problematical Recreations</td>
<td>0.052</td>
<td>1</td>
<td>75</td>
</tr>
<tr>
<td>273</td>
<td>Exercise 4.44 Nondeterministic eight queens</td>
<td>0.074</td>
<td>1</td>
<td>106</td>
</tr>
<tr>
<td>274</td>
<td>Exercise 4.45 Five parses</td>
<td>0.186</td>
<td>3</td>
<td>145</td>
</tr>
<tr>
<td>275</td>
<td>Exercise 4.46 Order of parsing</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>276</td>
<td>Exercise 4.47 Parse verb phrase by Louis</td>
<td>0.013</td>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>277</td>
<td>Exercise 4.48 Extending the grammar</td>
<td>0.037</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>278</td>
<td>Exercise 4.49 Alyssa’s generator</td>
<td>0.031</td>
<td>1</td>
<td>45</td>
</tr>
<tr>
<td>279</td>
<td>Exercise 4.51 Implementing <code>permanent-set!</code></td>
<td>0.030</td>
<td>1</td>
<td>43</td>
</tr>
<tr>
<td>280</td>
<td>Exercise 4.52 <code>if-fail</code></td>
<td>0.063</td>
<td>1</td>
<td>91</td>
</tr>
<tr>
<td>281</td>
<td>Exercise 4.53 test evaluation</td>
<td>0.005</td>
<td>1</td>
<td>7</td>
</tr>
<tr>
<td>282</td>
<td>Exercise 4.54 <code>analyze-require</code></td>
<td>0.468</td>
<td>2</td>
<td>31</td>
</tr>
<tr>
<td>283</td>
<td>Exercise 4.55 Simple queries</td>
<td>0.258</td>
<td>2</td>
<td>372</td>
</tr>
<tr>
<td>284</td>
<td>Exercise 4.56 Compound queries</td>
<td>0.018</td>
<td>1</td>
<td>26</td>
</tr>
<tr>
<td>285</td>
<td>Exercise 4.57 custom rules</td>
<td>0.147</td>
<td>3</td>
<td>112</td>
</tr>
<tr>
<td>286</td>
<td>Exercise 4.58 big shot</td>
<td>0.025</td>
<td>1</td>
<td>36</td>
</tr>
<tr>
<td>287</td>
<td>Exercise 4.59 meetings</td>
<td>0.031</td>
<td>1</td>
<td>45</td>
</tr>
<tr>
<td>288</td>
<td>Exercise 4.60 pairs live near</td>
<td>0.016</td>
<td>1</td>
<td>23</td>
</tr>
<tr>
<td>289</td>
<td>Exercise 4.61 next-to relation</td>
<td>0.008</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>290</td>
<td>Exercise 4.62 last-pair</td>
<td>0.033</td>
<td>1</td>
<td>48</td>
</tr>
<tr>
<td>291</td>
<td>Exercise 4.63 Genesis</td>
<td>0.423</td>
<td>2</td>
<td>40</td>
</tr>
<tr>
<td>292</td>
<td>Figure 4.6 How the system works</td>
<td>0.022</td>
<td>1</td>
<td>31</td>
</tr>
<tr>
<td>293</td>
<td>Exercise 4.64 broken outranked-by</td>
<td>0.065</td>
<td>1</td>
<td>94</td>
</tr>
<tr>
<td>294</td>
<td>Exercise 4.65 second-degree subordinates</td>
<td>0.012</td>
<td>1</td>
<td>17</td>
</tr>
<tr>
<td>295</td>
<td>Exercise 4.66 Ben’s accumulation</td>
<td>0.013</td>
<td>1</td>
<td>18</td>
</tr>
<tr>
<td>296</td>
<td>Exercise 4.70 Cons-stream delays its second argument</td>
<td>0.167</td>
<td>3</td>
<td>79</td>
</tr>
<tr>
<td>297</td>
<td>Exercise 4.72 interleave-stream</td>
<td>0.002</td>
<td>1</td>
<td>3</td>
</tr>
<tr>
<td>298</td>
<td>Exercise 4.73 flatten-stream delays</td>
<td>0.006</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>299</td>
<td>Exercise 4.67 loop detector</td>
<td>0.251</td>
<td>1</td>
<td>361</td>
</tr>
<tr>
<td>300</td>
<td>Exercise 4.68 reverse rule</td>
<td>0.686</td>
<td>2</td>
<td>321</td>
</tr>
<tr>
<td>301</td>
<td>Exercise 4.69 great grandchildren</td>
<td>0.080</td>
<td>2</td>
<td>65</td>
</tr>
<tr>
<td>302</td>
<td>Exercise 4.71 Louis’ simple queries</td>
<td>0.134</td>
<td>2</td>
<td>69</td>
</tr>
<tr>
<td>303</td>
<td>Exercise 4.74 Alyssa’s streams</td>
<td>0.044</td>
<td>1</td>
<td>64</td>
</tr>
<tr>
<td>304</td>
<td>Exercise 4.75 <code>unique</code> special form</td>
<td>0.055</td>
<td>1</td>
<td>79</td>
</tr>
<tr>
<td>305</td>
<td>Exercise 4.76 improving <code>and</code></td>
<td>0.797</td>
<td>2</td>
<td>438</td>
</tr>
<tr>
<td>306</td>
<td>Figure 5.2 Controller for a GCD Machine</td>
<td>0.167</td>
<td>3</td>
<td>124</td>
</tr>
<tr>
<td>307</td>
<td>Exercise 5.1 Register machine plot</td>
<td>0.020</td>
<td>1</td>
<td>29</td>
</tr>
<tr>
<td>308</td>
<td>Figure 5.1 Data paths for a Register Machine</td>
<td>0.599</td>
<td>2</td>
<td>115</td>
</tr>
<tr>
<td>309</td>
<td>Exercise 5.2 Register machine language description of Exerci</td>
<td>0.006</td>
<td>1</td>
<td>8</td>
</tr>
<tr>
<td>310</td>
<td>Exercise 5.3 Machine for <code>sqrt</code> using Newton Method</td>
<td>0.306</td>
<td>2</td>
<td>286</td>
</tr>
<tr>
<td>311</td>
<td>Exercise 5.4 Recursive register machines</td>
<td>1.001</td>
<td>4</td>
<td>274</td>
</tr>
<tr>
<td>312</td>
<td>Exercise 5.5 Hand simulation for factorial and Fibonacci</td>
<td>0.110</td>
<td>1</td>
<td>158</td>
</tr>
<tr>
<td>313</td>
<td>Exercise 5.6 Fibonacci machine extra instructions</td>
<td>0.011</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>314</td>
<td>Exercise 5.7 Test the 5.4 machine on a simulator</td>
<td>0.458</td>
<td>2</td>
<td>133</td>
</tr>
<tr>
<td>315</td>
<td>Exercise 5.8 Ambiguous labels</td>
<td>0.469</td>
<td>1</td>
<td>160</td>
</tr>
<tr>
<td>316</td>
<td>Exercise 5.9 Prohibit (op)s on labels</td>
<td>0.017</td>
<td>1</td>
<td>25</td>
</tr>
<tr>
<td>317</td>
<td>Exercise 5.10 Changing syntax</td>
<td>0.011</td>
<td>1</td>
<td>16</td>
</tr>
<tr>
<td>318</td>
<td>Exercise 5.11 Save and restore</td>
<td>0.619</td>
<td>3</td>
<td>186</td>
</tr>
<tr>
<td>319</td>
<td>Exercise 5.12 Data paths from controller</td>
<td>0.424</td>
<td>2</td>
<td>183</td>
</tr>
<tr>
<td>320</td>
<td>Exercise 5.13 Registers from controller</td>
<td>0.470</td>
<td>2</td>
<td>101</td>
</tr>
<tr>
<td>321</td>
<td>Exercise 1.3 Sum of squares</td>
<td>1.044</td>
<td>1</td>
<td>6</td>
</tr>
<tr>
<td>322</td>
<td>Exercise 5.14 Profiling</td>
<td>0.347</td>
<td>2</td>
<td>57</td>
</tr>
<tr>
<td>323</td>
<td>Exercise 5.15 Instruction counting</td>
<td>0.052</td>
<td>1</td>
<td>75</td>
</tr>
<tr>
<td>324</td>
<td>Exercise 5.16 Tracing execution</td>
<td>0.058</td>
<td>1</td>
<td>83</td>
</tr>
<tr>
<td>325</td>
<td>Exercise 5.18 Register tracing</td>
<td>0.631</td>
<td>2</td>
<td>90</td>
</tr>
<tr>
<td>326</td>
<td>Exercise 5.19 Breakpoints</td>
<td>0.149</td>
<td>1</td>
<td>215</td>
</tr>
<tr>
<td>327</td>
<td>Exercise 5.17 Printing labels</td>
<td>0.001</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>328</td>
<td>Exercise 5.20 Drawing a list “<code>(#1=(1 . 2) #1)</code>”</td>
<td>0.189</td>
<td>2</td>
<td>139</td>
</tr>
<tr>
<td>329</td>
<td>Exercise 5.21 Register machines for list operations</td>
<td>0.617</td>
<td>2</td>
<td>115</td>
</tr>
<tr>
<td>330</td>
<td>Exercise 5.22 <code>append</code> and <code>append!</code> as register machines</td>
<td>0.047</td>
<td>1</td>
<td>68</td>
</tr>
<tr>
<td>331</td>
<td>Exercise 5.23 Extending EC-evaluator with <code>let</code> and <code>cond</code></td>
<td>0.862</td>
<td>4</td>
<td>363</td>
</tr>
<tr>
<td>332</td>
<td>Exercise 5.24 Making <code>cond</code> a primitive</td>
<td>0.160</td>
<td>2</td>
<td>199</td>
</tr>
<tr>
<td>333</td>
<td>Exercise 5.25 Normal-order (lazy) evaluation</td>
<td>1.010</td>
<td>4</td>
<td>342</td>
</tr>
<tr>
<td>334</td>
<td>Exercise 5.26 Explore tail recursion with <code>factorial</code></td>
<td>0.195</td>
<td>2</td>
<td>26</td>
</tr>
<tr>
<td>335</td>
<td>Exercise 5.27 Stack depth for a recursive factorial</td>
<td>0.008</td>
<td>1</td>
<td>11</td>
</tr>
<tr>
<td>336</td>
<td>Exercise 5.28 Interpreters without tail recursion</td>
<td>0.028</td>
<td>1</td>
<td>40</td>
</tr>
<tr>
<td>337</td>
<td>Exercise 5.29 Stack in tree-recursive Fibonacci</td>
<td>0.015</td>
<td>1</td>
<td>21</td>
</tr>
<tr>
<td>338</td>
<td>Exercise 5.30 Errors</td>
<td>0.615</td>
<td>3</td>
<td>147</td>
</tr>
<tr>
<td>339</td>
<td>Exercise 5.31 a <code>preserving</code> mechanism</td>
<td>0.417</td>
<td>2</td>
<td>161</td>
</tr>
<tr>
<td>340</td>
<td>Exercise 5.32 symbol-lookup optimization</td>
<td>0.052</td>
<td>1</td>
<td>75</td>
</tr>
<tr>
<td>341</td>
<td>Exercise 5.33 compiling <code>factorial-alt</code></td>
<td>0.753</td>
<td>2</td>
<td>267</td>
</tr>
<tr>
<td>342</td>
<td>Exercise 5.34 compiling iterative factorial</td>
<td>0.169</td>
<td>1</td>
<td>243</td>
</tr>
<tr>
<td>343</td>
<td>Exercise 5.35 Decompilation</td>
<td>0.022</td>
<td>1</td>
<td>32</td>
</tr>
<tr>
<td>344</td>
<td>Exercise 5.36 Order of evaluation</td>
<td>0.845</td>
<td>4</td>
<td>256</td>
</tr>
<tr>
<td>345</td>
<td>Exercise 5.37 <code>preserving</code></td>
<td>0.135</td>
<td>1</td>
<td>194</td>
</tr>
<tr>
<td>346</td>
<td>Exercise 5.38 open code primitives</td>
<td>0.914</td>
<td>3</td>
<td>378</td>
</tr>
<tr>
<td>347</td>
<td>Exercise 5.41 <code>find-variable</code></td>
<td>0.028</td>
<td>1</td>
<td>40</td>
</tr>
<tr>
<td>348</td>
<td>Exercise 5.39 <code>lexical-address-lookup</code></td>
<td>0.044</td>
<td>1</td>
<td>64</td>
</tr>
<tr>
<td>349</td>
<td>Exercise 5.42 Rewrite <code>compile-variable</code> and ~compile-assign</td>
<td>0.679</td>
<td>2</td>
<td>118</td>
</tr>
<tr>
<td>350</td>
<td>Exercise 5.40 maintaining a compile-time environment</td>
<td>0.085</td>
<td>2</td>
<td>101</td>
</tr>
<tr>
<td>351</td>
<td>Exercise 5.43 Scanning out defines</td>
<td>0.249</td>
<td>3</td>
<td>261</td>
</tr>
<tr>
<td>352</td>
<td>Exercise 5.44 open code with compile-time environment</td>
<td>0.020</td>
<td>1</td>
<td>29</td>
</tr>
<tr>
<td>353</td>
<td>Exercise 5.45 stack usage analysis for a <code>factorial</code></td>
<td>0.528</td>
<td>1</td>
<td>61</td>
</tr>
<tr>
<td>354</td>
<td>Exercise 5.46 stack usage analysis for <code>fibonacci</code></td>
<td>0.017</td>
<td>1</td>
<td>25</td>
</tr>
<tr>
<td>355</td>
<td>Exercise 5.47 calling interpreted procedures</td>
<td>0.049</td>
<td>1</td>
<td>71</td>
</tr>
<tr>
<td>356</td>
<td>Exercise 5.48 <code>compile-and-run</code></td>
<td>1.020</td>
<td>3</td>
<td>264</td>
</tr>
<tr>
<td>357</td>
<td>Exercise 5.49 <code>read-compile-execute-print</code> loop</td>
<td>0.015</td>
<td>1</td>
<td>22</td>
</tr>
<tr>
<td>358</td>
<td>Exercise 4.77 lazy queries</td>
<td>4.129</td>
<td>9</td>
<td>1214</td>
</tr>
<tr>
<td>359</td>
<td>Exercise 5.50 Compiling the metacircular evaluator</td>
<td>0.007</td>
<td>1</td>
<td>10</td>
</tr>
<tr>
<td>360</td>
<td>Exercise 4.78 non-deterministic queries</td>
<td>0.867</td>
<td>6</td>
<td>602</td>
</tr>
<tr>
<td>361</td>
<td>Exercise 5.51 Translating the EC-evaluator into a low-level</td>
<td>28.962</td>
<td>33</td>
<td>5684</td>
</tr>
<tr>
<td>362</td>
<td>Exercise 5.52 Making a compiler for Scheme</td>
<td>22.975</td>
<td>13</td>
<td>2359</td>
</tr>
<tr>
<td>363</td>
<td>Exercise 4.79 prolog environments</td>
<td>4.285</td>
<td>5</td>
<td>940</td>
</tr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Translating Natural Language to First-Order Logic for Logical Fallacy Detection (214 pts)]]></title>
            <link>https://arxiv.org/abs/2405.02318</link>
            <guid>43257719</guid>
            <pubDate>Tue, 04 Mar 2025 17:36:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.02318">https://arxiv.org/abs/2405.02318</a>, See on <a href="https://news.ycombinator.com/item?id=43257719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.02318">View PDF</a></p><blockquote>
            <span>Abstract:</span>Translating natural language into formal language such as First-Order Logic (FOL) is a foundational challenge in NLP with wide-ranging applications in automated reasoning, misinformation tracking, and knowledge validation. In this paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework to autoformalize natural language to FOL step by step using Large Language Models (LLMs). Our approach addresses key challenges in this translation process, including the integration of implicit background knowledge. By leveraging structured representations generated by NL2FOL, we use Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity of natural language statements. We present logical fallacy detection as a case study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach also provides interpretable insights into the reasoning process and demonstrates robustness without requiring model fine-tuning or labeled training data. Our framework achieves strong performance on multiple datasets. On the LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing effectively to the LOGICCLIMATE dataset with an F1-score of 80%.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Abhinav Lalwani [<a href="https://arxiv.org/show-email/a716fc53/2405.02318" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2405.02318v1" rel="nofollow">[v1]</a></strong>
        Thu, 18 Apr 2024 00:20:48 UTC (106 KB)<br>
    <strong>[v2]</strong>
        Mon, 3 Mar 2025 00:38:48 UTC (1,297 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Learn How to Break AES (166 pts)]]></title>
            <link>https://davidwong.fr/blockbreakers/</link>
            <guid>43257583</guid>
            <pubDate>Tue, 04 Mar 2025 17:25:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidwong.fr/blockbreakers/">https://davidwong.fr/blockbreakers/</a>, See on <a href="https://news.ycombinator.com/item?id=43257583">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
					<p>This page is here to teach you about <strong>block cipher cryptanalysis</strong>. It does not pretend to be an exhaustive list of attacks, but will give you enough to get started.</p>
					<p>The course is split in several sets, the first being on <strong>AES</strong>. We could have made up a simple block cipher, but it's more fun to attack the real thing don't you think?</p>
					<p>So head up to <a href="https://davidwong.fr/blockbreakers/aes.html"><img src="https://davidwong.fr/blockbreakers/images/AES/aes_icons-11.jpg">Set 1</a> and start by implementing <strong>AES</strong>.</p>

					<p>Don't worry, we're holding your hand ;)</p>

					<p><a href="https://davidwong.fr/blockbreakers/aes.html">Get Started <i></i></a></p>

					

					<p>This page was started by <a href="https://www.cryptologie.net/">David Wong</a>.</p>

					

					<p>Check our friends <a href="http://cryptopals.com/">Cryptopals</a> and <a href="https://microcorruption.com/">Microcorruption</a>.</p>

					

					<p>Check this <a href="https://www.reddit.com/r/crypto/wiki/index">wiki page</a>.</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DARPA exploring growing bio structures of "unprecedented size" in microgravity (146 pts)]]></title>
            <link>https://sam.gov/opp/426e5868fcf74dd4ada3768b00b09234/view</link>
            <guid>43257473</guid>
            <pubDate>Tue, 04 Mar 2025 17:16:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sam.gov/opp/426e5868fcf74dd4ada3768b00b09234/view">https://sam.gov/opp/426e5868fcf74dd4ada3768b00b09234/view</a>, See on <a href="https://news.ycombinator.com/item?id=43257473">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The inspection paradox is everywhere (2015) (103 pts)]]></title>
            <link>http://allendowney.blogspot.com/2015/08/the-inspection-paradox-is-everywhere.html</link>
            <guid>43257358</guid>
            <pubDate>Tue, 04 Mar 2025 17:06:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://allendowney.blogspot.com/2015/08/the-inspection-paradox-is-everywhere.html">http://allendowney.blogspot.com/2015/08/the-inspection-paradox-is-everywhere.html</a>, See on <a href="https://news.ycombinator.com/item?id=43257358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-8428645127515984892" itemprop="description articleBody">


<p><span>The inspection paradox is a common source of confusion, an occasional source of error, and an opportunity for clever experimental design. &nbsp;Most people are unaware of it, but like the cue marks that appear in movies to signal reel changes, once you notice it, you can’t stop seeing it.</span></p>
<p><span>A common example is the apparent paradox of class sizes. &nbsp;Suppose you ask college students how big their classes are and average the responses. &nbsp;The result might be 56. &nbsp;But if you ask the school for the average class size, they might say 31. &nbsp;It sounds like someone is lying, but they could both be right.</span></p>
<p><span><br></span><span>The problem is that when you survey students, you oversample large classes. &nbsp;If there are 10 students in a class, you have 10 chances to sample that class. &nbsp;If there are 100 students, you have 100 chances. &nbsp;In general, if the class size is </span><span>x</span><span>, it will be overrepresented in the sample by a factor of </span><span>x</span><span>.</span></p>
<p><span>That’s not necessarily a mistake. &nbsp;If you want to quantify student experience, the average across students might be a more meaningful statistic than the average across classes. &nbsp;But you have to be clear about what you are measuring and how you report it.</span></p>

<p><span>From the data in their report, I estimate the actual distribution of class sizes; then I compute the “biased” distribution you would get by sampling students. &nbsp;The CDFs of these distributions are in Figure 1.</span></p>
<p><span>Going the other way, if you are given the biased distribution, you can invert the process to estimate the actual distribution. &nbsp;You could use this strategy if the actual distribution is not available, or if it is easier to run the biased sampling process.</span></p>
<p><span><img alt="purdue3.png" height="416px;" src="https://lh3.googleusercontent.com/9fsuQt-pbezfe7mkVDGWtgTg5wqkz4vvshLDYW7-Gv-YSAtl0Y9_LQ3T2kUBIm0VnN8qLtabVrxzswFAfjmFN9gPV6_KANFyjy5GV2oMhrD7niE6kvEzfq43FdxfeKxK8g3OVQ4" width="624px;"></span></p>
<p><span>Figure 1: Undergraduate class sizes at Purdue University, 2013-14 academic year: actual distribution and biased view as seen by students.</span></p>
<p><span>The same effect applies to passenger planes. &nbsp;Airlines complain that they are losing money because so many flights are nearly empty. &nbsp;At the same time passengers complain that flying is miserable because planes are too full. &nbsp;They could both be right. &nbsp;When a flight is nearly empty, only a few passengers enjoy the extra space. &nbsp;But when a flight is full, many passengers feel the crunch.</span></p>
<p><span>Once you notice the inspection paradox, you see it everywhere. &nbsp;Does it seem like you can never get a taxi when you need one? &nbsp;Part of the problem is that when there is a surplus of taxis, only a few customers enjoy it. &nbsp;When there is a shortage, many people feel the pain.</span></p>
<p><span>Another example happens when you are waiting for public transportation. &nbsp;Buses and trains are supposed to arrive at constant intervals, but in practice some intervals are longer than others. &nbsp;With your luck, you might think you are more likely to arrive during a long interval. &nbsp;It turns out you are right: a random arrival is more likely to fall in a long interval because, well, it’s longer.</span></p>
<p><span>To quantify this effect, I collected data from the Red Line in Boston. &nbsp;Using their real-time data service, I recorded the arrival times for 70 trains between 4pm and 5pm over several days. </span></p>
<p><span><img alt="redline2.png" height="416px;" src="https://lh5.googleusercontent.com/wXeSdkxHUickHH00uf0Bw1RbYTUa1fBRwzjpTBPN_wWd1SuKkTGqzZO9DiN9lxV2QiNPu4nXWHULM6dN-XAFsMKC6ciQIoeaHD-jd-NIM6uIBfRKcZxryM9c2e9WQ32kkktgHfA" width="624px;"></span></p>
<p><span>Figure 2: Distribution of time between trains on the Red Line in Boston, between 4pm and 5pm.</span></p>
<p><span>The shortest gap between trains was less than 3 minutes; the longest was more than 15. &nbsp;Figure 2 shows the actual distribution of time between trains, and the biased distribution that would be observed by passengers. &nbsp;The average time between trains is 7.8 minutes, so we might expect the average wait time to be 3.8 minutes. &nbsp;But the average of the biased distribution is 8.8 minutes, and the average wait time for passengers is 4.4 minutes, about 15% longer.</span></p>
<p><span>In this case the difference between the two distributions is not very big because the variance of the actual distribution is moderate. &nbsp;When the actual distribution is long-tailed, the effect of the inspection paradox can be much bigger.</span></p>
<p><span>An example of a long-tailed distribution comes up in the context of social networks. &nbsp;In 1991, Scott Feld presented the “friendship paradox”: the observation that most people have fewer friends than their friends have. &nbsp;He studied real-life friends, but the same effect appears in online networks: if you choose a random Facebook user, and then choose one of their friends at random, the chance is about 80% that the friend has more friends.</span></p>
<p><span>The friendship paradox is a form of the inspection paradox. &nbsp;When you choose a random user, every user is equally likely. &nbsp;But when you choose one of their friends, you are more likely to choose someone with a lot of friends. &nbsp;Specifically, someone with </span><span>x</span><span> friends is overrepresented by a factor of </span><span>x</span><span>.</span></p>
<p><span>To demonstrate the effect, I use data from the Stanford Large Network Dataset Collection (</span><a href="http://snap.stanford.edu/data"><span>http://snap.stanford.edu/data</span></a><span>), which includes a sample of about 4000 Facebook users. &nbsp;We can compute the number of friends each user has and plot the distribution, shown in Figure 3. &nbsp;The distribution is skewed: most users have only a few friends, but some have hundreds.</span></p>
<p><span><br></span><span>We can also compute the biased distribution we would get by choosing choosing random friends, also shown in Figure 3. &nbsp;The difference &nbsp;is huge. &nbsp;In this dataset, the average user has 42 friends; the average friend has more than twice as many, 91.</span></p>
<p><span><img alt="social4.png" height="416px;" src="https://lh5.googleusercontent.com/5T3I9chtDwXo6tzrWhkWARVTgE6SqtQCUCGVBn9N0-l5zJFblc7WvuW4-5S9QtCKYCPpE3U_-jVCVrO8N44ZZncPaoZhCkIJFGcHgHf0Wosp_bHKWp3Xobd4PdWEYs5kptnohX0" width="624px;"></span></p>
<p><span>Figure 3: Number of online friends for Facebook users: actual distribution and biased distribution seen by sampling friends.</span></p>
<p><span>Some examples of the inspection paradox are more subtle. &nbsp;One of them occurred to me when I ran a 209-mile relay race in New Hampshire. &nbsp;I ran the sixth leg for my team, so when I started running, I jumped into the middle of the race. &nbsp;After a few miles I noticed something unusual: when I overtook slower runners, they were usually much slower; and when faster runners passed me, they were usually much faster.</span></p>
<p><span>At first I thought the distribution of runners was bimodal, with many slow runners, many fast runners, and few runners like me in the middle. &nbsp;Then I realized that I was fooled by the inspection paradox.</span></p>
<p><span>In many long relay races, teams start at different times, and most teams include a mix of faster and slower runners. &nbsp;As a result, runners at different speeds end up spread over the course; if you stand at &nbsp;random spot and watch runners go by, you see a nearly representative sample of speeds. &nbsp;But if you jump into the race in the middle, the sample you see depends on your speed.</span></p>
<p><span>Whatever speed you run, you are more likely to pass very slow runners, more likely to be overtaken by fast runners, and unlikely to see anyone running at the same speed as you. &nbsp;The chance of seeing another runner is proportional to the difference between your speed and theirs.</span></p>
<p><span>We can simulate this effect using data from a conventional road race. &nbsp;Figure 4 shows the actual distribution of speeds from the James Joyce Ramble, a 10K race in Massachusetts. &nbsp;It also shows biased distributions that would be seen by runners at 6.5 and 7.5 mph. &nbsp;The observed distributions are bimodal, with fast and slow runners oversampled and fewer runners in the middle.</span></p>
<p><span><img alt="relay3.png" height="424" src="https://lh6.googleusercontent.com/SEjYB0ZDjxuDhimb0ha6YctjFcujlE2fGBxhT1sPNw2VYm_4DwzgJ9vZkmGC2jUgQ9pC3vW9JbByCd3RI0STXNFSszj7feFe4iEFyIcaDYLOcN1oBtXeTiSV5OtUIEJmVMZG0Ho" width="640"></span></p>
<p><span>Figure 4: Distribution of speed for runners in a 10K, and biased distributions as seen by runners at different speeds.</span></p>
<p><span>A final example of the inspection paradox occurred to me when I was reading </span><span>Orange is the New Black</span><span>, a memoir by Piper Kerman, who spent 13 months in a federal prison. &nbsp;At several points Kerman expresses surprise at the length of the sentences her fellow prisoners are serving. &nbsp;She is right to be surprised, but it turns out that she is the victim of not just an inhumane prison system, but also the inspection paradox.</span></p>
<p><span>If you arrive at a prison at a random time and choose a random prisoner, you are more likely to choose a prisoner with a long sentence. &nbsp;Once again, a prisoner with sentence </span><span>x</span><span> is oversampled by a factor of </span><span>x</span><span>.</span></p>
<p><span>Using data from the U.S. Sentencing Commission, I made a rough estimate of the distribution of sentences for federal prisoners, shown in Figure 5. &nbsp;I also computed the biased distribution as observed by a random arrival.</span></p>
<p><span><img alt="orange2.png" height="416px;" src="https://lh4.googleusercontent.com/N-6KgYuQY56lAecWYBWS4iXtNlKNLVoDCDkGmo7LIrqf9lEx1HYMZYWgg5i1LxnyB0tmKSJPT49GkUIKT48Fwj9fhYro74M7LCTOEwotlYls4gNJ0a9UM7FU8G-di9LN19A7d4U" width="624px;"></span></p>
<p><span>Figure 5: Approximate distribution of federal prison sentences, and a biased distribution as seen by a random arrival.</span></p>
<p><span>As expected, the biased distribution is shifted to the right. &nbsp;In the actual distibution the mean is 121 months; in the biased distribution it is 183 months.</span></p>
<p><span>So what happens if you observe a prison over an interval like 13 months? &nbsp;It turns out that if your sentence is </span><span>y</span><span> months, the chance of overlapping with a prisoner whose sentence is </span><span>x</span><span> months is proportional to </span><span>x</span><span> + </span><span>y</span><span>.</span></p>
<p><span>Figure 6 shows biased distributions as seen by hypothetical prisoners serving sentences of 13, 120, and 600 months.</span></p>
<p><span><img alt="orange6.png" height="268px;" src="https://lh4.googleusercontent.com/GU0vXLubR-R5pHwggShLLaR9HAQUOv8p713JhS06UGcNS_5S6KZAcZ7oLxzpqTWq_0OaqM4pgOJ1SSZZjx_GoZDOhrhA3_MPknsmX1uil_4cWfiQ0AbED8szw66S5o4qiOGGoyA" width="624px;"></span></p>
<p><span>Figure 6: Biased distributions as seen by prisoners with different sentences.</span></p>
<p><span>Over an interval of 13 months, the observed sample is not much better than the biased sample seen by a random arrival. &nbsp;After 120 months, the magnitude of the bias is about halved. &nbsp;Only after a very long sentence, 600 months, do we get a more representative sample, and even then it is not entirely unbiased.</span></p>
<p><span>These examples show that the inspection paradox appears in many domains, sometimes in subtle ways. &nbsp;If you are not aware of it, it can cause statistical errors and lead to invalid inferences. &nbsp;But in many cases it can be avoided, or even used deliberately as part of an experimental design.</span></p>
<p><span>Further reading</span></p>
<p><span>I discuss the class size example in my book, </span><span>Think Stats</span><span>, 2nd Edition, O’Reilly Media, 2014, and the Red Line example in </span><span>Think Bayes</span><span>, O’Reilly Media, 2013. &nbsp;I wrote about relay races, social networks, and </span><span>Orange Is the New Black</span><span> in my blog, “Probably Overthinking It”. &nbsp;</span><a href="http://allendowney.blogspot.com/"><span>http://allendowney.blogspot.com/</span></a></p>

<p><span>The original paper on the topic might be Scott Feld, &nbsp;“Why Your Friends Have More Friends Than You Do”, American Journal of Sociology, Vol. 96, No. 6 (May, 1991), pp. 1464-1477. </span><a href="http://www.jstor.org/stable/2781907"><span>http://www.jstor.org/stable/2781907</span></a></p>
<p><span>Amir Aczel discusses some of these examples, and a few different ones, in a Discover Magazine blog article, “On the Persistence of Bad Luck (and Good)”, 4 September 4, 2013.</span></p>

<p><span>The code I used to generate these examples is in these Jupyter notebooks:</span></p>





<p><span>Bio</span></p>

<p><span>Allen Downey is a Professor of Computer Science at Olin College of Engineering in Massachusetts. &nbsp;He is the author of several books, including </span><span>Think Python</span><span>, </span><span>Think Stats</span><span>, and </span><span>Think Bayes</span><span>. &nbsp;He is a runner with a maximum 10K speed of 8.7 mph.</span></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Enhanced Radar (YC W25) – A safety net for air traffic control (137 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43257323</link>
            <guid>43257323</guid>
            <pubDate>Tue, 04 Mar 2025 17:04:17 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43257323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="43257323">
      <td><span></span></td>      <td><center><a id="up_43257323" href="https://news.ycombinator.com/vote?id=43257323&amp;how=up&amp;goto=item%3Fid%3D43257323"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=43257323">Launch HN: Enhanced Radar (YC W25) – A safety net for air traffic control</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_43257323">106 points</span> by <a href="https://news.ycombinator.com/user?id=kristian1109">kristian1109</a> <span title="2025-03-04T17:04:17 1741107857"><a href="https://news.ycombinator.com/item?id=43257323">8 hours ago</a></span> <span id="unv_43257323"></span> | <a href="https://news.ycombinator.com/hide?id=43257323&amp;goto=item%3Fid%3D43257323">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Enhanced%20Radar%20%28YC%20W25%29%20%E2%80%93%20A%20safety%20net%20for%20air%20traffic%20control&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=43257323&amp;auth=f3af032166c0094530e6a205fe90cdddea346c7c">favorite</a> | <a href="https://news.ycombinator.com/item?id=43257323">75&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN, we’re Eric and Kristian of Enhanced Radar. We’re working on making air travel safer by augmenting control services in our stressed airspace system.</p><p>Recent weeks have put aviation safety on everyone’s mind, but we’ve been thinking about this problem for years. Both of us are pilots — we have 2,500 hours of flight time between us. Eric flew professionally and holds a Gulfstream 280 type rating and both FAA and EASA certificates. Kristian flies recreationally, and before this worked on edge computer vision for satellites.</p><p>We know from our flying experience that air traffic management is imperfect (every pilot can tell stories of that one time…), so this felt like an obvious problem to work on.</p><p>Most accidents are the result of an overdetermined “accident chain” (<a href="https://code7700.com/accident_investigation.htm" rel="nofollow">https://code7700.com/accident_investigation.htm</a>). The popular analogy here is the swiss cheese model, where holes in every slice line up perfectly to cause an accident. Often, at least one link in that chain is human error.</p><p>We’ll avoid dissecting this year’s tragedies and take a close call from last April at DCA as an example:</p><p>The tower cleared JetBlue 1554 to depart on Runway 04, but simultaneously a ground controller on a different frequency cleared a Southwest jet to cross that same runway, putting them on a collision course. Controllers noticed the conflict unfolding and jumped in to yell at both aircraft to stop, avoiding a collision with about 8 seconds to spare (<a href="https://www.youtube.com/watch?v=yooJmu30DxY" rel="nofollow">https://www.youtube.com/watch?v=yooJmu30DxY</a>).</p><p>Importantly, the error that caused this incident occurred approximately 23 seconds before the conflict became obvious. In this scenario, a good solution would be a system that understands when an aircraft has been cleared to depart from a runway, and then makes sure no aircraft are cleared to cross (or are in fact crossing) that runway until the departing aircraft is wheels-up. And so on.</p><p>To do this, we’ve developed Yeager, an ensemble of models including state of the art speech-to-text that can understand ATC audio. It’s trained on a large amount of our own labeled ATC audio collected from our VHF receivers located at airports around the US. We improve performance by injecting context such as airport layout details, nearby/relevant navaids, and information on all relevant aircraft captured via ADS-B.</p><p>Our product piggy-backs on the raw signal in the air (VHF radio from towers to pilots) by having our own antennas, radios, and software installed at the airport. This system is completely parallel to existing infrastructure, requires zero permission, and zero integration. It’s an extra safety net over existing systems (no replacement required). All the data we need is open-source and unencrypted.</p><p>Building models for processing ATC speech is our first step toward building a safety net that detects human error (by both pilots and ATC). The latest system transcribes the VHF control audio at about ~1.1% WER (Word Error Rate), down from a previous record of ~9%. We’re using these transcripts with NLP and ADS-B (the system that tracks aircraft positions in real time) for readback detection (ensuring pilots correctly repeat ATC instructions) and command compliance.</p><p>There are different views about the future of ATC. Our product is naturally based on our own convictions and experience in the field. For example, it’s sometimes said that voice comms are going away — we think they aren’t (<a href="https://www.ericbutton.co/p/speech" rel="nofollow">https://www.ericbutton.co/p/speech</a>). People also point out that airplanes are going to fly themselves — in fact they already do. But passenger airlines, for example, will keep a pilot onboard (or on the ground) with ultimate control, for a long time from now; the economics and politics and mind-boggling safety and legal standards for aviation make this inevitable. Also, while next-gen ATC systems like ASDE-X are already in place, they don’t eliminate the problem. The April 2024 scenario mentioned above occurred at DCA, an ASDE-X-equipped airport.</p><p>America has more than 5,000 public-use airports, but only 540 of these have control towers (due to cost). As a result, there are over 100 commercial airline routes that fly into uncontrolled airports, and 4.4M landings at these fields. Air traffic control from first principles looks significantly more automated, more remote-controlled, and much cheaper — and as a result, much more widespread.</p><p>We’ve known each other for 3 years, and decided independently that we needed to work on air traffic. Having started on this, we feel like it’s our mission for the next decade or two.</p><p>If you’re a pilot or an engineer who’s thought about this stuff, we’d love to get your input. We look forward to hearing everyone’s thoughts, questions, ideas!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Satellogic's Open Satellite Feed (224 pts)]]></title>
            <link>https://tech.marksblogg.com/satellogic-open-data-feed.html</link>
            <guid>43256349</guid>
            <pubDate>Tue, 04 Mar 2025 15:56:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tech.marksblogg.com/satellogic-open-data-feed.html">https://tech.marksblogg.com/satellogic-open-data-feed.html</a>, See on <a href="https://news.ycombinator.com/item?id=43256349">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_text">
            <p>Satellogic is a satellite designer, manufacturer and constellation operator. They were founded in 2010 and have offices in Argentina, Uruguay, Spain and the US.</p>
<p>They launched three prototype Cube satellites between 2013 and 2014, using China and Russia as their launch partners.</p>
<p>In 2016, they began launching ~38 KG, 10K-component, 51 x 57 x 82-cm NewSat microsatellites. These take around three months to build and are meant to last for three years in orbit. They've named the constellation of these satellites "Aleph-1".</p>
<p>Below is a rendering of their NewSat satellite.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/Aleph-1_internal_architecture.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/Aleph-1_internal_architecture.png"></a></p><p>They're aiming to have 300 satellites in orbit at some point which would provide revisit times up to every 5 minutes.</p>
<p>Last week, Satellogic announced an open satellite feed programme and named their dataset "Satellogic EarthView".</p>
<p>In this post, I'll examine Satellogic's constellation and open data feed.</p>
<div id="my-workstation">
<h2>My Workstation</h2>
<p>I'm using a 6 GHz Intel Core <a href="https://www.intel.com/content/www/us/en/products/sku/236773/intel-core-i9-processor-14900k-36m-cache-up-to-6-00-ghz/specifications.html">i9-14900K</a> CPU. It has 8 performance cores and 16 efficiency cores with a total of 32 threads and 32 MB of L2 cache. It has a liquid cooler attached and is housed in a spacious, full-sized Cooler Master HAF 700 computer case. I've come across videos on YouTube where people have managed to overclock the i9-14900KF to <a href="https://www.youtube.com/watch?v=fb7pl7PZOYo">9.1 GHz</a>.</p>
<p>The system has 96 GB of DDR5 RAM clocked at 6,000 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.</p>
<p>The system is powered by a 1,200-watt, fully modular Corsair Power Supply and is sat on an ASRock Z790 Pro RS Motherboard.</p>
<p>I'm running Ubuntu 22 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and I use ArcGIS Pro from time to time which only supports Windows natively.</p>
</div>
<div id="installing-prerequisites">
<h2>Installing Prerequisites</h2>
<p>I'll use GDAL 3.4.1, Python 3.8 and a few other tools to help analyse the data in this post.</p>
<div><pre><span></span>$<span> </span>sudo<span> </span>apt<span> </span>update
$<span> </span>sudo<span> </span>apt<span> </span>install<span> </span><span>\</span>
<span>    </span>gdal-bin<span> </span><span>\</span>
<span>    </span>jq<span> </span><span>\</span>
<span>    </span>libimage-exiftool-perl<span> </span><span>\</span>
<span>    </span>python3-pip<span> </span><span>\</span>
<span>    </span>python3-virtualenv
</pre></div>
<p>I'll set up a Python Virtual Environment and install some dependencies.</p>
<div><pre><span></span>$<span> </span>python3<span> </span>-m<span> </span>venv<span> </span>~/.satl
$<span> </span><span>source</span><span> </span>~/.satl/bin/activate
$<span> </span>python3<span> </span>-m<span> </span>pip<span> </span>install<span> </span><span>\</span>
<span>    </span>astropy<span> </span><span>\</span>
<span>    </span>awscli<span> </span><span>\</span>
<span>    </span>html2text<span> </span><span>\</span>
<span>    </span>requests<span> </span><span>\</span>
<span>    </span>rich<span> </span><span>\</span>
<span>    </span>sgp4<span> </span><span>\</span>
<span>    </span>utm
</pre></div>
<p>I'll use DuckDB, along with its <a href="https://github.com/isaacbrodsky/h3-duckdb">H3</a>, <a href="https://duckdb.org/docs/extensions/json">JSON</a>, <a href="https://community-extensions.duckdb.org/extensions/lindel.html">Lindel</a>, <a href="https://duckdb.org/docs/data/parquet/overview">Parquet</a> and <a href="https://duckdb.org/docs/extensions/spatial.html">Spatial</a> extensions, in this post.</p>
<div><pre><span></span>$<span> </span><span>cd</span><span> </span>~
$<span> </span>wget<span> </span>-c<span> </span>https://github.com/duckdb/duckdb/releases/download/v1.1.3/duckdb_cli-linux-amd64.zip
$<span> </span>unzip<span> </span>-j<span> </span>duckdb_cli-linux-amd64.zip
$<span> </span>chmod<span> </span>+x<span> </span>duckdb
$<span> </span>~/duckdb
</pre></div>
<div><pre><span></span><span>INSTALL</span><span> </span><span>h3</span><span> </span><span>FROM</span><span> </span><span>community</span><span>;</span>
<span>INSTALL</span><span> </span><span>lindel</span><span> </span><span>FROM</span><span> </span><span>community</span><span>;</span>
<span>INSTALL</span><span> </span><span>json</span><span>;</span>
<span>INSTALL</span><span> </span><span>parquet</span><span>;</span>
<span>INSTALL</span><span> </span><span>spatial</span><span>;</span>
</pre></div>
<p>I'll set up DuckDB to load every installed extension each time it launches.</p>

<div><pre><span></span>.timer on
.width 180
LOAD h3;
LOAD lindel;
LOAD json;
LOAD parquet;
LOAD spatial;
</pre></div>
<p>The maps in this post were rendered with <a href="https://www.qgis.org/en/site/forusers/download.html">QGIS</a> version 3.42. QGIS is a desktop application that runs on Windows, macOS and Linux. The application has grown in popularity in recent years and has ~15M application launches from users all around the world each month.</p>
<p>I used QGIS' <a href="https://github.com/marklit/tile_plus">Tile+ plugin</a> to add geospatial context with Bing's Virtual Earth Basemap to the maps. The dark, non-satellite imagery maps are mostly made up of vector data from Natural Earth and Overture.</p>
</div>
<div id="aleph-1-s-launch-history">
<h2>Aleph-1's Launch History</h2>
<p>Satellogic has launched 41 satellites for its Aleph-1 constellation as of January 14, 2025.</p>
<p>Their first seven satellites were launched by China between May 2016 and September 2020.</p>
<p>Their eighth satellite was launched by ESA from the Spaceport in Kourou, French Guiana in September 2020.</p>
<p>China then launched 10 satellites of theirs in one mission in November 2020.</p>
<p>Since June 2021, SpaceX have launched 23 of their satellites with the most recent  being on January 14th, 2025.</p>
</div>
<div id="active-satellites">
<h2>Active Satellites</h2>
<p>Below I've tried to find all of their active satellites' locations. The positions of their satellites should change each time the following script is executed.</p>
<p>I couldn't find an official listing of their satellites <a href="https://en.wikipedia.org/wiki/Two-line_element_set">Two-line elements</a> (TLEs) but I found a listing of NORAD IDs. I scraped the n2yo listings for each satellite and parsed the name and TLE for each of those results.</p>

<div><pre><span></span><span>import</span> <span>json</span>
<span>from</span>   <span>glob</span>            <span>import</span> <span>glob</span>
<span>from</span>   <span>random</span>          <span>import</span> <span>randint</span>
<span>from</span>   <span>time</span>            <span>import</span> <span>sleep</span>

<span>from</span>   <span>html2text</span>       <span>import</span> <span>html2text</span>
<span>import</span> <span>requests</span>
<span>from</span>   <span>rich.progress</span>   <span>import</span> <span>track</span>


<span>for</span> <span>norad_id</span> <span>in</span> <span>track</span><span>((</span><span>46832</span><span>,</span> <span>46829</span><span>,</span> <span>46827</span><span>,</span> <span>46833</span><span>,</span> <span>46831</span><span>,</span> <span>46830</span><span>,</span>
                       <span>46840</span><span>,</span> <span>46835</span><span>,</span> <span>46836</span><span>,</span> <span>48905</span><span>,</span> <span>48921</span><span>,</span> <span>48920</span><span>,</span>
                       <span>48919</span><span>,</span> <span>52168</span><span>,</span> <span>52178</span><span>,</span> <span>52171</span><span>,</span> <span>52184</span><span>,</span> <span>52172</span><span>,</span>
                       <span>52747</span><span>,</span> <span>52764</span><span>,</span> <span>42760</span><span>,</span> <span>52748</span><span>,</span> <span>52752</span><span>,</span> <span>55064</span><span>,</span>
                       <span>55047</span><span>,</span> <span>55045</span><span>,</span> <span>55048</span><span>,</span> <span>56190</span><span>,</span> <span>56203</span><span>,</span> <span>56202</span><span>,</span>
                       <span>56201</span><span>,</span> <span>56943</span><span>,</span> <span>56944</span><span>,</span> <span>56966</span><span>,</span> <span>56968</span><span>,</span> <span>59122</span><span>,</span>
                       <span>60498</span><span>,</span> <span>60500</span><span>,</span> <span>60493</span><span>,</span> <span>46272</span><span>,</span> <span>45017</span><span>,</span> <span>45018</span><span>,</span>
                       <span>46828</span><span>)):</span>
    <span>resp</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>'https://www.n2yo.com/satellite/?s=</span><span>%d</span><span>'</span> <span>%</span> <span>norad_id</span><span>)</span>
    <span>assert</span> <span>resp</span><span>.</span><span>status_code</span> <span>==</span> <span>200</span><span>,</span> <span>resp</span><span>.</span><span>status_code</span>

    <span>with</span> <span>open</span><span>(</span><span>'</span><span>%d</span><span>.md'</span> <span>%</span> <span>norad_id</span><span>,</span> <span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
        <span>f</span><span>.</span><span>write</span><span>(</span><span>html2text</span><span>(</span><span>resp</span><span>.</span><span>content</span><span>.</span><span>decode</span><span>(</span><span>'utf-8'</span><span>)))</span>

    <span>sleep</span><span>(</span><span>randint</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>))</span>

<span>tles</span> <span>=</span> <span>{}</span>

<span>for</span> <span>filename</span> <span>in</span> <span>glob</span><span>(</span><span>'*.md'</span><span>):</span>
    <span>lines</span> <span>=</span> <span>open</span><span>(</span><span>filename</span><span>)</span><span>.</span><span>readlines</span><span>()</span>
    <span>norad_id</span> <span>=</span> <span>int</span><span>(</span><span>filename</span><span>.</span><span>split</span><span>(</span><span>'.'</span><span>)[</span><span>0</span><span>])</span>
    <span>sat_name</span> <span>=</span> <span>None</span>

    <span>for</span> <span>num</span><span>,</span> <span>line</span> <span>in</span> <span>enumerate</span><span>(</span><span>lines</span><span>):</span>
        <span>if</span> <span>line</span><span>.</span><span>startswith</span><span>(</span><span>'# NUSAT-'</span><span>):</span>
            <span>sat_name</span> <span>=</span> <span>line</span><span>.</span><span>lstrip</span><span>(</span><span>'# '</span><span>)</span><span>.</span><span>strip</span><span>()</span>

        <span>if</span> <span>line</span><span>.</span><span>startswith</span><span>(</span><span>'    1'</span><span>):</span>
            <span>tles</span><span>[</span><span>norad_id</span><span>]</span> <span>=</span> <span>{</span>
                <span>'tle'</span><span>:</span> <span>[</span><span>line</span><span>.</span><span>strip</span><span>(),</span> <span>lines</span><span>[</span><span>num</span> <span>+</span> <span>1</span><span>]</span><span>.</span><span>strip</span><span>()],</span>
                <span>'name'</span><span>:</span> <span>sat_name</span><span>}</span>

            <span>break</span> <span># The TLE always comes after the sat name</span>

<span>open</span><span>(</span><span>'tles.json'</span><span>,</span> <span>'w'</span><span>)</span><span>.</span><span>write</span><span>(</span><span>json</span><span>.</span><span>dumps</span><span>(</span><span>tles</span><span>))</span>
</pre></div>
<p>The above produced names and TLEs for 22 of their satellites. I'm not sure if the above is an accurate reflection of their constellation's status or not so please don't treat this as gospel.</p>
<p>I then ran the following on March 4th, 2025. It produced a CSV file with names and estimated locations of those 22 satellites.</p>

<div><pre><span></span><span>from</span>   <span>datetime</span> <span>import</span> <span>datetime</span>
<span>import</span> <span>json</span>

<span>from</span>   <span>astropy</span> <span>import</span> <span>units</span> <span>as</span> <span>u</span>
<span>from</span>   <span>astropy.time</span> <span>import</span> <span>Time</span>
<span>from</span>   <span>astropy.coordinates</span> <span>import</span> <span>ITRS</span><span>,</span> \
                                  <span>TEME</span><span>,</span> \
                                  <span>CartesianDifferential</span><span>,</span> \
                                  <span>CartesianRepresentation</span>
<span>from</span>   <span>sgp4.api</span> <span>import</span> <span>Satrec</span>
<span>from</span>   <span>sgp4.api</span> <span>import</span> <span>SGP4_ERRORS</span>


<span>tles</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>open</span><span>(</span><span>'tles.json'</span><span>)</span><span>.</span><span>read</span><span>())</span>

<span>with</span> <span>open</span><span>(</span><span>'locations.csv'</span><span>,</span> <span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>for</span> <span>norad_id</span> <span>in</span> <span>tles</span><span>:</span>

        <span>name</span>  <span>=</span> <span>tles</span><span>[</span><span>norad_id</span><span>][</span><span>'name'</span><span>]</span>
        <span>line1</span> <span>=</span> <span>tles</span><span>[</span><span>norad_id</span><span>][</span><span>'tle'</span><span>][</span><span>0</span><span>]</span>
        <span>line2</span> <span>=</span> <span>tles</span><span>[</span><span>norad_id</span><span>][</span><span>'tle'</span><span>][</span><span>1</span><span>]</span>

        <span>satellite</span> <span>=</span> <span>Satrec</span><span>.</span><span>twoline2rv</span><span>(</span><span>line1</span><span>,</span> <span>line2</span><span>)</span>

        <span>t</span> <span>=</span> <span>Time</span><span>(</span><span>datetime</span><span>.</span><span>utcnow</span><span>()</span><span>.</span><span>isoformat</span><span>(),</span> <span>format</span><span>=</span><span>'isot'</span><span>,</span> <span>scale</span><span>=</span><span>'utc'</span><span>)</span>

        <span>error_code</span><span>,</span> <span>teme_p</span><span>,</span> <span>teme_v</span> <span>=</span> <span>satellite</span><span>.</span><span>sgp4</span><span>(</span><span>t</span><span>.</span><span>jd1</span><span>,</span> <span>t</span><span>.</span><span>jd2</span><span>)</span> <span># in km and km/s</span>

        <span>if</span> <span>error_code</span> <span>!=</span> <span>0</span><span>:</span>
            <span>raise</span> <span>RuntimeError</span><span>(</span><span>SGP4_ERRORS</span><span>[</span><span>error_code</span><span>])</span>

        <span>teme_p</span> <span>=</span> <span>CartesianRepresentation</span><span>(</span><span>teme_p</span> <span>*</span> <span>u</span><span>.</span><span>km</span><span>)</span>
        <span>teme_v</span> <span>=</span> <span>CartesianDifferential</span><span>(</span><span>teme_v</span> <span>*</span> <span>u</span><span>.</span><span>km</span> <span>/</span> <span>u</span><span>.</span><span>s</span><span>)</span>
        <span>teme</span> <span>=</span> <span>TEME</span><span>(</span><span>teme_p</span><span>.</span><span>with_differentials</span><span>(</span><span>teme_v</span><span>),</span> <span>obstime</span><span>=</span><span>t</span><span>)</span>

        <span>itrs_geo</span> <span>=</span> <span>teme</span><span>.</span><span>transform_to</span><span>(</span><span>ITRS</span><span>(</span><span>obstime</span><span>=</span><span>t</span><span>))</span>
        <span>location</span> <span>=</span> <span>itrs_geo</span><span>.</span><span>earth_location</span>
        <span>loc</span> <span>=</span> <span>location</span><span>.</span><span>geodetic</span>

        <span>f</span><span>.</span><span>write</span><span>(</span><span>'"</span><span>%s</span><span>", "POINT (</span><span>%f</span><span> </span><span>%f</span><span>)"</span><span>\n</span><span>'</span> <span>%</span> <span>(</span><span>name</span><span>.</span><span>lstrip</span><span>(</span><span>'0'</span><span>)</span><span>.</span><span>lstrip</span><span>(),</span>
                                             <span>loc</span><span>.</span><span>lon</span><span>.</span><span>deg</span><span>,</span>
                                             <span>loc</span><span>.</span><span>lat</span><span>.</span><span>deg</span><span>))</span>
</pre></div>
<p>Below is a rendering of the above CSV data in QGIS.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_gfkvrYHvtc.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_gfkvrYHvtc.jpg"></a>
</p></div>
<div id="open-data-feed">
<h2>Open Data Feed</h2>
<p>Their S3 bucket appears to contain ~7M unique images from more than 3M locations. There is a metadata file for each image they've collected as well as several versions, such as thumbnails, of each image available. There are both RGB and near-infrared imagery available with a resolution of one meter.</p>
<p>Below, I'll list out the contents of their S3 bucket.</p>
<div><pre><span></span>$<span> </span>aws<span> </span>--no-sign-request<span> </span><span>\</span>
<span>      </span>--output<span> </span>json<span> </span><span>\</span>
<span>      </span>s3api<span> </span><span>\</span>
<span>      </span>list-objects<span> </span><span>\</span>
<span>      </span>--bucket<span> </span>satellogic-earthview<span> </span><span>\</span>
<span>      </span>--max-items<span>=</span><span>100000000</span><span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>jq<span> </span>-c<span> </span><span>'.Contents[]'</span><span> </span><span>\</span>
<span>    </span>&gt;<span> </span>satellogic.s3.json
</pre></div>
<p>The resulting JSON file contains 35,480,124 lines and is 13 GB uncompressed. There are almost 10 TBs of content in this bucket.</p>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"Total objects: "</span><span> </span><span>`</span>wc<span> </span>-l<span> </span>satellogic.s3.json<span> </span><span>|</span><span> </span>cut<span> </span>-d<span>' '</span><span> </span>-f1<span>`</span>,<span> </span><span>\</span>
<span>        </span><span>" TB: "</span><span> </span><span>`</span>jq<span> </span>.Size<span> </span>satellogic.s3.json<span> </span><span>|</span><span> </span>awk<span> </span><span>'{s+=$1}END{print s/1024/1024/1024/1024}'</span><span>`</span>
</pre></div>
<div><pre><span></span>Total objects:  35480124,  TB:  9.79704
</pre></div>
<p>Each metadata filename contains its corresponding image's location as well as other useful attributes. I'll parse these filenames and produce a metadata database in DuckDB.</p>

<div><pre><span></span><span>import</span> <span>json</span>

<span>from</span>   <span>rich.progress</span> <span>import</span> <span>track</span>
<span>import</span> <span>utm</span>


<span>with</span> <span>open</span><span>(</span><span>'enriched.s3.json'</span><span>,</span> <span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>for</span> <span>line</span> <span>in</span> <span>track</span><span>(</span><span>open</span><span>(</span><span>'satellogic.s3.json'</span><span>),</span>
                      <span>total</span><span>=</span><span>35_480_124</span><span>):</span>
        <span>rec</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>line</span><span>)</span>
        <span>if</span> <span>not</span> <span>rec</span><span>[</span><span>'Key'</span><span>]</span><span>.</span><span>startswith</span><span>(</span><span>'data/json/'</span><span>):</span>
            <span>continue</span>

        <span>date_</span><span>,</span> \
        <span>time_</span><span>,</span> \
        <span>rec</span><span>[</span><span>'sat'</span><span>],</span> \
        <span>rec</span><span>[</span><span>'zone'</span><span>],</span> \
        <span>rec</span><span>[</span><span>'region'</span><span>],</span> \
        <span>rec</span><span>[</span><span>'region2'</span><span>],</span> \
        <span>_</span> <span>=</span> <span>rec</span><span>[</span><span>'Key'</span><span>]</span><span>.</span><span>split</span><span>(</span><span>'/'</span><span>)[</span><span>5</span><span>]</span><span>.</span><span>split</span><span>(</span><span>'_'</span><span>)</span>

        <span>rec</span><span>[</span><span>'captured_at'</span><span>]</span> <span>=</span> \
            <span>date_</span><span>[</span><span>0</span><span>:</span><span>4</span><span>]</span> <span>+</span> <span>'-'</span> <span>+</span> <span>date_</span><span>[</span><span>4</span><span>:</span><span>6</span><span>]</span> <span>+</span> <span>'-'</span> <span>+</span> <span>date_</span><span>[</span><span>6</span><span>:</span><span>8</span><span>]</span> <span>+</span> <span>'T'</span> <span>+</span> \
            <span>time_</span><span>[</span><span>0</span><span>:</span><span>2</span><span>]</span> <span>+</span> <span>':'</span> <span>+</span> <span>time_</span><span>[</span><span>2</span><span>:</span><span>4</span><span>]</span> <span>+</span> <span>':'</span> <span>+</span> <span>time_</span><span>[</span><span>4</span><span>:</span><span>6</span><span>]</span> <span>+</span> <span>'Z'</span>

        <span>try</span><span>:</span>
            <span>lat</span><span>,</span> <span>lon</span> <span>=</span> <span>utm</span><span>.</span><span>to_latlon</span><span>(</span><span>float</span><span>(</span><span>rec</span><span>[</span><span>'region'</span><span>]),</span>
                                     <span>float</span><span>(</span><span>rec</span><span>[</span><span>'region2'</span><span>]),</span>
                                     <span>int</span><span>(</span><span>rec</span><span>[</span><span>'zone'</span><span>][</span><span>0</span><span>:</span><span>2</span><span>]),</span>
                                     <span>northern</span><span>=</span><span>rec</span><span>[</span><span>'zone'</span><span>][</span><span>2</span><span>]</span> <span>==</span> <span>'N'</span><span>)</span>
        <span># WIP:</span>
        <span># OutOfRangeError: northing out of range</span>
        <span># (must be between 0 m and 10,000,000 m)</span>
        <span>except</span> <span>Exception</span> <span>as</span> <span>exc</span><span>:</span>
            <span>continue</span>

        <span>rec</span><span>[</span><span>'lat'</span><span>],</span> <span>rec</span><span>[</span><span>'lon'</span><span>]</span> <span>=</span> <span>float</span><span>(</span><span>lat</span><span>),</span> <span>float</span><span>(</span><span>lon</span><span>)</span>

        <span>f</span><span>.</span><span>write</span><span>(</span><span>json</span><span>.</span><span>dumps</span><span>(</span><span>rec</span><span>,</span> <span>sort_keys</span><span>=</span><span>True</span><span>)</span> <span>+</span> <span>'</span><span>\n</span><span>'</span><span>)</span>
</pre></div>
<p>The resulting JSON produced by the above script is 3.3 GB uncompressed and contains  7,095,985 lines. Below, I'll import it into DuckDB.</p>
<div><pre><span></span>$<span> </span>~/duckdb<span> </span>~/satellogic.duckdb
</pre></div>
<div><pre><span></span><span>CREATE</span><span> </span><span>OR</span><span> </span><span>REPLACE</span><span> </span><span>TABLE</span><span> </span><span>s3</span><span> </span><span>AS</span>
<span>    </span><span>SELECT</span><span> </span><span>*</span><span> </span><span>EXCLUDE</span><span>(</span><span>lat</span><span>,</span><span> </span><span>lon</span><span>),</span>
<span>           </span><span>ST_POINT</span><span>(</span><span>lon</span><span>,</span><span> </span><span>lat</span><span>)</span><span> </span><span>AS</span><span> </span><span>geom</span>
<span>    </span><span>FROM</span><span>   </span><span>READ_JSON</span><span>(</span><span>'enriched.s3.json'</span><span>);</span>
</pre></div>
</div>
<div id="data-fluency">
<h2>Data Fluency</h2>
<p>The imagery in this feed was captured between July and December 2022. Below is the breakdown by month and satellite of the number of images captured. I've rounded the amounts to the nearest thousand.</p>
<div><pre><span></span><span>WITH</span><span> </span><span>a</span><span> </span><span>AS</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span>   </span><span>sat</span><span>,</span>
<span>             </span><span>STRFTIME</span><span>(</span><span>captured_at</span><span>,</span><span>  </span><span>'%m'</span><span>)</span><span> </span><span>month_</span><span>,</span>
<span>             </span><span>ROUND</span><span>(</span><span>COUNT</span><span>(</span><span>*</span><span>)</span><span> </span><span>/</span><span> </span><span>1000</span><span>)::</span><span>INT</span><span> </span><span>num_rec</span>
<span>    </span><span>FROM</span><span>     </span><span>s3</span>
<span>    </span><span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span>
<span>    </span><span>ORDER</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span>
<span>)</span>
<span>PIVOT</span><span>    </span><span>a</span>
<span>ON</span><span>       </span><span>month_</span>
<span>USING</span><span>    </span><span>SUM</span><span>(</span><span>num_rec</span><span>)</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>sat</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>sat</span><span>[</span><span>3</span><span>:]::</span><span>INT</span><span>;</span>
</pre></div>
<div><pre><span></span>┌─────────┬────────┬────────┬────────┬────────┬────────┬────────┐
│   sat   │   07   │   08   │   09   │   10   │   11   │   12   │
│ varchar │ int128 │ int128 │ int128 │ int128 │ int128 │ int128 │
├─────────┼────────┼────────┼────────┼────────┼────────┼────────┤
│ SN8     │      3 │        │        │        │        │        │
│ SN9     │     54 │     66 │     68 │     69 │    100 │     58 │
│ SN11    │     55 │     52 │     82 │     51 │     80 │     78 │
│ SN12    │        │        │      1 │        │        │        │
│ SN13    │     66 │     15 │        │        │     13 │     41 │
│ SN14    │        │        │      7 │      4 │        │      1 │
│ SN15    │     86 │     52 │     95 │     86 │     49 │        │
│ SN16    │     71 │     53 │     37 │     72 │     67 │     96 │
│ SN18    │     80 │     75 │     60 │     63 │    118 │    153 │
│ SN20    │     29 │     71 │     56 │     94 │    130 │     73 │
│ SN21    │     70 │     51 │     60 │    117 │    101 │     82 │
│ SN22    │     61 │     33 │     63 │     95 │    119 │    111 │
│ SN23    │     66 │     72 │     70 │     16 │        │        │
│ SN24    │     68 │     75 │     66 │     95 │     78 │    131 │
│ SN25    │     93 │     39 │     88 │      9 │        │        │
│ SN27    │     71 │     77 │    106 │     99 │     86 │    100 │
│ SN28    │        │     52 │     94 │    107 │    129 │    110 │
│ SN29    │     18 │     44 │     71 │    104 │     75 │    159 │
│ SN30    │     39 │     45 │     70 │     79 │     95 │     60 │
│ SN31    │     51 │    100 │     64 │     81 │    146 │    107 │
├─────────┴────────┴────────┴────────┴────────┴────────┴────────┤
│ 20 rows                                             7 columns │
└───────────────────────────────────────────────────────────────┘
</pre></div>
<p>I'll produce a heatmap of where the imagery was captured.</p>
<div><pre><span></span>$<span> </span>~/duckdb<span> </span>~/satellogic.duckdb
</pre></div>
<div><pre><span></span><span>COPY</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span>   </span><span>h3_cell_to_boundary_wkt</span><span>(</span>
<span>                  </span><span>h3_latlng_to_cell</span><span>(</span><span>ST_Y</span><span>(</span><span>geom</span><span>),</span>
<span>                                    </span><span>ST_X</span><span>(</span><span>geom</span><span>),</span>
<span>                                    </span><span>5</span><span>))::</span><span>GEOMETRY</span><span> </span><span>geom</span><span>,</span>
<span>             </span><span>COUNT</span><span>(</span><span>*</span><span>)</span><span> </span><span>num_recs</span>
<span>    </span><span>FROM</span><span>     </span><span>s3</span>
<span>    </span><span>WHERE</span><span>    </span><span>ST_X</span><span>(</span><span>geom</span><span>)</span><span> </span><span>BETWEEN</span><span> </span><span>-</span><span>175</span><span> </span><span>AND</span><span> </span><span>175</span>
<span>    </span><span>AND</span><span>      </span><span>ST_Y</span><span>(</span><span>geom</span><span>)</span><span> </span><span>BETWEEN</span><span> </span><span>-</span><span>90</span><span>  </span><span>AND</span><span>  </span><span>90</span>
<span>    </span><span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span>
<span>)</span><span> </span><span>TO</span><span> </span><span>'h3_5.gpkg'</span>
<span>    </span><span>WITH</span><span> </span><span>(</span><span>FORMAT</span><span> </span><span>GDAL</span><span>,</span>
<span>          </span><span>DRIVER</span><span> </span><span>'GPKG'</span><span>,</span>
<span>          </span><span>LAYER_CREATION_OPTIONS</span><span> </span><span>'WRITE_BBOX=YES'</span><span>);</span>
</pre></div>
<p>This is a global view of their imagery footprints.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_7L3wLAfGAc.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_7L3wLAfGAc.png"></a></p><p>Below is a zoomed-in view of North America.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_gfxsBb5stx.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_gfxsBb5stx.png"></a></p><p>Below is a zoomed-in view of Europe, North Africa and the Middle East. Hotspots in Albania, Saudi Arabia and between India and China really stand out.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_QDvTFxYjIN.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_QDvTFxYjIN.png"></a></p><p>The image below might be a bit easier to see on a mobile device.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_ISlxCDKTTQ.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_ISlxCDKTTQ.jpg"></a>
</p></div>
<div id="imagery-by-country">
<h2>Imagery by Country</h2>
<p>I'll use the Database of Global Administrative Areas (<a href="https://gadm.org/">GADM</a>) to get the number of images per country in this dataset.</p>
<div><pre><span></span>$<span> </span>wget<span> </span>-c<span> </span>https://geodata.ucdavis.edu/gadm/gadm4.1/gadm_410-gpkg.zip
$<span> </span>unzip<span> </span>gadm_410-gpkg.zip
</pre></div>
<div><pre><span></span>$<span> </span>~/duckdb<span> </span>~/satellogic.duckdb
</pre></div>
<div><pre><span></span><span>CREATE</span><span> </span><span>OR</span><span> </span><span>REPLACE</span><span> </span><span>TABLE</span><span> </span><span>gadm</span><span> </span><span>AS</span>
<span>    </span><span>FROM</span><span>   </span><span>ST_READ</span><span>(</span><span>'/mnt/f/gis/Global/gadm/gadm_410.gpkg'</span><span>);</span>
</pre></div>
<p>The heatmap GeoPackage (GPKG) file I produced earlier has a record count for each hexagon across the globe containing at least one image. It contains 6,122 records in total. I'll use the centroid of each hexagon to find its matching country.</p>
<div><pre><span></span><span>CREATE</span><span> </span><span>OR</span><span> </span><span>REPLACE</span><span> </span><span>TABLE</span><span> </span><span>countries</span><span> </span><span>AS</span>
<span>    </span><span>SELECT</span><span>   </span><span>b</span><span>.</span><span>COUNTRY</span><span> </span><span>AS</span><span> </span><span>country</span><span>,</span>
<span>             </span><span>SUM</span><span>(</span><span>a</span><span>.</span><span>num_recs</span><span>)</span><span> </span><span>AS</span><span> </span><span>num_recs</span>
<span>    </span><span>FROM</span><span>     </span><span>ST_READ</span><span>(</span><span>'h3_5.gpkg'</span><span>)</span><span> </span><span>a</span>
<span>    </span><span>JOIN</span><span>     </span><span>gadm</span><span> </span><span>b</span><span> </span><span>ON</span><span> </span><span>ST_CONTAINS</span><span>(</span><span>b</span><span>.</span><span>geom</span><span>,</span>
<span>                                   </span><span>ST_CENTROID</span><span>(</span><span>a</span><span>.</span><span>geom</span><span>))</span>
<span>    </span><span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span><span>;</span>
</pre></div>
<p>Not every centroid landed on a polygon for any of the countries listed in GADM. It could be that the missing ~1.5M image locations are just off of the coast or out in territorial waters.</p>
<div><pre><span></span><span>SELECT</span><span> </span><span>SUM</span><span>(</span><span>num_recs</span><span>)</span>
<span>FROM</span><span>   </span><span>countries</span><span>;</span>
</pre></div>

<p>Nonetheless, the following should be representative of how often any one country features in this dataset.</p>
<div><pre><span></span><span>FROM</span><span>     </span><span>countries</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>num_recs</span><span> </span><span>DESC</span><span>;</span>
</pre></div>
<div><pre><span></span>┌────────────────────────┬──────────┐
│        country         │ num_recs │
│        varchar         │  int128  │
├────────────────────────┼──────────┤
│ United States          │   886871 │
│ Saudi Arabia           │   428122 │
│ Australia              │   423079 │
│ Spain                  │   326465 │
│ China                  │   324572 │
│ México                 │   304916 │
│ Pakistan               │   293720 │
│ Russia                 │   250520 │
│ Ukraine                │   184756 │
│ Brazil                 │   176929 │
│ Argentina              │   156790 │
│ Albania                │   115472 │
│ United Arab Emirates   │   114530 │
│ Oman                   │   102500 │
│ Bangladesh             │   101888 │
│ Iran                   │    98160 │
│ India                  │    81026 │
│ Iraq                   │    72990 │
│ Canada                 │    65153 │
│ North Korea            │    64570 │
│   ·                    │       ·  │
│   ·                    │       ·  │
│   ·                    │       ·  │
│ Haiti                  │     1519 │
│ Denmark                │     1428 │
│ Moldova                │     1384 │
│ Romania                │     1291 │
│ Mozambique             │     1217 │
│ South Korea            │     1188 │
│ Colombia               │     1139 │
│ Northern Cyprus        │     1088 │
│ Slovakia               │     1075 │
│ Indonesia              │     1070 │
│ Dominican Republic     │      717 │
│ Trinidad and Tobago    │      592 │
│ Somalia                │      563 │
│ Switzerland            │      511 │
│ Serbia                 │      472 │
│ Guam                   │      431 │
│ Cuba                   │      370 │
│ Bosnia and Herzegovina │      289 │
│ Philippines            │      263 │
│ Barbados               │      244 │
├────────────────────────┴──────────┤
│ 127 rows (40 shown)     2 columns │
└───────────────────────────────────┘
</pre></div>
</div>

<div id="qatari-imagery">
<h2>Qatari Imagery</h2>
<p>I drew a bounding box around Qatar to see how many images in this dataset feature the country. Due to the country's shape and proximity to Bahrain and Saudi Arabia, a few of their images were also included in this count.</p>
<div><pre><span></span>$<span> </span>~/duckdb<span> </span>~/satellogic.duckdb
</pre></div>
<div><pre><span></span><span>SELECT</span><span> </span><span>COUNT</span><span>(</span><span>*</span><span>)</span>
<span>FROM</span><span>   </span><span>s3</span>
<span>WHERE</span><span>  </span><span>ST_X</span><span>(</span><span>geom</span><span>)</span><span> </span><span>BETWEEN</span><span> </span><span>50</span><span>.</span><span>6515</span><span> </span><span>AND</span><span> </span><span>51</span><span>.</span><span>8086</span>
<span>AND</span><span>    </span><span>ST_Y</span><span>(</span><span>geom</span><span>)</span><span> </span><span>BETWEEN</span><span> </span><span>24</span><span>.</span><span>3595</span><span> </span><span>AND</span><span> </span><span>26</span><span>.</span><span>3612</span><span>;</span>
</pre></div>
<div><pre><span></span>┌──────────────┐
│ count_star() │
│    int64     │
├──────────────┤
│        64375 │
└──────────────┘
</pre></div>
<p>I'll get the URLs of the metadata for those 64,375 images and download them using eight threads.</p>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"SELECT Key</span>
<span>        FROM   s3</span>
<span>        WHERE  ST_X(geom) BETWEEN 50.6515 AND 51.8086</span>
<span>        AND    ST_Y(geom) BETWEEN 24.3595 AND 26.3612"</span><span> </span><span>\</span>
<span>        </span><span>|</span><span> </span>~/duckdb<span> </span>-csv<span> </span>-noheader<span> </span>~/satellogic.duckdb<span> </span><span>\</span>
<span>        </span>&gt;<span> </span>manifest.txt
</pre></div>
<div><pre><span></span>$<span> </span>mkdir<span> </span>qatar
$<span> </span><span>cd</span><span> </span>qatar

$<span> </span>cat<span> </span>../manifest.txt<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>xargs<span> </span><span>\</span>
<span>        </span>-P8<span> </span><span>\</span>
<span>        </span>-I%<span> </span><span>\</span>
<span>        </span>wget<span> </span>-c<span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/%"</span>
</pre></div>
<p>The above downloaded 252 MB of JSON and took almost an hour to complete. I'll concatenate those files into a single line-delimited JSON file. I'll then use DuckDB to convert the JSON into a GPKG file with a few fields cleaned up for the sake of ergonomics.</p>
<div><pre><span></span>$<span> </span><span>cd</span><span> </span>../
$<span> </span>find<span> </span>qatar/<span> </span><span>\</span>
<span>    </span>-type<span> </span>f<span> </span><span>\</span>
<span>    </span>-exec<span> </span>cat<span> </span><span>{}</span><span> </span>+<span> </span><span>\</span>
<span>    </span>&gt;<span> </span>qatar.json
$<span> </span>~/duckdb
</pre></div>
<div><pre><span></span><span>COPY</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span> </span><span>properties</span><span>.</span><span>*</span><span> </span><span>EXCLUDE</span><span>(</span><span>providers</span><span>,</span>
<span>                                </span><span>"proj:shape"</span><span>,</span>
<span>                                </span><span>"proj:transform"</span><span>),</span>
<span>           </span><span>assets</span><span>.</span><span>preview</span><span>.</span><span>href</span><span>          </span><span>AS</span><span> </span><span>url_preview</span><span>,</span>
<span>           </span><span>assets</span><span>.</span><span>visual</span><span>.</span><span>href</span><span>           </span><span>AS</span><span> </span><span>url_visual</span><span>,</span>
<span>           </span><span>assets</span><span>.</span><span>analytic</span><span>.</span><span>href</span><span>         </span><span>AS</span><span> </span><span>url_analytic</span><span>,</span>
<span>           </span><span>assets</span><span>.</span><span>thumbnail</span><span>.</span><span>href</span><span>        </span><span>AS</span><span> </span><span>url_thumbnail</span><span>,</span>
<span>           </span><span>ST_GEOMFROMGEOJSON</span><span>(</span><span>geometry</span><span>)</span><span> </span><span>AS</span><span> </span><span>geom</span>
<span>    </span><span>FROM</span><span>   </span><span>'qatar.json'</span>
<span>)</span><span> </span><span>TO</span><span> </span><span>'qatar.footprints.gpkg'</span>
<span>    </span><span>WITH</span><span> </span><span>(</span><span>FORMAT</span><span> </span><span>GDAL</span><span>,</span>
<span>          </span><span>DRIVER</span><span> </span><span>'GPKG'</span><span>,</span>
<span>          </span><span>LAYER_CREATION_OPTIONS</span><span> </span><span>'WRITE_BBOX=YES'</span><span>);</span>
</pre></div>
<p>Below is an example record of the above GPKG file.</p>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"FROM  ST_READ('qatar.footprints.gpkg')</span>
<span>        LIMIT 1"</span><span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>~/duckdb<span> </span>-json<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>jq<span> </span>-S<span> </span>.
</pre></div>
<div><pre><span></span><span>[</span>
<span>  </span><span>{</span>
<span>    </span><span>"datetime"</span><span>:</span><span> </span><span>"2022-07-14T11:06:22.192899+00:00"</span><span>,</span>
<span>    </span><span>"geom"</span><span>:</span><span> </span><span>"POLYGON ((50.65626187904185 26.24828637962653, 50.656251675908884 26.25175366324302, 50.65240649343577 26.25174441056527, 50.652416810695826 26.24827712835427, 50.65626187904185 26.24828637962653))"</span><span>,</span>
<span>    </span><span>"grid:code"</span><span>:</span><span> </span><span>"39N-465288_2903610"</span><span>,</span>
<span>    </span><span>"gsd"</span><span>:</span><span> </span><span>1</span><span>,</span>
<span>    </span><span>"license"</span><span>:</span><span> </span><span>"CC-BY-4.0"</span><span>,</span>
<span>    </span><span>"platform"</span><span>:</span><span> </span><span>"newsat21"</span><span>,</span>
<span>    </span><span>"proj:epsg"</span><span>:</span><span> </span><span>"32639"</span><span>,</span>
<span>    </span><span>"satl:altitude"</span><span>:</span><span> </span><span>461.4741909855675</span><span>,</span>
<span>    </span><span>"satl:altitude_units"</span><span>:</span><span> </span><span>"km"</span><span>,</span>
<span>    </span><span>"satl:product_name"</span><span>:</span><span> </span><span>"L1"</span><span>,</span>
<span>    </span><span>"url_analytic"</span><span>:</span><span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/data/tif/zone=39N/region=465288_2903610/date=2022-07-14/20220714_110622_SN21_39N_465288_2903610_TOA.tif"</span><span>,</span>
<span>    </span><span>"url_preview"</span><span>:</span><span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/data/png/zone=39N/region=465288_2903610/date=2022-07-14/20220714_110622_SN21_39N_465288_2903610_preview.png"</span><span>,</span>
<span>    </span><span>"url_thumbnail"</span><span>:</span><span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/data/png/zone=39N/region=465288_2903610/date=2022-07-14/20220714_110622_SN21_39N_465288_2903610_thumbnail.png"</span><span>,</span>
<span>    </span><span>"url_visual"</span><span>:</span><span> </span><span>"https://satellogic-earthview.s3.us-west-2.amazonaws.com/data/tif/zone=39N/region=465288_2903610/date=2022-07-14/20220714_110622_SN21_39N_465288_2903610_VISUAL.tif"</span><span>,</span>
<span>    </span><span>"view:azimuth"</span><span>:</span><span> </span><span>97.95684658243894</span><span>,</span>
<span>    </span><span>"view:off_nadir"</span><span>:</span><span> </span><span>9.250787005630361</span><span>,</span>
<span>    </span><span>"view:sun_azimuth"</span><span>:</span><span> </span><span>269.67692021214305</span><span>,</span>
<span>    </span><span>"view:sun_elevation"</span><span>:</span><span> </span><span>57.10934720889825</span>
<span>  </span><span>}</span>
<span>]</span>
</pre></div>
<p>Below are the footprints of those 64,375 images. I've colour-coded them by the date of their capture. I've tinted the basemap blue to help the footprints stand out. The red bounding box was the search area.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_m7ijcDVW5m.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_m7ijcDVW5m.jpg"></a></p><p>When I zoom in on any one of the large stripes of footprints, the individual footprints of each image can be seen.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_7rISpM7bAw.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_7rISpM7bAw.jpg"></a></p><p>I'll pick a small part of Qatar along its coastline and download that area's images.</p>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"SELECT url_visual</span>
<span>        FROM   ST_READ('qatar.footprints.gpkg')</span>
<span>        WHERE  ST_X(ST_CENTROID(geom)) BETWEEN 50.811115 AND 50.829002</span>
<span>        AND    ST_Y(ST_CENTROID(geom)) BETWEEN 25.522661 AND 25.542736"</span><span> </span><span>\</span>
<span>        </span><span>|</span><span> </span>~/duckdb<span> </span>-csv<span> </span>-noheader<span> </span><span>\</span>
<span>        </span>&gt;<span> </span>url_visual.txt

$<span> </span>mkdir<span> </span>imagery
$<span> </span><span>cd</span><span> </span>imagery

$<span> </span>cat<span> </span>../url_visual.txt<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>xargs<span> </span><span>\</span>
<span>        </span>-P8<span> </span><span>\</span>
<span>        </span>-I%<span> </span><span>\</span>
<span>        </span>wget<span> </span>-c<span> </span><span>"%"</span>
</pre></div>
<p>The above downloaded 24 GeoTIFFs totalling 9.1 MB. Each image is 384x384-pixels.</p>
<p>Each image has geospatial information embedded within it, so I can group them together into a single image that will retain the spatial data. I'll re-compress the image using WebP, so the resulting image is much smaller than the original source imagery.</p>
<div><pre><span></span>$<span> </span>gdalbuildvrt<span> </span>mosaic.vrt<span> </span>*VISUAL.tif

$<span> </span>gdal_translate<span> </span><span>\</span>
<span>    </span>-co<span> </span><span>NUM_THREADS</span><span>=</span>ALL_CPUS<span> </span><span>\</span>
<span>    </span>-of<span> </span>COG<span> </span><span>\</span>
<span>    </span>-co<span> </span><span>COMPRESS</span><span>=</span>WEBP<span> </span><span>\</span>
<span>    </span>-co<span> </span><span>PREDICTOR</span><span>=</span><span>2</span><span> </span><span>\</span>
<span>    </span>mosaic.vrt<span> </span><span>\</span>
<span>    </span>mosaic.tif
</pre></div>
<p>The resulting mosaic GeoTIFF is 225 KB.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_ZBoY0S4gxb.jpg"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_ZBoY0S4gxb.jpg"></a></p><p>The 24 images look very different from one another and don't blend seamlessly. They share the same processing level and capture statistics, as far as I can tell.</p>
<div><pre><span></span><span>.</span><span>mode</span><span> </span><span>line</span>

<span>SELECT</span><span>   </span><span>datetime</span><span>,</span>
<span>         </span><span>platform</span><span>,</span>
<span>         </span><span>"satl:product_name"</span><span>,</span>
<span>         </span><span>"view:azimuth"</span><span>,</span>
<span>         </span><span>"view:off_nadir"</span><span>,</span>
<span>         </span><span>"view:sun_azimuth"</span><span>,</span>
<span>         </span><span>"view:sun_elevation"</span><span>,</span>
<span>         </span><span>COUNT</span><span>(</span><span>*</span><span>)</span>
<span>FROM</span><span>     </span><span>ST_READ</span><span>(</span><span>'qatar.footprints.gpkg'</span><span>)</span>
<span>WHERE</span><span>    </span><span>ST_X</span><span>(</span><span>ST_CENTROID</span><span>(</span><span>geom</span><span>))</span><span> </span><span>BETWEEN</span><span> </span><span>50</span><span>.</span><span>811115</span><span> </span><span>AND</span><span> </span><span>50</span><span>.</span><span>829002</span>
<span>AND</span><span>      </span><span>ST_Y</span><span>(</span><span>ST_CENTROID</span><span>(</span><span>geom</span><span>))</span><span> </span><span>BETWEEN</span><span> </span><span>25</span><span>.</span><span>522661</span><span> </span><span>AND</span><span> </span><span>25</span><span>.</span><span>542736</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>,</span><span> </span><span>5</span><span>,</span><span> </span><span>6</span><span>,</span><span> </span><span>7</span><span>;</span>
</pre></div>
<div><pre><span></span>          datetime = 2022-10-10T07:11:56.894566+00:00
          platform = newsat16
 satl:product_name = L1
      view:azimuth = 283.5615135582265
    view:off_nadir = 23.243003077262802
  view:sun_azimuth = 149.1609429350105
view:sun_elevation = 53.39441361193675
      count_star() = 24
</pre></div>
<p>For clarity, below are the grid codes for each image in the mosaic.</p>
<p><a href="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_QmnW2jl4ox.png"><img alt="Satellogic" src="https://tech.marksblogg.com/theme/images/satellogic/qgis-bin_QmnW2jl4ox.png"></a></p><p>The following are types of imagery deliverables that Satellogic offers:</p>
<ul>
<li>L0: Raw</li>
<li>L1A: Raw corrected</li>
<li>L1B: L1 Basic (Cloud, shadow and usable data mask)</li>
<li>L1C: Ortho ready</li>
<li>L1D/L1D_SR: Ortho</li>
<li>L1: TOA Reflectance</li>
<li>L1_SR: TOA Reflectance SuperResolution</li>
</ul>
<p>The product listed in the metadata for these images was L1 which I'm assuming means "Radiometric-corrected imagery (TOA reflectance imagery)". Their product documentation says the imagery is corrected for sensor, optical and terrain distortions (orthorectified).</p>
<p>My best guess is that the images have been processed independently of one another. Basemap producers often strive to blend images next to one another seamlessly but that might not be a requirement for this imagery's original deliverable.</p>
</div>

        </div><p>
            Thank you for taking the time to read this post. I offer both consulting and hands-on development services to clients in North America and Europe. If you'd like to discuss how my offerings can help your business please contact me via <a href="https://uk.linkedin.com/in/marklitwintschik/">LinkedIn</a>.
        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The owner of ip4.me/ip6.me, Kevin Loch, passed away (276 pts)]]></title>
            <link>https://ip4only.me/</link>
            <guid>43256298</guid>
            <pubDate>Tue, 04 Mar 2025 15:54:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ip4only.me/">https://ip4only.me/</a>, See on <a href="https://news.ycombinator.com/item?id=43256298">Hacker News</a></p>
<div id="readability-page-1" class="page">
<h2>!! NOTICE !!</h2>
<p>
Notice: The owner of ip4.me/ip6.me, Kevin Loch, passed away.<br>
The Kevin M Loch Estate will be shutting down Kevin's websites in the near future (4/1/2025).<br>
Inquiries for purchasing Kevin's domains may be sent to ipadmin@dulles-ix.net.
</p>
<p>
Click this link to continue to your ip4/ip6 address reporting <a href="https://ip4only.me/home.cgi">Website</a>
</p>
<p>
List of Websites impacted:
ip4.me, ip4only.me<br>
ip6addr.com, ip6addr.net, ip6addr.org<br>
ip6.me, ip6only.me<br>
ipv6addr.com, ipv6addr.net, ipv6addr.org<br>
onlyip4.me, onlyip6.me<br>
whatismyipv6address.com, whatismyipv6address.net, whatismyipv6address.org<br>
whatismyv6.com, whatismyv6.net, whatismyv6.org<br>
</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Should managers still code? (183 pts)]]></title>
            <link>https://theengineeringmanager.substack.com/p/should-managers-still-code</link>
            <guid>43256113</guid>
            <pubDate>Tue, 04 Mar 2025 15:41:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theengineeringmanager.substack.com/p/should-managers-still-code">https://theengineeringmanager.substack.com/p/should-managers-still-code</a>, See on <a href="https://news.ycombinator.com/item?id=43256113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>This month we have a mailbag question from a reader who asks:</p><blockquote><p>Hi James,</p><p><span>Your newsletter </span><a href="https://www.theengineeringmanager.com/managing-managers/being-in-the-details/" rel="">"being in the details"</a><span> resonated with me.</span></p><p>I would like to ask your opinion about managers writing code. I skim PRs but don’t critically review them and don’t write code. Should EMs write code in their day job?</p><p>Thanks!</p></blockquote><p>It's a great question, and given the higher scrutiny we've seen on the role of engineering managers in the past few years, it's worth spending some time on it.</p><p><span>Before we go any deeper, the short answer is that it depends exactly on what you mean by coding. I think that there is a big difference between being </span><em>in</em><span> the code and </span><em>writing</em><span> code. All managers should be in the code, but not all managers should be writing code.</span></p><p><span>But spending some time digging into the nuances of the question can, I think, highlight both the practical aspects of ensuring that managers are sufficiently </span><em>in</em><span> the code, and, more importantly, identify the existential worry that many engineering managers may have about their role, and what they can do about it.</span></p><p>Let's get going.</p><p><span>We'll begin by revisiting Andy Grove's equation for measuring a manager's impact, which states that </span><em>the output of a manager is the output of their team, plus the output of the neighboring teams under their influence</em><span>. This is always useful to refer to when thinking about how to spend your time. I contemplate it a lot.</span></p><p>There are, of course, a multitude of ways to increase your output as a manager. These range from, and are not limited to:</p><ul><li><p>Hiring and retaining great people.</p></li><li><p>Owning the team's strategy and roadmap, and ensuring efficient execution.</p></li><li><p>Making decisions to ensure that the team is working on the right things and saying no to the things that don't matter.</p></li><li><p>Dealing with fires, escalations, and other crises that pop up all of the time.</p></li><li><p>Building a strong culture within the team so that people are engaged, challenged, and motivated.</p></li><li><p>Mentoring and coaching your reports so they get better and can have more work delegated to them, thus increasing output further.</p></li><li><p>Managing the team's stakeholders so they can offer their steer to the team early and often.</p></li><li><p>Actively performance managing the team so that superstars can continue to shine and underperformers can be coached or exited.</p></li><li><p>Building close working relationships with other teams so that smooth collaboration happens across the organization, leading to a better and more cohesive product.</p></li></ul><p>And so on.</p><p><span>Now, it isn't a stretch to say that engineering managers, who typically get to a senior individual contributor level before transitioning to management, are </span><em>also</em><span> often very good at writing code.</span></p><p><span>However, there's a whole </span><em>team</em><span> of people working for them who are </span><em>also</em><span> very good at writing code, and typically they won't be spending as much time on the items listed above, which means they can be more productive at writing code than the manager can.</span></p><p><span>So, therefore, surely it makes sense for managers to focus on the things that </span><em>only they can do</em><span>, and leave the coding to the people who have more time for it and are better at it as a result?</span></p><p>Right?</p><p>Or is there something else at play here?</p><p>The last few years have created somewhat of an existential crisis for engineering managers.</p><p><span>Since the tail end of the Covid-19 pandemic, where the </span><a href="https://www.theengineeringmanager.com/growth/2024-year-in-review/" rel="">zero-percent interest rate environment ended</a><span>, leading to high inflation and interest rates, the end of cheap debt and plentiful investment, and the consequential slowdown in growth and corrective layoffs at many technology companies, managers have </span><a href="https://www.businessinsider.com/middle-manager-hiring-white-collar-recession-layoffs-jobs-efficiency-2024-12" rel="">felt the brunt of the "great flattening"</a><span>, where the number of managers has been reduced in favor of more individual contributors.</span></p><p>This has typically meant that the average number of direct reports per manager has increased, the amount of scope per manager has increased too, and the number of total layers in the org chart has shrunk.</p><p>This, therefore, feeds into a narrative that managers worth their salt need to be more productive, more impactful, and more efficient than before.</p><p>Depending on where you read your opinions on the internet, It can also build a more toxic picture: that managers are just unnecessary overhead for having some number of individual contributors; that it is a non-technical job for those that aren't as good as others at actually building the product.</p><p>Hmm.</p><p><span>For those that made a conscious decision to move into management and learn this new role, and especially for those who take their craft </span><em>seriously</em><span> as managers, it can feel like the tide has turned against them.</span></p><p>As such, engineering managers, and the organizations that they work for, are pushing for ways for managers to differentiate themselves, typically by being more technical, more hands-on, and more in the details than they may have been before.</p><p><span>This isn't a bad thing. When done right, it can be extremely beneficial. I wrote extensively about </span><a href="https://www.theengineeringmanager.com/managing-managers/being-in-the-details/" rel="">being in the details</a><span> from a senior leadership perspective.</span></p><p><span>I would argue that being in the details is the key tenet of not just being a great manager in the climate that we find ourselves in, but also being a great manager </span><em>full stop</em><span>.</span></p><p>For senior leaders such as Directors and VPs, being in the details covers ideas such as having ICs report to you, as well as managers; doing regular, hands-on deep dives into the architecture and codebases your team owns; mixing up 1:1s with pair programming sessions, code reviews, and other technical activities, and more.</p><p>But what does this mean for frontline engineering managers? Is the new normal just about writing more code and doing less of the other things that peacetime managers would normally do?</p><p>It's more nuanced than that.</p><p>Here is a list of statements that represent how I would want my engineering managers to be in terms of their relationship with the codebase:</p><ul><li><p><span>Should they be </span><em>able</em><span> to write code? Yes.</span></p></li><li><p>Should they understand how the codebase and their features and services are built? Yes.</p></li><li><p>Should they be able to do code reviews? Yes.</p></li><li><p>Should they review all design documents and architecture proposals from their team? Yes.</p></li><li><p>Should they be able to debug and triage production issues? Yes.</p></li><li><p>Should they be able to pair program with their reports? Yes.</p></li><li><p>Should they be accountable for the quality of the code that their team produces? Yes.</p></li><li><p><span>Should they write code themselves? </span><em>Maybe.</em></p></li></ul><p>Why maybe?</p><p><span>It depends on the manager, the team, and the organization. As a senior leader, I would rather my managers be </span><em>in</em><span> the code as per the above list, but not necessarily putting themselves in the critical path by </span><em>writing</em><span> code, given that they are likely to be interrupted more often, have more meetings, and be pulled in more directions than their reports.</span></p><p>I expect you to know how everything works, insofar that if I asked you to show me how a feature works by tracing through the code, you could do it. However, I would also know that there are other people on the team that are better placed to be the primary implementers of features in the area that you understand really well.</p><p><span>But, if you are a manager that is absolutely </span><em>itching</em><span> to write code and stay close to the details, then so be it and more power to you.</span></p><p>Here's some approaches that might work for you:</p><ul><li><p><strong>Explicitly set aside uninterrupted time for coding.</strong><span> This could be a day a week, or a few hours a day, or whatever works for your team. Make sure that your team knows that you are doing this, to keep interruptions to a minimum. Block it out in your calendar, and set your statuses accordingly.</span></p></li><li><p><strong>Pair program with your reports.</strong><span> This is a great way to get into the code, and also to mentor your reports at the same time. It's a win-win. Sure, you're not taking the lead on the coding yourself, but you're in the details by proxy of collaborating with others. I love doing this myself.</span></p></li><li><p><strong>Do code reviews.</strong><span> Don't just skim PRs (sorry, reader!), but really dig into them: run the branch locally, test it, think critically about the design and the implementation, and provide feedback. Record a video of your review to highlight things that could be better.</span></p></li><li><p><strong>Increase your coding involvement during specific occasions.</strong><span> Depending on how you and your team work, you may find that prototyping phases are where you can get your hands dirty effectively. That code isn't going into production, so go to town! Alternatively, you might be great at digging in during incidents or times of high stress. Find the times that work for you and your team, and lean into them. This approach isn't about spending a consistent amount of time coding every week; it's about finding the </span><em>right</em><span> times to be in the code where you can be most effective.</span></p></li><li><p><strong>Find time for exploratory coding that expands the knowledge of your team.</strong><span> Block out some regular time to stay on top of the latest technologies in a way that is fun and engaging for you, sharpens your skills, but also brings back learnings to your team. For example, if your team uses Apache Flink and the latest version has new aggregation functionality that could make your pipelines simpler, spend a couple of hours doing a prototype with it for a team-specific use case and then share it back.</span></p></li></ul><p>Going back to the original question: should managers write code? I think it's best to rephrase that.</p><p><span>Should managers be </span><em>in</em><span> the code? Yes, absolutely.</span></p><p><span>Should managers </span><em>write</em><span> code? Maybe, but it also depends on what you mean by writing code.</span></p><p>If you mean being the primary implementer of features, then probably not. If you mean being an integral part of how your team produces code, then yes, absolutely. I recommend it highly.</p><p>So there we are.</p><p>I enjoyed doing this month's article as a mailbag question. If you ever have a question that you think would make a good article, please send it in! I'd love to hear from you, and I'll try my best to answer it.</p><p>Happy coding (in some form), managers.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bayleaf · Building a low-profile wireless split keyboard (576 pts)]]></title>
            <link>https://www.graz.io/articles/bayleaf-wireless-keyboard</link>
            <guid>43255529</guid>
            <pubDate>Tue, 04 Mar 2025 15:00:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.graz.io/articles/bayleaf-wireless-keyboard">https://www.graz.io/articles/bayleaf-wireless-keyboard</a>, See on <a href="https://news.ycombinator.com/item?id=43255529">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[DIY "infinity contrast" TV – with 100% recycled parts [video] (111 pts)]]></title>
            <link>https://www.youtube.com/watch?v=qXrn4MqY1Wo</link>
            <guid>43255446</guid>
            <pubDate>Tue, 04 Mar 2025 14:55:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=qXrn4MqY1Wo">https://www.youtube.com/watch?v=qXrn4MqY1Wo</a>, See on <a href="https://news.ycombinator.com/item?id=43255446">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[What a crab sees before it gets eaten by a cuttlefish (157 pts)]]></title>
            <link>https://www.nytimes.com/2025/03/03/science/cuttlefish-camouflage-huting-crabs.html</link>
            <guid>43254995</guid>
            <pubDate>Tue, 04 Mar 2025 14:24:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/03/03/science/cuttlefish-camouflage-huting-crabs.html">https://www.nytimes.com/2025/03/03/science/cuttlefish-camouflage-huting-crabs.html</a>, See on <a href="https://news.ycombinator.com/item?id=43254995">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/03/03/science/cuttlefish-camouflage-huting-crabs.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla electric car sales plunge again in Australia – Model 3 down more than 81 p (102 pts)]]></title>
            <link>https://thedriven.io/2025/03/04/tesla-electric-car-sales-plunge-again-in-australia-model-3-down-more-than-81-per-cent/</link>
            <guid>43254637</guid>
            <pubDate>Tue, 04 Mar 2025 13:57:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thedriven.io/2025/03/04/tesla-electric-car-sales-plunge-again-in-australia-model-3-down-more-than-81-per-cent/">https://thedriven.io/2025/03/04/tesla-electric-car-sales-plunge-again-in-australia-model-3-down-more-than-81-per-cent/</a>, See on <a href="https://news.ycombinator.com/item?id=43254637">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

										
			
							<section>

								<p>The plunge in Tesla electric vehicle sales has continued into February, according to the latest official data, with combined sales of the Model Y and Model 3 EVs plunging 71.9 per cent in the month of February, compared to the same month a year earlier.</p>
<p>The data from the Electric Vehicle Council shows that Tesla recorded just 1,592 EV sales in February, down from &nbsp;5,665 for &nbsp;February last year. For the first two months of the year, sales have slumped 66 per cent to 2,331 from 6,772 in 2024.</p>
<p>Tesla supporters insist the sales plunge – which <a href="https://thedriven.io/2025/03/04/tesla-sales-continue-to-plummet-across-europe-despite-overall-ev-market-growth/">is also intense in European countries</a> – is only the result of inventory levels and customers waiting for refreshed Model Y, and some increased competition.</p>
<p>But most analysts and observers also point to the influence that CEO Elon Musk is having on the market because of his partnership with US president Donald Trump and his open support for far right political causes.</p>
<p>In Australia, sales of the Model Y fell to 924 in February from 2,072 in February last year. If that were to be the result only of inventory issues and customers waiting for the refreshed Model Y, it does not explain the 81.4 per cent fall in Model 3 sales to just 668 units in February, from 3,593 in the same month of 2024, and 2,671 in February, 2023.</p>
<p>It could also be that the Model 3 is simply past its use-by date, particularly with the arrival of competition from China – such as the BYD Seal.</p>

<figure id="attachment_185703" aria-describedby="caption-attachment-185703"><a href="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales.jpg?lossy=1&amp;strip=0&amp;webp=1"><img loading="lazy" decoding="async" src="https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1160x375.jpg" alt="" width="1160" height="375" srcset="https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1160x375.jpg 1160w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-388x126.jpg 388w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-800x259.jpg 800w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1536x497.jpg 1536w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-120x39.jpg 120w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-90x29.jpg 90w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-320x104.jpg 320w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-560x181.jpg 560w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-150x49.jpg 150w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-240x78.jpg 240w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-180x58.jpg 180w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-640x207.jpg 640w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1120x362.jpg 1120w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-1600x518.jpg 1600w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales-300x97.jpg 300w, https://thedriven.io/wp-content/uploads/2025/03/EVC-sales.jpg 1854w" sizes="(max-width: 1160px) 100vw, 1160px" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1160x375.jpg?lossy=1&amp;strip=0&amp;webp=1" data-srcset="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1160x375.jpg?lossy=1&amp;strip=0&amp;webp=1 1160w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-388x126.jpg?lossy=1&amp;strip=0&amp;webp=1 388w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-800x259.jpg?lossy=1&amp;strip=0&amp;webp=1 800w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1536x497.jpg?lossy=1&amp;strip=0&amp;webp=1 1536w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-120x39.jpg?lossy=1&amp;strip=0&amp;webp=1 120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-90x29.jpg?lossy=1&amp;strip=0&amp;webp=1 90w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-320x104.jpg?lossy=1&amp;strip=0&amp;webp=1 320w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-560x181.jpg?lossy=1&amp;strip=0&amp;webp=1 560w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-150x49.jpg?lossy=1&amp;strip=0&amp;webp=1 150w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-240x78.jpg?lossy=1&amp;strip=0&amp;webp=1 240w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-180x58.jpg?lossy=1&amp;strip=0&amp;webp=1 180w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-640x207.jpg?lossy=1&amp;strip=0&amp;webp=1 640w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1120x362.jpg?lossy=1&amp;strip=0&amp;webp=1 1120w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-1600x518.jpg?lossy=1&amp;strip=0&amp;webp=1 1600w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales-300x97.jpg?lossy=1&amp;strip=0&amp;webp=1 300w, https://b2232832.smushcdn.com/2232832/wp-content/uploads/2025/03/EVC-sales.jpg?lossy=1&amp;strip=0&amp;webp=1 1854w"></a><figcaption id="caption-attachment-185703">Source: Electric Vehicle Council.</figcaption></figure>
<p>Polestar is also seeing sales of its Polestar 2 electric fasts-back – once seen as a rival to the M3 – slump sharply, down to just 36 from 112 a year earlier.</p>
<p>But Polestar is also rolling out a number of new models, and the EVC data reveals that its overall sales are up 11.6 per cent over the same month a year earlier. This has been driven by the newly launched Polestar 4 has achieved 83 sales, taking its year to date total to 144, while the more expensive Polestar 3 achieved just 6 sales in February, taking its year to date sales to 8.</p>
<p>The health of the overall EV market in Australia will be revealed in more detail on Wednesday with the release of data from other car makers via the main car lobby, the FCAI. Tesla and Polestar left the FCAI last year because of differences about policies, including the federal government’s new vehicle emissions standards.</p>
<p>Tesla, however, has an outsize influence on the Australian market, having accounted for well over half of all EV sales in recent years, although the emergence of lower cost Chinese EVs ate away some of that market dominance in 2024.</p>
<p>In Europe, where the EV market is resurgent – Tesla EVs account for between 5 and 15 per cent of individual country markets. Its sales, however,<a href="https://thedriven.io/2025/03/04/tesla-sales-continue-to-plummet-across-europe-despite-overall-ev-market-growth/"> are also plunging badly</a>, amid community push back against his support of the far right AfD party in Germany, and his intervention on behalf of far right politicians and individuals in the UK and elsewhere.</p>
<p>In the US, demonstrations were held at more than 50 Tesla dealerships over the weekend, according to CNN, while in Canada more than 300,000 people have signed a petition calling on the government to rescind Musk’s citizenship, offended by his statements that include a declaration that Canada is “not a real country.”</p>
<p>Tesla stock has also fallen sharply, reversing almost all of the gains it made in the post Trump election victory euphoria. That are now growing calls for the Tesla board to consider replacing Musk, although there is no sign they intend to do that and are more interested in selling their shares.</p>
<p>According to Electrek, chairperson Robyn Denholm, an Australia, has sold more Tesla stock, <a href="https://electrek.co/2025/03/03/tesla-chairwoman-sells-33-million-worth-of-tsla-as-she-lets-elon-musk-destroy-the-brand/">taking her total sales over the last</a> three months (since the Trump election) to more than $US100 million.</p>
<p>Kimball Musk, Elon’s brother, and Tesla’s Chief Financial Officer Taneja Vaibhav also recently sold ahead of a recent drop in the company’s stock price. See: <a href="https://electrek.co/2025/03/03/tesla-chairwoman-sells-33-million-worth-of-tsla-as-she-lets-elon-musk-destroy-the-brand/">Tesla chairwoman sells $33 million worth of Tesla stock as she lets Elon Musk destroy the brand</a>.</p>

<p>The FCAI data is due to be released on Wednesday.</p>
<p>See also:&nbsp;<a href="https://thedriven.io/2025/03/04/tesla-sales-continue-to-plummet-across-europe-despite-overall-ev-market-growth/">Tesla sales continue to plummet across Europe despite overall EV market growth</a></p>
<p>And:&nbsp;<a href="https://thedriven.io/2025/03/04/australian-electric-vehicle-sales-by-month-and-by-model-in-2025/">Australian electric vehicle sales by month and by model in 2025</a></p>
<div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img loading="lazy" decoding="async" src="https://thedriven.io/wp-content/uploads/2021/11/giles_100x100.jpg" width="100" height="100" alt="giles parkinson" itemprop="image" data-old-src="https://thedriven.io/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://b2232832.smushcdn.com/2232832/wp-content/uploads/2021/11/giles_100x100.jpg?lossy=1&amp;strip=0&amp;webp=1"></p><div><p>Giles Parkinson is founder and editor of <a href="https://thedriven.io/">The Driven</a>, and also edits and founded the <a href="https://reneweconomy.com.au/">Renew Economy</a> and <a href="http://onestepoffthegrid.com.au/">One Step Off The Grid</a>&nbsp;web sites. He has been a journalist for nearly 40 years, is a former business and deputy editor of the Australian Financial Review, and owns a Tesla Model 3.</p></div></div><!-- AI CONTENT END 1 -->

								
							</section>

										
			
						</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Fork of Claude-code working with local and other LLM providers (120 pts)]]></title>
            <link>https://github.com/dnakov/anon-kode</link>
            <guid>43254351</guid>
            <pubDate>Tue, 04 Mar 2025 13:35:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dnakov/anon-kode">https://github.com/dnakov/anon-kode</a>, See on <a href="https://news.ycombinator.com/item?id=43254351">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>

      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p>

<react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false" data-attempted-ssr="false">
  
  
  
</react-partial>




      

          

              


<header role="banner" data-is-top="true" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code Review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>Code Search</p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>

                  <li>
      
      
</li>

                    <li>
      
      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      <div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:dnakov/anon-kode" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="dhVu3lFe87tcSfb0aFuQZy5s6OlGMzM36mCAR1Ha-5PW5WxTpmescQov-E7t1gv8ZvslpEJf-rZrk7DwfkfWvw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="dnakov/anon-kode" data-current-org="" data-current-owner="dnakov" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=dnakov%2Fanon-kode" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/dnakov/anon-kode&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="4dbd6159b652241c5c897196051dc18c8414e63dd8f77e4148009093278bef06" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div>
</header>

      
    </div>

  








    


    






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-project-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div data-view-component="true" id="repo-content-pjax-container">      











<react-partial partial-name="repos-overview" data-ssr="true" data-attempted-ssr="true">
  
  
  <div data-hpc="true" data-target="react-partial.reactRoot"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ANON KODE</h2><a id="user-content-anon-kode" aria-label="Permalink: ANON KODE" href="#anon-kode"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description anon-kode.mov">anon-kode.mov</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/3777433/419028277-7a9253a7-8bb0-40d5-a3f3-5e6096d7c789.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDExMzg1MDgsIm5iZiI6MTc0MTEzODIwOCwicGF0aCI6Ii8zNzc3NDMzLzQxOTAyODI3Ny03YTkyNTNhNy04YmIwLTQwZDUtYTNmMy01ZTYwOTZkN2M3ODkubW92P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDMwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAzMDVUMDEzMDA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWJjMjZjNjE2MzZhMTdjYTRmMTdiZTczY2NkMDM4M2NkNzc0ZjNlNzZmMTlhMzUwY2Y0ZGM1ZWZiZTE5ODk3MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.akFo9ApY6zqPoggOt2O4iAJOCyZQr35f4DUDyt3PHjg" data-canonical-src="https://private-user-images.githubusercontent.com/3777433/419028277-7a9253a7-8bb0-40d5-a3f3-5e6096d7c789.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDExMzg1MDgsIm5iZiI6MTc0MTEzODIwOCwicGF0aCI6Ii8zNzc3NDMzLzQxOTAyODI3Ny03YTkyNTNhNy04YmIwLTQwZDUtYTNmMy01ZTYwOTZkN2M3ODkubW92P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDMwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTAzMDVUMDEzMDA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWJjMjZjNjE2MzZhMTdjYTRmMTdiZTczY2NkMDM4M2NkNzc0ZjNlNzZmMTlhMzUwY2Y0ZGM1ZWZiZTE5ODk3MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.akFo9ApY6zqPoggOt2O4iAJOCyZQr35f4DUDyt3PHjg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Terminal-based AI coding tool that can use any model that supports the OpenAI-style API.</p>
<ul dir="auto">
<li>Fixes your spaghetti code</li>
<li>Explains wtf that function does</li>
<li>Runs tests, shell commands and stuff</li>
<li>Whatever else claude-code can do, depending on the model you use</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">HOW TO USE</h2><a id="user-content-how-to-use" aria-label="Permalink: HOW TO USE" href="#how-to-use"></a></p>
<div data-snippet-clipboard-copy-content="npm install -g anon-kode
cd your-project
kode"><pre><code>npm install -g anon-kode
cd your-project
kode
</code></pre></div>
<p dir="auto">You can use the onboarding to set up the model, or <code>/model</code>.
If you don't see the models you want on the list, you can manually set them in <code>/config</code>
As long as you have an openai-like endpoint, it should work.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Warning</h2><a id="user-content-warning" aria-label="Permalink: Warning" href="#warning"></a></p>
<p dir="auto">Use at own risk.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">YOUR DATA</h2><a id="user-content-your-data" aria-label="Permalink: YOUR DATA" href="#your-data"></a></p>
<ul dir="auto">
<li>There's no telemetry or backend servers other than the AI providers you choose</li>
</ul>
</article></div>
</react-partial>

      </div>

</turbo-frame>


    </main>
  </div>

          



    <ghcc-consent id="ghcc" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>



  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Federal workers ordered to return to offices without desks, Wi-Fi and lights (279 pts)]]></title>
            <link>https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html</link>
            <guid>43253562</guid>
            <pubDate>Tue, 04 Mar 2025 12:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html">https://www.cnn.com/2025/03/04/politics/federal-employees-return-to-office-problems/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43253562">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                <p><cite>
      <span data-editable="location"></span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thliwo000v2cozgh2xbdev@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Millions of federal workers were <a href="https://www.cnn.com/2025/01/23/business/trump-federal-workers-rto-mandate/index.html">ordered to return to offices</a> across the country in recent weeks, marking an end to Covid-era rules allowing more flexibility to work from home.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thlk6200003b6m4ysl92ra@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Many have come back to workplaces that weren’t ready for them.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thv5ao00083b6mw2qej22u@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In one Department of Health and Human Services office, there was no Wi-Fi or full electricity in the first hours when people returned last week.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok3000e3b6mmdfnpnek@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Department of Education employees at an office in Dallas returned to ethernet cords in piles around the floor, random wires sticking out of walls, and motion-sensor lights that weren’t working correctly, leading to dark workspaces. One employee tripped over a pile of cords on her first day back, resulting in a large gash on her foot. She’s submitted a workers’ compensation complaint.
    </p>

  
<div data-image-variation="image_inline-small" data-breakpoints="{&quot;image_inline-small--eq-extra-small&quot;: 115, &quot;image_inline-small--eq-small&quot;: 300, &quot;image_inline-small--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti30t500013b6mvbtdzglx@published" data-name="c-IMG_2545.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.666875" data-original-height="1067" data-original-width="1600" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?c=original" data-editable="settings">
       <picture><source height="1067" width="1600" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill/f_webp" type="image/webp"><source height="1067" width="1600" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-img-2545.jpg?q=w_256,c_fill" alt="Ethernet cords lay on the floor at a Department of Education office in Dallas. " onload="this.classList.remove('image_inline-small__dam-img--loading')" onerror="imageLoadError(this)" height="1067" width="1600" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000f3b6mlbe4g8bi@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            And a Department of Defense employee who returned to in-office work and handles sensitive information was stuck in a conference room with people on different teams, forcing them to leave the room to make calls. The employee was eventually moved to an office — but one without Wi-Fi, so they had to use their phone’s spotty hot spot.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000g3b6mvpdz4pmj@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The only thing a return to the office has given me is an hour of traffic while driving and a loss in efficiency,” said the worker, who requested anonymity for fear of job reprisals.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000h3b6my9a5k5lc@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The problems, confusion and slipups that federal employees told CNN they’ve encountered returning to the office have only added to the chaos inside the workforce six weeks into a Trump administration determined to <a href="https://www.cnn.com/politics/tracking-federal-workforce-firings-dg/index.html">slash the size and scope</a> of the federal government.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000i3b6m1ztsyzip@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Some federal workers being told to return to the office have no space to return to. At least two office buildings used by the Interior Department in the Western US were told last week their leases had been canceled, according to a source familiar with the matter — while a third office housing hundreds of people was notified its lease will be up in June.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000j3b6maute1e0f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            One source said the General Services Administration, which manages federal buildings, did not appear to coordinate the lease terminations with Interior officials, leaving employees unclear of what to do. An Interior spokesperson said the department was “working with GSA to ensure facilities or alternative options will be available” for employees.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000k3b6myjlfpb4f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            President <a href="https://www.cnn.com/politics/president-donald-trump-47">Donald Trump</a> has repeatedly demanded all federal workers return to the office as his administration has undertaken sweeping actions seeking to drastically reduce the size of the federal workforce, including mass firings of probationary workers and employees in government diversity departments.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000l3b6mpk1596b3@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump’s in-office mandate has been coupled with a push to slash government real estate, setting up a dilemma of too little space for too many people. Even before Trump took office in January, the federal government was downsizing office space due in part to the shift toward telework during the pandemic.
    </p>

  





    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000m3b6m9c6j2qe2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Nearly half of the more than 2.3 million civilian federal workers were eligible for telework, and 10% were in remote positions with no expectation of in-person work, according to a 2024 Office of Management and Budget report.<strong> </strong>Many workers stopped working full time in their offices during the Covid-19 pandemic, but others had long-held arrangements for working from home<strong> </strong>— and now could face the prospect of choosing between fundamentally changing their job or leaving it altogether.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000n3b6m0hjimos2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            More than 80% of federal workers reside outside the Washington, DC, metro area, meaning the return-to-office mandate will cause ripple effects across the country.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000o3b6m93ver5jt@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Some government employees are now making the post-pandemic transition that millions of private-sector workers already have — upending their lives and schedules in the process. For many federal employees, however, those worries have been subsumed by their larger fear of losing jobs entirely in Trump’s multipronged effort to shrink the federal workforce.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000p3b6m4evksl91@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Federal workers and union officials told CNN they see this as part of an attempt by the Trump administration to <a href="https://www.propublica.org/article/video-donald-trump-russ-vought-center-renewing-america-maga" target="_blank">make life uncomfortable</a> for federal workers in hopes that some will quit.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000q3b6mquhadaqy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump and Elon Musk, de facto chief of the Department of Government Efficiency, have both threatened to fire workers who do not come back to the office, even those represented by unions who have signed long-term telework agreements. Many federal agencies set February 24 as the first date for some of their employees to comply with Trump’s return-to-office directive.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000r3b6mvsxcej23@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “If they don’t report for work, we’re firing them. In other words, you have to go to office,” Trump said at a conservative political conference last month, while claiming that his <a href="https://www.cnn.com/2025/02/18/politics/mar-a-lago-trump-remote-work-golf/index.html">golf game would improve dramatically</a> if he were to work remotely.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000s3b6mjad0bg9i@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Federal employee unions have argued Trump’s demand to break long-term telework agreements <a href="https://www.cnn.com/2025/02/07/politics/trump-musk-federal-workforce/index.html">is unlawful</a>, though the return-to-office mandate isn’t central to the major legal challenges unions have brought against the president. But there have been pockets of pushback, such as the employee union at the Environmental Protection Agency, which is demanding to bargain around the return-to-office mandate. Additionally, some individual EPA employees represented by the union are filing grievances around returning to the office.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000t3b6moq891utg@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            One of their biggest concerns, an EPA union official told CNN, is “local fire and safety regulations — how many people can you jam into a room?”
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000v3b6m57ha4i02@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            DOGE has kept a running list on its website touting more than 200 building leases the Musk-run agency says it’s canceled. The canceled leases, which include Social Security Administration and US attorneys’ offices, have <a href="https://www.kristv.com/news/local-news/d-o-g-e-terminates-lease-for-local-u-s-attorneys-office" target="_blank">raised questions locally</a> about where those employees are supposed to go.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000w3b6mla7qqoax@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Trump also signed an executive order last week instructing each government agency to identify all leases that can be terminated and submit a plan to dispose of “government-owned real property which has been deemed by the agency as no longer needed.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000x3b6mj4swzprz@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The drive to shrink the federal government’s real estate portfolio predates the Trump administration. The Office of Management and Budget issued a “Reduce the Footprint” directive in 2015 that requires agencies to make more efficient use of federal property and dispose of surplus assets.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000y3b6mlikxf8fh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Last year’s OMB report included a list of agencies’ efforts to downsize their holdings. For instance, the Department of Veterans Affairs reduced office space in its headquarters locations in the Washington, DC, metro area by 16% between 2020 and 2022, saving $15 million annually.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4000z3b6mrrjwv4ij@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The GSA cut its own footprint by more than 2 million square feet over 10 years, avoiding $300 million in costs. And the Department of Energy moved out of 193,000 square feet of leased space in 2023, saving $9 million annually.
    </p>

<div data-image-variation="image_large" data-breakpoints="{&quot;image_large--eq-extra-small&quot;: 115, &quot;image_large--eq-small&quot;: 300, &quot;image_large--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti3k1r00033b6m5wj2k31k@published" data-name="c-GettyImages-2201176517.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6666666666666666" data-original-height="1600" data-original-width="2400" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?c=original" data-editable="settings">
       <picture><source height="1600" width="2400" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="1600" width="2400" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2201176517.jpg?q=w_1110,c_fill" alt="A pedestrian near a General Services Administration building in Washington, DC, on Monday, February 24. " onload="this.classList.remove('image_large__dam-img--loading')" onerror="imageLoadError(this)" height="1600" width="2400" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400103b6mb5d7s02n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Those downsizings have had an impact. One federal employee still waiting for their date to return to the office told CNN they suspect it’s been delayed because of a lack of room. Their agency, which the employee asked not to name because of concerns for their job, has been shedding office space for several years, so teleworking staffers have had to reserve desks in the remaining locations for the days they come in.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400113b6moszzk9vr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The space constraint, coupled with the Trump administration’s <a href="https://www.cnn.com/2025/02/26/politics/federal-mass-layoffs-trump-memo/index.html">reduction-in-force order</a>, has the worker fearing their agency will suffer heavy layoffs. “The only way RTO (return-to-office) works in these types of situations is if you now reduce the number of people,” the employee said.
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400133b6myk6mc303@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Multiple federal agencies brought the bulk of their employees back last week, a return that was met in some cases with a lack of desks, basic supplies and working equipment.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400143b6mo8c8mzg8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “There was very little prep and planning and it was messy with equipment,” an HHS staffer told CNN. The employee, who asked for their specific location not to be named, said there were reports of Wi-Fi and electricity not working.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7tzbkg60000356mwlpg5w7y@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Asked about the issue, Andrew Nixon, HHS’ communications director, said the agency is complying with Trump’s return to the office executive order. “We look forward to seeing and collaborating with our colleagues in person to Make America Healthy Again,” he continued.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400153b6mwhuve4yt@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The Department of Education sent an email last week to staff in regional offices acknowledging the shortcomings in some facilities on Day 1 of their return.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400163b6mbx9ev3n5@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “At the present time we are unable to deploy full peripheral setups in the regional/remote offices,” the department’s Office of the Chief Information Officer told employees in an email, which was obtained by CNN.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400173b6mmbs9f9rw@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Sheria Smith, president of the American Federation of Government Employees Local 252 in Dallas and a Department of Education employee, said her office was “chaos” when employees returned. She filed the workers’ comp complaint after injuring her foot tripping over a pile of cords on the floor.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400183b6mgniynv3n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The facilities were not actually ready for us to return,” Smith said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok400193b6m4wt9l8ci@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “No one is on-site to try to fix the issues,” she added, saying the mess leads her to believe “they would be hoping that we would quit, I guess — that they didn’t expect us to come.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001a3b6mit76tbc8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Another Department of Education employee in Washington, DC, said the first week back lacked basic office needs: computers, pens and headsets — as well as private space and conference rooms necessary for confidential job requirements.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001b3b6m0l2xr20t@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the National Oceanic and Atmospheric Administration headquarters in Silver Spring, Maryland, a telework policy has been in place for more than 20 years, officials said. Many employees had been teleworking three to four days a week and are now adapting to the rigidity of returning to the office.
    </p>

<div data-image-variation="image_large" data-breakpoints="{&quot;image_large--eq-extra-small&quot;: 115, &quot;image_large--eq-small&quot;: 300, &quot;image_large--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cm7ti3w9800053b6m4thchr42@published" data-name="c-GettyImages-2203083573.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6670833333333334" data-original-height="1601" data-original-width="2400" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?c=original" data-editable="settings">
       <picture><source height="1601" width="2400" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="1601" width="2400" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c-gettyimages-2203083573.jpg?q=w_1110,c_fill" alt="Hundreds of demonstrators gather to protest against Department of Government Efficiency (DOGE) cuts outside the headquarters of the National Oceanic and Atmospheric Administration on March 3, in Silver Spring, Maryland." onload="this.classList.remove('image_large__dam-img--loading')" onerror="imageLoadError(this)" height="1601" width="2400" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001c3b6mt5ajyxux@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “Morale is pretty low,” a staffer in the agency said. “If you have a dentist appointment at 3 p.m. near where you live, and it ends at 4:15 p.m., you cannot work that last hour from home. You have to go back to the office or take sick leave.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001d3b6m8sdzmo6f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The return-to-office mandate comes as budget cuts at EPA led to reduced cleaning and facility services at key offices, even before the Trump administration took over.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001e3b6mokfltdj8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In the three major EPA offices around the country that make up the agency’s headquarters — Washington, Cincinnati and Research Triangle Park, North Carolina — health units were closed and in-office mail delivery was cut back, according to a memo obtained by CNN. In Cincinnati, drinking fountain cleaning schedules were reduced to once per week, carpets and hallways were swept once a month, and bathroom cleanings were pared back to once a day.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001f3b6m81n56i52@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “With these people coming back to the offices, they’re going to have fewer facility services,” the EPA union official said.
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001h3b6mxf5m3uln@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The 2024 OMB report found those eligible for telework spend about 60% of their working hours in the office, on average, though that figure varies widely by agency. About 10% of staffers have remote jobs, where they are not expected to report to an office at all, according to the report, which noted the Biden administration also directed agencies to increase the amount of time federal workers spent at the office.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001i3b6m9ga6xl82@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The Biden administration’s goal was for teleworkers to spend at least 50% of the time in the office. Now Trump turned that into a full-time mandate.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001j3b6mw8ryoj8k@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But not all federal workers are returning at the same pace, as Trump’s executive order, issued hours after he took office in January, directed agencies to terminate remote work arrangements “as soon as practicable.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001k3b6movkqhmp4@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Several agencies, including the departments of Veterans Affairs and Health and Human Services, have said political appointees, senior executives and other senior staffers could no longer work remotely or telework as of February 24, and neither could supervisors who live within 50 miles of an agency facility.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001l3b6mqhwzi9zp@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Others have more time. For instance, at the VA, lower-level non-union employees who live within 50 miles of a facility will have their remote and telework arrangements terminated by April 28, except for ad hoc or situational circumstances, the agency said. But these arrangements for supervisors and other non-union employees who reside farther away were not ended — those staffers will receive additional guidance.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001m3b6mcycmncdy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            As for union workers at the VA, their return-to-office date will be announced at a later time, the agency said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7tpdftm00053b6mfxan4xsn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the Department of Education, more than 70% of its workforce started reporting to the Washington and regional offices full time last week, the agency said in a news release. The rest are expected back by June 1 after building renovations and relocation arrangements are complete.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001n3b6maetty1jv@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            However, many union members at the Department of Housing and Urban Development who were previously able to telework on certain days had to return to the office full time in late February, said Antonio Gaines, president of the AFGE Council 222, which represents 5,300 employees at the agency. Workers at some regional and field offices are exempt for now because of lack of space and safety concerns, including building renovations and inadequate HVAC systems.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001o3b6mk4fzp47r@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The mandate violates the union’s collective bargaining agreement, he argued.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001p3b6mwn7j8d8n@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “They made a unilateral decision to bypass the negotiating process,” Gaines said of the agency, adding that the union plans to file a complaint on this matter as part of a bigger grievance package.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cm7thvok4001q3b6meyhnp4y8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The department did not return a request for comment.
    </p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Italy moves to reverse anti-nuclear stance (217 pts)]]></title>
            <link>https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance</link>
            <guid>43253407</guid>
            <pubDate>Tue, 04 Mar 2025 11:51:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance">https://www.world-nuclear-news.org/articles/cabinet-moves-to-reverse-italys-anti-nuclear-stance</a>, See on <a href="https://news.ycombinator.com/item?id=43253407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
                    Monday, 3 March 2025

                    </p>

                        <p>Italy's Council of Ministers has approved a draft law calling for the government to adopt a series of legislative decrees to create the legal framework for the reintroduction of nuclear power, which was phased out following a referendum in 1987.</p>
                    
                            <figure>
                                <img src="https://www.world-nuclear-news.org/images/articles/GilbertoPichettoFratin_67730.jpg" alt="Cabinet moves to reverse Italy's anti-nuclear stance">
                                    <figcaption>Pichetto Fratin speaking at a press conference following the cabinet meeting (Image: Ministry of Environment and Energy Security)</figcaption>

                            </figure>
                    
                    


               

                <p>On 28 February, on the proposal of President Giorgia Meloni and the Minister of the Environment and Energy Security Gilberto Pichetto Fratin, the Council of Ministers (the Italian cabinet) approved the draft delegation law on 'sustainable nuclear energy'.</p>

<p>The government said the text is aimed at "the inclusion of sustainable nuclear and fusion in the so-called 'Italian energy mix' and intervenes organically from an economic, social and environmental perspective, within the framework of European decarbonisation policies with a time horizon of 2050, consistently with the objectives of carbon neutrality and security of supply".</p>

<p>It added that the intervention aims to: ensure continuity of energy supply in the presence of a constant increase in demand and promote the achievement of energy independence; contribute to the decarbonisation objectives necessary to tackle climate change; and ensure the sustainability of costs borne by end users and the competitiveness of the national industrial system.</p>

<p>The draft law says that Italy should make "a clear break ... with respect to the nuclear plants of the past" and "use of the best available technologies, including modular and advanced technologies". It calls for the government to establish an independent authority for the regulation, supervision and control of nuclear infrastructures.</p>

<p>"Promoters of nuclear projects must provide adequate financial and legal guarantees to cover the costs of construction, operation and decommissioning of the plants and for risks, even those not directly attributable to them, arising from nuclear activity," it adds.</p>

<p>The draft law requires the government to adopt a series of legislative decrees, within 12 months of entry into force, to "organically regulate the entire life cycle of the new sustainable energy, through the drafting of a national programme: from the testing, localisation, construction and operation of the new reactors, to the issue of manufacturing and reprocessing of the fuel will be addressed in a circular economy vision". It will also "serve to provide training and information tools, train new technicians and professionals in the sector, and identify benefits for the territories involved".</p>

<p>The draft law will now be submitted to parliament for final approval.</p>

<p>"With the latest generation nuclear, together with renewables, we will be able to achieve the objectives of decarbonisation, guaranteeing the full energy security of the country," Minister Pichetto Fratin said. "In this way, Italy is ready to face the challenges of the future."</p>

<h4><span>The background</span><br>
&nbsp;</h4>

<p>Italy operated a total of four nuclear power plants starting in the early 1960s but decided to phase out nuclear power in a referendum that followed the 1986 Chernobyl accident. It closed its last two operating plants, Caorso and Trino Vercellese, in 1990.</p>

<p>In late March 2011, following the Fukushima Daiichi accident, the Italian government approved a moratorium of at least one year on construction of nuclear power plants in the country, which had been looking to restart its long-abandoned nuclear programme.</p>

<p>The public mood has changed since then, and in May 2023, the Italian Parliament approved a motion to urge the government to consider incorporating nuclear power into the country's energy mix. In September last year, the first meeting was held of the National Platform for a Sustainable Nuclear, set up by the government to define a time frame for the possible resumption of nuclear energy in Italy and identify opportunities for the country's industrial chain already operating in the sector.</p>

<p>Italy's government included potential nuclear capacity - up to 16 GW/20-22% of capacity by 2050 - in its National Integrated Energy and Climate Plan, which was submitted to the European Commission on 1 July 2024.</p>

<p>Speaking the following day at the <em>Global Energy Transition Congress in Milan</em>, Pichetto Fratin, said: "We expect to be able to reach about 8 GW from nuclear power by 2050, covering more than 10% of the nation's electricity demand. This percentage may increase to over 20-22% by fully exploiting the potential of nuclear power in our country."</p>


                </div></div>]]></description>
        </item>
    </channel>
</rss>