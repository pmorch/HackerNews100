<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 29 Jul 2024 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Children should be allowed to get bored, expert says (2013) (109 pts)]]></title>
            <link>https://www.bbc.com/news/education-21895704</link>
            <guid>41098488</guid>
            <pubDate>Mon, 29 Jul 2024 07:17:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/education-21895704">https://www.bbc.com/news/education-21895704</a>, See on <a href="https://news.ycombinator.com/item?id=41098488">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p><b>Children should be allowed to get bored so they can develop their innate ability to be creative, an education expert says.</b></p><p>Dr Teresa Belton told the BBC cultural expectations that children should be constantly active could hamper the development of their imagination</p><p>She quizzed author Meera Syal and artist Grayson Perry about how boredom had aided their creativity as children.</p><p>Syal said boredom made her write, while Perry said it was a "creative state".</p><p>The senior researcher at the University of East Anglia's School of Education and Lifelong Learning interviewed a number of authors, artists and scientists in her exploration of the effects of boredom.</p><p>She heard Syal's memories of the small mining village, with few distractions, where she grew up.</p><p>Dr Belton said: "Lack of things to do spurred her to talk to people she would not otherwise have engaged with and to try activities she would not, under other circumstances, have experienced, such as talking to elderly neighbours and learning to bake cakes.</p><p>"Boredom is often associated with solitude and Syal spent hours of her early life staring out of the window across fields and woods, watching the changing weather and seasons.</p><p>"But importantly boredom made her write. She kept a diary from a young age, filling it with observations, short stories, poems, and diatribe. And she attributes these early beginnings to becoming a writer late in life."</p></div><div data-component="text-block"><p>The comedienne turned writer said: "Enforced solitude alone with a blank page is a wonderful spur."</p><p>While Perry said boredom was also beneficial for adults: "As I get older, I appreciate reflection and boredom. Boredom is a very creative state."</p><p>And neuroscientist and expert on brain deterioration Prof Susan Greenfield, who also spoke to the academic, recalled a childhood in a family with little money and no siblings until she was 13.</p><p>"She happily entertained herself with making up stories, drawing pictures of her stories and going to the library."</p><p>Dr Belton, who is an expert in the impact of emotions on behaviour and learning, said boredom could be an "uncomfortable feeling" and that society had "developed an expectation of being constantly occupied and constantly stimulated".</p><p>But she warned that being creative "involves being able to develop internal stimulus".</p><p>"Nature abhors a vacuum and we try to fill it," she said. "Some young people who do not have the interior resources or the responses to deal with that boredom creatively then sometimes end up smashing up bus shelters or taking cars out for a joyride."</p></div><div data-component="text-block"><p>The academic, who has previously studied the impact of television and videos on children's writing, said: "When children have nothing to do now, they immediately switch on the TV, the computer, the phone or some kind of screen. The time they spend on these things has increased. </p><p>"But children need to have stand-and-stare time, time imagining and pursuing their own thinking processes or assimilating their experiences through play or just observing the world around them."</p><p>It is this sort of thing that stimulates the imagination, she said, while the screen "tends to short circuit that process and the development of creative capacity".</p><p>Syal adds: "You begin to write because there is nothing to prove, nothing to lose, nothing else to do.</p><p>"It's very freeing being creative for no other reason other than you freewheel and fill time."</p><p>Dr Belton concluded: "For the sake of creativity perhaps we need to slow down and stay offline from time to time."</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ps aux written in bash without forking (181 pts)]]></title>
            <link>https://github.com/izabera/ps</link>
            <guid>41097241</guid>
            <pubDate>Mon, 29 Jul 2024 01:10:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/izabera/ps">https://github.com/izabera/ps</a>, See on <a href="https://news.ycombinator.com/item?id=41097241">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><code>ps aux</code> written entirely in bash without ever forking</h2><a id="user-content-ps-aux-written-entirely-in-bash-without-ever-forking" aria-label="Permalink: ps aux written entirely in bash without ever forking" href="#ps-aux-written-entirely-in-bash-without-ever-forking"></a></p>
<p dir="auto">An interview question for a position that requires knowledge of bash/linux/stuff could be:</p>
<blockquote>
<p dir="auto">What if you're ssh'd into a machine, you're in your trusty bash shell, but unfortunately you cannot spawn any new processes because literally all other pids are taken.  What do you do?</p>
</blockquote>
<p dir="auto">And if that's what you're facing, this might be the tool for you!  Now you can kinda sorta pretend that you have access to a working <code>ps aux</code>.</p>
<p dir="auto">It definitely totally works on 100% of the machines in every situation ever, guaranteed.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LeanDojo: Theorem Proving in Lean Using LLMs (143 pts)]]></title>
            <link>https://leandojo.org/</link>
            <guid>41096486</guid>
            <pubDate>Sun, 28 Jul 2024 22:34:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leandojo.org/">https://leandojo.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41096486">Hacker News</a></p>
<div id="readability-page-1" class="page">


<div>
                    <h2>LeanDojo: Theorem Proving in Lean using Language Models</h2>
<!--                     <h3 class="title is-4 conference-authors"><a target="_blank" href="https://nips.cc/">NeurIPS 2023 (Datasets and Benchmarks Track), Oral presentation</a></h3> -->
                    <!--
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <a target="_blank" href="https://yangky11.github.io/">Kaiyu&#160;Yang</a> <a href="mailto:kaiyuy@caltech.edu"><i class="fas fa-envelope"></i></a><sup>1</sup>,
                            <a target="_blank" href="https://aidanswope.com/about">Aidan&#160;Swope</a><sup>2</sup>,
                            <a target="_blank"
                            href="https://minimario.github.io/">Alex&#160;Gu</a><sup>3</sup>,
                            <a target="_blank" href="https://rchalamala.github.io/">Rahul&#160;Chalamala</a><sup>1</sup>,
                            <a target="_blank" href="https://peiyang-song.github.io/">Peiyang&#160;Song</a><sup>4</sup>,
                            <a target="_blank"
                            href="https://billysx.github.io/">Shixing&#160;Yu</a><sup>5</sup>,
                            <br>
                            <a target="_blank" href="https://www.linkedin.com/in/saad-godil-9728353/">Saad&#160;Godil</a><sup>2</sup>,
                            <a target="_blank" href="https://www.linkedin.com/in/ryan-prenger-18797ba1/">Ryan&#160;Prenger</a><sup>2</sup>,
                            <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima&#160;Anandkumar</a><sup>1 2</sup>
                
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Caltech; </span>
                        <span class="author-block"><sup>2</sup>NVIDIA; </span>
                        <span class="author-block"><sup>3</sup>MIT; </span>
                        <span class="author-block"><sup>4</sup>UC Santa Barbara; </span>
                        <span class="author-block"><sup>5</sup>UT Austin </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><i class="fas fa-envelope"></i> Correspondence to: kaiyuy@caltech.edu</span>
                    </div>
                    -->

                    
                </div>


<div>
                    <h2><span>LLMs as Copilots for Theorem Proving</span></h2>
                    <video autoplay="" controls="">
            		  <source src="https://leandojo.org/images/Lean%20Copilot%20demo.mp4" type="video/mp4">
            		  Your browser does not support HTML video.
            		</video>
                    
                    <p>We introduce <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a> for LLMs to act as copilots in Lean for proof automation, e.g., suggesting tactics/premises and searching for proofs. Users can use our model or bring their own models that run either locally (w/ or w/o GPUs) or on the cloud.</p>
                </div>
    
<div>
                    <h2><span>Overview of LeanDojo</span></h2>
                    <p><img src="https://leandojo.org/images/LeanDojo.jpg" alt=""></p><p><b>Top right</b>: LeanDojo extracts proofs in <a target="_blank" href="https://leanprover.github.io/">Lean</a> into datasets for training machine learning models. It also enables the trained model to prove theorems by interacting with Lean's proof environment.</p>
                      
                    <p><b>Top left</b>: The proof tree of a Lean theorem <a target="_blank" href="https://github.com/leanprover-community/lean/blob/cce7990ea86a78bdb383e38ed7f9b5ba93c60ce0/library/init/data/nat/gcd.lean#L36">\(\forall n \in \mathbb{N},~\texttt{gcd n n = n}\)</a>, where <span>\(\texttt{gcd}\)</span> is the greatest common divisor. When proving the theorem, we start from the original theorem as the initial state (the root) and repeatedly apply tactics (the edges) to decompose states into simpler sub-states, until all states are solved (the leaf nodes). Tactics may rely on premises such as <span>\(\texttt{mod_self}\)</span> and  <span>\(\texttt{gcd_zero_left}\)</span> defined in a large math library. E.g., <span>\(\texttt{mod_self}\)</span> is an existing theorem <a target="_blank" href="https://github.com/leanprover-community/lean/blob/cce7990ea86a78bdb383e38ed7f9b5ba93c60ce0/library/init/data/nat/lemmas.lean#L861">\(\forall n \in \mathbb{N},~\texttt{n % n = 0}\)</a> used in the proof to simplify the goal.</p>
                    
                    <p><b>Bottom</b>: Our ReProver model. Given a state, it retrieves premises from the math library, which are concatenated with the state and fed into an encoder-decoder Transformer to generate the next tactic.</p>
                </div>
<!--
<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. 
                        However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute 
                        requirements. This has created substantial barriers to research on machine learning methods for theorem proving. 
                        This paper removes these barriers by introducing <em>LeanDojo</em>: an open-source Lean playground consisting of toolkits, 
                        data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. 
                        It contains fine-grained annotations of premises in proofs, providing valuable data for <em>premise selection</em>&mdash;
                        a key bottleneck in theorem proving. Using this data, we develop <em>ReProver</em> (<u>Re</u>trieval-Augmented <u>Prover</u>): 
                        an LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. 
                        It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis 
                        capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. 
                        Furthermore, we construct a new benchmark consisting of 98,641 theorems and proofs extracted from Lean's math library. 
                        It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never 
                        used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness 
                        of ReProver over non-retrieval baselines and GPT-4. <b>We thus provide the first set of open-source LLM-based theorem provers without 
                        any proprietary datasets and release it under a permissive MIT license to facilitate further research.</b>
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
-->


<div>
                    <h2><span>Benchmarks</span></h2>
                    <ul>
                            <li><a href="https://doi.org/10.5281/zenodo.8016385"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.8016385.svg" alt="DOI"></a> <b>LeanDojo Benchmark</b>: 98,734 theorems/proofs, 217,776 tactics, and 130,262 premises from <a target="_blank" href="https://github.com/leanprover-community/mathlib/tree/19c869efa56bbb8b500f2724c0b77261edbfa28c">mathlib</a>.</li>
                            <li><a href="https://doi.org/10.5281/zenodo.8040109"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.8040109.svg" alt="DOI"></a> <b>LeanDojo Benchmark 4</b>: 122,517 theorems/proofs, 259,580 tactics, and 167,779 premises from <a target="_blank" href="https://github.com/leanprover-community/mathlib4/tree/29dcec074de168ac2bf835a77ef68bbe069194c5">mathlib4</a>.</li>
                        </ul>
                    
                    
                    <p>LeanDojo can extract data from any GitHub repos in Lean (supporting both Lean 3 and Lean 4). The data contains rich information not directly visible in the raw Lean code, including file dependencies, abstract syntax trees (ASTs), proof states, tactics, and premises.</p>
                    
                    <p><b>Key feature 1: Premise information</b>: LeanDojo Benchmark contains fine-grained annotations of premises (where they are used in proofs and where they are defined in the library), providing valuable data for premise selection—a key bottleneck in theorem proving.</p>
                    
                    <p><b>Key feature 2: Challenging data split</b>: Splitting theorems randomly into training/testing leads to overestimated performance. LLMs can prove seemingly difficult theorems simply by memorizing the proofs of similar theorems during training. We mitigate this issue by designing challenging data split requiring the model to generalize to theorems relying on novel premises that are never used in training.</p>
                </div>

<div>
                    <h2><span>Interacting with Lean</span></h2>
                    <p><img src="https://leandojo.org/images/interaction.jpg" width="50%" alt=""></p><p>LeanDojo turns Lean into a <a target="_blank" href="https://github.com/openai/gym">gym-like</a> environment, in which the prover can observe the proof state, run tactics to change the state, and receive feedback on errors or on proof completion. This environment is indispensable for evaluating/deploying the prover or training it through RL.</p>
                </div>

<!--
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">LeanDojo Benchmark</span></h2>
                
                    <img src="images/quaternion.jpg" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">ReProver: Retrieval-Augmented Theorem Prover</span></h2>
                </div>
            </div>
        </div>
    </div>
</section>
-->

<div>
                    <h2><span>Experiments</span></h2>
                    <p><img src="https://leandojo.org/images/results.png" width="40%" alt=""></p><p>We use LeanDojo Benchmark to train and evaluate ReProver. During testing, the tactic generator is combined with best-first search to search for complete proofs. The table shows the percentage of theorem proved within 10 minutes. The \(\texttt{novel_premises}\) spilt is much more challenging than the \(\texttt{random}\) split. On both data splits, ReProver outperforms Lean's built-in proof automation tactic (tidy), a baseline that generates tactics directly without retrieval, and another baseline using GPT-4 to generate tactics in a zero-shot manner.</p>
                </div>

<div>
                    <h2><span>Discovering New Proofs and Uncovering Formalization Bugs</span></h2>
                    <p>We evaluate ReProver on theorems in miniF2F and ProofNet. It discovers <a target="_blank" href="https://github.com/facebookresearch/miniF2F/pull/13">33 proofs in miniF2F</a> and <a target="_blank" href="https://github.com/zhangir-azerbayev/ProofNet/pull/14">39 proofs in ProofNet</a> of theorems that did not have existing proofs in Lean. Our proofs have helped ProofNet uncover multiple bugs in the formalization of theorem statements.</p>
                </div>

<div>
                    <h2><span id="chatgpt-for-theorem-proving">ChatGPT for Theorem Proving</span></h2>
                    
                    
                    <p>We build a <a target="_blank" href="https://github.com/lean-dojo/LeanDojoChatGPT">LeanDojo ChatGPT plugin</a> that enables ChatGPT to prove theorems by interacting with Lean. Compared to specialized LLMs finetuned for theorem proving (e.g., ReProver), ChatGPT can interleave informal mathematics with formal proof steps, similar to how humans interact with proof assistants. It can explain error messages from Lean and is more steerable (by prompt engineering) than specialized provers. However, it struggles to find correct proofs in most cases, due to weakness in search and planning.</p>
                </div>
    

<div>
                    <h2><span>Team</span></h2>
                    
                    <br>
                    

                </div>
    
<div id="BibTeX">
        <h2>BibTeX</h2>
        <pre><code>@inproceedings{yang2023leandojo,
    title={{LeanDojo}: Theorem Proving with Retrieval-Augmented Language Models},
    author={Yang, Kaiyu and Swope, Aidan and Gu, Alex and Chalamala, Rahul and Song, Peiyang and Yu, Shixing and Godil, Saad and Prenger, Ryan and Anandkumar, Anima},
    booktitle={Neural Information Processing Systems (NeurIPS)},
    year={2023}
}

@article{song2024towards,
    title={Towards Large Language Models as Copilots for Theorem Proving in {Lean}},
    author={Peiyang Song and Kaiyu Yang and Anima Anandkumar},
    year={2024},
    journal = {arXiv preprint arXiv: Arxiv-2404.12534}
}</code></pre>
    </div>





</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Higher-kinded bounded polymorphism in OCaml (2021) (126 pts)]]></title>
            <link>https://okmij.org/ftp/ML/higher-kind-poly.html</link>
            <guid>41096187</guid>
            <pubDate>Sun, 28 Jul 2024 21:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://okmij.org/ftp/ML/higher-kind-poly.html">https://okmij.org/ftp/ML/higher-kind-poly.html</a>, See on <a href="https://news.ycombinator.com/item?id=41096187">Hacker News</a></p>
<div id="readability-page-1" class="page">



<div><p>Higher-kinded polymorphism&nbsp;-- the abstraction over a type
<em>constructor</em> to be later supplied with arguments&nbsp;-- is often needed,
for expressing generic operations over collections or embedding typed
DSLs, particularly in tagless-final style. Typically, the abstracted type
constructor is not arbitrary, but must implement a particular
interface (e.g., an abstract sequence)&nbsp;-- so-called bounded
polymorphism. 
</p><p>OCaml does not support higher-kinded polymorphism directly: OCaml type
variables range over types rather than type constructors, and type
constructors may not appear in type expressions without being applied
to the right number of arguments.  Nevertheless, higher-kinded
polymorphism is expressible in OCaml&nbsp;-- in fact, in several, more or
less cumbersome ways. The less cumbersome ways are particularly less
known, and kept being rediscovered. This page summarizes the different
ways of expressing, and occasionally avoiding, higher-kinded
polymorphism. They are collected from academic papers and messages on
the caml-list spread over the years&nbsp;-- and adjusted to fit the story
and differently explained.
</p></div>
<ul>
<li><a href="#intro">Introduction</a>
</li>
<li><a href="#complex">Why higher-kinded polymorphism is not supported directly in OCaml</a>
</li>
<li><a href="#functors">Higher-kinded functions as Functors</a>
</li>
<li><a href="#yw">Yallop and White's Lightweight higher-kinded polymorphism</a>
</li>
<li><a href="#avoid">Sidestepping higher-kinded polymorphism</a>
</li>
<li><a href="#algebra">Algebras</a>
</li>
<li><a href="#hkname">What's in a higher-kinded type name</a>
</li>
<li><a href="#conc">Conclusions</a>
</li></ul>
<hr>


<h2><a name="intro">Introduction</a></h2>
<dl>
<dd>``Polymorphism abstracts types, just as functions abstract
values. Higher-kinded polymorphism takes things a step further,
abstracting both types and types constructors, just as higher-order
functions abstract both first-order values and functions.''&nbsp;-- write
Yallop and White (FLOPS 2014).
<p>This remarkably concise summary is worth expounding upon, to
demonstrate how (bounded) higher-kinded polymorphism tends to
arise. The example introduced here is used all throughout the page.
</p><p>Summing up numbers frequently occurs in practice; abstracting from
concrete numbers leads to a function&nbsp;-- an operation that can be
uniformly performed on any collection (list) of numbers:

</p><pre>    let rec sumi : int list -&gt; int = function [] -&gt; 0 | h::t -&gt; h + sumi t
</pre>We may further abstract over <code>0</code> and the operation
<code>+</code>, which itself is a function (a parameterized value, so to
speak). The result is a higher-order function:

<pre>    let rec foldi (f: int-&gt;int-&gt;int) (z: int) : int list -&gt; int =
        function [] -&gt; z | h::t -&gt; f h (foldi f z t)
</pre>Folding over a list, say, of floating-point numbers proceeds similarly, so we
may abstract yet again&nbsp;-- this time not over values but over the type
<code>int</code>, replacing it with a type variable:

<pre>    let rec fold (f: 'a-&gt;'a-&gt;'a) (z: 'a) : 'a list -&gt; 'a =
        function [] -&gt; z | h::t -&gt; f h (fold f z t)
</pre>thus giving us the polymorphic function: the function that describes an
operation performed over lists of various types, uniformly.
The operation <code>f</code> and the value <code>z</code> can be collected into a 
parameterized record

<pre>    type 'a monoid = {op: 'a-&gt;'a-&gt;'a; unit: 'a}
</pre>The earlier <code>fold</code> then takes the form

<pre>    let rec foldm (m: 'a monoid) : 'a list -&gt; 'a =
        function [] -&gt; m.unit | h::t -&gt; m.op h (foldm m t)
</pre>When using <code>foldm</code> on a concrete list of the type <code>t list</code>, 
the type variable <code>'a</code> gets
instantiated to the type <code>t</code> of the elements of this list. The type is not
completely arbitrary, however: there must exist the value 
<code>t monoid</code>, to be passed to <code>foldm</code> as the argument. We say the type
<code>t</code> must (at least) implement/support the <code>'a monoid</code> interface; the <code>t monoid</code> value is then the witness that <code>t</code> indeed does so. Hence the
polymorphism in <code>foldm</code> is <em>bounded</em>.
<p><em>Exercise:</em> if <code>'a monoid</code> really describes a monoid, 
<code>op x unit = x</code> holds. Write a more optimal version of <code>foldm</code> (and
its subsequent variants) taking advantage of this identity.
</p><p>A file, a string, an array, a sequence&nbsp;-- all can be folded over in the
same way. Any collection is foldable so long as it supports the
deconstruction operation, which tells if the collection is empty, or
gives its element and the rest of the sequence. One is tempted to
abstract again&nbsp;-- this time not over a mere type like <code>int</code> or <code>int list</code>, but over a type constructor such as <code>list</code>, and introduce

</p><pre>    type ('a,'F) seq = {decon: 'a 'F -&gt; ('a * 'a 'F) option}
</pre>This is a hypothetical OCaml: the type variable 'F (with the
upper-case name) is to be instantiated not with types but one-argument
type constructors: technically, one says it has the <em>higher-kind</em>
<code>* -&gt; *</code> rather than the ordinary kind <code>*</code> of types and ordinary type variables
such as <code>'a</code>. The record <code>seq</code> is, hence, higher-kind polymorphic. The
function <code>foldm</code> then generalizes to

<pre>    let rec folds (m: 'a monoid) (s: ('a,'F) seq) : 'a 'F -&gt; 'a = fun c -&gt;
        match s.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (folds m s t)
</pre>Again, <code>'F</code> is instantiated not with just any type constructor, but only that
for which we can find the value <code>('a,'F) seq</code>; thus <code>folds</code> exhibits 
<em>bounded higher-kinded</em> polymorphism.
<p>Alas, higher-kind type variables are not possible in OCaml. The
next section explains why. The following sections tell what we can do in
OCaml instead. There are several alternatives. In some, the end result
ends up looking almost exactly as the above imagined
higher-kind--polymorphic code.</p></dd></dl>


<h2><a name="complex">Why higher-kinded polymorphism is not supported directly in OCaml</a></h2>
<dl>
<dd>Higher-kind type variables are not supported in OCaml.  Yallop and
White's FLOPS 2014 paper (Sec. 1.1) explains why: in a word, type
aliasing. For completeness, we recount their explanation here, with
modifications.
<p>Consider the following two modules:

</p><pre>    module Tree = struct
      type 'a t = Leaf | Branch of 'a t * 'a * 'a t
    end
    
    module TreeA : dcont = struct
      type 'a t = ('a * 'a) Tree.t
    end
</pre>Here, <code>'a Tree.t</code> is a data type: a fresh type, distinct from
all other existing types. On the other hand, <code>'a TreeA.t</code> is an alias:
as its declaration says, it is equal to an existing type, viz. <code>('a * 'a) Tree.t</code>.
<p>Suppose OCaml had higher-kind <code>* -&gt; *</code> type variables, such as <code>'F</code>
hypothesized in the previous section. Type checking is, in the end,
solving/checking type equalities, such as <code>'a 'F = 'b 'G</code>. If
higher-kind type variables ranged only over <em>data type</em> constructors,
the solution is easy: <code>'a = 'b</code> and <code>'F = 'G</code>: a data type is fresh, hence
equal only to itself. This is the situation in Haskell. To ensure that
only data type constructors can be substituted for higher-kind type
variables, a Haskell compiler keeps track of type aliases, even across
module boundaries. Module system in Haskell is rather simple, so such
tracking is unproblematic.
</p><p>Module system of ML is, in contrast, sophisticated. It has functors,
signatures, etc., and extensively relies on type aliases, for example:

</p><pre>    module F(T: sig type 'a t  val empty: 'a t end) = struct
      type 'a ft = 'a T.t
    end
</pre>If we preclude substitution of type aliases for higher-kind type
variables, we severely restrict expressiveness. For example, <code>'a ft</code>
above is a type alias; hence <code>F(TRee).ft</code> cannot be substituted for a
higher-kind type variable, even though one may feel <code>F(TRee).ft</code> is
the same as <code>Tree.t</code>, which is substitutable.
<p>On the other hand, if we allow type aliases to be substituted for
higher-kind type variables, the equivalence of <code>'a 'F = 'b 'G</code> and <code>'a = 'b, 'F = 'G</code> breaks down. Indeed, consider <code>(int*int) 'F = int 'G</code>. This equation now has the solution: <code>'F = Tree.t</code> and <code>'G = TreeA.t</code>. Parameterized type aliases like <code>'a TreeA.t</code> are type
functions, and type expressions like <code>int TreeA.t</code> are applications of
those functions, expanding to the right-hand-side of the alias
declaration with <code>'a</code> substituted for <code>int</code>. Thus, with type aliases,
the type equality problem becomes the higher-order unification problem,
which is not decidable.</p></dd>
<dt><strong>References</strong></dt>
<dd>Jeremy Yallop and Leo White: Lightweight higher-kinded
polymorphism. FLOPS 2014.
</dd></dl>


<h2><a name="functors">Higher-kinded functions as Functors</a></h2>
<dl>
<dd>Although OCaml does not support higher-kind type variables, higher-kinded
polymorphism is not out of the question. There are other ways of
parameterizing by a type constructor: the module system (functor)
abstraction is the first to come to mind. 
It is however rather verbose and cumbersome. Let us see.
<p>We now re-write the hypothetical higher-kind--polymorphic OCaml code
at the end of [<a href="#intro">Introduction</a>] in the real OCaml&nbsp;-- by raising the level, so to
speak, from term-level to module-level. The hypothetical record

</p><pre>    type ('a,'F) seq = {decon: 'a 'F -&gt; ('a * 'a 'F) option}
</pre>becomes the module signature

<pre>    module type seq_i = sig
      type 'a t                                (* sequence type *)
      val decon : 'a t -&gt; ('a * 'a t) option
    end
</pre>which represents the higher-kind type variable <code>'F</code>, not supported in
OCaml, with an ordinary type constructor <code>t</code> (type
constant). Different implementations of <code>seq_i</code> (see, e.g., <code>ListS</code>
below) instantiate <code>'a t</code> in their own ways; hence
<code>t</code> does in effect act like a variable. The hypothetical
higher-kind--polymorphic function

<pre>    let rec folds (m: 'a monoid) (s: ('a,'F) seq) : 'a 'F -&gt; 'a = fun c -&gt;
        match s.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (folds m s t)
</pre>becomes the functor, parameterized by the <code>seq_i</code> signature:

<pre>    module FoldS(S:seq_i) = struct
      let rec fold (m: 'a monoid) : 'a S.t -&gt; 'a = fun c -&gt;
        match S.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (fold m t)
    end
</pre>We got what we wanted: abstraction over a sequence. To use
it to define other higher-kinded polymorphic functions, such as
<code>sums</code> to sum up a sequence, we also
need functors. Functors are infectious, one may say.

<pre>    module SumS(S:seq_i) = struct
      open S
      open FoldS(S)
      let sum : int t -&gt; int = fold monoid_plus
    end
</pre>
<p>Finally, an example of instantiating and using
higher-kind--polymorphic functions: summing a list. First we need an
instance of <code>seq_i</code> for a list: the witness that a list is a sequence.

</p><pre>    module ListS = struct
      type 'a t = 'a list
      let decon = function [] -&gt; None | h::t -&gt; Some (h,t)
    end
</pre>which we pass to the <code>SumS</code> functor:

<pre>    let 6 =
      let module M = SumS(ListS) in M.sum [1;2;3]
</pre>The accompanying code shows another example: using the same <code>SumS</code> to
sum up an array, which also can be made a sequence.
<p>Thus in this approach, all higher-kind--polymorphic functions are
functors, which leads to verbosity, awkwardness and boilerplate. For
example, we cannot even write a <code>SumS</code> application as <code>SumS(ListS).sum [1;2;3]</code>; we have to use the verbose expression above.</p></dd>
<dt><strong>References</strong></dt>
<dd><a href="https://okmij.org/ftp/ML/HKPoly_seq.ml">HKPoly_seq.ml</a>&nbsp;[11K]<br>

The complete code with tests and other examples
</dd></dl>


<h2><a name="yw">Yallop and White's Lightweight higher-kinded polymorphism</a></h2>
<dl>
<dd>Perhaps surprisingly, higher-kinded polymorphism can always be reduced
to the ordinary polymorphism, as Yallop and White's FLOPS 2014 paper
cleverly demonstrated. They explained their approach as
defunctionalization. Here we recap it and explain in a different way.
<p>Consider the type <code>'a list</code> again. It is a parameterized type: <code>'a</code> is
the type of elements, and <code>list</code> is the name of the collection: `the
base name', so to speak. The combination of the element type and the
base name can be expressed differently, for example, as
<code>('a,list_name) app</code>, where <code>('a,'b) app</code> is some fixed type, and
<code>list_name</code> is the ordinary type that tells the base name.  The fact
that the two representations are equivalent is witnessed by the
bijection:

</p><pre>    inj: 'a list -&gt; ('a,list_name) app
    prj: ('a,list_name) app -&gt; 'a list
</pre>
<p>Here is a way to implement it. First, we introduce the dedicated
`pairing' data type. It is extensible, to let us define as many
pairings as needed.

</p><pre>    type ('a,'b) app = ..
</pre>For <code>'a list</code>, we have:

<pre>    type list_name
    type ('a,'b) app += List_name : 'a list -&gt; ('a,list_name) app
</pre>In this case the bijection <code>'a list &lt;-&gt; ('a,list_name) app</code> is:

<pre>    let inj x = List_name x and
    let prj (List_name x) = x
</pre>and the two functions are indeed inverses of each other.
<p><em>Exercise:</em> Actually, that the above <code>inj</code> and <code>prj</code> are
the inverses of each other is not as straightforward. It requires a
side-condition, which is satisfied in our case. State it.
</p><p>In this new representation of the polymorphic list as <code>('a,list_name) app</code>, the base name <code>list_name</code> is the ordinary (kind <code>*</code>)
type. Abstraction over it is straightforward: replacing with a
type variable. The base-name-polymorphism is, hence, the ordinary
polymorphism. We can then write the desired sequence-polymorphic
<code>folds</code> almost literally as the hypothetical code at the end of [<a href="#intro">Introduction</a>]:

</p><pre>    type ('a,'n) seq = {decon: ('a,'n) app -&gt; ('a * ('a,'n) app) option}
    
    let rec folds (m: 'a monoid) (s: ('a,'n) seq) : ('a,'n) app -&gt; 'a = fun c -&gt;
        match s.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (folds m s t)
</pre>Instead of <code>'a 'F</code> we write <code>('a,'n) app</code>. That's it. Using <code>folds</code> in
other higher-kinded functions is straightforward, as if it were
a regular polymorphic function (which it actually is):

<pre>    let sums s c = folds monoid_plus s c
     (* val sums : (int, 'a) seq -&gt; (int, 'a) app -&gt; int = &lt;fun&gt; *)
</pre>Type annotations are not necessary: the type inference works. Here is a
usage example, summing a list:

<pre>    let list_seq : ('a,list_name) seq =
      {decon = fun (List_name l) -&gt;
       match l with [] -&gt; None | h::t -&gt; Some (h,List_name t)}
    
    let 6 = sums list_seq (List_name [1;2;3])
</pre>
<p>There is still a bit of awkwardness remains: the user have to think up
the base name like <code>list_name</code> and the tag like <code>List_name</code>, and
ensure uniqueness.  Yallop and White automate using the module system,
see the code accompanying this page, or Yallop and White's paper (and
the Opam package `higher').
</p><p>We shall return to Yallop and White's approach later on this page,
with another perspective and implementation.</p></dd>
<dt><strong>References</strong></dt>
<dd>Jeremy Yallop and Leo White: Lightweight higher-kinded
polymorphism. FLOPS 2014.

<p><a href="https://okmij.org/ftp/ML/HKPoly_seq.ml">HKPoly_seq.ml</a>&nbsp;[11K]<br>

The complete code with tests and other examples
</p></dd></dl>


<h2><a name="avoid">Sidestepping higher-kinded polymorphism</a></h2>
<dl>
<dd>At times, higher-kinded polymorphism can be avoided altogether: upon
close inspection it may turn out that the problem at hand does not
actually require higher-kinded polymorphism. In fact, our running
example is such a problem.
<p>Let us examine the sequence interface,
parameterized both by the type of the sequence elements and the sequence
itself. The definition that first comes to mind, which cannot be written
as such in OCaml, is (from <a href="#intro">Introduction</a>):

</p><pre>    type ('a,'F) seq = {decon: 'a 'F -&gt; ('a * 'a 'F) option}
</pre>It has a peculiarity: the sole operation <code>decon</code> consumes and produces
sequences of the same type <code>'a 'F</code> (i.e., the same sort of sequence
with the elements of the same type). That is, <code>'F</code>
always occurs as the type <code>'a 'F</code>, where <code>'a</code> is <code>seq</code>'s parameter: <code>'a</code> and
<code>'F</code> do not vary independently. Therefore, there is actually
no higher-kinded polymorphism here. The sequence interface can be written
simply as 

<pre>    type ('a,'t) seq = {decon: 't -&gt; ('a * 't) option}
</pre>with <code>folds</code> taking <em>exactly</em> the desired form:

<pre>    let rec folds (m: 'a monoid) (s: ('a,'t) seq) : 't -&gt; 'a = fun c -&gt;
        match s.decon c with None -&gt; m.unit | Some (h,t) -&gt; m.op h (folds m s t)
</pre>It is the ordinary polymorphic function. There is no problem in using
it to define other such sequence-polymorphic functions, e.g.:

<pre>    let sums s c = folds monoid_plus s c
    (* val folds : 'a monoid -&gt; ('a, 't) seq -&gt; 't -&gt; 'a = &lt;fun&gt; *)
</pre>and applying it, say, to a list:

<pre>    let list_seq : ('a,'a list) seq =
      {decon = function [] -&gt; None | h::t -&gt; Some (h,t)}
    
    let 6 = sums list_seq [1;2;3]
</pre>
<p><em>Exercise:</em> Consider the interface of collections that may be
`mapped', in the hypothetical OCaml with higher-kind type variables:

</p><pre>    type ('a,'b,'F) ftor = {map: ('a-&gt;'b) -&gt; ('a 'F -&gt; 'b 'F)}
</pre>Now <code>'F</code> is applied to different types. Can this interface be still
expressed using the ordinary polymorphism, or higher-kinded
polymorphism is really needed here?
<p>Looking very closely at the higher-kinded polymorphic interface
<code>('a,'F) seq</code> and the ordinary polymorphic <code>('a,'t) seq</code>, one may
notice that the latter is larger. The higher-kinded interface
describes only polymorphic sequences such as <code>'a list</code>, whereas
<code>('a,'t) seq</code> applies also to files, strings, buffers, etc. Such an
enlargement is welcome here: we can apply the same <code>folds</code> to
sequences whose structure is optimized for the type of their
elements. In Haskell terms, <code>('a,'t) seq</code> corresponds to `data
families', a later Haskell extension. Here is an
example, of applying <code>folds</code> to a string, which is not a polymorphic
sequence:

</p><pre>    let string_seq : (char,int*string) seq =
      {decon = fun (i,s) -&gt;
        if i &gt;= String.length s || i &lt; 0 then None else Some (s.[i], (i+1, s))}
    
    let 'c' = folds monoid_maxchar string_seq (0,"bca")
</pre>We can hence use <code>folds</code> with any collection, polymorphic or not, for
which there is an implementation of the <code>('a,'t) seq</code> interface. We have
encountered the old, and very useful trick: enlarging the type but
restricting the set of its values by having to be able to define `witnesses'
such as <code>('a,'t) seq</code>.
<p><em>Exercise:</em> Yallop and White's approach can also deal with
non-polymorphic collections. Use it to implement <code>string_seq</code>.</p></dd>
<dt><strong>References</strong></dt>
<dd><a href="https://okmij.org/ftp/ML/HKPoly_seq.ml">HKPoly_seq.ml</a>&nbsp;[11K]<br>

The complete code with tests and other examples
</dd></dl>


<h2><a name="algebra">Algebras</a></h2>
<dl>
<dd>The running example, the <code>seq</code> interface, was about deconstruction of
sequences: technically, about co-algebras. Let us now turn to
construction: building of values using a fixed set of operations,
which can be considered an embedded DSL. The abstraction over a DSL
implementation gives rise to polymorphism. If the embedded
DSL is typed, the polymorphism becomes higher-kinded&nbsp;-- as commonly seen
in DSL embeddings in tagless-final style.
<p>Here we briefly recount how the higher-kinded polymorphism arises in DSL
embeddings, and how it can be hidden away. The key idea is initial
algebra, which is, by definition, the abstraction over any concrete
algebra of the same signature, i.e., the abstraction over DSL implementations.
</p><p>Our running example in this section is a simple programming language
with integers and booleans: a dialect of the language used in Chap. 3
of Pierce's `Types and Programming Languages' (TAPL). Here is the familiar
tagless-final embedding in OCaml. The grammar of the language is
represented as an OCaml signature:

</p><pre>    module type sym = sig
      type 'a repr
      val int    : int -&gt; int repr
      val add    : int repr -&gt; int repr -&gt; int repr
      val iszero : int repr -&gt; bool repr
      val if_    : bool repr -&gt; 'a repr -&gt; 'a repr -&gt; 'a repr
    end
</pre>The language is typed; therefore, the type <code>'a repr</code>, which represents DSL
terms, is indexed by the term's type: an <code>int</code> or a <code>bool</code>. The
signature <code>sym</code> also defines the type system of the DSL: almost like
in TAPL, but with the typing rules written in a
vertical-space--economic way.
<p>Here is a sample term of the DSL:

</p><pre>    module SymEx1(I:sym) = struct
      open I
      let t1 = add (add (int 1) (int 2)) (int 3) (* intermediate binding *)
      let res = if_ (iszero t1) (int 0) (add t1 (int 1))
    end
</pre>It is written as a functor parameterized by <code>sym</code>: a DSL
implementation is abstracted out. The term is  polymorphic over
<code>sym</code> and, hence, may be evaluated in any implementation of the DSL. Since
<code>sym</code> contains a higher-kinded type <code>repr</code>, the polymorphism is
higher-kinded.
<p>The just presented (tagless-final) DSL embedding followed the
approach described in [<a href="#functors">Higher-kinded functions as Functors</a>]. Let us move away from functors
to ordinary terms. Actually, we never quite escape functors, but we
hide them in terms, relying on first-class modules.
As we have seen, a DSL term of the type <code>int</code> such as <code>SymEx1</code> is the functor

</p><pre>    functor (I:sym) -&gt; sig val res : int I.repr end
</pre>To abstract over <code>int</code>, we wrap it into a module

<pre>    module type symF = sig
      type a
      module Term(I:sym) : sig val res : a I.repr end
    end
</pre>which can then be turned into ordinary polymorphic type:

<pre>    type 'a sym_term = (module (symF with type a = 'a))
</pre>which lets us represent the functor <code>SymEx1</code> as an ordinary OCaml value:

<pre>    let sym_ex1 : _ sym_term = 
      (module struct type a = int module Term = SymEx1 end)
</pre>Here, the type annotation is needed. However, we let the type of the term
to be <code>_</code>,&nbsp;as a schematic variable. OCaml infers it as <code>int</code>.
If we have an implementation of <code>sym</code>, say, module <code>R</code>, we can use it
to run the example (and obtain the <code>sym_ex1</code>'s value in <code>R</code>'s interpretation):

<pre>    let _ = let module N = (val sym_ex1) in 
            let module M = N.Term(R) in M.res
</pre>
<p>The type <code>'a sym_term</code> can itself implement the <code>sym</code> signature, in 
a `tautological' sort of way:

</p><pre>    module SymSelf : (sym with type 'a repr = 'a sym_term) = struct
      type 'a repr = 'a sym_term
      let int : int -&gt; int repr = fun n -&gt;
        let module M(I:sym) = struct let res = I.int n end in
        (module struct type a = int module Term = M end)
      let add : int repr -&gt; int repr -&gt; int repr = fun (module E1) (module E2) -&gt;
        let module M(I:sym) = 
          struct module E1T = E1.Term(I) module E2T = E2.Term(I)
                 let res = I.add (E1T.res) (E2T.res) 
          end in
        (module struct type a = int module Term = M end)
       ...
    end
</pre>That was a mouthful. But writing <code>sym</code> DSL terms becomes much
easier, with no functors and no type annotations. The earlier <code>sym_ex1</code>
can now be written as

<pre>    let sym_ex1 = 
      let open SymSelf in
      let t1 = add (add (int 1) (int 2)) (int 3) in (* intermediate binding *)
      if_ (iszero t1) (int 0) (add t1 (int 1))
</pre>It can be evaluated in <code>R</code> or other implementation as shown before.
<p>Technically, <code>SymSelf</code> is the initial algebra: an implementation of
the DSL that can be mapped to any other implementation, and in a
unique way. That means its terms like <code>sym_ex1</code> can be evaluated in
any <code>sym</code> DSL implementation: they are polymorphic over DSL implementation.
</p><p>On the down-side, we have <code>SymSelf</code>, which is the epitome of
boilerplate: utterly trivial and voluminous code that has to be
written. On the up side, writing DSL terms cannot be easier: no type
annotations, no functors, no implementation passing&nbsp;-- and no overt
polymorphism, higher-kind or even the ordinary kind. Still, the terms
can be evaluated in any implementation of the DSL.
</p><p><em>Exercise:</em> Apply Yallop and White's method to this DSL
example. Hint: the first example in Yallop and White's paper, monad
representations, is an example of a DSL embedding in tagless-final
style.</p></dd>
<dt><strong>References</strong></dt>
<dd><a href="https://okmij.org/ftp/ML/HKPoly_tf.ml">HKPoly_tf.ml</a>&nbsp;[13K]<br>

The complete code with tests and detailed development

<p><a href="https://okmij.org/ftp/tagless-final/Algebra.html#init">Initial Algebra</a><br>

The initial algebra construction using first-class functors,
in the case of one-sorted algebras (corresponding to untyped DSLs)

</p><p>Stephen Dolan: phantom type. Message on the caml-list
posted on Mon, 27 Apr 2015 12:51:11 +0100
</p></dd></dl>


<h2><a name="hkname">What's in a higher-kinded type name</a></h2>
<dl>
<dd>We now look back at Yallop and White's approach of reducing
higher-kinded polymorphism to the ordinary polymorphism, from a
different perspective. It gives if not a new insight, at least new
implementations.
<p>A polymorphic type like <code>'a list</code> represents a family of types,
indexed by a type (of list elements, in this example). A higher-kinded
type abstraction such as <code>'a 'F</code> with the hypothetical (in OCaml)
higher-kind type variable <code>'F</code> is the abstraction over a family name,
so to speak, while still keeping track of the index. Here is another
way of accomplishing such an abstraction. 
</p><p>Consider the existential type <code>exists a. a list</code> (realizable in OCaml
in several ways, although not in the shown notation. We will keep the
notation for clarity). The existential is now the ordinary, rank <code>*</code>
type and can be abstracted in a type variable, e.g., <code>'d</code>. The `family
name' is, hence, the family type with the hidden index. We have
lost track of the index, however. Therefore, we tack it back, ending
up with the type <code>('a,'d) hk</code>. Thus <code>(t,exists a. a list) hk</code> is meant
to be the same as <code>t list</code> (for any type <code>t</code>).
</p><p>There is a problem however: <code>('a,'d) hk</code> is a much bigger type. We
need the condition that in <code>(t,exists a. a list) hk</code>, the index <code>t</code> is
exactly the one that we hid in the existential quantification&nbsp;-- we
need dependent pairs, not supported in OCaml. Remember the old trick,
however: we may have a bigger type so long as we control the producers
of its values and ensure only the values satisfying the condition are
built. To be concrete, we must make certain that the only way to
produce <code>('a,'d) hk</code> values is by using functions like <code>inj: 'a list -&gt; ('a, exists a. a list) hk</code> that expose the same index they hide.
At some point the type checker will demand a
proof: when implementing the
inverse mapping <code>('a, exists a. a list) hk -&gt; 'a list</code> and extracting
the list out of the existential. There are several ways of going about
the proof.
</p><p>The simplest is to give our word&nbsp;-- that the condition always holds for
all <code>('a,'d) hk</code> values actually produced, and we have a proof of that
on some piece of paper or in a <code>.v</code> file. This leads to the exceptionally
simple implementation, which does nothing at all
(all of its operations are the identity).

</p><pre>    module HK : sig
      type ('a,'d) hk                       (* abstract *)
      module MakeHK : functor (S: sig type 'a t end) -&gt; sig
        type anyt                           (* also abstract *)
        val inj : 'a S.t -&gt; ('a,anyt) hk
        val prj : ('a,anyt) hk -&gt; 'a S.t
      end
     end = struct
      type ('a,'d) hk = 'd
      module MakeHK(S:sig type 'a t end) = struct
        type anyt = Obj.t
        let inj : 'a S.t -&gt; ('a,anyt) hk = Obj.repr
        let prj : ('a,anyt) hk -&gt; 'a S.t = Obj.obj
      end
    end
</pre>The accompanying code shows a different, also quite simple
implementation without any <code>Obj</code> magic.
<p>After enriching the <code>sym</code> signature of the DSL from the previous
section with fake higher-kinded types:

</p><pre>    module type sym_hk = sig
      include sym
      include module type of HK.MakeHK(struct type 'a t = 'a repr end)
    end
</pre>we can write the earlier <code>SymEx1</code> example as a function (a term)
rather than a functor:

<pre>    let sym_ex1 (type d) (module I:(sym_hk with type anyt=d)) : (_,d) HK.hk =
      let open I in
      let t1 = add (add (int 1) (int 2)) (int 3) |&gt; inj in (* intermediate term *)
      let res = if_ (iszero t1) (int 0) (add t1 (int 1)) in
      inj res
</pre>It can be evaluated simply as

<pre>    sym_ex1 (module RHK) |&gt; RHK.prj
</pre>where <code>RHK</code> is a module implementing <code>sym_hk</code>. Incidentally, if <code>SHK</code>
is another module implementing <code>sym_hk</code> and we attempt
<code>sym_ex1 (module RHK) |&gt; SHK.prj</code>, we discover that
<code>(int,RHK.anyt) bk</code> and <code>(int,SHK.anyt) bk</code> are actually different
types. Although <code>HK</code> does not do anything (at runtime), it does
maintain safety and soundness.</dd>
<dt><strong>References</strong></dt>
<dd><a href="https://okmij.org/ftp/ML/HKPoly_tf.ml">HKPoly_tf.ml</a>&nbsp;[13K]<br>

The complete code with tests and detailed development
</dd></dl>


<h2><a name="conc">Conclusions</a></h2>
<dl>
<dd>We have surveyed various ways of abstracting over a type
constructor&nbsp;-- or, writing interface-parameterized terms
when the interface involves a polymorphic type. Even if the
language does not support type-constructor--polymorphism directly,
such interface parameterization can still be realized, as:

<ul>
<li>interface abstraction as a functor abstraction
</li>
<li>reducing higher-kinded polymorphism to ordinary polymorphism,
by establishing a bijection between type constructors and
ordinary types
</li>
<li>hiding the polymorphism over DSL implementations
behind initial algebra
(if the interface is algebraic, as often happens in tagless-final
DSL embeddings)
</li>
<li>and in some problems, higher-kinded polymorphism is not actually
needed, on close inspection
</li></ul></dd></dl>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A football/soccer pass visualizer made with Three.js (162 pts)]]></title>
            <link>https://statsbomb-3d-viz.vercel.app/</link>
            <guid>41095839</guid>
            <pubDate>Sun, 28 Jul 2024 20:45:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statsbomb-3d-viz.vercel.app/">https://statsbomb-3d-viz.vercel.app/</a>, See on <a href="https://news.ycombinator.com/item?id=41095839">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Don't blindly prefer `emplace_back` to `push_back` (128 pts)]]></title>
            <link>https://quuxplusone.github.io/blog/2021/03/03/push-back-emplace-back/</link>
            <guid>41095814</guid>
            <pubDate>Sun, 28 Jul 2024 20:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quuxplusone.github.io/blog/2021/03/03/push-back-emplace-back/">https://quuxplusone.github.io/blog/2021/03/03/push-back-emplace-back/</a>, See on <a href="https://news.ycombinator.com/item?id=41095814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>In one of my recent training courses, a student informed me that both clang-tidy
and PVS-Studio were complaining about some code of the form</p>

<div><pre><code>std::vector&lt;Widget&gt; widgets;
~~~
widgets.push_back(Widget(foo, bar, baz));
</code></pre></div>

<p>Both tools flagged this line as “bad style.”
clang-tidy even offered a (SARCASM ALERT) helpful fixit:</p>

<div><pre><code>warning: use emplace_back instead of push_back [modernize-use-emplace]
    widgets.push_back(Widget(foo, bar, baz));
            ^~~~~~~~~~~~~~~~~             ~
            emplace_back(
</code></pre></div>

<p>The student dutifully changed the line, and both tools reported their
satisfaction with the replacement:</p>

<div><pre><code>widgets.emplace_back(Widget(foo, bar, baz));
</code></pre></div>

<p>The original line materializes a temporary <code>Widget</code> object on the stack;
takes an rvalue reference to it; and passes that reference to
<code>vector&lt;Widget&gt;::push_back(Widget&amp;&amp;)</code>, which move-constructs a <code>Widget</code>
into the vector. Then we destroy the temporary.</p>

<p>The student’s replacement materializes a temporary <code>Widget</code> object on the stack;
takes an rvalue reference to it; and passes that reference to
<code>vector&lt;Widget&gt;::emplace_back&lt;Widget&gt;(Widget&amp;&amp;)</code>, which move-constructs
a <code>Widget</code> into the vector. Then we destroy the temporary.</p>

<p><em>Absolutely no difference.</em></p>

<p>The change clang-tidy meant to suggest — and in fact <em>did</em> suggest,
if you pay very close attention to the underlining in the fixit — was actually this:</p>

<div><pre><code>widgets.emplace_back(foo, bar, baz);
</code></pre></div>

<p>This version does <em>not</em> materialize any <code>Widget</code> temporaries. It simply
passes <code>foo, bar, baz</code> to <code>vector&lt;Widget&gt;::emplace_back&lt;Foo&amp;, Bar&amp;, Baz&amp;&gt;(Foo&amp;, Bar&amp;, Baz&amp;)</code>,
which constructs a <code>Widget</code> into the vector using whatever
constructor of <code>Widget</code> best matches that bunch of arguments.</p>

<h2 id="emplace_back-is-not-magic-c11-pixie-dust"><code>emplace_back</code> is not magic C++11 pixie dust</h2>

<p>Even a decade after C++11 was released, I still sometimes see programmers assume
that <code>emplace_back</code> is somehow related to move semantics. (In the same way that
some programmers assume lambdas are somehow the same thing as <code>std::function</code>,
you know?) For example, they’ll rightly observe that this code makes an
unnecessary copy:</p>

<div><pre><code>void example() {
    auto w = Widget(1,2,3);
    widgets.push_back(w);  // Copy-constructor alert!
}
</code></pre></div>

<p>So they’ll change it to this:</p>

<div><pre><code>void example() {
    auto w = Widget(1,2,3);
    widgets.emplace_back(w);  // Fixed? Nope!
}
</code></pre></div>

<p>The original line constructs a <code>Widget</code> object into <code>w</code>, then
passes <code>w</code> by reference to <code>vector&lt;Widget&gt;::push_back(const Widget&amp;)</code>,
which copy-constructs a <code>Widget</code> into the vector.</p>

<p>The replacement constructs a <code>Widget</code> object into <code>w</code>, then
passes <code>w</code> by reference to <code>vector&lt;Widget&gt;::emplace_back&lt;Widget&amp;&gt;(Widget&amp;)</code>,
which copy-constructs a <code>Widget</code> into the vector.</p>

<p><em>Absolutely no difference.</em></p>

<p>What the student should have done is ask the compiler to make an
<em>rvalue</em> reference to <code>w</code>, by saying either</p>

<div><pre><code>widgets.push_back(std::move(w));
</code></pre></div>

<p>or</p>

<div><pre><code>widgets.emplace_back(std::move(w));
</code></pre></div>

<p>It doesn’t matter which verb you use; what matters is the value category of
<code>w</code>. You must explicitly mention <code>std::move</code>, so that the language (and the
human reader) understand that you’re done using <code>w</code> and it’s okay for
<code>widgets</code> to pilfer its guts.</p>

<p><code>emplace_back</code> was added to the language at the same time as <code>std::move</code> — just
like lambdas were added at the same time as <code>std::function</code> — but that doesn’t
make them the same thing. <code>emplace_back</code> may “look more C++11-ish,” but it’s
not magic move-enabling pixie dust and it will never insert a move in a place
you don’t explicitly request one.</p>

<h2 id="when-all-else-is-equal-prefer-push_back-to-emplace_back">When all else is equal, prefer <code>push_back</code> to <code>emplace_back</code></h2>

<p>So, given that these two lines do the same thing and are equally efficient
at runtime, which should I prefer, stylistically?</p>

<div><pre><code>widgets.push_back(std::move(w));
widgets.emplace_back(std::move(w));
</code></pre></div>

<p>I recommend sticking with <code>push_back</code> for day-to-day use. You should definitely
use <code>emplace_back</code> when you need its particular set of skills — for example, <code>emplace_back</code>
is your only option when dealing with a <code>deque&lt;mutex&gt;</code> or other non-movable type —
but <code>push_back</code> is the appropriate default.</p>

<p>One reason is that <code>emplace_back</code> is more work for the compiler.
<code>push_back</code> is an overload set of two non-template member functions.
<code>emplace_back</code> is a single variadic template.</p>

<div><pre><code>void push_back(const Widget&amp;);
void push_back(Widget&amp;&amp;);

template&lt;class... Ts&gt;
reference emplace_back(Ts&amp;&amp;...);
</code></pre></div>

<p>When you call <code>push_back</code>, the compiler must do overload resolution, but that’s all.
When you call <code>emplace_back</code>, the compiler must do template type deduction, followed
by (easy-peasy) overload resolution, followed by function template instantiation and
code generation. That’s a much larger amount of work for the compiler.</p>

<h2 id="the-benchmark-program">The benchmark program</h2>

<p>I wrote a simple test program to demonstrate the difference in compiler workload.
Of course <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s Law</a> applies:
my benchmark displays a massive difference because it’s doing <em>nothing but</em>
instantiating <code>emplace_back</code>, whereas any production codebase will be doing vastly
more other stuff relative to the number of times it instantiates <code>emplace_back</code>.
Still, I hope this benchmark gives you a sense of why I recommend “<code>push_back</code> over
<code>emplace_back</code>” and not vice versa.</p>

<p>This Python 3 script generates the benchmark:</p>

<div><pre><code>import sys
print('#include &lt;vector&gt;')
print('#include &lt;string&gt;')
print('extern std::vector&lt;std::string&gt; v;')
for i in range(1000):
    print('void test%d() {' % i)
    print('    v.%s_back("%s");' % (sys.argv[1], 'A' * i))
    print('}')
</code></pre></div>

<p>Generate like this:</p>

<div><pre><code>python generate.py push &gt;push.cpp
python generate.py emplace &gt;emplace.cpp
time g++ -c push.cpp
time g++ -c emplace.cpp
</code></pre></div>

<p>With Clang trunk on my laptop, I get consistently about 1.0s for the <code>push</code> version,
and 4.2s for the <code>emplace</code> version. This big difference is due to the fact that the
<code>push</code> version is merely code-generating a thousand <code>test</code> functions, whereas
the <code>emplace</code> version is code-generating that same thousand <code>test</code> functions <em>and</em>
another thousand template instantiations of <code>emplace_back</code> with different parameter
types:</p>

<div><pre><code>vector&lt;string&gt;::emplace_back&lt;const char(&amp;)[1]&gt;(const char (&amp;)[1])
vector&lt;string&gt;::emplace_back&lt;const char(&amp;)[2]&gt;(const char (&amp;)[2])
vector&lt;string&gt;::emplace_back&lt;const char(&amp;)[3]&gt;(const char (&amp;)[3])
vector&lt;string&gt;::emplace_back&lt;const char(&amp;)[4]&gt;(const char (&amp;)[4])
~~~
</code></pre></div>

<p>See, <code>push_back</code> knows that it expects a <code>string&amp;&amp;</code>, and so it knows to call the
non-explicit constructor <code>string(const char *)</code> on the caller’s side. The same
constructor is called in each case, and the temporary <code>string</code> is passed to
the same overload of <code>push_back</code> in each case. <code>emplace_back</code>, on the other hand,
is a dumb perfect-forwarding template: it doesn’t know that the relevant constructor
overload will end up being <code>string(const char *)</code> in each case. So it takes
an lvalue reference to the specific <em>array type</em> being passed by the caller.
Perfect-forwarding has no special cases for <code>const char *</code>!</p>

<p>If we change <code>vector&lt;string&gt;</code> to <code>vector&lt;const char *&gt;</code>, the compile-time-performance
gap widens: now it’s 0.7s for <code>push</code>, 3.8s for <code>emplace</code>. This is because we’ve cut
out some of the work that was common to both versions (constructing <code>std::string</code> objects)
without affecting the source of the gap (that one version instantiates a
thousand copies of <code>emplace_back</code> and the other doesn’t). Amdahl’s Law in action!</p>

<p>My conclusions:</p>

<blockquote>
  <p>Use <code>push_back</code> by default.</p>
</blockquote>

<blockquote>
  <p>Use <code>emplace_back</code> where it is semantically significant to your algorithm
(such as when the element type’s move-constructor is absent or has been
benchmarked as expensive).</p>
</blockquote>

<blockquote>
  <p>Avoid mixing string literals and perfect-forwarding templates,
especially in repetitive machine-generated code.</p>
</blockquote>

<hr>

<p>Previously on this blog:</p>

<ul>
  <li><a href="https://quuxplusone.github.io/blog/2018/06/26/cost-of-static-lifetime-constructors/">“The surprisingly high cost of static-lifetime constructors”</a> (2018-06-26)</li>
</ul>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[tolower() with AVX-512 (216 pts)]]></title>
            <link>https://dotat.at/@/2024-07-28-tolower-avx512.html</link>
            <guid>41095790</guid>
            <pubDate>Sun, 28 Jul 2024 20:38:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dotat.at/@/2024-07-28-tolower-avx512.html">https://dotat.at/@/2024-07-28-tolower-avx512.html</a>, See on <a href="https://news.ycombinator.com/item?id=41095790">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>A couple of years ago I wrote about <a href="https://dotat.at/@/2022-06-27-tolower-swar.html">tolower() in bulk at speed using
SWAR tricks</a>. A couple of days ago I was interested by Olivier
Giniaux’s article about <a href="https://ogxd.github.io/articles/unsafe-read-beyond-of-death/">unsafe read beyond of death</a>, an
optimization for handling small strings with SIMD instructions, for a
fast hash function written in Rust.</p>
<p>I’ve long been annoyed that SIMD instructions can easily eat short
strings whole, but it’s irritatingly difficult to transfer short
strings between memory and vector registers. Oliver’s post caught my
eye because it seemed like a fun way to avoid the problem, at least
for loads. (Stores remain awkward!)</p>
<p>Actually, to be frank, Olivier nerdsniped me.</p>
<ul>
<li><a href="#signs-of-hope">signs of hope</a></li>
<li><a href="#tolower64-">tolower64()</a></li>
<li><a href="#bulk-load-and-store">bulk load and store</a></li>
<li><a href="#masked-load-and-store">masked load and store</a></li>
<li><a href="#benchmarking">benchmarking</a></li>
<li><a href="#conclusion">conclusion</a></li>
</ul>
<h2><a name="signs-of-hope" href="#signs-of-hope">signs of hope</a></h2>
<p>Reading more around the topic, I learned that <em>some</em> SIMD instruction
sets do, in fact, have useful masked loads and stores that are
suitable for string processing, that is, they have byte granularity.
They are:</p>
<ul>
<li>
<p>ARM SVE, which is available on recent big-ARM Neoverse cores, such
as Amazon Graviton, but not Apple Silicon.</p>
</li>
<li>
<p>AVX-512-BW, the bytes and words extension, which is available on
recent AMD Zen processors. AVX-512 is a complicated mess of
extensions that might or might not be available; support on Intel
is particularly random.</p>
</li>
</ul>
<p>I have an AMD Zen 4 box, so I thought I would try a little AVX-512-BW.</p>
<h2><a name="tolower64-" href="#tolower64-">tolower64()</a></h2>
<p>Using the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">Intel intrinsics guide</a> I wrote a basic <code>tolower()</code>
function that can munch 64 bytes at once.</p>
<p>Top tip: You can use <code>*</code> as a wildcard in the search box, so I made
heavy use of <code>mm512*epi8</code> to find byte-wise AVX-512 functions (<code>epi8</code>
is an obscure alias for byte).</p>
<p>First, we fill a few registers with 64 copies of some handy bytes.</p>
<p>We need the letters A and Z:</p>
<pre><code>    __m512i A = _mm512_set1_epi8('A');
    __m512i Z = _mm512_set1_epi8('Z');
</code></pre>
<p>We need a number to add to uppercase letters to make them lowercase:</p>
<pre><code>    __m512i to_upper = _mm512_set1_epi8('a' - 'A');
</code></pre>
<p>We compare our input characters <code>c</code> with A and Z. The result of each
comparison is a 64 bit mask which has bits set for the bytes where
the comparison is true:</p>
<pre><code>    __mmask64 ge_A = _mm512_cmpge_epi8_mask(c, A);
    __mmask64 le_Z = _mm512_cmple_epi8_mask(c, Z);
</code></pre>
<p>If it’s greater than or equal to A, <em>and</em> less than or equal to Z,
then it is upper case. (AVX mask registers have names beginning with
<code>k</code>.)</p>
<pre><code>    __mmask64 is_upper = _kand_mask64(ge_A, le_Z);
</code></pre>
<p>Finally, we do a masked add. We pass <code>c</code> twice: bytes from the first
<code>c</code> are copied to the result when <code>is_upper</code> is false, and when
<code>is_upper</code> is true the result is <code>c + to_upper</code>.</p>
<pre><code>    return  _mm512_mask_add_epi8(c, is_upper, c, to_upper);
</code></pre>
<h2><a name="bulk-load-and-store" href="#bulk-load-and-store">bulk load and store</a></h2>
<p>The <code>tolower64()</code> kernel in the previous section needs to be wrapped
up in more convenient functions such as copying a string while
converting it to lower case.</p>
<p>For long strings, the bulk of the work uses unaligned vector load and
store instructions:</p>
<pre><code>	__m512i src_vec = _mm512_loadu_epi8(src_ptr);
	__m512i dst_vec = tolower64(src_vec);
	_mm512_storeu_epi8(dst_ptr, dst_vec);
</code></pre>
<h2><a name="masked-load-and-store" href="#masked-load-and-store">masked load and store</a></h2>
<p>Small strings and the stub end of long strings use masked unaligned
loads and stores.</p>
<p><strong>This is the magic!</strong> Here is the reason I wrote this blog post!</p>
<p>The mask has its lowest <code>len</code> bits set (its first <code>len</code> bits in
little-endian order). I wrote these two lines with perhaps more
ceremony than required, but I thought it was helpful to indicate that
the mask is not any old 64 bit integer: it has to be loaded into one
of the SIMD unit’s mask registers.</p>
<pre><code>	uint64_t len_bits = (~0ULL) &gt;&gt; (64 - len);
	__mmask64 len_mask =  _cvtu64_mask64(len_bits);
</code></pre>
<p>The load and store look fairly similar to the full-width versions, but
with the mask stuff added. The <code>z</code> in <code>maskz</code> means zero the
destination register when the mask is clear, as opposed to copying
from another register (like in <code>mask_add</code> above).</p>
<pre><code>	__m512i src_vec = _mm512_maskz_loadu_epi8(len_mask, src_ptr);
	__m512i dst_vec = tolower64(src_vec);
	_mm512_mask_storeu_epi8(dst_ptr, len_mask, dst_vec);
</code></pre>
<p>That’s the essence of it: you can <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower64.c">see the complete version of
<code>copytolower64()</code></a> in my git repository.</p>
<h2><a name="benchmarking" href="#benchmarking">benchmarking</a></h2>
<p>To see how well it works, I benchmarked several similar functions.
Here’s a chart of the results, compiled with Clang 16 on Debian 11,
and run on an AMD Ryzen 9 7950X.</p>
<p>The benchmark measures the time to copy about 1 MiByte, in chunks of
various lengths from 1 byte to 1 kilobyte. I wanted to take into
account differences in alignment in the source and destination
strings, so there are a few bytes between each source and destination
string, which are not counted as part of the megabyte.</p>
<p>On this CPU the L2 cache is 1 MiB per core, so I expect each run of
the test spills into the L3 cache.</p>
<p>To be sure I was measuring what I thought I was, I compiled each
function separately to avoid interference from inlining and code
motion. In real code it’s more likely that you would want to encourage
inlining, not prevent it!</p>
<p><img src="https://dotat.at/@/2024-07-vectolower.svg" alt="benchmark results"></p>
<ul>
<li>
<p>The pink <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower64.c"><code>tolower64</code></a> line is the code described in this blog post.
It is consistently near the fastest of all the functions under
test. (It drops a little at 65 bytes long, where it spills into a
second vector.)</p>
<p>The interesting feature of the line for my purposes is that it
rises fast and lacks deep troughs, showing that the masked loads
and stores were effective at handling small string fragments
quickly.</p>
</li>
<li>
<p>The green <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copybytes64.c"><code>copybytes64</code></a> line is a version of <code>memcpy</code> using
AVX-512 in a similar manner to <code>tolower64</code>. It is (maybe
surprisingly) not much faster. I had to compile <code>copybytes64</code> with
Clang 11 because more recent versions are able to recognise what
the function does and rewrite it completely.</p>
</li>
<li>
<p>The orange <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copybytes1.c"><code>copybytes1</code></a> line is a byte-by-byte version of <code>memcpy</code>
again compiled using Clang 11. It illustrates that Clang 11 had
relatively poor autovectorizer heuristics and was pretty bad for
string fragments less than 256 bytes long.</p>
</li>
<li>
<p>The very slow red <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower.c"><code>tolower</code></a> line calls the standard
<code>tolower()</code> from <code>&lt;ctype.h&gt;</code> to provide a baseline.</p>
</li>
<li>
<p>The purple <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower1.c"><code>tolower1</code></a> line is a simple byte-by-byte version of
<code>tolower()</code> compiled with Clang 16. It shows that Clang 16 has a
much better autovectorizer than Clang 11, but it is slower and
<a href="https://godbolt.org/z/9dPMo8h8a">much more complicated</a> than my
hand-written version. It is very spiky because the autovectorizer
did not handle short string fragments as well as <code>tolower64</code> does.</p>
</li>
<li>
<p>The brown <a href="https://dotat.at/cgi/git/vectolower.git/blob/HEAD:/copytolower8.c"><code>tolower8</code></a> line is <a href="https://dotat.at/@/2022-06-27-tolower-swar.html">the SWAR <code>tolower()</code> from my
previous blog post</a>. Clang valiantly tries to autovectorize
it, but the result is not great because the function is too
complicated. (It has the Clang-11-style 256-byte performance
cliffs despite being compiled with Clang 16.)</p>
</li>
<li>
<p>The blue <code>memcpy</code> line calls glibc’s <code>memcpy</code>. There’s something
curious going on here: it starts off fast but drops off to about
half the speed of <code>copybytes64</code>. Dunno why!</p>
</li>
</ul>
<h2><a name="conclusion" href="#conclusion">conclusion</a></h2>
<p>So, AVX-512-BW is <em>very nice indeed</em> for working with strings,
especially short strings. On Zen 4 it’s very fast, and the intrinsic
functions are reasonably easy to use.</p>
<p>The most notable thing is AVX-512-BW’s smooth performance: there’s
very little sign of the performance troughs that the autovectorizer
suffers from as it shifts to scalar code for small string fragments.</p>
<p>I don’t have convenient access to an ARM box with SVE support, so I
have not investigated it in detail. It’ll be interesting to see how
well SVE works for short strings.</p>
<p>I would like both of these instruction set extensions to be much more
widely available. They should improve the performance of string
handling tremendously.</p>
<p><a href="https://dotat.at/cgi/git/vectolower.git">The code for this blog post is available from my web site.</a></p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft technical breakdown of CrowdStrike incident (359 pts)]]></title>
            <link>https://www.microsoft.com/en-us/security/blog/2024/07/27/windows-security-best-practices-for-integrating-and-managing-security-tools/</link>
            <guid>41095530</guid>
            <pubDate>Sun, 28 Jul 2024 19:55:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.microsoft.com/en-us/security/blog/2024/07/27/windows-security-best-practices-for-integrating-and-managing-security-tools/">https://www.microsoft.com/en-us/security/blog/2024/07/27/windows-security-best-practices-for-integrating-and-managing-security-tools/</a>, See on <a href="https://news.ycombinator.com/item?id=41095530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
<p>Windows is an open and flexible platform used by many of the world’s top businesses for high availability use cases where security and availability are non-negotiable.</p>



<p>To meet those needs:</p>



<ol>
<li>Windows provides a range of operating modes that customers can choose from. This includes the ability to limit what can run to only approved software and drivers. This can increase security and reliability by making Windows operate in a mode closer to mobile phones or appliances.</li>



<li>Customers can choose integrated security monitoring and detection capabilities that are included with Windows. Or they can choose to replace or supplement this security with a wide variety of choices from a vibrant open ecosystem of vendors.</li>
</ol>



<p>In this blog post, we examine the recent CrowdStrike outage and provide a technical overview of the root cause. We also explain why security products use kernel-mode drivers today and the safety measures Windows provides for third-party solutions. In addition, we share how customers and security vendors can better leverage the integrated security capabilities of Windows for increased security and reliability. Lastly, we provide a look into how Windows will enhance extensibility for future security products.</p>



<p>CrowdStrike recently published a <a href="https://www.crowdstrike.com/falcon-content-update-remediation-and-guidance-hub/" target="_blank" rel="noreferrer noopener">Preliminary Post Incident Review</a> analyzing their outage. In their blog post, CrowdStrike describes the root cause as a memory safety issue—specifically a read out-of-bounds access violation in the CSagent driver. We leverage the Microsoft <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/debugger/debugger-download-tools" target="_blank" rel="noreferrer noopener">WinDBG Kernel Debugger</a> and <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/debuggercmds/-reg" target="_blank" rel="noreferrer noopener">several extensions</a> that are available free to anyone to perform this analysis. Customers with crash dumps can reproduce our steps with these tools.</p>



<p>Based on Microsoft’s analysis of the Windows Error Reporting (<a href="https://learn.microsoft.com/en-us/windows/win32/wer/windows-error-reporting" target="_blank" rel="noreferrer noopener">WER</a>) kernel crash dumps related to the incident, we observe global crash patterns that reflect this:</p>


<div><pre title="">FAULTING_THREAD:  ffffe402fe868040

READ_ADDRESS:  ffff840500000074 Paged pool

MM_INTERNAL_CODE:  2

IMAGE_NAME:  csagent.sys

MODULE_NAME: csagent

FAULTING_MODULE: fffff80671430000 csagent

PROCESS_NAME:  System

TRAP_FRAME:  ffff94058305ec20 -- (.trap 0xffff94058305ec20)
.trap 0xffff94058305ec20
NOTE: The trap frame does not contain all registers.
Some register values may be zeroed or incorrect.
rax=ffff94058305f200 rbx=0000000000000000 rcx=0000000000000003
rdx=ffff94058305f1d0 rsi=0000000000000000 rdi=0000000000000000
rip=fffff806715114ed rsp=ffff94058305edb0 rbp=ffff94058305eeb0
 r8=ffff840500000074  r9=0000000000000000 r10=0000000000000000
r11=0000000000000014 r12=0000000000000000 r13=0000000000000000
r14=0000000000000000 r15=0000000000000000
iopl=0         nv up ei ng nz na po nc
csagent+0xe14ed:
fffff806`715114ed 458b08          mov     r9d,dword ptr [r8] ds:ffff8405`00000074=????????
.trap
Resetting default scope

STACK_TEXT:  
ffff9405`8305e9f8 fffff806`5388c1e4     : 00000000`00000050 ffff8405`00000074 00000000`00000000 ffff9405`8305ec20 : nt!KeBugCheckEx 
ffff9405`8305ea00 fffff806`53662d8c     : 00000000`00000000 00000000`00000000 00000000`00000000 ffff8405`00000074 : nt!MiSystemFault+0x1fcf94  
ffff9405`8305eb00 fffff806`53827529     : ffffffff`00000030 ffff8405`af8351a2 ffff9405`8305f020 ffff9405`8305f020 : nt!MmAccessFault+0x29c 
ffff9405`8305ec20 fffff806`715114ed     : 00000000`00000000 ffff9405`8305eeb0 ffff8405`b0bcd00c ffff8405`b0bc505c : nt!KiPageFault+0x369 
ffff9405`8305edb0 fffff806`714e709e     : 00000000`00000000 00000000`e01f008d ffff9405`8305f102 fffff806`716baaf8 : csagent+0xe14ed
ffff9405`8305ef50 fffff806`714e8335     : 00000000`00000000 00000000`00000010 00000000`00000002 ffff8405`b0bc501c : csagent+0xb709e
ffff9405`8305f080 fffff806`717220c7     : 00000000`00000000 00000000`00000000 ffff9405`8305f382 00000000`00000000 : csagent+0xb8335
ffff9405`8305f1b0 fffff806`7171ec44     : ffff9405`8305f668 fffff806`53eac2b0 ffff8405`afad4ac0 00000000`00000003 : csagent+0x2f20c7
ffff9405`8305f430 fffff806`71497a31     : 00000000`0000303b ffff9405`8305f6f0 ffff8405`afb1d140 ffffe402`ff251098 : csagent+0x2eec44
ffff9405`8305f5f0 fffff806`71496aee     : ffff8405`afb1d140 fffff806`71541e7e 00000000`000067a0 fffff806`7168f8f0 : csagent+0x67a31
ffff9405`8305f760 fffff806`7149685b     : ffff9405`8305f9d8 ffff8405`afb1d230 ffff8405`afb1d140 ffffe402`fe8644f8 : csagent+0x66aee
ffff9405`8305f7d0 fffff806`715399ea     : 00000000`4a8415aa ffff8eee`1c68ca4f 00000000`00000000 ffff8405`9e95fc30 : csagent+0x6685b
ffff9405`8305f850 fffff806`7148efbb     : 00000000`00000000 ffff9405`8305fa59 ffffe402`fe864050 ffffe402`fede62c0 : csagent+0x1099ea
ffff9405`8305f980 fffff806`7148edd7     : ffffffff`ffffffa1 fffff806`7152e5c1 ffffe402`fe864050 00000000`00000001 : csagent+0x5efbb
ffff9405`8305fac0 fffff806`7152e681     : 00000000`00000000 fffff806`53789272 00000000`00000002 ffffe402`fede62c0 : csagent+0x5edd7
ffff9405`8305faf0 fffff806`53707287     : ffffe402`fe868040 00000000`00000080 fffff806`7152e510 006fe47f`b19bbdff : csagent+0xfe681
ffff9405`8305fb30 fffff806`5381b8e4     : ffff9680`37651180 ffffe402`fe868040 fffff806`53707230 00000000`00000000 : nt!PspSystemThreadStartup+0x57 
ffff9405`8305fb80 00000000`00000000     : ffff9405`83060000 ffff9405`83059000 00000000`00000000 00000000`00000000 : nt!KiStartSystemThread+0x34 

</pre></div>


<p>Digging in more to this crash dump, we can restore the stack frame at the time of the access violation to learn more about its origin. Unfortunately, with WER data we only receive a compressed version of state and thus we cannot disassemble backwards to see a larger set of instructions prior to the crash, but we can see in the disassembly that there is a check for NULL before performing a read at the address specified in the R8 register:</p>


<div><pre title="">6: kd&gt; .trap 0xffff94058305ec20
.trap 0xffff94058305ec20
NOTE: The trap frame does not contain all registers.
Some register values may be zeroed or incorrect.
rax=ffff94058305f200 rbx=0000000000000000 rcx=0000000000000003
rdx=ffff94058305f1d0 rsi=0000000000000000 rdi=0000000000000000
rip=fffff806715114ed rsp=ffff94058305edb0 rbp=ffff94058305eeb0
 r8=ffff840500000074  r9=0000000000000000 r10=0000000000000000
r11=0000000000000014 r12=0000000000000000 r13=0000000000000000
r14=0000000000000000 r15=000000000000
000
iopl=0         nv up ei ng nz na po nc
csagent+0xe14ed:
fffff806`715114ed 458b08          mov     r9d,dword ptr [r8] ds:ffff8405`00000074=????????
6: kd&gt; !pte ffff840500000074
!pte ffff840500000074
                                           VA ffff840500000074
PXE at FFFFABD5EAF57840    PPE at FFFFABD5EAF080A0    PDE at FFFFABD5E1014000    PTE at FFFFABC202800000
contains 0A00000277200863  contains 0000000000000000
pfn 277200    ---DA--KWEV  contains 0000000000000000
not valid

6: kd&gt; ub fffff806`715114ed
ub fffff806`715114ed
csagent+0xe14d9:
fffff806`715114d9 04d8            add     al,0D8h
fffff806`715114db 750b            jne     csagent+0xe14e8 (fffff806`715114e8)
fffff806`715114dd 4d85c0          test    r8,r8
fffff806`715114e0 7412            je      csagent+0xe14f4 (fffff806`715114f4)
fffff806`715114e2 450fb708        movzx   r9d,word ptr [r8]
fffff806`715114e6 eb08            jmp     csagent+0xe14f0 (fffff806`715114f0)
fffff806`715114e8 4d85c0          test    r8,r8
fffff806`715114eb 7407            je      csagent+0xe14f4 (fffff806`715114f4)
6: kd&gt; ub fffff806`715114d9
ub fffff806`715114d9
                          ^ Unable to find valid previous instruction for 'ub fffff806`715114d9'
6: kd&gt; u fffff806`715114eb
u fffff806`715114eb
csagent+0xe14eb:
fffff806`715114eb 7407            je      csagent+0xe14f4 (fffff806`715114f4)
fffff806`715114ed 458b08          mov     r9d,dword ptr [r8]
fffff806`715114f0 4d8b5008        mov     r10,qword ptr [r8+8]
fffff806`715114f4 4d8bc2          mov     r8,r10
fffff806`715114f7 488d4d90        lea     rcx,[rbp-70h]
fffff806`715114fb 488bd6          mov     rdx,rsi
fffff806`715114fe e8212c0000      call    csagent+0xe4124 (fffff806`71514124)
fffff806`71511503 4533d2          xor     r10d,r10d

6: kd&gt; db ffff840500000074
db ffff840500000074
ffff8405`00000074  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`00000084  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`00000094  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000a4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000b4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000c4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000d4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????
ffff8405`000000e4  ?? ?? ?? ?? ?? ?? ?? ??-?? ?? ?? ?? ?? ?? ?? ??  ????????????????

</pre></div>


<p>Our observations confirm CrowdStrike’s analysis that this was a read-out-of-bounds memory safety error in the CrowdStrike developed CSagent.sys driver.</p>



<p>We can also see that the csagent.sys module is registered as a <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/ifs/flt-parameters-for-irp-mj-create-named-pipe" target="_blank" rel="noreferrer noopener">file system filter driver</a> commonly used by anti-malware agents to receive notifications about file operations such as the creation or modification of a file. This is often used by security products to scan any new file saved to disk, such as downloading a file via the browser.</p>



<p>File System filters can also be used as a signal for security solutions attempting to monitor the behavior of the system. CrowdStrike noted in their blog that part of their content update was changing the sensor’s logic relating to data around named pipe creation. The File System filter driver API allows the driver to receive a call when named pipe activity (e.g., named pipe creation) occurs on the system that could enable the detection of malicious behavior. The general function of the driver correlates to the information shared by CrowdStrike.</p>


<div><pre title="">6: kd&gt;!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csagent

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f67f9c

[SubKeyAddr]         [SubKeyName]
ffff8405a6f683ac     Instances
ffff8405a6f6854c     Sim

 Use '!reg keyinfo ffff84059ca7b000 &lt;SubKeyAddr&gt;' to dump the subkey details

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          2
REG_DWORD           Start                         1
REG_DWORD           ErrorControl                  1
REG_EXPAND_SZ       ImagePath                     \??\C:\Windows\system32\drivers\CrowdStrike\csagent.sys
REG_SZ              DisplayName                   CrowdStrike Falcon
REG_SZ              Group                         FSFilter Activity Monitor
REG_MULTI_SZ        DependOnService               FltMgr\0
REG_SZ              CNFG                          Config.sys
REG_DWORD           SupportedFeatures             f

</pre></div>


<p>We can see the control channel file version 291 specified in the CrowdStrike analysis is also present in the crash indicating the file was read.</p>



<p>Determining how the file itself correlates to the access violation observed in the crash dump would require additional debugging of the driver using these tools but is outside of the scope of this blog post.</p>


<div><pre title="">!ca ffffde8a870a8290

ControlArea  @ ffffde8a870a8290
  Segment      ffff880ce0689c10  Flink      ffffde8a87267718  Blink        ffffde8a870a7d98
  Section Ref                 0  Pfn Ref                   b  Mapped Views                0
  User Ref                    0  WaitForDel                0  Flush Count                 0
  File Object  ffffde8a879b29a0  ModWriteCount             0  System Views                0
  WritableRefs                0  PartitionId                0  
  Flags (8008080) File WasPurged OnUnusedList 

      \Windows\System32\drivers\CrowdStrike\C-00000291-00000000-00000032.sys

1: kd&gt; !ntfskd.ccb ffff880ce06f6970
!ntfskd.ccb ffff880ce06f6970

   Ccb: ffff880c`e06f6970
 Flags: 00008003 Cleanup OpenAsFile IgnoreCase
Flags2: 00000841 OpenComplete AccessAffectsOplocks SegmentObjectReferenced
  Type: UserFileOpen
FileObj: ffffde8a879b29a0

(018)  ffff880c`db937370  FullFileName [\Windows\System32\drivers\CrowdStrike\C-00000291-00000000-00000032.sys]
(020) 000000000000004C  LastFileNameOffset 
(022) 0000000000000000  EaModificationCount 
(024) 0000000000000000  NextEaOffset 
(048) FFFF880CE06F69F8  Lcb 
(058) 0000000000000002  TypeOfOpen 

</pre></div>


<p>We can leverage the crash dump to determine if any other drivers supplied by CrowdStrike may exist on the running system during the crash.</p>


<div><pre title="">6: kd&gt; lmDvmCSFirmwareAnalysis
lmDvmCSFirmwareAnalysis
Browse full module list
start             end                 module name
fffff806`58920000 fffff806`5893c000   CSFirmwareAnalysis   (deferred)             
    Image path: \SystemRoot\system32\DRIVERS\CSFirmwareAnalysis.sys
    Image name: CSFirmwareAnalysis.sys
    Browse all global symbols  functions  data  Symbol Reload
    Timestamp:        Mon Mar 18 11:32:14 2024 (65F888AE)
    CheckSum:         0002020E
    ImageSize:        0001C000
    Translations:     0000.04b0 0000.04e4 0409.04b0 0409.04e4
    Information from resource tables:
6: kd&gt; lmDvmcspcm4
lmDvmcspcm4
Browse full module list
start             end                 module name
fffff806`71870000 fffff806`7187d000   cspcm4     (deferred)             
    Image path: \??\C:\Windows\system32\drivers\CrowdStrike\cspcm4.sys
    Image name: cspcm4.sys
    Browse all global symbols  functions  data  Symbol Reload
    Timestamp:        Mon Jul  8 18:33:22 2024 (668C9362)
    CheckSum:         00012F69
    ImageSize:        0000D000
    Translations:     0000.04b0 0000.04e4 0409.04b0 0409.04e4
    Information from resource tables:
6: kd&gt; lmDvmcsboot.sys
lmDvmcsboot.sys
Browse full module list
start             end                 module name

Unloaded modules:
fffff806`587d0000 fffff806`587dc000   CSBoot.sys
    Timestamp: unavailable (00000000)
    Checksum:  00000000
    ImageSize:  0000C000

6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csboot
!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csboot

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f68924

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          1
REG_DWORD           Start                         0
REG_DWORD           ErrorControl                  1
REG_EXPAND_SZ       ImagePath                     system32\drivers\CrowdStrike\CSBoot.sys
REG_SZ              DisplayName                   CrowdStrike Falcon Sensor Boot Driver
REG_SZ              Group                         Early-Launch
6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csdevicecontrol
!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csdevicecontrol

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f694ac

[SubKeyAddr]         [VolatileSubKeyName]
ffff84059ce196c4     Enum

 Use '!reg keyinfo ffff84059ca7b000 &lt;SubKeyAddr&gt;' to dump the subkey details

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          1
REG_DWORD           Start                         3
REG_DWORD           ErrorControl                  1
REG_DWORD           Tag                           1f
REG_EXPAND_SZ       ImagePath                     \SystemRoot\System32\drivers\CSDeviceControl.sys
REG_SZ              DisplayName                   @oem40.inf,%DeviceControl.SVCDESC%;CrowdStrike Device Control Service
REG_SZ              Group                         Base
REG_MULTI_SZ        Owners                        oem40.inf\0!csdevicecontrol.inf_amd64_b6725a84d4688d5a\0!csdevicecontrol.inf_amd64_016e965488e83578\0
REG_DWORD           BootFlags                     14
6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csagent
!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csagent

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f67f9c

[SubKeyAddr]         [SubKeyName]
ffff8405a6f683ac     Instances
ffff8405a6f6854c     Sim

 Use '!reg keyinfo ffff84059ca7b000 &lt;SubKeyAddr&gt;' to dump the subkey details

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          2
REG_DWORD           Start                         1
REG_DWORD           ErrorControl                  1
REG_EXPAND_SZ       ImagePath                     \??\C:\Windows\system32\drivers\CrowdStrike\csagent.sys
REG_SZ              DisplayName                   CrowdStrike Falcon
REG_SZ              Group                         FSFilter Activity Monitor
REG_MULTI_SZ        DependOnService               FltMgr\0
REG_SZ              CNFG                          Config.sys
REG_DWORD           SupportedFeatures             f

6: kd&gt; lmDvmCSFirmwareAnalysis
lmDvmCSFirmwareAnalysis
Browse full module list
start             end                 module name
fffff806`58920000 fffff806`5893c000   CSFirmwareAnalysis   (deferred)             
    Image path: \SystemRoot\system32\DRIVERS\CSFirmwareAnalysis.sys
    Image name: CSFirmwareAnalysis.sys
    Browse all global symbols  functions  data  Symbol Reload
    Timestamp:        Mon Mar 18 11:32:14 2024 (65F888AE)
    CheckSum:         0002020E
    ImageSize:        0001C000
    Translations:     0000.04b0 0000.04e4 0409.04b0 0409.04e4
    Information from resource tables:
6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csfirmwareanalysis
!reg querykey \REGISTRY\MACHINE\system\ControlSet001\services\csfirmwareanalysis

Hive         ffff84059ca7b000
KeyNode      ffff8405a6f69d9c

[SubKeyAddr]         [VolatileSubKeyName]
ffff84059ce197cc     Enum

 Use '!reg keyinfo ffff84059ca7b000 &lt;SubKeyAddr&gt;' to dump the subkey details

[ValueType]         [ValueName]                   [ValueData]
REG_DWORD           Type                          1
REG_DWORD           Start                         0
REG_DWORD           ErrorControl                  1
REG_DWORD           Tag                           6
REG_EXPAND_SZ       ImagePath                     system32\DRIVERS\CSFirmwareAnalysis.sys
REG_SZ              DisplayName                   @oem43.inf,%FirmwareAnalysis.SVCDESC%;CrowdStrike Firmware Analysis Service
REG_SZ              Group                         Boot Bus Extender
REG_MULTI_SZ        Owners                        oem43.inf\0!csfirmwareanalysis.inf_amd64_12861fc608fb1440\0
6: kd&gt; !reg querykey \REGISTRY\MACHINE\system\Controlset001\control\earlylaunch
!reg querykey \REGISTRY\MACHINE\system\Controlset001\control\earlylaunch
</pre></div>


<p>As we can see from the above analysis, CrowdStrike loads four driver modules. One of those modules receives dynamic control and content updates frequently based on the CrowdStrike Preliminary Post-incident-review timeline.</p>



<p>We can leverage the unique stack and attributes of this crash to identify the Windows crash reports generated by this specific CrowdStrike programming error. It’s worth noting the number of devices which generated crash reports is a subset of the number of impacted devices previously shared by <a href="https://blogs.microsoft.com/blog/2024/07/20/helping-our-customers-through-the-crowdstrike-outage/" target="_blank" rel="noreferrer noopener">Microsoft in our blog post</a>, because crash reports are sampled and collected only from customers who choose to upload their crashes to Microsoft. Customers who <a href="https://learn.microsoft.com/en-us/windows/privacy/configure-windows-diagnostic-data-in-your-organization" target="_blank" rel="noreferrer noopener">choose to enable crash dump sharing</a> help both driver vendors and Microsoft to identify and remediate quality issues and crashes.</p>


<figure><img decoding="async" src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/07/Picture1-3.webp" alt="" srcset="" data-orig-src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/07/Picture1-3.webp"><figcaption>Figure 1 CrowdStrike driver associated crash dump reports over time</figcaption></figure>



<p>We make this information available to driver owners so they can assess their own reliability via the <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/dashboard/hardware-program-register" target="_blank" rel="noreferrer noopener">Hardware Dev Center analytics</a> dashboard. As we can see from the above, any reliability problem like this invalid memory access issue can lead to widespread availability issues when not combined with safe deployment practices. Let’s dig into why security solutions leverage kernel drivers on Windows.</p>



<h2 id="why-do-security-solutions-leverage-kernel-drivers">Why do security solutions leverage kernel drivers?</h2>



<p>Many security vendors such as CrowdStrike and Microsoft leverage a kernel driver architecture and there are several reasons for this.</p>



<h3 id="visibility-and-enforcement-of-security-related-events">Visibility and enforcement of security related events</h3>



<p>Kernel drivers allow for system wide visibility, and the capability to load in early boot to detect threats like <a href="https://learn.microsoft.com/en-us/defender-endpoint/malware/rootkits-malware" target="_blank" rel="noreferrer noopener">boot kits and root kits</a> which can load before user-mode applications. In addition, Microsoft provides a rich set of capabilities such as system event callbacks for process and thread creation and filter drivers which can watch for events like file creation, deletion, or modification. Kernel activity can also trigger call backs for drivers to decide when to block activities like file or process creations. Many vendors also use drivers to collect a variety of network information in the kernel using the <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/network/ndis-drivers" target="_blank" rel="noreferrer noopener">NDIS driver class</a>.</p>



<h3 id="performance">Performance</h3>



<p>Kernel drivers are often utilized by security vendors for potential performance benefits. For example, analysis or data collection for high throughput network activity may benefit from a kernel driver. There are many scenarios where data collection and analysis can be optimized for operation outside of kernel mode and Microsoft continues to partner with the ecosystem to improve performance and provide best practices to achieve parity outside of kernel mode.</p>



<h3 id="tamper-resistance">Tamper resistance</h3>



<p>A second benefit of loading into kernel mode is tamper resistance. Security products want to ensure that their software cannot be disabled by malware, targeted attacks, or malicious insiders, even when those attackers have admin-level privileges. They also want to ensure that their drivers load as early as possible so that they can observe system events at the earliest possible time. Windows provides a mechanism to launch drivers marked as <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/install/early-launch-antimalware" target="_blank" rel="noreferrer noopener">Early Launch Antimalware</a> (ELAM) early in the boot process for this reason. CrowdStrike signs the above CSboot driver as ELAM, enabling it to load early in the boot sequence.</p>



<p>In the general case, there is a tradeoff that security vendors must rationalize when it comes to kernel drivers. Kernel drivers provide the above properties at the cost of resilience. Since kernel drivers run at the most trusted level of Windows, where containment and recovery capabilities are by nature constrained, security vendors must carefully balance needs like visibility and tamper resistance with the risk of operating within kernel mode.</p>



<p>All code operating at kernel level requires extensive validation because it cannot fail and restart like a normal user application. This is universal across all operating systems. Internally at Microsoft, we have invested in moving complex Windows core services from kernel to user mode, such as font file parsing from <a href="https://techcommunity.microsoft.com/t5/microsoft-security-baselines/dropping-the-quot-untrusted-font-blocking-quot-setting/ba-p/701068" target="_blank" rel="noreferrer noopener">kernel to user mode</a>.</p>



<p>It is possible today for security tools to balance security and reliability. For example, security vendors can use minimal sensors that run in kernel mode for data collection and enforcement limiting exposure to availability issues. The remainder of the key product functionality includes managing updates, parsing content, and other operations can occur isolated within user mode where recoverability is possible. This demonstrates the best practice of minimizing kernel usage while still maintaining a robust security posture and strong visibility.</p>


<figure><img decoding="async" src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/07/Picture2-2.webp" alt="" srcset="" data-orig-src="https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2024/07/Picture2-2.webp"><figcaption>Figure 2 Example security product architecture which balances security and reliability</figcaption></figure>



<p>Windows provides several user mode protection approaches for anti-tampering, like Virtualization-based security <a href="https://learn.microsoft.com/en-us/windows/win32/trusted-execution/vbs-enclaves" target="_blank" rel="noreferrer noopener">(VBS) Enclaves</a> and <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/install/early-launch-antimalware" target="_blank" rel="noreferrer noopener">Protected Processes</a> that vendors can use to protect their key security processes. Windows also provides <a href="https://techcommunity.microsoft.com/t5/windows-it-pro-blog/new-security-capabilities-in-event-tracing-for-windows/ba-p/3949941" target="_blank" rel="noreferrer noopener">ETW events</a> and user-mode interfaces like <a href="https://learn.microsoft.com/en-us/windows/win32/amsi/antimalware-scan-interface-portal" target="_blank" rel="noreferrer noopener">Antimalware Scan Interface</a> for event visibility. These robust mechanisms can be used to reduce the amount of kernel code needed to create a security solution, which balances security and robustness.</p>







<p>Microsoft engages with third-party security vendors through an industry forum called the <a href="https://learn.microsoft.com/en-us/defender-xdr/virus-initiative-criteria" target="_blank" rel="noreferrer noopener">Microsoft Virus Initiative</a> (MVI). This group consists of Microsoft and Security Industry and was created to establish a dialogue and collaboration across the Windows security ecosystem to improve robustness in the way security products use the platform. With MVI, Microsoft and vendors collaborate on the Windows platform to define reliable extension points and platform improvements, as well as share information about how to best protect our customers.</p>



<p>Microsoft works with members of MVI to ensure compatibility with Windows updates, improve performance, and address reliability issues. MVI partners actively participating in the program contribute to making the ecosystem more resilient and gain benefits including technical briefings, feedback loops with Microsoft product teams, and access to antimalware platform features such as ELAM and Protected Processes. Microsoft also provides runtime protection such as <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/kernel/driver-x64-restrictions" target="_blank" rel="noreferrer noopener">Patch Guard</a> to prevent disruptive behavior from kernel driver types like anti-malware.</p>



<p>In addition, all drivers signed by the Microsoft Windows Hardware Quality Labs (WHQL) must run a series of tests and attest to a number of quality checks, including using <a href="https://learn.microsoft.com/en-us/windows-hardware/test/hlk/testref/236b8ad5-0ba1-4075-80a6-ae9dafb71c94" target="_blank" rel="noreferrer noopener">fuzzers</a>, running <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/devtest/prefast-for-drivers-warnings" target="_blank" rel="noreferrer noopener">static code analysis</a> and testing under <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/devtest/driver-verifier" target="_blank" rel="noreferrer noopener">runtime driver verification</a>, among other techniques. These tests have been developed to ensure that best practices around security and reliability are followed. Microsoft includes all these tools in the Windows Driver Kit used by all driver developers. A list of the resources and tools is <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/driversecurity/driver-security-checklist" target="_blank" rel="noreferrer noopener">available here</a>.</p>



<p>All WHQL signed drivers are run through Microsoft’s ingestion checks and malware scans and must pass before being approved for signing. Additionally, if a third-party vendor chooses to distribute their driver via Windows Update (WU), the driver also goes through Microsoft’s flighting and gradual rollout processes to observe quality and ensure the driver meets the necessary quality criteria for a broad release.</p>



<h2 id="can-customers-deploy-windows-in-a-higher-security-mode-to-increase-reliability">Can customers deploy Windows in a higher security mode to increase reliability?</h2>



<p>Windows at its core is an open and versatile OS, and it can easily be locked down for increased security using integrated tools. In addition, Windows is constantly increasing security defaults, including dozens of new security features enabled by default in Windows 11.</p>



<h3 id="security-features-enabled-by-default-in-windows-11">Security features enabled by default in Windows 11</h3>



<figure><table><tbody><tr><td><strong>Area</strong></td><td><strong>Feature</strong></td></tr><tr><td><strong>Hardware Security Baseline</strong></td><td><a href="https://learn.microsoft.com/en-us/windows/security/hardware-security/tpm/tpm-fundamentals" target="_blank" rel="noreferrer noopener">TPM2.0</a><br><a href="https://learn.microsoft.com/en-us/mem/intune/user-help/you-need-to-enable-secure-boot-windows" target="_blank" rel="noreferrer noopener">Secure boot</a><br><a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/oem-vbs" target="_blank" rel="noreferrer noopener">Virtualization-based security (VBS)<br></a><a href="https://learn.microsoft.com/en-us/windows/security/hardware-security/enable-virtualization-based-protection-of-code-integrity" target="_blank" rel="noreferrer noopener">Memory integrity (Hypervisor-protected Code Integrity (HVCI))<br></a><a href="https://techcommunity.microsoft.com/t5/windows-os-platform-blog/understanding-hardware-enforced-stack-protection/ba-p/1247815" target="_blank" rel="noreferrer noopener">Hardware-enforced stack protection<br></a><a href="https://learn.microsoft.com/en-us/windows/security/hardware-security/kernel-dma-protection-for-thunderbolt" target="_blank" rel="noreferrer noopener">Kernel Direct Memory Access (DMA) protection<br></a>HW-based kernel protection (HLAT)<br><a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello-enhanced-sign-in-security" target="_blank" rel="noreferrer noopener">Enhanced sign-in security (ESS) for built-in biometric sensors</a></td></tr><tr><td><strong>Encryption</strong></td><td><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/data-protection/bitlocker/" target="_blank" rel="noreferrer noopener">BitLocker</a> (commercial)<br><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/data-protection/bitlocker/#device-encryption" target="_blank" rel="noreferrer noopener">Device Encryption</a> (consumer)</td></tr><tr><td><strong>Identity Management</strong></td><td><a href="https://learn.microsoft.com/en-us/windows/security/identity-protection/credential-guard/" target="_blank" rel="noreferrer noopener">Credential Guard<br></a><a href="https://learn.microsoft.com/en-us/entra/identity/conditional-access/concept-token-protection" target="_blank" rel="noreferrer noopener">Entra primary refresh token (PRT) hardware protected<br></a>MDM deployed SCEP certs hardware protected<br>MDM enrollment certs hardware protected<br>Local Security Authority (LSA) PPL prevents token/credential dumping<br>Account lockout policy (for 10 failed sign-ins)<br>Enhanced phishing protection with Microsoft Defender<br><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/virus-and-threat-protection/microsoft-defender-smartscreen/" target="_blank" rel="noreferrer noopener">Microsoft Defender SmartScreen<br></a>NPLogonNotification doesn’t include password<br>WDigest SSO removed to reduce password disclosure<br>AD Device Account protected by CredGuard*</td></tr><tr><td><strong>Multi-Factor Authentication<br>(Passwordless)</strong></td><td>MSA &amp; Entra users lead through Hello enablement by default<br>MSA password automatically removed from Windows if never used<br>Hello container VSM protected<br>Peripheral biometric sensors blocked for ESS enabled devices<br>Lock on leave integrated into Hello</td></tr><tr><td><strong>Security Incident Reduction</strong></td><td>Common Log File Systems run from trusted source<br>Move tool-tip APIs from kernel to user mode<br>Modernize print stack by removing untrusted drivers<br>DPAPI moved from 3DES to AES<br>TLS 1.3 default with TLS 1.0/1.1 disabled by default<br>NTLM-less*</td></tr><tr><td><strong>OS lockdown</strong></td><td><a href="https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control/design/microsoft-recommended-driver-block-rules#microsoft-vulnerable-driver-blocklist" target="_blank" rel="noreferrer noopener">Microsoft Vulnerable Driver Blocklist<br></a>3P driver security baseline enforced via WHCP<br>Smart App Control*</td></tr></tbody></table><figcaption>*Feature available in the Windows Insider Program or currently off by default and on a path for default enablement</figcaption></figure>



<p>Windows has integrated security features to self-defend. This includes key anti-malware features enabled by default, such as:</p>



<ol>
<li><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/system-security/secure-the-windows-10-boot-process" target="_blank" rel="noreferrer noopener">Secure Boot</a>, which helps prevent early boot malware and rootkits by enforcing signing consistently across Windows boots.</li>



<li><a href="https://learn.microsoft.com/en-us/windows/security/operating-system-security/system-security/secure-the-windows-10-boot-process" target="_blank" rel="noreferrer noopener">Measured Boot</a>, which provides TPM-based hardware cryptographic measurements on boot-time properties available through integrated attestation services such as <a href="https://learn.microsoft.com/en-us/windows-server/security/device-health-attestation" target="_blank" rel="noreferrer noopener">Device Health Attestation</a>.</li>



<li><a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/bringup/device-guard-and-credential-guard" target="_blank" rel="noreferrer noopener">Memory integrity</a> (also known as hypervisor-protected code integrity or HVCI), which prevents runtime generation of dynamic code in the kernel and helps ensure control flow integrity.</li>



<li><a href="https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control/design/microsoft-recommended-driver-block-rules#microsoft-vulnerable-driver-blocklist" target="_blank" rel="noreferrer noopener">Vulnerable driver blocklist</a>, which is on by default, integrated into the OS, and managed by Microsoft. This complements the malicious driver block list.</li>



<li><a href="https://learn.microsoft.com/en-us/windows-server/security/credentials-protection-and-management/configuring-additional-lsa-protection" target="_blank" rel="noreferrer noopener">Protected Local Security Authority</a> is on by default in Windows 11 to protect a range of credentials. <a href="https://learn.microsoft.com/en-us/windows/security/identity-protection/credential-guard/?toc=%2Fwindows-server%2Fsecurity%2Ftoc.json&amp;bc=%2Fwindows-server%2Fbreadcrumbs%2Ftoc.json" target="_blank" rel="noreferrer noopener">Hardware-based credential protection</a> is on by default for enterprise versions of Windows.</li>



<li><a href="https://learn.microsoft.com/en-us/defender-endpoint/microsoft-defender-antivirus-windows" target="_blank" rel="noreferrer noopener">Microsoft Defender Antivirus</a> is enabled by default in Windows and offers anti-malware capabilities across the OS.</li>
</ol>



<p>These security capabilities provide layers of protection against malware and exploitation attempts in modern Windows. Many Windows customers have leveraged our security baseline and Windows security technologies to harden their systems and these capabilities collectively have reduced the attack surface significantly.</p>



<p>Using the integrated security features of Windows to prevent adversary attacks such as those displayed in the <a href="https://attack.mitre.org/" target="_blank" rel="noreferrer noopener">MITRE ATT&amp;CK® framework</a> increases security while reducing cost and complexity. It leverages best practices to achieve maximum security and reliability. These best practices include:</p>



<ol>
<li>Using <a href="https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control/" target="_blank" rel="noreferrer noopener">App Control for Business</a> (formerly Windows Defender Application Control), you can author a security policy to allow only trusted and/or business-critical apps. Your policy can be crafted to deterministically and durably prevent nearly all malware and “living off the land” style attacks. It can also specify which kernel drivers are allowed by your organization to durably guarantee that only those drivers will load on your managed endpoints.</li>



<li>Use <a href="https://learn.microsoft.com/en-us/windows-hardware/drivers/bringup/device-guard-and-credential-guard" target="_blank" rel="noreferrer noopener">Memory integrity</a> with a <a href="https://learn.microsoft.com/en-us/windows/security/application-security/application-control/windows-defender-application-control/design/wdac-wizard" target="_blank" rel="noreferrer noopener">specific allow list policy</a> to further protect the Windows kernel using <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/oem-vbs" target="_blank" rel="noreferrer noopener">Virtualization-based security</a> (VBS). Combined with App Control for Business, memory integrity can reduce the attack surface for kernel malware or boot kits. This can also be used to limit any drivers that might impact reliability on systems.</li>



<li>Running as <a href="https://learn.microsoft.com/en-us/windows-server/remote/multipoint-services/create-a-standard-user-account" target="_blank" rel="noreferrer noopener">Standard User</a> and elevating only as necessary. Companies that follow the best practices to run as standard user and reduce privileges mitigate many of the <a href="https://attack.mitre.org/" target="_blank" rel="noreferrer noopener">MITRE ATT&amp;CK®</a> techniques.</li>



<li>Use <a href="https://learn.microsoft.com/en-us/windows-server/security/device-health-attestation" target="_blank" rel="noreferrer noopener">Device Health Attestation</a> (DHA) to monitor devices for the right security policy, including hardware-based measurements for the security posture of the machine. This is a modern and exceptionally durable approach to ensure security for high availability scenarios and uses Microsoft’s <a href="https://www.microsoft.com/en-us/security/business/security-101/what-is-zero-trust-architecture?msockid=04462a6256e861da2e753a3d57346023" target="_blank" rel="noreferrer noopener">Zero Trust architecture</a>.</li>
</ol>



<h2 id="what-is-next">What is next?</h2>



<p>Windows is a self-protecting operating system that has produced dozens of new security features and architectural changes <a href="https://www.microsoft.com/en-us/security/blog/2024/05/20/new-windows-11-features-strengthen-security-to-address-evolving-cyberthreat-landscape/" target="_blank" rel="noreferrer noopener">in recent versions</a>. We plan to work with the anti-malware ecosystem to take advantage of these integrated features to modernize their approach, helping to support and even increase security along with reliability.</p>



<p>This includes helping the ecosystem by:</p>



<ol>
<li>Providing safe rollout guidance, best practices, and technologies to make it safer to perform updates to security products.</li>



<li>Reducing the need for kernel drivers to access important security data.</li>



<li>Providing enhanced isolation and anti-tampering capabilities with technologies like our recently <a href="https://techcommunity.microsoft.com/t5/windows-os-platform-blog/securely-design-your-applications-and-protect-your-sensitive/ba-p/4179543" target="_blank" rel="noreferrer noopener">announced VBS enclaves</a>.</li>



<li>Enabling zero trust approaches like <a href="https://learn.microsoft.com/en-us/azure/attestation/overview" target="_blank" rel="noreferrer noopener">high integrity attestation</a> which provides a method to determine the security state of the machine based on the health of Windows native security features.</li>
</ol>



<p>As we move forward, Windows is continuing to innovate and offer new ways for security tools to detect and respond to emerging threats safely and securely. Windows has <a href="https://www.microsoft.com/en-us/security/blog/2024/03/06/enhancing-protection-updates-on-microsofts-secure-future-initiative/" target="_blank" rel="noreferrer noopener">announced a commitment around the Rust programming language</a> as part of Microsoft’s <a href="https://www.microsoft.com/en-us/microsoft-cloud/resources/secure-future-initiative" target="_blank" rel="noreferrer noopener">Secure Future Initiative</a> (SFI) and has recently expanded the <a href="https://www.youtube.com/watch?v=8T6ClX-y2AE" target="_blank" rel="noreferrer noopener">Windows kernel to support Rust</a>.</p>



<p>The information in this blog post is provided as part of our commitment to communicate learnings and next steps after the CrowdStrike incident. We will continue to share ongoing guidance on security best practices for Windows and work across our broad ecosystem of customers and partners to develop new security capabilities based on your feedback.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perfectionism – one of the biggest productivity killers in the eng industry (148 pts)]]></title>
            <link>https://newsletter.eng-leadership.com/p/perfectionism-one-of-the-biggest</link>
            <guid>41094485</guid>
            <pubDate>Sun, 28 Jul 2024 17:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.eng-leadership.com/p/perfectionism-one-of-the-biggest">https://newsletter.eng-leadership.com/p/perfectionism-one-of-the-biggest</a>, See on <a href="https://news.ycombinator.com/item?id=41094485">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg" width="800" height="445" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:445,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:27704,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597a1df9-bf38-4e9c-a3a0-7f20097379ef_800x445.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Perfectionism is one of these things that hurt us without even realizing it. We may think it’s great to put so much effort into ensuring the result is “perfect”, but at the end of the day, we never finish it and it becomes an endless “work in progress”, which is a big problem.</p><p><span>This article is done in collaboration with </span><a href="https://www.linkedin.com/in/jordancutler1/" rel="">Jordan Cutler</a><span>, Senior Software Engineer at Pinterest and the author of the newsletter </span><a href="https://read.highgrowthengineer.com/" rel="">High Growth Engineer</a><span>.</span></p><p>We both made a lot of mistakes thinking that perfecting is the way to go and after some time we realized that progress is so much better!</p><p>And today, we are sharing our stories with you! Let’s start with Jordan’s first.</p><p>Early in my career, I spent a lot of time writing code. In fact, I shipped over 1200 pull requests (PRs) in my first 3 years as an engineer, averaging ~1-2 PRs per day.</p><p>While the code velocity was high, I was focusing on the wrong things. Most of the PRs were dedicated to “perfecting” our codebase.</p><p>At the time, I thought I was doing great things. Who wouldn’t love the code to be spruced up? But looking back, I realized how I could have spent my time so much better.</p><p>Instead of perfecting code, often code that hadn’t been touched in years and didn’t need to be touched, I could have been adding value for the team.</p><p>Things like:</p><ul><li><p>Reducing the plate of my teammates, rather than adding to it with more code reviews to give them</p></li><li><p>Shipping the current feature faster and getting ahead on the next one</p></li><li><p>Asking my manager how I can help them more</p></li></ul><p>So, even though I shipped all those PRs early in my career, today I realize how meaningless it is.</p><p>If I had spent less time perfecting code, I could have shipped 50% fewer PRs and added 2x more value to the team.</p><p>Today, I’m lucky if I ship more than 3 PRs in a week. That’s because my focus is on working on the right things, which often means helping the team, creating partnerships, writing proposals, and scaling myself through docs. When I do ship code, it’s with clear intent to move value for the team or business.</p><p>Outside of this example, I avoid perfectionism by doing 3 things:</p><ol><li><p><strong>Writing down my priorities</strong><span> at the start of the week, which helps me know what’s most important to accomplish. Hint: It’s usually not perfecting something.</span></p></li><li><p><strong>Shipping the smallest unit of value</strong><span> I can, either to internal or external customers. For example, shipping an 80% working prototype behind a feature flag to get early feedback internally.</span></p></li><li><p><strong>Seeking early feedback</strong><span>. I set up 4-hour focus time blocks at the start of my day and make as much progress as I can. Then, I ask myself, “Based on what I accomplished, is there anything I should share or request feedback on?” I do this all the time with technical docs. I ask my team, “Am I going in a completely wrong direction here, or should I continue on this path?” Sometimes, I find there’s a simpler solution than my proposal, it’s already explored, or not valuable. That saves me hours of time from perfecting the doc.</span></p></li></ol><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png" width="1456" height="505" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:505,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:314206,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e1da5-498c-4028-8c3d-257bfbc53493_1600x555.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Less perfection. More progress.</p><p>Perfectionism has been a problem for me starting from an early age. I’ve always wanted to make things the best quality possible, especially things that matter a lot to me.</p><p>It took a long time for me to realize it’s actually counter-productive. Here are the most prominent things that I wanted to perfect as a Software Engineer and a manager:</p><p><strong>I wanted to learn “perfectly” from tutorials</strong></p><p>In my early days as a self-taught engineer, I did SO much learning from tutorials.</p><p>Looking back at that time, I could see how I could have progressed much faster if I hadn’t wanted to always make sure that I finished every single tutorial 100%, plus with the mindset of “I shouldn’t miss any second of it”.</p><p><span>What I know now, the best way to learn from tutorials is to get what you need and apply that to your own project. Building your own project is the best way to learn. You can find more details about this here: </span><a href="https://newsletter.eng-leadership.com/p/become-a-better-engineer-by-working" rel="">Become a better engineer by working on side projects</a><span> (paid article).</span></p><p><strong>I wanted to design “perfect” things</strong></p><p>In one of my student jobs as a Software Engineer, I was also doing a lot of design related things like designing banners, websites also doing some graphic design. I saw a repeating pattern when designing.</p><p>That pattern was that I would create a draft quite quickly, but then spend the 2-4x amount of time applying finishing touches. Which was quite counter-productive and also stressful for me.</p><p>I spent so much time trying to make sure it’s “pixel perfect”: right colors, right fonts, right spacing, etc. I would make so much more progress if I would just be fine with it. The funny thing is that the end result wasn’t much better than the first draft (maybe a tad more).</p><p><strong>I wanted to create “perfect” code</strong></p><p>Especially early on in my career as a Software Engineer, I would feel that I need to write perfect code, before showing it to others.</p><p>I spent so much time refactoring functions, renaming variables, rethinking the approach and overall looking for ways to optimize it. It was my impostor syndrome telling me that I was not good enough to do this.</p><p>That got me to the place where my overall progression was hurting and similar with designing → it caused additional stress to me.</p><p>I would be making SO much more progress if I would be focusing on good enough code and getting feedback early on! So much wasted time trying to “perfect”.</p><p><strong>As a manager, I wanted to wait for “perfect” timings and make “perfect” decisions</strong></p><p>I thought that my timing to give feedback needed to be perfect. Well, the outcome of waiting was that things got a lot worse.</p><p>I said to myself: “Maybe things will get better if I wait for a bit more time”. Well, the reality was that things didn’t. And my waiting for perfect timing made it a LOT worse.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png" width="800" height="400" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:46210,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7425a3-c8d3-45d9-98bc-fb5e42d76671_800x400.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Similar to making decisions. “They need to be perfect”, I said to myself. And that got me to the place, where I didn’t make the decisions fast enough, which actually blocked my team from progressing.</p><p>And I realized over time, that not all the decisions will be the absolute right ones, but it’s much better to make a decision and reverse it later if needed, instead of not making it at all.</p><p><strong>Here are my top 3 learnings:</strong></p><ul><li><p>Focusing on progression instead of perfecting will make you SO much more productive + it’s so much better for your mental health.</p></li><li><p>Perfect moments do not exist and waiting for them will just cause a lot more issues. Doing things now or as soon as you can is the way to go when you are dealing with important things.</p></li><li><p>Whenever I feel like I am trying to “perfect”, I think of this: 100% does not exist and 95% is good enough in the majority of the cases. That shifts my mind into making progress instead of perfecting.</p></li></ul><p><span>Thanks to Jordan for sharing his story and his insights on this very important topic. Make sure to check him out on </span><a href="https://www.linkedin.com/in/jordancutler1/" rel="">LinkedIn</a><span> and also check out his </span><a href="https://read.highgrowthengineer.com/" rel="">newsletter</a><span>!</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png" width="800" height="400" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2a065581-415f-4ca6-8506-8051f5fec840_800x400.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:400,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:386666,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a065581-415f-4ca6-8506-8051f5fec840_800x400.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Jordan (left) and me (right) rocking the Progress over Perfection T-shirts!</figcaption></figure></div><p>Let’s end this article with this important message:</p><blockquote><p>There is no such thing as “perfection” and the faster we realize it, the better our progress is going to be and the more things we’ll get done!</p></blockquote><ul><li><p><a href="https://newsletter.eng-leadership.com/p/biggest-productivity-killers-in-the" rel="">Biggest productivity killers in the engineering industry</a><span> (paid article)</span></p></li><li><p><a href="https://newsletter.eng-leadership.com/p/context-switching-one-of-the-worst" rel="">Context-switching - one of the worst productivity killers in the engineering industry</a><span> (paid article)</span></p></li><li><p><a href="https://newsletter.eng-leadership.com/t/productivity" rel="">Check out the full Productivity learning track</a><span> (8 articles)</span></p></li></ul><p>We are not over yet!</p><p>After months of testing and especially trying so many different T-shirts, the store is now officially available to all of you!</p><div><figure><a target="_blank" href="https://store.eng-leadership.com/" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png" width="800" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:115372,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://store.eng-leadership.com/&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd0cf284-fa91-4005-a8aa-0ff698e8bd32_800x600.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You can get the same Progress over Perfection T-shirt that both me and Jordan are wearing. Or others available. I wear these shirts proudly almost every day. I especially love to wear them when doing exercise!</p><p data-attrs="{&quot;url&quot;:&quot;https://store.eng-leadership.com/&quot;,&quot;text&quot;:&quot;I want to check it out!&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://store.eng-leadership.com/" rel=""><span>I want to check it out!</span></a></p><p><span>Paid subscribers, you are getting </span><strong>20% off</strong><span> on everything in the store!</span></p><p><span>You can find the discount code here: </span><a href="https://newsletter.eng-leadership.com/p/special-deals-for-paid-subscribers" rel="">💰 Special Deals for paid subscribers</a></p><p>Liked this article? Make sure to 💙 click the like button.</p><p>Feedback or addition? Make sure to 💬 comment.</p><p>Know someone that would find this helpful? Make sure to 🔁 share this post.</p><ul><li><p><span>Join the Cohort course Senior Engineer to Lead: Grow and thrive in the role </span><a href="https://maven.com/gregor-ojstersek/senior-engineer-to-lead?promoCode=ENGLEADERSHIP" rel="">here</a><span>.</span></p></li><li><p><span>Book a Coaching and Mentoring or Consulting and Advising call with me </span><a href="https://tidycal.com/gregorojstersek" rel="">here</a><span>.</span></p></li><li><p><span>Interested in sponsoring this newsletter? Check the sponsorship options </span><a href="https://calico-cabinet-fbf.notion.site/Sponsor-Engineering-Leadership-fa0579535d6f4422a6da350580a54546" rel="">here</a><span>.</span></p></li></ul><p><span>You can find me on </span><a href="https://www.linkedin.com/mynetwork/discovery-see-all/?usecase=PEOPLE_FOLLOWS&amp;followMember=gregorojstersek" rel="">LinkedIn</a><span> or </span><a href="https://twitter.com/gregorojstersek" rel="">Twitter</a><span>.</span></p><p>If you wish to make a request on particular topic you would like to read, you can send me an email to info@gregorojstersek.com.</p><p>This newsletter is funded by paid subscriptions from readers like yourself.</p><p>If you aren’t already, consider becoming a paid subscriber to receive the full experience!</p><p data-attrs="{&quot;url&quot;:&quot;https://newsletter.eng-leadership.com/about#§paid-subscribers-get&quot;,&quot;text&quot;:&quot;Check the benefits of the paid plan&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://newsletter.eng-leadership.com/about#%C2%A7paid-subscribers-get" rel=""><span>Check the benefits of the paid plan</span></a></p><p>You are more than welcome to find whatever interests you here and try it out in your particular case. Let me know how it went! Topics are normally about all things engineering related, leadership, management, developing scalable products, building teams etc.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How simultaneous multithreading works under the hood (269 pts)]]></title>
            <link>https://blog.codingconfessions.com/p/simultaneous-multithreading</link>
            <guid>41093916</guid>
            <pubDate>Sun, 28 Jul 2024 15:35:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.codingconfessions.com/p/simultaneous-multithreading">https://blog.codingconfessions.com/p/simultaneous-multithreading</a>, See on <a href="https://news.ycombinator.com/item?id=41093916">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Simultaneous multithreading (SMT) is a feature that lets a processor handle instructions from two different threads at the same time. But have you ever wondered how this actually works? How does the processor keep track of two threads and manage its resources between them?</p><p>In this article, we’re going to break it all down. Understanding the nuts and bolts of SMT will help you decide if it’s a good fit for your production servers. Sometimes, SMT can turbocharge your system's performance, but in other cases, it might actually slow things down. Knowing the details will help you make the best choice.</p><p>So, let’s dive in and figure out how SMT works, why it was invented in the first place, and what it means for you. </p><blockquote><h5><em><strong>Disclaimer:</strong></em><span> </span><em>Much of the discussion in this article is about Intel’s implementation of SMT, also called hyper-threading. It is based on their white paper published in 2002</em></h5></blockquote><p><strong>Watch Video Instead:</strong><span> </span><em>If you prefer video over text, then you can also watch the recording of a live session I did on this topic:</em><span> </span></p><div data-component-name="DigestPostEmbed"><a href="https://blog.codingconfessions.com/p/recording-how-hyper-threading-works" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F146388589%2Fbe1acb70-4217-4ce5-81bb-b051d8c3c24e%2Ftranscoded-00001.png"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-video.s3.amazonaws.com%2Fvideo_upload%2Fpost%2F146388589%2Fbe1acb70-4217-4ce5-81bb-b051d8c3c24e%2Ftranscoded-00001.png" sizes="100vw" alt="Recording: How Hyper-Threading Works — A Microarchitectural Perspective" width="140" height="140"></picture></div></a></div><p>SMT was introduced to improve the utilization of the resources in the processor. At the microarchitecture level, processors consist of hundreds of registers, multiple load/store units and multiple arithmetic units. To utilize these better, processors also employ various techniques for instruction level parallelism (ILP), such as instruction pipelining, superscalar architecture, out-of-order execution to name a few.</p><p>A pipelined processor improves the resource utilization by breaking down the execution of an instruction into multiple stages which form a pipeline, like the assembly line of a factory. In each cycle, an instruction moves from one stage of the pipeline to the next and the processor adds a new instruction to the first stage of the pipeline. The following figure shows how pipelining works for a pipeline of depth 5. As you can see, 5th cycle onwards, the processor will have upto 5 instructions in flight each cycle, and it will finish 1 instruction each cycle after that.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png" width="607" height="133" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:133,&quot;width&quot;:607,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Illustration of a five-stage instruction pipeline. In each cycle, an instruction moves to the next stage which makes up space in the first stage and a new instruction can also start getting processed in each cycle. A super deep pipeline will have many instructions being processed in parallel.&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Illustration of a five-stage instruction pipeline. In each cycle, an instruction moves to the next stage which makes up space in the first stage and a new instruction can also start getting processed in each cycle. A super deep pipeline will have many instructions being processed in parallel." title="Illustration of a five-stage instruction pipeline. In each cycle, an instruction moves to the next stage which makes up space in the first stage and a new instruction can also start getting processed in each cycle. A super deep pipeline will have many instructions being processed in parallel." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cbbeb2e-ed04-4f44-bb88-683713a2a740_607x133.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Illustration of a five-stage instruction pipeline. In each cycle, an instruction moves to the next stage which makes up space in the first stage and a new instruction can also start getting processed in each cycle. A super deep pipeline will have many instructions being processed in parallel.</figcaption></figure></div><p>Modern processors are also superscalar, which means instead of issuing one instruction each cycle, they can issue multiple instructions. For instance, the recent Intel core i7 processors can issue 4 instructions each cycle (also called the issue width of the processor).</p><p>Instruction pipelining and superscalar architecture significantly improve the instruction throughput and resource utilization of the processor. However, in practice, this max utilization can be difficult to achieve. To execute so many instructions in parallel, the processor needs to find enough independent instructions in the program, which is very hard.</p><p><span>This typically leads to two kinds of wastages. One is </span><em>horizontal waste</em><span> which occurs when the processor is not able to find enough independent instructions in the thread to saturate the issue width of the processor.</span></p><p><span>The other type of wastage is  </span><em>vertical waste</em><span> that occurs when the processor is unable to issue any instructions in a cycle because all the next instructions in the program are dependent on the currently executing ones.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png" width="665" height="351" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:351,&quot;width&quot;:665,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:45702,&quot;alt&quot;:&quot;Illustration of horizontal and vertical state in a processor with issue width of 5 instructions per cycle. The empty boxes represent the instruction slots where the processor could not issue an instruction&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Illustration of horizontal and vertical state in a processor with issue width of 5 instructions per cycle. The empty boxes represent the instruction slots where the processor could not issue an instruction" title="Illustration of horizontal and vertical state in a processor with issue width of 5 instructions per cycle. The empty boxes represent the instruction slots where the processor could not issue an instruction" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0121e9e-b8b1-4374-ba66-2683ad8364fa_665x351.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Illustration of horizontal and vertical state in a processor with issue width of 5 instructions per cycle. The empty boxes represent the instruction slots where the processor could not issue an instruction</figcaption></figure></div><p>One way to better utilize the processing power is using traditional multithreading where the processor context switches between multiple threads. In this scheme, within a given cycle the processor issues instructions for only one thread, so this may still result in horizontal waste. However, in the next cycle the processor can context switch and issue instructions for another thread and avoid vertical wastage. This results in improved CPU utilization, however, with larger issue width processors, the horizontal wastage can still be significant. Also, there is the overhead of context switching between the threads.</p><p>This is where the idea of simultaneous multithreading was introduced. It enables the processor to issue instructions for multiple threads in the same cycle without any overhead of context switching. By definition, instructions of different threads are independent and can be executed in parallel which ultimately results in full utilization of the execution resources. </p><blockquote><p><em>Even though the idea of SMT doesn’t put a limit on the number of threads, Intel’s implementation of SMT (called hyper-threading) restricts it to two threads per core.</em></p></blockquote><p>We understand why SMT was introduced, now let’s learn about how it is implemented. Along with the implementation details we will also cover how it actually works.</p><p><span>A normal non-SMT processor can only execute instructions for one thread at a time. This is because every thread has an associated context to represent the current state of the program on the processor, which is also called the </span><a href="https://en.wikipedia.org/wiki/Architectural_state" rel="">architecture state</a><span>. This includes the data in the registers, the program counter value, the control registers etc. </span></p><p>To simultaneously execute instructions of two threads, the processor needs to be able to represent the state of the two threads simultaneously. So to implement the SMT capability, the hardware designers duplicated the architecture state of the processor. By doing so, a single physical processor appears as two logical processors to the operating system (OS), so that it can schedule threads for execution on them.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png" width="1342" height="747" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:747,&quot;width&quot;:1342,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:60239,&quot;alt&quot;:&quot;Illustration of a two core processor without SMT (top) and a two core processor with SMT (bottom). For implementing SMT the architecture state has been duplicated in the bottom processor and it will appear as having four processing cores to the OS.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Illustration of a two core processor without SMT (top) and a two core processor with SMT (bottom). For implementing SMT the architecture state has been duplicated in the bottom processor and it will appear as having four processing cores to the OS." title="Illustration of a two core processor without SMT (top) and a two core processor with SMT (bottom). For implementing SMT the architecture state has been duplicated in the bottom processor and it will appear as having four processing cores to the OS." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ddb618f-a2b8-4524-b4e7-7a3f45c10d48_1342x747.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Illustration of a two core processor without SMT (top) and a two core processor with SMT (bottom). For implementing SMT the architecture state has been duplicated in the bottom processor and it will appear as having four processing cores to the OS.</figcaption></figure></div><p>Apart from that, at the microarchitecture level, the processor also has various buffers and execution resources as well. To execute the instructions of two threads simultaneously, these resources are also either duplicated or shared between the two logical processors. The decision of whether to duplicate or to share a resource is based on many factors. For instance, how costly is it to duplicate a resource, in terms of power consumption and real-estate on the chip. </p><p>The crucial details about how SMT works lies in its microarchitectural implementation, so let’s go deeper into that.</p><p><em>The processor exposes the instruction set architecture (ISA) as the public interface for the programmers to program the CPU. The ISA includes the set of instructions, and the registers that the instructions can use. The microarchitecture of the processor is its internal implementation detail. Different processor models can support the same ISA but at the microarchitecture level they might be different.</em></p><p><span>The microarchitecture has three parts: the </span><em>frontend</em><span>, the </span><em>backend</em><span> and the </span><em>retirement unit</em><span>. The following diagram shows the schematics of the microarchitecture of a modern day processor:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png" width="695" height="631" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:631,&quot;width&quot;:695,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:56859,&quot;alt&quot;:&quot;The schematics of the microarchitecture of a modern processor consisting of the frontend, the backend, and the retirement unit&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="The schematics of the microarchitecture of a modern processor consisting of the frontend, the backend, and the retirement unit" title="The schematics of the microarchitecture of a modern processor consisting of the frontend, the backend, and the retirement unit" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F987bee1e-0bec-4753-947c-5ff58cb1a260_695x631.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The schematics of the microarchitecture of a modern processor consisting of the frontend, the backend, and the retirement unit</figcaption></figure></div><p>The frontend is the part which contains the instruction control unit that fetches and decodes the program instructions which should be executed next.</p><p>The backend consists of the execution resources, such as the physical registers, the arithmetic units, and the load/store units. It picks up the decoded instructions provided by the frontend, allocates execution resources for them and schedules them for execution.</p><p>The retirement unit is where the results of the executed instructions are finally committed to the architecture state of the processor.</p><p>To understand how SMT works we will go deeper into each of three components of the CPU microarchitecture. Let’s start with the frontend.</p><p>The following figure shows a more zoomed in view of the microarchitecture frontend. It consists of several components with each having a distinct role behind the fetching and decoding of instructions. Let’s talk about them one by one.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png" width="497" height="486" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:486,&quot;width&quot;:497,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:70383,&quot;alt&quot;:&quot;A zoomed in view of the frontend of an X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="A zoomed in view of the frontend of an X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002." title="A zoomed in view of the frontend of an X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1bf4a84-1c63-461e-9050-cd925a66749c_497x486.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A zoomed in view of the frontend of an X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002.</figcaption></figure></div><p>To track which instructions to fetch, the frontend contains an instruction pointer which contains the address of the next instruction of the program.</p><p>In the case of an SMT capable processor, there are two sets of instruction pointers which track the next instruction for the two programs independently.</p><p>The instruction pointers gives the addresses of the next instructions of the threads and the frontend has to read the instructions from those addresses. Before doing that it first checks for the existence of those instructions in the trace cache. </p><p>The trace cache contains recently decoded traces of instructions. Instruction decoding is an expensive operation and some instructions need to be executed frequently. Having this cache helps the processor cut down the instruction execution latency.</p><p>Trace cache is shared dynamically between the two logical processors on an as needed basis. If one thread is executing more instructions than the other, it is allowed to occupy more entries in the trace cache.</p><p>Each entry in the cache is tagged with the thread information to distinguish the instructions of the two threads. The access to the trace cache is arbitrated between the two logical processors each cycle. </p><p>If there is a miss in the trace cache, then the frontend looks for the instruction for the given address in the L1 instruction cache. If there is a miss in the L1 instruction cache, then it needs to fetch the instruction from the next level cache or the main memory.</p><p><span>The L1 instruction caches data using its </span><a href="https://en.wikipedia.org/wiki/Virtual_address_space" rel="">virtual address</a><span>, but main memory lookups require physical addresses. To translate the virtual addresses into physical addresses, the instruction lookaside buffer (ITLB) is used which contains the recently translated virtual addresses. </span></p><p>In an SMT capable processor, each logical processors has its own ITLB cache.</p><p>The instruction fetch logic for fetching the instructions from the main memory works on a first come first served basis, but it reserves at least one request slot for each logical processor so that both can make progress.</p><p>Once the instructions arrive from the main memory, they are kept in a small streaming buffer before they get picked up for decoding. These buffers are also small structures and duplicated for the logical processors in an SMT capable processor.</p><p>Once the instructions are fetched, they are decoded into smaller and simpler instructions called micro instructions (uops). These uops are put into the uop queue which acts as the boundary between the CPU frontend and backend. </p><p>The uop queue is shared equally between the two logical processors. This static partitioning enables both the logical processors to make independent progress.</p><p>Once the Uop queue has microinstructions ready, the role of the backend starts. The following figure shows a zoomed in view of the backend of an Intel X86 processor.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png" width="1056" height="479" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:479,&quot;width&quot;:1056,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;The zoomed in view of the backend of an Intel X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002.&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="The zoomed in view of the backend of an Intel X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002." title="The zoomed in view of the backend of an Intel X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66314505-33d7-441b-9cfb-8c4bbeb61085_1056x479.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The zoomed in view of the backend of an Intel X86 processor. Source: Intel Technology Journal, Vol 06, Issue 01, 2002.</figcaption></figure></div><p>Let’s talk about what happens in the backend component wise. </p><p>The backend picks up the micro instructions from the uop queue and executes them. However, it executes them out of their original program order. </p><p>Nearby instructions in a program are typically dependent on each other and these instructions may stall because they may perform a long latency operation, such as reading from main memory. As a result, all of their dependent instructions will also have to wait. In such a situation, the processor’s resources are wasted. To alleviate this problem, out-of-order execution engine is employed which picks up later instructions of the program and executes them out of their original order.</p><p>The out-of-order execution engine consists of an allocator which identifies the resources required by these micro instructions and allocates them based on their availability.</p><p>The allocator allocates resources for the micro instructions of one logical processor in one cycle and then switches to the other logical processor in the next cycle. If the uop queue has micro instructions for only one of the logical processors, or one of the logical processors has exhausted its share of resources, then the allocator uses all the cycles for the other logical processor.</p><p>So what are these resources that the allocator allocates to the micro instructions and how are they shared?</p><p>The first resource that the micro instructions need is registers. At the ISA level the processor might only have a very few registers (e.g. X86-64 has 16 general purpose integer registers), but at the microarchitecture level there are hundreds of physical integer registers, and similar number of floating-point registers. In an SMT enabled processor, these registers are divided equally between the two logical processors.</p><p>Apart from the registers, the backend also has a number of load and store buffers. These buffers are used for doing memory read and write operations. Again, in an SMT enabled processor, they are divided equally between the logical processors.</p><p>To enable out-of-order execution, the backend also needs to perform register renaming. Because at the ISA level there are only a handful of architectural registers, the program instructions will reuse the same register in many independent instructions. And, the out-of-order execution engine wants to execute these instructions ahead of their original order and in parallel. For doing so, it renames the original logical registers used in the program instructions to one of the physical registers. This mapping is maintained in the register alias table (RAT). </p><p>Because the two logical processors have their own sets of architectural registers, they also have their own copy of the RAT. </p><p>After the register renaming and allocator stages, the instructions are almost ready to execute. They are put into two sets of queues — one is for the memory read/write instructions and the other is for all other general instructions. These queues are also partitioned equally between the two logical processors in an SMT enabled core.</p><p>The processor has multiple instruction schedulers which operate in parallel. In each CPU cycle, some of the instructions from the instruction ready queues are pushed to the schedulers. The queues switch between the instructions of the two logical processors each cycle, i.e., in one cycle they push the instruction of one logical processor and in the next they switch to the second logical processor. </p><p>Each scheduler itself has a small internal buffer to store these pushed instructions temporarily until the scheduler can schedule them for execution. Each of these instructions need certain operands and execution units to be available. As soon as the required data and resources for one of the instructions become available, the scheduler dispatches that instruction for execution. </p><p>The schedulers do not care about the logical processors, they will execute a micro instruction as soon as the resources required by that instruction are available. But to ensure fairness, there is a limit on the number of active entries for a logical processor in the scheduler’s queue. </p><p>After the execution of an instruction finishes and its result is ready, it is placed in the reorder buffer. Even though the instructions are executed out-of-order, they need to be committed to the processor’s architecture state in their original program order. The reorder buffer enables this.</p><p>The reorder buffer is split equally between the two logical processors in an SMT enabled core.</p><p>The retirement unit tracks when the instructions are ready to be committed to the architecture state of the processor and retires them in their correct program order.</p><p>In an SMT enabled processor core, the retirement unit alternates between the micro instructions for each logical processor. If one of the logical processors does not have any micro instructions to be retired, then the retirement unit spends all the bandwidth on the other logical processor.</p><p>After an instruction retires, it might also have to write to the L1 cache. At this point the selection logic comes into the picture to do these writes, and that also alternates between the two logical processors each cycle to write the data to the cache.</p><p>While we have covered how the execution resources of a processor core are shared between the logical processors for an SMT enabled system, we have not talked about memory access. Let’s discuss what happens there.</p><p>The translation lookaside buffer (TLB) is a small cache which holds the translation of virtual addresses to physical addresses for data requests. The TLB is shared dynamically between the two logical processors on an as needed basis. To distinguish the entries for the two logical processors, each entry is also tagged with the logical processor id. </p><p>Each CPU core has its own private L1 cache. Depending on the microarchitecture, the L2 cache might also be private or it might be shared between the cores. If there is an L3 cache, it is shared between the cores. The caches are also oblivious to the existence of the logical processors.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png" width="866" height="470" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6010b920-377a-41bf-9a47-d5378699661d_866x470.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:470,&quot;width&quot;:866,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:77007,&quot;alt&quot;:&quot;Depiction of L1 and L2 caches in a 2 core processor. Each core has its own private L1 and L2 caches.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Depiction of L1 and L2 caches in a 2 core processor. Each core has its own private L1 and L2 caches." title="Depiction of L1 and L2 caches in a 2 core processor. Each core has its own private L1 and L2 caches." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6010b920-377a-41bf-9a47-d5378699661d_866x470.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Depiction of L1 and L2 caches in a 2 core processor. Each core has its own private L1 and L2 caches.</figcaption></figure></div><p>As the L1 (and possibly L2) cache is private to the core, it will contain data for both the logical processors on an as needed basis. This can cause conflict and eviction of the data and hamper the performance. On the other hand if the threads running on the two logical processors are working with the same set of data, the shared cache might improve their performance.</p><p>At this point we have covered almost everything about how SMT is implemented at the microarchitecture level and how the instructions for the two logical processors are executed in parallel. Now let’s discuss the performance impact.</p><p>As we have seen, enabling SMT on a CPU core requires sharing many of the buffers and execution resources between the two logical processors. Even if there is only one thread running on an SMT enabled core, these resources remain unavailable to that thread which reduces its potential performance.</p><p>Apart from the wasted shared resources in the absence of the 2nd thread, there is another performance impact. The operating system runs an idle loop on the unused logical processor which waits for instructions to arrive. This loop also wastes resources which could otherwise be spent on letting the other logical processor at its peak potential.</p><p>If you have two threads running on the two logical processors, then one of the things to think about is their cache access patterns. If the threads are using the cache aggressively and competing for it then they are bound to run into conflicts and evict each other’s data, which will degrade their performance.</p><p>On the other hand, if the threads are cooperating in nature, they might help improve each other’s performance. For instance, one thread is producing some data which is consumed by the other thread, then their performance will improve because of the data sharing in the cache.</p><p>If the two threads are not competing for cache, then they might be able to run fine without hampering each other’s performance, while improving the resource usage of the CPU core. </p><p>However, many experts believe that when absolute maximum performance is needed for a program, it is best to disable SMT so that the single thread will have all the resources available to it.</p><p><span>Apart from performance, there are also security issues associated with SMT which were discovered in the recent few years (see </span><a href="https://docs.oracle.com/en/operating-systems/oracle-linux/notice-smt/" rel="">this</a><span> and </span><a href="https://access.redhat.com/solutions/rhel-smt" rel="">this</a><span> for examples). Because of the shared resources and speculative execution of instructions, a lot of these issues open up possibilities of leaks of sensitive data to the attacker. As a result, the general advice has been to disable SMT in the systems. There is also </span><a href="https://www.extremetech.com/computing/intels-arrow-lake-cpus-will-allegedly-ditch-hyper-threading-leak" rel="">rumor</a><span> that because of these issues Intel might remove hyperthreading from their next generation of processors (Arrow Lake). </span></p><p>Let's wrap things up. Understanding how Simultaneous Multithreading (SMT) works is super helpful when you’re deciding whether or not to use it in your production servers. SMT was designed to make better use of CPU resources and boost instruction throughput. While it does a good job of that by letting multiple threads run at the same time, there are definitely some trade-offs to keep in mind.</p><p>Inside the processor, SMT means duplicating certain parts and sharing or dividing up others between the threads. This can lead to mixed results depending on what kind of work your CPU is handling. Sure, SMT can improve resource usage and system throughput overall, but it can also cause competition for shared resources, which can slow down individual threads.</p><p>Security is another big factor. Recent vulnerabilities have shown that sharing resources in SMT-enabled CPUs can be risky. Sensitive data could end up getting exposed, which is why some experts often recommend disabling SMT in security-critical systems.</p><p>So, should you use SMT? It really depends. If your workloads need the highest performance and lowest latency, turning SMT off might give you that edge. But if you’re running general-purpose tasks that can benefit from more parallelism, keeping SMT on could be a win.</p><p>By getting a handle on these details, you’ll be better equipped to decide what's best for your setup, ensuring you get the most efficient—and secure—performance out of your servers.</p><ul><li><p><a href="https://www.intel.com/content/dam/www/public/us/en/documents/research/2002-vol06-iss-1-intel-technology-journal.pdf" rel="">Intel Technology Journal, 2002, Vol 06, Issue 1</a></p></li><li><p><a href="https://www.princeton.edu/~rblee/ELE572Papers/SMT_Eggers.pdf" rel="">Simultaneous Multithreading: Maximizing On-Chip Parallelism</a></p></li><li><p><a href="https://arxiv.org/pdf/2310.12786" rel="">SYNPA: SMT Performance Analysis and Allocation of Threads to Cores in ARM Processors</a></p></li></ul><p>If you find my work interesting and valuable, you can support me by opting for a paid subscription (it’s $6 monthly/$60 annual). As a bonus you get access to monthly live sessions, and all the past recordings. </p><p><span>Many people report failed payments, or don’t want a recurring subscription. For that I also have a </span><a href="https://buymeacoffee.com/codeconfessions" rel="">buymeacoffee page</a><span>. Where you can buy me coffees or become a member. I will upgrade you to a paid subscription for the equivalent duration here.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://buymeacoffee.com/codeconfessions&quot;,&quot;text&quot;:&quot;Buy me a coffee&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://buymeacoffee.com/codeconfessions" rel=""><span>Buy me a coffee</span></a></p><p>I also have a GitHub Sponsor page. You will get a sponsorship badge, and also a complementary paid subscription here.</p><p data-attrs="{&quot;url&quot;:&quot;https://github.com/sponsors/abhinav-upadhyay&quot;,&quot;text&quot;:&quot;Sponsor me on GitHub&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://github.com/sponsors/abhinav-upadhyay" rel=""><span>Sponsor me on GitHub</span></a></p><p data-attrs="{&quot;url&quot;:&quot;https://blog.codingconfessions.com/p/simultaneous-multithreading?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:&quot;button-wrapper&quot;}" data-component-name="ButtonCreateButton"><a href="https://blog.codingconfessions.com/p/simultaneous-multithreading?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
    </channel>
</rss>