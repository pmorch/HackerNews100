<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 05 Feb 2024 13:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Parisians vote in favour of tripling parking costs for SUVs (112 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/feb/04/parisians-vote-in-favour-of-tripling-parking-costs-for-suvs</link>
            <guid>39259533</guid>
            <pubDate>Mon, 05 Feb 2024 10:30:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/feb/04/parisians-vote-in-favour-of-tripling-parking-costs-for-suvs">https://www.theguardian.com/world/2024/feb/04/parisians-vote-in-favour-of-tripling-parking-costs-for-suvs</a>, See on <a href="https://news.ycombinator.com/item?id=39259533">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Parisians have voted to <a href="https://www.theguardian.com/world/2023/dec/08/paris-mayor-plans-to-triple-suv-parking-tariffs-cut-air-pollution" data-link-name="in body link">triple parking costs</a> for sports utility vehicles (SUVs), as the city aims to tackle air pollution and climate breakdown by targeting rich drivers in heavy, large and polluting cars.</p><p>In a referendum on Sunday, which was closely watched by other capital cities, including <a href="https://www.theguardian.com/environment/2024/feb/02/london-could-introduce-suv-parking-charge-sadiq-khan-indicates" data-link-name="in body link">London</a>, 54.6% voted in favour of special parking fees for SUVs, according to provisional results. However, the turnout – at about 5.7% of Paris’s registered voters – was lower than green campaigners had hoped for.</p><p>“Parisians have made a clear choice … other cities will follow,” said Paris’s Socialist mayor, Anne Hidalgo, adding that road safety and air pollution were key reasons for the vote.</p><p>Hidalgo had previously described the move to curb the presence of SUVs through <a href="https://www.theguardian.com/world/2024/feb/02/paris-residents-set-to-vote-on-plan-to-triple-parking-charges-for-suvs" data-link-name="in body link">raising parking prices</a> as “a form of social justice”. She said the aim was to deliberately target the richest drivers of expensive, heavy and polluting cars who had not yet made changes to their behaviour to address the climate crisis.</p><figure id="e167ef5d-bdd4-4d1c-a179-7df58a0b180e" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:4,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Sadiq Khan says he will monitor effectiveness of Paris plan to raise charges on SUVs&quot;,&quot;elementId&quot;:&quot;e167ef5d-bdd4-4d1c-a179-7df58a0b180e&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/environment/2024/feb/02/london-could-introduce-suv-parking-charge-sadiq-khan-indicates&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>The new parking tariffs could come into force at the start of September. The cost of on-street parking for an SUV or 4x4 car would rise to €18 (£15) an hour in the centre of Paris and €12 an hour in the rest of the city.</p><p>The prices will apply to vehicles weighing more than 1.6 tonnes with a combustion engine or hybrid vehicles, and more than 2 tonnes for electric vehicles. The move will not apply to Paris residents’ parking.</p><p>Tony Renucci, director of the air quality campaign group Respire, said: “The result of the vote is a victory for Paris residents’ quality of life.” He added that Paris was sending a message that “the presence of these monsters on wheels was no longer desirable on our streets”.</p><p>Emmanuel Grégoire, Paris’s deputy mayor, posted on X as voting began: “Heavier, more dangerous, more polluting … SUVs are an environmental disaster.”</p><p>Last year, Paris held a similar vote on whether to ban <a href="https://www.theguardian.com/world/2023/aug/31/rented-e-scooters-cleared-from-paris-streets-on-eve-of-ban" data-link-name="in body link">rented electric scooters</a> and subsequently became the first European capital to do so. The turnout for that vote – 103,000 people, about 7% of registered voters – was higher than for the vote on SUVs.</p><p>Under Hidalgo, Paris has for years raised pressure on drivers by increasing parking costs and gradually banning diesel vehicles, while expanding the bicycle lane network in the congested capital. The city has reduced the number of on-street parking spaces in order to make drivers use underground parking. There was a 71% rise in the use of bikes between the end of the Covid lockdowns and 2023, city hall said.</p><p>Paris’s deputy mayor in charge of transport, David Belliard, of the Green party, said about 10% of vehicles in Paris would be hit by the higher parking fees, which could bring in up to €35m for the city each year.</p><p>The motorists’ lobby group 40 Millions d’Automobilistes had argued that drivers should be free to choose whatever vehicle they want, warning that the move to raise parking tariffs was unjustified and the work of “an ultra-urban and anti-car minority”.</p><p>Christophe Béchu, France’s environment minister, Christophe Béchu, told broadcaster RTL that the SUV surcharge amounted to “a kind of punitive environmentalism” – even if drivers should “opt for lighter vehicles”.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deno in 2023 (151 pts)]]></title>
            <link>https://deno.com/blog/deno-in-2023</link>
            <guid>39259068</guid>
            <pubDate>Mon, 05 Feb 2024 09:23:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/deno-in-2023">https://deno.com/blog/deno-in-2023</a>, See on <a href="https://news.ycombinator.com/item?id=39259068">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In 2023, Deno shifted into high gear with respect to Node/npm compatibility and
performance work, while continuing to move towards our goal of radically
simplifying web development. Here’s a summary of what changed in 2023:</p>
<ul>
<li>Deno now understands <code>package.json</code> files and has the ability to import
<strong>built-in Node modules</strong> using <code>node:</code> specifiers like <code>node:fs</code> and
<code>node:crypto</code>. <a href="#enhanced-compatibility-with-node-and-npm">Read more.</a></li>
<li>A new web server API,
<a href="#simpler-faster-web-server-with-denoserve"><code>Deno.serve</code>, was stabilized</a> and
HTTP throughput improved ~73% over the year.</li>
<li><strong><code>deno compile</code></strong> got support for
<a href="https://deno.com/blog/v1.32#deno-compile-support-for-web-workers-and-dynamic-import" rel="noopener noreferrer">workers, dynamic imports</a>,
and <a href="https://deno.com/blog/v1.34#deno-compile-supports-npm-packages" rel="noopener noreferrer">npm modules</a>.</li>
<li>Deno made better use of <strong>web streams</strong> (<code>ReadableStream</code> and
<code>WriteableStream</code>) and <code>AbortController</code> throughout its APIs to narrow the gap
between browser and server-side programming.</li>
<li><a href="#more-flexible-denojson">Deno’s configuration file, <code>deno.json</code> , now doubles as an import map</a>.
We flattened the schema, added glob support, and a useful top-level
<code>"exclude"</code> attribute — all which allows for terse adjustments to how Deno is
executed.</li>
<li><a href="#jupyter-notebooks"><strong>Jupyter</strong>, the open source notebook tool, added support for JavaScript and TypeScript using Deno</a></li>
<li><a href="#webgpu"><strong>WebGPU</strong> was finally added to Deno</a> after nearly a year of
development.</li>
<li>Deno’s <strong>zero configuration TypeScript</strong> got better editor integration with
<a href="#smoother-development-experience-with-lsp">substantial improvements in the LSP and VS Code integration</a>.</li>
<li><a href="#fresh"><strong>Fresh</strong>, the Deno native web framework grew in functionality</a> with
proper Tailwind support, Partials, and layout files.</li>
<li><a href="#expanding-denos-cloud-business"><strong>Deno Deploy</strong> became more powerful</a> with
globally distributed primatives: <strong>Deno KV, Queues, Cron</strong>.</li>
<li><a href="#expanding-denos-cloud-business">We released <strong>self-service Subhosting</strong></a> for
platforms looking to deploy and execute untrusted multi-tenant JavaScript.</li>
</ul>
<p>Let’s dive deeper into the changes below.</p>
<h2 id="enhanced-compatibility-with-node-and-npm">Enhanced Compatibility with Node and npm</h2><p>In a significant move toward ecosystem harmony, Deno expanded its capabilities
in 2023 by incorporating built-in Node modules, such as <code>node:fs</code> and
<code>node:child_process</code>, accessible through the <code>node:</code> specifier. This addition
builds on the native npm support introduced in late 2022, using the <code>npm:</code>
specifier, further bridging the gap between Deno and the Node ecosystem. As of
now, Deno boasts
<a href="https://docs.deno.com/runtime/manual/node/compatibility" rel="noopener noreferrer">partial or full support for 38 of the 42 Node built-in APIs</a>,
marking a substantial stride in compatibility.</p>
<p>To facilitate a smoother transition from Node to Deno, we introduced several new
features:</p>
<ul>
<li>The
<a href="https://docs.deno.com/runtime/manual/tools/unstable_flags#--unstable-byonm" rel="noopener noreferrer"><code>--unstable-byonm</code> flag</a>
allows the use of your preferred npm package manager within Deno, enhancing
flexibility.</li>
<li>With
<a href="https://docs.deno.com/runtime/manual/tools/unstable_flags#--unstable-sloppy-imports" rel="noopener noreferrer">the <code>--unstable-sloppy-imports</code> flag</a>,
we’ve relaxed the strictness of module imports, accommodating a wider range of
coding styles and practices.</li>
<li>The
<a href="https://docs.deno.com/runtime/manual/tools/unstable_flags#--unstable-unsafe-proto" rel="noopener noreferrer"><code>--unstable-unsafe-proto</code> flag</a>
introduces support for <code>Object.prototype.__proto__</code>, a feature upon which
numerous npm packages depend.</li>
</ul>
<p>These enhancements are particularly useful for developers looking to run
existing Node projects in Deno without extensive modifications.</p>
<p>A notable advancement in the past year was the
<a href="https://deno.com/blog/v1.31#stabilization-of-node-api" rel="noopener noreferrer">stabilization of the Node-API</a>. This
development eliminates the need for the <code>--unstable</code> flag when utilizing npm
packages dependent on Node-API, thereby broadening the range of supported npm
packages and streamlining the integration process.</p>
<h2 id="simpler-faster-web-server-with-denoserve">Simpler, faster web server with <code>Deno.serve()</code></h2><p>We’ve significantly streamlined web server creation in Deno by introducing and
stabilizing the <code>Deno.serve()</code> function, allowing developers to launch a server
with minimal code:</p>
<div><pre><span>Deno</span><span>.</span><span>serve</span><span>(</span><span>(</span><span>req</span><span>)</span> <span>=&gt;</span> <span>new</span> <span>Response</span><span>(</span><span>"Hello, world"</span><span>)</span><span>)</span><span>;</span></pre></div><p>This enhancement is part of our ongoing commitment to simplify development
processes and reduce the need for extensive boilerplate code. The <code>Deno.serve()</code>
function embodies this philosophy, enabling efficient and concise server setup.</p>
<p>Performance has seen substantial improvements through targeted optimizations in
the core libraries and the event loop mechanism. In benchmark tests using a
basic “Hello, world” server setup, Deno’s HTTP throughput has seen remarkable
gains, nearly doubling since late 2023. When compared to a similar Node.js
server, Deno now demonstrates a ~61% increase in throughput, alongside notable
enhancements in tail latency and memory efficiency.</p>
<figure>

<p><img src="https://deno.com/blog/deno-in-2023/http-benchmark.png" alt="HTTP benchmark in requests per second" title=""></p>
<figcaption>Run on bare metal 8 core,  64GB ram, Intel Xeon E-2378G with `wrk -d 10s`</figcaption>

</figure>

<p>These advancements are not limited to Deno-specific projects; they extend to
modules and applications utilizing the <code>node:http</code> module, thanks to
<code>Deno.serve</code>’s underlying architecture.</p>
<p>A key factor in these performance gains is the improved integration of Deno’s
HTTP interfaces with the <a href="https://hyper.rs/" rel="noopener noreferrer">Hyper</a> and
<a href="https://docs.rs/reqwest/latest/reqwest/" rel="noopener noreferrer">Reqwest</a> libraries. This integration
has minimized unnecessary data allocation and duplication across different
layers of the Deno runtime.</p>
<p>Furthermore, we’ve optimized the Deno event loop, responsible for managing
asynchronous operations and resource monitoring, to reduce overhead and enhance
the runtime’s overall efficiency. These collective improvements underscore our
dedication to providing a robust, high-performance environment for web
development.</p>
<h2 id="more-flexible-denojson">More flexible <code>deno.json</code></h2><p>At Deno, we stand by the principle of zero-configuration programming,
particularly valuing the simplicity of single-file programs, even for those
written in TypeScript. Recognizing that larger projects often demand more
sophisticated setups, we’ve continuously improved our optional <code>deno.json</code>
configuration file to meet these complex needs without sacrificing ease of use:</p>
<ul>
<li><strong>Streamlined Configuration</strong>: We’ve transformed <code>deno.json</code> to double as an
import map, effectively reducing the need for separate configuration files and
simplifying project setups.
<a href="https://deno.com/blog/v1.30#denojson-becomes-an-import-map" rel="noopener noreferrer">Learn more about import maps</a>.</li>
<li><strong>Enhanced Formatting Options</strong>: <code>deno fmt</code> now supports semicolons, offering
more flexibility in code styling to accommodate diverse developer preferences.</li>
<li><strong>Node and npm Compatibility</strong>: Integration with <code>package.json</code> enhances
compatibility, making it easier for projects to transition between Node and
Deno environments.
<a href="https://deno.com/blog/v1.31#packagejson-support" rel="noopener noreferrer">See how we improved compatibility</a>.</li>
<li><strong>Simplified Configuration Structure</strong>: We’ve flattened the <code>deno.json</code>
structure, making it more intuitive and easier to navigate for developers.
<a href="https://deno.com/blog/v1.33#flatter-denojson-configuration" rel="noopener noreferrer">Discover the simpler structure</a>.</li>
<li><strong>Glob Support</strong>: The introduction of glob patterns in <code>deno.json</code> allows for
more precise control over file and directory inclusion or exclusion in various
operations such as formatting, linting, and testing.
<a href="https://deno.com/blog/v1.34#glob-support-in-denojson-and-cli-flags" rel="noopener noreferrer">Explore glob support details</a>.</li>
</ul>
<p>These enhancements are part of our ongoing commitment to making Deno not only
powerful and versatile for large-scale applications but also simple and
accessible for smaller projects.</p>
<h2 id="smoother-development-experience-with-lsp">Smoother development experience with LSP</h2><p>Deno’s Language Server Protocol (LSP) integration elevates the development
experience within editors and IDEs, offering robust features like precise
go-to-definition, comprehensive IntelliSense, and seamless code formatting for
TypeScript projects. This year, we’ve dedicated significant effort to enhance
the LSP, making coding in Deno smoother and more intuitive:</p>
<ul>
<li><strong>Extended Auto-Complete</strong>: Now includes support for <code>npm:</code> and <code>node:</code>
specifiers, streamlining the use of Node modules within Deno.</li>
<li><strong>VSCode Extension Activation</strong>: The Deno VSCode extension is now triggered
automatically when a <code>deno.json</code> file is detected in your project, ensuring
immediate access to Deno’s powerful tooling.</li>
<li><strong>Intelligent Import Management</strong>: Imports in TypeScript and JavaScript files
are now updated automatically when files are renamed, maintaining code
consistency and reducing manual refactoring.</li>
<li><strong>Efficient Document Pre-Loading</strong>: Ensures features like “find references”
work seamlessly across all files in a project, enhancing code navigation and
understanding.</li>
</ul>
<p>To fully leverage these improvements, try the
<a href="https://marketplace.visualstudio.com/items?itemName=denoland.vscode-deno" rel="noopener noreferrer">Deno extension for Visual Studio Code</a>,
designed to integrate these enhancements directly into your development
workflow.</p>
<h2 id="webgpu">WebGPU</h2><p>Deno has now integrated
<a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API" rel="noopener noreferrer">WebGPU</a>, a
cutting-edge technology that empowers developers to harness the power of GPU
hardware directly with JavaScript. This high-performance, low-level interface is
designed for a wide range of applications, from graphics rendering to data
analysis, and machine learning, all within the familiar environment of
web-standard JavaScript.</p>
<p>After a year of dedicated development, WebGPU is accessible in Deno behind the
<code>--unstable-webgpu</code> flag, marking a significant milestone in expanding the
capabilities of Deno applications. This feature is especially promising for
developers looking to push the boundaries of what’s possible with JavaScript in
areas requiring intense computational power.</p>
<p>We are also actively developing features to enable WebGPU for rendering in
native GUI windows, further broadening the potential use cases for Deno
developers.
<a href="https://deno.com/blog/v1.40#webgpu-windowing--bring-your-own-window" rel="noopener noreferrer">Stay updated on this feature’s progress</a>.</p>
<p>To explore practical applications and see WebGPU in action within Deno, visit
our <a href="https://github.com/denoland/webgpu-examples" rel="noopener noreferrer">WebGPU examples repository</a>,
which provides a variety of sample projects and code snippets.</p>
<h2 id="jupyter-notebooks">Jupyter notebooks</h2><p><a href="https://jupyter.org/" rel="noopener noreferrer">Jupyter</a>, the open source notebook tool, added support
for JavaScript and TypeScript using Deno. This means data science,
visualization, and more can all be done using modern JavaScript and TypeScript
and web standards APIs.</p>
<p>Here’s an example of grabbing data with <code>fetch</code> and visualizing it with
<code>observablehq/plot</code>:</p>
<p><img src="https://deno.com/blog/deno-in-2023/jupyter-scatter-plot.png" alt="A scatter plot in Jupyter notebooks using TypeScript with Deno" title=""></p>
<p>Jupyter support has also enabled building generative AI/ML models using
JavaScript and TypeScript, as Andrew Ng and DeepLearning.AI have developed
<a href="https://www.deeplearning.ai/short-courses/build-llm-apps-with-langchain-js/" rel="noopener noreferrer">a generative AI course on building LLM Apps with LangChain.js</a>
that uses Deno.</p>
<h2 id="notable-open-source-rust-crates">Notable open source rust crates</h2><p>Deno’s surface area touches a wide range of open source projects, which we
eagerly contribute to in order to expand Deno’s feature set and optimize
performance. This year, we released a few Rust crates that developers might find
useful independently of Deno itself:</p>
<ul>
<li><a href="https://github.com/denoland/rustls-tokio-stream" rel="noopener noreferrer"><code>rustls-tokio-stream</code></a> a
Rust crate that replaces tokio-rustls adding more advanced features like
supporting duplex I/O via&nbsp;<code>tokio::io::split</code>. Critically it does not require
either read or write polling to perform handshakes.</li>
<li><a href="https://github.com/denoland/fastwebsockets" rel="noopener noreferrer"><code>fastwebsockets</code></a>&nbsp;is a minimal,
fast WebSocket server implementation that sits behind Deno’s WebSocket
implementation. It completely passes the&nbsp;Autobahn TestSuite&nbsp;and fuzzed with
LLVM’s&nbsp;libfuzzer. You can use it as a raw websocket frame parser and deal with
spec compliance yourself, or you can use it as a full-fledged websocket
server.</li>
<li><a href="https://github.com/denoland/monch" rel="noopener noreferrer"><code>monch</code></a> is a parser inspired
by&nbsp;<a href="https://crates.io/crates/nom" rel="noopener noreferrer"><code>nom</code></a>, but specifically for strings and
with some additional combinators we use in Deno. It backs <code>deno_task_shell</code>.</li>
<li><a href="https://github.com/denoland/deno_task_shell" rel="noopener noreferrer"><code>deno_task_shell</code></a> is a cross
platform shell implementation that helps <code>deno task</code> run across windows and
unix. Think of it as a more advanced version of the common Node.js utility
<code>cross-env</code>.</li>
</ul>
<h2 id="fresh">Fresh</h2><p>We continued to make&nbsp;<a href="https://fresh.deno.dev/" rel="noopener noreferrer">Fresh</a>, Deno’s modern full stack
web framework that sends zero client-side JavaScript by default, easier to use
and more performant.</p>
<ul>
<li>We’ve added support
for&nbsp;<a href="https://deno.com/blog/fresh-1.4#layouts" rel="noopener noreferrer"><code>_layout</code>&nbsp;files to allow for sharing components across routes</a>,
increased flexibility
in&nbsp;<a href="https://deno.com/blog/fresh-1.4#organise-your-code-with-route-groups" rel="noopener noreferrer">organizing code with route groups</a>,
and removed boilerplate in passing data from a route handler to a component by
introducing&nbsp;<a href="https://deno.com/blog/fresh-1.3#async-route-components" rel="noopener noreferrer">async route components</a>.
We
also&nbsp;<a href="https://deno.com/blog/fresh-1.3#fresh-linting-rules" rel="noopener noreferrer">improved linting rules and error messages in the editor</a>,
as well
as&nbsp;<a href="https://deno.com/blog/fresh-1.2#simplified-testing-of-fresh-projects" rel="noopener noreferrer">simplified testing</a>.</li>
<li>Islands, the core of Fresh’s design, also received a ton of new features
making it more robust so it can handle a wider range of use cases.
We&nbsp;<a href="https://deno.com/blog/fresh-1.2#passing-signals-uint8arrays-and-circular-data-in-island-props" rel="noopener noreferrer">broadened the kinds of data that you could pass to island props</a>&nbsp;to
include Preact Signals, JSX, JSON, and more. Also, islands can
now&nbsp;<a href="https://deno.com/blog/fresh-1.2#passing-jsx-to-islands-and-nesting-islands-within-each-other" rel="noopener noreferrer">be nested</a>,&nbsp;<a href="https://deno.com/blog/fresh-1.2#subdirectories-in-the-islands-folder" rel="noopener noreferrer">organized in subdirectories</a>,
and&nbsp;<a href="https://deno.com/blog/fresh-1.3#export-multiple-islands-in-the-same-file" rel="noopener noreferrer">many of them can be exported from the same file</a>.</li>
<li>Fresh’s performance also increased, with the
new&nbsp;<a href="https://deno.com/blog/fresh-1.4#faster-page-loads-with-ahead-of-time-compilation-1" rel="noopener noreferrer">ahead-of-time compilation step to cache client-side assets</a>&nbsp;(you
can enable this optimization step in Deno Deploy with GitHub Actions instead
of GitHub Automatic), snappier client-side navigation
with&nbsp;<a href="https://deno.com/blog/fresh-1.5#client-side-navigation-with-partials" rel="noopener noreferrer">the introduction of Partials</a>,
as well as
an&nbsp;<a href="https://deno.com/blog/fresh-1.6#improved-islands-bundling-strategy" rel="noopener noreferrer">improved islands bundling strategy</a>.</li>
<li>We’ve improved Fresh plugin system to be able
add&nbsp;<a href="https://deno.com/blog/fresh-1.6#plugin-api-enhancements" rel="noopener noreferrer">islands</a>,&nbsp;<a href="https://deno.com/blog/fresh-1.3#adding-routes-andor-middlewares-from-plugins" rel="noopener noreferrer">middlewares, and routes</a>.
One notable win
is&nbsp;<a href="https://deno.com/blog/fresh-1.6#first-class-tailwind-css-plugin" rel="noopener noreferrer">moving off TwindCSS to TailwindCSS</a>,
which is actively maintained and is a performance boost. The Fresh community
also grew,
with&nbsp;<a href="https://github.com/uki00a/awesome-fresh/" rel="noopener noreferrer">more projects either built in Fresh or libraries and plugins built for Fresh</a>.</li>
</ul>
<p>We’ve got a lot planned for Fresh in 2024, such
as&nbsp;<a href="https://github.com/denoland/fresh/issues/2108" rel="noopener noreferrer">view transitions, hot module reloading, and faster JSX transforms</a>.</p>
<h2 id="expanding-denos-cloud-business">Expanding Deno’s cloud business</h2><p>Though we’ve covered the big updates from our open source projects, the overall
Deno picture would be incomplete without mentioning developments on the
commercial side.</p>
<p>Though&nbsp;<a href="https://deno.com/subhosting" rel="noopener noreferrer"><strong>Deno Subhosting</strong></a>&nbsp;has been around for a
while&nbsp;<a href="https://deno.com/blog/netlify-edge-functions-on-deno-deploy" rel="noopener noreferrer">powering Netlify’s edge functions</a>&nbsp;and&nbsp;<a href="https://deno.com/blog/deco-cx-subhosting-serve-their-clients-storefronts-fast" rel="noopener noreferrer">Deco.cx’s customer’s e-commerce stores</a>,
this year&nbsp;<a href="https://deno.com/blog/subhosting" rel="noopener noreferrer">we made it self-service</a>&nbsp;so that anyone can deploy
and run their user’s code
programmatically&nbsp;<a href="https://apidocs.deno.com/#get-/projects/-projectId-/deployments" rel="noopener noreferrer">via our Subhosting API</a>&nbsp;for
free. It’s built to run third-party, untrusted code securely, as it’s
<a href="https://deno.com/blog/subhosting-security-run-untrusted-code" rel="noopener noreferrer">designed from the ground up for maximum tenant isolation</a>.
(Not sure what to use Deno Subhosting API for? Check out
<a href="https://deno.com/blog/deno-in-2023#fresh-easier-and-faster-is-gaining-momentum-%EF%B8%8F" rel="noopener noreferrer">this tutorial on how to build your own cloud IDE</a>.)</p>
<p>We’re made strides towards our vision of radically simplifying web development
with the launch of cloud primitives: <strong><a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a>, <a href="https://deno.com/blog/queues" rel="noopener noreferrer">Queues</a>,
and <a href="https://deno.com/blog/cron" rel="noopener noreferrer">Cron</a></strong>. They’re built right into the runtime so you can get
setup without juggling API keys or futzing with config:</p>
<div><pre><span>const</span> kv <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span></pre></div><p>These cloud primitives seamlessly become globally distributed services when you
use Deno Deploy, optimizing your servers and applications for performance.</p>
<p>We’ve also broadened access to Deno KV by turning it
into&nbsp;<a href="https://github.com/denoland/denokv" rel="noopener noreferrer">its own open sourced binary</a>, added
support&nbsp;<a href="https://docs.deno.com/kv/manual/on_deploy#connect-to-managed-databases-from-outside-of-deno-deploy" rel="noopener noreferrer">to remotely connect to any Deno KV instance</a>,
made it accessible in Node/npm
via&nbsp;<a href="https://www.npmjs.com/package/@deno/kv" rel="noopener noreferrer">our official Deno KV npm module</a>,
as well as
adding&nbsp;<a href="https://docs.deno.com/kv/manual/backup" rel="noopener noreferrer">support for continuous replication with point-in-time recovery to S3 and GCS</a>.</p>
<p>We’ve got some big plans to simplify cloud development even further with more
features and new primitives, so stay tuned.</p>
<h2 id="deno-2-">Deno 2 👀</h2><p>We’re preparing for Deno 2, which will offer improved compatibility with Node
and npm, by
<a href="https://docs.deno.com/runtime/manual/advanced/migrate_deprecations" rel="noopener noreferrer">providing a migration guide</a>
to ensure a smooth transition. Alongside an improved runtime, we also have some
exciting announcements around managing and optimizing dependencies for your
projects. Stay tuned in the coming weeks for a more detailed roadmap of what’s
to come. If you want a sneak peak - <a href="https://jsr.io/waitlist" rel="noopener noreferrer">look here</a>.</p>
<p><em>Don’t miss any updates! <a href="https://twitter.com/deno_land" rel="noopener noreferrer">Follow us on Twitter</a>.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Caesars abruptly cancels contract with DEF CON (423 pts)]]></title>
            <link>https://forum.defcon.org/node/248360</link>
            <guid>39256930</guid>
            <pubDate>Mon, 05 Feb 2024 03:22:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.defcon.org/node/248360">https://forum.defcon.org/node/248360</a>, See on <a href="https://news.ycombinator.com/item?id=39256930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
							
								
								<div> <p><a href="https://filedata/fetch?id=248362&amp;d=1707096806" target="_blank" rel="nofollow"></a><a href="https://forum.defcon.org/filedata/fetch?id=248362&amp;d=1707096806"><img alt="Click image for larger version  Name:	IMG_0685.jpg Views:	0 Size:	91.2 KB ID:	248362" title="IMG_0685.jpg" data-attachmentid="248362" width="600" height="464" data-align="center" src="https://forum.defcon.org/filedata/fetch?id=248362&amp;d=1707096806&amp;type=medium" data-fullsize-url="filedata/fetch?id=248362&amp;d=1707096806" data-thumb-url="filedata/fetch?id=248362&amp;d=1707096806&amp;type=thumb" data-title="Click on the image to see the original version" data-caption="IMG_0685.jpg"></a></p><br>
 </div>    <div>
<p><span><b>DEF CON was canceled. We un-canceled it.</b></span></p></div> <p>


After a great 25 year relationship Caesars abruptly terminated their contract with DEF CON, leaving us with no venue for DC 32, and just about seven months to Con!</p><p>

We don’t know why Caesars canceled us, they won’t say beyond it being a strategy change and it is not related to anything that DEF CON or our community has done. This kind of no-notice cancellation of a contract is unheard of in the conference business. The parting is confusing, but amicable.</p><p>

So now we have a challenge. Without a venue, will we be able to UN-Cancel DEF CON 32 before time runs out?</p><p>

Hackers are flexible. We find solutions. We need a space that can handle an event our size, and configurable enough to accommodate our content. We need a location close to our announced dates, and with super short notice... No small feat.</p><p>

We immediately scrambled a venue strike team to Las Vegas. Floors were walked. Meetings were held. Hands were shook and options weighed. When the smoke cleared, the field narrowed to one obvious choice and we began forging the requisite agreements.</p><p>

W00T! DEF CON Is UN-CANCELED!</p><p>

DEF CON 32 will still be August 8-11 2024, but now held at the Las Vegas Convention Center (LVCC) with workshops and training at the Sahara.</p><p>

DEF CON 32 will be an adventure where we can try things not possible in our old Casino Hotel spaces. What specifically you ask? Well we are still learning all the specifics but we will have more space, a proper food court, and the largest indoor venue LCD wall in the country.<br>
There are still many questions to be answered, and we have started a live FAQ section on the Forums for DEF CON 32 where we will be updating questions and answers. The initial FAQ is located here: <a href="https://forum.defcon.org/node/248358" target="_blank">https://forum.defcon.org/node/248358</a></p><p>

I look forward to seeing everyone this summer, the start of a new DEF CON era!</p><p>

The Dark Tangent</p><p>

P.S. We made shirts and stickers:<br>
<a href="https://shop.defcon.org/products/def-con-un-canceled-mens-t-shirt" target="_blank">https://shop.defcon.org/products/def...d-mens-t-shirt</a><br>
<a href="https://shop.defcon.org/products/def-con-un-canceled-sticker-set" target="_blank">https://shop.defcon.org/products/def...ed-sticker-set</a></p></div><div><p>
		
		
			Last edited by <a href="https://forum.defcon.org/member/6-dark-tangent" data-vbnamecard="6">The Dark Tangent</a>; <span>1 hour ago</span>.
		
		

	</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A practical guide to quitting your smartphone (139 pts)]]></title>
            <link>https://www.nytimes.com/2024/02/01/technology/iphone-mental-health-flip-phone.html</link>
            <guid>39256176</guid>
            <pubDate>Mon, 05 Feb 2024 01:03:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/02/01/technology/iphone-mental-health-flip-phone.html">https://www.nytimes.com/2024/02/01/technology/iphone-mental-health-flip-phone.html</a>, See on <a href="https://news.ycombinator.com/item?id=39256176">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/02/01/technology/iphone-mental-health-flip-phone.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Netflix: Piracy is difficult to compete against and growing rapidly (362 pts)]]></title>
            <link>https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/</link>
            <guid>39254807</guid>
            <pubDate>Sun, 04 Feb 2024 21:44:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/">https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/</a>, See on <a href="https://news.ycombinator.com/item?id=39254807">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>

<span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to TorrentFreak." href="https://torrentfreak.com/"><span property="name">Home</span></a><meta property="position" content="1"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Piracy category archives." href="https://torrentfreak.com/category/piracy/"><span property="name">Piracy</span></a><meta property="position" content="2"></span> &gt; <span></span>
</p>
<p>
<span> </span>
As a member of ACE and the MPA, Netflix is at the frontline of the global battle against online piracy. The company doesn't often address the subject directly but in a recent SEC filing, Netflix writes that it's difficult to compete against the free entertainment piracy offers. Not only that, it's growing rapidly too.
</p>
</div><div>
<p><img decoding="async" src="https://torrentfreak.com/images/netflix-logo-1.jpg" alt="netflix logo" width="300" height="180" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20180'%3E%3C/svg%3E" data-lazy-src="https://torrentfreak.com/images/netflix-logo-1.jpg">From the launch of its online streaming service fifteen years ago, Netflix positioned itself as a piracy competitor.</p>
<p>The idea was to take market share away from piracy sites, by offering a legal and more convenient streaming platform.</p>
<p>Initially, this seemed to work. Netflix amassed hundreds of millions of subscribers, some of whom left their piracy habits behind. However, as the ‘streaming wars’ turned legal and convenient streaming platforms into isolated and pricey content silos, momentum started to shift. </p>
<p>In recent years piracy <a href="https://torrentfreak.com/canada-is-a-video-piracy-hotspot-while-brazil-shows-positive-signs-240121/">started to grow again</a>, including in well-served markets such as the United States. In theory, this <a href="https://torrentfreak.com/could-piracy-help-netflix-win-the-streaming-wars-240108/">may help Netflix</a> in its battle with other legal platforms, but that’s a consolation prize if the war against piracy is lost. </p>
<p>There are no concrete signs that Netflix is crumbling, but piracy is a concern. This <a href="https://torrentfreak.com/netflix-sees-popcorn-time-as-a-serious-competitor-150121/">isn’t breaking news</a>; piracy has been repeatedly highlighted as tough competition in the company’s <a href="https://www.investopedia.com/terms/1/10-k.asp">10-K filings</a> at the SEC.</p>
<h2>Piracy is a Tough Competitor</h2>
<p>Earlier this week, Netflix submitted its latest 10-K filing. The mandatory document provides information that helps investors to gather key information about publicly traded companies. In the “competition” section of the annual overview, piracy is again mentioned several times.</p>
<center><img decoding="async" src="https://torrentfreak.com/images/sec-netflix.jpg" alt="sec netflix" width="600" height="366" srcset="https://torrentfreak.com/images/sec-netflix.jpg 1166w, https://torrentfreak.com/images/sec-netflix-300x183.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20600%20366'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/sec-netflix.jpg 1166w, https://torrentfreak.com/images/sec-netflix-300x183.jpg 300w" data-lazy-src="https://torrentfreak.com/images/sec-netflix.jpg"></center>
<p>Netflix explains that the online video landscape is a competitive business. New services and distribution models could impact the business of the leading video streaming platform. This includes legal competitors as well as piracy. </p>
<p>“The various economic models underlying these channels include subscription, transactional, ad-supported and piracy-based models. All of these have the potential to capture meaningful segments of the entertainment video market,” Netflix writes. </p>
<p>These are in part standard disclosures, as every company faces competition. However, Netflix believes that online piracy is particularly compelling because it’s free for consumers. That makes it very hard to compete against. </p>
<p>“Piracy also threatens to damage our business, as its fundamental proposition to consumers is so compelling and difficult to compete against: virtually all content for free,” Netflix writes.</p>
<h2>Growing and Hard to Stop</h2>
<p>When Netflix launched, its on-demand streaming experience was more convenient than most pirate sites. At the time, torrent sites were dominant but still required users to have some technical knowledge and the patience to wait for content to download.</p>
<p>Today, most pirate sites use on-demand streaming, taking away a major edge for Netflix. And because piracy is so compelling for consumers, it is growing rapidly worldwide, threatening legal services. </p>
<p>“In light of the compelling consumer proposition, piracy services are subject to rapid global growth, and our efforts to prevent that growth may be insufficient,” Netflix notes. </p>
<p>“If we are unable to successfully or profitably compete with current and new competitors, our business will be adversely affected, and we may not be able to increase or maintain market share, revenues or profitability.”</p>
<h2>(Un)authorized Copying?</h2>
<p>The concerns voiced by Netflix are real, but the company isn’t near its demise. These 10-K filings are supposed to detail risks and Netflix is not the only company mentioning piracy as a potential threat. </p>
<center><strong>A Netflix Competitor</strong></center><br><center><img decoding="async" src="https://torrentfreak.com/images/netflix-compet.jpg" alt="netflix competitor" width="600" height="464" srcset="https://torrentfreak.com/images/netflix-compet.jpg 1342w, https://torrentfreak.com/images/netflix-compet-300x232.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20600%20464'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/netflix-compet.jpg 1342w, https://torrentfreak.com/images/netflix-compet-300x232.jpg 300w" data-lazy-src="https://torrentfreak.com/images/netflix-compet.jpg"></center>
<p>When we started looking for similar mentions by other businesses, we stumbled upon similar concerns and, strangely enough, some identical ones. Apparently, there’s quite a bit of copying going on, as SEC filings from several companies include identical passages.</p>
<p><em><a href="https://d18rn0p25nwr6d.cloudfront.net/CIK-0001065280/c5e64982-659f-4726-97c9-c57767c3bec3.pdf">Netflix</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</em></p><p><em>
<p><a href="https://www.sec.gov/Archives/edgar/data/1936037/000119312524007757/d356530ds1a.htm">Triller Corp</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</p>
<p><a href="https://www.sec.gov/Archives/edgar/data/1484769/000162828023005135/fubo-20221231.htm">FuboTV</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</p>
<p><a href="https://ir.cssentertainment.com/node/11811/html">Redbox Entertainment</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</p>
<p><a href="https://finance.yahoo.com/sec-filing/IMAQ/0001654954-23-010817_1846235?guccounter=1">IMAQ</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</p>
</em></p><p><em><a href="https://www.sec.gov/Archives/edgar/data/1776909/000121390020035136/f424b31120_curiositystream.htm">CuriosityStream</a>: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth”</em></p>
<p>We don’t know where these references originate. Netflix has mentioned it for a while, that’s for sure, and apparently, the use of this language is widespread and subject to rapid global growth.</p>
<p>It’s clear, however, that piracy is a concern for Netflix. While Reed Hastings <a href="https://torrentfreak.com/netflix-uses-pirate-sites-to-determine-what-shows-to-buy-130914/">wasn’t worried about piracy</a> a decade ago, the company now spends millions of dollars tackling the problem. </p>
<p>The streaming giant <a href="https://torrentfreak.com/netflix-becomes-a-member-of-the-mpaa/">joined the MPA</a> a few years ago and is also a <a href="https://torrentfreak.com/mpaa-dramatically-expanding-ace-global-anti-piracy-coalition-190507/">member of anti-piracy coalition ACE</a>. In addition, Netflix also has an <a href="https://torrentfreak.com/netflix-continues-to-expand-its-global-anti-piracy-team-220307">in-house anti-piracy department</a> that keeps an eye on piracy threats.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Solving the darknet riddle (2022) (111 pts)]]></title>
            <link>https://sizeof.life/posts/darknet-riddle/</link>
            <guid>39254274</guid>
            <pubDate>Sun, 04 Feb 2024 20:47:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sizeof.life/posts/darknet-riddle/">https://sizeof.life/posts/darknet-riddle/</a>, See on <a href="https://news.ycombinator.com/item?id=39254274">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p>An ‘.onion’ link was posted on Reddit back in June 2020 with a suspected riddle. It got me curious.</p>
<p>Some random characters and a sentence: “How deep can you enter?”</p>
<p>Within the page source-code a comment was present with a series of numbers/letters that looked like a hexadecimal.</p>

  <figure>
    <img src="https://sizeof.life/img/hex.png">
    
  </figure>


<p>After converting the hex to ASCII it read: ‘chaos is the key’.</p>
<p>I noticed that when the page is refreshed, every time a different letter is highlighted in yellow.</p>
<p>When the ‘x’ char was highlighted a link was appended to the word ‘enter’…

  </p><figure>
    <img src="https://sizeof.life/img/x.png">
    
  </figure>


<p>The link was to an image (below).</p>

  <figure>
    <img src="https://sizeof.life/img/hidden_message01.png">
    
  </figure>


<p>After google translating the text in the image which did not reveal anything useful, it was time to examine the image file itself.</p>
<p>I suspected the next “message” would be hidden in image metadata or perhaps a pixel analysis would need to be done but after downloading the file and opening it, the message was already visible.</p>
<p>See the image area below, it may not be visible in the browser but if you download it and look closely enough you’ll see it.</p>

  <figure>
    <img src="https://sizeof.life/img/hidden_message02.png">
    
  </figure>


<blockquote>
<p>The hidden message contained another riddle:
Congrats! You passed level 1
#2 Send an email
TO: A000668(9)@8,41235641483227
SUBJECT: hello world</p>
</blockquote>
<p>The domain part of the email was easier to figure out, after a quick google it was apparent that it’s ‘protonmail.com’.</p>
<p>The first part of the email turned out to be a the 9th <a href="https://en.wikipedia.org/wiki/Mersenne_prime">Mersenne prime</a> number. Making the email address <a href="https://sizeof.life/cdn-cgi/l/email-protection#6e5c5d5e5b565a5d5e5e575c5f5d58575d575b5f2e1e1c011a0100030f0702400d0103"><span data-cfemail="1d2f2e2d2825292e2d2d242f2c2e2b242e24282c5d6d6f72697273707c7471337e7270">[email&nbsp;protected]</span></a>.</p>
<p>Few hours after sending the email I received a response with a binary code in the message.</p>

  <figure>
    <img src="https://sizeof.life/img/binary.png">
    
  </figure>


<p>Binary conversion revealed an URL that contained a text file called ‘haystack.txt’.</p>

   <figure>
     <img src="https://sizeof.life/img/haystack.png">
     
   </figure>
 

<p>There was also an instruction in another file that stated:</p>

  <figure>
    <img src="https://sizeof.life/img/hint.png">
    
  </figure>


<p>At this point it was apparent that there is a ‘needle in a haystack’ file.</p>
<p>The numbers in the text file appeared to be prime numbers and my next thought was to check if there are any non-primes in the file.</p>
<p>There was 1000000 numbers in the file. Manually checking each one was out of the question so I turned for help to Python. I wrote a quick script to check for primes, output non-prime numbers (if any) and left it running.</p>
<p>After a while one non-prime number was returned.
<img src="https://sizeof.life/img/script.png" alt="script"></p>
<p>Next part of the puzzle was to navigate to the provided URL and use the needle somehow. The URL contained a a short message:</p>
<blockquote>
<p>‘If you can not get me what I need,
maybe you must start at Level 0‘</p>
</blockquote>
<p><img src="https://sizeof.life/img/needle.png" alt="nedle"></p>
<p>There was no inputs or buttons on the page but there was some clues left. It took me a short while to figure it out. The clues were hidden in the following parts of the puzzle:</p>
<p>Clue 1:
<img src="https://sizeof.life/img/clue1.png" alt="clue1"></p>
<p>Clue 2:
<img src="https://sizeof.life/img/clue2.png" alt="clue2"></p>
<p>Clue 3:
<img src="https://sizeof.life/img/clue3.png" alt="clue3"></p>
<p>The ‘get‘ word was chosen purposely and it referred to a http GET request.</p>
<p>After making the GET request while passing the non-prime ‘needle’ as the parameter another message appeared.
<img src="https://sizeof.life/img/get.png" alt="get-request"></p>
<p>The message was in hexadecimal and after converting it, another puzzle was revealed:</p>
<blockquote>
<p>uonlyneed3dovetails2findwhere2go
mfcJE9adl3v7z/Tuxeo5XqM.eTzKB/Y#nsglRevPnr7atQ</p>
</blockquote>
<p><strong>‘You only need 3 dovetails 2 find where 2 go’</strong></p>

  <figure>
    <img src="https://sizeof.life/img/640px-Riffle_shuffle.jpg">
    
  </figure>


<p>This took me a while to figure out. At first I though it may have something to do with the ‘ / ‘ but after a quick research I realized that <a href="https://en.wikipedia.org/wiki/Shuffling#Riffle">dovetail</a> is also a thing in card games and it’s a riffle-shuffle technique.</p>
<p>This would mean that the provided string needs to be riffle-shuffled 3 times to reveal <em>where to go</em>.</p>
<p>I seem to have lost the python script I wrote back then to do this for me but after it has done shuffling, the string has formed another URL.</p>
<p>After entering the URL, it seemed like it was the last puzzle (at least until more parts are developed):</p>
<p><img src="https://sizeof.life/img/end.png" alt="get-request"></p>
<p>That was FUN!</p>
<p>To whoever designed this puzzle: well done! It was very interesting.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stract: Open-souce, non-profit search engine (338 pts)]]></title>
            <link>https://stract.com/</link>
            <guid>39254172</guid>
            <pubDate>Sun, 04 Feb 2024 20:36:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stract.com/">https://stract.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39254172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Customise your search with an  <a href="https://stract.com/settings/optics" data-svelte-h="svelte-1prlvom">optic</a>:</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dijkstra's interview on Dutch TV (2000) (127 pts)]]></title>
            <link>https://pncnmnp.github.io/blogs/translating-dijakstra.html</link>
            <guid>39253944</guid>
            <pubDate>Sun, 04 Feb 2024 20:16:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pncnmnp.github.io/blogs/translating-dijakstra.html">https://pncnmnp.github.io/blogs/translating-dijakstra.html</a>, See on <a href="https://news.ycombinator.com/item?id=39253944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a id="rss-feed" href="https://pncnmnp.github.io/rss.xml">If you like this blog post, do subscribe to my RSS feed</a></p>
<h2>First Published: 1<sup>st</sup> May 2021</h2>
<p><i>Writers make national literature, while translators make universal literature. - José Saramago</i></p><p>Among computer science graduates, <a href="https://en.wikipedia.org/wiki/Edsger_W._Dijkstra">Edsgar W. Dijkstra</a> is a man who needs little introduction. A few decades back, an interesting interview of his <a href="https://www.cs.utexas.edu/users/EWD/video-audio/NoorderlichtVideo.html">was broadcasted on Dutch TV</a>. The video description is as follows:</p>
<blockquote>In the autumn of 2000, the Dutch broadcasting organization VPRO Television visited Austin to make a video of a visit with the most famous Dutch computing scientist. The product of this project was broadcast in April 2001 as a 25-minute episode of the science series Noorderlicht, under the title "Denken Als Discipline".</blockquote>
<p>Roughly a year after this video was broadcasted, <a href="https://www.cnet.com/news/computer-science-pioneer-dijkstra-dies/">Prof. Dijkstra died</a> after a long struggle with cancer. This blog post would not have been possible without the help of some kind souls (Jos Wassink, Karin Spiegel, and Chris Kotrla) who dedicated their time to digitize and translate the above interview in English. I am merely curating the thoughts Prof. Dijkstra expressed during the interview, for improved readability.</p> 

<h2>The Interview</h2>
<p><b>Prof. Dijkstra:</b> You just cobble something together to sell. It need not be any good. As long as you can fool people into buying it, you can always try to make better versions later. So then you get these version numbers, even with decimals, version 2.6 or 2.7. That's nonsense. While version 1 should have been the finished product.</p>
<blockquote>Computer science is no more about computers than astronomy is about telescopes.</blockquote>
<p><b>Narrator:</b> Professor Edsger W. Dijkstra is Holland's first programmer. In 1972, he received the Turing Award, the Nobel Prize of Computer Science. He and his wife now live in Austin, Texas, where they moved in 1984.</p>
<p><b>Prof. Dijkstra:</b> At the time, most university departments in the Netherlands aimed to water down their curriculum. At the same time, the University of Texas in Austin tried to reduce the student enrollment and to increase their quality. It was an opposite development which was much more attractive than what was happening in Dutch higher education. </p>
<blockquote>The universities will continue to lack the courage to teach hard science, they will continue to misguide the students, and each next stage of infantilization of the curriculum will be hailed as educational progress.</blockquote>
<p><b>Narrator:</b> Quality, correctness, and elegance are what Dijkstra thinks should characterize a computer program. In 1954 he resolved to make programming a science, but it has been an uphill struggle.</p>
<p><b>Prof. Dijkstra:</b> I lose no sleep that businesses feel they cannot afford to deliver first-rate products. It doesn't keep me from continuing my work.</p>
<blockquote>You should not give the world what it asks for, but what it needs.</blockquote>
<p><b>Prof. Dijkstra:</b> There are very different programming styles. I tend to see them as Mozart versus Beethoven. When Mozart started to write, the composition was finished. He wrote the manuscript in one go. In beautiful handwriting, too. Beethoven was a doubter and a struggler, who started writing before he finished the composition and then glued corrections onto the page. In one place he did this nine times. When they peeled them, the last version proved identical to the first one. That iterative method of programming is somehow a very Anglo-Saxon custom. British education is pervaded by it. People learn, when they write, not to try to get it right the first time. Just write what's on your mind and then rewrite repeatedly, to get the product you want. That's partly why word processors are marketed so aggressively and partly why they have been so successful there. While it is one of the advantages of working with pen and paper that when you start a sentence, you should have it ready.</p>
<p><b>Narrator:</b> Dijkstra is a prolific writer: musings, talks, mathematical proofs, and books. His EWD's, these initials followed by a serial number are considered his best scientific contributions.</p>
<p><b>Prof. Dijkstra:</b> For myself, the most important thing has been the daily discipline of neatly writing down your thoughts and what you do. Due to modern technology, they have been much more influential than they would have been in the past. You might describe them as a modern form of scientific correspondence. Albeit that the intellectual traffic has been mostly one-directional. The serial numbers have crept up to over 1300. In length, they vary enormously, from 80 pages to one page. As time goes by, they tend to become ever shorter. Everyone to whom I sent them, was implicitly willing to function as an internal node of the dissemination tree and to send second-generation copies on to others. How many people they have reached, I have never been able to estimate. A few hundred, I'd say.</p>
<p><b>Prof. Dijkstra:</b> In order to compose, you have to write scores. But to be a composer is not to write scores. To be a composer is to conceive music. In the early days of programming, you had to write machine code. Meaningless sequences of capitals and numbers. That's the analog of writing scores. People thought that <i>that</i> was programming. Later, that was made easier by the invention of the higher programming languages: Fortran, Pascal, C++, and suchlike. People thought that those languages would solve the programming problem. But when you look closely, the trivial aspects of programming had been automated while the harder ones remained. The higher programming languages which had been intended to facilitate programming proved, coupled with the increasing ambitions of the applications to be more intellectually demanding to the programmer.</p>
<blockquote>The competent programmer is fully aware of the limited size of his own skull. He, therefore, approaches his tasks in full humility and avoids clever tricks like the plague. - EWD 340</blockquote>
<p><b>Prof. Dijkstra:</b> I remember, in 1970 or thereabouts, I first went to explain to companies how to develop programs and keep them under tight control. I first went to Paris and then to Brussels. In Paris, I delivered a lecture at the Sorbonne and people were very enthusiastic. On the way home, I told the same story at a large software house in Brussels. The lecture was a complete failure. In a sense, one of my least successful lectures. I later found out why. The management didn't want faultless programs because the company derived its stability from maintenance contracts. And the programmers weren't interested because they derived their intellectual excitement from the fact that they didn't quite know what they were doing. They felt that if you knew precisely what you were doing and didn't run risks, it was a boring job.</p>
<blockquote>We should not add bugs to a program out of nonchalance. We should do so systematically and with great care.</blockquote>
<p><b>Prof. Dijkstra:</b> If in physics there's something you don't understand you can always hide behind the uncharted depths of nature. You can always blame God. You didn't make it so complex yourself. But if your program doesn't work, there is no one to hide behind. You cannot hide behind an obstinate nature. A zero is a zero, a one is a one. If it doesn't work, you've messed up.</p>
<blockquote>I realized my previous projects had been finger exercises. I had to tackle complexity itself. But it took a long time to muster the courage to do so.</blockquote>
<p><b>Narrator:</b> At the end of the 60s, Dijkstra saw that the complexity got the better of the programmers. And that it threatened the most prestigious projects.</p>
<p><b>Prof. Dijkstra:</b> That happened in 1969 just after the first successful moon landing. I was at a NATO conference on software engineering in Rome where I met Joel Aron, who was head of IBM's Federal Systems Division, which had been responsible for the software of the moon shot. I knew that each Apollo flight required some 40,000 new lines of code. I don't know what unit a line of code is but 40,000 is a lot. I was duly impressed that they got so many lines of code correct. So when I met Joel, I said: "how do you do it?". "Do what?", he asked. "Getting that software right". "Right?", he said. He said that in one of the calculations of the orbit of the lunar module, the moon had been defined as repelling instead of attracting. They had discovered that error by accident. Imagine, <i>by accident</i> five days before the shot. I went white and said: "Those guys have been lucky!" Yes, Joel Aron agreed.</p>
<blockquote>Program testing can convincingly show the presence of bugs, but it hopelessly inadequate to show their absence.</blockquote>
<p><b>Narrator:</b> Dijkstra knew personally how frighteningly complex programs could become. He had finished the operating system of Holland's biggest computer, the X-8 under great pressure.</p>
<p><b>Prof. Dijkstra:</b> I feared that I wouldn't get it right. That I would lose control of it. For the X-8 in Eindhoven, we tried to design a multiprogramming system. We were working with a machine with a real-time interrupt which really means that you cannot test your program. I knew that if we designed the program with insufficient care, we would end up with something which, once entered into the computer wouldn't work, and would make irreproducible errors, the cause of which we would be unable to determine. That was the reason why we applied all possible control mechanisms in the development stage.</p>
<blockquote>Elegance is not a dispensable luxury but a factor that decides between success and failure.</blockquote>
<p><b>Prof. Dijkstra:</b> One of the things I discovered as early as the 1960s is that mathematical elegance is not a question of esthetics, of taste, or fashion, but something you can translate into a technical notion. The Concise Oxford Dictionary gives one of the meanings of elegant as <i>ingeniously simple and effective</i>. In practice, a program is manageable if you make a truly elegant program, firstly because it is shorter than most alternatives and consists of discrete paths each of which you can replace by an alternative implementation without influencing the rest of the program. But also, curiously the most elegant programs are often the most efficient.</p>
<blockquote>When there were no computers, programming was no problem. When we had a few weak computers, it became a mild problem. Now that we have gigantic computers, programming is a gigantic problem.</blockquote>
<p><b>Prof. Dijkstra:</b> My first programming years were a bit strange compared to now in that I programmed for non-existent machines. My friends Bram Loopstra and Carel Scholten built the machine and in the meantime, I wrote the relevant software. I was used to not testing a program because the machine to test it on wasn't finished. I knew right away that you had to create something that could keep under your intellectual control.</p>
<p><b>Prof. Dijkstra:</b> I started work at the Mathematical Centre in March 1952. My wife-to-be had worked there since the summer of 1949. She had a job as a calculator. I was taken on as a programmer. And I rather liked her. Our first date was the concert on the occasion of the Mathematical Congress in 1954 in Amsterdam. Then the news was out that I was sweet on her. People complimented me on my taste. Professionally, I was strongly influenced by my mother. She was a brilliant mathematician. I remember when I had bought my books for the next school year I saw the goniometry book, which scared me with all those Greek letters. I asked my mother if goniometry was hard. She said, "not at all". "Make sure you know all the formulae by heart and if you need more than five lines, you're on the wrong track."</p>
<p><b>Prof. Dijkstra:</b> Why has elegance found so little following? That is the reality of it. Elegance has the disadvantage if that's what it is that hard work is needed to achieve it and a good education to appreciate it.</p>

<p><a id="back-link" href="https://pncnmnp.github.io/blog.html">←</a>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sapling: Experimental vi-inspired editor where you edit code, not text (166 pts)]]></title>
            <link>https://github.com/kneasle/sapling</link>
            <guid>39253798</guid>
            <pubDate>Sun, 04 Feb 2024 20:01:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kneasle/sapling">https://github.com/kneasle/sapling</a>, See on <a href="https://news.ycombinator.com/item?id=39253798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:kneasle/sapling" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Fsrp2c6q423zz6YqE3COJ13hI1gxgZJF7u1vW0MznVOU3CfAiahsOiJGTwC0rg9BW4yt5wuwFh4xtYWYtITvjA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="kneasle/sapling" data-current-org="" data-current-owner="kneasle" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=eIDiJxiRn9X4Ldpmp%2FYINDNuwHLz1I8s2NIaxvfFgi%2F%2FniPwb2VIgOMzeCXS8YpMtM%2BxzMsBWlAHPXcxZhZcOq0eC3q%2Bymk%2FPbNphvk84Nb1EFTy87QL2MXuqfzMkLJPVYWM9CutROcLSZwBR0TS5Ln%2FWqy%2FlLgOmlijy0J1KTv5IiRKsEyv0gTx15ANtPf1jc%2FxuHsAspCdzeiVadX2VN%2Fg6Y0JlYjmz6gvW47YkYpMUGdB5nbDlLxd82dOUw9g0ZxlA7%2B3Pb7wW%2FbM6NjbRt4RXlbIdmNJwsOgAi0g9YGOLnvK6AOBWn1Qh2bXvifCt05JuYAKAVAouse1KJ2%2BWO5BvYmlvHSUqgIslLmQ4itFpomkHftLyHUBlJlU0Q3Ar9wjt0nlI9dtSFlvkZpNaRgn9nhiQkj5%2FISgU0uuNrSZL3BEdYfTGQyMcnX7wj2wRVaE1XE6xJpzh5xH%2Fn9%2FLjr9glF0JeFy%2FRCO30YLvQCJ6pBOXQm3raXBKBl28UYg2PyxtFfPvfFgZA%3D%3D--6hPzDRoQuNTrC3ih--Q53Zv4sllbX0E3fLdnEPQw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=kneasle%2Fsapling" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/kneasle/sapling&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="5b16bce859c90f213c6c1a71abc030b2573635ddcd37619222be5d27b402d63f" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How the codpiece flopped (123 pts)]]></title>
            <link>https://www.bbc.com/future/article/20240202-what-happened-to-the-codpiece</link>
            <guid>39253603</guid>
            <pubDate>Sun, 04 Feb 2024 19:40:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/future/article/20240202-what-happened-to-the-codpiece">https://www.bbc.com/future/article/20240202-what-happened-to-the-codpiece</a>, See on <a href="https://news.ycombinator.com/item?id=39253603">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="futurearticle20240202-what-happened-to-the-codpiece"><div id="headline-futurearticle20240202-what-happened-to-the-codpiece"><div><p>(Image credit: </p><!-- --><p>Getty Images</p><!-- --><p>)</p></div><div><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h8ywt4.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h8ywt4.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h8ywt4.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h8ywt4.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h8ywt4.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h8ywt4.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h8ywt4.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h8ywt4.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="A replica of Henry VIII's suit of armour being loaded into a car (Credit: Getty Images)" src="https://ychef.files.bbci.co.uk/976x549/p0h8ywt4.jpg" alt="A replica of Henry VIII's suit of armour being loaded into a car (Credit: Getty Images)" id=""></picture></div></div><div><article><div><p>Some codpieces were empty – while others were used to store potpourri.</p><div><p>S</p><div><p>Some time around 1536, Hans Holbein the Younger was finessing Henry VIII's crotch. With a fine brush in his hand and a palette of watercolour paints beside him, the master artist took pains to give his client's ornately decorated bulge its due prominence.</p>
<p>In the resulting sketch – a full-size preparatory drawing for <a href="https://www.npg.org.uk/collections/search/portrait/mw03080/King-Henry-VIII-King-Henry-VII">a mural</a> that once covered an entire wall at Whitehall Palace in London – the king is, it's often said, majestic and <a href="https://link.springer.com/chapter/10.1057/978-1-137-51144-7_8">virile</a>. Henry VIII's feet are planted firmly apart, with both hands resting suggestively below his waist, clutching objects that seem to <a href="https://books.google.com/books/about/Thrust.html?id=Fd_kDwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;hl=en&amp;newbks=1&amp;newbks_redir=1">direct the viewer</a> towards his ludicrously proportioned genitals. According to a contemporary account, the final painting left viewers feeling "<a href="https://brill.com/downloadpdf/book/edcoll/9781848881372/BP000006.pdf">abashed and annihilated</a>".</p>
<p>For a brief moment in the Renaissance, in between the invention of the microscope, printing press, and pencils – along with other technologies that uphold modern society – upper class men were rather preoccupied with erecting another innovation: the codpiece.</p>
<p>These "<a href="https://www.vice.com/en/article/wdzd49/metallic-package-379-v17n3">pretty personal palaces for penises</a>", as one writer has called them, consisted of pockets of fabric worn over the crotch and padded out to form a bizarre array of evocative shapes: spirals, orbs, and upwards-curling sausages. Some even had faces on them. This was a rare opportunity for men to give their nether regions a dash of flair, and many opted for confections of sumptuous fabrics, such as silk, velvet, and satin, embellished with jewels, gold, and – the ultimate show of macho fecundity – cute little bows.</p>
<p>But what were codpieces for? And why did they disappear?</p></div></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qt6v"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qt6v.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qt6v.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qt6v.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qt6v.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qt6v.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qt6v.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="In Henry VIII's most famous portrait, he's dripping with furs, gold, and rubies – but it's his codpiece that really commands attention (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qt6v.jpg" alt="In Henry VIII's most famous portrait, he's dripping with furs, gold, and rubies – but it's his codpiece that really commands attention (Credit: Alamy)" id=""></picture><div><p>In Henry VIII's most famous portrait, he's dripping with furs, gold, and rubies – but it's his codpiece that really commands attention (Credit: Alamy)</p></div></div><div><p>Initially, codpieces were made of steel and added to armour, to help protect knights' fertility on the battlefield. But soon they presented a neat solution for an awkward everyday problem.</p>
<p>Until the late 15thCentury, it was common for men to wear a long tunic or doublet – essentially, a dress – with <a href="https://www.cam.ac.uk/research/features/what-goes-up-must-come-down-a-brief-history-of-the-codpiece">hose (tights) on their legs</a>. Then the fashion changed. Doublets gradually inched their way upwards over the years, becoming so short that they no longer covered the crotch. This was particularly dangerous, because the hose men wore at the time came individually, like socks, leaving open spaces that were somewhat… revealing.&nbsp;</p>
<p>"So basically, you would put one leg in one hose, and then tie it to your doublet. And then you do the same with the other leg," says Victoria Bartels, a professor of early modern Italy at Syracuse University in New York. With the trendy new cropped versions, "it kind of left an area that needed to be covered," she says. &nbsp;</p>
<p>The problem was highlighted in a particularly graphic description in the medieval poem <a href="https://chaucer.fas.harvard.edu/pages/parsons-prologue-and-tale">The Canterbury Tales</a>. "Alas, some of them show the bulge of their shape, and the horrible swollen members, that it seems like the malady of hernia," complains a character known as "the person" in the story's prologue.</p>
<p>The gaps led to a moral panic, with priests worrying that the new style would prove irresistible to "sodomites", and lead to <a href="https://www.cam.ac.uk/research/features/what-goes-up-must-come-down-a-brief-history-of-the-codpiece">the corruption of young men</a>.</p></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qvmg"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qvmg.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qvmg.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qvmg.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qvmg.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qvmg.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qvmg.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Choosing a codpiece in a contrasting colour helped men to make their genitals the centre of attention (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qvmg.jpg" alt="Choosing a codpiece in a contrasting colour helped men to make their genitals the centre of attention (Credit: Alamy)" id=""></picture><div><p>Choosing a codpiece in a contrasting colour helped men to make their genitals the centre of attention (Credit: Alamy)</p></div></div><div><p>The first codpieces were limp triangles of fabric that were tucked in to cover the openings between each hose. But it didn't take long for men to take full advantage of these new garments, and start padding them.</p>
<p>Within a couple of decades, these loose flaps had morphed into phallic objects of monstrous proportions. Early modern wannabe lotharios would pack their codpieces with horsehair, fabric and straw, sometimes stashing useful items away inside such as <a href="https://books.google.com/books/about/Thrust.html?id=Fd_kDwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;hl=en&amp;newbks=1&amp;newbks_redir=1">handkerchiefs and money</a> – Bartels has even has encountered accounts of their use for storing potpourri.</p>
<p>From the streets of Florence, where they were known as <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1445-5994.2004.00635.x">sacco</a>, to Paris, where they were called <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1445-5994.2004.00635.x">braguettes</a>, young men swaggered around with their prosthetic genitalia, drawing the eye downwards wherever they went. What had begun as a modesty device was now being used to the opposite effect.&nbsp;&nbsp;&nbsp;</p>
<p>The symbolism of these protruding packages wasn't lost on the Renaissance men who wore them. The very word "codpiece" comes from the Old English <a href="https://www.oed.com/dictionary/codpiece_n?tl=true&amp;tab=etymology">"cod", meaning "scrotum"</a>, while one satirical text Bartels found compared the protection they offered – especially on armour – to that provided by the shells of nuts and seeds. Like these <a href="https://www.cam.ac.uk/research/features/what-goes-up-must-come-down-a-brief-history-of-the-codpiece">"braguettes naturelles"</a> – nature's codpieces – they helped to ensure the propagation of the next generation.</p></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qtyc"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qtyc.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qtyc.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qtyc.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qtyc.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qtyc.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qtyc.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Some Renaissance men liked to adorn their crotches with cute little bows (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qtyc.jpg" alt="Some Renaissance men liked to adorn their crotches with cute little bows (Credit: Alamy)" id=""></picture><div><p>Some Renaissance men liked to adorn their crotches with cute little bows (Credit: Alamy)</p></div></div><div><p>Henry VIII, whose <a href="https://www.sciencedaily.com/releases/2011/03/110303153114.htm">reproductive woes</a> led him to invent a new branch of Christianity, notoriously used the figurative associations of the codpiece to <a href="https://muse.jhu.edu/pub/4/article/41213/summary">maximum effect</a>. The mural at Whitehall Palace was destroyed in a fire, but the original drawing survived and others were encouraged to copy it. Consequently, the king's ornamental crotch lives on in tens of paintings to this day, reassuring those who view it that he is more than <a href="https://lucyworsley.com/a-little-article-on-the-history-of-the-codpiece/">capable of producing an heir</a>. &nbsp;</p>
<p>Even 200 years after his death, admirers could visit Henry VIII's statue and marvel at his fecundity, at the Tower of London. A painted wooden effigy there came with flowing fabric robes and a secret mechanism that revealed a <a href="https://www.google.co.uk/books/edition/Thrust/Fd_kDwAAQBAJ?hl=en&amp;gbpv=1&amp;printsec=frontcover">swinging codpiece</a>. "If you press a spot on the floor with your feet, you will see something surprising with regard to this figure, but I will not say more," wrote one visitor, according to the book Thrust: A Spasmodic Pictorial History of the Codpiece in Art. Women would <a href="https://www.google.co.uk/books/edition/Thrust/Fd_kDwAAQBAJ?hl=en&amp;gbpv=1&amp;printsec=frontcover">stick pins</a> into it in the hope that it would help them to have children.</p>
<p>At the time, such brazen displays of virility weren't unusual. On the <a href="https://www.atlasobscura.com/places/cappella-colleoni">wrought iron front gate</a> to the Cappella Colleoni chapel in Bergamot, Italy, built in the late 15th Century, is the Colleoni coat of arms – and on it, are three comma-like shapes. These are testicles, thought to have been included as a display of macho strength, and because of the similarity of the family name to the <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8365.2008.00635.x">word for them, coglioni</a>.</p></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qszc"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qszc.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qszc.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qszc.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qszc.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qszc.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qszc.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Even Henry VIII's suits of armour were given suggestive bulges (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qszc.jpg" alt="Even Henry VIII's suits of armour were given suggestive bulges (Credit: Alamy)" id=""></picture><div><p>Even Henry VIII's suits of armour were given suggestive bulges (Credit: Alamy)</p></div></div><div><p>Men also used their new crotch adornments to project military prowess. Firstly, there was the fact that codpieces were sometimes added to armour. But Bartels explains that the rise of the codpiece also coincided with the Italian Wars, in which <a href="https://www.cambridge.org/core/books/abs/cambridge-history-of-war/warfare-and-italian-states-13001500/DBEBA97FAE6018EA0243F6AC7DCA60AE">mercenaries</a> from Northern Europe went to battle on behalf of Spain, France and Italy. One work-perk the soldiers received was an exemption from Sumptuary Laws – regulations that determined how lavish different social groups were allowed to be – and they seized this sartorial opportunity. "They are super flashy dressers… they don these huge codpieces," says Bartels. This forged yet another tie between the codpiece and military culture.</p>
<p>Even to their contemporaries, however, these overt and determined displays of masculinity were often the focus of great ridicule. As the historian <a href="https://books.google.com/books/about/Materializing_Gender_in_Early_Modern_Eng.html?id=ItOtNF1u24sC&amp;printsec=frontcover&amp;source=kp_read_button&amp;hl=en&amp;newbks=1&amp;newbks_redir=1">Will Fisher writes</a> in the book Materializing Gender in Early Modern English Literature and Culture, satirists would thrill their audiences with suggestive scenes in which it seems that a character is about to reveal their genitals… before pulling out something unexpected, like an orange. Other unusual items found in fictional codpieces included "ballads, bottles, napkins, pistols, hair, and even a looking glass".</p></div><div id="future/article/20240202-what-happened-to-the-codpiece-p0h8qv8c"><picture><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qv8c.webp" type="image/webp"><source media="(min-width:624px)" srcset="https://ychef.files.bbci.co.uk/1024x1280/p0h8qv8c.jpg" type="image/jpeg"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qv8c.webp" type="image/webp"><source media="(min-width:485px)" srcset="https://ychef.files.bbci.co.uk/885x1280/p0h8qv8c.jpg" type="image/jpeg"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qv8c.webp" type="image/webp"><source media="(min-width:320px)" srcset="https://ychef.files.bbci.co.uk/720x900/p0h8qv8c.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Some codpieces were more evocative than others (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/720x900/p0h8qv8c.jpg" alt="Some codpieces were more evocative than others (Credit: Alamy)" id=""></picture><div><p>Some codpieces were more evocative than others (Credit: Alamy)</p></div></div><div><p>By as early as the late 16th Century, the codpiece was already in decline – and sources start to refer to them as out of fashion, says Bartels. Oddly, just before these engorgements vanished they began to shrink down to <a href="https://www.theguardian.com/books/2015/apr/30/wolf-hall-codpieces-too-small-says-literature-researcher">minute proportions</a>. "You start to see this other fashion trend called the peascod," says Bartels. These puffed-out, distended doublets were worn over a shirt, protruding out like prosthetic potbellies. "It looks ridiculous to modern eyes," she says. These were often accompanied by pillowy or even skirt-like breeches, and the combination was in competition for the same bodily real estate as the codpiece, since they both incorporated the genitals, she notes.&nbsp;&nbsp;</p>
<p>Today, very few codpieces survive. Those that remain include the metallic bulges in armouries, a set of <a href="https://www.cam.ac.uk/research/features/what-goes-up-must-come-down-a-brief-history-of-the-codpiece">wool and velvet ones</a> that belonged to a Swedish count and his sons, and a drawerful at the Museum of London – initially <a href="https://lucyworsley.com/i-went-to-the-museum-of-london/">classified as shoulder pads</a> by a starchy Victorian curator, according to the historian Lucy Worsley. Other than that, we can only glimpse the priapic grandeur of this lost garment though paintings and sculptures from the era.</p>
<p>However, though authentic renaissance codpieces are now rare, public enthusiasm for them hasn't disappeared completely. In the 1970s and 80s, rock bands such as Jethro Tull and Kiss began aweing their audiences with leopard-print, leather, metallic studded, and demon-faced versions – the latter even had their own <a href="https://www.cbc.ca/radio/day6/kiss-band-costume-designer-rebecca-sevrin-1.7039209">codpiece-seamstress</a> until they disbanded last year. &nbsp;&nbsp;</p>
<p>Codpieces have also been making a comeback in <a href="https://www.theguardian.com/fashion/2019/dec/27/codpiece-envy-fashion-reinvents-16th-century-accessory">high fashion</a> – part of a trend for so-called "<a href="https://www.theguardian.com/fashion/2019/dec/27/codpiece-envy-fashion-reinvents-16th-century-accessory">Tudor power dressing</a>" – and on historical television series including Wolf Hall. There's just one problem: modern producers can't bring themselves to <a href="https://www.theguardian.com/books/2015/apr/30/wolf-hall-codpieces-too-small-says-literature-researcher">make them big enough</a>.</p>
<p>--</p>
<p><em>If you liked this story,&nbsp;</em><a href="https://cloud.email.bbc.com/SignUp10_08?&amp;at_bbc_team=studios&amp;at_medium=Onsite&amp;at_objective=acquisition&amp;at_ptr_name=bbc.com&amp;at_link_origin=featuresarticle&amp;at_campaign=essentiallist&amp;at_campaign_type=owned"><em><strong>sign up for The Essential List newsletter</strong></em></a><em>&nbsp;– a handpicked selection of features, videos and can't-miss news delivered to your inbox every Friday.</em></p>
<p><em>Join one million Future fans by liking us on&nbsp;</em><a href="https://www.facebook.com/BBCFuture/"><em><strong>Facebook</strong></em></a><em>, or follow us on&nbsp;</em><a href="https://twitter.com/BBC_Future"><em><strong>Twitter</strong></em></a><em>&nbsp;or&nbsp;</em><a href="https://www.instagram.com/bbcfuture_official/"><em><strong>Instagram</strong></em></a><em>.</em></p></div></div></article></div>;</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Gödel, Escher, Bach is the most influential book in my life (343 pts)]]></title>
            <link>https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428</link>
            <guid>39253099</guid>
            <pubDate>Sun, 04 Feb 2024 18:48:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428">https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428</a>, See on <a href="https://news.ycombinator.com/item?id=39253099">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://philosophygeek.medium.com/?source=post_page-----49d785a4e428--------------------------------"><div aria-hidden="false"><p><img alt="Mark Johnson" src="https://miro.medium.com/v2/resize:fill:88:88/1*ivAx6b9Z9gnoFsJceGMkFQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="2061"><a href="https://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567" rel="noopener ugc nofollow" target="_blank"><em>Gödel, Escher, Bach: An Eternal Golden Braid</em></a><em> </em>(henceforth: GEB), the Pulitzer Prize winning book written in 1978 by Douglas Hofstadter, is described in its cryptic tagline as “a metaphorical fugue on minds and machines in the spirit of Lewis Carroll.”</p><p id="9ab3">I recently reread GEB and got fired up by how brilliantly Hofstadter fuses computation, epistemology, and consciousness. After failed attempts at explaining the book to three of my smartest friends, I decided to write something up.</p><p id="7f72">The problem is that a simple reduction like “GEB is about how complex systems arise from simpler systems” is akin to describing <em>Ulysses</em> as “a day in the life of Leopold Bloom.” More detailed descriptions run the risk of diving into the depth that’s only understandable after having read the book.</p><p id="ca55">This post is a more modest attempt to explain to myself why GEB is important, and focuses on three mental models that have profoundly affected my life: <strong>epistemic limits</strong>,<strong> self-reference</strong>, and <strong>isomorphism</strong>.</p><p id="b371">If it causes you to read or reread it, then all the better.</p><p id="00df">Here we go!</p><figure><figcaption>Kurt and Albert, hanging out at Princeton.</figcaption></figure><p id="d440">The main character of the book is <a href="https://en.wikipedia.org/wiki/Kurt_G%C3%B6del" rel="noopener ugc nofollow" target="_blank">Kurt Gödel</a>, the most important person in the 20th century you’ve never heard of. Gödel is the kind of guy that shows up to <a href="https://en.wikipedia.org/wiki/Albert_Einstein" rel="noopener ugc nofollow" target="_blank">his buddy’s</a> 70th birthday with <a href="https://en.wikipedia.org/wiki/G%C3%B6del_metric" rel="noopener ugc nofollow" target="_blank">an exact solution to the Einstein field equations</a> as a present. Despite being the greatest mathematician of his generation, he wasn’t stuffy in the least: his favorite movie was Snow White and the Seven Dwarves.</p><p id="b348">Gödel is most famous for his Incompleteness Theorems, which established limits on mathematics. For the first chunk of the 20th century, mathematicians were obsessed with formalizing mathematics and then proving meta-theorems <em>about</em> those formal systems. In particular, there was a strongly-held belief that for any well-formed formula (a “grammatically correct” statement in math, e.g., A=B is well-formed whereas AA==+B is not), you could use mathematics to <em>decide</em> whether it was true or false.</p><p id="909f">If you think about it for a second, this makes perfect sense: it <em>seems</em> like you should be able to determine whether any statement is true or false.</p><p id="3010">Nope! Gödel proved in 1931 that mathematics is not decidable, an earth-shattering result. He proved that there are statements in mathematics, which are <em>true but not provable</em> within the system. Worse yet, it turns out that you can’t build a more powerful mathematical system. Once a system becomes sufficiently complex, there will always be statements which are undecidable. You’re left with a choice: either have weak system of mathematics or accept that there will always be theorems out of reach. A rough analogy to incompleteness Heisenberg’s Uncertainty Principle, which shows that physics makes it impossible to determine <em>both</em> the position and velocity of a particle with exact precision.</p><p id="fa72">Wouldn’t it be nice if every question had an answer? That’s a lovely fantasy, but Gödel shows that there are <strong>fundamental epistemic limits to the universe</strong>, things that no genius will help us to know, no alien race could teach us, no machine could be built to solve, and no new kinds of mathematics will uncover. How frustrating.</p><p id="167d">A key feature of powerful mathematical systems (or perhaps, any system that generates complexity…) is that they involve <strong>self-reference</strong>, that is, they contain ways of talking about themselves. “This sentence is true” is an example. Because self-referential systems can manipulate and talk about themselves, they systems are very powerful and immediately run into fun paradoxes. Is the statement “This sentence is false.” true or false? Either way, it doesn’t end well.</p><p id="c684">A third major theme of the book is <strong>isomorphism</strong>, which is unique to Hofstadter’s vernacular. In formal mathematics, “isomorphism” takes on a version of “equivalence.” For example, it turns out that many different formalizations of mathematics are provably isomorphic, like Turing Machines, arithmetic, set theory, and formal logic. Hofstadter deliberately uses the term more loosely to describe two systems that are structurally similar. I find this quite useful because it forces one to define the structures of the system, why they are similar, and why other parts of the system are less important. We might describe the way that planets fly around stars as <em>isomorphic</em> to the way that electrons fly around nuclei.</p><figure><figcaption>Escher’s famous Drawing Hands.</figcaption></figure><p id="8a61">The two minor characters, <a href="https://mcescher.com/" rel="noopener ugc nofollow" target="_blank">M.C.Escher</a> and <a href="https://en.wikipedia.org/wiki/Johann_Sebastian_Bach" rel="noopener ugc nofollow" target="_blank">Johann Sebastian Bach</a>, are reflections of Gödel in art and both liberally use self reference. Escher draws pictures of hands drawing hands (!) and water “falling” in an infinite loop. His images don’t just play tricks on the eye, they force paradoxical conclusions, regardless of your angle of interpretation. On the musical side, Papa Bach was most famous for his complex fugues, which are basically the same melody played on top of each other. Common versions of this you might have sung as a child are “Row, row, row your boat” and “Frère Jacques.” Both Escher and Bach are woven into the story (like a fugue?), providing tangible examples to the more abstruse mathematical concepts.</p><figure><figcaption>A playlist full of Bach, fugues, and other tracks referenced in GEB.</figcaption></figure><p id="b898">Perhaps the most astonishing part of the book is the quality of the writing itself. Each chapter begins with a clever dialog between Achilles and the Tortoise (inspired by Lewis Carroll), and a few of their anthropomorphic friends. They deal with a whole range of bizarre situations, like record player so powerful that it can play any record (including a record that can destroy the record player) and asking a Djinn for a meta-wish (“I wish for 5 more wishes”). Hofstadter’s greatest achievement is his palindromic Crab Canon in Chapter VII, which is a dialog that can be read backwards and forwards. Of course, these aren’t just cute dialogs: each is isomorphic to the themes in the following chapter. Oftentimes a dialog is a more understandable exposition of the chapter’s theme than the chapter itself.</p><p id="440f">And, naturally in a book about self-reference, GEB itself is highly self-referential. Themes are often resolved hundreds of pages later and require going back to appreciate fully the depth of Hofstadter’s argument. Mercifully, he’s a gifted and lucid writer so, even though there are chapters that are dense, it’s always tractable to read.</p><p id="5afb">After 742 pages and even after having written the paragraphs above, I still struggle with a simple answer to the question: “What is this book about?” The best I can come up with is that GEB equips you with mental models to contemplate philosophy.</p><p id="04c7">So to end, a few personal examples about how GEB has influenced my own thinking.</p><p id="1696"><a rel="noopener" href="https://philosophygeek.medium.com/moving-from-com-to-org-34150bea9ade">I recently joined Stand Together</a>, who shares my strong belief in <strong>bottom up solutions</strong>. Perhaps the idea that bottom up solutions are better isn’t just an empirical statement of sociology, but fundamental to the nature of complex systems. Indeed, Hofstadter goes through many examples of how complexity emerges from simpler systems, often which look nothing like the higher-level systems. Consciousness itself doesn’t exist in neurons, and yet neurons as a system create consciousness in humans (this is critical to Hoftstadter’s argument that machines can think). There’s also a fantastical example in a dialog with the Anteater, who has conversations with Aunt Hilary, an ant colony. She is perfectly capable of having a robust conversation with the Anteater, powered by the ants in the colony. Of course, the ants themselves are individuals with their own cares and concerns and has no knowledge of the emergent intelligence, much like Aunt Hilary has no knowledge of her inner workings.</p><p id="0ddf">How DNA expresses as proteins, how the brain functions at multiple levels, how we understand and use words, how programs don’t have access to the underlying transistors, how Aunt Hilary doesn’t know what the ants are doing… all of these are a set of isomorphisms that suggest that bottom up is better than top down. An additional tenet of Stand Together is “believe in people,” which means that the smallest units act intelligently. Like ants or neurons, we make local decisions every day that bubble up into the structure of society, without anyone telling us what to do.</p><p id="fbbe">The idea that epistemic limits exist in something as universal as mathematics has humbled me about the limits of knowledge for complex human systems. Utopian thought experiments often generate useful frames of exploration, but ought not be confused with reality. Utopians often try to pull out the “bugs” from human systems, often which are endemic to the system, or “features,” as we might say in the trade. Bugs might not be desirable, but sometimes, <em>the bugs can’t just be ripped out of the system without destroying the system itself</em>. Our time would be better spent figuring out—within the system—optimize for minimizing the downsides of the “bugs” while maximizing the value of the features. Think about this with respect to capitalism, socialism, and communism…</p><p id="c7f8">A final area where GEB has influenced me is in designing software products. Hugh Dubberly has been my collaborator for years, starting off with our deep dive into cybernetics, the study of feedback loops. We believe that iteration is key to quality; perfection is impossible out of the gate. Further, the system used to generate quality software is a series of feedback loops between customers and the company, product and engineering, and so on. Though specific product frameworks have changed over the years, that obsession with iteration and feedback permeates everything I’ve implemented.</p><p id="ea19">My modest goal in writing this post was to have something I could send to a friend, rather than to spend an hour fumbling a feeble explanation of <em>Gödel, Escher, Bach</em>. I had a secondary goal in the back of my head… if you have a copy of GEB on your shelf collecting dust and you’ve never read more than a chapter or two, dust it off and see how it goes this time.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ProofWiki: Online compendium of mathematical proofs (133 pts)]]></title>
            <link>https://proofwiki.org/wiki/Main_Page</link>
            <guid>39252531</guid>
            <pubDate>Sun, 04 Feb 2024 17:51:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://proofwiki.org/wiki/Main_Page">https://proofwiki.org/wiki/Main_Page</a>, See on <a href="https://news.ycombinator.com/item?id=39252531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr"><h2><span id="Welcome_to_.7F.27.22.60UNIQ-MathJax-1-QINU.60.22.27.7F"></span><span id="Welcome_to_'&quot;`UNIQ-MathJax-1-QINU`&quot;'">
Welcome to $\mathsf{Pr} \infty \mathsf{fWiki}$
</span></h2>
<div><p><a href="https://proofwiki.org/wiki/File:Logo.png"><img alt="Logo.png" src="https://proofwiki.org/w/images/c/c9/Logo.png" decoding="async" width="135" height="135"></a></p>
<p><b>$\mathsf{Pr} \infty \mathsf{fWiki}$</b> is an online compendium of mathematical proofs! Our goal is the collection, collaboration and classification of mathematical proofs. If you are interested in helping create an online resource for math proofs feel free to <b><a href="https://proofwiki.org/wiki/Special:RequestAccount" title="Special:RequestAccount">register for an account</a></b>. Thanks and enjoy!
</p><p>If you have any questions, comments, or suggestions please post on the <b><a href="https://proofwiki.org/wiki/Talk:Main_Page" title="Talk:Main Page">discussion</a></b> page, or contact one of the <span><a rel="nofollow" href="https://www.proofwiki.org/w/index.php?title=Special%3AListUsers&amp;username=&amp;group=sysop&amp;limit=50">administrators</a></span>. Also, feel free to take a look at the <a href="https://proofwiki.org/wiki/Help:FAQ" title="Help:FAQ">frequently asked questions</a> because you may not be the first with your idea.
</p><p>To see what's currently happening in the community, visit the <b><a href="https://proofwiki.org/wiki/ProofWiki:Community_Portal" title="ProofWiki:Community Portal"> community portal</a></b>.
</p>
</div>
<center><big><a href="https://proofwiki.org/wiki/Category:Proofs" title="Category:Proofs">26,727 Proofs</a> <b>—</b> <a href="https://proofwiki.org/wiki/Category:Definitions" title="Category:Definitions">25,226 Definitions</a> <b>—</b> <a href="https://proofwiki.org/wiki/Help:Contents" title="Help:Contents">Help</a></big></center>
<p><a href="https://twitter.com/ProofWiki" data-show-count="false">Follow @ProofWiki</a>
</p>
<h2><span id="Featured_Proof">
Featured Proof
</span></h2>


<h2><span id="Theorem">Theorem</span></h2>
<dl><dd>$\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^n} = 2$</dd></dl>
<p>where $F_n$ is the $n$th <a href="https://proofwiki.org/wiki/Definition:Fibonacci_Number" title="Definition:Fibonacci Number">Fibonacci number</a>.
</p>
<h2><span id="Proof">Proof</span></h2>
<p>Let us define a <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> which satisfies the <a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">Kolmogorov Axioms</a> such that it is the set of all combinations of <a href="https://proofwiki.org/wiki/Definition:Coin/Coin-Tossing" title="Definition:Coin/Coin-Tossing">flipping</a> a fair <a href="https://proofwiki.org/wiki/Definition:Coin" title="Definition:Coin">coin</a> until you receive two <a href="https://proofwiki.org/wiki/Definition:Coin/Head" title="Definition:Coin/Head">heads</a> in a row.
</p><p>Let $X_n$ be the event of some outcome from <a href="https://proofwiki.org/wiki/Definition:Coin/Coin-Tossing" title="Definition:Coin/Coin-Tossing">flipping</a> $n$ fair <a href="https://proofwiki.org/wiki/Definition:Coin" title="Definition:Coin">coins</a> in a row, then $\Pr(X_n) = \dfrac 1 {2^n}$.
</p><p>In the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> defined above, we now demonstrate that for a given number of flips $n$, there are exactly $F_{n - 1}$ outcomes contained in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a>.
</p>
<h3><span id="Illustration">Illustration</span></h3>
<dl><dd>$\begin{array}{c|c|cc}
n &amp; \map f n &amp; \text {Sample Space}: \Omega \\
\hline
1 &amp; 0 &amp; \text {impossible} \\
2 &amp; 1 &amp; HH \\
3 &amp; 1 &amp; THH \\
4 &amp; 2 &amp; (HTHH), (TTHH) \\
5 &amp; 3 &amp; (THTHH), (HTTHH), (TTTHH) \\
6 &amp; 5 &amp; (HTHTHH), (TTHTHH), (THTTHH), (HTTTHH), (TTTTHH) \\
\hline
\cdots &amp; \cdots &amp; \cdots \\
\hline
n &amp; F_{n - 1} &amp; \cdots \\
\hline
\end{array}$</dd></dl>
<p><br>
Reviewing the illustration above, for any given value of $n$:
</p><p>For <b>ALL</b> combinations displayed in <a href="https://proofwiki.org/wiki/Definition:Matrix/Row" title="Definition:Matrix/Row">row</a> $n$ (that is $\map f n$) , we can place a $T$ in front and that new combination would exist in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> for $\paren {n + 1}$.
</p><p>For example:
</p>
<dl><dd>$\paren {HTHH}, \paren {TTHH} \to \paren {THTHH}, \paren {TTTHH}$</dd></dl>
<p><br>
However, we also see that for only those combinations starting with a $T$ (that is $\map f {n - 1}$), can we place an $H$ in front and that new combination will also exist in the <a href="https://proofwiki.org/wiki/Definition:Sample_Space" title="Definition:Sample Space">sample space</a> for $\paren {n + 1}$.
</p><p>For example:
</p>
<dl><dd>$\paren {TTHH} \to \paren {HTTHH}$</dd></dl>
<p><br>
Therefore, we have:
</p>
<table>
<tbody><tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f n\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_{n - 1}\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f {n + 1}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \map f n + \map f {n - 1}\)
</td>
<td>
</td>
<td>
</td>
<td>$\map f n$ is adding a $T$ in front and $\map f {n - 1}$ is adding an $H$ in front
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_{n - 1} + F_{n - 2}\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds F_n\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></tbody></table>
<p>The sum of the probabilities of outcomes in a sample space is one by the second <a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">Kolmogorov Axiom</a>.
</p>
<table>
<tbody><tr>
<td>\((\text {II})\) &nbsp;
</td>
<td>$:$ &nbsp;
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>&nbsp;&nbsp; \(\ds \map \Pr \Omega \)
</td>
<td>&nbsp; \(\ds = \) &nbsp;
</td>
<td>\(\ds 1 \) &nbsp;&nbsp;
</td>
<td>&nbsp;&nbsp;
</td></tr></tbody></table>
<p>Hence:
</p>
<table>
<tbody><tr>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 1}^\infty \frac {F_{n - 1} } {2^n}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 1\)
</td>
<td>
</td>
<td>
</td>
<td><a href="https://proofwiki.org/wiki/Definition:Probability_Measure/Definition_3" title="Definition:Probability Measure/Definition 3">$2$nd Kolmogorov Axiom</a>
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>\(\ds \leadsto \ \ \)
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^{n + 1} }\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 1\)
</td>
<td>
</td>
<td>
</td>
<td>reindexing the sum
</td>
<td>
</td></tr>
<tr>
<td>
</td>
<td>
</td>
<td>\(\ds \leadsto \ \ \)
</td>
<td>
</td>
<td>
</td>
<td>\(\ds \sum_{n \mathop = 0}^\infty \frac {F_n} {2^n}\)
</td>
<td>\(=\)
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>\(\ds 2\)
</td>
<td>
</td>
<td>
</td>
<td>multiplying both sides by $2$
</td>
<td>
</td></tr></tbody></table>
<p>$\blacksquare$
</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple fixes zero-day bug in Apple Vision Pro that 'may have been exploited' (109 pts)]]></title>
            <link>https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/</link>
            <guid>39252321</guid>
            <pubDate>Sun, 04 Feb 2024 17:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/">https://techcrunch.com/2024/01/31/apple-vision-pro-zero-day-security-bug-exploited/</a>, See on <a href="https://news.ycombinator.com/item?id=39252321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">A day after reporters published their first hands-on review of Apple’s Vision Pro, the technology giant released its first security patch for the mixed reality headset to fix a vulnerability that “may have been exploited” by hackers in the wild.</p>
<p>On Wednesday, Apple released visionOS 1.0.2, the software that runs on the Vision Pro, with a fix for a vulnerability in WebKit, the browser engine that runs Safari and other web apps. Apple said the bug, if exploited, allowed malicious code to run on an affected device.</p>
<p>It’s the same vulnerability that Apple patched last week when <a href="https://techcrunch.com/2024/01/23/iphone-users-should-turn-on-apples-stolen-device-protection-feature/" target="_blank" rel="noopener">it rolled out iOS 17.3</a>, which included fixes for iPhones, iPads, Macs and Apple TV — all of which rely on WebKit. No patches for this bug, <a href="https://support.apple.com/en-us/HT214070" target="_blank" rel="noopener">officially tracked as CVE-2024-23222</a>, were released for Apple Watch.</p>
<p>It’s not immediately clear if malicious hackers used the vulnerability to specifically exploit Apple’s Vision Pro, and Apple spokesperson Scott Radcliffe would not say when asked by TechCrunch.</p>
<p>It also isn’t yet known who was exploiting the vulnerability, or for what reason.</p>
<p>It is not uncommon for malicious actors, such as spyware makers, to target weaknesses in WebKit as a way to break into the device’s underlying operating system and the user’s personal data. WebKit bugs can sometimes be exploited when a victim visits a malicious domain in their browser, or the in-app browser.</p>
<p>Apple rolled out several patches for WebKit bugs last year.</p>
<p>Vision Pro is expected to be available starting Friday.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How do transformers work? (119 pts)]]></title>
            <link>https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi</link>
            <guid>39252235</guid>
            <pubDate>Sun, 04 Feb 2024 17:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi">https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi</a>, See on <a href="https://news.ycombinator.com/item?id=39252235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><span>👋 Hi, this is</span><a href="https://twitter.com/gergelyorosz" rel="nofollow ugc noopener"> </a><span>Venkat and here with a free, full issue of the The ZenMode Engineer Newsletter. In every issue, I cover one topic explained in a simpler terms in areas related to computer technologies and beyond.</span></em></p><p>Transformers have become synonymous with cutting-edge AI, particularly in the realm of natural language processing (NLP).</p><p>But what exactly makes them tick? How do these models navigate the intricacies of language with such remarkable efficiency and accuracy? </p><p>Buckle up, because we're about to  learn the heart of the transformer architecture.</p><p>But.. Before we deep dive into it lets understand where its been used.. if you have used google translate/ ChatGPT both rely on these.</p><blockquote><p><em><strong>Google Translate:</strong><span> This widely used platform relies heavily on transformers to achieve fast and accurate translations across over 100 languages. It considers the entire sentence context, not just individual words, leading to more natural-sounding translations.</span></em></p><p><em><strong>Netflix Recommendation System:</strong><span> Ever wondered how Netflix suggests shows and movies you might enjoy? Transformers analyze your viewing history and other users' data to identify patterns and connections, ultimately recommending content tailored to your preferences.</span></em></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png" width="727" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:727,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><strong>The Big Picture: Encoder and Decoder Dance</strong></p><p>Imagine a factory, but instead of assembling physical objects, it processes language. This factory has two main departments:</p><ol><li><p><strong>The Encoder:</strong><span> This is the information extractor, meticulously dissecting the input text, understanding its individual elements, and uncovering the hidden connections between them.</span></p></li><li><p><strong>The Decoder:</strong><span> Armed with the encoder's insights, the decoder crafts the desired output, be it a translated sentence, a concise summary, or even a brand new poem.</span></p></li></ol><p><strong>Encoder: Decoding the Input Labyrinth</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png" width="255" height="338.1111111111111" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:537,&quot;width&quot;:405,&quot;resizeWidth&quot;:255,&quot;bytes&quot;:167254,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9be7be14-7e28-432a-9857-c2eefbb52793_405x537.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The encoder's journey begins with </span><strong>Input Embedding</strong><span>, where each word is transformed from its textual form into a numerical representation (vector). Think of it as assigning each word a unique identifier.  </span></p><p>Consider this example.</p><ol><li><p><strong>Input Text:</strong><span> The process begins with the raw text sentence, such as "The cat sat on the mat."</span></p></li><li><p><strong>Input Embedding Layer:</strong></p><ul><li><p>This layer acts as a translator, converting each word into a numerical vector.</p></li><li><p>Imagine a large dictionary where each word has a corresponding vector address.</p></li><li><p>These vectors capture various aspects of word meaning:</p><ul><li><p>Semantic relationships (e.g., "cat" is closer to "pet" than "chair").</p></li><li><p>Syntactic roles (e.g., "cat" is often a noun, while "sat" is a verb).</p></li><li><p>Context within the sentence (e.g., "mat" here likely refers to a floor mat).</p></li></ul></li></ul></li><li><p><strong>Vector Representation:</strong></p><ul><li><p>The output of this layer is a sequence of numerical vectors, each representing a word:</p><ul><li><p>"The" -&gt; [0.2, 0.5, -0.1, ...]</p></li><li><p>"cat" -&gt; [0.8, -0.3, 0.4, ...]</p></li><li><p>"sat" -&gt; [-0.1, 0.7, 0.2, ...]</p></li><li><p>...</p></li></ul></li></ul></li></ol><p>But the encoder doesn't stop there. It employs the following key mechanisms to delve deeper:</p><ul><li><p><strong>Self-Attention Layer:</strong><span> This is the game-changer. Imagine shining a spotlight on each word, but instead of illuminating it in isolation, you also highlight how it connects to all other words in the sentence. This allows the encoder to grasp the context, nuances, and relationships within the text, not just the individual words.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif" width="1456" height="876" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:876,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F588d2d7b-45e4-4fc2-8ccc-ecf296911acd_1578x949.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>ref from Raimi Karim blog (used only to refernce)</figcaption></figure></div><p><span>Consider  this example sentence again "</span><em><strong>The quick brown fox jumps over the lazy dog.</strong></em><span>" </span></p><ol><li><p><strong>Word Embeddings:</strong><span> First, each word is transformed into a numerical representation called a "word embedding." Think of it as assigning each word a unique identifier in a giant vocabulary map.</span></p></li><li><p><strong>Query, Key, Value:</strong><span> Next, the Self-Attention mechanism creates three special vectors for each word:</span></p><ul><li><p><strong>Query (Q):</strong><span> This vector asks "What information do I need from other words?"</span></p></li><li><p><strong>Key (K):</strong><span> This vector acts like a label, saying "This is the information I have to offer."</span></p></li><li><p><strong>Value (V):</strong><span> This vector holds the actual information, like the word's meaning and context.</span></p></li></ul></li><li><p><strong>Attention Scores:</strong><span> Now comes the interesting part. The Self-Attention layer compares the Query vector of each word with the Key vectors of all other words in the sentence. </span></p><p><span>This helps it understand how relevant each word is to the current word. Based on this comparison, it calculates an </span><strong>attention score</strong><span> for each pair of words.</span></p><p>Imagine shining a spotlight on each word. The brighter the spotlight on another word, the higher the attention score, meaning the more relevant that word is to the current word.</p></li><li><p><strong>Weighted Values:</strong><span> Finally, the Self-Attention layer uses the attention scores to weigh the Value vectors of all other words. Words with higher attention scores get more weight, contributing more to the final representation of the current word.</span></p><p>Think of it like taking a weighted average of the information from other words, where the weights are determined by how relevant they are.</p></li><li><p><strong>New Word Representation:</strong><span> By considering the context provided by other words, the Self-Attention layer creates a new, enriched representation of each word. This new representation captures not just the word's own meaning, but also how it relates to and is influenced by other words in the sentence.</span></p></li></ol></li><li><p><strong>Multi-Head Attention:</strong><span> This is like having multiple teams of analysts, each focusing on different aspects of the connections between words. It allows the encoder to capture various facets of the relationships, enriching its understanding.</span></p><p><strong>Sentence:</strong><span> "The quick brown fox jumps over the lazy dog."</span></p><ol><li><p><strong>Individual Heads:</strong><span> Instead of one Self-Attention mechanism, Multi-Head Attention uses several independent "heads" (often 4-8). Each head has its own set of Query, Key, and Value vectors for each word.</span></p></li><li><p><strong>Diverse Attention:</strong><span> Each head computes attention scores differently, focusing on various aspects of word relationships:</span></p><ul><li><p>One head might attend to grammatical roles (e.g., "fox" and "jumps").</p></li><li><p>Another might focus on word order (e.g., "the" and "quick").</p></li><li><p>Another might capture synonyms or related concepts (e.g., "quick" and "fast").</p></li></ul></li><li><p><strong>Combining Perspectives:</strong><span> After each head generates its own weighted values, their outputs are concatenated. This combines the diverse insights from different attention mechanisms.</span></p></li><li><p><strong>Final Representation:</strong><span> This combined representation holds a richer understanding of the sentence, incorporating various relationships between words, not just a single focus.</span></p></li></ol></li><li><p><strong>Positional Encoding:</strong><span> Since transformers don't process word order directly, this layer injects information about each word's position in the sentence. It's like giving the analysts a map so they know the order in which to consider the words.</span></p><p>Sure, let's delve into positional encoding using an example sentence:</p><p><strong>Sentence:</strong><span> "The quick brown fox jumps over the lazy dog."</span></p><p><strong>Here's how positional encoding works step-by-step:</strong></p><ol><li><p><strong>Word Embeddings:</strong></p><ul><li><p>Each word ("The", "quick", etc.) is converted into a numerical representation called a word embedding, like a unique identifier in a vast vocabulary map.</p></li><li><p>Imagine these embeddings as vectors:</p><ul><li><p>"The": [0.2, 0.5, -0.1, ...]</p></li><li><p>"quick": [0.8, -0.3, 0.4, ...]</p></li><li><p>"brown": [..., ...]</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Positional Information:</strong></p><ul><li><p>Each word's embedding is combined with additional values based on its position in the sentence.</p></li><li><p>These values are calculated using sine and cosine functions at different frequencies:</p><ul><li><p>Lower frequencies capture long-range dependencies (e.g., "quick" and "fox" are related).</p></li><li><p>Higher frequencies encode short-range relationships (e.g., "jumps" and "over" are close).</p></li></ul></li><li><p>Think of these additional values as "position vectors":</p><ul><li><p>"The": [position 1 vector]</p></li><li><p>"quick": [position 2 vector]</p></li><li><p>"brown": [position 3 vector]</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Combining Embeddings and Positions:</strong></p><ul><li><p>The original word embedding and the position vector are added together, creating a new, enriched representation for each word:</p><ul><li><p>"The": [0.2, 0.5, -0.1, ...] + [position 1 vector] = new enriched embedding</p></li><li><p>"quick": [0.8, -0.3, 0.4, ...] + [position 2 vector] = new enriched embedding</p></li><li><p>"brown": [..., ...] + [position 3 vector] = new enriched embedding</p></li><li><p>...</p></li></ul></li></ul></li><li><p><strong>Understanding Order:</strong></p><ul><li><p>Even if the sentence order changes (e.g., "Dog lazy jumps..."), the position vectors ensure relative positions are maintained.</p></li><li><p>The model can still learn that "jumps" is more related to "over" than, say, "The".</p></li></ul></li></ol></li><li><p><strong>Feed Forward Network(FFN):</strong><span> This adds a layer of non-linearity, enabling the model to learn more complex relationships that might not be easily captured by attention mechanisms alone.</span></p><p>You've already delved into the sentence through previous layers. You understand individual words, their relationships, and their positions. Now, the FFN arrives like a detective magnifying glass, ready to uncover intricate details not immediately visible.</p><p><strong>The FFN does this through three key steps:</strong></p><ol><li><p><strong>Non-linear Transformation:</strong><span> Instead of straightforward calculations, the FFN uses non-linear functions like ReLU to add complexity. Think of it as applying a special filter to the existing information, revealing hidden patterns and connections that simple arithmetic might miss. This allows the FFN to capture more nuanced relationships between words.</span></p></li><li><p><strong>Multi-layered Analysis:</strong><span> The FFN isn't just one step; it's typically a chain of two or more fully connected layers. Each layer builds upon the previous one, transforming the information step-by-step. Imagine you're examining the sentence under increasing magnification, uncovering finer details with each layer.</span></p></li><li><p><strong>Dimensionality Shift:</strong><span> The FFN expands the information's size (e.g., from 512 dimensions to 2048) in the first layer. This allows it to analyze a wider range of features and capture more complex patterns. Think of it as spreading out the information on a larger canvas for deeper examination. Then, it contracts it back to the original size (e.g., 512 again) in the final layer to ensure compatibility with subsequent layers.</span></p></li></ol><p><strong>Applying this to our sentence:</strong></p><ul><li><p>Imagine the FFN helps identify that "quick" and "brown" not only describe the "fox" but also subtly connect to its perceived speed through their combined meaning.</p></li><li><p>Or, it might delve deeper into the relationship between "jumps" and "over," understanding the action and spatial context beyond just their individual definitions.</p></li></ul></li><li><p><strong>Repeat, Refine, Repeat:</strong><span> These layers (self-attention, multi-head attention, etc.) are stacked and repeated multiple times. With each iteration, the encoder refines its understanding, building a comprehensive representation of the input text.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif" width="480" height="322" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:322,&quot;width&quot;:480,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fb706d-5faf-4431-b11a-2dc9f6302714_480x322.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>image source: pillow lab blog</figcaption></figure></div><p><strong>Decoder: Weaving the Output Tapestry</strong></p><p>Now, the decoder takes the baton. But unlike the encoder, it has an additional challenge: generating the output word by word without peeking at the future. To achieve this, it utilizes:</p><ul><li><p><strong>Masked Self-Attention:</strong><span> Similar to the encoder's self-attention, but with a twist. The decoder only attends to previously generated words, ensuring it doesn't cheat and use future information. It's like writing a story one sentence at a time, without knowing how it ends.</span></p></li><li><p><strong>Encoder-Decoder Attention:</strong><span> This mechanism allows the decoder to consult the encoded input, like referring back to a reference document while writing. It ensures the generated output stays coherent and aligned with the original text.</span></p></li><li><p><strong>Multi-Head Attention and Feed Forward Network:</strong><span> Just like the encoder, these layers help the decoder refine its understanding of the context and relationships within the text.</span></p></li><li><p><strong>Output Layer:</strong><span> Finally, the decoder translates its internal representation into the actual output word, one by one. It's like the final assembly line, putting the pieces together to form the desired outcome.</span></p></li></ul><p><strong>Beyond the Basics:</strong></p><p>Remember, this is just a glimpse into the fascinating world of transformers. The specific architecture can vary depending on the task and dataset, with different numbers of layers and configurations. </p><p>Additionally, each layer involves complex mathematical operations that go beyond the scope of this explanation. </p><p>But hopefully, this has equipped you with a fundamental understanding of how transformers work and why they have revolutionized the field of NLP. </p><p>So, the next time you encounter a seamless machine translation or marvel at the creativity of an AI-powered text generator, remember the intricate dance of the encoder and decoder within the transformer, weaving magic with the power of attention and parallel processing.</p><p><em>Paper: https://arxiv.org/abs/1706.03762 </em></p><div data-attrs="{&quot;url&quot;:&quot;https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p>Thank you for reading The ZenMode. This post is public so feel free to share it.</p><p data-attrs="{&quot;url&quot;:&quot;https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://nintyzeros.substack.com/p/how-do-transformer-workdesign-a-multi?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel="nofollow ugc noopener"><span>Share</span></a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond self-attention: How a small language model predicts the next token (384 pts)]]></title>
            <link>https://shyam.blog/posts/beyond-self-attention/</link>
            <guid>39251909</guid>
            <pubDate>Sun, 04 Feb 2024 16:54:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shyam.blog/posts/beyond-self-attention/">https://shyam.blog/posts/beyond-self-attention/</a>, See on <a href="https://news.ycombinator.com/item?id=39251909">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="single"><p><time datetime="2024-01-29 17:25:21 -0700 -0700">Jan 29, 2024</time>
<span>·</span>
<span>17754 words</span>
<span>·</span>
<span>84 minute read</span></p><div><p>I trained a small (~10 million parameter) <a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" target="_blank" rel="noopener">transformer</a> following <a href="https://karpathy.ai/" target="_blank" rel="noopener">Andrej Karpathy</a>’s excellent tutorial, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let’s build GPT: from scratch, in code, spelled out</a>. After getting it working, I wanted to understand, as deeply as possible, what it was doing internally and how it produced its results.</p><p>The <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">original paper</a>, as well every transformer tutorial I found, focuses primarily on <a href="https://machinelearningmastery.com/the-transformer-attention-mechanism/" target="_blank" rel="noopener">multi-head self-attention</a>, the mechanism by which transformers learn multiple relationships between tokens without relying on recurrences or convolution. But none of the papers or tutorials I encountered give a satisfying explanation of what happens <em>after attention</em>: <strong>how exactly do the results of the attention computation turn into accurate predictions for the next token?</strong></p><p>I thought I could run a few example prompts through the small but working transformer I’d trained, examine the internal states, and figure this out. What I thought would be a quick investigation turned out to be a 6-month deep dive, but yielded some results I think are worth sharing. Specifically, I have a working theory that explains how the transformer produces its predictions and some empirical evidence that suggests this explanation is at least plausible.</p><p>For those readers familiar with transformers and eager for the punchline, here it is: Each transformer block (containing a multi-head self-attention layer and feed-forward network) learns weights that associate a given prompt with a class of strings found in the training corpus. <strong>The distribution of tokens that follow those strings in the training corpus is, approximately, what the block outputs as its predictions for the next token.</strong> Each block may associate the same prompt with a different class of training corpus strings, resulting in a different distribution of next tokens and thus different predictions. The final transformer output is a linear combination of each block’s predictions.</p><p>I implemented imperative code that does what I’m proposing the transformer is doing. It produces outputs very similar to the transformer, which I’ll review in detail in a <a href="#evaluating-the-approximation">later section</a>.</p><p>In this post, I’m going to briefly introduce the model and training data, demo some evidence for my proposed explanation, give a detailed walkthrough of the imperative code implementation of it, and present the supporting evidence I have for my theory. I’ve tried to keep the main narrative succinct, with links to relevant technical details and justifications in the <a href="#appendices">appendices</a> or other notebooks in the <a href="https://shyam.blog/posts/beyond-self-attention/%28https://github.com/spather/transformer-experiments%29">repo</a>.</p><blockquote><p>This project is my first foray into this type of open-ended ML research. I’m sure I have made errors or omissions that would be obvious to more experienced researchers. I welcome any feedback on this work at <code>shyam.pather at gmail dot com</code>.</p></blockquote><h2 id="the-model-and-setup">The Model and Setup <a href="#the-model-and-setup">🔗</a></h2><blockquote><h3 id="disclaimer">Disclaimer <a href="#disclaimer">🔗</a></h3><p>I want to start by saying upfront: the code for the model I trained isn’t mine. It came from <a href="https://karpathy.ai/" target="_blank" rel="noopener">Andrej Karpathy</a>’s video, <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Let’s build GPT: from scratch, in code, spelled out</a> (highly recommend).</p><p>I typed in the code by copying what I saw on the screen as I watched the video. For things that weren’t clear onscreen, I referenced the <a href="https://github.com/karpathy/ng-video-lecture" target="_blank" rel="noopener">GitHub repo for the video</a> and the <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener">nanoGPT repo</a>. After getting it working, I made only minor changes to make it work with the rest of the code in/structure of <a href="https://github.com/spather/transformer-experiments" target="_blank" rel="noopener">my repository</a>, resulting in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">this implementation</a>. In summary: the core language model is Andrej Karpathy’s work, not mine. The analysis and all the supporting code behind it are my original contributions. I’ll acknowledge and cite influential papers, posts, tutorials, and other resources in the relevant places.</p></blockquote><h3 id="model-overview">Model Overview <a href="#model-overview">🔗</a></h3><p>The model is a 6-block, decoder-only <a href="https://en.wikipedia.org/wiki/Transformer_%28machine_learning_model%29" target="_blank" rel="noopener">transformer</a>:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a9f2adc6c1c25ebb263caf42df37f4429c4ed44eda0a0a228cba52b7a00aeb9d.png" alt=""></p><p>It’s trained on the <a href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt" target="_blank" rel="noopener">TinyShakespeare data set</a> which contains 40,000 lines of Shakespeare’s plays. After about an hour of training on an RTX 4000 GPU, it is able to produce reasonable-looking faux Shakespeare.</p><p>Given a prompt, the model predicts tokens that it thinks should follow. Let’s look at an example: starting with the prompt, <code>ROMEO:</code>, and sampling 500 tokens from the model’s predictions, we get:</p><pre tabindex="0"><code>ROMEO:
If thou wilt triumphant be virtue, and since from any
bold virtue that is made a bawd of earth, then the
duke desires of patience and perish:
take up the other husband, dislike his tent
back.

First Citizen:
Ourself goes, go back: you have no consul, but the disguised gods.

Second Citizen:
We choose him in the world, he did runk itself.

First Citizen:
Sir, I am I a man changed him and thriving, I have heard the
king.

CORIOLANUS:
Consider him!

AUFIDIUS:
Most gracious irice, and you must danc
</code></pre><p>It’s not Shakespeare but structurally, it’s plausible Shakespeare. It looks like the script for a play, the language sounds archaic, the character names/titles come from real Shakespeare plays. Most of the words are English words. Punctuation and capitalization are mostly sensible. Clearly, none of the text actually makes sense, but still, it’s not bad for an hour of training.</p><p>The <strong>tokens in the model are characters</strong>, not words. Given a prompt, the model predicts a probability distribution for the next character. For example, given the prompt <code>'my most gr</code>, the model predicts these probabilities for the next token:</p><pre tabindex="0"><code>'a' 0.819
'e' 0.081
'i' 0.059
'o' 0.036
'u' 0.004
'y' 0.001
'w' 0.000
'r' 0.000
'g' 0.000
's' 0.000
</code></pre><p><a href="#i-model-details">Appendix I</a> provides a few more details about the model. Beyond that, if you want to know more, <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">the code</a> and <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener">Andrej’s video</a> are the best resources.</p><h3 id="transformer-block-structure">Transformer Block Structure <a href="#transformer-block-structure">🔗</a></h3><p>Each of the 6 blocks in the architecture diagram above contains two significant sub-components: a multi-head self-attention layer and a feed-forward network, wired together via a mix of direct and residual connections as follows:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/72a30adc39ebf5f278c0a257fb46f26e6d666d113736e36ce394db587110260c.png" alt=""></p><p>The <code>Block</code> module implements this wiring in PyTorch:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Block</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>"""One transformer block"""</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> __init__(self, n_embed, n_head):
</span></span><span><span>        super()<span>.</span>__init__()
</span></span><span><span>        head_size <span>=</span> n_embed <span>//</span> n_head
</span></span><span><span>        self<span>.</span>sa <span>=</span> MultiHeadAttention(n_head, head_size)
</span></span><span><span>        self<span>.</span>ffwd <span>=</span> FeedForward(n_embed)
</span></span><span><span>        self<span>.</span>ln1  <span>=</span> nn<span>.</span>LayerNorm(n_embed)
</span></span><span><span>        self<span>.</span>ln2 <span>=</span> nn<span>.</span>LayerNorm(n_embed)
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>sa(self<span>.</span>ln1(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>ffwd(self<span>.</span>ln2(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> x
</span></span></code></pre></div><p>While many words have been written and spoken about multi-head attention, comparatively little has been said about the feed-forward network because, it seems, comparatively little is known:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/b8214cdd1f6c9466bb984529984c757d780148fb4fe44bfed7714216e12bff73.png" alt=""></p><p>Screenshot from <a href="https://stats.stackexchange.com/q/485910">https://stats.stackexchange.com/q/485910</a></p><p>I started this investigation wondering what comes after attention. Literally, the feed-forward network does. In the transformer I studied, across all 6 blocks, the feed-forward networks comprise over 65% of the total trainable parameters, so they must play some important role.</p><p>As I’ll show <a href="#transformation-via-vector-addition">later</a>, it turns out that the output of the feed-forward network is the primary factor that determines how a block transforms its input into its output.</p><h2 id="demo-my-proposal-in-action">Demo: My Proposal In Action <a href="#demo-my-proposal-in-action">🔗</a></h2><p>In this section, I’m going to show an example that illustrates what I’m proposing the transformer is doing. In the next section, I’ll go into detail about how this is implemented.</p><p>Imagine we did the following:</p><ul><li>Ran the prompt, <code>'And only l'</code>, through the model and extracted the output value of the feed-forward network in the first transformer block.</li><li>Went back to the training corpus, found all substrings of the same length as our prompt (10-characters), ran all of them through the model, and filtered out just the ones whose feed-forward network outputs in the first block have a cosine similarity of 0.95 or greater when compared to that of the prompt, <code>'And only l'</code>.</li></ul><p>We’d come up with this set of strings:</p><pre tabindex="0"><code>'hat only l'    's sickly l'    ' as\nthey l'   'r kingly l'    're; they l'
'eby they l'    'ar, they l'    'im, only l'    'ling any l'    'life may l'
'nobility l'    'e\nBy any l'   ' as they l'    ', if any l'    ' hastily l'
'tly they l'    ' ghastly l'    '\nMy only l'   'For many l'    'r in any l'
' till my l'    'all they l'    'hen they l'    'at Henry l'    'oolishly l'
'er:\nThey l'   'may they l'    'or stony l'    'ur Henry l'    'l gladly l'
'yet they l'    'y;\nDelay l'   'e, on my l'    'or Henry l'    'I dearly l'
' if they l'    ' she may l'    't\nfairly l'   'ould say l'    'd all my l'
'her they l'    ' Stanley l'    ' and may l'    'uld they l'    'u all my l'
'friendly l'    'h gently l'    'e deadly l'    'f all my l'    'n all my l'
'Ere they l'    'steel my l'    ' tell my l'    'e kingly l'    'learn my l'
'd he say l'    't basely l'    'Thursday l'    'iciously l'    " 'if any l"
' as many l'    'hy glory l'    'not very l'    'a goodly l'    'e surely l'
'quiously l'    ', fairly l'    'lord! my l'    'entle my l'    ', he may l'
'our holy l'    ' worldly l'    ' my only l'    ' all, my l'
'ul, they l'    'o lately l'    's in any l'    ' no lady l'
'ter many l'    'Our holy l'    't vainly l'    'e\nA lady l'
' you may l'    'y greedy l'    'untimely l'    'directly l'
'er on my l'    'e wistly l'    'ng Henry l'    'And only l'
's kindly l'    'KE:\nThey l'   ' of many l'    'o, on my l'
</code></pre><p>There’s a clear pattern across these: they all end in <code>y l</code> and several of them end in <code>ly l</code>. Similarity in the space of feed-forward network outputs seems to correspond to human-interpretable patterns.</p><p>Next, imagine we went back to the training corpus, found each of these strings and built a distribution of all the characters that came after them. We’d find, for example:</p><ul><li><code>'hat only l'</code> is followed by <code>i</code> (“T<code>hat only l</code><strong>i</strong>ke a gulf it did remain”)</li><li><code>'l gladly l'</code> is followed by <code>e</code> (“I’l<code>l gladly l</code><strong>e</strong>arn.”)</li><li><code>'n all my l'</code> is followed by both <code>a</code> and <code>i</code> (“I<code>n all my l</code><strong>a</strong>nds and leases whatsoever” and “never saw you before i<code>n all my l</code><strong>i</strong>fe”)</li></ul><p>Doing this for the complete set of 94 strings, we’d end up with this distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/170aed320bd4ab2e2647d8d1ef50b499b215ce1905cff1b5db6fe78dd83c3df3.png" alt=""></p><p>The various tokens in our model’s vocabulary appear on the x-axis and the normalized frequency of occurrence on the y-axis. This plot shows that <code>i</code> was the most frequent, then <code>o</code>, then <code>a</code>, and finally, <code>e</code>.</p><p>Now let’s look at the final output of the transformer as a whole when given <code>And only l</code> as a prompt:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/08cb64e75759e2a329a0a296931433b06c02c2fb92b6dd0bb440af807cde1d86.png" alt=""></p><p>This is a probability distribution representing the model’s predictions for the next token. Notice that it’s strikingly similar to the normalized frequency distribution shown in the previous plot!</p><p>We can quantify how similar they are. <a href="https://en.wikipedia.org/wiki/Hellinger_distance" target="_blank" rel="noopener">Hellinger distance</a> is a measure of overlap between probability distributions. Given distributions \(P\) and \(Q\), the Hellinger distance between them is:</p><p>$$
H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_{i=1}^n (\sqrt{p_i} - \sqrt{q_i})^2}
$$</p><p>Or, in code:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>hellinger_distance</span>(
</span></span><span><span>    p: torch<span>.</span>Tensor,
</span></span><span><span>    q: torch<span>.</span>Tensor,
</span></span><span><span>):
</span></span><span><span>    <span>return</span> ((p<span>.</span>sqrt() <span>-</span> q<span>.</span>sqrt())<span>**</span><span>2</span>)<span>.</span>sum(dim<span>=-</span><span>1</span>)<span>.</span>sqrt() <span>/</span> math<span>.</span>sqrt(<span>2</span>)
</span></span></code></pre></div><p>Hellinger distance of 0 means the two distributions are identical and 1 means they have no overlap.</p><p>The Hellinger distance between the two distributions above - the distribution formed from the tokens that follow the strings with similar feed-forward network outputs and the distribution the model predicts - is 0.07: very nearly identical.</p><p>For the sake of keeping the demo brief, I chose an example where the first block’s similar strings alone are enough to produce a distribution that closely matches the final output of the transformer. Typically, we’d need to need to do the same exercise - finding the strings in the training corpus that produce similar feed-forward network outputs to the prompt and building a distribution from the tokens that succeed them - for all 6 transformer blocks, and then calculate a weighted sum of the resulting distributions in order to get a good match. We’ll do that in the next section and see that <strong>across a sample of 20,000 prompts, the average Hellinger distance between distributions computed this way and the corresponding transformer output was just 0.17</strong>.</p><p>This small average Hellinger distances suggests the results produced by this approach are a good approximation for the transformer’s outputs. In addition, as I’ll explain in the <a href="#interpretation-why-does-the-approximation-work">interpretation</a> section, I think the approach itself is a reasonable approximation of what the transformer is actually doing.</p><h2 id="implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">Implementation: Approximating the Transformer Output with Feed-forward Network Outputs <a href="#implementation-approximating-the-transformer-output-with-feed-forward-network-outputs">🔗</a></h2><p>In this section, I’m going to walk through in some detail and with code, the exact procedure I used to approximate the transformer’s output using strings that produced similar feed-forward network outputs. If you’re not interested in the implementation, skip this section and proceed to the <a href="#evaluating-the-approximation">evaluation</a> section.</p><p>To recap, this is the procedure to compute the approximation:</p><ol><li>Run a prompt through the model and save the feed-forward network outputs for each block.</li><li>For each block:<ul><li>Find the strings in the training corpus that produce the most similar feed-forward network outputs to the prompt for that block.</li><li>For each string found, build a frequency distribution of the tokens that come after it in the training corpus.</li><li>Sum the frequency distributions for all strings found for the current block.</li></ul></li><li>Compute a weighted sum of the frequency distributions for each block computed in the previous step.</li><li>Normalize the weighted sum to get a probability distribution.</li></ol><h3 id="procedure-setup">Procedure Setup <a href="#procedure-setup">🔗</a></h3><p>The first step of the procedure - running a prompt through the model and saving the feed-forward network outputs for each block - is straightforward to accomplish with some basic PyTorch hooks. But the first part of step two - finding the strings in the training corpus that produce similar feed-forward network outputs - requires some additional machinery to do efficiently.</p><p>I did all the analysis with length 10 strings for compute and storage efficiency (but I also observed that the results hold for both shorter and longer strings). The 1,115,394-character long training corpus contains 858,923 unique, length 10 substrings. Each feed-forward network output is a 384-dimensional vector of <code>float32</code> values and the model produces 6 of them (one for each block). Comparing the 6 384-dimensional feed-forward outputs for any prompt to 6 * 858,923 = 5,153,538 feed-forward outputs from all the other strings takes a long time. To able to work with this data, I had to pre-compute things. I built the following pipeline:</p><ol><li>I chose 20,000 length 10 strings from the training corpus at random to use as prompts in this experiment.</li><li>Overnight, I ran a process to compute the cosine similarity between the feed-forward network outputs the model produced for the 20,000 prompts and those it produced for the 858,923 unique length 10 substrings of the training corpus. I did this in batches and saved the results to disk.</li><li>Even after pre-computing the cosine similarity results, searching through all of them to find the closest matches took a long time. Experiments showed matches of interest never had a cosine similarity below 0.7, so I ran another step to pre-filter the results of step 2 to just those entries with cosine similarity &gt;= 0.7. This greatly reduced the number of entries to search through.</li></ol><p>The code for this pre-computation and pre-filtering is too much to include in this post, but the implementation is available in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb" target="_blank" rel="noopener">the <code>cosine-sims</code> experiment notebook</a>.</p><h3 id="procedure-walkthrough">Procedure Walkthrough <a href="#procedure-walkthrough">🔗</a></h3><p>In this section, we’ll build up the code step by step and run it on one prompt at a time and for just one block. Over the following sections, we’ll extend it to additional blocks, run it across a large number of prompts, and examine the results.</p><p>First, we need to grab 20,000 length 10 strings from the training corpus to use as prompts:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Get all the unique substrings in the text</span>
</span></span><span><span>strings10 <span>=</span> all_unique_substrings(text<span>=</span>ts<span>.</span>text, substring_length<span>=</span><span>10</span>)
</span></span><span><span>
</span></span><span><span>n_prompts <span>=</span> <span>20000</span>
</span></span><span><span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>1337</span>)
</span></span><span><span>indices <span>=</span> torch<span>.</span>randperm(len(strings10))[:n_prompts]
</span></span><span><span>prompts <span>=</span> [strings10[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span></code></pre></div><p>As described in the <a href="#procedure-setup">Procedure Setup</a> section, I previously ran all these strings through the model, grabbed the feed-forward network outputs for each block, and pre-computed the cosine similarities to all the unique length 10 substrings in the training corpus. And then I pre-filtered the results to just those with cosine similarity &gt;= 0.7.</p><p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/cosine-sims.ipynb" target="_blank" rel="noopener">the <code>cosine-sims</code> experiment notebook</a> that implements all this also exports a helper function, <code>filter_on_prefiltered_results()</code>, that we can use to find the most similar strings to a given prompt by searching over the pre-filtered results.</p><blockquote><p>If you’re curious about how this works, check out the notebook. It’s pretty straightforward and the unit test provides a simple example that illustrates the shape of the inputs and outputs.</p></blockquote><p>To use <code>filter_on_prefiltered_results()</code>, we just need to tell it how to find the prefiltered files:</p><div><pre tabindex="0"><code data-lang="python"><span><span>prefiltered_threshold<span>=</span><span>0.7</span>
</span></span><span><span>prefiltered_results_folder <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'cosine_sim_results/large_files/slen10'</span> <span>/</span> <span>f</span><span>'prefiltered_</span><span>{</span>prefiltered_threshold<span>}</span><span>'</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>prefiltered_filename</span>(block_idx: int, q_idx: int) <span>-&gt;</span> Path:
</span></span><span><span>    <span>return</span> prefiltered_results_folder <span>/</span> <span>f</span><span>'cosine_sim_ffwd_out_</span><span>{</span>q_idx<span>:</span><span>05d</span><span>}</span><span>_</span><span>{</span>block_idx<span>:</span><span>02d</span><span>}</span><span>.pt'</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>load_prefiltered_data</span>(block_idx: int, q_idx: int):
</span></span><span><span>    <span>return</span> torch<span>.</span>load(prefiltered_filename(block_idx, q_idx))
</span></span></code></pre></div><blockquote><p>Note on the use of <code>q_idx</code> here and in the rest of the code: <code>q_idx</code> refers to “query index”. The job that pre-computes all the cosine similarities takes a set of “queries” or values to compare to. These queries are the feed-forward network outputs the model produces for the prompts. There is a 1:1 correspondence between queries and prompts and so I’ve used the terms interchangeably in the code.</p></blockquote><p>To start, we’ll use the same prompt - <code>'And only l'</code> - we used in the earlier demo. It happens to be the prompt at index 57:</p><pre tabindex="0"><code>'And only l'
</code></pre><p>We’ll find the strings whose feed-forward network outputs in block 0 had a cosine similarity of 0.95 or greater when compared to the block 0 feed forward network output of the prompt.</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.95</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>94
</code></pre><p>This produced the 94 similar strings we saw in the demo. We can print them again to be sure:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(<span>f</span><span>"Original string: </span><span>{</span>repr(prompts[q_idx])<span>}</span><span>"</span>)
</span></span><span><span>print(<span>"Similar strings: </span><span>\n</span><span>"</span>)
</span></span><span><span>
</span></span><span><span>data_columns<span>=</span>[
</span></span><span><span>    [repr(s) <span>for</span> s <span>in</span> similar_strings[<span>0</span>][i : i <span>+</span> <span>20</span>]] <span>for</span> i <span>in</span> range(<span>0</span>, len(similar_strings[<span>0</span>]), <span>20</span>)
</span></span><span><span>]
</span></span><span><span>
</span></span><span><span>print(text_table(
</span></span><span><span>    headers<span>=</span>[],
</span></span><span><span>    data_columns<span>=</span>data_columns,
</span></span><span><span>    col_widths<span>=</span>[<span>18</span> <span>for</span> _ <span>in</span> data_columns]
</span></span><span><span>))
</span></span></code></pre></div><pre tabindex="0"><code>Original string: 'And only l'
Similar strings:

'hat only l'      's sickly l'      ' as\nthey l'     'r kingly l'      're; they l'
'eby they l'      'ar, they l'      'im, only l'      'ling any l'      'life may l'
'nobility l'      'e\nBy any l'     ' as they l'      ', if any l'      ' hastily l'
'tly they l'      ' ghastly l'      '\nMy only l'     'For many l'      'r in any l'
' till my l'      'all they l'      'hen they l'      'at Henry l'      'oolishly l'
'er:\nThey l'     'may they l'      'or stony l'      'ur Henry l'      'l gladly l'
'yet they l'      'y;\nDelay l'     'e, on my l'      'or Henry l'      'I dearly l'
' if they l'      ' she may l'      't\nfairly l'     'ould say l'      'd all my l'
'her they l'      ' Stanley l'      ' and may l'      'uld they l'      'u all my l'
'friendly l'      'h gently l'      'e deadly l'      'f all my l'      'n all my l'
'Ere they l'      'steel my l'      ' tell my l'      'e kingly l'      'learn my l'
'd he say l'      't basely l'      'Thursday l'      'iciously l'      " 'if any l"
' as many l'      'hy glory l'      'not very l'      'a goodly l'      'e surely l'
'quiously l'      ', fairly l'      'lord! my l'      'entle my l'      ', he may l'
'our holy l'      ' worldly l'      ' my only l'      ' all, my l'
'ul, they l'      'o lately l'      's in any l'      ' no lady l'
'ter many l'      'Our holy l'      't vainly l'      'e\nA lady l'
' you may l'      'y greedy l'      'untimely l'      'directly l'
'er on my l'      'e wistly l'      'ng Henry l'      'And only l'
's kindly l'      'KE:\nThey l'     ' of many l'      'o, on my l'
</code></pre><p>Next, we’ll need to build a frequency distribution for the tokens that came after these strings in the text. To make this easy and efficient (we’ll eventually be doing many times), we can pre-compute the next token frequency distributions for all the unique length 10 substrings in the training corpus. The helper function <code>build_next_token_map()</code>, implemented in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/text-analysis.ipynb" target="_blank" rel="noopener">text-analysis module</a>, does this.</p><div><pre tabindex="0"><code data-lang="python"><span><span>next_token_map10 <span>=</span> build_next_token_map(
</span></span><span><span>    text<span>=</span>ts<span>.</span>text,
</span></span><span><span>    prefix_len<span>=</span><span>10</span>,
</span></span><span><span>    vocab_size<span>=</span>tokenizer<span>.</span>vocab_size,
</span></span><span><span>    stoi<span>=</span>tokenizer<span>.</span>stoi
</span></span><span><span>)
</span></span></code></pre></div><p>The return value stored in <code>next_token_map10</code> is a dictionary that maps each unique length 10 substring in the training corpus to a frequency distribution of the tokens that come after it. Conceptually, it looks something like this:</p><div><pre tabindex="0"><code data-lang="python"><span><span>{
</span></span><span><span>    <span>'the common'</span>: {
</span></span><span><span>        <span>' '</span>: <span>12</span>, <span>"'"</span>: <span>1</span>, <span>','</span>: <span>1</span>, <span>'?'</span>: <span>1</span>, <span>'a'</span>: <span>1</span>, <span>'s'</span>: <span>5</span>, <span>'w'</span>: <span>3</span>
</span></span><span><span>    },
</span></span><span><span>    <span>' the gods '</span>: {
</span></span><span><span>        <span>'b'</span>: <span>1</span>, <span>'c'</span>: <span>1</span>, <span>'d'</span>: <span>2</span>, <span>'f'</span>: <span>1</span>, <span>'g'</span>: <span>1</span>, <span>'h'</span>: <span>2</span>, <span>'k'</span>: <span>2</span>, <span>'s'</span>: <span>2</span>, <span>'t'</span>: <span>1</span>, <span>'w'</span>: <span>2</span>
</span></span><span><span>    },
</span></span><span><span>    <span>' authority'</span>: {
</span></span><span><span>        <span>'</span><span>\n</span><span>'</span>: <span>1</span>, <span>' '</span>: <span>5</span>, <span>','</span>: <span>5</span>, <span>':'</span>: <span>2</span>, <span>';'</span>: <span>1</span>
</span></span><span><span>    },
</span></span><span><span>    <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>In reality, the values are actually tensors of shape <code>(vocab_size,)</code> where <code>vocab_size</code> is the number of unique tokens the vocabulary (65, in our case). The item at index <code>i</code> in the tensor is the count of occurrences of the <code>i</code>th token after the string in that entry’s key. So it looks more like:</p><div><pre tabindex="0"><code data-lang="python"><span><span>{
</span></span><span><span>      <span>'the common'</span>: torch<span>.</span>tensor([
</span></span><span><span>            <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>      <span>' the gods '</span>: torch<span>.</span>tensor([
</span></span><span><span>            <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>            <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>      <span>' authority'</span>: torch<span>.</span>tensor([
</span></span><span><span>          <span>0</span>, <span>12</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>1</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,
</span></span><span><span>          <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>5</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>,  <span>3</span>,  <span>0</span>,  <span>0</span>,  <span>0</span>
</span></span><span><span>      ]),
</span></span><span><span>    <span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p>Next, we need to sum the frequency distributions for all the strings we found to have similar feed-forward network outputs to our prompt. Because <code>next_token_map10</code> stores the individual frequency distributions as tensors, this is easy to accomplish:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span></code></pre></div><p>We stack up the distributions for each similar string into a single tensor and then sum across all of them. We can now turn this into a probability distribution by dividing each entry by the sum of all the entries:</p><div><pre tabindex="0"><code data-lang="python"><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span></code></pre></div><p>Finally, we can visualize this distribution:</p><div><pre tabindex="0"><code data-lang="python"><span><span>plot_prob_distribution_for_tokens(prob_distribution, title<span>=</span><span>'Probability distribution using only block 0 similar strings'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/c04c3fbe83a543ea834691f8ef6c5ecdee18522f3e3e456cd9ea81209eb60b00.png" alt=""></p><p>It’s the same distribution we saw in the demo.</p><p>Now let’s code the comparison to the model output:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>logits<span>.</span>plot_probs(title<span>=</span><span>'Probability distribution from model'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/08cb64e75759e2a329a0a296931433b06c02c2fb92b6dd0bb440af807cde1d86.png" alt=""></p><p>Again, the two distributions look very similar, and in this example, the approximation uses only values from the first block. To better compare them, we can look at the distributions in text form:</p><div><pre tabindex="0"><code data-lang="python"><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.389
o: 0.204            o: 0.250
a: 0.195            a: 0.222
e: 0.160            e: 0.139
</code></pre><p>Finally, we can also compare the Hellinger distance between these distributions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.0711)
</code></pre><p>By combining the next token frequency distributions of the similar strings from just the first layer of the model, we are able to pretty closely approximate the output of the transformer. Of course, I chose an example that works particularly well.</p><p>Here’s an example where the frequency distribution from just the first layer doesn’t work well:</p><pre tabindex="0"><code>'hing tremb'
</code></pre><p>Using the same method, we can identify 57 strings from the training corpus that produce similar feed-forward network outputs to the prompt:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.95</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>57
</code></pre><p>We can look up, sum, and normalize the frequency distributions of tokens that follow these strings in the training corpus, and compare the result to the model outputs, as we did before:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            e: 0.543
e: 0.000            l: 0.343
r: 0.000            r: 0.114
</code></pre><p>Unlike the previous example, these distributions are quite different. The top 3 tokens are the same in each, but they’re in the wrong order and their probabilities are far apart. These differences contribute to a large Hellinger distance:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.6305)
</code></pre><p>For the prompt, <code>'hing tremb'</code>, just using the values from the first block results in a poor approximation of the transformer’s output. We’ll soon add the contributions from other blocks and when we do, we’ll get the Hellinger distance between the approximation and the real transformer output for this prompt down from 0.63 to just 0.02.</p><h3 id="similarity-thresholds">Similarity Thresholds <a href="#similarity-thresholds">🔗</a></h3><p>In the preceding examples, I used a similarity threshold of 0.95: I searched for strings whose feed-forward network outputs in block 0 produced values with a cosine similarity of 0.95 or greater when compared to the feed-forward network output of the prompt.</p><p>A different threshold would have yielded different results. For example, doing the same exercise for prompt id 57 (<code>'And only l'</code>) with a threshold of 0.90 finds 612 similar strings, vs the 94 we had before:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.90</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>612
</code></pre><p>If we do the rest of the approximation procedure, we see different (and worse) results:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.584
o: 0.204            i: 0.251
a: 0.195            a: 0.095
e: 0.160            e: 0.066
u: 0.004            u: 0.002
l: 0.000            y: 0.001
</code></pre><p>The top 5 tokens are the same, but when ranked by probability, the approximation has a different ordering than the model. The Hellinger distance is also higher:</p><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.2856)
</code></pre><p>Loosening the similarity threshold introduced strings into the calculation that resulted in a worse approximation. Tightening beyond 0.95 also produces worse results than we got with 0.95, presumably because we’re excluding strings that were needed to produce a good approximation:</p><div><pre tabindex="0"><code data-lang="python"><span><span>block_idx <span>=</span> <span>0</span>
</span></span><span><span>similarity_threshold<span>=</span><span>0.97</span>
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>    load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>    q_idx_start<span>=</span>q_idx,
</span></span><span><span>    q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>    filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_threshold
</span></span><span><span>)
</span></span><span><span>similar_strings <span>=</span> [
</span></span><span><span>    [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>    <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>]
</span></span><span><span>len(similar_strings[<span>0</span>])
</span></span></code></pre></div><pre tabindex="0"><code>33
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum()
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution, tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            o: 0.278
o: 0.204            i: 0.250
a: 0.195            a: 0.250
e: 0.160            e: 0.222
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.1498)
</code></pre><p>For the first block, 0.95 appears to be a sweet spot. I came up with this threshold through manual tuning: trying different values and binary searching towards one that produced the best results. The full history of this tuning exercise is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb" target="_blank" rel="noopener">the similar space analysis notebook</a>.</p><p>In the end, I found the following thresholds produce the best results for each block:</p><table><thead><tr><th>Block</th><th>Similarity Threshold</th></tr></thead><tbody><tr><td>0</td><td>0.95</td></tr><tr><td>1</td><td>0.94</td></tr><tr><td>2</td><td>0.85</td></tr><tr><td>3</td><td>0.76</td></tr><tr><td>4</td><td>0.81</td></tr><tr><td>5</td><td>0.89</td></tr></tbody></table><blockquote><p>When I first started exploring this space, I assumed the approximation would get better the more similarity I could find. I tried a number of techniques, including experimenting with Euclidean distance vs cosine similarity, searching across strings of different lengths, etc. Every time I succeeded in finding strings with more similar feed-forward network outputs to use in the approximation, the results got worse. I realized that, at least for some blocks, including <em>less</em> similar values in the mix produced better approximations, probably because those blocks had learned to map prompts to broader classes of strings in the training corpus.</p></blockquote><h3 id="going-beyond-the-first-block">Going Beyond the First Block <a href="#going-beyond-the-first-block">🔗</a></h3><p>Thus far, we’ve only considered feed-forward network outputs from the first block. Now we’ll incorporate the contributions from the other blocks.</p><p>First, let’s find the strings that produce similar feed-forward network outputs in each block, using the similarity thresholds listed above. For now, we’ll do this for just one query (index 57, <code>'And only l'</code>):</p><div><pre tabindex="0"><code data-lang="python"><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>q_idx <span>=</span> <span>57</span>
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span>q_idx,
</span></span><span><span>        q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span></code></pre></div><p>Let’s summarize how many strings we found for each block based on these thresholds:</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(text_table(
</span></span><span><span>    headers<span>=</span>[<span>"Block Index"</span>, <span>"Similarity Threshold"</span>, <span>"# of Similar Strings"</span>],
</span></span><span><span>    data_columns<span>=</span>[
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>block_idx<span>:</span><span>&gt;10</span><span>}</span><span>"</span> <span>for</span> block_idx <span>in</span> range(n_layer)],
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>threshold<span>:</span><span>&gt;19</span><span>}</span><span>"</span> <span>for</span> threshold <span>in</span> similarity_thresholds],
</span></span><span><span>        [<span>f</span><span>"</span><span>{</span>len(similar_strings[<span>0</span>])<span>:</span><span>&gt;19</span><span>}</span><span>"</span> <span>for</span> similar_strings <span>in</span> similar_strings_per_block],
</span></span><span><span>    ],
</span></span><span><span>    col_widths<span>=</span>[<span>14</span>, <span>23</span>, <span>23</span>]
</span></span><span><span>))
</span></span></code></pre></div><pre tabindex="0"><code>Block Index   Similarity Threshold   # of Similar Strings
-----------   --------------------   --------------------
         0                   0.95                     94
         1                   0.94                     47
         2                   0.85                     70
         3                   0.76                    108
         4                   0.81                    175
         5                   0.89                   2237
</code></pre><p>Now that we’ve identified the right strings for each block, we can do the next step of the approximation procedure: build the frequency distributions for the tokens that follow those strings, and sum them up. We’re going to be doing this several times over, so let’s define a function for it:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>frequency_distribution_from_similar_strings</span>(
</span></span><span><span>    similar_strings_per_block: Sequence[Sequence[Sequence[str]]],
</span></span><span><span>    next_token_map: Dict[str, torch<span>.</span>Tensor],
</span></span><span><span>) <span>-&gt;</span> torch<span>.</span>Tensor:
</span></span><span><span>    <span># freqs_per_block_per_query is a list of lists of tensors. The outer list has</span>
</span></span><span><span>    <span># one item per block. The inner list has one item per query. Each</span>
</span></span><span><span>    <span># tensor is the next token frequency distribution for a particular</span>
</span></span><span><span>    <span># block and query.</span>
</span></span><span><span>    freqs_per_block_per_query: List[List[torch<span>.</span>Tensor]] <span>=</span> [[] <span>for</span> _ <span>in</span> range(n_layer)]
</span></span><span><span>
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>        <span>for</span> similar_strings <span>in</span> similar_strings_per_block[block_idx]:
</span></span><span><span>            freqs_per_block_per_query[block_idx]<span>.</span>append(
</span></span><span><span>                torch<span>.</span>stack([next_token_map[string] <span>for</span> string <span>in</span> similar_strings])<span>.</span>sum(
</span></span><span><span>                    dim<span>=</span><span>0</span>
</span></span><span><span>                )
</span></span><span><span>            )
</span></span><span><span>
</span></span><span><span>    <span># Stack all frequency tensors into a single tensor of shape</span>
</span></span><span><span>    <span># (n_layer, n_queries, vocab_size)</span>
</span></span><span><span>    freqs <span>=</span> torch<span>.</span>stack(
</span></span><span><span>        [
</span></span><span><span>            torch<span>.</span>stack(freqs_per_block_per_query[block_idx])
</span></span><span><span>            <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>        ]
</span></span><span><span>    )
</span></span><span><span>
</span></span><span><span>    <span>return</span> freqs
</span></span></code></pre></div><p>This function, <code>frequency_distribution_from_similar_strings()</code>, does the equivalent of this code we looked at earlier:</p><div><pre tabindex="0"><code data-lang="python"><span><span>total_freq_distribution <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    next_token_map10[string] <span>for</span> string <span>in</span> similar_strings[<span>0</span>]
</span></span><span><span>])<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span></code></pre></div><p>But with two key differences:</p><ul><li>It does this calculation for all the blocks, using the similar strings we found for each block above.</li><li>It allows for more than one query. In the code we’ve looked at so far, we only evaluated the approximation for a single prompt. In the next section, we’ll be running it for lots of prompts so I’ve written the code in a more general form to a allow for this. Specifically, the code allows for <code>similar_strings_per_block</code> to contain not just a single list of strings per block but multiple: one for each query.</li></ul><p>Let’s run this on the <code>similar_strings_per_block</code> we constructed earlier:</p><div><pre tabindex="0"><code data-lang="python"><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>freq_distribution<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([6, 1, 65])
</code></pre><p>It produces a tensor of shape <code>(6, 1, 65)</code>: 6 blocks, 1 query, 65 tokens in the vocabulary. If we’d been working with more queries, the middle dimension would be larger.</p><p>So now we have a frequency distribution for each block, based on the strings found for each block using the similarity thresholds. We now need to turn this into a probability distribution.</p><p>Earlier, when we just had a single frequency distribution for a single block, we just normalized it. But now we have multiple frequency distributions - one for each block - and need to combine them. In my experiments, I found that a weighted sum of these distributions produced the best results.</p><p>As with the similarity thresholds, I was able to find a set of good weights by trial and error. I also tried a deep-learning approach to find weights, but did not get better results than with the hand-tuned approach. The procedure for both hand-tuning and learning weights is implemented in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/40_widening_similar_space.ipynb" target="_blank" rel="noopener">the similar space notebook</a>, the same one used for tuning thresholds.</p><p>For now, let’s use the optimal weights I found:</p><div><pre tabindex="0"><code data-lang="python"><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span></code></pre></div><p>We multiply the frequency distributions by the weights, sum across all blocks, and then normalize into a probability distribution. We can now look at how the approximation’s distribution compares to the model’s.</p><blockquote><p>Note: in the code below, we have to index into the <code>prob_distribution</code> tensor with <code>[0]</code> because its first dimension is the number of queries. We’re only working with a single query, so we can just take the first element.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution[<span>0</span>], tokenizer<span>.</span>itos)
</span></span><span><span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
i: 0.437            i: 0.363
o: 0.204            o: 0.265
a: 0.195            a: 0.213
e: 0.160            e: 0.147
u: 0.004            u: 0.011
l: 0.000            y: 0.000
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution[<span>0</span>], logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor(0.0731)
</code></pre><p>In this particular case, adding the other layers didn’t change the approximation much (if anything, it’s very slightly worse based on Hellinger distance). But let’s look at the example that didn’t work well when we considered just the first layer: prompt id 40 (<code>'hing tremb'</code>).</p><div><pre tabindex="0"><code data-lang="python"><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>q_idx <span>=</span> <span>40</span>
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span>q_idx,
</span></span><span><span>        q_idx_end<span>=</span>q_idx<span>+</span><span>1</span>,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span><span><span>
</span></span><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_string(prompts[q_idx])
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>
</span></span><span><span>approx_top_tokens <span>=</span> top_nonzero_tokens(prob_distribution[<span>0</span>], tokenizer<span>.</span>itos)
</span></span><span><span>model_top_tokens <span>=</span> logits<span>.</span>topk_tokens(k<span>=</span><span>10</span>)[<span>0</span>][<span>-</span><span>1</span>]
</span></span><span><span>
</span></span><span><span>print_distribution_comparison(approx_top_tokens, model_top_tokens)
</span></span></code></pre></div><pre tabindex="0"><code>Model Predictions   Approximation Predictions
-----------------   -------------------------
l: 0.999            l: 0.997
e: 0.000            e: 0.002
r: 0.000            r: 0.000
</code></pre><div><pre tabindex="0"><code data-lang="python"><span><span>hellinger_distance(prob_distribution, logits<span>.</span>probs()[<span>0</span>][<span>-</span><span>1</span>])
</span></span></code></pre></div><pre tabindex="0"><code>tensor([0.0233])
</code></pre><p>Remember that for this example, when we used just the first layer’s similar strings, the approximation was quite different from the model’s prediction and had a Hellinger distance of &gt;0.63. Now it’s nearly identical and has a Hellinger distance of 0.02. So using the rest of the layers really helped this example.</p><p>In the next section, we’ll extend the code to evaluate the approximation over the whole set of 20,000 prompts. The section after that will look at how well the approximation does across all the prompts.</p><h3 id="extending-to-all-20000-prompts">Extending to All 20,000 Prompts <a href="#extending-to-all-20000-prompts">🔗</a></h3><p>We now have all the pieces we need to run the approximation procedure for all 20,000 prompts. First, let’s find the strings with similar feed-forward network outputs for all the prompts, for all blocks:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Takes about 7 minutes to run</span>
</span></span><span><span>
</span></span><span><span>similarity_thresholds<span>=</span>[<span>0.95</span>, <span>0.94</span>, <span>0.85</span>, <span>0.76</span>, <span>0.81</span>, <span>0.89</span>]
</span></span><span><span>
</span></span><span><span>similar_strings_per_block <span>=</span> []
</span></span><span><span>
</span></span><span><span><span>for</span> block_idx <span>in</span> range(n_layer):
</span></span><span><span>    similar_indices <span>=</span> filter_on_prefiltered_results(
</span></span><span><span>        load_prefiltered<span>=</span><span>lambda</span> q_idx: load_prefiltered_data(block_idx, q_idx),
</span></span><span><span>        q_idx_start<span>=</span><span>0</span>,
</span></span><span><span>        q_idx_end<span>=</span>n_prompts,
</span></span><span><span>        filter_fn<span>=</span><span>lambda</span> values: values <span>&gt;</span> similarity_thresholds[block_idx]
</span></span><span><span>    )
</span></span><span><span>    similar_strings <span>=</span> [
</span></span><span><span>        [strings10[i] <span>for</span> i <span>in</span> indices]
</span></span><span><span>        <span>for</span> indices <span>in</span> similar_indices
</span></span><span><span>    ]
</span></span><span><span>    similar_strings_per_block<span>.</span>append(similar_strings)
</span></span></code></pre></div><p>Next, we compute the frequency distributions for each query based on the strings we found, perform the weighted sum, and normalize to produce a probability distribution.</p><div><pre tabindex="0"><code data-lang="python"><span><span>freq_distribution <span>=</span> frequency_distribution_from_similar_strings(
</span></span><span><span>    similar_strings_per_block,
</span></span><span><span>    next_token_map10,
</span></span><span><span>)
</span></span><span><span>weights <span>=</span> torch<span>.</span>tensor([<span>0.01</span>, <span>0.01</span>, <span>0.1</span>, <span>1.5</span>, <span>6</span>, <span>0.01</span>])<span>.</span>unsqueeze(dim<span>=</span><span>1</span>)<span>.</span>unsqueeze(dim<span>=</span><span>2</span>) <span># (n_layer, 1, 1)</span>
</span></span><span><span>total_freq_distribution <span>=</span> (freq_distribution <span>*</span> weights)<span>.</span>sum(dim<span>=</span><span>0</span>)
</span></span><span><span>prob_distribution <span>=</span> total_freq_distribution <span>/</span> total_freq_distribution<span>.</span>sum(dim<span>=-</span><span>1</span>, keepdim<span>=</span><span>True</span>)
</span></span><span><span>prob_distribution<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([20000, 65])
</code></pre><p>The output is a tensor of shape (20000, 65): one 65-entry distribution for each of 20,000 prompts.</p><p>In order to compare, we need to run all the prompts through the model and get the output probability distributions the model predicts:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>logits, _ <span>=</span> m(tokens)
</span></span><span><span>logits <span>=</span> LogitsWrapper(logits<span>.</span>detach(), tokenizer)
</span></span><span><span>model_probs <span>=</span> logits<span>.</span>probs()
</span></span><span><span>model_probs <span>=</span> model_probs[:, <span>-</span><span>1</span>, :] <span># We're only interested in the last token</span>
</span></span></code></pre></div><p>Now we have outputs from the approximation and from the model for all prompts. In the next section, we’ll measure the Hellinger distance between them and evaluate the results.</p><h2 id="evaluating-the-approximation">Evaluating the Approximation <a href="#evaluating-the-approximation">🔗</a></h2><p>In earlier sections, we compared output from the approximation to output from the model for individual prompts. Now that we have both outputs for all prompts, we can compare them and look at aggregate results.</p><p>First, we can compute the Hellinger distance between the approximation and the model’s prediction for each prompt:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h <span>=</span> hellinger_distance(prob_distribution, model_probs)
</span></span><span><span>h<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([20000])
</code></pre><p>This produced 20,000 Hellinger distance scores, one for each prompt. We can start by looking at some basic stats:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h<span>.</span>mean(), h<span>.</span>std(), h<span>.</span>min(), h<span>.</span>max()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(0.1677), tensor(0.1215), tensor(0.0013), tensor(0.9994))
</code></pre><p>The average Hellinger distance is just below 0.17, with a standard deviation of around 0.12, suggesting a distribution that skews low (a good thing). We’ve also got at least one really excellent sample (a min of 0.0013) and at least one really terrible one (max of 0.9994).</p><p>Let’s look at the distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/e9c543696d0748c74bccfac0780e9e6a5cd7610dafc6e650fb5dab2192fc8399.png" alt=""></p><p>Indeed, the distribution is skewed left, indicating most queries have Hellinger distance scores on the lower end.</p><p>The numbers and the distribution graph look promising, but is the approximation really a good one? It’s hard to say without something to compare against and it’s not obvious what a good comparison might be.</p><p>A thought experiment: let’s imagine that for some prompt, the model produced a distribution that looked like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/d01e3755f6278cc6f19ae5656ab3ba6fd7b4ecb59c69303db814b6cc43fb0435.png" alt=""></p><p>The tokens <code>b</code> and <code>d</code> have nearly the same predicted probability (0.49 vs 0.51). The model predicts an approximately equal chance of these tokens coming next. Now imagine our approximation, or another model, predicted this distribution:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/02bddd27aedd082ab84a2b5dd45dacae4e745dc49ed363df5725916e4b370844.png" alt=""></p><p>Nearly the same, but the probabilities are reversed: <code>b</code> has probability 0.51 and <code>d</code> has 0.49. Would we care about this difference? Clearly both distributions are saying that <code>b</code> and <code>d</code> are about equally likely. If used for inference, either distribution would probably produce acceptable results. For most use cases I could imagine, the difference would just be noise.</p><p>The Hellinger distance between the two imagined distributions above is 0.0141. Not zero, but we’re saying it doesn’t matter for practical purposes. If 0.0141 is a Hellinger distance that doesn’t matter much, what about 0.02? Or 0.025? We can imagine there is some threshold Hellinger distance below which we wouldn’t care and above which we would consider distributions to be meaningfully different. What is that threshold value?</p><p>If we knew it, then we could look at how close the average Hellinger distance between our approximation’s predictions and model’s come to this threshold. That would be a measure of the goodness of the approximation.</p><p>I did an experiment to estimate what the threshold is. I trained the same transformer architecture three more times, starting with a different random seed each time and stopping at approximately the same training and validation loss as I did for the original model. This gave me three alternative transformers with roughly the same performance, but with different weights due to the different random initial starting points:</p><table><thead><tr><th>Model</th><th>Seed</th><th>Est. Training Loss</th><th>Est. Validation Loss</th></tr></thead><tbody><tr><td>Original Model</td><td>1337</td><td>0.9334</td><td>1.5063</td></tr><tr><td>Alternate 1</td><td>1442</td><td>0.9293</td><td>1.5038</td></tr><tr><td>Alternate 2</td><td>88</td><td>0.9294</td><td>1.4991</td></tr><tr><td>Alternate 3</td><td>99999</td><td>0.9339</td><td>1.4941</td></tr></tbody></table><blockquote><p>I used the same training/validation sets, hyperparameters, optimizer, etc. for the three alternate models as for the original model. The training code and output for the alternate models is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/alternate-models.ipynb" target="_blank" rel="noopener">the <code>alternate-models</code> experiment notebook</a>. Training code for the original model is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer.ipynb" target="_blank" rel="noopener">the main transformer notebook</a>.</p></blockquote><p>I then ran the same 20,000 prompts through the alternative models and calculated the Hellinger distance between their outputs and that of the original model. <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">Appendix II</a> shows the code used to do this. The table below shows the aggregate results.</p><table><thead><tr><th>Comparison</th><th>Mean Hellinger Distance</th></tr></thead><tbody><tr><td>Original vs Alternate 1</td><td>0.1064 ± 0.0823</td></tr><tr><td>Original vs Alternate 2</td><td>0.1057 ± 0.0817</td></tr><tr><td>Original vs Alternate 3</td><td>0.1053 ± 0.0828</td></tr></tbody></table><p>The original model and the three alternate models are “equivalent” in the sense that they perform about equally well in terms of training and validation loss. I could have used any of them as the basis for this post. In other words, the differences between them likely aren’t meaningful - just noise.</p><p>Across all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. We only have 3 data points, so it’s not a perfect measure, but ~0.11 is probably a reasonable lower bound for the threshold Hellinger distance we are looking for.</p><p>For comparison, the average Hellinger distance between the model and the approximation was ~0.17. A little higher than 0.11, but within a standard deviation.</p><p>Plotting the distributions of the various Hellinger distances shows this nicely:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/d82fe84ec51461c861f5fbc1c2c273935d4d8ac8ceceaeeb292d79cd5fb9ee19.png" alt=""></p><p>There is clearly less deviation between the alternates and the original model than between the approximation and the original model, but it’s not wildly different. I think this result suggests the approximation is quite good. Most of the difference is within the “acceptable noise” threshold.</p><h2 id="interpretation-why-does-the-approximation-work">Interpretation: Why Does the Approximation Work? <a href="#interpretation-why-does-the-approximation-work">🔗</a></h2><p>The analysis in the previous section shows that the outputs of the approximation are quite similar to the transformer’s outputs. But that doesn’t necessarily mean that the approximation procedure is similar to what the transformer is actually doing. The approximation and the transformer might just represent two different ways of computing the same result.</p><p>My intuition is that this is not the case: <strong>I think the approximation is at least something like what the transformer is doing</strong>. In this section, I’ll break down <em>how</em> I think the transformer computes something similar to the approximation and then present some supporting evidence.</p><p>The key ideas are:</p><ul><li>The transformer, as its name suggests*, performs a series of transformations on its embedded input. The transformer blocks transform embeddings within embedding space and the final linear layer at the end transforms from embedding space to logit space.</li><li>Within each transformer block, the transformation from input to output embedding is done via vector addition: the block’s output embedding is its input embedding plus the output of the self-attention layer, plus the output of the feed-forward network. Of the two added components, the feed-forward network output value is dominant in determining the final output.</li><li>Within embedding space, subspaces exist that correspond to specific tokens. An embedding within the subspace for a particular token produces an output distribution in which all the probability is concentrated on that token (that token has probability near 1 and all other tokens have probability near 0). Embeddings that lie between the subspaces for multiple tokens result in outputs that distribute all the probability across those tokens.</li><li>The feed-forward network output at each block is an “adjustment vector” that orients the block output towards the subspaces for the tokens that the approximation procedure would predict: those that follow the strings in the training corpus that produce similar feed-forward network outputs at that block.</li></ul><p>In the subsections below, I’ll go into each of these ideas in more detail.</p><blockquote><p>*It’s unclear whether the name “transformer” alludes to transforming an input sequence to an output sequence (the use case in the original paper was machine translation) or the transformations within the layers of the model.</p></blockquote><h3 id="the-model-is-a-series-of-transformations">The Model is a Series of Transformations <a href="#the-model-is-a-series-of-transformations">🔗</a></h3><p>Once the input to the model has been embedded, we can view the model as a series of transformations:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/c0b6c591002c7e1f931a0bcc794b88454aab02158d1125bef2f8422dbc0e8264.png" alt=""></p><p>The sequence of 6 transformer blocks takes a tensor in embedding space (\(\mathbb{R}^{384}\), since <code>n_embed=384</code>) as input and outputs another tensor in embedding space. In this sense, represents a transformation <em>within</em> embedding space. In fact, each transformer block is itself a transformation within embedding space and the stack of all 6 blocks composes these individual transformations. It isn’t literally implemented this way in code, but its equivalent to:</p><div><pre tabindex="0"><code data-lang="python"><span><span>output_embedding <span>=</span> block6(block5(block4(block3(block2(block1(input_embedding))))))
</span></span></code></pre></div><p>At the end of the sequence of blocks, the model sends the output embedding through a LayerNorm operation and then a linear layer that transforms from embedding space into logit space (\(\mathbb{R}^{65}\), since <code>vocab_size=65</code>). Finally, the softmax layer at the end turns the logits into probabilities for the next token.</p><h3 id="transformation-via-vector-addition">Transformation via Vector Addition <a href="#transformation-via-vector-addition">🔗</a></h3><p>We looked at the internal logic within a transformer block in the earlier <a href="#transformer-block-structure">Transformer Block Structure</a> section. To recap, the <code>forward()</code> method of the <code>Block</code> module looks like this:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Block</span>(nn<span>.</span>Module):
</span></span><span><span>    <span>"""One transformer block"""</span>
</span></span><span><span>
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>sa(self<span>.</span>ln1(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>        x <span>=</span> x <span>+</span> self<span>.</span>ffwd(self<span>.</span>ln2(x)) <span># The `x +` part is a skip connection</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> x
</span></span></code></pre></div><p><a id="block-logic-with-intermediates"></a>
This is equivalent to the following code, which, by using some intermediate local variables, clarifies what’s really going on:</p><div><pre tabindex="0"><code data-lang="python"><span><span>    <span>def</span> <span>forward</span>(self, x):
</span></span><span><span>        sa_out <span>=</span> self<span>.</span>sa(self<span>.</span>ln1(x))
</span></span><span><span>        ffwd_out <span>=</span> self<span>.</span>ffwd(self<span>.</span>ln2(x <span>+</span> sa_out))
</span></span><span><span>
</span></span><span><span>        <span>return</span> x <span>+</span> sa_out <span>+</span> ffwd_out
</span></span></code></pre></div><p><strong>The output of the block is equal to the input (<code>x</code>), plus the self-attention output (<code>sa_out</code>), plus the feed forward network output (<code>ffwd_out</code>).</strong> We can think of the block as taking the input embedding, and then making two adjustments to it.</p><p>These values being added together are vectors in \(\mathbb{R}^{384}\). If we imagine the embedding space reduced to just two dimensions, it might look something like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/12e04e2fdd477aaa55660b4020a4ae6b9a72c55038f98009049330cfecfc4291.png" alt=""></p><p>The red vector represents the input embedding. The green vector represents the self-attention output (<code>sa_out</code> in code), and the blue vector represents the feed-forward network output (<code>ffwd_out</code> in code). The gray arrow represent the final sum, or the output of the first block: where you end up when you arrange the individual vectors tip to tail.</p><p>The plot above shows the additions that happen within just one block. Subsequent blocks add their self-attention outputs and feed-forward network outputs, starting from the output of this block. If we add the vectors from those other blocks to the diagram, it looks like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/24b90fb31e5e330043b12b6a3b0bf9e8bed15bbd4cd57e98f0cbdc4b393fc71f.png" alt=""></p><p>Again, the red arrow represents the input vector, each green arrow represents one block’s self-attention output, each blue arrow represents one block’s feed-forward network output. Arranged tip to tail, their endpoint represents the final output from the stack of 6 blocks, depicted by the gray arrow.</p><p>Though it’s only in two dimensions, the diagram above is based on real data and is drawn “to scale”, in a way: the length of each 2D vector is the same as the \(\mathbb{R}^{384}\) vector it represents for a real query (index 57). In addition, the cosine similarity between each 2D blue / green arrow and the sum of the arrows that precede it is the same as the cosine similarity between the corresponding self-attention/feed-forward network output and the block input in the real data.</p><blockquote><p>Code to generate the 2D representation from real data is in the <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a>.</p></blockquote><p>We can observe two interesting patterns:</p><ul><li>The feed-forward network outputs are generally longer than the self-attention outputs (the vectors have larger norms)</li><li>Within a given block, the feed-forward network output and the self-attention output point in roughly the same direction.</li></ul><p>Look at what happens when we eliminate the self-attention outputs from the vector sum, leaving just the feed-forward network outputs:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/7fff50753ede8a54541e69eaf00215ea285f523817e8361d1fe08ac5e0c6cd8a.png" alt=""></p><p>The inner blue curve in the above plot represents the sum of the input vector and only the feed-forward network outputs from each block. The tip to tail arrangement of these vectors ends at a point far from where the previous arrangement (including the self-attention outputs) ended. But notice that the feed-forward-only endpoint (shorter gray arrow) is quite closely aligned in <em>direction</em> with the original endpoint (longer gray arrow).</p><p>This plot shows values for only one query and we lose a lot of information dropping from 384 dimensions to 2. But the pattern does seem to hold in general and in the full, high-dimensional embedding space. The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a> provides a deep-dive into this phenomenon across all 20,000 queries.</p><p>The takeaway is that <strong>simplifying the transformation performed by the blocks to just the contributions of the feed-forward networks results in an output vector that is shorter (has a smaller norm) than the original output but points in roughly the same direction</strong>. And the difference in norms would have no impact on the transformer’s final output, because of the LayerNorm operation after the stack of blocks. That LayerNorm step will adjust the norm of any input vector to similar value regardless of its initial magnitude; the final linear layer that follows it will always see inputs of approximately the same norm (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for a walk-through of this).</p><p>An important clarification: I’m not suggesting that we could remove the self attention computation from the transformer. The feed-forward networks take the self-attention output as part of their input (<code>ffwd_out = self.ffwd(self.ln2(x + </code><strong><code>sa_out</code></strong><code>))</code>); they would compute very different values were the self-attention outputs removed. What I am saying is that, after all block processing has been completed as normal including the self-attention computations, we get roughly the same result if we consider only the feed-forward network contributions, as our approximation does. This is probably because the feed-forward network outputs pass on some of the information they receive as input from the self-attention output.</p><p>For some additional evidence that an approximation based only on feed-forward network outputs can produce similar outputs to the transformer, see <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">Appendix IV</a>.</p><h3 id="token-subspaces">Token Subspaces <a href="#token-subspaces">🔗</a></h3><p>In the examples we’ve seen so far, the model outputs have been distributions that include significant non-zero probabilities for several tokens. For example:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a90c56ba1e72733d29a614671ad61789b5196562087c1464273da469843cb54d.png" alt=""></p><p>Though we haven’t seen one yet, we might wonder whether <strong>specific inputs</strong> exist that compel the model to predict a <strong>single token</strong> with <strong>near certainty</strong>. In other words, do some inputs cause the model to output a probability distribution in which just one token has probability very near 1 and all other tokens very near zero? Such a distribution might look like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/ecc30428196c016ef2970ee406b6ea46a79a2a24c1521d8843c2dc90f83ffc83.png" alt=""></p><p>In fact, we can ask this question about any stage of the model. “Input” doesn’t have to refer to the initial input to the model, but could be the input to any layer within the model. For example, consider only the layers that transform the final block’s output embedding to logit space (the final LayerNorm and linear layers):</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/4c4339d5f6612480cf52b0d34f2c1732c99485033c1196076543f86ec0925af5.png" alt=""></p><p>Is there some embedding block 6 might emit that would yield an output probability distribution in which some token, say the letter <code>a</code>, has probability very near 1?</p><h4 id="learning-token-subspaces">Learning Token Subspaces <a href="#learning-token-subspaces">🔗</a></h4><p>With the right math, it may be possible to find this embedding analytically. But it’s also possible to “learn” (in the sense of deep learning) such an embedding. Here’s the basic idea:</p><ul><li>Pick a point in the transformer where the input to subsequent layers is an embedding. This could be the input to any of the transformer blocks, or the point right after the final block (as shown in the diagram above).</li><li>Pick a token to learn an embedding for.</li><li>Create an embedding tensor and initialize it with random values. This tensor is the parameter the learning algorithm will optimize; the weights of the transformer are fixed.</li><li>Execute a forward pass by evaluating the transformer from the selected point, using the embedding as input. This will produce some set of logits.</li><li>Compute <a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/#nll" target="_blank" rel="noopener">negative log likelihood loss</a> relative to the token we’re learning an embedding for.</li><li>Do a backward pass, updating the embedding tensor according to the gradients.</li></ul><p>My implementation of this is in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/experiments/learn-embeddings.ipynb" target="_blank" rel="noopener">the learned embeddings notebook</a>. I used it to learn embeddings for all tokens at various stages of the model and saved them. We can load one - a learned embedding that produces a distribution giving token <code>a</code> probability almost 1 - and check that it does what we expect when given to the part of the model shown in the diagram above:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/0dce2cb6b27518fe0a1a26685995c9050cf791b48cc4e407a080182039905dca.png" alt=""></p><p>As expected, all the probability mass is concentrated on <code>a</code>. Inference using this distribution would generate <code>a</code> with near certainty.</p><p>The same procedure can learn embeddings for use at other parts of the model. If we wanted to find an embedding for <code>a</code> that could be input to block 6, we could run the same learning algorithm but use this part of the transformer in the forward pass:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/37991d03eeb45809255009149e32de771715221bb9d315efb6e2aa78b26a897c.png" alt=""></p><p>It’s more computationally expensive to learn embeddings at earlier stages of the model because the optimizer has to contend with a larger computation graph involving operations from all included blocks. Thankfully, as I’ll explain <a href="#use-only-final-subspaces">shortly</a>, we need only the embeddings learned for the part of the transformer after all the blocks (embeddings that go straight into the final LayerNorm layer) to show how the transformer operates like the approximation.</p><h4 id="from-embeddings-to-subspaces">From Embeddings to Subspaces <a href="#from-embeddings-to-subspaces">🔗</a></h4><p>For any token, the procedure described in the previous section can learn an embedding that makes the model predict that token with probability near 1. It turns out <strong>there isn’t just one such embedding for each token.</strong> We can learn many different embeddings that all produce probability distributions that assign a given token nearly all the probability mass. It was easy to learn thousands of unique embeddings for every token in the vocabulary.</p><p>I think <strong>the model has learned a complex, non-linear embedding subspace corresponding to each token</strong>. Any embedding within that subspace results in an output distribution that assigns the token near certain probability. Each embedding I was able to learn is probably a point in the embedding subspace for the corresponding token.</p><p>If we imagine the full embedding space (\(\mathbb{R}^{384}\)) reduced to \(\mathbb{R}^3\) (and the complex subspaces reduced to 2D planes), it might look something like this:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/04a243b63386cc0e853d350cb0177eeb33f2c15d2aacdaf59ddb4dc38f48b444.png" alt=""></p><p>I don’t know how to determine the exact subspaces for each token mathematically. But I do know how to get a workable approximation of them <strong>if we’re willing to pretend that they are linear</strong>. They are almost certainly not linear, even at the end of the model, because of the non-linear LayerNorm operation. But they are likely <em>closer</em> to linear near the end of the model because the LayerNorm is the only non-linearity. Earlier in the model, each feed-forward network introduces an additional non-linearity via its ReLU operation.</p><blockquote><p><a href="https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm" target="_blank" rel="noopener">This post on LessWrong</a> illustrates of the non-linearity of LayerNorm clearly.</p></blockquote><p>Pretending the subspaces are linear actually works quite well for the part of the model after the transformer blocks. And that is the only part of the model we need to consider for this analysis (as I’ll explain <a href="#use-only-final-subspaces">soon</a>).</p><h4 id="linear-approximations-for-subspaces">Linear Approximations for Subspaces <a href="#linear-approximations-for-subspaces">🔗</a></h4><p>The idea is quite simple: for a given token, we can learn a whole lot of different embeddings, treating each one as a data point. Then we can determine the best fitting line, plane, or other low-dimensional linear subspace that fits the data.</p><p>Again, if we imagine our embedding space reduced to just 3 dimensions, it might look something like the following diagram. The blue dots each represent a learned embedding and the red arrow is the line that minimizes projected distance from each point.</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/a32cb4b311513c8c1bbb0af5cae1d68b1e96efeddfbde076f8e9fca02772d605.png" alt=""></p><p>We can use Singular Value Decomposition (SVD) to find the best fitting linear subspace for the learned embeddings.</p><blockquote><p>To learn more about singular value decomposition in this context, I recommend reading Jeremy Kun’s excellent two-part post. <a href="https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/" target="_blank" rel="noopener">Part 1</a> <a href="https://jeremykun.com/2016/05/16/singular-value-decomposition-part-2-theorem-proof-algorithm/" target="_blank" rel="noopener">Part 2</a>.</p></blockquote><p><a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">Appendix V</a> walks through the code that uses SVD to find a linear approximation for the subspace corresponding to one token. I did this for all tokens, using the embeddings I learned for the final stages of the transformer. In every case, I was able to find a single vector (1-D subspace) that approximates the token subspace quite well.</p><blockquote><p>For completeness, I also tried this at earlier stages of the transformer and found, as expected, that the linear approximations, even at higher dimensions, didn’t fit the data as well. The relevant experiments are in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a></p></blockquote><h4 id="mixing-subspace-approximations">Mixing Subspace Approximations <a href="#mixing-subspace-approximations">🔗</a></h4><p>By learning a large number of embeddings for each token and then using SVD on them, we can find one vector for each token that approximates its subspace. Given one of these vectors, any embedding that falls on its span will produce an output distribution that concentrates all the probability mass on the corresponding token. But many of the real transformer outputs we’ve seen distribute the probability mass across several tokens. How do we get from subspaces for individual tokens to embeddings that produce these more diverse distributions?</p><p>We can create embeddings that produce probability distributions where several tokens have substantial probability via linear combinations of the subspace approximation vectors for those tokens. This is the distribution we get when we create an embedding by simply adding the approximation vectors for the subspaces for <code>a</code> and <code>b</code>:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/211fa3aed3f5cbbe1a5adf41e014e27068c39ffaf21aff36c833a851b500e211.png" alt=""></p><p>The sum of subspace approximations vectors for two tokens is an embedding somewhere in between the two subspaces, which results in a final distribution that is the combination of the two tokens.</p><p>Sadly, adding the approximation vectors for <code>a</code> and <code>b</code>, without weighting either one, results in not quite a 50-50 distribution across the two tokens (as shown above). I think there are three reasons for this:</p><ol><li>The approximation vectors are just approximations and not perfect representations of their subspaces.</li><li>The subspace approximation vectors are not perfectly orthogonal. To the extent that <code>a</code>’s vector has a small component that points in the direction of <code>b</code>, the sum results in an overweighting of <code>b</code>.</li><li>The final linear layer of the model produces logits of different magnitudes for different tokens. For example, given the approximation for <code>a</code>, the logit for <code>a</code> is ~18.2. The logit for <code>b</code> from its approximation is ~19.5.</li></ol><p>Together, these errors accumulate and the softmax function at the very end exaggerates even small differences. For more analysis on the reasoning behind the differences and how they might be compensated for, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/60_combining_token_subspaces.ipynb" target="_blank" rel="noopener">the combining token subspaces notebook</a>.</p><p>These imperfections aside, I think we can conclude that <strong>it’s possible to derive an embedding that produces a distribution for multiple tokens via a linear combination of the approximation vectors for those tokens’ subspaces</strong>.</p><h3 id="putting-it-all-together">Putting it All Together <a href="#putting-it-all-together">🔗</a></h3><p>To summarize where we are, the preceding sections have shown:</p><ul><li>The transformer blocks perform a series of transformations in embedding space.</li><li>Those transformations can be thought of as moving from one point in embedding space to another by adding the feed-forward network output vector to the input embedding.</li><li>Embedding space contains subspaces corresponding to predicting particular tokens and embeddings between subspaces for multiple tokens result in predictions including all those tokens.</li></ul><p>This section adds the final piece, which is the correspondence between what the transformer is doing and what the approximation is doing:</p><ul><li>Within a block, adding the feed-forward network output vector to the input produces an output embedding that better aligns with the embedding subspaces of specific tokens. And <strong>those tokens are the same ones predicted in the approximation</strong>: they’re the tokens that follow the strings in the training corpus that yield similar feed-forward network outputs to the current prompt.</li></ul><p>Let’s look at an example that shows this. The following is the output distribution predicted by the approximation for the prompt, <code>med me Aut</code> (query index 33), using only the feed-forward network outputs from the final block:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/0a46316fd1ec97000bed4b44242e7aad0809b16acd65a98507ff1b987e313291.png" alt=""></p><p>Based on the strings in the training corpus with similar feed-forward network outputs at the final block, the approximation predicts <code>o</code> is the most likely next token and <code>h</code> is next.</p><p>Next, we need to look at the feed-forward network output for the prompt in this block and determine which token subspaces it’s most oriented towards. I’m going to show a little code here, because I think it’s the best way to explain what’s going on. Readers who aren’t interested in the implementation can focus only on the output.</p><p>First we need to actually grab the feed-forward outputs (we haven’t needed them so far because we’ve been working with precomputed/prefiltered similarity data). We’ll use some <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/models/transformer-helpers.ipynb" target="_blank" rel="noopener">helper functions</a> that provide easy access to the transformer’s intermediate representations:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Tokenize the strings</span>
</span></span><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>
</span></span><span><span><span># Embed the tokens</span>
</span></span><span><span>embeddings <span>=</span> accessors<span>.</span>embed_tokens(tokens)
</span></span><span><span>
</span></span><span><span><span># Instantiate TransformerAccessors</span>
</span></span><span><span>accessors <span>=</span> TransformerAccessors(m, device)
</span></span><span><span>
</span></span><span><span><span># Run them through the model with hooks attached that let us look at</span>
</span></span><span><span><span># intermediate values</span>
</span></span><span><span>_, io_accessors <span>=</span> accessors<span>.</span>run_model(embeddings)
</span></span><span><span>
</span></span><span><span><span># Grab the outputs of the ffwd networks at each layer</span>
</span></span><span><span>ffwd_outs <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    io_accessors[block_idx]<span>.</span>output(<span>'ffwd'</span>)[:, <span>-</span><span>1</span>, :]<span>.</span>clone()
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>])
</span></span><span><span>
</span></span><span><span><span># Free up some memory</span>
</span></span><span><span><span>del</span> io_accessors
</span></span><span><span>_ <span>=</span> gc<span>.</span>collect()
</span></span><span><span>
</span></span><span><span>ffwd_outs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([6, 20000, 384])
</code></pre><p>To determine which token subspaces the feed-forward network output aligns with, we’ll project it onto the subspace approximation for each token, then determine which projections are most similar to the original vector. To do this, we’ll need to get the projection matrix for the rank 1 approximation to each token subspace:</p><blockquote><p>The code below uses the <code>projection_matrix_for_rank_k_approximation()</code> helper function, defined in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/common/svd-helpers.ipynb" target="_blank" rel="noopener">the SVD helpers notebook</a>.</p></blockquote><blockquote><p>In the case of a rank 1 approximation, the projection isn’t really necessary. We could just take the cosine similarity with the approximation vector, but I wanted to keep this code general because I tried out higher-dimensional approximations in <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">other places</a>.</p></blockquote><div><pre tabindex="0"><code data-lang="python"><span><span>filename_for_token <span>=</span> FilenameForToken(tokenizer)
</span></span><span><span>subspace_dims <span>=</span> <span>1</span>
</span></span><span><span>projection_matrices <span>=</span> torch<span>.</span>stack([
</span></span><span><span>    projection_matrix_for_rank_k_approximation(
</span></span><span><span>        original_matrix<span>=</span>torch<span>.</span>load(
</span></span><span><span>            learned_embeddings_dir <span>/</span>  <span>'no_blocks'</span> <span>/</span> <span>f</span><span>"</span><span>{</span>filename_for_token(token)<span>}</span><span>.pt"</span>,
</span></span><span><span>            map_location<span>=</span>device,
</span></span><span><span>        )[:, <span>0</span>, :],
</span></span><span><span>        k<span>=</span>subspace_dims,
</span></span><span><span>    )
</span></span><span><span>    <span>for</span> token <span>in</span> tokenizer<span>.</span>chars
</span></span><span><span>])
</span></span></code></pre></div><p>Now we’ll perform the projections and find the top 5 most similar ones to the original feed-forward output vector:</p><div><pre tabindex="0"><code data-lang="python"><span><span>projections <span>=</span> projection_matrices <span>@</span> ffwd_outs[block_idx, q_idx, :]
</span></span><span><span>values, indices <span>=</span> torch<span>.</span>topk(
</span></span><span><span>    F<span>.</span>cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span>=-</span><span>1</span>),
</span></span><span><span>    k<span>=</span><span>5</span>,
</span></span><span><span>    dim<span>=</span><span>0</span>,
</span></span><span><span>)
</span></span><span><span>tokens <span>=</span> [tokenizer<span>.</span>chars[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span><span><span>list(zip(tokens, values<span>.</span>tolist()))
</span></span></code></pre></div><pre tabindex="0"><code>[('o', 0.5074884295463562),
 ('h', 0.40787822008132935),
 ('i', 0.26926180720329285),
 ('u', 0.22823508083820343),
 ('y', 0.20325089991092682)]
</code></pre><p>It turns out that <code>o</code> and <code>h</code> are the most similar, indicating that the feed-forward network output is most oriented towards the subspaces for these tokens. And these are the same tokens that the approximation predicted from the strings with similar feed-forward network outputs (see the distribution above).</p><p>Another example, this time looking at query index 36 (<code>if and thy</code>), but staying in the final block:</p><p><img src="https://shyam.blog/posts/beyond-self-attention/images/fb2e608eb48c085de00f28831641ff4d49f7f3fd63b198fc3fc08d02a3cc7c45.png" alt=""></p><div><pre tabindex="0"><code data-lang="python"><span><span>projections <span>=</span> projection_matrices <span>@</span> ffwd_outs[block_idx, q_idx, :]
</span></span><span><span>values, indices <span>=</span> torch<span>.</span>topk(
</span></span><span><span>    F<span>.</span>cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim<span>=-</span><span>1</span>),
</span></span><span><span>    k<span>=</span><span>5</span>,
</span></span><span><span>    dim<span>=</span><span>0</span>,
</span></span><span><span>)
</span></span><span><span>tokens <span>=</span> [tokenizer<span>.</span>chars[i<span>.</span>item()] <span>for</span> i <span>in</span> indices]
</span></span><span><span>list(zip(tokens, values<span>.</span>tolist()))
</span></span></code></pre></div><pre tabindex="0"><code>[(' ', 0.5869003534317017),
 ('s', 0.47689366340637207),
 ('\n', 0.38412901759147644),
 ('$', 0.23048195242881775),
 ('a', 0.21783535182476044)]
</code></pre><p>Here <code></code>(space), <code>s</code>, and <code>\n</code> (newline) were the tokens predicted from what follows the strings with similar feed-forward outputs, and indeed these are the token subspaces most aligned with the prompt’s feed-forward output.</p><h4 id="aggregate-performance">Aggregate Performance <a href="#aggregate-performance">🔗</a></h4><p>In the previous section, I purposely picked examples that exhibit strong correlation between the approximation’s predictions and the most aligned subspaces, to illustrate the point most clearly. Of course, there are other examples for which the correlation is less strong. Rather than looking at specific cases, let’s try to get a sense of how well the correlation holds up across all 20,000 prompts.</p><p>This immediately leads to a question: what is the right measure of aggregate performance? Unfortunately, even if the hypothesis - that the prompt’s feed-forward output aligns with the subspaces for tokens predicted from the strings with similar feed forward outputs - is true, a few practical issues make it difficult to demonstrate objectively:</p><ul><li>We don’t have exact definitions of the token subspaces, just imperfect, linear approximations.</li><li>Magnitudes don’t line up: the tokens with the most probability mass in the approximation’s predictions don’t always correspond to the subspaces with the greatest cosine similarity (because of the imperfect approximations, because the adjustment required may be bigger or smaller for some tokens vs others based on the input embedding’s current alignment, because, as explained in the <a href="#mixing-subspace-approximations">Mixing Subspace Approximations</a> section, the model is more “sensitive” to some tokens than others).</li></ul><p>Given these impediments, we can’t just do something simple like normalizing the cosine similarities and computing Hellinger distance with the predicted probability distribution.</p><p>Instead, we need to devise a criterion on which to judge whether the data from a particular prompt supports the hypothesis or not. Then we can evaluate aggregate performance by how many of the 20,000 prompts satisfy the criterion. I experimented with several different approaches and in the end came up with this candidate criterion:</p><p>High-level description: <em>Do the subspaces for the tokens containing 90% of the probability mass in the approximation’s predictions appear in the top half of all token subspaces when ranked by cosine similarity with the prompt’s feed-forward output vector?</em></p><p>Exact definition:</p><ul><li>Define <code>top_n</code> as the number of tokens required to cover at least 90% of the probability mass in the approximation’s predictions for this prompt.</li><li>Define <code>n_subspaces</code> as <code>tokenizer.vocab_size // 2</code> (32, based on our 65-token vocabulary).</li><li>Determine: Are the subspaces for the first <code>top_n</code> tokens predicted by the approximation in the first <code>n_subspaces</code> subspaces ranked by cosine similarity with the prompt’s feed-forward output vector?</li></ul><p>Admittedly, this is an arbitrary definition and reasonable people could debate any of the specifics. But I do think it gives as an indication of whether the data from a particular example prompt supports the hypothesis, while allowing for some of the measurement challenges noted above.</p><p>I evaluated this criteria at three places: the outputs of blocks 6, 5, and 4, using projection matrices derived from learned embeddings at each of these places.</p><blockquote><p>I didn’t evaluate at earlier blocks because the GPU time required to learn embeddings at those blocks became prohibitive. The further back in the model, the bigger the computation graph that the learning algorithm needs to optimize over.</p></blockquote><p>The table below shows the results:</p><blockquote><p>The code that produced these results appears at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a></p></blockquote><table><thead><tr><th>Block</th><th># of Prompts Satisfying Criterion</th></tr></thead><tbody><tr><td>6</td><td>16357 (81.78%)</td></tr><tr><td>5</td><td>10142 (50.71%)</td></tr><tr><td>4</td><td>7760 (38.80%)</td></tr></tbody></table><p>These numbers aren’t exactly a ringing endorsement. As expected, they get worse the further back we go, probably due to the increased non-linearity.</p><p><a id="use-only-final-subspaces"></a>
What if we always used the subspace approximations from the very end of the transformer (which are likely to be the most linear), even when comparing against feed-forward network outputs from earlier blocks? The results get better:</p><table><thead><tr><th>Block</th><th># of Prompts Satisfying Criterion</th></tr></thead><tbody><tr><td>6</td><td>16357 (81.78%)</td></tr><tr><td>5</td><td>13652 (68.26%)</td></tr><tr><td>4</td><td>11630 (58.15%)</td></tr><tr><td>3</td><td>11469 (57.34%)</td></tr><tr><td>2</td><td>10404 (52.02%)</td></tr><tr><td>1</td><td>9942 (49.71%)</td></tr></tbody></table><blockquote><p>Like many good findings, this one resulted from a bug. I accidentally ran the analysis using the projection matrices for the final part of the transformer with the feed-forward network outputs from earlier blocks and was surprised when the numbers turned out to be so good.</p></blockquote><p>It’s valid to use the subspace approximations (and corresponding projection matrices) from the end of the transformer at earlier stages. All blocks operate in the same embedding space and each one seems to make a small refinement on the output of its predecessors, rather than wild changes in direction. So if any block’s feed-forward network output adjusts an embedding towards the subspaces for a set of tokens as defined at the end of the transformer, it is likely also adjusting it towards whatever the subspaces would be for those same tokens at the block where it operates.</p><blockquote><p>The <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank" rel="noopener">logit lens post</a>, in the section “why? / is this surprising?” provides an explanation that supports this idea. In summary, the residual connections encourage the transformer to learn weights that operate within the same basis across blocks and the use of weight decay in training results in a computation that’s spread out over as many layers as possible, with each layer making only a small, incremental change.</p></blockquote><p>To put these numbers in perspective, I investigated how likely it would be for the criterion I’ve defined here to be satisfied by chance. In other words, if we assume the hypothesis is false and that the cosine similarities between the feed-forward network output and the token subspace approximation vectors are random, rather than expressing meaningful relationships, how likely would it be for the criterion to still be satisfied?</p><p>I ran a simulation of this, taking care to ensure that the distribution of randomly generated cosine similarities matches the real data, among other details. The implementation is at the end of <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/50_approximation_details.ipynb" target="_blank" rel="noopener">the approximation details notebook</a>. The final results are:</p><table><thead><tr><th>Block</th><th>Likely % of Prompts Satisfying Criteria By Chance</th></tr></thead><tbody><tr><td>6</td><td>20.76% ± 0.25%</td></tr><tr><td>5</td><td>20.55% ± 0.26%</td></tr><tr><td>4</td><td>18.37% ± 0.24%</td></tr><tr><td>3</td><td>18.20% ± 0.24%</td></tr><tr><td>2</td><td>17.04% ± 0.23%</td></tr><tr><td>1</td><td>16.31% ± 0.23%</td></tr></tbody></table><p>So the best performance numbers we have are clearly much better than chance. But in fairness, they’re still not a slam dunk.</p><p>Even when we use approximations for the most linear subspaces we have, I think there is still a lot of noise in the measurement, for all the reasons outlined earlier in this section. Personally, I take the numbers to be overall supportive of the hypothesis, at least directionally, though I wish the evidence was more conclusive.</p><h3 id="final-summary-of-correspondence-between-transformer-and-approximation">Final Summary of Correspondence Between Transformer and Approximation <a href="#final-summary-of-correspondence-between-transformer-and-approximation">🔗</a></h3><p>The analyses in this post point towards two ideas. First, that the approximation and the transformer produce similar outputs. Second, that there is a correspondence between the approximation procedure and what the transformer is doing. I think the evidence for the first idea is quite strong. The evidence for the second is less clear cut, but still suggests it’s probably at least partially right.</p><p>To close, I want to provide a high-level summary of what I think that correspondence is, even if I can’t yet demonstrate it more definitively:</p><table><thead><tr><th>Concept</th><th>Transformer</th><th>Approximation</th></tr></thead><tbody><tr><td>Prompts map to classes of strings in the training corpus.</td><td>The transformer learns an embedding scheme, along with weights in its self-attention and feed-forward networks, that cause strings in the training corpus with similar characteristics to produce similar output values. Prompts that share those characteristics also produce similar output values.</td><td>The approximation performs the same mapping as the transformer by examining the feed-forward network outputs from all substrings in the training corpus and identifying the ones similar to the outputs from a given prompt.</td></tr><tr><td>Predictions for the tokens likely to follow a prompt derive from the frequency distribution of tokens that follow strings in the training corpus that produce feed-forward network output values similar to those of the prompt.</td><td>A feed-forward network output is a compressed, latent representation, in the embedding space, of the frequency distribution of the tokens that follow strings in the training corpus that produce similar outputs. The weights in the final linear layer map the latent representation into logit space such that it become the correct probability distribution after applying the softmax operation.</td><td>The approximation reconstructs the same frequency distribution manually, by looking up the strings identified as having similar outputs in the training corpus and counting the tokens that follow them. Normalizing the frequency distribution turns it into a probability distribution.</td></tr><tr><td>Final output is a weighted sum of predictions from each block.</td><td>As shown <a href="#transformation-via-vector-addition">earlier</a>, the transformer output is roughly the vector sum of all feed-forward network outputs and the input embedding. The learned weights in the layers within a block determine the magnitude and direction of the output and thus how much it influences the overall direction of the final sum.</td><td>The approximation performs a weighted sum of the distributions determined for each block. The weights control the degree of influence of any given block and are manually selected to produce results as close to the transformer’s as possible.</td></tr></tbody></table><h3 id="what-about-attention">What About Attention? <a href="#what-about-attention">🔗</a></h3><p>I began this post by observing that most explanations of how transformers work focus on attention but don’t say how attention results turn into the final predictions. I may be guilty of the opposite: I’ve written at length about how the transformers produce their output probabilities and said very little about attention.</p><p>To wrap up the analysis, I’d like to rectify this with a few words about attention. In the mechanism I’ve laid out, whether executed in the form of the approximation or the transformer, a key operation is mapping the prompt to a class of strings from the training corpus at each block. Predictions for the next token follow directly from the distribution of tokens that follow those strings in the training corpus. <strong>Making good predictions depends on mapping the prompt to the right class of training corpus strings. And that is the job of self-attention.</strong></p><p>The self-attention layers learn to identify patterns across the tokens that make up a prompt. Those patterns might be simple, such as a common sequence appearing at the beginning or end of the prompt (for example, as we saw <a href="#demo-my-proposal-in-action">earlier</a>, strings that end in ‘y l’). They can also be more general: instead of matching specific tokens, they might match <em>kinds</em> of tokens, such as vowels or capitals, in specific places. The learned weights in the attention heads determine which patterns they respond to, and thus which strings in the training corpus produce similar values. The output of the self-attention heads, when passed through the feed-forward network, yield representations in embedding space that encode information about the distribution of tokens in the training corpus that follow those strings.</p><p>Because the transformer has multiple blocks and each block has multiple attention heads (6 blocks and 6 heads per block in the one we looked at), it’s possible to evaluate each prompt against a large number of different potential patterns. The richness and diversity of the patterns that the attention heads can identify gives the transformer its predictive power.</p><h2 id="closing-thoughts">Closing Thoughts <a href="#closing-thoughts">🔗</a></h2><p>I started this project because I wanted to understand the transformer architecture. It’s given me a satisfying explanation of what at least one transformer is doing, but has been even more fruitful as an exercise in learning how to learn. This was my first foray into an open-ended ML research project on my own. It taught me how to interrogate the internals of models, how to set up experiments to answer questions, and, perhaps most importantly, how to keep moving the project forward when I felt stuck.</p><p>Language models have always seemed magical to me, from the first time I used ChatGPT. I wondered if finding a reductive explanation for what happens internally would rob them of their magic. In fact, I think the opposite has happened. I’ve come to appreciate the beauty in an elegantly simple mechanism that produces such rich complexity in its outputs.</p><p>I don’t know whether the results I found here have any generality beyond the small transformer I trained or if any of it will be of use to anyone else. Regardless, it’s been a joy to do this work and I’m grateful to have had the things I needed along the way: time, resources, and endless support from my family and mentors.</p><h2 id="appendices">Appendices <a href="#appendices">🔗</a></h2><h3 id="i-model-details">I: Model Details <a href="#i-model-details">🔗</a></h3><p>Some notable specs:</p><ul><li>Vocabulary size: 65 (the unique characters in the TinyShakespeare dataset)</li><li>Embedding size (<code>n_embed</code>): 384</li><li>Number of transformer blocks (<code>n_layer</code>): 6</li><li>Number of attention heads (<code>n_head</code>): 6</li><li>Context window size (<code>block_size</code>): 256</li></ul><p>The feed-forward networks comprise over 65% of the total trainable parameters:</p><div><pre tabindex="0"><code data-lang="python"><span><span>all_trainable_params <span>=</span> [p <span>for</span> p <span>in</span> m<span>.</span>parameters() <span>if</span> p<span>.</span>requires_grad]
</span></span><span><span>n_all_trainable_params <span>=</span> sum([np<span>.</span>prod(p<span>.</span>size()) <span>for</span> p <span>in</span> all_trainable_params])
</span></span><span><span>
</span></span><span><span>ffwd_trainable_params <span>=</span> [
</span></span><span><span>    p
</span></span><span><span>    <span>for</span> block_idx <span>in</span> range(n_layer)
</span></span><span><span>    <span>for</span> p <span>in</span> m<span>.</span>blocks[block_idx]<span>.</span>ffwd<span>.</span>parameters()
</span></span><span><span>    <span>if</span> p<span>.</span>requires_grad
</span></span><span><span>]
</span></span><span><span>n_ffwd_trainable_params <span>=</span> sum([np<span>.</span>prod(p<span>.</span>size()) <span>for</span> p <span>in</span> ffwd_trainable_params])
</span></span><span><span>
</span></span><span><span>print(
</span></span><span><span>    <span>f</span><span>"</span><span>{</span>n_ffwd_trainable_params<span>:</span><span>,</span><span>}</span><span> ffwd params out of </span><span>{</span>n_all_trainable_params<span>:</span><span>,</span><span>}</span><span> total params (</span><span>{</span>n_ffwd_trainable_params <span>/</span> n_all_trainable_params<span>:</span><span>.2%</span><span>}</span><span>)"</span>
</span></span><span><span>)
</span></span></code></pre></div><pre tabindex="0"><code>7,089,408 ffwd params out of 10,788,929 total params (65.71%)
</code></pre><h3 id="ii-evaluation-of-main-model-vs-3-alternate-models">II: Evaluation of Main Model vs 3 Alternate Models <a href="#ii-evaluation-of-main-model-vs-3-alternate-models">🔗</a></h3><p>As described in the <a href="#evaluating-the-approximation">Evaluation section</a>, I trained the same transformer architecture used for the main model in this post three additional times, starting from a different random seed each time. This appendix shows the code used to measure the average Hellinger distance between the output of the main models and each of the three alternates, across the 20,000 sample prompts. The results provide a plausible lower bound for the threshold Hellinger distance that indicates a meaningful change in an output probability distribution.</p><p>First, we instantiate the three alternate models from their saved weights:</p><div><pre tabindex="0"><code data-lang="python"><span><span>alt_models_dir <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'alternate-models/model-training/20240112-training/outputs/'</span>
</span></span><span><span><span>assert</span> alt_models_dir<span>.</span>exists(), <span>"Alternate models directory does not exist. Run the training code in ../experiments/alternate-models.ipynb."</span>
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span># Instantiate the three alternative trained models</span>
</span></span><span><span>m_alt1, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-1.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span><span><span>m_alt2, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-2.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span><span><span>m_alt3, _ <span>=</span> create_model_and_tokenizer(
</span></span><span><span>    saved_model_filename<span>=</span>alt_models_dir <span>/</span> <span>'shakespeare-20240112-3.pt'</span>,
</span></span><span><span>    dataset<span>=</span>ts,
</span></span><span><span>    device<span>=</span>device,
</span></span><span><span>)
</span></span></code></pre></div><p>Next, we feed the input tokens into the original model and the three alternate models and get their output probability distributions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>tokens <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts)
</span></span><span><span>
</span></span><span><span>model_probs <span>=</span> get_model_probs(m, tokens)
</span></span><span><span>alt_model_probs1 <span>=</span> get_model_probs(m_alt1, tokens)
</span></span><span><span>alt_model_probs2 <span>=</span> get_model_probs(m_alt2, tokens)
</span></span><span><span>alt_model_probs3 <span>=</span> get_model_probs(m_alt3, tokens)
</span></span></code></pre></div><p>Now we can compute the Hellinger distance between the outputs from the three alternative models and the outputs from the original model. Remember that each of the model probabilities tensors (<code>model_probs</code> and <code>alt_model_probs*</code>) is a 20,000x65 tensor i.e. 20,000 probability distributions of 65 elements each.</p><p>We’re computing the Hellinger distance between those probability distributions. So for each alternative model, we end up with 20,000 Hellinger distance values. These values tell us, for each prompt, how the probability distribution for the next token predicted by one of the alternate models differed from the probability distribution predicted by the original model.</p><div><pre tabindex="0"><code data-lang="python"><span><span>h_alt1 <span>=</span> hellinger_distance(model_probs, alt_model_probs1)
</span></span><span><span>h_alt2 <span>=</span> hellinger_distance(model_probs, alt_model_probs2)
</span></span><span><span>h_alt3 <span>=</span> hellinger_distance(model_probs, alt_model_probs3)
</span></span></code></pre></div><p>With the Hellinger distances computed, we can look at aggregate stats:</p><div><pre tabindex="0"><code data-lang="python"><span><span>h_alts <span>=</span> torch<span>.</span>stack([h_alt1, h_alt2, h_alt3], dim<span>=</span><span>1</span>)
</span></span><span><span>h_alts<span>.</span>mean(dim<span>=</span><span>0</span>), h_alts<span>.</span>std(dim<span>=</span><span>0</span>), h_alts<span>.</span>min(dim<span>=</span><span>0</span>)<span>.</span>values, h_alts<span>.</span>max(dim<span>=</span><span>0</span>)<span>.</span>values
</span></span></code></pre></div><pre tabindex="0"><code>(tensor([0.1064, 0.1057, 0.1053]),
 tensor([0.0823, 0.0817, 0.0828]),
 tensor([0.0005, 0.0008, 0.0008]),
 tensor([0.8351, 0.7881, 0.8743]))
</code></pre><p>For all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. All had very small minimums (&lt;= 0.0008) and maximums around ~0.80.</p><h3 id="iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">III: The Output Embedding’s Norm Doesn’t Matter Because of the Final LayerNorm <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">🔗</a></h3><p>This appendix demonstrates the assertion from the <a href="#transformation-via-vector-addition">Transformation via Vector Addition</a> that the norm of the output embeddings from the final transformer block does not matter, because of the LayerNorm before the final linear layer.</p><p>To begin, let’s grab the final block outputs for the first 1000 prompts:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># Get the block outputs for the first 1000 prompts</span>
</span></span><span><span>
</span></span><span><span>tokens_sample <span>=</span> encoding_helpers<span>.</span>tokenize_strings(prompts[:<span>1000</span>])
</span></span><span><span>_, io_accessors_sample <span>=</span> accessors<span>.</span>run_model(accessors<span>.</span>embed_tokens(tokens_sample))
</span></span><span><span>
</span></span><span><span>final_block_outputs <span>=</span> io_accessors_sample[<span>-</span><span>1</span>]<span>.</span>output(<span>'.'</span>)[:, <span>-</span><span>1</span>, :]<span>.</span>clone()
</span></span><span><span>
</span></span><span><span><span>del</span> io_accessors_sample
</span></span><span><span>_ <span>=</span> gc<span>.</span>collect()
</span></span><span><span>
</span></span><span><span>final_block_outputs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([1000, 384])
</code></pre><p>Next, let’s create a copy of those outputs scaled by a factor of 10:</p><div><pre tabindex="0"><code data-lang="python"><span><span>scaled_final_block_outputs <span>=</span> final_block_outputs <span>*</span> <span>10</span>
</span></span><span><span>scaled_final_block_outputs<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([1000, 384])
</code></pre><p>Comparing average norms, we see that those of the scaled outputs indeed are 10 times bigger:</p><div><pre tabindex="0"><code data-lang="python"><span><span>final_block_outputs<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean(), scaled_final_block_outputs<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(22.8909), tensor(228.9091))
</code></pre><p>Now, let’s put both the original and scaled outputs through the final LayerNorm of the model and calculate the average norm of the results:</p><div><pre tabindex="0"><code data-lang="python"><span><span>layer_normed_original <span>=</span> m<span>.</span>ln_f(final_block_outputs)<span>.</span>detach()
</span></span><span><span>layer_normed_scaled <span>=</span> m<span>.</span>ln_f(scaled_final_block_outputs)<span>.</span>detach()
</span></span><span><span>
</span></span><span><span>layer_normed_original<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean(), layer_normed_scaled<span>.</span>norm(dim<span>=-</span><span>1</span>)<span>.</span>mean()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1262), tensor(23.1263))
</code></pre><p>They’re virtually identical.</p><p>In the preceding example, the output norms were so close to identical because the two inputs differed only in scale: they had the same direction, or cosine similarity of 1. Vectors that have different norms and different directions will emerge from the LayerNorm with norms that are still quite similar but a little further apart.</p><p>To see an example, we can add a little noise to one of the vectors and then scale it:</p><div><pre tabindex="0"><code data-lang="python"><span><span>original_vector <span>=</span> final_block_outputs[<span>0</span>]
</span></span><span><span>
</span></span><span><span><span># Add some random noise to the original vector</span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>42</span>) <span># keep the noise consistent</span>
</span></span><span><span>comparison_vector <span>=</span> original_vector <span>+</span> torch<span>.</span>randn_like(original_vector) <span>*</span> <span>0.1</span>
</span></span><span><span>
</span></span><span><span><span># And scale it</span>
</span></span><span><span>comparison_vector <span>=</span> comparison_vector <span>/</span> comparison_vector<span>.</span>norm()
</span></span><span><span>comparison_vector <span>*=</span> <span>10</span> <span>*</span> original_vector<span>.</span>norm()
</span></span><span><span>
</span></span><span><span>original_vector<span>.</span>norm(), comparison_vector<span>.</span>norm(), F<span>.</span>cosine_similarity(original_vector, comparison_vector, dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.7909), tensor(237.9092), tensor(0.9967))
</code></pre><p>The <code>comparison_vector</code>’s norm is exactly 10x that of <code>original_vector</code>, but they’re not perfectly aligned in direction, though still quite close.</p><div><pre tabindex="0"><code data-lang="python"><span><span>m<span>.</span>ln_f(original_vector)<span>.</span>detach()<span>.</span>norm(), m<span>.</span>ln_f(comparison_vector)<span>.</span>detach()<span>.</span>norm()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1496), tensor(23.1671))
</code></pre><p>Their norms after layer norm are close but further apart than in the previous example.</p><p>If we add a lot more noise, we’ll end up with two vectors with quite different directions:</p><div><pre tabindex="0"><code data-lang="python"><span><span>original_vector <span>=</span> final_block_outputs[<span>0</span>]
</span></span><span><span>
</span></span><span><span><span># Add some random noise to the original vector</span>
</span></span><span><span>torch<span>.</span>manual_seed(<span>4211</span>) <span># keep the noise consistent</span>
</span></span><span><span>comparison_vector <span>=</span> original_vector <span>+</span> torch<span>.</span>randn_like(original_vector) <span>*</span> <span>2</span>
</span></span><span><span>
</span></span><span><span><span># And scale it</span>
</span></span><span><span>comparison_vector <span>=</span> comparison_vector <span>/</span> comparison_vector<span>.</span>norm()
</span></span><span><span>comparison_vector <span>*=</span> <span>10</span> <span>*</span> original_vector<span>.</span>norm()
</span></span><span><span>
</span></span><span><span>original_vector<span>.</span>norm(), comparison_vector<span>.</span>norm(), F<span>.</span>cosine_similarity(original_vector, comparison_vector, dim<span>=-</span><span>1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.7909), tensor(237.9093), tensor(0.5178))
</code></pre><p>But their norms after layer norm are only a little more divergent:</p><div><pre tabindex="0"><code data-lang="python"><span><span>m<span>.</span>ln_f(original_vector)<span>.</span>detach()<span>.</span>norm(), m<span>.</span>ln_f(comparison_vector)<span>.</span>detach()<span>.</span>norm()
</span></span></code></pre></div><pre tabindex="0"><code>(tensor(23.1496), tensor(23.0546))
</code></pre><p>So in summary:</p><ul><li>The LayerNorm will remove substantial differences in input norms.</li><li>Norms of the outputs from the LayerNorm will vary a little depending on how closely aligned the input vectors were.</li></ul><h3 id="iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">IV: Summary of Experiment on Relative Impact of Self-Attention and Feed Forward Network Outputs <a href="#iv-summary-of-experiment-on-relative-impact-of-self-attention-and-feed-forward-network-outputs">🔗</a></h3><p>The <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a> contains the implementation of an experiment to understand the relative impact of the self-attention outputs and the feed-forward network outputs on the final output of the transformer. This appendix summarizes the experiment and the results.</p><p>Experiment procedure:</p><ul><li>I ran all 20,000 prompts through the model and captured the final output probability distributions as well as the intermediate self-attention outputs, feed-forward network outputs, and final block outputs for each block.</li><li>For each block, I then ran two tests. First, instead of sending the block output as normally implemented (<code>x + sa_out + ffwd_out</code>, as shown <a href="#block-logic-with-intermediates">earlier</a>) to the next stage of the model, I sent a version that omits the self-attention output i.e. just <code>x + ffwd_out</code>, and saved the final output probability distribution that resulted. Then, I did the same thing but removed the feed-forward network output instead, sending on just <code>x + sa_out</code>.</li><li>I then calculated the Hellinger distance between the probability distribution produced with the regular block output and that produced by each of the two modifications.</li></ul><p>The table below shows the results, averaged across all 20,000 prompts:</p><blockquote><p>For the implementation of this analysis, see <a href="https://github.com/spather/transformer-experiments/blob/master/nbs/analyses/70_embedding_adjustments.ipynb" target="_blank" rel="noopener">embedding adjustments analysis notebook</a></p></blockquote><table><thead><tr><th>Block</th><th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+ffwd_out</code>))</th><th>H(output(<code>x+sa_out+ffwd_out</code>), output(<code>x+sa_out</code>))</th></tr></thead><tbody><tr><td>1</td><td>0.11 ± 0.07</td><td>0.70 ± 0.17</td></tr><tr><td>2</td><td>0.07 ± 0.04</td><td>0.19 ± 0.11</td></tr><tr><td>3</td><td>0.09 ± 0.07</td><td>0.15 ± 0.10</td></tr><tr><td>4</td><td>0.06 ± 0.05</td><td>0.13 ± 0.10</td></tr><tr><td>5</td><td>0.04 ± 0.03</td><td>0.14 ± 0.10</td></tr><tr><td>6</td><td>0.03 ± 0.03</td><td>0.17 ± 0.10</td></tr></tbody></table><blockquote><p>Remember that Hellinger distance ranges between 0 and 1, with 0 meaning identical and 1 meaning no overlap. A larger Hellinger distance in this table means a larger divergence between the experiment output and the normal transformer output.</p></blockquote><p>The effect is most pronounced in the first block: omitting the feed-forward network output results in an almost completely different probability distribution (H = 0.70) but omitting the self-attention output results in a very similar distribution (H = 0.11). Across the rest of the layers, the difference is less dramatic, but the feed-forward network output always has the larger impact.</p><blockquote><p>Though I think these results support the notion that an approximation based only on feed-forward network outputs can produce similar results to the transformer, it would be interesting to see if the approximation would improve if we include the self-attention outputs, particularly for some of the intermediate layers. But I’m leaving that as an area for future investigation.</p></blockquote><h3 id="v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">V: Performing SVD to Get a Linear Approximation of a Token Subspace <a href="#v-performing-svd-to-get-a-linear-approximation-of-a-token-subspace">🔗</a></h3><p>This appendix walks through an example of how we can find a linear approximation for a token subspace using SVD. First, let’s load all the embeddings learned for the token <code>a</code> at the output of the last block of the transformer (input to the final layer norm and linear layer):</p><div><pre tabindex="0"><code data-lang="python"><span><span>learned_embeddings_dir <span>=</span> environment<span>.</span>data_root <span>/</span> <span>'learned_embeddings'</span>
</span></span><span><span>multi_emb_a <span>=</span> torch<span>.</span>load(learned_embeddings_dir <span>/</span> <span>'no_blocks'</span> <span>/</span> <span>'lower_a.pt'</span>, map_location<span>=</span>device)
</span></span><span><span>multi_emb_a<span>.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>torch.Size([100, 1, 384])
</code></pre><p>We’ve got 100 different 384-dimensional embedding vectors. Each one, when given as input to the final blocks in the transformer, produces an output distribution that assigns nearly all the probability mass to the token, <code>a</code>. Each one can be thought of as a point in the subspace for token <code>a</code>.</p><p>We can stack these embeddings form a 100x384 matrix:</p><p>$$
\begin{bmatrix}
e_{1,1} &amp; e_{1,2} &amp; \dots &amp; e_{1,384} \\
e_{2,1} &amp; e_{2,2} &amp; \dots &amp; e_{2,384} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
e_{100,1} &amp; e_{100,2} &amp; \dots &amp; e_{100,384}
\end{bmatrix}
$$</p><p>Next, we can run SVD on this matrix:</p><div><pre tabindex="0"><code data-lang="python"><span><span>_, S, V <span>=</span> torch<span>.</span>linalg<span>.</span>svd(multi_emb_a[:, <span>-</span><span>1</span>, :])
</span></span></code></pre></div><p>For this analysis, we’re only interested in the singular values (<code>S</code>) and the right singular vectors (<code>V</code>). We can plot the singular values:</p><div><pre tabindex="0"><code data-lang="python"><span><span>_ <span>=</span> plt<span>.</span>plot(S<span>.</span>numpy(), <span>'-o'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/ec5755d7859f77e55c3bf34a432c33226741616e9b493b5eec96c716ac1e7fe5.png" alt=""></p><p>The first singular value is much larger (over 6x) than the next, which suggests the first right singular vector alone might be a good approximation for the subspace that predicts token <code>a</code>. We can test what gets predicted when we use this first right singular vector as an embedding:</p><div><pre tabindex="0"><code data-lang="python"><span><span>v0a <span>=</span> adjust_singular_vector_sign(V[<span>0</span>], multi_emb_a[:, <span>-</span><span>1</span>, :])
</span></span><span><span>logits <span>=</span> LogitsWrapper(accessors<span>.</span>logits_from_embedding(unsqueeze_emb(v0a)), tokenizer)
</span></span><span><span>logits<span>.</span>plot_probs(title<span>=</span><span>'Next Token Probability Distribution from First Right Singular Vector of embeddings for Token "a"'</span>)
</span></span></code></pre></div><p><img src="https://shyam.blog/posts/beyond-self-attention/images/968e8e424522937d5366586abd902715e1cdf771fddbb2d4a36144cbe07746e2.png" alt=""></p><p>In this distribution, <code>a</code> has probability near 1 and every other token has probability near 0. So the first right singular vector is effectively another embedding that produces an output predicting <code>a</code> with near certainty.</p><p>But it’s different from the other 100 learned embeddings in an important way: it’s the vector that is best aligned with <em>all</em> of them. More formally, it’s the vector that minimizes the squared distance to all 100 other embedding vectors. In this way, the first right singular vector is like a good summary of the embedding vectors we started from.</p><p>The first right singular vector is a unit vector (as are all the singular vectors):</p><pre tabindex="0"><code>tensor(1.0000)
</code></pre><p>Any vector along its span will produce an output distribution predicting <code>a</code>, similar to the one above (see <a href="#iii-the-output-embeddings-norm-doesnt-matter-because-of-the-final-layernorm">Appendix III</a> for an explanation of why the transformer output is invariant to the scale of the final embedding). So the span of this vector is a good, linear approximation to the subspace for token <code>a</code>.</p><p>The same results we saw here for token <code>a</code> hold for the other tokens too. For each, if we stack all the learned embeddings and perform SVD, we find that the first right singular vector forms a good linear approximation of the token subspace.</p></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Browser extensions are underrated: the promise of hackable software (522 pts)]]></title>
            <link>https://www.geoffreylitt.com/2019/07/29/browser-extensions</link>
            <guid>39251095</guid>
            <pubDate>Sun, 04 Feb 2024 15:43:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geoffreylitt.com/2019/07/29/browser-extensions">https://www.geoffreylitt.com/2019/07/29/browser-extensions</a>, See on <a href="https://news.ycombinator.com/item?id=39251095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure>
  <img src="https://www.geoffreylitt.com/images/article_images/legos.jpg?1705878965" alt="Lego bricks">
  <figcaption>Photo by <a href="https://unsplash.com/photos/2FaCKyEEtis">Rick Mason on Unsplash</a></figcaption>
</figure>

<p>Recent conversations about web browser extensions have focused on controversy: <a href="https://arstechnica.com/information-technology/2019/07/dataspii-inside-the-debacle-that-dished-private-data-from-apple-tesla-blue-origin-and-4m-people/">malicious browser extensions capturing web history</a>, and <a href="https://www.wired.com/story/google-chrome-ad-blockers-extensions-api/?verso=true">Google limiting the capabilities used by ad blockers</a>. These are important discussions, but we shouldn’t lose sight of the big picture: browser extensions are a special ecosystem worth celebrating.</p>

<p>Among major software platforms today, <strong>browser extensions are the rare exception that allow and encourage users to modify the apps that we use</strong>, in creative ways not intended by their original developers. On smartphone and desktop platforms, this sort of behavior ranges from unusual to impossible, but in the browser it’s an everyday activity.</p>

<p>Browser extensions remind us what it’s like to have deep control over how we use our computers.</p>

<h2 id="assembling-our-own-software">Assembling our own software</h2>

<p>Once a software platform reaches a certain level of openness, it can fundamentally change the way that normal users relate to their software. By installing four different Gmail extensions that modify everything from the visual design to the core functionality, in some sense, I’ve put together my own email client. <strong>Instead of being a passive user of pre-built applications, I can start assembling my own personalized way of using my computer.</strong></p>

<p>The popularity of browser extensions proves that many people are interested in customizing their software, and it’s not just  a hobby for power users. There are over 180,000 extensions on the Chrome store, and nearly half of all Chrome users have browser extensions installed.<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup> When people have an easy way to extend their software with useful functionality, they apparently take advantage.</p>

<h2 id="hackable-platforms-not-custom-apis">Hackable platforms, not custom APIs</h2>

<p>Browser extensions have remarkably broad use cases. I personally use Chrome extensions that fill in my passwords, help me read Japanese kanji, simplify the visual design of Gmail, let me highlight and annotate articles, save articles for later reading, play videos at 2x speed, and of course, block ads.</p>

<p><strong>The key to this breadth is that most extensions modify applications in ways that the original developers didn’t specifically plan for.</strong> When Japanese newspapers publish articles, they’re not thinking about compatibility with the kanji reading extension. Extension authors gain creative freedom because they don’t need to use application-specific APIs that reflect the original developers’ view of how people might want to extend their application.</p>

<p>The web platform has a few qualities that enable this sort of unplanned extensibility. The foundational one is that the classic web deployment style is to ship all the client code to the browser in human-readable form. (Source maps are a key to preserving this advantage as we ship more code that’s minified or compiled from other languages.) The web’s layout model also promotes extensibility by encouraging standardized semantic markup—my password manager extension works because web pages reliably use form tags for password submissions instead of building their own version.</p>

<p>Even with these advantages, it can still require clever tricks to modify a site in ways that it wasn’t built for. But it’s often a reasonable amount of work, not a years-long reverse engineering effort. The sheer variety of extensions available shows that extension authors are willing to jump through a few hoops to create useful software.</p>

<p>Occasionally there are tensions between website developers and extension authors, but it seems far more common that developers are fine with their sites being extended in creative ways, as long as they don’t have to do any extra work. Extensions can even make life easier for application developers: if there’s a niche request that a small minority of users want, a motivated community member can just build an extension to support it. By building on a hackable platform, developers allow their users to get even more value out of their applications.</p>



<p>Many browser extensions are generic tools designed to enhance my use of all websites. I can use my annotation extension on every website everywhere, instead of needing a different highlighting tool for each article I read. Just like using a physical highlighter with paper articles, I can master the tool once, and get a lot of leverage by applying it in different contexts.</p>

<p>In many software platforms, we think of the operating system as providing the cross-cutting tools, and third parties as providing standalone “apps” that are used in isolation. <strong>With browser extensions, third parties are also adding tools;</strong> a single piece of software has the leverage to change my experience across all the apps I use.</p>

<p>When software is built in small units, it also changes the economics. Most extensions I use are free, and are perhaps too small in their feature set to support a full-blown business. And yet, people still choose to make them, and I benefit immensely from these little bits of software. Browsing the extension store feels more like going to a local flea market than going to a supermarket. Massive software built by huge companies isn’t the only way.</p>

<h2 id="the-origins-of-openness">The origins of openness</h2>

<p>It’s not an accident that this openness emerged on the web platform.</p>

<p>Since the beginning of personal computing, there’s been a philosophical tradition that encourages using computers as an interactive medium where people contribute their own ideas and build their own tools—authorship over consumption. This idea is reflected in systems like Smalltalk, Hypercard, and more recently, <a href="https://dynamicland.org/">Dynamicland</a>.</p>

<p>When Tim Berners-Lee created the World Wide Web, he imagined it fitting into this tradition. “My vision was a system in which sharing what you knew or thought should be as easy as learning what someone else knew.”<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup> There were some hiccups along the way<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>, but eventually that vision largely won out, and the Web became a place where anyone can publish their opinions or photos through social media platforms.</p>

<p>Still, there’s a catch. When you’re using Facebook, you’re operating within a confined experience. You’re forced to publish in a certain format, and to use their app in a certain way (that includes, of course, seeing all the ads). There’s more room for authorship than just browsing a news website, but only within the strict lines the app has painted for you.</p>

<p><strong>Browser extensions offer a deeper type of control.</strong> Instead of merely typing into the provided text box, we can color outside the lines and deeply modify the way we use any application on the web. Browser extensions offer a kind of decentralization: large companies building major websites don’t get to dictate all the details of our experience.</p>

<h2 id="improving-on-extensions">Improving on extensions</h2>

<p>We clearly need to work on protecting people from malicious extensions that invade their privacy. But beyond that, here are some bigger picture opportunities I see for improving on extensions:</p>

<p><strong>Accessibility:</strong> Today, it requires a big jump to go from using browser extensions to creating them: you need to learn a fair amount of web development to get started, and you can’t easily develop extensions in the browser itself. What if there were a quick way to get started developing and sharing extensions in the browser? You could imagine smoothly transitioning from editing a website in the developer tools to publishing a small extension.</p>

<p><em>Update</em>: I’ve started working on a system called <a href="https://sdg.csail.mit.edu/projects/wildcard">Wildcard</a> to work towards this vision.</p>

<p><strong>Compatibility:</strong> Because extensions hook into websites in unsupported ways, updates to websites often result in extensions temporarily breaking, and extension authors scrambling to fix them. Can we make it easier for website developers and extension authors to form stable connections between their software, without necessarily resorting to using explicit extension APIs?</p>

<p>There are existing practices that fit into this category already—for example, using clean semantic markup, human-readable CSS, and source maps makes it easier to develop an extension.</p>

<p>A simple change that would allow for more stable extensions would be to give users more control over when they upgrade to new versions of cloud software. If I have a 3 month window to continue using an old version after the new one is released, that would give extension authors more time to upgrade their software for the new version.</p>

<p><strong>Power:</strong> Web extensions are limited in their power by the typical architecture of web applications: they have broad rights to modify the browser client, but the server is off limits. For example, if my social media app’s server only provides an endpoint for querying my posts in chronological order, no browser extension can ever search through all my posts by keyword. How could we rethink the client-server boundary to enable extensions to make even deeper modifications?</p>

<p>This raises tough questions around security and privacy. The modern browser extension API has done a good job balancing extensibility with security, and yet we’re still grappling with the consequences of browser extensions invading people’s privacy. Giving extensions more power would raise the stakes further. Still, we shouldn’t give up in the name of security—we should fight for extensibility as a value and find ways to balance these interests.</p>

<h2 id="the-next-platform">The next platform</h2>

<p>I’m intrigued by a couple projects that are rethinking the web in ways that might make it more extensible:</p>

<p>The <a href="https://beakerbrowser.com/about/">Beaker Browser</a> and the decentralized web community are exploring how the web works without centralized servers. It seems like their proposed architecture would give users fuller control over modifying the “server” side of web applications.</p>

<p>Tim Berners-Lee is working on a new project called <a href="https://inrupt.com/blog/one-small-step-for-the-web">SOLID</a>. I don’t yet understand precisely what they’re up to, but given Tim’s involvement I figure it’s worth paying attention. A key principle is giving users more ownership over their data, which would enable people to use extensions and other software to manipulate their data in flexible ways beyond what application server APIs allow.</p>

<p>Computing is still young, and platforms are changing quickly. Modern browser extensions and smartphone platforms have only been around for about a decade. These platforms will evolve, and there will be new platforms after them, and we will get to collectively decide how open they will be.</p>

<p>Browser extensions give us one example to strive for: a place where we routinely hack the software we use and make it our own. <span>▪</span></p>

<p><a href="https://news.ycombinator.com/item?id=20556382"><em>Discuss on Hacker News</em>
</a></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rolling Airframe Missile (177 pts)]]></title>
            <link>https://www.navalgazing.net/RAM</link>
            <guid>39250896</guid>
            <pubDate>Sun, 04 Feb 2024 15:24:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.navalgazing.net/RAM">https://www.navalgazing.net/RAM</a>, See on <a href="https://news.ycombinator.com/item?id=39250896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I've previously discussed <a href="https://www.navalgazing.net/Standard-Part-2">Standard</a>, <a href="https://www.navalgazing.net/Sea-Sparrow">Sea Sparrow</a>, <a href="https://www.navalgazing.net/ESSM">ESSM</a> and <a href="https://www.navalgazing.net/Phalanx">Phalanx</a>, but there is one last air-defense weapon that deserves discussion.  This is the <a href="https://en.wikipedia.org/wiki/RIM-116_Rolling_Airframe_Missile" rel="nofollow">RIM-116 Rolling Airframe Missile</a>, better known as RAM.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/RAMLaunchGreenBay.jpg?v=1706810557.jpg" alt=""><br><span>A RAM is launched from USS <em>Green Bay</em>  </span></span></p>
<p>Much like the other point-defense systems, RAM's origins trace back to <em><a href="https://www.navalgazing.net/Eilat">Eilat</a></em>, and the panic that it provoked within the USN.  It was conceived to work in pretty much the same niche as Phalanx, providing a last-ditch defense against incoming anti-ship missiles.  Effective as it was, Phalanx had a serious limitation, even while it was still in development.  The use of a gun limited effective range to no more than 1500 yards, which was a serious problem in the face of supersonic missiles.  The available window to engage such a weapon was short, and even if the Phalanx did shoot it down, the debris was likely to strike the defended ship.  The obvious solution was to use a missile, which could engage at significantly longer range.
<a name="break" id="break"></a>
</p>
<p>The initial program that led to RAM was based on the <a href="https://en.wikipedia.org/wiki/FIM-43_Redeye" rel="nofollow">Redeye missile</a>, the first American man-portable SAM system, although it would be fitted with a combined radar/IR seeker to allow it to engage closing targets (which were difficult to engage purely with IR seekers at the time) at reasonable range.  The only real concern was the small size of the missile, 2.75" in diameter and about 18 lb, and Congress directed the Navy to study something the size of Sidewinder, 5" and about 160 lb, instead.  The initial contract was signed with General Dynamics in 1976, with West Germany coming onboard as a development partner.  Development didn't go particularly smoothly, with delays from testing and cost adding up to around 5 years, and both the US and Germany came close to withdrawing from the program at various points.  But things were eventually worked out, and RAM entered in the early 90s.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/RAMMissileFlightGHWB.jpg?v=1706810558.jpg" alt=""><br><span>A RAM in flight</span></span></p>
<p>RAM is a rather unusual missile.  It gets its name because it is fired from a rifled tube and rolls in flight thanks to the tube and four fins.  The roll allows it to use only two control fins in flight, instead of the usual four.  The basic airframe, motor, fuze and warhead initially came from the <a href="https://en.wikipedia.org/wiki/AIM-9_Sidewinder" rel="nofollow">Sidewinder</a>, while the IR seeker was derived from the <a href="https://en.wikipedia.org/wiki/FIM-92_Stinger" rel="nofollow">Stinger</a>.  Because RAM was expected to be fired at incoming targets, where the hot engine would not be visible, the seeker would need to rely on glint (reflected IR radiation from the sun), which sharply limited range. As a result, initial guidance would be provided by a passive RF system, which could home in on the radar seeker of a typical cruise missile.<a id="fnr1_1" href="#fn1_1"><sup>1</sup></a>  The rolling missile could also get away with a 2-sensor radar inferometer, instead of requiring four sensors, like a more conventional missile.  The accuracy of the RF seeker was limited, hence the inclusion of the IR seeker, but it was in theory possible for the missile to use it all the way to the target if it's dark or cloudy and the IR seeker doesn't work.  The wide cone of the RF sensor also allows the missile to be fired "around the corner", reducing the size of sectors blocked by the ship's structure by 10-15°.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/SailorLoadingRAMTruman.jpg?v=1706810695.jpg" alt=""><br><span>Sailors load a RAM launcher aboard the <em>Truman</em></span></span></p>
<p>But there were serious concerns about the effectiveness of RAM against missiles that used IR or semi-active homing, so even before the Block 0 missile entered service, work began on Block 1, with an imaging IR seeker that is capable of searching for targets on its own.  It still has the RF seeker, and is capable of using the original dual-sensor mode, homing entirely on IR or switching to IR search if it loses RF track.  Block 1 entered service in 1999, and achieved a 95% success rate in intercepting incoming missiles across 180 trials.  A further upgrade took place in 2002, to give better performance against helicopters, slow aircraft and surface targets.  This upgrade, known as HAS, was implemented entirely as a software upgrade.  In the mid-2000s, a second upgrade, Block 2, was started.  It was a considerably bigger overhaul than Block 1, with a new 6.25" motor<a id="fnr1_2" href="#fn1_2"><sup>2</sup></a> (which increased range significantly, from 6 nm to 10 nm), a 4-fin steering system, and an improved RF seeker.  Block 2 was cleared for service in 2015, and is currently in production.
</p>
<p><span> <img width="600" src="https://www.navalgazing.net/attach/JMSDF_DDH-183_SeaRamIzumo.jpg?v=1706810829.jpg" alt=""><br><span>SeaRAM onboard Japanese helicopter destroyer <em>Izumo</em></span></span></p>
<p>The fire-and-forget nature of RAM meant that it imposed relatively minimal burdens on the firing ship's combat system, particularly given the ability of the missile to search for targets after launch.  The combat system still mattered in terms of firing at the correct time and in the right direction, but it opened up new possibilities for smaller ships that couldn't support Sea Sparrow or the like.  The most extreme version of this was SeaRAM, which was essentially a <a href="https://www.navalgazing.net/Phalanx">Phalanx</a> system with the gun removed and replaced by an 11-round RAM launcher.  Like Phalanx, it is capable of operating independently of the ship that carries it, automatically detecting and engaging incoming missiles.  SeaRAM is primarily carried by the <a href="https://www.navalgazing.net/LCS-Part-1"><em>Independence</em> class LCS</a>, although a few <a href="https://www.navalgazing.net/The-Arleigh-Burke-Class"><em>Burke</em>s</a> are also fitted with the system to provide some defense against cruise missiles when the main radar is in ballistic missile defense mode.
</p>
<p><span> <img height="440" src="https://www.navalgazing.net/attach/RIM-116_Rolling_Airframe_Missile_Launcher_Ozelot.jpg?v=1706810828.jpg" alt=""><br><span>A RAM launcher on the German missile boat <em>Ozelot</em></span></span></p>
<p>But the more common launcher is the 21-round Mk 49, a trainable launcher integrated with the ship's combat system and fitted to a number of ships, including all American aircraft carriers and amphibious ships, as well as the <em>Freedom</em> class LCS.  Germany has equipped all of its warships with RAM, while Egypt, Greece, Japan, Mexico, Qatar, South Korea, Saudi Arabia, Turkey and the UAE have bought the system for their ships as well.  If anything, RAM seems destined for wider service in the years to come.  It is the most minimal system capable of providing protection against anti-ship missiles, a threat that has been graphically demonstrated in the Red Sea in recent months, and which is only likely to continue to proliferate.
</p>
<hr>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Plastic bans work. Billions of plastic bags were avoided in the US alone (251 pts)]]></title>
            <link>https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/</link>
            <guid>39250434</guid>
            <pubDate>Sun, 04 Feb 2024 14:28:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/">https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/</a>, See on <a href="https://news.ycombinator.com/item?id=39250434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								
<p>“The bottom line is that <a href="https://www.zmescience.com/ecology/environmental-issues/congo-bans-plastic-bags-321313/">plastic bag bans</a> work,” said Faye Park, president of the U.S. PIRG Education Fund, in a statement. “People realize quickly it’s easy to live without plastic bags and get used to bringing a bag from home or skipping a bag when they can.”</p>



<figure><a href="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-scaled.jpg"><picture><source srcset="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.webp 1024w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1536x1024.jpg 1536w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-2048x1365.jpg 2048w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-750x500.jpg 750w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1140x760.jpg 1140w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.jpg" height="683" width="1024" srcset="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.jpg 1024w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1536x1024.jpg 1536w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-2048x1365.jpg 2048w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-750x500.jpg 750w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1140x760.jpg 1140w" sizes="(max-width: 1024px) 100vw, 1024px" alt="plastic bag waste" fetchpriority="high" decoding="async"> </picture></a><figcaption>Image via Unsplash.</figcaption></figure>



<h2 id="plastic-ain-t-all-that-fantastic">Plastic ain’t all that fantastic</h2>



<p>Plastic bags are a victim of their own success. When they <a href="https://www.unep.org/news-and-stories/story/birth-ban-history-plastic-shopping-bag">were first patented in Europe in 1965</a>, society was shocked to see how cheap and durable they could be. Within a decade or two they became mainstream on the continent and in North America, and it wasn’t long before they started being widely used on the entire planet.</p>



<p>But plastics were just a little too durable. They didn’t go away. They started accumulating in landfills and in the oceans. The environmental impact of plastic bags gained attention with the discovery of the Great <a href="https://www.zmescience.com/ecology/pollution-ecology/great-pacific-garbage-patch-06102016/">Pacific Garbage Patch</a> in 1997. Plastic bags (and plastic in general) had left its mark on the planet in an unprecedented form of pollution.</p><!-- Tag ID: zmescience_300x250_InContent -->





<p>Fast forward a couple more decades, and countries started fighting their urge to use <a href="https://www.zmescience.com/research/materials/cheap-fabrics-from-plastic-15032021/">cheap plastics</a> and implement bans or other measures against plastic bags — and finally, there’s some good news.</p>



<p>San Francisco pioneered the movement in the U.S. by passing the <a href="https://www.zmescience.com/ecology/environmental-issues/bottled-water-ban-national-park-43423/">nation’s first plastic bag ban</a> in 2007. Several other U.S. cities and <a href="https://www.zmescience.com/ecology/hawaii-bans-plastic-bags-04062012/">states implemented plastic bag bans</a> or restrictions. By 2023, ten states had statewide bans, with similar laws proposed in others​​. To get a state of how much this of a difference this made, five studied bans resulted in an average elimination of <a href="https://www.zmescience.com/ecology/plastic-bag-tax-uk-22112016/">almost 300 plastic bags</a> per person per year​​. Overall, in the US alone, <a href="https://www.zmescience.com/research/tea-plastic-particles-ocean-234523521/">billions of plastic bags</a> were avoided with anti-plastic bag measures.</p>




<h2 id="h-the-case-against-plastic">The case against plastic</h2>



<p>The case against plastic bags is straightforward.  Plastic pollution kills at least <a href="https://wwf.org.au/blogs/plastic-in-our-oceans-is-killing-marine-mammals/">100,000 marine mammals</a> and <a href="https://sustainabledevelopment.un.org/content/documents/Ocean_Factsheet_Pollution.pdf">1 million seabirds</a> every year and entanglement in plastic and other types of litter kills roughly 1,000 turtles per year. Plastic bags aren’t responsible for all of that, but they make up an important part of the problem.</p>







<blockquote>

</blockquote>



<p>The results, which were published in a report, also highlight that imperfect measures leave loopholes or encourage buyers to opt for other single use bags.</p>



<blockquote>
<p>Well-designed <a href="https://www.zmescience.com/science/news-science/europe-single-use-plastic-24102018/">single-use plastic bag bans</a> across the country have successfully reduced single-use plastic bag consumption, cut down on plastic bag litter and driven consumers to make more sustainable bag choices. Policymakers should pursue these policies at the state and local levels,” the report says.</p>
</blockquote>



<p>The idea isn’t to shift from one type of single-use bag to another type of single-use bag. Paper bags are easier to recycle than plastic, but they take 3-4 times more energy to produce and usually generate more solid waste.</p><!-- Tag ID: zmescience_300x250_InContent_3 -->




<p>Ultimately, the report concludes that regulation is the best current way to address plastic waste and plastic pollution.</p>



<blockquote>
<p>“Grocery stores, restaurants and retail shops should not be permitted to distribute plastic film bags of any thickness at checkout. Stores should be required to charge a fee of at least 10 cents for single-use paper bags. A 10-cent paper bag fee will limit the expected increase in paper bag use after a bag ban is imposed and may even reduce paper bag consumption altogether.”</p>



<p>“Local and state governments should conduct regular enforcement to ensure compliance.”</p>
</blockquote>



<p>You can read the <a href="https://environmentamerica.org/center/resources/plastic-bag-bans-work/">report in its entirety here</a>.</p>

<!-- AI CONTENT END 1 -->
								
								
																	
																	
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Write code for the web - Apple doesn't care about you, Mr. Developer (426 pts)]]></title>
            <link>https://mrmr.io/apple/</link>
            <guid>39250406</guid>
            <pubDate>Sun, 04 Feb 2024 14:24:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mrmr.io/apple/">https://mrmr.io/apple/</a>, See on <a href="https://news.ycombinator.com/item?id=39250406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" id="___gatsby"><main><div><h3>Write code for the web</h3><p>This is a yarn of three threads, and it got a bit long. The tldr is</p>
<ol>
<li>
<p>Apple doesn’t care for me as a developer</p>
</li>
<li>
<p>I should write code for the web</p>
</li>
<li>
<p>Nothing is set in stone, really</p>
</li>
</ol>
<p>The story is personal, but I hope readers find something they can relate to in
their own life stream (I guess that's the point of blogs)</p>
<h3>Apple doesn't care for you, Mr. Developer</h3>
<p>Apple cares for me as a customer, but it doesn’t care for me as a developer.
This dynamic had been subconsciously griefing me for years, until I was able to
formulate it consciously recently.</p>
<p>The dependency goes <em>Developer -&gt; Apple</em> and <em>Apple -&gt; Consumer</em>, there is no
reverse arrow from Apple to the developers.</p>
<p>If all developers stopped building for Apple's platforms tomorrow, Apple will
still survive, almost intact, since it doesn’t hurt their core value prop (I'll
expand on this below). Apple has no dependency on individual developers. They
care for and need to collaborate with corporate dev “partners”, but that's
different.</p>
<p>Since companies, especially ginormous multinationals, behave in purely game
theoretic cost/benefit terms, and since Apple has no reason to care about
developers: indeed it doesn’t.</p>
<blockquote>
<p>Just to restate the obvious - such a stance isn't necessarily the case. There
are other multinationals who've figured that developers form a core part of
their strategy.</p>
</blockquote>
<p>This realisation has made me happier since I now know my place. I can like their
products without wanting to develop for them.</p>
<hr>
<p>Google has a bug. I have dynamic light/dark mode, and if I search on Google at
night, the first page shows up in light mode. With the rest of my machine in a
subdued state, the glaring white of the background hurts my eyes.</p>
<p>In the morning, my laptop automatically switches back to light mode. Now when I
search on Google, the results first show up in an unreadable black background.</p>
<p>One would think that of all the leetcode certified staff, there must be someone
there who would know an O(n) algorithm for fixing this bug. But no, this bug has
persisted for years (I've counted), and it is unlikely to be fixed in the future
unless it gets accidentally fixed as part of some overhaul.</p>
<p>Apologies for the snide, I know some great people who work there. My point is
that Google isn't fixing this bug not because it doesn’t know how to, but
because it doesn’t care. This bug has zero impact on its bottom line.</p>
<p>The people like me who use alternate search engines like DDG have already moved
on years ago. The rest of the (overwhelming) majority is stuck with Google. No
matter how bad their search is - UX or results - they have a captive audience.</p>
<blockquote>
<p>I think DDG etc also have a marketing blind spot - they keep pitching
themselves as a more privacy friendly alternative, they never go after the
main course. I don’t use DDG because of its privacy benefits, I use it because
it reminds me of early Google - simple low clutter UX, good quality verbatim
search results, and unobtrusive ads.</p>
</blockquote>
<p>Okay Manav, but I thought you were ranting about Apple, why bring in Google?</p>
<p>Google is an example where I never grieved much because I understood the
dynamics. I know that I, the customer, am not their game theoretic target. So
when I have to invariably use their products and face another user hostile
interaction, I try to shrug it off and move on. I know that Google has entered a
rent seeking phase, and while it is sad that the world is giving all its video
content up to it for even more of a hostage situation in the future, but that’s
for governments to deal with.</p>
<p>With Apple I didn’t understand the dynamics.</p>
<hr>
<p>2009-ish. I struggled to find a computer for my mom. Windows (at that time, I
don't know about now) was just too insecure. Linux required constant tech
support. Eventually I prepared for her an OpenBSD machine running Firefox and a
basic game (Bubbles I think).</p>
<p>Obviously, this was not ideal. It fulfilled some goals - it worked without
requiring any tech support when I wasn't around, and I was ensuring her data
safety and privacy - and she was surprisingly happy with too, but I was not
happy about how this was such a shrivelled parody of what things could be, and
how it limited the ways she could use computers to enrich her life.</p>
<p>Around this time, I joined a new job as a developer for a company that was
making iOS apps. After a week or so of using the mac at work it hit me - <em>this
is the computer I wanted for my mom!</em></p>
<p>That is Apple’s value prop.</p>
<p>As soon as I had saved enough I bought her a MacBook. And heartfeltly thanked
Steve Jobs for engendering it.</p>
<p>The earth has done many a revolutions since then, and Jobs has left earth, and
the form factor my mother uses has changed from a laptop to an iPad. But it
still satisfies that core value prop.</p>
<p>Let's consider a different context. Even if there were no apps in my phone, I
would still buy an iPhone. For myself likely, but most certainly for her.</p>
<p>The people running Apple know all this. In the deep dungeons of Menlo Park when
there are meetings of the core council, after all the sacrificial lambs have
been slain and the blood and gore washed away, out comes the elder spreadsheet
that encodes Apple’s business model, but nowhere in them is any cell, input or
output, which involves developers. Sure, Apple doesn't mind if developers are
also happy. But it knows it doesn’t need to care if they are.</p>
<p>This lack of caring is never expressed out loud. It isn’t some conspiracy, it
just is one of those things - you wouldn’t walk up to someone who you don’t care
about and tell them you don’t care.</p>
<p>Unfortunately all this results in sometimes schizophrenic behaviour on Apple’s
part, as there are many individuals working at Apple who <em>do care</em> about
developers and are trying to make things better.</p>
<hr>
<p>2016-ish. As part of my annual Apple simping I was watching WWDC when they
announced Apple Music APIs.</p>
<p>I was ecstatic. My coworkers were puzzled at my ecstasy, “All this, can’t we
already do with Spotify’s API?”. I didn’t know how to answer that, so I just
repeated how <em>this changes everything</em>.</p>
<p>Of course, and as is usual, I was wrong. It didn’t change everything. In fact,
it didn’t change <em>anything</em>.</p>
<p>Apple’s own music player was, and still is, unusably bad. Even talking about it
makes me angry. The people making it can’t be so incompetent, and it seems to be
working for the rest of the world, so I've never known what to make of this
situation.</p>
<p>With the APIs out, I'd thought maybe things will change. But nothing happened.
Apple’s own player continues to make my blood pressure high anytime I have to
use it. And whatever alternative players I have tried just all seem to go for
the same generic “Spotify” approach to music.</p>
<blockquote>
<p>I think it is because the people who're making these apps were never around in
the Justin Frankel era of Winamp, and haven't seen how a music player can
provide a fast, seamless, endless <em>yet</em> still personal approach to music.</p>
<p>That era is not coming back because it relied on piracy, but luckily we don't
need to anymore - that's the great thing about Apple's music catalog! We can
recreate that experience without needing to sail the high seas.</p>
</blockquote>
<p>So that's the backstory. Now recently I had a bunch of free time, and thought
that I’ll write a music player. Mostly for myself, but I also wanted to write a
tutorial. This is what I wrote in the README in the first commit:</p>
<blockquote>
<p>Here I'll be writing down my notes as I build Flowers. The world needs not one
music player, or two, but many: each of us has our own way of connecting with
music, and so maybe these notes will help others build the flower they want,
nay need.</p>
</blockquote>
<p>Cute, dumb, and in vain.</p>
<p>As I went about it, I realized that 8 years down the line, not only is the API
still buggy, it is also still not public!</p>
<p>Firstly you need to pay Apple. No, not for bulk usage etc, but <em>just to try out
the API</em>. So there there goes my dream of writing a walkthrough – nobody’s going
to pay Apple 100 bucks just to try things out. And it also partially explains
why there has been no innovation.</p>
<p>But that's not even it - Even if you pay them, you get a restricted API.</p>
<p>The point where I disgustedly gave up was when I found out that while I was
jumping all these kafkaesque hoops, instead you could just go to Apple's own web
music player, type <code>MusicKit.getInstance().developerToken</code> in your browser
console, and you’ll get an unrestricted root token for free! </p>
<hr>
<p>All these anecdotes Manav, what does all this mean?</p>

<p>Write code that runs on the web. We’re lucky to have a shared platform that no
single entity owns. Even benevolence can be ruined by incompetence.</p>
<p>The web platform is in a precarious place – overreaching governments, browser
duopolies, a complex developer ecosystem – so it is not a given it’ll remain
thriving. But so far it has survived. And every year longer it survives, the
more the chances that it’ll continue to thrive in the future.</p>
<p>Ironically Google is the good guy here, they’re doing great work for the web. On
the other hand, literally every single workaround I’ve had to write in the
recent past in web related code has been due to Safari's princessness.</p>
<h2>Nothing is set in stone</h2>
<p>Which brings me to the third thread of this story, how all this made me
reevalute my relationship with companies.</p>
<p>Someone I know, someone who has a better grip on living than me, told me once
that it's not useful to put people into the buckets of good and bad. People are
a mix. Bad folks can do good actions sometimes, and vice versa.</p>
<p>I don't know to what extent I've been able to internalize their message, but
that's for another day. What I realized is that the same applies to companies.</p>
<p>Companies are like people. I don't know if they're sentient, but otherwise they
share many attributes with us: they're intelligent (they were the AI before AI),
have personalities, they are born, thrive, live and die, they're even legal
persons.</p>
<p>Just like we can't live without other people, we can't live without companies.
They have many inhumane characteristics, but like them or not, such fractal
conceptions of human organizations will always be around.</p>
<p>By not permanently bucketing companies into good or bad, I can have a more fluid
interaction with them - I can reduce my dependence on them when they try to put
me in a zero sum game, or reengage more with them if they're willing to be more
symbiotic. Nothing is set in stone, really.</p>
<hr>
<blockquote><p>I think most large companies and medium-size companies, and even small
companies, are starting to look at the web as the ultimate direct-to-customer
distribution chain, bypassing all middlemen, going directly from the supplier to
the consumer.</p><p>– Steve Jobs, Make Something Wonderful</p></blockquote></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A reasonable configuration language (131 pts)]]></title>
            <link>https://ruudvanasseldonk.com/2024/a-reasonable-configuration-language</link>
            <guid>39250320</guid>
            <pubDate>Sun, 04 Feb 2024 14:12:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ruudvanasseldonk.com/2024/a-reasonable-configuration-language">https://ruudvanasseldonk.com/2024/a-reasonable-configuration-language</a>, See on <a href="https://news.ycombinator.com/item?id=39250320">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="content" itemscope=""><header><p>written by <br>published <time datetime="2024-02-04" itemprop="datePublished">4 February, 2024</time></p></header><p><span>About six months ago</span>, I was fed up with it. The particular <em>it</em> was <abbr>HCL</abbr> — Hashicorp Configuration Language — but that was just the trigger, it was hardly the only offender. The issue I was struggling with that day was to define six cloud storage buckets in Terraform. They were similar, but not quite identical. The kind of thing you’d do with <a href="https://github.com/ruuda/rcl/blob/bedbd3eea1129ba6053427d67b77a955240ceca8/examples/buckets.rcl#L9-L10">a two-line nested loop</a> in any general-purpose language, but where all the ways of achieving that in <abbr>HCL</abbr> were so much hassle, that is was far simpler to just copy-paste the config six times.</p><p>Although this <abbr>HCL</abbr> episode was the droplet, my bucket of frustration had been filling up for a long time:</p><ul><li><strong>GitHub Actions workflows</strong> that differ in only a few commands — there is no native way to abstract those. The same applies to jobs within a workflow.</li><li><strong>Kubernetes manifests</strong> that are 80% the same for most applications, and an entire industry that fails to adopt a proper solution for this, and instead resorts to templating yaml, which to me is <a href="https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell#templating-yaml-is-a-terrible-terrible-idea">very obviously very wrong on so many levels</a>.</li><li><strong>The prevalence of yaml in general</strong>, a format that does solve some problems (adding comments and a lighter syntax to json), but in the process introduces <a href="https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell">so many new problems</a> that the cure is almost as bad as the disease.</li><li><strong>Ansible playbooks</strong> that are too similar to justify duplicating, but different enough that parametrizing over data is insufficient. Related to this, the parameter data is difficult to share between Ansible and other tools.</li></ul><p>So that day, when I was in a particularly defiant mood, I decided to write my own configuration language. With list comprehensions. And types.</p><p><img alt="I’ll build my own configuration language. With list comprehensions. And types." src="https://ruudvanasseldonk.com/images/ill-build-my-own-configuration-language.png" width="1024" height="768"></p><p>I never expected or intended for it to go anywhere — it was just a way to vent. But six months later, <a href="https://github.com/ruuda/rcl"><em>Ruud’s Configuration Language</em></a> is no longer completely vaporware. I find it increasingly useful, and I think it might benefit others too. So let’s dive in!</p><h2 id="a-functional-foundation"><a href="#a-functional-foundation"></a>A functional foundation</h2><p>To be clear, I’m not criticizing the designers of Ansible or <abbr>HCL</abbr>. The limits of these tools are a natural consequence of their organic growth: you start out with a tool that needs simple configuration, adoption grows and people start doing more complex things with it, and suddenly you find yourself without a good way to do abstraction. So as a quick stopgap, you <a href="https://developer.hashicorp.com/terraform/language/v1.7.x/meta-arguments/for_each">bolt on</a> control flow <a href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_loops.html#standard-loops">encoded inside</a> the <a href="https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstrategymatrix">data format</a>, because that’s easy to do within the limits of the existing syntax.</p><p>When it comes to adding more principled abstraction features, the authors have a background in infrastructure administration, not in language design or type theory. So they accidentally implement <a href="https://developer.hashicorp.com/terraform/language/v1.7.x/functions/flatten">some functions</a> in an ad-hoc way that seemed helpful, but causes surprises down the line. (A <code>flatten</code> that <em>sometimes</em> flattens recursively can’t be typed properly, which breaks generic code.) Many of JavaScript and <abbr>PHP</abbr>’s idiosyncrasies can be explained in the same way.</p><p>The <a href="https://nixos.org/manual/nix/stable/language/index.html">Nix language</a> had a more solid foundation in functional programming from the start, which enables abstraction in a natural way. Even though it predates Terraform by more than a decade, the language has stood the test of time far better than <abbr>HCL</abbr>. With very few changes, it scaled to massive configuration repositories like <a href="https://github.com/NixOS/nixpkgs">Nixpkgs</a>, and although Nix has issues, abstracting repetition away is not one of them. I’ve used Nix to generate repetitive GitHub Actions workflows, and of course it is at the heart of NixOS, where it generates configuration files such as systemd units from a consistent declarative specification. This is the power of having few simple features that compose well.</p><p>Though Nix is great, I don’t think it is the answer to all configuration problems. Nix-the-language is intimately tied to Nix-the-package-manager and the Nix store, and the <abbr>ML</abbr>-style syntax can look foreign to people who are used to more mainstream languages. Still, Nix has many good ideas that have been proven to work, and my own configuration language is heavily inspired by it.</p><p>The other language that I take a lot of inspiration from is Python. Python is not primarily a functional language, but you can certainly use it in that way (avoid mutation, write pure functions, prefer list comprehensions over loops, etc.), and this is very natural. I find the syntax pleasant and readable: the meaning of idiomatic Python code is clear even to people who are not intimately familiar with the language. As a configuration language, Python is not bad! In fact, I’ve <em>also</em> used Python to generate repetitive GitHub Actions configurations. List and dict literals are very similar to json, and with functions, list comprehensions, and format strings, there is ample room to abstract repetitive configuration. Types can help to document and enforce structure.</p><p>But like Nix, I don’t think that Python is the answer to all configuration problems. A Python file is still primarily code, not data. You can have an entry point <code>json.dump</code> data to files or stdout, but it’s not always easy to import or evaluate intermediate pieces in isolation. Python’s module system is great for larger codebases, but less suitable for sharing pieces of data between many small scripts.</p><p>For my own language, I took the parts that I like about Nix: functional, more data than code, but with enough room to code when needed, and simple features that compose well. I took what I like about Python: the clean and familiar syntax, list comprehensions, format strings, and types. And consciously or unconsciously, I’m influenced by many more languages that I’ve been exposed to. Those ideas I combined into a language that <em>I</em> like working with.</p><h2 id="oh-no-yet-another-configuration-language"><a href="#oh-no-yet-another-configuration-language"></a>Oh no, yet another configuration language!</h2><p>I am not the first person to be frustrated by the lack of abstraction features in various tools, nor am I the first person to think that a configuration language would solve that. There exist more configuration languages than I can count on one hand already (see <a href="#appendix-a-non-exhaustive-list-of-configuration-languages">the appendix</a>), and probably many more that I’m not aware of. So why add one more to the mix? Why is <em>this one</em> going to <em>really</em> solve all our problems, when five more mature ones haven’t seen widespread adoption (yet)?</p><p>First of all, I did not start out writing my own language thinking it would be a viable alternative to existing configuration languages. I started it to vent, because I find it fun to work on, because it’s a good learning exercise, and because I can do things in exactly the way that <em>I</em> want to. Dhall has been around longer, has wider support, and a bigger community. But I don’t really like the syntax and the way it names some things. That’s a superficial complaint, and if I was looking for a tool to solve my configuration problems with the least amount of effort, then I can set my taste aside — I’ll get used to it. But for a personal project that I spend my free time on, I enjoy exploring ideas and building exactly the tool that <em>I</em> want to have.</p><p>So that’s how it started, as a toy project. I put a big vaporware warning on it, expecting that I would lose interest in it before it got to a point where it was useful. It’s certainly <a href="https://ruudvanasseldonk.com/2017/04/27/a-language-for-designing-slides">not the first time</a> that I’m writing a toy language that stalled, and maybe this one will meet the same fate. (I do still occasionally use Pris, and occasionally I get excited about adding features, but it’s mostly abandoned, like many of my side projects.) But then my tool started being useful. First in unexpected places (as a <code>jq</code> replacement, more on that below), and as I added features, in more places, to the point where now — despite its shortcomings — I would prefer it over some of the tools that I use at my day job.</p><p>So now what, is it a Serious Software Project now? No, it’s still a hobby project without stability promise. I don’t recommend using it for anything serious. But it’s also <em>useful</em> to the point where I expect I’ll keep it in my toolbelt for the forseeable future — if only as a <code>jq</code> replacement. And if it’s useful to me, maybe it’s useful to others, so that’s why I’m writing about it today.</p><h2 id="ruuds-configuration-language"><a href="#ruuds-configuration-language"></a>Ruud’s Configuration Language</h2><p>So what is this language? I call it <abbr>RCL</abbr>, named after myself in Bender meme style, but it turns out that <code>rcl</code> is a pretty good file extension and name for a command-line tool. If you prefer, it might stand for Reasonable Configuration Language, or, in classic <abbr>GNU</abbr> style, for <abbr>RCL</abbr> configuration language.</p><p>The language is a superset of json. This makes it easy to export data from many tools and incrementally upgrade it to <abbr>RCL</abbr>, including from yaml: just serialize it to json, and you’re good to go. This is a valid <abbr>RCL</abbr> document:</p><div id="cb1"><pre><code><span id="cb1-1"><span>{</span></span>
<span id="cb1-2">  <span>"buckets"</span><span>:</span> <span>[</span></span>
<span id="cb1-3">    <span>{</span></span>
<span id="cb1-4">      <span>"name"</span><span>:</span> <span>"bucket-0"</span><span>,</span></span>
<span id="cb1-5">      <span>"location"</span><span>:</span> <span>"eu-west1"</span><span>,</span></span>
<span id="cb1-6">      <span>"delete-after-seconds"</span><span>:</span> <span>86400</span></span>
<span id="cb1-7">    <span>}</span><span>,</span></span>
<span id="cb1-8">    <span>{</span></span>
<span id="cb1-9">      <span>"name"</span><span>:</span> <span>"bucket-1"</span><span>,</span></span>
<span id="cb1-10">      <span>"location"</span><span>:</span> <span>"eu-west1"</span><span>,</span></span>
<span id="cb1-11">      <span>"delete-after-seconds"</span><span>:</span> <span>86400</span></span>
<span id="cb1-12">    <span>}</span></span>
<span id="cb1-13">  <span>]</span></span>
<span id="cb1-14"><span>}</span></span></code></pre></div><p>It’s 2024, so <abbr>RCL</abbr> has some features that you might expect from a “modern” language: trailing commas and numeric underscores. Furthermore, dicts can be written with <code>ident = value</code> syntax to omit the quotes and reduce some line noise:</p><pre><code>{
  <span>buckets</span> = [
    {
      <span>name</span> = <span>"bucket-0"</span>,
      <span>location</span> = <span>"eu-west1"</span>,
      <span>delete-after-seconds</span> = <span>86_400</span>,
    },
    {
      <span>name</span> = <span>"bucket-1"</span>,
      <span>location</span> = <span>"eu-west1"</span>,
      <span>delete-after-seconds</span> = <span>86_400</span>,
    },
  ],
}</code></pre><p>There are arithmetic expressions as you would expect, list comprehensions, format strings, and functions:</p><pre><code>{
  <span>buckets</span> = [
    <span>for</span> <span>i</span> <span>in</span> <span>std</span>.<span>range</span>(<span>0</span>, <span>2</span>):
    {
      <span>name</span> = <span>f"bucket-</span><span>{</span><span>i</span><span>}</span><span>"</span>,
      <span>location</span> = <span>"eu-west1"</span>,
      <span>delete-after-seconds</span> = <span>24</span> <span>*</span> <span>3600</span>,
    },
  ],
}</code></pre><p>For validation, the <a href="https://docs.ruuda.nl/rcl/type_list/#key_by"><code>key_by</code></a> method is useful. In the above example, if we’d name the buckets by hand and there are many of them, how do we ensure that we don’t accidentally create two buckets with the same name? We can do that by building a mapping from name to bucket:</p><pre><code><span>let</span> <span>buckets</span> = [
  <span>// Omitted here for brevity, defined as before.</span>
];

<span>// Build a mapping of bucket name to bucket. If a key (bucket name)</span>
<span>// occurs multiple times, this will fail with an error that reports</span>
<span>// the offending key and the associated values. The type annotation</span>
<span>// is for clarification, it is not mandatory.</span>
<span>let</span> <span>buckets_by_name</span>: <span>Dict</span>[<span>String</span>, <span>Dynamic</span>] = <span>buckets</span>.<span>key_by</span>(<span>b</span> <span>=&gt;</span> <span>b</span>.<span>name</span>);

<span>// Constructing the mapping is enough for validation, the document still</span>
<span>// evaluates to the same dict as before. Note, the left "buckets" is the</span>
<span>// name of the field, the right "buckets" is a variable reference.</span>
{ <span>buckets</span> = <span>buckets</span> }
</code></pre><p>This is just a quick overview of some features. For a more thorough introduction, check out <a href="https://docs.ruuda.nl/rcl/tutorial/">the tutorial</a> and <a href="https://docs.ruuda.nl/rcl/syntax/">the syntax guide</a>.</p><p>An <abbr>RCL</abbr> document is always an expression, and you can evaluate it to a json document with the <code>rcl</code> command-line tool:</p><pre><code>rcl evaluate --output=json buckets.rcl</code></pre><p>The tool can also output in <abbr>RCL</abbr> syntax, which is a bit less noisy when inspecting data, and it’s a way to upgrade json documents to <abbr>RCL</abbr>. Aside from the standalone command-line tool, I also recently added a Python module that enables importing <abbr>RCL</abbr> documents in much the same way as <code>json.loads</code>.</p><p>Abstraction in a single document is nice, but the real power comes from <em>imports</em>. These allow you to break down configuration into small reusable pieces. Let’s say that all your cloud resources are in the same location. Then we might have a file <code>cloud_config.rcl</code>:</p><pre><code>{
  default_location = <span>"eu-west1"</span>,
}</code></pre><p>Then in <code>buckets.rcl</code>, we can use that like so:</p><pre><code><span>let</span> cloud_config = <span>import</span> <span>"cloud_config.rcl"</span>;
{
  <span>buckets</span> = [
    <span>for</span> <span>i</span> <span>in</span> <span>std</span>.<span>range</span>(<span>0</span>, <span>2</span>):
    {
      <span>name</span> = <span>f"bucket-</span><span>{</span><span>i</span><span>}</span><span>"</span>,
      <span>location</span> = cloud_config.default_location,
      <span>delete-after-seconds</span> = <span>24</span> <span>*</span> <span>3600</span>,
    },
  ],
}</code></pre><p>Because every document is an expression, you can always evaluate it and inspect it, even if it’s only an intermediate stage in a larger configuration. For more fine-grained inspection there is <a href="https://docs.ruuda.nl/rcl/syntax/#debug-tracing"><code>trace</code></a>, and with <a href="https://docs.ruuda.nl/rcl/rcl_query/"><code>rcl query</code></a> you can evaluate an expression against a document to drill down into it. For example, to look only at the first bucket:</p><pre><code>rcl query buckets.rcl 'input.buckets[0]'</code></pre><p>This feature is what made <abbr>RCL</abbr> useful for a use case that I did not anticipate: querying json documents.</p><h2 id="an-unexpected-jq-replacement"><a href="#an-unexpected-jq-replacement"></a>An unexpected jq replacement</h2><p>I use <a href="https://jqlang.github.io/jq/">jq</a> a lot. Most of the time, only to pretty-print a json document returned from some <abbr>API</abbr>. Because <abbr>RCL</abbr> is a superset of json, <code>rcl</code> can do that too now:</p><pre><code>curl --silent https://api.example.com | rcl evaluate</code></pre><p>By itself that is nothing special, the true power comes when querying. Jq features its own stream processing <abbr>DSL</abbr>, and for simple expressions I can usually remember the syntax — unpack the list, extract a few fields. But when it gets more complex, I’m at a loss. A while while ago, I was dealing with a json document that had roughly this structure:</p><div id="cb5"><pre><code><span id="cb5-1"><span>[</span></span>
<span id="cb5-2">  <span>{</span> <span>"name"</span><span>:</span> <span>"server-1"</span><span>,</span> <span>"tags"</span><span>:</span> <span>[</span><span>"amd"</span><span>,</span> <span>"fast"</span><span>]</span> <span>}</span><span>,</span></span>
<span id="cb5-3">  <span>{</span> <span>"name"</span><span>:</span> <span>"server-2"</span><span>,</span> <span>"tags"</span><span>:</span> <span>[</span><span>"intel"</span><span>,</span> <span>"slow"</span><span>]</span> <span>}</span><span>,</span></span>
<span id="cb5-4">  <span>{</span> <span>"name"</span><span>:</span> <span>"server-3"</span> <span>}</span><span>,</span></span>
<span id="cb5-5">  <span>{</span> <span>"name"</span><span>:</span> <span>"server-4"</span><span>,</span> <span>"tags"</span><span>:</span> <span>[</span><span>"amd"</span><span>,</span> <span>"vm"</span><span>,</span> <span>"slow"</span><span>]</span> <span>}</span></span>
<span id="cb5-6"><span>]</span></span></code></pre></div><p>I wanted to know the names of all the machines that had a particular tag applied. That the <code>tags</code> field is missing from some machines complicates that, and the real input consisted of hundreds of machines, so fixing that by hand was not feasible. I spent about 10 minutes struggling with <code>jq</code> and scrolling through unhelpful Stack Overflow answers. I did not think to try ChatGPT at the time, but in hindsight it <em>almost</em> gets the query right to a point where I could then get it working myself. But fundamentally, these kind of queries come up so infrequently that the things I learn about <code>jq</code> never really stick. ChatGPT is no excuse to tolerate bad tools: if the one-liner is easy to write, that’s still faster than leaving your terminal. At that point I remembered: I have a language in which this query is straightforward to express, and it can import json!</p><pre><code>$ rcl query --output=raw machines.json '[
  for m in input:
  if m.get("tags", []).contains("amd"):
  m.name
]'
server-1
server-4</code></pre><p>That’s how <abbr>RCL</abbr>, even though it is intended as a configuration language, became one of my most frequently used query languages.</p><h2 id="the-future-of-rcl"><a href="#the-future-of-rcl"></a>The future of <abbr>RCL</abbr></h2><p>That day when I was fed up with <abbr>HCL</abbr> and I ran <code>git init</code>, I didn’t expect to produce anything useful aside from entertaining myself for a few evenings. Now six months later, <abbr>RCL</abbr> is no longer vaporware, and it regularly solves real problems for me!</p><p>Some parts of <abbr>RCL</abbr> are already quite polished. It has mostly good error reporting, <a href="https://docs.ruuda.nl/rcl/">there is reference documentation</a>, it has an autoformatter, and it is very well tested with a suite of golden tests and fuzzers. Although I’m not sure at what point it starts being worth the complication of an additional tool, <abbr>RCL</abbr> can define cloud storage buckets today with <a href="https://developer.hashicorp.com/terraform/language/syntax/json">Terraform’s json syntax</a>. But <abbr>RCL</abbr> is also far from ready for prime time: there is no syntax highlighting for any editor aside from Vim, the type system is a work in progress, it doesn’t support floats yet, the Python module doesn’t expose errors nicely, the autoformatter has quirks, and I’m still ambivalent about whether there should be a <code>:</code> after <code>else</code>.</p><p>But most of all, I’m not sure whether I <em>want</em> <abbr>RCL</abbr> to experience prime time. Of course it is very gratifying to see your project be adopted and solve real-world problems for other people. I’m proud of what I built so far and I <em>want</em> people to see it and try it — that’s why I publish everything as free and open source software, and that’s why I’m writing this post. It always cheers me up when somebody who found one of my projects useful or interesting sends me an e-mail. But I also already experience a bit of maintainer fatigue from some of my successful Rust crates, and I don’t always spend the time on them that they deserve. When a project takes off, inevitably users start making requests, having opinions, and submitting well-intentioned but low-quality contributions. Keeping up with that takes time and mental energy. I like working on <abbr>RCL</abbr> right now, because I get to build it in exactly the way I want, and it solves exactly the problems that I have. Building a tool for the open source community would require making different trade-offs. For now, I’m treating it as a source-available project. It solves a need for me, and if others find it useful that’s great, but it is provided as-is. Maybe Haskell’s <em>avoid success at all cost</em> isn’t such a bad idea.</p><h2 id="appendix-a-non-exhaustive-list-of-configuration-languages"><a href="#appendix-a-non-exhaustive-list-of-configuration-languages"></a>Appendix: A non-exhaustive list of configuration languages</h2><p>Aside from Nix, Python, and <abbr>HCL</abbr>, which I’ve already discussed extensively, I am aware of the following configuation languages. For the ones that I’ve used or at least evaluated briefly, I added my personal impressions, but beware that these are very superficial.</p><p><a href="https://github.com/Azure/bicep"><strong>Bicep</strong></a> — Microsoft’s <abbr>DSL</abbr> for configuring Azure resources declaratively. I haven’t looked into it in much detail because I don’t work with Azure, but it looks potentially interesting.</p><p><a href="https://cuelang.org/"><strong>Cue</strong></a> — Out of all the configuration languages that I evaluated during a company hackathon, I found Cue to be the most promising one. Its type system is interesting: it helps to constrain and validate configuration (as you would expect from a type system), but it also plays a role in eliminating boilerplate. Like Nix, Cue is based on few simple constructs that compose well, and grounded in solid theory. It took me some time before it clicked, but when it did, Cue became really powerful. A few things I don’t like about it are the package/module system that has its roots in the Go ecosystem, and its string interpolation syntax which is hideous. The command-line tooling works but could be more polished, and I found it to become slow quickly, even for fairly small configurations. It has <a href="https://cuelang.org/docs/usecases/configuration/#comparisons">a page</a> comparing itself against a few other configuation languages.</p><p><a href="https://dhall-lang.org/"><strong>Dhall</strong></a> — This is the first configuration language that I learned about many years ago. From what I can tell, it is one of the most mature and widely supported configuration languages. I use <a href="https://github.com/purescript/spago">Spago</a>, the PureScript package manager, in some of my projects, and it uses Dhall as its configuration format. Unfortunately it looks like it is being <a href="https://github.com/purescript/spago/tree/bbe37b6cd497aa544bd0761fa7a56a5f5d002a87#migrate-from-spagodhall-to-spagoyaml">deprecated</a> in favor of yaml. I tried to use Dhall once to solve an Advent of Code challenge, but got stuck immediately because it’s not possible to split strings in Dhall. Of course, this is an unfair test to evaluate a configuration language on, but it does give an impression of the expressivity of a language. I’ve used Nix to <a href="https://github.com/ruuda/adventofcode/blob/c452562c72cdd203df4dd0fd631596e6c0e2aa13/2022/03/main.nix">solve</a> a few Advent of Code challenges in the past, and this year I <a href="https://github.com/ruuda/adventofcode/blob/c452562c72cdd203df4dd0fd631596e6c0e2aa13/2023/11/main.rcl">solved</a> a few in <abbr>RCL</abbr>, which went pretty well for small inputs, but the lack of unbounded loops and tail calls make it unsuitable as a general-purpose language. Although I used to work as a Haskell developer, the formatting and names of built-in functions in Dhall look awkward to me.</p><p><a href="https://json-e.js.org/"><strong><abbr>JSON</abbr>-e</strong></a> — A json parametrization language. I discovered this one in Rimu’s list of related projects. I think I’ve seen it mentioned a few times before, but I haven’t evaluated it at all.</p><p><a href="https://jsonnet.org/"><strong>Jsonnet</strong></a> — I never properly evaluated Jsonnet, but probably I should. Superficially it looks like one of the more mature formats, and in many ways it looks similar to <abbr>RCL</abbr>. Its has <a href="https://jsonnet.org/articles/comparisons.html">a page</a> comparing itself against other configuration languages.</p><p><a href="https://kcl-lang.io/"><strong><abbr>KCL</abbr></strong></a> — This is an odd one. From the website and repository it looks like a lot of resources went into this project, but somehow I’ve never seen it come up or be used anywhere. I only learned about it when I started searching for configuration languages. From the way it describes itself, it sounds like the tool I want, but I am generally wary of tools that use lots of buzzwords, especially when it involves the words “modern” and “cloud native”. I should evaluate it properly at some point. It has <a href="https://kcl-lang.io/docs/0.6.0/user_docs/getting-started/intro/#how-to-choose">a page</a> comparing itself against other configuation languages.</p><p><a href="https://nickel-lang.org/"><strong>Nickel</strong></a> — <a href="https://www.tweag.io/blog/2020-10-22-nickel-open-sourcing/">An attempt to create a language similar to Nix</a>, but without being tied to the package manager and Nix store. It looked very promising to me, but after evaluating it during a company hackathon, I found it difficult or impossible to express sanity checks that I can easily express in Cue and <abbr>RCL</abbr>. Its has <a href="https://github.com/tweag/nickel/blob/6cf2902d3db768618e1d990c549671e308dd3ff4/RATIONALE.md#comparison-with-alternatives">a page</a> comparing itself against other configuation languages.</p><p><a href="https://pkl-lang.org/"><strong>Pkl</strong></a> — A configuration language by Apple. The timing is eerie: I wrote this post on a Saturday with the intention of proofreading and publishing it the next day, and right that Sunday morning, <a href="https://pkl-lang.org/blog/introducing-pkl.html">the Pkl announcement post</a> was on the Hacker News frontpage. From the comments, it <a href="https://news.ycombinator.com/item?id=39248081">has been in use</a> at Apple internally for a few years already. I haven’t had the opportunity to evaluate it yet. Its has <a href="https://pkl-lang.org/main/current/introduction/comparison.html">a page</a> comparing itself against other configuation languages, but only superficially.</p><p><a href="https://www.pulumi.com/"><strong>Pulumi</strong></a> — Not a configuration language, but an infrastructure automation tool like Terraform. It can be configured using existing general-purpose programming languages. I haven’t had the opportunity to try it, but I suppose I don’t get to complain about <abbr>HCL</abbr> without at least acknowledging Pulumi’s existence.</p><p><a href="https://rimu.dev/"><strong>Rimu</strong></a> — I stumbled upon this one recently while browsing <a href="https://github.com/topics/configuration-language">the configuration-language tag</a> on GitHub. It might be an eerie case of <a href="https://en.wikipedia.org/wiki/Multiple_discovery">parallel discovery</a>: like <abbr>RCL</abbr>, it looks like a configuration language developed as a side project, written in Rust, started in August 2023, and not ready for serious use yet. Unlike <abbr>RCL</abbr>, its syntax is based on yaml.</p><p><a href="https://bazel.build/rules/language"><strong>Starlark</strong></a> — A Python dialect used by the Bazel build tool. I used it intensively when I was working with Blaze/Bazel, and it works well for defining build targets. Starlark has multiple implementations, including <a href="https://github.com/facebookexperimental/starlark-rust">one in Rust</a> that can be used as a standalone command-line tool, but all the implementations clearly focus on being embedded. From my limited attempts to use them in an infrastructure-as-code repository, they are not suitable for incremental adoption there.</p><p><a href="https://www.typescriptlang.org/"><strong>TypeScript</strong></a> — Not a configuration language, but it deserves a mention here, because <abbr>RCL</abbr> intends to be json with abstraction and types, and since TypeScript is a superset of JavaScript, which is a superset of json, it falls in the same category of tools that can type and abstract json. I haven’t used TypeScript enough to have a strong opinion on its type system. Possibly <abbr>RCL</abbr>’s type system will end up being similar.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Inuit Parents Teach Kids To Control Their Anger (2019) (130 pts)]]></title>
            <link>https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger</link>
            <guid>39250304</guid>
            <pubDate>Sun, 04 Feb 2024 14:10:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger">https://www.npr.org/sections/goatsandsoda/2019/03/13/685533353/a-playful-way-to-teach-kids-to-control-their-anger</a>, See on <a href="https://news.ycombinator.com/item?id=39250304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="res702589837">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-1_custom-98daf4166412002168cd2b2024ad671195d512f2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                For more than 30 years, the Inuit welcomed anthropologist Jean Briggs into their lives so she could study how they raise their children. Briggs is pictured during a 1974 visit to Baffin Island.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Back in the 1960s, a Harvard graduate student made a landmark discovery about the nature of human anger.</p>   <p>At age 34, Jean Briggs traveled above the Arctic Circle and lived out on the tundra for 17 months. There were no roads, no heating systems, no grocery stores. Winter temperatures could easily dip below minus 40 degrees Fahrenheit.</p>   <p>Briggs persuaded an Inuit family to "adopt" her and "try to keep her alive," as the anthropologist <a href="https://link.springer.com/article/10.1007%2FBF02805482">wrote </a>in 1970.</p>   <div id="res701118871">
                  <p>This story is part of a series from NPR's Science desk called <strong><a href="https://www.npr.org/series/688838187/the-other-side-of-anger">The Other Side of Anger.</a></strong> There's no question we are in angry times. It's in our politics, our schools and homes. Anger can be a destructive emotion, but it can also be a positive force.</p>         <p>Join NPR in our exploration of anger and what we can learn from this powerful emotion. <strong><a href="https://www.npr.org/series/688838187/the-other-side-of-anger">Read and listen to stories in the series here. </a></strong></p>
      </div>
   
<!-- END ID="RES701118871" CLASS="BUCKETWRAP LISTTEXT" -->
   <p>At the time, many Inuit families lived similar to the way their ancestors had for thousands of years. They built igloos in the winter and tents in the summer. "And we ate only what the animals provided, such as fish, seal and caribou," says <a href="https://www.eagle-eye.com/Myna-Ishulutak">Myna Ishulutak</a>, a film producer and language teacher who lived a similar lifestyle as a young girl.</p>   <p>Briggs quickly realized something remarkable was going on in these families: The adults had an extraordinary ability to control their anger.</p>   
   <p>"They never acted in anger toward me, although they were angry with me an awful lot," Briggs told the Canadian Broadcasting Corp. in an interview.</p>   <div id="res702588179">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-2_custom-16992df4f27f318b95e4953adab7977a35df8b0d-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Myna Ishulutak (upper right, in blue jacket) lived a seminomadic life as a child. Above: photos of the girl and her family in the hunting camp of Qipisa during the summer of 1974.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Even just showing a smidgen of frustration or irritation was considered weak and childlike, Briggs observed.</p>   <p>For instance, one time someone knocked a boiling pot of tea across the igloo, damaging the ice floor. No one changed their expression. "Too bad," the offender said calmly and went to refill the teapot.</p>   <p>In another instance, a fishing line — which had taken days to braid — immediately broke on the first use. No one flinched in anger. "Sew it together," someone said quietly.</p>   <p>By contrast, Briggs seemed like a wild child, even though she was trying very hard to control her anger. "My ways were so much cruder, less considerate and more impulsive," she told the CBC. "[I was] often impulsive in an antisocial sort of way. I would sulk or I would snap or I would do something that they never did."</p>   <p>Briggs, who died in 2016, wrote up her observations in <a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674608283">her first book</a>, <em>Never in Anger</em>. But she was left with a lingering question: How do Inuit parents instill this ability in their children? How do Inuit take tantrum-prone toddlers and turn them into cool-headed adults?</p>   
   <p>Then in 1971, Briggs found a clue.</p>   
   
<!-- END ID="RES702773512" CLASS="BUCKETWRAP INTERNALLINK MEDIAPROMO PRIMARY" -->
   
   
<!-- END ID="RES702766744" CLASS="BUCKETWRAP INTERNALLINK MEDIAPROMO PRIMARY" -->
   <p>She was walking on a stony beach in the Arctic when she saw a young mother playing with her toddler — a little boy about 2 years old. The mom picked up a pebble and said, "'Hit me! Go on. Hit me harder,'" Briggs remembered.</p>   <p>The boy threw the rock at his mother, and she exclaimed, "Ooooww. That hurts!"</p>   <p>Briggs was completely befuddled. The mom seemed to be teaching the child the opposite of what parents want. And her actions seemed to contradict everything Briggs knew about Inuit culture.</p>   <p>"I thought, 'What is going on here?' " Briggs said in the radio interview.</p>   <p>Turns out, the mom was executing a powerful parenting tool to teach her child how to control his anger — and one of the most intriguing parenting strategies I've come across.</p>   <div id="res702586950">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Iqaluit, pictured in winter, is the capital of the Canadian territory of Nunavut.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-3_slide-d5b0b5e79eb7bf76a96aeba2952409b50c2dbcae-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Iqaluit, pictured in winter, is the capital of the Canadian territory of Nunavut.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <h3><strong>No scolding, no timeouts</strong></h3>   <p>It's early December in the Arctic town of Iqaluit, Canada. And at 2 p.m., the sun is already calling it a day. Outside, the temperature is a balmy minus 10 degrees Fahrenheit. A light snow is swirling.</p>   <p>I've come to this seaside town, after reading Briggs' book, in search of parenting wisdom, especially when it comes to teaching children to control their emotions. Right off the plane, I start collecting data.</p>   <p>I sit with elders in their 80s and 90s while they lunch on "country food" —stewed seal, frozen beluga whale and raw caribou. I talk with moms selling hand-sewn sealskin jackets at a high school craft fair. And I attend a parenting class, where day care instructors learn how their ancestors raised small children hundreds — perhaps even thousands — of years ago.</p>   <div id="res702586420">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                The elders of Iqaluit have lunch at the local senior center. On Thursdays, what they call "country food" is on the menu, things like caribou, seal and ptarmigan.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-4_slide-9d750168061b4c6b5a64e371e5c69d8e6b96b0a2-s1200.jpg">
        </picture>
    </div>
<div>
        <p>The elders of Iqaluit have lunch at the local senior center. On Thursdays, what they call "country food" is on the menu, things like caribou, seal and ptarmigan.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Across the board, all the moms mention one golden rule: Don't shout or yell at small children.</p>   <p>Traditional Inuit parenting is incredibly nurturing and tender. If you took all the parenting styles around the world and ranked them by their gentleness, the Inuit approach would likely rank near the top.<strong> </strong>(They even have a special kiss for babies, where you put your nose against the cheek and sniff the skin.)</p>   <p>The culture views scolding — or even speaking to children in an angry voice — as inappropriate, says Lisa Ipeelie, a radio producer and mom who grew up with 12 siblings. "When they're little, it doesn't help to raise your voice," she says. "It will just make your own heart rate go up."</p>   
   <p>Even if the child hits you or bites you, there's no raising your voice?</p>   <p>"No," Ipeelie says with a giggle that seems to emphasize how silly my question is. "With little kids, you often think they're pushing your buttons, but that's not what's going on. They're upset about something, and you have to figure out what it is."</p>   <div id="res702585934">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-5_sq-02bd491594824b2269ba4bc1832d23cde6593bcd-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Traditionally, the women and children in the community eat with an ulu knife.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>Traditionally, the Inuit saw yelling at a small child as demeaning. It's as if the adult is having a tantrum; it's basically stooping to the level of the child, Briggs documented.</p>   <p>Elders I spoke with say intense colonization over the past century is damaging these traditions. And, so, the community is working hard to keep the parenting approach intact.</p>   <p>Goota Jaw is at the front line of this effort. She teaches the parenting class at the <a href="https://www.arcticcollege.ca/programs">Arctic College</a>. Her own parenting style is so gentle that she doesn't even believe in giving a child a timeout for misbehaving.</p>   <p>"Shouting, 'Think about what you just did. Go to your room!' " Jaw says. "I disagree with that. That's not how we teach our children. Instead you are just teaching children to run away."</p>   <p>And you are teaching them to be angry, says clinical psychologist and author Laura Markham. "When we yell at a child — or even threaten with something like 'I'm starting to get angry,' we're training the child to yell," says <a href="https://www.psychologytoday.com/us/experts/laura-markham-phd">Markham</a>. "We're training them to yell when they get upset and that yelling solves problems."</p>   <p>In contrast, parents who control their own anger are helping their children learn to do the same, Markham says. "Kids learn emotional regulation from us."</p>   <p>I asked Markham if the Inuit's no-yelling policy might be their first secret of raising cool-headed kids. "Absolutely," she says.</p>   <h3><strong>Playing soccer with your head</strong></h3>   <p>Now at some level, all moms and dads know they shouldn't yell at kids. But if you don't scold or talk in an angry tone, how do you discipline? How do you keep your 3-year-old from running into the road? Or punching her big brother?</p>   
   <p>For thousands of years, the Inuit have relied on an ancient tool with an ingenious twist: "We use storytelling to discipline," Jaw says.</p>   
   
<!-- END ID="RES703045640" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Jaw isn't talking about fairy tales, where a child needs to decipher the moral. These are oral stories passed down from one generation of Inuit to the next, designed to sculpt kids' behaviors in the moment.<strong> </strong>Sometimes even save their lives.</p>   <p>For example, how do you teach kids to stay away from the ocean, where they could easily drown? Instead of yelling, "Don't go near the water!" Jaw says Inuit parents take a pre-emptive approach and tell kids a special story about what's inside the water. "It's the sea monster," Jaw says, with a giant pouch on its back just for little kids.</p>   <p>"If a child walks too close to the water, the monster will put you in his pouch, drag you down to the ocean and adopt you out to another family," Jaw says.</p>   <p>"Then we don't need to yell at a child," Jaw says, "because she is already getting the message."</p>   <p>Inuit parents have an array of stories to help children learn respectful behavior, too. For example, to get kids to listen to their parents, there is a story about ear wax, says film producer Myna Ishulutak.</p>   <p>"My parents would check inside our ears, and if there was too much wax in there, it meant we were not listening," she says.</p>   <p>And parents tell their kids: If you don't ask before taking food, long fingers could reach out and grab you, Ishulutak says.</p>   <div id="res702594556">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Inuit parents tell their children to beware of the northern lights. If you don't wear your hat in the winter, they'll say, the lights will come, take your head and use it as a soccer ball!
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-7_slide-c115aca35f3cc49b0210923b65890d26e5cea6e2-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Inuit parents tell their children to beware of the northern lights. If you don't wear your hat in the winter, they'll say, the lights will come, take your head and use it as a soccer ball!</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Then there's the story of northern lights, which helps kids learn to keep their hats on in the winter.</p>   <p>"Our parents told us that if we went out without a hat, the northern lights are going to take your head off and use it as a soccer ball," Ishulutak says. "We used to be so scared!" she exclaims and then erupts in laughter.</p>   <p>At first, these stories seemed to me a bit too scary for little children. And my knee-jerk reaction was to dismiss them. But my opinion flipped 180 degrees after I watched my own daughter's response to similar tales — and after I learned more about humanity's intricate relationship with storytelling.</p>   
   <p>Oral storytelling is what's known as a human universal. For tens of thousands of years, it has been a key way that parents teach children about values and how to behave.</p>   <p>Modern hunter-gatherer groups use stories to teach sharing, respect for both genders and conflict avoidance, a recent study <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5717173">reported</a>, after analyzing 89 stories from nine different tribes in Southeast Asia and Africa. With the Agta, a hunter-gatherer population of the Philippines, good storytelling skills are prized more than hunting skills or medicinal knowledge, the study found.</p>   <p>Today many American parents outsource their oral storytelling to screens. And in doing so, I wonder if we're missing out on an easy — and effective — way of disciplining and changing behavior. Could small children be somehow "wired" to learn through stories?</p>   <div id="res702585548">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-6_sq-666182fa4ed5783e7a921c52d760ba87d71746b5-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Inuit parenting is gentle and tender. They even have a special kiss for kids called <em>kunik</em>. (Above) Maata Jaw gives her daughter the nose-to-cheek Inuit sniff.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>"Well, I'd say kids learn well through narrative and explanations," says psychologist <a href="http://starlabkids.org/deena_weisberg/">Deena Weisberg</a> at Villanova University, who studies how small children interpret fiction. "We learn best through things that are interesting to us. And stories, by their nature, can have lots of things in them that are much more interesting in a way that bare<strong> </strong>statements don't."</p>   <p>Stories with a dash of danger pull in kids like magnets, Weisberg says. And they turn a tension-ridden activity like disciplining into a playful interaction that's — dare, I say it — fun.</p>   <p>"Don't discount the playfulness of storytelling," Weisberg says. "With stories, kids get to see stuff happen that doesn't really happen in real life. Kids think that's fun. Adults think it's fun, too."</p>   <h3><strong>Why don't you hit me?</strong></h3>   <div id="res702583935">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-8_custom-22e7fc39c60f5c1ef1c8321e5dd7c0cbc7153120-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Inuit filmmaker and language teacher Myna Ishulutak as a little girl. Anthropologist Jean Briggs spent six months with the family in the 1970s documenting the child's upbringing.
                <b aria-label="Image credit">
                    
                    Jean Briggs Collection / American Philosophical Society
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Jean Briggs Collection / American Philosophical Society
        
    </span>
</p></div>
   </div>
   <p>Back up in Iqaluit, Myna Ishulutak is reminiscing about her childhood out on the land. She and her family lived in a hunting camp with about 60 other people. When she was a teenager, her family settled in a town.</p>   <p>"I miss living on the land so much," she says as we eat a dinner of baked Arctic char. "We lived in a sod house. And when we woke up in the morning, everything would be frozen until we lit the oil lamp."</p>   
   <p>I ask her if she's familiar with the work of Jean Briggs. Her answer leaves me speechless.</p>   <p>Ishulutak reaches into her purse and brings out Briggs' second book, <em>Inuit Morality Play, </em>which details the life of a 3-year-old girl dubbed Chubby Maata.</p>   <p>"This book is about me and my family," Ishulutak says. "I am Chubby Maata."</p>   <p>In the early 1970s, when Ishulutak was about 3 years old, her family welcomed Briggs into their home for six months and allowed her to study the intimate details of their child's day-to-day life.</p>   <div id="res702582321">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-9_sq-c7fb56becd0eb0ddb508343cff62e820a1e6c14b-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Myna Ishulutak today in Iqaluit, Canada. As the mother of two grown boys, she says, "When you're shouting at them all the time they tend to kind of block you. So there's a saying: 'Never shout at them.' "
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
   </div>
   <p>What Briggs documented is a central component to raising cool-headed kids.</p>   <p>When a child in the camp acted in anger — hit someone or had a tantrum — there was no punishment. Instead, the parents waited for the child to calm down and then, in a peaceful moment, did something that Shakespeare would understand all too well: They put on a drama. (As the Bard once wrote, "the play's the thing wherein I'll catch the conscience of the king.")</p>   <p>"The idea is to give the child experiences that will lead the child to develop rational thinking," Briggs told the CBC in 2011.</p>   <p>In a nutshell, the parent would act out what happened when the child misbehaved, including the real-life consequences of that behavior.</p>   <p>The parent always had a playful, fun tone. And typically the performance starts with a question, tempting the child to misbehave.</p>   <p>For example, if the child is hitting others, the mom may start a drama by asking: "Why don't you hit me?"</p>   <p>Then the child has to think: "What should I do?" If the child takes the bait and hits the mom, she doesn't scold or yell but instead acts out the consequences. "Ow, that hurts!" she might exclaim.</p>   <p>The mom continues to emphasize the consequences by asking a follow-up question. For example: "Don't you like me?" or "Are you a baby?" She is getting across the idea that hitting hurts people's feelings, and "big girls" wouldn't hit. But, again, all questions are asked with a hint of playfulness.</p>   
   <p>The parent repeats the drama from time to time until the child stops hitting the mom during the dramas and the misbehavior ends.</p>   <p>Ishulutak says these dramas teach children not to be provoked easily. "They teach you to be strong emotionally," she says, "to not take everything so seriously or to be scared of teasing."</p>   <p>Psychologist <a href="http://www.psychology.illinois.edu/people/pjm">Peggy Miller</a>, at the University of Illinois, agrees: "When you're little, you learn that people will provoke you, and these dramas teach you to think and maintain some equilibrium."</p>   <p>In other words, the dramas offer kids a chance to <em>practice </em>controlling their anger, Miller says, during times when they're not actually angry.</p>   <p>This practice is likely critical for children learning to control their anger. Because here's the thing about anger: Once someone is already angry, it is not easy for that person to squelch it — even for adults.</p>   <p>"When you try to control or change your emotions in the moment, that's a really hard thing to do," says <a href="https://lisafeldmanbarrett.com/">Lisa Feldman Barrett</a>, a psychologist at Northeastern University who studies how emotions work.</p>   <p>But if you <em>practice </em>having a different response or a different emotion at times when you're not angry, you'll have a better chance of managing your anger in those hot-button moments, Feldman Barrett says.</p>   <p>"That practice is essentially helping to rewire your brain to be able to make a different emotion [besides anger] much more easily," she says.</p>   <p>This emotional practice may be even more important for children, says psychologist Markham, because kids' brains are still developing the circuitry needed for self-control.</p>   <p>"Children have all kinds of big emotions," she says. "They don't have much prefrontal cortex yet. So what we do in responding to our child's emotions shapes their brain."</p>   <div id="res702582120">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                A lot has changed in the Arctic since the Canadian government forced Inuit families to settle in towns. But the community is trying to preserve traditional parenting practices.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-10-b2b0e95c1ebc2a8744b0694bc533ed6fb8e55bec-s1200.jpg">
        </picture>
    </div>
<div>
        <p>A lot has changed in the Arctic since the Canadian government forced Inuit families to settle in towns. But the community is trying to preserve traditional parenting practices.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <p>Markham recommends an approach close to that used by Inuit parents. When the kid misbehaves, she suggests, wait until everyone is calm. Then in a peaceful moment, go over what happened with the child. You can simply tell them the story about what occurred or use two stuffed animals to act it out.</p>   <p>"Those approaches develop self-control," Markham says.</p>   <p>Just be sure you do two things when you replay the misbehavior, she says. First, keep the child involved by asking many questions. For example, if the child has a hitting problem, you might stop midway through the puppet show and ask,"Bobby, wants to hit right now. Should he?"</p>   
   <p>Second, be sure to keep it fun. Many parents overlook play as a tool for discipline, Markham says. But fantasy play offers oodles of opportunities to teach children proper behavior.</p>   <p>"Play is their work," Markham says. "That's how they learn about the world and about their experiences."</p>   <p>Which seems to be something the Inuit have known for hundreds, perhaps even, thousands of years.</p>   <div id="res702581804">
            <div data-crop-type="">
        <picture>
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s400-c85.webp 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s800-c85.webp 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1000-c85.webp 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1300-c85.webp 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1600-c85.webp 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2000-c85.webp 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2600-c85.webp 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s400-c85.jpg 400w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s800-c85.jpg 800w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1000-c85.jpg 1000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1300-c85.jpg 1300w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1600-c85.jpg 1600w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2000-c85.jpg 2000w,
https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s2600-c85.jpg 2600w" sizes="(min-width: 1300px) 1238px, (min-width: 1025px) calc(100vw - 60px), (min-width: 768px) calc(100vw - 60px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1100-c50.jpg" alt="" loading="lazy">
        </picture>
        
</div>
<div>
    <div>
        <p>
                Inuit parents value the playful side of kids even when disciplining them. Above: Maata Jaw and daughter.
                <b aria-label="Image credit">
                    
                    Johan Hallberg-Campbell for NPR
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Johan Hallberg-Campbell for NPR
        
    </span>
</p></div>
<div>
        <picture>
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.webp" type="image/webp">
            <source data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg" type="image/jpeg">
            <img data-original="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg" alt="" src="https://media.npr.org/assets/img/2019/03/12/inuit-anger-11-cde85d0d6494553eee42114b003cb3b80f0858dd-s1200.jpg">
        </picture>
    </div>
<div>
        <p>Inuit parents value the playful side of kids even when disciplining them. Above: Maata Jaw and daughter.</p>
        <p><span aria-label="Image credit">
            
            Johan Hallberg-Campbell for NPR
            
        </span>
    </p></div>
   </div>
   <h3><strong>Share Your Tips</strong></h3>   <p><em>How do you get your kids to do things without yelling or shouting? Or, how did your parents get you to do things without yelling or scolding? Share your advice, tips and stories, and we may include them in a story for NPR.</em></p>   <p><strong>This submission form is now closed. </strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Linux glibc flaw lets attackers get root on major distros (151 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/</link>
            <guid>39250076</guid>
            <pubDate>Sun, 04 Feb 2024 13:35:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/">https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/</a>, See on <a href="https://news.ycombinator.com/item?id=39250076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="Linux" height="900" src="https://www.bleepstatic.com/content/hl-images/2023/06/22/Linux.jpg" width="1600"></p>
<p>​Unprivileged attackers can get root access on multiple major Linux distributions in default configurations by exploiting a newly disclosed local privilege escalation (LPE) vulnerability in the GNU C Library (glibc).</p>
<p>Tracked as <a href="https://www.qualys.com/2024/01/30/cve-2023-6246/syslog.txt" target="_blank" rel="nofollow noopener">CVE-2023-6246</a>, this security flaw was found in glibc's __vsyslog_internal() function, called by the widely-used syslog and vsyslog functions for writing messages to the system message logger.</p>
<p>The bug is due to a <a href="https://cwe.mitre.org/data/definitions/122.html" target="_blank" rel="nofollow noopener">heap-based buffer overflow weakness</a> accidentally introduced in glibc 2.37 in August 2022 and later backported to glibc 2.36 when addressing a less severe vulnerability tracked as CVE-2022-39046.</p>
<p>"The buffer overflow issue poses a significant threat as it could allow local privilege escalation, enabling an unprivileged user to gain full root access through crafted inputs to applications that employ these logging functions," Qualys security researchers said.</p>
<p>"Although the vulnerability requires specific conditions to be exploited (such as an unusually long argv[0] or openlog() ident argument), its impact is significant due to the widespread use of the affected library."</p>
<h2>Impacts Debian, Ubuntu, and Fedora systems</h2>
<p>While testing their findings, Qualys confirmed that Debian 12 and 13, Ubuntu 23.04 and 23.10, and Fedora 37 to 39 were all vulnerable to CVE-2023-6246 exploits, allowing any unprivileged user to escalate privileges to full root access on default installations.</p>
<p>Although their tests were limited to a handful of distros, the researchers added that "other distributions are probably also exploitable."</p>
<p>While analyzing glibc for other potential security issues, the researchers also found three other vulnerabilities, two of them—harder to exploit—in the __vsyslog_internal() function (CVE-2023-6779 and CVE-2023-6780) and a third one (a <a href="https://www.qualys.com/2024/01/30/qsort.txt" target="_blank" rel="nofollow noopener">memory corruption issue</a> still waiting for a CVEID) in glibc's qsort () function.</p>
<p>"The recent discovery of these vulnerabilities is not just a technical concern but a matter of widespread security implications,"&nbsp;<a href="https://blog.qualys.com/vulnerabilities-threat-research/2024/01/30/qualys-tru-discovers-important-vulnerabilities-in-gnu-c-librarys-syslog" target="_blank" rel="nofollow noopener">said</a> Saeed Abbasi, Product Manager at Qualys' Threat Research Unit.</p>
<p>"These flaws highlight the critical need for strict security measures in software development, especially for core libraries widely used across many systems and applications."</p>
<h2>Other Linux root escalation flaws found by Qualys</h2>
<p>Over the past few years, researchers at Qualys have found several other Linux security vulnerabilities that can let attackers gain complete control over unpatched Linux systems, even in default configurations.</p>
<p>Vulnerabilities they discovered include a flaw in glibc's ld.so dynamic loader (<a href="https://www.bleepingcomputer.com/news/security/new-looney-tunables-linux-bug-gives-root-on-major-distros/" target="_blank">Looney Tunables</a>), one in Polkit's pkexec component (<a href="https://www.bleepingcomputer.com/news/security/linux-system-service-bug-gives-root-on-all-major-distros-exploit-released/" target="_blank">dubbed PwnKit</a>), another in the Kernel's filesystem layer (<a href="https://www.bleepingcomputer.com/news/security/new-linux-kernel-bug-lets-you-get-root-on-most-modern-distros/" target="_blank">dubbed Sequoia</a>), and in the Sudo Unix program (aka <a href="https://www.bleepingcomputer.com/news/security/new-linux-sudo-flaw-lets-local-users-gain-root-privileges/" target="_blank">Baron Samedit</a>).</p>
<p>Days after the Looney Tunables flaw (<a href="https://access.redhat.com/security/cve/cve-2023-4911" target="_blank" rel="nofollow noopener">CVE-2023-4911</a>) was disclosed, proof-of-concept (PoC) exploits were <a href="https://www.bleepingcomputer.com/news/security/exploits-released-for-linux-flaw-giving-root-on-major-distros/" target="_blank">published online</a>, and threat actors <a href="https://www.bleepingcomputer.com/news/security/hackers-exploit-looney-tunables-linux-bug-steal-cloud-creds/" target="_blank">started exploiting it</a> one month later to steal cloud service provider (CSP) credentials in Kinsing malware attacks.</p>
<p>The Kinsing gang is known for deploying cryptocurrency mining malware on compromised cloud-based systems, including Kubernetes, Docker APIs, Redis, and Jenkins servers.</p>
<p>CISA later <a href="https://www.bleepingcomputer.com/news/security/cisa-orders-federal-agencies-to-patch-looney-tunables-linux-bug/" target="_blank">ordered U.S. federal agencies</a> to secure their Linux systems against CVE-2023-4911 attacks after adding it to its catalog of actively exploited bugs and tagging it as posing "significant risks to the federal enterprise."</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Acme Klein Bottle (122 pts)]]></title>
            <link>https://www.kleinbottle.com/</link>
            <guid>39249938</guid>
            <pubDate>Sun, 04 Feb 2024 13:15:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kleinbottle.com/">https://www.kleinbottle.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39249938">Hacker News</a></p>
Couldn't get https://www.kleinbottle.com/: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>