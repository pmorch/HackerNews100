<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 08 Jul 2023 23:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why Use Pascal? (176 pts)]]></title>
            <link>https://castle-engine.io/why_pascal</link>
            <guid>36646890</guid>
            <pubDate>Sat, 08 Jul 2023 17:54:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://castle-engine.io/why_pascal">https://castle-engine.io/why_pascal</a>, See on <a href="https://news.ycombinator.com/item?id=36646890">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h3 id="_modern_clean_language_to_develop_maintainable_applications">2.1. Modern clean language to develop maintainable applications</h3>
<div>
<ul>
<li>
<p>Object Pascal is a modern programming language. It supports classes, units, properties, generics, interfaces, reflection, closures…​ Everything you expect from a modern OOP language.</p>
</li>
<li>
<p>The syntax puts emphasis on readable code.</p>
</li>
<li>
<p>The language is type-safe. E.g. special types for booleans, strings, chars, sets, enums, ranges. Type conversions are either really safe, or have to be done explicitly.</p>
</li>
<li>
<p>There are additional run-time checks, e.g. array range checking, integer overflow checking, assertions, memory leak checking. Notes:</p>
<div>
<ul>
<li>
<p>You can turn off these checks in <em>release</em> version, but use them in <em>debug</em>. When compiling using CGE build tool / editor, we have debug / release modes that automatically do this for you.</p>
</li>
<li>
<p><a href="https://github.com/michaliskambi/modern-pascal-introduction/wiki/What-are-range-and-overflow-checks-(and-errors)-in-Pascal">What are range and overflow checks (and errors) in Pascal</a></p>
</li>
<li>
<p><a href="https://castle-engine.io/detecting_memory_leaks_using_heaptrc">Detecting Memory Leaks</a></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_fast">2.2. Fast</h3>
<div>
<ul>
<li>
<p>It is compiled to a native code and so is fast <em>"out of the box"</em>. There’s seldom any need to do low-level optimizations.</p>
</li>
<li>
<p>But if you need to, language can be as low-level as you want. E.g. you can use pointers, do pointer math, write OS and CPU-specific code, even add pieces in assembly. You can work on the same level as C or C++ does.</p>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
But you will probably not need to get too "low level" in usual applications. E.g. <a href="https://castle-engine.io/">Castle Game Engine</a> has <strong>zero assembler code</strong> to maximize portability and code readability and we’re still fast.
</td>
</tr>
</tbody></table>
</div>
</li>
<li>
<p>Compilation is also fast.</p>
<div>
<p><img src="https://castle-engine.io/images/not_resized/pascal-fast-compilation.webp" alt="pascal fast compilation">
</p>
</div>
<p>2.5 seconds to get desktop build, 10.1 seconds to get Android build <strong>of a new project, opened for the 1st time</strong>. Try to match that with your engine :)</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_cross_platform">2.3. Cross-platform</h3>
<div>
<ul>
<li>
<p>Desktop (Windows, Linux, macOS, Raspberry Pi, FreeBSD, probably every Unix…​),</p>
</li>
<li>
<p>mobile (Android, iOS),</p>
</li>
<li>
<p>consoles (Nintendo Switch, special in CGE),</p>
</li>
<li>
<p>web (both WebAssembly and JS (using pas2js)).</p>
</li>
</ul>
</div>

</div>
<div>
<h3 id="_welcoming">2.4. Welcoming</h3>
<div>
<ul>
<li>
<p>In <a href="https://castle-engine.io/">Castle Game Engine</a> case, engine code and game code are in the same language. Every user is contributor!</p>
</li>
<li>
<p>And the engine is open-source.</p>
</li>
</ul>
</div>
<p>Don’t hesitate to fork CGE to adjust it to your needs.</p>
</div>
<div>
<h3 id="_general_purpose">2.5. General purpose</h3>
<p>There are existing libraries (units) in Pascal for everything:</p>
<div>
<ul>
<li>
<p>database</p>
</li>
<li>
<p>XML, JSON</p>
</li>
<li>
<p>A.I.</p>
</li>
<li>
<p>blockchain</p>
</li>
<li>
<p>networking</p>
</li>
</ul>
</div>
<p>Moreover you can easily integrate with (link to) any existing library with C API. Any renderer, sound library, physics - we can use everything.</p>

</div>
<div>
<h3 id="_ecosystem_of_tools">2.6. Ecosystem of tools</h3>
<div>
<ul>
<li>
<p><a href="https://www.freepascal.org/">FPC</a> - Free Pascal Compiler, open-source.</p>
</li>
<li>
<p><a href="https://www.lazarus-ide.org/">Lazarus</a> - IDE for Pascal, on top of FPC, also open-source.</p>
</li>
<li>
<p><a href="https://www.embarcadero.com/products/Delphi">Delphi</a> - commercial compiler and IDE for Pascal.</p>
</li>
<li>
<p><a href="https://castle-engine.io/vscode">VS Code</a> support - CGE, as well as many others in the Pascal ecosystem, explicitly support integration with VS Code.</p>
</li>
</ul>
</div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How and Why I Stopped Buying New Laptops (2020) (170 pts)]]></title>
            <link>https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/</link>
            <guid>36646791</guid>
            <pubDate>Sat, 08 Jul 2023 17:47:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/">https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/</a>, See on <a href="https://news.ycombinator.com/item?id=36646791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<div id="content"><div>
<figure data-imgstate="dither">
<img alt="Image: Low-tech Magazine is now written and published on a 2006 ThinkPad X60s." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/X60-on-its-side-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/X60-on-its-side-white_hu753fd43d283e60bf1aefb8ea5c1440fe_3642029_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/X60-on-its-side-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Low-tech Magazine is now written and published on a 2006 ThinkPad X60s. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<p>Being an independent journalist – or an office worker if you wish – I always reasoned that I needed a decent computer and that I need to pay for quality. Between 2000 and 2017, I consumed three laptops that I bought new and which cost me around 5,000 euros in total – roughly 300 euros per year over the entire period. The average useful life of my three laptops was 5.7 years.</p>
<p>In 2017, somewhere between getting <a href="https://solar.lowtechmagazine.com/2018/09/how-to-build-a-low-tech-website/">my office</a>, I decided not to buy any more new laptops. Instead, I switched to a 2006 second-hand machine that I purchased online for 50 euros and which does everything that I want and need. Including a new battery and a simple hardware upgrade, I invested less than 150 euros.</p>
<p>If my 2006 laptop lasts as long as my other machines – if it runs for another 1.7 years – it will have cost me only 26 euros per year. That’s more than 10 times less than the cost of my previous laptops. In this article, I explain my motivations for not buying new laptops, and how you could do the same.</p>
<h2 id="energy-and-material-use-of-a-laptop">Energy and material use of a laptop</h2>
<p>Not buying new laptops saves a lot of money, but also a lot of resources and environmental destruction. According to the most recent life cycle analysis, it takes 3,010 to 4,340 megajoules of primary energy to make a laptop – this includes mining the materials, manufacturing the machine, and bringing it to market. <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p>
<p>Each year, we purchase between 160 and 200 million laptops. Using the data above, this means that the production of laptops requires a yearly energy consumption of 480 to 868 petajoules, which corresponds to between one quarter and almost half of all solar PV energy produced worldwide in 2018 (2,023 petajoules). <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> The making of a laptop also involves a high material consumption, which includes a wide variety of minerals that may be considered scarce due to different types of constraints: economic, social, geochemical, and geopolitical. <sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p>
<p>The <a href="https://solar.lowtechmagazine.com/2009/06/the-monster-footprint-of-digital-technology/">production of microchips is a very energy- and material-intensive process</a>, but that is not the only problem. The high resource use of laptops is also because they have a very short lifespan. Most of the 160-200 million laptops sold each year are replacement purchases. The average laptop is replaced every 3 years (in business) to five years (elsewhere). <sup id="fnref1:3"><a href="#fn:3" role="doc-noteref">3</a></sup> My 5.7 years per laptop experience is not exceptional.</p>
<h2 id="laptops-dont-change">Laptops don’t change</h2>
<p>The study cited dates from 2011, and it refers to a machine made in 2001: a Dell Inspiron 2500.  You are forgiven for thinking that this “most recent life cycle analysis of a laptop” is outdated, but it’s not. A 2015 research paper discovered that the embodied energy of laptops is static over time. <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p>
<p>The scientists disassembled 11 laptops of similar size, made between 1999 and 2008, and weighed the different components. Also, they measured the silicon die area for all motherboards and 30 DRAM cards produced over roughly the same period (until 2011). They found that the mass and material composition of all key components – battery, motherboard, hard drive, memory – did not change significantly, even though manufacturing processes became more efficient in energy and material use.</p>
<p>The reason is simple: improvements in functionality balance the efficiency gains obtained in the manufacturing process. Battery mass, memory, and hard disk drive mass decreased per unit of functionality but showed roughly constant totals per year. The same dynamic explains why newer laptops don’t show lower operational electricity consumption compared to older laptops. New laptops may be more energy-efficient per computational power, but these gains are offset by more computational power. <a href="https://solar.lowtechmagazine.com/2018/01/bedazzled-by-energy-efficiency/">Jevon’s paradox</a> is nowhere as evident as it is in computing.</p>
<h2 id="the-challenge">The challenge</h2>
<p>All this means that there’s no environmental or financial benefit whatsoever to replacing an old laptop with a new one. On the contrary, the only thing a consumer can do to improve their laptop’s ecological and economic sustainability is to use it for as long as possible. This is facilitated by the fact that laptops are now a mature technology and have more than sufficient computational power. One problem, though. Consumers who try to keep working on their old laptops are likely to end up frustrated. I shortly explain my frustrations below, and I’m pretty confident that they are not exceptional.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: The three new laptops I used from 2000 to 2017." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/3-laptops-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/3-laptops-white_hud91339aa34e3ccb38ac1a26db4506f2c_3604453_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/3-laptops-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: The three new laptops I used from 2000 to 2017. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-first-laptop-apple-ibook-2000-2005">My first laptop: Apple iBook (2000-2005)</h2>
<p>In 2000, when I was working as a freelance science and tech journalist in Belgium, I bought my first laptop, an Apple iBook. Little more than two or three years later, the charger started malfunctioning. When informed of the price for a new charger, I was so disgusted with Apple’s sales practices – chargers are very cheap to produce, but Apple sold them for a lot of money – that I refused to buy it. Instead, I managed to keep the charger working for a few more years, first by putting it under the weight of books and furniture, and when that didn’t work anymore, by putting it in a firmly tightened clamp.</p>
<h2 id="my-second-laptop-ibm-thinkpad-r52-2005-2013">My second laptop: IBM ThinkPad R52 (2005-2013)</h2>
<p>When the charger eventually died entirely in 2005, I decided to look for a new laptop. I had only one demand: it should have a charger that lasts or is at least cheap to replace. I found more than I was looking for. I bought an <a href="http://www.thinkwiki.org/wiki/Category:R52" target="_blank">IBM Thinkpad R52</a>, and it was love at first use. My IBM laptop was the Apple iBook counterpart, not just in terms of design (a rectangular box available in all colours as long as it’s black). More importantly, the entire machine was built to last, built to be reliable, and built to be repairable.</p>
<p><a href="https://solar.lowtechmagazine.com/2019/06/how-to-make-wind-power-sustainable-again/">Circular and modular products are all the hype these days</a>, its lifetime could be extended endlessly by gradually repairing and replacing every part that it consists of. The question is not how we can evolve towards a circular economy, but instead why we continue to evolve away from it.</p>
<blockquote>
<p>The question is not how we can evolve towards a circular economy, but instead why we continue to evolve away from it.</p>
</blockquote>
<p>My Thinkpad was more expensive to buy than my iBook, but at least I didn’t spend all that money on a cute design but a decent computer. The charger gave no problems, and when I lost it during a trip and had to buy a new one, I could do so for a fair price. Little did I know that my happy purchase was going to be a once-in-a-lifetime experience.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: The IBM ThinkPad R52 from 2005." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/Thinkpad-r52-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/Thinkpad-r52-white_hu02c8b35b61eabe045fd7fdd9f38bf53d_4952865_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/Thinkpad-r52-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: The IBM ThinkPad R52 from 2005. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-third-laptop-lenovo-thinkpad-t430-2013-2017">My third laptop: Lenovo Thinkpad T430 (2013-2017)</h2>
<p>Fast forward to 2013. I am now living in Spain and I’m running Low-tech Magazine. I’m still working on my IBM Thinkpad R52, but there are some problems on the horizon. First of all, Microsoft will soon force me to upgrade my operating system, because support for Windows XP is to end in 2014. I don’t feel like spending a couple of hundred euros on a new operating system that would be too demanding for my old laptop anyway. Furthermore, the laptop had gotten a bit slow, even after it had been restored to its factory settings. In short, I fell into the trap that the hardware and software industries have set up for us and made the mistake of thinking that I needed a new laptop.</p>
<p>Having been so fond of my Thinkpad, it was only logical to get a new one. Here’s the problem: in 2005, shortly after I had bought my first Thinkpad, Lenovo, a Chinese manufacturer that is now the largest computer maker in the world, bought IBM’s PC business. Chinese companies don’t have a reputation for building quality products, especially not at the time. However, since Lenovo was still selling Thinkpads that looked almost identical to those built by IBM, I decided to try my luck and bought a <a href="http://www.thinkwiki.org/wiki/Category:T430" target="_blank">Lenovo Thinkpad T430</a> in April 2013. At a steep price, but I assumed that quality had to be paid for.</p>
<p>My mistake was clear from the beginning. I had to send the new laptop back twice because its case was deformed. When I finally got one that didn’t wobble on my desk, I quickly ran into another problem: the keys started breaking off. I can still remember my disbelief when it happened for the first time. The IBM Thinkpad is known for its robust keyboard. If you want to break it, you need a hammer. Lenovo obviously didn’t find that so important and had quietly replaced the keyboard with an inferior one. Mind you, I can be an aggressive typist, but I have never broken any other keyboard.</p>
<p>I grumpily ordered a replacement key for 15 euros. In the months after that, replacement keys became a recurring cost. After spending more than 100 euros on plastic keys, which would soon break again, I calculated that my keyboard had 90 keys and that replacing them all just once would cost me 1,350 euros. I stopped using the keyboard altogether, temporarily finding a solution in an external keyboard. However, this was impractical, especially for working away from home – and why else would I want a laptop?</p>
<p>There was no getting around it anymore: I needed a new laptop. Again. But which one? For sure it would not be one made by Lenovo or Apple.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: Replacing all keys on my Lenovo T430 would have cost me 1,350 euros." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/broken-keyboard-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/broken-keyboard-white_huaf323a230a1b423848610266c7325189_7786692_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/broken-keyboard-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Replacing all keys on my Lenovo T430 would have cost me 1,350 euros.  <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="my-fourth-laptop-ibm-thinkpad-x60s-2017-now">My fourth laptop: IBM Thinkpad X60s (2017-now)</h2>
<p>Not finding what I was looking for, I decided to go back in time. By now, it had dawned on me that new laptops are of inferior quality compared to older laptops, even if they carry a much higher price tag.  I found out that Lenovo switched keyboards around 2011 and started searching auction sites for Thinkpads built before that year. I could have changed back to my ThinkPad R52 from 2005, but by now, I had become accustomed to a Spanish keyboard, and the R52 had a Belgian one.</p>
<p>In April 2017, I settled on a used <a href="http://www.thinkwiki.org/wiki/Category:X60s" target="_blank">Thinkpad X60s</a> from 2006. <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> As of December 2020, the machine is in operation for almost 4 years and is 14 years old – three to five times older than the average laptop. If I loved my Thinkpad R52 from 2005, I adore my Thinkpad X60s from 2006. It’s just as sturdily built – it already survived a drop from a table on a concrete floor – but it’s much smaller and also lighter: 1.43 kg vs. 3.2 kg.</p>
<p>My 2006 Thinkpad X60s does everything I want it to do. I use it to write articles, do research, and maintain the websites. I have also used it on-stage to give lectures, projecting images on a large screen. There’s only one thing missing on my laptop, especially nowadays, and that’s a webcam. I solve this by firing up the cursed 2013 laptop with the broken keys whenever I need to, happy to give it some use that doesn’t involve its keyboard. It could also be solved by a switch to the <a href="http://www.thinkwiki.org/wiki/Category:X200" target="_blank">Thinkpad X200</a> from 2008, which is a newer version of the same model and has a webcam.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: My ThinkPad X60s." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-x60s-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/thinkpad-x60s-white_hu30a06554e31ecbc7d8a684e77fe1da4d_3965389_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-x60s-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: My ThinkPad X60s. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="how-to-make-an-old-laptop-run-like-its-new">How to make an old laptop run like it’s new</h2>
<p>Not buying any more new laptops is not as simple as buying a used laptop. It’s advisable to upgrade the hardware, and it’s essential to downgrade the software. There are two things you need to do:</p>
<h2 id="1-use-low-energy-software">1. Use low energy software</h2>
<p>My laptop runs on <a href="https://www.linuxliteos.com/" target="_blank">Linux Lite</a>, one of several open-source operating systems <a href="https://lotoftech.com/10-best-lightweight-operating-system-for-old-computers/" target="_blank">specially designed to work on old computers</a>. The use of a Linux operating system is not a mere suggestion. There’s no way you’re going to revive an old laptop if you stick to Microsoft Windows or Apple OS because the machine would freeze instantly. Linux Lite does not have the flashy visuals of the newest Apple and Windows interfaces, but it has a familiar graphical interface and looks anything but obsolete. It takes very little space on the hard disk and demands even less computing power. The result is that an old laptop, despite its limited specifications, runs smoothly. I also use light browsers: <a href="https://vivaldi.com/" target="_blank">Vivaldi</a> and <a href="https://astian.org/en/midori-browser/" target="_blank">Midori</a>.</p>
<p>Having used Microsoft Windows for a long time, I find Linux operating systems to be remarkably better, even more so because they are free to download and install. Furthermore, Linux operating systems do not steal your personal data and do not try to lock you in, like the newest operating systems from both Microsoft and Apple do. That said, even with Linux, obsolescence cannot be ruled out. For example, Linux Lite will stop its support for 32-bit computers in 2021, which means that I will soon have to look for an alternative operating system, or buy a slightly younger 64-bit laptop.</p>
<h2 id="2-replace-the-hard-disk-drive-with-a-solid-state-drive">2. Replace the hard disk drive with a solid-state drive</h2>
<p>In recent years, solid-state drives (SSD) have become available and affordable, and they are much faster than hard disk drives (HDD). Although you can revive an old laptop by merely switching to a light-weight operating system, if you also replace the hard disk drive with a solid-state drive, you’ll have a machine that is just as fast as a brand new laptop. Depending on the storage capacity you want, an SSD will cost you between 20 euro (120 GB) and 100 euro (960 GB).</p>
<p>Installment is pretty straightforward and well documented online. Solid-state drives run silently and are more resistant to physical shock, but they have a shorter life expectancy than hard disk drives. Mine is now working for almost 4 years. It seems that both from an environmental and financial viewpoint, an old laptop with SSD is a much better choice than buying a new laptop, even if the solid-state drive needs replacement now and then.</p>
<h2 id="spare-laptops">Spare laptops</h2>
<p>Meanwhile, my strategy has evolved. I have bought two identical models for a similar price, in 2018 and early 2020, to use as spare laptops. Now I plan to keep working on these machines for as long as possible, having more than sufficient spare parts available. Since I bought the laptop, it had two technical issues. After roughly a year of use, the fan died. I had it repaired overnight in a tiny and messy IT shop run by a Chinese man in Antwerp, Belgium. He said that my patched fan would run for another six months, but it’s still working more than two years later.</p>
<p>Then, last year, my X60s suddenly refused to charge its battery, an issue that had also appeared with my cursed 2013 laptop. It seems to be a common problem with Thinkpads, but I could not solve it yet. Neither did I really have to because I had a spare laptop ready and started using that one whenever I needed or wanted to work outside.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: Three identical 2006 laptops, all in working order, for less than 200 euros." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/spare-laptops-white_hu3766708df1a6ba8bdbeb0add86f0194b_6142617_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: Three identical 2006 laptops, all in working order, for less than 200 euros. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<div>
<figure data-imgstate="dither">
<img alt="Image: Inside the Thinkpad X60s. Source: Hardware Maintenance Manual." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-inside_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/thinkpad-inside_huad20844be5ac8ba7ef947df62b823a02_178972_800x800_fit_q90_h2_box_3.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/thinkpad-inside_dithered.png"> </figure>

</div>
<h2 id="the-magical-sd-card">The magical SD-card</h2>
<p>Now to introduce you to my magical SD-card, which is another hardware upgrade that facilitates the use of old (but also new) laptops. Many people have their personal documents stored on their laptop’s hard drive and then make backups to external storage media if all goes well. I do it the other way around.</p>
<p>I have all my data on a 128 GB SD-card, which I can plug into any of the Thinkpads that I own. I then make monthly backups of the SD-card, which I store on an external storage medium, as well as regular backups of the documents that I am working on, which I temporarily store on the drive of the laptop that I am working on. This has proven to be very reliable, at least for me: I have stopped losing work due to computer problems and insufficient backups.</p>
<p>The other advantage is that I can work on any laptop that I want and that I’m not dependent on a particular machine to access my work. You can get similar advantages when you keep all your data in the cloud, but the SD-card is <a href="https://solar.lowtechmagazine.com/2015/10/why-we-need-a-speed-limit-for-the-internet/">the more sustainable option</a>, and it works without internet access.</p>
<p>Hypothetically, I could have up to two hard drive failures in one day and keep working as if nothing happened. Since I am now using both laptops alternately – one with battery, the other one without – I can also leave them at different locations and cycle between these places while carrying only the SD-card in my wallet. Try that with your brand new, expensive laptop. I can also use my laptops together if I need an extra screen.</p>
<p>In combination with a hard disk drive, the SD-card also increases the performance of an old laptop and can be an alternative to installing a solid-state drive. My spare laptop does not have one and it can be slow when browsing heavy-weight websites. However, thanks to the SD-card, opening a map or document happens almost instantly, as does scrolling through a document or saving it. The SD-card also keeps the hard disk running smoothly because it’s mostly empty. I don’t know how practical using an SD-card is for other laptops, but all my Thinkpads have a slot for them.</p>
<h2 id="the-costs">The costs</h2>
<p>Let’s make a complete cost calculation, including the investment in spare laptops and SD-card, and using today’s prices for both solid-state drives and SD-cards, which have become much cheaper since I have bought them:</p>
<ul>
<li>ThinkPad X60s: 50 euro</li>
<li>ThinkPad X60s spare laptop: 60 euro</li>
<li>ThinkPad X60 spare laptop: 75 euro</li>
<li>Two replacement batteries: 50 euro</li>
<li>240 GB solid-state drive: 30 euro</li>
<li>128 GB SD-card: 20 euro</li>
<li>Total: 285 euros</li>
</ul>
<p>Even if you buy all of this, you only spent 285 euros. For that price, you may be able to buy the crappiest new laptop on the market, but it surely won’t get you two spare laptops. If you manage to keep working with this lot for ten years, your laptop costs would be 28.5 euros per year. You may have to replace a few solid-state drives and SD-cards, but it won’t make much difference. Furthermore, you save the ecological damage that is caused by the production of a new laptop every 5.7 years.</p>
<div>
<figure data-imgstate="dither">
<img alt="Image: My laptop needs are met for the foreseeable future." data-dither="/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-2-white_dithered.png" data-original="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/spare-laptops-2-white_hu2dc6db9faa5724cc3c1faebbecf747bd_5522562_800x800_fit_q90_h2_box.webp" loading="lazy" src="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops/images/dithers/spare-laptops-2-white_dithered.png"> </figure>
<div>
<figcaption>
<p>Image: My laptop needs are met for the foreseeable future. <svg viewBox="0 0 100 100" xmlns="http://www.w3.org/2000/svg">
<rect height="24.28" width="24.28" x="13.51" y="13.58"></rect><rect height="24.28" width="24.28" x="37.93" y="37.86"></rect><rect height="24.28" width="24.28" x="62.21" y="13.58"></rect><rect height="24.28" width="24.28" x="13.51" y="62.14"></rect><rect height="24.28" width="24.28" x="62.21" y="62.14"></rect>
</svg></p>
<p><span>
  View original image
</span>
<span>
  View dithered image
</span>
</p>
</figcaption>
</div>
</div>
<h2 id="dont-take-it-too-far">Don’t take it too far</h2>
<p>Although I have used my Thinkpad X60s as an example, the same strategy works with other Thinkpad models – <a href="http://www.thinkwiki.org/wiki/ThinkPad_History" target="_blank">here’s an overview of all historical models</a> – and laptops from other brands (which I know nothing about). If you prefer not to buy on auction sites, you can walk to the nearest pawnshop and get a used laptop with a guarantee. The chances are that you don’t even need to buy anything, as many people have old laptops lying around.</p>
<p>There’s no need to go back to a 2006 machine. I hope it’s clear that I am trying to make a statement here, and I probably went as far back as one can while keeping things practical. My first try was a used ThinkPad X30 from 2002, but that was one step too far. It uses a different charger type, it has no SD-card slot, and I could not get the wireless internet connection working. For many people, it may serve to choose a somewhat younger laptop. That will give you a webcam and a 64-bit architecture, which makes things easier. Of course, you can also try to beat me and go back to the 1990s, but then you’ll have to do without USB and wireless internet connection.</p>
<p>Your choice of laptop also depends on what you want to do with it. If you use it mainly for writing, surfing the web, communication, and entertainment, you can do it as cheaply as I did. If you do graphical or audiovisual work, it’s more complicated, because in that case, you’re probably an Apple user. The same strategy could be applied, on a somewhat younger and more expensive laptop, but it would suggest switching from a Mac to a Linux operating system. When it comes to office applications, Linux is clearly better than its commercial alternatives. For a lack of experience, I cannot tell you if that holds for other software as well.</p>
<h2 id="this-is-a-hack-not-a-new-economical-model">This is a hack, not a new economical model</h2>
<p>Although capitalism could provide us with used laptops for decades to come, the strategy outlined above should be considered a hack, not an economical model. It’s a way to deal with or escape from an economic system that tries to force you and me to consume as much as possible. It’s an attempt to break that system, but it’s not a solution in itself. We need another economical model, in which we build all laptops like pre-2011 Thinkpads. As a consequence, laptop sales would go down, but that’s precisely what we need. Furthermore, with today’s computing efficiency, we could significantly reduce the operational and embodied energy use of a laptop if we reversed the trend towards ever higher functionality.</p>
<p>Significantly, hardware and software changes drive the fast obsolescence of computers, but the latter has now become the most crucial factor. A computer of 15 years old has all the hardware you need, but it’s not compatible with the newest (commercial) software. This is true for operating systems and every type of software, from games to office applications to websites. Consequently, to make laptop use more sustainable, the software industry would need to start making every new version of its products lighter instead of heavier. The lighter the software, the longer our laptops will last, and we will need less energy to use and produce them.</p>
<p>Kris De Decker</p>
<p>Images: Jordi Manrique Corominas, Adriana Parra, Roel Roscam Abbing</p>
<p>Proofreading: Eric Wagner</p>
</div>



</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Langchain Is Pointless (219 pts)]]></title>
            <link>https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/</link>
            <guid>36645575</guid>
            <pubDate>Sat, 08 Jul 2023 15:56:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/">https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/</a>, See on <a href="https://news.ycombinator.com/item?id=36645575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It's filled with crap like this:</p>

<pre><code>    for i in range(n_results, 0, -1):
        try:
            return self._collection.query(
                query_texts=query_texts,
                query_embeddings=query_embeddings,
                n_results=i,
                where=where,
                **kwargs,
            )
</code></pre>

<p>and this:</p>

<pre><code>def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:
    texts = list(map(lambda x: x.replace("\n", " "), texts))
    embeddings = self.client.encode(texts, **self.encode_kwargs)
    return embeddings.tolist()
</code></pre>

<p>and this:</p>

<pre><code>class CharacterTextSplitter(TextSplitter):
    """Implementation of splitting text that looks at characters."""

def __init__(self, separator: str = "\n\n", **kwargs: Any):
    """Create a new TextSplitter."""
    super().__init__(**kwargs)
    self._separator = separator

def split_text(self, text: str) -&gt; List[str]:
    """Split incoming text and return chunks."""
    # First we naively split the large input into a bunch of smaller ones.
    if self._separator:
        splits = text.split(self._separator)
    else:
        splits = list(text)
</code></pre>

<p>In short: <a href="https://i.imgur.com/OffEJTR.gifv">https://i.imgur.com/OffEJTR.gifv</a></p>

<p>Embeddings is just a do-nothing wrapper for SentenceTransformers. Chroma is just a do-nothing wrapper for ChromaDB. It's filled with "helper" functions that just call normal Python functions. A dedicated TextSplitter that calls split() from builtins.py? What? Why? Templates are no more useful than calling .replace() on a string. "texts" are just strings and "documents" are just a pointless dict that contain "texts." Just load the strings from your datasource yourself. The README is both grandiose and vague. The documentation is out-of-date and inconsistent. The import footprint is weirdly massive--highly modularized but nothing seems to do anything that'd take more than a few CPU cycles. There's not really a standard interoperable datatype, so you're actually led further afield than if you had just clearly defined the simple lists and strings required for hitting an LLM. </p>

<p>The very concept of chaining operations when interacting with LLMs doesn't really make sense to me: it's basically one <code>requests</code> call to a generation backend, but it's not like it even handles websockets and streaming for you. Why chain together wrapper classes when you can just do the operations yourself?</p>

<p>This seems like a beginner's project that blew up because it's riding a tidal wave of interest in the broader topic.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flickr Foundation is building a new bridge between Flickr and Wikimedia Commons (118 pts)]]></title>
            <link>https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/</link>
            <guid>36645448</guid>
            <pubDate>Sat, 08 Jul 2023 15:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/">https://diff.wikimedia.org/2023/07/07/flickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons/</a>, See on <a href="https://news.ycombinator.com/item?id=36645448">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-97236">
	<div>
			


<p>We are pleased to announce a new partnership with the <a href="https://www.flickr.org/">Flickr Foundation</a> to extend the great work already done via the <a href="https://commons.wikimedia.org/wiki/Commons:Flickr2Commons">Flickr2Commons</a> tool to make it even easier to upload CC-licensed images from Flickr into Wikimedia Commons.&nbsp;</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97298"><img decoding="async" loading="lazy" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231" alt="" width="256" height="231" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/Flickr_Foundation_Colour_Logo_Compact_Version.svg_.png?resize=256%2C231?w=768 768w" sizes="(max-width: 256px) 100vw, 256px" data-recalc-dims="1"></a></figure></div>


<p>Wikipedia is a foundational source of information on the internet. It provides content to Google and other search engines, social media platforms, voice assistants, and, increasingly, AI applications. To illustrate that information, we have Wikimedia Commons, the central visual platform for Wikipedia and one of the primary sources for open licensed visual content online. You may not know that one of the largest sources for Wikimedia Commons is Flickr.&nbsp;&nbsp;</p>



<p>Since 2004, Flickr has been one of the most popular platforms for photographers and amateurs to upload photographs, videos, illustrations, and more online. It is also one of the largest online repositories of Creative Commons-licensed content. Flickr members can assign a license to their uploads, including those Creative Commons licenses accepted on Wikimedia Commons: Attribution (CC-BY), Attribution-ShareAlike (CC-BY-SA), Public Domain Dedication (CC0), and the Public Domain Mark.</p>





<p>In 2008, Flickr launched the <a href="https://www.flickr.com/commons">Flickr Commons</a> program, to increase public access to photography collections held at libraries, museums, and archives around the world. Images in Flickr Commons are shared with something a bit different from a license. It’s an <em>assertion</em> called&nbsp; “no known copyright restrictions.”&nbsp; The program supports <a href="https://www.flickr.com/commons/institutions">over 100 member institutions</a>, including <a href="https://www.flickr.com/photos/usnationalarchives/">The U.S. National Archives</a>, <a href="https://www.flickr.com/photos/nasacommons/">NASA on Commons</a>, the <a href="https://www.flickr.com/photos/nlscotland/">National Library of Scotland</a>, and <a href="https://www.flickr.com/photos/reykjavikmuseumofphotography/">Ljósmyndasafn Reykjavíkur</a>.</p>



<p>In 2022, the <a href="https://www.flickr.org/">Flickr Foundation</a> was established. It’s a US 501(c)(3) non-profit organization with the objective of safeguarding Flickr and its tens of billions of photos for the future. It seeks to develop and sustain “…an accessible social and technical infrastructure to protect [this] invaluable collection.”&nbsp;<br>This bridge between Flickr and Wikimedia Commons—which we’ve started calling “<strong>Flickypedia</strong>”—is one of the flagship projects of the Flickr Foundation. Building in partnership with the Wikimedia Foundation, and supported by the Culture and Heritage team, we will be building on the utility of the <a href="https://commons.wikimedia.org/wiki/Commons:Flickr2Commons">Flickr2Commons</a> tool, extending it, and then tending it for the long term.</p>



<p>This project has been mentioned in the 2023-2024 <a href="https://meta.wikimedia.org/wiki/Wikimedia_Foundation_Annual_Plan/2023-2024/Goals/Equity">Wikimedia Foundation Annual Plan</a> under the Equity / Culture &amp; Heritage section, in <a href="https://web.archive.org/web/20230602045536/https://mailchi.mp/c6a1d40b1748/new-news-for-you-about-flickrorg">this mailing list update</a> from Flickr Foundation, and <a href="https://commons.wikimedia.org/wiki/Commons:Village_pump/Archive/2023/06#Flickr_Foundation_adopts_Flickr2Commons">in the ensuing discussion</a> on Wikimedia Commons’s Village Pump.</p>



<h2>About Flickr2Commons</h2>



<p>Flickr2Commons is a popular tool used by Wikimedia Commons contributors to upload single or multiple files from Flickr into Wikimedia Commons. It was created by <a href="https://en.wikipedia.org/wiki/Magnus_Manske">Magnus Manske</a>, and first launched in 2013, ten years ago! The tool allows for user authentication, checks for the required licenses, includes a metadata editing step, and then file transfer.&nbsp;</p>



<h2>Metrics important for Flickypedia</h2>



<p>In order to gauge the possible reach of Flickypedia, we wanted to understand Flickr2Commons metrics. Magnus helped pull together the stats to show that roughly 5.4M files have been uploaded by about 2K users since launch. Using the <a href="https://hashtags.wmcloud.org/">Wikimedia Hashtags</a> tool, we can also see how much Flickr2Commons is used today. In June 2023 only, for example, 71,689 files were uploaded by 147 users.</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97279"><img decoding="async" loading="lazy" width="1024" height="427" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?resize=1024%2C427" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1920 1920w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2023/07/edits_over_time-1.jpg?w=1536 1536w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Number of edits (ie. uploads), with the Flickr2Commons tool in June 2023 (<a href="https://hashtags.wmcloud.org/graph/?query=flickr2commons&amp;project=&amp;startdate=2023-06-01&amp;enddate=2023-06-30&amp;search_type=or&amp;user=">Hashtags tool</a>)</figcaption></figure></div>


<p>We were also able to discover the most active users of Flickr2Commons in the last six months, from January to June 2023.</p>


<div>
<figure><a href="https://diff.wikimedia.org/?attachment_id=97281"><img decoding="async" loading="lazy" width="1024" height="1024" src="https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?resize=1024%2C1024" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=1254 1254w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=150 150w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=768 768w, https://diff.wikimedia.org/wp-content/uploads/2023/07/top_users-2.jpg?w=1024 1024w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></a><figcaption>Most active users of Flickr2Commons January to June 2023 (<a href="https://hashtags.wmcloud.org/graph/?query=flickr2commons&amp;project=&amp;startdate=2023-01-01&amp;enddate=2023-06-30&amp;search_type=or&amp;user=">Hashtags</a>)</figcaption></figure></div>


<p>It’s been great to collate all these usage statistics for Flickr2Commons—both the more recent numbers, but also in total over the last 10 years. Seeing it all together gives us a clear target for the new version to try to match.&nbsp;</p>



<p>It is also worth noting that another tool connects Flickr to Wikimedia Commons, called the UploadWizard. We’re bearing in mind that this means there will have been even more images from Flickr through that tool. Preparing these metrics has given us ideas on how we might make it even simpler to count into the future using Flickypedia.</p>



<h2>Our timeline</h2>



<p>The Flickypedia partnership project officially started in June 2023. We plan to spend the next six months or so building our Alpha (hopefully to show in October) and then Version 1.0 (hopefully December). Please <a href="https://commons.wikimedia.org/wiki/Commons_talk:Flickypedia">stay in touch</a> if you’d like to be involved in testing or have feedback about Flickr2Commons we should know about.</p>



<ul>
<li>For the July-December plan, please <a href="https://commons.wikimedia.org/wiki/Commons:Flickypedia">visit the project page</a> on Wikimedia Commons.</li>



<li>For feedback, please contact us via the <a href="https://commons.wikimedia.org/wiki/Commons_talk:Flickypedia">talk page</a> on Wikimedia Commons.</li>
</ul>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt="">
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2023%2F07%2F07%2Fflickr-foundation-is-building-a-new-bridge-between-flickr-and-wikimedia-commons%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div>

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When an app asks for permissions, it should have a “feed fake data” option (755 pts)]]></title>
            <link>https://mastodon.gamedev.place/@Nifflas/110668040598715116</link>
            <guid>36644895</guid>
            <pubDate>Sat, 08 Jul 2023 14:43:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.gamedev.place/@Nifflas/110668040598715116">https://mastodon.gamedev.place/@Nifflas/110668040598715116</a>, See on <a href="https://news.ycombinator.com/item?id=36644895">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[iVentoy (171 pts)]]></title>
            <link>https://www.iventoy.com/en/index.html</link>
            <guid>36644806</guid>
            <pubDate>Sat, 08 Jul 2023 14:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iventoy.com/en/index.html">https://www.iventoy.com/en/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=36644806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentpage"> 


<h2>News</h2>

<p>
2023/07/05 ---  New release <a href="https://www.iventoy.com/en/download.html">iventoy-1.0.08 </a>
<span> <a href="https://www.iventoy.com/en/doc_news.html">More ...</a></span>
</p>

<h2>What is iVentoy</h2>
<p>   
    iVentoy is an enhanced version of the PXE server. <br>
    With iVentoy you can boot and install OS on multiple machines at the same time through the network.<br> 
    iVentoy is extremely easy to use, without complicated configuration, just put the ISO file in the specified location and select PXE boot in the client machine.<br>
    iVentoy supports x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI and ARM64 UEFI mode at the same time.<br>
    iVentoy support 110+ common types of OS (Windows/WinPE/Linux/VMware) (<a href="https://www.iventoy.com/en/isolist.html">list</a>)。
</p>

<br>

<h2>Features</h2>
<div>
<div>
<ul>
  <li>Simple to use <a href="https://www.iventoy.com/en/doc_start.html">(Get Started)</a>  </li>  
  <li>Cross-platform, can run in both Windows and Linux.</li>      
  <li>Specially optimized for PXE scenarios, with flexible functions.</li>      
  <li>Directly boot ISO files, no extraction needed.</li>
  <li>Native boot menu style for Legacy &amp; UEFI</li> 
  <li>Directory layout corresponded boot menu.</li> 
  <li>Supports Legacy BIOS and IA32/X86_64/ARM64 UEFI mode.</li>    
  <li>Supports 110+ common types of OS (Windows/WinPE/Linux/VMware) </li>      
  <li>System or ISO level boot password protection.</li>  
  <li>Multiple devices to install different OSs at the same time.</li>
</ul>
</div>

<div>
<ul>  

  <li>Device filtering by MAC address.</li>
  <li>Support querying MAC address filtering status.</li>
  <li>Support MAC address attribution query.</li>
  <li>Client device information. (Manufacture, product name etc.)</li>
  <li>Directly get ISO internal files with HTTP.<a href="https://www.iventoy.com/en/doc_http_url.html">Notes</a></li>  
  <li>File Injection feature. <a href="https://www.iventoy.com/en/doc_injection.html">Notes</a> </li>  
  <li>Windows auto installation supported. <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Linux auto installation supported. <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Variables Expansion supported for install script <a href="https://www.iventoy.com/en/doc_autoinstall.html">Notes</a></li>
  <li>Automatically solve the driver missing during Linux installation.</li>
  
</ul>
</div>
</div>


<p><img src="https://www.iventoy.com/static/img/screen/boot_ground.png?v=4">
</p>





    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A $182B Chip Maker: AMD's Labs – Full Documentary [video] (156 pts)]]></title>
            <link>https://www.youtube.com/watch?v=7H4eg2jOvVw</link>
            <guid>36644506</guid>
            <pubDate>Sat, 08 Jul 2023 14:02:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=7H4eg2jOvVw">https://www.youtube.com/watch?v=7H4eg2jOvVw</a>, See on <a href="https://news.ycombinator.com/item?id=36644506">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Thermochromic Breadboard (119 pts)]]></title>
            <link>https://www.improwis.com/projects/hw_ThermochromicBreadboard/</link>
            <guid>36644026</guid>
            <pubDate>Sat, 08 Jul 2023 13:10:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.improwis.com/projects/hw_ThermochromicBreadboard/">https://www.improwis.com/projects/hw_ThermochromicBreadboard/</a>, See on <a href="https://news.ycombinator.com/item?id=36644026">Hacker News</a></p>
<div id="readability-page-1" class="page">

<hr><hr><a name="Why"></a><h2>Why
</h2>
<p>
When working on a&nbsp;<a href="https://en.wikipedia.org/wiki/Breadboard#Solderless_breadboard" title="Wikipedia link: Breadboard#Solderless_breadboard" target="_blank">solderless breadboard</a><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAMFBMVEWEg4R6e3pIR0jp6Om5urmpqKmYmJhpamlXWFf8/fw8OjwoKCjY2dgZGBnIx8gEAgRb5tWnAAAAcklEQVR4nGOYCQUMmAwPlamKJyXOVDBM2DJT+zDb5DKGOdctc+MqJ1cyzOx9eWhD5JRIhpm+nNN+zDTwZJg5SWfOopkHgdonfJ2z2jMNZM4O06pKZhCDS/TgNk4QI3rrlN0vQYxJOjOrwXZNeTnzCJKlAFOARWjqKuE5AAAAAElFTkSuQmCC"> (not only there, but... well...),
it often happens that a&nbsp;power rating of some part is exceeded or its cooling is insufficient.
It would be beneficial to see such situations before the&nbsp;parts express their distress with a
smoke signal.
</p>
<hr><a name="How"></a><h2>How
</h2>
<p>
A breadborad was modified with a&nbsp;<a href="https://en.wikipedia.org/wiki/Thermochromism" title="Wikipedia link: Thermochromism" target="_blank">thermochromic paint</a><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAMFBMVEWEg4R6e3pIR0jp6Om5urmpqKmYmJhpamlXWFf8/fw8OjwoKCjY2dgZGBnIx8gEAgRb5tWnAAAAcklEQVR4nGOYCQUMmAwPlamKJyXOVDBM2DJT+zDb5DKGOdctc+MqJ1cyzOx9eWhD5JRIhpm+nNN+zDTwZJg5SWfOopkHgdonfJ2z2jMNZM4O06pKZhCDS/TgNk4QI3rrlN0vQYxJOjOrwXZNeTnzCJKlAFOARWjqKuE5AAAAAElFTkSuQmCC">, mixed from a&nbsp;white
acrylic model-grade paint and a&nbsp;thermochromic pigment obtained from <a href="http://www.mutr.co.uk/" title="remote link: http://www.mutr.co.uk/" target="_blank">Middlesex University Teaching Resources</a><img src="data:image/gif;base64,R0lGODlhCgAKAIABAGZmZv///yH5BAEAAAEALAAAAAAKAAoAAAIVjA8Jx6FvVmrL1chu3c1h+mXRoxgFADs=">
webshop, using a&nbsp;makeshift pot made from the&nbsp;bottom part of a&nbsp;beverage can. 
The orange color was chosen on the&nbsp;basis of availability (read: mistakenly ordering four
oranges instead of intended four different hues.) The threshold temperature of the&nbsp;pigment
was chosen also on the&nbsp;basis of availability (they did not have any other than 29-30&nbsp;°C).
</p>
<p>
The active area, where the&nbsp;parts are located by at least one pin, was coated with the
mixed paint. Care was taken to not let too much of it drip into the&nbsp;pin holes.
The power buses were not painted.
</p>
<hr><a name="Results"></a><h2>Results
</h2>
<p>
The first tests were done during a&nbsp;particularly warm summer night, when the&nbsp;indoor temperature
reached close to threshold temperature of the&nbsp;pigment. At such conditions, the&nbsp;sensitivity of the
paint was outstanding; a&nbsp;250-milliwatt resistor loaded with 350&nbsp;milliwatts shown a&nbsp;color change around
its leg within several seconds.
</p>
<p>
The reverse change was much slower. Due to some hysteresis of the&nbsp;pigment and the&nbsp;closeness of the
room temperature to the&nbsp;threshold temperature, the&nbsp;thermal trace was present for a&nbsp;fairly long time
(minutes). Putting the&nbsp;board out of the&nbsp;window, where the&nbsp;temperature was slightly lower, markedly
accelerated restoring of the&nbsp;color.
</p>
<p>
The thermal trace (the discoloration of the&nbsp;pigment) tended to bleed around the&nbsp;board as its material
spread the&nbsp;heat. The width of the&nbsp;trace, and the&nbsp;speed of its spreading, can provide a&nbsp;visual clue
about how much is the&nbsp;part heating.
</p>
<p>
Caveat: The color change shows the&nbsp;part's color indirectly. There is a&nbsp;delay between the&nbsp;temperature
change of the&nbsp;part itself and temperature change of the&nbsp;board, which is usually provided via thermal
conduction through the&nbsp;part's legs. There is also a&nbsp;thermal differential between the&nbsp;part itself,
along its leg, and over the&nbsp;board.
</p>
<hr><a name="Possibleimprovements"></a><h2>Possible improvements
</h2>
<ul><li> Use a&nbsp;darker pigment with a&nbsp;different hue (most likely blue or black)
</li><li> Use a&nbsp;pigment with a&nbsp;different threshold temperature (e.g. 43&nbsp;°C, that could be optimal)
</li><li> Best: Use a&nbsp;pair of pigments with different colors and different thresholds, yielding two color changes at two temperatures
</li><li> Incorporate the&nbsp;pigment directly into the&nbsp;material the&nbsp;breadboard is made from
</li></ul><div><hr><div>
<table>
<tbody><tr><td colspan="2"><small>If you have any comments or questions about the topic, please let me know here:</small></td></tr>


<tr><td>Your name:</td><td></td></tr>
<tr><td>Your email:</td><td></td></tr>

<tr><td>Feedback:</td><td></td></tr>
<tr><td>&nbsp;</td><td></td></tr>

</tbody></table>
</div>
</div>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[If PEP 703 is accepted, Meta can commit three engineer-years to nogil CPython (511 pts)]]></title>
            <link>https://discuss.python.org/t/a-fast-free-threading-python/27903/99</link>
            <guid>36643670</guid>
            <pubDate>Sat, 08 Jul 2023 12:18:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discuss.python.org/t/a-fast-free-threading-python/27903/99">https://discuss.python.org/t/a-fast-free-threading-python/27903/99</a>, See on <a href="https://news.ycombinator.com/item?id=36643670">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
      <meta itemprop="headline" content="A fast, free threading Python">
        <meta itemprop="articleSection" content="Ideas">
      <meta itemprop="keywords" content="">
      

          <div itemprop="comment" id="post_80" itemscope="" itemtype="http://schema.org/Comment">
              
<p>This will work as long as you give it an initial empty <code>Counter</code> to start with (otherwise it starts with <code>0</code> and complains)</p>
            </div>
          <div id="post_81" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/Rosuav"><span itemprop="name">Rosuav</span></a>
                (Chris Angelico)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T15:35:51Z">
                    June 26, 2023,  3:35pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T15:35:51Z">
              <span itemprop="position">81</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Ah thanks. Anyhow, the idea is to minimize the work done in the single-threaded “gather” phase at the end, by having each thread individually count in a lock-free way.</p>
            </div>

            

            

          </div>
          <div id="post_82" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lieryan"><span itemprop="name">lieryan</span></a>
                (Lie Ryan)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:20:39Z">
                    June 26, 2023,  4:20pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:28:05Z">
              <span itemprop="position">82</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I don’t think that is true. If free threading is possible, the cat will be out of the bag, even developers that only cares about single threaded work will still be affected by threading issues. If a library starts a thread in the background for whatever reason, they can cause threading issue in my code even though I never subscribed for having threading problems.</p>
<p>Many libraries that had async-to-sync bridges spawns threads to simulate async tasks. Django, FastAPI, SQLAlchemy is just a few off the top of my head. And then there’s tools like IPython that starts a couple background threads for who knows what reasons.</p>
<p>Multithreading has a reputation for being hard. But really, I think they are considered hard <strong>because</strong> of the existence of free threading. Languages like Rust that doesn’t have free threading (or to be more precise, it has an almost free threading with some severe restrictions) actually fared better at making multithreading a lot easier to use.</p>
<p>The arena-based threading with subinterpreters I had mentioned earlier would be that similar sort of that almost-free threading with forced discipline.</p>
<p>One way to think of arena-based threading is that it’s basically like a dynamic/runtime borrow checker, enforcing acquisition of the arena locks before working with any objects owned by the arena. I think it can even be flexible enough to allow future experimentation with non standard arenas that have different borrowing rules.</p>
            </div>

            

            

          </div>
          <div id="post_83" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lunixbochs"><span itemprop="name">lunixbochs</span></a>
                (Ryan Hileman)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:25:27Z">
                    June 26, 2023,  4:25pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:25:27Z">
              <span itemprop="position">83</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>There are language-level tools like golang’s race condition detector, thread sanitizer, etc, which take the common mistakes and test for them. It’s also possible someone could implement something like a borrow checker or thread safety heuristics on top of python’s type system, e.g. with passthrough types along the lines of Mutable / Immutable / Shared / Local, and auditing nonlocal variable access or object types passed into threads.</p>

<p>This wouldn’t be the case with my proposal to make threads take a voluntary lock by default. In a sense, you could leave something like the GIL in place, but make it safe to release for specific threads while accessing python code/objects.</p>
            </div>

            

            

          </div>
          <div itemprop="comment" id="post_84" itemscope="" itemtype="http://schema.org/Comment">
              
<p>I don’t think anyone has demonstrated how this would happen, and I’d view it as a fundamental flaw in the implementation if it could.</p>
<p>I’m not saying it’s impossible, but I think it would be useful to have specific examples–even if only theoretical–before it’s considered a significant problem.</p>
            </div>
          <div id="post_85" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/lunixbochs"><span itemprop="name">lunixbochs</span></a>
                (Ryan Hileman)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T16:33:27Z">
                    June 26, 2023,  4:33pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T16:33:27Z">
              <span itemprop="position">85</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I guess this is a bit too open ended. I think the thread in question can only interact with your code unintentionally if you happen to share a resource with that thread in an unsafe way, furthermore to be scary it would need to be an unsafe way that isn’t possible today. Even with the GIL another thread can already do a lot of things, like mess with your file descriptors, stdout, signals. And threads sharing access to any variable is already inconsistent for non-atomic ops at the Python level.</p>
            </div>

            

            

          </div>
          <div id="post_86" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/smontanaro"><span itemprop="name">smontanaro</span></a>
                (Skip Montanaro)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T17:01:58Z">
                    June 26, 2023,  5:01pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T17:01:58Z">
              <span itemprop="position">86</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Can you provide a (hypothetical?) example?</p>
            </div>

            

            

          </div>
          <div id="post_87" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/Eclips4"><span itemprop="name">Eclips4</span></a>
                (Kirill Podoprigora)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T17:13:12Z">
                    June 26, 2023,  5:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T17:13:12Z">
              <span itemprop="position">87</span>
              </span>
            </p></div>
            <p>Yep, CPython can switch threads in the middle of these two operations. So, there’s a problem.</p>

            

            

          </div>
          <div id="post_88" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ntessore"><span itemprop="name">ntessore</span></a>
                (Nicolas Tessore)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T21:16:35Z">
                    June 26, 2023,  9:16pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T21:16:35Z">
              <span itemprop="position">88</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>Again, of course. <sup><a href="#footnote-102472-1" id="footnote-ref-102472-1">[1]</a></sup> But I understood that <a href="https://discuss.python.org/u/pf_moore">@pf_moore</a> made the very fine point that due to specialisations we are discussing here (e.g. <code>BINARY_SUBSCR_DICT</code>), and hence the GIL, things which are nominally not thread-safe are effectively so in current CPython, because they are specialised to a single native instruction. And this, I think, only needs an explicit specification for whether or not such operations are to be considered effectively “atomic” or not. Otherwise, yes, these are just undiscovered bugs, currently protected by a CPython implementation detail.</p>
<hr>

<ol>
<li id="footnote-102472-1"><p>Except that there’s the timer. <a href="#footnote-ref-102472-1">↩︎</a></p>
</li>
</ol>
            </div>

            

            

          </div>
          <div id="post_89" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/tjreedy"><span itemprop="name">tjreedy</span></a>
                (Terry Jan Reedy)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T22:13:25Z">
                    June 26, 2023, 10:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T22:13:25Z">
              <span itemprop="position">89</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>… of a library starting a background thread.  Not exactly a library, but idlelib.run has</p>
<pre><code>    sockthread = threading.Thread(target=manage_socket,
                                  name='SockThread',
                                  args=((LOCALHOST, port),))
</code></pre>
<p>as a result of which <code>threading.activecount()</code> and <code>theading.enumerate()</code> returns are greater when running on IDLE.  Someone once asked why the difference on Stackoverflow.   (I have not idea whether no-gil will require any change to <code>manage_socket</code> or that chance of user code having a problem.)</p>
            </div>

            

            

          </div>
          <div id="post_90" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ppolewicz"><span itemprop="name">ppolewicz</span></a>
                (Pawel Polewicz)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T22:13:51Z">
                    June 26, 2023, 10:13pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T22:13:51Z">
              <span itemprop="position">90</span>
              </span>
            </p></div>
            <div itemprop="text">
              <pre><code>import threading

THREAD_COUNT = 3
BY_HOW_MUCH = 1_000_000


class Incrementor:
    def __init__(self):
        self.c = 0

def incr(incrementor, by_how_much):
    for i in range(by_how_much):
        incrementor.c += 1

incrementor = Incrementor()

threads = [
    threading.Thread(target=incr, args=(incrementor, BY_HOW_MUCH))
    for i in range(THREAD_COUNT)
]

for t in threads:
    t.start()

for t in threads:
    t.join()

print(incrementor.c)

</code></pre>
<p>prints 3 million when ran it on 3.10. Does it mean you can rely on <code>+=</code> being atomic when writing Python code? No! If you run it on 3.9 it prints between 1.5 and 2 million. Soon a Faster CPython team member can swoop in and change (not break!) it again.</p>
<p>BTW if Java and .net developers can have free threads, then so can we.</p>
            </div>

            

            

          </div>
          <div id="post_91" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/h-vetinari"><span itemprop="name">h-vetinari</span></a>
                (H. Vetinari)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-26T23:08:41Z">
                    June 26, 2023, 11:08pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-26T23:08:41Z">
              <span itemprop="position">91</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>The word “can” here translates to (potentially) decades of work, which was the case for Java:</p>

<p>Yes we “can” (and likely should), but it requires serious commitment, and off-hand “others do it too” is not helpful here.</p>
<p>In the context Java and threading, it’s worth noting how threads commonly need quite a lot of developer-facing infrastructure (e.g. thread pools) that’s probably very hard to make beginner-friendly / “Pythonic”, and that they’re on a similarly large multi-{year,person} effort to move from free threads to virtual threads<sup><a href="#footnote-102482-1" id="footnote-ref-102482-1">[1]</a></sup> under <a href="https://openjdk.org/projects/loom/" rel="noopener nofollow ugc">Project “Loom”</a> (where – arguably – the boundaries to async programming start getting blurred), and encapsulating a lot of that in simpler interfaces through <a href="https://openjdk.org/jeps/453" rel="noopener nofollow ugc">“structured concurrency”</a>, which we have already (at least through <code>trio</code>).</p>
<p>All that to say: if we argue “Java can”, then we should also look at where those choices have led them, and what they consider as “moving forward” from there. But realistically, we’re very far from an apples-to-apples comparison in any case, and it’s better to leave that rhetorical tool hanging in the shed.</p>
<hr>

<ol>
<li id="footnote-102482-1"><p>latest <a href="https://openjdk.org/jeps/444" rel="noopener nofollow ugc">incarnation</a> in JDK 21 <a href="#footnote-ref-102482-1">↩︎</a></p>
</li>
</ol>
            </div>

            

            

          </div>
          <div id="post_92" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ppolewicz"><span itemprop="name">ppolewicz</span></a>
                (Pawel Polewicz)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T04:39:41Z">
                    June 27, 2023,  4:39am
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T07:13:02Z">
              <span itemprop="position">92</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>I meant it from the user perspective, referring to “maybe PEP-684 is better, because it’s safer” part of the discussion. For years Python was “parallel, but…” and now adding subinterpreters will help, but it won’t solve the entire problem.</p>
<p>PEP-703 on the other hand, goes pretty much all the way (though stop-the-world GC may still be a limitation) for those willing to learn how to use it and for those who already have experience with threads from other languages. Python “popularity” will increase with projects choosing it for a multicore program when it will become an option.</p>
<p>Will some users hurt themselves with free threading? I’ve been tracking nogil for a long while now and from what I’ve seen, for someone who writes threadsafe code already (but not native extensions) it will be really hard to run into trouble.</p>
            </div>

            

            

          </div>
          <div id="post_93" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/bluss"><span itemprop="name">bluss</span></a>
                (bluss)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T19:25:35Z">
                    June 27, 2023,  7:25pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T19:30:00Z">
              <span itemprop="position">93</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>There are some multithreading traps inside glibc (relevant for Linux) that are unfortunate and sort of perennial issues (not just in Python!). <a href="http://rachelbythebay.com/w/2017/01/30/env/" rel="noopener nofollow ugc">For example getenv and setenv</a>; glibc maintains that multithreaded programs must not use setenv, it’s not thread safe.</p>
<p>A library could start a thread, and the library wants to and would use C getenv in this thread (getenv is allowed according to glibc in a multithreaded program, following the usual logic).<br>
The user’s program then has a threading issue: they must not use setenv, that could possibly cause segfaults (Python has setenv interfaces through os.environ and os.putenv).</p>
<p>(Does this issue exist in Python already today? Is there some mitigating factor that I don’t know about? How do subinterpreters deal with this? It would be great if C getenv/setenv had a major revision to be somewhat compatible with threading.)</p>
            </div>

            

            

          </div>
          <div id="post_94" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ings"><span itemprop="name">ings</span></a>
                (Christoph)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T20:19:25Z">
                    June 27, 2023,  8:19pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T20:19:25Z">
              <span itemprop="position">94</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p>I just want to throw in a use case which has not yet been discussed here and in the discussions of PEP 703: GUI toolkits.</p>
<p>GUI toolkits are naturally using threads and therefore an approach where free threading is replaced by a different concept like sub interpreters or multiprocessing is problematic in the design &amp; architecture of GUI applications written in python (because the toolkits being exposed in python are not aware of such concepts and most likely offer plain threading for offloading computational workload from the GUI). I’m a user of the PySide (Qt for python) project, and the PySide devs did struggle with the GIL as explained here <a href="https://www.qt.io/blog/qt-for-python-5.15.0-is-out" rel="noopener nofollow ugc">Qt for Python 5.15.0 is out!</a> (and also the links inside the document).</p>
<p>The problem of when it’s better to release or not to release the GIL in the C extensions of GUI toolkits is not straightforward, sometimes counter-intuitive (at least to me) and often there is a compromise involved depending on most common use cases but with drawbacks for other use cases.</p>
<p>Moreover, when creating GUI applications in python, you start struggling with the GIL when you have huge workloads happening in the background. My use case is a computer vision GUI which acts as a monitor and development environment for remote embedded systems. It is similar to what <a href="https://discuss.python.org/u/lunixbochs">@lunixbochs</a> reported for the realtime audio use case - keeping latencies and stutters on an acceptable level is unnecessarily difficult when you have to fight against the GIL mechanism.</p>
            </div>

            

            

          </div>
          <div id="post_95" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/smontanaro"><span itemprop="name">smontanaro</span></a>
                (Skip Montanaro)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-27T20:34:59Z">
                    June 27, 2023,  8:34pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-27T20:34:59Z">
              <span itemprop="position">95</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>Wouldn’t this be a potential problem today? “Not thread-safe” doesn’t mean “not thread-safe only when used in a free-threaded environment.”</p>
<p>I’m not trying to be difficult, maybe a bit pedantic. The presence of the GIL can obscure threading bugs or make them rear their ugly heads less often, but it doesn’t make code thread-safe. <a href="https://discuss.python.org/u/colesbury">@colesbury</a>’s work to remove the GIL has done a lot to remove places in the interpreter, stdlib, and some third-party libraries that relied on the GIL (knowingly or not). Most (all? almost all?) of that work will have been in C code, not Python code.</p>
            </div>

            

            

          </div>
          <div id="post_96" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/bluss"><span itemprop="name">bluss</span></a>
                (bluss)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-06-28T20:06:44Z">
                    June 28, 2023,  8:06pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-28T20:06:44Z">
              <span itemprop="position">96</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>I think it could be a problem today, don’t know, I have the same question. I’m here to be curious.</p>
<p>Again as an interesting anecdotal data point here is a port of that troublesome C code to Python: <a href="https://gist.github.com/bluss/a3d2ad94d55382f682897ee1efae6d74" rel="noopener nofollow ugc">mtenv.py</a><br>
Here’s how it runs:</p>
<ul>
<li>Using Python 3.11.3 this program loops for a long time without problem</li>
<li>I compiled and used nogil-3.12 commit 4526c07caee8f2e (current tip of the repo)<br>
and it runs 1-2 seconds before it <strong>segfaults</strong> in getenv just like the C code from 2017.</li>
</ul>
<p>This is a reduced example. It doesn’t look like a normal Python program, it has a strange shape so that it can reproduce a crash easily. But the fundamental elements can occur in normal Python programs - various C calls that libraries use that use getenv - let’s say mktime to use the example from that blog post - and for setenv we have plain interface to it in <code>os</code> (<code>os.getenv</code> is <em>not</em> a plain interface to C <code>getenv</code>).</p>
            </div>

            

            

          </div>
          <div id="post_97" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Yes, it is a problem today, without free threading. getenv + setenv thread safety is a problem for Python applications I run at work. We had to do a bunch of whackamole to work around segfaults resulting from extension libraries using getenv + setenv (for a while we gave up and used a terrible <code>LD_PRELOAD</code> hack)</p>

            

            

          </div>
          <div id="post_98" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/gpshead"><span itemprop="name">gpshead</span></a>
                (Gregory P. Smith)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-07-07T23:46:24Z">
                    July 7, 2023, 11:46pm
                  </time>
                  <meta itemprop="dateModified" content="2023-07-07T23:46:24Z">
              <span itemprop="position">98</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>&amp;</p>

<p>At the high level this is the kind of thing I’d <strong>love someone to try creating</strong> for per-subinterpreter-GIL use! This is also quite hard, but I assume there are interested folks out there.</p>
<p>Intuitively I <em>expect</em> this winds up being the same problem that needs to be solved for free threading <em>(which PEP-703 appears to do)</em>: our pure reference counting model is the most significant reason we have a GIL - in order to share objects between multiple threads you need to make the reference counts work without that single lock.</p>
<p>Someone really needs to try creating explicitly shared objects implementation for CPython and subinterpreters to prove or disprove it’s actual utility. In the absence of that, I wouldn’t point to it and suggest it is a <em>better</em> solution. I consider it an open avenue of future work. <em>(Even if we get free threading, performant explicit sharing would be something I expect many would appreciate having.)</em></p>
            </div>

            

            

          </div>
          <div id="post_99" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <div>
              <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/carljm"><span itemprop="name">carljm</span></a>
                (Carl Meyer)
              </span></p>


              <p><span>
                  <time itemprop="datePublished" datetime="2023-07-07T23:53:54Z">
                    July 7, 2023, 11:53pm
                  </time>
                  <meta itemprop="dateModified" content="2023-07-07T23:53:54Z">
              <span itemprop="position">99</span>
              </span>
            </p></div>
            <div itemprop="text">
              
<p>We’ve had a chance to discuss this internally with the right people. Our team believes in the value that nogil will provide, and we are committed to working collaboratively to improve Python for everyone.</p>
<p>If PEP 703 is accepted, Meta can commit to support in the form of three engineer-years (from engineers experienced working in CPython internals) between the acceptance of PEP 703 and the end of 2025, to collaborate with the core dev team on landing the PEP 703 implementation smoothly in CPython and on ongoing improvements to the compatibility and performance of nogil CPython.</p>
            </div>

            

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scams upon scams: The data-driven advertising grift (209 pts)]]></title>
            <link>https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/</link>
            <guid>36643630</guid>
            <pubDate>Sat, 08 Jul 2023 12:12:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/">https://anotherangrywoman.com/2023/07/05/scams-upon-scams-the-data-driven-advertising-grift/</a>, See on <a href="https://news.ycombinator.com/item?id=36643630">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-6748">
	
	<!-- .entry-header -->

	<div>
		
<p>Digital advertising is a scam from top to bottom. In fact, it’s several scams stacked on top of each other, wearing a trenchcoat, and some of the foundations of fibs are so effective that otherwise reasonable people entirely buy into them. </p>



<h2>Data-driven ads are anything but</h2>



<p>I’ll start with a few examples of the data which is definitely held on me, and just how entirely bad my targeted advertising is. </p>



<p>Facebook know my age and date of birth. They have had this data since I signed up for the website, 15 years ago. They know exactly how old I am. They also know where I live. Hell, sometimes I used to check into places with my location on. Despite knowing I am way north of 30 and way south of Birmingham, they are incredibly keen on advertising me events explicitly limited to people under the age of 30 in the Birmingham area. </p>



<p>Google knew I wanted to buy a mattress. They knew this because I googled it. And I clicked through to a brand selling mattresses, and I bought myself a mattress. The brand know I googled said mattress. Google know I clicked through. From Google’s own analytics, they ought to know I bought the mattress. Since buying that mattress, I’ve been constantly advertised mattresses, especially the one I already own and they know I already own. </p>



<p>Some might claim that in fact the advertisers are being incredibly smart and they’re advertising me activities for women under 30 in Birmingham so I go and tell my friends who are under 30 in Birmingham to go and do that. But of course, Facebook would also know that I don’t have any friends in that demographic. Or maybe that mattress seller is trying to tell me to refer a friend to buy that mattress by reminding me that I own a very nice mattress. In which case, why isn’t it advertising the referral programme, which I know they have because I received several emails and a physical leaflet about it with the fucking mattress?</p>



<p>The more simple answer is that the advertisers aren’t being data driven at all. They’re ticking default boxes or casting wider nets. I’m getting advertised mattresses because I have ~an interest in mattresses~. I’m getting activities for women under 30 in Birmingham because I’m under 40 and on the same island as Birmingham.</p>



<p>For all the buzzwords about “data-driven” and “smart” and whatever else you want to call it, the advertisers are just going “eh, sounds about right” and letting a robot automate their job. </p>



<p>This, then, is the first grift in the chain. Despite claiming to their boss that they’re using “data-driven” advertising, they’re targeting their ads even less than taking out a quarter page in the local newspaper. </p>



<h2>The product: they could spy on you (but don’t)</h2>



<p>Everyone is rightly nervy about the sheer quantity of data that big companies hold on us. Social media companies know all about your demographic information, social connections and interests. Amazon knows exactly when you have an outbreak of aphids because you buy things to kill the nasty little beasties, and it probably also knows when you’ve had a nasty breakup because nobody listens to Fleetwood Mac’s Rumours on repeat at 3am when they’re in a good place. Google basically knows everything about you. </p>



<p>At least that’s the theory. And that’s the product that they’re selling to advertisers. They have an enormous dataset from which everything an advertiser could ever dream of about a person can be garnered. They’re the world’s biggest, bestest spy network, which means they have quality data to help <em>your</em> business be the biggest, bestest business reaching the biggest, bestest customers.</p>



<p>At least that’s what they say. </p>



<p>Actual spying requires actual spies. There’s a reason intelligence agencies are such big employers: they have all of their fancy spy computers, but they know they need to hire humans to actually deduce patterns and sort signal from noise. They’re aware that a human brain is always superior to a computer in figuring this out, so they get humans to do the work.</p>



<p>Meanwhile, tech companies break into hives at the thought of getting a human to do a job. Their ethos is that if a human can do a task, a machine can do that task better, and not cost them anything such as salary, pensions or or a basic level of respect. Tech companies are fatally allergic to getting a human to do a human job, so content moderation is largely an algorithm looking for the word “boobies”. A tech company would go into anaphylactic shock at the very notion of employing a human to analyse their vast dataset.</p>



<p>So it’s all machine learning, and the machines are very, very stupid. Have you ever looked at your list inferred interests on a social media platform? If you ever tweeted “I don’t like Game of Thrones, it’s not for me,” you’ll be classified as interested in Game of Thrones and possibly get served ads for it. These machines may also attempt to deduce your age, gender, and so forth based on half-baked crap fed into them, and it seldom comes up right. Maybe that’s why it thinks I’m under 30 and in Birmingham. Perhaps I internet in a Brummie accent. </p>



<p>It’s no wonder that on multiple occasions, big tech has been caught out completely making things up when communicating with advertisers, and they continue to do so. Facebook was famously found to have inflated or outright fabricated video metrics. GA4 very quietly admits that the data is padded out with machine learning. The data is a lie, and a lot of it is because they literally haven’t the first clue on what to do with it, they just need to steeple their fingers and act all evil so advertisers think they have it.</p>



<p>Advertisers, then, are getting served a steaming turd on a plate rather than the medium-rare filet mignon they were promised. </p>



<p>And meanwhile, the spies don’t even need that data, because your posts are public anyway.</p>



<p>But enough about that. The problem is this grift is, too, built upon a grift. </p>



<h2>Marketing science is a grift</h2>



<p>I work in marketing, for my sins. This is mostly why I’m so entirely down on the marketing industry and many of the people who work in it. I also happen to have an MSc in psychology – actual psychology! – with a focus on behaviour change. </p>



<p>On day 1 of your class about behaviour change in a science course, you learn that behaviour change is not a simple matter of information in, behaviour out. Human behaviour, and changing it, is big and complex. </p>



<p>Meanwhile, on your marketing courses, which I have had the misfortune to attend, the model of changing behaviour is pretty much this: information in, behaviour out.</p>



<p>The thing with the entire “science” of marketing is the underpinning theory base is basic common sense which has been treated with a bit of a brand makeover, turned into a couple of overcomplicated diagrams with some neologisms obscuring meaning. Digital marketing has become very popular because baked into it are a whole bunch of metrics so you have something to show your manager that you’re not spending the entire day tending your geraniums, but do the metrics really mean anything?</p>



<p>The metrics that marketers are told they need are marketed to them by the marketing department of a company that specialises in making products for marketers. And that company was probably started up by someone who worked in marketing. </p>



<p>Marketing theory is never tested rigorously. The <s>common sense</s> incredibly sound scientific view based on heaps of scientific evidence view – showing your ads to people more likely to buy your product is more efficient because they’re more likely to buy your product anyway – is entirely untested.</p>



<p>There’s an anecdote that a glitch with Facebook led to ads no longer being targeted over a period of several weeks. And absolutely nobody noticed because the metrics all looked normal, the engagement and purchasing was just the same.</p>



<p>There isn’t any evidence to suggest that an ad targeted to 35 year old men with children with an interest in football is any more likely to result in sales of Football Dad socks than a poster for Football Dad socks at a bus stop. But an entire industry is based on pretending that this is the case.</p>



<h2>tl;dr</h2>



<p>Facebook will try to sell you Football Dad socks even if you’re a 55 year old childfree woman who posted once about hating football, because that data is utterly useless. </p>



<p>Spies are probably reading your posts though, no matter how boring.</p>



<p>_</p>



<p><em>Enjoyed what you read? Consider&nbsp;<a href="https://www.patreon.com/user?u=2332646">becoming a Patron&nbsp;</a>or&nbsp;<a href="https://www.paypal.me/stavvers">leave a tip</a></em></p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Farmer ordered to pay $CAD82k after confusion over meaning of thumbs-up emoji (114 pts)]]></title>
            <link>https://www.abc.net.au/news/2023-07-08/canadian-farmer-pay-92k-fine-after-emoji-confusion/102579514</link>
            <guid>36643278</guid>
            <pubDate>Sat, 08 Jul 2023 11:04:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abc.net.au/news/2023-07-08/canadian-farmer-pay-92k-fine-after-emoji-confusion/102579514">https://www.abc.net.au/news/2023-07-08/canadian-farmer-pay-92k-fine-after-emoji-confusion/102579514</a>, See on <a href="https://news.ycombinator.com/item?id=36643278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="ArticleWeb"><header></header><div data-component="LayoutContainer" id="body"><div><p>A Canadian judge has ordered a farmer to pay&nbsp;more than $CAD82,000 ($92,000) in damages following a legal battle over what the thumbs-up emoji means.</p><p>Chris Achter, the owner of a farming company in Swift Current, Saskatchewan, had sent a thumbs-up emoji in response to a photograph of a flax-buying contract from a grains buyer in 2021.</p><p>Months later, the buyer — which&nbsp;had been doing business with Mr Achter for several years — did not receive the flax as expected.</p><p>That started a dispute that led to "a far-flung search" to unearth what the thumbs-up emoji means, according to the June court ruling that surfaced in local media this week.</p><p>The buyer, South West Terminal, argued that the emoji implied acceptance of contractual terms, while Mr Achter said he used it only to indicate that he had received the contract, but not to indicate his agreement.</p><p>In a summary judgement that contained 24 instances of the emoji, Judge T J&nbsp;Keene resolved the issue by ruling that a&nbsp;thumbs-up emoji is enough to&nbsp;accept contractual terms.</p><p>He said: "I am satisfied on the balance of probabilities that Chris okayed or approved the contract just like he had done before except this time he used a thumbs-up emoji."</p><p>"In my opinion the signature requirement was met by the thumbs-up emoji originating from Chris and his unique cell phone," the judge said.</p><p><strong>Reuters</strong></p></div><p><span data-component="Text">Posted<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2023-07-08T10:25:05.000Z">7 hours ago</time><time data-component="Text">Sat 8 Jul 2023 at 10:25am</time></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I am drowning in mutes: The current Threads experience (139 pts)]]></title>
            <link>https://internettalk.substack.com/p/i-am-drowning-in-mutes-please-help</link>
            <guid>36642932</guid>
            <pubDate>Sat, 08 Jul 2023 09:57:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://internettalk.substack.com/p/i-am-drowning-in-mutes-please-help">https://internettalk.substack.com/p/i-am-drowning-in-mutes-please-help</a>, See on <a href="https://news.ycombinator.com/item?id=36642932">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>After 2 days of Threads, my muted list has grown from empty to this.</p><p><span>An unholy concoction of celebrities, meme pages, and brands, making painfully unfunny&nbsp;</span><a href="https://www.theverge.com/2023/7/6/23784842/threads-instagram-tweets-post-name-thread" rel="nofollow ugc noopener">tweets</a><span>&nbsp;form the majority of this list, with religious groups, "gurus", and meta (no, not Meta, people just talking about the platform itself) tweets forming the rest.</span></p><p>This too, is true of the algorithmic feed. While Instagram Reels has an uncanny ability to focus in on what you enjoy (to the point where staring at a tractor meme for too long has caused my feed to orbit farming Reels), Threads shares none of this ability. A blast of whatever content the platform has, with little thought.</p><p>It seemed that you could batter it into submission by mass muting anything unfunny - but then a new wave of celebrities and brands appears. Keeping up becomes an impossible challenge.</p><p>And thus, unless this situation resolves itself in another way, I will eventually grow tired of muting people, and leave the platform.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bevy XPBD: A physics engine for the Bevy game engine (129 pts)]]></title>
            <link>https://joonaa.dev/blog/02/bevy-xpbd-0-1-0</link>
            <guid>36642867</guid>
            <pubDate>Sat, 08 Jul 2023 09:41:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joonaa.dev/blog/02/bevy-xpbd-0-1-0">https://joonaa.dev/blog/02/bevy-xpbd-0-1-0</a>, See on <a href="https://news.ycombinator.com/item?id=36642867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>I’ve been working on a Rust-based physics engine for about a year now, and a week ago I finally released it. Here is <a href="https://github.com/Jondolf/bevy_xpbd">Bevy XPBD</a> 0.1.0!</p>
<h2 id="what-is-bevy-xpbd">What is Bevy XPBD?</h2>
<p>Bevy XPBD is a 2D and 3D physics engine for <a href="https://bevyengine.org/">Bevy</a>, a refreshingly simple data-driven game engine built in Rust.</p>
<p>It uses a newer physics simulation method called <em>Extended Position Based Dynamics</em>, which provides unconditionally stable, time step independent, and physically accurate simulations. Unlike other physics engines in the Bevy ecosystem, Bevy XPBD is made specifically <em>for</em> Bevy <em>with</em> Bevy, and it uses the <em>Entity Component System</em> (ECS) for both the public API and the internals.</p>
<p>Bevy XPBD 0.1 already has tons of features, including these:</p>
<ul>
<li>Dynamic, kinematic and static rigid bodies</li>
<li>Collision detection and collision response</li>
<li>Collision events</li>
<li>Access to colliding entities</li>
<li>Sensor colliders</li>
<li>Collision layers</li>
<li>Restitution and friction</li>
<li>Gravity, external forces and torque</li>
<li>Joints</li>
<li>Built-in XPBD constraints and support for custom constraints</li>
<li>Modular plugin architecture, allowing you to swap existing functionality with custom implementations</li>
<li>Configurable timestep and subtepping</li>
</ul>
<h2 id="getting-started">Getting started</h2>
<p>If you are new to Rust or Bevy, you should go through Bevy’s <a href="https://bevyengine.org/learn/book/getting-started/">Getting started guide</a>.</p>
<p>Once you are ready, you need to add Bevy XPBD to your <code>Cargo.toml</code>. You should use <code>bevy_xpbd_2d</code> for 2D projects and <code>bevy_xpbd_3d</code> for 3D projects.</p>
<pre is:raw="" tabindex="0"><code><span><span>[</span><span>dependencies</span><span>]</span></span>
<span><span>bevy_xpbd_3d = </span><span>"0.1.0"</span></span></code></pre>
<p>By default, Bevy XPBD uses <code>f32</code> numbers. If you encounter instability or use a large number of substeps, you might want to use <code>f64</code> instead. You can disable the default features and manually specify the feature flags you want:</p>
<pre is:raw="" tabindex="0"><code><span><span>[</span><span>dependencies</span><span>]</span></span>
<span><span># Add 3D Bevy XPBD with double-precision floating point numbers</span></span>
<span><span>bevy_xpbd_3d = { version = </span><span>"0.1"</span><span>, default-features = </span><span>false</span><span>, features = [</span><span>"3d"</span><span>, </span><span>"f64"</span><span>] }</span></span></code></pre>
<p>Next, add the <code>PhysicsPlugins</code> plugin group. Bevy XPBD is highly modular, and this plugin group adds all of the default physics plugins to your application.</p>
<pre is:raw="" tabindex="0"><code><span><span>use</span><span> </span><span>bevy</span><span>::</span><span>prelude</span><span>::*</span><span>;</span></span>
<span><span>use</span><span> </span><span>bevy_xpbd_3d</span><span>::</span><span>prelude</span><span>::*</span><span>;</span></span>
<span></span>
<span><span>fn</span><span> </span><span>main</span><span>() {</span></span>
<span><span>    </span><span>App</span><span>::</span><span>new</span><span>()</span></span>
<span><span>        </span><span>.</span><span>add_plugins</span><span>(</span><span>DefaultPlugins</span><span>)</span></span>
<span><span>        </span><span>.</span><span>add_plugins</span><span>(</span><span>PhysicsPlugins</span><span>)</span></span>
<span><span>        // ...your other plugins, systems and resources</span></span>
<span><span>        </span><span>.</span><span>run</span><span>();</span></span>
<span><span>}</span></span></code></pre>
<p>Now you can use all of Bevy XPBD’s components and resources to build whatever you want! For example, adding a rigid body with a collider is as simple as spawning an entity with the <code>RigidBody</code> and <code>Collider</code> components:</p>
<pre is:raw="" tabindex="0"><code><span><span>fn</span><span> </span><span>setup</span><span>(</span><span>mut</span><span> commands</span><span>:</span><span> </span><span>Commands</span><span>) {</span></span>
<span><span>    commands</span><span>.</span><span>spawn</span><span>((</span><span>RigidBody</span><span>::</span><span>Dynamic</span><span>, </span><span>Collider</span><span>::</span><span>ball</span><span>(</span><span>0.5</span><span>)));</span></span>
<span><span>}</span></span></code></pre>
<p>To learn more, refer to the <a href="https://docs.rs/bevy_xpbd_3d/0.1.0/bevy_xpbd_3d/">official documentation</a> and check out the <a href="https://github.com/Jondolf/bevy_xpbd">GitHub repository</a> and the <a href="https://github.com/Jondolf/bevy_xpbd/tree/main/crates/bevy_xpbd_2d/examples">2D</a> and <a href="https://github.com/Jondolf/bevy_xpbd/tree/main/crates/bevy_xpbd_3d/examples">3D</a> examples.</p>
<h2 id="why-make-a-physics-engine-for-bevy">Why make a physics engine for Bevy?</h2>
<p>I started development purely out of interest. I had been playing around with Bevy for a little while when I saw a <a href="https://johanhelsing.studio/posts/bevy-xpbd">tutorial series by Johan Helsing</a> where he created a simple XPBD physics engine for Bevy from scratch. I had seen a <a href="https://www.youtube.com/watch?v=F0QwAhUnpr4">Two Minute Papers video</a> on the topic earlier, so I decided to follow the tutorial and build along.</p>
<p>The tutorial series wasn’t/isn’t finished yet, so I quickly reached the end. I had 2D objects colliding realistically with each other, but lots of features were missing: friction, 3D support, joints, and a lot of other common physics engine capabilities. I decided to continue developing the engine, reading academic papers on XPBD and taking inspiration from existing physics engines like <a href="https://rapier.rs/">Rapier</a>.</p>
<p>As I continued adding features and improving the API, I realized that I was actually starting to have a decently functional physics engine. I started seeing some interest in my engine on the <a href="https://discord.gg/bevy">Bevy Discord server</a>, and I realized that this was truly a lacking area in the ecosystem. Bevy doesn’t have an official physics engine yet, and <code>bevy_rapier</code> is essentially the only good option, but it’s just a wrapper over Rapier and doesn’t use Bevy’s ECS for any of its internals. People would prefer a more native solution that is made specifically for Bevy and uses the ECS directly instead of relying on a separate physics world and synchronizing a monolithic data structure with the Bevy world.</p>
<p>This is what I am trying to accomplish with Bevy XPBD. As far as I know, it is the only pure Bevy ECS physics engine that is currently maintained, and I hope to continue on this road to help improve the state of physics in the Bevy ecosystem.</p>
<h2 id="whats-next">What’s next?</h2>
<p>Bevy XPBD is already quite usable in many cases, but it is missing several important features, and there are some stability and performance issues. Keep in mind that the engine is very young and physics engines can be quite massive, so it can take a while to reach a production ready state.</p>
<p>0.2 will be released in a couple of weeks, right after the release of Bevy 0.11. Here are some of the features and improvements that I have in mind:</p>
<ul>
<li>Update to Bevy 0.11</li>
<li>Spatial queries
<ul>
<li>Ray casting</li>
<li>Shape casting</li>
<li>Point projection</li>
</ul>
</li>
<li>Filters for collisions and spatial queries (exclude entities etc.)</li>
<li>Multiple colliders per body</li>
<li>Collider offset and scale</li>
<li>Bug fixes and documentation improvements</li>
</ul>
<p>Further into the future, I am planning on implementing some of these features:</p>
<ul>
<li>Physics scale (e.g. pixels per meter)</li>
<li>Joint motors</li>
<li>Soft bodies and cloth simulation</li>
<li>Locking translational and rotational axes</li>
<li>Parallel solver to improve performance</li>
<li>Full cross-platform determinism</li>
<li>A website for WASM demos</li>
</ul>
<p>Note that after 0.2, development might be a lot more inconsistent for a while because of my studies, as I will be graduating from high school next year, and I will have my matriculation exams in September and March. However, I will try to keep up with Bevy releases and respond to issues and questions, and I hope to work on the engine more actively once I am done with my exams.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Huge thanks to everyone on the <a href="https://discord.gg/bevy">Bevy Discord server</a> who has been interested in the project and helped me get this far, and especially to <a href="https://johanhelsing.studio/">Johan Helsing</a> who inspired me to originally begin development and later helped me with the project. Without your help I would have probably stopped development and abandoned the project, and I am very glad I didn’t.</p>
<p>It’s crazy to me how we have reached over 120 stars on GitHub just a week after release, and I think it just shows how much the community wants a native physics engine that is made specifically for Bevy in an ergonomic ECS-based way.</p>
<p>I hope to continue working on Bevy XPBD as much as I can, and to continue being a part of the amazing community. Stay tuned for the next update in a couple of weeks when 0.2 is released!</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The ARM powered ThinkPad x13s, as a developer (102 pts)]]></title>
            <link>https://www.dev-eloper.com/thinkpad-x13s-as-a-developer/</link>
            <guid>36642825</guid>
            <pubDate>Sat, 08 Jul 2023 09:34:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dev-eloper.com/thinkpad-x13s-as-a-developer/">https://www.dev-eloper.com/thinkpad-x13s-as-a-developer/</a>, See on <a href="https://news.ycombinator.com/item?id=36642825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        

            <p>The ARM powered Thinkpad x13s is the first truly premium Windows on ARM laptop.  Let's see how that actually stacks up as a software engineer!</p>

        <section>

            <ul>
                <li>
                    <a href="https://www.dev-eloper.com/author/devin/"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M3.513 18.998C4.749 15.504 8.082 13 12 13s7.251 2.504 8.487 5.998C18.47 21.442 15.417 23 12 23s-6.47-1.558-8.487-4.002zM12 12c2.21 0 4-2.79 4-5s-1.79-4-4-4-4 1.79-4 4 1.79 5 4 5z" fill="#FFF"></path></g></svg>
</a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2023-06-24">Jun 24, 2023</time>
                        <span><span>•</span> 13 min read</span>
                </p>
            </div>

        </section>

            <figure>
                <img srcset="https://www.dev-eloper.com/content/images/size/w300/2023/06/Thinkpadx13s_header2.jpg 300w,
                            https://www.dev-eloper.com/content/images/size/w600/2023/06/Thinkpadx13s_header2.jpg 600w,
                            https://www.dev-eloper.com/content/images/size/w1000/2023/06/Thinkpadx13s_header2.jpg 1000w,
                            https://www.dev-eloper.com/content/images/size/w2000/2023/06/Thinkpadx13s_header2.jpg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://www.dev-eloper.com/content/images/size/w2000/2023/06/Thinkpadx13s_header2.jpg" alt="Picture of my Thinkpad x13s">
            </figure>

    </header>

    <section>
        <h3 id="the-long-and-winding-intro-that-leads-to-your-door">The long and winding intro (That leads to your door)</h3><p>So, you're a software engineer who loves Windows (We exist, I swear). &nbsp;More than that, you're a software engineer who loves being able to work detached from a desk, free to work in coffee shops, on busses, out at a park, or really wherever your heart desires at any given moment. &nbsp;You know you COULD switch to a Mac laptop, such as the Macbook Pro with an M-series chip, to get that sweet, sweet battery life you desired, but either you need to work in the Windows ecosystem or maybe like me you just don't like MacOS. &nbsp;What do you do?</p><p>So, you start by looking around at laptops. &nbsp;Microsoft's own Surface Laptop Studio sure sounds slick, but the battery life is atrocious. &nbsp;Same for the Framework laptop, even though you'd love to support the company. &nbsp;There are some Asus laptops that seem like they'd fit the bill, but they're just so clunky and large, which gets in the way of the portable dream. &nbsp;That's when you see it: &nbsp;A new Lenovo that claims insane 28-hour battery life, all in a thirteen-inch form factor. &nbsp;That is EXACTLY what you need!</p><p>You order it without reading too much into it and wait for it to show up. &nbsp;The day finally arrives, and the small brown box opens to reveal a slick little black-ish (Maybe more of a dark gray) laptop, a 65-watt USB-C charging brick, and precious little else. &nbsp;You boot it up, set up your account, and start looking through the specs. &nbsp;32GB of RAM, Snapdragon 8cx Gen 3 processor, 1TB NVMe SS- wait what was that last one?</p><p>Oh right, as it turns out this slick little device runs Windows on ARM on the latest Snapdragon processor, the Snapdragon 8cx Gen 3. &nbsp;"Well fuck," you say disappointedly, "That might be a problem. &nbsp;Do my development tools even run on this OS? &nbsp;What about git? &nbsp;Am I going to actually be able to compile code on this?"</p><p>Well fear not, imaginary developer, for I have already bought this device to use as my own development machine, and I want to share with you what I've discovered in my exploration of the device. &nbsp;While this isn't my first WoA powered device (I've been a long-time user of the SQ1 Surface Pro X and the Galaxy Book2 before that) this is the first one I've really bought for the express purpose of software engineering. &nbsp;I want to dive into the pros, the cons, and just my overall impressions about the Thinkpad x13s, Windows on ARM in general, and life as a developer on one of these machines.</p><h3 id="introductory-statements">Introductory statements</h3><p>A few things to note first and foremost:</p><ol><li>I primarily work in the JVM language space. &nbsp;My preferred language (and the one I use in a professional capacity) is Kotlin. &nbsp;I also have over a decade of professional Java development under my belt. &nbsp;The JVM and its languages are what I know and where I do my work. &nbsp;I will do my best to speak to development using Windows native languages like the .net suite, but my scope there is limited.</li><li>I enjoy using Windows, but I'm not tied to it. &nbsp;Most of my professional life involves Linux rather than Windows, and most of my engineering effort takes place in WSL2. &nbsp;That being said, I spent many years before WSL ever launched running everything on just Windows.</li><li>Straightup, I very much enjoy this laptop, but I will not mince words with the issues I have with it. &nbsp;There are a number of glaring issues with the device that people need to be aware of, and most reviewers looking at devices like this are not software engineers and will not run into these issues.</li><li>This is a long-term review, following nearly a year with my own personal Thinkpad x13s device.</li></ol><p>OK, now that I've gotten a lengthy and boring intro out of the way, along with some overly long introductory statements, let's start actually reviewing this thing.</p><h3 id="about-my-device">About my device</h3><p>Ah, ok, wait, no hold on, one more thing, we should probably actually go over what device specs we're looking at here so we can be clear about what we're talking about. </p><!--kg-card-begin: markdown--><table>
<thead>
<tr>
<th>Component</th>
<th>Spec</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>Snapdragon 8cx Gen 3 (3.0 GHz)</td>
</tr>
<tr>
<td>Storage</td>
<td>512GB SSD M.2 2242 PCIe Gen4 TLC Opal</td>
</tr>
<tr>
<td>Memory</td>
<td>32GB LPDDR4x 4266MHz (Dual Channel, Soldered)</td>
</tr>
<tr>
<td>Battery</td>
<td>49.5 Whr</td>
</tr>
<tr>
<td>Display</td>
<td>13.3" WUXGA (1920 x 1200), IPS, Anti-Glare, Touch, 72%NTSC, 300 nits, 60Hz, LED Backlight</td>
</tr>
<tr>
<td>Graphics</td>
<td>Integrated Qualcomm Adreno 690 Graphics</td>
</tr>
<tr>
<td>Camera</td>
<td>5MP RGB+IR with Microphone</td>
</tr>
<tr>
<td>WLAN</td>
<td>Qualcomm Wi-Fi 6E WCN6855 2x2 AX &amp; Bluetooth 5.1</td>
</tr>
<tr>
<td>OS</td>
<td>Windows 11 Pro on ARM</td>
</tr>
</tbody>
</table>
<!--kg-card-end: markdown--><h3 id="hardware">Hardware</h3><p>OK, let's talk about the hardware feel and build quality of this thing. &nbsp;The x13s chassis is made of 90% recycled magnesium and covered with a soft-touch black paint (I think? &nbsp;Maybe a polycarbonate?) layer. &nbsp;The keyboard has very little flex and is reasonably sized. &nbsp;The laptop screen is a 300-400 nit matte touch screen with a pair of hinges that very easily pass the one-handed open test.</p><p>The keyboard itself is very similar to the Macbook Air, both in size and feel. &nbsp;As is typical with Thinkpads, the track point and physical mouse buttons join the keyboard layout, which is useful for a certain kind of person. &nbsp;Personally, I prefer the touchpad, which is of a reasonable size. &nbsp;Certainly, the touchpad could be bigger, but the aforementioned physical mouse buttons take up space below the keyboard that could otherwise be taken up by a bigger trackpad. &nbsp;Additionally, the trackpad is just slick plastic rather than glass, which is a shame especially considering the price tag of the laptop, but we'll come back to that.</p><p>The screen on mine is the 300-nit touchscreen variant, which runs at 60Hz. &nbsp;Color-wise it's. . . fine, covering only 72% of the NTSC color gamut. &nbsp;Definitely not amazing, it's not something I'd ever recommend to someone who is doing photo or video editing. &nbsp;The 60hz refresh rate <em>feels</em> like a 60Hz refresh rate. &nbsp;If you've ever used a 90 or 120Hz display, you'll know what I mean. &nbsp;As with the plastic trackpad, the price of this laptop really demands better on this front.</p><p>In terms of I/O, we have two USB-C 3.2 Gen 2 ports (with display out), a 3.5mm headphone jack, a Kensington lock slot, and an optional SIM slot for the on-board 5G radios. &nbsp;I don't have the 5G enabled version as it wasn't available when I bought mine, which I do regret, but if it's even half as good as it is on the SPX then it'll be totally fine. Additionally, the onboard wifi is 6E which is a nice addition. &nbsp;All in all this isn't <em>terrible</em> I/O. &nbsp;People get too caught up in laptops missing USB-A ports, but for a travel-focused device I don't really see the appeal. &nbsp;Maybe it's my software engineer talking, but I haven't used a peripheral on a laptop that wasn't USB-C or Bluetooth since the mid 2010s.</p><p>In general, the laptop feels good to use. &nbsp;The magnesium chassis feels cool to the touch and solid at the same time. &nbsp;The keyboard is comfortable to type on with good key travel. &nbsp;The track point, mouse buttons, and trackpad are all good enough. &nbsp;The screen works fine in daylight, though the low nit count requires using light mode rather than dark mode to compensate. &nbsp;All in all, it's a nice little package.</p><figure><div><p><img src="https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_side.jpg" width="1024" height="768" loading="lazy" alt="" srcset="https://www.dev-eloper.com/content/images/size/w600/2023/06/Thinkpadx13s_side.jpg 600w, https://www.dev-eloper.com/content/images/size/w1000/2023/06/Thinkpadx13s_side.jpg 1000w, https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_side.jpg 1024w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_header-2.jpg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.dev-eloper.com/content/images/size/w600/2023/06/Thinkpadx13s_header-2.jpg 600w, https://www.dev-eloper.com/content/images/size/w1000/2023/06/Thinkpadx13s_header-2.jpg 1000w, https://www.dev-eloper.com/content/images/size/w1600/2023/06/Thinkpadx13s_header-2.jpg 1600w, https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_header-2.jpg 2048w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_back.jpg" width="1024" height="768" loading="lazy" alt="" srcset="https://www.dev-eloper.com/content/images/size/w600/2023/06/Thinkpadx13s_back.jpg 600w, https://www.dev-eloper.com/content/images/size/w1000/2023/06/Thinkpadx13s_back.jpg 1000w, https://www.dev-eloper.com/content/images/2023/06/Thinkpadx13s_back.jpg 1024w" sizes="(min-width: 720px) 720px"></p></div></figure><h3 id="software-compatibility-and-emulation">Software compatibility and Emulation</h3><p>Easily the biggest question everyone is going to have about this laptop is "Can I run the software I need?" Given the ARM nature of the laptop that's a completely reasonable question to ask. &nbsp;I, obviously, cannot speak to every piece of software ever written, but I can talk about the good and the bad that I've seen.</p><p>We'll start with the restrictions. &nbsp;Unlike a traditional laptop, the x13s uses an ARM64 processor, meaning that nearly all software written for Windows in the last 30 years won't run natively on it. &nbsp;To compensate for this, Microsoft has two emulation layers in place to translate traditional x86 and x86_64 machine code to ARM64 machine code. &nbsp;There are limitations here, of course. &nbsp;Emulation is inherently slower than native code, there isn't any way around that. &nbsp;Secondly, the emulation layer is user-space only meaning drivers can't be emulated, so anything that relies on custom drivers is a no-go unless they've been recompiled for ARM. &nbsp;The final note is that OpenGL calls are translated to DirectX calls, but that only goes as far as OpenGL 3.3, and Vulkan is right out.</p><p>In terms of actual performance of the emulation, we're going to avoid hard numbers and talk more about "feel," because most of that time that's what you're going to experience. &nbsp;There are really 3 groups of programs that we need to talk about, and each one is going to have a different experience as they make use of different elements of the emulation. &nbsp;There are "office" applications, these are things like Slack, Outlook, or Chrome. &nbsp;Then there are "Performance" applications, for us software engineers these are going to be things like our IDE, compiler, database software, etc. &nbsp;Finally, there are "GPU-intensive" programs, which generally means games and photo/video editing, but can also mean GPU-reliant libraries like Tensorflow.</p><p>For "Office" applications, 8 times out of 10 you won't even realize you're not using a native program. &nbsp;Most of these applications don't rely on anything CPU or GPU-intensive and once the code is cached in the emulation layer it'll run basically without issue. &nbsp;However, those remaining two times are going to be <em>painful</em>. &nbsp;Chrome and Slack are great examples of this. &nbsp;Chrome is slow and frustrating to use, with delayed animations and noticeable input lag, it's such a frustration that I entirely gave up on Chrome in favor of Edge which runs natively (Though for those who want to avoid Chromium in general, Firefox also has native ARM64 Windows builds). &nbsp;Slack is frustrating in its own way, running fine one second, then freezing for 10-15 seconds at a time, usually when switching to a new conversation or channel. &nbsp;To make matters worse, Slack from the Microsoft Store installs as x86_64, which makes the problem significantly worse for whatever reason. &nbsp;If you need Slack either download the x86 version directly from their website, or run it as a browser tab.</p><p>"Performance" applications are more of a mixed bag. &nbsp;As a JVM focused developer, I primarily use IntelliJ as my IDE so I'm going to focus on that experience. &nbsp;Luckily, this works out because Jetbrains has, in the last few months, released ARM64-compatible versions of its products, including IntelliJ. &nbsp;As such I've used the x86 emulation version, a hacked together ARM64 solution using an ARM version of Java, and the fully native solution. &nbsp;The difference here is night and day between all three. &nbsp;The emulated version ran, and that's about as much as I'm willing to give it. &nbsp;It worked, you could open/compile using it, but it wasn't pleasant, and larger projects were noticably slower to interact with and navigate around. &nbsp;It also wasn't able to access WSL, meaning I was locked out of using Linux tooling all together. &nbsp; The hacked solution was immediately better, but like the emulated version, it wasn't able to access WSL, and it required a fair amount of extra setup to even get working that most users (even other software engineers) wouldn't have even known to do. &nbsp;The native solution solved these problems entirely and runs so well that you'd never know it wasn't an Intel/AMD machine.</p><p>But like I said, it's a mixed bag. &nbsp;Other tools, such as MySQL, node, git, etc. have no issue when running under emulation, at least in the context of development (I doubt I'd say the same if I was running production using emulation). &nbsp;Most of the time you won't even realize these are emulated! A good rule of thumb is "If there isn't a UI, the performance is going to be on-par with a native build."</p><p>Finally, "GPU-Intensive" applications are. . . &nbsp;not great. &nbsp;There does exist a Tensorflow library aimed at ARM platforms that you can import, but it's not official so you're at risk of that falling through. &nbsp;Other GPU intensive tasks like video and photo editing either lack support (I don't think there is a single high-quality video editor with Windows on ARM support) or are a nightmare to use (Photoshop has Windows on ARM support, but it's glitchy as hell). &nbsp;Games are almost entirely out, though I won't say 100% out. &nbsp;Starcraft 2 runs, but you wouldn't want to play beyond the 4 minute mark. &nbsp;Demeo on the other hand runs incredibly well to the point where they demoed the game at PAX on the x13s. At the end of the day the emulation works for GPU intensive apps, but not with enough consistency or performance to recommend those things.</p><p>Long story short, if you're using GPU heavy workflows you're probably going to be fine. &nbsp;More and more development tools are making their way over to being ARM native, but you should absolutely check if yours are before you commit. &nbsp;I should also mention that Visual Studio Code does have an ARM64 release, which would likely cover just about everybody in a pinch.</p><p>One thing I should note here is that this is really a Qualcomm problem rather than a Microsoft problem. &nbsp;The emulation layer is actually quite fantastic, and when you Windows on ARM on an M-series Mac you don't see most of these issues. &nbsp;Honestly, Microsoft has done an incredible job with their emulation layer, and I have to commend them on that.</p><h3 id="native-performance">Native Performance</h3><p>The performance of native applications, as I've already alluded, is totally fine. &nbsp;Geekbench and other benchmarks will tell you that the Snapdragon 8cx Gen 3 is at best on par with an anemic i5, but in practice I don't think I've ever noticed performance problems with native applications. &nbsp;Intellij, Edge, Spotify (Beta), WSA, WSL, and many other ARM compiled programs run without any issue. &nbsp;Far and away, my experience with the performance has been very positive, and hasn't impeded my software development experience in the slightest.</p><p>On top of that, more and more development tools are becoming available for Windows on ARM. &nbsp;For example, Java has had native builds for nearly 3 years now, and Node got Windows on ARM support as of Node 20. &nbsp;While the performance bottlenecks of these pipeline tools has been minimal, the more tools that move over, the better off we'll all be. &nbsp;As I mentioned before, Visual Studio Code has been ARM64 compatible since 2020, and Visual Studio since 2022, so between those and the Jetbrains suite of IDEs most every major language has at least one native IDE now.</p><h3 id="battery-life">Battery Life</h3><p>OK, so what about battery life? &nbsp;I started this whole post off repeating Lenovo's claim of a 28-hour battery life, so does it actually live up to that?</p><p>No, of course not, not in any meaningful way. &nbsp;As with all battery life claims, the 28 hours is under <strong><em>extremely specific</em></strong><em> </em>conditions. &nbsp;In the real world, though, the battery life is still impressive! &nbsp;Much of my job as an engineering manager is spent doing admin work in Jira and taking Zoom/Meet calls. &nbsp;On those days I get around 8-10 hours or so of battery life, though the video calls cut into that severely. &nbsp;On days I'm doing development work, I get closer to 5-6 hours. &nbsp;This is <em>significantly better</em> than the 2 hours of development work or 3 hours of admin-only work I get on my Framework (11th gen, 55wh battery). &nbsp; As with all things battery life, your mileage may vary, especially if you're toolchain is dependent on non-native code.</p><p>This might all sound like a kick in the teeth considering the 28-hour promised battery life, but it's a day an night difference against most Intel/AMD machines, and lines up with my experience doing dev work on an M1 Macbook Pro. &nbsp;It helps the device feel like a truly portable work device.</p><h3 id="issues">Issues</h3><p>Now it's time to talk about the less pleasant aspects of the x13s, though honestly it's nothing we haven't touched on before, and they all really come back to one thing: The Thinkpad x13s is <em>mad expensive.</em></p><p>How expensive, you ask? At the time of writing the cost of a same-spec x13s as mine has a price tag of $2466 USD. &nbsp;Bumping that up to a 1 TB SSD takes the total cost to $2898.00. &nbsp;That's a high price tag, even for high-end Intel/AMD Windows machines. &nbsp;The Surface Laptop Studio with an i7 and 32GB of RAM and 1 TB of space is $2699 (2199.99 on sale!). &nbsp;To compound that, the device lacks the high-end features one would expect from a device at this price point: &nbsp;Low max brightness, mediocre NTSC color gamut, plastic track pad — all of these are more in line with an $1200 price tag, not twice that!</p><p>Luckily (or perhaps shadily), you will rarely ever pay the full price for the Thinkpad x13s. &nbsp;As is tradition, Lenovo seems to be perpetually running deals, taking up to 40% off (at time of writing, 45% time of purchase), bringing the laptop to a much more reasonable $1479.60. &nbsp;That's still a high price tag for the device, but much closer to reasonable.</p><p>For a quick comparison, a Macbook Pro 14" with an M2 Pro processor, 1 TB of storage, and 32GB of RAM clocks in at $3099.00 (Edit: At some point after I published this, the prices reflected on Apple's site changed. &nbsp;This may have been due to some failure on my part when I wrote the article, but the accurate price for a 1TB 32GB RAM M2 pro is $2599.00. &nbsp;Thanks to <a href="https://news.ycombinator.com/user?id=justinclift&amp;ref=dev-eloper.com">justinclift</a> for the correction!). &nbsp;A refurbished Macbook Pro 14" with an M1 Pro and 1 TB of space comes in at $1999.00 at the low-end. &nbsp;While these undoubtedly out-perform the Thinkpad x13s, it's hard to argue that the performance difference for most users, even engineers, is worth the extra $500.</p><h3 id="round-up">Round-up</h3><p>So, where do we land here? &nbsp;We have a device with well above average battery life in real world conditions that, when running native code, is indistinguishable performance wise from your average Intel/AMD fair, and is perfectly capable of being a full-time software engineering machine for your average full stack web developer. &nbsp;It struggles with GPU tasks that aren't ARM optimized, lacks the power needed to really push through on more complicated emulation tasks, and lacks the premium fit and finish you would expect for a device that costs nearly $2500.</p><p>At the end of the day, this is a niche product that is best aimed at people who believe more in "Work from Anywhere" than "Work from Home." &nbsp;If you're a coffee shop warrior, or someone who believes that anywhere can be their office this might be an ideal companion if you'd rather not hop the OS divide to Apple. &nbsp;That's definitely the case for me, and I would happily recommend it to people who are tech-savvy enough to troubleshoot and work around issues, and patient enough wait for developers to produce native binaries when needed. &nbsp;But I will be the first to admit that I would not give this a <em>general</em> recommendation. &nbsp;There are enough rough edges left with Windows on ARM, especially when paired with the current Snapdragon series of processors, that it's still not the right solution for most.</p>
    </section>


</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software engineers hate code (294 pts)]]></title>
            <link>https://www.dancowell.com/software-engineers-hate-code/</link>
            <guid>36642796</guid>
            <pubDate>Sat, 08 Jul 2023 09:28:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dancowell.com/software-engineers-hate-code/">https://www.dancowell.com/software-engineers-hate-code/</a>, See on <a href="https://news.ycombinator.com/item?id=36642796">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>This is the best-kept secret of the software engineering profession: engineers hate code. Especially code written by other people. It's why they love working on greenfield projects so much. No code, no maintenance, no headaches!</p><p>Ever wondered why microservices took off in teams of all sizes? A microservice architecture is the perfect way to pretend that the code you wrote last month no longer exists! Now that it has been stuffed into a container and tucked behind a load balancer it's a <em>service</em> and we can forget all about it until it breaks, then we deprecate it as <em>legacy</em> and replace it with something new. Rolling green fields forever!</p><p>Got a question about how one of your dependencies works? You could look at its implementation, or the test suite, but most engineers prefer to go to the place that everyone congregates to talk about - but not look at - code. Stack Overflow is a great resource for finding the code needed to solve your problem without having to look at a lot of code yourself!</p><p>I'm sure you've been stuck for hours begging your colleagues to review your pull request. Why do you think it's taking so long? You're asking them to do the thing they hate most - look at someone else's code!</p><p>Most people reading this have at one point or another approved a non-trivial pull request with a simple "LGTM." You got tagged in on a bad day and didn't have the time to take a proper look at code written by someone else. You had work to do!</p><h2 id="only-one-thing-can-overcome-engineers-hatred-of-code-their-love-of-writing-code">Only one thing can overcome engineers' hatred of code: their love of <em>writing</em> code.</h2><p>Software engineers will lock themselves in a room and do nothing but write code for hours. Some forget to eat, sleep or poop.</p><p>Notable engineers online invest hours of time writing <em>about </em>their code, or about <em>how </em>they write code. Paradoxically, engineers love reading this stuff, even if they never read the example code attached!</p><p>Meetings of all kinds, technical and user documentation, testing, post-release monitoring, refactoring - all are common sources of frustration that cut into valuable time that could otherwise be spent writing code!</p><p>Engineers will spend enormous effort learning or building tools to help them write more code. In the past couple of years, we've seen an entirely new generation of tooling emerge that can actually write code by itself, turning 10x engineers into 1,000x engineers*!</p><figure><img src="https://www.dancowell.com/content/images/2023/07/use-ai.jpg" alt="" loading="lazy" width="1920" height="1080" srcset="https://www.dancowell.com/content/images/size/w600/2023/07/use-ai.jpg 600w, https://www.dancowell.com/content/images/size/w1000/2023/07/use-ai.jpg 1000w, https://www.dancowell.com/content/images/size/w1600/2023/07/use-ai.jpg 1600w, https://www.dancowell.com/content/images/2023/07/use-ai.jpg 1920w" sizes="(min-width: 720px) 720px"><figcaption>* Scientists observing engineers using AI in the wild have seen them writing 100x more SLOC/hour!</figcaption></figure><p>Rarely, you will encounter engineers who have learned to temper their baser instincts and instead find a sick kind of joy in reading, understanding, modifying and even <em>deleting</em> other peoples' code. We call these odd folks "Senior Engineers."</p><p>Senior engineers have learned through hard-won experience that writing code is the ultimate diminishing return.</p><p>They know that code becomes legacy the moment the first byte is saved to disk. The rolling green fields of their youth are a happy delusion, distracting from the cold, hard truth that all code demands maintenance. They have felt the pain of an unmaintained system breaking at the worst possible time.</p><p>There are a limited number of hours in the day. The more code that gets written, the more things there are to break, and more of those precious hours will be taken up by maintenance.</p><p>The only logical course of action is to minimize the amount of code in production at any given time. Senior engineers' passion for writing code has been augmented with an even stronger desire to delete it.</p><p>Code that doesn't exist can't hurt us, or the people we love.</p><p>It demands no maintenance; it causes no downtime; it requires no testing. Senior engineers understand that unnecessary code should be eliminated at all costs, and all new code must prove its worth before being allowed to live. This is part of what drives them to pore over other peoples' code and provide meticulous review.</p><p>This isn't to say that senior engineers are cynics. There's still beauty to be found in creating an elegant solution to a complex problem. The joy of creation hasn't diminished, but it has been tempered by an understanding that less is more, and that every line of code they write comes at a cost.</p><p>Senior engineers hate <em>extraneous</em> code. They hate seeing time and effort invested in building yet another solution to an already-solved problem. They hate code that doesn't <em>need</em> to exist; code that isn't providing value.</p><h2 id="be-mindful-of-the-cost-that-your-code-incurs">Be mindful of the cost that your code incurs.</h2><p>Don't write new code when you can use, improve or fix what already exists. If you must write new code, write only what you need to get the job done.</p><p>Understand your tools, and the systems your code runs on. Leverage the features of those systems to minimize the code you need to write, and by extension, the cost that it imposes on you and your team.</p><div><p>💡</p><div><p>Some artistic liberties have been taken in this post for the sake of the narrative, however the qualities described above reflect attitudes I've witnessed in engineers that I've worked with in the past.</p><p>There's more to being a senior engineer than just a healthy skepticism about writing code to solve a problem, but it's an important quality to develop, and will serve any engineer well.</p></div></div>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Train an AI model once and deploy on any cloud (178 pts)]]></title>
            <link>https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/</link>
            <guid>36642315</guid>
            <pubDate>Sat, 08 Jul 2023 07:54:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/">https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/</a>, See on <a href="https://news.ycombinator.com/item?id=36642315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Organizations are increasingly adopting hybrid and multi-cloud strategies to access the latest compute resources, consistently support worldwide customers, and optimize cost. However, a major challenge that engineering teams face is operationalizing AI applications across different platforms as the stack changes. This requires MLOps teams to familiarize themselves with different environments and developers to customize applications to run across target platforms.</p>



<p>NVIDIA offers a consistent, full stack to develop on a GPU-powered on-premises or on-cloud instance. You can then deploy that AI application on any GPU-powered platform without code changes.<strong></strong></p>



<h2>Introducing the latest NVIDIA Virtual Machine Image</h2>



<p>The NVIDIA Cloud Native Stack Virtual Machine Image (VMI) is GPU-accelerated. It comes pre-installed with Cloud Native Stack, which is a reference architecture that includes upstream Kubernetes and the NVIDIA GPU Operator. NVIDIA Cloud Native Stack VMI enables you to build, test, and run GPU-accelerated containerized applications orchestrated by Kubernetes.</p>



<p>The NVIDIA GPU Operator automates the lifecycle management of the software required to expose GPUs on Kubernetes. It enables advanced functionality, including better GPU performance, utilization, and telemetry. Certified and validated for compatibility with industry-leading Kubernetes solutions, GPU Operator enables organizations to focus on building applications, rather than managing Kubernetes infrastructure.</p>



<p>NVIDIA Cloud Native Stack VMI is available on AWS, Azure, and GCP.</p>



<h2>Now Available: Enterprise support by NVIDIA</h2>



<p>For enterprise support for NVIDIA Cloud Native Stack VMI and GPU Operator, &nbsp;purchase NVIDIA AI Enterprise through an <a href="https://www.nvidia.com/en-us/about-nvidia/partners/partner-locator/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA partner</a>.</p>



<p>Developing AI solutions from concept to deployment is not easy. Keep your AI projects on track with NVIDIA AI Enterprise Support Services. Included with the purchase of the NVIDIA AI Enterprise software suite, this comprehensive offering gives you direct access to NVIDIA AI experts, defined service-level agreements, and control of your upgrade and maintenance schedules with long-term support options. Additional services, including training and AI workload onboarding, are available.<strong></strong></p>



<h2>Run:ai is now certified on NVIDIA AI Enterprise</h2>



<p>Run:ai, an industry leader in compute orchestration for AI workloads, has certified NVIDIA AI Enterprise, an end-to-end, secure, cloud-native suite of AI software, on their Atlas platform.&nbsp;This additional certification enables enterprises to accelerate the data science pipeline. They can focus on streamlining the development and deployment of predictive AI models to automate essential processes and gain rapid insights from data.</p>



<p>Run:ai provides an AI Computing platform that simplifies the access, management, and utilization of GPUs in cloud and on-premises clusters. Smart scheduling and advanced fractional GPU capabilities ensure that you get the right amount of compute for the job.</p>



<p>Run:ai Atlas includes GPU Orchestration capabilities to help researchers consume GPUs more efficiently. They do this by automating the orchestration of AI workloads and the management and virtualization of hardware resources across teams and clusters.</p>



<p>Run:ai can be installed on any Kubernetes cluster, to provide efficient scheduling and monitoring capabilities to your AI infrastructure. With the NVIDIA Cloud Native Stack VMI, you can add cloud instances to a Kubernetes cluster so that they become GPU-powered worker nodes of the cluster.</p>



<p>Here’s testimony from one of our team members: “As an engineer, without the NVIDIA Cloud Native Stack VMI, there is a lot of manual work involved. With the Cloud Native Stack VMI, it was two clicks and took care of provisioning Kubernetes and Docker and the GPU Operator. It was easier and faster to get started on my work.”</p>



<h2>Set up a Cloud Native Stack VMI on AWS</h2>



<p>In the AWS marketplace, <a href="https://aws.amazon.com/marketplace/pp/prodview-cucpqpmvqkajy?sr=0-2&amp;ref_=beagle&amp;applicationId=AWSMPContessa" data-wpel-link="external" target="_blank" rel="follow external noopener">launch an NVIDIA Cloud Native Stack VMI</a> using the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/launch-marketplace-console.html" data-wpel-link="external" target="_blank" rel="follow external noopener">Launch an AWS Marketplace instance</a> instructions.</p>



<p>Ensure that the <a href="https://docs.run.ai/admin/runai-setup/cluster-setup/cluster-prerequisites/" data-wpel-link="external" target="_blank" rel="follow external noopener">necessary prerequisites</a> have been met and install Run:ai using the <a href="https://docs.run.ai/admin/runai-setup/cluster-setup/cluster-install/" data-wpel-link="external" target="_blank" rel="follow external noopener">Cluster Install</a> instructions. After the installation, on the <strong>Overview</strong> dashboard, you should see that the metrics begin to populate. On the <strong>Clusters</strong> tab, you should also see the cluster as connected.</p>



<p>Next, add a few command components to the kube-apiserver.yaml file to enable user authentication on the Run:ai platform. For more information, see <a href="https://docs.run.ai/admin/runai-setup/authentication/researcher-authentication/?h=researcher+api#administration-user-interface-setup" data-wpel-link="external" target="_blank" rel="follow external noopener">Administration User Interface Setup</a>.</p>



<p>By default, you can find the kube-apiserver.yaml file in the following directory:</p>


<div><pre title="">/etc/kubernetes/manifests/kube-apiserver.yaml
</pre></div>


<p>You can <a href="https://docs.run.ai/admin/runai-setup/authentication/researcher-authentication/?h=oidc#mandatory-kubernetes-configuration" data-wpel-link="external" target="_blank" rel="follow external noopener">validate that the oidc commands were successfully applied</a> by the kube-apiserver. Look for the <code>oidc</code> commands in the output.</p>



<pre><code>spec:
&nbsp; containers:
&nbsp; - command:
&nbsp; &nbsp; - kube-apiserver
&nbsp; &nbsp; - --oidc-client-id=runai
&nbsp; &nbsp; - --oidc-issuer-url=https://app.run.ai/auth/realms/nvaie
&nbsp; &nbsp; - --oidc-username-prefix=-</code></pre>



<p>Set up the <a href="https://docs.run.ai/admin/admin-ui-setup/overview/#setup" data-wpel-link="external" target="_blank" rel="follow external noopener">Unified UI</a> and <a href="https://docs.run.ai/admin/admin-ui-setup/project-setup/?h=projects" data-wpel-link="external" target="_blank" rel="follow external noopener">create a new project</a>. Projects help to dictate GPU quota guarantees for data scientists and researchers who are using the Run:ai platform.</p>



<p>Name the new project and give the project at least one assigned GPU. For this post, I created one project with a two-GPU quota and another project with no GPU quota, labeled <code>nvaie-high-priority</code> and <code>nvaie-low-priority</code>, respectively After the project is created, you can <a href="https://docs.run.ai/admin/researcher-setup/cli-install/?h=researcher+command+line+interface#install-runai-cli" data-wpel-link="external" target="_blank" rel="follow external noopener">install the Run:ai CLI tool</a>, which enables you to submit workloads to the cluster.</p>



<p>The following commands use the runai CLI to submit a job (job1 or job2) leveraging a Docker image called quickstart. Quickstart contains TensorFlow, CUDA, a model, and data that feeds in and trains the model. It leverages one GPU for training (-g 1) and is submitted on behalf of the low-priority or high-priority project denoted by the <code>-p</code> parameter. </p>



<p>Deploy a few test jobs to show some of Run:ai’s orchestration functionality by running:</p>


<div><pre title="">runai submit job1 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-high-priority 
runai submit job2 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-low-priority
</pre></div>


<p>You can check the status of the jobs by running:</p>


<div><pre title="">runai describe job job1 -p nvaie-high-priority
runai describe job job2 -p nvaie-low-priority
</pre></div>


<p>Both workloads are now training on the GPUs, as you can see on the <strong>Overview</strong> dashboard.</p>



<p>You can submit an additional workload to highlight your job preemption capabilities. Currently, the <code>nvaie-high-priority</code> project is guaranteed access to both GPUs since their Assigned GPU quota is set to 2. You can submit an additional workload for the <code>nvaie-high-priority</code> project and observe that you are preempting the <code>nvaie-low-priority</code> job.</p>



<p>The job preemption enables you to look at the <a href="https://docs.run.ai/Researcher/best-practices/convert-to-unattended/?h=checkpoint#checkpoints" data-wpel-link="external" target="_blank" rel="follow external noopener">checkpointing process</a> for the training workloads, save the current progress at the checkpoint, and then preempt the workload to remove it from the GPU. Save the training progress and free up the GPU for a higher-priority workload to run.</p>


<div><pre title="">runai submit job3 -i gcr.io/run-ai-demo/quickstart -g 1 -p nvaie-high-priority
</pre></div>


<p>You can check the status of the job by running:</p>


<div><pre title="">runai describe job job3 -p nvaie-high-priority
</pre></div>


<p>If you go back to the overview dashboard, you’ll see the two jobs running for the <code>nvaie-high-priority</code> project and the workload from <code>nvaie-low-priority</code> preempted and placed back into the pending queue. The workload in the pending queue is automatically rescheduled when a GPU becomes available.</p>



<p>To clean up your jobs, run the following commands:</p>


<div><pre title="">runai delete job job1 -p nvaie-low-priority 
runai delete job job2 job3 -p nvaie-high-priority 
</pre></div>


<h2>Summary</h2>



<p>NVIDIA offers a consistent, full stack to develop on a GPU-powered on-premises or on-cloud instance. Developers and MLOps can then deploy that AI application on any GPU-powered platform without code change. </p>



<p>Run:ai, an industry leader in compute orchestration for AI workloads, has certified NVIDIA AI Enterprise, an end-to-end, secure, cloud-native suite of AI software, on its Atlas platform. You can purchase NVIDIA AI Enterprise through an <a href="https://www.nvidia.com/en-us/about-nvidia/partners/partner-locator/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA Partner</a> to obtain enterprise support for NVIDIA VMI and GPU Operator. Included with the purchase of the NVIDIA AI Enterprise software suite, this comprehensive offering gives you direct access to NVIDIA AI experts, defined service-level agreements, and control of your upgrade and maintenance schedules with long-term support options.</p>



<p>For more information, see the following resources:</p>



<ul>
<li><a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA AI Enterprise</a></li>



<li><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/collections/nvidia_vmi" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA VMI</a></li>



<li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html" data-wpel-link="internal" target="_self" rel="noopener noreferrer">NVIDIA GPU Operator</a> </li>



<li><a href="https://www.run.ai/" data-wpel-link="external" target="_blank" rel="follow external noopener">Run:ai solutions</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When did Alan Partridge first appear on television? (210 pts)]]></title>
            <link>https://www.dirtyfeed.org/2023/07/better-on-television/</link>
            <guid>36641782</guid>
            <pubDate>Sat, 08 Jul 2023 06:07:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dirtyfeed.org/2023/07/better-on-television/">https://www.dirtyfeed.org/2023/07/better-on-television/</a>, See on <a href="https://news.ycombinator.com/item?id=36641782">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary">

		
<article id="post-21721">
	<!-- .entry-header -->

	
	<div>
		<p>Here’s a question for you. When did Alan Partridge <em>first</em> appear on television? </p>
<p>Caveats: a) I specifically mean television. Radio is brilliant, and also outside the scope of this article. b) For now, ignore any unbroadcast pilots. I’m talking about actual, broadcast telly. c) I do mean material <em>exclusive</em> to television, not just part of a radio programme aired on TV.</p>
<p>If you immediately went for the first episode of <cite>The Day Today</cite>, on the <strong>19th January 1994</strong>, then join the club. That’s exactly where my mind went at first. So that would be this trademark awkward exchange between Chris and Alan:</p>

<p>But wait! The day before each episode of <cite>The Day Today</cite> aired, BBC2 broadcast <cite>The Day Today MiniNews</cite>, three minutes of extra material which served essentially as an extended trail for the next day’s episode. Or in other words: the closest you’d get to deleted scenes this side of a LaserDisc, at least in the first half of the 90s.</p>
<p>Partridge makes an appearance in the first one, which was broadcast on the <strong>18th January 1994</strong>:</p>
<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/nniEc-k3At0?start=40" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p>
<p>So is that the answer? Not quite. Because, of course, there were trails<sup id="rf1-21721"><a href="#fn1-21721" title="The BBC term is trails, not trailers, despite someone trying to correct me on this earlier in the year." rel="footnote">1</a></sup> running for the series the week before air. Here’s one from the <strong>14th January 1994</strong>, which features a brief bit of Partridge:</p>
<p>
<iframe loading="lazy" width="560" height="315" src="https://www.youtube-nocookie.com/embed/VtkYbUZ7-pc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p>
<p>Incidentally, isn’t that a great trail? For all that Chris Morris has the reputation for scowling at publicity, you couldn’t ask for a better introduction to the show.</p>
<p>The above would usually cause me to make a variety of shrill and unpleasant noises, as I vainly tried to find the first transmission of a trail for the series. Luckily, we can sidestep that problem entirely. Because Partridge had an even earlier appearance on TV.</p>
<p><span id="more-21721"></span>On the <strong>18th December 1993</strong>, BBC Two ran one of it’s regular theme nights. You know, a <em>proper</em> theme night, when the Beeb actually had the money to make new programmes, and craft beautiful links between them. This one was under the <cite>Arena</cite> banner, and was called <cite><a href="https://genome.ch.bbc.co.uk/3a839bae20c64e0f8217f2bcb77da017">Radio Night</a></cite>.</p>
<p>I would describe it as a simulcast between BBC2 and Radio 4, except that isn’t strictly true; that would imply that both services played exactly the same thing. Part of the joy of the evening was that both services were playing slightly different soundtracks, so your radio and television could <em>talk</em> to each other. This video of the opening programme, which combines the two soundtracks, gives you a rough idea of what it was like:</p>
<p>
<iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/n_j7dOejmZI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p>
<p>Of course, these days, this kind of experimentation would be not only virtually impossible, but <em>actually</em> impossible. The varying digital delays between the different versions of each service would kill the idea stone dead. That’s progress for you.</p>
<p>But we could talk about <cite>Radio Night</cite> all, erm, night. Let’s get to the point. One of the segments that evening was called “TV Theft, Radio Rip-Off”, all about radio shows which transferred to television. And who do you think showed up?</p>
<p>
<iframe loading="lazy" width="560" height="315" src="https://www.youtube-nocookie.com/embed/7xfQue7daD8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p>
<p>OK, OK, so it’s just a voiceover with a picture, sure. But I think it counts. The very first televised appearance of Alan Partridge. Almost exactly a month before the first transmission of <cite>The Day Today</cite>.</p>
<p>*&nbsp;&nbsp;&nbsp;*&nbsp;&nbsp;&nbsp;*</p>
<p>The following really is for hardcore production date nerds only. You have been warned. But something about that <cite>Radio Night</cite> segment gives me pause. </p>
<p>Here’s the relevant bit from the Partridge voiceover:</p>
<blockquote><p>“So, given that screen image is all-important, I need more time to contemplate the optical implications of Alan Partridge. In the meantime, I’m happy to enclose a photograph which is strictly to be considered a work in progress.”</p></blockquote>
<p>In other words: TV Partridge isn’t quite cooked yet. Fair enough. At this point, <cite>The Day Today</cite> hadn’t even aired.</p>
<p>Mind you, it would be instructive to find out exactly <em>when</em> the photograph was taken, so we could know exactly when this “work in progress” snapshot was. Alas, this has proved difficult; unless you’re lucky, and the photo you want to investigate shows up in a stock library, you’re often out of luck.<sup id="rf2-21721"><a href="#fn2-21721" title="The paperwork for <cite>Radio Night</cite> is no help; it simply gives Steve Coogan’s agent as the source." rel="footnote">2</a></sup> Suffice to say that photos from this session have shown up in many places, including the cover to commercial releases of the radio <cite>Knowing Me, Knowing You</cite> itself.</p>
<p>Luckily, we don’t actually need to know the date of the photo; there’s another way to figure out when this version of Partridge hails from. Let’s take a look at another of the earliest “optical implications” of him – the unbroadcast pilot for <cite>The Day Today</cite>, on the 2004 DVD release:</p>

<p>You will note that pilot Partridge looks virtually identical to the photo used in <cite>Radio Night</cite>, with even the top being the same:</p>
<div>
<p><img decoding="async" src="https://www.dirtyfeed.org/wordpress/wp-content/uploads/2023/07/earlypartridge1.jpeg" alt="Steve Coogan as Partridge in Radio Night">
</p>
<p><img decoding="async" src="https://www.dirtyfeed.org/wordpress/wp-content/uploads/2023/07/earlypartridge2.jpeg" alt="Steve Coogan as Partridge in The Day Today pilot">
</p>
</div>
<p>In his book <cite>Disgusting Bliss: The Brass Eye of Chris Morris</cite>, Lucian Randall says that the pilot of <cite>The Day Today</cite> was “completed in January 1993”. I can independently verify this; the VT clock for that pilot is dated <strong>29th January 1993</strong>. So the pilot for <cite>The Day Today</cite> was shot in <strong>either December 1992 or January 1993</strong>. Which gives us a rough date for this particular limbo incarnation of Partridge.</p>
<p>Next, let’s talk about the actual series of <cite>The Day Today</cite>. The location shoot, as per the production paperwork, was <strong>“June/July 1993”</strong>; around six months after the pilot. The studio sessions were <strong>6th-8th October 1993</strong>, and the <strong>13th-15th October 1993</strong>; around ten months after the pilot.<sup id="rf3-21721"><a href="#fn3-21721" title="I have to say, despite the huge amounts of location material, I’m still vaguely surprised that <cite>The Day Today</cite> only had six days in the studio, considering the complexity of the show." rel="footnote">3</a></sup></p>
<div>
<div><p><img decoding="async" src="https://www.dirtyfeed.org/wordpress/wp-content/uploads/2023/07/tdt-location.jpeg" alt="Alan Partridge, on location"></p>
<p><strong>Location (June/July 1993)</strong></p>
</div>
<div><p><img decoding="async" src="https://www.dirtyfeed.org/wordpress/wp-content/uploads/2023/07/tdt-studio.jpeg" alt="Alan Partridge, in the studio"></p>
<p><strong>Studio (October 1993)</strong></p>
</div>
</div>
<p>Meanwhile, the Partridge section of Arena’s <cite>Radio Night</cite>? The paperwork confirms that it was written by Steve Coogan and Patrick Marber; but the production dates for the programme as a whole are what we’re interested in here:</p>
<blockquote><p><strong>13-16; 22, 23 September/<br>
14, 28 &amp; 29 October 1993</strong></p></blockquote>
<p>Which really does leave us in an interesting quandry. The location footage for <cite>The Day Today</cite> was shot before the <cite>Radio Night</cite> piece, that we can be fairly sure of. But the studio material is rather more debatable. If the <cite>Radio Night</cite> skit was made in September, it was indeed before the studio material in <cite>The Day Today</cite>; once you creep into October, it starts getting more questionable.</p>
<p>In other words: it is very possible that the “optical implications” of <cite>Partridge</cite> were still in flux at the time of the making of the <cite>Arena</cite> documentary. But they were a little less in flux than the team strictly let on to the viewer, seeing as all the location material for <cite>The Day Today</cite> had <em>already been recorded</em>. And by the time <cite>Radio Night</cite> was actually broadcast, Partridge was fully out of limbo anyway, as <cite>The Day Today</cite> had finished production.</p>
<p>Comedy history is always messier than anyone wants it to be, isn’t it?</p>
<p><em>With thanks to <a href="https://twitter.com/whenisbirths">Justin Lewis</a>, Darrell Maclaine and <a href="https://twitter.com/IrkthePurists">Mike Scott</a>.</em></p>
<hr><ol><li id="fn1-21721"><p>The BBC term is trails, not trailers, despite someone trying to correct me on this earlier in the year.&nbsp;<a href="#rf1-21721" title="Return to footnote 1.">↩</a></p></li><li id="fn2-21721"><p>The paperwork for <cite>Radio Night</cite> is no help; it simply gives Steve Coogan’s agent as the source.&nbsp;<a href="#rf2-21721" title="Return to footnote 2.">↩</a></p></li><li id="fn3-21721"><p>I have to say, despite the huge amounts of location material, I’m still vaguely surprised that <cite>The Day Today</cite> only had six days in the studio, considering the complexity of the show.&nbsp;<a href="#rf3-21721" title="Return to footnote 3.">↩</a></p></li></ol>		
		
				<p>Read more about...</p>
		
		<p><a href="https://www.dirtyfeed.org/tag/alan-partridge/" rel="tag">alan partridge</a></p>	</div><!-- .entry-content -->
</article>
<!-- #comments -->

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google to explore alternatives to robots.txt (106 pts)]]></title>
            <link>https://blog.google/technology/ai/ai-web-publisher-controls-sign-up/</link>
            <guid>36641607</guid>
            <pubDate>Sat, 08 Jul 2023 05:30:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/ai-web-publisher-controls-sign-up/">https://blog.google/technology/ai/ai-web-publisher-controls-sign-up/</a>, See on <a href="https://news.ycombinator.com/item?id=36641607">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      



    

    
      

    

    
    <div data-component="uni-drop-cap|uni-tombstone" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;A principled approach to evolving choice and control for web content&quot;
         }" data-reading-time="true"><p data-block-key="5e535">At Google I/O, <a href="https://blog.google/technology/developers/io-2023/">we announced</a> new AI-driven products and experiments that build on our years of research in the field. We also spoke about Google’s commitment to developing AI responsibly in ways that maximize the positive benefits to society while addressing the challenges, guided by our <a href="https://ai.google/principles/">AI Principles</a> and in line with our <a href="https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-unveils-ai-and-ml-privacy-commitment">customer privacy commitment</a>.</p><p data-block-key="a8e2c">We believe everyone benefits from a vibrant content ecosystem. Key to that is web publishers having choice and control over their content, and opportunities to derive value from participating in the web ecosystem. However, we recognize that existing web publisher controls were developed before new AI and research use cases.</p><p data-block-key="3r4ei">As new technologies emerge, they present opportunities for the web community to evolve standards and protocols that support the web’s future development. One such community-developed web standard, <a href="https://www.rfc-editor.org/rfc/rfc9309.html">robots.txt</a>, was created nearly 30 years ago and has proven to be a simple and transparent way for web publishers to control how search engines crawl their content. We believe it’s time for the web and AI communities to explore additional machine-readable means for web publisher choice and control for emerging AI and research use cases.</p><p data-block-key="d5t7i">Today, we’re kicking off a public discussion, inviting members of the web and AI communities to weigh in on approaches to complementary protocols. We’d like a broad range of voices from across web publishers, civil society, academia and more fields from around the world to join the discussion, and we will be convening those interested in participating over the coming months.</p><p data-block-key="6mj1l">You can join the web and AI communities’ discussion by signing up <a href="https://services.google.com/fb/forms/ai-web-publisher-controls-external/">on our website</a> and we'll share more information about this process soon.</p></div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Khoj: An AI Personal Assistant for Your Digital Brain (147 pts)]]></title>
            <link>https://khoj.dev/</link>
            <guid>36641542</guid>
            <pubDate>Sat, 08 Jul 2023 05:15:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://khoj.dev/">https://khoj.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=36641542">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[LlamaIndex: Unleash the power of LLMs over your data (189 pts)]]></title>
            <link>https://www.llamaindex.ai/</link>
            <guid>36641393</guid>
            <pubDate>Sat, 08 Jul 2023 04:39:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.llamaindex.ai/">https://www.llamaindex.ai/</a>, See on <a href="https://news.ycombinator.com/item?id=36641393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img src="https://uploads-ssl.webflow.com/6459a0e3f348e9e898b7df80/645a932624f9bd2311371e94_Connectors%20Icon.png" loading="lazy" width="150" alt=""></p><div><h3>Data Ingestion</h3><p>Connect your existing data sources and data formats (API's, PDF's, documents, SQL, etc.) to use with a large language model application.<br></p></div></div><div><p><img src="https://uploads-ssl.webflow.com/6459a0e3f348e9e898b7df80/645a9125fd7645a0751da170_Data%20Indices%20White.png" loading="lazy" alt=""></p><div><h3>Data Indexing</h3><p>Store and index your data for different use cases. Integrate with downstream vector store and database providers.</p></div></div><div><p><img src="https://uploads-ssl.webflow.com/6459a0e3f348e9e898b7df80/645a93398de9934ba27a4e23_Query%20Icon.png" loading="lazy" alt=""></p><div><h3>Query Interface</h3><p>LlamaIndex provides a query interface that accepts any input prompt over your data and returns a knowledge-augmented response.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Horror Game of the Year Is a Doom II Mod (246 pts)]]></title>
            <link>https://kotaku.com/doom-2-free-mods-myhouse-download-gzdoom-goty-1850616515</link>
            <guid>36640769</guid>
            <pubDate>Sat, 08 Jul 2023 02:23:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kotaku.com/doom-2-free-mods-myhouse-download-gzdoom-goty-1850616515">https://kotaku.com/doom-2-free-mods-myhouse-download-gzdoom-goty-1850616515</a>, See on <a href="https://news.ycombinator.com/item?id=36640769">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>To some of MyHouse.wad’s biggest fans, the free mod for 1994’s <em>Doom II</em> might even be the best horror game released this year. There are two reasons for this: the technology and the people.<br></p><p>It took some trial and error, but players <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://drive.google.com/drive/folders/1IdH20R6bPg_Sp2Htu0Z-u_wP2AbKoaBz&quot;,{&quot;metric25&quot;:1}]]" href="https://drive.google.com/drive/folders/1IdH20R6bPg_Sp2Htu0Z-u_wP2AbKoaBz" target="_blank" rel="noopener noreferrer">who downloaded MyHouse.wad</a></span> from its Google Drive eventually realized that the map included things that were <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.youtube.com/watch?v=Iq1-TZXz9xo&quot;,{&quot;metric25&quot;:1}]]" href="https://www.youtube.com/watch?v=Iq1-TZXz9xo" target="_blank" rel="noopener noreferrer"><em>not possible</em> in <em>Doom</em></a></span>, like mutated, two-story buildings. They methodically began searching for more of its secrets on a still-active, 58-page-long <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.doomworld.com/forum/topic/134292&quot;,{&quot;metric25&quot;:1}]]" href="https://www.doomworld.com/forum/topic/134292" target="_blank" rel="noopener noreferrer">Doomworld discussion thread</a></span>. Then, there’s the faceless person (or, some say, ghost) who started it.<br></p><p><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.doomworld.com/forum/topic/134292-myhousewad/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.doomworld.com/forum/topic/134292-myhousewad/" target="_blank" rel="noopener noreferrer">Doomworld user Veddge</a></span> had been planting strange seeds for a year, telling strangers that <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.doomworld.com/forum/topic/132016-lets-talk-about-your-week/?page=2&amp;tab=comments#comment-2566129&quot;,{&quot;metric25&quot;:1}]]" href="https://www.doomworld.com/forum/topic/132016-lets-talk-about-your-week/?page=2&amp;tab=comments#comment-2566129" target="_blank" rel="noopener noreferrer">he hasn’t been sleeping recently</a></span>, and wondering if other modders also felt like their “map had <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.doomworld.com/forum/topic/133252-sensory-deprivation-chamber-20-5-maps-udmf/?tab=comments#comment-2611178&quot;,{&quot;metric25&quot;:1}]]" href="https://www.doomworld.com/forum/topic/133252-sensory-deprivation-chamber-20-5-maps-udmf/?tab=comments#comment-2611178" target="_blank" rel="noopener noreferrer">a mind of its own</a></span>.” When he ultimately posted MyHouse.wad—a “pretty adorable” map his deceased friend Thomas modeled after his own house in the 2000s, he said, that he completed after recently discovering it on a floppy disc—on March 2 and then disappeared, users wanted to scavenge his secrets, too. They dissected his game and <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://drive.google.com/drive/folders/18Nx7kUQwmxUGoXqL6FiUwFY--up64fgo&quot;,{&quot;metric25&quot;:1}]]" href="https://drive.google.com/drive/folders/18Nx7kUQwmxUGoXqL6FiUwFY--up64fgo" target="_blank" rel="noopener noreferrer">Google Drive folders</a></span> with Reddit threads and hours of YouTube documentaries, but found nothing satisfying other than the mutual understanding that…this shared restlessness? This throbbing stomach ache for truth? It’s the mark of a perfect horror game.<br></p><h2 id="h142215"><a id=""></a><strong>MyHouse.wad is the horror GOTY</strong></h2><p>“I don’t care if it doesn’t count, this is going in my [Game of the Year] 2023 lineup,” says a popular comment in <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.reddit.com/r/Games/comments/13oxrt7/myhousewad_inside_dooms_most_terrifying_mod/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.reddit.com/r/Games/comments/13oxrt7/myhousewad_inside_dooms_most_terrifying_mod/" target="_blank" rel="noopener noreferrer">a Reddit discussion</a></span> on MyHouse. “It’s crazy good. [...] It pulls off so much shit I didn’t know DOOM was capable of, even with [<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://zdoom.org/wiki/GZDoom&quot;,{&quot;metric25&quot;:1}]]" href="https://zdoom.org/wiki/GZDoom" target="_blank" rel="noopener noreferrer">source port</a></span>] GZDoom.”<br></p><p>id Software co-founder and <em>Doom</em> designer John Romero also called it “great” after <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.youtube.com/watch?v=gIl_TqFJNO8&quot;,{&quot;metric25&quot;:1}]]" href="https://www.youtube.com/watch?v=gIl_TqFJNO8" target="_blank" rel="noopener noreferrer">playing it in June</a></span>, and Mark Danielewski, who wrote psychological horror novel <em>House of Leaves</em>, shared a video <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://twitter.com/markdanielewski/status/1659710281734574080&quot;,{&quot;metric25&quot;:1}]]" href="https://twitter.com/markdanielewski/status/1659710281734574080" target="_blank" rel="noopener noreferrer">on Twitter</a></span> explaining the connection between <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://twitter.com/likaluca/status/1659650073846751259&quot;,{&quot;metric25&quot;:1}]]" href="https://twitter.com/likaluca/status/1659650073846751259" target="_blank" rel="noopener noreferrer">his book</a></span> and MyHouse’s story and hallucinatory level design.</p><p>But the way that Veddge tells it in <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://docs.google.com/document/d/1YZN1Gxa-moVq-7N_ckJsauUWHulJ4_Yvw-Ot5hKmppc/edit&quot;,{&quot;metric25&quot;:1}]]" href="https://docs.google.com/document/d/1YZN1Gxa-moVq-7N_ckJsauUWHulJ4_Yvw-Ot5hKmppc/edit" target="_blank" rel="noopener noreferrer">a journal entry</a></span>, the house and its flustering idiosyncrasies—the rooms that light on fire when you’re not looking, like <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://kotaku.com/layers-of-fear-review-2023-release-date-ps5-1850541040&quot;,{&quot;metric25&quot;:1}]]" href="https://kotaku.com/layers-of-fear-review-2023-release-date-ps5-1850541040">in <em>Layers of Fear</em></a></span>, the filled bathtubs that are portals, the hallways that feel infinite—were not on purpose. They’re evidence that the “map [was] using [him],” shoving him toward bad dreams of storm clouds and dead babies, trapping him in a void without his friend, without anything.<br></p><p>“I tried to delete this map but it continues to change and evolve without any input from me,” says <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://drive.google.com/drive/folders/1IdH20R6bPg_Sp2Htu0Z-u_wP2AbKoaBz&quot;,{&quot;metric25&quot;:1}]]" href="https://drive.google.com/drive/folders/1IdH20R6bPg_Sp2Htu0Z-u_wP2AbKoaBz" target="_blank" rel="noopener noreferrer">a txt file</a></span> Veddge put in the mod’s Google Drive. “What began as a tribute to a lost friend has consumed my entire life.”<br></p><p>Ignoring Veddge’s urges not to and playing MyHouse anyway indicates as much. MyHouse starts as expected, in a Middle America clapboard house with healthy shrubs outside and <em>Doom </em>demons inside. But once all the doors disappear and you find out you can phase through mirrors into another unnatural world, you accept that the house isn’t a happy memory. You’re the food it’s playing with.<br></p><p>“[The mod] builds you up as the demon-slaying Doomguy with a simple looking map, before robbing you of your power fantasy with an enemy[—the house—]you can’t understand, let alone defeat, even though it’s all around you,” Jack Nicholls, the YouTuber behind the video Danielewski shared (which now has nearly seven million views) tells me over email. Each of MyHouse’s three possible endings also remind you there’s no outrunning the inevitable; “In dark, uncertain awe it waits / The common doom, to die,” <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.poetryfoundation.org/poems/51734/time-to-come&quot;,{&quot;metric25&quot;:1}]]" href="https://www.poetryfoundation.org/poems/51734/time-to-come" target="_blank" rel="noopener noreferrer">says Walt Whitman</a></span>.<br></p><figure data-id="90516365b2ac8f1018c733bba1c35cf7" data-recommend-id="image://90516365b2ac8f1018c733bba1c35cf7" data-format="jpg" data-width="1280" data-height="720" data-lightbox="true" data-alt="This sketchbook Cerberus lives in Veddge's Google Drive." data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-alt="This sketchbook Cerberus lives in Veddge's Google Drive." data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-hide="false" data-hidecredit="false"><p><span><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_645/90516365b2ac8f1018c733bba1c35cf7.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_490/90516365b2ac8f1018c733bba1c35cf7.jpg"><img alt="This sketchbook Cerberus lives in Veddge's Google Drive." data-chomp-id="90516365b2ac8f1018c733bba1c35cf7" data-format="jpg" data-alt="This sketchbook Cerberus lives in Veddge's Google Drive." data-anim-src="" data-src="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_645/90516365b2ac8f1018c733bba1c35cf7.jpg" src="https://i.kinja-img.com/gawker-media/image/upload/c_fit,f_auto,g_center,q_60,w_645/90516365b2ac8f1018c733bba1c35cf7.jpg"></picture></div></span></p><p><figcaption>This sketchbook Cerberus lives in Veddge’s Google Drive.</figcaption><figcaption>Photo<!-- -->: <!-- -->Veddge</figcaption></p></div><span data-id="90516365b2ac8f1018c733bba1c35cf7" data-recommend-id="image://90516365b2ac8f1018c733bba1c35cf7" data-format="jpg" data-width="1280" data-height="720" data-lightbox="true" data-alt="This sketchbook Cerberus lives in Veddge's Google Drive." data-recommended="false" data-hide="false"></span></figure><p>As the house map shifts and flips around you, it lets the music drop out suddenly sometimes, or repopulates enemies for no clear reason. Its fickleness seems to encourage you to kneel so that fate can run you over. Once you surrender, you’re free from responsibility, and can now keep dreaming until you can’t.<br></p><p>“[While I was playing,] it was like my feet weren’t touching the floor, and I had no comprehension where I was or what constituted ‘<!-- -->where’<!-- --> anymore,” Nicholls says. That’s the only gratifying thing about being trapped—it feels dangerous, but it’s not your fault. “I wouldn’t change a single thing about it,” he continues.<br></p><p>Though, in terms of its reception “I did find it disappointing that some took things too far,” Nicholls says, “trying to find the identities of the author and where the House itself was, leading to the Doomworld thread needing to be locked.”<br></p><h2 id="h142216"><a id=""></a><strong>Into another portal</strong></h2><p>What makes a worthwhile mystery also reddens a deep itch in your brain. Answers might extinguish your wonder, but they at least satisfy your curiosity.<br></p><p>For months, Doomworld users fixated on details, like when Veddge first started posting (2006), where they might have seen that game location before (on a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://kotaku.com/best-creepypasta-what-is-backrooms-tiktok-steam-reddit-1850298058&quot;,{&quot;metric25&quot;:1}]]" href="https://kotaku.com/best-creepypasta-what-is-backrooms-tiktok-steam-reddit-1850298058">4chan copypasta</a></span>), and whether or not it would be a good idea to try to find the house on Google Maps (no).<br></p><p>Veddge, who continues to be anonymous and did not respond to <em>Kotaku</em>’s requests for comment, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.doomworld.com/forum/topic/136107-myhouse-split-debate-about-whats-respectful-in-art-about-death-etc/?tab=comments#comment-2649383&quot;,{&quot;metric25&quot;:1}]]" href="https://www.doomworld.com/forum/topic/136107-myhouse-split-debate-about-whats-respectful-in-art-about-death-etc/?tab=comments#comment-2649383" target="_blank" rel="noopener noreferrer">apparently contacted a forum member</a></span> to tell them he was disappointed to “watch the [public’s] focus be on anything other than a journey of grief” presented in the mod. Another forum member kevansevans, who tells me over private message they assisted Veddge with GZDoom’s “fancy scripting language” ZScript, says that Veddge never expected “the virality.”<br></p><p>“We definitely knew it had a really high chance of becoming popular in the community,﻿” kevansevans says, but “community content for classic <em>Doom</em> these days is a niche corner of the internet. Even the most ambitious maps never leave discussions outside the community.”<br></p><p>“Anything actually leaving the circle [...] is recognized as a big achievement and very unexpected,” they continue.<br></p><p>While anonymous accounts discussed the merits of background checks and other 3 a.m. theories, along with effusive praise—“I’ve come out of [MyHouse] feeling I see things differently,” Nicholls tells me, “My time with it has been unforgettable.”—Veddge’s soon-to-be ex-wife Amy was posting the suburban truth on TikTok.<br></p><h2 id="h142217"><a id=""></a><strong>The mystery of love</strong></h2><p>According to her replies to comments from curious MyHouse fans (Amy did not immediately respond to a request for comment), the <em>Doom</em> mod is a computer adaptation of their impending divorce.<br></p><p>“As our marriage fell apart, so did the house in the game,” she said.<br></p><p>The small details players dissected into rice grains then atoms turned out to be one-to-one <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://doomwiki.org/wiki/Doom_community_jargon#:~:text=DoomCute%20%2D%20An%20adjective%20applied%20to,A%20term%20invented%20by%20kmxexii.&quot;,{&quot;metric25&quot;:1}]]" href="https://doomwiki.org/wiki/Doom_community_jargon#:~:text=DoomCute%20%2D%20An%20adjective%20applied%20to,A%20term%20invented%20by%20kmxexii." target="_blank" rel="noopener noreferrer">DoomCute</a></span> copies of Veddge’s life, including a painting on the living room wall <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.tiktok.com/@bananapantsamy/video/7028620202841787694&quot;,{&quot;metric25&quot;:1}]]" href="https://www.tiktok.com/@bananapantsamy/video/7028620202841787694" target="_blank" rel="noopener noreferrer">of pink lotus flowers</a></span>, or a set of <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.tiktok.com/@bananapantsamy/video/7237150150819433771&quot;,{&quot;metric25&quot;:1}]]" href="https://www.tiktok.com/@bananapantsamy/video/7237150150819433771" target="_blank" rel="noopener noreferrer">black-and-white triptychs</a></span> that, in the mod, catalogue found items.<br></p><p>“He hid so many things <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.tiktok.com/@bananapantsamy/video/7238823975663455531&quot;,{&quot;metric25&quot;:1}]]" href="https://www.tiktok.com/@bananapantsamy/video/7238823975663455531" target="_blank" rel="noopener noreferrer">about our life</a></span> in the game,” she said, “I’m sure even I don’t know all of it.”<br></p><p>I’ve been surprised by how few MyHouse players, despite their dogged search for resolution, have acknowledged Amy’s perspective. It seems possible that once a game—or, more accurately in this case, a tangle of unshakeable fear, a snake around the neck—has inflamed so many imaginations, real life stops feeling real. People want answers, but they don’t want them to be boring.<br></p><p>But “it can’t be helped: boredom is not simple,” French theorist Roland Barthes writes in his 1973 book <em>The Pleasure of the Text</em>. “It is bliss seen from the shores of pleasure.”<br></p><p>As a whole, even with real life attached to it, MyHouse.wad is now an infamous piece of internet horror, though it’s too popular to be tied directly to its creator’s experience<!-- -->. For people that make art, its ability to stand on its own can be terrifying (the map has a mind of its own, after all)—it can feel wrong, but it can also be connective.<br></p><p>Horror has the same effect. Once you experience that feeling with someone, like letting them sip your favorite cherry Coke, you’re bonded. So while MyHouse.wad may have been an unlikely GOTY contender, its artful, personal horror was always going to bring people together. That’s just what happens when you share something from the heart, as dark as it can be. <br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Matrix Multiplication Using Only Addition (213 pts)]]></title>
            <link>https://arxiv.org/abs/2307.01415</link>
            <guid>36640148</guid>
            <pubDate>Sat, 08 Jul 2023 00:44:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2307.01415">https://arxiv.org/abs/2307.01415</a>, See on <a href="https://news.ycombinator.com/item?id=36640148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    
    
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2307.01415">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  Matrix multiplication consumes a large fraction of the time taken in many
machine-learning algorithms. Thus, accelerator chips that perform matrix
multiplication faster than conventional processors or even GPU's are of
increasing interest. In this paper, we demonstrate a method of performing
matrix multiplication without a scalar multiplier circuit. In many cases of
practical interest, only a single addition and a single on-chip copy operation
are needed to replace a multiplication. It thus becomes possible to design a
matrix-multiplier chip that, because it does not need time, space- and
energy-consuming multiplier circuits, can hold many more processors, and thus
provide a net speedup.

    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Jeffrey Ullman [<a href="https://arxiv.org/show-email/867329e7/2307.01415">view email</a>]
      <br>
    <strong>[v1]</strong>
    
        Tue, 4 Jul 2023 00:38:40 UTC (94 KB)<br>
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ProtonMail Rewrites Your Emails (376 pts)]]></title>
            <link>http://jfloren.net/b/2023/7/7/0</link>
            <guid>36639530</guid>
            <pubDate>Fri, 07 Jul 2023 23:18:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://jfloren.net/b/2023/7/7/0">http://jfloren.net/b/2023/7/7/0</a>, See on <a href="https://news.ycombinator.com/item?id=36639530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<p><a href="http://jfloren.net/b/">Back to blog archive</a></p><p><strong>Posted 2023/7/7</strong></p>
<div>


<p>I switched to ProtonMail a few years back after getting fed up with Google. It’s been mostly acceptable, but a few months ago the Android client started mangling emails – if I hit “send” too soon, it would send out only part of what I’d typed; I had to wait 10 seconds or so after I finished typing before hitting send to be sure it sent. I figured I’d switch to some other email client instead, since I wasn’t particularly in love with the Proton app anyway.</p>

<p>My solution was to set up the <a href="https://proton.me/mail/bridge">Proton Bridge</a> in a VM on my NAS, then used rinetd to forward incoming connections on the IMAP and SMTP ports to the bridge (which only listens on 127.0.0.1). I then set up tailscale on that box and on my phone; with that, I could connect any Android email client (I like <a href="https://email.faircode.eu/">FairEmail</a>) to my Proton account. I was also accessing it from Linux using Claws.</p>

<p>Everything was great until I decided the other day that I’d <em>also</em> like to do PGP signing on my outgoing messages. I exported a signing-only subkey to my Android device and configured FairEmail+OpenKeyring to use it, then I also set up Claws on Linux for PGP/MIME.</p>

<p>When I sent a test message to myself, though, Claws and FairEmail didn’t have any clue that it was signed. If I switched to PGP inline, it worked. I sent an email to one of the Claws maintainers, who reported that my MIME structure was all messed up. He sent me a signed message back, and Claws was able to verify the signature just fine.</p>

<p>It turns out that Proton has been breaking outgoing PGP signatures from the beginning: <a href="https://github.com/ProtonMail/proton-bridge/issues/26">https://github.com/ProtonMail/proton-bridge/issues/26</a>, <a href="https://github.com/ProtonMail/proton-bridge/issues/320">https://github.com/ProtonMail/proton-bridge/issues/320</a>. It seems that their argument is this:</p>

<ul>
<li>When you send a regular email via Proton to another Proton client, they automatically PGP sign+encrypt the message. (I think this is great!)</li>
<li>Their automatic signing+encryption cannot coexist with a user-applied signature.</li>
<li>Therefore, all user-applied signatures will be broken. Tough luck, bucko, we’re the SECURE email company, you’ll upload your private key to our servers and you’ll like it!</li>
</ul>

<p>It’s absurd that there’s no way to disable this, no option to tell Proton “if you see a multipart/signed or multipart/encrypted message, just leave it the hell alone.”</p>

<p>I’m looking at other potential email hosts. I know PGP isn’t widely used, but I have a hard time swallowing Proton’s silent mangling of my email, and I especially dislike their smarmy we-know-better attitude when people complain about it.</p>

</div>
</div></div>]]></description>
        </item>
    </channel>
</rss>