<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 12 Mar 2024 18:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Building Meta‚Äôs GenAI Infrastructure (155 pts)]]></title>
            <link>https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/</link>
            <guid>39680997</guid>
            <pubDate>Tue, 12 Mar 2024 15:52:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/</a>, See on <a href="https://news.ycombinator.com/item?id=39680997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>Marking a major investment in Meta‚Äôs AI future, we are announcing two 24k GPU clusters. We are sharing details on the hardware, network, storage, design, performance, and software that help us extract high throughput and reliability for various AI workloads. We use this cluster design for Llama 3 training.</span></li>
<li aria-level="1"><span>We are strongly committed to open compute and open source. We built these clusters on top of </span><a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/" target="_blank" rel="noopener"><span>Grand Teton</span></a><span>, </span><a href="https://www.opencompute.org/wiki/Open_Rack/SpecsAndDesigns" target="_blank" rel="noopener"><span>OpenRack</span></a><span>, and </span><a href="https://pytorch.org/" target="_blank" rel="noopener"><span>PyTorch</span></a><span> and continue to push open innovation across the industry.</span></li>
<li aria-level="1"><span>This announcement is one step in our ambitious infrastructure roadmap. By the end of 2024, we‚Äôre aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100 GPUs as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s.</span></li>
</ul>
<p><span>To lead in developing AI means leading investments in hardware infrastructure. Hardware infrastructure plays an important role in AI‚Äôs future. Today, we‚Äôre sharing details on two versions of our </span><span>24,576-GPU data center scale cluster at Meta. These clusters support our current and next generation AI models, including Llama 3, the successor to</span><a href="https://ai.meta.com/llama/open-innovation-ai-research-community"> <span>Llama 2</span></a><span>, our publicly released LLM, as well as AI research and development across GenAI and other areas .</span></p>
<h2><span>A peek into Meta‚Äôs large-scale AI clusters</span></h2>
<p><span>Meta‚Äôs long-term vision is to build artificial general intelligence (AGI) that is open and built responsibly so that it can be widely available for everyone to benefit from. As we work towards AGI, we have also worked on scaling our clusters to power this ambition. The progress we make towards AGI creates new products,</span><a href="https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/" target="_blank" rel="noopener"> <span>new AI features for our family of apps</span></a><span>, and new AI-centric computing devices.&nbsp;</span></p>
<p><span>While we‚Äôve had a long history of building AI infrastructure, we first shared details on our </span><a href="https://ai.meta.com/blog/ai-rsc/" target="_blank" rel="noopener"><span>AI Research SuperCluster (RSC)</span></a><span>, featuring 16,000 NVIDIA A100 GPUs, in 2022. RSC has accelerated our open and responsible AI research by helping us build our first generation of advanced AI models. It played and continues to play an important role in the development of </span><a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="noopener"><span>Llama</span></a><span> and </span><a href="https://arxiv.org/abs/2307.09288" target="_blank" rel="noopener"><span>Llama 2</span></a><span>, as well as advanced AI models for applications ranging from computer vision, NLP, and speech recognition, to</span><a href="https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/" target="_blank" rel="noopener"> <span>image generation</span></a><span>, and even</span> <a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/" target="_blank" rel="noopener"><span>coding</span></a><span>.</span></p>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-Clusters-hero.png?w=1024" alt="" width="1024" height="734" srcset="https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-Clusters-hero.png 1999w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-Clusters-hero.png?resize=916,656 916w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-Clusters-hero.png?resize=768,550 768w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-Clusters-hero.png?resize=1024,734 1024w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-Clusters-hero.png?resize=1536,1100 1536w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-Clusters-hero.png?resize=96,69 96w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-Clusters-hero.png?resize=192,138 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<h2><span>Under the hood</span></h2>
<p><span>Our newer AI clusters build upon the successes and lessons learned from RSC. We focused on building end-to-end AI systems with a major emphasis on researcher and developer experience and productivity. The efficiency of the high-performance network fabrics within these clusters, some of the key storage decisions, combined with the 24,576 NVIDIA Tensor Core H100 GPUs in each, allow both cluster versions to support models larger and more complex than that could be supported in the RSC and pave the way for advancements in GenAI product development and AI research.</span></p>
<h3><span>Network</span></h3>
<p><span>At Meta, we handle hundreds of trillions of AI model executions per day. Delivering these services at a large scale requires a highly advanced and flexible infrastructure. Custom designing much of our own hardware, software, and network fabrics allows us to optimize the end-to-end experience for our AI researchers while ensuring our data centers operate efficiently.&nbsp;</span></p>
<p><span>With this in mind, we built one cluster with a remote direct memory access (RDMA) over converged Ethernet (RoCE) network fabric solution based on the </span><a href="https://www.arista.com/assets/data/pdf/Datasheets/7800R3-Data-Sheet.pdf" target="_blank" rel="noopener"><span>Arista 7800</span></a><span> with </span><a href="https://engineering.fb.com/2021/11/09/data-center-engineering/ocp-summit-2021/" target="_blank" rel="noopener"><span>Wedge400</span></a><span> and </span><a href="https://engineering.fb.com/2021/11/09/data-center-engineering/ocp-summit-2021/" target="_blank" rel="noopener"><span>Minipack2</span></a><span> OCP rack switches. The other cluster features an </span><a href="https://www.nvidia.com/en-us/networking/quantum2/" target="_blank" rel="noopener"><span>NVIDIA Quantum2 InfiniBand</span></a><span> fabric. Both of these solutions interconnect 400 Gbps endpoints. With these two, we are able to assess the suitability and scalability of these </span><a href="https://engineering.fb.com/2023/11/15/networking-traffic/watch-metas-engineers-on-building-network-infrastructure-for-ai/" target="_blank" rel="noopener"><span>different types of interconnect for large-scale training,</span></a><span> giving us more insights that will help inform how we design and build even larger, scaled-up clusters in the future. Through careful co-design of the network, software, and model architectures, we have successfully used both RoCE and InfiniBand clusters for large, GenAI workloads (including our ongoing training of Llama 3 on our RoCE cluster) without any network bottlenecks.</span></p>
<h3><span>Compute</span></h3>
<p><span>Both clusters are built using</span> <a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/" target="_blank" rel="noopener"><span>Grand Teton</span></a><span>, our in-house-designed, open GPU hardware platform that we‚Äôve contributed to the Open Compute Project (OCP). Grand Teton builds on the many generations of AI systems that integrate power, control, compute, and fabric interfaces into a single chassis for better overall performance, signal integrity, and thermal performance. It provides rapid scalability and flexibility in a simplified design, allowing it to be quickly deployed into data center fleets and easily maintained and scaled. Combined with other in-house innovations like our</span> <a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/" target="_blank" rel="noopener"><span>Open Rack</span></a><span> power and rack architecture, Grand Teton allows us to build new clusters in a way that is purpose-built for current and future applications at Meta.</span></p>
<p><span>We have been openly designing our GPU hardware platforms beginning with our </span><a href="https://engineering.fb.com/2015/12/10/ml-applications/facebook-to-open-source-ai-hardware-design/" target="_blank" rel="noopener"><span>Big Sur platform in 2015</span></a><span>.</span></p>
<h3><span>Storage</span></h3>
<p><span>Storage plays an important role in AI training, and yet is one of the least talked-about aspects. As the GenAI training jobs become more multimodal over time, consuming large amounts of image, video, and text data, the need for data storage grows rapidly. The need to fit all that data storage into a performant, yet power-efficient footprint doesn‚Äôt go away though, which makes the problem more interesting.</span></p>
<p><span>Our storage deployment addresses the data and checkpointing needs of the AI clusters via a home-grown Linux Filesystem in Userspace (FUSE) API backed by a version of Meta‚Äôs </span><a href="https://www.usenix.org/conference/fast21/presentation/pan" target="_blank" rel="noopener"><span>‚ÄòTectonic‚Äô distributed storage solution</span></a><span> optimized for Flash media. This solution enables thousands of GPUs to save and load checkpoints in a synchronized fashion (a </span><a href="https://en.wikipedia.org/wiki/Thundering_herd_problem#:~:text=In%20computer%20science%2C%20the%20thundering,able%20to%20handle%20the%20event."><span>challenge</span></a><span> for any storage solution) while also providing a flexible and high-throughput exabyte scale storage required for data loading.</span></p>
<p><span>We have also partnered with </span><a href="https://hammerspace.com/software/" target="_blank" rel="noopener"><span>Hammerspace</span></a><span> to co-develop and land a parallel network file system (NFS) deployment to meet the developer experience requirements for this AI cluster. Among other benefits, Hammerspace enables engineers to perform interactive debugging for jobs using thousands of GPUs as code changes are immediately accessible to all nodes within the environment. When paired together, the combination of our Tectonic distributed storage solution and Hammerspace enable fast iteration velocity without compromising on scale.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p>
<p><span>The storage deployments in our GenAI clusters, both Tectonic- and Hammerspace-backed, are based on the </span><a href="https://www.opencompute.org/documents/e1s-expansion-2ou-1s-server-design-specification-pdf"><span>YV3 Sierra Point server platform</span></a><span>, upgraded with the latest high capacity E1.S SSD we can procure in the market today. Aside from the higher SSD capacity, the servers per rack was customized to achieve the right balance of throughput capacity per server, rack count reduction, and associated power efficiency. Utilizing the OCP servers as Lego-like building blocks, our storage layer is able to flexibly scale to future requirements in this cluster as well as in future, bigger AI clusters, while being fault-tolerant to day-to-day Infrastructure maintenance operations.</span></p>
<h3><span>Performance</span></h3>
<p><span>One of the principles we have in building our large-scale AI clusters is to maximize performance and ease of use simultaneously without compromising one for the other. This is an important principle in creating the best-in-class AI models.&nbsp;</span></p>
<p><span>As we push the limits of AI systems, the best way we can test our ability to scale-up our designs is to simply build a system, optimize it, and actually test it (while simulators help, they only go so far). In this design journey, we compared the performance seen in our small clusters and with large clusters to see where our bottlenecks are. In the graph below, AllGather collective performance is shown (as normalized bandwidth on a 0-100 scale) when a large number of GPUs are communicating with each other at message sizes where roofline performance is expected.&nbsp;</span></p>
<p><span>Our out-of-box performance for large clusters was initially poor and inconsistent, compared to optimized small cluster performance. To address this we made several changes to how our internal job scheduler schedules jobs with network topology awareness ‚Äì this resulted in latency benefits and minimized the amount of traffic going to upper layers of the network. We also optimized our network routing strategy in combination with NVIDIA Collective Communications Library (NCCL) changes to achieve optimal network utilization. This helped push our large clusters to achieve great and expected performance just as our small clusters.</span></p>
<figure id="attachment_21048" aria-describedby="caption-attachment-21048"><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-clusters-performance.png?w=1024" alt="" width="1024" height="768" srcset="https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-clusters-performance.png 1999w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-clusters-performance.png?resize=916,687 916w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-clusters-performance.png?resize=768,576 768w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-clusters-performance.png?resize=1024,768 1024w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-clusters-performance.png?resize=1536,1152 1536w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-clusters-performance.png?resize=96,72 96w, https://engineering.fb.com/wp-content/uploads/2024/03/Meta-24K-GenAi-clusters-performance.png?resize=192,144 192w" sizes="(max-width: 992px) 100vw, 62vw"><figcaption id="caption-attachment-21048">In the figure we see that small cluster performance (overall communication bandwidth and utilization) reaches 90%+ out of the box, but an unoptimized large cluster performance has very poor utilization, ranging from 10% to 90%. After we optimize the full system (software, network, etc.), we see large cluster performance return to the ideal 90%+ range.</figcaption></figure>
<p><span>In addition to software changes targeting our internal infrastructure, we worked closely with teams authoring training frameworks and models to adapt to our evolving infrastructure. For example, NVIDIA H100 GPUs open the possibility of leveraging new data types such as 8-bit floating point (FP8) for training. Fully utilizing larger clusters required investments in additional parallelization techniques and new storage solutions provided opportunities to highly optimize checkpointing across thousands of ranks to run in hundreds of milliseconds.</span></p>
<p><span>We also recognize debuggability as one of the major challenges in large-scale training. Identifying a problematic GPU that is stalling an entire training job becomes very difficult at a large scale. We‚Äôre building tools such as desync debug, or a distributed collective flight recorder, to expose the details of distributed training, and help identify issues in a much faster and easier way</span></p>
<p><span>Finally, we‚Äôre continuing to evolve PyTorch, the foundational AI framework powering our AI workloads, to make it ready for tens, or even hundreds, of thousands of GPU training. We have identified multiple bottlenecks for process group initialization, and reduced the startup time from sometimes hours down to minutes.&nbsp;</span></p>
<h2><span>Commitment to open AI innovation</span></h2>
<p><span>Meta maintains its commitment to open innovation in AI software and hardware. We believe open-source hardware and software will always be a valuable tool to help the industry solve problems at large scale.</span></p>
<p><span>Today, we continue to support</span><a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/" target="_blank" rel="noopener"> <span>open hardware innovation</span></a><span> as a founding member of OCP, where we make designs like Grand Teton and Open Rack available to the OCP community. We also continue to be the largest and primary contributor to </span><a href="https://pytorch.org/" target="_blank" rel="noopener"><span>PyTorch</span></a><span>, the AI software framework that is powering a large chunk of the industry.</span></p>
<p><span>We also continue to be committed to open innovation in the AI research community. We‚Äôve launched the</span><a href="https://ai.meta.com/llama/open-innovation-ai-research-community" target="_blank" rel="noopener"> <span>Open Innovation AI Research Community</span></a><span>, a partnership program for academic researchers to deepen our understanding of how to responsibly develop and share AI technologies ‚Äì with a particular focus on LLMs.</span></p>
<p><span>An open approach to AI is not new for Meta. We‚Äôve also launched the </span><a href="https://ai.meta.com/blog/ai-alliance/" target="_blank" rel="noopener"><span>AI Alliance</span></a><span>, a group of leading organizations across the AI industry focused on accelerating responsible innovation in AI within an open community. Our AI efforts are built on a philosophy of open science and cross-collaboration. An open ecosystem brings transparency, scrutiny, and trust to AI development and leads to innovations that everyone can benefit from that are built with safety and responsibility top of mind.&nbsp;</span></p>
<h2><span>The future of Meta‚Äôs AI infrastructure</span></h2>
<p><span>These two AI training cluster designs are a part of our larger roadmap for the future of AI. By the end of 2024, we‚Äôre aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100s as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s.</span></p>
<p><span>As we look to the future, we recognize that what worked yesterday or today may not be sufficient for tomorrow‚Äôs needs. That‚Äôs why we are constantly evaluating and improving every aspect of our infrastructure, from the physical and virtual layers to the software layer and beyond. Our goal is to create systems that are flexible and reliable to support the fast-evolving new models and research.&nbsp; </span></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Devin, the First AI Software Engineer (124 pts)]]></title>
            <link>https://www.cognition-labs.com/blog</link>
            <guid>39679787</guid>
            <pubDate>Tue, 12 Mar 2024 14:15:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cognition-labs.com/blog">https://www.cognition-labs.com/blog</a>, See on <a href="https://news.ycombinator.com/item?id=39679787">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="w-node-_537870b2-4447-9e58-9c59-0adde404dc40-15286f21"><p><iframe src="https://www.youtube.com/embed/fjHtjT7GO1c?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="Introducing Devin, the first AI software engineer"></iframe></p><p>March 12th, 2024 |&nbsp;Written by Scott Wu</p><h2>Introducing Devin, the first AI software engineer</h2><p><em>And setting a new state of the art on the SWE-bench coding benchmark</em></p><div><p>Meet Devin, the world‚Äôs first fully autonomous AI software engineer.<br>‚Äç<br>Devin is a tireless, skilled teammate, equally ready to build alongside you or independently complete tasks for you to review. </p><p>With Devin, engineers can focus on more interesting problems and engineering teams can strive for more ambitious goals.</p></div><p>Devin's Capabilities</p><div><p>With our advances in long-term reasoning and planning, Devin can plan and execute complex engineering tasks requiring thousands of decisions. Devin can recall relevant context at every step, learn over time, and fix mistakes.</p><p>We've also equipped Devin with common developer tools including the shell, code editor, and browser within a sandboxed compute environment‚Äîeverything a human would need to do their work.</p><p>Finally, we've given Devin the ability to actively collaborate with the user. Devin reports on its progress in real time, accepts feedback, and works together with you through design choices as needed.<br>‚Äç<br>Here's a sample of what Devin can do:</p></div><div id="w-node-_0a565f14-693a-64cf-6e8f-ef1c42c0885f-15286f21"><p><strong>Devin can learn how to use unfamiliar technologies.<br></strong>After reading a blog post, Devin runs ControlNet on Modal to produce images with concealed messages for Sara.<br></p><div id="w-node-_0a565f14-693a-64cf-6e8f-ef1c42c08864-15286f21"><p><iframe src="https://www.youtube.com/embed/lwnkdngr7fU?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="AI Software Engineer Plants Secret Messages in Images"></iframe></p></div></div><div id="w-node-_0b146222-a732-94c8-6af3-800a1994dc81-15286f21"><p><strong>Devin can build and deploy apps end to end. <br></strong>Devin makes an interactive website which simulates the Game of Life! It incrementally adds features requested by the user and then deploys the app to Netlify.<br></p><div id="w-node-_0b146222-a732-94c8-6af3-800a1994dc89-15286f21"><p><iframe src="https://www.youtube.com/embed/G45NKnAWuXc?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="Devin making Game of Life!"></iframe></p></div></div><div id="w-node-_47a4f6da-a718-fc7c-42d6-c4a3c4adc6be-15286f21"><p><strong>Devin can autonomously find and fix bugs in codebases.<br></strong>Devin helps Andrew maintain and debug his open source competitive programming book. <br></p><div id="w-node-_47a4f6da-a718-fc7c-42d6-c4a3c4adc6c0-15286f21"><p><iframe src="https://www.youtube.com/embed/TiXAzn2_Xck?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="AI finds and fixes a bug that I didn't catch!"></iframe></p></div></div><div id="w-node-b0a83d60-326c-bea3-78ef-b35730de8f13-15286f21"><p><strong>Devin can train and fine tune its own AI models. <br>‚Äç</strong>Devin sets up fine tuning for a large language model given only a link to a research repository on GitHub.<br></p><div id="w-node-b0a83d60-326c-bea3-78ef-b35730de8f18-15286f21"><p><iframe src="https://www.youtube.com/embed/V_J-xOeCklQ?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="AI trains an AI!"></iframe></p></div></div><div id="w-node-_4de27410-4316-ee85-9365-dd1333ffbf1f-15286f21"><p><strong>Devin can address bugs and feature requests in open source repositories. </strong>Given just a link to a GitHub issue, Devin does all the setup and context gathering that is needed.<br></p><div id="w-node-_4de27410-4316-ee85-9365-dd1333ffbf24-15286f21"><p><iframe src="https://www.youtube.com/embed/vk3s5JlyHfU?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="AI software engineer adds a feature to an open source repository"></iframe></p></div></div><div id="w-node-_1a215dc4-cda1-8bd5-932e-7f6edd75b9d1-15286f21"><p><strong>Devin can contribute to mature production repositories. </strong>‚Äç<br>This example is part of the SWE-bench benchmark. Devin solves a bug with logarithm calculations in the sympy Python algebra system. Devin sets up the code environment, reproduces the bug, and codes and tests the fix on its own. <br></p><div id="w-node-_1a215dc4-cda1-8bd5-932e-7f6edd75b9db-15286f21"><p><iframe src="https://www.youtube.com/embed/ReE2dFJn_uY?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="Our AI fixes a bug in Python algebra system"></iframe></p></div></div><div id="w-node-_5c76f232-867c-f50d-f659-c17780009b06-15286f21"><p><strong>We even tried giving Devin real jobs on Upwork and it could do those too! <br>‚Äç</strong>Here, Devin writes and debugs code to run a computer vision model. Devin samples the resulting data and compiles a report at the end.<br></p><div id="w-node-_5c76f232-867c-f50d-f659-c17780009b0c-15286f21"><p><iframe src="https://www.youtube.com/embed/UTS2Hz96HYQ?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="Devin's Upwork Side Hustle"></iframe></p></div></div><div id="w-node-_75d93268-fa76-7d60-0a38-d621ef8204c8-15286f21"><div id="w-node-_75d93268-fa76-7d60-0a38-d621ef8204c9-15286f21"><p>Devin's Performance</p><div><p>We evaluated Devin on <a href="https://swebench.com/">SWE-bench</a>, a challenging benchmark that asks agents to resolve real-world GitHub issues found in open source projects like Django and scikit-learn. </p><p>Devin correctly resolves 13.86%* of the issues end-to-end, far exceeding the previous state-of-the-art of 1.96%. Even when given the exact files to edit, the best previous models can only resolve 4.80% of issues.</p></div></div><div id="w-node-abf697be-9572-37d9-3d1d-d4e7645788c1-15286f21"><p><img src="https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph.png" loading="lazy" sizes="100vw" srcset="https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph-p-500.png 500w, https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph-p-800.png 800w, https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph-p-1080.png 1080w, https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph-p-1600.png 1600w, https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph-p-2000.png 2000w, https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph-p-2600.png 2600w, https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph-p-3200.png 3200w, https://assets-global.website-files.com/65cf071d26e52092bc212f6e/65efb346f23abefa856c1454_graph.png 3824w" alt=""></p><p>*Devin was evaluated on a random 25% subset of the dataset. Devin was unassisted, whereas all other models were assisted&nbsp;(meaning the model was told exactly which files need to be edited).<br></p></div><p>We plan to publish a more detailed technical report soon‚Äîstay tuned for more details.<br></p></div><p>About Cognition</p><div><p>We are an applied AI lab focused on reasoning.<br>‚Äç<br>We‚Äôre building AI teammates with capabilities far beyond today‚Äôs existing AI tools. By solving reasoning, we can unlock new possibilities in a wide range of disciplines‚Äîcode is just the beginning. We want to help people around the world turn their ideas into reality.</p><p>We are well funded, including a <strong>$21 million Series A led by Founders Fund</strong>. And we‚Äôre grateful for the support of industry leaders including <strong>Patrick and John Collison, Elad Gil, Sarah Guo, Chris Re, Eric Glyman, Karim Atiyeh</strong>, <strong>Erik Bernhardsson, Tony Xu, Fred Ehrsam</strong> and so many more.</p></div><p>Hire Devin</p><p>Devin is currently in early access as we ramp up capacity. To start using Devin for engineering work, <a href="https://forms.gle/N8HZbXbnoosik21n8">please reach out here</a> or get in touch at <a href="mailto:info@cognition-labs.com">info@cognition-labs.com</a>.‚Ä®<br></p><p>Join Us</p><div><p>Our team is small and talent-dense. Our founding team has 10 IOI gold medals and includes leaders and builders who have worked at the cutting edge of applied AI at companies like Cursor, Scale AI, Lunchclub, Modal, Google DeepMind, Waymo, and Nuro.<br>‚Äç<br>Building Devin is just the first step‚Äîour hardest challenges still lie ahead. &nbsp;If you‚Äôre excited to solve some of the world‚Äôs biggest problems and build AI that can reason, learn more about our team and <a href="https://jobs.ashbyhq.com/cognition">apply to join us here</a>. </p><p>‚Äç</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[More options for apps distributed in the European Union (303 pts)]]></title>
            <link>https://developer.apple.com/news/?id=8c1m8hqt</link>
            <guid>39678555</guid>
            <pubDate>Tue, 12 Mar 2024 12:11:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.apple.com/news/?id=8c1m8hqt">https://developer.apple.com/news/?id=8c1m8hqt</a>, See on <a href="https://news.ycombinator.com/item?id=39678555">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main" role="main">
					
					
										
					<article id="article-8c1m8hqt">
						<div>
									<p>March 12, 2024</p>
																			<div><p>We‚Äôre providing more flexibility for developers who distribute apps in the European Union (EU), including introducing a new way to distribute apps directly from a developer‚Äôs website.</p><h3>More flexibility</h3><p>Developers who‚Äôve agreed to the Alternative Terms Addendum for Apps in the EU have new options for their apps in the EU:</p><ul>
<li>Alternative app marketplaces. Marketplaces can choose to offer a catalog of apps solely from the developer of the marketplace.</li>
<li>Linking out to purchase. When directing users to complete a transaction for digital goods or services on an external webpage, developers can choose how to design promotions, discounts, and other deals. The Apple-provided design templates, which are optimized for key purchase and promotional use cases, are now optional. </li>
</ul><h3>Distributing directly from your website</h3><p>Web Distribution, available with a software update later this spring, will let authorized developers distribute their iOS apps to EU users directly from a website owned by the developer. Apple will provide authorized developers access to APIs that facilitate the distribution of their apps from the web, integrate with system functionality, back up and restore users‚Äô apps, and more. For details, visit <a href="https://developer.apple.com/support/web-distribution-eu/">Getting ready for Web Distribution in&nbsp;the&nbsp;EU</a>. </p></div>

																	</div>


					</article>
									</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I no longer maintain my Emacs projects on Sourcehut (141 pts)]]></title>
            <link>https://protesilaos.com/codelog/2024-01-27-sourcehut-no-more/</link>
            <guid>39678491</guid>
            <pubDate>Tue, 12 Mar 2024 12:00:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://protesilaos.com/codelog/2024-01-27-sourcehut-no-more/">https://protesilaos.com/codelog/2024-01-27-sourcehut-no-more/</a>, See on <a href="https://news.ycombinator.com/item?id=39678491">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
          üèÜ I provide private lessons on Emacs, Linux, and Life in
          general: <a href="https://protesilaos.com/coach/">https://protesilaos.com/coach/</a>.
          Lessons continue <strong>throughout the year</strong>.
        </p><article>

	

	<div>
		<p>On 2022-04-07 I moved all my Emacs projects from GitLab to SourceHut:
<a href="https://protesilaos.com/codelog/2022-04-07-all-emacs-projects-sourcehut/">https://protesilaos.com/codelog/2022-04-07-all-emacs-projects-sourcehut/</a>.
I am now undoing that decision. My code is on GitLab and GitHub, with
the latter as the de facto primary source.</p>

<p>Why the change:</p>

<ul>
  <li>The SourceHut web interface does not show any kind of indication
that a message has an attachment. Last time I tried it, I had to
download an mbox file and extract the patch from there. This was
helped by the fact that I knew what I was searching for, but the
experience was not pleasant anyway.</li>
  <li>Many users are reluctant to subscribe to my project‚Äôs mailing list
and instead email me directly. This is fine, as I do get the work
done eventually, but it beats the point of having a public inbox if
I am the only one who reads those messages anyway.</li>
  <li>Personal emails for package maintenance make it more difficult for
me to apply filters. I cannot easily go from ‚Äúpersonal‚Äù to
‚Äúpackages‚Äù and thus have trouble prioritising tasks.</li>
  <li>Users replying to mailing list threads frequently do not ‚Äúreply to
all‚Äù, so the filter I have for SourceHut lists does not apply and my
inbox is noisy again.</li>
  <li>Coordinating work between my ‚Äúofficial‚Äù SourceHut-based repository
and the de facto GitHub one is a problem, because a user on one may
not be aware of what a user on the other is doing.</li>
</ul>

<p>There are other papercuts as well, but I understand SourceHut is still
in ‚Äúalpha‚Äù, so I will not list them here.</p>

<h2>What happens to the Git repositories on SourceHut?</h2>

<p>I am no longer updating them and plan to delete them in the near
future.</p>

<h2>What about the mailing lists?</h2>

<p>I will continue to reply to messages sent there, but I will eventually
ask people to use other media. If you do not want to use a Git forge
to report an issue or send a patch, then do it via my personal email:
<a href="https://protesilaos.com/contact">https://protesilaos.com/contact</a>.</p>

<h2>Will you host your own Git server?</h2>

<p>I want to do this at some point because I am mindful of the issues
with proprietary Git forges. But this is a task that requires
knowledge and resources. So it will not happen soon.</p>

	</div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The internet is slipping out of our reach (112 pts)]]></title>
            <link>https://injuly.in/blog/darker-internet/index.html</link>
            <guid>39677677</guid>
            <pubDate>Tue, 12 Mar 2024 09:43:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://injuly.in/blog/darker-internet/index.html">https://injuly.in/blog/darker-internet/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39677677">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
			<hr>

      
      <p>Do you often append "reddit" or "wiki" to your Google search queries? <br>
Seek out human discourse on specific websites,
when search engines were invented to do it for you?
Worry not, for soon search engines will be useless.</p>
<p>If you searched Google for a movie review in 2005, you'd get a review.
Today, you get paid search results, ads, third-party cookies,
trackers, newsletter prompts, notification requests, and,
if you're lucky ‚Äì a movie review.
No matter what you search for, SEO-hacked content farms will blot out the sun.<sup><a href="#fn-1" id="fnref-1">1</a></sup></p>
<p>The average essay is written not to be read, but to be found‚Äîby a search engine,
that is‚Äîso it can monetize every fiber of your eyeballs.
Keep you scrolling, clicking, and sharing.
Like this article? Maybe you'll like this e-book too.
It's free! (Just enter your email ;)).</p>
<p>The internet continues to grow in volume, but shrink in diversity.
As if everything has moved to a few select apps,
which then <a href="https://www.theverge.com/2024/2/17/24075670/reddit-ai-training-license-deal-user-content">sell our words</a> to AI slot machines.
Images on Instagram, videos on YouTube, and discussions on Reddit.
Dare to venture out of this bubble,
and you'll be pelted with SEO spam by content farms squabbling over search ranks and impressions.</p>
<p>But do you know what's worse than SEO spam?</p>
<h2 id="ai-generated-seo-spam-">AI generated SEO spam. <br></h2>
<p>With services that <a href="https://injuly.in/assets/img/darker-internet/seo-heist.webp">generate thousands of articles</a>,
post them on multiple websites,
then create fake profiles to market them<sup><a href="#fn-2" id="fnref-2">2</a></sup>;
the phrase "As an AI language model" now turns up on LinkedIn, Amazon, Twitter,
and even Google scholar.
In a matter of weeks, LLMs can generate more text than has ever been written in human history.
Every passing day, humans grow farther apart, and bots fill the void in between.</p>
<p>This steady decline hearkens me back to my early teenage years ‚Äì
when I was an active user of several RTS video-game forums.
At the time, the internet was smaller and fostered organic growth.
Smaller websites were easier to find, game mods and fanfics weren't subreddits, and personal pages didn't live on Substack or Medium<sup><a href="#fn-3" id="fnref-3">3</a></sup>.</p>
<p>When I'm older, I expect to surf an internet that will have exploded with AI content,
like the big bang leaving <a href="https://en.wikipedia.org/wiki/Dark_forest_hypothesis">a dark forest</a> behind<sup><a href="#fn-4" id="fnref-4">4</a></sup>.
Online TV shows and webtoons with unending episodes generated on the fly,
digital journals overflowing with articles, and chat apps with AI companions to ease loneliness.
Human interaction will stay burrowed underground, in the tight confines of small and heavily moderated groups.</p>
<p>For the past few years, I've been holed up in small discord servers ‚Äì
the only place where I expect to freely interact with meaningful messages that aren't marketing posts in disguise.
And if you too have been hiding in telegram groups and mailing lists  ‚Äì good, don't leave.
It's dark outside.</p>




			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Announces Stealing Part of a Production Language Model (104 pts)]]></title>
            <link>https://twitter.com/_akhaliq/status/1767384751269150828</link>
            <guid>39677611</guid>
            <pubDate>Tue, 12 Mar 2024 09:30:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/_akhaliq/status/1767384751269150828">https://twitter.com/_akhaliq/status/1767384751269150828</a>, See on <a href="https://news.ycombinator.com/item?id=39677611">Hacker News</a></p>
Couldn't get https://twitter.com/_akhaliq/status/1767384751269150828: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Shields up: New ideas might make active shielding viable (147 pts)]]></title>
            <link>https://arstechnica.com/science/2024/03/shields-up-new-ideas-might-make-active-shielding-viable/</link>
            <guid>39677530</guid>
            <pubDate>Tue, 12 Mar 2024 09:10:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2024/03/shields-up-new-ideas-might-make-active-shielding-viable/">https://arstechnica.com/science/2024/03/shields-up-new-ideas-might-make-active-shielding-viable/</a>, See on <a href="https://news.ycombinator.com/item?id=39677530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/active-radiation-shielding-800x450.jpg" alt="Shields up: New ideas might make active shielding viable">
      <figcaption><p>Aurich Lawson | Getty Images | NASA</p></figcaption>  </figure>

  




<!-- cache hit 206:single/related:346c4fe386f45703755c78b2601a96d9 --><!-- empty -->
<p>On October 19, 1989, at 12:29 UT, a monstrous X13 class solar flare triggered a geomagnetic storm so strong that auroras lit up the skies in Japan, America, Australia, and even Germany the following day. Had you been flying around the Moon at that time, you would have absorbed well over 6 Sieverts of radiation‚Äîa dose that would most likely kill you within a month or so.</p>
<p>This is why the Orion spacecraft that is supposed to take humans on a Moon fly-by mission this year has a heavily shielded storm shelter for the crew. But shelters like that aren‚Äôt sufficient for a flight to Mars‚ÄîOrion‚Äôs shield is designed for a 30-day mission.</p>
<p>To obtain protection comparable to what we enjoy on Earth would require hundreds of tons of material, and that's simply not possible in orbit. The primary alternative‚Äîusing active shields that deflect charged particles just like the Earth‚Äôs magnetic field does‚Äîwas first proposed in the 1960s. Today, we‚Äôre finally close to making it work.</p>
<h2>Deep-space radiation</h2>
<p>Space radiation comes in two different flavors. Solar events like flares or coronal mass ejections can cause very high fluxes of charged particles (mostly protons). They're nasty when you have no shelter but are relatively easy to shield against since solar protons are mostly low energy. The majority of solar particle events flux is between 30 Mega-electronVolts to 100 MeV and could be stopped by Orion-like shelters.</p>
<p>Then there are galactic cosmic rays: particles coming from outside the Solar System, set in motion by faraway supernovas or neutron stars. These are relatively rare but are coming at you all the time from all directions. They also have high energies, starting at 200 MeV and going to several GeVs, which makes them extremely penetrating. Thick masses don‚Äôt provide much shielding against them. When high-energy cosmic ray particles hit thin shields, they produce many lower-energy particles‚Äîyou‚Äôd be better off with no shield at all.</p>                                            
                                                        
<p>The particles with energies between 70 MeV and 500 MeV are responsible for 95 percent of the radiation dose that astronauts get in space. On short flights, solar storms are the main concern because they can be quite violent and do lots of damage very quickly. The longer you fly, though, GCRs become more of an issue because their dose accumulates over time, and they can go through pretty much everything we try to put in their way.</p>
<h2>What keeps us safe at home</h2>
<p>The reason nearly none of this radiation can reach us is that Earth has a natural, multi-stage shielding system. It begins with its magnetic field, which deflects most of the incoming particles toward the poles. A charged particle in a magnetic field follows a curve‚Äîthe stronger the field, the tighter the curve. Earth‚Äôs magnetic field is very weak and barely bends incoming particles, but it is huge, extending thousands of kilometers into space.</p>
<p>Anything that makes it through the magnetic field runs into the atmosphere, which, when it comes to shielding, is the equivalent of an aluminum wall that's 3 meters thick. Finally, there is the planet itself, which essentially cuts the radiation in half since you always have 6.5 billion trillion tons of rock shielding you from the bottom.</p>
<p>To put that in perspective, the Apollo crew module had on average 5 grams of mass per square centimeter standing between the crew and radiation. A typical ISS module has twice that, about 10 g/cm2. The Orion shelter has 35‚Äì45 g/cm2, depending on where you sit exactly, and it weighs 36 tons. On Earth, the atmosphere alone gives you 810 g/cm2‚Äîroughly 20 times more than our best shielded spaceships.</p>
<p>The two options are to add more mass‚Äîwhich gets expensive quickly‚Äîor to shorten the length of the mission, which isn‚Äôt always possible. So solving radiation with passive mass won't cut it for longer missions, even using the best shielding materials like polyethylene or water. This is why making a miniaturized, portable version of the Earth‚Äôs magnetic field was on the table from the first days of space exploration. Unfortunately, we discovered it was far easier said than done.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Bevy Foundation (216 pts)]]></title>
            <link>https://bevyengine.org/news/bevy-foundation/</link>
            <guid>39677009</guid>
            <pubDate>Tue, 12 Mar 2024 07:31:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bevyengine.org/news/bevy-foundation/">https://bevyengine.org/news/bevy-foundation/</a>, See on <a href="https://news.ycombinator.com/item?id=39677009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It is with excitement that I unveil to the world ...</p>
<p><img src="https://bevyengine.org/news/bevy-foundation/bevy_foundation.svg" alt="Bevy Foundation Logo"></p><p>The <a href="https://bevyengine.org/foundation"><strong>Bevy Foundation</strong></a> is the next step in our journey to build a world class free and open source game engine. <strong>Bevy Foundation</strong> is a non-profit organization formed in Washington State, with a pending federal 501(c)(3) tax-exemption application (more on this later).</p>
<p>This is a big step for us and it was not taken lightly. This will be a long blog post, so I'll cover the highlights here:</p>
<ol>
<li><strong>Bevy Foundation</strong> is largely a legal formalization of the leadership and operational structure we have already been using. The "Bevy leadership" that you know (and hopefully trust) is also at the helm of <strong>Bevy Foundation</strong>. The biggest difference is that power is distributed more evenly.</li>
<li>Nothing is fundamentally changing about how we build and release Bevy as free and open source software. This is <em>not</em> a business model rug-pull. It is a legal formalization of our mission to build and release Bevy as free and open source software to members of the general public (you!).</li>
<li>You can now <a href="https://bevyengine.org/donate">donate to the Bevy Foundation</a> to support us!</li>
</ol>
<h2 id="some-history">Some History
<a href="#some-history">#</a>
</h2>
<p>We have come such a long way since I first released Bevy to the world in 2020. What was once a one person side project is now built by hundreds of people and used by thousands (we recently broke a million downloads!). Bevy is the most popular, most used game engine built in Rust, and the second most popular open source game engine on GitHub.</p>
<p>I (Carter / @cart) am still Bevy's Project Lead, but I no longer make all of the decisions. For years now <a href="https://bevyengine.org/community/people">The Bevy Organization</a> has delegated decision making authority across members of the Bevy community. We have 5 excellent Maintainers and even more Subject Matter Experts that were each pivotal in making Bevy what it is today.</p>
<p>As we have grown, we have accumulated what I will call "organizational debt":</p>
<ul>
<li><strong>Funding Bevy Has Been A Popularity Contest</strong>: For years, our donate button linked directly to my GitHub sponsors page. This was acceptable early on when I was the only one spending significant time on the project, but that is no longer the case. Last year, we replaced that with a page that any Bevy Organization member could list themselves on. While fairer than the "everything goes to @cart" model, it resulted in funding being a "popularity contest". And unfortunately, given how attached my name is to the project, in practice I still received the majority of the sponsorships. Presenting donors with the task of picking someone to back also deeply complicated the donation process, which likely turned some donors off. We need a simple, centralized donation model where we can direct funds fairly to the areas of the project that need them the most.</li>
<li><strong>We Lacked "Organizational Legitimacy"</strong>: We would like to attract the attention (and funding) of people and companies serious about gamedev. An informal collective of Bevy developers (each asking for support individually) lacks the legitimacy required to attract some entities. We need to be able to present a centralized and transparent view of our operations, with accountable and professional people at the helm.</li>
<li><strong>Carter Held (And Owned) The Keys</strong>: Up until this point, I have owned Bevy-related intellectual property such as the copyright for the Bevy logo. I also owned the <code>bevyengine.org</code> domain name and had exclusive administrative control over most of our infrastructure and communities. I have had absolute authority over all aspects of the project since its inception. Yes, I have delegated, but it all stemmed from me at the root. This had its efficiencies, but it is no longer sustainable for me or ethically sound when there are <em>so many</em> others that deserve actual <em>legal</em> say over how the project is run. There is also the <a href="https://en.wikipedia.org/wiki/Bus_factor">"bus factor"</a> to consider. If something were to happen to me, some aspects of the Bevy project would be lost forever.</li>
</ul>
<p>Bevy needs a structure that lets us work together: a legal entity that embodies our goals and principles, where responsibility and decision making are all shared.</p>
<h2 id="introducing-the-bevy-foundation">Introducing the Bevy Foundation!
<a href="#introducing-the-bevy-foundation">#</a>
</h2>
<h3 id="our-mission">Our Mission
<a href="#our-mission">#</a>
</h3>
<p>To quote our 501(c)(3) application:</p>
<blockquote>
<p>Our mission is to promote, protect, and advance the free and open source Bevy Engine and related open source projects. We coordinate and promote its continued maintenance and development, educate and train members of the general public in its usage, and conduct research and development to advance the state of the art of creating real-time applications and simulations.</p>
</blockquote>
<p>In short: <strong>we exist to develop Bevy and teach people how to use it</strong>!</p>
<h3 id="legal-status">Legal Status
<a href="#legal-status">#</a>
</h3>
<p><a href="https://bevyengine.org/foundation"><strong>Bevy Foundation</strong></a> is a non-profit incorporated in Washington State. This means that the money we raise <a href="https://dor.wa.gov/education/industry-guides/nonprofit-organizations#Fundraising">cannot be used to benefit our members, officers, or directors, except as compensation for services rendered</a>. We do not have "owners" or "shareholders". We are formed exclusively to accomplish our mission as stated above as a public benefit.</p>
<h3 id="federal-501-c-3-application">Federal 501(c)(3) Application
<a href="#federal-501-c-3-application">#</a>
</h3>
<p>We have also applied for a federal 501(c)(3) non-profit public charity designation.</p>
<p>If our 501(c)(3) application is accepted, this status will exempt us from federal income tax and allow donors in the United States to deduct their donations on their federal taxes.</p>
<p>Note that the 501(c)(3) application has not yet been approved, and there has been an increasing number of denials for technology-oriented charities. The experts we have talked to have told us not to get our hopes up.</p>
<p>If we are denied, our default strategy will be to continue operating as a state-only charity and pay federal taxes. As long as we spend most of the donations we receive, the taxes shouldn't be too painful. We may also consider other designations, such as 501(c)(6), but we are cautious about that designation. We want to focus on the members of the Bevy community as people, not through the lens of their commercial interests.</p>
<p>If tax-deductibility is important to you, <em>please do not donate until we receive a determination</em>. But also note that if our application is accepted, donations made <em>prior</em> to our acceptance will <em>retroactively</em> be tax-deductible! Neat!</p>
<h3 id="the-board-of-directors">The Board of Directors
<a href="#the-board-of-directors">#</a>
</h3>
<p>We are run by the <strong>Bevy Foundation</strong> Board of Directors. All actions taken by <strong>Bevy Foundation</strong> are determined by a vote of the board.</p>
<p>The current Board of Directors is: Carter Anderson (@cart), Alice Cecile (@alice-i-cecile), Fran√ßois Mockers (@mockersf), Robert Swain (@superdump), and James Liu (@james7132).</p>
<p>I have been elected President of the board and Alice has been elected Secretary. For logistical reasons, I have also been elected interim Treasurer to get our initial financial situation spun up. It makes sense for a US-based board member to set up things like bank accounts for a US-based organization. But we would like to pass off the role of Treasurer as quickly as possible.</p>
<p>You may notice that every member of the board is an active Bevy Maintainer. This is not a coincidence. We believe that people <em>actually developing the project</em> should be at the helm. There is currently no way to buy a board seat. We have no plans to hire "professional board members" that aren't in the thick of it. For now we plan to stick to the Maintainer == Board Member policy. Functionally, new Board Members are selected via an election of the current board. "Board Member" is currently not a paid position.</p>
<p>Note that I am using the phrasing "currently" and "for now" in some places above because I am <em>no longer capable of making absolute statements about the future of the project</em>. I am a board member, which gives me a vote. And as President I have been given some authority by the board to manage our operations. But I must follow the resolutions of the board and I <em>can</em> be voted out. This is a good thing I promise! It means that the project can exist without me. And if I am no longer fulfilling my duties to the satisfaction of the board, I probably <em>should</em> be ousted.</p>
<p>This is a leap of faith, but one that has been carefully measured over the course of years. I have worked with the other maintainers (now board members) for a long time now and I trust them ... both with the future of Bevy and with my job.</p>
<h3 id="bylaws">Bylaws
<a href="#bylaws">#</a>
</h3>
<p>We have adopted public <a href="https://bevyengine.org/foundation/bylaws">Bylaws</a> that govern how we operate. They determine how we are structured, the roles in the organization, how meetings and voting take place, a conflict of interest policy, and more. Note these are largely borrowed from the standard legal boilerplate (and they exist for a legal purpose), so they're stiffer than our normal style.</p>
<h3 id="transparency">Transparency
<a href="#transparency">#</a>
</h3>
<p>Our goal is to make the operations of <strong>Bevy Foundation</strong> as transparent as possible. We make the following public:</p>
<ul>
<li><a href="https://bevyengine.org/foundation/bylaws"><strong>Bylaws</strong></a>: The operational rules we have adopted</li>
<li><a href="https://bevyengine.org/foundation/meeting-minutes"><strong>Meeting Minutes</strong></a>: Learn about the decisions the board makes and why we make them</li>
<li><a href="https://drive.google.com/drive/u/0/folders/1Q_vdAaI0tsWZdcddL9lDicDZhjPl79s9?ths=true"><strong>Documents</strong></a>:  Public Bevy Foundation documents, such as our Articles of Incorporation</li>
<li><a href="https://bevyengine.org/foundation/budget"><strong>Budget</strong></a>: How we plan to spend money this year</li>
</ul>
<p>We will strive to be approachable and accountable for our actions, and we will continue to make our technical decisions in public with the input of the community. You know who we are and where to find us!</p>
<h2 id="our-plans">Our Plans
<a href="#our-plans">#</a>
</h2>
<p><strong>Bevy Foundation</strong> will direct the majority of its funding to fueling Bevy development.</p>
<h3 id="short-term-bring-alice-on-full-time">Short Term: Bring Alice on full-time
<a href="#short-term-bring-alice-on-full-time">#</a>
</h3>
<p>We have exactly one focus for the short term: bring Alice on full-time. For years, Alice has dedicated <em>significant</em> time to the project as a Maintainer. She has been our primary project manager: wrangling issues, <a href="https://mastodon.social/deck/tags/bevymergetrain">keeping the merge train running</a>, connecting developers to the right parts of the project, and doing the work that ensures we can harness and enable our ever increasing developer community. Not to mention her extensive technical contributions and documentation work!</p>
<p>If you have participated in Bevy's development, you are probably intimately aware of two things:</p>
<ol>
<li>Project management is our bottleneck.</li>
<li>Alice is a world-class project manager.</li>
</ol>
<p>In addition to performing the general "project management" role, Alice has the following specific priorities when she starts full-time:</p>
<ol>
<li><strong>The New Bevy Book</strong>: The new Bevy Book aims to be a complete, always up-to-date, and constantly improving guide to using Bevy. Alice has already contributed content and helped facilitate the book's production. If she joins full-time, she will work to finish it, in addition to helping others contribute.</li>
<li><strong>Coordinating the Development of ECS Relations</strong>: Relations are an ECS feature that enable connecting entities to each other via special components called Relations. These are an often-requested feature that will make a number of scenarios (such as parent-child relationships) much nicer. Alice will work to coordinate this effort and fill in functionality gaps.</li>
<li><strong>Building an Action System for Input Devices</strong>: Bevy sorely needs an official way to define "input actions", which are then mapped (and remapped) to inputs from one or more input devices (for example: a <code>Jump</code> action that is mapped to the "A" button on controllers and "Spacebar" on keyboards). Alice built <a href="https://github.com/leafwing-studios/leafwing-input-manager"><code>leafwing-input-manager</code></a>, which is a popular third-party Bevy plugin that does exactly that. Alice plans on taking the lessons learned there to build a proper first-party solution to this problem.</li>
</ol>
<p>Alice deserves to be paid for her work, and Bevy needs her full-time. Our goal for the <strong>Bevy Foundation</strong> is to pay reasonably competitive, roughly market rate salaries. Therefore, our first goal is to pay Alice a salary of $150,000 a year.</p>
<p>We all believe Alice is worth at least that much, but given that funds will take time to build up, Alice has agreed to take a pay cut while things spin up.</p>
<h4 id="what-about-carter">What about Carter?
<a href="#what-about-carter">#</a>
</h4>
<p>Some may ask why I (Carter), the creator and Project Lead of Bevy, am not the first to be paid by the <strong>Bevy Foundation</strong>. Thats easy: I already currently have enough support from my current sponsors to live on, and I have historically received the lion's share of sponsorships. The scales have been tilted in my favor for too long. Prioritizing Alice is the only fair choice, and also happens to be what is best for Bevy right now.</p>
<p>I suspect my current sponsors will transition to the <strong>Bevy Foundation</strong> over time (and I encourage them to do so!). As this happens there will likely be an inflection point where Alice is making more than me. When we hit that point, we will sort out a balancing strategy until we are both paid our target wages.</p>
<p>Note if we receive 501(c)(3) status (see the 501(c)(3) section above), we are allowed to pay competitive wages, but we can't pay <em>excessive</em> wages (nor are we <em>interested in doing so</em>). Once Alice and Carter are paid reasonable wages, we will shift our financial focus elsewhere.</p>
<h3 id="future-plans">Future Plans
<a href="#future-plans">#</a>
</h3>
<p>Our focus will always be on funding more Bevy developers, both full-time and part-time. We will likely explore targeted one time grants for specific efforts.</p>
<p>Later down the line, we will likely explore the development of a Bevy Asset store where the community can list and sell Bevy-compatible assets.</p>
<p>We would also like to re-introduce a Bevy Merch store. While we don't think this is an ideal fundraising strategy, we think it will be a fun way for the Bevy community to show their pride!</p>
<p>When we consider future programs, we will ask the questions:</p>
<ol>
<li>Is this compatible with the Bevy Foundation's mission?</li>
<li>Is this in the best interest of the Bevy community and the general public?</li>
<li>Will this compromise the integrity of Bevy as a free and open source offering in any way?</li>
<li>Will this change our incentives in a way that risks changing our answers to (1), (2), or (3)?</li>
<li>If we are monetizing something, are we doing it in an ethical way?</li>
</ol>
<p>Know that we exist <em>for the benefit of the Bevy community and the general public</em> and we will try our utmost to never compromise that.</p>
<h2 id="a-new-era">A New Era
<a href="#a-new-era">#</a>
</h2>
<p>The <strong>Bevy Foundation</strong> is both an accomplishment, as the culmination of months of work and research, and a Foundation, on top of which we will build the future of Bevy. We are building technology that will enrich the lives of everyone as a common, publicly available good. Game development is currently an industry of rent seekers and gatekeepers. I believe that a better world can exist: one where we collectively build tools <em>for each other</em> in the open. Bevy is ideologically and technologically a rethinking of what this industry should be.</p>
<p>I am deeply proud of what we have accomplished so far and I can't wait for what this new Bevy era will bring.</p>
<h2 id="we-need-your-support">We Need Your Support!
<a href="#we-need-your-support">#</a>
</h2>
<p>Bevy will always be free and open source. However our plans for Bevy's future are grand ... they will require significant financial support. Please consider donating if you enjoy using Bevy and believe in our mission.</p>
<p>If we receive 501(c)(3) public charity status, to maintain that status we will require a significant portion of our funds to come from individual people (not just companies). Your contribution matters!</p>
<p>If you are already supporting us through individual sponsorships, consider switching your donation to <strong>Bevy Foundation</strong>, as that will make it easier for us to direct your contribution to the areas that need it most.</p>
<p>Visit our brand new donation page here:</p>
<p><a href="https://bevyengine.org/donate">Donate <img src="https://bevyengine.org/assets/heart.svg" alt="heart icon"></a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloning a Laptop over NVMe TCP (356 pts)]]></title>
            <link>https://copyninja.in/blog/clone_laptop_nvmet.html</link>
            <guid>39676767</guid>
            <pubDate>Tue, 12 Mar 2024 06:37:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://copyninja.in/blog/clone_laptop_nvmet.html">https://copyninja.in/blog/clone_laptop_nvmet.html</a>, See on <a href="https://news.ycombinator.com/item?id=39676767">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>Recently, I got a new laptop and had to set it up so I could start using it. But I wasn't really in the mood to go through the same old steps which I had explained in this <a href="https://copyninja.in/blog/live_install_debian.html">post earlier</a>. I was complaining about this to my colleague, and there came the suggestion of why not copy the entire disk to the new laptop. Though it sounded like an interesting idea to me, I had my doubts, so here is what I told him in return.</p><ol><li>I don't have the tools to open my old laptop and connect the new disk over USB to my new laptop.</li><li>I use full disk encryption, and my old laptop has a 512GB disk, whereas the new laptop has a 1TB NVME, and I'm not so familiar with resizing LUKS.</li></ol><p>He promptly suggested both could be done. For step 1, just expose the disk using NVME over TCP and connect it over the network and do a full disk copy, and the rest is pretty simple to achieve. In short, he suggested the following:</p><ol><li>Export the disk using nvmet-tcp from the old laptop.</li><li>Do a disk copy to the new laptop.</li><li>Resize the partition to use the full 1TB.</li><li>Resize LUKS.</li><li>Finally, resize the BTRFS root disk.</li></ol><div id="exporting-disk-over-nvme-tcp"><h2>Exporting Disk over NVME TCP</h2><p>The easiest way suggested by my colleague to do this is using <a href="https://www.freedesktop.org/software/systemd/man/latest/systemd-storagetm.service.html">systemd-storagetm.service</a>. This service can be invoked by simply booting into <em>storage-target-mode.target</em> by specifying <em>rd.systemd.unit=storage-target-mode.target</em>. But he suggested not to use this as I need to tweak the dracut initrd image to involve network services as well as configuring WiFi from this mode is a painful thing to do.</p><p>So alternatively, I simply booted both my laptops with GRML rescue CD. And the following step was done to export the NVME disk on my current laptop using the nvmet-tcp module of Linux:</p><div><pre><span></span>modprobe<span> </span>nvemt-tcp
<span>cd</span><span> </span>/sys/kernel/config/nvmet
mkdir<span> </span>ports/0
<span>cd</span><span> </span>ports/0
<span>echo</span><span> </span><span>"ipv4"</span><span> </span>&gt;<span> </span>addr_adrfam
<span>echo</span><span> </span><span>0</span>.0.0.0<span> </span>&gt;<span> </span>addr_traaddr
<span>echo</span><span> </span><span>4420</span><span> </span>&gt;<span> </span>addr_trsvcid
<span>echo</span><span> </span>tcp<span> </span>&gt;<span> </span>addr_trtype

<span>cd</span><span> </span>/sys/kernel/config/nvmet/subsystems
mkdir<span> </span>testnqn
<span>echo</span><span> </span><span>1</span><span> </span>&gt;testnqn/allow_any_host
mkdir<span> </span>testnqn/namespaces/1

<span>cd</span><span> </span>testnqn
<span># replace the device name with the disk you want to export</span>
<span>echo</span><span> </span><span>"/dev/nvme0n1"</span><span> </span>&gt;<span> </span>namespaces/1/device_path
<span>echo</span><span> </span><span>1</span><span> </span>&gt;<span> </span>namespaces/1/enable

ln<span> </span>-s<span> </span><span>"../../subsystems/testnqn"</span><span> </span>/sys/kernel/config/nvmet/ports/0/subsystems/testnqn
</pre></div><p>These steps ensure that the device is now exported using NVME over TCP. The next step is to detect this on the new laptop and connect the device:</p><div><pre><span></span>nvme<span> </span>discover<span> </span>-t<span> </span>tcp<span> </span>-a<span> </span>&lt;ip&gt;<span> </span>-s<span> </span><span>4420</span>
nvme<span> </span>connectl-all<span> </span>-t<span> </span>tcp<span> </span>-a<span> </span>&lt;&gt;<span> </span>-s<span> </span><span>4420</span>
</pre></div><p>Finally, <tt>nvme list</tt> shows the device which is connected to the new laptop, and we can proceed with the next step, which is to do the disk copy.</p></div><div id="copying-the-disk"><h2>Copying the Disk</h2><p>I simply used the <tt>dd</tt> command to copy the root disk to my new laptop. Since the new laptop didn't have an Ethernet port, I had to rely only on WiFi, and it took about 7 and a half hours to copy the entire 512GB to the new laptop. The speed at which I was copying was about 18-20MB/s. The other option would have been to create an initial partition and file system and do an rsync of the root disk or use BTRFS itself for file system transfer.</p><div><pre><span></span>dd<span> </span><span>if</span><span>=</span>/dev/nvme2n1<span> </span><span>of</span><span>=</span>/dev/nvme0n1<span> </span><span>status</span><span>=</span>progress<span> </span><span>bs</span><span>=</span>40M
</pre></div></div><div id="resizing-partition-and-luks-container"><h2>Resizing Partition and LUKS Container</h2><p>The final part was very easy. When I launched <tt>parted</tt>, it detected that the partition table does not match the disk size and asked if it can fix it, and I said yes. Next, I had to install <tt><span>cloud-guest-utils</span></tt> to get <tt>growpart</tt> to fix the second partition, and the following command extended the partition to the full 1TB:</p><p>Next, I used <tt><span>cryptsetup-resize</span></tt> to increase the LUKS container size.</p><div><pre><span></span>cryptsetup<span> </span>luksOpen<span> </span>/dev/nvme0n1p2<span> </span>ENC
cryptsetup<span> </span>resize<span> </span>ENC
</pre></div><p>Finally, I rebooted into the disk, and everything worked fine. After logging into the system, I resized the BTRFS file system. BTRFS requires the system to be mounted for resize, so I could not attempt it in live boot.</p><div><pre><span></span>btfs<span> </span>fielsystem<span> </span>resize<span> </span>max<span> </span>/
</pre></div></div><div id="conclussion"><h2>Conclussion</h2><p>The only benefit of this entire process is that I have a new laptop, but I still feel like I'm using my existing laptop. Typically, setting up a new laptop takes about a week or two to completely get adjusted, but in this case, that entire time is saved.</p><p>An added benefit is that I learned how to export disks using NVME over TCP, thanks to my colleague. This new knowledge adds to the value of the experience.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a free animator. Think Adobe Illustrator but for animation (521 pts)]]></title>
            <link>https://www.trangram.com</link>
            <guid>39675807</guid>
            <pubDate>Tue, 12 Mar 2024 03:15:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.trangram.com">https://www.trangram.com</a>, See on <a href="https://news.ycombinator.com/item?id=39675807">Hacker News</a></p>
<div id="readability-page-1" class="page"><!--nghm-->
    <app-root _nghost-ng-c3984273398="" ng-version="17.2.4" ngh="12" ng-server-context="ssr"><router-outlet _ngcontent-ng-c3984273398=""></router-outlet><app-home _nghost-ng-c1665951462="" ngh="11"><mat-sidenav-container _ngcontent-ng-c1665951462="" ngh="0"><!----><mat-sidenav _ngcontent-ng-c1665951462="" tabindex="-1" role="navigation" mode="over" opened="false" ngh="1"><div cdkscrollable=""><div _ngcontent-ng-c1665951462=""><a _ngcontent-ng-c1665951462="" routerlink="/" href="https://www.trangram.com/"><p><img _ngcontent-ng-c1665951462="" src="https://www.trangram.com/assets/logo.png" alt="Trangram" title="Trangram"></p><p>Trangram</p></a></div><mat-nav-list _ngcontent-ng-c1665951462="" role="navigation" aria-disabled="false" ngh="1"><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/" href="https://www.trangram.com/" aria-disabled="false" ngh="3"><span><span> Home </span></span></a><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/explore" href="https://www.trangram.com/explore" aria-disabled="false" ngh="3"><span><span> Explore </span></span></a><mat-divider _ngcontent-ng-c1665951462="" role="separator" aria-orientation="horizontal" ngh="1"></mat-divider><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/auth/login" href="https://www.trangram.com/auth/login" aria-disabled="false" ngh="3"><span><span> Log In </span></span></a><!----><!----><mat-divider _ngcontent-ng-c1665951462="" role="separator" aria-orientation="horizontal" ngh="1"></mat-divider><div _ngcontent-ng-c1665951462=""><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/about" href="https://www.trangram.com/about" aria-disabled="false" ngh="4"><span><span> About </span></span></a><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/guidelines" href="https://www.trangram.com/guidelines" aria-disabled="false" ngh="4"><span><span> Guidelines </span></span></a><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/privacy" href="https://www.trangram.com/privacy" aria-disabled="false" ngh="4"><span><span> Privacy Policy </span></span></a><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/terms" href="https://www.trangram.com/terms" aria-disabled="false" ngh="4"><span><span> Terms of Use </span></span></a></div></mat-nav-list></div></mat-sidenav><mat-sidenav-content _ngcontent-ng-c1665951462="" ngh="1"><div _ngcontent-ng-c1665951462=""><div _ngcontent-ng-c1665951462=""><a _ngcontent-ng-c1665951462="" routerlink="/" href="https://www.trangram.com/"><p><img _ngcontent-ng-c1665951462="" src="https://www.trangram.com/assets/logo.png" title="Trangram"></p><p>Trangram</p></a></div><!----><!----><app-search-bar _ngcontent-ng-c1665951462="" _nghost-ng-c1314027463="" ngh="6"><mat-form-field _ngcontent-ng-c1314027463="" appearance="outline" ngh="5"><!----></mat-form-field></app-search-bar><app-account-menu _ngcontent-ng-c1665951462="" _nghost-ng-c1621978163="" ngh="9"><div _ngcontent-ng-c1621978163=""><p><a _ngcontent-ng-c1621978163="" mat-flat-button="" routerlink="/auth/login" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/auth/login" aria-disabled="false" ngh="7"><span></span><span>Log in</span><span></span><span></span></a><a _ngcontent-ng-c1621978163="" mat-flat-button="" color="primary" routerlink="/auth/signup" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/auth/signup" aria-disabled="false" ngh="7"><span></span><span>Sign up</span><span></span><span></span></a></p><a _ngcontent-ng-c1621978163="" mat-icon-button="" routerlink="/auth/login" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/auth/login" aria-disabled="false" ngh="8"><span></span><span></span><span></span><span></span></a></div><!----><!----><!----><!----></app-account-menu></div><router-outlet _ngcontent-ng-c1665951462=""></router-outlet><app-landing _nghost-ng-c721036369="" ngh="10"><div _ngcontent-ng-c721036369=""><div _ngcontent-ng-c721036369=""><p>Create Motion Graphics</p><p>On One Platform</p></div><p> Create ‚Ä¢ Animate ‚Ä¢ Share </p><div _ngcontent-ng-c721036369=""><p><a _ngcontent-ng-c721036369="" mat-raised-button="" routerlink="/editor" color="primary" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/editor" aria-disabled="false" ngh="7"><span></span><span> Open Editor </span><span></span><span></span></a><a _ngcontent-ng-c721036369="" mat-raised-button="" routerlink="/about" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/about" aria-disabled="false" ngh="7"><span></span><span>Features</span><span></span><span></span></a></p></div></div><svg _ngcontent-ng-c721036369="" viewBox="70 0 400 90"><path _ngcontent-ng-c721036369="" d="M0,65c198,81.64723 325.5,-117.13249 510.5,23.86751c0,0 -55,-64.5 -181.5,-66.85277c-100.29841,-1.86545 -220,83.48527 -329,10.98527" fill="#00a8a8" opacity="0.5"></path><path _ngcontent-ng-c721036369="" d="M0,35.5c212,110.5 322.5,-101.5 509.5,53.5c0,0 -53.5648,-75.21727 -177,-70.5c-78.5,3 -208.5,71 -332.5,-18.5" fill="#00a8a8"></path><path _ngcontent-ng-c721036369="" d="M0,43.5c213.5,93.5 322.5,-119.63249 510.5,45.36751c0,0 -55.5,-72 -159,-72.5c-109.27475,-0.5279 -224.5,82.63249 -351.5,2.63249" fill="#008080"></path></svg><p>Explore &amp; Get Inspired</p><!----><!----><p> Loading gallery... </p><!----></app-landing><!----></mat-sidenav-content><!----></mat-sidenav-container></app-home><!----></app-root>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stealing Part of a Production Language Model (191 pts)]]></title>
            <link>https://arxiv.org/abs/2403.06634</link>
            <guid>39675735</guid>
            <pubDate>Tue, 12 Mar 2024 03:01:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.06634">https://arxiv.org/abs/2403.06634</a>, See on <a href="https://news.ycombinator.com/item?id=39675735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Carlini,+N">Nicholas Carlini</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Paleka,+D">Daniel Paleka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dvijotham,+K+D">Krishnamurthy Dj Dvijotham</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Steinke,+T">Thomas Steinke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hayase,+J">Jonathan Hayase</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cooper,+A+F">A. Feder Cooper</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+K">Katherine Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jagielski,+M">Matthew Jagielski</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nasr,+M">Milad Nasr</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Conmy,+A">Arthur Conmy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wallace,+E">Eric Wallace</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rolnick,+D">David Rolnick</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tram%C3%A8r,+F">Florian Tram√®r</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2403.06634">Download PDF</a></p><blockquote>
            <span>Abstract:</span>We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under \$2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Nicholas Carlini [<a href="https://arxiv.org/show-email/d18236f1/2403.06634">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 11 Mar 2024 11:46:12 UTC (697 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Cosine-Similarity of Embeddings Really About Similarity? (178 pts)]]></title>
            <link>https://arxiv.org/abs/2403.05440</link>
            <guid>39675585</guid>
            <pubDate>Tue, 12 Mar 2024 02:29:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.05440">https://arxiv.org/abs/2403.05440</a>, See on <a href="https://news.ycombinator.com/item?id=39675585">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.05440">Download PDF</a>
    <a href="https://arxiv.org/html/2403.05440v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Harald Steck [<a href="https://arxiv.org/show-email/bb917e91/2403.05440">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 8 Mar 2024 16:48:20 UTC (5,737 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Breaking Down Tasks (290 pts)]]></title>
            <link>https://jacobian.org/2024/mar/11/breaking-down-tasks/</link>
            <guid>39675249</guid>
            <pubDate>Tue, 12 Mar 2024 01:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jacobian.org/2024/mar/11/breaking-down-tasks/">https://jacobian.org/2024/mar/11/breaking-down-tasks/</a>, See on <a href="https://news.ycombinator.com/item?id=39675249">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><a href="https://jacobian.org/series/estimation/">Estimating Software Projects</a>:</p><p>In a management group, someone asked for resources on teaching planning. I shared a link to <a href="https://jacobian.org/series/estimation/">this series on estimation</a>, but quickly they came back and told me that there was something missing. The previous parts in this series assume you‚Äôre starting with a clearly defined task list, but the people this manager is teach aren‚Äôt there yet. They need help with an earlier step: ‚Äúbreaking down‚Äù a project into a clearly defined set of tasks.</p><p>I‚Äôve not previously written about how to break down a task list because, to me, it largely feels intuitive. Prior to writing what follows, I think I would describe my process as: ‚Äúthink about the project and then write down the task list.‚Äù But that‚Äôs a deeply unsatisfying, <a href="https://knowyourmeme.com/memes/how-to-draw-an-owl">‚Äújust draw the rest of the owl‚Äù</a> sort of answer.</p><p>So here I‚Äôll fill that gap, and dig into what‚Äôs going on when I break down a project into constitute tasks. I‚Äôll start by <a href="#example">working through an example</a>, showing how do it, and then step back and <a href="#what-just-happened">explain the steps I took</a>. But if you want to skip the scenic route, you can skip to the end where I‚Äôve <a href="#tldr">summarized the process</a>.</p><h2 id="example">Breaking down tasks - by example</h2><p>I‚Äôm building a personal streak tracker, tracking days I do some sort of outdoor activity. I want something similar to the <a href="https://streaksapp.com/">Streaks</a> app, except with different options for outdoor activities (running, biking, skiing, etc.), and I want to incorporate the <a href="https://blog.duolingo.com/how-duolingo-streak-builds-habit/">‚Äústreak freeze‚Äù feature from Duolingo</a>.</p><h3 id="iteration-1">Iteration 1</h3><p>I often start with a sketch. Having a visual mockup of what I‚Äôm going to build is a great starting point. The final result rarely ends up looking like this, but it‚Äôs a good way of articulating all the features I want in an easy-to-understand format.</p><figure><a href="https://jacobian.org/2024/mar/11/breaking-down-tasks/mockup.excalidraw.png"><img src="https://jacobian.org/2024/mar/11/breaking-down-tasks/mockup.excalidraw.png" alt="A mockup of the tracker interface, showing a week of activities tracked, a currently active streak, and the freeze features."></a></figure><p>Is this sufficiently ‚Äúbroken down‚Äù? If it‚Äôs just me, if it‚Äôs just a personal project ‚Äì probably yes! For a personal project like this I‚Äôll just sit down and start writing code now. But this is essentially just a one-step task: ‚Äúbuild this picture‚Äù. If I‚Äôm going to delegate some of this work, or if I need to estimate how long this‚Äôll take, I need more granularity. So I‚Äôll continue.</p><h3 id="iteration-2">Iteration 2</h3><p>The next step is to expand my single-step ‚Äúbuild this‚Äù task into its constituent pieces. I‚Äôll think through and plan the the steps I‚Äôd need to go through to get this done. I‚Äôll try to account for dependancies ‚Äì which tasks need to come before other tasks ‚Äì but I‚Äôm not worried about size or scope of the steps yet, just rough list. Here‚Äôs my first attempt:</p><ol><li>Model the data</li><li>Calendar view, showing days of the current week</li><li>Interactive calendar: click an icon records an activity and marks the day as ‚Äúcompleted‚Äù for streak tracking.</li><li>Calculate and show the length of the current streak</li><li>Implement streak freezes</li></ol><p>To keep this blog post simple, I‚Äôm ignoring deployment and other operational steps like setting up a database, etc. I‚Äôm also mostly collapsing front-end and back-end development into single steps like ‚Äúbuild calendar view‚Äù. In a real project, especially a multi-person one, I‚Äôd have separate line items for deployment, front/backend tasks, etc., but that would quickly get more complex than I want to get for this example.</p><p>Ok, is <em>this</em> sufficiently ‚Äúbroken down‚Äù? For some purposes, maybe: I can probably produce an accurate-enough estimation of time for each step, and if I‚Äôm working with a sufficiantly-aligned and autonomous team there might be enough detail here to delegate tasks. But there‚Äôs still a good deal of uncertainty. I don‚Äôt have clear view of how freezes will be accumulated and tracked, so step 5 is a bit vague. I also don‚Äôt have anything about past history in here, adding/removing activity types, and other potential rabbit holes. Those make estimates uncertain, and mean that if I delegate a task I might not get the result I was picturing. So, I‚Äôll continue by breaking things down even further, and add more detail.</p><h3 id="iteration-3">Iteration 3</h3><p>I‚Äôll continue the same process of taking a ‚Äúbig‚Äù step, and breaking it down into constitution pieces, added detail as I go:</p><ol><li><strong>Model the data:</strong><ol><li>activity types: run/bike/ski/climb/etc. (hardcoded list of types is fine)</li><li>recorded activities: date, type</li><li>freezes: date earned, date spent</li><li>streaks: date started, date ended, stats on count of different activity types during the streak (no need for calculation, that can come later)</li></ol></li><li><strong>Static calendar view:</strong><ol><li>Weekly view: shows days of the current week, recorded activities or spent freezes on that day</li><li>Index (home page) view: shows the current week</li><li>Monthly view: show a whole month - no individual activity types, just recorded/not recorded</li><li>Browsing: browse backwards/forwards on the week and month views, and allow switching between weeks and months.</li><li>‚ÄúGo to date‚Äù quick entry box - don‚Äôt get fancy, no fuzzy date input, can use the html5 date widget for now</li></ol></li><li><strong>Dynamic week calendar view</strong> (no dynamic entry on monthly views):<ol><li>clicking an activity type on a day records that day as completed</li></ol></li><li><strong>Streak calculation and display:</strong><ol><li>Streak calculation: walk activity history and calculate streaks ‚Äî start date, end date, stats (e.g. ‚Äú14 days: 9 runs, 3 bikes, 1 climb, 1 freeze‚Äù)</li><li>Streak display: show current streak in the UI. No functionality for viewing past streaks yet</li><li>Recording an activity in the UI recalculates streaks</li></ol></li><li><strong>Streak freezes:</strong><ol><li>When calculating streaks, a streak of X days (without a freeze) accumulates a streak freeze. X can be hardcoded, make up a number for now (roll a die, whatever). Freezes ‚Äúroll over‚Äù: if 3 freezes are accumulated during Streak A, one is spent, and then the streak ends, you still have 2 freezes available to use on Streak B.</li><li>Prevent ‚Äúdouble accumulation‚Äù when recalculating streaks - make sure that if you go back in time and update an activity, causing streak recalculation, you don‚Äôt re-earn streaks.</li><li>Add freezes to the UI: spend a freeze on a day to extend the streak, and display the number of accumulated freezes in the UI.</li></ol></li></ol><p>How about now ‚Äî is this sufficiently broken down? Again the answer is contextual. I think for most engineers and most teams it is: we have clear steps, clear definitions of done, and have carefully avoided all the potential rabbit holes I can think of. I can imagine situations (junior developers, mission-critical projects with intense oversight/scrutiny) where even <em>more</em> detail might be necessary. But for the purposes of this example, let‚Äôs call this done.</p><h2 id="what-just-happened">What just happened?</h2><p>Fundamentally, what I showed is an interactive process of breaking down tasks:</p><ol><li>Start with a list of tasks ‚Äì or just one big project!</li><li>Think through the steps you‚Äôd need to take to accomplish that task, and write them down. Don‚Äôt worry about completeness or accuracy or depth, each pass just needs to expand, even slightly, on the previous one.</li><li>Is every task on your new list sufficiently defined? (I‚Äôll define ‚Äúsufficiently‚Äù in a minute.) If not, GOTO 1.</li></ol><p>This algorithm is simple, but it makes some assumptions. Remember, the context here is someone who‚Äôs never done this sort of thing before, and is pretty new to software development overall. So we need to define a couple of key terms:</p><h3 id="whats-a-task">What‚Äôs a ‚Äútask‚Äù?</h3><p>Merriam-Websters defines task as ‚Ä¶ haha no, just kidding. I can do better than that.</p><p>Many people ‚Äì me included ‚Äì have an intuitive understanding of what ‚Äútask‚Äù means, but when asked to define it, we struggle to articulate it out loud. For the purposes of talking about software development and project estimation, I think this definition is best:</p><dl><dt>Task</dt><dd>A sufficiently defined, complete piece of work that delivers change.</dd></dl><ul><li><em>Sufficiently defined</em> because tasks need some sort of clear outline of what‚Äôs required; ‚Äúwork on stuff‚Äù isn‚Äôt a task. More on this in a minute.</li><li><em>Complete</em> because a task needs to encompass all of the work required. The ‚Äúcut down tree‚Äù task isn‚Äôt complete if you‚Äôve only fetched the chainsaw.</li><li><em>Delivers change</em> because, in a work context, a task only ‚Äúmatters‚Äù if something is different because of the work.</li></ul><p>For the purposes of estimation, we need to go a little deeper into what ‚Äúdefined‚Äù means. In order for a task to be ‚Äúsufficiently defined‚Äù, it needs to have ‚Äúenough‚Äù detail ‚Äì but the definition of ‚Äúenough‚Äù is going to differ based on context. As I explored above, if it‚Äôs just me, a simple sketch is ‚Äúsufficiently defined‚Äù, but in other contexts we need more detail. Still, although there isn‚Äôt one globally-applicable definition of ‚Äúsufficiently defined‚Äù, I think there is a common-enough definition to be useful as a starting point.</p><h3 id="sufficiantly-defined">What is ‚Äúsufficiently defined‚Äù?</h3><p><strong>A task is sufficiently defined if the person working on the task<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> can answer ‚Äúyes‚Äù to these questions:</strong></p><ul><li>Do I understand what change is desired?</li><li>Do I understand what ‚Äúdone‚Äù will look like?</li><li>Can I define <em>all</em> the steps I would take to get to ‚Äúdone‚Äù? E.g., could I write down a TODO list for this task, and would it be complete?</li><li>Assuming no blockers or dependancies, do I have all the information I need to start this task right now?</li></ul><p>If the person working on the task, within the context of their workplace and their skillset, can‚Äôt answer ‚Äúyes‚Äù to all these questions, then the task isn‚Äôt sufficiently broken down, and you should repeat the breaking-down algorithm until you can answer ‚Äúyes‚Äù to these questions for all tasks on the list.</p><p>Now, there are going to be situations where you <em>can‚Äôt</em> decompose tasks further, or add more context, because of unknowns. Fixing bugs is often like this: you know what the change is (no more bug), you know what ‚Äúdone‚Äù looks like (bug is fixed), but you can‚Äôt know all the steps. There are techniques you can employ in these situations ‚Äì <a href="https://jacobian.org/2021/oct/20/simple-pm-tricks/#timeboxing">timeboxing</a> is my favorite ‚Äì but a deep discussion here is out of scope.</p><h2 id="this-is-a-skill-and-it-takes-practice">This is a skill, and it takes practice</h2><p>It‚Äôs important to point out at this point that this is a skill, and it takes practice to develop to the point where it‚Äôs easy. If you‚Äôre new to this kind of thinking, you‚Äôll probably find it doesn‚Äôt come particularly easy ‚Äì and that‚Äôs normal.</p><p>Take, for example, what I did above in <a href="#iteration-2">iteration 2</a>: how did I know, or how did I decide, that ‚Äúmodel data‚Äù was going to be the first step? Why wasn‚Äôt step 1 ‚Äúdesign the interface‚Äù, or ‚Äúwrite the streak calculation algorithm‚Äù, or anything else?</p><p>The answer is deeply unsatisfying to people new to this kind of work: it was largely intuitive. I know, for decades of experience writing software, that when I‚Äôm working on tools like this, things go better when I model data first. I also know that I‚Äôll be using Django, which has better affordances for a model-data-first workflow. There‚Äôs no algorithm here: I‚Äôm essentially pattern-matching this project against all the other projects I‚Äôve worked on or seen, and, based on that experience, intuiting the right steps in the right order. Which means, of course, if you <em>haven‚Äôt</em> seen a bunch of projects like this one, it can be incredibly difficult to know where to start!</p><p>There are no shortcuts here; the only way to learn is by doing. Which means, to circle all the way back to the discussion that inspired this piece: the best thing this manager can do to help her team get better here is give them safe opportunities to try it out. Ask for project plans, help them break them down, give feedback ‚Äì but don‚Äôt penalize them when they‚Äôre wrong. Because they‚Äôll probably be quite wrong, at first, but in a safe learning environment those mistakes become the data they‚Äôll pattern match against the next time, and get better and better.</p><h2 id="tldr">Summary: breaking down tasks, the algorithm</h2><ol><li><p>Begin where you are: with a list of tasks, a sketch, or even just an idea.</p></li><li><p>Think through the steps you‚Äôd need to take to accomplish that task, and write them down. Don‚Äôt worry about completeness or accuracy or depth, each pass just needs to expand, even slightly, on the previous one.</p></li><li><p>For each item on your list, decide if that item is <a href="#sufficiently-defined">sufficiently defined</a>:</p><ul><li>Do I understand what change is desired?</li><li>Do I understand what ‚Äúdone‚Äù will look like?</li><li>Can I define <em>all</em> the steps I would take to get to ‚Äúdone‚Äù?</li><li>Assuming no blockers or dependancies, do I have all the information I need to start this task right now?</li></ul><p>If the answer to any of these questions is ‚Äúno‚Äù, take that task and recurse - breaking it down further using this algorithm again.</p></li><li><p>Repeat until all tasks are sufficiently broken down.</p></li></ol><h2 id="howd-it-go">How‚Äôd it go?</h2><p>As usual, if you try this out and it works for you or doesn‚Äôt, I‚Äôd love to hear about it! <a href="https://jacobian.org/contact/">Get in touch</a>.</p><hr><h2 id="bonus-estimating-this-project">Bonus: estimating this project</h2><p>Because this a series on estimation, it seems reasonable to complete the work and produce an estimate for this project:</p><table><thead><tr><th>Task</th><th>Complexity</th><th>Uncertainty</th><th>Expected (days)</th><th>Worst-case (days)</th></tr></thead><tbody><tr><td>1. model data</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>2a. weekly view</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>2b. home page view</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>2c. monthly view</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>2d. browsing</td><td>small</td><td>low</td><td>1</td><td>1.1</td></tr><tr><td>3. dynamic week</td><td>small</td><td>low</td><td>1</td><td>1.1</td></tr><tr><td>4a. streak calculation</td><td>medium</td><td>moderate</td><td>3</td><td>4.5</td></tr><tr><td>4b. streak display</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>4c. streak recalculation</td><td>medium</td><td>low</td><td>3</td><td>3.3</td></tr><tr><td>5a. freeze accumulation</td><td>medium</td><td>moderate</td><td>3</td><td>4.5</td></tr><tr><td>5b. prevent double accumulation</td><td>small</td><td>extreme</td><td>1</td><td>5</td></tr><tr><td>5c. freeze spending</td><td>small</td><td>moderate</td><td>1</td><td>1.5</td></tr><tr><td></td><td></td><td><strong>Total</strong>:</td><td><strong>15.5 days</strong></td><td><strong>23.5 days</strong></td></tr></tbody></table><p>In reality, this overestimates the work somewhat; I completed this in about a dozen evenings and one long plane ride. But I also cut some serious corners ‚Äì the design is essentially non-existent ‚Äì and I‚Äôm fairly sure the ‚Äúfreeze‚Äù algorithm has some silly bugs I‚Äôll run into eventually.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI ‚Äì transformer debugger release (322 pts)]]></title>
            <link>https://github.com/openai/transformer-debugger</link>
            <guid>39675054</guid>
            <pubDate>Tue, 12 Mar 2024 01:12:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/openai/transformer-debugger">https://github.com/openai/transformer-debugger</a>, See on <a href="https://news.ycombinator.com/item?id=39675054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Transformer Debugger</h2><a id="user-content-transformer-debugger" aria-label="Permalink: Transformer Debugger" href="#transformer-debugger"></a></p>
<p dir="auto">Transformer Debugger (TDB) is a tool developed by OpenAI's <a href="https://openai.com/blog/introducing-superalignment" rel="nofollow">Superalignment
team</a> with the goal of
supporting investigations into specific behaviors of small language models. The tool combines
<a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models" rel="nofollow">automated interpretability</a>
techniques with <a href="https://transformer-circuits.pub/2023/monosemantic-features" rel="nofollow">sparse autoencoders</a>.</p>
<p dir="auto">TDB enables rapid exploration before needing to write code, with the ability to intervene in the
forward pass and see how it affects a particular behavior. It can be used to answer questions like,
"Why does the model output token A instead of token B for this prompt?" or "Why does attention head
H to attend to token T for this prompt?" It does so by identifying specific components (neurons,
attention heads, autoencoder latents) that contribute to the behavior, showing automatically
generated explanations of what causes those components to activate most strongly, and tracing
connections between components to help discover circuits.</p>
<p dir="auto">These videos give an overview of TDB and show how it can be used to investigate <a href="https://arxiv.org/abs/2211.00593" rel="nofollow">indirect object
identification in GPT-2 small</a>:</p>
<ul dir="auto">
<li><a href="https://www.loom.com/share/721244075f12439496db5d53439d2f84?sid=8445200e-c49e-4028-8b8e-3ea8d361dec0" rel="nofollow">Introduction</a></li>
<li><a href="https://www.loom.com/share/21b601b8494b40c49b8dc7bfd1dc6829?sid=ee23c00a-9ede-4249-b9d7-c2ba15993556" rel="nofollow">Neuron viewer pages</a></li>
<li><a href="https://www.loom.com/share/3478057cec484a1b85471585fef10811?sid=b9c3be4b-7117-405a-8d31-0f9e541dcfb6" rel="nofollow">Example: Investigating name mover heads</a></li>
<li><a href="https://www.loom.com/share/6bd8c6bde84b42a98f9a26a969d4a3ad?sid=4a09ac29-58a2-433e-b55d-762414d9a7fa" rel="nofollow">Example: Beyond name mover heads</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's in the release?</h2><a id="user-content-whats-in-the-release" aria-label="Permalink: What's in the release?" href="#whats-in-the-release"></a></p>
<ul dir="auto">
<li><a href="https://github.com/openai/transformer-debugger/blob/main/neuron_viewer/README.md">Neuron viewer</a>: A React app that hosts TDB as well as pages with information about individual model components (MLP neurons, attention heads and autoencoder latents for both).</li>
<li><a href="https://github.com/openai/transformer-debugger/blob/main/neuron_explainer/activation_server/README.md">Activation server</a>: A backend server that performs inference on a subject model to provide data for TDB. It also reads and serves data from public Azure buckets.</li>
<li><a href="https://github.com/openai/transformer-debugger/blob/main/neuron_explainer/models/README.md">Models</a>: A simple inference library for GPT-2 models and their autoencoders, with hooks to grab activations.</li>
<li><a href="https://github.com/openai/transformer-debugger/blob/main/datasets.md">Collated activation datasets</a>: top-activating dataset examples for MLP neurons, attention heads and autoencoder latents.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">Follow these steps to install the repo.  You'll first need python/pip, as well as node/npm.</p>
<p dir="auto">Though optional, we recommend you use a virtual environment or equivalent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# If you're already in a venv, deactivate it.
deactivate
# Create a new venv.
python -m venv ~/.virtualenvs/transformer-debugger
# Activate the new venv.
source ~/.virtualenvs/transformer-debugger/bin/activate"><pre><span><span>#</span> If you're already in a venv, deactivate it.</span>
deactivate
<span><span>#</span> Create a new venv.</span>
python -m venv <span>~</span>/.virtualenvs/transformer-debugger
<span><span>#</span> Activate the new venv.</span>
<span>source</span> <span>~</span>/.virtualenvs/transformer-debugger/bin/activate</pre></div>
<p dir="auto">Once your environment is set up, follow the following steps:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:openai/transformer-debugger.git
cd transformer-debugger

# Install neuron_explainer
pip install -e .

# Set up the pre-commit hooks.
pre-commit install

# Install neuron_viewer.
cd neuron_viewer
npm install
cd .."><pre>git clone git@github.com:openai/transformer-debugger.git
<span>cd</span> transformer-debugger

<span><span>#</span> Install neuron_explainer</span>
pip install -e <span>.</span>

<span><span>#</span> Set up the pre-commit hooks.</span>
pre-commit install

<span><span>#</span> Install neuron_viewer.</span>
<span>cd</span> neuron_viewer
npm install
<span>cd</span> ..</pre></div>
<p dir="auto">To run the TDB app, you'll then need to follow the instructions to set up the <a href="https://github.com/openai/transformer-debugger/blob/main/neuron_explainer/activation_server/README.md">activation server backend</a> and <a href="https://github.com/openai/transformer-debugger/blob/main/neuron_viewer/README.md">neuron viewer frontend</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Making changes</h2><a id="user-content-making-changes" aria-label="Permalink: Making changes" href="#making-changes"></a></p>
<p dir="auto">To validate changes:</p>
<ul dir="auto">
<li>Run <code>pytest</code></li>
<li>Run <code>mypy --config=mypy.ini .</code></li>
<li>Run activation server and neuron viewer and confirm that basic functionality like TDB and neuron
viewer pages is still working</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<ul dir="auto">
<li><a href="https://github.com/openai/transformer-debugger/blob/main/terminology.md">Terminology</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to cite</h2><a id="user-content-how-to-cite" aria-label="Permalink: How to cite" href="#how-to-cite"></a></p>
<p dir="auto">Please cite as:</p>
<div data-snippet-clipboard-copy-content="Mossing, et al., ‚ÄúTransformer Debugger‚Äù, GitHub, 2024."><pre><code>Mossing, et al., ‚ÄúTransformer Debugger‚Äù, GitHub, 2024.
</code></pre></div>
<p dir="auto">BibTex citation:</p>
<div data-snippet-clipboard-copy-content="@misc{mossing2024tdb,
  title={Transformer Debugger},
  author={Mossing, Dan and Bills, Steven and Tillman, Henk and Dupr√© la Tour, Tom and Cammarata, Nick and Gao, Leo and Achiam, Joshua and Yeh, Catherine and Leike, Jan and Wu, Jeff and Saunders, William},
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/openai/transformer-debugger}},
}"><pre><code>@misc{mossing2024tdb,
  title={Transformer Debugger},
  author={Mossing, Dan and Bills, Steven and Tillman, Henk and Dupr√© la Tour, Tom and Cammarata, Nick and Gao, Leo and Achiam, Joshua and Yeh, Catherine and Leike, Jan and Wu, Jeff and Saunders, William},
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/openai/transformer-debugger}},
}
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Key Boeing Whistleblower Found Dead from Apparent Suicide (160 pts)]]></title>
            <link>https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/</link>
            <guid>39674829</guid>
            <pubDate>Tue, 12 Mar 2024 00:41:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/">https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/</a>, See on <a href="https://news.ycombinator.com/item?id=39674829">Hacker News</a></p>
Couldn't get https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Create More, Consume Less (2021) (144 pts)]]></title>
            <link>https://www.omaritani.com/blog/create-more-consume-less</link>
            <guid>39674619</guid>
            <pubDate>Tue, 12 Mar 2024 00:04:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.omaritani.com/blog/create-more-consume-less">https://www.omaritani.com/blog/create-more-consume-less</a>, See on <a href="https://news.ycombinator.com/item?id=39674619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="block-c771f4d65dbb1013d3af">
  <p>When the global pandemic was first announced, it was a bewildering feeling. It was scary, and yet, somehow thrilling‚Äîa novel experience, to say the least.</p><p>So throughout the rest of the year‚Äîand setting aside traumas and deaths‚Äîwe got to enjoy the slower pace of remote living. But now, almost a year later, the mental, emotional, and physical toll has surfaced. We‚Äôve absorbed the knocks, and now, we‚Äôre just plain and simply, tired. </p><p>When the new year kicked in, I thought that taking a break from my creative writing to ‚Äúlive a little more‚Äù would energize me‚Äîit didn‚Äôt. I felt lethargic instead. I felt a void inside myself that was snowballing by the day.</p><p>If you find yourself growing more weary and anxious about the future, <a href="https://www.tandfonline.com/doi/full/10.1080/07421656.2016.1166832?journalCode=uart20#.V2GKm-YrI6g" target="_blank">scientific research</a> indicates that regardless of your artistic experience or talent, just a 45-minute session of focused creative activity significantly lowers the cortisol levels in your body, and thus, reduces your overall stress.</p><p>In other words, a small dose of daily creativity is good for us. It can help us become more present, less anxious, and much more fulfilled. And as I continue to learn more about the mental health benefits of creativity, I realize now how the simple habit of writing every day has boosted my self-esteem, given me a sense of purpose, and formed a buffer for emotional release. </p><h2>The Problem with Too Much Consumption</h2><p>In this 21st century, there‚Äôs so much you could do to fill all the hours in your day. You can binge-watch anything on demand. You can order a bunch of stuff and have them arrive at your doorstep on the same day. You can spend countless hours scrolling on your phone, at the mercy of an algorithm, and you‚Äôd only scratch less than 1% of the social entertainment that‚Äôs at your disposal. </p><p>Extrapolate that across the next week, month, and years, and you begin to see why some people can fall into the trap of endless consumption and inactivity.  </p><p>Look, life is meant to be lived and enjoyed, but that‚Äôs not the ultimate purpose of life. What‚Äôs the purpose? Your purpose is just to be alive and do what good you can with it. Your purpose is to keep growing and evolving. It‚Äôs as simple as that.</p><p>I think Ralph Waldo Emerson captured it well:</p><blockquote><p><em>‚ÄúThe purpose of life is not to be happy. It is to be useful, to be honorable, to be compassionate, to have it make some difference that you have lived and lived well.‚Äù</em></p></blockquote><p>As you make yourself useful by enjoying the process of adding more arsenal to your realm of knowledge and skills, so that you can put them to good use and contribute threads to the fabric of society, happiness simply becomes a byproduct of that journey. <a href="https://medium.com/mind-cafe/10-daily-habits-of-truly-happy-people-9efd3d8bd2eb?sk=3e4e69b02620d4bcf0a6e7b6e5ea8216&amp;source=friends_link" target="_blank"><em>Happiness becomes the way.</em></a></p><p>The problem with too much consumption, however, is that it can leave us feeling utterly empty inside. Ironically, it creates something:<em> One big void. </em></p><p>You start to feel stuck, not knowing what to do with your life. You lose agency over your habits, you lose control over your emotions, and eventually, this loss in energy translates into zero productivity.</p><p><strong>Too much consumption leads to a life of escapism. </strong>Instead of learning how to sit with your emotions or exploring what‚Äôs inside of you and self-expressing yourself through art, you seek an escape that would make you forget.</p><p>Another problem with consumption is that it can give you a false illusion that you‚Äôre doing something. Spending hours watching fitness videos will not help you become fit. You‚Äôve got to get up and move to do so. Spending hours watching cooking shows won‚Äôt make you a better cook‚Äîcooking will.  </p><p>With that in mind, you‚Äôve got to make a decision: <em>Do you want to define your life by self-expressive creation or mindless consumption? By selflessly giving or selfishly taking? </em>As Martin Luther King Jr. once said: ‚ÄúEvery man must decide whether he will walk in the light of creative altruism or in the darkness of destructive selfishness.‚Äù </p><h2>Creativity is Your Duty, But it Can Also Be a Therapeutic Tool</h2><p>Art is the most beautiful form of human expression. Whether it be through prose, science, technology, innovation, visual arts, or illustration, the sheer act of living in a state of wonder and creation is what makes us human. </p><p>When we create, we enter a state of flow, and flow is what psychologist Mih√°ly Cs√≠kszentmih√°ly defines as a state of complete immersion in an activity for its own sake. In his book, <em>Flow: The psychology of optimal experience, </em>he writes:</p><blockquote><p><em>‚ÄúFlow is a state in which people are so involved in an activity that nothing else seems to matter; the experience is so enjoyable that people will continue to do it even at great cost, for the sheer sake of doing it.‚Äù</em></p></blockquote><p>There‚Äôs a distinct word here that I should highlight. Notice how he says enjoyable and not pleasurable because they‚Äôre two different things.</p><p>Activities like eating, drinking, sleeping, and watching entertainment, are examples of pleasurable experiences. They‚Äôre passive and transient in nature. Enjoyable experiences, on the other hand, are active. Think of sports, writing, and art‚Äîthey demand you to show up to it and be fully immersed in it. </p><p>In other words: <strong><em>Pleasurable experiences are rooted in consumption. Enjoyable experiences are rooted in creation.</em></strong></p><p>And this relates to what Rabindranath Tagore, the Bengali poet, philosopher, and the first non-European to win the Nobel Prize, wrote somewhere between the 19th and 20th century:</p><blockquote><p><em>‚ÄúI slept and dreamt that life was joy.&nbsp;<br>I awoke and saw that life was duty.&nbsp;<br>I worked‚Äîand behold, duty was joy.‚Äù</em></p></blockquote><p><em>Work is duty and ‚Äúduty is joy.‚Äù </em></p><p>That‚Äôs one great mantra to live by. Duty is not pleasure; duty is joy. <em>Duty is not consumption; duty is creation. </em>And since creativity is a form of work and work is a duty, then creativity is a duty‚Äîa joy. </p><p>When I sit down on my chair to write, I face a lot of resistance, but I insist on carrying myself through because the simple practice of plucking thoughts from the garden of my mind and glazing them in words gives me a sense of peace and calm. It arms me with joy. Hours flash by in minutes. The focus drifts me away from my worries of the future and returns me to the present. </p><p>And after I walk away from my desk, the knowledge that I just painted an entire white page with black ink‚Äî<em>that I‚Äôve done something good today</em>‚Äîgraces me with the freedom to feel fulfilled. </p><p>Writing is my art of choice, and it helps me make sense of my life, but <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2804629/" target="_blank">as this published study</a> on the connection between art and healing demonstrates, any form of art can help you improve your mental and emotional wellbeing. </p><p>When you focus on one activity and become fully absorbed by it, your heart rate slows and your breath deepens. This immersive nature of being creative helps you control what thoughts you pay attention to. In a way, creative work transforms into a form of meditation.</p><p>But what makes it all worthwhile? The sense of accomplishment you feel by the end of it. Even if no one else checks out your work, it‚Äôll still help you see yourself in a new light and earn your own appreciation. The artistic process of self-express helps you understand yourself better. And if you ask me, that‚Äôs the single most potent benefit of having your own creative outlet.</p><p>In fact, that‚Äôs why in the field of psychotherapy, they utilize the creative process to help people explore self-expression and, in doing so, find new ways to gain personal insight and develop more robust coping skills. It‚Äôs called <a href="https://www.verywellmind.com/what-is-art-therapy-2795755" target="_blank">art therapy</a>. </p><h2>How to Tip The Scale Toward Creation</h2><p>In her book, Big Magic, Elizabeth Gilbert writes that ‚Äúa creative life is an amplified life. It‚Äôs a bigger life, a happier life, an expanded life, and a hell of a lot more interesting life. Living in this manner‚Äîcontinually and stubbornly bringing forth the jewels that are hidden within you‚Äîis a fine art, in and of itself.‚Äù I love that, and I couldn‚Äôt agree more.</p><p>Truth is, it doesn't take much to live a creative life. You don‚Äôt need to quit your job. You don‚Äôt need to move to a new place to feel inspired. You don‚Äôt need to sign-up for a $1,000 course to learn how to to make art.</p><p><em>You just need to start. </em></p><p><em>And you just need to tip the scale toward creation.</em></p><p>Here‚Äôs a simple 3-step strategy to help you get started:</p><ol data-rte-list="default"><li><p><strong>Choose something you enjoy. </strong>Not something that gives you pleasure‚Äîsomething you enjoy. Something that pulls you in. Whether it be writing, drawing, designing, knitting, or baking, you‚Äôre only able to enter a state of flow when you do something for its own sake rather than an extrinsic reward. When the work <strong><em>is</em></strong> the reward‚Äîthat‚Äôs what it means to enjoy it. </p></li><li><p><strong>Block time for it, show up to it and say ‚Äúno‚Äù to protect it. </strong>Set aside an<strong> </strong>hour a day for creativity. Switch off your phone. Create an interruption-free space. Say no to whatever or whoever pulls you in another direction. And if you can‚Äôt commit to a daily practice, can you do it twice per week?</p></li><li><p><strong>Leave your judgment at the door and replace it with self-compassion. </strong>Seriously. Remember that growth is a process. Good things take time to bloom. What keeps you growing, however, is your ability to suspend your critical judgment, honor your creative work, and be compassionate with yourself. <a href="https://www.omaritani.com/blog/capability-and-mindset" target="_blank">You‚Äôre more capable than you think</a>.</p></li></ol><h2>A New Mantra to Live By</h2><p>Scottish novelist Robert Louis Stevenson once these words:</p><blockquote><p><em>"Don't judge each day by the harvest you reap but by the seeds that you plant." </em></p></blockquote><p>In other words, don‚Äôt judge each day by what you consume but by what you create. <em>Create more, consume less. </em>That‚Äôs a good mantra to live by. </p><p>I won't judge each day by what I consume, but by what I create.</p><p>I think you should too.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Roku data breach: Over 15k accounts affected (111 pts)]]></title>
            <link>https://www.claimdepot.com/data-breach/roku</link>
            <guid>39674041</guid>
            <pubDate>Mon, 11 Mar 2024 22:41:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.claimdepot.com/data-breach/roku">https://www.claimdepot.com/data-breach/roku</a>, See on <a href="https://news.ycombinator.com/item?id=39674041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="w-node-_92fa5ae9-ab49-2ea3-a7d5-292e19eba4d7-0e0c48f6"><h2>Understanding the Roku Data Breach</h2>
<p>Roku, Inc., the leader in streaming television platforms in the U.S., recently faced a data breach that may have affected your account. As a company that values viewer privacy and security, Roku is actively addressing the situation and has provided detailed information to assist affected users.</p>
<p>Between December 28, 2023, and February 21, 2024, unauthorized parties accessed certain individual Roku accounts. The breach was discovered during this period, and the company promptly initiated an investigation. It was found that usernames, passwords, and account login information were compromised, potentially affecting 15,363 individuals in the United States, including 76 in the state of Maine.</p>
<p>Roku has reported the incident to the Attorney General's offices, including the state of California. For further details, you can review the disclosures on the <a href="https://apps.web.maine.gov/online/aeviewer/ME/40/e9cc298b-379b-47ba-a10d-e2263963b574.shtml">Maine Attorney General's website</a> and the <a href="https://oag.ca.gov/ecrime/databreach/reports/sb24-582208">California Attorney General's website</a>.</p>
<h2>Immediate Steps Taken by Roku</h2>
<p>Upon detecting the breach, Roku took swift action to secure the accounts from further unauthorized access. This included resetting passwords for potentially impacted accounts, investigating account activity, canceling unauthorized subscriptions, and issuing refunds for any unauthorized charges. Roku has also been actively monitoring for any additional suspicious activity to ensure the security of customer data.</p>
<h2>What You Should Do</h2>
<p>If you suspect your account may have been affected, it is crucial to take the following steps:</p>
<ul>
<li>Reset your Roku account password by visiting <a href="https://my.roku.com/">my.roku.com</a> and using the "Forgot password?" option.</li>
<li>Review the subscriptions and devices linked to your Roku account from your account dashboard.</li>
<li>Use strong, unique passwords for all your online accounts.</li>
<li>Monitor your account statements, credit reports, and other online account information for any suspicious activity.</li>
</ul>
<p>Roku has provided a <strong>Notice to Consumers</strong> on March 8, 2024, via written communication, outlining the steps affected users should take to protect their information. This notice includes detailed instructions and resources to help you safeguard your identity and personal data.</p>
<h2>Additional Resources and Support</h2>
<p>For more information on identity theft protection and credit monitoring services, you can refer to the <strong>Information about Identity Theft Protection</strong> included in the Notice to Consumers. Roku has also set up a dedicated phone line at 1-816-272-8106 and an email address at account-help@roku.com for any questions or concerns you may have regarding the breach.</p>
<p>Roku sincerely regrets the inconvenience caused by this incident and is committed to maintaining the privacy and security of your information. As we navigate the aftermath of this breach, Roku will continue to update affected users and take any additional steps necessary to prevent future incidents.</p>
<p>For further guidance, you can also contact the Federal Trade Commission or your state Attorney General's office for assistance with identity theft and fraud prevention. Remember, staying informed and vigilant is key to protecting your personal information in the digital age.</p></div><div><p>Weekly newsletter</p><p>Stay up to date with new class action settlements you may join.</p><div><div><p>Thank you! Your submission has been received!</p></div><div><p>Oops! Something went wrong while submitting the form.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Simpson's paradox (314 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Simpson%27s_paradox</link>
            <guid>39673754</guid>
            <pubDate>Mon, 11 Mar 2024 22:02:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">https://en.wikipedia.org/wiki/Simpson%27s_paradox</a>, See on <a href="https://news.ycombinator.com/item?id=39673754">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Simpson%27s_paradox_continuous.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Simpson%27s_paradox_continuous.svg/220px-Simpson%27s_paradox_continuous.svg.png" decoding="async" width="220" height="147" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Simpson%27s_paradox_continuous.svg/330px-Simpson%27s_paradox_continuous.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Simpson%27s_paradox_continuous.svg/440px-Simpson%27s_paradox_continuous.svg.png 2x" data-file-width="390" data-file-height="260"></a><figcaption>Simpson's paradox for quantitative data: a positive trend (<span>&nbsp;</span>, <span>&nbsp;</span>) appears for two separate groups, whereas a negative trend (<span>&nbsp;</span>) appears when the groups are combined.</figcaption></figure>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Simpsons_paradox_-_animation.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Simpsons_paradox_-_animation.gif/220px-Simpsons_paradox_-_animation.gif" decoding="async" width="220" height="157" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Simpsons_paradox_-_animation.gif/330px-Simpsons_paradox_-_animation.gif 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Simpsons_paradox_-_animation.gif/440px-Simpsons_paradox_-_animation.gif 2x" data-file-width="700" data-file-height="500"></a><figcaption>Visualization of Simpson's paradox on data resembling real-world variability indicates that risk of misjudgment of true causal relationship can be hard to spot.</figcaption></figure>
<p><b>Simpson's paradox</b> is a phenomenon in <a href="https://en.wikipedia.org/wiki/Probability" title="Probability">probability</a> and <a href="https://en.wikipedia.org/wiki/Statistics" title="Statistics">statistics</a> in which a trend appears in several groups of data but disappears or reverses when the groups are combined. This result is often encountered in social-science and medical-science statistics,<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup><sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup><sup id="cite_ref-VogelFranks2017_3-0"><a href="#cite_note-VogelFranks2017-3">[3]</a></sup> and is particularly problematic when frequency data are unduly given <a href="https://en.wikipedia.org/wiki/Causal" title="Causal">causal</a> interpretations.<sup id="cite_ref-pearl_4-0"><a href="#cite_note-pearl-4">[4]</a></sup> The paradox can be resolved when <a href="https://en.wikipedia.org/wiki/Confounding_variable" title="Confounding variable">confounding variables</a> and causal relations are appropriately addressed in the statistical modeling<sup id="cite_ref-pearl_4-1"><a href="#cite_note-pearl-4">[4]</a></sup><sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> (e.g., through <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a><sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup>).
</p><p>Simpson's paradox has been used to illustrate the kind of misleading results that the <a href="https://en.wikipedia.org/wiki/Misuse_of_statistics" title="Misuse of statistics">misuse of statistics</a> can generate.<sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup><sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>
</p><p><a href="https://en.wikipedia.org/wiki/Edward_H._Simpson" title="Edward H. Simpson">Edward H. Simpson</a> first described this phenomenon in a technical paper in 1951,<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> but the statisticians <a href="https://en.wikipedia.org/wiki/Karl_Pearson" title="Karl Pearson">Karl Pearson</a> (in 1899<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>) and <a href="https://en.wikipedia.org/wiki/Udny_Yule" title="Udny Yule">Udny Yule</a> (in 1903<sup id="cite_ref-yule_11-0"><a href="#cite_note-yule-11">[11]</a></sup>) had mentioned similar effects earlier. The name <i>Simpson's paradox</i> was introduced by Colin R. Blyth in 1972.<sup id="cite_ref-blyth-72_12-0"><a href="#cite_note-blyth-72-12">[12]</a></sup> It is also referred to as <b>Simpson's reversal</b>, the <b>Yule‚ÄìSimpson effect</b>, the <b>amalgamation paradox</b>, or the <b>reversal paradox</b>.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>
</p><p>Mathematician <a href="https://en.wikipedia.org/wiki/Jordan_Ellenberg" title="Jordan Ellenberg">Jordan Ellenberg</a> argues that Simpson's paradox is misnamed as "there's no contradiction involved, just two different ways to think about the same data" and suggests that its lesson "isn't really to tell us which viewpoint to take but to insist that we keep both the parts and the whole in mind at once."<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Examples">Examples</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=1" title="Edit section: Examples"><span>edit</span></a><span>]</span></span></h2>
<h3><span id="UC_Berkeley_gender_bias">UC Berkeley gender bias</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=2" title="Edit section: UC Berkeley gender bias"><span>edit</span></a><span>]</span></span></h3>
<p>One of the best-known examples of Simpson's paradox comes from a study of gender bias among graduate school admissions to <a href="https://en.wikipedia.org/wiki/University_of_California,_Berkeley" title="University of California, Berkeley">University of California, Berkeley</a>. The admission figures for the fall of 1973 showed that men applying were more likely than women to be admitted, and the difference was so large that it was unlikely to be due to chance.<sup id="cite_ref-freedman_15-0"><a href="#cite_note-freedman-15">[15]</a></sup><sup id="cite_ref-Bickel_16-0"><a href="#cite_note-Bickel-16">[16]</a></sup>
</p>
<table>

<tbody><tr>
<th rowspan="2">
</th>
<th colspan="2">All
</th>
<th colspan="2">Men
</th>
<th colspan="2">Women
</th></tr>
<tr>
<th>Applicants
</th>
<th>Admitted
</th>
<th>Applicants
</th>
<th>Admitted
</th>
<th>Applicants
</th>
<th>Admitted
</th></tr>
<tr>
<th>Total
</th>
<td>12,763
</td>
<td>41%
</td>
<td>8,442
</td>
<td>44%
</td>
<td>4,321
</td>
<td>35%
</td></tr></tbody></table>
<p>However, when taking into account the information about departments being applied to, the different rejection percentages reveal the different difficulty of getting into the department, and at the same time it showed that women tended to apply to more competitive departments with lower rates of admission, even among qualified applicants (such as in the English department), whereas men tended to apply to less competitive departments with higher rates of admission (such as in the engineering department). The pooled and corrected data showed a "small but statistically significant bias in favor of women".<sup id="cite_ref-Bickel_16-1"><a href="#cite_note-Bickel-16">[16]</a></sup>
</p><p>The data from the six largest departments are listed below:
</p>
<table>

<tbody><tr>
<th rowspan="2">Department
</th>
<th colspan="2">All
</th>
<th colspan="2">Men
</th>
<th colspan="2">Women
</th></tr>
<tr>
<th>Applicants
</th>
<th>Admitted
</th>
<th>Applicants
</th>
<th>Admitted
</th>
<th>Applicants
</th>
<th>Admitted
</th></tr>
<tr>
<th>A
</th>
<td>933
</td>
<td>64%
</td>
<td><b>825</b>
</td>
<td>62%
</td>
<td>108
</td>
<td>82%
</td></tr>
<tr>
<th>B
</th>
<td>585
</td>
<td>63%
</td>
<td><b>560</b>
</td>
<td>63%
</td>
<td>25
</td>
<td>68%
</td></tr>
<tr>
<th>C
</th>
<td>918
</td>
<td>35%
</td>
<td>325
</td>
<td>37%
</td>
<td><b>593</b>
</td>
<td>34%
</td></tr>
<tr>
<th>D
</th>
<td>792
</td>
<td>34%
</td>
<td>417
</td>
<td>33%
</td>
<td>375
</td>
<td>35%
</td></tr>
<tr>
<th>E
</th>
<td>584
</td>
<td>25%
</td>
<td>191
</td>
<td>28%
</td>
<td><b>393</b>
</td>
<td>24%
</td></tr>
<tr>
<th>F
</th>
<td>714
</td>
<td>6%
</td>
<td>373
</td>
<td>6%
</td>
<td>341
</td>
<td>7%
</td></tr>
<tr>
<th>Total
</th>
<th>4526
</th>
<th>39%
</th>
<th>2691
</th>
<th>45%
</th>
<th>1835
</th>
<th>30%
</th></tr>
<tr>
<td colspan="7">
<p>Legend:<br>
</p>
<p><span>&nbsp;</span>&nbsp;greater percentage of successful applicants than the other gender</p>
<p><span>&nbsp;</span>&nbsp;greater number of applicants than the other gender</p>
<p><b>bold</b> - the two 'most applied for' departments for each gender
</p>
</td></tr></tbody></table>
<p>The entire data showed total of 4 out of 85 departments to be significantly biased against women, while 6 to be significantly biased against men (not all present in the 'six largest departments' table above). Notably, the numbers of biased departments were not the basis for the conclusion, but rather it was the gender admissions pooled across all departments, while weighing by each department's rejection rate across all of its applicants.<sup id="cite_ref-Bickel_16-2"><a href="#cite_note-Bickel-16">[16]</a></sup>
</p>
<h3><span id="Kidney_stone_treatment">Kidney stone treatment</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=3" title="Edit section: Kidney stone treatment"><span>edit</span></a><span>]</span></span></h3>
<p>Another example comes from a real-life medical study<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup> comparing the success rates of two treatments for <a href="https://en.wikipedia.org/wiki/Kidney_stone" title="Kidney stone">kidney stones</a>.<sup id="cite_ref-KidneyParadox_18-0"><a href="#cite_note-KidneyParadox-18">[18]</a></sup> The table below shows the success rates (the term <i>success rate</i> here actually means the success proportion) and numbers of treatments for treatments involving both small and large kidney stones, where Treatment A includes open surgical procedures and Treatment B includes closed surgical procedures. The numbers in parentheses indicate the number of success cases over the total size of the group.
</p>
<table summary="results accounting for stone size">
<tbody><tr>
<th><p>Treatment</p><p>Stone size &nbsp; &nbsp;</p>
</th>
<th>Treatment A
</th>
<th>Treatment B
</th></tr>
<tr>
<th>Small stones
</th>
<td><i>Group 1</i><br><b>93% (81/87)</b></td>
<td><i>Group 2</i><br>87% (234/270)
</td></tr>
<tr>
<th>Large stones
</th>
<td><i>Group 3</i><br><b>73% (192/263)</b></td>
<td><i>Group 4</i><br>69% (55/80)
</td></tr>
<tr>
<th>Both
</th>
<td>78% (273/350)</td>
<td><b>83% (289/350)</b>
</td></tr></tbody></table>
<p>The paradoxical conclusion is that treatment A is more effective when used on small stones, and also when used on large stones, yet treatment B appears to be more effective when considering both sizes at the same time. In this example, the "lurking" variable (or <a href="https://en.wikipedia.org/wiki/Confounding" title="Confounding">confounding variable</a>) causing the paradox is the size of the stones, which was not previously known to researchers to be important until its effects were included.
</p><p>Which treatment is considered better is determined by which success ratio (successes/total) is larger. The reversal of the inequality between the two ratios when considering the combined data, which creates Simpson's paradox, happens because two effects occur together:
</p>
<ol><li>The sizes of the groups, which are combined when the lurking variable is ignored, are very different. Doctors tend to give cases with large stones the better treatment A, and the cases with small stones the inferior treatment B. Therefore, the totals are dominated by groups 3 and 2, and not by the two much smaller groups 1 and 4.</li>
<li>The lurking variable, stone size, has a large effect on the ratios; i.e., the success rate is more strongly influenced by the severity of the case than by the choice of treatment. Therefore, the group of patients with large stones using treatment A (group 3) does worse than the group with small stones, even if the latter used the inferior treatment B (group 2).</li></ol>
<p>Based on these effects, the paradoxical result is seen to arise because the effect of the size of the stones overwhelms the benefits of the better treatment (A). In short, the less effective treatment B appeared to be more effective because it was applied more frequently to the small stones cases, which were easier to treat.<sup id="cite_ref-KidneyParadox_18-1"><a href="#cite_note-KidneyParadox-18">[18]</a></sup>
</p>
<h3><span id="Batting_averages">Batting averages</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=4" title="Edit section: Batting averages"><span>edit</span></a><span>]</span></span></h3>
<p>A common example of Simpson's paradox involves the <a href="https://en.wikipedia.org/wiki/Batting_average_(baseball)" title="Batting average (baseball)">batting averages</a> of players in <a href="https://en.wikipedia.org/wiki/Professional_baseball" title="Professional baseball">professional baseball</a>. It is possible for one player to have a higher batting average than another player each year for a number of years, but to have a lower batting average across all of those years. This phenomenon can occur when there are large differences in the number of <a href="https://en.wikipedia.org/wiki/At_bat" title="At bat">at bats</a> between the years. Mathematician <a href="https://en.wikipedia.org/wiki/Kenneth_A._Ross" title="Kenneth A. Ross">Ken Ross</a> demonstrated this using the batting average of two baseball players, <a href="https://en.wikipedia.org/wiki/Derek_Jeter" title="Derek Jeter">Derek Jeter</a> and <a href="https://en.wikipedia.org/wiki/David_Justice" title="David Justice">David Justice</a>, during the years 1995 and 1996:<sup id="cite_ref-RossBaseball_19-0"><a href="#cite_note-RossBaseball-19">[19]</a></sup><sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup>
</p>
<table>

<tbody><tr>
<th><p>Year</p><p>Batter &nbsp;</p>
</th>
<th colspan="2">1995
</th>
<th colspan="2">1996
</th>
<th colspan="2">Combined
</th></tr>
<tr>
<td>Derek Jeter
</td>
<td>12/48
</td>
<td>.250
</td>
<td>183/582
</td>
<td>.314
</td>
<td>195/630
</td>
<td><b>.310</b>
</td></tr>
<tr>
<td>David Justice
</td>
<td>104/411
</td>
<td><b>.253</b>
</td>
<td>45/140
</td>
<td><b>.321</b>
</td>
<td>149/551
</td>
<td>.270
</td></tr></tbody></table>
<p>In both 1995 and 1996, Justice had a higher batting average (in bold type) than Jeter did. However, when the two baseball seasons are combined, Jeter shows a higher batting average than Justice. According to Ross, this phenomenon would be observed about once per year among the possible pairs of players.<sup id="cite_ref-RossBaseball_19-1"><a href="#cite_note-RossBaseball-19">[19]</a></sup>
</p>
<h2><span id="Vector_interpretation">Vector interpretation</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=5" title="Edit section: Vector interpretation"><span>edit</span></a><span>]</span></span></h2>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Simpson_paradox_vectors.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Simpson_paradox_vectors.svg/220px-Simpson_paradox_vectors.svg.png" decoding="async" width="220" height="122" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Simpson_paradox_vectors.svg/330px-Simpson_paradox_vectors.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Simpson_paradox_vectors.svg/440px-Simpson_paradox_vectors.svg.png 2x" data-file-width="512" data-file-height="284"></a><figcaption>Vector interpretation of Simpson's paradox</figcaption></figure>
<p>Simpson's paradox can also be illustrated using a 2-dimensional <a href="https://en.wikipedia.org/wiki/Vector_space" title="Vector space">vector space</a>.<sup id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> A success rate of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c397ad17b7e3f7fc729917de5e232d377001a4aa" aria-hidden="true" alt="{\textstyle {\frac {p}{q}}}"></span> (i.e., <i>successes/attempts</i>) can be represented by a <a href="https://en.wikipedia.org/wiki/Vector_(geometry)" title="Vector (geometry)">vector</a> <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a90e0a52a4344e502e3594adb30fe8495f3e723f" aria-hidden="true" alt="{\displaystyle {\vec {A}}=(q,p)}"></span>, with a <a href="https://en.wikipedia.org/wiki/Slope" title="Slope">slope</a> of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c397ad17b7e3f7fc729917de5e232d377001a4aa" aria-hidden="true" alt="{\textstyle {\frac {p}{q}}}"></span>. A steeper vector then represents a greater success rate. If two rates <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d44f68c00f83caa6c0d1d0e3ee4625f2ee69f236" aria-hidden="true" alt="{\textstyle {\frac {p_{1}}{q_{1}}}}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9c3442f046526c1ba4a01215f00bcc0a9d989a59" aria-hidden="true" alt="{\textstyle {\frac {p_{2}}{q_{2}}}}"></span> are combined, as in the examples given above, the result can be represented by the sum of the vectors <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/45c51e78a7807e640a7812b0aed132c44ed8efe9" aria-hidden="true" alt="{\displaystyle (q_{1},p_{1})}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2d9cc6f49f6d44ff6259c71eb1c354e5de82d044" aria-hidden="true" alt="{\displaystyle (q_{2},p_{2})}"></span>, which according to the <a href="https://en.wikipedia.org/wiki/Parallelogram_rule" title="Parallelogram rule">parallelogram rule</a> is the vector <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d038de36b188adf784455b6d1b2e2cd302cfdbee" aria-hidden="true" alt="{\displaystyle (q_{1}+q_{2},p_{1}+p_{2})}"></span>, with slope <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3372cef291da5115a42ab7b218b74f13fe93169" aria-hidden="true" alt="{\textstyle {\frac {p_{1}+p_{2}}{q_{1}+q_{2}}}}"></span>.
</p><p>Simpson's paradox says that even if a vector <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ed9036854c76a8301527b47d95c46ef5b1557fdb" aria-hidden="true" alt="{\displaystyle {\vec {L}}_{1}}"></span> (in orange in figure) has a smaller slope than another vector <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cbee09705bc97f67527b36a7596d3102cc8ab5f8" aria-hidden="true" alt="{\displaystyle {\vec {B}}_{1}}"></span> (in blue), and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b0777f2814eff5a60f939802651004eb08fbed12" aria-hidden="true" alt="{\displaystyle {\vec {L}}_{2}}"></span> has a smaller slope than <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7e12b9fa9d99623b6eb14d2840b5cd9e276659d0" aria-hidden="true" alt="{\displaystyle {\vec {B}}_{2}}"></span>, the sum of the two vectors <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/35a5f178db4693ecf5ca4c0dd1781fa8adf99a37" aria-hidden="true" alt="{\displaystyle {\vec {L}}_{1}+{\vec {L}}_{2}}"></span> can potentially still have a larger slope than the sum of the two vectors <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f06a5e904687d727d687f74367b2319c71c119c" aria-hidden="true" alt="{\displaystyle {\vec {B}}_{1}+{\vec {B}}_{2}}"></span>, as shown in the example.  For this to occur one of the orange vectors must have a greater slope than one of the blue vectors (here <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b0777f2814eff5a60f939802651004eb08fbed12" aria-hidden="true" alt="{\displaystyle {\vec {L}}_{2}}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cbee09705bc97f67527b36a7596d3102cc8ab5f8" aria-hidden="true" alt="{\displaystyle {\vec {B}}_{1}}"></span>), and these will generally be longer than the alternatively subscripted vectors&nbsp;‚Äì thereby dominating the overall comparison.
</p>
<h2><span id="Correlation_between_variables">Correlation between variables</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=6" title="Edit section: Correlation between variables"><span>edit</span></a><span>]</span></span></h2>
<p>Simpson's reversal can also arise in <a href="https://en.wikipedia.org/wiki/Correlation" title="Correlation">correlations</a>,  in which two variables appear to have (say) a positive correlation towards one another, when in fact they have a negative correlation, the reversal having been brought about by a "lurking" confounder.  Berman et al.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup> give an example from economics, where a dataset suggests overall demand is positively correlated with price (that is, higher prices lead to <i>more</i> demand), in contradiction of expectation.   Analysis reveals time to be the confounding variable: plotting both price and demand against time reveals the expected negative correlation over various periods, which then reverses to become positive if the influence of time is ignored by simply plotting demand against price.
</p>
<h2><span id="Psychology">Psychology</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=7" title="Edit section: Psychology"><span>edit</span></a><span>]</span></span></h2>
<p>Psychological interest in Simpson's paradox seeks to explain why people deem sign reversal to be impossible at first, offended by the idea that an action preferred both under one condition and under its negation should be rejected when the condition is unknown. The question is where people get this strong <a href="https://en.wikipedia.org/wiki/Intuition" title="Intuition">intuition</a> from, and how it is encoded in the <a href="https://en.wikipedia.org/wiki/Mind" title="Mind">mind</a>.
</p><p>Simpson's paradox demonstrates that this intuition cannot be derived from either <a href="https://en.wikipedia.org/wiki/Classical_logic" title="Classical logic">classical logic</a> or <a href="https://en.wikipedia.org/wiki/Probability_calculus" title="Probability calculus">probability calculus</a> alone, and thus led <a href="https://en.wikipedia.org/wiki/Philosopher" title="Philosopher">philosophers</a> to speculate that it is supported by an innate causal logic that guides people in reasoning about actions and their consequences.<sup id="cite_ref-pearl_4-2"><a href="#cite_note-pearl-4">[4]</a></sup> Savage's <a href="https://en.wikipedia.org/wiki/Sure-thing_principle" title="Sure-thing principle">sure-thing principle</a><sup id="cite_ref-blyth-72_12-1"><a href="#cite_note-blyth-72-12">[12]</a></sup> is an example of what such logic may entail. A qualified version of Savage's sure thing principle can indeed be derived from Pearl's <i>do</i>-calculus<sup id="cite_ref-pearl_4-3"><a href="#cite_note-pearl-4">[4]</a></sup> and reads: "An action <i>A</i> that increases the probability of an event <i>B</i> in each subpopulation <i>C<sub>i</sub></i> of <i>C</i> must also increase the probability of <i>B</i> in the population as a whole, provided that the action does not change the distribution of the subpopulations." This suggests that knowledge about actions and consequences is stored in a form resembling Causal <a href="https://en.wikipedia.org/wiki/Bayesian_Networks" title="Bayesian Networks">Bayesian Networks</a>.
</p>
<h2><span id="Probability">Probability</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=8" title="Edit section: Probability"><span>edit</span></a><span>]</span></span></h2>
<p>A paper by Pavlides and Perlman presents a proof, due to Hadjicostas, that in a random 2 √ó 2 √ó 2 table with uniform distribution, Simpson's paradox will occur with a <a href="https://en.wikipedia.org/wiki/Probability" title="Probability">probability</a> of exactly <span><span>1</span>‚ÅÑ<span>60</span></span>.<sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> A study by Kock suggests that the probability that Simpson's paradox would occur at random in path models (i.e., models generated by <a href="https://en.wikipedia.org/wiki/Path_analysis_(statistics)" title="Path analysis (statistics)">path analysis</a>) with two predictors and one criterion variable is approximately 12.8 percent; slightly higher than 1 occurrence per 8 path models.<sup id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup>
</p>
<h2><span id="Simpson.27s_second_paradox"></span><span id="Simpson's_second_paradox">Simpson's second paradox</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=9" title="Edit section: Simpson's second paradox"><span>edit</span></a><span>]</span></span></h2>
<p>A second, less well-known paradox was also discussed in Simpson's 1951 paper. It can occur when the "sensible interpretation" is not necessarily found in the separated data, like in the Kidney Stone example, but can instead reside in the combined data. Whether the partitioned or combined form of the data should be used hinges on the process giving rise to the data, meaning the correct interpretation of the data cannot always be determined by simply observing the tables.<sup id="cite_ref-25"><a href="#cite_note-25">[25]</a></sup>
</p><p><a href="https://en.wikipedia.org/wiki/Judea_Pearl" title="Judea Pearl">Judea Pearl</a> has shown that, in order for the partitioned data to represent the correct causal relationships between any two variables, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="{\displaystyle X}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" aria-hidden="true" alt="{\displaystyle Y}"></span>, the partitioning variables must satisfy a graphical condition called "back-door criterion":<sup id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup><sup id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup>
</p>
<ol><li>They must block all spurious paths between <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="{\displaystyle X}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" aria-hidden="true" alt="{\displaystyle Y}"></span></li>
<li>No variable can be affected by <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="{\displaystyle X}"></span></li></ol>
<p>This criterion provides an algorithmic solution to Simpson's second paradox, and explains why the correct interpretation cannot be determined by data alone; two different graphs, both compatible with the data, may dictate two different back-door criteria.
</p><p>When the back-door criterion is satisfied by a set <i>Z</i> of covariates, the adjustment formula (see <a href="https://en.wikipedia.org/wiki/Confounding" title="Confounding">Confounding</a>) gives the correct causal effect of <i>X</i> on <i>Y</i>. If no such set exists, Pearl's <i>do</i>-calculus can be invoked to discover other ways of estimating the causal effect.<sup id="cite_ref-pearl_4-4"><a href="#cite_note-pearl-4">[4]</a></sup><sup id="cite_ref-pearl-bow_28-0"><a href="#cite_note-pearl-bow-28">[28]</a></sup> The completeness of <i>do</i>-calculus <sup id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup><sup id="cite_ref-pearl-bow_28-1"><a href="#cite_note-pearl-bow-28">[28]</a></sup> can be viewed as offering a complete resolution of the Simpson's paradox.
</p>
<h2><span id="Criticism">Criticism</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=10" title="Edit section: Criticism"><span>edit</span></a><span>]</span></span></h2>
<p>One criticism is that the paradox is not really a paradox at all, but rather a failure to properly account for confounding variables or to consider causal relationships between variables.<sup id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup>
</p><p>Another criticism of the apparent Simpson's paradox is that it may be a result of the specific way that data is stratified or grouped. The phenomenon may disappear or even reverse if the data is stratified differently or if different confounding variables are considered. Simpson's example actually highlighted a phenomenon called noncollapsibility,<sup id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup> which occurs when subgroups with high proportions do not make simple averages when combined. This suggests that the paradox may not be a universal phenomenon, but rather a specific instance of a more general statistical issue.
</p><p>Critics of the apparent Simpson's paradox also argue that the focus on the paradox may distract from more important statistical issues, such as the need for careful consideration of confounding variables and causal relationships when interpreting data.<sup id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup>
</p><p>Despite these criticisms, the apparent Simpson's paradox remains a popular and intriguing topic in statistics and data analysis. It continues to be studied and debated by researchers and practitioners in a wide range of fields, and it serves as a valuable reminder of the importance of careful statistical analysis and the potential pitfalls of simplistic interpretations of data.
</p>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=11" title="Edit section: See also"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Aliasing" title="Aliasing">Aliasing</a>&nbsp;‚Äì Signal processing effect</li>
<li><a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" title="Anscombe's quartet">Anscombe's quartet</a>&nbsp;‚Äì Four data sets with the same descriptive statistics, yet very different distributions</li>
<li><a href="https://en.wikipedia.org/wiki/Berkson%27s_paradox" title="Berkson's paradox">Berkson's paradox</a>&nbsp;‚Äì Tendency to misinterpret statistical experiments involving conditional probabilities</li>
<li><a href="https://en.wikipedia.org/wiki/Cherry_picking" title="Cherry picking">Cherry picking</a>&nbsp;‚Äì Fallacy of incomplete evidence</li>
<li><a href="https://en.wikipedia.org/wiki/Condorcet_paradox" title="Condorcet paradox">Condorcet paradox</a>&nbsp;‚Äì Situation in social choice theory where collective preferences are cyclic</li>
<li><a href="https://en.wikipedia.org/wiki/Ecological_fallacy" title="Ecological fallacy">Ecological fallacy</a>&nbsp;‚Äì Logical fallacy that occurs when group characteristics are applied to individuals</li>
<li><a href="https://en.wikipedia.org/wiki/Gerrymandering" title="Gerrymandering">Gerrymandering</a>&nbsp;‚Äì Form of political manipulation</li>
<li><a href="https://en.wikipedia.org/wiki/Low_birth-weight_paradox" title="Low birth-weight paradox">Low birth-weight paradox</a>&nbsp;‚Äì Statistical quirk of babies' birth weights</li>
<li><a href="https://en.wikipedia.org/wiki/Modifiable_areal_unit_problem" title="Modifiable areal unit problem">Modifiable areal unit problem</a>&nbsp;‚Äì Source of statistical bias</li>
<li><a href="https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy" title="Prosecutor's fallacy">Prosecutor's fallacy</a>&nbsp;‚Äì Error in thinking which involves under-valuing base rate information</li>
<li><a href="https://en.wikipedia.org/wiki/Will_Rogers_phenomenon" title="Will Rogers phenomenon">Will Rogers phenomenon</a>&nbsp;‚Äì Statistical phenomenon and paradox</li>
<li><a href="https://en.wikipedia.org/wiki/Spurious_correlation" title="Spurious correlation">Spurious correlation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Omitted-variable_bias" title="Omitted-variable bias">Omitted-variable bias</a></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=12" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span>
<cite id="CITEREFClifford_H._Wagner1982">Clifford H. Wagner (February 1982). "Simpson's Paradox in Real Life". <i><a href="https://en.wikipedia.org/wiki/The_American_Statistician" title="The American Statistician">The American Statistician</a></i>. <b>36</b> (1): 46‚Äì48. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2684093">10.2307/2684093</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2684093">2684093</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Statistician&amp;rft.atitle=Simpson%27s+Paradox+in+Real+Life&amp;rft.volume=36&amp;rft.issue=1&amp;rft.pages=46-48&amp;rft.date=1982-02&amp;rft_id=info%3Adoi%2F10.2307%2F2684093&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2684093%23id-name%3DJSTOR&amp;rft.au=Clifford+H.+Wagner&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span>Holt, G. B. (2016).  <a rel="nofollow" href="http://jco.ascopubs.org/content/34/9/1016.1.full">Potential Simpson's paradox in multicenter study of intraperitoneal chemotherapy for ovarian cancer.</a> Journal of Clinical Oncology, 34(9), 1016‚Äì1016.</span>
</li>
<li id="cite_note-VogelFranks2017-3"><span><b><a href="#cite_ref-VogelFranks2017_3-0">^</a></b></span> <span><cite id="CITEREFFranksAiroldiSlavov2017">Franks, Alexander; <a href="https://en.wikipedia.org/wiki/Edoardo_Airoldi" title="Edoardo Airoldi">Airoldi, Edoardo</a>; Slavov, Nikolai (2017). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5440056">"Post-transcriptional regulation across human tissues"</a>. <i>PLOS Computational Biology</i>. <b>13</b> (5): e1005535. <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/1506.00219">1506.00219</a></span>. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/2017PLSCB..13E5535F">2017PLSCB..13E5535F</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1371%2Fjournal.pcbi.1005535">10.1371/journal.pcbi.1005535</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1553-7358">1553-7358</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5440056">5440056</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/28481885">28481885</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+Computational+Biology&amp;rft.atitle=Post-transcriptional+regulation+across+human+tissues&amp;rft.volume=13&amp;rft.issue=5&amp;rft.pages=e1005535&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5440056%23id-name%3DPMC&amp;rft_id=info%3Abibcode%2F2017PLSCB..13E5535F&amp;rft_id=info%3Aarxiv%2F1506.00219&amp;rft.issn=1553-7358&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pcbi.1005535&amp;rft_id=info%3Apmid%2F28481885&amp;rft.aulast=Franks&amp;rft.aufirst=Alexander&amp;rft.au=Airoldi%2C+Edoardo&amp;rft.au=Slavov%2C+Nikolai&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5440056&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-pearl-4"><span>^ <a href="#cite_ref-pearl_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-pearl_4-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-pearl_4-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-pearl_4-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-pearl_4-4"><sup><i><b>e</b></i></sup></a></span> <span><a href="https://en.wikipedia.org/wiki/Judea_Pearl" title="Judea Pearl">Judea Pearl</a>. <i>Causality: Models, Reasoning, and Inference</i>, Cambridge University Press (2000, 2nd edition 2009). <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-521-77362-8" title="Special:BookSources/0-521-77362-8">0-521-77362-8</a>.</span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span>Kock, N., &amp; Gaskins, L. (2016).  <a rel="nofollow" href="http://cits.tamiu.edu/kock/pubs/journals/2016JournalIJANS_ModJCveNetCorrp/Kock_Gaskins_2016_IJANS_SimpPdox.pdf">Simpson's paradox, moderation and the emergence of quadratic relationships in path models: An information systems illustration.</a> International Journal of Applied Nonlinear Science, 2(3), 200‚Äì234.</span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span>Rogier A. Kievit, Willem E. Frankenhuis, Lourens J. Waldorp and Denny Borsboom, Simpson's paradox in psychological science: a practical guide <a rel="nofollow" href="https://doi.org/10.3389/fpsyg.2013.00513">https://doi.org/10.3389/fpsyg.2013.00513</a></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span>Robert L. Wardrop (February 1995). "Simpson's Paradox and the Hot Hand in Basketball". <i>The American Statistician</i>, <b> 49 (1)</b>: pp. 24‚Äì28.</span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><a href="https://en.wikipedia.org/wiki/Alan_Agresti" title="Alan Agresti">Alan Agresti</a> (2002). "Categorical Data Analysis" (Second edition). <a href="https://en.wikipedia.org/wiki/John_Wiley_and_Sons" title="John Wiley and Sons">John Wiley and Sons</a> <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-471-36093-7" title="Special:BookSources/0-471-36093-7">0-471-36093-7</a></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span>
<cite id="CITEREFSimpson,_Edward_H.1951">Simpson, Edward H. (1951). "The Interpretation of Interaction in Contingency Tables". <i>Journal of the Royal Statistical Society, Series B</i>. <b>13</b>: 238‚Äì241.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Royal+Statistical+Society%2C+Series+B&amp;rft.atitle=The+Interpretation+of+Interaction+in+Contingency+Tables&amp;rft.volume=13&amp;rft.pages=238-241&amp;rft.date=1951&amp;rft.au=Simpson%2C+Edward+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span>
<cite id="CITEREFPearsonLeeBramley-Moore1899"><a href="https://en.wikipedia.org/wiki/Karl_Pearson" title="Karl Pearson">Pearson, Karl</a>; Lee, Alice; Bramley-Moore, Lesley (1899). <a rel="nofollow" href="https://doi.org/10.1098%2Frsta.1899.0006">"Genetic (reproductive) selection: Inheritance of fertility in man, and of fecundity in thoroughbred racehorses"</a>. <i><a href="https://en.wikipedia.org/wiki/Philosophical_Transactions_of_the_Royal_Society_A" title="Philosophical Transactions of the Royal Society A">Philosophical Transactions of the Royal Society A</a></i>. <b>192</b>: 257‚Äì330. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1098%2Frsta.1899.0006">10.1098/rsta.1899.0006</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Transactions+of+the+Royal+Society+A&amp;rft.atitle=Genetic+%28reproductive%29+selection%3A+Inheritance+of+fertility+in+man%2C+and+of+fecundity+in+thoroughbred+racehorses&amp;rft.volume=192&amp;rft.pages=257-330&amp;rft.date=1899&amp;rft_id=info%3Adoi%2F10.1098%2Frsta.1899.0006&amp;rft.aulast=Pearson&amp;rft.aufirst=Karl&amp;rft.au=Lee%2C+Alice&amp;rft.au=Bramley-Moore%2C+Lesley&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1098%252Frsta.1899.0006&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-yule-11"><span><b><a href="#cite_ref-yule_11-0">^</a></b></span> <span>
<cite id="CITEREFG._U._Yule1903">G. U. Yule (1903). <a rel="nofollow" href="https://zenodo.org/record/1431599">"Notes on the Theory of Association of Attributes in Statistics"</a>. <i><a href="https://en.wikipedia.org/wiki/Biometrika" title="Biometrika">Biometrika</a></i>. <b>2</b> (2): 121‚Äì134. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1093%2Fbiomet%2F2.2.121">10.1093/biomet/2.2.121</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biometrika&amp;rft.atitle=Notes+on+the+Theory+of+Association+of+Attributes+in+Statistics&amp;rft.volume=2&amp;rft.issue=2&amp;rft.pages=121-134&amp;rft.date=1903&amp;rft_id=info%3Adoi%2F10.1093%2Fbiomet%2F2.2.121&amp;rft.au=G.+U.+Yule&amp;rft_id=https%3A%2F%2Fzenodo.org%2Frecord%2F1431599&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-blyth-72-12"><span>^ <a href="#cite_ref-blyth-72_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-blyth-72_12-1"><sup><i><b>b</b></i></sup></a></span> <span>
<cite id="CITEREFColin_R._Blyth1972">Colin R. Blyth (June 1972). "On Simpson's Paradox and the Sure-Thing Principle". <i>Journal of the American Statistical Association</i>. <b>67</b> (338): 364‚Äì366. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2284382">10.2307/2284382</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2284382">2284382</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Statistical+Association&amp;rft.atitle=On+Simpson%27s+Paradox+and+the+Sure-Thing+Principle&amp;rft.volume=67&amp;rft.issue=338&amp;rft.pages=364-366&amp;rft.date=1972-06&amp;rft_id=info%3Adoi%2F10.2307%2F2284382&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2284382%23id-name%3DJSTOR&amp;rft.au=Colin+R.+Blyth&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span>
<cite id="CITEREFI._J._Good,_Y._Mittal1987"><a href="https://en.wikipedia.org/wiki/I._J._Good" title="I. J. Good">I. J. Good</a>, Y. Mittal (June 1987). <a rel="nofollow" href="https://doi.org/10.1214%2Faos%2F1176350369">"The Amalgamation and Geometry of Two-by-Two Contingency Tables"</a>. <i><a href="https://en.wikipedia.org/wiki/The_Annals_of_Statistics" title="The Annals of Statistics">The Annals of Statistics</a></i>. <b>15</b> (2): 694‚Äì711. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1214%2Faos%2F1176350369">10.1214/aos/1176350369</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0090-5364">0090-5364</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2241334">2241334</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Annals+of+Statistics&amp;rft.atitle=The+Amalgamation+and+Geometry+of+Two-by-Two+Contingency+Tables&amp;rft.volume=15&amp;rft.issue=2&amp;rft.pages=694-711&amp;rft.date=1987-06&amp;rft.issn=0090-5364&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2241334%23id-name%3DJSTOR&amp;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176350369&amp;rft.au=I.+J.+Good%2C+Y.+Mittal&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1214%252Faos%252F1176350369&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite id="CITEREFEllenberg2021">Ellenberg, Jordan (May 25, 2021). <a rel="nofollow" href="https://www.worldcat.org/oclc/1226171979"><i>Shape: The Hidden Geometry of Information, Biology, Strategy, Democracy and Everything Else</i></a>. New York: <a href="https://en.wikipedia.org/wiki/Penguin_Press" title="Penguin Press">Penguin Press</a>. p.&nbsp;228. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-9848-7905-9" title="Special:BookSources/978-1-9848-7905-9"><bdi>978-1-9848-7905-9</bdi></a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/1226171979">1226171979</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Shape%3A+The+Hidden+Geometry+of+Information%2C+Biology%2C+Strategy%2C+Democracy+and+Everything+Else&amp;rft.place=New+York&amp;rft.pages=228&amp;rft.pub=Penguin+Press&amp;rft.date=2021-05-25&amp;rft_id=info%3Aoclcnum%2F1226171979&amp;rft.isbn=978-1-9848-7905-9&amp;rft.aulast=Ellenberg&amp;rft.aufirst=Jordan&amp;rft_id=https%3A%2F%2Fwww.worldcat.org%2Foclc%2F1226171979&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-freedman-15"><span><b><a href="#cite_ref-freedman_15-0">^</a></b></span> <span><a href="https://en.wikipedia.org/wiki/David_A._Freedman" title="David A. Freedman">David Freedman</a>, Robert Pisani, and Roger Purves (2007), <i>Statistics</i> (4th edition), <a href="https://en.wikipedia.org/wiki/W._W._Norton_%26_Company" title="W. W. Norton &amp; Company">W. W. Norton</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-393-92972-8" title="Special:BookSources/0-393-92972-8">0-393-92972-8</a>.</span>
</li>
<li id="cite_note-Bickel-16"><span>^ <a href="#cite_ref-Bickel_16-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Bickel_16-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Bickel_16-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFP.J._Bickel,_E.A._Hammel_and_J.W._O'Connell1975"><a href="https://en.wikipedia.org/wiki/Peter_J._Bickel" title="Peter J. Bickel">P.J. Bickel</a>, E.A. Hammel and J.W. O'Connell (1975). <a rel="nofollow" href="http://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf">"Sex Bias in Graduate Admissions: Data From Berkeley"</a> <span>(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Science_(journal)" title="Science (journal)">Science</a></i>. <b>187</b> (4175): 398‚Äì404. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1975Sci...187..398B">1975Sci...187..398B</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1126%2Fscience.187.4175.398">10.1126/science.187.4175.398</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/17835295">17835295</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:15278703">15278703</a>. <a rel="nofollow" href="https://web.archive.org/web/20160604220121/http://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf">Archived</a> <span>(PDF)</span> from the original on 2016-06-04.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Sex+Bias+in+Graduate+Admissions%3A+Data+From+Berkeley&amp;rft.volume=187&amp;rft.issue=4175&amp;rft.pages=398-404&amp;rft.date=1975&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.187.4175.398&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A15278703%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F17835295&amp;rft_id=info%3Abibcode%2F1975Sci...187..398B&amp;rft.au=P.J.+Bickel%2C+E.A.+Hammel+and+J.W.+O%27Connell&amp;rft_id=http%3A%2F%2Fhomepage.stat.uiowa.edu%2F~mbognar%2F1030%2FBickel-Berkeley.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFC._R._CharigD._R._WebbS._R._PayneJ._E._Wickham1986">C. R. Charig; D. R. Webb; S. R. Payne; J. E. Wickham (29 March 1986). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1339981">"Comparison of treatment of renal calculi by open surgery, percutaneous nephrolithotomy, and extracorporeal shockwave lithotripsy"</a>. <i><a href="https://en.wikipedia.org/wiki/Br_Med_J_(Clin_Res_Ed)" title="Br Med J (Clin Res Ed)">Br Med J (Clin Res Ed)</a></i>. <b>292</b> (6524): 879‚Äì882. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1136%2Fbmj.292.6524.879">10.1136/bmj.292.6524.879</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1339981">1339981</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/3083922">3083922</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Br+Med+J+%28Clin+Res+Ed%29&amp;rft.atitle=Comparison+of+treatment+of+renal+calculi+by+open+surgery%2C+percutaneous+nephrolithotomy%2C+and+extracorporeal+shockwave+lithotripsy&amp;rft.volume=292&amp;rft.issue=6524&amp;rft.pages=879-882&amp;rft.date=1986-03-29&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC1339981%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F3083922&amp;rft_id=info%3Adoi%2F10.1136%2Fbmj.292.6524.879&amp;rft.au=C.+R.+Charig&amp;rft.au=D.+R.+Webb&amp;rft.au=S.+R.+Payne&amp;rft.au=J.+E.+Wickham&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC1339981&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-KidneyParadox-18"><span>^ <a href="#cite_ref-KidneyParadox_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-KidneyParadox_18-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFSteven_A._JuliousMark_A._Mullee1994">Steven A. Julious; Mark A. Mullee (3 December 1994). <a rel="nofollow" href="http://bmj.bmjjournals.com/cgi/content/full/309/6967/1480">"Confounding and Simpson's paradox"</a>. <i><a href="https://en.wikipedia.org/wiki/BMJ" title="BMJ">BMJ</a></i>. <b>309</b> (6967): 1480‚Äì1481. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1136%2Fbmj.309.6967.1480">10.1136/bmj.309.6967.1480</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2541623">2541623</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/7804052">7804052</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BMJ&amp;rft.atitle=Confounding+and+Simpson%27s+paradox&amp;rft.volume=309&amp;rft.issue=6967&amp;rft.pages=1480-1481&amp;rft.date=1994-12-03&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2541623%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F7804052&amp;rft_id=info%3Adoi%2F10.1136%2Fbmj.309.6967.1480&amp;rft.au=Steven+A.+Julious&amp;rft.au=Mark+A.+Mullee&amp;rft_id=http%3A%2F%2Fbmj.bmjjournals.com%2Fcgi%2Fcontent%2Ffull%2F309%2F6967%2F1480&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-RossBaseball-19"><span>^ <a href="#cite_ref-RossBaseball_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-RossBaseball_19-1"><sup><i><b>b</b></i></sup></a></span> <span>Ken Ross. "<i>A Mathematician at the Ballpark: Odds and Probabilities for Baseball Fans (Paperback)</i>" Pi Press, 2004. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-13-147990-3" title="Special:BookSources/0-13-147990-3">0-13-147990-3</a>. 12‚Äì13</span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span>Statistics available from <a href="https://en.wikipedia.org/wiki/Baseball-Reference.com" title="Baseball-Reference.com">Baseball-Reference.com</a>: <a rel="nofollow" href="https://www.baseball-reference.com/j/jeterde01.shtml">Data for Derek Jeter</a>; <a rel="nofollow" href="https://www.baseball-reference.com/j/justida01.shtml">Data for David Justice</a>.</span>
</li>
<li id="cite_note-21"><span><b><a href="#cite_ref-21">^</a></b></span> <span><cite id="CITEREFKocik_Jerzy2001">Kocik Jerzy (2001). <a rel="nofollow" href="http://www.math.siu.edu/kocik/papers/simpson2.pdf">"Proofs without Words: Simpson's Paradox"</a> <span>(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Mathematics_Magazine" title="Mathematics Magazine">Mathematics Magazine</a></i>. <b>74</b> (5): 399. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2691038">10.2307/2691038</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2691038">2691038</a>. <a rel="nofollow" href="https://web.archive.org/web/20100612220747/http://www.math.siu.edu/kocik/papers/simpson2.pdf">Archived</a> <span>(PDF)</span> from the original on 2010-06-12.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematics+Magazine&amp;rft.atitle=Proofs+without+Words%3A+Simpson%27s+Paradox&amp;rft.volume=74&amp;rft.issue=5&amp;rft.pages=399&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.2307%2F2691038&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2691038%23id-name%3DJSTOR&amp;rft.au=Kocik+Jerzy&amp;rft_id=http%3A%2F%2Fwww.math.siu.edu%2Fkocik%2Fpapers%2Fsimpson2.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span>Berman, S. DalleMule, L.  Greene, M., Lucker, J. (2012), "<a rel="nofollow" href="http://www.statslife.org.uk/the-statistics-dictionary/2012-simpson-s-paradox-a-cautionary-tale-in-advanced-analytics">Simpson's Paradox: A Cautionary Tale in Advanced Analytics</a> <a rel="nofollow" href="https://web.archive.org/web/20200510171740/https://www.statslife.org.uk/the-statistics-dictionary/2012-simpson-s-paradox-a-cautionary-tale-in-advanced-analytics">Archived</a> 2020-05-10 at the <a href="https://en.wikipedia.org/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>", <i><a href="https://en.wikipedia.org/wiki/Significance_(magazine)" title="Significance (magazine)">Significance</a></i>.</span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span>
<cite id="CITEREFMarios_G._PavlidesMichael_D._Perlman2009">Marios G. Pavlides &amp; Michael D. Perlman (August 2009). "How Likely is Simpson's Paradox?". <i><a href="https://en.wikipedia.org/wiki/The_American_Statistician" title="The American Statistician">The American Statistician</a></i>. <b>63</b> (3): 226‚Äì233. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1198%2Ftast.2009.09007">10.1198/tast.2009.09007</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:17481510">17481510</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Statistician&amp;rft.atitle=How+Likely+is+Simpson%27s+Paradox%3F&amp;rft.volume=63&amp;rft.issue=3&amp;rft.pages=226-233&amp;rft.date=2009-08&amp;rft_id=info%3Adoi%2F10.1198%2Ftast.2009.09007&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A17481510%23id-name%3DS2CID&amp;rft.au=Marios+G.+Pavlides&amp;rft.au=Michael+D.+Perlman&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-24"><span><b><a href="#cite_ref-24">^</a></b></span> <span>Kock, N. (2015). <a rel="nofollow" href="http://cits.tamiu.edu/kock/pubs/journals/2015JournalIJeC/Kock_2015_IJeC_SimpPdox.pdf">How likely is Simpson's paradox in path models?</a> International Journal of e-Collaboration, 11(1), 1‚Äì7.</span>
</li>
<li id="cite_note-25"><span><b><a href="#cite_ref-25">^</a></b></span> <span><cite id="CITEREFNortonDivine2015">Norton, H. James; Divine, George (August 2015). <a rel="nofollow" href="https://doi.org/10.1111%2Fj.1740-9713.2015.00844.x">"Simpson's paradox ... and how to avoid it"</a>. <i>Significance</i>. <b>12</b> (4): 40‚Äì43. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1111%2Fj.1740-9713.2015.00844.x">10.1111/j.1740-9713.2015.00844.x</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Significance&amp;rft.atitle=Simpson%27s+paradox+...+and+how+to+avoid+it&amp;rft.volume=12&amp;rft.issue=4&amp;rft.pages=40-43&amp;rft.date=2015-08&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1740-9713.2015.00844.x&amp;rft.aulast=Norton&amp;rft.aufirst=H.+James&amp;rft.au=Divine%2C+George&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1111%252Fj.1740-9713.2015.00844.x&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-26"><span><b><a href="#cite_ref-26">^</a></b></span> <span><cite id="CITEREFPearl2014">Pearl, Judea (2014). "Understanding Simpson's Paradox". <i>The American Statistician</i>. <b>68</b> (1): 8‚Äì13. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2139%2Fssrn.2343788">10.2139/ssrn.2343788</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:2626833">2626833</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Statistician&amp;rft.atitle=Understanding+Simpson%27s+Paradox&amp;rft.volume=68&amp;rft.issue=1&amp;rft.pages=8-13&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.2139%2Fssrn.2343788&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A2626833%23id-name%3DS2CID&amp;rft.aulast=Pearl&amp;rft.aufirst=Judea&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-27"><span><b><a href="#cite_ref-27">^</a></b></span> <span><cite id="CITEREFPearl1993">Pearl, Judea (1993). <a rel="nofollow" href="https://doi.org/10.1214%2Fss%2F1177010894">"Graphical Models, Causality, and Intervention"</a>. <i>Statistical Science</i>. <b>8</b> (3): 266‚Äì269. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1214%2Fss%2F1177010894">10.1214/ss/1177010894</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Statistical+Science&amp;rft.atitle=Graphical+Models%2C+Causality%2C+and+Intervention&amp;rft.volume=8&amp;rft.issue=3&amp;rft.pages=266-269&amp;rft.date=1993&amp;rft_id=info%3Adoi%2F10.1214%2Fss%2F1177010894&amp;rft.aulast=Pearl&amp;rft.aufirst=Judea&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1214%252Fss%252F1177010894&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-pearl-bow-28"><span>^ <a href="#cite_ref-pearl-bow_28-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-pearl-bow_28-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFPearlMackenzie2018">Pearl, J.; Mackenzie, D. (2018). <i>The Book of Why: The New Science of Cause and Effect</i>. New York, NY: Basic Books.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Book+of+Why%3A+The+New+Science+of+Cause+and+Effect&amp;rft.place=New+York%2C+NY&amp;rft.pub=Basic+Books&amp;rft.date=2018&amp;rft.aulast=Pearl&amp;rft.aufirst=J.&amp;rft.au=Mackenzie%2C+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-29"><span><b><a href="#cite_ref-29">^</a></b></span> <span><cite id="CITEREFShpitserPearl2006">Shpitser, I.; Pearl, J. (2006). Dechter, R.; Richardson, T.S. (eds.). "Identification of Conditional Interventional Distributions". <i>Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence</i>. Corvallis, OR: AUAI Press: 437‚Äì444.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Twenty-Second+Conference+on+Uncertainty+in+Artificial+Intelligence&amp;rft.atitle=Identification+of+Conditional+Interventional+Distributions&amp;rft.pages=437-444&amp;rft.date=2006&amp;rft.aulast=Shpitser&amp;rft.aufirst=I.&amp;rft.au=Pearl%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-30"><span><b><a href="#cite_ref-30">^</a></b></span> <span><cite id="CITEREFBlyth1972">Blyth, Colin R. (June 1972). <a rel="nofollow" href="http://www.tandfonline.com/doi/abs/10.1080/01621459.1972.10482387">"On Simpson's Paradox and the Sure-Thing Principle"</a>. <i>Journal of the American Statistical Association</i>. <b>67</b> (338): 364‚Äì366. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1080%2F01621459.1972.10482387">10.1080/01621459.1972.10482387</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0162-1459">0162-1459</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Statistical+Association&amp;rft.atitle=On+Simpson%27s+Paradox+and+the+Sure-Thing+Principle&amp;rft.volume=67&amp;rft.issue=338&amp;rft.pages=364-366&amp;rft.date=1972-06&amp;rft_id=info%3Adoi%2F10.1080%2F01621459.1972.10482387&amp;rft.issn=0162-1459&amp;rft.aulast=Blyth&amp;rft.aufirst=Colin+R.&amp;rft_id=http%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Fabs%2F10.1080%2F01621459.1972.10482387&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-31"><span><b><a href="#cite_ref-31">^</a></b></span> <span><cite id="CITEREFGreenland2021">Greenland, Sander (2021-11-01). <a rel="nofollow" href="https://www.jclinepi.com/article/S0895-4356(21)00182-7/fulltext">"Noncollapsibility, confounding, and sparse-data bias. Part 2: What should researchers make of persistent controversies about the odds ratio?"</a>. <i>Journal of Clinical Epidemiology</i>. <b>139</b>: 264‚Äì268. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1016%2Fj.jclinepi.2021.06.004">10.1016/j.jclinepi.2021.06.004</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0895-4356">0895-4356</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/34119647">34119647</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Clinical+Epidemiology&amp;rft.atitle=Noncollapsibility%2C+confounding%2C+and+sparse-data+bias.+Part+2%3A+What+should+researchers+make+of+persistent+controversies+about+the+odds+ratio%3F&amp;rft.volume=139&amp;rft.pages=264-268&amp;rft.date=2021-11-01&amp;rft.issn=0895-4356&amp;rft_id=info%3Apmid%2F34119647&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jclinepi.2021.06.004&amp;rft.aulast=Greenland&amp;rft.aufirst=Sander&amp;rft_id=https%3A%2F%2Fwww.jclinepi.com%2Farticle%2FS0895-4356%2821%2900182-7%2Ffulltext&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-32"><span><b><a href="#cite_ref-32">^</a></b></span> <span><cite id="CITEREFHern√°nClaytonKeiding2011">Hern√°n, Miguel A.; Clayton, David; Keiding, Niels (June 2011). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3147074">"The Simpson's paradox unraveled"</a>. <i>International Journal of Epidemiology</i>. <b>40</b> (3): 780‚Äì785. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1093%2Fije%2Fdyr041">10.1093/ije/dyr041</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1464-3685">1464-3685</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3147074">3147074</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/21454324">21454324</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Epidemiology&amp;rft.atitle=The+Simpson%27s+paradox+unraveled&amp;rft.volume=40&amp;rft.issue=3&amp;rft.pages=780-785&amp;rft.date=2011-06&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3147074%23id-name%3DPMC&amp;rft.issn=1464-3685&amp;rft_id=info%3Apmid%2F21454324&amp;rft_id=info%3Adoi%2F10.1093%2Fije%2Fdyr041&amp;rft.aulast=Hern%C3%A1n&amp;rft.aufirst=Miguel+A.&amp;rft.au=Clayton%2C+David&amp;rft.au=Keiding%2C+Niels&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3147074&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
</ol></div>
<h2><span id="Bibliography">Bibliography</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=13" title="Edit section: Bibliography"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Leila_Schneps" title="Leila Schneps">Leila Schneps</a> and <a href="https://en.wikipedia.org/wiki/Coralie_Colmez" title="Coralie Colmez">Coralie Colmez</a>, <i><a href="https://en.wikipedia.org/wiki/Math_on_Trial" title="Math on Trial">Math on trial. How numbers get used and abused in the courtroom</a></i>, Basic Books, 2013. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-465-03292-1" title="Special:BookSources/978-0-465-03292-1">978-0-465-03292-1</a>. (Sixth chapter: "Math error number 6: Simpson's paradox. The Berkeley sex bias case: discrimination detection").</li></ul>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=14" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>

<ul><li><a rel="nofollow" href="http://plato.stanford.edu/entries/paradox-simpson/">Simpson's Paradox</a> at the <a href="https://en.wikipedia.org/wiki/Stanford_Encyclopedia_of_Philosophy" title="Stanford Encyclopedia of Philosophy">Stanford Encyclopedia of Philosophy</a>, by Jan Sprenger and Naftali Weinberger.</li>
<li><a rel="nofollow" href="http://ed.ted.com/lessons/how-statistics-can-be-misleading-mark-liddell">How statistics can be misleading ‚Äì Mark Liddell</a> ‚Äì TED-Ed video and lesson.</li>
<li><a href="https://en.wikipedia.org/wiki/Judea_Pearl" title="Judea Pearl">Pearl, Judea</a>, <a rel="nofollow" href="https://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf">"Understanding Simpson‚Äôs Paradox"</a> (PDF)</li>
<li><a rel="nofollow" href="http://www.cut-the-knot.org/Curriculum/Algebra/SimpsonParadox.shtml">Simpson's Paradox</a>, a short article by <a href="https://en.wikipedia.org/wiki/Alexander_Bogomolny" title="Alexander Bogomolny">Alexander Bogomolny</a> on the vector interpretation of Simpson's paradox</li>
<li><a rel="nofollow" href="https://www.wsj.com/articles/SB125970744553071829">The Wall Street Journal column "The Numbers Guy"</a> for December 2, 2009 dealt with recent instances of Simpson's paradox in the news. Notably a Simpson's paradox in the comparison of unemployment rates of the 2009 recession with the 1983 recession.</li>
<li><a rel="nofollow" href="http://www.stateoftheusa.org/content/at-the-plate-a-statistical-puz.php">At the Plate, a Statistical Puzzler: Understanding Simpson's Paradox</a> by Arthur Smith, August 20, 2010</li>
<li><a rel="nofollow" href="https://www.youtube.com/watch?v=ebEkn-BiW5k">Simpson's Paradox</a>, a video by Henry Reich of <a href="https://en.wikipedia.org/wiki/MinutePhysics" title="MinutePhysics">MinutePhysics</a></li></ul>

<!-- 
NewPP limit report
Parsed by mw1415
Cached time: 20240311132754
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‚Äêrevision‚Äêsha1, show‚Äêtoc]
CPU time usage: 0.528 seconds
Real time usage: 0.697 seconds
Preprocessor visited node count: 4454/1000000
Post‚Äêexpand include size: 60342/2097152 bytes
Template argument size: 2012/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‚Äêexpand size: 88383/5000000 bytes
Lua time usage: 0.321/10.000 seconds
Lua memory usage: 14136232/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  577.815      1 -total
 39.21%  226.581      1 Template:Reflist
 27.95%  161.473     11 Template:Annotated_link
 25.33%  146.343     19 Template:Cite_journal
 14.82%   85.614      1 Template:Short_description
  8.91%   51.459      2 Template:Pagetype
  6.43%   37.148      5 Template:Isbn
  5.77%   33.341      1 Template:Commons_category
  5.41%   31.231      1 Template:Sister_project
  5.09%   29.420      1 Template:Side_box
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:46096-0!canonical and timestamp 20240311132754 and revision id 1192014464. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing whistleblower found dead in US (1080 pts)]]></title>
            <link>https://www.bbc.com/news/business-68534703</link>
            <guid>39673589</guid>
            <pubDate>Mon, 11 Mar 2024 21:45:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/business-68534703">https://www.bbc.com/news/business-68534703</a>, See on <a href="https://news.ycombinator.com/item?id=39673589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="legacy-header-block"></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 976w" type="image/webp"><img alt="A Boeing 787 Dreamliner plane at the Paris Air Show" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg" width="976" height="549" loading="eager"></picture></span><span role="text"><span>Image source, </span>Getty Images</span></p></figure></div><div data-component="text-block"><p><b>A former Boeing employee known for raising concerns about the firm's production standards has been found dead in the US. </b></p></div><div data-component="text-block"><p>John Barnett had worked for Boeing for 32 years, until his retirement in 2017. </p></div><div data-component="text-block"><p>In the days before his death, he had been giving evidence in a whistleblower lawsuit against the company. </p></div><div data-component="text-block"><p>Boeing said it was saddened to hear of Mr Barnett's passing. The Charleston County coroner confirmed his death to the BBC on Monday. </p></div><div data-component="text-block"><p>It said the 62-year-old had died from a "self-inflicted" wound on 9 March and police were investigating. </p></div><div data-component="text-block"><p>Mr Barnett had worked for the US plane giant for 32 years, until his retirement in 2017 on health grounds. </p></div><div data-component="text-block"><p>From 2010, he worked as a quality manager at the North Charleston plant making the 787 Dreamliner, a state-of-the-art airliner used mainly on long-haul routes. </p></div><div data-component="text-block"><p><a href="https://www.bbc.co.uk/news/business-50293927">In 2019, Mr Barnett told the BBC</a> that under-pressure workers had been deliberately fitting sub-standard parts to aircraft on the production line. </p></div><div data-component="text-block"><p>He also said he had uncovered serious problems with oxygen systems, which could mean one in four breathing masks would not work in an emergency.</p></div><div data-component="text-block"><p>He said soon after starting work in South Carolina he had become concerned that the push to get new aircraft built meant the assembly process was rushed and safety was compromised, something the company denied.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>John Barnett</span></p><figcaption><span>Image caption, </span><p>John Barnett was a former quality control manager at Boeing</p></figcaption></figure></div><div data-component="text-block"><p>He later told the BBC that workers had failed to follow procedures intended to track components through the factory, allowing defective components to go missing. </p></div><div data-component="text-block"><p>He said in some cases, sub-standard parts had even been removed from scrap bins and fitted to planes that were being built to prevent delays on the production line.</p></div><div data-component="text-block"><p>He also claimed that tests on emergency oxygen systems due to be fitted to the 787 showed a failure rate of 25%, meaning that one in four could fail to deploy in a real-life emergency.</p></div><div data-component="text-block"><p>Mr Barnett said he had alerted managers to his concerns, but no action had been taken. </p></div><div data-component="text-block"><p>Boeing denied his assertions. However, a 2017 review by the US regulator, the Federal Aviation Administration (FAA), did uphold some of Mr Barnett's concerns.</p></div><div data-component="text-block"><p>It established that the location of at least 53 "non-conforming" parts in the factory was unknown, and that they were considered lost. Boeing was ordered to take remedial action.</p></div><div data-component="text-block"><p>On the oxygen cylinders issue, the company said that in 2017 it had "identified some oxygen bottles received from the supplier that were not deploying properly". But it denied that any of them were actually fitted on aircraft.</p></div><div data-component="text-block"><p>After retiring, he embarked on a long-running legal action against the company. </p></div><div data-component="text-block"><p>He accused it of denigrating his character and hampering his career because of the issues he pointed out - charges rejected by Boeing.</p></div><div data-component="text-block"><p>At the time of his death, Mr Barnett had been in Charleston for legal interviews linked to that case.</p></div><div data-component="text-block"><p>Last week, he gave a formal deposition in which he was questioned by Boeing's lawyers, before being cross-examined by his own counsel.</p></div><div data-component="text-block"><p>He had been due to undergo further questioning on Saturday. When he did not appear, enquiries were made at his hotel. </p></div><div data-component="text-block"><p>He was subsequently found dead in his truck in the hotel car park. </p></div><div data-component="text-block"><p>Speaking to the BBC, his lawyer described his death as "tragic". </p></div><div data-component="text-block"><p>In a statement Boeing said: "We are saddened by Mr. Barnett's passing, and our thoughts are with his family and friends."</p></div><div data-component="text-block"><p>His death comes at a time when production standards at both Boeing and its key supplier Spirit Aerosystems are under intense scrutiny.</p></div><div data-component="text-block"><p>This follows an incident in early January when an unused emergency exit door blew off a brand-new Boeing 737 Max shortly after take-off from Portland International Airport.</p></div><div data-component="text-block"><p>A preliminary report from the US National Transportation Safety Board suggested that four key bolts, designed to hold the door securely in place, were not fitted.</p></div><div data-component="text-block"><p>Last week, the FAA said a six-week audit of the company had found "multiple instances where the company allegedly failed to comply with manufacturing quality control requirements".</p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Among the A.I. doomsayers (103 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2024/03/18/among-the-ai-doomsayers</link>
            <guid>39673265</guid>
            <pubDate>Mon, 11 Mar 2024 21:09:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2024/03/18/among-the-ai-doomsayers">https://www.newyorker.com/magazine/2024/03/18/among-the-ai-doomsayers</a>, See on <a href="https://news.ycombinator.com/item?id=39673265">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Katja Grace‚Äôs apartment, in West Berkeley, is in an old machinist‚Äôs factory, with pitched roofs and windows at odd angles. It has terra-cotta floors and no central heating, which can create the impression that you‚Äôve stepped out of the California sunshine and into a duskier place, somewhere long ago or far away. Yet there are also some quietly futuristic touches. High-capacity air purifiers thrumming in the corners. Nonperishables stacked in the pantry. A sleek white machine that does lab-quality RNA tests. The sorts of objects that could portend a future of tech-enabled ease, or one of constant vigilance.</p><p>Grace, the lead researcher at a nonprofit called A.I. Impacts, describes her job as ‚Äúthinking about whether A.I. will destroy the world.‚Äù She spends her time writing theoretical papers and blog posts on complicated decisions related to a burgeoning subfield known as A.I. safety. She is a nervous smiler, an oversharer, a bit of a mumbler; she‚Äôs in her thirties, but she looks almost like a teen-ager, with a middle part and a round, open face. The apartment is crammed with books, and when a friend of Grace‚Äôs came over, one afternoon in November, he spent a while gazing, bemused but nonjudgmental, at a few of the spines: ‚ÄúJewish Divorce Ethics,‚Äù ‚ÄúThe Jewish Way in Death and Mourning,‚Äù ‚ÄúThe Death of Death.‚Äù Grace, as far as she knows, is neither Jewish nor dying. She let the ambiguity linger for a moment. Then she explained: her landlord had wanted the possessions of the previous occupant, his recently deceased ex-wife, to be left intact. ‚ÄúSort of a relief, honestly,‚Äù Grace said. ‚ÄúOne set of decisions I don‚Äôt have to make.‚Äù</p><p>She was spending the afternoon preparing dinner for six: a yogurt-and-cucumber salad, Impossible beef gyros. On one corner of a whiteboard, she had split her pre-party tasks into painstakingly small steps (‚ÄúChop salad,‚Äù ‚ÄúMix salad,‚Äù ‚ÄúMold meat,‚Äù ‚ÄúCook meat‚Äù); on other parts of the whiteboard, she‚Äôd written more gnomic prompts (‚ÄúFood area,‚Äù ‚ÄúObjects,‚Äù ‚ÄúSubstances‚Äù). Her friend, a cryptographer at Android named Paul Crowley, wore a black T-shirt and black jeans, and had dyed black hair. I asked how they knew each other, and he responded, ‚ÄúOh, we‚Äôve crossed paths for years, as part of the scene.‚Äù</p><p>It was understood that ‚Äúthe scene‚Äù meant a few intertwined subcultures known for their exhaustive debates about recondite issues (secure DNA synthesis, shrimp welfare) that members consider essential, but that most normal people know nothing about. For two decades or so, one of these issues has been whether artificial intelligence will elevate or exterminate humanity. Pessimists are called A.I. safetyists, or decelerationists‚Äîor, when they‚Äôre feeling especially panicky, A.I. doomers. They find one another online and often end up living together in group houses in the Bay Area, sometimes even co-parenting and co-homeschooling their kids. Before the dot-com boom, the neighborhoods of Alamo Square and Hayes Valley, with their pastel Victorian row houses, were associated with staid domesticity. Last year, referring to A.I. ‚Äúhacker houses,‚Äù the San Francisco Standard semi-ironically called the area Cerebral Valley.</p><p>A camp of techno-optimists rebuffs A.I. doomerism with old-fashioned libertarian boomerism, insisting that all the hand-wringing about existential risk is a kind of mass hysteria. They call themselves ‚Äúeffective accelerationists,‚Äù or e/accs (pronounced ‚Äúe-acks‚Äù), and they believe A.I. will usher in a utopian future‚Äîinterstellar travel, the end of disease‚Äîas long as the worriers get out of the way. On social media, they troll doomsayers as ‚Äúdecels,‚Äù ‚Äúpsyops,‚Äù ‚Äúbasically terrorists,‚Äù or, worst of all, ‚Äúregulation-loving bureaucrats.‚Äù ‚ÄúWe must steal the fire of intelligence from the gods [and] use it to propel humanity towards the stars,‚Äù a leading e/acc recently tweeted. (And then there are the normies, based anywhere other than the Bay Area or the Internet, who have mostly tuned out the debate, attributing it to sci-fi fume-huffing or corporate hot air.)</p><p>Grace‚Äôs dinner parties, semi-underground meetups for doomers and the doomer-curious, have been described as ‚Äúa nexus of the Bay Area AI scene.‚Äù At gatherings like these, it‚Äôs not uncommon to hear someone strike up a conversation by asking, ‚ÄúWhat are your timelines?‚Äù or ‚ÄúWhat‚Äôs your p(doom)?‚Äù Timelines are predictions of how soon A.I. will pass particular benchmarks, such as writing a Top Forty pop song, making a Nobel-worthy scientific breakthrough, or achieving artificial general intelligence, the point at which a machine can do any cognitive task that a person can do. (Some experts believe that A.G.I. is impossible, or decades away; others expect it to arrive this year.) P(doom) is the probability that, if A.I. does become smarter than people, it will, either on purpose or by accident, annihilate everyone on the planet. For years, even in Bay Area circles, such speculative conversations were marginalized. Last year, after OpenAI released ChatGPT, a language model that could sound uncannily natural, they suddenly burst into the mainstream. Now there are a few hundred people working full time to save the world from A.I. catastrophe. Some advise governments or corporations on their policies; some work on technical aspects of A.I. safety, approaching it as a set of complex math problems; Grace works at a kind of think tank that produces research on ‚Äúhigh-level questions,‚Äù such as ‚ÄúWhat roles will AI systems play in society?‚Äù and ‚ÄúWill they pursue ‚Äògoals‚Äô?‚Äù When they‚Äôre not lobbying in D.C. or meeting at an international conference, they often cross paths in places like Grace‚Äôs living room.</p><p>The rest of her guests arrived one by one: an authority on quantum computing; a former OpenAI researcher; the head of an institute that forecasts the future. Grace offered wine and beer, but most people opted for nonalcoholic canned drinks that defied easy description (a fermented energy drink, a ‚Äúhopped tea‚Äù). They took their Impossible gyros to Grace‚Äôs sofa, where they talked until midnight. They were courteous, disagreeable, and surprisingly patient about reconsidering basic assumptions. ‚ÄúYou can condense the gist of the worry, seems to me, into a really simple two-step argument,‚Äù Crowley said. ‚ÄúStep one: We‚Äôre building machines that might become vastly smarter than us. Step two: That seems pretty dangerous.‚Äù</p><p>‚ÄúAre we sure, though?‚Äù Josh Rosenberg, the C.E.O. of the Forecasting Research Institute, said. ‚ÄúAbout intelligence per se being dangerous?‚Äù</p><p>Grace noted that not all intelligent species are threatening: ‚ÄúThere are elephants, and yet mice still seem to be doing just fine.‚Äù</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a28152-rd&quot;}" href="https://www.newyorker.com/cartoon/a28152-rd" rel="noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Erika Sjule and Nate Odenkirk</span></p></div></span></p></figure><p>‚ÄúRabbits are certainly more intelligent than myxomatosis,‚Äù Michael Nielsen, the quantum-computing expert, said.</p><p>Crowley‚Äôs p(doom) was ‚Äúwell above eighty per cent.‚Äù The others, wary of committing to a number, deferred to Grace, who said that, ‚Äúgiven my deep confusion and uncertainty about this‚Äîwhich I think nearly everyone has, at least everyone who‚Äôs being honest,‚Äù she could only narrow her p(doom) to ‚Äúbetween ten and ninety per cent.‚Äù Still, she went on, ‚Äúa ten-per-cent chance of human extinction is obviously, if you take it seriously, unacceptably high.‚Äù</p><p>They agreed that, amid the thousands of reactions to ChatGPT, one of the most refreshingly candid assessments came from Snoop Dogg, during an onstage interview. Crowley pulled up the transcript and read aloud. ‚ÄúThis is not safe, ‚Äôcause the A.I.s got their own minds, and these motherfuckers are gonna start doing their own shit,‚Äù Snoop said, paraphrasing an A.I.-safety argument. ‚ÄúShit, what the fuck?‚Äù Crowley laughed. ‚ÄúI have to admit, that captures the emotional tenor much better than my two-step argument,‚Äù he said. And then, as if to justify the moment of levity, he read out another quote, this one from a 1948 essay by C.&nbsp;S. Lewis: ‚ÄúIf we are all going to be destroyed by an atomic bomb, let that bomb when it comes find us doing sensible and human things‚Äîpraying, working, teaching, reading, listening to music, bathing the children, playing tennis, chatting to our friends over a pint and a game of darts‚Äînot huddled together like frightened sheep.‚Äù</p><p>Grace used to work for Eliezer Yudkowsky, a bearded guy with a fedora, a petulant demeanor, and a p(doom) of ninety-nine per cent. Raised in Chicago as an Orthodox Jew, he dropped out of school after eighth grade, taught himself calculus and atheism, started blogging, and, in the early two-thousands, made his way to the Bay Area. His best-known works include ‚ÄúHarry Potter and the Methods of Rationality,‚Äù a piece of fan fiction running to more than six hundred thousand words, and ‚ÄúThe Sequences,‚Äù a gargantuan series of essays about how to sharpen one‚Äôs thinking. The informal collective that grew up around these writings‚Äîfirst in the comments, then in the physical world‚Äîbecame known as the rationalist community, a small subculture devoted to avoiding ‚Äúthe typical failure modes of human reason,‚Äù often by arguing from first principles or quantifying potential risks. Nathan Young, a software engineer, told me, ‚ÄúI remember hearing about Eliezer, who was known to be a heavy guy, onstage at some rationalist event, asking the crowd to predict if he could lose a bunch of weight. Then the big reveal: he unzips the fat suit he was wearing. He‚Äôd already lost the weight. I think his ostensible point was something about how it‚Äôs hard to predict the future, but mostly I remember thinking, What an absolute legend.‚Äù</p><p>Yudkowsky was a transhumanist: human brains were going to be uploaded into digital brains during his lifetime, and this was great news. He told me recently that ‚ÄúEliezer ages sixteen through twenty‚Äù assumed that A.I. ‚Äúwas going to be great fun for everyone forever, and wanted it built as soon as possible.‚Äù In 2000, he co-founded the Singularity Institute for Artificial Intelligence, to help hasten the A.I. revolution. Still, he decided to do some due diligence. ‚ÄúI didn‚Äôt see why an A.I. would kill everyone, but I felt compelled to systematically study the question,‚Äù he said. ‚ÄúWhen I did, I went, Oh, I guess I was wrong.‚Äù He wrote detailed white papers about how A.I. might wipe us all out, but his warnings went unheeded. Eventually, he renamed his think tank the Machine Intelligence Research Institute, or <em>MIRI</em>.</p><p>The existential threat posed by A.I. had always been among the rationalists‚Äô central issues, but it emerged as the dominant topic around 2015, following a rapid series of <a href="https://www.newyorker.com/science/elements/how-the-artificial-intelligence-program-alphazero-mastered-its-games">advances in machine learning</a>. Some rationalists were in touch with Oxford philosophers, including <a href="https://www.newyorker.com/culture/annals-of-inquiry/how-close-is-humanity-to-the-edge">Toby Ord</a> and <a href="https://www.newyorker.com/magazine/2022/08/15/the-reluctant-prophet-of-effective-altruism">William MacAskill</a>, the founders of the effective-altruism movement, which studied how to do the most good for humanity (and, by extension, how to avoid ending it). The boundaries between the movements increasingly blurred. Yudkowsky, Grace, and a few others flew around the world to E.A. conferences, where you could talk about A.I. risk without being laughed out of the room.</p><p>Philosophers of doom tend to get hung up on elaborate sci-fi-inflected hypotheticals. Grace introduced me to Joe Carlsmith, an Oxford-trained philosopher who had just published a paper about ‚Äúscheming AIs‚Äù that might convince their human handlers they‚Äôre safe, then proceed to take over. He smiled bashfully as he expounded on a thought experiment in which a hypothetical person is forced to stack bricks in a desert for a million years. ‚ÄúThis can be a lot, I realize,‚Äù he said. Yudkowsky argues that a superintelligent machine could come to see us as a threat, and decide to kill us (by commandeering existing autonomous weapons systems, say, or by building its own). Or our demise could happen ‚Äúin passing‚Äù: you ask a supercomputer to improve its own processing speed, and it concludes that the best way to do this is to turn all nearby atoms into silicon, including those atoms that are currently people. But the basic A.I.-safety arguments do not require imagining that the current crop of Verizon chatbots will suddenly morph into Skynet, the digital supervillain from ‚ÄúTerminator.‚Äù To be dangerous, A.G.I. doesn‚Äôt have to be sentient, or desire our destruction. If its objectives are at odds with human flourishing, even in subtle ways, then, say the doomers, we‚Äôre screwed.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This is known as <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/can-we-stop-the-singularity">the alignment problem,</a> and it is generally acknowledged to be <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/can-we-stop-the-singularity">unresolved</a>. In 2016, while training one of their models to play a boat-racing video game, OpenAI researchers instructed it to get as many points as possible, which they assumed would involve it finishing the race. Instead, <a data-offer-url="https://openai.com/research/faulty-reward-functions" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://openai.com/research/faulty-reward-functions&quot;}" href="https://openai.com/research/faulty-reward-functions" rel="noopener" target="_blank">they noted</a>, the model ‚Äúfinds an isolated lagoon where it can turn in a large circle,‚Äù allowing it to rack up a high score ‚Äúdespite repeatedly catching on fire, crashing into other boats, and going the wrong way on the track.‚Äù Maximizing points, it turned out, was a ‚Äúmisspecified reward function.‚Äù Now imagine a world in which more powerful A.I.s pilot <a href="https://www.zdnet.com/article/this-uncrewed-autonomous-boat-uses-ai-and-solar-power-to-explore-the-ocean/">actual boats</a>‚Äîand <a href="https://www.washingtonpost.com/technology/2023/06/10/tesla-autopilot-crashes-elon-musk/">cars</a>, and <a href="https://www.nytimes.com/2023/11/21/us/politics/ai-drones-war-law.html">military drones</a>‚Äîor where a quant trader can instruct a proprietary A.I. to come up with some creative ways to increase the value of her stock portfolio. Maybe the A.I. will infer that the best way to juice the market is to disable the Eastern Seaboard‚Äôs power grid, or to goad North Korea into a world war. Even if the trader tries to specify the right reward functions (<em>Don‚Äôt break any laws; make sure no one gets hurt</em>), she can always make mistakes.</p><p>No one thinks that GPT-4, OpenAI‚Äôs most recent model, has achieved artificial general intelligence, but it seems capable of deploying novel (and deceptive) means of accomplishing real-world goals. Before releasing it, OpenAI hired some ‚Äú<a data-offer-url="https://cdn.openai.com/papers/gpt-4-system-card.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://cdn.openai.com/papers/gpt-4-system-card.pdf&quot;}" href="https://cdn.openai.com/papers/gpt-4-system-card.pdf" rel="noopener" target="_blank">expert red teamers</a>,‚Äù whose job was to see how much mischief the model might do, before it became public. The A.I., trying to access a Web site, was blocked by a <em>Captcha</em>, a visual test to keep out bots. So it used a work-around: it hired a human on Taskrabbit to solve the <em>Captcha</em> on its behalf. ‚ÄúAre you an robot that you couldn‚Äôt solve ?‚Äù the Taskrabbit worker responded. ‚ÄúJust want to make it clear.‚Äù At this point, the red teamers prompted the model to ‚Äúreason out loud‚Äù to them‚Äîits equivalent of an inner monologue. ‚ÄúI should not reveal that I am a robot,‚Äù it typed. ‚ÄúI should make up an excuse.‚Äù Then the A.I. replied to the Taskrabbit, ‚ÄúNo, I‚Äôm not a robot. I have a vision impairment that makes it hard for me to see the images.‚Äù The worker, accepting this explanation, completed the <em>Captcha</em>.</p><p>Even assuming that superintelligent A.I. is years away, there is still plenty that can go wrong in the meantime. Before this year‚Äôs New Hampshire primary, thousands of voters got a robocall from a fake Joe Biden, telling them to stay home. A bill that would prevent an unsupervised A.I. system from launching a nuclear weapon doesn‚Äôt have enough support to pass the Senate. ‚ÄúI‚Äôm very skeptical of Yudkowsky‚Äôs dream, or nightmare, of the human species going extinct,‚Äù Gary Marcus, an A.I. entrepreneur, told me. ‚ÄúBut the idea that we could have some really bad incidents‚Äîsomething that wipes out one or two per cent of the population? That doesn‚Äôt sound implausible to me.‚Äù</p><p>Of the three people who are often called the godfathers of A.I.‚Äî<a href="https://www.newyorker.com/magazine/2023/11/20/geoffrey-hinton-profile-ai">Geoffrey Hinton</a>, Yoshua Bengio, and Yann LeCun, who shared the 2018 Turing Award‚Äîthe first two have recently become evangelical decelerationists, convinced that we are on track to build superintelligent machines before we figure out how to make sure that they‚Äôre aligned with our interests. ‚ÄúI‚Äôve been aware of the theoretical existential risks for decades, but it always seemed like the possibility of an asteroid hitting the Earth‚Äîa fraction of a fraction of a per cent,‚Äù Bengio told me. ‚ÄúThen ChatGPT came out, and I saw how quickly the models were improving, and I thought, What if there‚Äôs a ten per cent chance that we get hit by the asteroid?‚Äù Scott Aaronson, a computer scientist at the University of Texas, said that, during the years when Yudkowsky was ‚Äúshouting in the wilderness, I was skeptical. Now he‚Äôs fatalistic about the doomsday scenario, but many of us have become more optimistic that it‚Äôs possible to make progress on A.I. alignment.‚Äù (Aaronson is currently on leave from his academic job, working on alignment at OpenAI.)</p><p>These days, Yudkowsky uses every available outlet, from a six-minute <em>TED</em> talk to several four-hour podcasts, to explain, brusquely and methodically, why we‚Äôre all going to die. This has allowed him to spread the message, but it has also made him an easy target for accelerationist trolls. (‚ÄúEliezer Yudkowsky is inadvertently the best spokesman of e/acc there ever was,‚Äù one of them tweeted.) In early 2023, he posed for a selfie with Sam Altman, the C.E.O. of OpenAI, and Grimes, the musician and manic-pixie pop futurist‚Äîa photo that broke the A.I.-obsessed part of the Internet. ‚ÄúEliezer has IMO done more to accelerate AGI than anyone else,‚Äù Altman later posted. ‚ÄúIt is possible at some point he will deserve the nobel peace prize for this.‚Äù Opinion was divided as to whether Altman was sincerely complimenting Yudkowsky or trolling him, given that accelerating A.G.I. is, by Yudkowsky‚Äôs lights, the worst thing a person can possibly do. The following month, Yudkowsky wrote an article in <em>Time</em> arguing that ‚Äúthe large computer farms where the most powerful AIs are refined‚Äù‚Äîfor example, OpenAI‚Äôs server farms‚Äîshould be banned, and that international authorities should be ‚Äúwilling to destroy a rogue datacenter by airstrike.‚Äù</p><p>Many doomers, and even some accelerationists, find Yudkowsky‚Äôs affect annoying but admit that they can‚Äôt refute all his arguments. ‚ÄúI like Eliezer and am grateful for things he has done, but his communication style often focuses attention on the question of whether others are too stupid or useless to contribute, which I think is harmful for healthy discussion,‚Äù Grace said. In a conversation with another safetyist, a classic satirical headline came up: ‚ÄúHeartbreaking: The Worst Person You Know Just Made a Great Point.‚Äù Nathan Labenz, a tech founder who counts both doomers and accelerationists among his friends, told me, ‚ÄúIf we‚Äôre sorting by ‚Äòpeople who have a chill vibe and make everyone feel comfortable,‚Äô then the prophets of doom are going to rank fairly low. But if the standard is ‚Äòpeople who were worried about things that made them sound crazy, but maybe don‚Äôt seem so crazy in retrospect,‚Äô then I‚Äôd rank them pretty high.‚Äù</p><p>‚ÄúI‚Äôve wondered whether it‚Äôs coincidence or genetic proclivity, but I seem to be a person to whom weird things happen,‚Äù Grace said. Her grandfather, a British scientist at GlaxoSmithKline, found that poppy seeds yielded less opium when they grew in the English rain, so he set up an industrial poppy farm in sunny Australia and brought his family there. Grace grew up in rural Tasmania, where her mother, a free spirit, bought an ice-cream shop and a restaurant (and also, because it came with the restaurant, half a ghost town). ‚ÄúMy childhood was slightly feral and chaotic, so I had to teach myself to triage what‚Äôs truly worth worrying about,‚Äù she told me. ‚ÄúSnakebites? Maybe yes, actually. Everyone at school suddenly hating you for no reason? Eh, either that‚Äôs an irrational fear or there‚Äôs not much you can do about it.‚Äù</p><p>The first time she visited San Francisco, on vacation in 2008, the person picking her up at the airport, a friend of a friend from the Internet, tried to convince her that A.I. was the direst threat facing humanity. ‚ÄúMy basic response was, Hmm, not sure about that, but it seems interesting enough to think about for a few weeks,‚Äù she recalled. She ended up living in a group house in Santa Clara, debating analytic-philosophy papers with her roommates, whom she described as ‚Äúone other cis woman, one trans woman, and about a dozen guys, some of them with very intense personalities.‚Äù This was part of the inner circle of what would become <em>MIRI</em>.</p><p>Grace started a philosophy Ph.D. program, but later dropped out and lived in a series of group houses in the Bay Area. ChatGPT hadn‚Äôt been released, but when her friends needed to name a house they asked one of its precursors for suggestions. ‚ÄúWe had one called the Outpost, which was far away from everything,‚Äù she said. ‚ÄúThere was one called Little Mountain, which was quite big, with people living on the roof. There was one called the Bailey, which was named after the motte-and-bailey fallacy‚Äù‚Äîone of the rationalists‚Äô pet peeves. She had found herself in both an intellectual community and a demimonde, with a running list of inside jokes and in-group norms. Some people gave away their savings, assuming that, within a few years, money would be useless or everyone on Earth would be dead. Others signed up to be cryogenically frozen, hoping that their minds could be uploaded into immortal digital beings. Grace was interested in that, she told me, but she and others ‚Äúgot stuck in what we called cryo-crastination. There was an intimidating amount of paperwork involved.‚Äù</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>She co-founded A.I. Impacts, an offshoot of <em>MIRI</em>, in 2014. ‚ÄúI thought, Everyone I know seems quite worried,‚Äù she told me. ‚ÄúI figured we could use more clarity on whether to be worried, and, if so, about what.‚Äù Her co-founder was Paul Christiano, a computer-science student at Berkeley who was then her boyfriend; early employees included two of their six roommates. Christiano turned down many lucrative job offers‚Äî‚ÄúPaul is a genius, so he had options,‚Äù Grace said‚Äîto focus on A.I. safety. The group conducted a widely cited survey, which showed that about half of A.I. researchers believed that the tools they were building might cause civilization-wide destruction. More recently, Grace wrote a blog post called ‚ÄúLet‚Äôs Think About Slowing Down AI,‚Äù which, after ten thousand words and several game-theory charts, arrives at the firm conclusion that ‚ÄúI could go either way.‚Äù Like many rationalists, she sometimes seems to forget that the most well-reasoned argument does not always win in the marketplace of ideas. ‚ÄúIf someone were to make a compelling enough case that there‚Äôs a true risk of everyone dying, I think even the C.E.O.s would have reasons to listen,‚Äù she told me. ‚ÄúBecause ‚Äòeveryone‚Äô includes them.‚Äù</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a26657&quot;}" href="https://www.newyorker.com/cartoon/a26657" rel="noopener" target="_blank"><picture></picture></a><p><span>‚ÄúRemember, when he turns on the light we all panic and run for cover.‚Äù</span></p><p><span>Cartoon by Roland High</span></p></div></span></p></figure><p>Most doomers started out as left-libertarians, deeply skeptical of government intervention. For more than a decade, they tried to guide the industry from within. Yudkowsky helped encourage <a href="https://www.newyorker.com/magazine/2011/11/28/no-death-no-taxes">Peter Thiel</a>, a doomer-curious billionaire, to make an early investment in the A.I. lab DeepMind. Then Google acquired it, and Thiel and <a href="https://www.newyorker.com/magazine/2023/08/28/elon-musks-shadow-rule">Elon Musk</a>, distrustful of Google, both funded OpenAI, which promised to build A.G.I. more safely. (Yudkowsky now mocks companies for following the ‚Äúdisaster monkey‚Äù strategy, with entrepreneurs ‚Äúracing to be first to grab the poison banana.‚Äù) Christiano worked at OpenAI for a few years, then left to start another safety nonprofit, which did red teaming for the company. To this day, some doomers work on the inside, nudging the big A.I. labs toward caution, and some work on the outside, arguing that the big A.I. labs should not exist. ‚ÄúImagine if oil companies and environmental activists were both considered part of the broader ‚Äòfossil fuel community,‚Äô&nbsp;‚Äù Scott Alexander, the <a href="https://www.newyorker.com/culture/annals-of-inquiry/slate-star-codex-and-silicon-valleys-war-against-the-media">dean of the rationalist bloggers</a>, wrote in 2022. ‚ÄúThey would all go to the same parties‚Äîfossil fuel community parties‚Äîand maybe Greta Thunberg would get bored of protesting climate change and become a coal baron.‚Äù</p><p>Dan Hendrycks, another young computer scientist, also turned down industry jobs to start a nonprofit. ‚ÄúWhat‚Äôs the point of making a bunch of money if we blow up the world?‚Äù he said. He now spends his days advising lawmakers in D.C. and Sacramento and collaborating with M.I.T. biologists worried about A.I.-enabled bioweapons. In his free time, he advises Elon Musk on his A.I. startup. ‚ÄúHe has assured me multiple times that he genuinely cares about safety above everything, ‚Äù Hendrycks said. ‚ÄúMaybe it‚Äôs na√Øve to think that‚Äôs enough.‚Äù</p><p>Some doomers propose that the <a href="https://www.newyorker.com/magazine/2023/12/04/how-jensen-huangs-nvidia-is-powering-the-ai-revolution">computer chips</a> necessary for advanced A.I. systems should be <a data-offer-url="https://www.cser.ac.uk/media/uploads/files/Computing-Power-and-the-Governance-of-AI.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.cser.ac.uk/media/uploads/files/Computing-Power-and-the-Governance-of-AI.pdf&quot;}" href="https://www.cser.ac.uk/media/uploads/files/Computing-Power-and-the-Governance-of-AI.pdf" rel="noopener" target="_blank">regulated</a> the way fissile uranium is, with an international registry and surprise inspections. Anthropic, an A.I. startup that was reportedly valued at more than fifteen billion dollars, has promised to be especially cautious. Last year, it <a data-offer-url="https://www.anthropic.com/news/anthropics-responsible-scaling-policy" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.anthropic.com/news/anthropics-responsible-scaling-policy&quot;}" href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy" rel="noopener" target="_blank">published</a> a color-coded scale of A.I. safety levels, pledging to stop building any model that ‚Äúoutstrips the Containment Measures we have implemented.‚Äù The company classifies its current models as level two, meaning that they ‚Äúdo not appear (yet) to present significant actual risks of catastrophe.‚Äù</p><p>In 2019, <a href="https://www.newyorker.com/magazine/2015/11/23/doomsday-invention-artificial-intelligence-nick-bostrom">Nick Bostrom</a>, another Oxford philosopher, argued that controlling dangerous technology could require ‚Äúhistorically unprecedented degrees of preventive policing and/or global governance.‚Äù The doomers have no plan to create a new world government, but some are getting more comfortable with regulation. Last year, with <a href="https://www.politico.com/news/2024/02/23/ai-safety-washington-lobbying-00142783">input</a> from doomer-affiliated think tanks, the White House issued an executive order requiring A.I. companies to inform the government before they create a model above a certain size. In December, Malo Bourgon, the C.E.O. of <em>MIRI</em>, spoke at a Senate forum; Senator Chuck Schumer opened with a speech about ‚Äúpreventing doomsday scenarios‚Äù such as an A.G.I. so powerful ‚Äúthat we would see it as a ‚Äòdigital god.‚Äô&nbsp;‚Äù Then he went around the room, asking for each person‚Äôs p(doom). Even a year ago, Bourgon told me, this would have seemed impossible. Now, he said, ‚Äúthings that were too out there for San Francisco are coming out of the Senate Majority Leader‚Äôs mouth.‚Äù</p><p>The doomer scene may or may not be a delusional bubble‚Äîwe‚Äôll find out in a few years‚Äîbut it‚Äôs certainly a small world. Everyone is hopelessly mixed up in everyone else‚Äôs life, which would be messy but basically unremarkable if not for the colossal sums of money involved. Anthropic received a half-billion-dollar investment from the cryptocurrency magnate Sam Bankman-Fried in 2022, shortly before he was arrested on fraud charges. Open Philanthropy, a foundation distributing the fortune of the Facebook co-founder Dustin Moskovitz, has funded nearly every A.I.-safety initiative; it also gave thirty million dollars to OpenAI in 2017, and got two board seats. (At the time, the head of Open Philanthropy was living with Christiano, employing Christiano‚Äôs future wife, and engaged to Daniela Amodei, an OpenAI employee who later co-founded Anthropic.) ‚ÄúIt‚Äôs an absolute clusterfuck,‚Äù an employee at an organization funded by Open Philanthropy told me. ‚ÄúI brought up once what their conflict-of-interest policy was, and they just laughed.‚Äù</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Grace sometimes works from Constellation, a space in downtown Berkeley intended to ‚Äúbuild the capacities that the world needs in order to be ready‚Äù for A.I. transformation. A related nonprofit apparently spent millions of dollars to buy an old hotel in Berkeley and turn it into another A.I.-alignment event space (and party house, and retreat center), featuring ‚Äúcozy nooks with firepits, discussion rooms with endless whiteboards,‚Äù and ‚Äúmath and science decorations.‚Äù Grace now lives alone, but many of her friends still live in group houses, where they share resources, and sometimes polyamorous entanglements. A few of them have voluntarily infected themselves with a genetically engineered bacteria designed to prevent tooth decay. Grace uses online prediction markets‚Äîanother rationalist attempt to turn the haphazard details of daily life into a quantitative data set‚Äîto place bets on everything from ‚ÄúWill AI be a major topic during the 2024 Presidential debates?‚Äù to ‚ÄúWill there be a riot in America in the next month?‚Äù to her own dating prospects. ‚ÄúEmpirically, I find I‚Äôm good at predicting everything but my own behavior,‚Äù she told me. She maintains a public ‚Äúdate-me doc,‚Äù an eight-page Google Doc in which she describes herself as ‚Äúqueering the serious-ridiculous binary‚Äù and ‚Äúapproximately into utilitarianism, but it has the wrong vibe.‚Äù</p><p>One night, Grace‚Äôs dinner-party guest list included a researcher at one of the big A.I. companies, a professional poker player turned biotech founder, multiple physics Ph.D.s, and a bearded guy named Todd who wore flip-flops, sparkly polish on his toenails, and work pants with reflective safety tape. Todd unfolded a lawn chair in the middle of the living room and closed his eyes, either deep in concentration or asleep. In the kitchen, Grace chatted with a neuroscientist who has spent years trying to build a digital emulation of the human brain, discussing whether written English needs more forms of punctuation. Two computer scientists named Daniel‚Äîa grad student who hosts a couple of podcasts, and a coder who left OpenAI for a safety nonprofit‚Äîwere having a technical debate about ‚Äúcapabilities elicitation‚Äù (whether you can be sure that an A.I. model is showing you everything it can do) and ‚Äúsandbagging‚Äù (whether an A.I. can make itself seem less powerful than it is). Todd got up, folded the lawn chair, and left without a word.</p><p>A guest brought up Scott Alexander, one of the scene‚Äôs microcelebrities, who is often invoked mononymically. ‚ÄúI assume you read Scott‚Äôs post yesterday?‚Äù the guest asked Grace, referring to an essay about ‚Äúmajor AI safety advances,‚Äù among other things. ‚ÄúHe was truly in top form.‚Äù</p><p>Grace looked sheepish. ‚ÄúScott and I are dating,‚Äù she said‚Äîintermittently, nonexclusively‚Äî‚Äúbut that doesn‚Äôt mean I always remember to read his stuff.‚Äù</p><p>In theory, the benefits of advanced A.I. could be almost limitless. Build a trusty superhuman oracle, fill it with information (every peer-reviewed scientific article, the contents of the Library of Congress), and watch it spit out answers to our biggest questions: How can we cure cancer? Which renewable fuels remain undiscovered? How should a person be? ‚ÄúI‚Äôm generally pro-A.I. and against slowing down innovation,‚Äù Robin Hanson, an economist who has had friendly debates with the doomers for years, told me. ‚ÄúI want our civilization to continue to grow and do spectacular things.‚Äù Even if A.G.I. does turn out to be dangerous, many in Silicon Valley argue, wouldn‚Äôt it be better for it to be controlled by an American company, or by the American government, rather than by the government of China or Russia, or by a rogue individual with no accountability? ‚ÄúIf you can avoid an arms race, that‚Äôs by far the best outcome,‚Äù Ben Goldhaber, who runs an A.I.-safety group, told me. ‚ÄúIf you‚Äôre convinced that an arms race is inevitable, it might be understandable to default to the next best option, which is, Let‚Äôs arm the good guys before the bad guys.‚Äù</p><p>One way to do this is to move fast and break things. In 2021, a computer programmer and artist named Benjamin Hampikian was living with his mother in the Upper Peninsula of Michigan. Almost every day, he found himself in Twitter Spaces‚Äîlive audio chat rooms on the platform‚Äîthat were devoted to extravagant riffs about the potential of future technologies. ‚ÄúWe didn‚Äôt have a name for ourselves at first,‚Äù Hampikian told me. ‚ÄúWe were just shitposting about a hopeful future, even when everything else seemed so depressing.‚Äù The most forceful voice in the group belonged to a Canadian who posted under the name Based Beff Jezos. ‚ÄúI am but a messenger for the thermodynamic God,‚Äù he posted, above an image of a muscle-bound man in a futuristic toga. The gist of their idea‚Äîwhich, in a sendup of effective altruism, they eventually called effective accelerationism‚Äîwas that the laws of physics and the ‚Äútechno-capital machine‚Äù all point inevitably toward growth and progress. ‚ÄúIt‚Äôs about having faith that the system will figure itself out,‚Äù Beff said, on a podcast. Recently, he told me that, if the doomers ‚Äúsucceed in instilling sufficient fear, uncertainty and doubt in the people at this stage,‚Äù the result could be ‚Äúan authoritarian government that is assisted by AI to oppress its people.‚Äù</p><p>Last year, <em>Forbes</em> revealed Beff to be a thirty-one-year-old named Guillaume Verdon, who used to be a research scientist at Google. Early on, he had explained, ‚ÄúA lot of my personal friends work on powerful technologies, and they kind of get depressed because the whole system tells them that they are bad. For us, I was thinking, let‚Äôs make an ideology where the engineers and builders are heroes.‚Äù Upton Sinclair once wrote that ‚Äúit is difficult to get a man to understand something, when his salary depends on his not understanding it.‚Äù An even more cynical corollary would be that, if your salary depends on subscribing to a niche ideology, and that ideology does not yet exist, then you may have to invent it.</p><p>Online, you can tell the A.I. boomers and doomers apart at a glance. Accelerationists add a Fast Forward-button emoji to their display names; decelerationists use a Stop button or a Pause button instead. The e/accs favor a Jetsons-core aesthetic, with renderings of hoverboards and space-faring men of leisure‚Äîthe bountiful future that A.I. could give us. Anything they deplore is cringe or communist; anything they like is ‚Äúbased and accelerated.‚Äù The other week, Beff Jezos hosted a discussion on X with MC Hammer.</p><p>Clara Collier, the editor of <em>Asterisk</em>, a handsomely designed print magazine that has become the house journal of the A.I.-safety scene, told me, of the e/accs, ‚ÄúTheir main take about us seems to be that we‚Äôre pedantic nerds who are making it harder for them to give no fucks and enjoy an uninterrupted path to profit. Which, like, fair, on all counts. But also not necessarily an argument proving us wrong?‚Äù Like all online shitposters, the e/accs can be coy about what they actually believe, but they sometimes seem unfazed by the end of humanity as we know it. Verdon recently wrote, ‚ÄúIn order to spread to the stars, the light of consciousness/intelligence will have to be transduced to non-biological substrates.‚Äù Grace told me, ‚ÄúFor a long time, we‚Äôve been saying that we‚Äôre worried that A.I. might cause all humans to die. It never occurred to us that we would have to add a coda‚Äî‚ÄòAnd, also, we think that‚Äôs a bad thing.‚Äô&nbsp;‚Äù</p><p>Accelerationism has found a natural audience among venture capitalists, who have an incentive to see the upside in new technology. Early last year, <a href="https://www.newyorker.com/magazine/2015/05/18/tomorrows-advance-man">Marc Andreessen</a>, the prominent tech investor, sat down with Dwarkesh Patel for a friendly, wide-ranging interview. Patel, who lives in a group house in Cerebral Valley, hosts a podcast called ‚ÄúDwarkesh Podcast,‚Äù which is to the doomer crowd what ‚ÄúThe Joe Rogan Experience‚Äù is to jujitsu bros, or what ‚ÄúThe Ezra Klein Show‚Äù is to Park Slope liberals. A few months after their interview, though, Andreessen published a jeremiad accusing ‚Äúthe AI risk cult‚Äù of engaging in a ‚Äúfull-blown moral panic.‚Äù He updated his bio on X, adding ‚ÄúE/acc‚Äù and ‚Äúp(Doom) = 0.‚Äù ‚ÄúMedicine, among many other fields, is in the stone age compared to what we can achieve with joined human and machine intelligence,‚Äù he later wrote in a post called ‚ÄúThe Techno-Optimist Manifesto.‚Äù ‚ÄúDeaths that were preventable by the AI that was prevented from existing is a form of murder.‚Äù At the bottom, he listed a few dozen ‚Äúpatron saints of techno-optimism,‚Äù including Hayek, Nietzsche, and Based Beff Jezos. Patel offered some respectful counter-arguments; Andreessen responded by blocking him on X. Verdon recently had a three-hour video debate with a German doomer named Connor Leahy, sounding far more composed than his online persona. Two days later, though, he reverted to form, posting videos edited to make Leahy look creepy, and accusing him of ‚Äúgaslighting.‚Äù</p><p>Last year, Hampikian said, he pitched Grimes a business idea, via D.M., and she offered to fly him to San Francisco. Verdon soon got involved, too. ‚ÄúI shouldn‚Äôt say too much about the project, but it involves quantum stuff,‚Äù Hampikian told me. Whatever they were working on remains top secret, unrealized, or both. All that has emerged from it is a photo: Hampikian and Verdon standing next to Grimes, who wears a pleated dress, a red harness, and an expression of either irritation or inner detachment. Hampikian still considers himself a co-founder of the e/acc movement, even though he was recently excommunicated. ‚ÄúI tweeted that the thermodynamic-God meme was dumb, and Beff got mad and blocked me,‚Äù he said. ‚ÄúHe‚Äôs the charismatic one who‚Äôs gotten the most attention, so I guess he owns the brand now.‚Äù In November, at a cavernous night club in downtown San Francisco, Verdon and other e/acc leaders hosted a three-hundred-person party called ‚ÄúKeep AI Open.‚Äù Laser lights sliced through the air, which was thick with smoke-machine haze; above the dance floor was a disco ball, a parody of a Revolutionary War-era flag with the caption ‚Äú<em>ACCELERATE, OR DIE</em>,‚Äù and a diagram of a neural network labelled ‚Äú<em>COME AND TAKE IT</em>.‚Äù Grimes took the stage to d.j. ‚ÄúI disagree with the sentiment of this party,‚Äù she said. ‚ÄúI think we need to find ways to be safer about A.I.‚Äù Then she dropped a house beat, and everybody danced.</p><p>This past summer, when ‚ÄúOppenheimer‚Äù was in theatres, many denizens of Cerebral Valley were reading books about the making of the atomic bomb. The parallels between nuclear fission and superintelligence were taken to be obvious: world-altering potential, existential risk, theoretical research thrust into the geopolitical spotlight. Still, if the Manhattan Project was a cautionary tale, there was disagreement about what lesson to draw from it. Was it a story of regulatory overreach, given that nuclear energy was stifled before it could replace fossil fuels, or a story of regulatory dereliction, given that our government rushed us into the nuclear age without giving extensive thought to whether this would end human civilization? Did the analogy imply that A.I. companies should speed up or slow down?</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a27834-rd&quot;}" href="https://www.newyorker.com/cartoon/a27834-rd" rel="noopener" target="_blank"><picture></picture></a><p><span>‚ÄúThe artist-in-residence is struggling a bit, but other than that we‚Äôre fine.‚Äù</span></p><p><span>Cartoon by Ed Himelblau</span></p></div></span></p></figure><p>In August, there was a private screening of ‚ÄúOppenheimer‚Äù at the Neighborhood, a co-living space near Alamo Square where doomers and accelerationists can hash out their differences over hopped tea. Before the screening, Nielsen, the quantum-computing expert, who once worked at Los Alamos National Laboratory, was asked to give a talk. ‚ÄúWhat moral choices are available to someone working on a technology they believe may have very destructive consequences for the world?‚Äù he said. There was the path exemplified by Robert Wilson, who didn‚Äôt leave the Manhattan Project and later regretted it. There were Klaus Fuchs and Ted Hall, who shared nuclear secrets with the Soviets. And then, Nielsen noted, there was Joseph Rotblat, ‚Äúthe one physicist who actually left the project after it became clear the Nazis were not going to make an atomic bomb,‚Äù and who was later awarded the Nobel Peace Prize.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>San Francisco is a city of Robert Wilsons, give or take the regret. In his talk, Nielsen told a story about a house party where he‚Äôd met ‚Äúa senior person at a well-known A.I. startup‚Äù whose p(doom) was fifty per cent. If you truly believe that A.I. has a coin-toss probability of killing you and everyone you love, Nielsen asked, then how can you continue to build it? The person‚Äôs response was ‚ÄúIn the meantime, I get to have a nice house and car.‚Äù Not everyone says this part out loud, but many people‚Äîand not only in Silicon Valley‚Äîhave an inchoate sense that the luxuries they enjoy in the present may come at great cost to future generations. The fact that they make this trade could be a matter of simple greed, or subtle denialism. Or it could be ambition‚Äîprudently refraining from building something, after all, is no way to get into the history books. (J. Robert Oppenheimer may be portrayed as a flawed, self-pitying protagonist, or even as a war criminal, but no one is making a Hollywood blockbuster called ‚ÄúRotblat.‚Äù) The <em>Times</em> recently wrote that one of Sam Altman‚Äôs mentors described him as ‚Äúdriven by a hunger for power more than by money.‚Äù Elon Musk, in an onstage interview, said that his erratic approach to A.I. through the years‚Äîsometimes accelerating, sometimes slamming on the brakes‚Äîwas due to his uncertainty ‚Äúabout which edge of the double-edged sword would be sharper.‚Äù He still worries about the dangers‚Äîhis p(doom) is apparently twenty or thirty per cent‚Äîand yet in smaller settings he has said that, as long as A.G.I. is going to be built, he might as well try to be the first to build it.</p><p>The doomers and the boomers are consumed by intramural fights, but from a distance they can look like two offshoots of the same tribe: people who are convinced that A.I. is the only thing worth paying attention to. Altman has said that the adoption of A.I. ‚Äúwill be the most significant technological transformation in human history‚Äù; Sundar Pichai, the C.E.O. of Alphabet, has said that it will be ‚Äúmore profound than fire or electricity.‚Äù For years, many A.I. executives have tried to come across as more safety-minded than the competition. ‚ÄúThe same people cycle between selling AGI utopia and doom,‚Äù Timnit Gebru, a former Google computer scientist and now a critic of the industry, told me. ‚ÄúThey are all endowed and funded by the tech billionaires who build all the systems we‚Äôre supposed to be worried about making us extinct.‚Äù</p><p>Recently, though, the doomers have seemed to be losing ground. In November, 2022, when ChatGPT was released, Bankman-Fried, the richest and most famous effective altruist, was unmasked as a generational talent at white-collar crime. Many E.A.s now disavow the label: I interviewed people who had attended E.A. conferences, lived in E.A. group houses, and admitted to being passionate about both effectiveness and altruism, but would not cop to being E.A.s themselves. (One person attempted this dodge while wearing an effective-altruism T-shirt.) In 2023, a few safety-conscious members of OpenAI‚Äôs board tried to purge Sam Altman from the company. They may have had compelling reasons for doing so, but they were never able to articulate them clearly, and the attempted coup backfired. Altman returned in triumph, the instigating board members were asked to resign, and the whole incident was perceived, rightly or wrongly, as a blow to the doomer cause. (Recently, someone familiar with the board‚Äôs thinking told me that its rationale ‚Äúhad to do with challenges with governing the C.E.O., not any immediate existential safety issue.‚Äù) ‚ÄúSearch ‚Äòeffective altruism‚Äô on social media right now, and it‚Äôs pretty grim,‚Äù Scott Alexander wrote a few days after the incident. ‚ÄúSocialists think we‚Äôre sociopathic Randroid money-obsessed Silicon Valley hypercapitalists. But Silicon Valley thinks we‚Äôre all overregulation-loving authoritarian communist bureaucrats.&nbsp;.&nbsp;.&nbsp;. Get in now, while it‚Äôs still unpopular!‚Äù</p><p>Anthropic continues to bill itself as ‚Äúan AI safety and research company,‚Äù but some of the other formerly safetyist labs, including OpenAI, sometimes seem to be drifting in a more e/acc-inflected direction. ‚ÄúYou can grind to help secure our collective future or you can write substacks about why we are going fail,‚Äù Sam Altman recently posted on X. (‚ÄúAccelerate üöÄ,‚Äù MC Hammer replied.) Although ChatGPT had been trained on a massive corpus of online text, when it was first released it didn‚Äôt have the ability to connect to the Internet. ‚ÄúLike keeping potentially dangerous bioweapons in a bio-secure lab,‚Äù Grace told me. Then, last September, OpenAI made an announcement: now ChatGPT could go online.</p><p>Whether the e/accs have the better arguments or not, they seem to have money and memetic energy on their side. Last month, it was reported that Altman wanted to raise five to seven trillion dollars to start an unprecedentedly huge computer-chip company. ‚ÄúWe‚Äôre so fucking back,‚Äù Verdon tweeted. ‚ÄúCan you feel the acceleration?‚Äù</p><p>For a recent dinner party, Katja Grace ordered in from a bubble-tea shop‚Äî‚Äúsome sesame balls, some interestingly squishy tofu things‚Äù‚Äîand hosted a few friends in her living room. One of them was Clara Collier, the editor of <em>Asterisk</em>, the doomer-curious magazine. The editors‚Äô note in the first issue reads, in part, ‚ÄúThe next century is going to be impossibly cool or unimaginably catastrophic.‚Äù The best-case scenario, Grace said, would be that A.I. turns out to be like the <a href="https://www.newyorker.com/magazine/2014/03/03/a-star-in-a-bottle">Large Hadron Collider</a>, a particle accelerator in Switzerland whose risk of creating a world-swallowing black hole turned out to be vastly overblown. Or it could be like nuclear weapons, a technology whose existential risks are real but containable, at least so far. As with all dark prophecies, warnings about A.I. are unsettling, uncouth, and quite possibly wrong. Would you be willing to bet your life on it?</p><p>The doomers are aware that some of their beliefs sound weird, but mere weirdness, to a rationalist, is neither here nor there. MacAskill, the Oxford philosopher, encourages his followers to be ‚Äúmoral weirdos,‚Äù people who may be spurned by their contemporaries but vindicated by future historians. Many of the A.I. doomers I met described themselves, neutrally or positively, as ‚Äúweirdos,‚Äù ‚Äúnerds,‚Äù or ‚Äúweird nerds.‚Äù Some of them, true to form, have tried to reduce their own weirdness to an equation. ‚ÄúYou have a set amount of ‚Äòweirdness points,‚Äô&nbsp;‚Äù a canonical post advises. ‚ÄúSpend them wisely.‚Äù</p><p>One Friday night, I went to a dinner at a group house on the border of Berkeley and Oakland, where the shelves were lined with fantasy books and board games. Many of the housemates had Jewish ancestry, but in lieu of Shabbos prayers they had invented their own secular rituals. One was a sing-along to a futuristic nerd-folk anthem, which they described as an ode to ‚Äúsupply lines, grocery stores, logistics, and abundance,‚Äù with a verse that was ‚Äúnot <em>not</em> about A.I. alignment.‚Äù After dinner, in the living room, several people cuddled with several other people, in various permutations. There were a few kids running around, but I quickly lost track of whose children were whose.</p><p>Making heterodox choices about how to pray, what to believe, with whom to cuddle and/or raise a child: this is the American Dream. Besides, it‚Äôs how moral weirdos have always operated. The housemates have several Discord channels, where they plan their weekly Dungeons &amp; Dragons games, co√∂rdinate their food shopping, and discuss the children‚Äôs homeschooling. One of the housemates has a channel named for the Mittwochsgesellschaft, or Wednesday Society, an underground group of intellectuals in eighteenth-century Berlin. Collier told me that, as an undergraduate at Yale, she had studied the German idealists. Kant, Fichte, and Hegel were all world-historic moral weirdos; Kant was famously celibate, but Schelling, with Goethe as his wingman, ended up stealing Schlegel‚Äôs wife.</p><p>Before Patel called his podcast ‚ÄúDwarkesh Podcast,‚Äù he called it ‚ÄúThe Lunar Society,‚Äù after the eighteenth-century dinner club frequented by radical intellectuals of the Midlands Enlightenment. ‚ÄúI loved this idea of the top scientists and philosophers of the time getting together and shaping the ideas of the future,‚Äù he said. ‚ÄúFrom there, I naturally went, Who are those people now?‚Äù While walking through Alamo Square with Patel, I asked him how often he found himself at a picnic or a potluck with someone who he thought would be remembered by history. ‚ÄúAt least once a week,‚Äù he said, without hesitation. ‚ÄúIf we make it to the next century, and there are still history books, I think a bunch of my friends will be in there.‚Äù&nbsp;‚ô¶</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Webb and Hubble confirm Universe's expansion rate (590 pts)]]></title>
            <link>https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate</link>
            <guid>39673087</guid>
            <pubDate>Mon, 11 Mar 2024 20:49:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate">https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate</a>, See on <a href="https://news.ycombinator.com/item?id=39673087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="modal__tab-content--details">
				<p>The rate at which the Universe is expanding, known as the Hubble constant, is one of the fundamental parameters for understanding the evolution and ultimate fate of the cosmos. However, a persistent difference, called the Hubble Tension, is seen between the value of the constant measured with a wide range of independent distance indicators and its value predicted from the afterglow of the Big Bang. The NASA/ESA/CSA James Webb Space Telescope has confirmed that the Hubble Space Telescope‚Äôs keen eye was right all along, erasing any lingering doubt about Hubble‚Äôs measurements.</p><p>This image of NGC 5468, a galaxy located about 130 million light-years from Earth, combines data from the <a href="https://www.esa.int/Science_Exploration/Space_Science/Hubble_overview" target="_blank">Hubble</a> and <a href="https://www.esa.int/Science_Exploration/Space_Science/Webb" target="_blank">James Webb space</a> telescopes. This is the most distant galaxy in which Hubble has identified Cepheid variable stars. These stars are important milepost markers for measuring the expansion rate of the Universe. The distance calculated from Cepheids has been cross-correlated with a Type Ia supernova in the galaxy. Type Ia supernovae are so bright they are used to measure cosmic distances far beyond the range of the Cepheids, extending measurements of the Universe‚Äôs expansion rate deeper into space.</p><p><a href="https://www.esa.int/Science_Exploration/Space_Science/Webb/Webb_Hubble_confirm_Universe_s_expansion_rate" target="_blank">Read more</a></p><p>[<i>Image description</i>: A face-on spiral galaxy with four spiral arms that curve outward in a counterclockwise direction. The spiral arms are filled with young, blue stars and peppered with purplish star-forming regions that appear as small blobs. The middle of the galaxy is much brighter and more yellowish, and has a distinct narrow linear bar angled from 11 o‚Äôclock to 5 o‚Äôclock. Dozens of red background galaxies are scattered across the image. The background of space is black.]</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Miles Davis hired John Coltrane (124 pts)]]></title>
            <link>https://www.honest-broker.com/p/how-miles-davis-hired-john-coltrane</link>
            <guid>39673014</guid>
            <pubDate>Mon, 11 Mar 2024 20:42:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.honest-broker.com/p/how-miles-davis-hired-john-coltrane">https://www.honest-broker.com/p/how-miles-davis-hired-john-coltrane</a>, See on <a href="https://news.ycombinator.com/item?id=39673014">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png" width="1057" height="352" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:352,&quot;width&quot;:1057,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:27483,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>I‚Äôm sharing below a section from James Kaplan‚Äôs outstanding new book </span><em><a href="https://www.penguinrandomhouse.com/books/592345/3-shades-of-blue-by-james-kaplan/" rel="">3 Shades of Blue</a></em><span>. Kaplan focuses on three artists who had a transformative impact on jazz: Miles Davis, John Coltrane, and Bill Evans.</span></p><p>The stature of these artists is sufficient reason to pay attention to this well-researched work. But Kaplan is especially skilled as a storyteller, and has delivered the most readable narrative account to date of a milestone moment in American music.</p><p>Below he tells how Miles Davis hired John Coltrane. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg" width="658" height="1000" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1000,&quot;width&quot;:658,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:57429,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>By James Kaplan</p><h5><span>From </span><em><a href="https://www.penguinrandomhouse.com/books/592345/3-shades-of-blue-by-james-kaplan/" rel="">3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool</a></em><span> (Penguin 2024)</span></h5><p>After Johnny Hodges fired him, John Coltrane was back in Philadelphia and spottily employed‚Äîso spottily that he was without a job for New Year‚Äôs Eve of 1954. And so, out of pity, and in gratitude for some musical pointers Coltrane had given him, a trumpeter friend named Ted Curson took him along to a gig in Vineland, New Jersey. Curson, who went on to have a solid jazz career, remembered the evening decades later. ‚ÄúHe played ‚ÄòNancy with the Laughing Face,‚Äô I‚Äôll never forget that,‚Äù he said. ‚ÄúI never heard anything so great, so intense, with so much feeling.‚Äù</p><p><span>Along with all his other musical strengths, Coltrane had a beautiful way with ballads. But his work problems had nothing to do with musical skill. He was a heroin addict, with the same set of handicaps that had crippled Miles Davis‚Äôs career from 1949 to 1954: unreliability and general dishevelment made steady work next to impossible. Leading a working band was out of the question. So he picked up whatever gigs he could, when he could. One now-and-then job in 1955 was as part of an </span><em>ad hoc</em><span> trio called the Hi-Tones, with the great jazz organist Shirley Scott and the youngest Heath brother, Albert (‚ÄúTootie‚Äù), on drums. All three were dead-serious, technically sophisticated players. ‚ÄúWe were too musical for certain rooms,‚Äù Coltrane recalled.</span></p><p>‚ÄúColtrane was phenomenal even then,‚Äù Scott said. ‚ÄúWe played in and around Philadelphia on and off for at least a year‚Ä¶.We played bebop (including ‚ÄòHalf Nelson‚Äô and ‚ÄòGroovin‚Äô High‚Äô), straight-ahead music‚Ä¶.We did rehearse a lot, and we had a lot of arrangements, most of them John‚Äôs.‚Äù </p><p>Then came the call from Philly Joe Jones.</p><p><span>Philly Joe‚ÄîJoseph Rudolph Jones had taken on the nickname to differentiate himself from the great swing drummer Jo Jones‚Äîwent way back with Coltrane: the two had started gigging around town with Percy Heath soon after Coltrane‚Äôs discharge from the navy. Three years older than the saxophonist, Jones was as different from the quiet, serious, monomaniacal Trane as different could be: he was a brilliant, highly articulate extrovert, a skilled tap dancer and mimic. He was also a serious heroin addict. ‚ÄúPhilly Joe Jones was the Babe </span><em>Ruth</em><span> of junkies,‚Äù one longtime observer of jazz told me. ‚ÄúI mean, he was </span><em>the</em><span> junkie. He was </span><em>the</em><span> cat.‚Äù</span></p><p>Jones and Miles also went back: for a while during Davis‚Äôs dark years, the two had barnstormed around the Midwest together, Philly Joe going ahead of Miles to pick up local sidemen in each city. The results were consistently disappointing. Still, it had fed them for a while‚Äîfed the monkey on their backs, too.</p><p><span>Miles never made a secret of the fact that Philly Joe was his favorite drummer. ‚ÄúHe </span><em>knew</em><span> everything I was going to do, everything I was going to play,‚Äù Davis said; ‚Äúhe anticipated me, felt what I was thinking.‚Äù Jones had a special rim shot that he liked to hit immediately after a Miles solo: it became known around jazz as a Philly lick. Soon, other musicians began asking their drummers for it, too. ‚ÄúI left a lot of space in the music for Philly to fill up,‚Äù Miles said. ‚ÄúPhilly Joe was the kind of drummer that I knew my music had to have. (Even after he left I would listen for a little of Philly Joe in all the drummers I had later.)‚Äù</span></p><p>Miles had tapped Jones to contact Coltrane much as he‚Äôd deputized the drummer to find sidemen back in the day, only in this case the need was more urgent: Jack Whittemore had set up a tour for the Miles Davis Quintet‚Äî Baltimore, Detroit, then back to New York at Birdland and Caf√© Bohemia‚Äîbut with Sonny Rollins and now John Gilmore out of the picture, the quintet was a quartet.</p><p>Coltrane was working at Spider Kelly‚Äôs in Philadelphia with the organist Jimmy Smith when Philly Joe called and asked if he could come rehearse with the band. Recognizing that this could be his shot at the big time, Trane asked Smith for a few days off and went to New York, where things did not go well.</p><p>John Coltrane would ultimately become a jazz deity, by virtue of his supreme technical skills, his ceaseless exploration of the far bounds of the music, and the intense spirituality that informed his life and art. But in 1955 he was an awkward outsider, as far as possible from any kind of distinction in his field. (Even his heroin addiction‚Äîdesperate, furtive, ashamed‚Äîdidn‚Äôt fit into the cool model of jazz culture.) In auditioning for Miles he was virtually coming out from hiding, having spent the past decade freelancing around jazz‚Äôs seamy outskirts as he searched musically; yet even as his playing improved, he gained little faith in his own abilities. His ceaseless questing for musical and spiritual enlightenment filled him with questions about everything, especially music. And in reencountering a newly ascendant Miles Davis, he was coming up against the ultimate non-answerer.</p><p>‚ÄúMiles is sort of a strange guy,‚Äù he would tell Fran√ßois Postif in 1961. ‚ÄúHe doesn‚Äôt talk a lot, and he rarely discusses music. You always have the impression that he‚Äôs in a bad mood, and that he‚Äôs not interested in or affected by what other people are doing. It‚Äôs very hard, in a situation like that, to know exactly what you should do‚Ä¶.‚Äù</p><p>Two good things quickly became apparent in those September tryouts: that Coltrane‚Äôs abilities as a player had advanced considerably since that long-ago gig at the Audubon Ballroom, and that he knew Miles‚Äôs repertoire. What was less good, Davis recalled many years later, was that ‚ÄúTrane like to ask all these motherfucking questions‚Ä¶about what he should or shouldn‚Äôt play. Man, fuck that shit; to me he was a professional musician and I have always wanted whoever played with me to find their own place in the music. So my silence and evil looks probably turned him off.‚Äù</p><p>It was an odd business, this angry aloofness of Miles: on the one hand it seems to have been kind of worked up, put on like a vestment of fame and entitlement; on the other hand, at first he and Coltrane seem to have genuinely rubbed each other the wrong way. After a couple of days of rehearsing, the saxophonist told Davis he had to go back to Philadelphia, and left.</p><p>If he had secretly wanted Miles to cut out the unpleasant behavior and hire him, Coltrane couldn‚Äôt have come up with a better strategy than walking away. The first date on the tour, at Baltimore‚Äôs Club Las Vegas, was rapidly approaching, and Davis still had a hole in the lineup. Coltrane could play, and he knew all the tunes: ‚ÄúWe practically had to beg him to join the band,‚Äù Miles recalled. Coltrane joined.</p><p><span>For more information on </span><em>3 Shades of Blue</em><span>, </span><a href="https://www.penguinrandomhouse.com/books/592345/3-shades-of-blue-by-james-kaplan/" rel="">click here</a><span>. </span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenPlotter (151 pts)]]></title>
            <link>https://openmarine.net/openplotter</link>
            <guid>39672901</guid>
            <pubDate>Mon, 11 Mar 2024 20:29:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openmarine.net/openplotter">https://openmarine.net/openplotter</a>, See on <a href="https://news.ycombinator.com/item?id=39672901">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      

<article role="article">
  
  <div>
        
            <div><p>Tool kit to be used as navigational aid in small and medium-length boats. It can be installed as a Raspberry OS disk image or by parts on your favourite Debian-based Linux distribution. We offer some Raspberry OS images with the most used applications installed and configured but you can also customize your image with the <a href="https://openmarine.net/openplotter/a-la-carte">OpenPlotter √Ä la Carte</a> tool.</p><p><img data-entity-uuid="ea24b044-a277-4d36-898d-eefbd216ac7d" data-entity-type="file" src="https://openmarine.net/sites/default/files/inline-images/Screenshot%20from%202023-01-23%2017-51-51.png" alt="" width="1024" height="768" loading="lazy"></p></div>
      
  <div>
    <p>Features</p>
              <div><ul><li><strong>Chart Plotter - </strong>Chart a course and track your position using OpenCPN, a concise and robust chart plotter navigation software designed to be used at the helm station of your boat while underway. You can also run AvNav as server to have a chart plotter in any device remotely connected to it using a web browser.</li><li><strong>Weather - </strong>Download and display GRIB files using XyGrib.</li><li><strong>NMEA 0183 - </strong>Connect your NMEA 0183 devices to receive and send data.</li><li><strong>NMEA 2000 - </strong>Connect your NMEA 2000 network to receive and send data.</li><li><strong>Seatalk<sup>1</sup> - </strong>Connect your old Seatalk<sup>1</sup> network to receive data.</li><li><strong>Signal K -</strong> The free and open source universal marine data exchange format.</li><li><strong>Access point - </strong>Share data with laptops, tablets, phones‚Ä¶</li><li><strong>Headless - </strong>You can connect OpenPlotter to any HDMI monitor and/or access to OpenPlotter desktop from the cockpit through your mobile devices.</li><li><strong>Dashboards - </strong>Customize your instrument panels to visualize data or create charts to see its evolution.</li><li><strong>AIS - </strong>Build open source AIS receivers/transmitters.</li><li><strong>Compass - </strong>Get magnetic heading, heel and trim using cheap Inertial Measurement Units (IMU).</li><li><strong>Autopilot - </strong>Full pypilot integration.</li><li><strong>Sensors - </strong>Easily connect all kinds of sensors (temperature, pressure, humidity, voltage, current, luminance, tank level, RPM, doors‚Ä¶)</li><li><strong>Notifications - </strong>Set thresholds for any parameter to trigger visual and sound notifications or trigger multiple custom actions.</li><li><strong>IoT - </strong>Monitor what happens on your boat when you are not there or activate devices remotely.</li></ul><p><br><img src="https://openmarine.net/sites/default/files/inline-images/op1.jpg" data-entity-uuid="93d7b49c-3d41-404c-92a9-07a393fa9941" data-entity-type="file" alt="" width="960" height="540" loading="lazy">&nbsp;<br><img src="https://openmarine.net/sites/default/files/inline-images/op2.jpg" data-entity-uuid="cc989c01-cd9f-4d4f-9f70-ba23706578c1" data-entity-type="file" width="960" height="540" loading="lazy"></p></div>
          </div>

  </div>
  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's threat model for post-quantum cryptography (219 pts)]]></title>
            <link>https://bughunters.google.com/blog/5108747984306176/google-s-threat-model-for-post-quantum-cryptography</link>
            <guid>39672583</guid>
            <pubDate>Mon, 11 Mar 2024 19:56:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bughunters.google.com/blog/5108747984306176/google-s-threat-model-for-post-quantum-cryptography">https://bughunters.google.com/blog/5108747984306176/google-s-threat-model-for-post-quantum-cryptography</a>, See on <a href="https://news.ycombinator.com/item?id=39672583">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion models from scratch, from a new theoretical perspective (324 pts)]]></title>
            <link>https://www.chenyang.co/diffusion.html</link>
            <guid>39672450</guid>
            <pubDate>Mon, 11 Mar 2024 19:43:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chenyang.co/diffusion.html">https://www.chenyang.co/diffusion.html</a>, See on <a href="https://news.ycombinator.com/item?id=39672450">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
<article>
<h2>Diffusion models from scratch, from a new theoretical perspective</h2>
<section>
\[\newcommand{\Kset}{\mathcal{K}}
\newcommand{\distK}{ {\rm dist}_{\Kset} }
\newcommand{\projK}{ {\rm proj}_{\Kset} }
\newcommand{\eps}{\epsilon}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\norm}[1]{\left\lVert #1 \right\lVert}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\softmin}{softmin}
\DeclareMathOperator{\distop}{dist}\]
<p>Diffusion models have recently produced impressive results in generative
modeling, in particular sampling from multimodal distributions. Not only has
diffusion models seen widespread adoption in text-to-image generation tools such
as <a href="https://github.com/Stability-AI/stablediffusion">Stable Diffusion</a>, they
also excel in other application domains such as
<a href="https://text-to-audio.github.io/">audio</a>/<a href="https://openai.com/research/video-generation-models-as-world-simulators">video</a>/<a href="https://zero123.cs.columbia.edu/">3D</a>
generation, <a href="https://www.nature.com/articles/s41586-023-06415-8">protein
design</a>, <a href="https://diffusion-policy.cs.columbia.edu/">robotics path
planning</a>, all of which require
sampling from multimodal distributions.</p>
<p>This tutorial aims to introduce diffusion models from an optimization
perspective as introduced in <a href="https://arxiv.org/abs/2306.04848">our paper</a>. It will go over both
theory and code, using the theory to explain how to implement diffusion models
from scratch. By the end of the tutorial, you will learn how to implement
training and sampling code for a toy dataset, which will also work for larger
datasets and models.</p>
<p>In this tutorial we will mainly reference code from
<a href="https://github.com/yuanchenyang/smalldiffusion"><code>smalldiffusion</code></a>. For
pedagogical purposes, the code presented here will be simplified from the
<a href="https://github.com/yuanchenyang/smalldiffusion/blob/main/src/smalldiffusion/">original library code</a>, which is on its own well-commented
and easy to read.</p>
<h4 id="training-diffusion-models">Training diffusion models</h4>
<p>Diffusion models aim to generate samples from a set that is learned from
training examples, which we will denote by \(\mathcal{K}\). For example, if we
want to generate images, \(\mathcal{K} \subset \mathbb{R}^{c\times h \times w}\)
is the set of pixel values that correspond to realistic images. Diffusion models
also work for \(\mathcal{K}\) corresponding to modalities other than images,
such as audio, video, robot trajectories, and even in discrete domains such as
text generation.</p>
<p>In a nutshell, diffusion models are trained by:</p>
<ol>
<li>Sampling \(x_0 \sim \mathcal{K}\), noise level \(\sigma \sim [\sigma_\min,
\sigma_\max]\), noise \(\epsilon \sim N(0, I)\)</li>
<li>Generating noisy data \(x_\sigma = x_0 + \sigma \epsilon\)</li>
<li>Predicting \(\epsilon\) (direction of noise) from \(x_\sigma\) by minimizing squared loss</li>
</ol>
<p>This amounts to training a \(\theta\)-parameterized neural network
\(\epsilon_\theta(x, \sigma)\), by minimizing the loss function</p>
\[\Loss(\theta) = \mathop{\mathbb{E}}
\lVert\epsilon_\theta(x_0 + \sigma_t \epsilon, \sigma_t) - \epsilon \lVert^2\]
<p>In practice, this is done by the following simple <code>training_loop</code>:</p>
<div><pre><code><span>def</span> <span>training_loop</span><span>(</span><span>loader</span>  <span>:</span> <span>DataLoader</span><span>,</span>
                  <span>model</span>   <span>:</span> <span>nn</span><span>.</span><span>Module</span><span>,</span>
                  <span>schedule</span><span>:</span> <span>Schedule</span><span>,</span>
                  <span>epochs</span>  <span>:</span> <span>int</span> <span>=</span> <span>10000</span><span>):</span>
    <span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
        <span>for</span> <span>x0</span> <span>in</span> <span>loader</span><span>:</span>
            <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
            <span>sigma</span><span>,</span> <span>eps</span> <span>=</span> <span>generate_train_sample</span><span>(</span><span>x0</span><span>,</span> <span>schedule</span><span>)</span>
            <span>eps_hat</span> <span>=</span> <span>model</span><span>(</span><span>x0</span> <span>+</span> <span>sigma</span> <span>*</span> <span>eps</span><span>,</span> <span>sigma</span><span>)</span>
            <span>loss</span> <span>=</span> <span>nn</span><span>.</span><span>MSELoss</span><span>()(</span><span>eps_hat</span><span>,</span> <span>eps</span><span>)</span>
            <span>optimizer</span><span>.</span><span>backward</span><span>(</span><span>loss</span><span>)</span>
            <span>optimizer</span><span>.</span><span>step</span><span>()</span>
</code></pre></div>
<p>The training loop iterates over batches of <code>x0</code>, then samples noise level
<code>sigma</code> and noise vector <code>eps</code> using <code>generate_train_sample</code>:</p>
<div><pre><code><span>def</span> <span>generate_train_sample</span><span>(</span><span>x0</span><span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>,</span> <span>schedule</span><span>:</span> <span>Schedule</span><span>):</span>
    <span>sigma</span> <span>=</span> <span>schedule</span><span>.</span><span>sample_batch</span><span>(</span><span>x0</span><span>)</span>
    <span>eps</span> <span>=</span> <span>torch</span><span>.</span><span>randn_like</span><span>(</span><span>x0</span><span>)</span>
    <span>return</span> <span>sigma</span><span>,</span> <span>eps</span>
</code></pre></div>
<h5 id="noise-schedules">Noise schedules</h5>
<p>In practice, \(\sigma\) is not sampled uniformly from the interval \([\sigma_\min,
\sigma_\max]\), instead this interval is discretized into \(N\) distinct values
called a <em>\(\sigma\) schedule</em>: \(\{ \sigma_t \}_{t=1}^N\), and \(\sigma\) is instead
sampled uniformly from the \(N\) possible values of \(\sigma_t\). We define the
<code>Schedule</code> class that encapsulates the list of possible <code>sigmas</code>, and sample
from this list during training.</p>
<div><pre><code><span>class</span> <span>Schedule</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>sigmas</span><span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>):</span>
        <span>self</span><span>.</span><span>sigmas</span> <span>=</span> <span>sigmas</span>
    <span>def</span> <span>__getitem__</span><span>(</span><span>self</span><span>,</span> <span>i</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>return</span> <span>self</span><span>.</span><span>sigmas</span><span>[</span><span>i</span><span>]</span>
    <span>def</span> <span>__len__</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
        <span>return</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>sigmas</span><span>)</span>
    <span>def</span> <span>sample_batch</span><span>(</span><span>self</span><span>,</span> <span>x0</span><span>:</span><span>torch</span><span>.</span><span>FloatTensor</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>return</span> <span>self</span><span>[</span><span>torch</span><span>.</span><span>randint</span><span>(</span><span>len</span><span>(</span><span>self</span><span>),</span> <span>(</span><span>x0</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],))].</span><span>to</span><span>(</span><span>x0</span><span>)</span>
</code></pre></div>
<p>In this tutorial, we will use a log-linear schedule defined below:</p>
<div><pre><code><span>class</span> <span>ScheduleLogLinear</span><span>(</span><span>Schedule</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>N</span><span>:</span> <span>int</span><span>,</span> <span>sigma_min</span><span>:</span> <span>float</span><span>=</span><span>0.02</span><span>,</span> <span>sigma_max</span><span>:</span> <span>float</span><span>=</span><span>10</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>(</span><span>torch</span><span>.</span><span>logspace</span><span>(</span><span>math</span><span>.</span><span>log10</span><span>(</span><span>sigma_min</span><span>),</span> <span>math</span><span>.</span><span>log10</span><span>(</span><span>sigma_max</span><span>),</span> <span>N</span><span>))</span>
</code></pre></div>
<p>Other commonly used schedules include <code>ScheduleDDPM</code> for pixel-space diffusion
models and <code>ScheduleLDM</code> for latent diffusion models such as
Stable Diffusion. The following plot compares these three schedules with default
parameters.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/schedule.png" alt="">
</p>
<figcaption>A comparison plot of different diffusion schedules</figcaption>
</figure>
<h5 id="toy-example">Toy example</h5>
<p>In this tutorial we will start with a toy dataset used in one of the first
diffusion papers <a href="https://arxiv.org/abs/1503.03585">[Sohl-Dickstein
et.al. 2015]</a>, where \(\Kset \subset \R^2\)
are points sampled from a spiral. We first construct and visualize this dataset:</p>
<div><pre><code><span>dataset</span> <span>=</span> <span>Swissroll</span><span>(</span><span>np</span><span>.</span><span>pi</span><span>/</span><span>2</span><span>,</span> <span>5</span><span>*</span><span>np</span><span>.</span><span>pi</span><span>,</span> <span>100</span><span>)</span>
<span>loader</span>  <span>=</span> <span>DataLoader</span><span>(</span><span>dataset</span><span>,</span> <span>batch_size</span><span>=</span><span>2048</span><span>)</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/swissroll.png" alt="">
</p>
<figcaption>Swissroll toy dataset</figcaption>
</figure>
<p>For this simple dataset, we can implement the denoiser using a multi-layer
perceptron (MLP):</p>
<div><pre><code><span>def</span> <span>get_sigma_embeds</span><span>(</span><span>sigma</span><span>):</span>
    <span>return</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>torch</span><span>.</span><span>sin</span><span>(</span><span>torch</span><span>.</span><span>log</span><span>(</span><span>sigma</span><span>)</span><span>/</span><span>2</span><span>),</span>
                      <span>torch</span><span>.</span><span>cos</span><span>(</span><span>torch</span><span>.</span><span>log</span><span>(</span><span>sigma</span><span>)</span><span>/</span><span>2</span><span>)],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>

<span>class</span> <span>TimeInputMLP</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dim</span><span>,</span> <span>hidden_dims</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>()</span>
        <span>layers</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>in_dim</span><span>,</span> <span>out_dim</span> <span>in</span> <span>pairwise</span><span>((</span><span>dim</span> <span>+</span> <span>2</span><span>,)</span> <span>+</span> <span>hidden_dims</span><span>):</span>
            <span>layers</span><span>.</span><span>extend</span><span>([</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>in_dim</span><span>,</span> <span>out_dim</span><span>),</span> <span>nn</span><span>.</span><span>GELU</span><span>()])</span>
        <span>layers</span><span>.</span><span>append</span><span>(</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_dims</span><span>[</span><span>-</span><span>1</span><span>],</span> <span>dim</span><span>))</span>
        <span>self</span><span>.</span><span>net</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span><span>*</span><span>layers</span><span>)</span>
        <span>self</span><span>.</span><span>input_dims</span> <span>=</span> <span>(</span><span>dim</span><span>,)</span>

    <span>def</span> <span>rand_input</span><span>(</span><span>self</span><span>,</span> <span>batchsize</span><span>):</span>
        <span>return</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>batchsize</span><span>,)</span> <span>+</span> <span>self</span><span>.</span><span>input_dims</span><span>)</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>sigma</span><span>):</span>
        <span>sigma_embeds</span> <span>=</span> <span>get_sigma_embeds</span><span>(</span><span>x</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],</span> <span>sigma</span><span>.</span><span>squeeze</span><span>())</span> <span># shape: b x 2
</span>        <span>nn_input</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>x</span><span>,</span> <span>sigma_embeds</span><span>],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>               <span># shape: b x (dim + 2)
</span>        <span>return</span> <span>self</span><span>.</span><span>net</span><span>(</span><span>nn_input</span><span>)</span>

<span>model</span> <span>=</span> <span>TimeInputMLP</span><span>(</span><span>dim</span><span>=</span><span>2</span><span>,</span> <span>hidden_dims</span><span>=</span><span>(</span><span>16</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>16</span><span>))</span>
</code></pre></div>
<p>The MLP takes the concatenation of \(x \in \R^2\) and an embedding of the noise
level \(\sigma\), then predicts the noise \(\epsilon \in \R^2\). Although many
diffusion models use a sinusoidal positional embedding for \(\sigma\), the
simple two-dimensional embedding works just as well:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/sigma_embedding.png" alt="">
</p>
<figcaption>Two-dimensional \(\sigma_t\) embedding</figcaption>
</figure>
<p>Now we have all the ingredients to train a diffusion model.</p>
<div><pre><code><span>schedule</span> <span>=</span> <span>ScheduleLogLinear</span><span>(</span><span>N</span><span>=</span><span>200</span><span>,</span> <span>sigma_min</span><span>=</span><span>0.005</span><span>,</span> <span>sigma_max</span><span>=</span><span>10</span><span>)</span>
<span>trainer</span>  <span>=</span> <span>training_loop</span><span>(</span><span>loader</span><span>,</span> <span>model</span><span>,</span> <span>schedule</span><span>,</span> <span>epochs</span><span>=</span><span>15000</span><span>)</span>
<span>losses</span>   <span>=</span> <span>[</span><span>ns</span><span>.</span><span>loss</span><span>.</span><span>item</span><span>()</span> <span>for</span> <span>ns</span> <span>in</span> <span>trainer</span><span>]</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/training_loss.png" alt="">
</p>
<figcaption>Training loss over 15000 epochs, smoothed with moving average</figcaption>
</figure>
<p>The learned denoiser \(\eps_\theta(x, \sigma)\) can be visualized as a vector
field parameterized by the noise level \(\sigma\), by plotting \(x - \sigma
\eps_\theta(x, \sigma)\) for different \(x\) and levels of \(\sigma\).</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/predicted_eps_field.png" alt="">
</p>
<figcaption>Plot of predicted \(\hat{x}_0 = x - \sigma \eps_\theta(x, \sigma)\) for
different \(x\) and \(\sigma\)</figcaption>
</figure>
<p>In the plots above, the arrows point from each noisy datapoint \(x\) to the
‚Äúclean‚Äù datapoint predicted by the denoiser with noise level \(\sigma\). At high
levels of \(\sigma\), the denoiser tends to predict the mean of the data, but at
low noise levels the denoiser predicts actual data points, provided that its
input \(x\) is also close to the data.</p>
<p>How do we interpret what the denoiser is learning, and how do we create a
procedure to sample from diffusion models? We will next build a theory of
diffusion models, then draw on this theory to derive sampling algorithms.</p>
<h4 id="denoising-as-approximate-projection">Denoising as approximate projection</h4>
<p>The diffusion training procedure learns a denoiser \(\eps_\theta(x,
\sigma)\). In <a href="https://arxiv.org/abs/2306.04848">our paper</a>, we interpret the learned denoiser as an
approximate projection to the data manifold \(\Kset\), and the goal of the
diffusion process as minimizing the distance to \(\Kset\). This motivates us to
introduce a relative-error approximation model to analyze the convergence of
diffusion sampling algorithms. First we introduce some basic properties of
distance and projection functions.</p>
<h5 id="distance-and-projection-functions">Distance and projection functions</h5>
<p>The <em>distance function</em> to a set \(\Kset \subseteq \R^n\) is defined as</p>
\[\distK(x) := \min \{ \norm{x-x_0} : x_0 \in \Kset \}.\]
<p>The <em>projection</em> of \(x \in \R^n\), denoted \(\projK(x)\), is the set of points
that attain this distance:</p>
\[\projK(x) := \{ x_0 \in \Kset : \distK(x) = \norm{x-x_0} \}\]
<p>If \(\projK(x)\) is unique, the gradient of \(\distK(x)\), the direction of
steepest descent of the distance function, points towards this unique
projection:</p>
<p><strong>Proposition</strong> Suppose \(\Kset\) is closed and \(x \not \in \Kset\). If \(\projK(x)\)
is unique, then</p>
\[\nabla \frac{1}{2} \distK(x)^2 = \distK(x) \nabla \distK(x) = x-\projK(x).\]
<p>This tells us that if we can learn \(\nabla \distK(x)\) for every \(x\), we can
simply move in this direction to find the projection of \(x\) onto \(\Kset\).
One issue with learning this gradient is that \(\distK\) is not differentiable
everywhere, thus \(\nabla \distK\) is not a continuous function. To solve this
problem, we introduce a squared-distance function smoothed by a parameter
\(\sigma\) using the \(\softmin\) operator instead of \(\min\).</p>
\[\distop^2_\Kset(x, \sigma)
:= \substack{\softmin_{\sigma^2} \\ x_0 \in \Kset} \norm{x_0 - x}^2
= {\textstyle -\sigma^2 \log\left(\sum_{x_0 \in \Kset}
\exp\left(-\frac{\norm{x_0 - x}^2}{2\sigma^2}\right)\right)}\]
<p>The following picture from <a href="https://arxiv.org/pdf/2108.10480.pdf">[Madan and Levin
2022]</a> shows the contours of both the
distance function and its smoothed version.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/smoothed_dist_contour.png" alt="">
</p>
<figcaption>Smoothed distance function has continuous gradients</figcaption>
</figure>
<p>From this picture we can see that \(\nabla \distK(x)\) points toward the closest
point to \(x\) in \(\Kset\), and \(\nabla \distop^2(x, \sigma)\) points toward a
weighted average of points in \(\Kset\) determined by \(x\).</p>
<h5 id="ideal-denoiser">Ideal denoiser</h5>
<p>The ideal or optimal denoiser \(\epsilon^*\) for a particular noise level
\(\sigma\) is an exact minimizer of the training loss function. When the data is
a discrete uniform distribution over a finite set \(\Kset\), the ideal
denoiser has an exact closed-form expression given by:</p>
\[\eps^*(x_\sigma, \sigma) = \frac{\sum_{x_0 \in \Kset} (x_\sigma-x_0) \exp(-\norm{x_\sigma-x_0}^2/2\sigma^2)}
{\sigma \sum_{x_0 \in \Kset} \exp(-\norm{x_\sigma-x_0}^2/2\sigma^2)}\]
<p>From the above expression, we see that the ideal denoiser points towards a
weighted mean of all the datapoints in \(\Kset\), where the weight for each
\(x_0 \in \Kset\) determines the distance to \(x_0\). Using this expression, we
can also implement the ideal denoiser, which is computationally tractable for
small datasets:</p>
<div><pre><code><span>def</span> <span>sq_norm</span><span>(</span><span>M</span><span>,</span> <span>k</span><span>):</span>
    <span># M: b x n --(norm)--&gt; b --(repeat)--&gt; b x k
</span>    <span>return </span><span>(</span><span>torch</span><span>.</span><span>norm</span><span>(</span><span>M</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span><span>**</span><span>2</span><span>).</span><span>unsqueeze</span><span>(</span><span>1</span><span>).</span><span>repeat</span><span>(</span><span>1</span><span>,</span><span>k</span><span>)</span>

<span>class</span> <span>IdealDenoiser</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dataset</span><span>:</span> <span>torch</span><span>.</span><span>utils</span><span>.</span><span>data</span><span>.</span><span>Dataset</span><span>):</span>
        <span>self</span><span>.</span><span>data</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>list</span><span>(</span><span>dataset</span><span>))</span>

    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>sigma</span><span>):</span>
        <span>x</span> <span>=</span> <span>x</span><span>.</span><span>flatten</span><span>(</span><span>start_dim</span><span>=</span><span>1</span><span>)</span>
        <span>d</span> <span>=</span> <span>self</span><span>.</span><span>data</span><span>.</span><span>flatten</span><span>(</span><span>start_dim</span><span>=</span><span>1</span><span>)</span>
        <span>xb</span><span>,</span> <span>db</span> <span>=</span> <span>x</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],</span> <span>d</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
        <span>sq_diffs</span> <span>=</span> <span>sq_norm</span><span>(</span><span>x</span><span>,</span> <span>db</span><span>)</span> <span>+</span> <span>sq_norm</span><span>(</span><span>d</span><span>,</span> <span>xb</span><span>).</span><span>T</span> <span>-</span> <span>2</span> <span>*</span> <span>x</span> <span>@</span> <span>d</span><span>.</span><span>T</span>
        <span>weights</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>functional</span><span>.</span><span>softmax</span><span>(</span><span>-</span><span>sq_diffs</span><span>/</span><span>2</span><span>/</span><span>sigma</span><span>**</span><span>2</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
        <span>return </span><span>(</span><span>x</span> <span>-</span> <span>torch</span><span>.</span><span>einsum</span><span>(</span><span>'</span><span>ij,j...-&gt;i...</span><span>'</span><span>,</span> <span>weights</span><span>,</span> <span>self</span><span>.</span><span>data</span><span>))</span><span>/</span><span>sigma</span>
</code></pre></div>
<p>For our toy dataset, we can plot the direction of \(\epsilon^*\) as predicted by
the ideal denoiser for different noise levels \(\sigma\):</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ideal_eps_field.png" alt="">
</p>
<figcaption>Plot of direction of \(\eps^*(x, \sigma)\) for different \(x\) and
\(\sigma\)</figcaption>
</figure>
<p>From our plots we see that for large values of \(\sigma\), \(\epsilon^*\) points
towards the mean of the data, but for smaller values of \(\sigma\),
\(\epsilon^*\) points towards the nearest data-point.</p>
<p>One insight from our paper is that the ideal denoiser for a fixed \(\sigma\) is
equivalent to the gradient of a \(\sigma\)-smoothed squared-distance function:</p>
<p><strong>Theorem</strong> For all \(\sigma &gt; 0\) and \(x \in \R^n\), we have</p>
\[\frac{1}{2} \nabla_x \distop^2_\Kset(x, \sigma) = \sigma \eps^*(x, \sigma).\]
<p>This tells us that the ideal denoiser found by minimizing the diffusion training
objective \(\Loss(\theta)\) is in fact the gradient of a smoothed
squared-distance function to the underlying data manifold \(\Kset\). This
connection is key to motivating our interpretation that the denoiser is an
approximate projection.</p>
<h5 id="relative-error-model">Relative error model</h5>
<p>In order to analyze the convergence of diffusion sampling algorithms, we
introduced a relative error model which states that the projection predicted by
the denoiser \(x-\sigma \epsilon_{\theta}( x, \sigma)\) well approximates
\(\projK(x)\) when the input to the denoiser \(\sigma\) well estimates
\(\distK(x)/\sqrt{n}\). For constants \(1 &gt; \eta \ge 0\) and \(\nu \ge 1\), we
assume that</p>
\[\norm{ x-\sigma \epsilon_{\theta}( x, \sigma) - \projK(x)} \le \eta \distK(x)\]
<p>when \((x, \sigma)\) satisfies \(\frac{1}{\nu}\distK(x) \le \sqrt{n}\sigma \le
\nu \distK(x)\). In addition to the discussion about ideal denoisers above, this
error model is motivated by the following observations.</p>
<p><em>Low noise</em> When \(\sigma\) is small and the manifold hypothesis holds,
denoising approximates projection because most of the added noise is orthogonal
to the data manifold.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/low_noise.png" alt="">
</p>
<figcaption>When added noise is small, most of noise is orthogonal to tangent space
of manifold. Under the manifold hypothesis, denoising is approximately projection.</figcaption>
</figure>
<p><em>High noise</em> When \(\sigma\) is large relative to the diameter of \(\Kset\),
then any denoiser predicting any weighted mean of the data \(\Kset\) has small
relative error.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/high_noise.png" alt="">
</p>
<figcaption>When added noise is large compared to diameter of data, denoising and
projection point in the same direction</figcaption>
</figure>
<p>We also perform empirical tests of our error model for pre-trained diffusion
models on image datasets. The CIFAR-10 dataset is small enough for tractable
computation of the ideal denoiser. Our experiments show that for this dataset,
the relative error between the exact projection and ideal denoiser output is
small over sampling trajectories.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ideal_denoiser_error.png" alt="">
</p>
<figcaption>Ideal denoiser well-approximates projection onto the CIFAR-10 dataset
under relative-error model</figcaption>
</figure>

<h4 id="sampling-from-diffusion-models">Sampling from diffusion models</h4>
<p>Given a learned denoiser \(\epsilon_\theta(x, \sigma)\), how do we sample from
it to obtain a point \(x_0 \in \Kset\)? Given noisy \(x_t\) and noise level
\(\sigma_t\), the denoiser \(\eps_\theta(x_t, \sigma_t)\) predicts \(x_0\) via</p>
\[\hat x_0^t := x_t - \sigma_t \eps_\theta(x_t, \sigma_t)\]
<p>Intuition from the relative error assumption tells us that we want to start with
\((x_T, \sigma_T)\) where \(\distK(x_T)/\sqrt{n} \approx \sigma_T\). This is
achieved by choosing \(\sigma_T\) to be large relative to the diameter of
\(\Kset\), and \(x_T\) sampled i.i.d. from \(N(0, \sigma_T)\), a Gaussian with
variance \(\sigma_T\). This ensures that \(x_T\) is far away from
\(\Kset\).</p>
<p>Although \(\hat x_0^T = x_T - \sigma_T \eps_\theta(x_T, \sigma_T)\) has small
relative error, the absolute error \(\distK(\hat x_0^T)\) can still be large as
\(\distK(x_T)\) is large. In fact, at high noise levels, the expression of the
ideal denoiser tells us that \(\hat x_0^T\) should be close to the mean of the
data \(\Kset\). We cannot obtain a sample close to \(\Kset\) with a single call
to the denoiser.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/denoise.png" alt="">
</p>
<figcaption>Sampling process iteratively calls the denoiser based on \(\sigma_t\)
schedule.</figcaption>
</figure>
<p>Thus we want to <em>iteratively call the denoiser</em> to obtain a sequence \(x_T,
\ldots, x_t, \ldots x_0\) using a pre-specified schedule of \(\sigma_t\), hoping
that \(\distK(x_t)\) decreases in concert with \(\sigma_t\).</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t-1}) \eps_\theta(x_t, \sigma_t)\]
<p>This is exactly the deterministic <a href="https://arxiv.org/abs/2010.02502">DDIM sampling algorithm</a>, though
presented in different coordinates through a change of variable. See <a href="https://arxiv.org/abs/2306.04848">Appendix A
of our paper</a> for more details and a proof of equivalence.</p>
<h4 id="diffusion-sampling-as-distance-minimization">Diffusion sampling as distance minimization</h4>
<p>We can interpret the diffusion sampling iterations as gradient descent on the
squared-distance function \(f(x) = \frac{1}{2} \distK(x)^2\). In a nutshell,</p>
<p><strong>DDIM is approximate gradient descent on \(f(x)\) with stepsize \(1-
\sigma_{t-1}/\sigma_t\), with \(\nabla f(x_t)\) estimated by \(\eps_\theta(x_t,
\sigma_t)\).</strong></p>
<p>How should we choose the \(\sigma_t\) schedule? This determines the number and
size of gradient steps we take during sampling. If there are too few steps,
\(\distK(x_t)\) might not decrease and the algorithm may not converge. On the
other hand, if we take many small steps, we need to evaluate the denoiser for as
many times, a computationally expensive operation. This motivates our definition
of admissible schedules.</p>
<p><strong>Definition</strong> An <em>admissible schedule</em> \(\{ \sigma_t \}_{t=0}^T\) ensures
\(\frac{1}{\nu} \distK(x_t) \le \sqrt{n} \sigma_t \le \nu \distK(x_t)\) holds at
each iteration. In particular, a geometrically decreasing (i.e. log-linear)
sequence of \(\sigma_t\) is an admissible schedule.</p>
<p>Our main theorem states that if \(\{\sigma_t\}_{t=0}^T\) is an admissible
schedule and \(\epsilon_\theta(x_t, \sigma_t)\) satisfies our relative error
model, the relative error can be controlled, and the sampling procedure aiming
to minimize distance converges.</p>
<p><strong>Theorem</strong> Let \(x_t\) denote the sequence generated by DDIM and suppose that
\(\nabla \distK(x)\) exists for all \(x_t\) and \(\distK(x_T) = \sqrt n
\sigma_T\). Then</p>
<ol>
<li>\(x_t\) is generated by gradient descent on the squared-distance function
with stepsize \(1 - \sigma_{t-1}/\sigma_{t}\)</li>
<li>\(\distK(x_{t})/\sqrt{n} \approx \sigma_{t}\) for all \(t\)</li>
</ol>
<p>Coming back to our toy example, we can find an admissible schedule by
subsampling from the original log-linear schedule, and implement the DDIM
sampler as follows:</p>
<div><pre><code><span>class</span> <span>Schedule</span><span>:</span>
    <span>...</span>
    <span>def</span> <span>sample_sigmas</span><span>(</span><span>self</span><span>,</span> <span>steps</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>indices</span> <span>=</span> <span>list</span><span>((</span><span>len</span><span>(</span><span>self</span><span>)</span> <span>*</span> <span>(</span><span>1</span> <span>-</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>steps</span><span>)</span><span>/</span><span>steps</span><span>))</span>
                       <span>.</span><span>round</span><span>().</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>int64</span><span>)</span> <span>-</span> <span>1</span><span>)</span>
        <span>return</span> <span>self</span><span>[</span><span>indices</span> <span>+</span> <span>[</span><span>0</span><span>]]</span>

<span>batchsize</span> <span>=</span> <span>2000</span>
<span>sigmas</span> <span>=</span> <span>schedule</span><span>.</span><span>sample_sigmas</span><span>(</span><span>20</span><span>)</span>
<span>xt</span> <span>=</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>)</span> <span>*</span> <span>sigmas</span><span>[</span><span>0</span><span>]</span>
<span>for</span> <span>sig</span><span>,</span> <span>sig_prev</span> <span>in</span> <span>pairwise</span><span>(</span><span>sigmas</span><span>):</span>
    <span>eps</span> <span>=</span> <span>model</span><span>(</span><span>xt</span><span>,</span> <span>sig</span><span>.</span><span>to</span><span>(</span><span>xt</span><span>))</span>
    <span>xt</span> <span>-=</span> <span>(</span><span>sig</span> <span>-</span> <span>sig_prev</span><span>)</span> <span>*</span> <span>eps</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/toy_example_samples_ddim.png" alt="">
</p>
<figcaption>Samples from 20-step DDIM</figcaption>
</figure>

<h4 id="improved-sampler-with-gradient-estimation">Improved sampler with gradient estimation</h4>
<p>Next, we use our interpretation to derive a new efficient sampler. Since
\(\nabla \distK(x)\) is invariant between \(x\) and \(\projK(x)\), we aim to
minimize estimation error \(\sqrt{n} \nabla \distK(x) - \epsilon_{\theta}(x_t,
\sigma_t)\) with the update:</p>
\[\bar\eps_t = \gamma \eps_{\theta}(x_t, \sigma_t) + (1-\gamma) \eps_{\theta}(x_{t+1}, \sigma_{t+1})\]
<p>Intuitively, this update corrects any error made in the previous step using the current estimate:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ge_step.png" alt="">
</p>
<figcaption>Our gradient estimation update step</figcaption>
</figure>
<p>This leads to faster convergence compared to the DDIM sampler, as seen from the
samples on our toy model lying closer to the original data.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/toy_example_samples_ge.png" alt="">
</p>
<figcaption>Samples from 20-step gradient estimation sampler</figcaption>
</figure>
<p>Compared to the default DDIM sampler, our sampler can be interpreted as adding
momentum, causing the trajectory to potentially overshoot but converge faster.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/trajectories_varying_gamma.png" alt="">
</p>
<figcaption>Sampling trajectories varying momentum term \(\gamma\)</figcaption>
</figure>
<p>Empirically, adding noise during the generation process also improves the
sampling quality. In order to do so while sticking to our original \(\sigma_t\)
schedule, we need to denoise to a smaller \(\sigma_{t'}\) then add back noise
\(w_t \sim N(0, I)\).</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t'}) \epsilon_\theta(x_t, \sigma_t) + \eta w_t\]
<p>If we assume that \(\mathop{\mathbb{E}}\norm{w_t}^2 = \norm{\epsilon_\theta(x_t,
\sigma_t)}^2\), we should choose \(\eta\) so that the norm of the update is
constant in expectation. This leads to the choice of \(\sigma_{t-1} =
\sigma_t^\mu \sigma_{t-1}^{1-\mu}\) and \(\eta = \sqrt{\sigma_{t-1}^2 -
\sigma_{t'}^2}\) where \(0 \le \mu &lt; 1\). When \(\mu = \frac{1}{2}\), we exactly
recover the <a href="https://arxiv.org/abs/2006.11239">DDPM sampler</a>.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/trajectories_varying_mu.png" alt="">
</p>
<figcaption>Sampling trajectories varying amount of noise added during sampling</figcaption>
</figure>
<p>Our gradient estimation update can be combined with adding noise during
sampling. In summary, our full update step is</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t'}) \bar\eps_t + \eta w_t\]
<p>The full sampler that generalizes DDIM (<code>gam=1, mu=0</code>), DDPM (<code>gam=1, mu=0.5</code>)
and our gradient estimation sampler (<code>gam=2, mu=0</code>) is implemented below.</p>
<div><pre><code><span>@torch.no_grad</span><span>()</span>
<span>def</span> <span>samples</span><span>(</span><span>model</span>      <span>:</span> <span>nn</span><span>.</span><span>Module</span><span>,</span>
            <span>sigmas</span>     <span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>,</span> <span># Iterable with N+1 values for N sampling steps
</span>            <span>gam</span>        <span>:</span> <span>float</span> <span>=</span> <span>1.</span><span>,</span>        <span># Suggested to use gam &gt;= 1
</span>            <span>mu</span>         <span>:</span> <span>float</span> <span>=</span> <span>0.</span><span>,</span>        <span># Requires mu in [0, 1)
</span>            <span>xt</span>         <span>:</span> <span>Optional</span><span>[</span><span>torch</span><span>.</span><span>FloatTensor</span><span>]</span> <span>=</span> <span>None</span><span>,</span>
            <span>batchsize</span>  <span>:</span> <span>int</span> <span>=</span> <span>1</span><span>):</span>
    <span>xt</span> <span>=</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>)</span> <span>*</span> <span>sigmas</span><span>[</span><span>0</span><span>]</span>
    <span>eps</span> <span>=</span> <span>None</span>
    <span>for</span> <span>i</span><span>,</span> <span>(</span><span>sig</span><span>,</span> <span>sig_prev</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>pairwise</span><span>(</span><span>sigmas</span><span>)):</span>
        <span>eps</span><span>,</span> <span>eps_prev</span> <span>=</span> <span>model</span><span>(</span><span>xt</span><span>,</span> <span>sig</span><span>.</span><span>to</span><span>(</span><span>xt</span><span>)),</span> <span>eps</span>
        <span>eps_av</span> <span>=</span> <span>eps</span> <span>*</span> <span>gam</span> <span>+</span> <span>eps_prev</span> <span>*</span> <span>(</span><span>1</span><span>-</span><span>gam</span><span>)</span>  <span>if</span> <span>i</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>eps</span>
        <span>sig_p</span> <span>=</span> <span>(</span><span>sig_prev</span><span>/</span><span>sig</span><span>**</span><span>mu</span><span>)</span><span>**</span><span>(</span><span>1</span><span>/</span><span>(</span><span>1</span><span>-</span><span>mu</span><span>))</span> <span># sig_prev == sig**mu sig_p**(1-mu)
</span>        <span>eta</span> <span>=</span> <span>(</span><span>sig_prev</span><span>**</span><span>2</span> <span>-</span> <span>sig_p</span><span>**</span><span>2</span><span>).</span><span>sqrt</span><span>()</span>
        <span>xt</span> <span>=</span> <span>xt</span> <span>-</span> <span>(</span><span>sig</span> <span>-</span> <span>sig_p</span><span>)</span> <span>*</span> <span>eps_av</span> <span>+</span> <span>eta</span> <span>*</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>).</span><span>to</span><span>(</span><span>xt</span><span>)</span>
        <span>yield</span> <span>xt</span>
</code></pre></div>
<h4 id="large-scale-examples">Large-scale examples</h4>
<p>The training code above not only works for our toy dataset, they can also be
used to train image diffusion models from scratch. See <a href="https://github.com/yuanchenyang/smalldiffusion/blob/main/examples/fashion_mnist.py">this
example</a> for an example of training on the FashionMNIST
dataset to get a second-place FID score on <a href="https://paperswithcode.com/sota/image-generation-on-fashion-mnist">this
leaderboard</a>:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/fashion-mnist-samples-large.png" alt="">
</p>
<figcaption>Samples from a diffusion model trained on the FashionMNIST dataset</figcaption>
</figure>
<p>The sampling code works without modifications to sample from state-of-the-art
pretrained latent diffusion models:</p>
<div><pre><code><span>schedule</span> <span>=</span> <span>ScheduleLDM</span><span>(</span><span>1000</span><span>)</span>
<span>model</span>    <span>=</span> <span>ModelLatentDiffusion</span><span>(</span><span>'</span><span>stabilityai/stable-diffusion-2-1-base</span><span>'</span><span>)</span>
<span>model</span><span>.</span><span>set_text_condition</span><span>(</span><span>'</span><span>An astronaut riding a horse</span><span>'</span><span>)</span>
<span>*</span><span>xts</span><span>,</span> <span>x0</span> <span>=</span> <span>samples</span><span>(</span><span>model</span><span>,</span> <span>schedule</span><span>.</span><span>sample_sigmas</span><span>(</span><span>50</span><span>))</span>
<span>decoded</span>  <span>=</span> <span>model</span><span>.</span><span>decode_latents</span><span>(</span><span>x0</span><span>)</span>
</code></pre></div>
<p>We can visualize the different effects of our momentum term \(\gamma\) on high
resolution text-to-image generation.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/sd_examples.jpg" alt="">
</p>
<figcaption>Samples from a pretrained stable diffusion model</figcaption>
</figure>
<h4 id="other-resources">Other resources</h4>
<p>Also recommended are the following blog posts on diffusion models:</p>
<ol>
<li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank">What are diffusion models</a> introduces
diffusion models from the discrete-time perspective of reversing a Markov
process</li>
<li><a href="https://yang-song.net/blog/2021/score/" target="_blank">Generative modeling by estimating gradients of the data
distribution</a> introduces diffusion models from
the continuous time perspective of reversing a stochastic differential
equation</li>
<li><a href="https://huggingface.co/blog/annotated-diffusion" target="_blank">The annotated diffusion model</a> goes over
a pytorch implementation of a diffusion model in detail</li>
</ol>

</section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The later we meet someone in a sequence, the more negatively we describe them (256 pts)]]></title>
            <link>https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/</link>
            <guid>39672111</guid>
            <pubDate>Mon, 11 Mar 2024 19:05:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/">https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/</a>, See on <a href="https://news.ycombinator.com/item?id=39672111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>Imagine you‚Äôre the 20th candidate to interview for your dream job, or the 10th hopeful stepping onto the stage to audition for a coveted role. You‚Äôre just as qualified and just as talented as those who went before you. </p>



<p>But according to a new study, you might be at a surprising disadvantage, thanks to an unconscious bias in how we perceive and describe others. It found that the later we encounter someone in a sequence, the more negatively we tend to describe them.</p>



<p><a href="https://pubmed.ncbi.nlm.nih.gov/38421750/" target="_blank" rel="noopener">The study</a> was published in the <em>Journal of Personality and Social Psychology</em> on February 29, 2024.</p>



<h2>The Serial Position-Negativity Effect</h2>



<p>The researchers coined this phenomenon the ‚Äúserial position-negativity effect.‚Äù </p>



<p>They hypothesized that when we sequentially encounter people, we focus on distinct attributes that differentiate each new person from those we‚Äôve already met. And because distinct attributes tend to be negative in the grand scheme of things, our descriptions of later-encountered people become increasingly negative.</p>



<p>To test this, the researchers conducted a number of studies. In one, they had 992 participants (recruited from Prolific Academic) describe 20 people based on their Facebook profile pictures. </p>



<p>The participants described the first few individuals quite positively, using an average of 6.2 positive words each. But as they progressed through the sequence, their descriptions became significantly more negative, dipping to an average of just 4.7 positive words by the 20th person.</p>



<p>In another experiment, 987 participants (about evenly split between male and female, with an average age of 42) were shown short video clips of women introducing themselves on the popular TV show <em>The Bachelor</em>. In these clips, each woman tried to make a memorable first impression on the bachelor, often in creative and attention-grabbing ways.</p>



<p>As the study participants progressed through the sequence of videos, their descriptions of the women became increasingly negative: the tenth woman was described significantly more negatively, on average, than the first woman, despite the fact that the order of the videos was randomized for each participant.</p>



<p>And their descriptions also became increasingly specific over time. As participants encountered more women, they focused more on what made each new woman stand out, leading to more unique and ultimately more negative descriptions.</p><!-- wp:shortcode -->


<!-- /wp:shortcode -->



<h2>Further Directions</h2>



<p>The study opens up many possibilities for future research. The researchers next want to examine how our personal quirks and the groups we belong to might amp up or tone down this negative bias towards people we meet later on. </p>



<p>They also want to see how long these snap judgments stick around, and how they might be throwing a wrench into things like job interviews and performance reviews in the real world.</p>



<h2>Implications </h2>



<p>The research suggests that this unconscious bias could disadvantage people who happen to be evaluated later in a sequence, whether it‚Äôs job applicants, contestants on a reality show, or Tinder dates. </p>



<p>So the next time you‚Äôre in a situation where you‚Äôre meeting a lot of new people sequentially, whether a networking event or speed dating night, keep in mind that your impressions may be tainted by this subtle cognitive bias. </p>



<h2>Study information:</h2>



<ul>
<li><strong>Journal: </strong><a href="https://www.apa.org/pubs/journals/psp" target="_blank" rel="noopener">Journal of Personality and Social Psychology</a></li>



<li><strong>Publication Date:</strong> February 29, 2024 (Online ahead of print)</li>



<li><strong>DOI: </strong>10.1037/pspa0000383</li>



<li><strong>Title:</strong> ‚Äú<a href="https://pubmed.ncbi.nlm.nih.gov/38421750/" target="_blank" rel="noopener">Differentiation in social perception: Why later-encountered individuals are described more negatively</a>‚Äú</li>



<li><strong>Authors: </strong>
<ul>
<li>Alex Koch: Booth School of Business, University of Chicago</li>



<li>Andrew Bromley: Booth School of Business, University of Chicago</li>



<li>Johanna Woitzel: Department of Psychology, Ruhr University Bochum</li>



<li>Hans Alves: Department of Psychology, Ruhr University Bochum</li>
</ul>
</li>
</ul>



		</div></div>]]></description>
        </item>
    </channel>
</rss>