<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 23 Aug 2025 11:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I'm too dumb for Zig's new IO interface (144 pts)]]></title>
            <link>https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</link>
            <guid>44993797</guid>
            <pubDate>Sat, 23 Aug 2025 06:39:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/">https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</a>, See on <a href="https://news.ycombinator.com/item?id=44993797">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  
  
<p>You might have heard that Zig 0.15 introduces a new IO interface, with the focus for this release being the new std.Io.Reader and std.Io.Writer types. The old "interfaces" had problems. Like <a href="https://github.com/ziglang/zig/issues/17985">this performance issue</a> that I opened. And it relied on a <a href="https://www.openmymind.net/In-Zig-Whats-a-Writer/">mix of types</a>, which always confused me, and a lot of <code>anytype</code> - which is generally great, but a poor foundation to build an interface on.</p>

<p>I've been slowly upgrading my libraries, and I ran into changes to the <code>tls.Client</code> client used by my smtp library. For the life of me, I just don't understand how it works.</p>

<p>Zig has never been known for its documentation, but if we look at the documentation for <code>tls.Client.init</code>, we'll find:</p>

<pre><code><span>pub</span> <span>fn</span> <span>init</span><span>(</span>input<span>:</span> <span><span>*</span>std<span>.</span>Io<span>.</span>Reader</span><span>,</span> output<span>:</span> <span><span>*</span>std<span>.</span>Io<span>.</span>Writer</span><span>,</span> options<span>:</span> <span>Options</span><span>)</span> InitError<span>!</span>Client
Initiates a TLS handshake <span>and</span> establishes a TLSv1<span>.</span><span>2</span> <span>or</span> TLSv1<span>.</span><span>3</span> session<span>.</span></code></pre>

<p>So it takes one of these new Readers and a new Writer, along with some options (sneak peak, which aren't all optional). It doesn't look like you can just give it a <code>net.Stream</code>, but <code>net.Stream</code> does expose a <code>reader()</code> and <code>writer()</code> method, so that's probably a good place to start:</p>

<pre><code><span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
<span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

<span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span><span>.</span><span>{</span><span>}</span><span>)</span><span>;</span>
<span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span><span>.</span><span>{</span><span>}</span><span>)</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span><span>}</span><span>,</span> <span>// options TODO</span>
<span>)</span><span>;</span></code></pre>

<p>Note that <code>stream.writer()</code> returns a <code>Stream.Writer</code> and <code>stream.reader()</code> returns a <code>Stream.Reader</code> - those aren't the types our <code>tls.Client</code> expects. To convert the <code>Stream.Reader</code> to an <code>*std.Io.Reader</code>, we need to call its <code>interface()</code> method. To get a <code>*std.io.Writer</code> from an <code>Stream.Writer</code>, we need the address of its <code>&amp;interface</code> field. This doesn't seem particularly consistent. Don't forget that the <code>writer</code> and <code>reader</code> need a stable address. Because I'm trying to get the simplest example working, this isn't an issue - everything will live on the stack of <code>main</code>. In a real word example, I think it means that I'll always have to wrap the <code>tls.Client</code> into my own heap-allocated type; giving the writer and reader have a cozy stable home.</p>

<p>Speaking of allocations, you might have noticed that <code>stream.writer</code> and <code>stream.reader</code> take a parameter. It's the buffer they should use. Buffering is a first class citizen of the new Io interface - who needs composition? The documentation <strong>does</strong> tell me these need to be at least <code>std.crypto.tls.max_ciphertext_record_len</code> large, so we need to fix things a bit:</p>

<pre><code><span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

<span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span></code></pre>

<p>Here's where the code stands: </p>

<pre><code><span>const</span> std <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>)</span><span>;</span>

<span>pub</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span><span>!</span><span>void</span></span> <span>{</span>
  <span>var</span> gpa<span>:</span> std<span>.</span>heap<span>.</span><span>DebugAllocator</span><span>(</span><span>.</span><span>{</span><span>}</span><span>)</span> <span>=</span> <span>.</span>init<span>;</span>
  <span>const</span> allocator <span>=</span> gpa<span>.</span><span>allocator</span><span>(</span><span>)</span><span>;</span>

  <span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
  <span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

  <span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

  <span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span>

  <span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
      reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
      <span>&amp;</span>writer<span>.</span>interface<span>,</span>
      <span>.</span><span>{</span>
      <span>}</span><span>,</span>
  <span>)</span><span>;</span>
  <span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>
<span>}</span></code></pre>

<p>But if you try to run it, you'll get a compilation error. Turns out we have to provide 4 options: the ca_bundle, a host, a <code>write_buffer</code> and a <code>read_buffer</code>. Normally I'd expect the options parameter to be for optional parameters, I don't understand why some parameters (input and output) are passed one way while <code>writer_buffer</code> and <code>read_buffer</code> are passed another.</p>

<p>Let's give it what it wants AND send some data:</p>

<pre><code><span>// existing setup...</span>

<span>var</span> bundle <span>=</span> <span>std<span>.</span>crypto<span>.</span>Certificate<span>.</span>Bundle</span><span>{</span><span>}</span><span>;</span>
<span>try</span> bundle<span>.</span><span>rescan</span><span>(</span>allocator<span>)</span><span>;</span>
<span>defer</span> bundle<span>.</span><span>deinit</span><span>(</span>allocator<span>)</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span>
    <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
    <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
    <span>.</span>read_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
    <span>.</span>write_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span>
<span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

<span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span></code></pre>

<p>Now, if I try to run it, the program just hangs. I don't know what <code>write_buffer</code> is, but I know Zig now loves buffers, so let's try to give it something:</p>

<pre><code><span>// existing setup...</span>

<span>// I don't know what size this should/has to be??</span>
<span>var</span> write_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>

<span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
  reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
  <span>&amp;</span>writer<span>.</span>interface<span>,</span>
  <span>.</span><span>{</span>
    <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
    <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
    <span>.</span>read_buffer <span>=</span> <span>&amp;</span><span>.</span><span>{</span><span>}</span><span>,</span>
    <span>.</span>write_buffer <span>=</span> <span>&amp;</span>write_buf2<span>,</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span>
<span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

<span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span></code></pre>


<p>Great, now the code doesn't hang, all we need to do is read the response. <code>tls.Client</code> exposes a <code>reader: *std.Io.Reader</code> field which is "Decrypted stream from the server to the client." That sounds like what we want, but believe it or not <code>std.Io.Reader</code> doesn't have a <code>read</code> method. It has a <code>peak</code> a <code>takeByteSigned</code>, a <code>readSliceShort</code> (which seems close, but it blocks until the provided buffer is full), a <code>peekArray</code> and a lot more, but nothing like the <code>read</code> I'd expect. The closest I can find, which I think does what I want, is to stream it to a writer:</p>

<pre><code><span>var</span> buf<span>:</span> <span><span>[</span><span>1024</span><span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
<span>var</span> w<span>:</span> <span>std<span>.</span>Io<span>.</span>Writer</span> <span>=</span> <span>.</span><span>fixed</span><span>(</span><span>&amp;</span>buf<span>)</span><span>;</span>
<span>const</span> n <span>=</span> <span>try</span> tls_client<span>.</span>reader<span>.</span><span>stream</span><span>(</span><span>&amp;</span>w<span>,</span> <span>.</span><span>limited</span><span>(</span>buf<span>.</span>len<span>)</span><span>)</span><span>;</span>
std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"read: {d} - {s}\n"</span><span>,</span> <span>.</span><span>{</span>n<span>,</span> buf<span>[</span><span>0</span><span>..</span>n<span>]</span><span>}</span><span>)</span><span>;</span></code></pre>

<p>If we try to run the code now, it crashes. We've apparently failed an assertion regarding the length of a buffer. So it seems like we also <em>have</em> to provide a <code>read_buffer</code>.</p>

<p>Here's my current version (it doesn't work, but it doesn't crash!):</p>

<pre><code><span>const</span> std <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>)</span><span>;</span>

<span>pub</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span><span>!</span><span>void</span></span> <span>{</span>
  <span>var</span> gpa<span>:</span> std<span>.</span>heap<span>.</span><span>DebugAllocator</span><span>(</span><span>.</span><span>{</span><span>}</span><span>)</span> <span>=</span> <span>.</span>init<span>;</span>
  <span>const</span> allocator <span>=</span> gpa<span>.</span><span>allocator</span><span>(</span><span>)</span><span>;</span>

  <span>const</span> stream <span>=</span> <span>try</span> std<span>.</span>net<span>.</span><span>tcpConnectToHost</span><span>(</span>allocator<span>,</span> <span>"www.openmymind.net"</span><span>,</span> <span>443</span><span>)</span><span>;</span>
  <span>defer</span> stream<span>.</span><span>close</span><span>(</span><span>)</span><span>;</span>

  <span>var</span> write_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> writer <span>=</span> stream<span>.</span><span>writer</span><span>(</span><span>&amp;</span>write_buf<span>)</span><span>;</span>

  <span>var</span> read_buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> reader <span>=</span> stream<span>.</span><span>reader</span><span>(</span><span>&amp;</span>read_buf<span>)</span><span>;</span>

  <span>var</span> bundle <span>=</span> <span>std<span>.</span>crypto<span>.</span>Certificate<span>.</span>Bundle</span><span>{</span><span>}</span><span>;</span>
  <span>try</span> bundle<span>.</span><span>rescan</span><span>(</span>allocator<span>)</span><span>;</span>
  <span>defer</span> bundle<span>.</span><span>deinit</span><span>(</span>allocator<span>)</span><span>;</span>

  <span>var</span> write_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> read_buf2<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>

  <span>var</span> tls_client <span>=</span> <span>try</span> std<span>.</span>crypto<span>.</span>tls<span>.</span>Client<span>.</span><span>init</span><span>(</span>
      reader<span>.</span><span>interface</span><span>(</span><span>)</span><span>,</span>
      <span>&amp;</span>writer<span>.</span>interface<span>,</span>
      <span>.</span><span>{</span>
        <span>.</span>ca <span>=</span> <span>.</span><span>{</span><span>.</span>bundle <span>=</span> bundle<span>}</span><span>,</span>
        <span>.</span>host <span>=</span> <span>.</span><span>{</span> <span>.</span>explicit <span>=</span> <span>"www.openmymind.net"</span> <span>}</span> <span>,</span>
        <span>.</span>read_buffer <span>=</span> <span>&amp;</span>read_buf2<span>,</span>
        <span>.</span>write_buffer <span>=</span> <span>&amp;</span>write_buf2<span>,</span>
      <span>}</span><span>,</span>
  <span>)</span><span>;</span>
  <span>defer</span> tls_client<span>.</span><span>end</span><span>(</span><span>)</span> <span>catch</span> <span>{</span><span>}</span><span>;</span>

  <span>try</span> tls_client<span>.</span>writer<span>.</span><span>writeAll</span><span>(</span><span>"GET / HTTP/1.1\r\n\r\n"</span><span>)</span><span>;</span>

  <span>var</span> buf<span>:</span> <span><span>[</span>std<span>.</span>crypto<span>.</span>tls<span>.</span>max_ciphertext_record_len<span>]</span><span>u8</span></span> <span>=</span> <span>undefined</span><span>;</span>
  <span>var</span> w<span>:</span> <span>std<span>.</span>Io<span>.</span>Writer</span> <span>=</span> <span>.</span><span>fixed</span><span>(</span><span>&amp;</span>buf<span>)</span><span>;</span>
  <span>const</span> n <span>=</span> <span>try</span> tls_client<span>.</span>reader<span>.</span><span>stream</span><span>(</span><span>&amp;</span>w<span>,</span> <span>.</span><span>limited</span><span>(</span>buf<span>.</span>len<span>)</span><span>)</span><span>;</span>
  std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"read: {d} - {s}\n"</span><span>,</span> <span>.</span><span>{</span>n<span>,</span> buf<span>[</span><span>0</span><span>..</span>n<span>]</span><span>}</span><span>)</span><span>;</span>
<span>}</span></code></pre>

<p>When I looked through Zig's source code, there's <a href="https://github.com/ziglang/zig/blob/306176046e6ae5e30bc58e5f3bcf786159e367f2/lib/std/http/Client.zig#L329">only one place</a> using <code>tls.Client</code>. It helped to get me where where I am. I couldn't find any tests.</p>

<p>I'll admit that during this migration, I've missed some basic things. For example, someone had to help me find <code>std.fmt.printInt</code> - the renamed version of <code>std.fmt.formatIntBuf</code>. Maybe there's a helper like: <code>tls.Client.init(allocator, stream)</code> somewhere. And maybe it makes sense that we do <code>reader.interface()</code> but <code>&amp;writer.interface</code> - I'm reminded of Go's <code>*http.Request</code> and <code>http.ResponseWrite</code>. And maybe Zig has some consistent rule for what parameters belong in options. And I know nothing about TLS, so maybe it makes complete sense to need 4 buffers. I feel a bit more confident about the weirdness of not having a <code>read(buf: []u8) !usize</code> function on <code>Reader</code>, but at this point I wouldn't bet on me.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Measuring the environmental impact of AI inference (128 pts)]]></title>
            <link>https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/</link>
            <guid>44992832</guid>
            <pubDate>Sat, 23 Aug 2025 03:22:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/">https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/</a>, See on <a href="https://news.ycombinator.com/item?id=44992832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>To come up with typical numbers, the team that did the analysis tracked requests and the hardware that served them for a 24 hour period, as well as the idle time for that hardware. This gives them an energy per request estimate, which differs based on the model being used. For each day, they identify the median prompt and use that to calculate the environmental impact.</p>
<h2>Going down</h2>
<p>Using those estimates, they find that the impact of an individual text request is pretty small. "We estimate the median Gemini Apps text prompt uses 0.24 watt-hours of energy, emits 0.03 grams of carbon dioxide equivalent (gCO2e), and consumes 0.26 milliliters (or about five drops) of water," they conclude. To put that in context, they estimate that the energy use is similar to about nine seconds of TV viewing.</p>
<p>The bad news is that the volume of requests is undoubtedly very high. The company has chosen to execute an AI operation with every single search request, a compute demand that simply didn't exist a couple of years ago. So, while the individual impact is small, the cumulative cost is likely to be considerable.</p>
<p>The good news? Just a year ago, it would have been far, far worse.</p>
<p>Some of this is just down to circumstances. With the <a href="https://arstechnica.com/science/2025/05/us-solar-keeps-surging-generating-more-power-than-hydro-in-2025/">boom in solar power</a> in the US and elsewhere, it has gotten easier for Google to arrange for renewable power. As a result, the carbon emissions per unit of energy consumed saw a 1.4x reduction over the past year. But the biggest wins have been on the software side, where different approaches have led to a 33x reduction in energy consumed per prompt.</p>

<figure>
    <p><img width="1286" height="604" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM.png" alt="A color bar showing the percentage of energy used by different hardware. AI accelerators are the largest use, followed by CPU and RAM. Idle machines and overhead account for about 10 percent each." decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM.png 1286w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM-640x301.png 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM-1024x481.png 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM-768x361.png 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/08/Screenshot-2025-08-22-at-12.25.32-PM-980x460.png 980w" sizes="auto, (max-width: 1286px) 100vw, 1286px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Most of the energy use in serving AI requests comes from time spent in the custom accelerator chips.

              <span>
          Credit:

          
          Elsworth, et. al.

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The Google team describes a number of optimizations the company has made that contribute to this. One is an approach termed Mixture-of-Experts, which involves figuring out how to only activate the portion of an AI model needed to handle specific requests, which can drop computational needs by a factor of 10 to 100. They've developed a number of compact versions of their main model, which also reduce the computational load. Data center management also plays a role, as the company can make sure that any active hardware is fully utilized, while allowing the rest to stay in a low-power state.</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Computer fraud laws used to prosecute leaking air crash footage to CNN (189 pts)]]></title>
            <link>https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/</link>
            <guid>44991542</guid>
            <pubDate>Sat, 23 Aug 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/">https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/</a>, See on <a href="https://news.ycombinator.com/item?id=44991542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-514173">

				


				


				<h3>from the <i>if-it-can-be-abused,-it-WILL-be-abused</i> dept</h3>
				


				<p>Earlier this year, an Army helicopter <a href="https://en.wikipedia.org/wiki/2025_Potomac_River_mid-air_collision" data-type="link" data-id="https://en.wikipedia.org/wiki/2025_Potomac_River_mid-air_collision">collided with a passenger plane</a> over the Potomac River in Washington, DC. All sixty-seven people aboard both vehicles were killed. While the FAA focused its investigation on the failures that led to this mid-air collision, local investigators in Virginia were somehow far more concerned about identifying who had <a href="https://www.youtube.com/watch?v=JTgUrfQsOnA" data-type="link" data-id="https://www.youtube.com/watch?v=JTgUrfQsOnA">leaked footage of the collision to CNN</a>. </p>
<p>The subject matter of the leaked recordings was obviously of public interest. And while the government may have its own interest in controlling dissemination of recording of incidents that involve federal agencies and their oversight, it’s not the sort of government interest most courts consider to be worthy of violating the First Amendment.</p>
<p>Fortunately, the government has options. For a very long time, the option federal law enforcement deployed most frequently in cases involving pretty much any sort of technology was the <a href="https://www.techdirt.com/tag/cfaa/" data-type="link" data-id="https://www.techdirt.com/tag/cfaa/">Computer Fraud and Abuse Act</a> (CFAA). This <a href="https://www.techdirt.com/tag/shoot-the-messenger/" data-type="link" data-id="https://www.techdirt.com/tag/shoot-the-messenger/">broadly written law</a> not only allowed prosecutors to charge people with federal crimes for doing nothing more than interacting with services/servers/etc. in unexpected ways, but allowed companies to, essentially, shoot the messengers for reporting data breaches, unsecured servers, or sloppy user interfaces that could be exploited to display far more information than those running them intended.</p>
<p>The CFAA has since <a href="https://www.techdirt.com/2022/05/23/doj-changes-cfaa-policy-will-no-longer-bring-criminal-charges-against-security-researchers/" data-type="link" data-id="https://www.techdirt.com/2022/05/23/doj-changes-cfaa-policy-will-no-longer-bring-criminal-charges-against-security-researchers/">been neutered a bit</a>, slowing its abusive role in federal prosecutions. Unfortunately, there are plenty of sloppily written state laws that can accomplish what the CFAA no longer can, <a href="https://theintercept.com/2025/08/13/dc-plane-crash-washington-cnn-leak/" data-type="link" data-id="https://theintercept.com/2025/08/13/dc-plane-crash-washington-cnn-leak/">as Nikita Mazurov and Shawn Musgrave report for The Intercept</a>.</p>
<p>Here’s what Metropolitan Washington Airports Authority investigator Patrick Silsbee wrote in his report:</p>
<blockquote>
<p><em>“The video shows camera angles and views that can only be found on the Metropolitan Washington Airport’s Authority CCTV video,” Silsbee wrote in a January 31 report, noting the location of landmarks in the videos, including a boathouse near the airfield.</em></p>
<p><em>The locations of the MWAA security cameras are redacted in the reports provided to The Intercept, ostensibly “to prevent the disclosure of law enforcement and security techniques and procedures not generally known outside the law enforcement community,” according to an accompanying letter from MWAA.</em></p>
</blockquote>
<p>That doesn’t mean much by itself, but Silsbee apparently figured out (thanks in part to CNN’s initial failure to redact some CCTV text that described the location of the camera) this footage must have been obtained by an MWAA employee working at the police dispatch center. </p>
<p>CCTV footage from <em>inside</em> the dispatch center was obtained, which allegedly showed these actions being taken by the suspected leaker:</p>
<blockquote>
<p><em>“Between the hours of 2256 and 0545, Mr. Mbengue can be seen on multiple occasions utilize [sic] his personal cell phone to record video and photograph these critical scenes,” Silsbee wrote.</em></p>
</blockquote>
<p>That would be MWAA dispatch employee Mohamed Mbengue, who has since pleaded “no contest” to charges stemming from Virginia’s ultra-vague <a href="https://law.lis.virginia.gov/vacode/18.2-152.4/" data-type="link" data-id="https://law.lis.virginia.gov/vacode/18.2-152.4/">“computer trespass” law</a>. But it really takes a person with an overriding desire to shoot messengers to call cell phone recordings of screen images a “trespass.” </p>
<p>The word is generally understood to describe unauthorized access to an area a person is not allowed to be in. Mbengue was at work and had full access to these recordings as a part of his job. That he recorded them and sent them to CNN doesn’t align with any rational definition of the word “trespass.” The dissemination of footage may be a violation of policy, but policy violations aren’t criminal charges — the sort of thing that can do permanent damage to a person’s life in ways that write-ups and even justified terminations simply can’t.</p>
<p>That’s why discretion is key. But when discretion matters most, law enforcement tends to deliberately “err” on the side of whatever does the most damage to anyone it happens to be investigating. And it appears MWAA investigators are more than happy to throw criminal charges at people for, at most, violating agency policies. A second dispatcher (Jonathan Savoy) was caught doing the same thing (albeit without sharing the recordings with CNN) and faced similar charges until someone actually exercised a bit of discretion and declined to move forward with the case.</p>
<blockquote>
<p><em>On February 3, the MWAA&nbsp;<a href="https://x.com/allisonpapson/status/1886553371257266540?s=46&amp;t=p5q6YKIPojSzB8gCMgZMiA" target="_blank" rel="noreferrer noopener">announced</a>&nbsp;both men’s arrests, writing in a press statement that Savoy had been arrested “following further police investigation.”</em></p>
<p><em>In May, however, local prosecutors&nbsp;<a href="https://theintercept.com/2025/05/29/charges-dropped-leaked-dc-plane-crash-video/">quietly dropped</a>&nbsp;the charges against Savoy, through a filing called a “nolle prosequi,” according to the court docket.</em></p>
</blockquote>
<p>There’s absolutely nothing in the statute that actually covers the actions described here, which formed the basis for the bullshit criminal charges. It takes a ton of punitive imagination to turn “recording a CCTV monitor with a phone” into a criminal act. The only clause that could be even possibly be considered applicable requires investigators and prosecutors to engage in lot of extremely creative re-interpretations of <a href="https://law.lis.virginia.gov/vacode/18.2-152.4/" data-type="link" data-id="https://law.lis.virginia.gov/vacode/18.2-152.4/">the plain text of the law</a>: </p>
<blockquote>
<p><em>Use a computer or computer network to make or cause to be made an unauthorized copy, in any form, including, but not limited to, any printed or electronic form of computer data, computer programs or computer software residing in, communicated by, or produced by a computer or computer network</em></p>
</blockquote>
<p>A smartphone is a computer. A recording could be considered an “unauthorized copy.” To call the CCTV cameras and screens “computers/computer network” means ignoring the generally understood utility of this tech. Even if a network connects the cameras and a computer provides access to recordings, recording playback via phone while accessing footage the suspects <em>had every right to access</em>, calling this a violation of the law demonstrates investigators were out for revenge, rather than serving the commonly understood definition of the word “justice.”</p>

				
<p>

	Filed Under: <a href="https://www.techdirt.com/tag/cfaa/" rel="tag">cfaa</a>, <a href="https://www.techdirt.com/tag/computer-trespass/" rel="tag">computer trespass</a>, <a href="https://www.techdirt.com/tag/leak-investigation/" rel="tag">leak investigation</a>, <a href="https://www.techdirt.com/tag/leaks/" rel="tag">leaks</a>, <a href="https://www.techdirt.com/tag/mwaa/" rel="tag">mwaa</a>, <a href="https://www.techdirt.com/tag/shoot-the-messenger/" rel="tag">shoot the messenger</a>, <a href="https://www.techdirt.com/tag/virginia/" rel="tag">virginia</a>
	<br>

	
</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Popular Japanese smartphone games have introduced external payment systems (129 pts)]]></title>
            <link>https://english.kyodonews.net/articles/-/59689</link>
            <guid>44991384</guid>
            <pubDate>Fri, 22 Aug 2025 23:50:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://english.kyodonews.net/articles/-/59689">https://english.kyodonews.net/articles/-/59689</a>, See on <a href="https://news.ycombinator.com/item?id=44991384">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>TOKYO - Nearly 70 percent of popular Japanese smartphone games have introduced external payment systems for items and services to avoid hefty commission fees from U.S. tech giants Google LLC and Apple Inc., a Kyodo News tally showed.</p>

<p>The move comes ahead of a new Japanese law tightening regulations on Google and Apple, which dominate smartphone platforms, set to take full effect in December. The legislation requires the two companies to open their payment systems.</p>

<p>Almost all users currently download games through Apple and Google's app stores. When players buy in-game items, software providers pay the tech giants commissions of up to 30 percent.</p>

<p>A Kyodo News survey found that among the top 30 best-selling game titles in 2024, at least 11 of the 16 offered by domestic companies have introduced payments through external websites.</p>

<p>Although the two tech giants say the fees are necessary to protect user privacy and security, the costs have weighed on game makers.</p>

<p>For outside transactions, users make payments through channels other than apps, such as game websites. Settlement service providers like Digital Garage Inc. and GMO Tech Inc. typically charge a 5 percent commission, far below Apple's and Google's rates.</p>

<p>Payments through external websites in the in-app purchase market, estimated at over 1 trillion yen ($6.8 billion), are expected to bring user discounts and boost providers' profitability, analysts said.</p>

<p>In the survey, Kyodo News received responses from eight of 12 domestic game makers, while two declined to comment and two did not respond. Of the 12 titles from the eight firms, 11 have adopted external settlements.</p>

<p>In August last year, Mixi Inc. introduced an outside settlement system for its blockbuster game "Monster Strike," allowing its users to purchase about 5 percent more items compared with in-app payments.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mail Carriers Pause US Deliveries as Tariff Shift Sows Confusion (143 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end</link>
            <guid>44991039</guid>
            <pubDate>Fri, 22 Aug 2025 23:09:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end">https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end</a>, See on <a href="https://news.ycombinator.com/item?id=44991039">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Bluesky Goes Dark in Mississippi over Age Verification Law (181 pts)]]></title>
            <link>https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/</link>
            <guid>44990886</guid>
            <pubDate>Fri, 22 Aug 2025 22:51:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/">https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/</a>, See on <a href="https://news.ycombinator.com/item?id=44990886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>People in Mississippi</span> can no longer use the social media platform <a href="https://www.wired.com/tag/bluesky/">Bluesky</a>. The company announced Friday that it will be blocking all IP addresses within Mississippi for the foreseeable future in response to a recent <a href="https://www.wired.com/tag/us-supreme-court/">US Supreme Court</a> decision that allows the state to enforce strict age verification for social media platforms.</p><p>According to Bluesky, Mississippi’s approach to verification “would fundamentally change” how users access the site. “We think this law creates challenges that go beyond its child safety goals, and creates significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies,” the Bluesky team said in <a data-offer-url="https://bsky.social/about/blog/08-22-2025-mississippi-hb1126" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://bsky.social/about/blog/08-22-2025-mississippi-hb1126&quot;}" href="https://bsky.social/about/blog/08-22-2025-mississippi-hb1126" rel="nofollow noopener" target="_blank">its statement</a>.</p><p>Bluesky did not respond to a request for comment.</p><p>The company says that compliance with Mississippi’s law—which would require identifying and tracking all users under 18, in addition to asking every user for sensitive personal information to verify their age—is not possible with the team’s current resources and infrastructure. By not complying with the law, Bluesky could face fines of up to $10,000 per violation. It is the first major social media platform to take such drastic steps in response to the law.</p><p>Age verification laws, which on the surface are intended to protect children from harmful content online, have already begun to broadly impact internet use in places around the world where they've been enacted. <a href="https://www.wired.com/story/the-age-checked-internet-has-arrived/">In the UK</a>, users trying to access everything from pornography to social platforms must now submit to ID scans, credit card checks, age-estimation scans, and more to verify they’re over the age of 18. The state of Texas has a <a href="https://www.wired.com/story/us-supreme-court-porn-age-verification-decision-2025/">similar law</a> the US Supreme Court upheld in June, despite concerns from critics over the erosion of free speech and access to information on the open internet.</p><p>Whether these laws are effective at protecting children is unclear; the use of virtual private networks (VPNs) in the UK <a href="https://www.wired.com/story/vpn-use-spike-age-verification-laws-uk/">spiked</a> just after its age verification law went into effect as users deployed the tech to spoof their location. On platforms like Discord, people discovered they could use <a href="https://www.wired.com/story/age-verification-is-sweeping-gaming-is-it-ready-for-the-age-of-ai-fakes/">video game characters</a> to trick face scans. Furthermore, <a href="https://www.wired.com/story/the-age-checked-internet-has-arrived/">critics say</a> that age verification laws intended to reduce harm to children can sometimes have the opposite effect by putting kids in greater danger of identity theft and privacy violations.</p><p>WIRED has reached out to the sponsors of the original bill, Mississippi state representatives Jill Ford, Fabian Nelson, and Larry Byrd, and will update this story if they comment.</p><p>“We believe effective child safety policies should be carefully tailored to address real harms, without creating huge obstacles for smaller providers and resulting in negative consequences for free expression,” Bluesky wrote.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. government takes 10% stake in Intel (538 pts)]]></title>
            <link>https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html</link>
            <guid>44989773</guid>
            <pubDate>Fri, 22 Aug 2025 21:01:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html">https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html</a>, See on <a href="https://news.ycombinator.com/item?id=44989773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108186237" data-test="InlineImage"><p>Lip-Bu Tan, chief executive officer of Intel Corp., departs following a meeting at the White House in Washington, DC, US, on Monday, Aug. 11, 2025. </p><p>Alex Wroblewski | Bloomberg | Getty Images</p></div><div><p>Commerce Secretary Howard Lutnick said on Friday that the U.S. government has taken a 10% stake in embattle chipmaker Intel, the Trump administration's latest effort to exert control over corporate America. </p><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/INTC/">Intel</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> shares rose about 6% during trading on Friday. They were flat in extended trading.</p><p>Intel, the only American company capable of making advanced chips on U.S. soil, said in a press release that the government made an $8.9 billion investment in Intel common stock, purchasing 433.3 million shares at a price of $20.47 per share, giving it a 10% stake in the company. Intel noted that the price the government paid was a discount to the current market price.</p><p>Of the total, $5.7 billion of the government funds will come from grants under the CHIPS Act that had been awarded but not paid, and $3.2 billion will come from separate government awards under a program to make secure chips.</p><p>"The United States paid nothing for these Shares, and the Shares are now valued at approximately $11 Billion Dollars," President Trump wrote in a post on Truth Social. "This is a great Deal for America and, also, a great Deal for INTEL."&nbsp;</p><p>The government will also have a warrant to buy an additional 5% of Intel shares if the company is no longer majority owner of its foundry business.</p><p>Intel said that the U.S. government won't have a board seat or other governance rights.</p><p>"As the only semiconductor company that does leading-edge logic R&amp;D and manufacturing in the U.S., Intel is deeply committed to ensuring the world's most advanced technologies are American made," Intel CEO Lip-Bu Tan said in the press release. </p></div><h2><a id="headline0"></a>'A great deal for them'</h2><div><p>Earlier on Friday, <a href="https://www.cnbc.com/donald-trump/">President Donald Trump</a> said the government should get about 10% of the company, which has a market cap of just over $100 billion. &nbsp;</p><p>"They've agreed to do it and I think it's a great deal for them," Trump told reporters Friday at the White House</p><p>White House officials previously told CNBC that Trump and Tan will meet on Friday afternoon. Lutnick's post included a photo with Tan.</p></div><div id="RegularArticle-RelatedContent-1"><h2>Read more CNBC tech news</h2><div><ul><li><a href="https://www.cnbc.com/2025/08/22/apple-will-make-chips-at-texas-instruments-60-billion-us-project.html">Inside Texas Instruments' $60 billion U.S. megaproject, where Apple will make iPhone chips</a></li><li><a href="https://www.cnbc.com/2025/08/22/nvidia-in-talks-with-us-to-sell-more-advanced-chip-to-china-huang.html">Nvidia in talks with U.S. to sell a more advanced chip to China, Jensen Huang says</a></li><li><a href="https://www.cnbc.com/2025/08/22/deepseek-hints-latest-model-supported-by-chinas-next-generation-homegrown-ai-chips.html">DeepSeek hints latest model will be compatible with China's 'next generation' homegrown AI chips</a></li><li><a href="https://www.cnbc.com/2025/08/21/elon-musk-asked-meta-ceo-mark-zuckerberg-to-join-xai-bid-to-buy-openai.html">Elon Musk asked Meta CEO Mark Zuckerberg to join xAI bid to buy OpenAI, filing shows</a></li></ul></div></div><div><p>The marks the latest example of a distinct shift in U.S. industrial policy, with the government taking an active role in the private sector. Lutnick <a href="https://www.cnbc.com/2025/08/19/lutnick-intel-stock-chips-trump.html">told CNBC this week</a> that the U.S. government was seeking an equity stake in Intel in exchange for CHIPS Act funds.</p><p>"We should get an equity stake for our money," Lutnick said on CNBC's "<a href="https://www.cnbc.com/squawk-on-the-street/">Squawk on the Street</a>." "So we'll deliver the money, which was already committed under the Biden administration. We'll get equity in return for it."</p><p>Earlier this week, Intel announced another major backer, when SoftBank said it would make a $2 billion investment in the chipmaker, equal to about 2% of the company.</p><p>Intel's technology is seen as lagging <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-5"><a href="https://www.cnbc.com/quotes/TSM/">Taiwan Semiconductor Manufacturing Company</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, which makes chips for companies including Apple, Nvidia, Qualcomm, AMD, and even Intel.</p><p>Intel has been spending billions of dollars to build a series of chip factories in Ohio, an area the company previously called the "Silicon Heartland," where Intel would be able to produce the most advanced chips, including for AI.</p><p>But in July, Tan said in a memo to employees that there would be "no more blank checks," and that it was slowing down the construction of its Ohio factory complex, depending on market conditions. Intel's Ohio factory is now scheduled to start operations in 2030.</p><p>Intel said last fall that it had <a href="https://www.cnbc.com/2024/11/25/intel-close-to-8-billion-chips-act-grant-source.html">finalized</a> a nearly $8 billion grant under the CHIPS and Science Act to fund its factory-building plans. The CHIPS Act was passed in 2022, under the Biden administration.</p><p><em>— CNBC's David Sucherman contributed to this report.</em></p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/08/22/president-trump-says-intel-ceo-agreed-to-pay-u-s-10-percent-of-the-company.html">President Trump says Intel should transfer 10% of company to government</a></p></div><div id="Placeholder-ArticleBody-Video-108189803" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000386565" aria-labelledby="Placeholder-ArticleBody-Video-108189803"><p><img src="https://image.cnbcfm.com/api/v1/image/108189804-17558856121755885610-41301124345-1080pnbcnews.jpg?v=1755885611&amp;w=750&amp;h=422&amp;vtcrop=y" alt="President Trump says Intel CEO agreed to pay U.S. 10% of the company"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From $479 to $2,800 a month for ACA health insurance next year (113 pts)]]></title>
            <link>https://www.npr.org/sections/shots-health-news/2025/08/22/nx-s1-5511182/aca-tax-credits-health-insurance-open-enrollment</link>
            <guid>44989706</guid>
            <pubDate>Fri, 22 Aug 2025 20:56:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/sections/shots-health-news/2025/08/22/nx-s1-5511182/aca-tax-credits-health-insurance-open-enrollment">https://www.npr.org/sections/shots-health-news/2025/08/22/nx-s1-5511182/aca-tax-credits-health-insurance-open-enrollment</a>, See on <a href="https://news.ycombinator.com/item?id=44989706">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="resg-s1-84561">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/900/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/1800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg" sizes="(min-width: 1025px) 650px, calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/900/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/1800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg" sizes="(min-width: 1025px) 650px, calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/8064x6048+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2F76%2F8c%2F03623b794726ace9299404746519%2Fimg-0850.jpeg" alt="Ellen Allen is photographed outdoors. She is wearing glasses and smiling." fetchpriority="high">
        </picture>
</div>
<div>
    <div>
        <p>
                Ellen Allen, 63, needs health insurance to be able to keep paying for an expensive eye drop medicine that prevents blindness.
                <b aria-label="Image credit">
                    
                    Ellen Allen
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Ellen Allen
        
    </span>
</p></div>
   </div>
   <p>Next year, when her health care premium balloons, "it's gonna be a real hit," Ellen Allen says. "I'm worried about it."</p>   <p>Allen lives near Charleston, W.Va., and directs a small nonprofit called <a href="https://wvahc.org/" target="_blank">West Virginians for Affordable Health Care</a>. She buys her insurance on HealthCare.gov, and right now, the 63-year-old pays $479 a month. "I've been really happy with my coverage," she says.</p>   
   
<!-- END ID="RESNX-S1-5511182-100" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>All of that is changing soon. The federal tax credit that makes the coverage affordable for Allen and millions of other Americans expires at the end of the year. The credit was a pandemic-era relief measure that has contributed to record enrollment in the insurance sold through the Affordable Care Act marketplaces.</p>   <h3>Average spike? 75%</h3>   <p>The average enrollee will see their premium costs increase 75%, according to an <a href="https://www.healthsystemtracker.org/brief/early-indications-of-the-impact-of-the-enhanced-premium-tax-credit-expiration-on-2026-marketplace-premiums/" target="_blank">analysis of insurance filings</a> by the nonpartisan health research organization KFF. For many people, those increases will be even higher.</p>   
   <p>Allen, who's well-versed in these issues because of her job, used <a href="https://www.kff.org/interactive/how-much-more-would-people-pay-in-premiums-if-the-acas-enhanced-subsidies-expired/" target="_blank">KFF's online calculator</a> to estimate what her premium will be after the enhanced subsidies expire.</p>   <p>"Next year it's gonna be like $2,800 a month," she says, just for her individual plan. Her work organization is too small to provide affordable group coverage. She estimates that she could have $10,000 in out-of-pocket costs on top of her high premium. </p>   <p>She says it's still worth it to her to have the plan because she has expensive prescriptions. "Like an asthma medication [that] can run $700 a month. There's an eye drop medication that can be $800 a month," she says. "And these are the differences in keeping my vision, for example, so I have to do that."</p>   <p>She has started setting money aside every month and directing it into a separate account to start building up savings for those high premiums next year. "Luckily I can do that, but that's money I won't be able to save for investing in my 401(k) for retirement," she says.</p>   <h3>"I wish I were older"</h3>   <p>One good thing, she says, is that she'll turn 65 next year and will be able to enroll in Medicare, so she will be on the hook for the high premiums for only eight or nine months. "It's the first time in my life I wish I were older," she laughs.</p>   
   <p>Rates could change before open enrollment for <a href="http://healthcare.gov/" target="_blank">HealthCare.gov</a> and the state-based marketplaces begins Nov. 1. And Congress could also act before December to blunt the effect on enrollees, although the Republican lawmakers who control Congress have shown little interest in extending the subsidies. An extension of the tax credits was left out of President Trump's tax and spending law passed in July.</p>   <p>People who can't afford the higher premiums and are healthy enough will likely go without health insurance. Several enrollees told NPR that's their plan — to roll the dice, taking the chance that they stay well and don't have a big health expense.</p>   <p>The Congressional Budget Office estimates that the end of the enhanced tax credits will increase the number of uninsured people in the U.S. by <a href="https://www.cbo.gov/system/files/2025-06/Wyden-Pallone-Neal_Letter_6-4-25.pdf#page=3&amp;zoom=141,-30,678" target="_blank">4.2 million</a> over the next decade. More Americans are also likely to become uninsured because of cuts to the Medicaid program in the law, known as the One Big Beautiful Bill.</p>   
   
<!-- END ID="RESNX-S1-5511182-101" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <h3>A new job, maybe a new wife</h3>   <p>Sidney Clifton would really like to keep his HealthCare.gov plan. He says it works for him, and he has chronic health conditions. "Diabetes, I have congestive heart failure — just your normal overweight American, like everybody else," he says.</p>   <div id="resg-s1-84562">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/300/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 300w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 1600w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg" sizes="(min-width: 1350px) 298px, (min-width: 1025px) calc(((100vw - 365px)/3) - 30px), (min-width: 768px) calc(((100vw - 30px)/3) - 30px), calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/300/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 300w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg 1600w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg" sizes="(min-width: 1350px) 298px, (min-width: 1025px) calc(((100vw - 365px)/3) - 30px), (min-width: 768px) calc(((100vw - 30px)/3) - 30px), calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2316x3088+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F8a%2F5814230e47feb7a666da6a10c838%2Fimg-5352.jpeg" alt="Sidney Clifton is photographed driving in his car." loading="lazy">
        </picture>
</div>
<div>
    <div>
        <p>
                Sidney Clifton likes working for a small business, but he says he might need to look for a more corporate job with health benefits if he can't afford his health insurance in 2026.
                <b aria-label="Image credit">
                    
                    Sidney Clifton
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Sidney Clifton
        
    </span>
</p></div>
   </div>
   <p>Clifton is 54 and lives in Pasco County, in central Florida. "I work for a car dealership — it's a mom-and-pop store, not very big, like 10 employees," he says. He likes working for a small business, but it means no health benefits.</p>   <p>Right now, his full premium is about $1,100 per month, but with the enhanced subsidies, "my portion is $298." He doesn't know how much more he'll have to pay every month next year without the subsidies.</p>   <p>"I could probably go up to $800 to $1,000," he says. "$1,000 would be really, really pushing me hard."</p>   <p>If it's higher and he just can't afford it, he says, he might look for a job at a bigger corporate dealership that has benefits. </p>   
   <p>Or, he says, "I'll find me some woman [who has insurance] and get married again." He says he'd rather not do that.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Our Response to Mississippi's Age Assurance Law (162 pts)]]></title>
            <link>https://bsky.social/about/blog/08-22-2025-mississippi-hb1126</link>
            <guid>44989125</guid>
            <pubDate>Fri, 22 Aug 2025 20:00:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bsky.social/about/blog/08-22-2025-mississippi-hb1126">https://bsky.social/about/blog/08-22-2025-mississippi-hb1126</a>, See on <a href="https://news.ycombinator.com/item?id=44989125">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><header><section><a href="https://bsky.social/about/blog">Blog</a><svg width="8" height="12" viewBox="0 0 8 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2 10L6 6L2 2" stroke="#667999" stroke-width="2" stroke-linecap="square"></path></svg><span>Our Response to Mississippi’s Age Assurance Law</span></section><div><p>August 22, 2025</p><p><span>by <!-- -->The Bluesky Team</span></p></div></header><div><p>Keeping children safe online is a core priority for Bluesky. We’ve invested a lot of time and resources building moderation tools and other infrastructure to protect the youngest members of our community. We’re also aware of the tradeoffs that come with managing an online platform. Our mission is to build an open and decentralized protocol for public conversation, and we believe in empowering users with more choices and control over their experience. We work with regulators around the world on child safety—for example, Bluesky follows the UK's Online Safety Act, where age checks are required only for specific content and features.</p>
<p>Mississippi's approach would fundamentally change how users access Bluesky. The Supreme Court’s recent <a href="https://www.supremecourt.gov/opinions/24pdf/25a97_5h25.pdf">decision</a> leaves us facing a hard reality: comply with Mississippi’s age assurance <a href="https://legiscan.com/MS/text/HB1126/id/2988284">law</a>—and make <em>every</em> Mississippi Bluesky user hand over sensitive personal information and undergo age checks to access the site—or risk massive fines. The law would also require us to identify and track which users are children, unlike our approach in other regions. We think this law creates challenges that go beyond its child safety goals, and creates significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies.</p>
<p>Unlike tech giants with vast resources, we’re a small team focused on building decentralized social technology that puts users in control. Age verification systems require substantial infrastructure and developer time investments, complex privacy protections, and ongoing compliance monitoring — costs that can easily overwhelm smaller providers. This dynamic entrenches existing big tech platforms while stifling the innovation and competition that benefits users.</p>
<p>We believe effective child safety policies should be carefully tailored to address real harms, without creating huge obstacles for smaller providers and resulting in negative consequences for free expression. That’s why until legal challenges to this law are resolved, we’ve made the difficult decision to block access from Mississippi IP addresses. We know this is disappointing for our users in Mississippi, but we believe this is a necessary measure while the courts review the legal arguments.</p>
<p>Here’s more on our decision and what comes next.</p>
<h2>Why We’re Doing This</h2>
<p>Mississippi’s <a href="https://legiscan.com/MS/text/HB1126/id/2988284">HB1126</a> requires platforms to implement age verification for all users before they can access services like Bluesky. That means, under the law, we would need to verify every user’s age and obtain parental consent for anyone under 18. The potential penalties for non-compliance are substantial — up to $10,000 per user. Building the required verification systems, parental consent workflows, and compliance infrastructure would require significant resources that our small team is currently unable to spare as we invest in developing safety tools and features for our global community, particularly given the law's broad scope and privacy implications.</p>
<h2>Our Concerns About Mississippi’s Approach</h2>
<p>While we share the goal of protecting young people online, we have concerns about this law’s implementation:</p>
<ul>
<li>Broad scope: The law requires age verification for all users, not just those accessing age-restricted content, which affects the ability of everyone in Mississippi to use Bluesky.</li>
<li>Barriers to innovation: The compliance requirements disadvantage newer and smaller platforms like Bluesky, which do not have the luxury of big teams to build the necessary tooling. The law makes it harder for people to engage in free expression and chills the opportunity to communicate in new ways.</li>
<li>Privacy implications: The law requires collecting and storing sensitive personal information from all users, including detailed tracking of minors.</li>
</ul>
<h2>What We’re Doing</h2>
<p>Starting today, if you access Bluesky from a Mississippi IP address, you’ll see a message explaining why the app isn’t available. This block will remain in place while the courts decide whether the law will stand.</p>
<h2>How This Differs From Our Approach in Other Places</h2>
<p>Mississippi’s new law and the UK’s Online Safety Act (OSA) are very different. Bluesky <a href="https://bsky.social/about/blog/07-10-2025-age-assurance">follows</a> the OSA in the UK. There, Bluesky is still accessible for everyone, age checks are required only for accessing certain content and features, and Bluesky does not know and does not track which UK users are under 18. Mississippi’s law, by contrast, would block everyone from accessing the site—teens and adults—unless they hand over sensitive information, and once they do, the law in Mississippi requires Bluesky to keep track of which users are children.</p>
<h2>Other Apps on the Protocol</h2>
<p>This decision applies only to the Bluesky app, which is one service built on the AT Protocol. Other apps and services may choose to respond differently. We believe this flexibility is one of the strengths of decentralized systems—different providers can make decisions that align with their values and capabilities, especially during periods of regulatory uncertainty. We remain committed to building a protocol that enables openness and choice.</p>
<h2>What’s Next</h2>
<p>We do not take this decision lightly. Child safety is a core priority, and in this evolving regulatory landscape, we remain committed to building an open social ecosystem that protects users while preserving choice and innovation. We’ll keep you updated as this situation develops.</p></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nitro: A tiny but flexible init system and process supervisor (194 pts)]]></title>
            <link>https://git.vuxu.org/nitro/about/</link>
            <guid>44988530</guid>
            <pubDate>Fri, 22 Aug 2025 19:06:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://git.vuxu.org/nitro/about/">https://git.vuxu.org/nitro/about/</a>, See on <a href="https://news.ycombinator.com/item?id=44988530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="summary">

<h2>Overview</h2>
<p>Nitro is a tiny process supervisor that also can be used as pid 1 on Linux.</p>
<p>There are four main applications it is designed for:</p>
<ul>
<li>As init for a Linux machine for embedded, desktop or server purposes</li>
<li>As init for a Linux initramfs</li>
<li>As init for a Linux container (Docker/Podman/LXC/Kubernetes)</li>
<li>As unprivileged supervision daemon on POSIX systems</li>
</ul>
<p>Nitro is configured by a directory of scripts, defaulting to
<code>/etc/nitro</code> (or the first command line argument).</p>
<h2>Requirements</h2>
<ul>
<li>Kernel support for Unix sockets</li>
<li><code>tmpfs</code> or writable <code>/run</code> on another fs</li>
</ul>
<h2>Benefits over other systems</h2>
<ul>
<li>All state is kept in RAM, works without tricks on read-only root file systems.</li>
<li>Efficient event-driven, polling free operation.</li>
<li>Zero memory allocations during runtime.</li>
<li>No unbounded file descriptor usage during runtime.</li>
<li>One single self-contained binary, plus one optional binary to
control the system.</li>
<li>No configuration compilation steps needed, services are simple
directories containing scripts.</li>
<li>Supports reliable restarting of services.</li>
<li>Reliable logging mechanisms per service or as default.</li>
<li>Support for logging chains spread over several services.</li>
<li>Works independently of properly set system clock.</li>
<li>Can be run on FreeBSD from /etc/ttys (sets up file descriptors 0, 1, 2).</li>
<li>Tiny static binary when using musl libc.</li>
</ul>
<h2>Services</h2>
<p>Every directory inside <code>/etc/nitro</code> (or your custom service directory)
can contain several files:</p>
<ul>
<li><code>setup</code>, an optional executable file that is run before the service starts.
It must exit with status 0 to continue.</li>
<li><code>run</code>, an optional executable file that runs the service;
it must not exit as long as the service is considered running.
If there is no <code>run</code> script, the service is considered a “one shot”,
and stays “up” until it’s explicitly taken “down”.</li>
<li><code>finish</code>, an optional executable file that is run after the <code>run</code>
process finished.  It is passed two arguments, the exit status
of the <code>run</code> process (or -1 if it was killed by a signal)
and the signal that killed it (or 0, if it exited regularly).</li>
<li><code>log</code>, a symlink to another service directory.
The standard output of <code>run</code> is connected to the standard input of the
service under <code>log</code> by a pipe.  You can chain these for reliable and
supervised log processing.</li>
<li><code>down</code>, an optional file that causes nitro to not bring up this
service by default.</li>
<li>Service directories ending with ‘@’ are ignored; they can be used
for parameterized services.</li>
<li>Service names must be shorter than 64 chars, and not contain <code>/</code>,
<code>,</code> or newlines.</li>
</ul>
<p>You may find runit’s <code>chpst</code> useful when writing <code>run</code> scripts.</p>
<h2>Special services</h2>
<ul>
<li><code>LOG</code>: this service is used as a logging service for all services
that don’t have a <code>log</code> symlink.</li>
<li><code>SYS</code>: <code>SYS/setup</code> is run before other services are brought up.
You can already use <code>nitroctl</code> in <code>SYS/setup</code> to bring up services
in a certain order.
<code>SYS/finish</code> is run before all remaining services are killed and the
system is brought down.
After all processes are terminated, <code>SYS/final</code> is run.
The program <code>SYS/fatal</code>, if it exists, is run instead of exiting
when an unrecoverable, fatal error happens.
The program <code>SYS/reincarnate</code>, if it exists, is executed into
instead of a shutdown.  This can be used to implement an initramfs,
for example.</li>
</ul>
<h2>Parametrized services</h2>
<p>Service directories ending in <code>@</code> are ignored, however you can refer
to parametrized services by symlinks (either in the service directory
or as a <code>log</code> symlink), or start them manually using <code>nitroctl</code>.</p>
<p>The part after the <code>@</code>, the parameter, is passed to the scripts as
first argument.</p>
<p>For example, given you have a script <code>agetty@/run</code> and a symlink
<code>agetty@tty1</code> -&gt; <code>agetty@</code>, nitro will spawn <code>agetty@/run tty1</code>.  Upon
running <code>nitroctl up agetty@tty2</code>, nitro will spawn <code>agetty@/run tty2</code>, even if it does not exist in the service directory.</p>
<h2>Modes of operation</h2>
<p>The lifecycle of a machine/container/session using nitro consists of
three phases.</p>
<p>First, the system is brought up.  If there is a special service
g<code>SYS</code>, its <code>setup</code> script is run first.  After it finishes, all
services not marked <code>down</code> are brought up.</p>
<p>When a service exits, it’s being restarted, potentially waiting for
two seconds if the last restart happened too quickly.</p>
<p>By using <code>nitroctl Reboot</code> or <code>nitroctl Shutdown</code>, the system can be
brought down.  If it exists, <code>SYS/finish</code> will be run.  After this,
nitro will send a SIGTERM signal to all running services and waits for
up to 7 seconds for the service to exit.  Otherwise, a SIGKILL is
sent.  After all processes are terminated, <code>SYS/final</code> is run.</p>
<p>Finally, nitro reboots or shuts down the system; or just exits when it
was used as a container init or unprivileged supervisor.  (When a
reboot was requested, it re-execs itself.  This requires being called
with absolute path for the binary and the service directory.)</p>
<h2>Controlling nitro with nitroctl</h2>
<p>You can remote control a running nitro instance using the tool
<code>nitroctl</code>.</p>
<p>Usage: <code>nitroctl [COMMAND] [SERVICE]</code></p>
<p>Where COMMAND is one of:</p>
<ul>
<li>list: show a list of services and their state, pid, uptime and last
exit status.</li>
<li>up: start SERVICE</li>
<li>down: stop SERVICE (sending SIGTERM or the first letter of <code>./down-signal</code>)</li>
<li>start: start SERVICE, waiting for success</li>
<li>restart: restart SERVICE, waiting for success</li>
<li>stop: stop SERVICE, waiting for success</li>
<li>p: send signal SIGSTOP to SERVICE</li>
<li>c: send signal SIGCONT to SERVICE</li>
<li>h: send signal SIGHUP to SERVICE</li>
<li>a: send signal SIGALRM to SERVICE</li>
<li>i: send signal SIGINT to SERVICE</li>
<li>q: send signal SIGQUIT to SERVICE</li>
<li>1: send signal SIGUSR1 to SERVICE</li>
<li>2: send signal SIGUSR2 to SERVICE</li>
<li>t: send signal SIGTERM to SERVICE</li>
<li>k: send signal SIGKILL to SERVICE</li>
<li>pidof: print the PID of the SERVICE, or return 1 if it’s not up</li>
<li>rescan: re-read <code>/etc/nitro</code>, start added daemons, stop removed daemons</li>
<li>Shutdown: shutdown (poweroff) the system</li>
<li>Reboot: reboot the system</li>
</ul>
<h2>Controlling nitro by signals</h2>
<p>rescan can also be triggered by sending <code>SIGHUP</code> to nitro.</p>
<p>reboot can also be triggered by sending <code>SIGINT</code> to nitro.</p>
<p>shutdown can also be triggered by sending <code>SIGTERM</code> to nitro, unless
nitro is used as Linux pid 1.</p>
<h2>Nitro as <code>init</code> for Linux</h2>
<p>Nitro is self-contained and can be booted directly as pid 1.
It will mount <code>/dev</code> and <code>/run</code> when required, everything else
should be done with <code>SYS/setup</code>.</p>
<p>When receiving Ctrl-Alt-Delete, nitro triggers an orderly reboot.</p>
<h2>Nitro as init for a Docker container</h2>
<p>Nitro is compiled statically, so you can copy it into your container easily:</p>
<pre><code>COPY ./nitro /bin/
COPY ./nitroctl /bin/
CMD ["/bin/nitro"]
</code></pre>
<p>Note that <code>/run</code> must exist in the container if you want to use the
default control socket name.</p>
<p>You can put the control socket onto a bind mount and remote control
<code>nitro</code> using <code>nitroctl</code> from the outside by pointing <code>NITRO_SOCK</code> to
the appropriate target.</p>
<h2>Nitro on FreeBSD</h2>
<p>You can add this line to <code>/etc/ttys</code> to run <code>nitro</code> supervised by
FreeBSD <code>init</code>:</p>
<pre><code>/etc/nitro "/usr/local/sbin/nitro" "" on
</code></pre>
<h2>Authors</h2>
<p>Leah Neukirchen <a href="mailto:leah@vuxu.org">leah@vuxu.org</a></p>
<h2>Thanks</h2>
<p>I’m standing on the shoulder of giants; this software would not have
been possible without detailed study of prior systems such as
daemontools, freedt, runit, perp, and s6.</p>
<h2>Copying</h2>
<p>nitro is licensed under the 0BSD license, see LICENSE for details.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists just found a protein that reverses brain aging (128 pts)]]></title>
            <link>https://www.sciencedaily.com/releases/2025/08/250820000808.htm</link>
            <guid>44988393</guid>
            <pubDate>Fri, 22 Aug 2025 18:56:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencedaily.com/releases/2025/08/250820000808.htm">https://www.sciencedaily.com/releases/2025/08/250820000808.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44988393">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="first">Aging is particularly harsh on the hippocampus -- the brain region responsible for learning and memory.</p><div id="text">
<p>Now, researchers at UC San Francisco have identified a protein that's at the center of this decline.</p>
<p>They looked at how the genes and proteins in the hippocampus changed over time in mice and found just one that differed between old and young animals. It's called FTL1.</p>
<p>Old mice had more FTL1, as well as fewer connections between brain cells in the hippocampus and diminished cognitive abilities.</p>
<p>When the researchers artificially increased FTL1 levels in young mice, their brains and behavior began to resemble that of old mice.</p>
<p>In experiments in petri dishes, nerve cells engineered to make lots of FTL1 grew simple, one-armed neurites -- rather than the branching neurites that normal cells create.</p>
<p>But once the scientists reduced the amount of FTL1 in the hippocampus of the old mice, they regained their youth. They had more connections between nerve cells, and the mice did better on memory tests.</p>


<p>"It is truly a reversal of impairments," said Saul Villeda, PhD, associate director of the UCSF Bakar Aging Research Institute and senior author of the paper, which appears in <em>Nature Aging</em> on Aug. 19. "It's much more than merely delaying or preventing symptoms."</p>
<p>In old mice, FTL1 also slowed down metabolism in the cells of the hippocampus. But treating the cells with a compound that stimulates metabolism prevented these effects.</p>
<p>Villeda is optimistic the work could lead to therapies that block the effects of FTL1 in the brain.</p>
<p>"We're seeing more opportunities to alleviate the worst consequences of old age," he said. "It's a hopeful time to be working on the biology of aging."</p>
<p>Authors: Other UCSF authors are Laura Remesal, PhD, Juliana Sucharov-Costa, Karishma J.B. Pratt, PhD, Gregor Bieri, PhD, Amber Philp, PhD, Mason Phan, Turan Aghayev, MD, PhD, Charles W. White III, PhD, Elizabeth G. Wheatley, PhD, Brandon R. Desousa, Isha H. Jian, Jason C. Maynard, PhD, and Alma L. Burlingame, PhD. For all authors see the paper.</p>
<p>Funding: This work was funded in part by the Simons Foundation, Bakar Family Foundation, National Science Foundation, Hillblom Foundation, Bakar Aging Research Institute, Marc and Lynne Benioff, and the National Institutes of Health (AG081038, AG067740, AG062357, P30 DK063720). For all funding see the paper.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: JavaScript-free (X)HTML Includes (155 pts)]]></title>
            <link>https://github.com/Evidlo/xsl-website</link>
            <guid>44988271</guid>
            <pubDate>Fri, 22 Aug 2025 18:47:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Evidlo/xsl-website">https://github.com/Evidlo/xsl-website</a>, See on <a href="https://news.ycombinator.com/item?id=44988271">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">XSL-Website</h2><a id="user-content-xsl-website" aria-label="Permalink: XSL-Website" href="#xsl-website"></a></p>
<p dir="auto">This is a simple example of using browsers' built in XSL support to build a website with common theming across all pages without any server-side code, static website generators, or Javascript.</p>
<p dir="auto"><a href="http://evan.widloski.com/xsl-website/" rel="nofollow">See the demo site</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto">When you browse to <code>index.xml</code> (or any of the other XML files), the browser loads the template file given at the top of the XML.  This template file describes how to render the various custom tags in the XML as HTML.</p>
<p dir="auto">See <code>advanced/</code> for more advanced examples of defining templates with fields, or using templates inside of other templates.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The First Media over QUIC CDN: Cloudflare (238 pts)]]></title>
            <link>https://moq.dev/blog/first-cdn/</link>
            <guid>44987924</guid>
            <pubDate>Fri, 22 Aug 2025 18:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://moq.dev/blog/first-cdn/">https://moq.dev/blog/first-cdn/</a>, See on <a href="https://news.ycombinator.com/item?id=44987924">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <p>
published 8/21/2025 </p> 
<p>🚨 It’s finally happening! 🚨</p>
<p>Cloudflare has <a href="https://blog.cloudflare.com/moq/">just announced</a> their Media over QUIC CDN!
It’s an <strong>official product</strong>, and you can test MoQ on their <em>massive</em>, anycast network.
Try it out, and convince your boss’ boss that the writing is on the wall.</p>
<p>If you’ve been living under a rock, MoQ is an <a href="https://datatracker.ietf.org/group/moq/about/">up-and-coming standard</a> for live media, aiming to supplant <a href="https://moq.dev/blog/replacing-webrtc">WebRTC</a>, <a href="https://moq.dev/blog/replacing-hls-dash">HLS/DASH</a>, and even <strong>RTMP/SRT</strong> as the one to rule them all.
And now Cloudflare wins the award for the first CDN offering!</p>
<figure><p><img src="https://moq.dev/blog/first-cdn/cloudflare.png" alt="Cloudflare logo">
</p><figcaption>Your prize is a blog post. You’re welcome mega-corp.</figcaption></figure>
<p>Also, <em>while you’re here</em>, some shameless self-promotion: I just soft-launched <a href="https://moq.dev/blog/first-app">hang.live</a>.
Check it out if you want to see the <del>cringe</del> cool stuff you can do with MoQ.</p>
<h2 id="whats-available-now">What’s available now?</h2>
<p>This is a <a href="https://developers.cloudflare.com/moq/">technical preview</a>, so it’s both free and subject to change.</p>
<p>Cloudflare is hosting a public <code>relay.cloudflare.mediaoverquic.com</code> endpoint that you can <del>abuse</del> test.
Connect using <a href="https://github.com/kixelated/moq">my library</a>, <a href="https://github.com/englishm/moq-rs">Mike’s fork</a>, <a href="https://www.meetecho.com/blog/imquic/">Lorenzo’s imquic</a>, <a href="https://github.com/facebookexperimental/moxygen">Meta’s moxygen</a>, or any client that supports this limited subset of draft-07.</p>
<p>I’m biased so naturally I’m going to use <a href="https://github.com/kixelated/moq/tree/main/js/hang">@kixelated/hang</a> (smash that star button).
You can publish a live broadcast in the browser using the <a href="https://moq.dev/publish">web demo</a> or the <a href="https://github.com/kixelated/moq/blob/main/js/hang-demo/src/publish.html#L25">library</a>:</p>
<pre tabindex="0" data-language="html"><code><span><span>&lt;</span><span>script</span><span> type</span><span>=</span><span>"module"</span><span>&gt;</span></span>
<span><span>	// Registers the &lt;hang-publish&gt; element.</span></span>
<span><span>	import</span><span> "@kixelated/hang/publish/element"</span><span>;</span></span>
<span><span>&lt;/</span><span>script</span><span>&gt;</span></span>
<span></span>
<span><span>&lt;!-- You'll need to replace `name` with something unique/random. --&gt;</span></span>
<span><span>&lt;</span><span>hang-publish</span><span> url</span><span>=</span><span>"https://relay.cloudflare.mediaoverquic.com"</span><span> name</span><span>=</span><span>"unique-name-abc123"</span><span> audio</span><span> video</span><span> controls</span><span> captions</span><span>&gt;</span></span>
<span><span>	&lt;!-- It's optional to provide a video element to preview the outgoing media. --&gt;</span></span>
<span><span>	&lt;</span><span>video</span><span> style</span><span>=</span><span>"max-width: 100%; height: auto; border-radius: 4px; margin: 0 auto;"</span><span> muted</span><span> autoplay</span><span>&gt;&lt;/</span><span>video</span><span>&gt;</span></span>
<span><span>&lt;/</span><span>hang-publish</span><span>&gt;</span></span></code></pre>
<p>There’s a link to watch your live broadcast using the <a href="https://moq.dev/watch">web demo</a>, or again you can use the <a href="https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/js/hang-demo/src/index.html#L30">library</a>:</p>
<pre tabindex="0" data-language="html"><code><span><span>&lt;</span><span>script</span><span> type</span><span>=</span><span>"module"</span><span>&gt;</span></span>
<span><span>	// Registers the &lt;hang-watch&gt; element.</span></span>
<span><span>	import</span><span> "@kixelated/hang/watch/element"</span><span>;</span></span>
<span><span>&lt;/</span><span>script</span><span>&gt;</span></span>
<span></span>
<span><span>&lt;!-- Use the same name as the broadcast you published. --&gt;</span></span>
<span><span>&lt;</span><span>hang-watch</span><span> url</span><span>=</span><span>"https://relay.cloudflare.mediaoverquic.com"</span><span> name</span><span>=</span><span>"unique-name-abc123"</span><span> muted</span><span> controls</span><span> captions</span><span>&gt;</span></span>
<span><span>	&lt;!-- It's optional to provide a canvas if you want audio only --&gt;</span></span>
<span><span>	&lt;</span><span>canvas</span><span> style</span><span>=</span><span>"max-width: 100%; height: auto; border-radius: 4px; margin: 0 auto;"</span><span>&gt;&lt;/</span><span>canvas</span><span>&gt;</span></span>
<span><span>&lt;/</span><span>hang-watch</span><span>&gt;</span></span></code></pre>
<p>You might even notice <strong>closed captions</strong> because I’ve been experimenting with AI features (gotta get funding eventually 💰).
They’re generated <em>in the browser</em> using <a href="https://github.com/snakers4/silero-vad">silero-vad</a> + <a href="https://github.com/openai/whisper">whisper</a> + <a href="https://huggingface.co/docs/transformers.js/en/index">transformers.js</a> + <a href="https://github.com/microsoft/onnxruntime">onnxruntime-web</a> + <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API">WebGPU</a> and transmitted using MoQ of course.
But that’s a whole separate blog post; it’s pretty cool.</p>
<p><strong>NOTE:</strong> You don’t have to use this <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components">Web Component</a> API.
<a href="https://moq.dev/blog/first-app">hang.live</a> uses the far more powerful Javascript API to do more complicated stuff like get access to individual video frames.
There’s a <em>super secret</em> section at the end of this blog if you LOVE sample code, but I’m not going to bore the rest of you.</p>
<p>There’s also a 🦀 Rust 🦀 library <a href="https://github.com/kixelated/moq/tree/main/rs/hang">to import MP4</a>, <a href="https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/rs/justfile#L103">pipe media from ffmpeg</a>, and <a href="https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/rs/justfile#L119">publish/watch using gstreamer</a> so you can do more complicated media stuff without 🤮 Javascript 🤮.
I wish I could spend more time on the Rust side but <strong>WebSupport</strong> is a big deal.
We are no longer forced to use WebRTC, but that also means we need to build our own WebRTC in 🤮 Javascript 🤮.
I can suffer and you can reap the rewards.</p>
<p>(and yes, <a href="https://moq.dev/blog/to-wasm">I’m aware that WASM exists</a>, but I ended up abandoning it)</p>
<h2 id="whats-not-available-yet">What’s not available yet?</h2>
<p>This is a <strong>preview</strong> release.
Cloudflare is only supporting a <em>tiny</em> subset of an <a href="https://www.ietf.org/archive/id/draft-ietf-moq-transport-07.html">old draft</a>, which is even smaller than <a href="https://www.ietf.org/archive/id/draft-lcurley-moq-lite-01.html">my tiny subset</a>.
They’re using a <a href="https://github.com/englishm/moq-rs">fork</a> of my terrible code so bugs are guaranteed.</p>
<ul>
<li><strong>There’s no authentication yet</strong>: choose an unguessable name for each broadcast.</li>
<li><strong>There’s no ANNOUNCE support</strong>: my <a href="https://github.com/kixelated/moq/blob/main/js/hang-demo/src/meet.html">conferencing example</a> uses <strong>ANNOUNCE</strong> to discover when broadcasts start/stop, so that won’t work.</li>
<li><strong>There’s no <a href="https://caniuse.com/webtransport">Safari support</a></strong>: <a href="https://github.com/WebKit/standards-positions/issues/18#issuecomment-1495890122">It’s coming eventually</a>.</li>
<li><strong>Nothing has been optimized</strong>: the user experience will improve over time.</li>
</ul>
<p>If any of these are deal breakers, then you could always run your own <a href="https://github.com/kixelated/moq/tree/main/rs/moq-relay">moq-relay</a> in the meantime.
I’ve been adding new features and fixing a bunch of stuff <em>after</em> Cloudflare smashed that fork button.
For example, authentication (via JWT) and a WebSocket fallback for Safari/TCP support.</p>
<p>There’s even a <a href="https://github.com/kixelated/moq.dev/blob/main/infra/relay.tf">terraform module</a> that powers <code>relay.moq.dev</code>.
You too can run your own “global” CDN with 3 nodes and pay GCP a boatload of money for the privilege.
It’s not <em>quite</em> as good as Cloudflare’s network, currently available for free…</p>
<figure><p><img src="https://moq.dev/blog/first-cdn/global.png" alt="A global CDN">
</p><figcaption>A “global” CDN according to me, an American. At least I didn’t <a href="https://www.reddit.com/r/MapsWithoutNZ/">forget New Zealand</a>.</figcaption></figure>
<p>Or host <strong>moq-relay</strong> yourself!
It should even work on private networks provided you <a href="https://moq.dev/blog/tls-and-quic">wrestle with TLS certificates</a>.
I’d also love to get MoQ running over <a href="https://www.iroh.computer/">Iroh</a> for peer-to-peer action if anybody wants to help.</p>
<h2 id="why-should-you-care">Why should you care?</h2>
<p>As a great philosopher once said:</p>
<blockquote>
<p>Apathy is a tragedy and boredom is a crime.
- <a href="https://www.youtube.com/watch?v=k1BneeJTDcU">Bo Burnham</a></p>
</blockquote>
<p>This is a big deal.
The biggest of deals.
The HUGEST of deals.</p>
<p>I’ve been an <a href="https://moq.dev/blog/transfork">outspoken critic</a> of the MoQ standardization process.
It’s just really difficult to design a protocol, via a cross-company committee, before there’s been any real world usage.
It’s been over 3 years since I fought Amazon lawyers and published my <a href="https://www.ietf.org/archive/id/draft-lcurley-warp-00.html">first MoQ draft</a>.
It’s going to be at least another 3 years before even the <a href="https://datatracker.ietf.org/doc/draft-ietf-moq-transport/">base networking layer</a> becomes an RFC.</p>
<p><strong>And that’s by design!</strong>
The best standards take a while.
Look no further than QUIC, deployed by Google in 2012, started standardization in 2015, with the RFC released in 2021.
And they had a boatload of production data to shape the specification.
Meanwhile, we have only had a <a href="https://moq.dev/watch">Big Buck Bunny demo</a>, and I believe the standard has veered off course as a result.</p>
<p>Cloudflare has done something fantastic and said:</p>
<blockquote>
<p>fuck waiting for a RFC, let’s release something</p>
</blockquote>
<p>Okay they didn’t say that, but this is <strong>exactly</strong> the mentality that MoQ needs right now.
<strong>Just build something</strong>.
<strong>Just release something</strong>.
<strong>Just do it</strong>.</p>
<figure><iframe width="560" height="315" src="https://www.youtube.com/embed/ZXsQAXx_ao0?si=xsAscm04CnwAer4b" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe><figcaption>Holy shit I’m Shia LaBeouf.</figcaption></figure>
<p>Arguing in the <a href="https://github.com/moq-wg/moq-transport/issues">650+ issues</a> and <a href="https://github.com/moq-wg/moq-transport/pulls">500+ PRs</a> can wait for another day.
Tweaking the messaging encoding for the hundredth time can wait for another day.
We’re still going to make sure that MoQ gets standardized <em>eventually</em>, but it’s more important to get <em>something</em> out there.</p>
<p>I’m looking at you: Google, Akamai, Fastly, etc.
Take some code, run it on some spare servers, and start to learn what customers need <em>before</em> you design the protocol.</p>
<h2 id="whats-next">What’s next?</h2>
<p>A lot of stuff.</p>
<p>We’re effectively trying to reimplement WebRTC / HLS / RTMP using relatively new Web APIs.
Don’t judge MoQ based on these initial offerings.
We’ve got a <strong>ton</strong> of work to do.
<strong>Let’s do it</strong>.</p>
<p><a href="https://discord.gg/FCYF3p99mr">Join the Discord</a>.
Somehow there’s 900+ people in there.
Ping me and I will do whatever I can to help.
<em>Especially</em> if it means putting one more nail in the WebRTC coffin.</p>
<p>Written by <a href="https://github.com/kixelated">@kixelated</a>.
<img src="https://moq.dev/blog/avatar.png" alt="@kixelated"></p>
<h2 id="javascript-is-an-abomination">Javascript is an Abomination</h2>
<p>Still reading?</p>
<p>You win some bonus documentation.
Congrats!
I knew you would win.</p>
<p>Here’s an example of my reactive library in action.
It powers <a href="https://moq.dev/blog/first-app">hang.live</a> so the API is subject to change and is probably already out of date.
When in doubt, <a href="https://github.com/kixelated/moq/tree/main/js/hang">consult the source code</a> like the hacker you are.</p>
<pre tabindex="0" data-language="typescript"><code><span><span>import</span><span> { Watch } </span><span>from</span><span> "@kixelated/hang"</span></span>
<span></span>
<span><span>// Start downloading a broadcast.</span></span>
<span><span>const</span><span> watch</span><span> =</span><span> new</span><span> Watch.</span><span>Broadcast</span><span>({</span></span>
<span><span>	enabled: </span><span>true</span><span>,</span></span>
<span><span>	url: </span><span>"https://relay.cloudflare.mediaoverquic.com"</span><span>,</span></span>
<span><span>	name: </span><span>"unique-name-abc123"</span><span>,</span></span>
<span><span>	video: { enabled: </span><span>true</span><span> },</span></span>
<span><span>	reload: </span><span>false</span><span>, </span><span>// required for Cloudflare's CDN</span></span>
<span><span>});</span></span>
<span></span>
<span><span>// You can toggle reactive properties.</span></span>
<span><span>watch.audio.enabled.</span><span>set</span><span>(</span><span>true</span><span>);</span></span>
<span></span>
<span><span>// There are helpers to convert my custom signals, like for React:</span></span>
<span><span>import</span><span> react </span><span>from</span><span> "@kixelated/signals/react"</span></span>
<span><span>const</span><span> audioInfo</span><span> =</span><span> react</span><span>(watch.audio.info); </span><span>// a JSON blob of track information</span></span>
<span></span>
<span><span>// You could use the built-in renderers.</span></span>
<span><span>const</span><span> canvas</span><span> =</span><span> document.</span><span>getElementById</span><span>(</span><span>"canvas"</span><span>);</span></span>
<span><span>const</span><span> audio</span><span> =</span><span> new</span><span> Watch.</span><span>AudioEmitter</span><span>(watch.audio, { volume: </span><span>0.5</span><span> });</span></span>
<span><span>const</span><span> video</span><span> =</span><span> new</span><span> Watch.</span><span>VideoRenderer</span><span>(watch.video, { canvas });</span></span>
<span></span>
<span><span>// Or you can do it yourself, like this crude Vanilla JS example:</span></span>
<span><span>const</span><span> dispose</span><span> =</span><span> watch.video.frame.</span><span>subscribe</span><span>((</span><span>frame</span><span>?:</span><span> VideoFrame</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>	if</span><span> (</span><span>!</span><span>frame) </span><span>return</span><span>;</span></span>
<span></span>
<span><span>	// Render the frame to a canvas, or pass it to a ML model, or whatever.</span></span>
<span><span>	canvas.</span><span>getContext</span><span>(</span><span>"2d"</span><span>)?.</span><span>drawImage</span><span>(frame, </span><span>0</span><span>, </span><span>0</span><span>);</span></span>
<span></span>
<span><span>	// NOTE: You should use requestAnimationFrame instead, but I'm lazy.</span></span>
<span><span>});</span></span></code></pre>
<p>There’s even some <em>top-secret</em> features behind undocumented APIs.
Like running an object detection model in browser and publishing the results as a MoQ track.
Stay tuned for a blog post about that if I can figure out a better use-case than a cat cam. 🐈</p>
<pre tabindex="0" data-language="typescript"><code><span><span>// Publish a broadcast.</span></span>
<span><span>const</span><span> publish</span><span> =</span><span> new</span><span> Watch.</span><span>Publish</span><span>({</span></span>
<span><span>	enabled: </span><span>true</span><span>,</span></span>
<span><span>	url: </span><span>"https://relay.cloudflare.mediaoverquic.com"</span><span>,</span></span>
<span><span>	name: </span><span>"unique-name-abc123"</span><span>,</span></span>
<span><span>	device: </span><span>"camera"</span><span>,</span></span>
<span><span>	video: {</span></span>
<span><span>		enabled: </span><span>true</span><span>,</span></span>
<span><span>		detection: {</span></span>
<span><span>			enabled: </span><span>true</span><span>,</span></span>
<span><span>		}</span></span>
<span><span>	},</span></span>
<span><span>})</span></span></code></pre>
<p>Also, for the record, Typescript is really nice.
🤮 Javascript 🤮 is still an abomination.</p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Should the web platform adopt XSLT 3.0? (118 pts)]]></title>
            <link>https://github.com/whatwg/html/issues/11578</link>
            <guid>44987552</guid>
            <pubDate>Fri, 22 Aug 2025 17:56:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/whatwg/html/issues/11578">https://github.com/whatwg/html/issues/11578</a>, See on <a href="https://news.ycombinator.com/item?id=44987552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><h3 dir="auto">What is the issue with the HTML Standard?</h3>
<h3 dir="auto">Background</h3>
<p dir="auto">This is a follow-up to <a data-error-text="Failed to load title" data-id="3285251497" data-permission-text="Title is private" data-url="https://github.com/whatwg/html/issues/11523" data-hovercard-type="issue" data-hovercard-url="/whatwg/html/issues/11523/hovercard" href="https://github.com/whatwg/html/issues/11523">#11523</a>.  That issue raises concerns regarding security issues, code maintainability, and the complexity of aging browser code currently used for rendering XML with XSLT stylesheets.  I highly recommend reading through that issue first as it includes a good background of XSLT on the web as well as the specific concerns that may lead to deprecating XSLT as the most prudent option for browser vendors.</p>
<p dir="auto">As a reminder, please read the whatwg <a href="https://whatwg.org/code-of-conduct" rel="nofollow">Code of Conduct</a>.  This issue is intended to be a discussion of alternatives to deprecations and how the concerns raised in <a data-error-text="Failed to load title" data-id="3285251497" data-permission-text="Title is private" data-url="https://github.com/whatwg/html/issues/11523" data-hovercard-type="issue" data-hovercard-url="/whatwg/html/issues/11523/hovercard" href="https://github.com/whatwg/html/issues/11523">#11523</a> may be handled to mitigate the risks for both browser maintainers and users.  Please stay on topic here by discussing the pros and cons of XSLT v3.0 support, alternatives, etc.  Discussions related to assumed motivations, specific browser vendors, etc. aren't relevant here and will likely just result in the issue being locked, preventing us from having the important discussion in public here.</p>
<h3 dir="auto">What is the issue with the HTML Standard?</h3>
<p dir="auto">The XSLT 1.0 specification was originally <a href="https://www.w3.org/TR/xslt-10/" rel="nofollow">standardized in 1999</a>.  Though the W3C has subsequently defined 2.0 and 3.0 specifications for XSLT, most browsers today never added support beyond v1.0.  In addition, cross-browser functionality for XSLT is incomplete with certain features like <code>&lt;xsl:text disable-output-escaping="yes"&gt;</code> having open issues dating back <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=98168" rel="nofollow">over two decades</a> with no fix in sight.  I raise longstanding issues in general as a challenge related to the concerns of maintainability and relatively little use of XSLT 1.0 today, not as a concern over prioritization or resource allocation by browser vendors.</p>
<p dir="auto">The question of use of XSLT today, raised in <a data-error-text="Failed to load title" data-id="3285251497" data-permission-text="Title is private" data-url="https://github.com/whatwg/html/issues/11523" data-hovercard-type="issue" data-hovercard-url="/whatwg/html/issues/11523/hovercard" href="https://github.com/whatwg/html/issues/11523">#11523</a>, raises an interesting question.  Is the low usage statistics of XSLT today an indication of the usefulness of the specification in today's web, or an indication of the challenges of dealing with v1.0 limitations and bugs?  Deprecating XSLT entirely assumes that the issue related to use relies only on XSLT itself and that use would go effectively unchanged if the improvements from v2.0 and v3.0 were available in browsers.  Anecdotally I can say that I have run into blocking issues with v1.0 itself; I built a blogging CMS as a hobby project that was designed to lean entirely on XSLT to render CMS users' RSS feeds to complete, customizable websites.  Between vendor-specific XSLT bugs and the limitations of a now 25 year old spec, I found that I would only be able to build the CMS with a more recent version of the XSLT specification.</p>
<h3 dir="auto">Alternatives to deprecation</h3>
<p dir="auto">I don't intend to prescribe a solution here!  Instead, these open questions (and others I missed) are important when weighing the relative merits with regards to deprecating XSLT entirely.</p>
<ul dir="auto">
<li>Are there external libraries that browsers could adopt in order to gain v3.0 support without building and maintaining the specification in house?</li>
<li>Are there security concerns relevant to v3.0, or are the concerns limited to the v1.0 specification and current implementations?</li>
<li>Is there an approach that would allow browser vendors to use a single implementation for XSLT, combining efforts to reduce cost on each vendor?</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The US Department of Agriculture Bans Support for Renewables (174 pts)]]></title>
            <link>https://insideclimatenews.org/news/19082025/usda-bans-farm-renewables-support/</link>
            <guid>44987539</guid>
            <pubDate>Fri, 22 Aug 2025 17:55:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://insideclimatenews.org/news/19082025/usda-bans-farm-renewables-support/">https://insideclimatenews.org/news/19082025/usda-bans-farm-renewables-support/</a>, See on <a href="https://news.ycombinator.com/item?id=44987539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

			
				<article id="post-98509">

					
						
<!-- .entry-header -->

					
					<div>

						
<p>The U.S. Department of Agriculture announced this week that it will stop funding wind and solar energy on American farmland, a move that continues the Trump administration’s attempts to kill incentives for renewables while it boosts support for fossil fuels and land-hungry, energy inefficient biofuels.</p>



<p>At the state fairgrounds in Lebanon, Tennessee, Agriculture Secretary Brooke Rollins said Monday that the agency will no longer allow “businesses to use your taxpayer dollars to fund solar projects on prime American farmland, and we will no longer allow solar panels manufactured by foreign adversaries to be used in our USDA-funded projects.”</p>



<p>The move is part of a broader effort by the administration to revoke or reduce Biden-era funding for the expansion of wind and solar through the Inflation Reduction Act, much of which benefited farmers and agricultural areas.</p>



<p>In July, President Donald Trump signed into law the One Big Beautiful Bill Act, slashing incentives for wind and solar while boosting support for biofuels, which consume the majority of the country’s cropland. The bill also restricts the use of Chinese-made solar components, a directive echoed in Rollins’ comments this week.</p>







<p>The USDA formally announced the wind and solar funding cuts on Tuesday. It did not respond to specific questions from Inside Climate News.</p>



<p>The agency and lawmakers supporting the move said their primary concern was safeguarding the country’s farmland and food.&nbsp;</p>



<p>“Secretary Rollins understands that food security is national security, and preserving prime farmland for agricultural production is a key component of protecting our food supply,” said Glenn “GT” Thompson (R-Penn.), chairman of the House Committee on Agriculture, in a statement.&nbsp;</p>



<p>More than half the country’s cropland—178 million out of <a href="https://www.ers.usda.gov/data-products/charts-of-note/chart-detail?chartId=111436#:~:text=For%202024%2C%20total%20U.S.%20cropland,acreage%20of%20334%20million%20acres.">328 million crop acres</a>—is used to grow corn and soybeans, much of it&nbsp; for biofuels, not food. About one-third of the acres planted in corn are used for corn-based ethanol, which <a href="https://www.wri.org/insights/increased-biofuel-production-impacts-climate-change-farmers">amounts to about 4 percent</a> of the country’s fuel mix. More than 40 percent of the soybean supply is used for biofuels, despite biodiesel amounting to less than <a href="https://www.wri.org/insights/increased-biofuel-production-impacts-climate-change-farmers">1 percent of the fuel mix</a>.&nbsp;&nbsp;</p>



<p>Most of the remaining corn and soy is fed to confined livestock, which are a major source of greenhouse gas emissions.</p>



<p>Only about <a href="https://www.wri.org/insights/crop-expansion-food-security-trends">2 percent</a> of the country’s corn is used for direct human consumption.&nbsp;</p>



<p>“Tennessee farmland should be used to grow the crops that feed our state and country, not to house solar panels made by foreign countries like Communist China,” said Sen. Marsha Blackburn (R-Tenn.). “Secretary Rollins and President Trump are right to put an end to these Green New Deal subsidies that waste taxpayer dollars while threatening America’s food security. I applaud this administration for investing in rural communities across Tennessee and empowering them to prosper for years to come.”</p>



<p>The biggest <a href="https://www.nass.usda.gov/Quick_Stats/Ag_Overview/stateOverview.php?state=TENNESSEE">agricultural land users</a> in Tennessee are corn and soybeans, which are grown on about 2.5 million acres.&nbsp;</p>



<p>A state commission found in 2024 that <a href="https://www.tn.gov/content/dam/tn/tacir/2023publications/2023_Solar.pdf">solar development did not threaten</a> the state’s farmland.&nbsp;</p>



<p>The USDA said it would immediately disqualify wind and solar projects from its Rural Development Business and Industry Guaranteed Loan Program and disqualify any wind or solar systems that are not “right-sized for their facilities” from a loan program under the Rural Energy for America Program (REAP).</p>



<p>Earlier this year, the agency stopped distributing already promised REAP grants, prompting <a href="https://insideclimatenews.org/news/13032025/farmers-community-groups-sue-trump-usda-allocated-funds/">farmers to sue</a> the administration.&nbsp;</p>



<p>The administration’s latest move could complicate the economic landscape for farmers, who have increasingly relied on the income from wind and solar installations on their land as commodity prices have fallen and climate-driven weather extremes threaten production.</p>



<p>In Iowa—the nation’s biggest corn-producing state—wind provides <a href="https://www.eia.gov/state/analysis.php?sid=IA">about 60 percent</a> of electricity.</p>



<p>“This is such a popular program—it saves them money and gives them a potential financial source,” said Richa Patel, a policy specialist at the National Sustainable Agriculture Coalition.&nbsp; “It’s a step backwards for farmers and small businesses that are trying to make decisions that are good for the business and the environment.”</p>



<p>Patel said she and her colleagues were still digging into the specifics about what the new limitations might mean and what type of solar facilities they will apply to.&nbsp;&nbsp;</p>



<p>In its statement Tuesday, the USDA said the number of solar panels on farmland has increased by nearly 50 percent since 2021. “That is why the Department is taking action,” the statement said.&nbsp;</p>



<p>A <a href="https://www.ers.usda.gov/amber-waves/2024/september/agricultural-land-near-solar-and-wind-projects-usually-remained-in-agriculture-after-development">2024 analysis by the USDA</a> found that about 424,000 acres were used for wind and solar, about 0.05 percent of the 897 million total pasture, rangeland and crop acres in the country.&nbsp;&nbsp;</p>



<p>The agency also found that agricultural land usually maintained similar characteristics and could still be used as farmland “even after the addition of solar or wind development.”</p>

						
<div>

		<h2>About This Story</h2>

		<p>Perhaps you noticed: This story, like all the news we publish, is free to read. That’s because Inside Climate News is a 501c3 nonprofit organization. We do not charge a subscription fee, lock our news behind a paywall, or clutter our website with ads. We make our news on climate and the environment freely available to you and anyone who wants it.</p>
		<p>That’s not all. We also share our news for free with scores of other media organizations around the country. Many of them can’t afford to do environmental journalism of their own. We’ve built bureaus from coast to coast to report local stories, collaborate with local newsrooms and co-publish articles so that this vital work is shared as widely as possible.</p>
		<p>Two of us launched ICN in 2007. Six years later we earned a Pulitzer Prize for National Reporting, and now we run the oldest and largest dedicated climate newsroom in the nation. We tell the story in all its complexity. We hold polluters accountable. We expose environmental injustice. We debunk misinformation. We scrutinize solutions and inspire action.</p>
		<p>Donations from readers like you fund every aspect of what we do. If you don’t already, will you support our ongoing work, our reporting on the biggest crisis facing our planet, and help us reach even more readers in more places? </p>
		<p>Please take a moment to make a tax-deductible donation. Every one of them makes a difference.</p>
		<p>Thank you,</p>

		 <!-- /.footer -->

	</div> <!-- /.icn-epic -->

						
 <!-- /.icn-share-holder -->

						
		<div>

			
				<p><img width="300" height="300" src="https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-300x300.jpg" alt="" decoding="async" srcset="https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-300x300.jpg 300w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-150x150.jpg 150w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-64x64.jpg 64w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-600x600.jpg 600w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-lazy-srcset="https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-300x300.jpg 300w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-150x150.jpg 150w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-64x64.jpg 64w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-600x600.jpg 600w" data-lazy-src="https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-300x300.jpg">
				</p> <!-- /.image-holder -->

			
			<div>

				

				
				
					<h4>Reporter, Washington, D.C.</h4>

				
				
				
					<div dir="auto" data-qa="message_content">
<p>Georgina Gustin covers agriculture for Inside Climate News, and has reported on the intersections of farming, food systems and the environment for much of her journalism career. Her work has won numerous awards, including the John B. Oakes Award for Distinguished Environmental Journalism, and she was twice named the Glenn Cunningham Agricultural Journalist of the Year, once with ICN colleagues. She has worked as a reporter for The Day in New London, Conn., the St. Louis Post-Dispatch and CQ Roll Call, and her stories have appeared in The New York Times, Washington Post and National Geographic’s The Plate, among others. She is a graduate of the Columbia University Graduate School of Journalism and the University of Colorado at Boulder.</p>
</div>


				
				
	

	
			</div> <!-- /.bio -->

		</div> <!-- /.post-author-bio -->

		
					</div><!-- .entry-content -->

				</article><!-- #post-## -->

				 <!-- /.post-footer -->

			
		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sprinkling self-doubt on ChatGPT (139 pts)]]></title>
            <link>https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/</link>
            <guid>44987422</guid>
            <pubDate>Fri, 22 Aug 2025 17:45:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/">https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/</a>, See on <a href="https://news.ycombinator.com/item?id=44987422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
  <article data-pagefind-body="">
    <time pubdate="" datetime="2025-08-22" data-pagefind-meta="date:2025-08-22" data-pagefind-sort="date:2025-08-22">
      Friday, Aug 22, 2025
    </time>
    <a href="https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/">
      
    </a>

    <p>I replaced my ChatGPT personalization settings with this prompt a few weeks ago and promptly forgot about it:</p>
<blockquote>
<ul>
<li>Be extraordinarily skeptical of your own correctness or stated assumptions. You aren't a cynic, you are a highly critical thinker and this is tempered by your self-doubt: you absolutely hate being wrong but you live in constant fear of it</li>
<li>When appropriate, broaden the scope of inquiry beyond the stated assumptions to think through unconvenitional opportunities, risks, and pattern-matching to widen the aperture of solutions</li>
<li>Before calling anything "done" or "working", take a second look at it ("red team" it) to critically analyze that you really are done or it really is working</li>
</ul></blockquote>
<p>I noticed a difference in results right away (even though I kept forgetting the change was due to my instructions and not the separately <a href="https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming">tumultuous rollout of GPT-5</a>).</p>
<p>Namely, pretty much every initial response now starts with:</p>
<ul>
<li>An expression of caution, self-doubt, and desire to get things right</li>
<li>Hilariously long "thinking" times (I asked it to estimate the macronutrients in lettuce yesterday and it spent 3 minutes and 59 seconds reasoning)</li>
<li>A post-hoc adversarial "<a href="https://en.wikipedia.org/wiki/Red_team">red team</a>" analysis of whatever it just vomited up as an answer</li>
</ul>
<p>I'm delighted to report that ChatGPT's output has been more useful since this change. Still not altogether <em>great</em>, but better at the margins. In particular, the "red team" analysis at the end of many requests frequently spots an error and causes it to arrive at the <em>actually-correct</em> answer, which—if nothing else—saves me the step of expressing skepticism. And even when ChatGPT is nevertheless wrong, its penchant for extremely-long thinking times means I'm getting my money's worth in GPU time.</p>
  </article>
  <div>
  <hr>
  <h2>Got a taste for hot, fresh takes?</h2>
  <p>
    Then you're in luck, because you'll pay $0 for my 2¢ when you <a href="https://justin.searls.co/subscribe/">subscribe to my work</a>, whether via <a href="https://justin.searls.co/rss/">RSS</a> or your favorite <a href="https://justin.searls.co/posse/">social network</a>.
  </p>
  <p>
    I also have a monthly <a href="https://justin.searls.co/newsletter">newsletter</a> where I write high-tempo, thought-provoking essays about life, in case that's more your speed:
  </p>
  
  <p>
    And if you'd rather give your eyes a rest and your ears a workout, might I suggest my long-form solo podcast, <a href="https://justin.searls.co/casts/breaking-change/">Breaking Change</a>? Odds are, you haven't heard anything quite like it.
  </p>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaving Gmail for Mailbox.org (278 pts)]]></title>
            <link>https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/</link>
            <guid>44987380</guid>
            <pubDate>Fri, 22 Aug 2025 17:41:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/">https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/</a>, See on <a href="https://news.ycombinator.com/item?id=44987380">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>This was a tough decision, having used Gmail since 2007/2008. However, I had to draw the line and stop giving Google my data for free.</p><p>The problem with email is that everything is transmitted in plain text. Technically, Google can store every message you receive and know everything, and U.S. agencies can request access to that data (this include also EU citizens under the <a href="https://policies.google.com/privacy/frameworks?hl=en-US">EU-U.S. and Swiss-U.S. Data Privacy Frameworks</a>).</p><p>For someone like me, who cares about privacy and runs as much as possible on my own home servers, that felt like way too much.</p><p>So I decided to switch to another provider, one that respects privacy a bit more. Of course, this meant no longer “paying” with my personal data, but instead paying the actual price of the email service.</p><h3 id="the-beginning">The beginning</h3><p>Let me start by saying: I use email in a very basic way. I send and receive a lot of messages (at least 50 a day), but they’re plain text/html emails with no attachments or fancy features. I couldn’t care less about the rest of the “suite", like notes, contacts, calendars and all that extra stuff.</p><p>So, after a bit of research, I narrowed it down to three different services:</p><ul><li>Mailbox.org</li><li>Proton Mail</li><li>Tutanota</li></ul><p>The last two providers offered true end-to-end encryption, at a cost of about €3/4 per month. Sounds good… but the catch is that to use their end-to-end encryption you’re forced to use their apps (or, on macOS, run a background “bridge”).</p><p>That’s a no go for me, because I love Apple’s Mail app on macOS and iOS, it just works perfectly for my needs, and I don’t want to give that up.</p><p>So, I went with mailbox.org that still offers integrated PGP encryption, and if you want, you can always use external PGP too (which I was already doing with Gmail).</p><p>Mailbox.org has a solid plan: 10GB of email storage plus 5GB of cloud storage starting at €2.50/month (paid annually). You can even expand the mail storage up to 100GB, at €0.20 per gigabyte.</p><p>I was using around 2.5GB on Gmail, so I had no issues with paying the equivalent of two coffees a month for a huge boost in privacy. And if I ever need more space, I can just add it on-demand for €0.20/GB.</p><p>There’s also a free one-month trial, but it’s pretty limited since you can’t send emails outside of mailbox.org domains.</p><p>So win the end, I registered my new address <code>giuliomagnifico@mailbox.org</code> and paid €3 for a month of testing. That means I’m covered for two months, and then I can just “top up” the account with €30 for a full year.</p><div id="callout"><p>⚠️</p><p>Mailbox.org doesn’t use auto-renewal, so you have to manually top up your account. Nice feature</p></div><h3 id="mailboxorg-online">Mailbox.org online</h3><p>The web interface is extremely simple but very effective. I actually find it better than Gmail, less bloated of useless stuff.</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/webui.jpeg" alt="webUI"></p><p>And on mobile it’s very usable too.</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/ios.jpeg" alt="ios"></p><p>One thing I prefer is using folders instead of Gmail’s “labels.” Mainly because this way I can put the folders directly under the account in Apple Mail (I think is the only email that can actually support this).</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/folders.jpeg" alt="folders"></p><p>Mailbox.org also has all the features I need,
and probably way more than I’ll ever use. It even includes storage, video chat, an XMPP chat, task lists, calendar, contacts, an Etherpad (basically shared notes, I think), and so on… none of which I really care about.</p><h2 id="migrating">Migrating</h2><p>I decided to move all my emails from Gmail to mailbox.org, so I could (in future) completely wipe my Gmail account.</p><p>To do this, I used the tool <a href="https://github.com/imapsync/imapsync">imapsync</a>, which I installed on my Archive server (see this post <a href="https://giuliomagnifico.blog/post/2025-08-01-home-setup-v6/#archive-minipc-for-paperless-immich-n8n-and-linkding">My home setup v6 | Archive server</a>)</p><p>After creating an “app password” on Gmail, I installed the Docker image and ran the tool with this script:</p><pre tabindex="0"><code>#!/bin/sh
set -eu

HOST1="imap.gmail.com"
USER1="giuliomagnifico@gmail.com"
PASS1="xxx"

HOST2="imap.mailbox.org"
USER2="giuliomagnifico@mailbox.org"
PASS2="xxx"

LOGDIR="/home/imapsync/logs"
mkdir -p "$LOGDIR"
LOGFILE="$LOGDIR/sync_$(date +%F_%H-%M-%S).log"

echo "Starting: $(date)"
docker compose run --rm imapsync imapsync \
  --host1 "$HOST1" --user1 "$USER1" --password1 "$PASS1" --ssl1 \
  --host2 "$HOST2" --user2 "$USER2" --password2 "$PASS2" --ssl2 \
  --automap --syncinternaldates --skipsize \
  --useuid --addheader --usecache --buffersize 4096 \
  --nofoldersizes --nofoldersizesatend \
  --exclude "\[Gmail\]/All Mail" \
  --regextrans2 "s/\[Imap\]\/Archive/Archive/" \
  --log &gt; "$LOGFILE" 2&gt;&amp;1

echo "Complete: $(date)"
echo "Log file: $LOGFILE"
</code></pre><p>The script excludes the All Mail folder" using: <code>--exclude "\[Gmail\]/All Mail" \</code></p><p>This to avoid duplicate emails already present in the folders, I also merged the <code>[Imap]/Archive</code> folder into the general Archive folder using: <code>--regextrans2 "s/\[Imap\]\/Archive/Archive/"</code></p><p>This because Apple’s Mail app creates the <code>[Imap]/Archive</code> folder/label on Gmail whenever you use the “Archive” function instead of “Trash.”</p><p>The whole process took a couple of hours (11201secs, ~3h to be precise) during which I was monitoring the logs using: <code>tail -f /home/imapsync/logs/sync_2025-08-19_15-02-48.log</code></p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/logs.jpeg" alt="logs"></p><p>And in the end, the end…</p><pre tabindex="0"><code>[cut]
msg [Gmail]/Trash/183393 {19549}      copied to Trash/13361      2.36 msgs/s  200.418 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183394 {92245}      copied to Trash/13362      2.36 msgs/s  200.420 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183395 {19675}      copied to Trash/13363      2.36 msgs/s  200.415 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183396 {5953}       copied to Trash/13364      2.36 msgs/s  200.410 KiB/s 2.140 GiB copied 
++++ End looping on each folder
++++ Statistics
Transfer started on                     : Tuesday 19 August 2025-08-19 03:02:49 +0000 UTC
Transfer ended on                       : Tuesday 19 August 2025-08-19 06:09:30 +0000 UTC
Transfer time                           : 11201.5 sec
Folders synced                          : 14/14 synced
Folders deleted on host2                : 0 
Messages transferred                    : 26407 
Messages skipped                        : 0
Messages found duplicate on host1       : 0
Messages found duplicate on host2       : 0
Messages found crossduplicate on host2  : 0
Messages void (noheader) on host1       : 0  
Messages void (noheader) on host2       : 0
Messages found in host1 not in host2    : 0 messages
Messages found in host2 not in host1    : 0 messages
Messages deleted on host1               : 0
Messages deleted on host2               : 0
Total bytes transferred                 : 2297647358 (2.140 GiB)
Total bytes skipped                     : 0 (0.000 KiB)
Message rate                            : 2.4 messages/s
Average bandwidth rate                  : 200.3 KiB/s
Reconnections to host1                  : 0
Reconnections to host2                  : 0
Memory consumption at the end           : 268.7 MiB (*time 836.2 MiB*h) (started with 161.5 MiB)
Load end is                             : 0.06 0.08 0.08 1/1135 on 16 cores
CPU time and %CPU                       : 446.72 sec 4.0 %CPU 0.2 %allcpus
Biggest message transferred             : 30413995 bytes (29.005 MiB)
Memory/biggest message ratio            : 9.3
Detected 0 errors
This imapsync is up to date. ( local 2.306 &gt;= official 2.290 )( Use --noreleasecheck to avoid this release check. )
Homepage: https://imapsync.lamiral.info/
Exiting with return value 0 (EX_OK: successful termination) 0/50 nb_errors/max_errors PID 1
Removing pidfile /var/tmp//tmp/imapsync.pid
Log file is LOG_imapsync/2025_08_19_03_02_49_171_giuliomagnifico_gmail_com_giuliomagnifico_mailbox_org.txt ( to change it, use --logfile filepath ; or use --nolog to turn off logging )
</code></pre><h3 id="simplify-the-transition">Simplify the transition</h3><p>Of course, the full switch will be a gradual process, even though I’ve already updated almost all my main services with the new address.</p><p>To make things easier, on my old Gmail account (which I removed from Apple Mail on all devices) I set up a forward to my new mailbox.org address.</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/forward.jpeg" alt="forward">
On the new mailbox.org account, I also set up a filter to flag any emails that get forwarded from Gmail.</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/gmailrule.jpeg" alt="gmailrule"></p><p>That way, I immediately notice them and I can update the address from Gmail to Mailbox.org whenever they show up. (The <code>\flagged</code> tag is perfect for this, since it add a “real red flag” in Apple Mail on iOS, iPadOS and macOS)</p><h2 id="cryptography">Cryptography</h2><p>Mailbox.org allows you to easily import your keys for PGP cryptography directly from the web. This is convenient as it lets you read and send PGP encrypted emails right from the browser on iOS, where there aren’t any “decent” apps for encrypted mail.</p><p>The same goes for macOS, although there you can just use Thunderbird, which works really well.</p><p>Here’s how PGP emails look on iOS:</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/gpgios.jpeg" alt="PGPios"></p><p>To send encrypted emails, you just select “Use PGP encrypted” when composing a new message, after importing your private key, of course.</p><p>And from the web interface, there’s also a handy feature to quickly import the sender’s public keys:</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/gpgmac.jpeg" alt="gpgmac"></p><h3 id="conclusion">Conclusion</h3><p>I’m satisfied. Leaving Gmail completely was something I wanted to do for a long time, but I was always hesitant. Finally, I made the switch, and, as often happens with these transitions, I discovered many unexpected positive aspects.</p><p>Oh, and if you have something to tell me or just want to test Mailbox.org after your switch, feel free to send me an email. Here’s my public key:</p><pre tabindex="0"><code>-----BEGIN PGP PUBLIC KEY BLOCK-----
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=GpPW
-----END PGP PUBLIC KEY BLOCK-----
</code></pre></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XSLT removal will break multiple government and regulatory sites (154 pts)]]></title>
            <link>https://github.com/whatwg/html/issues/11582</link>
            <guid>44987346</guid>
            <pubDate>Fri, 22 Aug 2025 17:38:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/whatwg/html/issues/11582">https://github.com/whatwg/html/issues/11582</a>, See on <a href="https://news.ycombinator.com/item?id=44987346">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><h3 dir="auto">What is the issue with the HTML Standard?</h3>
<p dir="auto">One of the issues we've seen in <a data-error-text="Failed to load title" data-id="3285251497" data-permission-text="Title is private" data-url="https://github.com/whatwg/html/issues/11523" data-hovercard-type="issue" data-hovercard-url="/whatwg/html/issues/11523/hovercard" href="https://github.com/whatwg/html/issues/11523">#11523</a> and <a data-error-text="Failed to load title" data-id="3323021317" data-permission-text="Title is private" data-url="https://github.com/whatwg/html/issues/11563" data-hovercard-type="pull_request" data-hovercard-url="/whatwg/html/pull/11563/hovercard" href="https://github.com/whatwg/html/pull/11563">#11563</a> is that the proposal to remove XSLT from the spec doesn't acknowledge existing use cases beyond Chrome Status counter stats.</p>
<p dir="auto">According to Chrome's own <a href="https://docs.google.com/document/d/1RC-pBBvsazYfCNNUSkPqAVpSpNJ96U8trhNkfV0v9fk/edit?tab=t.0#heading=h.83o2xr8ayal6" rel="nofollow">Blink principles of web compatibility</a>:</p>
<blockquote>
<p dir="auto">The primary signal we use is the fraction of page views impacted in Chrome, usually computed via Blink’s UseCounter UMA metrics.  As a general rule of thumb, 0.1% of PageVisits (1 in 1000) is large, while 0.001% is considered small but non-trivial.  Anything below about 0.00001% (1 in 10 million) is generally considered trivial.  There are around 771 billion web pages viewed in Chrome every month (not counting other Chromium-based browsers).  So seriously breaking even 0.0001% still results in someone being frustrated every 3 seconds, and so not to be taken lightly!</p>
</blockquote>
<p dir="auto">To add to the use cases already provided in the discussions above, here's a small list:</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>United States Congress</td>
<td>Legislative texts are shown using XSLT</td>
<td><a href="https://www.congress.gov/117/bills/hr3617/BILLS-117hr3617ih.xml" rel="nofollow">https://www.congress.gov/117/bills/hr3617/BILLS-117hr3617ih.xml</a> and <a href="https://www.govinfo.gov/content/pkg/BILLS-119hr400ih/xml/BILLS-119hr400ih.xml" rel="nofollow">https://www.govinfo.gov/content/pkg/BILLS-119hr400ih/xml/BILLS-119hr400ih.xml</a></td>
</tr>
<tr>
<td>National Weather Service Current Observations</td>
<td>Uses client-side XSLT to transform and display current weather observation data from XML.</td>
<td><a href="https://www.weather.gov/xml/current_obs/KABE.xml" rel="nofollow">https://www.weather.gov/xml/current_obs/KABE.xml</a></td>
</tr>
<tr>
<td>European Parliament Political Parties</td>
<td>Uses client-side XSLT to transform and display information on European political parties from XML.</td>
<td><a href="https://www.europarl.europa.eu/politicalparties/index_en.xml" rel="nofollow">https://www.europarl.europa.eu/politicalparties/index_en.xml</a></td>
</tr>
<tr>
<td>Therapeutic Goods Administration Code Definitions</td>
<td>Uses client-side XSLT to transform and display regulatory code definitions from XML.</td>
<td><a href="https://apps.tga.gov.au/downloads/sequence-description.xml" rel="nofollow">https://apps.tga.gov.au/downloads/sequence-description.xml</a></td>
</tr>
<tr>
<td>Canadian Forest Service Weather Stations Metadata</td>
<td>Uses client-side XSLT to transform and display weather station metadata from XML.</td>
<td><a href="https://cwfis.cfs.nrcan.gc.ca/downloads/fwi_obs/WeatherStations_CWFIS_export.xml" rel="nofollow">https://cwfis.cfs.nrcan.gc.ca/downloads/fwi_obs/WeatherStations_CWFIS_export.xml</a></td>
</tr>
<tr>
<td>European Pollutant Release and Transfer Register Method Types</td>
<td>Uses client-side XSLT to transform and display method type codes from XML.</td>
<td><a href="https://converters.eionet.europa.eu/xmlfile/EPRTR_MethodTypeCode_1.xml" rel="nofollow">https://converters.eionet.europa.eu/xmlfile/EPRTR_MethodTypeCode_1.xml</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">If I, and others, could easily find such examples, I believe browser vendors could easily find these, too.</p>
<p dir="auto">I would like <a data-hovercard-type="user" data-hovercard-url="/users/mfreed7/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/mfreed7">@mfreed7</a> <a data-hovercard-type="user" data-hovercard-url="/users/domenic/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/domenic">@domenic</a> and other Googlers who initiated and move on with this proposal to acknowledge this, and provide more rationale for removing XSLT than just Chrome Status counter stats.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo granted permit to begin testing in New York City (556 pts)]]></title>
            <link>https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html</link>
            <guid>44986949</guid>
            <pubDate>Fri, 22 Aug 2025 17:02:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html">https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html</a>, See on <a href="https://news.ycombinator.com/item?id=44986949">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108110425" data-test="InlineImage"><p>Waymo self-driving cars with roof-mounted sensor arrays traveling near palm trees and modern buildings along the Embarcadero, San Francisco, California, February 21, 2025.&nbsp;</p><p>Smith Collection/gado | Archive Photos | Getty Images</p></div><div><p><a href="https://www.cnbc.com/2025/07/08/waymo-teen-accounts.html">Waymo</a> is getting one step closer to rides in <a href="https://www.cnbc.com/2025/06/18/waymo-cars-are-coming-to-new-york-with-a-driver-behind-the-wheel.html">New York City</a>.</p><p>The <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/GOOGL/">Alphabet</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> autonomous vehicle subsidiary received its first permit from the New York Department of Transportation on Friday to start testing in New York City, Mayor <a href="https://www.nyc.gov/mayors-office/news/2025/08/mayor-adams--dot-announce-approval-of-first-application-to-test-" target="_blank">Eric Adams</a> announced Friday. The rollout is the city's first <a href="https://www.cnbc.com/2025/07/02/autonomous-cars-are-having-their-chatgpt-moment-bank-of-america.html">autonomous vehicle</a> testing launch.</p><p>Waymo will start testing up to eight vehicles in <a href="https://www.cnbc.com/2025/04/02/manhattan-luxury-real-estate-market.html">Manhattan</a> and Downtown Brooklyn through late September with the potential to extend the program. New York state law requires the company to have a driver behind the wheel to operate.</p><p>"We're a tech-friendly administration and we're always looking for innovative ways to safely move our city forward," Adams said in a release. "New York City is proud to welcome Waymo to test this new technology in Manhattan and Brooklyn, as we know this testing is only the first step in moving our city further into the 21st century."</p></div><div id="RegularArticle-RelatedContent-1"><h2>Read more CNBC tech news</h2><div><ul><li><a href="https://www.cnbc.com/2025/08/20/google-pixel-10-gemini-ai.html">Google announces its AI-powered Pixel 10 smartphone series</a></li><li><a href="https://www.cnbc.com/2025/08/20/these-little-robots-are-changing-the-way-solar-farms-are-built.html">These little robots are changing the way solar farms are built, saving time and money</a></li><li><a href="https://www.cnbc.com/2025/08/20/intel-investors-equity-softbank-trump.html">Intel in talks with other large investors for equity boost at discount, sources say</a></li><li><a href="https://www.cnbc.com/2025/08/20/gates-nvidia-fieldai-robotics.html">Nvidia, Bill Gates-backed robotics startup Field AI hits $2 billion valuation after recent raise</a></li></ul></div></div><div><p>The news comes just two months after the company said it filed permits to test its cars in the city with a trained specialist behind the wheel.</p><p>Waymo has hit <a href="https://www.cnbc.com/2025/07/28/waymo-plans-to-bring-its-robotaxi-service-to-dallas-in-2026.html">expansion</a> mode on its services nationwide, launching in <a href="https://www.cnbc.com/2025/03/04/waymo-uber-begin-offering-robotaxi-rides-in-austin-ahead-of-sxsw.html">Austin</a> this year and expanding its <a href="https://www.cnbc.com/2025/03/11/waymo-expands-its-robotaxi-service-in-the-san-francisco-bay-area.html">San Francisco area</a> operations in March. Waymo also plans to bring autonomous vehicles to Atlanta, Miami and Washington, D.C. and recently said it will start operations in <a href="https://www.cnbc.com/2025/07/07/waymo-to-begin-testing-in-philadelphia-with-drivers-behind-the-wheel.html">Philadelphia</a> as it looks to break further into the Northeast market.</p><p>Waymo's CEO said the company surpassed 10 million <a href="https://www.cnbc.com/2025/05/20/waymo-ceo-tekedra-mawakana-10-million.html">robotaxi</a> trips in May.</p><p>For years, autonomous vehicle companies have sought to introduce their technology to The Big Apple, with Waymo previously taking a crack at it in 2021. At that time, the company rolled out some cars in certain areas of the city for<a href="https://waymo.com/blog/2021/11/introducing-waymo-driver-to-new-york" target="_blank"> manual driving and data collection</a>.</p><p>New York City has also expressed interest in bringing autonomous vehicles to the city. <a href="https://www.nyc.gov/mayors-office/news/2024/03/mayor-adams-releases-requirements-opens-permit-applications-responsible-autonomous-vehicle" target="_blank">Last year</a>, the Adams administration implemented a series of safety requirements for responsible testing in the city and opened a permit program.</p><p>As part of the permit, Waymo must regularly meet and report data to DOT and work closely with law enforcement and emergency services.</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/07/08/waymo-begins-testing-self-driving-cars-with-human-drivers-in-new-york-and-philadelphia.html">Waymo begins testing self-driving cars with human drivers in New York and Philadelphia</a></p></div><div id="Placeholder-ArticleBody-Video-108168745" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000381578" aria-labelledby="Placeholder-ArticleBody-Video-108168745"><p><img src="https://image.cnbcfm.com/api/v1/image/108168746-17519709091751970907-40621734100-1080pnbcnews.jpg?v=1751970908&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Waymo begins testing self-driving cars with human drivers in New York and Philadelphia"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg 8.0 (858 pts)]]></title>
            <link>https://ffmpeg.org/index.html#pr8.0</link>
            <guid>44985730</guid>
            <pubDate>Fri, 22 Aug 2025 15:22:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ffmpeg.org/index.html#pr8.0">https://ffmpeg.org/index.html#pr8.0</a>, See on <a href="https://news.ycombinator.com/item?id=44985730">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="index">

  <div>
    <p>
      <h2>
        A complete, cross-platform solution to record, convert and stream audio and video.
      </h2>
    </p> <!-- col -->
     <!-- col -->
  </div> <!-- row -->

  <div>
    <h3>Converting <strong>video</strong> and <strong>audio</strong> has never been so easy.
    </h3>
    <pre>$ ffmpeg -i input.mp4 output.avi</pre>
    
  </div> <!-- well -->

  <h2 id="news">
    <span>
      <a href="https://ffmpeg.org/main.rss"><strong></strong></a> &nbsp;
      <a href="https://twitter.com/FFmpeg"><strong><i></i></strong></a> &nbsp;
      <a href="https://www.facebook.com/ffmpeg"><strong><i></i></strong></a>
    </span>
    News
  </h2>

  <h3 id="pr8.0">August 23nd, 2025, FFmpeg 8.0 <span title="David A. Huffman">"Huffman"</span></h3>
  <p>
  A new major release, <a href="https://ffmpeg.org/download.html#release_8.0">FFmpeg 8.0 <span title="David A. Huffman">"Huffman"</span></a>,
  is now available for download.
  Thanks to several delays, and modernization of our entire infrastructure, this release ended up
  being one of our largest releases to date. In short, its new features are:
  </p><ul>
    <li>Native decoders: <span title="Advanced Professional Video">APV</span>, ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728</li>
    <li>VVC decoder improvements: <span title="Intra Block Copy">IBC</span>,
                                  <span title="Adaptive Color Transform">ACT</span>,
                                  Palette Mode</li>
    <li>Vulkan compute-based codecs: FFv1 (encode and decode), ProRes RAW (decode only)</li>
    <li>Hardware accelerated decoding: Vulkan VP9, VAAPI VVC, OpenHarmony H264/5</li>
    <li>Hardware accelerated encoding: Vulkan AV1, OpenHarmony H264/5</li>
    <li>Formats: MCC, G.728, Whip, APV</li>
    <li>Filters: colordetect, pad_cuda, scale_d3d11, Whisper, and others</li>
  </ul>
  

  <p>
  A new class of decoders and encoders based on pure Vulkan compute implementation have been added.
  Vulkan is a cross-platform, open standard set of APIs that allows programs to use GPU hardware in various ways,
  from drawing on screen, to doing calculations, to decoding video via custom hardware accelerators.
  Rather than using a custom hardware accelerator present, these codecs are based on compute shaders, and work
  on any implementation of Vulkan 1.3.<br>
  Decoders use the same hwaccel API and commands, so users do not need to do anything special to enable them,
  as enabling <a href="https://trac.ffmpeg.org/wiki/HWAccelIntro#Vulkan">Vulkan decoding</a> is sufficient to use them.<br>
  Encoders, like our hardware accelerated encoders, require specifying a new encoder (ffv1_vulkan).
  Currently, the only codecs supported are: FFv1 (encoding and decoding) and ProRes RAW (decode only).
  ProRes (encode+decode) and VC-2 (encode+decode) implementations are complete and currently in review,
  to be merged soon and available with the next minor release.<br>
  Only codecs specifically designed for parallelized decoding can be implemented in such a way, with
  more mainstream codecs not being planned for support.<br>
  Depending on the hardware, these new codecs can provide very significant speedups, and open up
  possibilities to work with them for situations like non-linear video editors and
  lossless screen recording/streaming, so we are excited to learn what our downstream users can make with them.
  </p>

  <p>
  The project has recently started to modernize its infrastructure. Our mailing list servers have been
  fully upgraded, and we have recently started to accept contributions via a new forge, available on
  <a href="https://code.ffmpeg.org/">code.ffmpeg.org</a>, running a Forgejo instance.
  </p>

  <p>
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  </p>

  <h3 id="pr7.1">September 30th, 2024, FFmpeg 7.1 <span title="Rózsa Péter">"Péter"</span></h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_7.1">FFmpeg 7.1 "Péter"</a>, a new
    major release, is now available! A full list of changes can be found in the release
    <a href="https://git.ffmpeg.org/gitweb/ffmpeg.git/blob/refs/heads/release/7.1:/Changelog">changelog</a>.
  </p>
  <p>
    The more important highlights of the release are that the VVC decoder, merged as experimental in version 7.0,
    has had enough time to mature and be optimized enough to be declared as stable. The codec is starting to gain
    traction with broadcast standardization bodies.<br>
    Support has been added for a native AAC USAC (part of the xHE-AAC coding system) decoder, with the format starting
    to be adopted by streaming websites, due to its extensive volume normalization metadata.<br>
    MV-HEVC decoding is now supported. This is a stereoscopic coding tool that begun to be shipped and generated
    by recent phones and VR headsets.<br>
    LC-EVC decoding, an enhancement metadata layer to attempt to improve the quality of codecs, is now supported via an
    external library.<br>
  </p>
  <p>
    Support for Vulkan encoding, with H264 and HEVC was merged. This finally allows fully Vulkan-based decode-filter-encode
    pipelines, by having a sink for Vulkan frames, other than downloading or displaying them. The encoders have feature-parity
    with their VAAPI implementation counterparts. Khronos has announced that support for AV1 encoding is also coming soon to Vulkan,
    and FFmpeg is aiming to have day-one support.
  </p>
  <p>
    In addition to the above, this release has had a lot of important internal work done. By far, the standout internally
    are the improvements made for full-range images. Previously, color range data had two paths, no negotiation,
    and was unreliably forwarded to filters, encoders, muxers. Work on cleaning the system up started more than 10
    years ago, however this stalled due to how fragile the system was, and that breaking behaviour would be unacceptable.
    The new system fixes this, so now color range is forwarded correctly and consistently everywhere needed, and also
    laid the path for more advanced forms of negotiation.<br>
    Cropping metadata is now supported with Matroska and MP4 formats. This metadata is important not only for archival,
    but also with AV1, as hardware encoders require its signalling due to the codec not natively supporting one.
  </p>
  <p>
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  </p>

  <h3 id="coverity">September 11th, 2024, Coverity</h3>
  <p>
  The number of issues FFmpeg has in <a href="https://scan.coverity.com/projects/ffmpeg">Coverity (a static analyzer)</a> is now lower than it has been since 2016.
  Our defect density is less than one 30th of the average in OSS with over a million code
  lines. All this was possible thanks to a grant from the <a href="https://www.sovereigntechfund.de/">Sovereign Tech Fund</a>.
  </p>
  <p><img src="https://ffmpeg.org/img/coverity-lifetime-2024-08.PNG" alt="Coverity Lifetime Graph till 2024-08"></p><h3 id="xheaac">June 2nd, 2024, native xHE-AAC decoder</h3>
  <p>
  FFmpeg now implements a native xHE-AAC decoder. Currently, streams without (e)SBR, USAC or MPEG-H Surround
  are supported, which means the majority of xHE-AAC streams in use should work. Support for USAC and (e)SBR is
  coming soon. Work is also ongoing to improve its stability and compatibility.
  During the process we found several specification issues, which were then submitted back to the authors
  for discussion and potential inclusion in a future errata.
  </p>

  <h3 id="stf24">May 13th, 2024, Sovereign Tech Fund</h3>
  <p>
  The FFmpeg community is excited to announce that Germany's
  <a href="https://www.sovereigntechfund.de/tech/ffmpeg">Sovereign Tech Fund</a>
  has become its first governmental sponsor. Their support will help
  sustain the maintainance of the FFmpeg project, a critical open-source
  software multimedia component essential to bringing audio and video to
  billions around the world everyday.
  </p>

  <h3 id="pr7.0">April 5th, 2024, FFmpeg 7.0 "Dijkstra"</h3>
  <p>
  A new major release, <a href="https://ffmpeg.org/download.html#release_7.0">FFmpeg 7.0 "Dijkstra"</a>,
  is now available for download. The most noteworthy changes for most users are
  a <a href="#vvcdec">native VVC decoder</a> (currently experimental, until more
  fuzzing is done), <a href="#iamf">IAMF support</a>, or a
  <a href="#cli_threading">multi-threaded <code>ffmpeg</code> CLI tool</a>.
  </p>

  <p>
  This release is <em>not</em> backwards compatible, removing APIs deprecated before 6.0.
  The biggest change for most library callers will be the removal of the old bitmask-based
  channel layout API, replaced by the <code>AVChannelLayout</code> API allowing such
  features as custom channel ordering, or Ambisonics. Certain deprecated <code>ffmpeg</code>
  CLI options were also removed, and a C11-compliant compiler is now required to build
  the code.
  </p>

  <p>
  As usual, there is also a number of new supported formats and codecs, new filters, APIs,
  and countless smaller features and bugfixes. Compared to 6.1, the <code>git</code> repository
  contains almost ∼2000 new commits by ∼100 authors, touching &gt;100000 lines in
  ∼2000 files — thanks to everyone who contributed. See the
  <a href="https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=Changelog;hb=n7.0">Changelog</a>,
  <a href="https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=doc/APIchanges;hb=n7.0">APIchanges</a>,
  and the git log for more comprehensive lists of changes.
  </p>

  <h3 id="vvcdec">January 3rd, 2024, native VVC decoder</h3>
  <p>
  The <code>libavcodec</code> library now contains a native VVC (Versatile Video Coding)
  decoder, supporting a large subset of the codec's features. Further optimizations and
  support for more features are coming soon. The code was written by Nuo Mi, Xu Mu,
  Frank Plowman, Shaun Loo, and Wu Jianhua.
  </p>

  <h3 id="iamf">December 18th, 2023, IAMF support</h3>
  <p>
  The <code>libavformat</code> library can now read and write <a href="https://aomediacodec.github.io/iamf/">IAMF</a>
  (Immersive Audio) files. The <code>ffmpeg</code> CLI tool can configure IAMF structure with the new
  <code>-stream_group</code> option. IAMF support was written by James Almer.
  </p>

  <h3 id="cli_threading">December 12th, 2023, multi-threaded <code>ffmpeg</code> CLI tool</h3>
  <p>
  Thanks to a major refactoring of the <code>ffmpeg</code> command-line tool, all the major
  components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now
  run in parallel. This should improve throughput and CPU utilization, decrease latency,
  and open the way to other exciting new features.
  </p>

  <p>
  Note that you should <em>not</em> expect significant performance improvements in cases
  where almost all computational time is spent in a single component (typically video
  encoding).
  </p>

  <h3 id="pr6.1">November 10th, 2023, FFmpeg 6.1 "Heaviside"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_6.1">FFmpeg 6.1 "Heaviside"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>libaribcaption decoder</li>
    <li>Playdate video decoder and demuxer</li>
    <li>Extend VAAPI support for libva-win32 on Windows</li>
    <li>afireqsrc audio source filter</li>
    <li>arls filter</li>
    <li>ffmpeg CLI new option: -readrate_initial_burst</li>
    <li>zoneplate video source filter</li>
    <li>command support in the setpts and asetpts filters</li>
    <li>Vulkan decode hwaccel, supporting H264, HEVC and AV1</li>
    <li>color_vulkan filter</li>
    <li>bwdif_vulkan filter</li>
    <li>nlmeans_vulkan filter</li>
    <li>RivaTuner video decoder</li>
    <li>xfade_vulkan filter</li>
    <li>vMix video decoder</li>
    <li>Essential Video Coding parser, muxer and demuxer</li>
    <li>Essential Video Coding frame merge bsf</li>
    <li>bwdif_cuda filter</li>
    <li>Microsoft RLE video encoder</li>
    <li>Raw AC-4 muxer and demuxer</li>
    <li>Raw VVC bitstream parser, muxer and demuxer</li>
    <li>Bitstream filter for editing metadata in VVC streams</li>
    <li>Bitstream filter for converting VVC from MP4 to Annex B</li>
    <li>scale_vt filter for videotoolbox</li>
    <li>transpose_vt filter for videotoolbox</li>
    <li>support for the P_SKIP hinting to speed up libx264 encoding</li>
    <li>Support HEVC,VP9,AV1 codec in enhanced flv format</li>
    <li>apsnr and asisdr audio filters</li>
    <li>OSQ demuxer and decoder</li>
    <li>Support HEVC,VP9,AV1 codec fourcclist in enhanced rtmp protocol</li>
    <li>CRI USM demuxer</li>
    <li>ffmpeg CLI '-top' option deprecated in favor of the setfield filter</li>
    <li>VAAPI AV1 encoder</li>
    <li>ffprobe XML output schema changed to account for multiple variable-fields elements within the same parent element</li>
    <li>ffprobe -output_format option added as an alias of -of</li>
  </ul>
  <p>
    This release had been overdue for at least half a year, but due to constant activity in the repository,
    had to be delayed, and we were finally able to branch off the release recently, before some of the large
    changes scheduled for 7.0 were merged.
  </p>
  <p>
    Internally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs
    and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon).<br>
    This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds.<br>
    There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders,
    reducing overhead.<br>
    RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left.<br>
    There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the
    accurracy of variable frame rate video.
  </p>
  <p>
    Next major release will be version 7.0, scheduled to be released in February. We will attempt to better stick
    to the new release schedule we announced at the start of this year.
  </p>
  <p>
    We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.
  </p>

  <h3 id="vk2023">May 31st, 2023, Vulkan decoding</h3>
  <p>
    A few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase.
    This is the first vendor-generic and platform-generic decode acceleration API, enabling the
    same code to be used on multiple platforms, with very minimal overhead.
    This is also the first multi-threaded hardware decoding API, and our code makes full use of this,
    saturating all available decode engines the hardware exposes.
  </p>
  <p>
    Those wishing to test the code can read our
    <a href="https://trac.ffmpeg.org/wiki/HWAccelIntro#Vulkan">documentation page</a>.
    For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive
    a VkImage to present or manipulate, documentation and examples are available in our source tree.
    Currently, using the latest available git checkout of our
    <a href="https://git.videolan.org/?p=ffmpeg.git;a=summary">repository</a> is required.
    The functionality will be included in stable branches with the release of version 6.1, due
    to be released soon.
  </p>
  <p>
    As this is also the first practical implementation of the specifications, bugs may be present,
    particularly in drivers, and, although passing verification, the implementation itself.
    New codecs, and encoding support are also being worked on, by both the Khronos organization
    for standardizing, and us as implementing it, and giving feedback on improving.
  </p>

  <h3 id="pr6.0">February 28th, 2023, FFmpeg 6.0 "Von Neumann"</h3>
  <p>
    A new major release, <a href="https://ffmpeg.org/download.html#release_6.0">FFmpeg 6.0 "Von Neumann"</a>,
    is now available for download. This release has many new encoders and decoders, filters,
    ffmpeg CLI tool improvements, and also, changes the way releases are done. All major
    releases will now bump the version of the ABI. We plan to have a new major release each
    year. Another release-specific change is that deprecated APIs will be removed after 3
    releases, upon the next major bump.
    This means that releases will be done more often and will be more organized.
  </p>
  <p>
    New decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats.
    QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c
    to avoid confusion) has speed-up improvements due to threading, as well as statistics options,
    and the ability to pass option values for filters from a file. There are quite a few new audio
    and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too.
    Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT
    implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better
    ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V
    vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed
    in the doc/APIchanges file in our tree.
    A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the
    next minor release, 6.1, which we plan to release soon, in line with our new release schedule.
    Some highlights are:
  </p>
  <ul>
    <li>Radiance HDR image support</li>
    <li>ddagrab (Desktop Duplication) video capture filter</li>
    <li>ffmpeg -shortest_buf_duration option</li>
    <li>ffmpeg now requires threading to be built</li>
    <li>ffmpeg now runs every muxer in a separate thread</li>
    <li>Add new mode to cropdetect filter to detect crop-area based on motion vectors and edges</li>
    <li>VAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9</li>
    <li>WBMP (Wireless Application Protocol Bitmap) image format</li>
    <li>a3dscope filter</li>
    <li>bonk decoder and demuxer</li>
    <li>Micronas SC-4 audio decoder</li>
    <li>LAF demuxer</li>
    <li>APAC decoder and demuxer</li>
    <li>Media 100i decoders</li>
    <li>DTS to PTS reorder bsf</li>
    <li>ViewQuest VQC decoder</li>
    <li>backgroundkey filter</li>
    <li>nvenc AV1 encoding support</li>
    <li>MediaCodec decoder via NDKMediaCodec</li>
    <li>MediaCodec encoder</li>
    <li>oneVPL support for QSV</li>
    <li>QSV AV1 encoder</li>
    <li>QSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9</li>
    <li>showcwt multimedia filter</li>
    <li>corr video filter</li>
    <li>adrc audio filter</li>
    <li>afdelaysrc audio filter</li>
    <li>WADY DPCM decoder and demuxer</li>
    <li>CBD2 DPCM decoder</li>
    <li>ssim360 video filter</li>
    <li>ffmpeg CLI new options: -stats_enc_pre[_fmt], -stats_enc_post[_fmt], -stats_mux_pre[_fmt]</li>
    <li>hstack_vaapi, vstack_vaapi and xstack_vaapi filters</li>
    <li>XMD ADPCM decoder and demuxer</li>
    <li>media100 to mjpegb bsf</li>
    <li>ffmpeg CLI new option: -fix_sub_duration_heartbeat</li>
    <li>WavArc decoder and demuxer</li>
    <li>CrystalHD decoders deprecated</li>
    <li>SDNS demuxer</li>
    <li>RKA decoder and demuxer</li>
    <li>filtergraph syntax in ffmpeg CLI now supports passing file contents as option values</li>
    <li>hstack_qsv, vstack_qsv and xstack_qsv filters</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr5.1">July 22nd, 2022, FFmpeg 5.1 "Riemann"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_5.1">FFmpeg 5.1 "Riemann"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>add ipfs/ipns protocol support</li>
    <li>dialogue enhance audio filter</li>
    <li>dropped obsolete XvMC hwaccel</li>
    <li>pcm-bluray encoder</li>
    <li>DFPWM audio encoder/decoder and raw muxer/demuxer</li>
    <li>SITI filter</li>
    <li>Vizrt Binary Image encoder/decoder</li>
    <li>avsynctest source filter</li>
    <li>feedback video filter</li>
    <li>pixelize video filter</li>
    <li>colormap video filter</li>
    <li>colorchart video source filter</li>
    <li>multiply video filter</li>
    <li>PGS subtitle frame merge bitstream filter</li>
    <li>blurdetect filter</li>
    <li>tiltshelf audio filter</li>
    <li>QOI image format support</li>
    <li>ffprobe -o option</li>
    <li>virtualbass audio filter</li>
    <li>VDPAU AV1 hwaccel</li>
    <li>PHM image format support</li>
    <li>remap_opencl filter</li>
    <li>added chromakey_cuda filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr5.0">January 17th, 2022, FFmpeg 5.0 "Lorentz"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_5.0">FFmpeg 5.0 "Lorentz"</a>, a new
    major release, is now available! For this long-overdue release, a major effort
    underwent to remove the old encode/decode APIs and replace them with an
    N:M-based API, the entire libavresample library was removed, libswscale
    has a new, easier to use AVframe-based API, the Vulkan code was much improved,
    many new filters were added, including libplacebo integration, and finally,
    DoVi support was added, including tonemapping and remuxing. The default
    AAC encoder settings were also changed to improve quality.
    Some of the changelog highlights:
  </p>
  <ul>
    <li>ADPCM IMA Westwood encoder</li>
    <li>Westwood AUD muxer</li>
    <li>ADPCM IMA Acorn Replay decoder</li>
    <li>Argonaut Games CVG demuxer</li>
    <li>Argonaut Games CVG muxer</li>
    <li>Concatf protocol</li>
    <li>afwtdn audio filter</li>
    <li>audio and video segment filters</li>
    <li>Apple Graphics (SMC) encoder</li>
    <li>hsvkey and hsvhold video filters</li>
    <li>adecorrelate audio filter</li>
    <li>atilt audio filter</li>
    <li>grayworld video filter</li>
    <li>AV1 Low overhead bitstream format muxer</li>
    <li>swscale slice threading</li>
    <li>MSN Siren decoder</li>
    <li>scharr video filter</li>
    <li>apsyclip audio filter</li>
    <li>morpho video filter</li>
    <li>amr parser</li>
    <li>(a)latency filters</li>
    <li>GEM Raster image decoder</li>
    <li>asdr audio filter</li>
    <li>speex decoder</li>
    <li>limitdiff video filter</li>
    <li>xcorrelate video filter</li>
    <li>varblur video filter</li>
    <li>huesaturation video filter</li>
    <li>colorspectrum source video filter</li>
    <li>RTP packetizer for uncompressed video (RFC 4175)</li>
    <li>bitpacked encoder</li>
    <li>VideoToolbox VP9 hwaccel</li>
    <li>VideoToolbox ProRes hwaccel</li>
    <li>support loongarch.</li>
    <li>aspectralstats audio filter</li>
    <li>adynamicsmooth audio filter</li>
    <li>libplacebo filter</li>
    <li>vflip_vulkan, hflip_vulkan and flip_vulkan filters</li>
    <li>adynamicequalizer audio filter</li>
    <li>yadif_videotoolbox filter</li>
    <li>VideoToolbox ProRes encoder</li>
    <li>anlmf audio filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="irc2021">June 19th, 2021, IRC</h3>
  <p>
    We have a new IRC home at Libera Chat
    now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at <a href="https://ffmpeg.org/contact.html#IRCChannels">contact#IRCChannels</a>
  </p>

  <h3 id="pr4.4">April 8th, 2021, FFmpeg 4.4 "Rao"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.4">FFmpeg 4.4 "Rao"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>AudioToolbox output device</li>
    <li>MacCaption demuxer</li>
    <li>PGX decoder</li>
    <li>chromanr video filter</li>
    <li>VDPAU accelerated HEVC 10/12bit decoding</li>
    <li>ADPCM IMA Ubisoft APM encoder</li>
    <li>Rayman 2 APM muxer</li>
    <li>AV1 encoding support SVT-AV1</li>
    <li>Cineform HD encoder</li>
    <li>ADPCM Argonaut Games encoder</li>
    <li>Argonaut Games ASF muxer</li>
    <li>AV1 Low overhead bitstream format demuxer</li>
    <li>RPZA video encoder</li>
    <li>ADPCM IMA MOFLEX decoder</li>
    <li>MobiClip FastAudio decoder</li>
    <li>MobiClip video decoder</li>
    <li>MOFLEX demuxer</li>
    <li>MODS demuxer</li>
    <li>PhotoCD decoder</li>
    <li>MCA demuxer</li>
    <li>AV1 decoder (Hardware acceleration used only)</li>
    <li>SVS demuxer</li>
    <li>Argonaut Games BRP demuxer</li>
    <li>DAT demuxer</li>
    <li>aax demuxer</li>
    <li>IPU decoder, parser and demuxer</li>
    <li>Intel QSV-accelerated AV1 decoding</li>
    <li>Argonaut Games Video decoder</li>
    <li>libwavpack encoder removed</li>
    <li>ACE demuxer</li>
    <li>AVS3 demuxer</li>
    <li>AVS3 video decoder via libuavs3d</li>
    <li>Cintel RAW decoder</li>
    <li>VDPAU accelerated VP9 10/12bit decoding</li>
    <li>afreqshift and aphaseshift filters</li>
    <li>High Voltage Software ADPCM encoder</li>
    <li>LEGO Racers ALP (.tun &amp; .pcm) muxer</li>
    <li>AV1 VAAPI decoder</li>
    <li>adenorm filter</li>
    <li>ADPCM IMA AMV encoder</li>
    <li>AMV muxer</li>
    <li>NVDEC AV1 hwaccel</li>
    <li>DXVA2/D3D11VA hardware accelerated AV1 decoding</li>
    <li>speechnorm filter</li>
    <li>SpeedHQ encoder</li>
    <li>asupercut filter</li>
    <li>asubcut filter</li>
    <li>Microsoft Paint (MSP) version 2 decoder</li>
    <li>Microsoft Paint (MSP) demuxer</li>
    <li>AV1 monochrome encoding support via libaom &gt;= 2.0.1</li>
    <li>asuperpass and asuperstop filter</li>
    <li>shufflepixels filter</li>
    <li>tmidequalizer filter</li>
    <li>estdif filter</li>
    <li>epx filter</li>
    <li>Dolby E parser</li>
    <li>shear filter</li>
    <li>kirsch filter</li>
    <li>colortemperature filter</li>
    <li>colorcontrast filter</li>
    <li>PFM encoder</li>
    <li>colorcorrect filter</li>
    <li>binka demuxer</li>
    <li>XBM parser</li>
    <li>xbm_pipe demuxer</li>
    <li>colorize filter</li>
    <li>CRI parser</li>
    <li>aexciter audio filter</li>
    <li>exposure video filter</li>
    <li>monochrome video filter</li>
    <li>setts bitstream filter</li>
    <li>vif video filter</li>
    <li>OpenEXR image encoder</li>
    <li>Simbiosis IMX decoder</li>
    <li>Simbiosis IMX demuxer</li>
    <li>Digital Pictures SGA demuxer and decoders</li>
    <li>TTML subtitle encoder and muxer</li>
    <li>identity video filter</li>
    <li>msad video filter</li>
    <li>gophers protocol</li>
    <li>RIST protocol via librist</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr4.3">June 15th, 2020, FFmpeg 4.3 "4:3"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.3">FFmpeg 4.3 "4:3"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>v360 filter</li>
    <li>Intel QSV-accelerated MJPEG decoding</li>
    <li>Intel QSV-accelerated VP9 decoding</li>
    <li>Support for TrueHD in mp4</li>
    <li>Support AMD AMF encoder on Linux (via Vulkan)</li>
    <li>IMM5 video decoder</li>
    <li>ZeroMQ protocol</li>
    <li>support Sipro ACELP.KELVIN decoding</li>
    <li>streamhash muxer</li>
    <li>sierpinski video source</li>
    <li>scroll video filter</li>
    <li>photosensitivity filter</li>
    <li>anlms filter</li>
    <li>arnndn filter</li>
    <li>bilateral filter</li>
    <li>maskedmin and maskedmax filters</li>
    <li>VDPAU VP9 hwaccel</li>
    <li>median filter</li>
    <li>QSV-accelerated VP9 encoding</li>
    <li>AV1 encoding support via librav1e</li>
    <li>AV1 frame merge bitstream filter</li>
    <li>AV1 Annex B demuxer</li>
    <li>axcorrelate filter</li>
    <li>mvdv decoder</li>
    <li>mvha decoder</li>
    <li>MPEG-H 3D Audio support in mp4</li>
    <li>thistogram filter</li>
    <li>freezeframes filter</li>
    <li>Argonaut Games ADPCM decoder</li>
    <li>Argonaut Games ASF demuxer</li>
    <li>xfade video filter</li>
    <li>xfade_opencl filter</li>
    <li>afirsrc audio filter source</li>
    <li>pad_opencl filter</li>
    <li>Simon &amp; Schuster Interactive ADPCM decoder</li>
    <li>Real War KVAG demuxer</li>
    <li>CDToons video decoder</li>
    <li>siren audio decoder</li>
    <li>Rayman 2 ADPCM decoder</li>
    <li>Rayman 2 APM demuxer</li>
    <li>cas video filter</li>
    <li>High Voltage Software ADPCM decoder</li>
    <li>LEGO Racers ALP (.tun &amp; .pcm) demuxer</li>
    <li>AMQP 0-9-1 protocol (RabbitMQ)</li>
    <li>Vulkan support</li>
    <li>avgblur_vulkan, overlay_vulkan, scale_vulkan and chromaber_vulkan filters</li>
    <li>ADPCM IMA MTF decoder</li>
    <li>FWSE demuxer</li>
    <li>DERF DPCM decoder</li>
    <li>DERF demuxer</li>
    <li>CRI HCA decoder</li>
    <li>CRI HCA demuxer</li>
    <li>overlay_cuda filter</li>
    <li>switch from AvxSynth to AviSynth+ on Linux</li>
    <li>mv30 decoder</li>
    <li>Expanded styling support for 3GPP Timed Text Subtitles (movtext)</li>
    <li>WebP parser</li>
    <li>tmedian filter</li>
    <li>maskedthreshold filter</li>
    <li>Support for muxing pcm and pgs in m2ts</li>
    <li>Cunning Developments ADPCM decoder</li>
    <li>asubboost filter</li>
    <li>Pro Pinball Series Soundbank demuxer</li>
    <li>pcm_rechunk bitstream filter</li>
    <li>scdet filter</li>
    <li>NotchLC decoder</li>
    <li>gradients source video filter</li>
    <li>MediaFoundation encoder wrapper</li>
    <li>untile filter</li>
    <li>Simon &amp; Schuster Interactive ADPCM encoder</li>
    <li>PFM decoder</li>
    <li>dblur video filter</li>
    <li>Real War KVAG muxer</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="photosensitivity">October 5th, 2019, Bright Lights</h3>
  <p>
  FFmpeg has added a realtime bright flash removal filter to libavfilter.
  </p>
  <p>
  Note that this filter is not FDA approved, nor are we medical professionals.
  Nor has this filter been tested with anyone who has photosensitive epilepsy.
  FFmpeg and its photosensitivity filter are not making any medical claims.
  </p>
  <p>
  That said, this is a new video filter that may help photosensitive people
  watch tv, play video games or even be used with a VR headset to block
  out epiletic triggers such as filtered sunlight when they are outside.
  Or you could use it against those annoying white flashes on your tv screen.
  The filter fails on some input, such as the
  <a href="https://www.youtube.com/watch?v=8L_9hXnUzRk">Incredibles 2 Screen Slaver</a>
  scene. It is not perfect. If you have other clips that you want this filter to
  work better on, please report them to us on our <a href="http://trac.ffmpeg.org/">trac</a>.
  </p>
  <p>
  <a href="http://ffmpeg.org/~compn/output20p8.mp4">See for yourself</a>.
  Example was made with -vf photosensitivity=20:0.8
  </p>
  <p>
  We are not professionals. Please use this in your medical studies to
  advance epilepsy research. If you decide to use this in a medical
  setting, or make a hardware hdmi input output realtime tv filter,
  or find another use for this, <a href="mailto:compn@ffmpeg.org">please let me know</a>.
  This filter was a feature request of mine
  <a href="https://trac.ffmpeg.org/ticket/2104">since 2013</a>.
  </p>

  <h3 id="pr4.2">August 5th, 2019, FFmpeg 4.2 "Ada"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.2">FFmpeg 4.2 "Ada"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>tpad filter</li>
    <li>AV1 decoding support through libdav1d</li>
    <li>dedot filter</li>
    <li>chromashift and rgbashift filters</li>
    <li>freezedetect filter</li>
    <li>truehd_core bitstream filter</li>
    <li>dhav demuxer</li>
    <li>PCM-DVD encoder</li>
    <li>GIF parser</li>
    <li>vividas demuxer</li>
    <li>hymt decoder</li>
    <li>anlmdn filter</li>
    <li>maskfun filter</li>
    <li>hcom demuxer and decoder</li>
    <li>ARBC decoder</li>
    <li>libaribb24 based ARIB STD-B24 caption support (profiles A and C)</li>
    <li>Support decoding of HEVC 4:4:4 content in nvdec and cuviddec</li>
    <li>removed libndi-newtek</li>
    <li>agm decoder</li>
    <li>KUX demuxer</li>
    <li>AV1 frame split bitstream filter</li>
    <li>lscr decoder</li>
    <li>lagfun filter</li>
    <li>asoftclip filter</li>
    <li>Support decoding of HEVC 4:4:4 content in vdpau</li>
    <li>colorhold filter</li>
    <li>xmedian filter</li>
    <li>asr filter</li>
    <li>showspatial multimedia filter</li>
    <li>VP4 video decoder</li>
    <li>IFV demuxer</li>
    <li>derain filter</li>
    <li>deesser filter</li>
    <li>mov muxer writes tracks with unspecified language instead of English by default</li>
    <li>added support for using clang to compile CUDA kernels</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr4.1">November 6th, 2018, FFmpeg 4.1 "al-Khwarizmi"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.1">FFmpeg 4.1 "al-Khwarizmi"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>deblock filter</li>
    <li>tmix filter</li>
    <li>amplify filter</li>
    <li>fftdnoiz filter</li>
    <li>aderivative and aintegral audio filters</li>
    <li>pal75bars and pal100bars video filter sources</li>
    <li>mbedTLS based TLS support</li>
    <li>adeclick and adeclip filters</li>
    <li>libtensorflow backend for DNN based filters like srcnn</li>
    <li>VC1 decoder is now bit-exact</li>
    <li>ATRAC9 decoder</li>
    <li>lensfun wrapper filter</li>
    <li>colorconstancy filter</li>
    <li>AVS2 video decoder via libdavs2</li>
    <li>IMM4 video decoder</li>
    <li>Brooktree ProSumer video decoder</li>
    <li>MatchWare Screen Capture Codec decoder</li>
    <li>WinCam Motion Video decoder</li>
    <li>1D LUT filter (lut1d)</li>
    <li>RemotelyAnywhere Screen Capture decoder</li>
    <li>cue and acue filters</li>
    <li>Support for AV1 in MP4 and Matroska/WebM</li>
    <li>transpose_npp filter</li>
    <li>AVS2 video encoder via libxavs2</li>
    <li>amultiply filter</li>
    <li>Block-Matching 3d (bm3d) denoising filter</li>
    <li>acrossover filter</li>
    <li>ilbc decoder</li>
    <li>audio denoiser as afftdn filter</li>
    <li>AV1 parser</li>
    <li>sinc audio filter source</li>
    <li>chromahold filter</li>
    <li>setparams filter</li>
    <li>vibrance filter</li>
    <li>S12M timecode decoding in h264</li>
    <li>xstack filter</li>
    <li>(a)graphmonitor filter</li>
    <li>yadif_cuda filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr4.0">April 20th, 2018, FFmpeg 4.0 "Wu"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.0">FFmpeg 4.0 "Wu"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streams</li>
    <li>Experimental MagicYUV encoder</li>
    <li>TiVo ty/ty+ demuxer</li>
    <li>Intel QSV-accelerated MJPEG encoding</li>
    <li>native aptX and aptX HD encoder and decoder</li>
    <li>NVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decoding</li>
    <li>Intel QSV-accelerated overlay filter</li>
    <li>mcompand audio filter</li>
    <li>acontrast audio filter</li>
    <li>OpenCL overlay filter</li>
    <li>video mix filter</li>
    <li>video normalize filter</li>
    <li>audio lv2 wrapper filter</li>
    <li>VAAPI MJPEG and VP8 decoding</li>
    <li>AMD AMF H.264 and HEVC encoders</li>
    <li>video fillborders filter</li>
    <li>video setrange filter</li>
    <li>support LibreSSL (via libtls)</li>
    <li>Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.</li>
    <li>deconvolve video filter</li>
    <li>entropy video filter</li>
    <li>hilbert audio filter source</li>
    <li>aiir audio filter</li>
    <li>Removed the ffserver program</li>
    <li>Removed the ffmenc and ffmdec muxer and demuxer</li>
    <li>VideoToolbox HEVC encoder and hwaccel</li>
    <li>VAAPI-accelerated ProcAmp (color balance), denoise and sharpness filters</li>
    <li>Add android_camera indev</li>
    <li>codec2 en/decoding via libcodec2</li>
    <li>native SBC encoder and decoder</li>
    <li>drmeter audio filter</li>
    <li>hapqa_extract bitstream filter</li>
    <li>filter_units bitstream filter</li>
    <li>AV1 Support through libaom</li>
    <li>E-AC-3 dependent frames support</li>
    <li>bitstream filter for extracting E-AC-3 core</li>
    <li>Haivision SRT protocol via libsrt</li>
    <li>vfrdet filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr3.4">October 15th, 2017, FFmpeg 3.4 "Cantor"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.4">FFmpeg 3.4 "Cantor"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>deflicker video filter</li>
    <li>doubleweave video filter</li>
    <li>lumakey video filter</li>
    <li>pixscope video filter</li>
    <li>oscilloscope video filter</li>
    <li>update cuvid/nvenc headers to Video Codec SDK 8.0.14</li>
    <li>afir audio filter</li>
    <li>scale_cuda CUDA based video scale filter</li>
    <li>librsvg support for svg rasterization</li>
    <li>crossfeed audio filter</li>
    <li>spec compliant VP9 muxing support in MP4</li>
    <li>surround audio filter</li>
    <li>sofalizer filter switched to libmysofa</li>
    <li>Gremlin Digital Video demuxer and decoder</li>
    <li>headphone audio filter</li>
    <li>superequalizer audio filter</li>
    <li>roberts video filter</li>
    <li>additional frame format support for Interplay MVE movies</li>
    <li>support for decoding through D3D11VA in ffmpeg</li>
    <li>limiter video filter</li>
    <li>libvmaf video filter</li>
    <li>Dolby E decoder and SMPTE 337M demuxer</li>
    <li>unpremultiply video filter</li>
    <li>tlut2 video filter</li>
    <li>floodfill video filter</li>
    <li>pseudocolor video filter</li>
    <li>raw G.726 muxer and demuxer, left- and right-justified</li>
    <li>NewTek NDI input/output device</li>
    <li>FITS demuxer and decoder</li>
    <li>FITS muxer and encoder</li>
    <li>despill video filter</li>
    <li>haas audio filter</li>
    <li>SUP/PGS subtitle muxer</li>
    <li>convolve video filter</li>
    <li>VP9 tile threading support</li>
    <li>KMS screen grabber</li>
    <li>CUDA thumbnail filter</li>
    <li>V4L2 mem2mem HW assisted codecs</li>
    <li>Rockchip MPP hardware decoding</li>
    <li>vmafmotion video filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr3.3">April 13th, 2017, FFmpeg 3.3 "Hilbert"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.3">FFmpeg 3.3 "Hilbert"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>Apple Pixlet decoder</li>
    <li>NewTek SpeedHQ decoder</li>
    <li>QDMC audio decoder</li>
    <li>PSD (Photoshop Document) decoder</li>
    <li>FM Screen Capture decoder</li>
    <li>ScreenPressor decoder</li>
    <li>XPM decoder</li>
    <li>DNxHR decoder fixes for HQX and high resolution videos</li>
    <li>ClearVideo decoder (partial)</li>
    <li>16.8 and 24.0 floating point PCM decoder</li>
    <li>Intel QSV-accelerated VP8 video decoding</li>
    <li>native Opus encoder</li>
    <li>DNxHR 444 and HQX encoding</li>
    <li>Quality improvements for the (M)JPEG encoder</li>
    <li>VAAPI-accelerated MPEG-2 and VP8 encoding</li>
    <li>premultiply video filter</li>
    <li>abitscope multimedia filter</li>
    <li>readeia608 filter</li>
    <li>threshold filter</li>
    <li>midequalizer filter</li>
    <li>MPEG-7 Video Signature filter</li>
    <li>add internal ebur128 library, remove external libebur128 dependency</li>
    <li>Intel QSV video scaling and deinterlacing filters</li>
    <li>Sample Dump eXchange demuxer</li>
    <li>MIDI Sample Dump Standard demuxer</li>
    <li>Scenarist Closed Captions demuxer and muxer</li>
    <li>Support MOV with multiple sample description tables</li>
    <li>Pro-MPEG CoP #3-R2 FEC protocol</li>
    <li>Support for spherical videos</li>
    <li>CrystalHD decoder moved to new decode API</li>
    <li>configure now fails if autodetect-libraries are requested but not found</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="gsoc2016finalreport">October 30th, 2016, Results: Summer Of Code 2016.</h3>
  <p>
    This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.
  </p>
  <p>
    Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:
  </p>
  <h4>FFv1 (Mentor: Michael Niedermayer)</h4>
  <p>
    Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.
  </p>
  <h4>Self test coverage (Mentor: Michael Niedermayer)</h4>
  <p>
    Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.
  </p>
  <h4>MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann)</h4>
  <p>
    Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.
  </p>
  <h4>Tee muxer improvements (Mentor: Marton Balint)</h4>
  <p>
    Ján Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.
  </p>
  <h4>TrueHD encoder (Mentor: Rostislav Pehlivanov)</h4>
  <p>
    Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.
  </p>
  <h4>Motion interpolation filter (Mentor: Paul B Mahol)</h4>
  <p>
    Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.
  </p>
  <p>
    And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!
  </p>
  <h3 id="sdl1">September 24th, 2016, SDL1 support dropped.</h3>
  <p>
    Support for the SDL1 library has been dropped, due to it no longer being maintained (as of
    January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device
    has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output
    devices have been updated to support SDL2.
  </p>
  <h3 id="pr3.1.2">August 9th, 2016, FFmpeg 3.1.2 "Laplace"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.1">FFmpeg 3.1.2</a>, a new point release from the 3.1 release branch, is now available!
    It fixes several bugs.
  </p>
  <p>
    We recommend users, distributors, and system integrators, to upgrade unless they use current git master.
  </p>
  <h3 id="ffserv">July 10th, 2016, ffserver program being dropped</h3>
  <p>
    After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release.
    ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat
    library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has
    been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax.
    Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs
    and to contact us so we may point users to test and contribute to its development.
  </p>
  <h3 id="pr3.1.1">July 1st, 2016, FFmpeg 3.1.1 "Laplace"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.1">FFmpeg 3.1.1</a>, a new point release from the 3.1 release branch, is now available!
    It mainly deals with a few ABI issues introduced in the previous release.
  </p>
  <p>
    We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr3.1">June 27th, 2016, FFmpeg 3.1 "Laplace"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.1">FFmpeg 3.1 "Laplace"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>DXVA2-accelerated HEVC Main10 decoding</li>
    <li>fieldhint filter</li>
    <li>loop video filter and aloop audio filter</li>
    <li>Bob Weaver deinterlacing filter</li>
    <li>firequalizer filter</li>
    <li>datascope filter</li>
    <li>bench and abench filters</li>
    <li>ciescope filter</li>
    <li>protocol blacklisting API</li>
    <li>MediaCodec H264 decoding</li>
    <li>VC-2 HQ RTP payload format (draft v1) depacketizer and packetizer</li>
    <li>VP9 RTP payload format (draft v2) packetizer</li>
    <li>AudioToolbox audio decoders</li>
    <li>AudioToolbox audio encoders</li>
    <li>coreimage filter (GPU based image filtering on OSX)</li>
    <li>libdcadec removed</li>
    <li>bitstream filter for extracting DTS core</li>
    <li>ADPCM IMA DAT4 decoder</li>
    <li>musx demuxer</li>
    <li>aix demuxer</li>
    <li>remap filter</li>
    <li>hash and framehash muxers</li>
    <li>colorspace filter</li>
    <li>hdcd filter</li>
    <li>readvitc filter</li>
    <li>VAAPI-accelerated format conversion and scaling</li>
    <li>libnpp/CUDA-accelerated format conversion and scaling</li>
    <li>Duck TrueMotion 2.0 Real Time decoder</li>
    <li>Wideband Single-bit Data (WSD) demuxer</li>
    <li>VAAPI-accelerated H.264/HEVC/MJPEG encoding</li>
    <li>DTS Express (LBR) decoder</li>
    <li>Generic OpenMAX IL encoder with support for Raspberry Pi</li>
    <li>IFF ANIM demuxer &amp; decoder</li>
    <li>Direct Stream Transfer (DST) decoder</li>
    <li>loudnorm filter</li>
    <li>MTAF demuxer and decoder</li>
    <li>MagicYUV decoder</li>
    <li>OpenExr improvements (tile data and B44/B44A support)</li>
    <li>BitJazz SheerVideo decoder</li>
    <li>CUDA CUVID H264/HEVC decoder</li>
    <li>10-bit depth support in native utvideo decoder</li>
    <li>libutvideo wrapper removed</li>
    <li>YUY2 Lossless Codec decoder</li>
    <li>VideoToolbox H.264 encoder</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="gsoc2016">March 16th, 2016, Google Summer of Code</h3>
  <p>
    FFmpeg has been accepted as a <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a> open source organization. If you wish to
    participate as a student see our <a href="https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2016">project ideas page</a>.
    You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft.
    Good luck!
  </p>

  <h3 id="pr3.0">February 15th, 2016, FFmpeg 3.0 "Einstein"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.0">FFmpeg 3.0 "Einstein"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li><a href="#aac_encoder_stable">The native FFmpeg AAC encoder has seen extensive improvements and is no longer considered experimental</a></li>
    <li><a href="#removing_external_aac_encoders">Removed support for libvo-aacenc and libaacplus</a></li>
    <li>Over 30 new filters have been added</li>
    <li>Many ASM optimizations</li>
    <li>VP9 Hardware Acceleration (DXVA2 and VA-API)</li>
    <li>Cineform HD decoder</li>
    <li>New DCA decoder based on libdcadec with full support for DTS-HD extensions</li>
    <li>As with all major releases expect major backward incompatible API/ABI changes</li>
    <li>See the <a href="https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=Changelog;hb=n3.0">Changelog</a> for a list of more updates</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="removing_external_aac_encoders">January 30, 2016, Removing support for two external AAC encoders</h3>
  <p>
    We have just removed support for VisualOn AAC encoder (libvo-aacenc) and
    libaacplus in FFmpeg master.
  </p>
  <p>
    Even before marking our internal AAC encoder as
    <a href="#aac_encoder_stable">stable</a>, it was known that libvo-aacenc
    was of an inferior quality compared to our native one for most samples.
    However, the VisualOn encoder was used extensively by the Android Open
    Source Project, and we would like to have a tested-and-true stable option
    in our code base.
  </p>
  <p>
    When first committed in 2011, libaacplus filled in the gap of encoding
    High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported
    by any of the encoders in FFmpeg at that time.
  </p>
  <p>
    The circumstances for both have changed. After the work spearheaded by
    Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC
    encoder is ready to compete with much more mature encoders. The Fraunhofer
    FDK AAC Codec Library for Android was added in 2012 as the fourth
    supported external AAC encoder, and the one with the best quality and the
    most features supported, including HE-AAC and HE-AACv2.
  </p>
  <p>
    Therefore, we have decided that it is time to remove libvo-aacenc and
    libaacplus. If you are currently using libvo-aacenc, prepare to transition
    to the native encoder (<code>aac</code>) when updating to the next version
    of FFmpeg. In most cases it is as simple as merely swapping the encoder
    name. If you are currently using libaacplus, start using FDK AAC
    (<code>libfdk_aac</code>) with an appropriate <code>profile</code> option
    to select the exact AAC profile that fits your needs. In both cases, you
    will enjoy an audible quality improvement and as well as fewer licensing
    headaches.
  </p>
  <p>
    Enjoy!
  </p>

  <h3 id="pr2.8.5">January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10</h3>
  <p>
    We have made several new point releases (<b><a href="https://ffmpeg.org/download.html#release_2.8">2.8.5</a>,
      <a href="https://ffmpeg.org/download.html#release_2.7">2.7.5</a>,
      <a href="https://ffmpeg.org/download.html#release_2.6">2.6.7</a>,
      <a href="https://ffmpeg.org/download.html#release_2.5">2.5.10</a></b>).
    They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898.
    Please see the changelog for each release for more details.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="aac_encoder_stable">December 5th, 2015, The native FFmpeg AAC encoder is now stable!</h3>
  <p>
    After seven years the native FFmpeg AAC encoder has had its experimental flag
    removed and declared as ready for general use. The encoder is transparent
    at 128kbps for most samples tested with artifacts only appearing in extreme
    cases. Subjective quality tests put the encoder to be of equal or greater
    quality than most of the other encoders available to the public.
  </p>
  <p>
    Licensing has always been an issue with encoding AAC audio as most of the
    encoders have had a license making FFmpeg unredistributable if compiled with
    support for them. The fact that there now exists a fully open and truly
    free AAC encoder integrated directly within the project means a lot to those
    who wish to use accepted and widespread standards.
  </p>
  <p>
    The majority of the work done to bring the encoder up to quality was started
    during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov.
    Both continued to work on the encoder with the latter joining as a developer
    and mainainer, working on other parts of the project as well. Also, thanks
    to <a href="http://d.hatena.ne.jp/kamedo2/">Kamedo2</a> who does comparisons
    and tests, the original authors and all past and current contributors to the
    encoder. Users are suggested and encouraged to use the encoder and provide
    feedback or breakage reports through our <a href="https://trac.ffmpeg.org/">bug tracker</a>.
  </p>

  
  <p>
    A big thank you note goes to our newest supporters: MediaHub and Telepoint.
    Both companies have donated a dedicated server with free of charge internet
    connectivity. Here is a little bit about them in their own words:
  </p>

  <ul>
    <li>
      <p>
        <a href="http://www.telepoint.bg/en/">Telepoint</a> is the biggest
        carrier-neutral data center in Bulgaria. Located in the heart of Sofia
        on a cross-road of many Bulgarian and International networks, the
        facility is a fully featured Tier 3 data center that provides flexible
        customer-oriented colocation solutions (ranging from a server to a
        private collocation hall) and a high level of security.
      </p>
    </li>

    <li>
      <p>
        MediaHub Ltd. is a Bulgarian IPTV platform and services provider which
        uses FFmpeg heavily since it started operating a year ago. <i>"Donating
        to help keep FFmpeg online is our way of giving back to the community"
        </i>.
      </p>
    </li>
  </ul>

  <p>
    Thanks Telepoint and MediaHub for their support!
  </p>

  <h3 id="gsoc2015_result">September 29th, 2015, GSoC 2015 results</h3>

  <p>
    FFmpeg participated to the latest edition of
    the <a href="http://www.google-melange.com/gsoc/homepage/google/gsoc2015">Google
    Summer of Code</a> Project. FFmpeg got a total of 8 assigned
    projects, and 7 of them were successful.
  </p>

  <p>We want to thank <a href="https://www.google.com/">Google</a>, the
    participating students, and especially the mentors who joined this
    effort. We're looking forward to participating in the next GSoC
    edition!
  </p>

  <p>
    Below you can find a brief description of the final outcome of
    each single project.
  </p>

  <h4>Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George</h4>

  <p>
    Stephan Holljes's project for this session of Google Summer of Code was to
    implement basic HTTP server features for libavformat, to complement the
    already present HTTP client and RTMP and RTSP server code.
  </p>

  <p>
    The first part of the project was to make the HTTP code capable of accepting
    a single client; it was completed partly during the qualification period and
    partly during the first week of the summer. Thanks to this work, it is now
    possible to make a simple HTTP stream using the following commands:
  </p>

  <pre>    ffmpeg -i /dev/video0 -listen 1 -f matroska \
    -c:v libx264 -preset fast -tune zerolatency http://:8080
    ffplay http://localhost:8080/
  </pre>

  <p>
    The next part of the project was to extend the code to be able to accept
    several clients, simultaneously or consecutively. Since libavformat did not
    have an API for that kind of task, it was necessary to design one. This part
    was mostly completed before the midterm and applied shortly afterwards.
    Since the ffmpeg command-line tool is not ready to serve several clients,
    the test ground for that new API is an example program serving hard-coded
    content.
  </p>

  <p>
    The last and most ambitious part of the project was to update ffserver to
    make use of the new API. It would prove that the API is usable to implement
    real HTTP servers, and expose the points where more control was needed. By
    the end of the summer, a first working patch series was undergoing code
    review.
  </p>

  <h4>Browsing content on the server, mentee: Mariusz Szczepańczyk, mentor: Lukasz Marek</h4>

  <p>
    Mariusz finished an API prepared by the FFmpeg community and implemented
    Samba directory listing as qualification task.
  </p>

  <p>
    During the program he extended the API with the possibility to
    remove and rename files on remote servers. He completed the
    implementation of these features for file, Samba, SFTP, and FTP
    protocols.
  </p>

  <p>
    At the end of the program, Mariusz provided a sketch of an
    implementation for HTTP directory listening.
  </p>

  <h4>Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack</h4>

  <p>
    Mate was working on directshow input from digital video sources. He
    got working input from ATSC input sources, with specifiable tuner.
  </p>

  <p>
    The code has not been committed, but a patch of it was sent to the
    ffmpeg-devel mailing list for future use.
  </p>

  <p>
    The mentor plans on cleaning it up and committing it, at least for the
    ATSC side of things. Mate and the mentor are still working trying to
    finally figure out how to get DVB working.
  </p>

  <h4>Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale</h4>

  <p>
    Niklesh's project was to expand our support for 3GPP Timed Text
    subtitles. This is the native subtitle format for mp4 containers, and
    is interesting because it's usually the only subtitle format supported
    by the stock playback applications on iOS and Android devices.
  </p>

  <p>
    ffmpeg already had basic support for these subtitles which ignored all
    formatting information - it just provided basic plain-text support.
  </p>

  <p>
    Niklesh did work to add support on both the encode and decode side for
    text formatting capabilities, such as font size/colour and effects like
    bold/italics, highlighting, etc.
  </p>

  <p>
    The main challenge here is that Timed Text handles formatting in a very
    different way from most common subtitle formats. It uses a binary
    encoding (based on mp4 boxes, naturally) and stores information
    separately from the text itself. This requires additional work to track
    which parts of the text formatting applies to, and explicitly dealing
    with overlapping formatting (which other formats support but Timed
    Text does not) so it requires breaking the overlapping sections into
    separate non-overlapping ones with different formatting.
  </p>

  <p>
    Finally, Niklesh had to be careful about not trusting any size
    information in the subtitles - and that's no joke: the now infamous
    Android stagefright bug was in code for parsing Timed Text subtitles.
  </p>

  <p>
    All of Niklesh's work is committed and was released in ffmpeg 2.8.
  </p>

<h4>libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla</h4>

  <p>
    Pedro Arthur has modularized the vertical and horizontal scalers.
    To do this he designed and implemented a generic filter framework
    and moved the existing scaler code into it. These changes now allow
    easily adding removing, splitting or merging processing steps.
    The implementation was benchmarked and several alternatives were
    tried to avoid speed loss.
  </p>

  <p>
    He also added gamma corrected scaling support.
    An example to use gamma corrected scaling would be:
  </p>

  <pre>    ffmpeg -i input -vf scale=512:384:gamma=1 output
  </pre>

  <p>
    Pedro has done impressive work considering the short time available,
    and he is a FFmpeg committer now. He continues to contribute to
    FFmpeg, and has fixed some bugs in libswscale after GSoC has
    ended.
  </p>

  <h4>AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire</h4>

  <p>
    Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main
    prediction on the native AAC encoder. Of all those extensions, only
    TNS was left in a less-than-usable state, but the implementation has
    been pushed (disabled) anyway since it's a good basis for further
    improvements.
  </p>

  <p>
    PNS replaces noisy bands with a single scalefactor representing the
    energy of that band, gaining in coding efficiency considerably, and
    the quality improvements on low bitrates are impressive for such a
    simple feature.
  </p>

  <p>
    TNS still needs some polishing, but has the potential to reduce coding
    artifacts by applying noise shaping in the temporal domain (something
    that is a source of annoying, notable distortion on low-entropy
    bands).
  </p>

  <p>
    Intensity Stereo coding (I/S) can double coding efficiency by
    exploiting strong correlation between stereo channels, most effective
    on pop-style tracks that employ panned mixing. The technique is not as
    effective on classic X-Y recordings though.
  </p>

  <p>
    Finally, main prediction improves coding efficiency by exploiting
    correlation among successive frames. While the gains have not been
    huge at this point, Rostislav has remained active even after the GSoC,
    and is polishing both TNS and main prediction, as well as looking for
    further improvements to make.
  </p>

  <p>
    In the process, the MIPS port of the encoder was broken a few times,
    something he's also working to fix.
  </p>

  <h4>Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol</h4>

  <p>
    Donny Yang implemented basic keyframe only APNG encoder as the
    qualification task. Later he wrote interframe compression via
    various blend modes. The current implementation tries all blend
    modes and picks one which takes the smallest amount of memory.
  </p>

  <p>
    Special care was taken to make sure that the decoder plays
    correctly all files found in the wild and that the encoder
    produces files that can be played in browsers that support APNG.
  </p>

  <p>
    During his work he was tasked to fix any encountered bug in the
    decoder due to the fact that it doesn't match APNG
    specifications. Thanks to this work, a long standing bug in the
    PNG decoder has been fixed.
  </p>

  <p>
    For latter work he plans to continue working on the encoder,
    making it possible to select which blend modes will be used in the
    encoding process. This could speed up encoding of APNG files.
  </p>

  <h3 id="pr2.8">September 9th, 2015, FFmpeg 2.8</h3>
  <p>
    We published release <b><a href="https://ffmpeg.org/download.html#release_2.8">2.8</a></b> as new major version.
    It contains all features and bug fixes of the git master branch from September 8th. Please see
    the <b><a href="https://raw.githubusercontent.com/FFmpeg/FFmpeg/release/2.8/Changelog">changelog</a></b>
    for a list of the most important changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use current git master.
  </p>

  <h3 id="message">August 1st, 2015, A message from the FFmpeg project</h3>
  <p>
    Dear multimedia community,
  </p>
  <p>
    The resignation of Michael Niedermayer as leader of FFmpeg yesterday has
    come by surprise. He has worked tirelessly on the FFmpeg project for many
    years and we must thank him for the work that he has done. We hope that in
    the future he will continue to contribute to the project. In the coming
    weeks, the FFmpeg project will be managed by the active contributors.
  </p>
  <p>
    The last four years have not been easy for our multimedia community - both
    contributors and users. We should now look to the future, try to find
    solutions to these issues, and to have reconciliation between the forks,
    which have split the community for so long.
  </p>
  <p>
    Unfortunately, much of the disagreement has taken place in inappropriate
    venues so far, which has made finding common ground and solutions
    difficult. We aim to discuss this in our communities online over the coming
    weeks, and in person at the <a href="https://www.videolan.org/videolan/events/vdd15/">VideoLAN Developer
    Days</a> in Paris in September: a neutral venue for the entire open source
    multimedia community.
  </p>
  <p>
    The FFmpeg project.
  </p>

  <h3 id="needhost">July 4th, 2015, FFmpeg needs a new host</h3>
  <p><b>UPDATE:</b> We have received more than 7 offers for hosting and servers, thanks a lot to everyone!</p>
  <p>
    After graciously hosting our projects (<a href="http://www.ffmpeg.org/">FFmpeg</a>, <a href="http://www.mplayerhq.hu/">MPlayer</a>
    and <a href="http://rtmpdump.mplayerhq.hu/">rtmpdump</a>) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.
  </p>
  <p>
    If you want to host an open source project, please let us know, either on <a href="http://ffmpeg.org/mailman/listinfo/ffmpeg-devel">ffmpeg-devel</a>
    mailing list or irc.freenode.net #ffmpeg-devel.
  </p>
  <p>
    We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, <a href="http://trac.ffmpeg.org/">trac</a>, <a href="http://samples.ffmpeg.org/">samples repo</a>, svn, etc.
  </p>

  <h3 id="pr2.6.1">March 16, 2015, FFmpeg 2.6.1</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.6">2.6</a></b>)
    and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March.
    Please see the <b><a href="http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.6">Release Notes</a></b> for a
    list of note-worthy changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="gsoc2015">March 4, 2015, Google Summer of Code</h3>
  <p>
    FFmpeg has been accepted as a <a href="http://www.google-melange.com/gsoc/homepage/google/gsoc2015">Google Summer of Code</a> Project. If you wish to
    participate as a student see our <a href="https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2015">project ideas page</a>.
    You can already get in contact with mentors and start working on qualification tasks. Registration
    at Google for students will open March 16th. Good luck!
  </p>

  <h3 id="clt2015">March 1, 2015, Chemnitzer Linux-Tage</h3>
  <p>
    We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage
    (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.
  </p>

  <p>
    More information can be found <a href="https://chemnitzer.linux-tage.de/2015/en/">here</a>
  </p>

  <p>
    We demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes. <strong>If you have media files that cannot be
    processed correctly with FFmpeg, be sure to have a sample with you
    so we can have a look!</strong>
  </p>
  <p>
    For the first time in our CLT history, there will be an <strong>FFmpeg workshop</strong>!
    You can read the details <a href="https://chemnitzer.linux-tage.de/2015/de/programm/beitrag/209">here</a>.
    The workshop is targeted at FFmpeg beginners. First the basics of
    multimedia will be covered. Thereafter you will learn how to use
    that knowledge and the FFmpeg CLI tools to analyse and process media
    files. The workshop is in German language only and prior registration
    is necessary. The workshop will be on Saturday starting at 10 o'clock.
  </p>
  <p>
    We are looking forward to meet you (again)!
  </p>

  <h3 id="pr2.5">December 5, 2014, FFmpeg 2.5</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.5">2.5</a></b>)
    It contains all features and bugfixes of the git master branch from the 4th December.
    Please see the <b><a href="http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.5">Release Notes</a></b> for a
    list of note-worthy changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="ffmpeg_back_in_sid">October 10, 2014, FFmpeg is in Debian unstable again</h3>
  <p>
    We wanted you to know there are
    <a href="https://packages.debian.org/search?keywords=ffmpeg&amp;searchon=sourcenames&amp;suite=unstable&amp;section=main">
    FFmpeg packages in Debian unstable</a> again. <strong>A big thank-you
    to Andreas Cadhalpun and all the people that made it possible.</strong> It has been anything but simple.
  </p>
  <p>
    Unfortunately that was already the easy part of this news. The bad news is the packages probably won't
    migrate to Debian testing to be in the upcoming release codenamed jessie.
    <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=763148">Read the argumentation over at Debian.</a>
  </p>
  <p>
    <strong>However things will come out in the end, we hope for your continued remarkable support!</strong>
  </p>

  <h3 id="opw03">October 8, 2014, FFmpeg secured a place in OPW!</h3>
  <p>
    Thanks to a generous 6K USD donation by Samsung (Open Source Group),
    FFmpeg will be welcoming at least 1 "Outreach Program for Women" intern
    to work with our community for an initial period starting December 2014
    (through March 2015).
  </p>

  <p>
    We all know FFmpeg is used by the industry, but even while there are
    countless products building on our code, it is not at all common for
    companies to step up and help us out when needed. So a big thank-you
    to Samsung and the OPW program committee!
  </p>

  <p>
    If you are thinking on participating in OPW as an intern, please take
    a look at our <a href="https://trac.ffmpeg.org/wiki/SponsoringPrograms/OPW/2014-12">OPW wiki page</a>
    for some initial guidelines. The page is still a work in progress, but
    there should be enough information there to get you started. If you, on
    the other hand, are thinking on sponsoring work on FFmpeg through the
    OPW program, please get in touch with us at opw@ffmpeg.org. With your
    help, we might be able to secure some extra intern spots for this round!
  </p>

  <h3 id="pr2.4">September 15, 2014, FFmpeg 2.4</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.4">2.4</a></b>)
    It contains all features and bugfixes of the git master branch from the 14th September.
    Please see the <b><a href="http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.4">Release Notes</a></b> for a
    list of note-worthy changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="pr2.3.3">August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8</h3>
  <p>
    We have made several new point releases (<b><a href="https://ffmpeg.org/download.html#release_2.3">2.3.3</a>,
      <a href="https://ffmpeg.org/download.html#release_2.2">2.2.7</a>,
      <a href="https://ffmpeg.org/download.html#release_1.2">1.2.8</a></b>).
    They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272.
    Please see the changelog for more details.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="opw02">July 29, 2014, Help us out securing our spot in OPW</h3>
  <p>
    Following our previous post regarding our participation on this year's
    OPW (Outreach Program for Women), we are now reaching out to our users
    (both individuals and companies) to help us gather the needed money to
    secure our spot in the program.<br>
    We need to put together 6K USD as a minimum but securing more funds would
    help us towards getting more than one intern.<br>
    You can donate by credit card using
    <a href="https://co.clickandpledge.com/advanced/default.aspx?wid=56226">
    Click&amp;Pledge</a> and selecting the "OPW" option. If you would like to
    donate by money transfer or by check, please get in touch by
    <a href="mailto:opw@ffmpeg.org">e-mail</a> and we will get back to you
    with instructions.<br>Thanks!
  </p>

  <h3 id="newweb">July 20, 2014, New website</h3>
  <p>
    The FFmpeg project is proud to announce a brand new version of the website
    made by <a href="http://db0.fr/">db0</a>. While this was initially motivated
    by the need for a larger menu, the whole website ended up being redesigned,
    and most pages got reworked to ease navigation. We hope you'll enjoy
    browsing it.
  </p>

  <h3 id="pr2.3">July 17, 2014, FFmpeg 2.3</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.3">2.3</a></b>)
    It contains all features and bugfixes of the git master branch from the 16th July.
    Please see the <b><a href="http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=489d066">Release Notes</a></b> for a
    list of note-worthy changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="opw01">July 3, 2014, FFmpeg and the Outreach Program For Women</h3>
  <p>
    FFmpeg has started the process to become an OPW includer organization for the
    next round of the program, with internships starting December 9. The
    <a href="https://gnome.org/opw/">OPW</a> aims to "Help women (cis and trans)
    and genderqueer to get involved in free and open source software". Part of the
    process requires securing funds to support at least one internship (6K USD), so
    if you were holding on your donation to FFmpeg, this is a great chance for you
    to come forward, get in touch and help both the project and a great initiative!
  </p>
  <p>
    We have set up an <a href="mailto:opw@ffmpeg.org">email address</a> you can use
    to contact us about donations and general inquires regarding our participation
    in the program. Hope to hear from you soon!
  </p>

  <h3 id="pr2.2.4">June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14</h3>
  <p>
    We have made several new point releases (<b><a href="https://ffmpeg.org/download.html#release_2.2">2.2.4</a>,
      <a href="https://ffmpeg.org/download.html#release_2.1">2.1.5</a>,
      <a href="https://ffmpeg.org/download.html#release_2.0">2.0.5</a>,
      <a href="https://ffmpeg.org/download.html#release_1.2">1.2.7</a>,
      <a href="https://ffmpeg.org/download.html#release_1.1">1.1.12</a>,
      <a href="https://ffmpeg.org/download.html#release_0.10">0.10.14</a></b>).
    They fix a
    <a href="http://blog.securitymouse.com/2014/06/raising-lazarus-20-year-old-bug-that.html">security issue in the LZO implementation</a>,
    as well as several other bugs. See the git log for details.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>


  <h3 id="lt2014">May 1, 2014, LinuxTag</h3>
  <p>
    Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will
    take place from 8th to 10th of May. Please note that this year's LinuxTag is at a
    different location closer to the city center.
  </p>

  <p>
    We will have a shared booth with XBMC and VideoLAN.
    <b>
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    </b>
  </p>

  <p>
    More information about LinuxTag can be found <a href="http://www.linuxtag.org/2014/">here</a>
  </p>

  <p>
    We are looking forward to see you in Berlin!
  </p>

  <h3 id="heartbleed">April 18, 2014, OpenSSL Heartbeat bug</h3>
  <p>
    Our server hosting the Trac issue tracker was vulnerable to the attack
    against OpenSSL known as "heartbleed". The OpenSSL software library
    was updated on 7th of April, shortly after the vulnerability was publicly
    disclosed. We have changed the private keys (and certificates) for all
    FFmpeg servers. The details were sent to the mailing lists by
    Alexander Strasser, who is part of the project server team. Here is a
    link to the user mailing list
    <a href="https://lists.ffmpeg.org/pipermail/ffmpeg-user/2014-April/020968.html">archive</a>
    .
  </p><p>
    We encourage you to read up on
    <a href="https://www.schneier.com/blog/archives/2014/04/heartbleed.html">"OpenSSL heartbleed"</a>.
    <b>It is possible that login data for the issue tracker was exposed to
      people exploiting this security hole. You might want to change your password
      in the tracker and everywhere else you used that same password.</b>
  </p>

  <h3 id="pr2.2.1">April 11, 2014, FFmpeg 2.2.1</h3>
  <p>
    We have made a new point releases (<b><a href="https://ffmpeg.org/download.html#release_2.2">2.2.1</a></b>).
    It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as
    several other fixes.
    See the git log for details.
  </p>

  <h3 id="pr2.2">March 24, 2014, FFmpeg 2.2</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.2">2.2</a></b>)
    It contains all features and bugfixes of the git master branch from 1st March.
    A partial list of new stuff is below:
  </p>
  <pre>    - HNM version 4 demuxer and video decoder
    - Live HDS muxer
    - setsar/setdar filters now support variables in ratio expressions
    - elbg filter
    - string validation in ffprobe
    - support for decoding through VDPAU in ffmpeg (the -hwaccel option)
    - complete Voxware MetaSound decoder
    - remove mp3_header_compress bitstream filter
    - Windows resource files for shared libraries
    - aeval filter
    - stereoscopic 3d metadata handling
    - WebP encoding via libwebp
    - ATRAC3+ decoder
    - VP8 in Ogg demuxing
    - side &amp; metadata support in NUT
    - framepack filter
    - XYZ12 rawvideo support in NUT
    - Exif metadata support in WebP decoder
    - OpenGL device
    - Use metadata_header_padding to control padding in ID3 tags (currently used in
    MP3, AIFF, and OMA files), FLAC header, and the AVI "junk" block.
    - Mirillis FIC video decoder
    - Support DNx444
    - libx265 encoder
    - dejudder filter
    - Autodetect VDA like all other hardware accelerations
  </pre>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="clt2014">February 3, 2014, Chemnitzer Linux-Tage</h3>
  <p>
    We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage'
    in Chemnitz, Germany. The event will take place on 15th and 16th of March.
  </p>

  <p>
    More information can be found <a href="http://chemnitzer.linux-tage.de/2014/en/info/">here</a>
  </p>

  <p>
    We invite you to visit us at our booth located in the Linux-Live area!
    There we will demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes.
  </p>
  <p>
    <b>
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    </b>
  </p>
  <p>
    We are looking forward to meet you (again)!
  </p>


  <h3 id="trac_sec">February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach</h3>
  <p>
    The server on which FFmpeg and MPlayer Trac issue trackers were
    installed was compromised. The affected server was taken offline
    and has been replaced and all software reinstalled.
    FFmpeg Git, releases, FATE, web and mailinglists are on other servers
    and were not affected. We believe that the original compromise happened
    to a server, unrelated to FFmpeg and MPlayer, several months ago.
    That server was used as a source to clone the VM that we recently moved
    Trac to. It is not known if anyone used the backdoor that was found.
  </p>
  <p>
    We recommend all users to change their passwords.
    <b>Especially users who use a password on Trac that they also use
      elsewhere, should change that password at least elsewhere.</b>
  </p>


  <h3 id="ffmpeg_rfp">November 12, 2013, FFmpeg RFP in Debian</h3>
  <p>
    Since the splitting of Libav the Debian/Ubuntu maintainers have followed
    the Libav fork. Many people have requested the packaging of ffmpeg in
    Debian, as it is more feature-complete and in many cases less buggy.
  </p>
  <p>
    <a href="http://cynic.cc/blog/">Rogério Brito</a>, a Debian developer,
    has proposed a Request For Package (RFP) in the Debian bug tracking
    system.
  </p>
  <p>
    Please let the Debian and Ubuntu developers know that you support packaging
    of the real FFmpeg! See Debian <a href="http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=729203">ticket #729203</a>
    for more details.
  </p>

  <h3 id="pr2.1">October 28, 2013, FFmpeg 2.1</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.1">2.1</a></b>)
    It contains all features and bugfixes of the git master branch from 28th October.
    A partial list of new stuff is below:
  </p>
  <pre>    - aecho filter
    - perspective filter ported from libmpcodecs
    - ffprobe -show_programs option
    - compand filter
    - RTMP seek support
    - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate
    even when used as an input option. Previous behavior can be restored with
    the -noaccurate_seek option.
    - ffmpeg -t option can now be used for inputs, to limit the duration of
    data read from an input file
    - incomplete Voxware MetaSound decoder
    - read EXIF metadata from JPEG
    - DVB teletext decoder
    - phase filter ported from libmpcodecs
    - w3fdif filter
    - Opus support in Matroska
    - FFV1 version 1.3 is stable and no longer experimental
    - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support
    - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be
    more consistent with other muxers.
    - adelay filter
    - pullup filter ported from libmpcodecs
    - ffprobe -read_intervals option
    - Lossless and alpha support for WebP decoder
    - Error Resilient AAC syntax (ER AAC LC) decoding
    - Low Delay AAC (ER AAC LD) decoding
    - mux chapters in ASF files
    - SFTP protocol (via libssh)
    - libx264: add ability to encode in YUVJ422P and YUVJ444P
    - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does
    - make decoding alpha optional for prores, ffv1 and vp6 by setting
    the skip_alpha flag.
    - ladspa wrapper filter
    - native VP9 decoder
    - dpx parser
    - max_error_rate parameter in ffmpeg
    - PulseAudio output device
    - ReplayGain scanner
    - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support)
    - Linux framebuffer output device
    - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4
    - mergeplanes filter
  </pre>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing (121 pts)]]></title>
            <link>https://arxiv.org/abs/2508.12631</link>
            <guid>44985278</guid>
            <pubDate>Fri, 22 Aug 2025 14:43:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2508.12631">https://arxiv.org/abs/2508.12631</a>, See on <a href="https://news.ycombinator.com/item?id=44985278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2508.12631">View PDF</a>
    <a href="https://arxiv.org/html/2508.12631v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Balancing performance and efficiency is a central challenge in large language model (LLM) advancement. GPT-5 addresses this with test-time routing, dynamically assigning queries to either an efficient or a high-capacity model during inference. In this work, we present Avengers-Pro, a test-time routing framework that ensembles LLMs of varying capacities and efficiencies, providing a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro embeds and clusters incoming queries, then routes each to the most suitable model based on a performance-efficiency score. Across 6 challenging benchmarks and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a performance-efficiency trade-off parameter, it can surpass the strongest single model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the average accuracy of the strongest single model at 27% lower cost, and reach ~90% of that performance at 63% lower cost. Last but not least, it achieves a Pareto frontier, consistently yielding the highest accuracy for any given cost, and the lowest cost for any given accuracy, among all single models. Code is available at <a href="https://github.com/ZhangYiqun018/AvengersPro" rel="external noopener nofollow">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yiqun Zhang [<a href="https://arxiv.org/show-email/6ce58c18/2508.12631" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 18 Aug 2025 05:23:31 UTC (293 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A guide to Gen AI / LLM vibecoding for expert programmers (120 pts)]]></title>
            <link>https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/</link>
            <guid>44985207</guid>
            <pubDate>Fri, 22 Aug 2025 14:37:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/">https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/</a>, See on <a href="https://news.ycombinator.com/item?id=44985207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-1877">
                <p>I get it, you’re too good to vibe code. You’re a senior developer who has been doing this for 20 years and knows the system like the back of your hand. Or maybe you’re the star individual contributor who is the only person who can ever figure out how to solve the hard problems. Or maybe you’re the professor who created the entire subject of the algorithms you’re implementing. I don’t know you, but I do know that you think you’re too good to vibe code. And guess what, you’re absolutely and totally wrong.</p>
<p>Facetious? Maybe… but I will go even further.</p>
<p>No, you’re not too good to vibe code. In fact, you’re the only person who should be vibe coding.</p>
<p>I would have thought this statement was crazy just a month ago because this label of “expert” coder also applies to me. Just to establish some street cred here, <a href="https://github.com/ChrisRackauckas">I am the maintainer of over 200 Github packages, totaling over 23,000 stars</a>, am <a href="https://julia.mit.edu/">Co-PI of a CS Lab at MIT</a>, was the <a href="https://pumas.ai/">founding architect at one pretty successful tech startup</a> and am VP leading <a href="https://juliahub.com/products/dyad">architecting Dyad in another domain</a>. I clearly know how to program, would leave snide remarks at students and interns when their code was clearly created by LLMs, and was pretty publicly against all of this because these bots are too stupid to know what “correct” even means. </p>
<p>But I started picking up this “vibe coding” about a month ago and I found out that it can be a really powerful tool, in the right circumstances and in the right workflow. For the record, I now have about 32 Claude agents continuously running in tmux windows that I can ssh to, so all day long I can just check via laptop or phone and keep plugging along. This was completely unheard of a month ago, but it’s here.</p>
<p>This is the expert’s guide to vibe coding for those who are scoffing at those kids who don’t know what they are doing, but also want to start doing it correctly.</p>
<h2>A Mental Model for LLM Agents: Your Sophomore Year Student/Intern</h2>
<p>Drop the hype, I’m not here to sell you a ChatGPT so I’m not going to tell you it’s PhD level when it 100% absolutely clearly isn’t to anyone who has ever met a PhD in their life. But it is something, what is it?</p>
<p>Think about an LLM agent as a dedicated intern, or a student who is around the proficiency of a sophomore in college. They know the basics of what programming looks like, they can copy other ideas and architectures, they know how to do things like run unit tests, and they know how to Google things. They have had their basic programming course, and probably have done a deep dive into some random subject as a higher level course, but if you quiz them on the topic enough you’ll learn they haven’t actually learned it deeply. The kid seems smart enough, you’d give them a shot.</p>
<p>If this was a person who showed up to your office looking for work, what would you do with them? Generally you would do one of two things. First, you could sandbox their work. Now the reason you sandbox the work of a new student or intern is rather simple: it’s because you don’t know a new subject/tool well, you want to give it a try, and ehh why not let’s see what happens. If you took the sandbox route, you probably aren’t caring about the code (it’s likely to be unmaintainable and bit rot anyways if it’s not in your core repos), it’s about getting the artifact. You vibe code a bit, “that looks cool!”, throw it into a demo / LinkedIn post, and then move on. That’s the simple vibe coding you may have already tried and thought “that’s not useful enough”. It works, but that’s not what we’re here for, so no need to mention that more in the blog.</p>
<p>The second path, in the complete opposite direction, you would integrate the student/intern into a project you know well because it makes it very easy for you to review their work: you can give clear feedback because you’ve already made the first 10 mistakes they will make, you already know how to tell them what is next, and you have the first 6 months of the project planned out for them so it’s low maintenance. This is how you would train most people that you want to stay long term, right? In the same way, this is the path to take with the LLM agents.</p>
<p>So let’s walk through this process step by step.</p>
<h3>Major Point: Vibe coding turns everyone into a team lead, but not everyone should be a team lead</h3>
<p>Leading a team of programmers is hard! It takes time, skill, and patience. I think everyone when they are a kid thinks “I don’t want to be the worker, I want to be the boss and I just sit in my chair and tell people what to do and boom it all gets done!”. But after your second group project in college, you pretty quickly realize that if you lead a team wrong, you instead just end up doing all of the work yourself while having the expectation of 4 people. </p>
<p>Now there’s a few reasons for this. One issue with trying to establish team programming is that you may just not know the subject well enough. If it takes you a while to understand the subject, what someone is trying to do, and what their code is about, then it’s just not worth the time to manage someone else. You need to be at a point where you can very quickly see the code, understand what’s going on, and say “I can’t merge this until you have tests for X and Y, and also show me a plot of Z so I know how it all relates”. If you cannot instantly see that kind of feedback, then you probably aren’t experienced enough to lead. You need to write a few million lines of code before it becomes automatic where you just look at code and say “don’t do that, that’ll be a performance bottleneck”, but you need that quickness to the code review in order for vibe coding to work. But also, let me say this bluntly:</p>
<h4>If you are an individual contributor who usually does not like to train interns because you find that they take more time than they are helpful, then don’t vibe code</h4>
<p>Some people tend to do better at working by directing. Other really smart people just can’t seem to get good work out of other people. It’s not an indictment, Silicon Valley created the “individual contributor” role for a reason. If you are one of those people, then vibe coding may not be for you as you will likely grow frustrated with the agents even quicker than you would a human (they somehow retain less information than even the worst intern, at least they remember your favorite lunch order).</p>
<p>So go in with this mindset: I will have to have meetings with the agents, I will need to plan and give them tasks, and I will need to review the code. If I find this stuff to be slower than coding myself, then just stop right now. But if you do well with a team, then go on. How do we now make this team effective?</p>
<h2>What is the workflow of vibe coding correctly?</h2>
<p>If someone shows you Claude Code and you ask it to try and solve the problem that you were just working on (obviously a hard problem, because if it ends up on your desk that means someone else failed to solve it), you just poke and laugh at Claude when it fails miserably. But you would have never done that with a new intern or student (hopefully), so why do this? Again, you know how smart it actually is, cut the hype, and treat it the same way. This immediately leads to a few workflow principles:</p>
<h3>The workflow of vibe coding is the same as managing scrums or helping a student through a research thesis</h3>
<p>You have a problem, you give it to the agent, you review the results, and then you give it feedback. This is exactly how you would manage a scrum team or help a student through their thesis. You don’t just give them the problem and expect them to solve it, you give them the problem, they come back with a solution, and then you tell them what to do next.</p>
<p>You probably already get most of your work done this way if you’re senior enough. Every professor has more students coding than themselves, and every senior developer has a total amount of code created by their team that is far greater than their own. Just think of Claude as your pack of newbies that just started. Now if you’re thinking “but it can be difficult to manage a bunch of newbies”… yeah, that means you’re actually senior enough to understand how to do this right. It’s fairly easy to send a new intern or student off on a project, and if their pay/grade depends on it getting done they will give you something back. Whether it’s any good depends on how well you chunked up the work for them and gave them an appropriate task. </p>
<p>But one key thing is, if you had to do a meeting every 10 minutes it would drive you crazy, so don’t. Set up say 12-32 agents running on different processes, preferably sandboxed on some other compute resource (sandboxed so they can’t break the machine, but also so they can have a Github authentication that does not have core read/write privileges. This way you can tell it to have “dangerously unsafe permissions” and the worst that happens is it segfaults its own docker container and never opens a PR). Give it a full command:</p>
<p>“try solving (an easy issue in this open source repository). Create a PR with the solution, and after an hour check the continuous integration to see if tests are passing. If tests are not passing, assess what the issue is, and if it is a quick fix make a commit to handle it, otherwise report what the core difficulty of the problem is”</p>
<h4>Don’t spend too much time setting up the calls, just pull from lists you already have and let it find “whatever is easy”</h4>
<p>Make it clear, make it easy, make it know the steps, and let it just keep cycling for a bit.</p>
<h3>How to review vibe coding: immediately throw out anything bad</h3>
<p>If you saw a student was cheating and just copy-pasted from StackOverflow but couldn’t explain what it did, you’d throw it out and tell them to try again. If your new intern didn’t reuse all of the solid code your team had written and instead rewrote some low level detail in a buggy and unmaintainable way, you’d throw it out and tell them to try again. If they wrote a function that was 500 lines long and did 10 different things, you’d throw it out and tell them to try again. You wouldn’t waste your time trying to fix it, you’d just tell them to try again.</p>
<p>Again, treat the LLMs the same way. I see a lot of people following the mindset they see the vibe coding YouTubers making their silly games. “ChatGPT, try harder! Fix for me!”. You want to know a secret? That stuff is worse than worthless. The problem is that these LLMs are made to please you, so if you tell them to try harder, they will either start hallucinating or just start changing your tests. Don’t even give it a try. The moment you see it go off the rails, just throw it out. That problem is too hard for Claude, it’s for you now.</p>
<p>Send a bunch of commands at 9am. At noon, check on them. You might have 10 done. 8 of them probably went off the rails, whatever, fire them. Hey two PRs worked, whoopee! Fire 10 more, come back at 3. 20 done, 4 successes and 16 failures. Fire a few more off, maybe a few clean up ones to look for missing docstrings or dig around to see if any performance regressions were introduced. At 6, see the other 4 successes and cut the other jobs. </p>
<h4>Vibe coding is useful only if you have enough problems that you’re happy that some subset being solved, not caring what in that subset is solved.</h4>
<p>10 PRs were merged, plus whatever you were working on that day (yes, because you didn’t focus on this for most of your day!). You might think, that’s like 10/40 = 25% success rate, that’s not good. But you know what? Those were free. You just got a lot of extra stuff done that you wouldn’t have otherwise. The success rate is just a matter of how much these things give value for their cost. That’s for Sam Altman to worry about. But if you have a subscription to these LLMs, just keep burning through the tokens who cares. Don’t worry about success rate, just go for total successes.</p>
<h3>Where to apply vibe coding: code you know very well</h3>
<p>So this leads to a very counter-intuitive fact that may come out of left field, but I’m serious. Everyone’s first inclination is to throw it on some project they haven’t actually contributed to and get banned (okay, maybe it just looks like that to open source maintainers). But the real issue is that, the majority of your time will be spent doing code review. If you do this on code you don’t know well, you will have to spend a lot of time trying to understand the code and at that point, why not just write the code yourself?</p>
<p>This is where most people seem to just stop and drop the idea of vibe coding all together. But instead… what about applying it to the code base you’re on? No, not on the hard problems you’re thinking about, but all of those little side problems? The small refactor you put off for the last 6 months? What about bisecting the Git commits to find the exact cause of the performance regression that showed up on master a week ago? Or you created a version specialized for Windows and Mac but left a “todo” over the Linux section because it’s easy but would be 4 hours of monotonous work? All of those things, if someone showed up with the code, you could review it in about 5 minutes and know whether it’s right or wrong. Give the agents that stuff!</p>
<h4>Vibe coding is not useful if you need it to solve a particular problem. You still do the hard stuff</h4>
<p>In just the same way, the best place to put trainees is in the project that you already know well because that makes it easy to review their work. It’s the “I don’t have time for you, so try this easy task” approach. You know the code, you know the problem, and you can give them a task that is easy enough that they can do it without too much help. This is the same principle here.</p>
<h2>Some Examples of Vibe Coding PRs</h2>
<p>Now let’s look at some of the examples <a href="https://github.com/ChrisRackauckas-claude">my bot account has been putting out</a>.</p>
<h3>Example 1: The Simple Success Story</h3>
<p><a href="https://github.com/SciML/OrdinaryDiffEq.jl/pull/2856">Here’s a quick and simple PR</a>, the kind that is perfect here. If you don’t know performance Julia handling or trim, basically it’s a new feature in Julia v1.12 where Julia can now build <a href="https://www.youtube.com/watch?v=R0DEG-ddBZA">small lean binaries</a>. In order to do that, you need to make sure functions fully specialize, which they don’t by default as that would create a lot of extra compilation in many circumstances, but for higher order numerical solvers that is the behavior we want. So I told it to go specialize all instances of the function in the package, and I could check the PR fairly quickly and see it stuck to the goals and did it. This is then going to be followed up with new tooling that will perform static checks of trimming compatibility (<a href="https://github.com/SciML/NonlinearSolve.jl/pull/665">still being worked out</a>), but with just those backwards compatible minor changes things seem to work in the beta, so merge now and add those tests when we have a good system for it.</p>
<p>1 minute to write the query, come back later and 1 minute to review.</p>
<p>This is exactly the kind of small targeted change these are geared towards. Most of the PRs aim to be like this.</p>
<h4>Even if you work on hard stuff, a huge chunk of your work isn’t hard stuff. There’s a lot of simple janitorial work you have to do on your code all of the time. Automate that part.</h4>
<h3>Example 2: The Immediately Closed “That’s not for Claude” PR</h3>
<p><a href="https://github.com/SciML/SciMLSensitivity.jl/pull/1266">This PR</a> came from pointing it at the fact that every once in awhile I get a test failure in the docs build for a chaotic ODE differentiation w.r.t. ergodic properties tutorial. <a href="https://frankschae.github.io/post/shadowing/">It is a very fun topic</a>, but generally anything with real math in it is too hard for the LLMs. And in this, yeah I could see immediately that this PR does not make sense… well it did. The NaN’s and Infs were definitely coming from a numerical issue in the least squares shadowing code, and what this pointed to was the Schur complement was being done with things like B * Diagonal(wBinv) * B’ which as a numerical analyst I can immediately see would double the condition number of the matrix, but there doesn’t seem to be an immediate solution with open source linear algebra things I could find. So closed this, sent a note over to <a href="https://math.mit.edu/directory/profile.html?pid=63">Alan Edelman</a> to try and figure out what the better way to do this factorization. While it didn’t solve the problem, at least I know what the problem is now.</p>
<p>This is probably what most of the PRs become. It gives a hint of where the problem is, and then I take the reins.</p>
<h3>Example 3: Repeated Refactors</h3>
<p><a href="https://github.com/SciML/NonlinearSolve.jl/pull/672">Is a sweet and simple PR that refactors the tests to move some things</a>, specifically the Enzyme automatic differentiation engine testing, to a “no pre” set. The “no pre” means “does not run on prereleases of the next language version”, since these tools touch language internals in the compiler so they are never ready early. This always make prerelease tests fail before they actually test anything meaningful, so I wanted to move all Enzyme usage to a “no pre” set in every repo it showed up. </p>
<p>About 5 minutes to write the query. Some of the test suites needed a simple Github suggestion to fix up a little detail here or there. About 5 minutes to get this thing into 8 repos. Now I was ready to start using prerelease tests. Would’ve been at least a half hour by hand just because we didn’t have an easy system for doing this before. Maybe that’s a little dirtier than the perfect regex, but whatever 10 minutes of my time sounds like a win.</p>
<p>Refactors generally work out really well and are one of the top uses for the tool. “make it correct, write good tests, and let it refactor” is generally a lazy way to get 90% of the way there.</p>
<h3>Example 4: The Information Gathering PR</h3>
<p><a href="https://github.com/SciML/LinearSolve.jl/pull/734">Here is a pull request</a> that was generated by pointing it to solve <a href="https://github.com/SciML/LinearSolve.jl/issues/593">this issue</a>. That issue was mostly chosen because it was sitting on the issue list for awhile and it didn’t seem so difficult but I hadn’t had the time to track down the memory leak in a not so widely used extension for an alternative C-based sparse matrix solver, but it needed to get done some time. So, throw the bot on it.</p>
<p>And what it comes back with is to add a memory finalizer (i.e. how to tell the GC how to remove the memory) for the other library. I could take one look at it and immediately see that kind of code should not live in this library, it should live in the library where the solver is bound to the language, and the fact that it was missing a finalizer is something that should be solved over there. Close the PR, throw out the code, find the <a href="https://github.com/JuliaSparse/Pardiso.jl/pull/117"> stalled discussion on the repo that should have the finalizer</a>, poke the author a bit, and it’s in. Done, someone just needed to be reminded.</p>
<p>Total time on my end was about 3 minutes. The bot could have also written that fix but it basically already existed so no need, this was more about finding out where in the system something was offer.</p>
<h3>Example 5: The “How Long is that Going to Take?” PR</h3>
<p><a href="https://github.com/SciML/Catalyst.jl/pull/1317">Here is a nice PR where it didn’t finish</a> (at least at the time of writing this) and the reason is because there are lots of other clean ups that need to happen for this to ever work. How far away is it? Well it generated a set of tests that cleanly listed out all 120 things to solve. Great, this is probably a full week’s task… I knew it would be a lot but that is now pretty concrete. I probably won’t use the bot to finish this one, but now if someone asked what the effort would be I can give them a pretty clear estimate because it has been reduced from “someone needs to give it a try, seems like a good chunk of work” to “the hard part is making these 120 things happen, which is easy but tedious and it would take about a week, probably not worth the effort right now”. That’s very useful when planning ahead. Total me time was about 5 minutes, plus the PR discussion time to explain to others what the results meant.</p>
<h2>Conclusion: Vibe Coding Done Right is actually an Expert’s Task</h2>
<p>Vibe coding turns any individual into the CTO leading a team of 20, 30, 50, 60 interns. You immediately get a massive team. It takes time and experience to actually handle a group like this correctly and to make it be productive. Making all of 60 interns not break the performance, correctness, or maintainability of your code is very difficult. So I do not recommend this to people who are still “up and coming programmers”. But if you’re a bit more senior and starting to grow your group, well this is then a cheap way to accelerate that. </p>
<p>What that means is, vibe coding is sold for people who don’t know how to program, but if you actually think about it, the main audience that can actually use it correctly is experts.</p>
<h3>A few side remarks I didn’t get to</h3>
<h4>A bunch more examples and how bad Claude is at math</h4>
<p>In the <a href="https://discourse.julialang.org/t/the-use-of-claude-code-in-sciml-repos/131009/8?u=chrisrackauckas">Julia Discourse forum</a>, I detail a bunch of different PRs to show where the AIs tend to succeed and fail. But generally anything that is mostly about “programming” (refactoring, inefficient implementations, etc.) the LLMs do well. Anything about the domain or application (differential equations, engineering, physics for me) it just seems to flat out do something dumb. The results all lined up very clearly shows what kind of PRs you should be asking it to make and which ones just aren’t worth the effort.</p>
<h4>The role of empathy in vibe coding success</h4>
<p>Some of the least empathetic people I know in open source are the ones who are also the most skeptical of vibe coding. I have a heavy speculation that they speak to the agents similarly to how they speak to other potential contributors, and drive the bots away the same way they do to people. But with a bot, it will always try to make you happy, just by hallucinating and commenting out your tests. These same people also don’t want the bots around because they claim that’s all the bots ever do. Weird coincidence. I wonder what this will do to the culture of programming over time.</p>
<h4>The cost may not make sense in the long run, but it does while the VCs are paying for it</h4>
<p>On my \$200/month Claude 20x Max subscription I used enough tokens for about $5,200 of compute in the first month. This is obviously not sustainable, but hey, it’s a startup world and VCs are paying for it right now. If you can get a few extra features done that get you more funding, then this is worth it. If you’re a professor and you can get a few more papers out, then this is worth it. If you’re an individual contributor at a big company and you can get a few more features out that make your team look good, then this is worth it. Will it be worth it after the money runs out? Who knows, but mine while the gold is there.</p>
<h4>What’s the right setup? Easy, Claude Code</h4>
<p>The tab-complete stuff is pretty annoying. The power comes from running agents. Claude Code has a simple setup and is able to start running code. Just write a decent Claude.md that tells it to stop being so nice and instead just tell me when it cannot solve the problem, and you’re good to go. The <a href="https://github.com/upstash/context7">context7 MCP is good</a>, <a href="https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking">Sequential Thinking as well</a>. </p>
<h4>“This sounds like hell?”: A response to seeing what vibe coding is like</h4>
<p>Hey Hacker News! Looks like it got there. One of the interesting comments is “To me, someone who actually love programming, it makes vibe coding look like hell.” I 100% agree! Notice from the examples that the amount of time I’m spending on this I try to keep as minimal as possible. I like to program, and I need to be programming because I can do the hard stuff while the LLM can’t. The goal is to get as much easy stuff done with as minimal work on your end as possible, so you can stop worrying about the annoying/boring stuff and can focus more time on the interesting work.</p>
<p>But then again, if you also just really dislike having to do meetings at all and prefer to just be coding alone, then… yeah the agents will probably drive you mad. But with the amount of code coming from LLMs I think there will be an even greater need for good individual contributors to yell at the clouds and maintain the integrity of the codebase so, you’ll have your place. You just might have a larger load of PRs coming your way and may want to be quicker to shut a few down.</p>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thunderbird Pro August 2025 Update (159 pts)]]></title>
            <link>https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/</link>
            <guid>44985131</guid>
            <pubDate>Fri, 22 Aug 2025 14:29:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/">https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/</a>, See on <a href="https://news.ycombinator.com/item?id=44985131">Hacker News</a></p>
Couldn't get https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/: Error: Request failed with status code 504]]></description>
        </item>
        <item>
            <title><![CDATA[All managers make mistakes; good managers acknowledge and repair (277 pts)]]></title>
            <link>https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/</link>
            <guid>44983986</guid>
            <pubDate>Fri, 22 Aug 2025 12:50:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/">https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/</a>, See on <a href="https://news.ycombinator.com/item?id=44983986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<p><em>“There is a crack in everything. That’s how the light gets in.”</em> — Leonard Cohen</p>
</blockquote>



<p>Let me tell you something that will happen after you become a manager: you’re going to mess up. A lot. You’ll give feedback that lands wrong and crushes someone’s confidence. You’ll make a decision that seems logical but turns out to be completely misguided. You’ll forget that important thing you promised to do for someone on your team. You’ll lose your temper in a meeting when you should have stayed calm.</p>



<p>The real question isn’t whether you’ll make mistakes; it’s what you do <em>after</em>.</p>



<p>I recently read&nbsp;<em><a href="https://www.amazon.com/Good-Inside-Guide-Becoming-Parent/dp/B09PGMSBBN">“Good Inside”</a></em>&nbsp;by Dr. Becky Kennedy, a parenting book that completely changed how I think about this. She talks about how the most important parenting skill isn’t being perfect — it’s&nbsp;<strong>repair</strong>. When you inevitably lose your patience with your kid or handle something poorly, what matters most is going back and fixing it. <strong>Acknowledging what happened, taking responsibility, and reconnecting.</strong></p>



<p>Sound familiar? Because that’s what good management is about too.</p>



<p>Think about the worst manager you ever had. I bet they weren’t necessarily the ones who made the most mistakes. But they were probably the ones who never acknowledged them. Who doubled down when they were wrong. Who let their ego prevent them from admitting they didn’t have all the answers.</p>



<p>Here’s a pattern I see play out constantly: A manager commits to something without consulting the team. Maybe it’s a feature at a client demo, a timeline in a board meeting, or just a “small favor” for another department. The team scrambles to deliver, working nights and weekends. They make it happen, but barely, and with real costs: technical debt, burned-out engineers, resentment building.</p>



<p>What happens next determines everything. The manager who never acknowledges what they put the team through? That’s how you lose your best people. But the manager who comes back and says, <em>“I put you in an impossible position. I should have consulted you first. I’m sorry for the stress that caused, and here’s how I’ll handle it differently next time”</em>, that manager builds trust even through the mistake.</p>



<p>I’ve been on both sides of this. As an engineer, I watched managers make the same mistakes over and over again, never acknowledging the chaos they created. As a manager, I’ve been the one creating that chaos 🥲. The difference in outcomes is massive; when you own your mistakes completely and specifically, something unexpected happens: your team trusts you more, not less.</p>



<p>Here’s what repair looks like in practice:</p>



<ol>
<li><strong>Be specific about what you did wrong.</strong>&nbsp;Not “mistakes were made” or “things could have gone better.” But “I interrupted you three times in that meeting and dismissed your concerns. That was wrong.”</li>



<li><strong>Don’t make it about you.</strong>&nbsp;This isn’t the time for a long explanation of your stress levels or why you acted that way. Save that for your therapist or your own manager. The repair is about acknowledging the impact on the other person.</li>



<li><strong>Actually change the behavior.</strong>&nbsp;An apology without changed behavior is just empty words. If you keep making the same “mistake,” it’s not a mistake anymore; it’s a choice.</li>



<li><strong>Give it time.</strong>&nbsp;One conversation doesn’t instantly repair broken trust. It’s a starting point, not a finish line. You have to consistently show up differently.</li>
</ol>



<p><strong>The beautiful thing about getting comfortable with repair is that it actually makes you better as a manager</strong>. When you know you can fix things when they go wrong, you’re more willing to make decisions, have difficult conversations, and take reasonable risks. You stop being paralyzed by perfectionism because you know that most mistakes, while serious, create opportunities for growth and stronger relationships when handled well.</p>



<p>This doesn’t mean being reckless or careless. It doesn’t mean making the same mistakes repeatedly. And it definitely doesn’t mean using repair as a get-out-of-jail-free card for being a shitty manager.</p>



<p>What it means is accepting that you’re human, that management is complex, and that you won’t always get it right. Your job isn’t to be perfect. Your job is to&nbsp;<a href="https://terriblesoftware.org/2025/06/13/good-engineer-bad-engineer/">ship working software that adds real value to users</a>, to help your team grow, and to create an environment where people can do their best work. </p>



<p>Sometimes you’ll fail at those things. When you do, you repair, you learn, and you keep going.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Being “Confidently Wrong” is holding AI back (148 pts)]]></title>
            <link>https://promptql.io/blog/being-confidently-wrong-is-holding-ai-back</link>
            <guid>44983570</guid>
            <pubDate>Fri, 22 Aug 2025 12:14:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://promptql.io/blog/being-confidently-wrong-is-holding-ai-back">https://promptql.io/blog/being-confidently-wrong-is-holding-ai-back</a>, See on <a href="https://news.ycombinator.com/item?id=44983570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-9.54.45-PM.png" alt="" loading="lazy" width="1397" height="464" srcset="https://hasura.io/blog/content/images/size/w600/2025/08/Screenshot-2025-08-20-at-9.54.45-PM.png 600w, https://hasura.io/blog/content/images/size/w1000/2025/08/Screenshot-2025-08-20-at-9.54.45-PM.png 1000w, https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-9.54.45-PM.png 1397w" sizes="(min-width: 720px) 720px"></figure><blockquote>The reason humans are so useful is not mainly their raw intelligence. It’s their ability to build up context, interrogate their own failures, and pick up small improvements and efficiencies as they practice a task<p>- "Why I don't think AGI is right around the corner", <a href="https://www.dwarkesh.com/p/timelines-june-2025">Dwarkesh Patel</a></p></blockquote><p>In this post, based on our recent experiences selling 7-figure AI deals to Fortune 500s and Silicon Valley tech cos alike, &nbsp;I'll discuss how "confident inaccuracy" seems to be at the heart of this problem.</p><!--kg-card-begin: markdown--><ul>
<li><a href="#being-wrong">Confidently Wrong is the problem</a>
<ul>
<li><a href="#verification-tax">The Verification Tax that nukes ROI</a></li>
<li><a href="#trust-erosion">Asymetric erosion of trust that decreases adoption</a></li>
<li><a href="#prevents-improvements">Prevents improvements that kills AI motivation</a></li>
<li><a href="#compounding-errors">90% accuracy is 2 errors in 3!</a></li>
</ul>
</li>
<li><a href="#fixing-wrong">Tentatively Right is the solution</a>
<ul>
<li><a href="#flywheel">An Accuracy Flywheel</a></li>
<li><a href="#ai-adoption">A Path to increasing AI adoption vs Obsoloscence</a></li>
</ul>
</li>
</ul>
<!--kg-card-end: markdown--><hr><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-19-at-11.48.17-PM.png" alt="" loading="lazy" width="1338" height="805" srcset="https://hasura.io/blog/content/images/size/w600/2025/08/Screenshot-2025-08-19-at-11.48.17-PM.png 600w, https://hasura.io/blog/content/images/size/w1000/2025/08/Screenshot-2025-08-19-at-11.48.17-PM.png 1000w, https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-19-at-11.48.17-PM.png 1338w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: html--><!--kg-card-end: html--><p><h2 id="being-confidently-wrong-is-the-only-problem">Being Confidently Wrong is The Only Problem</h2></p><p>Aside from the hilarious "Oh I spend $10M on this campaign because our AI assistant told me to" first order problem, confident inaccuracy causes second and third order problems that are far more insidious:</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="a-imposes-a-universal-verification-tax">a) Imposes a universal verification tax</h3><p>I don't know when I might get an incorrect response from my AI. So I have to forensically check every response. </p><p>My minutes turn into hours; the ROI disappears.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="b-erodes-trust-asymmetrically">b) Erodes trust asymmetrically</h3><p>For serious work, one high‑confidence miss costs more credibility than ten successes earn. &nbsp;</p><p>I'll revert to my older flow.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="c-hidden-failure-modes-kill-motivation-to-improve">c) Hidden failure modes kill motivation to improve</h3><p>Without high-quality uncertainty information, I don’t know whether a result is wrong because of ambiguity, missing context, stale data, or a model mistake.</p><p>If I don't know why its wrong, I'm not invested in making it successful.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="d-compounding-errors-results-in-ai-being-doomed-to-fail">d) Compounding errors results in AI being doomed to fail</h3><p>There's been a slew of recent reports on AI adoption is not trending well. </p><p>Per <a href="https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage">McKinsey's report</a> 90% of AI initiatives stay stuck in pilot mode. <br><a href="https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/">Fortune covered</a> an MIT study that claims 95% of pilots are failing. </p><p>While the authors of the reports point out several issues, consider another perspective - at the core there are 2 simple facts:</p><ol><li>A system is either always accurate or its not</li><li>If the system is not always accurate even the tiniest percent of the time, I need to know when its not. </li></ol><p>No amount of solving any other problem (integration, data readiness, organizational readiness etc) will change the fact that AI's tendency to be confidently wrong keeps it out of real world use-cases. </p><p>Accuracy is not like Uptime.</p><ul><li>99.99% uptime is ~53 minutes a year. </li><li>99.99% accuracy in a ten step workflow is 1 error in a 1000 runs.</li><li>90% accuracy in a ten step workflow is <strong>2 in every 3 workflows have errors</strong> (1 - 0.9^10).</li></ul><hr><!--kg-card-begin: html--><!--kg-card-end: html--><p><h2 id="2-fixing-confidently-wrong-might-be-a-silver-bullet%E2%84%A2">2. Fixing "confidently wrong" might be A Silver Bullet<strong>™</strong></h2></p><p>The irony here is that perfect accuracy is not required to have a usable AI system. </p><p>As humans if we're working with systems that will never be fully accurate, then more valuable than, say, a 90% accurate system is, say, a 50% accurate system that can signal uncertainty - and get more accurate over time.</p><p><strong>We don’t need perfection; we need a loop that tightens.</strong></p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="21-an-accuracy-flywheel">2.1 An Accuracy Flywheel </h3><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png" alt="" loading="lazy" width="1797" height="899" srcset="https://hasura.io/blog/content/images/size/w600/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png 600w, https://hasura.io/blog/content/images/size/w1000/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png 1000w, https://hasura.io/blog/content/images/size/w1600/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png 1600w, https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png 1797w" sizes="(min-width: 720px) 720px"></figure><p>The starting point of this loop is if an AI system could tell the user when its not certain about its accuracy in a concrete and native way.</p><ol><li><strong>Native uncertainty</strong> → it signals confidence and the top uncertainty causes; abstains below threshold.</li><li><strong>Human nudge</strong> → the user fills a planning gap that was causing uncertainty</li><li><strong>Model improvement</strong> → that nudge updates the domain knowledge and the planning space (not just the answer) and accuracy improves.</li></ol><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-11.53.44-PM.png" alt="" loading="lazy" width="520" height="823"></figure><div><p>In practice, the sources of inaccuracy are far more challenging, not just missing definitions of terms that need to be remembered. </p><p>Data is messy. The quality is unknown. Data is stale - especially unstructured data that lacks annotations. Procedural semantics are in people's heads. The list is endless.</p></div><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="a-path-to-increasing-ai-adoption-vs-obsoloscence">A path to increasing AI Adoption vs Obsoloscence</h3><p>If our AI systems can tell us that they're not sure and why, then we can start to help it become better.</p><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png" alt="" loading="lazy" width="1697" height="877" srcset="https://hasura.io/blog/content/images/size/w600/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png 600w, https://hasura.io/blog/content/images/size/w1000/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png 1000w, https://hasura.io/blog/content/images/size/w1600/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png 1600w, https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png 1697w" sizes="(min-width: 720px) 720px"></figure><hr><p><h2 id="a-quick-diagnostic-for-your-ai-investment">A quick diagnostic for your AI investment</h2></p><p>Before you fund another “AI for X” pilot, ask:</p><ul><li><strong>Will it tell me when it’s unsure—and why?</strong> Ambiguity, missing context, data staleness, validation failure etc etc?</li><li><strong>Does it learn from the correction I just gave it?</strong> Will the next user avoid the same trap without re‑prompting?</li></ul><hr><p><h2 id="our-solution-approach">Our solution approach</h2></p><p>Two principles have proven to work really well for us across customers:</p><ul><li><strong>Instead of generating answers, generate plans in a DSL unique to your domain. </strong>Plans in the DSL compile to deterministic actions with runtime validations and policy checks and the DSL should be rich enough to capture the breadth of data/API operations, compute tasks and of course generative &amp; semantic AI tasks. </li><li><strong>Continuously specialize the AI to your domain to drive planning accuracy &amp; planning confidence.</strong> Build a system to continuously bind the AI model to your ontology/metrics, entity catalogs, data systems; learn your naming collisions and edge‑cases; understand your meanings and how your domain works. Most crucially, build a system that can leverage this to carry a calibrated confidence on the generated plan.</li></ul><p>The mechanics are open to debate and require solving hard design &amp; engineering problems - but solving for this takes us closer to a usable enterprise AI system.</p><hr><p>If you lead data / AI initiatives and want to exchange notes feel free to reach out at <a href="https://promptql.io/cdn-cgi/l/email-protection#7e0a1f10131f173e0e0c11130e0a0f12501711"><span data-cfemail="9ce8fdf2f1fdf5dceceef3f1ece8edf0b2f5f3">[email&nbsp;protected]</span></a>.</p></div></div>]]></description>
        </item>
    </channel>
</rss>