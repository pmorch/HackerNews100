<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 22 Mar 2025 22:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA["Vibe Coding" vs. Reality (138 pts)]]></title>
            <link>https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</link>
            <guid>43448432</guid>
            <pubDate>Sat, 22 Mar 2025 20:35:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html">https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</a>, See on <a href="https://news.ycombinator.com/item?id=43448432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body"><article><time datetime="2025-03-19T15:00:00.000Z" title="3/19/2025, 10:00:00 AM">Published Mar 19, 2025</time> - 11 min read - <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.txt">Text Only</a><div id="table-of-contents-container"><p>Table of contents</p><ul><li><a href="#title" data-id="title">"Vibe Coding" vs Reality</a></li><li><a href="#working-around-the-problem" data-id="working-around-the-problem">Working around the problem</a></li><li><a href="#conclusion" data-id="conclusion">Conclusion</a></li></ul></div><p>There's a trend on social media where many repeat <a href="https://x.com/karpathy/status/1886192184808149383">Andrej Karpathy's words</a> (<a href="https://archive.is/yNSTA">archived</a>): "give in to the vibes, embrace exponentials, and forget that the code even exists." This belief — like many flawed takes humanity holds — comes from laziness, inexperience, and self-deluding imagination. It is called "Vibe Coding."</p><div><p><img width="128" height="128" alt="head-empty" src="https://cendyne.dev/s/128/cendyne/head-empty" sizes="128px"></p><div><p>"Embrace the exponentials" sounds like it came from an NFT junkie.</p></div></div><div><p><img width="128" height="128" alt="shinji-cup" src="https://cendyne.dev/s/128/cendyne/shinji-cup" sizes="128px"></p><div><p>Like the NFT crowd, there is a bubble of unreality they cling to justifying their perception of the world.</p></div></div><p>Producing software is now more accessible as newer tools allow people to describe what they want in a natural language to a large language model (LLM). This idea is catching on because LLM agents are now accessible to anyone willing to subscribe to vendors like <a href="https://www.cursor.com/en">Cursor</a>, <a href="https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode">GitHub</a>, Windsurf, and others. These editors have an "agent" option where users can request something and in response changes are made to the appropriate files, rather than only the file currently in focus. Over time, the agent will request to run commands to run tests or even run scripts it previously wrote to the file system, much as you would if you were solving the problem.</p><p>In 2022, folks could copy code into <a href="https://en.wikipedia.org/wiki/ChatGPT">ChatGPT</a> and ask questions or for rewrites.</p><p>In 2023, folks could ask it to review and edit a single file with an IDE integration like Copilot.</p><p>In 2024 and 2025, folks could ask it to solve a specific problem in the project and have it find out what files to edit, edit them, then verify its own work, and correct any mistakes it made with feedback from linting errors and unit tests.</p><p>With LLM agents having so much capability, people can delegate the idea of refining their imprecise ideas to a precise implementation elaborated by an LLM through "Vibe Coding."</p><div><p><a href="https://twitter.com/a16z">@a16z</a> <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a> First - what is vibe coding?</p><p>A concise definition from <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a>, and then an exploration of how technical vs. non-technical users approach these tools.</p></div><p>If you open a blank folder and tell it to set up an initial project, it can do a lot at once. With no rules, no patterns to mimic, and no constraints, it can produce something that feels more tailored for you in minutes than <code>npx create-react-app</code> ever could.</p><p>With a simple instruction like "I want to create a website for my ski resort" and about ten minutes of having it massage errors of its own making, I can have just that.</p><p><img data-blurhash="MSQc#U~WxtIW%LM|t6t7WCt7^*9Zt7%LR*" data-width="645" data-height="423" data-ratio="true" src="https://cendyne.dev/c/XH7rgh3H?width=645" alt="A generated website about a ski resort with a phrase like 'Easy to Reach, Hard to Leave'" width="645" height="423"></p><p>These leaps of progress are what fuels the "Vibe Coding" idea. To go from nothing to something shareable and personal sounds incredible.</p><div><p><img width="128" height="128" alt="beat-saber" src="https://cendyne.dev/s/128/cendyne/beat-saber" sizes="128px"></p><div><p>This moment provided a thrill I hadn't experienced in a long time when coding. However, this excitement drained quickly the further I got from a blank canvas.</p></div></div><p>Agents, as a concept, aren’t new. <a href="https://www.youtube.com/watch?v=ijeXop674Dg">Google IO made up buzzwords</a> like <a href="https://www.theverge.com/2024/12/11/24317436/google-deepmind-project-astra-mariner-ai-agent">"agentic era"</a> (<a href="https://archive.is/dw8XE">archived</a>) to describe this concept. It has been realized through open technologies like <a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>, <a href="https://github.com/OpenBMB/XAgent">XAgent</a>, and <a href="https://www.anthropic.com/news/model-context-protocol">more recently by Anthropic</a> with the <a href="https://modelcontextprotocol.io/introduction">Model Context Protocol</a> (MCP).</p><p>When the model can interact with more than just a person who proxies their outputs into different domains, it is autonomous. If it can perform searches on the web or in a codebase, it can enrich its own context with the information it needs to fulfill the current request. Further, when it can commit outputs and then gain immediate and automatic feedback on those outputs, it can refine its solution without a person intervening.</p><p>There are actions that do prompt the user for consent before proceeding, such as running commands in the console or deleting files. This consent can be pre approved with a mode called "YOLO."</p><p><img data-blurhash="E04ec*~qofxuoft7D%Rj?b-;xut7" data-width="645" data-height="149" data-ratio="true" src="https://cendyne.dev/c/E8IJPCCd?width=645" alt="Cursor settings YOLO mode, allows running commands automatically" width="645" height="149"></p><div><p><img width="128" height="127" alt="we-live-in-a-society" src="https://cendyne.dev/s/128/cendyne/we-live-in-a-society" sizes="128px"></p><div><p>A mode for "You Only Live Once"!? Really?</p></div></div><p>You can witness this autonomy for yourself today in Cursor.</p><p>The agent concept has merit and today can deliver proofs of concept that <a href="https://youtu.be/IACHfKmZMr8">VC firms like Y-Combinator</a> will invest in — proofs of concept that are trash by unskilled founders hoping to win the lottery while living the life of leisure.</p><div><p>I’ve cracked vibe coding, TrendFeed has almost hit its first 10k month, and Ai built the entire thing</p><p>Im just sitting here sipping coffee, coding with Ai + MCP</p><p>Also more time to shitpost on X haha</p></div><div><p><img width="128" height="128" alt="cheers" src="https://cendyne.dev/s/128/cendyne/cheers" sizes="128px" loading="lazy"></p><div><p>The optimal technical founder for a VC is not the 10x engineer. It is someone who'll deliver <em>enough</em> of a product to test its fitness in the market and then succeed in raising more investment money. Their execution on their vision and hiring prowess is more important than their technical skillset.</p></div></div><p>The execution of agents today is over-hyped and does not hold up to the needs of any functioning businesses which need experts to develop and maintain their technical capabilities instead of single points of failure on the internet.</p><div><p>babe, come to bed</p><p>i can't, i'm vibe coding</p></div><p>These models are trained on average sloppy code, wrong answers on Stack Overflow, and the junk that ends up on Quora. Despite the power and capability Claude 3.7 Sonnet has in small contexts, when faced with even a small codebase it makes constant silly mistakes that no normal developer would repeat and continue to repeat every hour of its operation.</p><div><p><span></span><span>Specific details on the mistakes, feel free to skip</span><span></span></p><div><ul><li>Regularly clones TypeScript interfaces instead of exporting the original and importing it.</li><li>Reinvents components all the time with the same structure without searching the code base for an existing copy of that component.</li><li>Writes trusted server side logic on the client side, using RPC calls to update the database.</li><li>As a feature develops, it prioritizes maintaining previous mistakes instead of re-evaluating its design, even when told to do so. You have to say the previous implementation is outright unusable for it to replace its design.</li><li>Cursor has some sort of <a href="https://www.reddit.com/r/ClaudeAI/comments/1i8n3wq/does_claude_have_stupid_mode_enabled_tonight/">"concise mode"</a> (<a href="https://archive.is/iU8gx">archived</a>) that they'll turn on when there is high load where the model will still be rated at the normal price but behaves in a useless manner. This mode will omit details, drop important findings, and corrupt the output that is being produced.</li><li>Cannot be trusted to produce unit tests with decent coverage.</li><li>Will often break the project's code to fit a unit test rather than fix the unit test when told to do so.</li><li>When told to fix styles with precise details, it will alter the wrong component entirely.</li><li>When told specifically where there are many duplicated components and instructed to refactor, will only refactor the first instance of that component in the file instead of all instances in all files.</li><li>When told to refactor code, fails to search for the breaks it caused even when told to do so.</li><li>Will merrily produce files over 1000 lines which exceed its context window over time, even when told to refactor early on.</li><li>Will regularly erase entire route handlers if not bound to the file hierarchy.</li></ul></div></div><p>As currently designed, these models cannot learn new information. They cannot do better than the dataset they were created with. Instead their capability is realized by how effective they can process tokens entering their context window.</p><p>If you ask Claude 3.7 Sonnet to develop a runtime schema for validating some domain specific language and then ask it to refactor the file — because it is too large for its context window to continue — it will degrade and output incoherent nonsense before finishing its work.</p><p><img data-blurhash="E042PB?bM{kCWBof%M%MRjRjtRay" data-width="645" data-height="244" data-ratio="true" src="https://cendyne.dev/c/l6YxDTlA?width=645" alt="Now that we've created all the schemado that: ... I'v schema files for each schema schemaschema schemaactored code?" width="645" height="244" loading="lazy"></p><div><p><img width="128" height="110" alt="wat" src="https://cendyne.dev/s/128/cendyne/wat" sizes="128px" loading="lazy"></p><div><p>It did not type "I've" correctly and conjoined the words "schema" and "refactored" into one.</p></div></div><div><p>my saas was built with Cursor, zero hand written code</p><p>AI is no longer just an assistant, it’s also the builder</p><p>Now, you can continue to whine about it or start building.</p><p>P.S. Yes, people pay for it</p></div><p>You cannot ask these tools today to develop a performant React application. You cannot ask these tools to implement a secure user registration flow. It will choose to execute functions like is user registered on the client instead of the server.</p><div><p><img width="128" height="128" alt="trash" src="https://cendyne.dev/s/128/cendyne/trash" sizes="128px" loading="lazy"></p><div><p>Others are learning this the hard way too.</p></div></div><div><p>guys, i'm under attack</p><p>ever since I started to share how I built my SaaS using Cursor</p><p>random thing are happening, maxed out usage on api keys, people bypassing the subscription, creating random shit on db</p><p>as you know, I'm not technical so this is taking me longer that usual to figure out</p><p>for now, I will stop sharing what I do publicly on X</p><p>there are just some weird ppl out there</p></div><p>Without expert intervention, the best these tools can do today is produce a somewhat functional mockup, where every future change beyond that risks destroying existing functionality.</p><p>I cannot — and would not — trust a team member who vibe codes in a production application. The constant negligence I observe when "Vibe Coding" is atrocious and unacceptable to a customer base of any size.</p><p>No available model demonstrates consistent and necessary attention to detail needed for a production environment. They are not yet equipped or designed to transform information involving multiple contexts inherent to producing a digital product.</p><p>These tools are optimized to produce solutions that fit in a single screen of markdown and are now being asked to do far more than they were trained for. As the context window overflows and the model degrades, it will fail to even format MCP calls correctly and upon reaching this point of no return, produces a log that comes across as being tortured. Like a robot losing a limb, it will try and try again to walk only to fall down until the editor pauses the conversation to save on resources.</p><p><img data-blurhash="L03+Dt~q%2ofM{WURjWBx]t7WUj[" data-width="645" data-height="628" data-ratio="true" src="https://cendyne.dev/c/xNtV9Ji3?width=645" alt="Let me try a different approach. Error calling tool." width="645" height="628" loading="lazy"></p><h2 id="working-around-the-problem">Working around the problem</h2><p>A modern <a href="https://en.wikipedia.org/wiki/Twitch_Plays_Pok%C3%A9mon">"Twitch plays Pokémon"</a> is going on right now: <a href="https://www.twitch.tv/claudeplayspokemon">Claude Plays Pokémon</a>. It mitigates this context window problem by starting a new context with seeded information provided by its previous incarnation in the form of many Markdown files, which it can then read as if new and search via MCP during its playthrough.</p><div><div><p>So, what makes this possible? Claude was given a knowledge base to store notes, vision to see the screen, and function calls which allow it to simulate button presses and navigate the game.</p><p>Together, they allow Claude to sustain gameplay with tens of thousands of interactions.</p></div><p><img data-blurhash="L04epF.7M-a1.6t6WDofI2VtxstR" data-width="645" data-height="645" data-ratio="true" src="https://cendyne.dev/c/a9W5MgiW?width=645" alt="Photo included with tweet" width="645" height="645" loading="lazy"></p></div><p>Even so, it can make bad assumptions and spend 43 hours intentionally blacking out over and over in Mt. Moon (an in-game route between story locations) making no effective progress towards achieving its next goal because by the time it could second guess itself, its context window is no longer fit to continue.</p><p><video poster="https://cendyne.dev/c/-PEsNE-L" preload="metadata" playsinline="" controls="" autoplay="" loop="" muted="" width="644" height="362"><source src="https://cendyne.dev/c-no-index/3RrkLRXm" type="video/mp4"><img data-blurhash="L98zorE19a~C^+IoIo-p9t%2xaE1" data-width="644" data-height="362" data-ratio="true" src="https://cendyne.dev/c/-PEsNE-L?width=645" alt="Claude plays pokemon going through a context clean up and restart" width="644" height="362" loading="lazy"></video></p><div><p><img width="128" height="128" alt="galaxy-brain2" src="https://cendyne.dev/s/128/cendyne/galaxy-brain2" sizes="128px" loading="lazy"></p><div><p>It did escape and progress, but only after the critic instance of the model suggested its assumption was incorrect.</p></div></div><p>After a context cleanup completes, which takes about five minutes (the video above is edited to the meaningful moments), the model proceeds to make the same mistakes its prior incarnation did. The notes it wrote are not meaningfully interpreted in context, I find the same happens too with the Cursor rules I write.</p><p>While increasing the length of the context window will improve some immediate experiences, this is a problem of scale that needs a different solution for agents to be more effective and, perhaps, move "Vibe Coding" closer to reality.</p><div><p><img width="107" height="128" alt="thinker" src="https://cendyne.dev/s/128/cendyne/thinker" sizes="128px" loading="lazy"></p><div><p>Would a formalized <a href="https://bulletjournal.com/">bullet journal</a> over MCP help a model be more complete in delivering more reliable results?</p></div></div><div><div><p>As long as the model correctly checks it before concluding its work is complete!</p></div><p><img width="128" height="123" alt="point-left" src="https://cendyne.dev/s/128/jacobi/point-left" sizes="128px" loading="lazy"></p></div><p><img data-blurhash="K5SF;MIV~q?vt7%Mxuxuj[" data-width="645" data-height="669" data-ratio="true" src="https://cendyne.dev/c/FD-gkvrg?width=645" alt="Bullet journal with ski examples" width="645" height="669" loading="lazy"></p><p>A bullet journal may be one of many tools that improve the reliability of the models we have today.</p><p>The next issue is that these models cannot ingest information from multiple concurrent real-time sources. In one terminal we may be running the server and in another some end-to-end tests. Both of these terminals were created at the agent's request. It either ignores or is not fed the stack trace logged by the server in the first terminal as it watches the output of the end-to-end tests fail and retry, fail and retry.</p><p>For agents to have the impact promised by the hype, LLMs need a robust mechanism to mimic the development of short and long term memory without fine-tuning the memories into the model.</p><p>Furthermore, for agents to contribute to a team, there must be a way to develop long-term memories bound to the organization and its products that seamlessly merge with and reconcile with memories personal to each team member.</p><p>And lastly, these memories have to be portable. As models improve and are integrated into our tools, domain specific memories must be usable by the next generation of large language models.</p><h2 id="conclusion">Conclusion</h2><p>"Vibe Coding" might get you 80% the way to a functioning concept. But to produce something reliable, secure, and worth spending money on, you’ll need experienced humans to do the hard work not possible with today’s models.</p><p>Agents do demonstrate enough capability that LinkedIn CEO influencers confidently spread the unreality that we can replace jobs with "agentic AI."</p><p>Agents do enable skilled people to create more independently than they ever have. For the time being, it will not replace those that can solve the hard problems that only experience and intuition can identify. Like other no-code solutions, agents do give the less skilled more capability than they had the day before. Until they develop their own competent skill set, "Vibe Coders" will not be able to release production quality software in this world, no matter how <em>exponential</em> the agent is over their own inferior skill set.</p><p>Keep an eye on how LLM agents develop and improve. For now, they are worth evaluating and discussing, but are not ready for us to delegate the precise task of creating reliable, secure, and scalable software that powers our society. "Vibe Coding" will not create the next big thing in 2025.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The polar vortex is hitting the brakes (103 pts)]]></title>
            <link>https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes</link>
            <guid>43448023</guid>
            <pubDate>Sat, 22 Mar 2025 19:31:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes">https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-hitting-brakes</a>, See on <a href="https://news.ycombinator.com/item?id=43448023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>For much of this winter season, the polar vortex winds at 60°N have been racing around the stratospheric polar region. During February alone, these west-to-east winds were two times stronger than normal for that time of year. However, the latest forecasts suggest that the polar vortex is about to switch gears with a major vortex disruption to happen this weekend. Read on to find out why the polar vortex could be bottoming out early this season.</span></p>
<figure><p><a href="https://www.climate.gov/media/16835"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/observed-forecasted-winds-2025-03-06_0.png?itok=yUooMOq2" width="620" height="359" alt="time series of stratospheric winds"></a></p>
      
            <p>Observed and forecasted (NOAA GEFSv12) polar vortex wind speeds at 60°N (bold blue line) compared to the natural range of variability (faint blue shading). Since mid-November, these stratospheric winds have been stronger than normal (thin blue line). However, that’s about to change as the latest forecasts (issued March 3, 2025) indicate the winds at 60°N are going to dramatically decrease over the next few days (bold purple line), indicating a polar vortex disruption. The big question is whether these winds will rebound toward their normal strength before the end of the season. NOAA Climate.gov image, adapted from original by Laura Ciasto.</p>
      
      </figure><h2><strong>Stratospheric pit stop</strong></h2>
<p><span><span>At the time of writing this post, the polar stratospheric west-to-east winds are still speeding around the Arctic [footnote #1], but forecasts suggest they are not only going to come to a screeching halt by the weekend, but they are then going to strongly reverse direction. When this wind reversal (i.e., winds become east-to-west) occurs at 60°N and 10 hPa (~19 mi/30 km above us), it’s called </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/cooking-stratospheric-polar-vortex-disruption"><span>a sudden stratospheric warming</span></a><span>. As the name suggests, these major polar vortex disruptions are linked to incredible stratospheric temperature increases over a short period of time [footnote #2]. For this upcoming event, temperatures in the mid-stratosphere could increase as much as 45°F (25°C) in less than 5 days. </span></span></p>
<figure><p><a href="https://www.climate.gov/media/16836"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/observed-forecasted-polar-cap-temperatures-2025-03-06.png?itok=evp_E-Sz" width="620" height="361" alt="time series of stratospheric temperatures"></a></p>
      
            <p>Observed and forecasted (NOAA GEFSv12) polar cap temperatures compared to the natural range of variability (faint orange shading). Since October, these stratospheric temperatures (bold red line) have been colder than normal (thin red line). This is expected because strong polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. As the polar vortex becomes disrupted, the stratosphere will warm quickly and intensely (bold pink line), hence the name sudden stratospheric warming. NOAA Climate.gov image, adapted from original by Laura Ciasto.</p>
      
      </figure><p><span><span>Sudden stratospheric warming events usually come in two possible flavors in which the polar vortex either displaces off the pole or splits into two smaller vortexes. This particular event may be a bit of both. The initial warming event kicks off with the polar vortex shifted toward Europe, but the forecasts also show pieces of the vortex splitting off from the main lobes several days later.</span></span></p>
<figure><p><a href="https://www.climate.gov/media/16838" hreflang="en"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_stretch_featured_image/public/2025-03/polar-vortex-temperature-winds-forecast-observed-2025-03-06--corrected-01.png?itok=9p9Vfm9d" width="1100" height="720" alt="maps of temperature and winds over Northern Hemisphere"></a>
</p>

            <p>Evolution and forecast of stratospheric conditions. Earlier this week (March 4 2025; left panel), the polar vortex winds (vectors) were situated closer to the pole keeping the relatively cold air (light shading) isolated from the warmer surrounding air (orange/red shading). By March 10, 2025 (middle panel), the GFS forecast indicates the polar vortex will be nudged farther off the pole, with warmer air flooding the Arctic. The average winds around 60°N will become east-to-west, characterizing a <em>sudden stratospheric warming</em>. This disruption to the polar vortex is expected to continue through at least the next two weeks with smaller lobes of the vortex periodically splitting off (e.g., March 13, 2025, right panel). Current forecasts suggest that the stratospheric winds will not recover this spring and become west-to-east again. If so, this event will be classified as a <em>final warming</em> instead of a mid-winter <em>sudden stratospheric warming</em>. NOAA Climate.gov image, based on Global Forecast System data provided by Laura Ciasto.</p>
      
      </figure><h2><strong>Will the polar vortex rev its engine again?</strong></h2>
<p><span>One of the big questions regarding this polar vortex disruption is whether the stratospheric winds at 60°N will recover and become west-to-east again, extending the polar vortex season (and its ability to influence weather patterns) into late spring. Forecasts [footnote #3] do not currently show a recovery, so this pit stop may be the end of the vortex’s racing season. If this turns out to be the case, then it would be classified as a “final stratospheric warming” rather than a major sudden stratospheric warming.</span></p>
<p><span><span><span>As we discussed in last season’s </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/last-hurrah-polar-vortex"><span>post</span></a><span>, final warmings occur every spring as sunlight returns to the North Pole and the temperature differences between the equator and pole decrease. As a result, the west-to-east winds that are maintained by that temperature difference decrease and transition to east-to-west winds. This transition usually happens sometime in mid-April, but there have been 5 years since 1958 when final warmings occurred before March 15.&nbsp; Like this year, those years corresponded to winters without a mid-winter sudden stratospheric warming [footnote #4].</span></span></span></p>
<h2><strong>A potential stratosphere-troposphere fender bender </strong></h2>
<figure><p><a href="https://www.climate.gov/media/16837"><img loading="lazy" src="https://www.climate.gov/sites/default/files/styles/full_width_620_original_image/public/2025-03/polar-vortex-geopotential-height-2025-03-06.png?itok=IoXKmOrY" width="620" height="307" alt="contour plot of atmospheric thickness anomalies over polar cap"></a></p>
      
            <p>Differences from average atmospheric thickness (“standardized geopotential height anomalies”) in the column of air over the Arctic for the stratosphere and troposphere. Since the beginning of the year, low-thickness anomalies (purple shading indicative of a stronger than average polar vortex) have dominated the stratosphere but only periodically coupled down to the troposphere. Latest forecasts show a dramatic change with thickness anomalies increasing (orange shading), consistent with a polar vortex disruption. These stratospheric anomalies are preceded by tropospheric anomalies of the same sign, hinting at a nudge from below. However, it’s too soon to tell whether these stratospheric anomalies will then drip down into the troposphere again. Standardized anomalies are based on departures from the 1991-2020 Climate Forecast System Reanalysis climatologies and have been divided by the standard deviation. Data are from the Global Forecast System observational analysis and forecast.</p>
      
      </figure><p><span>Regardless of whether this is the final warming or the vortex decides to ride again, both have the potential to impact our weather this spring. Disruptions to the polar vortex can communicate down to the troposphere and disrupt the jet stream. These disruptions to the jet stream can bring colder than normal Arctic air down into the eastern United States.&nbsp;</span></p>
<p><span>Now this doesn’t mean you need to bring your winter tires back out while your garden tools continue to collect dust. First, it’s too soon to tell whether this vortex disruption will make its way down to the troposphere as the latest forecast doesn’t show much stratosphere-troposphere interaction after the onset of the warming event. Second, though the impacts of March sudden warmings are very similar to those in mid-winter, spring is coming, so any Arctic air brought down in the US won't "feel" as cold compared to if it happened in January because we are in a warmer part of the year. </span></p>
<p><span>Even if the polar vortex season ends early this year, we’re hoping to have at least 1 or 2 more posts (including a guest author) so stay tuned!</span></p>
<h2><strong>Footnotes</strong></h2>
<p><span>[1] We spent several posts this winter talking about the strong, but sometimes stretchy, polar vortex and what that has meant for our winter weather. If you’re interested, please read more </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-causing-us-cold-air-outbreak"><span>here</span></a><span>, </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/another-blast-arctic-air-time-stretched-strong-polar-vortex"><span>here</span></a><span>, and </span><a href="https://www.climate.gov/news-features/blogs/polar-vortex/polar-vortex-intensifications-overlooked-influencer"><span>here</span></a><span>. </span></p>
<p><span>[2] The sudden increase in temperature over such a short period of time occurs for a couple of reasons.&nbsp; As the polar winds weaken and reverse direction during a major sudden stratospheric warming, there is a component of the air that moves poleward and descends rapidly over the Arctic and pressure increases. As the air descends it warms: this is one of the reasons why the temperatures can increase so impressively during a major warming event. Furthermore, the polar vortex winds act as a barrier between cold Arctic air and warmer mid-latitude air. When the winds/barrier weaken, warmer mid-latitude winds can enter the polar stratosphere and contribute to increasing temperatures.</span></p>
<p><span>[3] We show the American GEFS model in these posts, but the ECMWF model currently doesn’t show a vortex recovery in the next several weeks either.</span></p>
<p><span>[4] The link between winters with a sudden warming and late season final warmings (and correspondingly, years without a sudden warming and early season final warmings) is thought to be due to the tug of war in the stratosphere between dynamic and radiative processes that control the strength of the polar vortex. In particular, if a sudden warming occurs during mid-winter, the polar stratospheric winds will be pulled towards returning to a west-to-east flowing state to balance the stratospheric temperature gradient created by lack of sunlight over the pole. If this recovery of the stratospheric winds to west-to-east flow occurs, it provides potentially weeks to months of additional time for planetary waves to interact with the winds, extending the timing of the final warming until much later. On the other hand, if the sudden warming occurs near the spring equinox, when sunlight has returned to the pole, the stratospheric winds feel no radiative force to return to a west-to-east state, and so often the winds will stay east-to-west (corresponding to an early season final warming).&nbsp;</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California AG Rob Bonta Urgently Issues Consumer Alert for 23andMe Customers (206 pts)]]></title>
            <link>https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</link>
            <guid>43447421</guid>
            <pubDate>Sat, 22 Mar 2025 17:55:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers">https://oag.ca.gov/news/press-releases/attorney-general-bonta-urgently-issues-consumer-alert-23andme-customers</a>, See on <a href="https://news.ycombinator.com/item?id=43447421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><em>Californians have the right to direct the company to delete their genetic data</em>&nbsp;</p>
<p><b>OAKLAND</b>&nbsp;— California Attorney General Rob Bonta today issued a consumer alert to customers of 23andMe, a genetic testing and information company. The California-based company has publicly reported that it is in financial distress and&nbsp;stated in securities filings that there is substantial doubt about its ability to continue as a going concern.&nbsp;Due to the trove of sensitive consumer data 23andMe has amassed, Attorney General Bonta reminds Californians of their right to&nbsp;direct&nbsp;the deletion of their genetic data under the Genetic Information Privacy Act (GIPA) and California Consumer Protection Act (CCPA).&nbsp;Californians who want to invoke these rights can do so by going to 23andMe's website.&nbsp;</p>
<p>“California has robust privacy laws that allow consumers to take control and request that a company delete their genetic data,”&nbsp;<b>said Attorney General Bonta.</b>&nbsp;“Given 23andMe’s reported financial distress, I remind&nbsp;Californians to consider invoking their rights and directing 23andMe to delete their data and destroy any samples of genetic material held by the company.”&nbsp;</p>
<p><b>To Delete Genetic Data from 23andMe:</b></p>
<ol>
<li>Consumers can delete their account and personal information by taking the following steps:</li>
<li>Log into your 23andMe account on their website.&nbsp;</li>
<li>Go to the “Settings” section of your profile.</li>
<li>Scroll to a section labeled “23andMe Data” at the bottom of the page.&nbsp;</li>
<li>Click “View” next to “23andMe Data”</li>
<li>Download your data: If you want a copy of your genetic data for personal storage, choose the option to download it to your device before proceeding.</li>
<li>Scroll to the “Delete Data” section.&nbsp;</li>
<li>Click “Permanently Delete Data.”&nbsp;</li>
<li>Confirm your request:&nbsp;You’ll receive an email from 23andMe; follow the link in the email to confirm your deletion request.</li>
</ol>
<p><b>To Destroy Your 23andMe Test Sample:</b></p>
<p>If you previously opted to have your saliva sample and DNA stored by 23andMe, but want to change that preference, you can do so from your account settings page, under “Preferences.”</p>
<p><b>To Revoke Permission for Your Genetic Data to be Used for Research:</b></p>
<p>If you previously consented to 23andMe and third-party researchers to use your genetic data and sample for research, you may withdraw consent from the account settings page, under “Research and Product Consents.”</p>
<p>Under GIPA, California consumers can delete their account and genetic data and have their biological sample destroyed.&nbsp;In addition, GIPA permits California consumers to revoke consent that they provided a genetic testing company to collect, use, and disclose genetic data and to store biological samples after the initial testing has been completed.&nbsp;The CCPA also vests California consumers with the right to delete personal information, which includes genetic data, from businesses that collect personal information from the consumer. &nbsp;&nbsp;</p>
<p>To learn more about the CCPA, please visit&nbsp;<a href="https://oag.ca.gov/privacy/ccpa">here</a>.&nbsp;&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Map Features in OpenStreetMap with Computer Vision (138 pts)]]></title>
            <link>https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/</link>
            <guid>43447335</guid>
            <pubDate>Sat, 22 Mar 2025 17:42:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/">https://blog.mozilla.ai/map-features-in-openstreetmap-with-computer-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=43447335">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <h3 id="motivation">Motivation</h3><p>At Mozilla.ai, we believe that there are a lot of opportunities where artificial intelligence (AI) can empower communities driven by open collaboration.&nbsp;</p><p>These opportunities need to be designed carefully, though, as many members of these communities (and people in general) are increasingly worried about the amount of <a href="https://en.wikipedia.org/wiki/AI_slop?ref=blog.mozilla.ai"><u>AI slop</u></a> flooding the internet.</p><p>With this idea in mind we developed and released the <a href="https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>OpenStreetMap AI Helper</u></a> Blueprint. If you love maps and are interested in training your own computer vision model, you’ll enjoy diving into this Blueprint.</p><h3 id="why-openstreetmap">Why OpenStreetMap?</h3><p>Data is one of the most important components of any AI application, and <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> has a vibrant community that collaborates to maintain and extend the most complete open map database available. </p><p>If you haven’t heard of it, <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> is an open, editable map of the world created by a community of mappers who contribute and maintain data about roads, trails, cafés, railway stations, and more.</p><p>Combined with other sources, like satellite imagery, this database offers infinite possibilities to train different AI models.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQ5_VUhyWiLkZAqDkmqKu7p3vT5hC873l9vFSbduVam2uC3odROrGsOWLUdbYi9ZHAyWLHR-QT2SoowtHgqxcR_aaaJu6joEG6cNMhxdV2IiAvAToa_TQkic9Qx8sgkFxzTPBZ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="351"></figure><p>As a long-time user and contributor to <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a> , I wanted to build an end-to-end application where a model is first trained with this data and then used to contribute back.</p><p>The idea is to use AI to speed up the slower parts of the mapping process (roaming around the map, drawing polygons) while keeping a human in the loop for the critical parts (verifying that the generated data is correct).</p><h3 id="why-computer-vision">Why Computer Vision?</h3><p>Large Language Models (LLM) and, more recently, Visual Language Models (VLM) are sucking all the oxygen out of the AI room, but there are a lot of interesting applications that don’t (need to) use this type of models.</p><p>Many of the <a href="https://wiki.openstreetmap.org/wiki/Map_features?ref=blog.mozilla.ai"><u>Map Features</u></a> you can find in OpenStreetMap are represented with a polygon ('Area'). It turns out that finding and drawing these polygons is a very time consuming task for a human, but Computer Vision models can be easily trained for the task (when provided with enough data).</p><p>We chose to split the work of finding and drawing map features into 2 computer vision tasks using state-of-the-art non-LLM models: </p><ul><li><strong>Object Detection</strong> with <a href="https://docs.ultralytics.com/es/models/yolo11/?ref=blog.mozilla.ai"><u>YOLOv11</u></a>, by <a href="https://www.ultralytics.com/?ref=blog.mozilla.ai" rel="noreferrer">Ultralytics</a>, which identifies where relevant features exist in an image.</li><li><strong>Segmentation </strong>with <a href="https://ai.meta.com/sam2/?ref=blog.mozilla.ai"><u>SAM2</u></a>, by <a href="https://ai.meta.com/?ref=blog.mozilla.ai" rel="noreferrer">Meta</a>, which refines the detected features by outlining their exact shape.</li></ul><p>These models are lightweight, fast, and local-friendly – it’s refreshing to work with models that don’t demand a high-end GPU just to function. As an example, the combined weights of YOLOv11 and SAM2 take much less disk space (&lt;250MB) than any of the smallest Visual Language Models available, like <a href="https://huggingface.co/HuggingFaceTB/SmolVLM-Base?ref=blog.mozilla.ai"><u>SmolVLM </u></a>(4.5GB).</p><p>By combining these models, we can automate much of the mapping process while keeping humans in control for final verification.</p><h3 id="the-openstreetmap-ai-helper-blueprint">The OpenStreetMap AI Helper Blueprint</h3><p>The Blueprint can be divided into 3 stages:</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeAMhkrtYiDH7ZMsybzM7GF1KSiJMUTddPlc-xyfzlVdW6Mkd1xrtEFK81P_27vNHCMCnHGpEwEQN5-wIrkGKax_JpiBrClnzi1hVzMrrVvfpHk3fwd1fu_uiHmxxrpGiZcnKsqBA?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="160"></figure><p><strong>Stage 1: Create an Object Detection dataset from OpenStreetMap</strong></p><p>The first stage involves fetching data from OpenStreetMap, combining it with satellite images, and transforming it into a format suitable for training.</p><p>You can run it yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/create_dataset.ipynb?ref=blog.mozilla.ai"><u>Create Dataset Colab</u></a>.</p><p>For fetching OpenStreetMap data, we use:</p><ul><li>The <a href="https://nominatim.org/?ref=blog.mozilla.ai"><u>Nominatim API</u></a> to provide users with a flexible way of selecting an area of interest. In our swimming pool example, we use <a href="https://nominatim.openstreetmap.org/ui/details.html?osmtype=R&amp;osmid=349036&amp;class=boundary&amp;ref=blog.mozilla.ai"><u>Galicia</u></a> for training and <a href="https://nominatim.openstreetmap.org/ui/details.html?osmtype=R&amp;osmid=3808752&amp;class=boundary&amp;ref=blog.mozilla.ai"><u>Viana do Castelo</u></a> for validation.</li><li>The <a href="https://wiki.openstreetmap.org/wiki/Overpass_API?ref=blog.mozilla.ai"><u>Overpass API</u></a> to download all the relevant polygons using specific <a href="https://wiki.openstreetmap.org/wiki/Tags?ref=blog.mozilla.ai"><u>tags</u></a> within the selected area of interest. In our swimming pool example, we use <a href="https://wiki.openstreetmap.org/wiki/Tag:leisure=swimming_pool?ref=blog.mozilla.ai"><u>leisure=swimming_pool</u></a> discarding the ones also tagged with <a href="https://wiki.openstreetmap.org/wiki/Tag:location%3Dindoor?ref=blog.mozilla.ai"><u>location=indoor</u></a>.</li></ul><p>Once all the polygons have been downloaded, you can choose a <a href="https://docs.mapbox.com/help/glossary/zoom-level/?ref=blog.mozilla.ai"><u>zoom level</u></a>. We use this zoom level to first identify all the tiles that contain a polygon and then download them using the <a href="https://docs.mapbox.com/api/maps/static-tiles/?ref=blog.mozilla.ai"><u>Static Tiles API</u></a> from <a href="https://www.mapbox.com/?ref=blog.mozilla.ai"><u>Mapbox</u></a>.</p><p>The polygons in latitude and longitude coordinates are transformed to a bounding box in pixel coordinates relative to each tile and then saved in the <a href="https://docs.ultralytics.com/datasets/detect/?ref=blog.mozilla.ai#ultralytics-yolo-format"><u>Ultralytics YOLO format</u></a>.</p><p>Finally, the dataset is uploaded to the <a href="https://huggingface.co/docs/hub/datasets?ref=blog.mozilla.ai"><u>Hugging Face Hub</u></a>. You can check our example <a href="https://huggingface.co/datasets/mozilla-ai/osm-swimming-pools?ref=blog.mozilla.ai"><u>mozilla-ai/osm-swimming-pools</u></a>.</p><p><strong>Stage 2 - Finetune an Object Detection model</strong></p><p>Once the dataset is uploaded in the right format, finetuning a <a href="https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai"><u>YOLOv11</u></a> (or any other model supported by Ultralytics) is quite easy. </p><p>You can run it yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/finetune_model.ipynb?ref=blog.mozilla.ai"><u>Finetune Model Colab</u></a> and check all the <a href="https://docs.ultralytics.com/modes/train/?ref=blog.mozilla.ai#augmentation-settings-and-hyperparameters"><u>available hyperparameters</u></a>.</p><p>Once the model is trained, it is also uploaded to the <a href="https://huggingface.co/docs/hub/datasets?ref=blog.mozilla.ai"><u>Hugging Face Hub</u></a>. You can check our example <a href="https://huggingface.co/mozilla-ai/swimming-pool-detector?ref=blog.mozilla.ai"><u>mozilla-ai/swimming-pool-detector</u></a>.</p><p><strong>Stage 3 - Contributing to OpenStreetMap</strong></p><p>Once you have a finetuned Object Detection model, you can use it to run inference across multiple tiles. </p><p>You can run inference yourself in the <a href="https://colab.research.google.com/github/mozilla-ai/osm-ai-helper/blob/main/demo/run_inference.ipynb?ref=blog.mozilla.ai"><u>Run Inference Colab</u></a>. </p><p>We also provide a hosted demo where you can try our example swimming pool detector: <a href="https://huggingface.co/spaces/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>HuggingFace Demo</u></a>.</p><p>The inference requires a couple of human interactions. First, you need to first pick a point of interest in the map:</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXczl1Ib_aQv-G0IUYx7lKv4s1eTCGt1AKHI8G_d9IpbUKtRfV--HyjkxVYT-AdiZ5e-5VeF-TuhRotvvsMOx4SWVFDDaoZtPPxuoYHStzl-a1aWrIn_hy8WpxBbx97sQtEvMEaniw?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="315"></figure><p>After a point is selected, a bounding box is computed around it based on the <em>margin </em>argument.</p><p>All the existing elements of interest are downloaded from <a href="https://www.openstreetmap.org/?ref=blog.mozilla.ai"><u>OpenStreetMap</u></a>, and all the tiles are downloaded from <a href="https://www.mapbox.com/?ref=blog.mozilla.ai"><u>Mapbox</u></a> and joined to create a stacked image.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeP6u-8eEa5cfLIZurqx4zcct2TeWMMFD999MfRNC0F4uQ7kaqBZPmdeUdQrVwB1fiS4MBPlpL86Vljev1WlvxtZhXGB5qy9d1Ghbh9lKlim3UsDyZTkANaU2TLwgx13URfCJRJnQ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="599" height="601"></figure><p>The stacked image is divided into overlapping tiles. For each tile, we run the Object Detection model (<a href="https://docs.ultralytics.com/models/yolo11/?ref=blog.mozilla.ai">YOLOv11</a>). If an object of interest is detected (e.g. a swimming pool), we pass the bounding box to the Segmentation model (<a href="https://github.com/facebookresearch/sam2?ref=blog.mozilla.ai"><u>SAM2</u></a>) to obtain a segmentation mask.</p><figure><img src="https://blog.mozilla.ai/content/images/2025/02/image.png" alt="" loading="lazy" width="1106" height="590" srcset="https://blog.mozilla.ai/content/images/size/w600/2025/02/image.png 600w, https://blog.mozilla.ai/content/images/size/w1000/2025/02/image.png 1000w, https://blog.mozilla.ai/content/images/2025/02/image.png 1106w" sizes="(min-width: 720px) 720px"></figure><p>All the predicted polygons are checked against the existing ones, downloaded from OpenStreetMap, in order to avoid duplicates.&nbsp;All those identified as <em>new </em>are displayed one by one for manual verification and filtering.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfwAlhR0KxRyrjR8RPsYBbhdhYQn6udmJi_QeeAelz52YHK_K5UNLIbvI7RsAG1tsgzb0HKGB1MTezBjOlZRtfdpAfhIY1UMOlAmM_GxPSyQeYcn1J_cB9FdMLlu9-mUG7k13FAiQ?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="301"></figure><p>The ones you chose to keep will be then uploaded to OpenStreetMap in a single <a href="https://wiki.openstreetmap.org/wiki/Changeset?ref=blog.mozilla.ai"><u>changeset</u></a><u>.</u></p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXewc5Qru4SyT3SBDxHV7EDGZ8XEjwV7P4uFAIhY-zkIafmJ_GP8yBeul4yNb9qAVOVfsIgFrfs7oaLq2Tb0HKuF_oWWXcx75likKPmGkyUF_uc9hCDhsgLXOC4OthbZZVGSZAcf?key=ZPV5MptphH1GvdigS7RavJFc" alt="" loading="lazy" width="624" height="293"></figure><h3 id="closing-thoughts">Closing thoughts</h3><p>OpenStreetMap is a powerful example of open collaboration to create a rich, community-driven map of the world. </p><p>The OpenStreatMap AI Helper Blueprint shows that, with the right approach, AI can enhance human contributions while keeping human verification at the core.&nbsp;In the fully manual process it takes about 1 min to map 2-3 swimming pools, whereas using the blueprint, even without an optimized UX, I can map about 10-15 in the same time (~5x more).</p><p>It also highlights the value of high-quality data from projects like OpenStreetMap, which enables to easily train models like YOLOv11 to perform object detection – proving that you shouldn’t always throw an LLM at the problem.</p><p>We’d love for you to try the <a href="https://github.com/mozilla-ai/osm-ai-helper?ref=blog.mozilla.ai"><u>OpenStreetMap AI Helper Blueprint</u></a> and experiment with training a model on a different map feature. If you’re interested, feel free to contribute to the repo to help improve it, or fork it to extend it even further!</p><p>To find other Blueprints we’ve released, check out the <a href="https://developer-hub.mozilla.ai/blueprints?ref=blog.mozilla.ai" rel="noopener noreferrer">Blueprints Hub</a>.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook to stop targeting ads at UK woman after legal fight (119 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/c1en1yjv4dpo</link>
            <guid>43446821</guid>
            <pubDate>Sat, 22 Mar 2025 16:22:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/c1en1yjv4dpo">https://www.bbc.co.uk/news/articles/c1en1yjv4dpo</a>, See on <a href="https://news.ycombinator.com/item?id=43446821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="headline-block"></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg.webp 976w" type="image/webp"><img alt="A woman with blonde hair, blue eyes and pink lipstick stares at the camera" src="https://ichef.bbci.co.uk/ace/standard/1200/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/d1fc/live/8ccc8580-072a-11f0-97d3-37df2b293ed1.jpg 976w" width="1200" height="675"></picture></span><span role="text"><span>Image source, </span>Tanya O'Carroll</span></p><figcaption><span>Image caption, </span><p>Facebook has agreed to stop targeting adverts at Tanya O'Carroll after she filed a lawsuit against its parent company</p></figcaption></figure></div><div data-component="text-block"><p><b>Facebook has agreed to stop targeting adverts at an individual user using personal data after she filed a lawsuit against its parent company, tech giant Meta.</b></p><p>Tanya O'Carroll, 37, who lives in London and works in the tech policy and human rights sector, said it would open a "gateway" for other people wanting to stop the social media company from serving them adverts based on their demographics and interests.</p><p>The Information Commissioner's Office, the UK's data watchdog, said online targeted advertising should be considered direct marketing.</p><p>In a statement, Meta said it provided "robust settings and tools for users to control their data and advertising preferences".</p></div><div data-component="text-block"><p>Ms O'Carroll, who created her Facebook account about 20 years ago, filed a lawsuit against Meta in 2022, asking it to stop using her personal data to fill her social media feeds with targeted adverts based on topics it thought she was interested in.</p><p>"I knew that this kind of predatory, invasive advertising is actually something that we all have a legal right to object to," Ms O'Carroll told Radio 4's Today Programme. </p><p>"I don't think we should have to accept these unfair terms where we consent to all that invasive data tracking and surveillance."</p><p>It was when she found out she was pregnant in 2017 that she realised the extent to which Facebook was targeting adverts at her.</p><p>She said the adverts she got "suddenly started changing within weeks to lots of baby photos and other things - ads about babies and pregnancy and motherhood".</p><p>"I just found it unnerving - this was before I'd even told people in my private life, and yet Facebook had already determined that I was pregnant," she continued.</p></div><div data-component="text-block"><p>General Data Protection Regulation (GDPR) legislation controls how personal information is used by organisations.</p><p>Ms O'Carroll's lawsuit argued that Facebook's targeted advertising system was covered by the UK's definition of direct marketing, giving individuals the right to object.</p><p>Meta said that adverts on its platform could only be targeted to groups of a minimum size of 100 people, rather than individuals, so did not count as direct marketing. But the Information Commissioner's Office (ICO) disagreed.</p><p>"Organisations must respect people's choices about how their data is used," a spokesperson for the ICO said. "This means giving users a clear way to opt out of their data being used in this way."</p><p>Ms O'Carroll said that Meta had agreed to stop using her personal data for direct marketing purposes, "which in non-legalese means I've essentially been able to turn off all the creepy, invasive, targeted ads on Facebook".</p><p>She said that she did not want to stop using Facebook, saying that it is "filled with all of those connections and family and friends, and entire chapters of my life".</p></div><div data-component="text-block"><p>Ms O'Carroll said she hoped her individual settlement would make it easier for others who wanted Facebook to stop giving them targeted adverts.</p><p>"If other people want to exercise their right, I believe they now have a gateway to do so knowing that the UK regulator will back them up," she said.</p><p>Meta said it disagreed with Ms O'Carroll's claims, adding "no business can be mandated to give away its services for free."</p><p>A spokesperson added: "Facebook and Instagram cost a significant amount of money to build and maintain, and these services are free for British consumers because of personalised advertising."</p><p>"Our services support British jobs and economic growth by connecting businesses with the people most likely to buy their products, while enabling universal access to online services regardless of income. We will continue to defend its value while upholding user choice and privacy."</p><p>Facebook and Instagram have a <a href="https://www.bbc.co.uk/news/technology-67226394">subscription service</a> in most of Europe, where users can pay monthly so that they don't get ads on the platform.</p><p>The Meta spokesperson said the company was "exploring the option" of offering a similar service to UK users and would "share further information in due course."</p></div><section data-component="links-block"><p><h2 type="normal">More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon wants a product safety regulator declared unconstitutional (132 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/</link>
            <guid>43446103</guid>
            <pubDate>Sat, 22 Mar 2025 14:56:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/">https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/</a>, See on <a href="https://news.ycombinator.com/item?id=43446103">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2025/03/21/amazon-product-safety-regulators-trump/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[PyTorch Internals: Ezyang's Blog (228 pts)]]></title>
            <link>https://blog.ezyang.com/2019/05/pytorch-internals/</link>
            <guid>43445931</guid>
            <pubDate>Sat, 22 Mar 2025 14:39:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ezyang.com/2019/05/pytorch-internals/">https://blog.ezyang.com/2019/05/pytorch-internals/</a>, See on <a href="https://news.ycombinator.com/item?id=43445931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<!-- -*- mode: rst -*- -->
<p>This post is a long form essay version of a talk about PyTorch internals, that I gave at the PyTorch NYC meetup on May 14, 2019.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-01.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-01.png"></p></div>
<p>Hi everyone!  Today I want to talk about the internals of <a href="https://pytorch.org/">PyTorch</a>.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-02.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-02.png"></p></div>
<p>This talk is for those of you who have used PyTorch, and thought to yourself, "It would be great if I could contribute to PyTorch," but were scared by PyTorch's behemoth of a C++ codebase.  I'm not going to lie: the PyTorch codebase can be a bit overwhelming at times. The purpose of this talk is to put a map in your hands: to tell you about the basic conceptual structure of a "tensor library that supports automatic differentiation", and give you some tools and tricks for finding your way around the codebase.  I'm going to assume that you've written some PyTorch before, but haven't necessarily delved deeper into how a machine learning library is written.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-03.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-03.png"></p></div>
<p>The talk is in two parts: in the first part, I'm going to first introduce you to the conceptual universe of a tensor library.  I'll start by talking about the tensor data type you know and love, and give a more detailed discussion about what exactly this data type provides, which will lead us to a better understanding of how it is actually implemented under the hood.  If you're an advanced user of PyTorch, you'll be familiar with most of this material.  We'll also talk about the trinity of "extension points", layout, device and dtype, which guide how we think about extensions to the tensor class.  In the live talk at PyTorch NYC, I skipped the slides about autograd, but I'll talk a little bit about them in these notes as well.</p>
<p>The second part grapples with the actual nitty gritty details involved with actually coding in PyTorch.  I'll tell you how to cut your way through swaths of autograd code, what code actually matters and what is legacy, and also all of the cool tools that PyTorch gives you for writing kernels.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-04.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-04.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-05.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-05.png"></p></div>
<p>The tensor is the central data structure in PyTorch.  You probably have a pretty good idea about what a tensor intuitively represents: its an n-dimensional data structure containing some sort of scalar type, e.g., floats, ints, et cetera.  We can think of a tensor as consisting of some data, and then some metadata describing the size of the tensor, the type of the elements in contains (dtype), what device the tensor lives on (CPU memory? CUDA memory?)</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-06.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-06.png"></p></div>
<p>There's also a little piece of metadata you might be less familiar with: the stride.  Strides are actually one of the distinctive features of PyTorch, so it's worth discussing them a little more.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-07.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-07.png"></p></div>
<p>A tensor is a mathematical concept.  But to represent it on our computers, we have to define some sort of physical representation for them.  The most common representation is to lay out each element of the tensor contiguously in memory (that's where the term contiguous comes from), writing out each row to memory, as you see above. In the example above, I've specified that the tensor contains 32-bit integers, so you can see that each integer lies in a physical address, each offset four bytes from each other.  To remember what the actual dimensions of the tensor are, we have to also record what the sizes are as extra metadata.</p>
<p>So, what do strides have to do with this picture?</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-08.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-08.png"></p></div>
<p>Suppose that I want to access the element at position <tt>tensor[1, 0]</tt> in my logical representation.  How do I translate this logical position into a location in physical memory?  Strides tell me how to do this: to find out where any element for a tensor lives, I multiply each index with the respective stride for that dimension, and sum them all together.  In the picture above, I've color coded the first dimension blue and the second dimension red, so you can follow the index and stride in the stride calculation.  Doing this sum, I get two (zero-indexed), and indeed, the number three lives two below the beginning of the contiguous array.</p>
<p>(Later in the talk, I'll talk about TensorAccessor, a convenience class that handles the indexing calculation.  When you use TensorAccessor, rather than raw pointers, this calculation is handled under the covers for you.)</p>
<p>Strides are the fundamental basis of how we provide views to PyTorch users.  For example, suppose that I want to extract out a tensor that represents the second row of the tensor above:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-09.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-09.png"></p></div>
<p>Using advanced indexing support, I can just write <tt>tensor[1, :]</tt> to get this row.  Here's the important thing: when I do this, I don't create a new tensor; instead, I just return a tensor which is a different view on the underlying data.  This means that if I, for example, edit the data in that view, it will be reflected in the original tensor.  In this case, it's not too hard to see how to do this: three and four live in contiguous memory, and all we need to do is record an offset saying that the data of this (logical) tensor lives two down from the top.  (Every tensor records an offset, but most of the time it's zero, and I'll omit it from my diagrams when that's the case.)</p>
<!--  -->
<blockquote>
<p>Question from the talk: If I take a view on a tensor, how do I free the memory of the underlying tensor?</p>
<p>Answer: You have to make a copy of the view, thus disconnecting it from the original physical memory.  There's really not much else you can do.  By the way, if you have written Java in the old days, taking substrings of strings has a similar problem, because by default no copy is made, so the substring retains the (possibly very large string). Apparently, they <a href="https://stackoverflow.com/questions/14161050/java-string-substring-method-potential-memory-leak">fixed this in Java 7u6</a>.</p>
</blockquote>
<p>A more interesting case is if I want to take the first column:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-10.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-10.png"></p></div>
<p>When we look at the physical memory, we see that the elements of the column are not contiguous: there's a gap of one element between each one.  Here, strides come to the rescue: instead of specifying a stride of one, we specify a stride of two, saying that between one element and the next, you need to jump two slots.  (By the way, this is why it's called a "stride": if we think of an index as walking across the layout, the stride says how many locations we stride forward every time we take a step.)</p>
<p>The stride representation can actually let you represent all sorts of interesting views on tensors; if you want to play around with the possibilities, check out the <a href="https://ezyang.github.io/stride-visualizer/index.html">Stride Visualizer</a>.</p>
<p>Let's step back for a moment, and think about how we would actually implement this functionality (after all, this is an internals talk.)  If we can have views on tensor, this means we have to decouple the notion of the tensor (the user-visible concept that you know and love), and the actual physical data that stores the data of the tensor (called storage):</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-11.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-11.png"></p></div>
<p>There may be multiple tensors which share the same storage.  Storage defines the dtype and physical size of the tensor, while each tensor records the sizes, strides and offset, defining the logical interpretation of the physical memory.</p>
<p>One thing to realize is that there is always a pair of Tensor-Storage, even for "simple" cases where you don't really need a storage (e.g., you just allocated a contiguous tensor with <tt>torch.zeros(2, 2)</tt>).</p>
<!--  -->
<blockquote>
By the way, we're interested in making this picture not true; instead of having a separate concept of storage, just define a view to be a tensor that is backed by a base tensor.  This is a little more complicated, but it has the benefit that contiguous tensors get a much more direct representation without the Storage indirection.  A change like this would make PyTorch's internal representation a bit more like Numpy's.</blockquote>
<hr>
<p>We've talked quite a bit about the data layout of tensor (some might say, if you get the data representation right, everything else falls in place).  But it's also worth briefly talking about how operations on the tensor are implemented.  At the very most abstract level, when you call <tt>torch.mm</tt>, two dispatches happen:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-12.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-12.png"></p></div>
<p>The first dispatch is based on the device type and layout of a tensor: e.g., whether or not it is a CPU tensor or a CUDA tensor (and also, e.g., whether or not it is a strided tensor or a sparse one).  This is a dynamic dispatch: it's a virtual function call (exactly where that virtual function call occurs will be the subject of the second half of this talk).  It should make sense that you need to do a dispatch here: the implementation of CPU matrix multiply is quite different from a CUDA implementation.  It is a <em>dynamic</em> dispatch because these kernels may live in separate libraries (e.g., <tt>libcaffe2.so</tt> versus <tt>libcaffe2_gpu.so</tt>), and so you have no choice: if you want to get into a library that you don't have a direct dependency on, you have to dynamic dispatch your way there.</p>
<p>The second dispatch is a dispatch on the dtype in question.  This dispatch is just a simple switch-statement for whatever dtypes a kernel chooses to support.  Upon reflection, it should also make sense that we need to a dispatch here: the CPU code (or CUDA code, as it may) that implements multiplication on <tt>float</tt> is different from the code for <tt>int</tt>.  It stands to reason you need separate kernels for each dtype.</p>
<p>This is probably the most important mental picture to have in your head, if you're trying to understand the way operators in PyTorch are invoked.  We'll return to this picture when it's time to look more at code.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-13.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-13.png"></p></div>
<p>Since we have been talking about Tensor, I also want to take a little time to the world of tensor extensions.  After all, there's more to life than dense, CPU float tensors.  There's all sorts of interesting extensions going on, like XLA tensors, or quantized tensors, or MKL-DNN tensors, and one of the things we have to think about, as a tensor library, is how to accommodate these extensions.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-14.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-14.png"></p></div>
<p>Our current model for extensions offers four extension points on tensors.  First, there is the trinity three parameters which uniquely determine what a tensor is:</p>
<ul>
<li>The <strong>device</strong>, the description of where the tensor's physical memory is actually stored, e.g., on a CPU, on an NVIDIA GPU (cuda), or perhaps on an AMD GPU (hip) or a TPU (xla).  The distinguishing characteristic of a device is that it has its own allocator, that doesn't work with any other device.</li>
<li>The <strong>layout</strong>, which describes how we logically interpret this physical memory.  The most common layout is a strided tensor, but sparse tensors have a different layout involving a pair of tensors, one for indices, and one for data; MKL-DNN tensors may have even more exotic layout, like blocked layout, which can't be represented using merely strides.</li>
<li>The <strong>dtype</strong>, which describes what it is that is actually stored in each element of the tensor.  This could be floats or integers, or it could be, for example, quantized integers.</li>
</ul>
<p>If you want to add an extension to PyTorch tensors (by the way, if that's what you want to do, please talk to us!  None of these things can be done out-of-tree at the moment), you should think about which of these parameters you would extend.  The Cartesian product of these parameters define all of the possible tensors you can make.  Now, not all of these combinations may actually have kernels (who's got kernels for sparse, quantized tensors on FPGA?) but in <em>principle</em> the combination could make sense, and thus we support expressing it, at the very least.</p>
<p>There's one last way you can make an "extension" to Tensor functionality, and that's write a wrapper class around PyTorch tensors that implements your object type.  This perhaps sounds obvious, but sometimes people reach for extending one of the three parameters when they should have just made a wrapper class instead.  One notable merit of wrapper classes is they can be developed entirely out of tree.</p>
<p>When should you write a tensor wrapper, versus extending PyTorch itself?  The key test is whether or not you need to pass this tensor along during the autograd backwards pass.  This test, for example, tells us that sparse tensor should be a true tensor extension, and not just a Python object that contains an indices and values tensor: when doing optimization on networks involving embeddings, we want the gradient generated by the embedding to be sparse.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-15.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-15.png"></p></div>
<p>Our philosophy on extensions also has an impact of the data layout of tensor itself.  One thing we really want out of our tensor struct is for it to have a fixed layout: we don't want fundamental (and very frequently called) operations like "What's the size of a tensor?" to require virtual dispatches.  So when you look at the actual layout of a Tensor (defined in the <a href="https://github.com/pytorch/pytorch/blob/master/c10/core/TensorImpl.h">TensorImpl struct</a>),  what we see is a common prefix of all fields that we consider all "tensor"-like things to universally have, plus a few fields that are only really applicable for strided tensors, but are <em>so</em> important that we've kept them in the main struct, and then a suffix of custom fields that can be done on a per-Tensor basis.  Sparse tensors, for example, store their indices and values in this suffix.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-16.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-16.png"></p></div>
<p>I told you all about tensors, but if that was the only thing PyTorch provided, we'd basically just be a Numpy clone.  The distinguishing characteristic of PyTorch when it was originally released was that it provided automatic differentiation on tensors (these days, we have other cool features like TorchScript; but back then, this was it!)</p>
<p>What does automatic differentiation do?  It's the machinery that's responsible for taking a neural network:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-17.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-17.png"></p></div>
<p>...and fill in the missing code that actually computes the gradients of your network:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-18.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-18.png"></p></div>
<p>Take a moment to study this diagram.  There's a lot to unpack; here's what to look at:</p>
<ol>
<li>First, rest your eyes on the variables in red and blue.  PyTorch implements <a href="https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation">reverse-mode automatic differentiation</a>, which means that we effectively walk the forward computations "backward" to compute the gradients.  You can see this if you look at the variable names: at the bottom of the red, we compute <tt>loss</tt>; then, the first thing we do in the blue part of the program is compute <tt>grad_loss</tt>.  <tt>loss</tt> was computed from <tt>next_h2</tt>, so we compute <tt>grad_next_h2</tt>.  Technically, these variables which we call <tt>grad_</tt> are not really gradients; they're really Jacobians left-multiplied by a vector, but in PyTorch we just call them <tt>grad</tt> and mostly everyone knows what we mean.</li>
<li>If the structure of the code stays the same, the behavior doesn't: each line from forwards is replaced with a different computation, that represents the derivative of the forward operation.  For example, the <tt>tanh</tt> operation is translated into a <tt>tanh_backward</tt> operation (these two lines are connected via a grey line on the left hand side of the diagram).  The inputs and outputs of the forward and backward operations are swapped: if the forward operation produced <tt>next_h2</tt>, the backward operation takes <tt>grad_next_h2</tt> as an input.</li>
</ol>
<p>The whole point of autograd is to do the computation that is described by this diagram, but without actually ever generating this source.  PyTorch autograd doesn't do a source-to-source transformation (though PyTorch JIT does know how to do symbolic differentiation).</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-19.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-19.png"></p></div>
<p>To do this, we need to store more metadata when we carry out operations on tensors.  Let's adjust our picture of the tensor data structure: now instead of just a tensor which points to a storage, we now have a variable which wraps this tensor, and also stores more information (AutogradMeta), which is needed for performing autograd when a user calls <tt>loss.backward()</tt> in their PyTorch script.</p>
<!--  -->
<blockquote>
This is yet another slide which will hopefully be out of date in the near future.  Will Feng is working on a <a href="https://github.com/pytorch/pytorch/issues/13638">Variable-Tensor merge in C++</a>, following a simple merge which happened to PyTorch's frontend interface.</blockquote>
<p>We also have to update our picture about dispatch:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-20.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-20.png"></p></div>
<p>Before we dispatch to CPU or CUDA implementations, there is another dispatch on variables, which is responsible for unwrapping variables, calling the underlying implementation (in green), and then rewrapping the results into variables and recording the necessary autograd metadata for backwards.</p>
<p>Some implementations don't unwrap; they just call into other variable implementations.  So you might spend a while in the Variable universe.  However, once you unwrap and go into the non-Variable Tensor universe, that's it; you never go back to Variable (except by returning from your function.)</p>
<hr>
<p>In my NY meetup talk, I skipped the following seven slides.  I'm also going to delay writeup for them; you'll have to wait for the sequel for some text.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-21.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-21.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-22.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-22.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-23.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-23.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-24.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-24.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-25.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-25.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-26.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-26.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-27.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-27.png"></p></div>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-28.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-28.png"></p></div>
<p>Enough about concepts, let's look at some code.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-29.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-29.png"></p></div>
<p>PyTorch has a lot of folders, and there is a very detailed description of what they are in the <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#codebase-structure">CONTRIBUTING</a> document, but really, there are only four directories you really need to know about:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-30.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-30.png"></p></div>
<ul>
<li>First, <tt>torch/</tt> contains what you are most familiar with: the actual Python modules that you import and use.  This stuff is Python code and easy to hack on (just make a change and see what happens).  However, lurking not too deep below the surface is...</li>
<li><tt>torch/csrc/</tt>, the C++ code that implements what you might call the frontend of PyTorch.  In more descriptive terms, it implements the binding code that translates between the Python and C++ universe, and also some pretty important pieces of PyTorch, like the autograd engine and the JIT compiler.  It also contains the C++ frontend code.</li>
<li><tt>aten/</tt>, short for "A Tensor Library" (coined by Zachary DeVito), is a C++ library that implements the operations of Tensors.  If you're looking for where some kernel code lives, chances are it's in ATen.  ATen itself bifurcates into two neighborhoods of operators: the "native" operators, which are modern, C++ implementations of operators, and the "legacy" operators (TH, THC, THNN, THCUNN), which are legacy, C implementations.  The legacy operators are the bad part of town; try not to spend too much time there if you can.</li>
<li><tt>c10/</tt>, which is a pun on Caffe2 and A"Ten" (get it? Caffe 10) contains the core abstractions of PyTorch, including the actual implementations of the Tensor and Storage data structures.</li>
</ul>
<p>That's a lot of places to look for code; we should probably simplify the directory structure, but that's how it is.  If you're trying to work on operators, you'll spend most of your time in <tt>aten</tt>.</p>
<p>Let's see how this separation of code breaks down in practice:</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-31.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-31.png"></p></div>
<p>When you call a function like <tt>torch.add</tt>, what actually happens?  If you remember the discussion we had about dispatching, you already have the basic picture in your head:</p>
<ol>
<li>We have to translate from Python realm to the C++ realm (Python argument parsing)</li>
<li>We handle <strong>variable</strong> dispatch (VariableType--Type, by the way, doesn't really have anything to do programming language types, and is just a gadget for doing dispatch.)</li>
<li>We handle <strong>device type / layout</strong> dispatch (Type)</li>
<li>We have the actual kernel, which is either a modern native function, or a legacy TH function.</li>
</ol>
<p>Each of these steps corresponds concretely to some code.  Let's cut our way through the jungle.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-32.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-32.png"></p></div>
<p>Our initial landing point in the C++ code is the C implementation of a Python function, which we've exposed to the Python side as something like <tt>torch._C.VariableFunctions.add</tt>.  <tt>THPVariable_add</tt> is the implementation of one such implementation.</p>
<p>One important thing to know about this code is that it is auto-generated.  If you search in the GitHub repository, you won't find it, because you have to actually build PyTorch to see it.  Another important thing is, you don't have to really deeply understand what this code is doing; the idea is to skim over it and get a sense for what it is doing.  Above, I've annotated some of the most important bits in blue: you can see that there is a use of a class <tt>PythonArgParser</tt> to actually pull out C++ objects out of the Python <tt>args</tt> and <tt>kwargs</tt>; we then call a <tt>dispatch_add</tt> function (which I've inlined in red); this releases the global interpreter lock and then calls a plain old method on the C++ Tensor <tt>self</tt>.  On its way back, we rewrap the returned <tt>Tensor</tt> back into a <tt>PyObject</tt>.</p>
<p>(At this point, there's an error in the slides: I'm supposed to tell you about the Variable dispatch code.  I haven't fixed it here yet.  Some magic happens, then...)</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-33.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-33.png"></p></div>
<p>When we call the <tt>add</tt> method on the <tt>Tensor</tt> class, no virtual dispatch happens yet.  Instead, we have an inline method which calls a virtual method on a "Type" object.  This method is the actual virtual method (this is why I say Type is just a "gadget" that gets you dynamic dispatch.)  In the particular case of this example, this virtual call dispatches to an implementation of add on a class named <tt>TypeDefault</tt>.  This happens to be because we have an implementation of <tt>add</tt> that is the same for every device type (both CPU and CUDA); if we had happened to have different implementations, we might have instead landed on something like <tt><span>CPUFloatType::add</span></tt>.  It is this implementation of the virtual method that finally gets us to the actual kernel code.</p>
<!--  -->
<blockquote>
Hopefully, this slide will be out-of-date very soon too; Roy Li is working on replacing <tt>Type</tt> dispatch with another mechanism which will help us better support PyTorch on mobile.</blockquote>
<p>It's worth reemphasizing that all of the code, until we got to the kernel, is automatically generated.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-34.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-34.png"></p></div>
<p>It's a bit twisty and turny, so once you have some basic orientation about what's going on, I recommend just jumping straight to the kernels.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-35.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-35.png"></p></div>
<p>PyTorch offers a lot of useful tools for prospective kernel writers.  In this section, we'll walk through a few of them.  But first of all, what do you need to write a kernel?</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-36.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-36.png"></p></div>
<p>We generally think of a kernel in PyTorch consisting of the following parts:</p>
<ol>
<li>First, there's some metadata which we write about the kernel, which powers the code generation and lets you get all the bindings to Python, without having to write a single line of code.</li>
<li>Once you've gotten to the kernel, you're past the device type / layout dispatch. The first thing you need to write is error checking, to make sure the input tensors are the correct dimensions.  (Error checking is really important!  Don't skimp on it!)</li>
<li>Next, we generally have to allocate the result tensor which we are going to write the output into.</li>
<li>Time for the kernel proper.  At this point, you now should do the second, dtype dispatch, to jump into a kernel which is specialized per dtype it operates on.  (You don't want to do this too early, because then you will be uselessly duplicating code that looks the same in any case.)</li>
<li>Most performant kernels need some sort of parallelization, so that you can take advantage of multi-CPU systems.  (CUDA kernels are "implicitly" parallelized, since their programming model is built on top of massive parallelization).</li>
<li>Finally, you need to access the data and do the computation you wanted to do!</li>
</ol>
<p>In the subsequent slides, we'll walk through some of the tools PyTorch has for helping you implementing these steps.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-37.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-37.png"></p></div>
<p>To take advantage of all of the code generation which PyTorch brings, you need to write a <em>schema</em> for your operator.  The schema gives a mypy-esque type of your function, and also controls whether or not we generate bindings for methods or functions on Tensor.  You also tell the schema what implementations of your operator should be called for given device-layout combinations.  Check out the <a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md">README in native</a> is for more information about this format.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-38.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-38.png"></p></div>
<p>You also may need to define a derivative for your operation in <a href="https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml">derivatives.yaml</a>.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-39.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-39.png"></p></div>
<p>Error checking can be done by way of either a low level or a high level API.  The low level API is just a macro, <tt>TORCH_CHECK</tt>, which takes a boolean, and then any number of arguments to make up the error string to render if the boolean is not true.  One nice thing about this macro is that you can intermix strings with non-string data; everything is formatted using their implementation of <tt>operator&lt;&lt;</tt>, and most important data types in PyTorch have <tt>operator&lt;&lt;</tt> implementations.</p>
<p>The high level API saves you from having to write up repetitive error messages over and over again.  The way it works is you first wrap each <tt>Tensor</tt> into a <tt>TensorArg</tt>, which contains information about where the tensor came from (e.g., its argument name).  It then provides a number of pre-canned functions for checking various properties; e.g., <tt>checkDim()</tt> tests if the tensor's dimensionality is a fixed number.  If it's not, the function provides a user-friendly error message based on the <tt>TensorArg</tt> metadata.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-40.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-40.png"></p></div>
<p>One important thing to be aware about when writing operators in PyTorch, is that you are often signing up to write <em>three</em> operators: <tt>abs_out</tt>, which operates on a preallocated output (this implements the <tt>out=</tt> keyword argument), <tt>abs_</tt>, which operates inplace, and <tt>abs</tt>, which is the plain old functional version of an operator.</p>
<p>Most of the time, <tt>abs_out</tt> is the real workhorse, and <tt>abs</tt> and <tt>abs_</tt> are just thin wrappers around <tt>abs_out</tt>; but sometimes writing specialized implementations for each case are warranted.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-41.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-41.png"></p></div>
<p>To do dtype dispatch, you should use the <tt>AT_DISPATCH_ALL_TYPES</tt> macro.  This takes in the dtype of the tensor you want to dispatch over, and a lambda which will be specialized for each dtype that is dispatchable from the macro.  Usually, this lambda just calls a templated helper function.</p>
<p>This macro doesn't just "do dispatch", it also decides what dtypes your kernel will support.  As such, there are actually quite a few versions of this macro, which let you pick different subsets of dtypes to generate specializations for.  Most of the time, you'll just want <tt>AT_DISPATCH_ALL_TYPES</tt>, but keep an eye out for situations when you might want to dispatch to some more types.  There's guidance in <a href="https://github.com/pytorch/pytorch/blob/21ef4cc615a7d9d772ade52a5023900718b09e92/aten/src/ATen/Dispatch.h#L62">Dispatch.h</a> for how to select the correct one for your use-case.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-43.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-43.png"></p></div>
<p>On CPU, you frequently want to parallelize your code.  In the past, this was usually done by directly sprinkling OpenMP pragmas in your code.</p>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-42.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-42.png"></p></div>
<p>At some point, we have to actually access the data.  PyTorch offers quite a few options for doing this.</p>
<ol>
<li>If you just want to get a value at some specific location, you should use <tt>TensorAccessor</tt>.  A tensor accessor is like a tensor, but it hard codes the dimensionality and dtype of the tensor as template parameters.  When you retrieve an accessor like <tt>x.accessor&lt;float, <span>3&gt;();</span></tt>, we do a runtime test to make sure that the tensor really is this format; but after that, every access is unchecked.  Tensor accessors handle strides correctly, so you should prefer using them over raw pointer access (which, unfortunately, some legacy kernels do.)  There is also a <tt>PackedTensorAccessor</tt>, which is specifically useful for sending an accessor over a CUDA launch, so that you can get accessors from inside your CUDA kernel.  (One notable gotcha: <tt>TensorAccessor</tt> defaults to 64-bit indexing, which is much slower than 32-bit indexing in CUDA!)</li>
<li>If you're writing some sort of operator with very regular element access, for example, a pointwise operation, you are much better off using a higher level of abstraction, the <tt>TensorIterator</tt>.   This helper class automatically handles broadcasting and type promotion for you, and is quite handy.</li>
<li>For true speed on CPU, you may need to write your kernel using vectorized CPU instructions.  We've got helpers for that too!  The <tt>Vec256</tt> class represents a vector of scalars and provides a number of methods which perform vectorized operations on them all at once.  Helpers like <tt>binary_kernel_vec</tt> then let you easily run vectorized operations, and then finish everything that doesn't round nicely into vector instructions using plain old instructions.  The infrastructure here also manages compiling your kernel multiple times under different instruction sets, and then testing at runtime what instructions your CPU supports, and using the best kernel in those situations.</li>
</ol>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-44.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-44.png"></p></div>
<p>A lot of kernels in PyTorch are still written in the legacy TH style.  (By the way, TH stands for TorcH.  It's a pretty nice acronym, but unfortunately it is a bit poisoned; if you see TH in the name, assume that it's legacy.)  What do I mean by the legacy TH style?</p>
<ol>
<li>It's written in C style, no (or very little) use of C++.</li>
<li>It's manually refcounted (with manual calls to <tt>THTensor_free</tt> to decrease refcounts when you're done using tensors), and</li>
<li>It lives in <tt>generic/</tt> directory, which means that we are actually going to compile the file multiple times, but with different <tt>#define scalar_t</tt>.</li>
</ol>
<p>This code is pretty crazy, and we hate reviewing it, so please don't add to it.  One of the more useful tasks that you can do, if you like to code but don't know too much about kernel writing, is to port some of these TH functions to ATen.</p>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-45.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-45.png"></p></div>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-46.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-46.png"></p></div>
<p>To wrap up, I want to talk a little bit about working efficiently on PyTorch.  If the largeness of PyTorch's C++ codebase is the first gatekeeper that stops people from contributing to PyTorch, the efficiency of your workflow is the second gatekeeper.  If you try to work on C++ with Python habits, <strong>you will have a bad time</strong>: it will take forever to recompile PyTorch, and it will take you forever to tell if your changes worked or not.</p>
<p>How to work efficiently could probably be a talk in and of itself, but this slide calls out some of the most common anti-patterns I've seen when someone complains: "It's hard to work on PyTorch."</p>
<ol>
<li>If you edit a header, especially one that is included by many source files (and especially if it is included by CUDA files), expect a very long rebuild.  Try to stick to editing cpp files, and edit headers sparingly!</li>
<li>Our CI is a very wonderful, zero-setup way to test if your changes worked or not. But expect to wait an hour or two before you get back signal.  If you are working on a change that will require lots of experimentation, spend the time setting up a local development environment.  Similarly, if you run into a hard to debug problem on a specific CI configuration, set it up locally.  You can <a href="https://github.com/pytorch/ossci-job-dsl">download and run the Docker images locally</a></li>
<li>The <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache">CONTRIBUTING guide explains how to setup ccache</a>; this is highly recommended, because sometimes it will help you get lucky and avoid a massive recompile when you edit a header.  It also helps cover up bugs in our build system, when we recompile files when we shouldn't.</li>
<li>At the end of the day, we have a lot of C++ code, and you will have a much more pleasant experience if you build on a beefy server with CPUs and RAM.  In particular, I don't recommend doing CUDA builds on a laptop; building CUDA is sloooooow and laptops tend to not have enough juice to turnaround quickly enough.</li>
</ol>
<hr>
<div><p><img alt="http://blog.ezyang.com/img/pytorch-internals/slide-47.png" src="https://blog.ezyang.com/img/pytorch-internals/slide-47.png"></p></div>
<p>So that's it for a whirlwind tour of PyTorch's internals!  Many, many things have been omitted; but hopefully the descriptions and explanations here can help you get a grip on at least a substantial portion of the codebase.</p>
<p>Where should you go from here?  What kinds of contributions can you make?  A good place to start is our issue tracker.  Starting earlier this year, we have been triaging issues; issues labeled <strong>triaged</strong> mean that at least one PyTorch developer has looked at it and made an initial assessment about the issue.  You can use these labels to find out what issues we think are <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22high+priority%22+label%3Atriaged">high priority</a> or look up issues specific to some module, e.g., <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3A%22module%3A+autograd%22">autograd</a> or find issues which we think are <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3Atriaged+label%3Asmall">small</a> (word of warning: we're sometimes wrong!)</p>
<p>Even if you don't want to get started with coding right away, there are many other useful activities like improving documentation (I <em>love</em> merging documentation PRs, they are so great), helping us reproduce bug reports from other users, and also just helping us discuss RFCs on the issue tracker. PyTorch would not be where it is today without our open source contributors; we hope you can join us too!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Landrun: Sandbox any Linux process using Landlock, no root or containers (202 pts)]]></title>
            <link>https://github.com/Zouuup/landrun</link>
            <guid>43445662</guid>
            <pubDate>Sat, 22 Mar 2025 13:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Zouuup/landrun">https://github.com/Zouuup/landrun</a>, See on <a href="https://news.ycombinator.com/item?id=43445662">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">landrun</h2><a id="user-content-landrun" aria-label="Permalink: landrun" href="#landrun"></a></p>
<p dir="auto">A lightweight, secure sandbox for running Linux processes using Landlock LSM. Think firejail, but with kernel-level security and minimal overhead.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🔒 Kernel-level security using Landlock LSM</li>
<li>🚀 Lightweight and fast execution</li>
<li>🛡️ Fine-grained access control for directories</li>
<li>🔄 Support for read and write paths</li>
<li>⚡ Optional execution permissions for allowed paths</li>
<li>🌐 TCP network access control (binding and connecting)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Zouuup/landrun/blob/main/demo.gif"><img src="https://github.com/Zouuup/landrun/raw/main/demo.gif" alt="landrun demo" width="700" data-animated-image=""></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Linux kernel 5.13 or later with Landlock LSM enabled</li>
<li>Linux kernel 6.8 or later for network restrictions (TCP bind/connect)</li>
<li>Go 1.18 or later (for building from source)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Install</h3><a id="user-content-quick-install" aria-label="Permalink: Quick Install" href="#quick-install"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="go install github.com/zouuup/landrun/cmd/landrun@latest"><pre>go install github.com/zouuup/landrun/cmd/landrun@latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From Source</h3><a id="user-content-from-source" aria-label="Permalink: From Source" href="#from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/zouuup/landrun.git
cd landrun
go build -o landrun cmd/landrun/main.go
sudo cp landrun /usr/local/bin/"><pre>git clone https://github.com/zouuup/landrun.git
<span>cd</span> landrun
go build -o landrun cmd/landrun/main.go
sudo cp landrun /usr/local/bin/</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Basic syntax:</p>
<div dir="auto" data-snippet-clipboard-copy-content="landrun [options] <command> [args...]"><pre>landrun [options] <span>&lt;</span>command<span>&gt;</span> [args...]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Options</h3><a id="user-content-options" aria-label="Permalink: Options" href="#options"></a></p>
<ul dir="auto">
<li><code>--ro &lt;path&gt;</code>: Allow read-only access to specified path (can be specified multiple times)</li>
<li><code>--rw &lt;path&gt;</code>: Allow read-write access to specified path (can be specified multiple times)</li>
<li><code>--exec</code>: Allow executing files in allowed paths</li>
<li><code>--bind-tcp &lt;port&gt;</code>: Allow binding to specified TCP port (can be specified multiple times)</li>
<li><code>--connect-tcp &lt;port&gt;</code>: Allow connecting to specified TCP port (can be specified multiple times)</li>
<li><code>--best-effort</code>: Use best effort mode, falling back to less restrictive sandbox if necessary [default: enabled]</li>
<li><code>--log-level &lt;level&gt;</code>: Set logging level (error, info, debug) [default: "error"]</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Important Notes</h3><a id="user-content-important-notes" aria-label="Permalink: Important Notes" href="#important-notes"></a></p>
<ul dir="auto">
<li>You must explicitly add the path to the command you want to run with the <code>--ro</code> flag</li>
<li>For system commands, you typically need to include <code>/usr/bin</code>, <code>/usr/lib</code>, and other system directories</li>
<li>When using <code>--exec</code>, you still need to specify the directories containing executables with <code>--ro</code></li>
<li>Network restrictions require Linux kernel 6.8 or later with Landlock ABI v5</li>
<li>The <code>--best-effort</code> flag allows graceful degradation on older kernels that don't support all requested restrictions</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Environment Variables</h3><a id="user-content-environment-variables" aria-label="Permalink: Environment Variables" href="#environment-variables"></a></p>
<ul dir="auto">
<li><code>LANDRUN_LOG_LEVEL</code>: Set logging level (error, info, debug)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<ol dir="auto">
<li>Run a command with read-only access to a directory:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls /path/to/dir"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls /path/to/dir</pre></div>
<ol start="2" dir="auto">
<li>Run a command with write access to a directory:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --rw /path/to/dir touch /path/to/dir/newfile"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --rw /path/to/dir touch /path/to/dir/newfile</pre></div>
<ol start="3" dir="auto">
<li>Run a command with execution permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --exec /usr/bin/bash"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --exec /usr/bin/bash</pre></div>
<ol start="4" dir="auto">
<li>Run with debug logging:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --log-level debug --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls"><pre>landrun --log-level debug --ro /usr/bin --ro /lib --ro /lib64 --ro /path/to/dir ls</pre></div>
<ol start="5" dir="auto">
<li>Run with network restrictions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --bind-tcp 8080 --connect-tcp 53 /usr/bin/my-server"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --bind-tcp 8080 --connect-tcp 53 /usr/bin/my-server</pre></div>
<p dir="auto">This will allow the program to only bind to TCP port 8080 and connect to TCP port 53.</p>
<ol start="6" dir="auto">
<li>Run a DNS client with appropriate permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /etc/resolv.conf --connect-tcp 53 dig example.com"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /etc/resolv.conf --connect-tcp 53 dig example.com</pre></div>
<p dir="auto">This allows DNS resolution by granting access to /etc/resolv.conf and permitting connections to port 53 (DNS).</p>
<ol start="7" dir="auto">
<li>Run a web server with selective network permissions:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /var/www --rw /var/log --bind-tcp 80 --bind-tcp 443 /usr/bin/nginx"><pre>landrun --ro /usr/bin --ro /lib --ro /lib64 --ro /var/www --rw /var/log --bind-tcp 80 --bind-tcp 443 /usr/bin/nginx</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">landrun uses Linux's Landlock LSM to create a secure sandbox environment. It provides:</p>
<ul dir="auto">
<li>File system access control</li>
<li>Directory access restrictions</li>
<li>Execution control</li>
<li>TCP network restrictions</li>
<li>Process isolation</li>
</ul>
<p dir="auto">Landlock is an access-control system that enables processes to securely restrict themselves and their future children. As a stackable Linux Security Module (LSM), it creates additional security layers on top of existing system-wide access controls, helping to mitigate security impacts from bugs or malicious behavior in applications.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Landlock Access Control Rights</h3><a id="user-content-landlock-access-control-rights" aria-label="Permalink: Landlock Access Control Rights" href="#landlock-access-control-rights"></a></p>
<p dir="auto">landrun leverages Landlock's fine-grained access control mechanisms, which include:</p>
<p dir="auto"><strong>File-specific rights:</strong></p>
<ul dir="auto">
<li>Execute files (<code>LANDLOCK_ACCESS_FS_EXECUTE</code>)</li>
<li>Write to files (<code>LANDLOCK_ACCESS_FS_WRITE_FILE</code>)</li>
<li>Read files (<code>LANDLOCK_ACCESS_FS_READ_FILE</code>)</li>
<li>Truncate files (<code>LANDLOCK_ACCESS_FS_TRUNCATE</code>) - Available since Landlock ABI v3</li>
</ul>
<p dir="auto"><strong>Directory-specific rights:</strong></p>
<ul dir="auto">
<li>Read directory contents (<code>LANDLOCK_ACCESS_FS_READ_DIR</code>)</li>
<li>Remove directories (<code>LANDLOCK_ACCESS_FS_REMOVE_DIR</code>)</li>
<li>Remove files (<code>LANDLOCK_ACCESS_FS_REMOVE_FILE</code>)</li>
<li>Create various filesystem objects (char devices, directories, regular files, sockets, etc.)</li>
<li>Refer/reparent files across directories (<code>LANDLOCK_ACCESS_FS_REFER</code>) - Available since Landlock ABI v2</li>
</ul>
<p dir="auto"><strong>Network-specific rights</strong> (requires Linux 6.8+ with Landlock ABI v5):</p>
<ul dir="auto">
<li>Bind to specific TCP ports (<code>LANDLOCK_ACCESS_NET_BIND_TCP</code>)</li>
<li>Connect to specific TCP ports (<code>LANDLOCK_ACCESS_NET_CONNECT_TCP</code>)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Limitations</h3><a id="user-content-limitations" aria-label="Permalink: Limitations" href="#limitations"></a></p>
<ul dir="auto">
<li>Landlock must be supported by your kernel</li>
<li>Network restrictions require Linux kernel 6.8+ with Landlock ABI v5</li>
<li>Some operations may require additional permissions</li>
<li>Files or directories opened before sandboxing are not subject to Landlock restrictions</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Kernel Compatibility Table</h2><a id="user-content-kernel-compatibility-table" aria-label="Permalink: Kernel Compatibility Table" href="#kernel-compatibility-table"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>Minimum Kernel Version</th>
<th>Landlock ABI Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>Basic filesystem sandboxing</td>
<td>5.13</td>
<td>1</td>
</tr>
<tr>
<td>File referring/reparenting control</td>
<td>5.17</td>
<td>2</td>
</tr>
<tr>
<td>File truncation control</td>
<td>6.1</td>
<td>3</td>
</tr>
<tr>
<td>Network TCP restrictions</td>
<td>6.8</td>
<td>5</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">If you receive "permission denied" or similar errors:</p>
<ol dir="auto">
<li>Ensure you've added all necessary paths with <code>--ro</code> or <code>--rw</code></li>
<li>Try running with <code>--log-level debug</code> to see detailed permission information</li>
<li>Check that Landlock is supported and enabled on your system:
<div dir="auto" data-snippet-clipboard-copy-content="grep -E 'landlock|lsm=' /boot/config-$(uname -r)"><pre>grep -E <span><span>'</span>landlock|lsm=<span>'</span></span> /boot/config-<span><span>$(</span>uname -r<span>)</span></span></pre></div>
You should see <code>CONFIG_SECURITY_LANDLOCK=y</code> and <code>lsm=landlock,...</code> in the output</li>
<li>For network restrictions, verify your kernel version is 6.8+ with Landlock ABI v5:

</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technical Details</h2><a id="user-content-technical-details" aria-label="Permalink: Technical Details" href="#technical-details"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Implementation</h3><a id="user-content-implementation" aria-label="Permalink: Implementation" href="#implementation"></a></p>
<p dir="auto">This project uses the <code>landlock-lsm/go-landlock</code> package for sandboxing, which provides both filesystem and network restrictions. The current implementation supports:</p>
<ul dir="auto">
<li>Read/write/execute restrictions for files and directories</li>
<li>TCP port binding restrictions</li>
<li>TCP port connection restrictions</li>
<li>Best-effort mode for graceful degradation on older kernels</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Best-Effort Mode</h3><a id="user-content-best-effort-mode" aria-label="Permalink: Best-Effort Mode" href="#best-effort-mode"></a></p>
<p dir="auto">When using <code>--best-effort</code> (enabled by default), landrun will gracefully degrade to using the best available Landlock version on the current kernel. This means:</p>
<ul dir="auto">
<li>On Linux 6.8+: Full filesystem and network restrictions</li>
<li>On Linux 6.1-6.7: Filesystem restrictions including truncation, but no network restrictions</li>
<li>On Linux 5.17-6.0: Basic filesystem restrictions including file reparenting, but no truncation control or network restrictions</li>
<li>On Linux 5.13-5.16: Basic filesystem restrictions without file reparenting, truncation control, or network restrictions</li>
<li>On older Linux: No restrictions (sandbox disabled)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future Features</h2><a id="user-content-future-features" aria-label="Permalink: Future Features" href="#future-features"></a></p>
<p dir="auto">Based on the Linux Landlock API capabilities, we plan to add:</p>
<ul dir="auto">
<li>🔒 Enhanced filesystem controls with more fine-grained permissions</li>
<li>🌐 Support for UDP and other network protocol restrictions (when supported by Linux kernel)</li>
<li>🔄 Process scoping and resource controls</li>
<li>🛡️ Additional security features as they become available in the Landlock API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the GNU General Public License v2</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When you deleted /lib on Linux while still connected via SSH (2022) (114 pts)]]></title>
            <link>https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/</link>
            <guid>43444160</guid>
            <pubDate>Sat, 22 Mar 2025 07:24:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/">https://tinyhack.com/2022/09/16/when-you-deleted-lib-on-linux-while-still-connected-via-ssh/</a>, See on <a href="https://news.ycombinator.com/item?id=43444160">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Scallop – A Language for Neurosymbolic Programming (175 pts)]]></title>
            <link>https://www.scallop-lang.org/</link>
            <guid>43443640</guid>
            <pubDate>Sat, 22 Mar 2025 04:45:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.scallop-lang.org/">https://www.scallop-lang.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43443640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <div>
          <div>
                <p><img src="https://www.scallop-lang.org/img/icon-language.png"></p><h2>Language</h2>
                <p>
                  Scallop is a declarative language designed to support rich symbolic reasoning in AI applications.
		  It is based on Datalog, a logic rule-based query language for relational databases.
                </p>
              </div>
          <div>
                <p><img src="https://www.scallop-lang.org/img/icon-solver.png"></p><h2>Solver</h2>
                <p>
                  Scallop is a scalable Datalog solver equipped with support for discrete, probabilistic, and
                  differentiable modes of reasoning.
                  These modes are configurable to suit the needs of different AI applications.
                </p>
              </div>
          <div>
                <p><img src="https://www.scallop-lang.org/img/icon-framework.png"></p><h2>Framework</h2>
                <p>
                  Scallop provides bindings to support logic reasoning modules within Python programs.
                  As a result, Scallop can be deeply integrated with existing PyTorch machine
                  learning pipelines.
                </p>
              </div>
        </div>
      <div>
          <h2>Wide Range of Applications</h2>
          <p>
              Scallop can be used to develop a wide variety of applications in vision and NLP that involve symbolic reasoning.
              The reasoning component is specified via logic rules which can then be deeply
              integrated with machine learning models, such as convolutional neural networks and transformers.
            </p>
        </div>
      
      
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[George Foreman, Boxer Turned Foreman Grill Infomercial Star, Dies at 76 (299 pts)]]></title>
            <link>https://variety.com/2025/tv/news/george-foreman-boxer-infomercial-star-dies-1236345523/</link>
            <guid>43442917</guid>
            <pubDate>Sat, 22 Mar 2025 02:56:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://variety.com/2025/tv/news/george-foreman-boxer-infomercial-star-dies-1236345523/">https://variety.com/2025/tv/news/george-foreman-boxer-infomercial-star-dies-1236345523/</a>, See on <a href="https://news.ycombinator.com/item?id=43442917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
	<a href="https://variety.com/t/george-foreman/" id="auto-tag_george-foreman" data-tag="george-foreman">George Foreman</a>, the charismatic boxer turned infomercial star who had a retail hit with his Foreman Grill product line, died Friday. He was 76.</p>



<p>
	The Texas-born Foreman became Heavyweight Champion of the World, and segued into a TV staple and pop culture icon. He was swept up in the swirl of decade-defining events surrounding Muhammad Ali as well as Joe Frazier and other high-wattage pugilists of the 1970s. In the 1990s, Foreman took advantage of the availablity of low-cost TV time to launch his Foreman Grill home grill product through a series of  infomercials that he hosted. </p>



<p>
	Foreman famously had a close call in the ring in 1977 that drove him to quit boxing and declare himself a born-again Christian. He became an ordained minister in 1978 and began preaching in his hometown of Houston. He shocked the sports world when he returned to boxing in 1987 and wound up reclaiming his Heavyweight Champion title in 1994. Foreman retired from the sweet science for good in 1997.

	</p>






<p>
	In addition to his business ventures, Foreman led Houston’s Church of the Lord Jesus Christ, where he preached four times a week.


</p><div data-pmc-adm-ad-id="1234758890">
			<h3>
			Popular on Variety		</h3>
	
		
	</div>




<p>
	In recent years, Foreman had been involved with numerous documentary projects about his life, boxing and the era of his greatest fame. He was also the subject of the 2023 biopic “Big George Foreman,” from director George Tillman Jr. Khris Davis played Foreman in the Mandalay Pictures drama that focused on his improbable return to the ring in the 1980s and ’90s.</p>



<p>
	Foreman’s family confirmed his death in an Instagram post on Friday.</p>



<figure><div>
<blockquote data-instgrm-captioned="" data-instgrm-permalink="https://www.instagram.com/p/DHe5UtIJ4IQ/?utm_source=ig_embed&amp;utm_campaign=loading" data-instgrm-version="14"></blockquote>
</div></figure>



<p>
	Born Jan. 10, 1949, Foreman grew up in extreme poverity in the east Texas city of Marshall, about 40 miles west of Shreveport, La. He first gained national fame after winning an Olympic gold medal in boxing at the 1968 Summer Games in Mexico City.</p>



<p>
	“Foreman often bullied younger children and didn’t like getting up early for school. Foreman became a mugger and brawler on the hard streets of Houston’s Fifth Ward by age 15,” according to <a href="https://www.georgeforeman.com/pages/biography" rel="nofollow" target="_blank"><strong>Foreman’s official website.</strong></a></p>



<p>
	He was eventually steered into boxing through the Lone Star state’s Lyndon B. Johnson Job Corps program. Foreman gained stature in the late 1960s and ultimately secured the Heavyweight Championship in January 1973 by defeating Frazier with six knockouts in a bout held in Kingston, Jamaica. The event also had the distinction of being the first boxing broadcast to air on the then-fledgling pay TV service HBO.

	</p>




<p>
	The following year, Foreman faced a resurgent Ali in the event that received worldwide attention as the “Rumble in the Jungle,” held in what is now the Democratic Republic of Congo. Ali pummelled Foreman in the ring and dominated him on the PR front as well. Foreman went on to went his next five fights by knockout.</p>



<p>
	After his triumph of becoming the world’s oldest Heavyweight Champion, Foreman became a boldface name staple on TV, from daytime talk shows to “The Tonight Show” and “Late Night With David Letterman.” He was known for his folksy charm and for having a sprawling family of children and grandchildren. And his low-cost cooking device that allowed for easy indoor grilling — the George Foreman Lean Mean Grilling Machine — became a retail and direct response sales juggernaut starting in the early 1990s.</p>



<p>
	Foreman also starred in the short-lived 1993 ABC family comedy “George,” playing a retired boxer who runs an after-school program for troubled students. He hosted NBC’s “Saturday Night Live” in 1994.</p>



<p>
	Foreman had cameos and small roles in a host of TV shows and movies over the years, playing himself or a similar character, including “Night at the Museum: Battle of the Smithsonian,” “The Fighter,” “The Masked Singer,” “The Larry Sanders Show,” “Home Improvement” and “King of the Hill.”</p>
















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Monster Cables picked the wrong guy to threaten (2008) (498 pts)]]></title>
            <link>https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/</link>
            <guid>43442178</guid>
            <pubDate>Sat, 22 Mar 2025 00:30:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/">https://www.oncontracts.com/monster-cables-picked-the-wrong-guy-to-threaten/</a>, See on <a href="https://news.ycombinator.com/item?id=43442178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody" id="post-113" itemscope="" itemtype="https://schema.org/BlogPosting"><p>Monster Cables, which makes extremely high-priced stereo cables, has apparently sent a <a href="http://www.audioholics.com/news/industry-news/monster-sues-blue-jeans-cable" target="_blank">cease-and-desist letter</a> to <a href="http://www.bluejeanscable.com/">Blue Jeans Cable</a>, alleging various kinds of infringement.  Bad move – the president of Blue Jeans Cable, Kurt Denke, is a former litigator who <a href="http://www.audioholics.com/news/industry-news/blue-jeans-strikes-back">responded pretty forcefully</a>:</p><blockquote><p><em>… Once I have received the above materials and explanations from you, I will undertake to analyze this information and let you know whether we are willing to accede to any of the demands made in your letter. <strong>If my analysis shows that there is any reasonable likelihood that we have infringed in any way any of Monster Cable’s intellectual property rights, we will of course take any and all action necessary to resolve the situation. </strong> If I do not hear from you within the next fourteen days, or if I do hear from you but do not receive </em><em>all of the information requested above, I will assume that you have abandoned these claims and closed your file.</em></p><p><em> As for your requests for information, or for action, directed to me: I would remind you that it is you, not I, who are making claims; and it is you, not I, who must substantiate those claims.  You have not done so.</em></p><p><em> I have seen Monster Cable take untenable IP positions in various different scenarios in the past, and am generally familiar with what seems to be Monster Cable’s </em><em>modus operandi in these matters.  I therefore think that it is important that, before closing, I make you aware of a few points.</em></p><p><em> After graduating from the University of Pennsylvania Law School in 1985, I spent nineteen years in litigation practice, with a focus upon federal litigation involving large damages and complex issues.  My first seven years were spent primarily on the defense side, where <strong>I developed an intense frustration with insurance carriers who would settle meritless claims for nuisance value when the better long-term view would have been to fight against vexatious litigation as a matter of principle.</strong> In plaintiffs’ practice, likewise, I was always a strong advocate of standing upon principle and taking cases all the way to judgment, even when substantial offers of settlement were on the table.  I am “uncompromising” in the most literal sense of the word.  If Monster Cable proceeds with litigation against me I will pursue the same merits-driven approach; I do not compromise with bullies and <strong>I would rather spend fifty thousand dollars on defense than give you a dollar of unmerited settlement funds.</strong> As for signing a licensing agreement for intellectual property which I have not infringed: that will not happen, under any circumstances, whether it makes economic sense or not.</em></p><p><em> I say this because my observation has been that Monster Cable typically operates in a hit-and-run fashion.  Your client threatens litigation, expecting the victim to panic and plead for mercy; and what follows is a quickie negotiation session that ends with payment and a licensing agreement.  Your client then uses this collection of licensing agreements to convince others under similar threat to accede to its demands.  Let me be clear about this: <strong>there are only two ways for you to get anything out of me.  You will either need to (1) convince me that I have infringed, or (2) obtain a final judgment to that effect from a court of competent jurisdiction. </strong>It may be that my inability to see the pragmatic value of settling frivolous claims is a deep character flaw, and I am sure a few of the insurance carriers for whom I have done work have seen it that way; but it is how I have done business for the last quarter-century and you are not going to change my mind.  If you sue me, the case will go to judgment, and I will hold the court’s attention upon the merits of your claims–or, to speak more precisely, the absence of merit from your claims–from start to finish. <strong>Not only am I unintimidated by litigation; I sometimes rather miss it.</strong></em></p></blockquote><p>(Emphasis added; hat tip: Jeff Nolan at <a href="http://jeffnolan.com/wp/2008/04/15/monster-cables-gets-soundly-beaten/#comment-258095">Venture Chronicles</a>.)</p><p>I can relate to Denke’s final comment quoted above ….  I wonder what the attendant publicity is doing for his sales.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Not OK Cupid – A story of poor email address validation (113 pts)]]></title>
            <link>https://www.fastmail.com/blog/not-ok-cupid/</link>
            <guid>43441961</guid>
            <pubDate>Fri, 21 Mar 2025 23:54:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fastmail.com/blog/not-ok-cupid/">https://www.fastmail.com/blog/not-ok-cupid/</a>, See on <a href="https://news.ycombinator.com/item?id=43441961">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pagefind-body="" data-cms-edit="content"> <p>I don’t usually like to call out the bad behaviour of specific companies, but the egregious mis-design and lack of acknowledging it justify this case.</p> <h2 id="welcome-to-ok-cupid" tabindex="-1">Welcome to OkCupid</h2> <p>A couple of weeks ago, I started seeing many “Welcome to OkCupid” emails, both on my personal address and a couple of related addresses, but also to multiple Fastmail official contact addresses — legal, partnerships, press, etc. Specifically, this list included <code>trash@brong.net</code> — an address that has never been used to send or receive email and appears in precisely one place — <a href="https://www.fastmail.com/blog/a-tangled-path-of-workarounds/" target="_blank" rel="noopener">an article on our blog</a>! It seems quite clear that somebody scraped our website and used the addresses to sign up. I’m aware of at least 10 addresses, but there are likely others that either go to someone else or addresses that no longer exist.</p> <p>It didn’t stop there, though. I’ve been getting tons of “someone likes you”, “you have an intro,” and even an “IMPORTANT: We removed your photo on OkCupid.” email saying that inappropriate content was posted to “our” account!</p> <h2 id="the-real-world-consequences-of-poor-email-validation" tabindex="-1">The real-world consequences of poor email validation</h2> <p>This isn’t just an inconvenience — it has real security implications. Websites that fail to properly validate email ownership can be exploited for malicious purposes. Attackers can use unverified sign-ups to flood inboxes, making it easier to hide critical emails among the noise — something we’ve discussed our own experience of in our post on <a href="https://www.fastmail.com/blog/when-two-factor-authentication-is-not-enough/" target="_blank" rel="noopener">2FA vulnerabilities</a>. There are established <a href="https://www.m3aawg.org/sites/default/files/document/M3AAWG_Senders_BCP_Ver3-2015-02.pdf" target="_blank" rel="noopener">best practices</a> (PDF) for handling email sign-ups responsibly, practices that OkCupid is failing to follow.</p> <h2 id="no-way-out" tabindex="-1">No way out</h2> <p>When I tried to unsubscribe using the one-click unsubscribe button in one of the emails, I was met with an error: “Something went wrong, please try again later.”</p> <p>Curious, I tried to recover a password on one of these accounts (the one with my personal email address) and successfully changed the password. Then, I was asked to confirm my login with a message sent to the number associated with the account. A number I didn’t know. A number that wasn’t mentioned on that page, so I still don’t know anything about it — not even which country it was from.</p> <p>This raises further security concerns; the attacker could have also caused random recovery numbers to be texted to another poor victim’s phone. Alternatively, they could confirm that my email address is actively monitored, increasing its value for further attacks. Either way, what I couldn’t do was actually close the account.</p> <h2 id="whack-a-mole" tabindex="-1">Whack-a-mole</h2> <p>So, I contacted OkCupid’s support. Here’s what they said:</p> <p><em>I’ve removed the user from the site and banned the email address to prevent any new accounts from being created. That should resolve the issue, but if you encounter anything like this again in the future, please don’t hesitate to reach out, and we’ll address it right away.</em></p> <p>So, I need to contact support manually for each new email address. This is neither scalable nor acceptable; people don’t have this amount of time.</p> <p>Furthermore, my email address is now on another random blocklist somewhere on the internet, where I have no control and no way to unblock it. I don’t anticipate wanting to use OkCupid’s service, but if I did in the future, I would have to go through another dance to get the address unlocked again — or more likely, treat that particular email address as soiled and create another one.</p> <h2 id="not-ok" tabindex="-1">Not OK</h2> <p>So I say, not OK, OkCupid. Not OK.</p> <p>The usefulness of email depends on responsible behaviour from all service providers. Companies that engage in shady or outright inappropriate practices make the internet worse for everyone.</p> <p>OkCupid’s failure to implement even the <a href="https://en.wikipedia.org/wiki/Opt-in_email#Confirmed_opt-in_(COI)/double_opt-in_(DOI)" target="_blank" rel="noopener">simplest form</a> of email validation is unacceptable. Until they address these issues properly (not through the support response provided here), they remain part of the problem, not the solution.</p> <h2 id="could-we-have-avoided-this" tabindex="-1">Could we have avoided this?</h2> <p>In this case, we published those addresses online. There’s always a risk of receiving spam when you do that, one could even reasonably say “we were asking for it”. We expected spam. If you want to reduce your risk of being spammed, it helps to not publish your email address on the public web!</p> <p>What we we didn’t was expect a relatively reputable service being used to facilitate us being spammed.</p> <p>One great protection is using different address for each different organisation you deal with — that way if your address leaks (or they sell it), you know where the breach happened, and you can more easily block just the problem messages.</p> <p>Fastmail’s masked email feature is a great way to implement this strategy. Masked emails are designed, particularly when integrated with a password manager, to make it very easy to create new addresses, and track where they are expected to be used.</p> <p>Being a good internet citizen is one of <a href="https://www.fastmail.com/company/values/" target="_blank" rel="noopener">Fastmail’s core values</a>. We require verification for sending identities, ensuring that only legitimate users can send from an address they claim they own. This is the level of responsibility every email provider should uphold, and we applaud the others who also do.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EFF Border Search Pocket Guide (120 pts)]]></title>
            <link>https://www.eff.org/document/eff-border-search-pocket-guide</link>
            <guid>43441895</guid>
            <pubDate>Fri, 21 Mar 2025 23:41:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/document/eff-border-search-pocket-guide">https://www.eff.org/document/eff-border-search-pocket-guide</a>, See on <a href="https://news.ycombinator.com/item?id=43441895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>
            <h2>EFF Border Search Pocket Guide</h2>
    </p>
<div><div><p>This is a handy guide designed to be printed, folded, and carried in your pocket while traveling.</p></div>

<div><p><span><img alt="PDF icon" title="application/pdf" src="https://www.eff.org/modules/file/icons/application-pdf.png"> <a href="https://www.eff.org/files/2018/01/11/border-pocket-guide-2.pdf" type="application/pdf; length=799017">border-pocket-guide-2.pdf</a></span></p></div>
</div>
      </div></div>]]></description>
        </item>
    </channel>
</rss>