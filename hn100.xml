<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 20 Jul 2023 17:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[JPEG XL: How It Started, How It’s Going (138 pts)]]></title>
            <link>https://cloudinary.com/blog/jpeg-xl-how-it-started-how-its-going</link>
            <guid>36801448</guid>
            <pubDate>Thu, 20 Jul 2023 14:57:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloudinary.com/blog/jpeg-xl-how-it-started-how-its-going">https://cloudinary.com/blog/jpeg-xl-how-it-started-how-its-going</a>, See on <a href="https://news.ycombinator.com/item?id=36801448">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
<p>Last month at the annual Worldwide Developers Conference, Apple announced its support of JPEG XL. As someone who helped create JPEG XL, I don’t think anyone was more pleased than me to hear this news.</p>



<p>For a standard that’s not even three years old, this was a major win. Or rather, it’s a win for the web community, for photographers and artists, as well as for those of us who created the codec.</p>



<p>Before diving into what the Apple announcement means for JPEG XL moving forward, let’s take a quick look at its origin.</p>








<p>JPEG XL development began in 2018, when the JPEG committee launched a call for proposals on next-generation image compression, to which seven proposals were submitted. Of the seven two stood out: Google’s PIK and Cloudinary’s FUIF proposal. The ingredients from both proposals were eventually combined and refined to design a new codec that was better than the sum of its parts.</p>



<p>By the end of 2020, the main technical work was done and the bitstream was frozen, i.e., no more changes would be made that would change the format from the decoder point of view.</p>



<p>In November 2020 Cloudinary added JXL support and early in 2021, I wrote the blog post <a href="https://cloudinary.com/blog/time_for_next_gen_codecs_to_dethrone_jpeg" target="_blank" rel="noreferrer noopener">Time for Next-Gen Codecs to Dethrone JPEG</a>. In the piece I argued that modern codecs like JPEG XL can bring many benefits, and expressed the hope that they would be widely adopted.</p>



<p>In early April 2021, the Chrome browser added experimental support (behind a flag), even before the JPEG XL standard was officially published. (The final draft had been submitted to ISO, but it would still take until March 2022 before it was approved and published as the international standard ISO/IEC 18181.) Firefox followed suit quickly and added experimental support. Things were looking good.</p>



<p>Then, on Halloween 2022, Chrome developers suddenly announced that they would be removing JPEG XL support. This decision was quite unexpected and controversial. In my blog <a href="https://cloudinary.com/blog/the-case-for-jpeg-xl" target="_blank" rel="noreferrer noopener">The Case for JPEG XL</a>, I argued why this decision should be reversed. In December, Chrome developers provided test results that were used to justify the decision and invited feedback. I analyzed the results and <a href="https://cloudinary.com/blog/contemplating-codec-comparisons" target="_blank" rel="noreferrer noopener">pointed out</a> several methodological flaws and oversights. So far, my feedback has been ignored.</p>



<p>Beyond browsers, adoption of JPEG XL continued, in particular in image authoring software like Serif Affinity, Adobe Camera Raw, GIMP, Krita, etc. Unfortunately, Chrome’s decision has slowed wider adoption in web browsers of JPEG XL.</p>








<p>On June 5, 2023, at Apple’s annual Worldwide Developers Conference (WWDC23), a slide was presented listing new features of the Safari browser, and “JPEG XL” was on the slide. Not only is Safari 17 adding JPEG XL support, the new versions of iOS, iPadOS, macOS, watchOS, and visionOS will support JPEG XL.</p>



<figure><img width="1999" height="1124" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1999,h_1124/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA" alt="speaker presenting new Safari browser features, with &quot;JPEG XL&quot; listed" data-public-id="Web_Assets/blog/JPEG-XL_2.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1689095239" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 1999w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689095239/Web_Assets/blog/JPEG-XL_2/JPEG-XL_2-png?_i=AA 1536w" sizes="(max-width: 1999px) 100vw, 1999px"></figure>



<p>This was unexpectedly good news. None of the JPEG XL developers had anticipated that the first browser to support this format that was co-created by Google, would be Safari. This is, of course, great news for the adoption of JPEG XL and leads me to think the momentum is back!</p>








<p>Image compression is a crucial part of what we do at Cloudinary, and for that we need a deep understanding of when to use what codec — or rather, what encoder and settings. For that reason, we did a large experiment to create the Cloudinary Image Dataset (CID22), a big dataset of human-annotated compressed images. This helps us to better understand the impact compression has on perceptual quality.</p>



<p>Comparing image encoders is not an easy task. The distortions caused by different encoders are different and it is hard to predict the perceived image quality. Subjective testing is still the gold standard of image quality assessment and the testing methodology is still an active area of research, in which Cloudinary participates. (For example, in the AIC-3 project of the JPEG committee.)</p>



<p>Based on datasets of human-annotated images, objective metrics like SSIMULACRA 2 can be validated. While not perfect, metrics can be used at scale to test many more images and encoder configurations than what would be feasible in a subjective experiment.</p>



<p>In image compression, there is a trade-off to be made between three things:</p>



<ul>
<li><strong>Compression.</strong> How much can the file size be reduced?</li>



<li><strong>Quality. </strong>To what extent is the image degraded in visual quality?</li>



<li><strong>Speed. </strong>How long does it take to encode an image?</li>
</ul>



<p>The importance of these elements is relative to one another and depends on the specific use case. For the same visual quality, we can look at the performance of various encoders in terms of compression and speed. We use the SSIMULACRA v2.1 metric as an indicator of visual quality on the horizontal axis. On the vertical axis, we have the compression performance (as a percentage saving compared to unoptimized 4:4:4 JPEG) on the main plot on the left hand side. On the plot on the right hand side we have encoding speed expressed in megapixels per second (when using a single cpu core). The plot shows JPEG XL (red), AVIF/libaom (yellow), WebP (green) and mozjpeg (white), each at their default speed setting. The dotted lines use 4:2:0 chroma subsampling (yuv420) while the full lines use 4:4:4 (yuv444). The range goes from a very low quality (around q30 in libjpeg terms) to a very high quality (around q95 in libjpeg terms).</p>



<figure><img width="1999" height="1347" decoding="async" loading="lazy" src="https://res.cloudinary.com/cloudinary-marketing/images/w_1999,h_1347/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA" alt="graph showing compression, quality, and encode speed trade-offs according to SSIMULACRA_2.1" data-public-id="Web_Assets/blog/JPEG-XL_1.png" data-format="png" data-transformations="f_auto,q_auto" data-version="1689094929" srcset="https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 1999w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 300w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 768w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 1024w, https://res.cloudinary.com/cloudinary-marketing/images/f_auto,q_auto/v1689094929/Web_Assets/blog/JPEG-XL_1/JPEG-XL_1-png?_i=AA 1536w" sizes="(max-width: 1999px) 100vw, 1999px"><figcaption><em>Compression gains and encode speed for JPEG XL (red), AVIF (yellow), WebP (green) and mozjpeg (white), compared to unoptimized JPEG</em></figcaption></figure>



<p>The plot shows aggregated data corresponding to about 300 different source images. Some conclusions:</p>



<ul>
<li>WebP does bring a significant gain compared to unoptimized JPEG: a saving of 25 to 35% at the lower end of the quality spectrum, though at the higher end the savings diminish, mostly because WebP’s forced limited-range yuv420 does become a limiting factor (at q90+, it struggles to even reach the quality of JPEG, which can do full-range yuv444).</li>



<li>Compared to the optimized mozjpeg encoder though, which also brings significant gains compared to unoptimized JPEG, the additional gains WebP brings are not as large: only about 3 to 5% additional savings.</li>



<li>AVIF does bring a significant additional saving over WebP of about 10 to 15%. Moreover, since AVIF does not have forced yuv420, it can still bring significant savings over unoptimized JPEG even in the high-quality range, unlike WebP. However, these savings do come at a cost in speed: AVIF encoding at default speed is an order of magnitude slower than WebP or mozjpeg.</li>



<li>JPEG XL brings an additional saving over AVIF of about 5 to 10%. The additional saving is larger in the high-quality range than in the lower-quality range. Moreover, it does this at a more reasonable speed.</li>
</ul>








<p>Because Cloudinary was involved in the development of JPEG XL, we were the first to provide support. To convert an image to JXL with Cloudinary, simply add <code>f_jxl</code> to the url or change its extension to <code>.jxl</code>. Especially if you have significant traffic from visitors using iOS or Safari, it will likely be worthwhile to serve JPEG XL images (as soon as the new iOS and Safari have landed), even if you still need AVIF, WebP, or JPEG fallbacks for other visitors.</p>



<p>While in terms of average compression performance, JPEG XL is currently the best codec available, there is a lot of image-dependent variation. For some images, JPEG XL is a lot better than the other codecs, while for other images, AVIF is better than JPEG XL. For this reason, we are currently working on an AI-powered new version of our <code>f_auto,q_auto</code> feature, which can automatically select the optimal format to use on an image-by-image basis.</p>



<p>Have any questions or want to discuss the topic of this blog in greater detail? The <a href="https://community.cloudinary.com/" target="_blank" rel="noreferrer noopener">Cloudinary Community</a> is a great place to share your questions, ideas, and suggestions. You can also ping me on the <a href="https://discord.gg/cloudinary" target="_blank" rel="noreferrer noopener">Cloudinary Community Discord server</a> here: Jon: @_wb_</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple says it'll remove iMessage and FaceTime in UK rather than break encryption (819 pts)]]></title>
            <link>https://9to5mac.com/2023/07/20/apple-imessage-facetime-remove-security-law/</link>
            <guid>36800297</guid>
            <pubDate>Thu, 20 Jul 2023 13:41:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5mac.com/2023/07/20/apple-imessage-facetime-remove-security-law/">https://9to5mac.com/2023/07/20/apple-imessage-facetime-remove-security-law/</a>, See on <a href="https://news.ycombinator.com/item?id=36800297">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=1600" srcset="https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2022/06/iMessage-edit-victim.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1600" height="800" alt="iMessage" fetchpriority="high">
	
	</figure>

<p>Facing possible legislation that would require messaging services to offer backdoors in end-to-end encryption, Apple is saying it would rather remove apps like iMessage and FaceTime entirely from the UK market (<a href="https://www.bbc.co.uk/news/technology-66256081">via BBC News</a>).</p>



<p>The new Online Safety Bill is currently under review. Apple, WhatsApp, Signal, and other services have voiced their opposition to the proposal.</p>



<p>The UK government <a href="https://9to5mac.com/2023/06/27/apple-online-safety-bill-encryption/">wants the ability to scan end-to-end encrypted messages</a>, for child-abuse material and other illegal content. They argue the existing law accommodates this but is technically outdated by the security provisions of modern technology.</p>



<p>Apple has submitted a nine-page opposition to the planned bill. It strongly objects to requirements such as backdoors for end-to-end encryption, reporting changes to product security features before they are released and being forced to disable security features before an appeals process can take place.</p>



<p>The company said it would not make changes for one country that would weaken security for all of its users, threatening instead to disable iMessage and FaceTime for UK customers.</p>



<p>The proposed law is currently undergoing an eight-week consultation period. Obviously, Apple and others hope the government will revise the bill in response to the criticism.</p>



<p><a href="https://9to5mac.com/2022/12/07/apple-confirms-that-it-has-stopped-plans-to-roll-out-csam-detection-system/">Apple previously withdrew plans</a> for its own CSAM-scanning feature for iCloud Photos, following pushback from customers and human rights groups. Apple’s solution was more privacy-preserving than what is now proposed by the UK government.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Mac to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3PWlVJb"><img src="https://9to5mac.com/wp-content/uploads/sites/6/2023/07/PRBanner750x1501-1.png" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Accidentally Load Bearing (185 pts)]]></title>
            <link>https://www.jefftk.com/p/accidentally-load-bearing</link>
            <guid>36800151</guid>
            <pubDate>Thu, 20 Jul 2023 13:29:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jefftk.com/p/accidentally-load-bearing">https://www.jefftk.com/p/accidentally-load-bearing</a>, See on <a href="https://news.ycombinator.com/item?id=36800151">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<div>

    <p><span>

Sometimes people will talk about Chesterton's Fence, the idea that if
you want to change something—removing an apparently useless
fence—you should first determine why it was set up that way:

</span></p>
<blockquote>

The gate or fence did not grow there. It was not set up by
somnambulists who built it in their sleep. It is highly improbable
that it was put there by escaped lunatics who were for some reason
loose in the street. Some person had some reason for thinking it would
be a good thing for somebody. And until we know what the reason was,
we really cannot judge whether the reason was reasonable. It is
extremely probable that we have overlooked some whole aspect of the
question, if something set up by human beings like ourselves seems to
be entirely meaningless and mysterious. — <i>G. K. Chesterton, The
Drift From Domesticity</i>

</blockquote>

<p>

Figuring out something's designed purpose can be helpful in evaluating
changes, but a risk is that it puts you in a frame of mind where what
matters is the role the original builders intended.

</p>
<p>

A few years ago I was <a href="https://www.jefftk.com/p/bathroom-construction-framing">rebuilding a bathroom in our
house</a>, and there was a vertical stud that was in the way.  I could
easily tell why it was there: it was part of a partition for a closet.
And since I knew its designed purpose and no longer needed it for
that anymore, the Chesterton's Fence framing would suggest that it was
fine to remove it.  Except that over time it had become accidentally
load bearing: through other (ill conceived) changes to the structure
this stud was now helping hold up the second floor of the house.  In
addition to considering why something was created, you also need to
consider what additional purposes it may have since come to serve.

</p>
<p>

This is a concept I've run into a lot when making changes to complex
computer systems.  It's useful to look back through the change
history, read original design documents, and understand why a
component was built the way it was.  But you also need to look closely
at how the component integrates into the system today, where it can
easily have taken on additional roles.

  </p>
</div>


  
<p>Comment via: <a href="https://www.facebook.com/jefftk/posts/pfbid02i81EvJpGSRdnfgT8kbtgPtFKJ4t9c4uYCnNvwmZiCyVvYLbcVKkshj9YkoC1at3Zl">facebook</a>, <a href="https://lesswrong.com/posts/QBeN49SoKpDMX3kKk">lesswrong</a>, <a href="https://news.ycombinator.com/item?id=36800151">hacker news</a>, <a href="https://mastodon.mit.edu/@jefftk/110707644099272630">mastodon</a></p>

</div><section>
  <h3>Recent posts on blogs I like:</h3>
  <section>
    
    <div>
      <h4>
        <a href="https://www.lilywise.com/fiddle">Fiddle</a>
      </h4>
      <p>
  

I first started playing fiddle when I was five, just around my
birthday.  I had really wanted a fiddle because I wanted to learn how
to play it and my parents got me one for my birthday so I started
taking lessons.  Though after a couple of lessons I start…</p>
      <p><small>
        via <a href="https://www.lilywise.com/">Lily Wise's Blog Posts</a>
      </small>
      <small>July 13, 2023</small>
    </p></div>
    
    <div>
      <h4>
        <a href="https://juliawise.net/the-best-kind-of-music-to-learn-is-social-music/">The best kind of music to learn is social music</a>
      </h4>
      <p>Will they ever stay up late playing this kind of music for fun?
The post The best kind of music to learn is social music appeared first on Otherwise.
</p>
      <p><small>
        via <a href="https://juliawise.net/">Otherwise</a>
      </small>
      <small>July 1, 2023</small>
    </p></div>
    
    <div>
      <h4>
        <a href="https://lincolnquirk.com/2023/07/01/altruist_perks.html">Why altruists can’t have nice things</a>
      </h4>
      <p>I posted this on the Effective Altruism forum as part of the EA Strategy Fortnight. I’m cross posting it here.</p>
      <p><small>
        via <a href="https://lincolnquirk.com/">Home</a>
      </small>
      <small>July 1, 2023</small>
    </p></div>
    
  </section>
  <p>
    <a href="https://www.jefftk.com/ring">more</a>
    &nbsp;&nbsp;&nbsp;
    (<a href="https://git.sr.ht/~sircmpwn/openring">via openring</a>)
  </p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Crazily Unconstitutional New Laws Trying to Criminalize Filming Cops (155 pts)]]></title>
            <link>https://slate.com/news-and-politics/2023/07/jarrell-garris-bodycam-footage-filming-cops-law-indiana-florida.html</link>
            <guid>36800144</guid>
            <pubDate>Thu, 20 Jul 2023 13:29:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://slate.com/news-and-politics/2023/07/jarrell-garris-bodycam-footage-filming-cops-law-indiana-florida.html">https://slate.com/news-and-politics/2023/07/jarrell-garris-bodycam-footage-filming-cops-law-indiana-florida.html</a>, See on <a href="https://news.ycombinator.com/item?id=36800144">Hacker News</a></p>
<div id="readability-page-1" class="page"><article data-uri="slate.com/_components/article/instances/clka449ke00bvmgm5svypmy3c@published" data-has-roadblock="false" data-rubric="jurisprudence" itemscope="" itemtype="http://schema.org/Article">
  

  

<header>

  <a href="https://slate.com/news-and-politics/jurisprudence">      Jurisprudence</a>

  




    </header>
<div>
      <figure data-uri="slate.com/_components/image/instances/clka449ke00bomgm59by9x02o@published" data-editable="imageInfo"><p><img loading="lazy" src="https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0" alt="Someone filming a line of cops in riot gear on a tablet." width="1560" height="1040" srcset="https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=320 320w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=480 480w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=600 600w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=840 840w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=960 960w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1280 1280w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1440 1440w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1600 1600w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1920 1920w,
https://compote.slate.com/images/f0a31aa6-3b1a-4232-93a1-a0fe017851ed.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=2200 2200w">
        
      </p>
<figcaption>
<span>Videos like this give justice a fighting chance.</span>
<span>Ramin Talaie/Corbis via Getty Images</span>
</figcaption>
</figure>

  </div>
  

  <section>
      


      

    <div itemprop="mainEntityOfPage">
          <p data-word-count="102" data-uri="slate.com/_components/slate-paragraph/instances/clka449ke00bpmgm5yclnkvuw@published">Few seem to have noticed that a dangerous legislative campaign is underway. Its aim: to shield police brutality by shutting down our cameras. On July 1, an Indiana law <a href="https://iga.in.gov/pdf-documents/123/2023/house/bills/HB1186/HB1186.05.ENRS.pdf">went into effect</a> making it a crime to come within 25 feet of an on-duty police officer if ordered to stay back. Legislators in <a href="https://www.flsenate.gov/Session/Bill/2023/1539/BillText/c2/PDF">Florida</a>, <a href="https://legis.la.gov/legis/ViewDocument.aspx?d=1329883">Louisiana</a>, and <a href="https://legislation.nysenate.gov/pdf/bills/2021/s3464">New York</a> have produced similar laws. Other states have promised to follow suit. Under these laws, if you come close enough to film a police officer beating someone up, there’s a good chance you’ll spend two months in jail and end up with a criminal record.</p>

  

  <p data-word-count="92" data-uri="slate.com/_components/slate-paragraph/instances/clka44kg9000r3b6w7a1kmxpd@published">In 2023 alone, the police have killed <a href="https://mappingpoliceviolence.org/">more than 500 people</a> in the United States. Among them was <a href="https://www.nytimes.com/2023/07/13/nyregion/new-rochelle-police-shooting-jarrell-garris.html">Jarrell Garris</a>, who died last week in New Rochelle, New York, after police shot him during an arrest for allegedly stealing a banana and some grapes. Garris was unarmed, and tackled by three officers, handcuffed, and shot. The police claim he was reaching for an officer’s gun. They’ve released bodycam footage <a href="https://www.youtube.com/watch?v=KhrQl2BBLus">that mysteriously stops</a> just before the shooting. They want to make sure you don’t see exactly what happened. So do the new laws.</p>

  



  <p data-word-count="74" data-uri="slate.com/_components/slate-paragraph/instances/clka44kkg000s3b6woijmcqf3@published">The Indiana, Florida, and Louisiana laws are only the most recent offensives in an ongoing campaign to shield police violence. As the Black Lives Matter protests caught fire in 2020, right-wing organizations jumped into action. They had three goals: quell the protests; silence critics of the police; and keep scenes like George Floyd’s murder out of the public eye, freeing up officers like convicted murderer Derek Chauvin to use traditional techniques to break suspects.</p>

  


  <p data-word-count="100" data-uri="slate.com/_components/slate-paragraph/instances/clka44kos000t3b6wqxef7e18@published">These efforts have been a resounding success. In the wake of the protests, 42 states have enacted laws <a href="https://www.icnl.org/usprotestlawtracker/">creating extra penalties</a> for protesters, with new crimes such as “aggravated riot” and “mob intimidation.” Many states have passed hate-speech laws that <a href="https://scholarship.law.campbell.edu/cgi/viewcontent.cgi?article=1695&amp;context=clr">protect the police</a> from verbal or symbolic abuse, such as stomping on a “Back the Blue” sign while “smirking in an intimidating manner” (the crime for which a woman in Utah <a href="https://www.npr.org/2021/07/15/1016431004/a-woman-is-facing-a-hate-crime-charge-for-stomping-on-a-back-the-blue-sign-in-ut">was prosecuted</a>). Although some of these laws have been vetoed or deemed unconstitutional, their promoters have only grown more brazen. The Indiana, Louisiana, and Florida laws are the result.</p>

  


  <p data-word-count="76" data-uri="slate.com/_components/slate-paragraph/instances/clka44kr8000u3b6wuq8ceyv9@published">These laws take their cue not only from the anti-protest and police protection laws but from a 2022 <a href="https://www.azleg.gov/legtext/55leg/2R/bills/HB2319S.pdf">Arizona law</a> making it a crime to film the police from 8 feet or closer. That law’s sponsor, then–state Rep. John Kavanagh (a former police officer), was frank about his intentions. “There are groups hostile to the police that follow them around to <a href="https://www.azcentral.com/story/opinion/op-ed/2022/03/24/hb-2319-videotape-police-8-feet-during-violent-encounters/7130071001/">videotape police incidents</a>,” he declared. The new law would put a stop to this nuisance.</p>

  


  


  


  <p data-word-count="59" data-uri="slate.com/_components/slate-paragraph/instances/clka44ktm000v3b6wl4r9ygn7@published">After the bill passed, the <a href="https://www.aclu.org/cases/arizona-broadcasters-association-v-brnovich#:~:text=Summary,accountability%20tools%20against%20police%20wrongdoing.">ACLU and 10 media organizations</a> sued, arguing that the law was a patent violation of the First Amendment. Federal District Court Judge John Tuchi agreed, <a href="https://www.acluaz.org/sites/default/files/2319_pi_order_9.9.22.pdf">issuing a preliminary injunction</a>. We have a clearly established constitutional right to record the police, he explained. Allowing the law to stand for even a moment “constitutes irreparable injury.”</p>

  <p data-word-count="55" data-uri="slate.com/_components/slate-paragraph/instances/clka44kwc000w3b6wx4geiizh@published">The law may not stand. But <a href="https://www.azcentral.com/story/news/local/arizona/2022/09/09/federal-judge-halts-arizona-ban-filming-police-within-8-feet/8035972001/">Kavanagh has vowed</a> that if it is ruled unconstitutional, he will simply redraft it. “If the judge says, you know, ‘This is bad because of X,’ next year, though, the bill will come in ‘Bill minus X.’ ” In short, he will massage it to make it <em>appear</em> constitutional.</p>

  <p data-word-count="92" data-uri="slate.com/_components/slate-paragraph/instances/clka44kyl000x3b6w5sscdsff@published">This is precisely what Indiana, Florida, and Louisiana have tried to do. Anticipating constitutional objections, the new laws don’t mention video. “Any suggestion that this … denies somebody the opportunity to film is just not accurate,” protests the Louisiana law’s sponsor, <a href="https://lailluminator.com/2023/05/09/bill-that-would-make-it-illegal-to-be-within-25-feet-of-police-advances-to-house-floor/">state Rep. Mike Johnson</a>, who wrote the bill on behalf of the Fraternal Order of Police. (The Louisiana Fraternal Order named Johnson <a href="https://www.kalb.com/2022/07/27/rep-mike-johnson-recognized-legislator-year/">legislator of the year</a> in 2022.) The new law, he insists, merely creates “a bodily separation” that prevents “anyone, friend or foe,” from “walking up to” officers and interfering.</p>

  


  


  <p data-word-count="57" data-uri="slate.com/_components/slate-paragraph/instances/clka44l2m000y3b6wcl9voher@published">Such claims should fool no one. “Cops have long tried claiming that [filming them] <a href="https://reason.com/2021/07/27/florida-bill-criminalize-filming-cops-first-amendment/">obstructs their ability</a> to do their job,” observes First Amendment lawyer Ari Cohn. “Now that this argument failed, they are … transparently trying to create a safe space from observation.” The new laws are nothing but “a targeted assault on First Amendment activity.”</p>

  


  <p data-word-count="94" data-uri="slate.com/_components/slate-paragraph/instances/clka44l50000z3b6w2k844ec1@published">The new laws don’t in fact criminalize interference (already illegal in all 50 states). They criminalize only the proximity necessary for filming. They give police the <a href="https://www.reentry.net/ny/help/item.2915-Getting_Property_Back_After_an_Arrest#:~:text=Safekeeping%3A%20your%20property%20can%20be,related%20to%20an%20ongoing%20case.">right to seize</a> cameras in the no-approach zone. They create a visual and auditory buffer likely to produce reasonable doubt in the courtroom. Is the officer’s knee on the suspect’s shoulder, or on his windpipe? Is the suspect fighting, or flailing? From 25 feet, with officers blocking visibility, phone cameras can’t capture crucial details. From 25 feet, they can’t capture choking sounds or someone crying “I can’t breathe.”</p>

  


  

  <p data-word-count="92" data-uri="slate.com/_components/slate-paragraph/instances/clka44l7y00103b6wfrfvhi05@published">By now, we shouldn’t be surprised that the footage released by New Rochelle police contained critical gaps. Videos give the lie to official reports. The fatal shooting of <a href="https://www.nytimes.com/2023/06/28/world/europe/france-police-shooting-paris-nanterre.html">Nahel Merzouk</a> in France on June 27 offers an object lesson. The police initially claimed they shot Merzouk because he was driving his car directly at them, but the video <a href="https://www.youtube.com/watch?v=YSfcLfoNkFk">shows him driving away</a>. Such lies are routine in the United States: We were told that <a href="https://pix11.com/news/exclusive-police-report-in-eric-garners-death-conflicts-with-videos-witnesses/">Eric Garner</a> died of a heart attack and <a href="https://www.cnn.com/2021/04/21/us/minneapolis-police-george-floyd-death/index.html">George Floyd</a> of “medical distress.” Then we saw the videos.</p>

  


  


  <p data-word-count="27" data-uri="slate.com/_components/slate-paragraph/instances/clka44lcs00113b6wakreh9nm@published">Such videos may be imperfect witnesses. Prosecutors may ignore them, attorneys may manipulate them, judges and jurors may misread them. But they give justice a fighting chance.</p>

  <p data-word-count="89" data-uri="slate.com/_components/slate-paragraph/instances/clka44lew00123b6w39kpejke@published">The Florida bill, backed by Gov. Ron DeSantis, is likely to be passed at the next legislative session. <a href="https://www.kalb.com/2023/06/28/gov-edwards-vetoes-state-rep-mike-johnsons-house-bill-85/">Johnson will reintroduce</a> the Louisiana bill after the Democratic governor who vetoed it leaves office in January. These new laws are marching forward, one state at a time. They are brazen attempts to silence protest, designed to dodge the Constitution and cover up criminal violence. They are assaults on the First Amendment’s central function, the sine qua non of democracy: the freedom to protest abuses of power. They must not <span>pass.</span></p>

  

  

</div>

      <ul>
<li>
            <a href="https://slate.com/tag/black-lives-matter">
              Black Lives Matter
            </a>
          </li><li>
            <a href="https://slate.com/tag/criminal-justice">
              Criminal Justice
            </a>
          </li><li>
            <a href="https://slate.com/tag/florida">
              Florida
            </a>
          </li><li>
            <a href="https://slate.com/tag/indiana">
              Indiana
            </a>
          </li><li>
            <a href="https://slate.com/tag/jurisprudence">
              Jurisprudence
            </a>
          </li><li>
            <a href="https://slate.com/tag/arizona">
              Arizona
            </a>
          </li><li>
            <a href="https://slate.com/tag/ron-desantis">
              Ron DeSantis
            </a>
          </li>      </ul>

  </section>

      

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No-more-secrets: recreate the decryption effect seen in the 1992 movie Sneakers (440 pts)]]></title>
            <link>https://github.com/bartobri/no-more-secrets</link>
            <guid>36799776</guid>
            <pubDate>Thu, 20 Jul 2023 12:57:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bartobri/no-more-secrets">https://github.com/bartobri/no-more-secrets</a>, See on <a href="https://news.ycombinator.com/item?id=36799776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/52a166beedb33d757d4d8508d58e75f59ca0ef3fb07ca710cbb75f48bedec82e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302e312d677265656e2e737667"><img src="https://camo.githubusercontent.com/52a166beedb33d757d4d8508d58e75f59ca0ef3fb07ca710cbb75f48bedec82e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f56657273696f6e2d312e302e312d677265656e2e737667" alt="Version" data-canonical-src="https://img.shields.io/badge/Version-1.0.1-green.svg"></a></p>
<p dir="auto">Like this project? Consider tipping me: <a href="https://github.com/sponsors/bartobri">https://github.com/sponsors/bartobri</a></p>
<h2 tabindex="-1" dir="auto">No More Secrets</h2>
<p dir="auto">This project provides a command line tool called <code>nms</code> that recreates the
famous data decryption effect seen on screen in the 1992 hacker movie Sneakers.
For reference, you can see this effect at 0:35 in <a href="https://www.youtube.com/watch?v=F5bAa6gFvLs&amp;t=35" rel="nofollow">this movie clip</a>.</p>
<p dir="auto">This command works on piped data. Pipe any ASCII or UTF-8 text to <code>nms</code>,
and it will apply the Hollywood effect, initially showing encrypted data,
then starting a decryption sequence to reveal the original plain-text characters.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/68bbd6c555003c61a5e865edaf317f370a7b35544abb59a85ce179fde58f626f/68747470733a2f2f7777772e627269616e626172746f2e696e666f2f7374617469632f6e6d732f6e6d732e676966"><img src="https://camo.githubusercontent.com/68bbd6c555003c61a5e865edaf317f370a7b35544abb59a85ce179fde58f626f/68747470733a2f2f7777772e627269616e626172746f2e696e666f2f7374617469632f6e6d732f6e6d732e676966" alt="Screenshot" data-animated-image="" data-canonical-src="https://www.brianbarto.info/static/nms/nms.gif"></a></p>
<p dir="auto">Also included in this project is a program called <code>sneakers</code> that recreates
what we see in the above movie clip. Note that this program requires the
user to select one of the menu options before it terminates.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b1a797b0e5dc49c8f1714bad79660a89b2472cd67576fd1dfddeb03036f1601c/68747470733a2f2f7777772e627269616e626172746f2e696e666f2f7374617469632f6e6d732f736e65616b6572732e676966"><img src="https://camo.githubusercontent.com/b1a797b0e5dc49c8f1714bad79660a89b2472cd67576fd1dfddeb03036f1601c/68747470733a2f2f7777772e627269616e626172746f2e696e666f2f7374617469632f6e6d732f736e65616b6572732e676966" alt="Screenshot" data-animated-image="" data-canonical-src="https://www.brianbarto.info/static/nms/sneakers.gif"></a></p>
<p dir="auto">By default, this project has no dependencies, but it does rely on ANSI/VT100
terminal escape sequences to recreate the effect. Most modern terminal
programs support these sequences so this should not be an issue for most
users. If yours does not, this project also provides a ncurses implementation
which supports non-ANSI terminals, but at the expense of losing the inline
functionality (ncurses will always clear the screen prior to displaying output).</p>
<h2 tabindex="-1" dir="auto">Table of Contents</h2>
<ol dir="auto">
<li><a href="#download-and-install">Download and Install</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#the-nms-library">The NMS Library</a></li>
<li><a href="#license">License</a></li>
</ol>
<h2 tabindex="-1" dir="auto">Download and Install</h2>
<p dir="auto">More and more Unix/Linux platforms are including this project in their
package manager. You may wish to search your package manager to see if it
is an installation option. If you install from a package manager, please
check that you have the latest version (<code>nms -v</code>). If not, I suggest
installing from source by following the instructions below.</p>
<p dir="auto">To install this project from source, you will need to have the tools <code>git</code>,
<code>gcc</code>, and <code>make</code> to download and build it. Install them from your package
manager if they are not already installed.</p>
<p dir="auto">Once you have the necessary tools installed, follow these instructions:</p>
<h4 tabindex="-1" dir="auto">Install:</h4>
<div data-snippet-clipboard-copy-content="$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms
$ make sneakers             ## Optional
$ sudo make install"><pre><code>$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms
$ make sneakers             ## Optional
$ sudo make install
</code></pre></div>
<h4 tabindex="-1" dir="auto">Uninstall:</h4>

<h4 tabindex="-1" dir="auto">Install with Ncurses Support</h4>
<p dir="auto">If your terminal does not support ANSI/VT100 escape sequences, the effect
may not render properly. This project provides a ncurses implementation
for such cases. You will need the ncurses library installed. <a href="https://github.com/bartobri/no-more-secrets/blob/master/NCURSES.md">Install this
library from your package manager</a>. Next, follow these instructions:</p>
<div data-snippet-clipboard-copy-content="$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms-ncurses
$ make sneakers-ncurses     ## Optional
$ sudo make install"><pre><code>$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms-ncurses
$ make sneakers-ncurses     ## Optional
$ sudo make install
</code></pre></div>
<h2 tabindex="-1" dir="auto">Usage</h2>
<p dir="auto"><code>nms</code> works on piped data. Pipe any ASCII or UTF-8 characters to it and
enjoy the magic. In the below examples, I use a simple directory listing.</p>
<div data-snippet-clipboard-copy-content="$ ls -l | nms
$ ls -l | nms -a           // Set auto-decrypt flag
$ ls -l | nms -s           // Set flag to mask space characters
$ ls -l | nms -f green     // Set foreground color to green
$ ls -l | nms -c           // Clear screen
$ nms -v                   // Display version"><pre><code>$ ls -l | nms
$ ls -l | nms -a           // Set auto-decrypt flag
$ ls -l | nms -s           // Set flag to mask space characters
$ ls -l | nms -f green     // Set foreground color to green
$ ls -l | nms -c           // Clear screen
$ nms -v                   // Display version
</code></pre></div>
<p dir="auto">Note that by default, after the initial encrypted characters are displayed,
<code>nms</code> will wait for the user to press a key before initiating the decryption
sequence. This is how the it is depicted in the movie.</p>
<h4 tabindex="-1" dir="auto">Command Line Options</h4>
<p dir="auto"><code>-a</code></p>
<p dir="auto">Set the auto-decrypt flag. This will automatically start the
decryption sequence without requiring a key press.</p>
<p dir="auto"><code>-s</code></p>
<p dir="auto">Set a flag to mask space characters. This will only mask single blank space
characters. Other space characters such as tabs and newlines will not be masked.</p>
<p dir="auto"><code>-f &lt;color&gt;</code></p>
<p dir="auto">Set the foreground color of the decrypted text to the color
specified. Valid options are white, yellow, black, magenta, blue, green,
or red. This is blue by default.</p>
<p dir="auto"><code>-c</code></p>
<p dir="auto">Clear the screen prior to printing any output. Specifically,
it saves the state of the terminal (all current output), and restores it
once the effect is completed. Note that when using this option, <code>nms</code> requires
the user to press a key before restoring the terminal.</p>
<p dir="auto"><code>-v</code></p>
<p dir="auto">Display version info.</p>
<h2 tabindex="-1" dir="auto">The NMS Library</h2>
<p dir="auto">For those who would like to use this effect in their own projects, I have
created a C library that provides simple interface and can easily be used
for any program that runs from the command line.</p>
<p dir="auto">See <a href="https://github.com/bartobri/libnms">LibNMS</a> for more info.</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">This program is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License. See <a href="https://github.com/bartobri/no-more-secrets/blob/master/LICENSE">LICENSE</a> for
more details.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anytype – open-source, local-first, P2P Notion alternative (218 pts)]]></title>
            <link>https://anytype.io/?hn</link>
            <guid>36799548</guid>
            <pubDate>Thu, 20 Jul 2023 12:30:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anytype.io/?hn">https://anytype.io/?hn</a>, See on <a href="https://news.ycombinator.com/item?id=36799548">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The past is not true (217 pts)]]></title>
            <link>https://sive.rs/pnt</link>
            <guid>36798854</guid>
            <pubDate>Thu, 20 Jul 2023 10:52:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sive.rs/pnt">https://sive.rs/pnt</a>, See on <a href="https://news.ycombinator.com/item?id=36798854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<header>


<small>2023-07-20</small>
</header>

<p>
	When I was 17, I was driving recklessly and crashed into an oncoming car.
	I found out that I broke the other driver’s spine, and she’ll never walk again.
</p><p>
	I carried that burden with me everywhere, and felt so horrible about it for so many years that at age 35 I decided to find this woman to apologize.
	I found her name and address, went to her house, knocked on the door, and a middle-aged woman answered.
	As soon as I said, “I’m the teenager that hit your car eighteen years ago and broke your spine”, I started sobbing - a big ugly cry, surfacing years of regret.
	She was so sweet, and hugged me saying, “Oh sweetie, sweetie! Don’t worry. I’m fine!”
	Then she walked me into her living room.
	Walked.
</p><p>
	Turns out the news had been miscommunicated.
	Yes she fractured a couple vertebrae but it never stopped her from walking.
	She said “that little accident” helped her pay more attention to her fitness, lose weight, and since then has been in better health than ever.
	Then she apologized for causing the accident in the first place.
	Apologized.
</p><p>
	I said, “Well, no, it was my fault for ignoring the yield sign.”
</p><p>
	She said, “No, it was my fault because I was eating while driving and not watching the road. You didn’t hit me. I hit you.”
</p><p>
	Seems we had both been told the accident was our fault, and had spent eighteen years feeling bad about it.
	This time she started crying, sniffled, grabbed a tissue to wipe her eyes and said, “It’s so <em>stupid</em> - these stories.”
</p>
<hr>
<p>
	Aim a laser pointer at the moon, then move your hand the tiniest bit, and it’ll move a thousand miles at the other end.
	The tiniest misunderstanding long ago, amplified through time, leads to piles of misunderstandings in the present.
</p><p>
	We think of the past like it’s a physical fact - like it’s real.
	But the past is what we call our memory and stories about it.
	Imperfect memories, and stories built on one interpretation of incomplete information.
	That’s “<em>the</em> past”.
</p><p>
	History is not true.
<strong>
	You can change history.
</strong>
	The actual factual events are such a small part of the story.
	Everything else is interpretation.
</p><p>
	It’s never too late to change a story.
</p>
<figure>
<a href="https://www.flickr.com/photos/ashclements/290493334/"><img src="https://sive.rs/images/crashedcar.jpg" alt="crashed car photo by Ashley Jonathan Clements"></a>
<figcaption>photo by <a href="https://www.flickr.com/photos/ashclements/290493334/">Ashley Jonathan Clements</a></figcaption>
</figure>


</article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cool Retro Terminal (168 pts)]]></title>
            <link>https://github.com/Swordfish90/cool-retro-term</link>
            <guid>36798774</guid>
            <pubDate>Thu, 20 Jul 2023 10:37:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Swordfish90/cool-retro-term">https://github.com/Swordfish90/cool-retro-term</a>, See on <a href="https://news.ycombinator.com/item?id=36798774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">cool-retro-term</h2>
<table>
<thead>
<tr>
<th>&gt; Default Amber</th>
<th>C:\ IBM DOS</th>
<th>$ Default Green</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/121322/32070717-16708784-ba42-11e7-8572-a8fcc10d7f7d.gif"><img src="https://user-images.githubusercontent.com/121322/32070717-16708784-ba42-11e7-8572-a8fcc10d7f7d.gif" alt="Default Amber Cool Retro Term" data-animated-image=""></a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/121322/32070716-16567e5c-ba42-11e7-9e64-ba96dfe9b64d.gif"><img src="https://user-images.githubusercontent.com/121322/32070716-16567e5c-ba42-11e7-9e64-ba96dfe9b64d.gif" alt="IBM DOS" data-animated-image=""></a></td>
<td><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/121322/32070715-163a1c94-ba42-11e7-80bb-41fbf10fc634.gif"><img src="https://user-images.githubusercontent.com/121322/32070715-163a1c94-ba42-11e7-80bb-41fbf10fc634.gif" alt="Default Green Cool Retro Term" data-animated-image=""></a></td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Description</h2>
<p dir="auto">cool-retro-term is a terminal emulator which mimics the look and feel of the old cathode tube screens.
It has been designed to be eye-candy, customizable, and reasonably lightweight.</p>
<p dir="auto">It uses the QML port of qtermwidget (Konsole): <a href="https://github.com/Swordfish90/qmltermwidget">https://github.com/Swordfish90/qmltermwidget</a>.</p>
<p dir="auto">This terminal emulator works under Linux and macOS and requires Qt5. It's suggested that you stick to the latest LTS version.</p>
<p dir="auto">Settings such as colors, fonts, and effects can be accessed via context menu.</p>
<h2 tabindex="-1" dir="auto">Screenshots</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/26bae23283b3b91bf2409aae0de2a032408a50e97ca79aa62513b471626b5c3b/68747470733a2f2f692e696d6775722e636f6d2f544e756d6b446e2e706e67"><img src="https://camo.githubusercontent.com/26bae23283b3b91bf2409aae0de2a032408a50e97ca79aa62513b471626b5c3b/68747470733a2f2f692e696d6775722e636f6d2f544e756d6b446e2e706e67" alt="Image" data-canonical-src="https://i.imgur.com/TNumkDn.png"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/49da3e0faf874c2c0503632693a74efd0e5ee836b69821bbd676c8f8b8eebdd1/68747470733a2f2f692e696d6775722e636f6d2f68666a574f4d342e706e67"><img src="https://camo.githubusercontent.com/49da3e0faf874c2c0503632693a74efd0e5ee836b69821bbd676c8f8b8eebdd1/68747470733a2f2f692e696d6775722e636f6d2f68666a574f4d342e706e67" alt="Image" data-canonical-src="https://i.imgur.com/hfjWOM4.png"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a88479dfa171e61f02f831bff623eab504f495f61bc42b6c29830820ba0dd357/68747470733a2f2f692e696d6775722e636f6d2f47595244507a4a2e6a7067"><img src="https://camo.githubusercontent.com/a88479dfa171e61f02f831bff623eab504f495f61bc42b6c29830820ba0dd357/68747470733a2f2f692e696d6775722e636f6d2f47595244507a4a2e6a7067" alt="Image" data-canonical-src="https://i.imgur.com/GYRDPzJ.jpg"></a></p>
<h2 tabindex="-1" dir="auto">Install</h2>
<p dir="auto">If you want to get a hold of the latest version, just go to the Releases page and grab the latest AppImage (Linux) or dmg (macOS).</p>
<p dir="auto">Alternatively, most distributions such as Ubuntu, Fedora or Arch already package cool-retro-term in their official repositories.</p>
<h2 tabindex="-1" dir="auto">Building</h2>
<p dir="auto">Check out the wiki and follow the instructions on how to build it on <a href="https://github.com/Swordfish90/cool-retro-term/wiki/Build-Instructions-(Linux)">Linux</a> and <a href="https://github.com/Swordfish90/cool-retro-term/wiki/Build-Instructions-(macOS)">macOS</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Docuseal: Open-source DocuSign alternative. Create, fill, sign digital documents (300 pts)]]></title>
            <link>https://github.com/docusealco/docuseal</link>
            <guid>36798593</guid>
            <pubDate>Thu, 20 Jul 2023 10:04:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/docusealco/docuseal">https://github.com/docusealco/docuseal</a>, See on <a href="https://news.ycombinator.com/item?id=36798593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">
  <p><a href="https://www.docuseal.co/" rel="nofollow">
      <img alt="DocuSeal" src="https://github.com/docusealco/docuseal/assets/5418788/c12cd051-81cd-4402-bc3a-92f2cfdc1b06" width="80">
      <br>
    </a>
    DocuSeal
  </p>
</h2>
<h3 tabindex="-1" dir="auto">
  Open source document filling and signing
</h3>
<p dir="auto">
  <a href="https://hub.docker.com/r/docuseal/docuseal" rel="nofollow">
    <img alt="Docker releases" src="https://camo.githubusercontent.com/bf6921e570de205b15b2ffccdcfdb86a2ce7f56610b9a57d1239fe9563404387/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f762f646f63757365616c2f646f63757365616c" data-canonical-src="https://img.shields.io/docker/v/docuseal/docuseal">
  </a>
  <a href="https://discord.gg/qygYCDGck9" rel="nofollow">
    <img src="https://camo.githubusercontent.com/56c2ca2c4d2134ea4912b743333a79e1b57e985bfaca0e665406fd8d96932b44/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313132353131323634313137303434383435343f6c6f676f3d646973636f7264" data-canonical-src="https://img.shields.io/discord/1125112641170448454?logo=discord">
  </a>
  <a href="https://twitter.com/intent/follow?screen_name=docusealco" rel="nofollow">
    <img src="https://camo.githubusercontent.com/d31021dc3163b73ba470c870765521480715dfdc9c36fa4e2800376b87abae62/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f646f63757365616c636f3f7374796c653d736f6369616c" alt="Follow @docusealco" data-canonical-src="https://img.shields.io/twitter/follow/docusealco?style=social">
  </a>
</p>
<p dir="auto">
DocuSeal is an open source platform that provides secure and efficient digital document signing and processing. Create PDF forms to have them filled and signed online on any device with an easy-to-use, mobile-optimized web tool.
</p>
<h2 tabindex="-1" dir="auto">
  <a href="https://demo.docuseal.co/" rel="nofollow"><g-emoji alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">✨</g-emoji> Live Demo</a>
  <span>|</span>
  <a href="https://docuseal.co/sign_up" rel="nofollow"><g-emoji alias="cloud" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2601.png">☁️</g-emoji> Try in Cloud</a>
</h2>
<p dir="auto"><a href="https://demo.docuseal.co/" rel="nofollow"><gh:secured-asset-reference resource_type="UserAsset" resource_id="251969508"></gh:secured-asset-reference></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/docusealco/docuseal/assets/5418788/d8703ea3-361a-423f-8bfe-eff1bd9dbe14"><img src="https://github.com/docusealco/docuseal/assets/5418788/d8703ea3-361a-423f-8bfe-eff1bd9dbe14" alt="Demo"></a></p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul>
<li> PDF form fields builder (WYSIWYG)</li>
<li> 10 field types available (Signature, Date, File, Checkbox etc.)</li>
<li> Multiple submitters per document</li>
<li> Automated emails via SMTP</li>
<li> Files storage on AWS S3, Google Storage, or Azure</li>
<li> Automatic PDF eSignature</li>
<li> PDF signature verification</li>
<li> Users management</li>
<li> Mobile-optimized</li>
<li> Easy to deploy in minutes</li>
</ul>
<h2 tabindex="-1" dir="auto">Deploy</h2>
<table>
<thead>
<tr>
<th>Heroku</th>
<th>Railway</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://heroku.com/deploy?template=https://github.com/docusealco/docuseal-heroku" rel="nofollow"><img alt="Deploy on Heroku" src="https://camo.githubusercontent.com/6979881d5a96b7b18a057083bb8aeb87ba35fc279452e29034c1e1c49ade0636/68747470733a2f2f7777772e6865726f6b7563646e2e636f6d2f6465706c6f792f627574746f6e2e737667" height="40" data-canonical-src="https://www.herokucdn.com/deploy/button.svg"></a></td>
<td><a href="https://railway.app/template/IGoDnc?referralCode=ruU7JR" rel="nofollow"><img alt="Deploy on Railway" src="https://camo.githubusercontent.com/081df3dd8cff37aab35044727b02b94a8e948052487a8c6253e190f5940d776d/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667" height="40" data-canonical-src="https://railway.app/button.svg"></a></td>
</tr>
<tr>
<td><strong>DigitalOcean</strong></td>
<td><strong>Render</strong></td>
</tr>
<tr>
<td><a href="https://cloud.digitalocean.com/apps/new?repo=https://github.com/docusealco/docuseal-digitalocean/tree/master&amp;refcode=421d50f53990" rel="nofollow"><img alt="Deploy on DigitalOcean" src="https://camo.githubusercontent.com/df21703b4229f8d44f76c2d56073657a4ab450ca4566ba5d24d05bf528c298f8/68747470733a2f2f7777772e6465706c6f79746f646f2e636f6d2f646f2d62746e2d626c75652e737667" height="40" data-canonical-src="https://www.deploytodo.com/do-btn-blue.svg"></a></td>
<td><a href="https://render.com/deploy?repo=https://github.com/docusealco/docuseal-render" rel="nofollow"><img alt="Deploy to Render" src="https://camo.githubusercontent.com/3cae4655a3792a1d58dcd0e3f8815853cc88543acd4eb5d8c534ea24e0e46f89/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667" height="40" data-canonical-src="https://render.com/images/deploy-to-render-button.svg"></a></td>
</tr>
</tbody>
</table>
<h4 tabindex="-1" dir="auto">Docker</h4>
<div dir="auto" data-snippet-clipboard-copy-content="docker run --name docuseal -p 3000:3000 -v.:/data docuseal/docuseal"><pre>docker run --name docuseal -p 3000:3000 -v.:/data docuseal/docuseal</pre></div>
<p dir="auto">By default DocuSeal docker container uses an SQLite database to store data and configurations. Alternatively, it is possible use PostgreSQL or MySQL databases by specifying the <code>DATABASE_URL</code> env variable.</p>
<h4 tabindex="-1" dir="auto">Docker Compose</h4>
<p dir="auto">Download docker-compose.yml into your private server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl https://raw.githubusercontent.com/docusealco/docuseal/master/docker-compose.yml > docker-compose.yml"><pre>curl https://raw.githubusercontent.com/docusealco/docuseal/master/docker-compose.yml <span>&gt;</span> docker-compose.yml</pre></div>
<p dir="auto">Run the app under a custom domain over https using docker compose (make sure your DNS points to the server to automatically issue ssl certs with Caddy):</p>
<div dir="auto" data-snippet-clipboard-copy-content="HOST=your-domain-name.com docker-compose up"><pre>HOST=your-domain-name.com docker-compose up</pre></div>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">DocuSeal is released under the GNU Affero General Public License v3.0.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MDN Playground (182 pts)]]></title>
            <link>https://developer.mozilla.org/en-US/play</link>
            <guid>36798157</guid>
            <pubDate>Thu, 20 Jul 2023 08:39:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.mozilla.org/en-US/play">https://developer.mozilla.org/en-US/play</a>, See on <a href="https://news.ycombinator.com/item?id=36798157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="references-menu" aria-labelledby="references-button"><li><a href="https://developer.mozilla.org/en-US/docs/Web"><div><p>Overview / Web Technology</p><p>Web technology reference for developers</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/HTML"><div><p>HTML</p><p>Structure of content on the web</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/CSS"><div><p>CSS</p><p>Code used to describe document style</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript"><div><p>JavaScript</p><p>General-purpose scripting language</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/HTTP"><div><p>HTTP</p><p>Protocol for transmitting web resources</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web/API"><div><p>Web APIs</p><p>Interfaces for building web applications</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions"><div><p>Web Extensions</p><p>Developing extensions for web browsers</p></div></a></li><li><a href="https://developer.mozilla.org/en-US/docs/Web"><div><p>Web Technology</p><p>Web technology reference for developers</p></div></a></li></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Happened to Dolphin on Steam? (171 pts)]]></title>
            <link>https://dolphin-emu.org/blog/2023/07/20/what-happened-to-dolphin-on-steam/</link>
            <guid>36798092</guid>
            <pubDate>Thu, 20 Jul 2023 08:29:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dolphin-emu.org/blog/2023/07/20/what-happened-to-dolphin-on-steam/">https://dolphin-emu.org/blog/2023/07/20/what-happened-to-dolphin-on-steam/</a>, See on <a href="https://news.ycombinator.com/item?id=36798092">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header> 
<img src="https://dolphin-emu.org/m/user/blog/steam-release/steamwhat.jpg"> 
<img src="https://dolphin-emu.org/m/user/blog/steam-release/steamwhatmini.jpg"> 
</header>

<p>Well that blew up, huh?  If you follow emulation or just gaming on the whole, you've probably heard about the controversy around the Dolphin Steam release and the Wii Common Key.  There's been a lot of conclusions made, and while we've wanted to defend ourselves, we thought it would be prudent to contact lawyers first to make sure that our understanding of the situation was legally sound. That took some time, which was frustrating to ourselves and to our users, but now we are educated and ready to give an informed response.</p>
<p><a id="cuthere"></a>
We'd like to thank Kellen Voyer of <a href="https://voyerlaw.com/">Voyer Law</a> for providing us with legal council for this matter. And to be clear, all of the analysis below is specifically regarding US law. Without further delay, let's begin.</p>
<h3 id="what-actually-happened">What actually happened?<a href="#what-actually-happened" title="Permanent link">¶</a></h3>
<p>First things first - Nintendo <strong>did not</strong> send Valve or Dolphin a Digital Millenium Copyright Act (DMCA) section 512(c) notice (commonly known as a DMCA Takedown Notice) against our Steam page. Nintendo has not taken any legal action against Dolphin Emulator or Valve.</p>
<p>What actually happened was that Valve's legal department contacted Nintendo to inquire about the announced release of Dolphin Emulator on Steam.  In reply to this, a lawyer representing Nintendo of America requested Valve prevent Dolphin from releasing on the Steam store, citing the DMCA as justification. Valve then forwarded us the statement from Nintendo's lawyers, and told us that we had to come to an agreement <em>with Nintendo</em> in order to release on Steam.  Considering the strong legal wording at the start of the document and the citation of DMCA law, we took the letter very seriously.  We wanted to take some time and formulate a response, however after being flooded with questions, we wrote a <a href="https://dolphin-emu.org/blog/2023/05/27/dolphin-steam-indefinitely-postponed/">fairly frantic statement</a> on the situation as we understood it at the time, which turned out to only fuel the fires of speculation. </p>
<p>So, after a long stay of silence, we have a difficult announcement to make.  We are abandoning our efforts to release Dolphin on Steam.  Valve ultimately runs the store and can set any condition they wish for software to appear on it. But given Nintendo's long-held stance on emulation, we find Valve's requirement for us to get <em>approval</em> from Nintendo for a Steam release to be impossible. Unfortunately, that's that.  But there are some more serious matters to discuss, some that are much bigger than Dolphin's Steam Release.</p>
<h3 id="what-about-the-key">What about the key?<a href="#what-about-the-key" title="Permanent link">¶</a></h3>
<p>Over the past few weeks, a lot has been said about Dolphin including the Wii Common Key. As you may know, Wii games are encrypted, and the Wii uses the "common key" that is burned into the console to decrypt Wii discs. Wii software does not have any access to the key whatsoever, however, some smart engineers and a pair of tweezers was all it took to extract the key. If you haven't heard this story before, we highly recommend <a href="https://media.ccc.de/v/25c3-2799-en-console_hacking_2008_wii_fail">checking out the 25c3 presentation on the actual Tweezer Exploit that gave Team Twiizers its original name</a>.  It's an incredibly entertaining video that's worth your time.  If you aren't familiar with Team Twiizers, perhaps you know them under their modern name: <a href="https://fail0verflow.com/">fail0verflow</a>.</p>
<p>The extraction of the Wii Common Key did not elicit any kind of legal response from anyone.  It was <a href="https://hackmii.com/2008/04/keys-keys-keys/">freely shared</a> <em>everywhere</em>, and eventually made its way into Dolphin's codebase <a href="https://github.com/dolphin-emu/dolphin/commit/6ff164585ce5b63ef91a4c6635e35a7fe938ea1f">more than 15 years ago</a> (committed by a Team Twiizers member no less). </p>
<p>These keys have been publicly available for years and no one has really cared.  US law regarding this has not changed, yet a lot of armchair lawyers have come out talking about how foolish we were to ship the Wii Common Key.  Fueling this is Nintendo's letter to Valve, which cites the anti-circumvention provisions of the DMCA (<a href="https://www.law.cornell.edu/uscode/text/17/1201">17 U.S.C. § 1201</a>), particularly because Dolphin has to decrypt Wii games. </p>
<blockquote>Wii and Nintendo GameCube game files, or ROMs, are encrypted using proprietary cryptographic keys. The Dolphin emulator operates by incorporating these cryptographic keys without Nintendo’s authorization and decrypting the ROMs at or immediately before runtime. Thus, use of the Dolphin emulator unlawfully “circumvent[s] a technological measure that effectively controls access to a work protected under” the Copyright Act. 17 U.S.C. § 1201(a)(1). Distribution of the emulator, whether by the Dolphin developers or other third-party platforms, constitutes unlawful “traffic[king] in a[] technology . . . that . . . is primarily designed or produced for the purpose of circumventing a technological measure . . . .” 17 U.S.C. § 1201(a)(2)(A).<sup>3</sup></blockquote>
<p>This sounds extremely bad at a glance (and we certainly had a moment of panic after first reading it), but now that we have done our homework and talked to a lawyer, we are no longer concerned.</p>
<p>We have a very strong argument that Dolphin is <strong>not</strong> primarily designed or produced for the purpose of circumventing protection.  Dolphin is designed to recreate the GameCube and Wii hardware as software, and to provide the means for a user to interact with this emulated environment. Only an <em>incredibly tiny</em> portion of our code is actually related to circumvention. Additionally, GameCube games aren't actually encrypted at all, and Dolphin can also play homebrew and is used in the development of game mods.  There are even homebrew and mods that specifically target Dolphin as its own platform, given that it has the ability to emulate more memory and processing power than is possible on the original consoles.  That's why there are "Dolphin modes" in many modern homebrew games!</p>
<p>Considering that only a small fraction of what we do involves circumvention, we think that the claim that we are "primarily for circumvention" is a <strong>reach</strong>. We do not believe this angle would be successful in a US courtroom, if it were ever to come to that. The reason the lawyers representing Nintendo would make such a leap is because they wished to create a narrative where the DMCA's exemptions do not apply to us, as these exemptions are powerful and widely in our favor. Of particular note for Dolphin is the reverse engineering exemption in <a href="https://www.law.cornell.edu/uscode/text/17/1201#e">17 U.S.C. § 1201(f)</a> which states that:</p>
<blockquote>
...a person may develop and employ technological means to circumvent a technological measure, or to circumvent protection afforded by a technological measure, in order to enable the identification and analysis under paragraph (1), or for the purpose of enabling interoperability of an independently created computer program with other programs, if such means are necessary to achieve such interoperability, to the extent that doing so does not constitute infringement under this title.
</blockquote>
<p>Dolphin is an independently created computer program that is circumventing Wii disc encryption for interoperability with Wii software. According to this exemption, this does not constitute infringement under 17 U.S.C. § 1201. This exemption even allows distribution of information collected through circumvention, like encryption keys, if it is for software interoperability.</p>
<blockquote>The information acquired through the acts permitted under paragraph (1), and the means permitted under paragraph (2), may be made available to others if the person referred to in paragraph (1) or (2), as the case may be, provides such information or means solely for the purpose of enabling interoperability of an independently created computer program with other programs, and to the extent that doing so does not constitute infringement under this title or violate applicable law other than this section.
</blockquote>
<p>17 U.S.C. § 1201(f) is a significant legal protection for emulation in the US, and it is why Nintendo has yet to legally challenge any emulator with the DMCA anti-circumvention clauses despite the law going into effect <em>25 years ago</em>. Unless a court rules that our understanding of the law is incorrect, we have every reason to believe that our decryption of Wii game discs is covered by this exemption.</p>
<p>After this situation blew up, we received <em>many</em> requests, and even some demands, to remove all Wii keys from our codebase. We're disappointed that so many people on YouTube and social media <em>didn't even consider</em> that maybe the team had done their research and risk analysis before including the keys, and just assumed that now that it was "pointed out to us" we would remove them. However, <strong>we do not think that including the Wii Common Key actually matters</strong> - the law could easily be interpreted to say that circumventing a Wii disc's encryption <em>by any means</em> is a violation.  As such, it is our interpetation that <strong>removing the Wii keys would not change whether the exemption in 17 U.S.C. § 1201(f) applies to us or not.</strong></p>
<p>In fact, we think that offloading decryption tasks onto a potential 3rd party application would make the situation worse for everyone. As such, we believe leaving the keys as they are is the best course of action.</p>
<p>And to all the armchair lawyers out there, the letter to Valve did not make any claims that we were violating a US copyright by including the Wii Common Key, as a <em>short string of entirely random letters and numbers generated by a machine</em> <a href="https://www.copyright.gov/comp3/chap300/ch300-copyrightable-authorship.pdf">is not copyrightable</a> under current US copyright law. If that ever changes, the world will be far too busy to think about emulation.</p>
<h3 id="what-happens-now">What happens now?<a href="#what-happens-now" title="Permanent link">¶</a></h3>
<p><strong>We do not believe that Dolphin is in any legal danger</strong>.  We can look to the end of the message Valve forwarded to us to show this. After all of the scary language, Nintendo made no demands and made only a single request to Valve.</p>
<blockquote>We specifically request that Dolphin’s “coming soon” notice be removed and that you ensure the emulator does not release on the Steam store moving forward.</blockquote>
<p>In the end, Valve is the one running the Steam store front, and they have the right to allow or disallow anything they want on said store front for any reason.  As for Nintendo, this incident just continues their existing stance towards emulation.  We don't think that this incident should change anyone's view of either company.</p>
<p>As a silver lining, some of the features being developed for the Steam release will still work in Dolphin's normal builds, and are still being developed.  One of the features we are most excited for is a full "Big Picture" GUI that can be used directly with a controller.  That is still going to happen <em>regardless</em> of a Steam release, alongside several smaller features that were meant to be quality of life improvements for Steam builds.</p>
<p>The last thing we'd like to do before signing off is thank the developers who put a lot of effort into the Steam release. <strong><a href="https://github.com/OatmealDome">OatmealDome</a></strong> in particular was the architect of Dolphin's Steam Integration, working with Dolphin's infrastructure and Steam to take it from theory all the way to a fully-functional version of Dolphin. We'd also like to thank <strong><a href="https://github.com/delroth">delroth</a></strong> for the immense amount of CI work the past few months, which gave OatmealDome a solid foundation to build from. Finally, <strong><a href="https://github.com/MayImilae">MayImilae</a></strong> put in a large amount of media work toward the Steam release despite also working on a major upcoming feature.</p>


    
    
    

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Something in space has been lighting up every 20 minutes since 1988 (128 pts)]]></title>
            <link>https://arstechnica.com/science/2023/07/new-slow-repeating-radio-source-we-have-no-idea-what-it-is/</link>
            <guid>36797231</guid>
            <pubDate>Thu, 20 Jul 2023 05:42:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2023/07/new-slow-repeating-radio-source-we-have-no-idea-what-it-is/">https://arstechnica.com/science/2023/07/new-slow-repeating-radio-source-we-have-no-idea-what-it-is/</a>, See on <a href="https://news.ycombinator.com/item?id=36797231">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/07/GettyImages-1291354392-800x523.jpg" alt="image of a bright blue sphere on a dark background, with spikes of light emitted by two poles.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/07/GettyImages-1291354392.jpg" data-height="1399" data-width="2142">Enlarge</a> <span>/</span> Most of the explanations for this phenomenon involve a neutron star, depicted above. These explanations are uniformly terrible.</p></figcaption>  </figure>

  




<!-- cache hit 187:single/related:66376822a460a81ec816b18266551f55 --><!-- empty -->
<p>On Wednesday, researchers announced the discovery of a new astronomical enigma. The new object, GPM J1839–10, behaves a bit like a pulsar, sending out regular bursts of radio energy. But the physics that drives pulsars means that they'd stop emitting if they slowed down too much, and almost every pulsar we know of blinks at least once per minute.</p>
<p>GPM J1839–10 takes 22 minutes between pulses. We have no idea what kind of physics or what kind of objects can power that.</p>
<h2>A persistent transient</h2>
<p>GPM J1839–10 was discovered in a search of the galactic plane for transient objects—something that's not there when you first look, but appears the next time you check. The typical explanation for a transient object is something like a supernova, where a major event gives something an immense boost in brightness. They're found at the radio end of the spectrum—<a href="https://arstechnica.com/science/2020/11/its-coming-from-inside-the-galaxy-first-fast-radio-burst-source-idd/">fast radio bursts</a>—but are also very brief, and so fairly difficult to spot.</p>                                            
                                                        
<p>In any case, GPM J1839–10 showed up in the search in a rather unusual way: It appeared as a transient item twice in the same night of observation. Rather than delivering a short burst of immense energy, such as a fast radio burst, GPM J1839–10 was much lower energy and spread out over a 30-second-long burst.</p>
<p>Follow-on observations showed that the object repeated pretty regularly, with a periodicity of about 1,320 seconds (more commonly known as 22 minutes). There's a window of about 400 seconds centered on that periodicity, and a burst can appear anywhere within the window and will last anywhere from 30 to 300 seconds. While active, the intensity of GPM J1839–10 can vary, with lots of sub-bursts within the main signal. Occasionally, a window will also go by without any bursts.</p>
<p>A search through archival data showed that signals had been detected at the site as far back as 1988. So, whatever is producing this signal is not really a transient, in the sense that the phenomenon that's producing these bursts isn't a one-time-only event.</p>
<p>The list of known objects that can produce this sort of behavior is short and consists of precisely zero items.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Memoriam: Hans Petter William Sirevåg Selasky (108 pts)]]></title>
            <link>https://lists.freebsd.org/archives/freebsd-announce/2023-July/000076.html</link>
            <guid>36797178</guid>
            <pubDate>Thu, 20 Jul 2023 05:32:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.freebsd.org/archives/freebsd-announce/2023-July/000076.html">https://lists.freebsd.org/archives/freebsd-announce/2023-July/000076.html</a>, See on <a href="https://news.ycombinator.com/item?id=36797178">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="main">
    <label for="invert"></label>
    <header>
    
    <ul>
    
    
    <li><strong><i>Go to: </i></strong> [ <a href="#footer">bottom of page</a> ] [ <a href="https://lists.freebsd.org/archives/freebsd-announce/index.html">top of archives</a> ] [ <a href="https://lists.freebsd.org/archives/freebsd-announce/2023-July/index.html">this month</a> ] </li>
    </ul>
    </header>
    <strong><i>From:</i></strong> Sergio Carlavilla  &lt;carlavilla_at_freebsd.org&gt;<br>
    <strong><i>Date:</i></strong> Thu, 20 Jul 2023 03:58:14 UTC <br>
    <pre>The FreeBSD community was saddened this month by the tragic death of
one of its most prolific contributors.  We learned that Hans Petter
Selasky passed away in a traffic accident in Lillesand, Norway on June
23, 2023 at the age of 41.  Hans was an incredibly brilliant and kind
person, and made many valuable contributions to FreeBSD.  He was
preceded in death by his father Gordon, and is survived by his mother,
Inger Elisabeth, his brothers Mark and Leif Conrad, and his nieces and
nephews Petra, David and Signe.

Hans began contributing to FreeBSD roughly 25 years ago, with fixes to
FreeBSD’s ISDN support.  He was a FreeBSD committer for nearly 15
years, and was best known for re-writing and maintaining the USB
stack.  Hans wrote the webcamd package which supports running Linux
webcam drivers in userspace on FreeBSD, and which enables those of us
using FreeBSD on the desktop to participate in modern
teleconferencing.  Most recently, he worked for Mellanox (now Nvidia)
to support their ConnectX series of high speed NICs on FreeBSD.
Hans’s work included major contributions to the kernel TLS framework,
as well as support for NIC kTLS send and receive offload in the mce(4)
driver, and many improvements to the Linux device driver compatibility
layer.

I first met Hans in 2015, in the context of his work on the mce(4)
driver for Mellanox NICs.  We worked together to make the mce(4)
driver one of highest performance NIC drivers in FreeBSD.  It was
during this time that I learned how brilliant Hans was.  He often had
ideas that sounded “crazy”, but which were actually brilliant.  One
example of this was his idea to sort incoming TCP packets using the
NIC provided RSS flow identifiers in order to present LRO with all
packets from the same TCP connection back to back.  This idea, which I
initially discounted as impractical, was crucial to Netflix being able
to meet our performance target of serving 100Gb/s of video traffic
from a single machine, and continues to save Netflix a large amount of
CPU resources.

Hans was a very kind and welcoming person.  The first time I attended
EuroBSDCon was in 2019 in Lillehammer, Norway where Hans insisted on
playing host to me.  Hans had driven across Norway from his home in
Grimstad to EuroBSDCon in Lillehammer with his father, and took me
around to see the Olympic ski jump, along with several other sites in
the town.  He then took me out to dinner, and back to the house he’d
rented with his father for an evening of great conversation.

Outside of FreeBSD, Hans’s hobbies included music and mathematics.  He
was active in his church, and contributed to its sound team.  He was a
loving and dedicated uncle to his nieces and nephews.  He loved
animals, especially his cat Pumba.

Even if you don’t use FreeBSD yourself, odds are good that Han’s work
touches on your daily life. For example, if you use a Playstation,
chances are you are using Hans’ USB stack.  If you watch Netflix, the
odds are good that the show you’re watching was delivered to you by a
ConnectX NIC running Hans’s mce(4) driver.

Hans, if you are reading this, know that you will be missed.

-- Drew Gallatin
</pre>
    
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sidebery – A Firefox extension for managing tabs and bookmarks in sidebar (155 pts)]]></title>
            <link>https://github.com/mbnuqw/sidebery</link>
            <guid>36796422</guid>
            <pubDate>Thu, 20 Jul 2023 02:57:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mbnuqw/sidebery">https://github.com/mbnuqw/sidebery</a>, See on <a href="https://news.ycombinator.com/item?id=36796422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Sidebery</h2>
<p dir="auto">Firefox extension for managing tabs and bookmarks in sidebar.</p>
<h2 tabindex="-1" dir="auto">Install</h2>
<p dir="auto"><strong>Stable</strong> (4.10.2):
<a href="https://github.com/mbnuqw/sidebery/releases/tag/v4.10.2">Release page</a> |
<a href="https://addons.mozilla.org/firefox/addon/sidebery/" rel="nofollow">Addon page</a> |
<a href="https://addons.mozilla.org/firefox/downloads/file/3994928/sidebery-4.10.2.xpi" rel="nofollow">Install</a><br>
<strong>Beta</strong> (v5.0.0rc4):
<a href="https://github.com/mbnuqw/sidebery/releases/tag/v5.0.0rc4">Release page</a> |
<a href="https://github.com/mbnuqw/sidebery/releases/download/v5.0.0rc4/sidebery-5.0.0rc4.xpi">Install</a></p>
<blockquote>
<p dir="auto">Note: Before installing the beta version make sure to save backup of the Add-on data (Sidebery settings / Help / Export).</p>
</blockquote>
<h2 tabindex="-1" dir="auto">About</h2>
<p dir="auto">Sidebery is a highly configurable sidebar with panels of different types. Some of the key features:</p>
<ul dir="auto">
<li>Vertical tabs panels with tree or flat layout</li>
<li>Bookmarks panels</li>
<li>(v5) History panel</li>
<li>(v5) Search in panels</li>
<li>Customizable context menu</li>
<li>Customizable styles</li>
<li>Snapshots (saved windows/panels/tabs)</li>
<li>...and more</li>
</ul>
<h2 tabindex="-1" dir="auto">Build</h2>
<blockquote>
<p dir="auto">Prerequisites: latest LTS Node.js version</p>
</blockquote>
<ol dir="auto">
<li>Install dependencies: <code>npm install</code></li>
<li>Build all parts of Add-on: <code>npm run build</code></li>
<li>Create Add-on archive in <code>./dist</code>: <code>npm run build.ext</code></li>
</ol>
<p dir="auto">After creating the Add-on archive, you can then use the version in Firefox as follows:</p>
<ol dir="auto">
<li>Open Firefox</li>
<li>Go to <code>about:debugging</code></li>
<li>Go to "This Firefox"</li>
<li>At "Temporary Extensions" click on "Load Temporary Add-on..."</li>
<li>Select the <code>.zip</code> file in the <code>dist</code> directory.</li>
<li>Close the settings tab</li>
<li>Your Firefox now always runs with the development version</li>
<li>For updating: Repeat all steps.</li>
</ol>
<h2 tabindex="-1" dir="auto">Development</h2>
<blockquote>
<p dir="auto">Prerequisites: latest LTS Node.js version</p>
</blockquote>
<p dir="auto">Install dependencies: <code>npm install</code><br>
Build and watch for changes: <code>npm run dev</code><br>
Run browser with Add-on: <code>npm run dev.run -- &lt;firefox-executable&gt;</code></p>
<h2 tabindex="-1" dir="auto">Donate</h2>
<p dir="auto">You can donate to this project, which will motivate me to answer questions, fix reported bugs, implement requested features and generally will speed up development process. Thank you.</p>
<details><summary><b> Bitcoin (BTC) </b></summary>
<div data-snippet-clipboard-copy-content="bc1q2drx3x5pfl0c68urwztvjrwgksg9u3l7mn4g4m"><pre><code>bc1q2drx3x5pfl0c68urwztvjrwgksg9u3l7mn4g4m
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/6276694/215584021-b1eee3ab-ca62-4a81-acb4-cd69c27c734a.png"><img src="https://user-images.githubusercontent.com/6276694/215584021-b1eee3ab-ca62-4a81-acb4-cd69c27c734a.png" alt="btc-bc1q2drx3x5pfl0c68urwztvjrwgksg9u3l7mn4g4m"></a></p>
</details>
<details><summary><b> Ethereum (ETH), USDT (ERC20), USDC (ERC20) </b></summary>
<div data-snippet-clipboard-copy-content="0x11667D20AB328194AEEc68F9385CCcf713607929"><pre><code>0x11667D20AB328194AEEc68F9385CCcf713607929
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/6276694/215587549-39505f92-0f80-43ec-bec1-42bf8cd570c4.png"><img src="https://user-images.githubusercontent.com/6276694/215587549-39505f92-0f80-43ec-bec1-42bf8cd570c4.png" alt="eth-0x11667D20AB328194AEEc68F9385CCcf713607929"></a></p>
</details>
<details><summary><b> USDT (TRC20), USDC (TRC20) </b></summary>
<div data-snippet-clipboard-copy-content="TJEdp1TnsN7Jfhfi9Db8yXKDK8NEUovCZb"><pre><code>TJEdp1TnsN7Jfhfi9Db8yXKDK8NEUovCZb
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/6276694/247570039-bbdefadc-3430-4537-94f1-447244d0e72f.png"><img src="https://user-images.githubusercontent.com/6276694/247570039-bbdefadc-3430-4537-94f1-447244d0e72f.png" alt="TJEdp1TnsN7Jfhfi9Db8yXKDK8NEUovCZb"></a></p>
</details>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto"><a href="https://github.com/mbnuqw/sidebery/blob/v5/LICENSE">MIT</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kevin Mitnick has died (3302 pts)]]></title>
            <link>https://www.dignitymemorial.com/obituaries/las-vegas-nv/kevin-mitnick-11371668</link>
            <guid>36795173</guid>
            <pubDate>Wed, 19 Jul 2023 23:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dignitymemorial.com/obituaries/las-vegas-nv/kevin-mitnick-11371668">https://www.dignitymemorial.com/obituaries/las-vegas-nv/kevin-mitnick-11371668</a>, See on <a href="https://news.ycombinator.com/item?id=36795173">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                

                <h2 id="obit-name">Kevin David Mitnick</h2>

                

                    <p><img src="https://d3eguztg5751m.cloudfront.net/as/assets-mem-com/cmi/8/6/6/1/11371668/20230719_173037764_0_orig.jpg/-/kevin-mitnick-las-vegas-nv-obituary.jpg?crop=%28171.58333333333331%2C0%2C544.9583333333333%2C464%29&amp;cropxunits=696&amp;cropyunits=464&amp;maxheight=650" alt="Obituary of Kevin David Mitnick" layout="fill">
                    </p>

                

    


                <p>
Kevin David Mitnick, 59, died peacefully on Sunday, July 16, 2023, after valiantly battling pancreatic cancer for more than a year. Kevin is survived by his beloved wife, Kimberley Mitnick, who remained by his side throughout their 14-month ordeal. Kimberley is pregnant with their first child. Kevin was ecstatic about this new chapter in his and Kimberley's life together, which has now been sadly cut short.    

Kevin was preceded in death by his devoted mother, Shelly Jaffe, and his loving grandmother, Reba Vartanian, his father, Alan Mitnick, and his half-brother, Adam Mitnick. 

He is survived by his brother-in-law, Ricky Barry and his wife Roxy, and their three children: Millie, Winston, and George, his mother-in-law and father-in-law, Daisy and Andrew Tibbs, his stepmother Nanci King, his great aunt Sophie "Chickie" Leventhal and her longtime partner, Dr. Bob Berkowitz, Kevin's cousins Mitch Leventhal, Karen van den Berg, Jolie Mitnick, Mark Mitnick and Wendy Cohen.

Kevin would also want to acknowledge the tremendous love and support over the years of his dear long-time friends: Michael Morris who showed tremendous dedication to Kevin over many decades, Paul Dryman, Roy Eskapa, Shawn Nunley, Darci and Brianna Wood, Amy Gray, Alex Kasper (Kasperavicius), David Kennedy, David Fugate, Dr. Nick Spirtos, Stu Sjouwerman, and Apollo Robbins. It is impossible to list all of Kevin’s close friends. He was blessed to have so many. You know who you are. Your impact on Kevin was profound. Kevin was also very grateful for the legions of fans who in the mid-to-late 1990’s fueled the global “FREE KEVIN” movement. 

Kevin was an original; much of his life reads like a fiction story. The word that most of us who knew him would use – magnificent. 

He grew up brilliant and restless in the San Fernando Valley in California, an only child with a penchant for mischief, a defiant attitude toward authority, and a love for magic. Kevin's intelligence and delight in holding the rapt attention of audiences revealed themselves early in his childhood and continued throughout his life. In time, he transitioned from pranks and learning magic tricks to phone phreaking, social engineering, and computer hacking.  

When his desire to push boundaries led him too far astray, he landed in juvenile detention and eventually served a couple of stints in prison. His time on the FBI's Most Wanted List was well documented in his New York Times bestselling book, The Ghost in the Wires: My Adventures as the World's Most Wanted Hacker, and his other titles: The Art of Deception, The Art of Intrusion, both co-authored with William Simon, and The Art of Invisibility with Robert Vamosi.  

Kevin emerged from his final prison term, which he deemed a 'vacation,' in January 2000. He was a changed individual, and began constructing a new career, as a White Hat hacker and security consultant. He became a highly sought-after global public speaker, a writer, and established the successful Mitnick Security Consulting. In November 2011, he became the Chief Hacking Officer and part owner of security awareness training company KnowBe4, founded by close friend and business partner Stu Sjouwerman.  

Kevin attracted attention and support from unlikely sources. The bus driver who saw young Kevin memorize the bus schedules, punch cards and punch tool systems so he could ride the buses all day for free testified as a character witness for Kevin during his federal trial. The federal prosecutor offered his testimony that Kevin never tried to take one dime from any of his “victims.” The probation officer assigned to monitor Kevin after prison gave Kevin permission to write his first book on a laptop when he was not yet supposed to have access to computers. Shawn Nunley, the star witness in the FBI's case against Kevin, became so disillusioned with the government's treatment of Kevin that he contacted Kevin's defense team, helped garner Kevin's release, and became one of Kevin's dearest friends. Kevin had an irresistible way of converting foes to friends and keeping them as friends forever.  

To know Kevin was to be enthralled, exasperated, amazed, amused, irritated, and utterly charmed - in equal measure. He was insistent upon being kept updated at all times - even when it meant dozens of phone calls in a single day to the same person - just to be sure he had all the facts. He set incredibly high standards for himself and those who worked with him, and would get lost for hours in complex problems encountered in his work. He spent much of his time working with his Global Ghost Team, an elite pentesting team that spans Argentina, Spain, Germany, Canada, and beyond. Self-educated and driven by eagerness, intense drive, immense curiosity, and seemingly endless energy, he continually expanded his skills as a hacker. He was insatiable in pushing himself, and his team, to pursue excellence in their tradecraft. Kevin was a visionary and an expert at finding a way into every organization he was authorized to hack. He used this knowledge for the greater good and to develop hacking demonstrations that educated the business world and everyday people on how to protect themselves. Kevin’s body of work inspired many individuals to pursue a career in cybersecurity - the industry upon which he leaves an indelible mark and an incredible legacy.

Kevin applied that same relentless tenacity to attempting to beat pancreatic cancer.  He and Kimberley invested thousands of hours in searching for the very best treatments, finding the cutting edge research, and working with the most talented and aggressive doctors and surgeons. That search led him to the University of Pittsburgh Medical Center and Dr. Amer Zureikat, Dr. Randall Brand, and their incredible staff. Each individual did their utmost to help Kevin beat the odds and survive and for that we will always be grateful.  

Kevin was a gentleman: well-mannered and respectful, astoundingly generous with those he loved. He had a unique and unforgettable laugh - a delightful, loud, booming one - which he unleashed unexpectedly and often, frequently accompanied by a mischievous twinkle in his eyes. He saw the funny side of his compulsive perfectionism and work ethic, and enjoyed laughing at his own expense - a rare quality among the best of us. 

We knew him simply as Kev, our beloved friend, a devoted husband, and a trustworthy confidante. Kevin Mitnick crammed a dozen lifetimes into a single prematurely short one. He wanted nothing more than to live -- to keep enjoying the little "BIG" things like quality time with his wife and their growing family, his in-laws, his relatives, and his longtime friends. 

He had so much living left to do. And we know, with broken hearts, that there will never ever be anyone like him again. We will miss him for the rest of our days, hear his voice in our minds, and look forward to reconnecting with him in whatever version of the 'beyond' we each believe in. To imagine that Kev could be there to greet us, likely playing a prank, or inviting us to share an extraordinary meal and conversation, will be heaven indeed. We are each so deeply grateful for the time we had with this truly great man.  

We celebrate that a part of Kevin will live on with the upcoming birth of his and Kimberley’s child. We can only hope that the child knows, as he or she grows,  that around the world, the many friends of his father will be holding them in their hearts. 

Rest in peace, Kev, you are well loved and will be missed always.  

A private memorial and burial service will be held for close friends and family members.  

Donations can be made in Kevin's memory to The National Pancreas Foundation https://pancreasfoundation.org/ or The Equal Justice Initiative  https://eji.org/ 
These are two causes of great importance to Kimberley and Kevin; both organizations  put the majority of donated funds to work in the communities they serve.


                </p>
                <p><a href="#" onclick="return false;">
                    <span>See more</span>
                    
                </a>
            </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Danger of Popcorn Polymer: Incident at the TPC Group Chemical Plant [video] (180 pts)]]></title>
            <link>https://www.youtube.com/watch?v=6-3BFXpBcjc</link>
            <guid>36794756</guid>
            <pubDate>Wed, 19 Jul 2023 23:05:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=6-3BFXpBcjc">https://www.youtube.com/watch?v=6-3BFXpBcjc</a>, See on <a href="https://news.ycombinator.com/item?id=36794756">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[LinkedIn adopts protocol buffers and reduces latency up to 60% (175 pts)]]></title>
            <link>https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/</link>
            <guid>36794430</guid>
            <pubDate>Wed, 19 Jul 2023 22:33:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/">https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/</a>, See on <a href="https://news.ycombinator.com/item?id=36794430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								<p>LinkedIn <a href="https://engineering.linkedin.com/blog/2023/linkedin-integrates-protocol-buffers-with-rest-li-for-improved-m">adopted Protocol Buffers for exchanging data between microservices</a> more efficiently across its platform and integrated it with <a href="https://linkedin.github.io/rest.li/">Rest.li</a>, their open-source REST framework. After the company-wide rollout, they reduced the latency by up to 60% and improved resource utilization at the same time.</p>

<p>The LinkedIn platform employs a microservices architecture, and for years now, <a href="https://en.wikipedia.org/wiki/JSON">JSON</a> has been used as the serialization format for over 50 thousand API endpoints exposed by microservices at LinkedIn. To help their teams build consistent interactions between services, the company created a Java framework called Rest.li, which became open-sourced.</p>

<p>The framework helps create servers and clients that use the <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a> style of communication and abstracts away many aspects of data exchange, including networking, serialization, or service discovery. It primarily supports Java and Python but can also work with Scala, Kotlin, JavaScript, Go, etc.</p>

<p><img alt="" data-src="news/2023/07/linkedin-protocol-buffers-restli/en/resources/1RestLiClientServerFlow-1689612288438.jpeg" src="https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/07/linkedin-protocol-buffers-restli/en/resources/1RestLiClientServerFlow-1689612288438.jpeg" rel="share"></p>

<p><em>Data and Control Flow Between a Rest.li Server and Client (Source: <a href="https://linkedin.github.io/rest.li/user_guide/server_architecture">Rest.li Documentation</a>)</em></p>

<p>JSON is the default serialization format in Rest.li and has been selected due to its wide language support and being human-readable. The last property, however beneficial, introduces problems from the performance (and particularly latency) point of view.</p>

<p><a href="https://www.linkedin.com/in/karthikrg/">Karthik Ramgopal</a> and <a href="https://www.linkedin.com/in/aman1309/">Aman Gupta</a>, engineers at LinkedIn, share challenges with using JSON for inter-service communication:</p>

<blockquote>
<p>The first challenge is that JSON is a textual format, which tends to be verbose. This results in increased network bandwidth usage and higher latencies, which is less than ideal. [...] The second challenge we faced was that due to the textual nature of JSON, serialization and deserialization latency and throughput were suboptimal.</p>
</blockquote>

<p>The team has been considering alternatives to JSON, looking for a compact payload size and high serialization efficiency to reduce latency and increase throughput. They also didn’t want to limit the number of supported language stacks and enable gradual migration by integrating the new serialization mechanism into Rest.li. Finally, after a comprehensive review, they decided to go with <a href="https://protobuf.dev/">Protocol Buffers (Protobuf)</a>, which scored the highest, based on the defined criteria.</p>

<p>The main difficulty around integrating Protocol Buffers into Rest.li was the dynamic schema generation based on the framework's custom schema definition system, <a href="https://linkedin.github.io/rest.li/pdl_schema">PDL</a>. The solution involved generating a symbol table that is used to generate Protobuf schema definition dynamically, but the method for delivering symbol tables varied depending on the type of client. Backend clients fetch and cache symbol tables on-demand, while for web/mobile apps, symbol tables are generated at build-time and included as versioned dependencies.</p>

<p>After changes to the framework were rolled out, the team gradually reconfigured the clients to enable Protobuf instead of JSON using HTTP headers. The result of Protocol Buffers adoption was an average increase in throughput by 6.25% for responses and 1.77% for requests. The team also observed up to 60% latency reduction for large payloads.</p>

<p><img alt="" data-src="news/2023/07/linkedin-protocol-buffers-restli/en/resources/1linkedin-restli-protobuf-1689612288438.jpeg" src="https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2023/07/linkedin-protocol-buffers-restli/en/resources/1linkedin-restli-protobuf-1689612288438.jpeg" rel="share"></p>

<p><em>Latency comparison between JSON and Protobuf (Source: <a href="https://engineering.linkedin.com/blog/2023/linkedin-integrates-protocol-buffers-with-rest-li-for-improved-m">LinkedIn Integrates Protocol Buffers With Rest.li for Improved Microservices Performance</a>)</em></p>

<p>Based on the learnings from the Protocol Buffers rollout, the team is planning to follow up with migration from Rest.li to <a href="https://grpc.io/">gRPC</a>, which also uses Protocol Buffers but additionally supports streaming and has a large community behind it.</p>

<p>See also the&nbsp;InfoQ Podcast:&nbsp;<a href="https://www.infoq.com/podcasts/api-showdown-rest-graphql-grpc/">API Showdown: REST vs. GraphQL vs. gRPC – Which Should You Use?</a></p>

								









  
    <div> <!-- main wrapper for authors section -->
        <h2>About the Author</h2> <!-- section title -->

        
            
                
            
            <div data-id="author-Rafal-Gancarz">
                    <h4><strong>Rafal Gancarz</strong></h4>
                    
                </div>
        
    </div>

							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The open source learning curve for AI researchers (149 pts)]]></title>
            <link>https://www.supervised.news/p/the-open-source-learning-curve-for</link>
            <guid>36793881</guid>
            <pubDate>Wed, 19 Jul 2023 21:43:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.supervised.news/p/the-open-source-learning-curve-for">https://www.supervised.news/p/the-open-source-learning-curve-for</a>, See on <a href="https://news.ycombinator.com/item?id=36793881">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png" width="576" height="576" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:576,&quot;bytes&quot;:1672153,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934ee0-3e58-4043-8ae1-b437f240a847_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>a group of scientists playing in a rock band in front of a large crowd — midjourney</figcaption></figure></div><p>Tri Dao, the creator of an increasingly popular technique in language model development, is running head-first into a new problem for AI researchers: working with the incredibly fast LLM open source community.</p><p><span>Dao is one of the creators of FlashAttention, a technique now adopted by some developers to increase the amount of information that can go into a context window for a large language model more efficiently. The theory goes that if you can find ways to get larger amounts of useful information into that window (</span><a href="https://arxiv.org/pdf/2307.03172.pdf" rel="">without going overboard</a><span>), you could achieve better outcomes because the model has a better idea of what you’re trying to do. FlashAttention got an update this week to FlashAttention-2.</span></p><p><span>Dao this week </span><a href="https://together.ai/blog/tri-dao-flash-attention" rel="">joined</a><span> </span><a href="https://together.ai/" rel="">Together</a><span>, a startup that aims to build open source language models and associated technology, where he’ll work as chief scientist. Together </span><a href="https://www.supervised.news/p/the-most-popular-technique-in-ai" rel="">raised $20 million </a><span>in a round earlier this year led by Lux Capital (an investor in MosaicML which was very recently acquired by Databricks for $1.3 billion). SV Angel, First Round Capital, and Jakob Uszkoreit, one of the co-writers of the Transformers paper, are also investors.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png" width="524" height="354.85164835164835" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:986,&quot;width&quot;:1456,&quot;resizeWidth&quot;:524,&quot;bytes&quot;:490622,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fdd3727-b413-4279-b8b1-14df70e4ebef_1477x1000.png 1456w" sizes="100vw"></picture></div></a><figcaption>Tri Dao, chief scientist of Together</figcaption></figure></div><p>FlashAttention, which speeds up training and fine-tuning of LLMs, could help solve an important piece of the puzzle for making large language models address more practical use cases. For complicated tasks, users could hypothetically inject a lot of instructions and examples for how to address the task.</p><p>Dao is now one of many academic researchers that are now finding their work catching fire on the internet among hobbyists and non-academic practitioners, with developers quickly finding widely-applicable use cases. That’s different than the typical environment of academia, which is often concerned with finding novel technologies and resolving novel use cases that don’t necessarily consider concerns around performance, cost, or practicality.</p><p>“This is probably my first exposure to open source,” he told me in an interview. “Previously we had research where almost all our papers have just made code available, but it was more on the research-y side. And then some interesting researchers would use that and improve the methods. With FlashAttention it was solving a problem that people need and immediately got integrated into a bunch of frameworks like Hugging Face and PyTorch. It’s available in PyTorch and it benefits a huge number of folks. As a result we get great feedback.”</p><p>That, of course, comes with a learning curve to shift to a more production-level mindset that scientists like Dao now face. And it’s one that AI researchers will continue to face going forward as their work gets quickly adapted and iterated on in the open source community.</p><p><span>Dao’s interest in Together started with his colleagues working at the company in the first place. Chris Ré, </span><a href="https://arxiv.org/abs/2205.14135" rel="">a co-writer on the paper for FlashAttention</a><span>, is a co-founder of Together, and co-writer Dan Fu is an academic partner of the company. Together at its core is built around open source methodologies in contrast to closed-source model developers like OpenAI.</span></p><p>“What appealed to me about Together  was their focus on open source,” Dao said. “Philosophically, I think that aligns well with what I wanted to do and how I see the future looking like. The future might not be a few companies offering really good AI models. The future is probably a bunch of really good open source models with a bunch of players in this field. The models will be accessible and lots of people can contribute and improve them, or contribute data and things like that.”</p><p>But the biggest learning curve to academics entering the open source ecosystem is that production-grade open source tools are mostly focused on practicality, rather than achieving some optimal solution without taking concerns about resources, costs, or other considerations like that into account, Bob Van Luijt, CEO of open source vector database startup Weaviate, told me.</p><p>“So, as a researcher, you might squeeze a couple of microseconds out of your&nbsp;algorithm with double the memory footprint, making the index immutable, etc. and eureka! You are state-of-the-art,” he said. “The only downside is that the rather pragmatic community doesn't really adopt it because it's unusable. Pragmatically, the cheapest to run, good enough solution often wins.”</p><p><span>Many of the most widely-used technologies in the open source ecosystem found their roots in academia. One of the most notable ones is Apache Spark, coming out of UC Berkeley. That led to the birth of Databricks, a now $38 billion company that’s </span><a href="https://www.bloomberg.com/news/articles/2023-06-13/databricks-hits-1-billion-in-annual-sales-while-adding-data-warehouse-tool?in_source=embedded-checkout-banner" rel="">generating $1 billion in revenue</a><span> annually. But it certainly wasn’t an easy transition.</span></p><p>“I think it’s been great for Databricks,” CEO Ali Ghodsi told me. “We innovate largely thanks to our academic roots. In academia it’s all about novelty.”</p><p>In the case of Spark, it built up a foundational layer for managing colossal amounts of data. Databricks has also popularize the use of data lake architectures (and the “lakehouse” paradigm by extending functionality to data warehouses) and built essentially a one-stop shop for developing and deploying machine learning models. </p><p>Both have extremely practical use cases for businesses—and can lead to even more new open source technologies, like its open source Delta Lake framework. But it also meant growing beyond producing novel technologies and solving for niche use cases and focusing more on how it could have a measurable impact on products and businesses.</p><p>“I actually actively had to put hurdles in so people would stop innovating,” Ghodsi said. “It’s so ingrained in the bloodstream of the company. The original 20 people were researchers and their modus operandi was, ‘how can we improve how people do things in a novel way?’”</p><p><span>It’s one thing to appeal to data wonks looking to derive insights from their vast piles of data and build functionality around it. It’s another thing to run head-on into a community of avid hobbyists and practitioners that find immediate use cases that aren’t just practical—they’re widely accessible and, perhaps more importantly, </span><em>fun to play with</em><span>. </span></p><p><span>Large language models have captured the attention of those same hobbyists and practitioners in the same way they’ve captured the attention of the general public. That’s partly thanks to ChatGPT, but the </span><a href="https://www.supervised.news/p/the-race-for-open-source-language" rel="">explosion of open source development around language models like LLaMA</a><span> has enabled a vast array of new fun use cases for the technology that’s very accessible to non-scientist types. </span></p><p><span>Meta </span><a href="https://www.supervised.news/p/llama-2-has-entered-the-chat" rel="">released Llama 2, the first version of LLaMA that’s available for commercial use, just yesterday</a><span>. Within just a few hours, </span><a href="https://huggingface.co/TheBloke" rel="">quantized GGML versions of Llama 2 started appearing on Hugging Face</a><span> that would work with </span><a href="https://github.com/ggerganov/llama.cpp" rel="">Llama.cpp</a><span>, a package that enables users to run the models locally on a MacBook Pro. That technology is built using ggml, </span><em><a href="https://github.com/ggerganov/ggml" rel="">another</a></em><a href="https://github.com/ggerganov/ggml" rel=""> open source tensor library</a><span> built </span><a href="https://github.com/ggerganov" rel="">by Llama.cpp creator Georgi Gerganov</a><span>.</span></p><p><span>Tools like that, though, are focused on </span><em>practicality</em><span>—sacrificing certain levels of accuracy in a large language model for the sake of running them locally on a laptop. An emphasis on novelty takes a back seat to managing the tradeoffs to create a technology that has the most widely applicable capabilities.</span></p><p>“If academia wants to adapt to users, they need to broaden the scope, which in turn conflicts with creating state-of-the-art algorithms,” Luijt said. “It's very common that the academic work that's the easiest to use causing the least amount of friction to end users wins.”</p><p><span>Together is particularly focused on that tradeoff. It created the RedPajamas data set to mimic the data set used to train the original LLaMA, </span><a href="https://together.ai/models" rel="">as well as two accompanying open source models</a><span> in the form of RedPajamas-INCITE, to address a much broader set of use cases. It’s part of the appeal of scientists joining a startup like Together, where they can potentially have a larger impact than incrementally advancing the field with advanced research.</span></p><p>But there is an enormous appeal to research and academia, especially as we start to run into walls around the performance of Transformers, the most popular technique for AI today. The restrictions around context windows is one example that FlashAttention addresses. But Dao also said he’s exploring completely novel use cases that go beyond Transformers at Together—which tries to strike a balance between research and developing practical tools.</p><p>“If we want to understand why Transformers is so good, we should try to develop alternatives and see if we can come up with something just as good,” He said. “If we can’t maybe there’s something special about Transformers. If theres evidence theres alternatives, that gives us info about what’s important for models to perform well.”</p><p data-attrs="{&quot;url&quot;:&quot;https://www.supervised.news/p/the-open-source-learning-curve-for?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.supervised.news/p/the-open-source-learning-curve-for?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p><strong><a href="https://www.bloomberg.com/news/articles/2023-07-19/apple-preps-ajax-generative-ai-apple-gpt-to-rival-openai-and-google#xj4y7vzkg" rel="">Apple Tests ‘Apple GPT,’ Develops Generative AI Tools to Catch OpenAI (Bloomberg)</a><span>:</span></strong><span> Mark Gurman at Bloomberg basically confirms what we thought to be true with Apple building an in-house LLM for its products. Apple noted that it was using transformers-based approaches for language modeling in autocorrect, which was a clear indicator that there was something new under the hood. It also, per Bloomberg, built a framework called Ajax to build LLMs. Perhaps Apple is interested in personalized fine-tuned models for every user hosted directly on-device?</span></p><p><strong><a href="https://arxiv.org/abs/2307.09009" rel="">How is ChatGPT's behavior changing over time? (ArXiv)</a><span>: </span></strong><span>A team out of Stanford and Berkeley, including Databricks CTO Mateo Zaharia, investigate what seemed to be a pet theory on the internet that the quality of OpenAI’s language models was decreasing over time. The paper suggests that, indeed, that may be the case. OpenAI said it was updating its models earlier this year, including new feedback (typically RLHF) to “improve” the model. But we’re seeing lately that any changes and tuning can have a big downstream impact on the actual responses from the model.</span></p><p><strong><a href="https://www.newcomer.co/p/unstructured-raises-25-million-to" rel="">Unstructured Raises $25 Million to Bring Order to the Chaos for Language Model Data (Newcomer)</a></strong><span>: It wasn’t going to be long before we started getting new tools that make vector databases even more useful. Unstructured improves the process of transitioning company data into a vector database like Weaviate, Pinecone, or Chroma. Unstructured is getting $25 million in a round led by Madrona and including Bain Capital Ventures. Weaviate CEO Bob van Luijt and LangChain creator/CEO Harrison Chase also participated in the round. </span></p><p><strong><a href="https://www.nytimes.com/2023/07/18/technology/openai-chatgpt-facial-recognition.html" rel="">OpenAI Worries About What Its Chatbot Will Say About People’s Faces (New York Times)</a></strong><em><strong>:</strong></em><span> The Times details some reasoning behind OpenAI restricting its multimodal capabilities for GPT-4—namely, avoiding use cases that could invade privacy. But with all the major developers racing to adapt multi-modal capabilities into next-generation models it’s really only a matter of time before one comes out that’ll wade into that very complicated territory.</span></p><p><em>If you have any tips, please send me a note at m@supervised.news or contact me directly on Signal at +1-415-690-7086. As always, please send any and all feedback my way.</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Geothermal Ahead of Schedule (158 pts)]]></title>
            <link>https://twitter.com/TimMLatimer/status/1681304496234991620</link>
            <guid>36793372</guid>
            <pubDate>Wed, 19 Jul 2023 21:05:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/TimMLatimer/status/1681304496234991620">https://twitter.com/TimMLatimer/status/1681304496234991620</a>, See on <a href="https://news.ycombinator.com/item?id=36793372">Hacker News</a></p>
Couldn't get https://twitter.com/TimMLatimer/status/1681304496234991620: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[A framework to securely use LLMs in companies – Part 1: Overview of Risks (128 pts)]]></title>
            <link>https://boringappsec.substack.com/p/edition-21-a-framework-to-securely</link>
            <guid>36792968</guid>
            <pubDate>Wed, 19 Jul 2023 20:40:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boringappsec.substack.com/p/edition-21-a-framework-to-securely">https://boringappsec.substack.com/p/edition-21-a-framework-to-securely</a>, See on <a href="https://news.ycombinator.com/item?id=36792968">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1794460,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88c5b0d4-df8b-4087-9404-4b49f0319565_1024x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>This image was generated by the Dall-E as a response to the prompt: “Confused AI bot, pencil sketch”</figcaption></figure></div><p>The potential of using AI in enterprises has been on the rise for the last few years. The release of ChatGPT made it apparent to enterprises that AI could supercharge their existing applications. However, like with any new technology, the usage of LLMs also brings with it some risks. Depending on how the LLMs are deployed (training an in-house LLM v/s 3rd party LLMs) and how the LLMs are used (by individuals to supercharge their work v/s integrating with LLM APIs in applications), the risks LLMs pose will change. This post outlines key risks and helps prioritize them based on your organization’s use case.&nbsp;</p><p>Most posts in this newsletter are based on my personal AppSec experience. Like most people in Appsec, I do not have any significant experience building, managing, or securing LLM usage at scale.&nbsp; In that sense, this post is a departure from the norm. However, I am still publishing this as a submission to the growing body of work on this subject. In the coming months, I hope we will have some consensus on how to address this important topic.</p><p>Here are a few caveats you should keep in mind before you read this document:&nbsp;</p><ol><li><p>There is a reasonable chance that some of this information will be outdated a few weeks from now. That is just the nature of fast-changing technology. Most posts in Boring AppSec are “timeless topics”. This one is timely and fluid.&nbsp;</p></li><li><p>Most of my research for this post is secondary. This means, not all the information provided here is based on my experience securing LLMs. Much of it is from other authors who have published excellent, public work (e.g.: OWASP Top 10 for LLMs). For instance, if an excellent description of a vulnerability already exists, it has been used here. All the material which has influenced my thinking is linked in the references section.&nbsp;</p><ol><li><p>Side note: The reference section has some amazing resources. Please feel free to enter rabbit holes :)&nbsp;</p></li></ol></li></ol><p>There are many ways to slice and dice LLM use cases, but from a Security perspective, it may help to categorize them as follows:&nbsp;</p><ol><li><p><strong>Use-cases</strong></p><ol><li><p>Employees use LLM tools to improve productivity or help with their work. (e.g.: Github Copilot, ChatGPT, and Google BARD).&nbsp;</p></li><li><p>Integrating applications with LLM APIs&nbsp;</p><ol><li><p><strong>Internal applications</strong><span> leveraging LLMs to improve their efficacy or inform internal decision-making.&nbsp;</span></p></li><li><p><strong>Customer-facing applications</strong><span>, where customer input informs part (or all) of the prompts sent to the LLM, and the response is consumed in some form by the customer.</span></p><ol><li><p>Note: I am using “customer” as a proxy for “someone external”. In your context, this could be a non-paying user, a business that uses your product, and so on.  </p></li></ol></li></ol></li></ol></li><li><p><strong>Deployment type.</strong><span> Companies can choose from 2 broad paths:</span></p><ol><li><p><strong>3rd party LLMs: </strong><span>Integrating applications with 3rd party LLMs such as OpenAI.&nbsp;</span></p></li><li><p><strong>Self-hosted LLMs: </strong><span>Deploying an open-source LLM in-house and using proprietary data to train the LLM.</span></p></li></ol></li></ol><blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png" width="1200" height="486.2669245647969" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png&quot;,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:419,&quot;width&quot;:1034,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ab91f85-b068-44c8-acfc-a81106761ea7_1034x419.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Common LLM applications categorized by use-case and deployment type.</figcaption></figure></div></blockquote><p>A few points on the trade-off between self-hosted and 3rd party LLMs:</p><ol><li><p>Unless significant investments are made in building a cross-functional team involving ML engineers, security engineers, and privacy professionals, self-hosting an open-source LLM and training brings more security risk than leveraging a trusted 3rd party.</p></li><li><p>&nbsp;On the flip side, 3rd party hosted LLMs pose more privacy and data security risks. Over time, 3rd party hosted LLMs can also get really expensive.&nbsp;&nbsp;</p></li><li><p>If using an LLM has to be a business differentiator for your organization, deploying and training a model in-house is the right way to go</p></li><li><p>Irrespective of which path your organization chooses, it is critical to understand the risks it poses and find ways to manage them.&nbsp;</p></li></ol><p>There are many excellent lists of risks to using LLMs (see references for a solid list). In this post, I am focusing on seven patterns that can pose a high level of risk for your organization. While it's clear that new risk categories will emerge over time, this list should give us a broad overview of the risks out there and more importantly, help us understand what risks are applicable to our organization.&nbsp;</p><p>While it’s tempting to solve all risks, it is important to prioritize.  Companies should evaluate which use-case and deployment type is most prevalent in their organization and focus on mitigating High-risk items first. The below table provides a summary of risk levels for the use cases and deployment models mentioned above. In the next section, we will dive deeper into each of the highlighted “high-risk” scenarios. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png" width="1200" height="457.3875802997859" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png&quot;,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:534,&quot;width&quot;:1401,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:303578,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382b384e-a26a-4835-a3b2-c8d473dcbae7_1401x534.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Risk ranking of the most common LLM risks, categorized by use-case and deployment type</figcaption></figure></div><ol><li><p><strong>Prompt injection: </strong><em><span>“Prompt Injection Vulnerabilities in LLMs involve crafty inputs leading to undetected manipulations. The impact ranges from data exposure to unauthorized actions, serving attacker goals.” - </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em></p><ol><li><p><strong>Customer-facing applications: </strong><span>A malicious customer can take advantage of the concatenation of user input to a pre-written prompt string to override the guardrails put in place for user prompts. Much like other forms of injection, user input is used as instructions to control the outcome of the application.&nbsp;</span></p></li><li><p><strong>Self-hosted LLMs:</strong><span> An attacker can take advantage of the concatenation of user input to a pre-written prompt string to override the guardrails put in place for user prompts. Much like other forms of injection, user input is used as instructions to control the outcome of the application. Prompt injection is a broad attack vector and can take many forms, including&nbsp;</span></p><ol><li><p>Trusting data from plugins or other third parties&nbsp;</p></li><li><p>Exploiting LLM hallucination. If an attacker can predict that LLMs recommend made-up package names, the attacker can create and publish such packages with malicious software in them&nbsp;</p></li></ol></li></ol></li><li><p><strong>Data leakage:</strong><span> </span><em><span>“Data leakage in LLMs can expose sensitive information or proprietary details, leading to privacy and security breaches. Proper data sanitization and clear terms of use are crucial for prevention.” - </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em></p><ol><li><p><strong>Employees using online tools:</strong><span> Given there is a free version of these tools available, there is a risk of leaking sensitive data to the LLM tool. This risk is higher with tools like ChatGPT, where the prompts entered are used to train the underlying model (e.g.: Samsung code data). Note that other tools (such as Bard from Google) do not use prompts to train their model. However, the risk of leaking data to the tool owner still exists.</span></p></li><li><p><strong>Customer-facing applications:&nbsp; </strong><span>When used without sufficient guardrails, customer-facing applications leveraging LLMs can leak sensitive (PHI, PII) and proprietary information to the customer.</span></p><ol><li><p><em>Note: Even when guardrails are in place, Prompt Injection can be used to bypass the guardrails, leading to data leakage</em></p></li></ol></li><li><p><strong>3rd party LLMs:</strong><span> Insecure usage can lead to sensitive information (PII, PHI) or proprietary details being leaked to 3rd parties leading to privacy and security breaches.&nbsp;&nbsp;</span></p></li></ol></li><li><p><strong>Training data poisoning: </strong><em><strong>- </strong><span>“LLMs learn from diverse text but risk training data poisoning, leading to user misinformation. Overreliance on AI is a concern. Key data sources include Common Crawl, WebText, OpenWebText, and books” - </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em></p><ol><li><p><strong>Self-hosted LLMs: </strong><span>Training data poisoning is a significant risk for self-hosted LLMs. Insecure data training can lead to bias and hallucination. We need to break down the problem into various stages of the machine learning pipeline. For instance: Bias can be introduced in the training data (e.g.: all the training data came from a specific neighborhood), in the classifier algorithm, or in the prediction engine.&nbsp;</span></p></li></ol></li><li><p><strong>Denial of service: </strong><span>“</span><em><span>An attacker interacts with an LLM in a way that is particularly resource-consuming, causing the quality of service to degrade for them and other users, or for high resource costs to be incurred.” - </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em><span>&nbsp;</span></p><ol><li><p><strong>Self-hosted LLMs:</strong><span>&nbsp; LLMs are resource intensive to train and maintain. Carefully crafted prompts or malicious training data can lead to the LLM consuming a lot of infrastructure resources which can lead to resource exhaustion and hence, denial of service.&nbsp;</span></p></li></ol></li><li><p><strong>Money loss: </strong><span>Much like cloud computing resources, 3rd party LLMs charge based on consumption (OpenAI uses “tokens”, others use a similar mechanism). Unvalidated usage of these APIs can lead to a massive, unplanned cost escalation.&nbsp;</span></p><ol><li><p><strong>3rd party LLMs: </strong><span>Most 3rd party LLMs charge by usage. Higher the usage, the higher the cost. Attackers can carefully craft prompts to lead to massive charges on LLM usage. If the usage is capped to limit money loss, similar attacks can lead to DoS (denial of service).&nbsp;&nbsp;&nbsp;</span></p></li></ol></li><li><p><strong>Insecure supply chain: </strong><em>“LLM supply chains risk integrity due to vulnerabilities leading to biases, security breaches, or system failures. Issues arise from pre-trained models, crowdsourced data, and plugin extensions.” - </em><span>&nbsp;</span><em><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em><span>&nbsp;</span></p><ol><li><p><strong>3rd party LLMs:&nbsp;</strong><span> Most popular 3rd party LLMs allow developers to build plugins on top of their platform (e.g.: OpenAI). While we may have a trusted relationship with the LLM (through an NDA and a contract), managing risk from plugins is harder. While the risk is significant, this is not very different from the risk of using other platforms (e.g.: using Github actions and importing 3rd party actions).&nbsp;</span></p></li><li><p><strong>Self-hosted LLMs: &nbsp;</strong><span>An in-house LLM relies heavily on various kinds of third-party components. From malicious training data (which we may not have complete control of) to outsourcing suppliers who will train the data, there are many supply chain attack vectors to worry about</span></p></li></ol></li><li><p><strong>Overreliance on LLM-generated content: </strong><em>“Overreliance on LLMs can lead to misinformation or inappropriate content due to "hallucinations." Without proper oversight, this can result in legal issues and reputational damage” </em><span>&nbsp;-</span><em> </em><span>&nbsp;</span><em><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">OWASP Top 10 for LLMs</a></em><span>&nbsp;</span></p><ol><li><p><strong>Employees using online tools: </strong><span>A common use case is to use such tools to generate source code. While the tool can help reduce development time, it can lead to using insecure (e.g.: code generated is susceptible to SSRF) or unlicensed code (e.g.: the code generated is from an open source repo. Even outside engineering, using LLM responses to make important decisions at work can lead to unpredictable outcomes.&nbsp;</span></p></li><li><p><strong>Internal applications:</strong><span>&nbsp; Given its propensity to hallucinate and introduce undesired bias, relying solely on LLM output can lead to undesirable outcomes. This can especially lead to systemic, long-term issues if the training data itself is poisoned.</span><strong>&nbsp;</strong></p></li><li><p><strong>Customer-facing applications: </strong><span>When systems excessively depend on LLMs for decision-making or content generation without adequate oversight, validation mechanisms, or risk communication. LLMs are also susceptible to "hallucinations," producing content that is factually incorrect, nonsensical, or inappropriate. These hallucinations can lead to misinformation, miscommunication, potential legal issues, and damage to an organization's reputation if unchecked.</span></p></li></ol></li></ol><p>There is a possibility that there is no clarity on which of these use cases and deployment models are prevalent in your organization. In that scenario, your first job as a Security team would be to understand how LLMs are currently used and understand the plans for future usage. On the data gathered, you can apply the above framework to narrow down the risks that matter most to your organization.</p><p><span>That’s it for today! Are there significant risks that are missed in this post? What other aspects of leveraging LLMs worry you? Is there value in having yet another author talking about securing LLMs?  Tell me more! You can drop me a message on </span><a href="https://twitter.com/JubbaOnJeans" rel="">Twitter</a><span>, </span><a href="https://www.linkedin.com/in/anandsandesh/" rel="">LinkedIn</a><span>, or </span><a href="mailto:anand.sandesh@gmail.com" rel="">email</a><span>. If you find this newsletter useful, share it with a friend, or colleague, or on your social media feed. </span></p><p data-attrs="{&quot;url&quot;:&quot;https://boringappsec.substack.com/p/edition-21-a-framework-to-securely?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://boringappsec.substack.com/p/edition-21-a-framework-to-securely?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><ol><li><p>Playgrounds</p><ol><li><p><span>Aviary Explorer: A way to compare results from open source LLMs: </span><a href="https://aviary.anyscale.com/" rel="">Aviary Explorer (anyscale.com)</a></p></li><li><p><span>A playground for prompt injection. Basically tricking LLMs in revealing secrets </span><a href="https://gandalf.lakera.ai/" rel="">https://gandalf.lakera.ai/</a><span>&nbsp;</span></p></li><li><p><span>Holistic evaluation of LLMs (HELM) from Stanford: </span><a href="https://crfm.stanford.edu/helm/latest/" rel="">https://crfm.stanford.edu/helm/latest/</a></p></li></ol></li><li><p>Security </p><ol><li><p><span>LLM OWASP Top 10: Very useful, but some of them are a stretch. Currently at v0.5&nbsp; </span><a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf" rel="">https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v05.pdf</a></p></li><li><p><span>Prompt injection methods: </span><a href="https://github.com/greshake/llm-security" rel="">GitHub - greshake/llm-security: New ways of breaking app-integrated LLMs</a><span>&nbsp;</span></p></li><li><p><span>Skyflow data privacy for GPT: </span><a href="https://www.skyflow.com/post/generative-ai-data-privacy-skyflow-gpt-privacy-vault" rel="">https://www.skyflow.com/post/generative-ai-data-privacy-skyflow-gpt-privacy-vault</a></p></li><li><p><span>Lakera is an AI security company. They have specific products to protect against Prompt injection: </span><a href="https://www.lakera.ai/llms" rel="">Lakera Guard | Unlock LLMs for Production | Lakera – Protecting AI teams that disrupt the world.</a><span>&nbsp;</span></p></li><li><p><span>Daniel Miessler on AI Attack Surface Map: </span><a href="https://danielmiessler.com/blog/the-ai-attack-surface-map-v1-0/" rel="">https://danielmiessler.com/blog/the-ai-attack-surface-map-v1-0/</a><span>&nbsp;</span></p></li><li><p><span>&nbsp;</span><a href="https://www.salesforce.com/news/stories/generative-ai-guidelines/" rel="">Generative AI: 5 Guidelines for Responsible Development - Salesforce News</a></p></li><li><p><span>Nvidia’s AI red team framework: :</span><a href="https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/" rel="">https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/</a></p></li><li><p><span>IBM AI fairness 360 tools to detect bias: </span><a href="https://www.ibm.com/opensource/open/projects/ai-fairness-360/" rel="">https://www.ibm.com/opensource/open/projects/ai-fairness-360/</a></p></li><li><p><span>tldrsec on a similar topic: </span><a href="https://tldrsec.com/p/securely-build-product-ai-machine-learning" rel="">How to securely build product features using AI APIs (tldrsec.com)</a></p></li></ol></li><li><p>Enterprise related things</p><ol><li><p><a href="https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/" rel="">Enterprise architectures for LLMs</a><span> (a16z)</span></p></li><li><p><span>Should you buy or build: </span><a href="https://techcrunch.com/2023/01/25/when-it-comes-to-large-language-models-should-you-build-or-buy/" rel="">When it comes to large language models, should you build or buy? | TechCrunch</a><span>&nbsp;</span></p></li><li><p><span>Companies blocking ChatGPT and other publicly trained chatbots: </span><a href="https://fortune.com/2023/05/19/chatgpt-banned-workplace-apple-goldman-risk-privacy/" rel="">Employees are banned from using ChatGPT at these companies | Fortune</a></p></li><li><p><span>&nbsp;Google thinks open source LLMs will be as good as OpenAI soon: </span><a href="https://www.bigtechwire.com/2023/05/04/googles-leaked-document-reveals-open-source-threat-a-new-era-in-language-models/" rel="">Google’s Leaked Document Reveals Open Source Threat: A New Era in Language Models | BigTechWire</a><span>&nbsp;</span></p></li><li><p><span>Triveto language model whitepaper: </span><a href="https://www.truveta.com/wp-content/uploads/2023/04/Truveta-Language-Model.pdf" rel="">https://www.truveta.com/wp-content/uploads/2023/04/Truveta-Language-Model.pdf</a></p></li></ol></li></ol></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A full episode of South Park generated by AI (136 pts)]]></title>
            <link>https://fablestudio.github.io/showrunner-agents/?mc_cid=f9d1eb56dc&amp;mc_eid=bbcd57583d</link>
            <guid>36792566</guid>
            <pubDate>Wed, 19 Jul 2023 20:17:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fablestudio.github.io/showrunner-agents/?mc_cid=f9d1eb56dc&#x26;mc_eid=bbcd57583d">https://fablestudio.github.io/showrunner-agents/?mc_cid=f9d1eb56dc&#x26;mc_eid=bbcd57583d</a>, See on <a href="https://news.ycombinator.com/item?id=36792566">Hacker News</a></p>
<div id="readability-page-1" class="page">


  

<!-- Paper abstract -->
<div>
        <h2>Abstract</h2>
        <p>
            In this work we present our approach to generating high-quality episodic content for IP's (Intellectual Property) using large language models (LLMs), custom state-of-the art diffusion models and our multi-agent simulation for contextualization, story progression and behavioral control.
          </p>
      </div>
<!-- End paper abstract -->



<!-- Youtube video -->
<div>
          <h2>Video</h2>
          <div>
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
           
            <p><iframe src="https://player.vimeo.com/video/830748401?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen="" title="To Infinity and Beyond: Showrunner Systems in Multi-Agent Simulations Example Generated Episode"></iframe></p>
          </div>
        </div>
<!-- End youtube video -->



<div>
          <p>
            <i>Note: This web-version of the paper does not include footnotes. Please refer to the pdf-version above for all citations and references.</i>
          </p>

          <h2>Creative limitations of existing generative AI Systems</h2>
          <p>
              Current generative AI systems such as Stable Diffusion (Image Generator) and ChatGPT (Large Language Model) excel at short-term general tasks through prompt engineering. However, they do not provide contextual guidance or intentionality to either a user or an automated generative story system (showrunner) as part of a long-term creative process which is often essential to producing high-quality creative works, especially in the context of existing IP's.
             </p>
        </div>



<div>
      <h3>Living with uncertainty</h3>
      <div>
        <p>
          By using a multi-agent simulation as part of the process we can make use of data points such as a character's history, their goals and emotions, simulation events and localities to generate scenes and image assets more coherently and consistently aligned with the IP story world. The IP-based simulation also provides a clear, well known context to the user which allows them to judge the generated story more easily.
          Moreover, by allowing them to exert behavioral control over agents, observe their actions and engage in interactive conversations, the user's expectations and intentions are formed which we then funnel into a simple prompt to kick off the generation process.
          </p>
          <p>
       Our simulation is sufficiently complex and non-deterministic to favor a positive disconfirmation. Amplification effects can help mitigate what we consider an undesired "slot machine" effect which we'll briefly touch on later. We are used to watching episodes passively and the timespan between input and "end of scene/episode" discourages immediate judgment by the user and as a result reduces their desire to "retry". This disproportionality of the user's minimal input prompt and the resulting high-quality long-form output in the form of a full episode is a key factor for positive disconfirmation.
      </p>
      <p>
        While using and prompting a large language model as part of the process can introduce <i>"several challenges"</i>. Some of them, like hallucinations, which introduce uncertainty or in more creative terms "unexpectedness", can be regarded as creative side-effects to influence the expected story outcome in positive ways. As long as the randomness introduced by hallucination does not lead to implausible plot or agent behavior and the system can recover, they act as happy-accidents, a term often used during the creative process, further enhancing the user experience.

      </p>
  </div>
<br>
  <div>
    <h3>The Issue of 'The Slot Machine Effect' in current Generative AI tools</h3>
    <div>
      <p>
        The Slot Machine Effect refers to a scenario where the <i>generation of AI-produced content feels more like a random game of chance rather than a deliberate creative process.</i> This is due to the often unpredictable and instantaneous nature of the generation process.

Current off-the-shelf generative AI systems do not support or encourage multiple creative evaluation steps in context of a long-term creative goal.
Their interfaces generally feature various settings, such as sliders and input fields which increase the level control and variability. The final output however, is generated almost instantaneously by the press of a button.
This instantaneous generation process results in immediate gratification providing a dopamine rush to the user. This reward mechanism would be generally helpful to sustain a multi-step creative process over long periods of time but current interfaces, the frequency of the reward and a lack of progression (stuck in an infinite loop) can lead to negative effects such as frustration, the intention-action gap or <i>a loss of control over the creative process.</i>
<i>The gap results from behavioral bias favoring immediate gratification</i>, which can be detrimental to long-term creative goals.
</p>
<p><img src="https://fablestudio.github.io/showrunner-agents/static/images/ui%20comparison%20copy.png" alt="Comparison of Interfaces: Stable Diffusion, ChatGPT, Runway Gen-2"></p><p>
  While we do not directly solve these issues through interfaces, the contextualization of the process in a simulation and the above mentioned disproportionality and timespan between input and output help mitigate them. In addition we see opportunities in the simulation for in-character discriminators that participate in the creative evaluation process, such as an agent reflecting on the role they were assigned to or a scene they should perform in.
</p>
<p>
  The multi-step "trial and error" process of the generative story system is not presented to the user, therefore it doesn't allow for intervention or judgment, avoiding the negative effects of immediate gratification through a user's "accept or reject" decisions. It does not matter to the user experience how often the AI system has to retry different prompt chains as long as the generation process is not negatively perceived as idle time but integrated seamlessly with the simulation gameplay. 
  The user only acts as the discriminator in the very end of the process after having watched the generated scene or episode. This is also an opportunity to utilize the concept of Reinforcement Learning through Human Feedback (RLHF) for improving the multi-step creative process and as a result the automatically generated episode.
  </p>
     
</div>

</div></div>


<div>
          <h2>Large Language Models</h2>
          <div>
            <p>
              LLMs represent the forefront of natural language processing and machine learning research, demonstrating exceptional capabilities in understanding and generating human-like text. They are typically built on Transformer-based architectures, a class of models that rely on self-attention mechanisms. Transformers allow for efficient use of computational resources, enabling the training of significantly larger language models. GPT-4, for instance, comprises billions of parameters that are trained on extensive datasets, effectively encoding a substantial quantity of worldly knowledge in their weights.
            <br>
            </p><p><img src="https://fablestudio.github.io/showrunner-agents/static/images/The-Transformer-model-architecture%20(1).png" alt="">
            </p>
            <p>

          Central to the functioning of these LLMs is the concept of vector embeddings. These are mathematical representations of words or phrases in a high-dimensional space. These embeddings capture the semantic relationships between words, such that words with similar meanings are located close to each other in the embedding space. In the case of LLMs, each word in the model's vocabulary is initially represented as a dense vector, also known as an embedding. These vectors are adjusted during the training process, and their final values, or "embeddings", represent the learned relationships between words.
          During training, the model learns to predict the next word in a sentence by adjusting the embeddings and other parameters to minimize the difference between the predicted and actual words. The embeddings thus reflect the model's understanding of words and their context. Moreover, because Transformers can attend to any word in a sentence regardless of its position, the model can form a more comprehensive understanding of the meaning of a sentence. This is a significant advancement over older models that could only consider words in a limited window. The combination of vector embeddings and Transformer-based architectures in LLMs facilitates a deep and nuanced understanding of language, which is why these models can generate such high-quality, human-like text.
          </p>
            
              <p><img src="https://fablestudio.github.io/showrunner-agents/static/images/vector_embedding.png" alt=""></p><p>

          <br>
          As was mentioned previously, transformer-based language models excel at short-term general tasks. They are regarded as fast-thinkers. [Kahneman].
          Fast thinking pertains to instinctive, automatic, and often heuristic-based decision-making, while slow thinking involves deliberate, analytical, and effortful processes. LLMs generate responses swiftly based on patterns learned from training data, without the capacity for introspection or understanding the underlying logic behind their outputs. However, this also implies that LLMs lack the ability to deliberate, reason deeply, or learn from singular experiences in the way that slow-thinking entities, such as humans, can. While these models have made remarkable strides in text generation tasks, their fast-thinking nature may limit their potential in tasks requiring deep comprehension or flexible reasoning. 
          More recent approaches to imitate slow-thinking capabilities such as prompt-chaining (see Auto-GPT) showed promising results. Large language models seem powerful enough to act as their own discriminator in a multi-step process. <i>This can dramatically improve the ability to reason in different contexts, such as solving math problems.</i>
        <br>
              
              We make heavy use of GPT-4 to influence the agents in the simulation as well as generating the scenes for the south park episode. 
              Since transcriptions of most of the south park episodes are part of GPT-4's training dataset, it already has a good understanding of the character's personalities, talking style as well as overall humor of the show, eliminating the need for a custom fine-tuned model.
            <br>   
            However, we do imitate slow-thinking as part of a multi-step creative process. For this we use different prompt chains to compare and evaluate the events of different scenes and how they progress the overall story towards a satisfactory, IP-aligned result.
            Our attempt to generate episodes through prompt-chaining is due to the fact that story generation is a highly discontinuous task. <i>These are tasks where the content generation cannot be done in a gradual or continuous way, but instead requires a certain ”Eureka” idea that accounts for a discontinuous leap in the progress towards the solution of the task. The content generation involves discovering or inventing a new way of looking at or framing the problem, that enables the generation of the rest of the content. Examples of discontinuous tasks are solving a math problem that requires a novel or creative application of a formula, writing a joke or a riddle, coming up with a scientific hypothesis or a philosophical argument, or creating a new genre or style of writing.</i>
            </p>
            <h3>Diffusion models
            </h3><p>

            Diffusion models operate on the principle of gradually adding or removing random noise from data over time to generate or reconstruct an output. The image starts as random noise and, over many steps, gradually transforms into a coherent picture, or vice versa. 
          <br>
            <img src="https://fablestudio.github.io/showrunner-agents/static/images/strip_00000_1.png" alt="Image strip of a diffusion model generating a south park background">


            In order to train our custom diffusion models, we collected a comprehensive dataset comprising approximately 1200 characters and 600 background images from the TV show South Park. This dataset serves as the raw material from which our models learned the style of the show. 
        <br>
        To train these models, we employ Dream Booth.
The result of this training phase is the creation of two specialized diffusion models. 
 
<br>
The first model is dedicated to generating single characters set against a keyable background color. This facilitates the extraction of the generated character for subsequent processing and animation, allowing us to seamlessly integrate newly generated characters into a variety of scenes and settings. In addition, the character diffusion model allows the user to create a south park character based on their own looks via the image-to-image process of stable diffusion and then join the simulation as an equally participating agent. With the ability to clone their own voice, it's easy to imagine a fully realized autonomous character based on the user's characteristic looks, writing style and voice.
</p><p><img src="https://fablestudio.github.io/showrunner-agents/static/images/edward_southpark.png" alt="Summary of our findings">

</p>

<p>


The second model is trained to generate clean backgrounds, with a particular focus on both exterior and interior environments. This model provides the 'stage' upon which our generated characters can interact, allowing for a wide range of potential scenes and scenarios to be created.
</p><p><img src="https://fablestudio.github.io/showrunner-agents/static/images/stable_diffusion_background_exterior.png" alt="Summary of our findings">
  <img src="https://fablestudio.github.io/showrunner-agents/static/images/stable_diffusion_background_interior.png" alt="Summary of our findings">
</p><p>

However, it's important to note that the images produced by these models are inherently limited in their resolution due to the pixel-based nature of the output. To circumvent this limitation, we post-process the generated images using an AI upscaling technique, specifically R-ESRGAN-4x+-Anime6B, which refines and enhances the image quality.
</p><p><img src="https://fablestudio.github.io/showrunner-agents/static/images/vector_graphics-gpt4.png" alt="Example of a GPT-4 drawn TiKZ vector shape representing a unicorn">
</p>
<p>
For future 2D interactive work, training custom transformer based models that are capable of generating vector-based output would have several advantages. Unlike pixel-based images, vector graphics do not lose quality when resized or zoomed, thus offering the potential for infinite resolution. This will enable us to generate images that retain their quality and detail regardless of the scale at which they are viewed. Furthermore, vector based shapes are already separated into individual parts, solving pixel-based post-processing issues with transparency and segmentation which complicate the integration of generated assets into procedural world building and animation systems.</p><p><img src="https://fablestudio.github.io/showrunner-agents/static/images/vector%20gpt%204%20copy.png" alt="Example of a house and a street drawn by GPT-4 in SVG">
</p>
<br>

           
          </div>
        </div>



<div>
      <h2>Episode Generation</h2>
      <div>
        <p>
          We define an episode as a sequence of dialogue scenes in specific locations which add up to a total runtime of a regular 22 min south park episode.
<br>

</p><p>
  In order to generate a full south park episode, we prompt the story system with a high level idea, usually in the form of a title, synopsis and major events we want to see happen over the course of 1 week in simulation time (=roughly 3 hours of play time).

</p>
<p>
  From this, the story system automatically extrapolates up to 14 scenes by making use of simulation data as part of a prompt chain. The showrunner system takes care of casting the characters for each scene and how the story should progress through a plot pattern. Each scene is associated with a plot letter (e.g. A, B, C) which is then used by the showrunner to alternate between different character groups and follow their individual storylines over the course of an episode to keep the user engaged.

</p>
<p>
  In the end, each scene simply defines the location, cast and dialogue for each cast member.
The scene is played back according to the plot pattern (e.g. ABABC) after the staging system and AI camera system went through initial setup. The voice of each character has been cloned in advance and voice clips are generated on the fly for every new dialogue line.

</p>
        

        <p><img src="https://fablestudio.github.io/showrunner-agents/static/images/showrunnergraph.png" alt="Comparison of Response Speed: gpt-3.5-turbo, gpt-4">
          </p>
  </div>

</div>



<!-- Youtube video -->
<div>
          <h2>Video</h2>
          <div>
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
            <p><iframe src="https://player.vimeo.com/video/834880928?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen="" title="To Infinity and Beyond Showrunner Systems in Multi-Agent Simulations Example Episode with Prompt"></iframe></p>
       
          </div>
        </div>
<!-- End youtube video -->

<div>
    <h3>Reducing Latency</h3>
    <div>
      <p>
        In our experiments, generating a single scene can take a significant amount of time of up to one minute. Below is a response time comparison between GPT-3.5-turbo and GPT-4. 
        Speed will increase in the short-term as models and service infrastructure get improved and other factors like artificial throttling due to high user demand will get removed.
      </p>
      <p>
        Since we generate the episodes during gameplay, we have ways to hide most of the generation time in moments when the user is still interacting with the simulation or other user interfaces. Another way to reduce the time needed to generate a scene or episode is to use faster models such as GPT-3.5-turbo for specific prompts in the chain where the highest quality and accuracy is not so important.

      </p>        


<p><img src="https://fablestudio.github.io/showrunner-agents/static/images/Gpt3_gpt4_comparison.png" alt="Comparison of Response Speed: gpt-3.5-turbo, gpt-4">
</p>

<p>
  During scene playback, we avoid any unwanted pauses between dialogue lines related to audio generation by using a simple buffering system which generates at least one voice clip in advance. This means while one character is delivering their voice clip, we already make the web request for the next voice clip, wait for it to generate, download the file and then wait for the current speaker to finish his dialogue before playback (delay). In this way the next dialogue line's voice clip is always delivered without any delay. Text generation and voice cloning services become increasingly fast and allow for highly adaptive and near-real time voice conversations.

</p>
  <p><img src="https://fablestudio.github.io/showrunner-agents/static/images/AudioLatency_00000.png" alt="Timeline showing audio clips">
  </p>


<h3>Simulate creative thinking</h3>

<p>
  As stated earlier, the data produced by the simulation acts as creative fuel to both, the user who is writing the initial prompt and the generative story system which is interacting with the LLM via prompt-chaining.
Prompt-chaining is a technique, which involves supplying the language model with a sequence of related prompts to simulate a continuous thought process. Sometimes it can take on different roles in each step to act as the discriminator against the previous prompt and generated result.
</p><p>  


In our case we try to mimic that of a discontinuous creative thought process.
</p><p>

  For example, the creation of 14 distinct South Park scenes could be managed by initially providing a broad prompt to outline the general narrative, followed by specific prompts detailing and evaluating each scene's cast, location, and key plot points. This mimics the process of human brainstorming, where ideas are built upon and refined in multiple often discontinuous steps. By leveraging the generative capabilities of LLMs in conjunction with the iterative refinement offered by prompt-chaining, we can effectively construct a dynamic, detailed, and engaging narrative. 
</p>

<p>
  In addition, we explore new concepts like plot patterns and dramatic operators (DrOps) to enhance the episode structure overall but also the connective tissue between each scene. Stylistic devices like reversals, foreshadowing, cliffhangers are difficult to evaluate as part of a prompt chain. A user without a writing background would have equal difficulty in judging these stylistic devices for their effectiveness and proper placement. We propose a procedural approach, injecting these show specific patterns and stylistic devices into the prompt chain programmatically as plot patterns and DrOps which can operate on the level of act structures, scene structures and individual dialogue lines. We are investigating future opportunities to extract what we call a dramatic fingerprint which is specific to each IP and format and train our custom SHOW-1 model with these data points. This dataset combined with overall human feedback could further align tone, style and entertainment value between the user and the specified IP while offering a highly adaptive and interactive story system as part of the on-going simulation.

</p>

  <p><img src="https://fablestudio.github.io/showrunner-agents/static/images/southpark_episode_rating.png" alt="Episode chart of south park episode popularity from IMDB"></p>
     
</div>

<br>
</div>


<div>
 
<h3>Blank page problem
</h3>

<p>
  As mentioned above, one of the advantages of the simulation is that it avoids the blank page problem for both a user and a large language model by providing creative fuel. Even experienced writers can sometimes feel overwhelmed when asked to come up with a title or story idea without any prior incubation of related material. The same could be said for LLMs. The simulation provides context and data points before starting the creative prompt chain.
    </p>
   

    <h3>Who is driving the story?
    </h3>
    
    <div><p>
      The story generation process in our approach is a shared responsibility between the simulation, the user, and GPT-4. Each has strengths and weaknesses and a unique role to play depending on how much we want to involve them in the overall creative process. Their contributions can have different weights. While the simulation usually provides the foundational IP-based context, character histories, emotions, events, and localities that seed the initial creative process. The user introduces their intentionality, exerts behavioral control over the agents and provides the initial prompts that kick off the generative process. The user also serves as the final discriminator, evaluating the generated story content at the end of the process. GPT-4, on the other hand, serves as the main generative engine, creating and extrapolating the scenes and dialogue based on the prompts it receives from both the user and the simulation. It's a symbiotic process where the strengths of each participant contribute to a coherent, engaging story. Importantly, our multi-step approach in the form of a prompt-chain also provides checks and balances, mitigating the potential for unwanted randomness and allowing for more consistent alignment with the IP story world.
      </p><p>
      
      <img src="https://fablestudio.github.io/showrunner-agents/static/images/SouthPark_SimulationMockup%20Still%20(00271)%20copy.png" alt="User Interface of the Simulation Map of Southpark"></p></div>
       

        <h3>SHOW-1 and Intentionality
        </h3>
        
        <p>
          The formular (creative characteristics) and format (technical characteristics) of a show are often a function of real-world limitations and production processes. They usually don't change, even over the course of many seasons (South Park currently has 26 seasons and 325 episodes) 

        </p>
        <p>
          A single dramatic fingerprint of a show, which is used to train the proposed SHOW-1 model, can be regarded as a highly variable template or "formula"  for a procedural generator that produces South Park-like episodes.
        </p>
        <p>
          To train a model such as SHOW-1 we need to gather a sufficient amount of data points in relation to each other that characterize a show. A TV show does not just come into existence and is made up of the final dialogue lines and set descriptions as seen by the audience. Existing datasets on which current LLM's are trained on only consist of the final screenplay which has the cast, dialogue lines and sometimes a short scene header. A lot of information is missing, such as timing, emotional states, themes, contexts discussed in the writer's room and detailed directorial notes to give a few examples. The development and refinement of characters is also part of this on-going process. Fictional characters have personalities, backstories and daily routines which help authors to sculpt not only scenes but the arcs of whole seasons. Even during a show characters keep evolving based on audience feedback or changes in creative direction. With the Simulation, we can gather data continuously from both the user's input and the simulated agents. Over time, as episodes are created, refined and rated by the user we can start to train a show specific model and deploy it as a checkpoint which allows the user to continue to refine and iterate on either their own original show or alternatively push an already existing show such as south park into directions previously not conceived by the original show runners and IP holders. To illustrate this, we imagine a user generating multiple south park episodes in which Cartman, one of the main characters and known for his hot headedness, slowly changes to be shy and naive while the life of other characters such as Butters could be tuned to follow a much more dominant and aggressive path. Over time, this feedback loop of interacting with and fine-tuning the SHOW-1 model can lead to new interpretations of existing shows but more excitingly to new original shows based on the user's intention. One of the challenges in order to make this feedback loop engaging and satisfying is the frequency at which a model can be trained. A model which is fed by real-time simulation data and user input should not feel static or require expensive resources to adapt. Otherwise the output it generates can feel static and unresponsive as well.
        </p>
         <p>
          When a generative system is not limited in its ability to swiftly produce high amounts of content and there is no limit for the user to consume such content immediately and potentially simultaneously, the 10,000 Bowls of Oatmeal problem can become an issue. Everything starts to look and feel the same or even worse, the user starts to recognize a pattern which in turn reduces their engagement as they expect newly generated episodes to be like the ones before it, without any surprises.

         </p>
         <p>
          This is quite different from a predictable plot which in combination with the above mentioned "positive hallucinations" or happy accidents of a complex generative system can be a good thing. Surprising the user by balancing and changing the phases of certainty vs. uncertainty helps increase their overall engagement. If they would not expect or predict anything, they could also not get pleasantly surprised.
         </p>

         <p>
          With our work we aim for perceptual uniqueness. The OatMeal problem of procedural generators is mitigated by making use of an on-going simulation (a hidden generator) and the long-form content of 22 min episodes which are only generated every 3h. This way the user generally does not consume a high quantity of content simultaneously or in a very short amount of time. This artificial scarcity, natural game play limits and simulation time help.


         </p>
         <p>
          Another factor that keeps audiences engaged while watching a show and what makes episodes unique is intentionality from the authors. A satirical moral premise, twisted social commentary, recent world events or cameos by celebrities are major elements for South Park. Other show types, for example sitcoms, usually progress mainly through changes in relationship (some of which are never fulfilled), keeping the audience hooked despite following the same format and formula. 
         </p>

         <p>
          Intentionality from the user to generate a high-quality episode is another area of internal research. Even users without a background in dramatic writing should be able to come up with stories, themes or major dramatic questions they want to see played out within the simulation.
          <br>
          To support this, the showrunner system could guide the user by sharing its own creative thought process and make encouraging suggestions or prompting the user by asking the right questions. A sort of reversed prompt engineering where the user is answering questions.
         </p>

         <p>
          One of the remaining unanswered questions in the context of intentionality is how much entertainment value (or overall creative value) is directly attributed to the creative personas of living authors and directors. Big names usually drive ticket sales but the creative credit the audience gives to the work while consuming it seems different. 
Watching a Disney movie certainly carries with it a sense of creative quality, regardless of famous voice actors, as a result of brand attachment and its history.

         </p>

         <p>
          AI generated content is generally perceived as lower quality and the fact that it can get generated in abundance further decreases its value. How much this perception would change if Disney were to openly pride themselves on having produced a fully AI generated movie is hard to say. What if Steven Spielberg, single handedly generated an AI movie? Our assumption is that the perceived value of AI generated content would certainly increase.

         </p>
         <p>
          A new interesting approach to replicate this could be the embodiment of creative AI models such as SHOW-1 to allow them to build a persona outside their simulated world and build relationships via social media or real world events with their audience. As long as an AI model is perceived as a black box and does not share their creative process and reasoning in a human and accessible way, as is the case for living writers and directors, it's unlikely to get credit with real creative values. However, for now this is a more philosophical question in the context of AGI.
         </p>
            
           
</div>




<div>
          <h2>Conclusion</h2>
          <div>
            <p>
              Our approach of using multi-agent simulation and large language models for generating high-quality episodic content provides a novel and effective solution to many of the limitations of current AI systems in creative storytelling. By integrating the strengths of the simulation, the user, and the AI model, we provide a rich, interactive, and engaging storytelling experience that is consistently aligned with the IP story world. Our method also mitigates issues such as the 'slot machine effect', 'the oatmeal problem' and 'blank page problem' that plague conventional generative AI systems. As we continue to refine this approach, we are confident that we can further enhance the quality of the generated content, the user experience, and the creative potential of generative AI systems in storytelling.
            <br>
            Acknowledgements</p><p>
            We are grateful to Lewis Hackett for his help and expertise in training the custom Stable Diffusion Models.
            
           
          </p></div>
        </div>


<!--BibTex citation -->
  <div id="BibTeX">
      <h2>BibTeX</h2>
      <pre><code>
        @article{fable2023showrunner,
          author    = {Maas, Carey, Wheeler, Saatchi, Billington, Shamash},
          title     = {To Infinity and Beyond: SHOW-1 and Showrunner Agents in Multi-Agent Simulations},
          journal   = {arXiv preprint},
          year      = {2023}
        }
      </code></pre>
    </div>
<!--End BibTex citation -->


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/SouthPark_AI_Main_rc_originalIntro_watermarks_230518_v7.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Westland Chronicles - AI generated South Park episode.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->



<!-- Image carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here 
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here 
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->







<!-- Video carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here 
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here 
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->




  

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The single most impor­tant factor that dif­fer­enti­ates front-end frame­works (276 pts)]]></title>
            <link>https://themer.dev/blog/the-single-most-important-factor-that-differentiates-front-end-frameworks</link>
            <guid>36791506</guid>
            <pubDate>Wed, 19 Jul 2023 19:00:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://themer.dev/blog/the-single-most-important-factor-that-differentiates-front-end-frameworks">https://themer.dev/blog/the-single-most-important-factor-that-differentiates-front-end-frameworks</a>, See on <a href="https://news.ycombinator.com/item?id=36791506">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>



<p>There are tons of blog posts on the internet about how frameworks differ and which one to pick for your next web project. Usually they cover a few aspects of the framework like syntax, development setup, and community size.</p>
<p>This isn’t one of those posts.</p>
<p>Instead, we’ll go directly to the crux of the main problem front-end frameworks set out to solve: <em>change detection</em>, meaning <em>detecting changes to application state so that the UI can be updated accordingly.</em> Change detection is the fundamental feature of front-end frameworks, and the framework authors’ solution to this one problem determines everything else about it: developer experience, user experience, API surface area, community satisfaction and involvement, etc., etc.</p>
<p>And it turns out that examining various frameworks from this perspective will give you all of the information you need to determine the best choice for you and for your users. So let’s dive deep into how each framework tackles change detection.</p>
<h2>Major frameworks compared</h2>
<p>We’ll look at each of the major players and how they have tackled change detection, but the same critical eye can apply to any front-end JavaScript framework you may come across.</p>
<h3>React</h3>
<blockquote><p>“I’ll manage state so that I know when it changes.” —React</p></blockquote>
<p>True to its de-facto tagline, change detection in React is “just JavaScript.” Developers simply update state by calling directly into the React runtime through its API; since React is notified to make the state change, it also knows that it needs to re-render the component.</p>
<p>Over the years, the default style for writing components has changed (from <a href="https://react.dev/reference/react/Component#defining-a-class-component" rel="nofollow">class components</a> and <a href="https://react.dev/reference/react/PureComponent#purecomponent" rel="nofollow">pure components</a> to <a href="https://react.dev/reference/react/Component#migrating-a-simple-component-from-a-class-to-a-function" rel="nofollow">function components</a> to <a href="https://react.dev/reference/react#state-hooks" rel="nofollow">hooks</a>) but the core principle has remained the same. Here’s an example component that implements a button counter, written in the hooks style:</p>
<pre><!-- HTML_TAG_START --><code><span>export</span> <span>default</span> <span>function</span> <span>App</span><span>(</span><span>)</span> <span>{</span>
	<span>const</span> <span>[</span>count<span>,</span> setCount<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>0</span><span>)</span><span>;</span>
	<span>return</span> <span>(</span>
		<span><span><span>&lt;</span>div</span><span>&gt;</span></span><span>
			</span><span><span><span>&lt;</span>button</span> <span>onClick</span><span><span>=</span><span>{</span><span>(</span><span>)</span> <span>=&gt;</span> <span>setCount</span><span>(</span>count <span>-</span> <span>1</span><span>)</span><span>}</span></span><span>&gt;</span></span><span>decrement</span><span><span><span>&lt;/</span>button</span><span>&gt;</span></span><span>
			</span><span><span><span>&lt;</span>span</span><span>&gt;</span></span><span>{</span>count<span>}</span><span><span><span>&lt;/</span>span</span><span>&gt;</span></span><span>
			</span><span><span><span>&lt;</span>button</span> <span>onClick</span><span><span>=</span><span>{</span><span>(</span><span>)</span> <span>=&gt;</span> <span>setCount</span><span>(</span>count <span>+</span> <span>1</span><span>)</span><span>}</span></span><span>&gt;</span></span><span>increment</span><span><span><span>&lt;/</span>button</span><span>&gt;</span></span><span>
			</span><span><span><span>&lt;</span>button</span> <span>onClick</span><span><span>=</span><span>{</span><span>(</span><span>)</span> <span>=&gt;</span> <span>setTimeout</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>setCount</span><span>(</span>count <span>+</span> <span>1</span><span>)</span><span>,</span> <span>1000</span><span>)</span><span>}</span></span><span>&gt;</span></span><span>increment later</span><span><span><span>&lt;/</span>button</span><span>&gt;</span></span><span>
		</span><span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
	<span>)</span><span>;</span>
<span>}</span></code><!-- HTML_TAG_END --></pre>
<p>The key piece here is the <code>setCount</code> function returned to us by React’s <code>useState</code> hook. When this function is called, React can use its internal virtual DOM diffing algorithm to determine which pieces of the page to re-render. Note that this means the React runtime has to be included in the application bundle downloaded by the user.</p>
<div><p><span>Conclusion</span></p><p>React’s change detection paradigm is straightforward: the application state is maintained inside the framework (with APIs exposed to the developer for updating it) so that React knows when to re-render.
</p>
</div>
<h3>Angular</h3>
<blockquote><p>“I’ll make the developer do all the work.” —Angular</p></blockquote>
<p>When you scaffold a new Angular application, it appears that change detection happens automagically:</p>
<pre><!-- HTML_TAG_START --><code><span><span>@</span><span>Component</span></span><span>(</span><span>{</span>
	selector<span>:</span> <span>'counter'</span><span>,</span>
	template<span>:</span> <span><span>`</span><span>
		&lt;div&gt;
			&lt;button (click)="count = count - 1"&gt;decrement&lt;/button&gt;
			&lt;span&gt;{{ count }}&lt;/span&gt;
			&lt;button (click)="count = count + 1"&gt;increment&lt;/button&gt;
			&lt;button (click)="incrementLater()"&gt;increment later&lt;/button&gt;
		&lt;/div&gt;
	</span><span>`</span></span>
<span>}</span><span>)</span>
<span>export</span> <span>class</span> <span>Counter</span> <span>{</span>
	count <span>=</span> <span>0</span><span>;</span>

	<span>incrementLater</span><span>(</span><span>)</span> <span>{</span>
		<span>setTimeout</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
			<span>this</span><span>.</span>count<span>++</span><span>;</span>
		<span>}</span><span>,</span> <span>1000</span><span>)</span><span>;</span>
	<span>}</span>
<span>}</span></code><!-- HTML_TAG_END --></pre>
<p>What’s really happening, is that Angular uses <a href="https://angular.io/guide/zone" rel="nofollow"><code>NgZone</code></a> to observe user actions, and is checking <em>your entire component tree on every event.</em></p>
<p>For applications of any reasonable size, this causes performance issues, since checking the entire tree quickly becomes too costly. So Angular provides an escape hatch from this behavior by allowing the developer to choose a different change detection strategy: <code>OnPush</code>. <code>OnPush</code> means that the onus is on the developer to inform Angular when state changes so that Angular can re-render the component. Aside from the default naive strategy, <a href="https://angular.io/api/core/ChangeDetectionStrategy" rel="nofollow"><code>OnPush</code> is the only other change detection strategy Angular offers</a>. With <code>OnPush</code> enabled, we must manually tell Angular’s change detector to check the new state if it ever gets updated asynchronously:</p>
<pre><!-- HTML_TAG_START --><code><span><span>@</span><span>Component</span></span><span>(</span><span>{</span>
	selector<span>:</span> <span>'counter'</span><span>,</span>
	template<span>:</span> <span><span>`</span><span>
		&lt;div&gt;
			&lt;button (click)="count = count - 1"&gt;decrement&lt;/button&gt;
			&lt;span&gt;{{ count }}&lt;/span&gt;
			&lt;button (click)="count = count + 1"&gt;increment&lt;/button&gt;
			&lt;button (click)="incrementLater()"&gt;increment later&lt;/button&gt;
		&lt;/div&gt;
	</span><span>`</span></span><span>,</span>
	changeDetection<span>:</span> ChangeDetectionStrategy<span>.</span>OnPush
<span>}</span><span>)</span>
<span>export</span> <span>class</span> <span>Counter</span> <span>{</span>
	<span>constructor</span><span>(</span><span>private</span> <span>readonly</span> cdr<span>:</span> ChangeDetectorRef<span>)</span> <span>{</span><span>}</span>

	count <span>=</span> <span>0</span><span>;</span>

	<span>incrementLater</span><span>(</span><span>)</span> <span>{</span>
		<span>setTimeout</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
			<span>this</span><span>.</span>count<span>++</span><span>;</span>
			<span>this</span><span>.</span>cdr<span>.</span><span>markForCheck</span><span>(</span><span>)</span><span>;</span>
		<span>}</span><span>,</span> <span>1000</span><span>)</span><span>;</span>
	<span>}</span>
<span>}</span></code><!-- HTML_TAG_END --></pre>
<p>For applications of any reasonable complexity, this approach quickly becomes untenable.</p>
<p>Alternative solutions are introduced to wrangle this problem. The primary one that the Angular docs suggest is to use RxJS observables in conjunction with the <a href="https://angular.io/api/common/AsyncPipe" rel="nofollow"><code>AsyncPipe</code></a>:</p>
<pre><!-- HTML_TAG_START --><code><span>enum</span> Action <span>{</span>
	<span>INCREMENT</span><span>,</span>
	<span>DECREMENT</span><span>,</span>
	<span>INCREMENT_LATER</span>
<span>}</span>

<span><span>@</span><span>Component</span></span><span>(</span><span>{</span>
	selector<span>:</span> <span>'counter'</span><span>,</span>
	template<span>:</span> <span><span>`</span><span>
		&lt;div&gt;
			&lt;button (click)="update.next(Action.DECREMENT)"&gt;decrement&lt;/button&gt;
			&lt;span&gt;{{ count | async }}&lt;/span&gt;
			&lt;button (click)="update.next(Action.INCREMENT)"&gt;increment&lt;/button&gt;
			&lt;button (click)="update.next(Action.INCREMENT_LATER)"&gt;increment later&lt;/button&gt;
		&lt;/div&gt;
	</span><span>`</span></span><span>,</span>
	changeDetection<span>:</span> ChangeDetectionStrategy<span>.</span>OnPush
<span>}</span><span>)</span>
<span>export</span> <span>class</span> <span>Counter</span> <span>{</span>
	<span>readonly</span> update <span>=</span> <span>new</span> <span>Subject<span>&lt;</span>Action<span>&gt;</span></span><span>(</span><span>)</span><span>;</span>

	<span>readonly</span> count <span>=</span> <span>this</span><span>.</span>update<span>.</span><span>pipe</span><span>(</span>
		<span>switchScan</span><span>(</span><span>(</span>prev<span>,</span> action<span>)</span> <span>=&gt;</span> <span>{</span>
			<span>switch</span> <span>(</span>action<span>)</span> <span>{</span>
				<span>case</span> Action<span>.</span><span>INCREMENT</span><span>:</span>
					<span>return</span> <span>of</span><span>(</span>prev <span>+</span> <span>1</span><span>)</span><span>;</span>
				<span>case</span> Action<span>.</span><span>DECREMENT</span><span>:</span>
					<span>return</span> <span>of</span><span>(</span>prev <span>-</span> <span>1</span><span>)</span><span>;</span>
				<span>case</span> Action<span>.</span><span>INCREMENT_LATER</span><span>:</span>
					<span>return</span> <span>of</span><span>(</span>prev <span>+</span> <span>1</span><span>)</span><span>.</span><span>pipe</span><span>(</span><span>delay</span><span>(</span><span>1000</span><span>)</span><span>)</span><span>;</span>
			<span>}</span>
		<span>}</span><span>,</span> <span>0</span><span>)</span><span>,</span>
		<span>startWith</span><span>(</span><span>0</span><span>)</span>
	<span>)</span><span>;</span>

	<span>readonly</span> Action <span>=</span> Action<span>;</span>
<span>}</span></code><!-- HTML_TAG_END --></pre>
<p>Under the hood, <code>AsyncPipe</code> takes care of subscribing to the observable, informing the change detector when the observable emits a new value, and unsubscribing when the component is destroyed. Observables are a powerful way to model state changes over time, but they come with some serious drawbacks:</p>
<ul><li>They are difficult to debug.</li>
<li>They have a very steep learning curve.</li>
<li>They are great for modeling streams of values (think: mouse movements), but they are overkill for the more common use cases (simple state changes like the on/off state of a checkbox).</li></ul>
<p>To overcome the shortcomings of the default change detection paradigm, the Angular team is working on a new approach called <a href="https://angular.io/guide/signals" rel="nofollow">Signals</a>. Conceptually, signals are similar to Svelte stores (which we’ll get to later), and fundamentally, they solve the change detection problem the same way as React; the framework is taking control over the application’s state so that changes can be easily monitored and re-renders can be as efficient as possible.</p>
<p>From the Angular docs:</p>
<blockquote><p>Angular Signals&nbsp;is a system that granularly tracks how and where your state is used throughout an application, allowing the framework to optimize rendering updates.</p></blockquote>
<p>This is a large paradigm shift, making Angular applications more similar to the other frameworks.</p>
<div><p><span>Conclusion</span></p><p>Angular’s change detection is a disaster. The developer gets two suboptimal choices: (1) the slow and naive default implementation, or the complexity of managing change detection manually. Signals will make it much better, though nearly a decade too late.
</p>
</div>
<h3>Vue</h3>
<blockquote><p>“I’ll track changes to state and react accordingly.” —Vue</p></blockquote>
<p>Vue’s approach to change detection is subtly different than both React and Angular. Rather than calling a framework function to change state (React) or changing state and then informing the framework that it has been changed (Angular), you work with state objects that have been specially instrumented by the framework to intercept and detect changes.</p>
<p>Confusingly, Vue has two different APIs that wrap the same underlying change detection engine differently. Under the “Options API,” you define an object that contains your state, and Vue assigns a proxied version of that object as a member of <code>this</code> for use in the component’s functions:</p>
<pre><!-- HTML_TAG_START --><code><span><span><span>&lt;</span>template</span><span>&gt;</span></span>
	<span><span><span>&lt;</span>div</span><span>&gt;</span></span>
		<span><span><span>&lt;</span>button</span> <span>@click</span><span><span>=</span><span>"</span>decrement<span>"</span></span><span>&gt;</span></span>decrement<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
		<span><span><span>&lt;</span>span</span><span>&gt;</span></span>{{ count }}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
		<span><span><span>&lt;</span>button</span> <span>@click</span><span><span>=</span><span>"</span>increment<span>"</span></span><span>&gt;</span></span>increment<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
		<span><span><span>&lt;</span>button</span> <span>@click</span><span><span>=</span><span>"</span>incrementLater<span>"</span></span><span>&gt;</span></span>increment later<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
	<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
<span><span><span>&lt;/</span>template</span><span>&gt;</span></span>

<span><span><span>&lt;</span>script</span><span>&gt;</span></span><span><span>
	<span>export</span> <span>default</span> <span>{</span>
		<span>data</span><span>(</span><span>)</span> <span>{</span>
			<span>return</span> <span>{</span>
				<span>count</span><span>:</span> <span>0</span>
			<span>}</span><span>;</span>
		<span>}</span><span>,</span>
		<span>methods</span><span>:</span> <span>{</span>
			<span>decrement</span><span>(</span><span>)</span> <span>{</span>
				<span>this</span><span>.</span>count<span>--</span><span>;</span>
			<span>}</span><span>,</span>
			<span>increment</span><span>(</span><span>)</span> <span>{</span>
				<span>this</span><span>.</span>count<span>++</span><span>;</span>
			<span>}</span><span>,</span>
			<span>incrementLater</span><span>(</span><span>)</span> <span>{</span>
				<span>setTimeout</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
					<span>this</span><span>.</span>count<span>++</span><span>;</span>
				<span>}</span><span>,</span> <span>1000</span><span>)</span><span>;</span>
			<span>}</span>
		<span>}</span>
	<span>}</span><span>;</span>
</span></span><span><span><span>&lt;/</span>script</span><span>&gt;</span></span></code><!-- HTML_TAG_END --></pre>
<p>Alternatively, the “Composition API” is somewhat similar to React’s hooks: a framework function is called to retrieve a state object that Vue can monitor for changes:</p>
<pre><!-- HTML_TAG_START --><code><span><span><span>&lt;</span>script</span> <span>setup</span><span>&gt;</span></span><span><span>
	<span>import</span> <span>{</span> ref <span>}</span> <span>from</span> <span>'vue'</span><span>;</span>

	<span>const</span> count <span>=</span> <span>ref</span><span>(</span><span>0</span><span>)</span><span>;</span>

	<span>function</span> <span>increment</span><span>(</span><span>)</span> <span>{</span>
		count<span>.</span>value<span>++</span><span>;</span>
	<span>}</span>

	<span>function</span> <span>decrement</span><span>(</span><span>)</span> <span>{</span>
		count<span>.</span>value<span>--</span><span>;</span>
	<span>}</span>

	<span>function</span> <span>incrementLater</span><span>(</span><span>)</span> <span>{</span>
		<span>setTimeout</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
			count<span>.</span>value<span>++</span><span>;</span>
		<span>}</span><span>,</span> <span>1000</span><span>)</span><span>;</span>
	<span>}</span>
</span></span><span><span><span>&lt;/</span>script</span><span>&gt;</span></span>

<span><span><span>&lt;</span>template</span><span>&gt;</span></span>
	<span><span><span>&lt;</span>div</span><span>&gt;</span></span>
		<span><span><span>&lt;</span>button</span> <span>@click</span><span><span>=</span><span>"</span>decrement<span>"</span></span><span>&gt;</span></span>decrement<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
		<span><span><span>&lt;</span>span</span><span>&gt;</span></span>{{ count }}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
		<span><span><span>&lt;</span>button</span> <span>@click</span><span><span>=</span><span>"</span>increment<span>"</span></span><span>&gt;</span></span>increment<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
		<span><span><span>&lt;</span>button</span> <span>@click</span><span><span>=</span><span>"</span>incrementLater<span>"</span></span><span>&gt;</span></span>increment later<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
	<span><span><span>&lt;/</span>div</span><span>&gt;</span></span>
<span><span><span>&lt;/</span>template</span><span>&gt;</span></span></code><!-- HTML_TAG_END --></pre>
<p><a href="https://vuejs.org/guide/essentials/reactivity-fundamentals.html#why-refs" rel="nofollow">Conceptually, the object returned from <code>ref()</code> has a getter and a setter for <code>value</code></a>, which allows Vue to track changes to it.</p>
<div><p><span>Conclusion</span></p><p>Vue utilizes JavaScript language features to allow developers to work with stateful variables without thinking about change detection.
</p>
</div>
<h3>Svelte</h3>
<blockquote><p>“I’ll figure it out for you at compile time.” —Svelte</p></blockquote>
<p>On the surface, Svelte’s version of our counter component looks pretty similar to the other frameworks:</p>
<pre><!-- HTML_TAG_START --><code><span><span><span>&lt;</span>script</span><span>&gt;</span></span><span><span>
	<span>let</span> count <span>=</span> <span>0</span><span>;</span>
	<span>function</span> <span>decrement</span><span>(</span><span>)</span> <span>{</span>
		count<span>--</span><span>;</span>
	<span>}</span>
	<span>function</span> <span>increment</span><span>(</span><span>)</span> <span>{</span>
		count<span>++</span><span>;</span>
	<span>}</span>
	<span>function</span> <span>incrementLater</span><span>(</span><span>)</span> <span>{</span>
		<span>setTimeout</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
			count<span>++</span><span>;</span>
		<span>}</span><span>,</span> <span>1000</span><span>)</span><span>;</span>
	<span>}</span>
</span></span><span><span><span>&lt;/</span>script</span><span>&gt;</span></span>

<span><span><span>&lt;</span>div</span><span>&gt;</span></span>
	<span><span><span>&lt;</span>button</span> <span><span>on:</span>click</span><span><span>=</span><span>"</span>{decrement}<span>"</span></span><span>&gt;</span></span>decrement<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
	<span><span><span>&lt;</span>span</span><span>&gt;</span></span>{count}<span><span><span>&lt;/</span>span</span><span>&gt;</span></span>
	<span><span><span>&lt;</span>button</span> <span><span>on:</span>click</span><span><span>=</span><span>"</span>{increment}<span>"</span></span><span>&gt;</span></span>increment<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
	<span><span><span>&lt;</span>button</span> <span><span>on:</span>click</span><span><span>=</span><span>"</span>{incrementLater}<span>"</span></span><span>&gt;</span></span>increment later<span><span><span>&lt;/</span>button</span><span>&gt;</span></span>
<span><span><span>&lt;/</span>div</span><span>&gt;</span></span></code><!-- HTML_TAG_END --></pre>
<p>But Svelte’s approach to change detection is completely novel in comparison. At compile time, Svelte analyzes an AST (Abstract Syntax Tree) of the component’s code and <em>injects some code into the compiled output</em> that surgically updates the DOM when necessary. For example, here is what the compiled <code>decrement()</code> function looks like:</p>
<pre><!-- HTML_TAG_START --><code><span>function</span> <span>decrement</span><span>(</span><span>)</span> <span>{</span>
	<span>$$invalidate</span><span>(</span><span>0</span><span>,</span> count<span>--</span><span>,</span> count<span>)</span><span>;</span>
<span>}</span></code><!-- HTML_TAG_END --></pre>
<p>Where <code>$$invalidate</code> is a call to Svelte’s internals to instruct the compiled component to update the DOM.</p>
<p>This compile-time approach means that Svelte applications don’t need to bundle a large runtime along with the application itself.</p>
<div><p><span>Conclusion</span></p><p>Svelte strikes a rare win-win balance: developers don’t have to think about change detection at all, and can interact with stateful variables intuitively; yet the end user’s experience is improved through better performance because a bare-minimum application (with change detection baked in) is shipped to the browser.
</p>
</div>
<h2>So, what?</h2>
<p>The nuances of how various frameworks choose to tame this beast is not limited to how things work at the component level; it ripples out to <em>everything else</em> about the framework. To name just a few examples: the concepts used to <a href="https://react.dev/learn/reusing-logic-with-custom-hooks" rel="nofollow">create custom React hooks</a> composed of the basic hooks provided by React out of the box are not relevant to generalizing component behavior in Vue; the challenge of working with observables for state management in Angular has led folks to <a href="https://github.com/futhark/ngx-observable-input" rel="nofollow">try and find ways to convert component input props to observables</a>; the framework’s API, dictated by its change detection management paradigm, affects how well it integrates with productivity and quality tools like typechecking, testing, and linting. And so on, and so forth.</p>
<p>And those are just examples from the developer’s point of view. Each approach has implications on the performance of the application for the end user. React, Vue, and Angular each ship a runtime to the user’s browser that needs to be parsed and executed. Svelte’s choice to be a compile-time framework obviates this need in most cases, so the user gets a faster loading experience. Each framework has subtleties that make it more susceptible to particular classes of bugs (often around state management or change detection) that the end user will experience.</p>
<p>Find a change detection paradigm that fits the needs of your application, and everything else will fall into place. Pick one that doesn’t work, and you’ll be fighting against it for the life of the project.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Full-text search engine with PostgreSQL (part 2): Postgres vs. Elasticsearch (101 pts)]]></title>
            <link>https://xata.io/blog/postgres-full-text-search-postgres-vs-elasticsearch</link>
            <guid>36791465</guid>
            <pubDate>Wed, 19 Jul 2023 18:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xata.io/blog/postgres-full-text-search-postgres-vs-elasticsearch">https://xata.io/blog/postgres-full-text-search-postgres-vs-elasticsearch</a>, See on <a href="https://news.ycombinator.com/item?id=36791465">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In <a href="https://xata.io/blog/postgres-full-text-search-engine">Part 1</a>, we delved into the capabilities of PostgreSQL's full-text search and explored how advanced search features such as relevancy boosters, typo-tolerance, and faceted search can be implemented. In this part, we'll compare it with Elasticsearch.</p>
<p>First, let's note that Postgres and Elasticsearch are generally not in competition with each other. In fact, it's very common to see them together in architecture diagrams, often in a configuration like this:</p>
<div><p><a href="https://xata.io/mdx/blog/postgres-plus-elasticsearch.png"><img alt="Typical architecture with PostgreSQL and Elasticsearch" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=640&amp;q=75 640w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=750&amp;q=75 750w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=828&amp;q=75 828w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=1080&amp;q=75 1080w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=1200&amp;q=75 1200w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=1920&amp;q=75 1920w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=2048&amp;q=75 2048w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=3840&amp;q=75 3840w" src="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-plus-elasticsearch.png&amp;w=3840&amp;q=75"></a></p><p><figcaption>Typical architecture with PostgreSQL and Elasticsearch</figcaption></p></div>
<p>In this architecture, the source of truth for the data lives in Postgres, which serves the transactional CRUD operations. The data is continuously synced to Elasticsearch, either via something like Postgres logical replication events (change-data-capture) or by the application itself via custom code. During this data replication, denormalization might be required. The search functionality, including facets and aggregations, is served from Elasticsearch.</p>
<p>While this architecture is as common as it is for very good reasons, it does have a few challenges:</p>
<ol role="list"><li>Dealing with two types of stores means more operational burden and higher infrastructure costs.</li><li>Keeping the data in sync is more challenging than you might think. I'm planning a dedicated blog for this problem because it's quite interesting. Let's just say, it's pretty hard to get it completely right.</li><li>The data replication is, at best, <strong>near</strong> real-time, meaning that there can be consistency issues in the search service.</li></ol>
<p>Point 2 is generally solvable via engineering effort and careful dedicated code. From the existing tools, <a href="https://pgsync.com/">PGSync</a> is an open source project that aims to specifically solve this problem. <a href="https://github.com/zombodb/zombodb">ZomboDB</a> is an interesting Postgres extension that tackles point 2 (and I think partially point 3), by controlling and querying Elasticsearch <em>through</em> Postgres. I haven't yet tried either of these two projects, so I can't comment on their trade-offs, but I wanted to mention them.</p>
<p>And yes, a data platform like <a href="https://xata.io/">Xata</a> solves most of points 1 and 2, by taking that complexity and offering it as a service, together with other goodies.</p>
<p>That said, if the Postgres full-text search functionality is enough for your use case, making use of it promises to significantly simplify your architecture and application. In this version, Postgres serves both the CRUD app needs and the full-text search needs:</p>
<div><p><a href="https://xata.io/mdx/blog/postgres-only-search.png"><img alt="Postgres-only search architecture" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=640&amp;q=75 640w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=750&amp;q=75 750w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=828&amp;q=75 828w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=1080&amp;q=75 1080w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=1200&amp;q=75 1200w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=1920&amp;q=75 1920w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=2048&amp;q=75 2048w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=3840&amp;q=75 3840w" src="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-only-search.png&amp;w=3840&amp;q=75"></a></p><p><figcaption>Postgres-only search architecture</figcaption></p></div>
<p>This means you don't need to operate two types of stores, no more data replication, no more denormalization, no more eventual consistency. The search engine built into Postgres happens to support ACID transactions, joins between tables, constraints (e.g. not null or unique), referential integrity (foreign keys), and all the other Postgres goodies that make application development simpler.</p>
<p>Therefore, it's no wonder the <a href="https://news.ycombinator.com/item?id=36699016">Hacker News thread</a> for our <a href="https://xata.io/blog/postgres-full-text-search-engine">part 1 blog post</a> had a lively discussion about the pros and cons of this approach. Can we go for the Postgres-only solution, or does the best-tool-for-the-job argument wins?</p>
<p>We're going to compare the convenience, search relevancy, performance, and scalability of the two options.</p>

<p>As we showed in part 1, you can replicate a lot of the Elasticsearch functionality in Postgres, even more advanced things like relevancy boosters, typo-tolerance, suggesters/autocomplete, or semantic/vector search. However, it's not always straight forward.</p>
<p>An example where it's not too simple is with typo-tolerance (called fuzziness in Elasticsearch). It's not available out-of-the-box in Postgres, but you can <a href="https://xata.io/blog/postgres-full-text-search-engine#typo-tolerance-fuzzy-search">implement it with the following steps</a>:</p>
<ul role="list"><li>index all <em>lexemes (words)</em> from all documents in a separate table</li><li>for each word in the query, use similarity or Levenshtein distance to search in this table</li><li>modify the search query to include any words that are found</li></ul>
<p>While the above is quite doable, in dedicated search engines like Elasticsearch, you can enable typo-tolerance with a simple flag:</p>
<div role="group" data-rehype-pretty-code-fragment=""><pre data-theme="default" tabindex="0" data-language="json"><code><span data-line=""><span>// POST /recipes/_search</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>  </span><span>"</span><span>query</span><span>"</span><span>:</span><span> </span><span>{</span></span>
<span data-line=""><span>    </span><span>"</span><span>multi_match</span><span>"</span><span>:</span><span> </span><span>{</span></span>
<span data-line=""><span>      </span><span>"</span><span>query</span><span>"</span><span>:</span><span> </span><span>"</span><span>biscaits</span><span>"</span><span>,</span></span>
<span data-line=""><span>      </span><span>"</span><span>fuzziness</span><span>"</span><span>:</span><span> </span><span>1</span></span>
<span data-line=""><span>    </span><span>}</span></span>
<span data-line=""><span>  </span><span>}</span></span>
<span data-line=""><span>}</span></span></code></pre></div>

<p>The default ranking algorithm for keyword search in Elasticsearch is BM25. With the release of Elasticsearch 5.0 in 2016, it dethroned TF-IDF as the default ranking algorithm. Postgres doesn't support either of them, mainly because its ranking functions (explained in <a href="https://xata.io/blog/postgres-full-text-search-engine#tsrank">here</a>) don't have access to global word frequency data which is needed by these algorithms. To see how <em>relevant (pun intended) or</em> not so relevant that might be, let's look at the ranking functions and algorithms from simple to complex:</p>
<ul role="list"><li><code>ts_rank</code> (Postgres function) - ranks based on the term frequency. In other words, it does the “TF” (term frequency) part of TF-IDF. The principle is that if you are searching for a word, the more often that word shows up in the matching document, the higher the score. In addition to using simple TF, Postgres provides ways to normalize the term frequency into a score. For instance, <a href="https://www.postgresql.org/docs/current/textsearch-controls.html#TEXTSEARCH-RANKING">one approach</a> is to divide it by the document length.</li><li><code>ts_rank_cd</code> (Postgres function) - rank + cover density. In addition to the term frequency, this function also takes into account the “cover density”, meaning the proximity of the terms in the document.</li><li>TF-IDF - term frequency + inverse document frequency. In addition to the term frequency, this algorithm “penalizes” words that are very common in the overall data set. So if the word “egg” matches, but that word is super common because we have a recipes dataset, it is valued less compared to other words in the query.</li><li>BM25 - this algorithm is based on a probabilistic model of relevancy. While the <a href="https://lucene.apache.org/core/8_8_0/core/org/apache/lucene/search/similarities/TFIDFSimilarity.html">TF-IDF formula</a> is mostly based on intuition and practical experiments, BM25 is the result of more <a href="https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf">formal mathematical research</a>. If you're curious about the said mathematical research, I recommend this <a href="https://www.elastic.co/elasticon/conf/2016/sf/improved-text-scoring-with-bm25">talk</a> that makes it accessible. Interestingly, the resulting BM25 formula is not all that different from TF-IDF but it incorporates a couple more concepts: the frequency saturation and the document length. Ultimately, this gives better results over a wider range of document types.</li></ul>
<p>There's no question that BM25 is a more advanced relevancy algorithm than what <code>ts_rank</code> or <code>ts_rank_cd</code> use. BM25 uses more input signals, it's based on better heuristics, and it typically doesn't require tuning.</p>
<p>One practical effect of BM25 is that it automatically penalizes the very common words (”the”, “in”, “or”, etc.), also called “stop words”, which means that they don't need to be excluded from the index. This is why the Postgres <code>english</code> configuration for <code>to_tsvector</code> removes the stop words (details <a href="https://xata.io/blog/postgres-full-text-search-engine#tsvector">here in part 1</a>), but the Elasticsearch <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html">standard analyzer</a> doesn't. It doesn't need to.</p>
<p>While BM25 is superior, there are some pro-Postgres arguments to be considered:</p>
<ul role="list"><li>if you aggressively exclude the stop words, like the <code>english</code> configuration in Postgres does, that compensates for the lack of IDF in some cases.</li><li>in practice there might be stronger signals of relevancy in the data itself (upvotes, reviews, etc). See the <a href="https://xata.io/blog/postgres-full-text-search-engine#numeric-date-and-exact-value-boosters">section on boosters from part 1</a> on how to make use of them in Postgres.</li></ul>
<p>Could BM25 or TF-IDF be implemented on top of the existing Postgres functionality? Actually, yes. See this <a href="https://www.alibabacloud.com/blog/keyword-analysis-with-postgresql-cosine-and-linear-correlation-algorithms-for-text-analysis_595793">blog post</a> that uses <code>ts_stats</code> and <code>ts_debug</code> to compute TF-IDF. It's not very simple, but possible (as usual with Postgres).</p>

<p>Let's start by noting that the two systems couldn't be more different:</p>
<ul role="list"><li>PostgreSQL has a single master and multiple read replicas, Elasticsearch has horizontal scalability via sharding.</li><li>Postgres is relational, supports joining tables, has ACID transactions, and offers constraints, while Elasticsearch is document oriented and offers consistency guarantees only per document.</li><li>Postgres is row oriented, while Elasticsearch has an internal column store in the form of <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/doc-values.html">doc values</a>.</li><li>Postgres is native C code, while Elasticsearch runs on the JVM.</li><li>Postgres has a connection-oriented wire protocol; Elasticsearch has a REST-like DSL over HTTP.</li></ul>
<p>All of these impact performance and scalability, and it's no surprise then that the two tend to shine in different areas: PostgreSQL is commonly used as a primary data store, whereas Elasticsearch is usually utilized as a secondary store, particularly for search and analytics on time-series data such as logs. And yet, they do overlap on the use case of full-text search, which is the point of this blog post.</p>
<div><p><a href="https://xata.io/mdx/blog/postgres-and-elasticsearch-usecases.png"><img alt="Postgres and Elasticsearch use cases" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=640&amp;q=75 640w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=750&amp;q=75 750w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=828&amp;q=75 828w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=1080&amp;q=75 1080w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=1200&amp;q=75 1200w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=1920&amp;q=75 1920w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=2048&amp;q=75 2048w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=3840&amp;q=75 3840w" src="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpostgres-and-elasticsearch-usecases.png&amp;w=3840&amp;q=75"></a></p><p><figcaption>Postgres and Elasticsearch use cases</figcaption></p></div>
<p>I was curious to know at roughly what amount of data Postgres slows down compared to Elasticsearch. On the movies dataset (34K rows) that we used in part 1, all queries were reasonably fast (&lt;300 ms). So for the testing here, I chose a larger data set: a recipes dataset from Kaggle, containing <a href="https://www.kaggle.com/datasets/wilmerarltstrmberg/recipe-dataset-over-2m?resource=download">2.3M recipes</a>. The commands to load the CSV file in PostgreSQL can be found in this <a href="https://gist.github.com/tsg/bc64fcea28fca844db11c65b9b1cb4ca">gist</a>. For Elasticsearch, I've loaded the same CSV file using this <a href="https://github.com/ovidiugiorgi/csv2opensearch">tool</a>.</p>
<p>After loading the data, I started by running searches similar to the ones used in part 1:</p>
<div role="group" data-rehype-pretty-code-fragment=""><pre data-theme="default" tabindex="0" data-language="sql"><code><span data-line=""><span>SELECT</span><span> title, ts_rank(search, websearch_to_tsquery(</span><span>'</span><span>english</span><span>'</span><span>, </span><span>'</span><span>darth vader</span><span>'</span><span>)) rank</span></span>
<span data-line=""><span>   </span><span>FROM</span><span> recipes </span><span>WHERE</span><span> search @@ websearch_to_tsquery(</span><span>'</span><span>english</span><span>'</span><span>,</span><span>'</span><span>darth vader</span><span>'</span><span>)</span></span>
<span data-line=""><span>   </span><span>ORDER BY</span><span> rank </span><span>DESC</span><span> </span><span>limit</span><span> </span><span>10</span><span>;</span></span>
<span data-line=""><span>        title         |    rank</span></span>
<span data-line=""><span>----------------------+------------</span></span>
<span data-line=""><span> Darth Vader Biscuits | </span><span>0</span><span>.</span><span>09910322</span></span>
<span data-line=""><span> Cloud </span><span>9</span><span> Pancakes     | </span><span>0</span><span>.</span><span>09910322</span></span>
<span data-line=""><span>(</span><span>2</span><span> </span><span>rows</span><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>Time</span><span>: </span><span>100</span><span>.</span><span>468</span><span> ms</span></span></code></pre></div>
<p>For Elasticsearch I've used the following to run the search:</p>
<div role="group" data-rehype-pretty-code-fragment=""><pre data-theme="default" tabindex="0" data-language="json"><code><span data-line=""><span>// POST /recipes/_search</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>  </span><span>"</span><span>query</span><span>"</span><span>:</span><span> </span><span>{</span></span>
<span data-line=""><span>    </span><span>"</span><span>query_string</span><span>"</span><span>:</span><span> </span><span>{</span></span>
<span data-line=""><span>      </span><span>"</span><span>query</span><span>"</span><span>:</span><span> </span><span>"</span><span>darth AND vader</span><span>"</span></span>
<span data-line=""><span>    </span><span>}</span></span>
<span data-line=""><span>  </span><span>}</span></span>
<span data-line=""><span>}</span></span></code></pre></div>
<p>I ran each query five times and recorded the best and worst times. Typically, the first query of a kind was the slowest because the following queries benefited from having the relevant pages already in memory. While this approach is rather unscientific, and you should conduct your own benchmarking on your data before drawing definitive conclusions, it should be sufficient for drawing some initial conclusions.</p>
<p>Here are the results on a few queries:</p>
<table><thead><tr><th>query</th><th>Elasticsearch worst time (ms)</th><th>Elasticsearch best time (ms)</th><th>Postgres worst time (ms)</th><th>Postgres best time (ms)</th></tr></thead><tbody><tr><td>darth vader</td><td>52</td><td>4</td><td>100</td><td>3</td></tr><tr><td>chicken nuggets</td><td>85</td><td>10</td><td>313</td><td>13</td></tr><tr><td>pancake</td><td>60</td><td>4</td><td>618</td><td>157</td></tr><tr><td>curacao</td><td>286</td><td>7</td><td>230</td><td>10</td></tr><tr><td>mix</td><td>67</td><td>5</td><td>25182</td><td>8267</td></tr></tbody></table>
<p>As you can see, Postgres performs well on some queries such as "darth vader" or "curacao," responding within milliseconds. However, on other queries like "pancake" or "mix," it performs significantly worse than Elasticsearch, with response times measured in seconds. It gets as bad as 25 seconds latency! What's going on here?</p>
<p>The difference lies in how many rows match the query terms. Searching for “darth vader” in a recipes dataset matches 2 rows. But searching “mix” in a recipes dataset matches a million rows (literally, 1,038,914 to be precise). Since we order by rank, Postgres needs to call the <code>ts_rank</code> function for each of the million rows. The Postgres docs even <a href="https://www.postgresql.org/docs/current/textsearch-controls.html#TEXTSEARCH-RANKING">warn about this</a>:</p>

<p>Indeed, the issue is from ranking. If we're only interested in matching and we order by an (indexed) column, it is fast:</p>
<div role="group" data-rehype-pretty-code-fragment=""><pre data-theme="default" tabindex="0" data-language="sql"><code><span data-line=""><span>SELECT</span><span> title </span><span>FROM</span><span> recipes</span></span>
<span data-line=""><span>  </span><span>WHERE</span><span> search @@ websearch_to_tsquery(</span><span>'</span><span>english</span><span>'</span><span>,</span><span>'</span><span>mix</span><span>'</span><span>)</span></span>
<span data-line=""><span>  </span><span>ORDER BY</span><span> title </span><span>ASC</span><span> </span><span>LIMIT</span><span> </span><span>10</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>Time</span><span>: </span><span>24</span><span>.</span><span>681</span><span> ms</span></span></code></pre></div>
<p>But we're working on the assumption that ranking is necessary for a good search experience. One idea is to use what I call "sampling": before computing the ranks, take a sample of 10K rows that match. The assumption is that if your query matches so many documents, the ranking is likely to be ineffective anyway, so it's better to prioritize the response time.</p>
<p>The SQL to do this looks like this:</p>
<div role="group" data-rehype-pretty-code-fragment=""><pre data-theme="default" tabindex="0" data-language="sql"><code><span data-line=""><span>WITH</span><span> search_sample </span><span>AS</span><span> (</span></span>
<span data-line=""><span>	</span><span>SELECT</span><span> title, search </span><span>FROM</span><span> recipes</span></span>
<span data-line=""><span>  </span><span>WHERE</span><span> search @@ websearch_to_tsquery(</span><span>'</span><span>english</span><span>'</span><span>,</span><span>'</span><span>mix</span><span>'</span><span>)</span></span>
<span data-line=""><span>  </span><span>LIMIT</span><span> </span><span>10000</span><span>)</span></span>
<span data-line=""><span>SELECT</span><span> title, ts_rank(search, websearch_to_tsquery(</span><span>'</span><span>english</span><span>'</span><span>, </span><span>'</span><span>mix</span><span>'</span><span>)) rank</span></span>
<span data-line=""><span>  </span><span>FROM</span><span> search_sample</span></span>
<span data-line=""><span>  </span><span>ORDER BY</span><span> rank </span><span>DESC</span><span> </span><span>limit</span><span> </span><span>10</span><span>;</span></span></code></pre></div>
<p>Re-running the tests with this sample approach gives us closer results:</p>
<table><thead><tr><th>query</th><th>Elasticsearch worst time (ms)</th><th>Elasticsearch best time (ms)</th><th>Postgres worst time (ms)</th><th>Postgres best time (ms)</th></tr></thead><tbody><tr><td>darth vader</td><td>52</td><td>4</td><td>100</td><td>3</td></tr><tr><td>chicken nuggets</td><td>85</td><td>10</td><td>195</td><td>14</td></tr><tr><td>pancake</td><td>60</td><td>4</td><td>145</td><td>13</td></tr><tr><td>curacao</td><td>286</td><td>7</td><td>225</td><td>11</td></tr><tr><td>mix</td><td>67</td><td>5</td><td>400</td><td>144</td></tr></tbody></table>
<p>Much better! Of course, we did sacrifice on the relevancy, which might or might not be ok in your case.</p>
<p>Here are some conclusions and more considerations on the topic of performance and scalability:</p>
<ul role="list"><li>on search use cases over smaller datasets (&lt;100K rows) both systems will perform well, but a Postgres-only solution will require less resources.</li><li>on medium datasets (a few million rows), Elasticsearch is already faster, however, Postgres can perform within a 200ms latency budged if you use the sampling trick explained above.</li><li>when the number of documents is really large, for example logs or other time series, Elasticsearch has the additional advantage of horizontal scalability.</li><li>if you need a lot of aggregations or analytics (e.g. display a dashboard full of graphs) and the data set is large enough, Elasticsearch's columnar store will give it an advantage.</li><li>giving Postgres an extra workload can affect the performance of your main instance. The solution is to move the searching to a replica, but then you lose some of the consistency guarantees.</li></ul>

<p>Both part 1 and this blog post focused on keyword searching techniques. However, in the last few years, semantic/vector search has taken the world of search by storm, so I feel like I need to touch on this aspect as well in comparing the two.</p>
<p>Semantic search leverages language models to generate embeddings for each document. Embeddings are arrays of numbers that represent the text on a number of dimensions. Pieces of text that have similar embeddings have a similar <em>meaning</em>. In other words, semantic search can “search by meaning”, rather than “by keywords”. This is quite exciting now, because large language models (LLMs) give us very accurate understanding of meaning. It means you don't have to maintain list of synonyms or add different keywords to your documents to match how your users are searching.</p>
<p>Postgres supports vector search via the <a href="https://github.com/pgvector/pgvector">pgvector</a> extension, while Elasticsearch has it built-in via the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html">KNN search</a>. You can find benchmarks on <a href="https://ann-benchmarks.com/index.html">ann-benchmarks</a> (look for <code>pgvector</code> and <code>luceneknn</code>) but keep in mind that both implementations are under active development and their performance is being improved.</p>
<p>While exciting, it turns out that semantic search alone doesn't really work great on the typical search experiences that we have today - at least not on a majority of datasets. If you are curious, I recently wrote a <a href="https://xata.io/blog/keyword-vs-semantic-search-chatgpt">comparison between keyword and semantic search</a> for the particular use case of selecting the context for ChatGPT.</p>
<p>For search use cases like the recipes one in this blog post, hybrid search might give better results: use a combination of keyword and semantic search to improve the ranking.</p>
<p>Elastic has recently announced their “<a href="https://www.elastic.co/blog/may-2023-launch-announcement">Elasticsearch Relevance Engine</a>”, which includes hybrid search. In Postgres, given that it's all building blocks, you can combine the full-text search functionality and pgvector. I'm looking forward to diving deeper into this topic as well, but I'll leave that for a follow-up blog post.</p>

<p>Choosing between a Postgres-only architecture and a Postgres + Elasticsearch architecture will depend on your use case and scale.</p>
<p>For example, if you have a table or list in your application on which you support CRUD operations and you want to add full-text search functionality to it, Postgres will likely work well for you for quite some time.</p>
<p>On the other hand, if you have a large data set search and search relevancy is critical to your application (for example, in e-commerce), using a dedicated search engine like Elasticsearch is going to perform better both in latency and relevancy.</p>
<p>In many cases, it might make sense to start with the simpler Postgres-only approach, but be ready to pivot to the Postgres + Elasticsearch architecture when needed.</p>
<p>If you read this far, you might want to give <a href="https://xata.io/">Xata a try</a>. It offers both Postgres and Elasticsearch in the same data platform, and can also handle the syncing between them with no extra effort. If you have any feedback on this blog post, or are interested in the follow-up blog posts, you can follow us on <a href="https://twitter.com/xata">Twitter</a> or join us in <a href="https://xata.io/discord">Discord</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Twenty.com (YC S23) – Open-source CRM (364 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36791434</link>
            <guid>36791434</guid>
            <pubDate>Wed, 19 Jul 2023 18:54:50 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36791434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36791827"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36791827" href="https://news.ycombinator.com/vote?id=36791827&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>The biggest mistake a competitor can make is thinking Salesforce is a CRM. They may sell themselves as one, but that is just to get a foothold in a company's sales system. Salesforce actually sells (1) an ecosystem that lets your company connect to any other software out there and (2) a platform on which you can build...basically anything. These two together ensure that every one of their customers is 100% locked in and will continue to pay whatever Salesforce asks for all eternity. The actual CRM part (basically tables of data with a UI) is trivial, and not really their "special sauce" that you can disrupt with something shinier.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36791918"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791918" href="https://news.ycombinator.com/vote?id=36791918&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>100% agree. And we are not there yet. But I feel like open source is the best answer to 1 (building an ecosystem of integrations), and that it will also allow us to build a better platform for developers (2): why learn Aura components/Apex when you already know and can use Typescript/React?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36792435"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36792435" href="https://news.ycombinator.com/vote?id=36792435&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>A "real" development stack might enable users to build actually good experiences. The custom (proprietary) stacks I've seen so far have always been pretty "meh", lacking features, community and tooling, resulting in not-great DX and making building some features something between very hacky and impossible.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36791909"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791909" href="https://news.ycombinator.com/vote?id=36791909&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Accurate. The licensing (and the "kill switch") at a previous job were controlled entirely via Salesforce. Our app would "phone home" regularly to check which features are being paid for and we'd toggle them on and off in our app accordingly.<p>I had, at the same company, been asked to evaluate building Salesforce apps (using the <i>custom programming language</i> they provide) and contrasting that with building new apps on our own metadata-driven platform.</p><p>Developers hate it, business people love it. It won't be going away for lack of paying customers, that's for sure.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36791984"><td></td></tr>
                <tr id="36792178"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36792178" href="https://news.ycombinator.com/vote?id=36792178&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>100% - having to submit code server side to just get compilation results is the worst development loop I've ever experienced.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36792245"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36792245" href="https://news.ycombinator.com/vote?id=36792245&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>I'm really eager to get to the point where we can work on CRM extensibility and developer experience. We're hoping to bring traditional web development workflows and not re-invent anything. 
We opted for a multi-tenant infrastructure for the cloud hosted version so there will be some additional challenges to make it work in that context!</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="36792378"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792378" href="https://news.ycombinator.com/vote?id=36792378&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Replace Salesforce and CRM with ServiceNow and ITSM and it's a similar story.  They call it "Land and Expand".  There's a reason they have a 99% or so renewal rate.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36792668"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792668" href="https://news.ycombinator.com/vote?id=36792668&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>I think of Salesforce like an octopus that puts immovable tentacles into the organisation that are almost never removed. It provides as much opportunity to be misused as possible!</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36792086"><td></td></tr>
                <tr id="36792202"><td></td></tr>
                <tr id="36793149"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36793149" href="https://news.ycombinator.com/vote?id=36793149&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Same thing, as is servicenow, they're all cloud platforms angling to be the "Company OS" all coming at it from different angles. Salesforce comes in at the sales side of things, SAP invades as a finance app, and ServiceNow begins their encroachment as an IT ticketing system, but they all wanna be THE only cloud platform your company needs.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36792647"><td></td></tr>
            <tr id="36792305"><td></td></tr>
            <tr id="36792232"><td></td></tr>
                        <tr id="36791886"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791886" href="https://news.ycombinator.com/vote?id=36791886&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>You can say the same thing (lock in) about any product that has migration costs. If everybody thought that way nothing would get built. I say to Felix: just do it, but think about these problems.<p>Fortunately there is a whole field of business devoted to this problem: go-to-market strategy. Target a niche, offer a compatible product, offer a significantly better product, offer a different product, offer a cheaper product ... the options are endless.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792414"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36792414" href="https://news.ycombinator.com/vote?id=36792414&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>&gt; You can say the same thing (lock in) about any product that has migration costs. If everybody thought that way nothing would get built.<p>Not all "migration costs" are the same: to quote General Turgidson, <i>"It is necessary now to make a choice, to choose between two admittedly regrettable, but nevertheless </i>distinguishable<i>, postwar environments: one where you got twenty million people killed, and the other where you got a hundred and fifty million people killed."</i></p><p>In the case of software migration costs, the cost of migrating away from a proprietary application-platform with zero-to-little code and data portability, will be orders of magnitude higher than the cost of migrating away from a proprietary infrastructure-as-a-service platform.</p><p>This isn't anything new: while Cloud-y platforms like SalesForce present even higher barriers to exercising our rights to data-sovereignty than what we had previously with SAP (because at least with SAP you can defenestrate the machines), it's all too similar to the 4GL vs. SQL wars of the 1990s. I honestly can't think of any orgs from then that regrets betting on a SQL-based RDBMS, while there are still companies out there depending on FoxPro, Progress, or worse...</p><p>This is also why I flat-out refuse to use Firebase.</p><p>Another hidden-cost of 4GL-like systems is that eventually they run-out-of-steam: hype fades and the vendor becomes stagnant and/or can't attract the best minds in the industry to design and build the platforms they expect others to use, so they lose whatever advantages they might have had which justified their proprietary nature - or an even more insidious version, whereby too many slow-moving companies become dependent on a particular platform that the platform's vendors have to intentionally hold-back the platform to avoid imposing too many fast-moving potentially breaking-changes (Java comes to mind...).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36791946"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36791946" href="https://news.ycombinator.com/vote?id=36791946&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Right, but the bigger the organization, the larger those costs. Think of a city of Los Angeles switching to a new system for tracking their public works department versus a township of 3,000 people. Smaller orgs are much more tolerant to this. New orgs start small and don't need something huge like Salesforce immediately; they need something usable.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36792248"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792248" href="https://news.ycombinator.com/vote?id=36792248&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>I work at a company that uses Salesforce as the system that runs the whole business. CRM barely touches on what we use it for.<p>We use it for inventory, logistics, accounting, customer support, managing our partners (dealers), managing our suppliers... as well as sales.</p><p>I spent 2 years as a Salesforce developer, and still dabble in APEX from time to time.</p><p>All of that is context for what I'm about to say:</p><p>Salesforce is a nightmare to develop and maintain. The concept of a central system to run the whole business makes sense. But Salesforce has no focus, IMHO.</p><p>I like how simple and easy to use Twenty is. But what I'd love to see is something like AirTable. Yes, call it a CRM so that you have an instant use case. But make it easy to do custom development. Make sure there's a great API. Much like Wordpress, make it easy to know where I can safely extend the platform.</p><p>Do all of that, and I think you might have a winning product!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792555"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792555" href="https://news.ycombinator.com/vote?id=36792555&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Thanks! One of the reasons there hasn't been that many good alternatives is indeed that the product surface is very big. We asked ourselves before launching here if it was too early but the reality is that there will always be more features we can build.
Odoo is an example of a super successful open source project in that category but it took them many years of development to get there. I hope that we can find a product market fit faster, by starting with simpler CRM use-cases and expanding slowly to new use-cases (CPQ, Marketing, Commerce) once we've got the basics right.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36792539"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792539" href="https://news.ycombinator.com/vote?id=36792539&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>The context is the explanation: they don't need to excel because they've got you. Every business should consider this when picking solutions.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36792224"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792224" href="https://news.ycombinator.com/vote?id=36792224&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Congrats on launching!<p>It looks like you started with Hasura GQL and switched to your own implementation (<a href="https://github.com/twentyhq/twenty/pull/156">https://github.com/twentyhq/twenty/pull/156</a>).</p><p>Would it be possible to comment on what influenced your decision here? I've built ontop of Hasura in the past and it's permissions model seems like it'd be a good fit for a CRM
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792541"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792541" href="https://news.ycombinator.com/vote?id=36792541&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Sure! We want to build something flexible where users can define their own custom objects and fields. We were thinking about leveraging Hasura that's already providing a flexible graphql API based on metadata which is exactly what we need / will need to build in the future.<p>However, there are three reasons that pushed us to go through a different way:</p><p>1) We want to build a cloud version which is multitenant (I personally think that provisionning single tenant instance at scale is not a good vision in a world with restricted resources ; there is a significant resource saving when we mutualize resources). This means that different users can have different data schemas. This is not possible with Hasura that serves one unique schema for all users.</p><p>2) We want to offer a good developer experience and Hasura comes as a standalone service. This means that installing the project gets much more complex than a "yarn &amp;&amp; yarn start", and creates a harder onboarding curve which we want to avoid as much as possible. If you face a Hasura issue during installation, you would need to understand Hasura workflows, and probably docker too.</p><p>3) Very similar to 2, we want Twenty to be easily self-hostable with 1-click to deploy. This would had pushed us to create bigger joint images including Twenty + Hasura, making it harder to maintain and to debug.</p><p>There is a great article on how Salesforce is built here: <a href="https://architect.salesforce.com/fundamentals/platform-multitenant-architecture" rel="nofollow noreferrer">https://architect.salesforce.com/fundamentals/platform-multi...</a>. They basically have 1 metadata table and 1 data table (uuid, objectid, orgid, field1, ..., field500) where each column is a VARCHAR and they have built their own engine on top of it. I think we will likely need to do something similar and we cannot build it on top of Hasura / we lose the value of Hasura by building on top of it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36791854"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36791854" href="https://news.ycombinator.com/vote?id=36791854&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>I love this! Having been the “Salesforce guy” at a few companies, it’s good to see people working to solve those problems.<p>As a developer, I’m eager to have a solution that limits sales teams constructively. The ability to add, edit, and delete fields and properties on the fly made creating maintainable client software difficult.</p><p>It will be cool to see how you go about it!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792033"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792033" href="https://news.ycombinator.com/vote?id=36792033&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Thanks! We plan to add custom fields in August. 
Super interesting feedback. There's indeed a balance to strike between:
- easy to configure by everyone which ends up in a mess
- full config-as-code and developers become a bottleneck
Hasura is an example of an app that started as UI-driven and I feel is becoming more code-driven. Probably a frequent natural path as product evolve and move from SMB to Enterprise. Since the CRM buyer is often the Head of Sales, we'll have to be smart to sell them the more restrictive path!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36792519"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792519" href="https://news.ycombinator.com/vote?id=36792519&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Looks neat iFelix!<p>I'm interested to know how much time you've actually spent using CRMs or ERPs yourself. Not clicking around and exploring features, but actually using for something.</p><p>While I know this is a very early release, my biggest concern is naivety about real world usage requirements, and if you'll be able to manage the feature creep that will be coming with it without just turning into another Salesforce or NetSuite.</p><p>As soon as you get users it's going to get harder to add in those things you're putting off thinking about until later, and managing it will become a legacy suckfest. This manifests itself in a slow, slow, slow experience and necessary UX decisions that make the user scratch their head.</p><p>I'd say the biggest example of this is custom fields. It's not just another problem to solve at some point, it's probably one of the biggest ruins of CRMs.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792691"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792691" href="https://news.ycombinator.com/vote?id=36792691&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Your question is right on spot. We are three founders with a passion for design (1 designer + 2 ex-Airbnb were design was key to the culture), building this product 2 decades after Salesforce when the tech is different, so we will approach problems with different priorities and ways of thinking than Salesforce did. But there is definitely some naivety and some challenge ahead of us, as none us is an expert ERP/CRM user. We'll be very careful making design decisions on structural elements like like custom fields yes!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36792368"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792368" href="https://news.ycombinator.com/vote?id=36792368&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Congrats on the launch.<p>My two cents: 
I work in marketing and CRM for eCommerce lifestyle companies. When I evaluate a CRM I try to understand if it's for sales or eCommerce or both. I see in your docs that you mention that specific point of sales being dominant in CRM so that's a plus.</p><p>Then I look at if it natively or easily connects to the tools that the company uses. If I'm pointed to using Zapier for connections that's usually too costly because you're collecting a lot of emails and you can quickly get into millions of calls for low quality leads.</p><p>Another big miss I notice is not integrating with POS systems and only eCommerce so that quickly creates an issue with companies opening stores.</p><p>On the sales side there's usually less of a sales pipeline and more of a clienteling side where you've got sales people at store locations reaching out to customers to announce products or services or events. It's a lot less of stages because products are usually not all that high value.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36791482"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36791482" href="https://news.ycombinator.com/vote?id=36791482&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Congrats on the launch! JFYI, app. and docs.twenty.com are both showing cloudflare origin errors at present.<p>The signup from the main site asked me to message on WhatsApp, which I don't use, so I backed out from signing up.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36791528"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791528" href="https://news.ycombinator.com/vote?id=36791528&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Sorry about that, not sure if it's the HN trafic that crashed the server or something else. We're looking into it. The signup is on app.twenty.com and hopefully it should be back soon, I'll reply to this post when it is</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36791546"><td></td></tr>
                  <tr id="36791537"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791537" href="https://news.ycombinator.com/vote?id=36791537&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>This is disqualifying, for me. I tried to view the CRM and it asked me to log in. There are a lot of dark patterns just at first blush that make this feel spammy.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36791585"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36791585" href="https://news.ycombinator.com/vote?id=36791585&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Sorry about this. Our focus for this launch was the Github repo / local setup, I should have made that more clear. We haven't launched to non-developer audience yet, that's why our marketing website redirects to a waiting list.
If you want to try the app quickly you can just go to app.twenty.com and put a fake email address if you don't want to give yours. The signup is quick.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36791595"><td></td></tr>
                <tr id="36791746"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36791746" href="https://news.ycombinator.com/vote?id=36791746&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Sorry. It was down a few minutes ago but should be back now? Could you trying clearing the cache (cmd + shift + r)?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36791967"><td></td></tr>
                              <tr id="36792085"><td></td></tr>
                <tr id="36792142"><td></td></tr>
                              <tr id="36792638"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792638" href="https://news.ycombinator.com/vote?id=36792638&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Congrats on launch! I completely agree with the wisdom of launching early as soon as some basic functionality works. If I had to suggest the lowest-hanging fruits for your roadmap, it would be email automation and lead enrichment.<p>I'm an engineering consultant for various sales tech startups which operate within the ecosystem of "build a HubSpot/Salesforce/Freshworks/etc integration for a specific type of sales organization". What I would love to see in an open-source CRM is an easy way to bring these integrations directly into the CRM - imagine a `crm.json` file which configures a CRM web app and imports custom code modules from a central repository, similar to `package.json` and NPM.</p><p>A big issue end users bring up with Salesforce/HubSpot is the high cost, especially for sales organizations which only need the core features you have in your demo today (track leads, deals, companies, etc) but have to buy a seat for each salesperson. A managed service for a hosted CRM without feature/usage/seat limitations would be an easy sell if you can reliably ingest existing CRM data and provide some level of integrations/customization.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792976"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792976" href="https://news.ycombinator.com/vote?id=36792976&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>I like the "crm.json" image. We definitely want to work on doing something modular like this.<p>I also wanted to price differently than by seat initially. Because CRMs tends to be the source of truth for the whole organizations, and usually teams like customer support are left out because it's not worth paying a license for them to just read the information the sales have put in, while it would be useful. But we didn't find a better way to price in the end. Pricing by usage feels off since there is no cost associated to usage (a user that logs more activities is not going to cost us more). How would you charge then?</p><p>Noted for email automation / lead enrichment!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36793021"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36793021" href="https://news.ycombinator.com/vote?id=36793021&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>How do you get data out of this? I see you mention GraphQL, but is there sample output? Are filters and searches etc. supported?<p>Also, can you write/mutate data via GraphQL too or is it read only?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36791928"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36791928" href="https://news.ycombinator.com/vote?id=36791928&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>We are currently evaluating CRM packages. This is probably a bit too in development for me to recommend but I'll let you know our internal requirements so you have one extra data point in the market.<p>Phone integration is huge for us. We need the CRM to respond to an incoming call by bringing up the contacts details if it recognises the number. We also need a log that the cal began, was answered by, and how long it lasted. If we could do that on cell phones it would be amazing.</p><p>The next thing would be ecommerce integration followed by integration with our accounting system. Both need to feed contacts and contact details into the CRM. What have they ordered, how much have they spent. What are their payment terms.</p><p>After that it's all just notes.</p><p>Oh, and we would need SLAs with good up time and data protection.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792769"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792769" href="https://news.ycombinator.com/vote?id=36792769&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>You should evaluate close.com if not already. They have all the features you mentioned. We use them for sales for our business.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36792338"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792338" href="https://news.ycombinator.com/vote?id=36792338&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>What type of phone integration would you need? Do you use an external provider like Aircall? If you don't, I know Close.com has a built-in phone functionality so maybe their solution could fit your need?<p>We definitely need to work on integrations soon!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36791576"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36791576" href="https://news.ycombinator.com/vote?id=36791576&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>this is brilliant (and great name).
The true value of a salesorce/hubspot killer CRM is integrations. Most people use hubspot or salesforce just as a database and use a bunch of other tools.<p>If you want to do one thing - focus on integrations first and UI second. Let your integration architecture inform your UI/metadata driven architecture.</p><p>the good thing is - this is an easy step into monetization. Anyone would pay the same cost as hubspot for an opensource alternative...but with the same integrations. Managing the data pipelines is the hard part.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36791638"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791638" href="https://news.ycombinator.com/vote?id=36791638&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Yes that's exactly what we've seen! A lot of companies we've talked with don't have a datawarehouse / reverse-ETL, so they use the CRM for that instead,and the integrations are core the CRM value prop. That's what makes it hard for small CRMs with a nice UI to compete with large players. From that perspective, I think being open source will help us a lot build this network of integrations faster<p>Great feedback that we should prioritize it as early as possible, thanks!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36793099"><td></td></tr>
            <tr id="36792617"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792617" href="https://news.ycombinator.com/vote?id=36792617&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Best of luck. Would be happy to receive an email from your sales team once you have the equivalent of HubSpot marketing enterprise. As an engineer turned founder, I agree these products suck and the market is ready for disruption. Focusing on engineers and product quality is a good play.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36792703"><td></td></tr>
                  <tr id="36792308"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792308" href="https://news.ycombinator.com/vote?id=36792308&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Our team has recently found an open-source sales management CRM tool, Meow <a href="https://www.sales-funnel.app/" rel="nofollow noreferrer">https://www.sales-funnel.app/</a>, which they are happy with. Licensed under AGPL, it might be worth a look for startups.<p>I guess there are definite reasons why open-source isn't as successful in the CRM space. Buyers are mostly not developers and the requirements for a CRM are often very diverse. I agree with earlier comments, Salesforce is not just a CRM.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36792738"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792738" href="https://news.ycombinator.com/vote?id=36792738&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>We have been using the CRM as early users for the past weeks and are very happy. The Twenty team is super responsive and is shipping features quickly. All the best on the launch, Team Twenty!!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36793081"><td></td></tr>
                  <tr id="36792391"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792391" href="https://news.ycombinator.com/vote?id=36792391&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>&gt; Our tool only does a small part of what big CRM players offer, but we focused on providing a great user experience on the basics, instead of spreading ourselves thin across a vast range of features and delivering them half-heartedly.<p>I like this approach. So much software feels completely half-assed and frustrating to use. Quality can be a real differentiator (it just has to be empathized enough).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36793221"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36793221" href="https://news.ycombinator.com/vote?id=36793221&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Agree! Linear is a good example of a company picking up steam with a design/quality-first approach when the market leader Jira suffers from feature bloat. Hope we can make a similar comparaison one day!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36791987"><td></td></tr>
                <tr id="36792042"><td></td></tr>
                <tr id="36792112"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36792112" href="https://news.ycombinator.com/vote?id=36792112&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>I thought building on TipTap would take too much time but I was wrong. I've looked into TipTap extensions and it seems that in less than a week you can build a very decent Notion-like editor. So we will be rebuilding everything with TipTap soon.<p>Initially I chose Blocknote over EditorJS because I wanted draggable blocks / something that feels more like Notion. But it wasn't a good call to make the decision based on that.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792493"><td></td></tr>
                              <tr id="36792479"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792479" href="https://news.ycombinator.com/vote?id=36792479&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Very cool product, seems like a good example of an SLC version =)<p>The open-source approach makes a lot of sense especially given the need around integration and customization (&amp; awful developer experience of competition...)</p><p>Congrats on the launch!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36793126"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36793126" href="https://news.ycombinator.com/vote?id=36793126&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Thanks Maxime! 
Had to Google SLC, for those who are reading it's "Simple, Lovable and Complete" — and exactly where we'd like to get soon yes :-)</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36792070"><td></td></tr>
                <tr id="36792190"><td></td></tr>
                  <tr id="36792944"><td></td></tr>
                <tr id="36793016"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36793016" href="https://news.ycombinator.com/vote?id=36793016&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>SugarCRM has been sold to a PE fund and close the source years ago. There is a project called SuiteCRM that took over the source code, and they did a great job considering it's a small organization, but it didn't evolve the way it would have if SugarCRM had remained open and under the same ownership</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36792257"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792257" href="https://news.ycombinator.com/vote?id=36792257&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>The whatsapp app part does not open whatsapp just opens the App Store, deep links are tricky to get right,. Feels like a strange choice for a signup form /waitlist</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36792405"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792405" href="https://news.ycombinator.com/vote?id=36792405&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>It led to some fun conversations but most people just ignore the WhatsApp link. Definitely not something we're going to keep long-term. The initial goal was to be able to engage with a few users that were willing to, to do real customer interviews and not just have them fill an impersonal form</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36792237"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36792237" href="https://news.ycombinator.com/vote?id=36792237&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>The app looks nice and great to see it as open source. I have one thing to nitpick, though. The app hijacks the back and prevents the user from being able to <i>go back</i></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36792264"><td></td></tr>
                  <tr id="36792587"><td></td></tr>
                <tr id="36793166"><td></td></tr>
                  <tr id="36792471"><td></td></tr>
                <tr id="36792789"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36792789" href="https://news.ycombinator.com/vote?id=36792789&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>We'll focus on providing a cloud hosted version, because not everyone knows how to or want to self-host.<p>We don't really think in terms of runway yet as the company has just been setup. YC gave us 500k which we haven't spent yet.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36791817"><td></td></tr>
                <tr id="36791863"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791863" href="https://news.ycombinator.com/vote?id=36791863&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>It cost us ~$100k from a broker. I've always loved nice domain names and I had some cash because I sold my previous company to Airbnb so I was happy to spend it!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36792658"><td></td></tr>
                <tr id="36792761"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36792761" href="https://news.ycombinator.com/vote?id=36792761&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Haha, it's the usual market price for a nice one-word .com ; I don't see this as money thrown away like spending in ads for example, it's closer to a real estate investment in my opinion</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="36791597"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36791597" href="https://news.ycombinator.com/vote?id=36791597&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>ooooh, this is nifty. I was just thinking we needed a good CRM!  Are there any other good, self-hosted alternatives I hsould be aware of?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36791700"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791700" href="https://news.ycombinator.com/vote?id=36791700&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>Not trying to hide our competition but honestly, we came to build this because I didn't find any.<p>The leader 15 years ago was SugarCRM but sadly they ended up getting bought by a PE fund and closing the source. There is a project called SuiteCRM[1] that continued with an open source fork but in my opinion they lack the "modern" touch that we were looking for.</p><p>Besides that, other options I've seen are Yeti [2] or Odoo [3].
Odoo is very successful but it's different because it's an ERP so CRM is only a small part of what they do. They tend do do a lot of things so can't do all of them very well.</p><p>[1] <a href="https://github.com/salesagility/SuiteCRM">https://github.com/salesagility/SuiteCRM</a>
[2] <a href="https://github.com/YetiForceCompany/YetiForceCRM">https://github.com/YetiForceCompany/YetiForceCRM</a>
[3] <a href="https://github.com/odoo/odoo">https://github.com/odoo/odoo</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36792080"><td></td></tr>
                <tr id="36792167"><td></td></tr>
                <tr id="36792365"><td></td></tr>
                              <tr id="36791831"><td></td></tr>
                  <tr id="36791748"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36791748" href="https://news.ycombinator.com/vote?id=36791748&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>This will be DOA. CRM is a essentially a solved problem.<p>However, since you are open source I think there is an opportunity to either sell the framework to ISVs (think a property management system provider who wants to provide CRM functionality to their platform overnight... like embedded analytics such as Looker) or have the community create industry specific CRMs.</p><p>Good luck.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36791844"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791844" href="https://news.ycombinator.com/vote?id=36791844&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><br><div>
                  <p><span>Yes that's something we've considered. Veeva and nCino are two examples of billion-dollar companies built on top of Salesforce and verticalized in one industry, so there is a need for what you describe. If we want to go this path we would have to move to an MIT license. We will learn with the community and adapt!</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36791777"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36791777" href="https://news.ycombinator.com/vote?id=36791777&amp;how=up&amp;goto=item%3Fid%3D36791434"></a></center>    </td><td><p><span>&gt; This will be DOA. CRM is a essentially a solved problem.<p>I was about to ask if it's possible to make money on CRM today, given the near infinite number of choices at all levels.  But maybe it only works if you specialize, as you alluded to in your comment.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stanford president resigns over manipulated research, will retract 3 papers (1343 pts)]]></title>
            <link>https://stanforddaily.com/2023/07/19/stanford-president-resigns-over-manipulated-research-will-retract-at-least-3-papers/</link>
            <guid>36790301</guid>
            <pubDate>Wed, 19 Jul 2023 17:43:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stanforddaily.com/2023/07/19/stanford-president-resigns-over-manipulated-research-will-retract-at-least-3-papers/">https://stanforddaily.com/2023/07/19/stanford-president-resigns-over-manipulated-research-will-retract-at-least-3-papers/</a>, See on <a href="https://news.ycombinator.com/item?id=36790301">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="1a2cdf1" data-element_type="widget" id="single-content" data-widget_type="theme-post-content.default">
<p>Stanford President Marc Tessier-Lavigne will resign effective August 31 according to communications released by the University Wednesday morning. He will also retract or issue lengthy corrections to five widely cited papers for which he was principal author after a Stanford-sponsored investigation found “manipulation of research data.”</p>
<p><a href="https://boardoftrustees.stanford.edu/special-committee/#2023-07-19" target="_blank" rel="noopener">According to Jerry Yang</a>, chair of the Stanford Board of Trustees, Tessier-Lavigne will step down “in light of the report and its impact on his ability to lead Stanford.” Former Dean of Humanities Richard Saller will serve as interim president. In a separate <a href="https://tessier-lavigne-lab.stanford.edu/news/message-stanford-community" target="_blank" rel="noopener">statement</a>, Tessier-Lavigne defended his reputation but acknowledged that issues with his research, first raised in a <a href="https://stanforddaily.com/2022/11/29/stanford-presidents-research-under-investigation-for-scientific-misconduct-university-admits-mistakes/">Daily investigation</a> last autumn, meant that Stanford requires a president “whose leadership is not hampered by such discussions.”</p>
<p>“At various times when concerns with Dr. Tessier-Lavigne’s papers emerged—in 2001, the early 2010s, 2015-2016, and March 2021—Dr. Tessier-Lavigne failed to decisively and forthrightly correct mistakes in the scientific record,”&nbsp; Stanford’s <a href="https://boardoftrustees.stanford.edu/wp-content/uploads/sites/5/2023/07/Scientific-Panel-Final-Report.pdf" target="_blank" rel="noopener">report</a> said, identifying a number of apparent manipulations in Tessier-Lavigne’s neuroscientific research.</p>
<p>The report concluded that the fudging of results under Tessier-Lavigne’s purview “spanned labs at three separate institutions.” It identified a culture where Tessier-Lavigne “tended to reward the ‘winners’ (that is, postdocs who could generate favorable results) and marginalize or diminish the ‘losers’ (that is, postdocs who were unable or struggled to generate such data).”</p>
<p>There was no evidence that Tessier-Lavigne himself manipulated data in the papers reviewed, the report concluded, nor that he knew about manipulation at the time, but he “has not been able to provide an adequate explanation” for why he did not correct the scientific record when presented the opportunity on multiple occasions. In his statement, Tessier-Lavigne wrote that he was “gratified that the Panel concluded I did not engage in any fraud or falsification of scientific data.”</p>
<p>He also acknowledged that “the report identified some areas where I should have done better, and I accept the report’s conclusions.” For its part, the report identified “repeated instances of manipulation of research data and/or subpar scientific practices from different people and in labs run by Dr. Tessier-Lavigne at different institutions.”</p>
<p>Retracting a paper is a rare act, especially for a scientist of Tessier-Lavigne’s stature. A <a href="http://retractiondatabase.org/RetractionSearch.aspx?" target="_blank" rel="noopener">database</a> of retractions shows that only four in every 10,000 papers are retracted. The move is saved for when there is “clear evidence that the findings are unreliable,” according to guidelines from the nonprofit Committee on Publication Ethics. Tessier-Lavigne had <a href="https://stanforddaily.com/2022/11/29/stanford-presidents-research-under-investigation-for-scientific-misconduct-university-admits-mistakes/">claimed repeatedly</a> last autumn that the issues in his studies “do not affect the data, results or interpretation of the papers.”</p>
<p>For several papers worthy of retraction to have been principally authored by the same scientist represents “unusual frequency of manipulation of research data and/or substandard scientific practices,” the investigation concluded. Tessier-Lavigne is expected to retract or issue robust corrections to at least five papers with concerns he had not addressed for years, including a widely publicized study that he once claimed “turn[s] our current understanding of Alzheimer’s on its head.”</p>
<p>Stanford’s report, released in a 95-page document Wednesday morning, is the work of Mark Filip, a former deputy attorney general contracted by a special committee of the Stanford Board of Trustees to review allegations first identified in Daily reporting last November. Filip drafted several high-profile scientists, including Nobel laureate Randy Schekman, former Princeton president Shirley Tilghman, former Harvard provost Steve Hyman and two other members of the National Academies.</p>
<p>The investigation took eight months, with one member stepping off <a href="https://stanforddaily.com/2022/12/05/conflict-of-interest-leads-member-of-special-committee-investigating-stanford-president-to-step-aside/">after The Daily revealed</a> that he maintained an $18 million investment in a biotech company Tessier-Lavigne cofounded. <a href="https://stanforddaily.com/2023/07/19/sources-refused-to-participate-in-stanford-investigation-of-president-after-they-werent-guaranteed-anonymity/">Reporting by The Daily this week</a> shows that some witnesses to an alleged incident of fraud during Tessier-Lavigne’s time at the biotechnology company Genentech refused to cooperate because investigators would not guarantee them anonymity, even though they were bound by nondisclosure agreements.</p>
<p>The Daily has also reported that another incident was omitted altogether.</p>
<p>Not guaranteeing anonymity in an investigation of this importance is an “extremely unusual move,” said Jeffrey Flier, who ran a number of research misconduct inquiries in his time as Dean of Harvard Medical School. He and other outside observers wondered whether the decision hampered the investigation’s access.</p>
<p>With the access that it did have, the investigation concluded that the <a href="https://stanforddaily.com/2023/03/06/mtl-knew-misconduct-allegations-independently-corroborated-in-private-correspondence-to-special-committee/">accounts of five high-level Genentech executives</a> and scientists over one of the two incidents of reported fraud in Tessier-Lavigne’s laboratory “are not accurate.” Tessier-Lavigne’s statement said “the report clearly refutes the allegations of fraud and misconduct that were made against me.”</p>
<p>Despite spending nearly eight months on this investigation, none of the high-profile scientists would answer questions about their work or the decision not to guarantee anonymity. Filip and Aidan Ryan, an Edelman senior vice president for crisis communications serving as the spokesperson for the investigation, answered questions on background.</p>
<p>While the report said it had not found evidence of fraud in the 2009 paper, the panel concluded that the research that went into it “fell below accepted scientific practices, let alone Dr. Tessier-Lavigne’s self-described standard of scientific excellence” and that the paper ought to be retracted or face “a comprehensive and robust set of corrections.”</p>
<p>This statement was a rebuke of Tessier-Lavigne’s public defense. His lawyer Stephen Neal, chair emeritus of Cooley, <a href="https://stanforddaily.com/2023/02/17/internal-review-found-falsified-data-in-stanford-presidents-alzheimers-research-colleagues-allege/">had written on Tessier-Lavigne’s behalf in a letter to The Daily</a> last February that “a correction or retraction of those findings would have been unwarranted and inappropriate.”</p>
<p>The report broke new ground in other areas, while confirming <a href="https://stanforddaily.com/2023/06/15/stanford-ends-year-with-leadership-uncertainty/">many details previously reported.</a></p>
<p>“Within weeks after the publication of” a 2001 article in the journal Science now thought to contain doctored imagery, the report said, a colleague in the field identified an error. “Dr. Tessier-Lavigne stated to the colleague in writing that he would take corrective action, including both contacting the journal and attempting to issue a correction…He did not contact the journal and he did not attempt to issue an erratum, which is inadequate.”</p>
<p>The report noted that Tessier-Lavigne had not followed up for seven years on unpublished corrections to two of his papers in Science, concluding that “Dr. Tessier-Lavigne did not have an explanation for deciding to not follow up on the corrections beyond that he has a practice of drafting many emails to see how they read but only sends a portion of them and that he concluded the communication was unnecessary,” the report said. “To date, the scientific record remains uncorrected.”</p>
<p>The report noted that explanations made by Tessier-Lavigne in correspondence with editors at the journal Nature over manipulated research data in a 2004 paper were “not fully responsive to the range of publicly expressed concerns given the available forensic evidence.” Since then, Tessier-Lavigne “has acknowledged the presence of manipulation of research data,” and assented to a correction after the panel concluded it was “required and appropriate for the paper.”</p>
<p>Mark Filip, who led the review, said in an interview that more investigations could come out of the board’s report.</p>
<p><em>This story is breaking and will be updated.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Asynchronous Rust on Cortex-M Microcontrollers (174 pts)]]></title>
            <link>https://interrupt.memfault.com/blog/embedded-async-rust</link>
            <guid>36790238</guid>
            <pubDate>Wed, 19 Jul 2023 17:39:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://interrupt.memfault.com/blog/embedded-async-rust">https://interrupt.memfault.com/blog/embedded-async-rust</a>, See on <a href="https://news.ycombinator.com/item?id=36790238">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        <p>In the realm of embedded software, the demand for efficient and responsive
applications is ever-increasing. Asynchronous programming, with its ability
to handle concurrent tasks effectively, holds immense potential in this domain.
Let’s delve into the world of asynchronous Rust specifically tailored for
microcontrollers.</p>

<!-- excerpt start -->

<p>In this article, we explore the inner workings of Futures, cooperative
scheduling, and Async Rust executors, highlighting their significance in
optimizing resource utilization. Moreover, we introduce the Rust Embassy
project, an innovative framework designed to unlock the power of asynchronous
programming on microcontrollers.</p>

<!-- excerpt end -->

<blockquote>
  <p><strong>Note:</strong> This article assumes knowledge of Rust, as well as knowledge of
using Rust on microcontrollers. If you’re unfamiliar with Rust, I suggest
reading <a href="https://doc.rust-lang.org/book/" target="_blank">The Rust Book</a>. For a good
understanding of embedded Rust,
<a href="https://docs.rust-embedded.org/book/" target="_blank">The Embedded Rust Book</a> is a great
resource, as well as our
<a href="https://interrupt.memfault.com/blog/zero-to-main-rust-1">Zero to Main: Rust</a> article
covering how Rust runs on Cortex-M based microcontrollers.</p>
</blockquote>

<p>Like Interrupt? <a href="https://go.memfault.com/interrupt-subscribe" target="_blank">Subscribe</a> to get our latest posts straight to your mailbox.</p>

<div id="toc">

  <h2 id="table-of-contents">Table of Contents</h2>

  <!-- prettier-ignore -->
<ul id="markdown-toc">
  <li><a href="#what-is-asynchronous-programming" id="markdown-toc-what-is-asynchronous-programming">What is Asynchronous Programming?</a></li>
  <li>
<a href="#how-does-async-rust-work" id="markdown-toc-how-does-async-rust-work">How Does Async Rust Work?</a>    <ul>
      <li><a href="#rust-futures" id="markdown-toc-rust-futures">Rust Futures</a></li>
    </ul>
  </li>
  <li>
<a href="#async-executors" id="markdown-toc-async-executors">Async Executors</a>    <ul>
      <li><a href="#cooperative-vs-preemptive-scheduling" id="markdown-toc-cooperative-vs-preemptive-scheduling">Cooperative vs. Preemptive Scheduling</a></li>
    </ul>
  </li>
  <li>
<a href="#embassy" id="markdown-toc-embassy">Embassy</a>    <ul>
      <li><a href="#blinky-example" id="markdown-toc-blinky-example">Blinky Example</a></li>
      <li><a href="#button-example" id="markdown-toc-button-example">Button Example</a></li>
    </ul>
  </li>
  <li><a href="#benefits-of-async-rust-on-embedded-systems" id="markdown-toc-benefits-of-async-rust-on-embedded-systems">Benefits of Async Rust on Embedded Systems</a></li>
  <li><a href="#downsides-of-async-rust-on-embedded-systems" id="markdown-toc-downsides-of-async-rust-on-embedded-systems">Downsides of Async Rust on Embedded Systems</a></li>
  <li><a href="#final-thoughts" id="markdown-toc-final-thoughts">Final Thoughts</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

</div>

<h2 id="what-is-asynchronous-programming">What is Asynchronous Programming?</h2>

<p>Asynchronous programming is a type of concurrent programming where the
execution of a task can be paused if it does not have work to do. This is
especially useful for tasks that are waiting for some external event to occur,
such as a network packet arriving, or a button being pressed. In a synchronous
system, the task would have to wait for the event to occur before it could
continue execution. In an asynchronous system, the task can be paused, and
another task can be executed until the event occurs. This allows for more
efficient use of resources, as the thread this task is running on can be used
for other tasks while it is waiting. Additionally all resources for the thread,
such as the stack, an be used by other tasks while it is waiting.</p>

<h2 id="how-does-async-rust-work">How Does Async Rust Work?</h2>

<p>Similar to other languages Rust provides async/await syntax to make writing
concurrent code easier. The async/await syntax is built on top of the
<a href="https://doc.rust-lang.org/std/future/trait.Future.html" target="_blank"><code>Future</code></a> trait, which
is the core of Rust’s asynchronous programming story. Before we dive into how
async Rust works, let’s take a look at how async functions are defined in Rust:</p>

<div><pre><code><span>async</span> <span>fn</span> <span>foo</span><span>()</span> <span>-&gt;</span> <span>u32</span> <span>{</span>
    <span>bar</span><span>()</span><span>.await</span>
<span>}</span>

<span>async</span> <span>fn</span> <span>bar</span><span>()</span> <span>-&gt;</span> <span>u32</span> <span>{</span>
    <span>42</span>
<span>}</span>
</code></pre></div>

<p>This code is pretty simple, but there are a few things to note. First, the
<code>async</code> keyword is used to mark a function as async. It’s important to note that
the <code>async</code> keyword is just syntactic sugar for returning a future. The function
signature for <code>foo</code> could be rewritten as:</p>

<div><pre><code><span>fn</span> <span>foo</span><span>()</span> <span>-&gt;</span> <span>impl</span> <span>Future</span><span>&lt;</span><span>Output</span> <span>=</span> <span>u32</span><span>&gt;</span>
</code></pre></div>

<p>The <code>await</code> keyword is important, as it signals for the underlying future to
make progress. Futures in Rust are considered “lazy”, meaning they will
not make any progress until they are polled. We’ll cover polling in more detail
later, but for now, just think of it as a way to advance the state of the future.
The <code>await</code> keyword is what calls <code>poll</code> on the future, and returns the
result of the future.</p>

<p>Now that we have a good idea of how async functions generate futures, let’s look
in depth into how futures work.</p>

<h3 id="rust-futures">Rust Futures</h3>

<p>Futures in Rust represent a calculation that eventually yields some result. All
futures implement the
<a href="https://doc.rust-lang.org/std/future/trait.Future.html" target="_blank"><code>Future</code></a> trait:</p>

<div><pre><code><span>pub</span> <span>trait</span> <span>Future</span> <span>{</span>
    <span>type</span> <span>Output</span><span>;</span>

    <span>fn</span> <span>poll</span><span>(</span><span>self</span><span>:</span> <span>Pin</span><span>&lt;&amp;</span><span>mut</span> <span>Self</span><span>&gt;</span><span>,</span> <span>cx</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Context</span><span>&lt;</span><span>'_</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Poll</span><span>&lt;</span><span>Self</span><span>::</span><span>Output</span><span>&gt;</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Pretty simple right? Let’s unpack what we’re looking at. The <code>Output</code> type is
exactly what you’d expect, the output of our calculation. The <code>poll</code> function
hides a bit of complexity. Starting from the return value we see our defined
<code>Output</code> type, but what is the type wrapping it? Diving deeper we see the
following definition for
<a href="https://doc.rust-lang.org/std/task/enum.Poll.html" target="_blank"><code>Poll</code></a>:</p>

<div><pre><code><span>pub</span> <span>enum</span> <span>Poll</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>{</span>
    <span>Ready</span><span>(</span><span>T</span><span>),</span>
    <span>Pending</span><span>,</span>
<span>}</span>
</code></pre></div>

<p>This enum represents the two possible states of a future. The <code>Ready</code> state
indicates that our calculation represented by the future is complete, and
encloses our defined return type. The <code>Pending</code> state indicates that our future
is not yet complete, and has more work that needs to be done. Generally, this
means we’re waiting for some I/O to complete, say a packet from a network stack,
or a message over a SPI bus.</p>

<p>To advance the state of our future we could call <code>poll</code> repeatedly until we
get a <code>Ready</code> status, but that seems rather inefficient. If our <code>Future</code> is
dependent on an external I/O event, looping over calls to <code>poll</code> keeps our
processor awake and wastes precious power. This observation brings us to the
only argument passed into the <code>poll</code> function, the future
<a href="https://docs.rs/futures/latest/futures/task/struct.Context.html" target="_blank"><code>Context</code></a>.
The <code>Context</code> only has one role, to provide access to the <code>Future</code>’s
<a href="https://docs.rs/futures/latest/futures/task/struct.Waker.html" target="_blank"><code>Waker</code></a>.</p>

<p>The <code>Waker</code> gives us an elegant solution to calling our <code>poll</code> implementation in
a loop. When a future is not ready to complete, the asynchronous action can save
off the <code>Waker</code> and use it to inform the executor that there is more work to be
done on the future. An example of this would be an interrupt on a GPIO wired to
a button. We’ll go over this exact example later in the article!</p>

<p>Now that we understand how futures work, we can plot out the lifetime of one
like so:</p>

<p><img src="https://interrupt.memfault.com/img/embedded-rust-async/future-lifetime.excalidraw.png" alt=""></p>

<p>The last part of the poll function we haven’t covered is a fairly spicy bit.
The <code>self</code> reference here is a little strange. Let’s take a deeper look:</p>



<p>This is one of the stranger variants of self we can see on a trait.
The <code>&amp;mut self</code> is pretty standard, but what is this <code>Pin</code> thing? The
<a href="https://doc.rust-lang.org/std/pin/struct.Pin.html" target="_blank"><code>Pin</code></a> struct acts as a
marker that tells the compiler that the memory of the wrapped type will not be
moved. This is important when we’re dealing with futures, as we need to ensure
that all of the state tracking the future’s progress is not moved. If the
underlying calculation is not complete, and we move the memory, we could lose
track of the future’s progress. Any future poll would potentially be in an
invalid state, as it would attempt to access memory that is no longer valid.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup></p>

<h2 id="async-executors">Async Executors</h2>

<p>Now that we have a good understanding of Rust futures work we need to look at
the core of any async system, the executor. Unlike its contemporaries in other
languages like Go or Javascript, Rust does not have a built-in executor. This
means that we need to provide our executor implementation. While this may
add a bit of complexity, it also gives us a lot of flexibility. We can choose
the executor that best fits our needs, and we can even write our own if we need
to. This is what allows us to run async Rust on embedded systems, as well as
giant servers in the cloud.</p>

<p>Any executor implementation needs to fulfill a few requirements:</p>

<ol>
  <li>Keep track of all running tasks</li>
  <li>Provide the ability to spawn new tasks</li>
  <li>Provide a mechanism for
<a href="https://doc.rust-lang.org/core/future/index.html" target="_blank"><code>Waker</code></a>s to notify the
executor there is work to be done for a given task.</li>
  <li>Poll all tasks when they have work to complete</li>
</ol>

<p>Since Rust futures are state machines that expect to run to completion, we can’t
rely on preemptive scheduling like we would when running an RTOS. Instead, we
must use cooperative scheduling.</p>

<h3 id="cooperative-vs-preemptive-scheduling">Cooperative vs. Preemptive Scheduling</h3>

<h4 id="preemptive-scheduling">Preemptive Scheduling</h4>

<p>We’re all familiar with Preemptive scheduling. This is the type of scheduling
used in most RTOS, as well as the most prominent operating systems like Linux,
Windows, and MacOS. At a high level preemptive schedulers can stop the execution
of a task at any time it sees fit. Generally, there is a concept of priority,
and the OS kernel will favor higher priority threads when determining which
threads to run while attempting to allow all tasks to run.</p>

<p><img src="https://interrupt.memfault.com/img/embedded-rust-async/preemptive.excalidraw.png" alt=""></p>

<p>If you’re looking for a deeper dive into RTOS context switching, check out our
post on
<a href="https://interrupt.memfault.com/blog/cortex-m-rtos-context-switching">Cortex-M Task Switching</a>.
Additionally, check out this great article doing a
<a href="https://tweedegolf.nl/en/blog/65/async-rust-vs-rtos-showdown" target="_blank">Comparison of FreeRTOS and Embassy</a>.</p>

<h4 id="cooperative-scheduling">Cooperative Scheduling</h4>

<p>Cooperative scheduling is quite different. There is no priority, and tasks are
run until they return (yield) control back to the caller. It is the
responsibility of the task to be a good neighbor, and not run for too long.
Compare the preemptive scheduling diagram above with the cooperative scheduling
diagram below. Notice how the tasks can run to completion before
switching to the next task.</p>

<p><img src="https://interrupt.memfault.com/img/embedded-rust-async/cooperative.excalidraw.png" alt=""></p>

<p>On its own, this cooperative scheduling doesn’t seem to provide much value. Why
would we want to move the cognitive load of ensuring all tasks run from the
OS scheduler to the writer of the tasks? Doesn’t this also start to resemble a
super loop? I thought we moved to an RTOS to get away from having to worry about
these things?! If we combine cooperative scheduling with our understanding of
Rust futures, we start to see the value. Whenever a future yields, the next
future can be polled. This means that we can run multiple futures on a single
thread! That means concurrent execution without the overhead of context
switches, or a separate stack for each task!</p>

<h2 id="embassy">Embassy</h2>

<p><a href="https://embassy.dev/" target="_blank">Embassy</a> is probably the most popular async executor for
embedded systems. It provides a full suite of synchronization primitives and a
very small footprint. Additionally, it provides fully implemented HALs for many
popular microcontrollers.</p>

<p>So far we’ve done a lot of gabbing about how async executors work, but we
haven’t written any code! Let’s change that by looking at a simple
blinky example using Embassy.</p>

<blockquote>
  <p><strong>Note:</strong> The examples below are written for the
<a href="https://www.st.com/en/evaluation-tools/b-u585i-iot02a.html#overview" target="_blank">STM32U5 Discovery Kit</a>
If you want to look at the full code, check out the
<a href="https://github.com/memfault/interrupt/tree/master/example/embedded-rust-async" target="_blank">Interrupt Github Repo</a>.</p>
</blockquote>

<h3 id="blinky-example">Blinky Example</h3>

<div><pre><code><span>#[embassy_executor::main]</span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>spawner</span><span>:</span> <span>Spawner</span><span>)</span> <span>{</span>
    <span>let</span> <span>p</span> <span>=</span> <span>embassy_stm32</span><span>::</span><span>init</span><span>(</span><span>Default</span><span>::</span><span>default</span><span>());</span>
    <span>let</span> <span>led</span> <span>=</span> <span>Output</span><span>::</span><span>new</span><span>(</span><span>p</span><span>.PH7</span><span>,</span> <span>Level</span><span>::</span><span>Low</span><span>,</span> <span>Speed</span><span>::</span><span>Medium</span><span>);</span>

    <span>spawner</span><span>.spawn</span><span>(</span><span>blinky</span><span>(</span><span>led</span><span>))</span><span>.unwrap</span><span>();</span>
<span>}</span>

<span>#[embassy_executor::task]</span>
<span>async</span> <span>fn</span> <span>blinky</span><span>(</span><span>mut</span> <span>led</span><span>:</span> <span>Output</span><span>&lt;</span><span>'static</span><span>,</span> <span>PH7</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>!</span> <span>{</span>
    <span>const</span> <span>LED_PERIOD_MS</span><span>:</span> <span>u64</span> <span>=</span> <span>200</span><span>;</span>

    <span>let</span> <span>mut</span> <span>ticker</span> <span>=</span> <span>Ticker</span><span>::</span><span>every</span><span>(</span><span>Duration</span><span>::</span><span>from_millis</span><span>(</span><span>LED_PERIOD_MS</span><span>));</span>
    <span>loop</span> <span>{</span>
        <span>led</span><span>.toggle</span><span>();</span>
        <span>ticker</span><span>.next</span><span>()</span><span>.await</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>This example likely looks very familiar to anyone who has written a blinky
example before. We’re spawning a task that toggles an LED every 200ms. What
likely isn’t familiar is all this crazy <code>embassy_executor</code>, and <code>spawner</code> stuff.
Let’s break down this example piece by piece.</p>

<div><pre><code><span>#[embassy_executor::main]</span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>spawner</span><span>:</span> <span>Spawner</span><span>)</span> <span>{</span>
</code></pre></div>

<p>The <code>#[embassy_executor::main]</code> attribute macro has a few responsibilities<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">2</a></sup>.
First, it provides an
<a href="https://docs.rs/cortex-m-rt/latest/cortex_m_rt/attr.entry.html" target="_blank">entry point</a>
for our program. This is the function that will be called by the Cortex-M
reset handler on boot. If you want to learn more about this check out our
<a href="https://interrupt.memfault.com/blog/zero-to-main-1">Zero to Main: Bare Metal C</a>
article.</p>

<p>Secondly, it creates a new
<a href="https://docs.rs/embassy-executor/latest/embassy_executor/struct.Executor.html" target="_blank"><code>Executor</code></a>
for us. In our configuration, this executor  is expected to have a <code>'static</code>
lifetime, meaning that it’s expected to live for the entire lifetime of the
program. This makes sense, as we need our executor to live at least as long as
all of our tasks.</p>

<p>Lastly, it spawns a task that will run our <code>main</code> function and passes a copy
of the internal
<a href="https://docs.rs/embassy-executor/latest/embassy_executor/struct.Spawner.html" target="_blank"><code>Spawner</code></a>
handle for our executor. This handle allows us to spawn new tasks and send them
over to our executor. The <code>Spawner</code> implements
<a href="https://doc.rust-lang.org/core/marker/trait.Copy.html" target="_blank"><code>Copy</code></a>, so it can be
cheaply passed around to different tasks to allow them to spawn new tasks.</p>

<div><pre><code><span>let</span> <span>p</span> <span>=</span> <span>embassy_stm32</span><span>::</span><span>init</span><span>(</span><span>Default</span><span>::</span><span>default</span><span>());</span>
<span>let</span> <span>led</span> <span>=</span> <span>Output</span><span>::</span><span>new</span><span>(</span><span>p</span><span>.PH7</span><span>,</span> <span>Level</span><span>::</span><span>Low</span><span>,</span> <span>Speed</span><span>::</span><span>Medium</span><span>);</span>
</code></pre></div>

<p>This section sets up the LED that we’ll be blinking. Since we’re using an
stm32 dev kit, we’re using the
<a href="https://docs.embassy.dev/embassy-stm32/git/stm32c011d6/index.html" target="_blank"><code>embassy_stm32</code></a>
crate.  The call to <code>embassy_stm32::init</code> is responsible for setting up all of
our peripherals. The <code>Output</code> struct provides us with a safe interface for
interacting with the GPIO that our LED is connected to PH7<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">3</a></sup>.</p>

<div><pre><code><span>spawner</span><span>.spawn</span><span>(</span><span>blinky</span><span>(</span><span>led</span><span>))</span><span>.unwrap</span><span>();</span>
</code></pre></div>

<p>The final bit of our <code>main</code> function is to spawn our <code>blinky</code> task. This adds
the task to the executor’s task queue, and allows it to be polled when it has
work to do.</p>

<div><pre><code><span>#[embassy_executor::task]</span>
<span>async</span> <span>fn</span> <span>blinky</span><span>(</span><span>mut</span> <span>led</span><span>:</span> <span>Output</span><span>&lt;</span><span>'static</span><span>,</span> <span>PH7</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>!</span> <span>{</span>
</code></pre></div>

<p>Now we’re on to the fun part! The <code>#[embassy_executor::task]</code> attribute macro
is responsible for turning our <code>blinky</code> function into a task. The underpinnings
of this macro are a bit beyond the scope of this article, but if you’re
interested you can check out the
<a href="https://docs.rs/embassy-macro/latest/embassy_macro/" target="_blank"><code>embassy-macro</code></a> crate.
For now, we can just think of this macro as a special wrapper that turns the
future generated by our <code>blinky</code> function into a task that can be polled by the
embassy executor.</p>

<div><pre><code><span>const</span> <span>LED_PERIOD_MS</span><span>:</span> <span>u64</span> <span>=</span> <span>200</span><span>;</span>

<span>let</span> <span>mut</span> <span>ticker</span> <span>=</span> <span>Ticker</span><span>::</span><span>every</span><span>(</span><span>Duration</span><span>::</span><span>from_millis</span><span>(</span><span>LED_PERIOD_MS</span><span>));</span>
<span>loop</span> <span>{</span>
    <span>ticker</span><span>.next</span><span>()</span><span>.await</span><span>;</span>
    <span>led</span><span>.toggle</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>We’re almost done! The rest of our <code>blinky</code> function is just a simple loop that
toggles our LED every 200ms. The <code>Ticker</code> struct is provided by the
<a href="https://docs.rs/embassy/latest/embassy/time/index.html" target="_blank"><code>embassy::time</code></a> crate,
and provides a simple interface for waiting for a given duration. The
<a href="https://docs.rs/embassy/latest/embassy/time/struct.Ticker.html#method.next" target="_blank"><code>next</code></a>
method returns a future that will resolve once the given duration has elapsed.
I’m sure you’ve guessed what the last bit does, but just in case, it toggles
the GPIO pin that our LED is connected to.</p>

<p>This is a pretty simple example, but it demonstrates the basics of how to write
an async program using Embassy. Let’s modify the example to be a bit more
complex, and see how we can extract some more interesting value.</p>

<h3 id="button-example">Button Example</h3>

<p>Our button example will be largely the same as our blinky example, but we’ll
add a button to the mix. When the button is pressed we’ll toggle the blinking
of the LED. Let’s take a look at the code.</p>

<div><pre><code><span>static</span> <span>LED_BLINK_ACTIVE</span><span>:</span> <span>AtomicBool</span> <span>=</span> <span>AtomicBool</span><span>::</span><span>new</span><span>(</span><span>true</span><span>);</span>

<span>#[embassy_executor::main]</span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>spawner</span><span>:</span> <span>Spawner</span><span>)</span> <span>{</span>
    <span>let</span> <span>p</span> <span>=</span> <span>embassy_stm32</span><span>::</span><span>init</span><span>(</span><span>Default</span><span>::</span><span>default</span><span>());</span>

    <span>let</span> <span>led</span> <span>=</span> <span>Output</span><span>::</span><span>new</span><span>(</span><span>p</span><span>.PH7</span><span>,</span> <span>Level</span><span>::</span><span>High</span><span>,</span> <span>Speed</span><span>::</span><span>Medium</span><span>);</span>
    <span>let</span> <span>button</span> <span>=</span> <span>Input</span><span>::</span><span>new</span><span>(</span><span>p</span><span>.PC13</span><span>,</span> <span>Pull</span><span>::</span><span>Down</span><span>);</span>
    <span>let</span> <span>button</span> <span>=</span> <span>ExtiInput</span><span>::</span><span>new</span><span>(</span><span>button</span><span>,</span> <span>p</span><span>.EXTI13</span><span>);</span>

    <span>spawner</span><span>.spawn</span><span>(</span><span>buttony</span><span>(</span><span>button</span><span>))</span><span>.unwrap</span><span>();</span>
    <span>spawner</span><span>.spawn</span><span>(</span><span>blinky</span><span>(</span><span>led</span><span>))</span><span>.unwrap</span><span>();</span>
<span>}</span>

<span>#[embassy_executor::task]</span>
<span>async</span> <span>fn</span> <span>blinky</span><span>(</span><span>mut</span> <span>led</span><span>:</span> <span>Output</span><span>&lt;</span><span>'static</span><span>,</span> <span>PH7</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>!</span> <span>{</span>
    <span>const</span> <span>LED_PERIOD_MS</span><span>:</span> <span>u64</span> <span>=</span> <span>200</span><span>;</span>

    <span>let</span> <span>mut</span> <span>ticker</span> <span>=</span> <span>Ticker</span><span>::</span><span>every</span><span>(</span><span>Duration</span><span>::</span><span>from_millis</span><span>(</span><span>LED_PERIOD_MS</span><span>));</span>
    <span>loop</span> <span>{</span>
        <span>ticker</span><span>.next</span><span>()</span><span>.await</span><span>;</span>
        <span>if</span> <span>LED_BLINK_ACTIVE</span><span>.load</span><span>(</span><span>Ordering</span><span>::</span><span>Relaxed</span><span>)</span> <span>{</span>
            <span>led</span><span>.toggle</span><span>();</span>
        <span>}</span>
    <span>}</span>
<span>}</span>

<span>#[embassy_executor::task]</span>
<span>async</span> <span>fn</span> <span>buttony</span><span>(</span><span>mut</span> <span>button</span><span>:</span> <span>ExtiInput</span><span>&lt;</span><span>'static</span><span>,</span> <span>PC13</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>loop</span> <span>{</span>
        <span>button</span><span>.wait_for_rising_edge</span><span>()</span><span>.await</span><span>;</span>

        <span>LED_BLINK_ACTIVE</span><span>.fetch_xor</span><span>(</span><span>true</span><span>,</span> <span>Ordering</span><span>::</span><span>SeqCst</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>The first thing you’ll notice is that we’ve added a new static variable,
<code>LED_BLINK_ACTIVE</code>. It is an
<a href="https://doc.rust-lang.org/core/sync/atomic/struct.AtomicBool.html" target="_blank"><code>AtomicBool</code></a>
which allows us to safely share it between tasks. We’ll use this variable to
track whether or not the LED should be blinking.</p>

<div><pre><code><span>let</span> <span>button</span> <span>=</span> <span>Input</span><span>::</span><span>new</span><span>(</span><span>p</span><span>.PC13</span><span>,</span> <span>Pull</span><span>::</span><span>Down</span><span>);</span>
<span>let</span> <span>button</span> <span>=</span> <span>ExtiInput</span><span>::</span><span>new</span><span>(</span><span>button</span><span>,</span> <span>p</span><span>.EXTI13</span><span>);</span>
</code></pre></div>

<p>In this next section of the new code, we’re setting up our button. The first
line sets up our <code>PC13</code> GPIO as an input. The second line assigns the GPIO to an
interrupt in the <code>EXTI</code><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">4</a></sup> peripheral of the STM32U5. This sets up the GPIO as
an interrupt source and allows us to wait for the button to be pressed without
having to poll the GPIO.</p>

<div><pre><code><span>if</span> <span>LED_BLINK_ACTIVE</span><span>.load</span><span>(</span><span>Ordering</span><span>::</span><span>Relaxed</span><span>)</span> <span>{</span>
    <span>led</span><span>.toggle</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>Next, we have a small change in our <code>blinky</code> task. We’ve added a check to see if
the LED should be blinking. If it should, we toggle the LED. If not, we do
nothing. This is a simple way to pause the blinking of the LED when we press the
button.</p>

<div><pre><code><span>#[embassy_executor::task]</span>
<span>async</span> <span>fn</span> <span>buttony</span><span>(</span><span>mut</span> <span>button</span><span>:</span> <span>ExtiInput</span><span>&lt;</span><span>'static</span><span>,</span> <span>PC13</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>loop</span> <span>{</span>
        <span>button</span><span>.wait_for_rising_edge</span><span>()</span><span>.await</span><span>;</span>

        <span>LED_BLINK_ACTIVE</span><span>.fetch_xor</span><span>(</span><span>true</span><span>,</span> <span>Ordering</span><span>::</span><span>SeqCst</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>Our last bit of code is the meat of our new addition. The aptly named <code>buttony</code>
task. This task is responsible for waiting for the button to be pressed, and
then toggling the <code>LED_BLINK_ACTIVE</code>
<a href="https://doc.rust-lang.org/core/sync/atomic/struct.AtomicBool.html" target="_blank"><code>AtomicBool</code></a>
. Let’s dive deep into the <code>wait_for_rising_edge</code> method to get a good
understanding of how it works, and tie together our understanding of Async Rust
on embedded.</p>

<h4 id="asynchronous-interrupts-in-embassy">Asynchronous Interrupts in Embassy</h4>

<blockquote>
  <p>Note: We’re going to be diving into the source code of Embassy here. At the
time of writing Embassy is still in development, and the code outlined here
may change. Even if this implementation changes, the core concepts of how an
asynchronous interrupt would be implemented will remain the same.</p>
</blockquote>

<p>Let’s dive into the source code of the
<a href="https://github.com/embassy-rs/embassy/blob/c7ec45a004750f590c1d9ea4a721972efe133b8e/embassy-stm32/src/exti.rs#L117" target="_blank"><code>wait_for_rising_edge</code></a>
method.</p>

<div><pre><code><span>pub</span> <span>async</span> <span>fn</span> <span>wait_for_rising_edge</span><span>&lt;</span><span>'a</span><span>&gt;</span><span>(</span><span>&amp;</span><span>'a</span> <span>mut</span> <span>self</span><span>)</span> <span>{</span>
    <span>ExtiInputFuture</span><span>::</span><span>new</span><span>(</span><span>self</span><span>.pin.pin.pin</span><span>.pin</span><span>(),</span> <span>self</span><span>.pin.pin.pin</span><span>.port</span><span>(),</span> <span>true</span><span>,</span> <span>false</span><span>)</span><span>.await</span>
<span>}</span>
</code></pre></div>

<p>That’s a lot of pins! All of these calls to pin are a result of a few layers of
abstraction here that we won’t touch on. The important thing to note is that
this is the pin we initially assigned to the <code>EXTI</code> interrupt. In our case this
<code>PC13</code> and its associated port. The other arguments are the edge we’re waiting
on. One other note is that the <code>pin</code>s here are GPIO pins and not the Rust
future <code>Pin</code> type we discussed earlier.</p>

<p>It looks like the core of this function is in another castle! Let’s take a look
at the
<a href="https://github.com/embassy-rs/embassy/blob/c7ec45a004750f590c1d9ea4a721972efe133b8e/embassy-stm32/src/exti.rs#L202" target="_blank"><code>ExtiInputFuture</code></a>
struct, and see if we can learn anything interesting.</p>

<div><pre><code><span>struct</span> <span>ExtiInputFuture</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>pin</span><span>:</span> <span>u8</span><span>,</span>
    <span>phantom</span><span>:</span> <span>PhantomData</span><span>&lt;&amp;</span><span>'a</span> <span>mut</span> <span>AnyPin</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>ExtiInputFuture</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>fn</span> <span>new</span><span>(</span><span>pin</span><span>:</span> <span>u8</span><span>,</span> <span>port</span><span>:</span> <span>u8</span><span>,</span> <span>rising</span><span>:</span> <span>bool</span><span>,</span> <span>falling</span><span>:</span> <span>bool</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>critical_section</span><span>::</span><span>with</span><span>(|</span><span>_</span><span>|</span> <span>{</span>
            <span>let</span> <span>pin</span> <span>=</span> <span>pin</span> <span>as</span> <span>usize</span><span>;</span>
            <span>exticr_regs</span><span>()</span><span>.exticr</span><span>(</span><span>pin</span> <span>/</span> <span>4</span><span>)</span><span>.modify</span><span>(|</span><span>w</span><span>|</span> <span>w</span><span>.set_exti</span><span>(</span><span>pin</span> <span>%</span> <span>4</span><span>,</span> <span>port</span><span>));</span>
            <span>EXTI</span><span>.rtsr</span><span>(</span><span>0</span><span>)</span><span>.modify</span><span>(|</span><span>w</span><span>|</span> <span>w</span><span>.set_line</span><span>(</span><span>pin</span><span>,</span> <span>rising</span><span>));</span>
            <span>EXTI</span><span>.ftsr</span><span>(</span><span>0</span><span>)</span><span>.modify</span><span>(|</span><span>w</span><span>|</span> <span>w</span><span>.set_line</span><span>(</span><span>pin</span><span>,</span> <span>falling</span><span>));</span>

            <span>// clear pending bit</span>
            <span>#[cfg(not(any(exti_c0,</span> <span>exti_g0,</span> <span>exti_l5,</span> <span>exti_u5,</span> <span>exti_h5,</span> <span>exti_h50)))]</span>
            <span>EXTI</span><span>.pr</span><span>(</span><span>0</span><span>)</span><span>.write</span><span>(|</span><span>w</span><span>|</span> <span>w</span><span>.set_line</span><span>(</span><span>pin</span><span>,</span> <span>true</span><span>));</span>
            <span>#[cfg(any(exti_c0,</span> <span>exti_g0,</span> <span>exti_l5,</span> <span>exti_u5,</span> <span>exti_h5,</span> <span>exti_h50))]</span>
            <span>{</span>
                <span>EXTI</span><span>.rpr</span><span>(</span><span>0</span><span>)</span><span>.write</span><span>(|</span><span>w</span><span>|</span> <span>w</span><span>.set_line</span><span>(</span><span>pin</span><span>,</span> <span>true</span><span>));</span>
                <span>EXTI</span><span>.fpr</span><span>(</span><span>0</span><span>)</span><span>.write</span><span>(|</span><span>w</span><span>|</span> <span>w</span><span>.set_line</span><span>(</span><span>pin</span><span>,</span> <span>true</span><span>));</span>
            <span>}</span>

            <span>cpu_regs</span><span>()</span><span>.imr</span><span>(</span><span>0</span><span>)</span><span>.modify</span><span>(|</span><span>w</span><span>|</span> <span>w</span><span>.set_line</span><span>(</span><span>pin</span><span>,</span> <span>true</span><span>));</span>
        <span>});</span>

        <span>Self</span> <span>{</span>
            <span>pin</span><span>,</span>
            <span>phantom</span><span>:</span> <span>PhantomData</span><span>,</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>The members of this struct are mostly what we’d expect. The <code>pin</code> member is
the GPIO attached to our button. The
<a href="https://doc.rust-lang.org/std/marker/struct.PhantomData.html" target="_blank"><code>PhantomData</code></a>
member is a bit of a distraction here. We won’t cover it in detail, but it’s a
way to preserve the lifetime needed for the future even though we don’t
use the lifetime anywhere.</p>

<p>The <code>new</code> method is where we set up our interrupt. We won’t cover all of the
individual bit manipulations here. If you want to get a complete understanding
of what’s going on, check out section 23 of the STM32U5 reference manual<sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" rel="footnote">4</a></sup>.
At a high level here’s what’s going on:</p>

<ul>
  <li>First, we enter a critical section. This ensures that we aren’t interrupted
  while we’re setting up our interrupt.</li>
  <li>Next, we set the <code>EXTI</code> interrupt to use the pin we’re waiting on.</li>
  <li>Then we set the edge we’re waiting on. In our case, we’re waiting for a rising
  edge.</li>
  <li>Then we clear the pending bits of any interrupts on this line</li>
  <li>Finally, we enable the interrupt to wake up the CPU when it’s triggered.</li>
</ul>

<p>This is all pretty standard peripheral configuration. What we’re
interested in is how this future is driven to completion. Let’s take a look at
the <a href="https://doc.rust-lang.org/core/future/trait.Future.html" target="_blank"><code>Future</code></a>
implementation for
<a href="https://github.com/embassy-rs/embassy/blob/c7ec45a004750f590c1d9ea4a721972efe133b8e/embassy-stm32/src/exti.rs#L243" target="_blank"><code>ExtiInputFuture</code></a>.</p>

<div><pre><code><span>impl</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>Future</span> <span>for</span> <span>ExtiInputFuture</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span>
    <span>type</span> <span>Output</span> <span>=</span> <span>();</span>

    <span>fn</span> <span>poll</span><span>(</span><span>self</span><span>:</span> <span>Pin</span><span>&lt;&amp;</span><span>mut</span> <span>Self</span><span>&gt;</span><span>,</span> <span>cx</span><span>:</span> <span>&amp;</span><span>mut</span> <span>Context</span><span>&lt;</span><span>'_</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Poll</span><span>&lt;</span><span>Self</span><span>::</span><span>Output</span><span>&gt;</span> <span>{</span>
        <span>EXTI_WAKERS</span><span>[</span><span>self</span><span>.pin</span> <span>as</span> <span>usize</span><span>]</span><span>.register</span><span>(</span><span>cx</span><span>.waker</span><span>());</span>

        <span>let</span> <span>imr</span> <span>=</span> <span>cpu_regs</span><span>()</span><span>.imr</span><span>(</span><span>0</span><span>)</span><span>.read</span><span>();</span>
        <span>if</span> <span>!</span><span>imr</span><span>.line</span><span>(</span><span>self</span><span>.pin</span> <span>as</span> <span>_</span><span>)</span> <span>{</span>
            <span>Poll</span><span>::</span><span>Ready</span><span>(())</span>
        <span>}</span> <span>else</span> <span>{</span>
            <span>Poll</span><span>::</span><span>Pending</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>It’s taken a while, but we’re finally looking at a Rust future implementation
for a peripheral! Given how much we’ve covered so far, this implementation
should be pretty straightforward. Let’s break it down.</p>

<div><pre><code><span>EXTI_WAKERS</span><span>[</span><span>self</span><span>.pin</span> <span>as</span> <span>usize</span><span>]</span><span>.register</span><span>(</span><span>cx</span><span>.waker</span><span>());</span>
</code></pre></div>

<p>This line is the key to how this future is driven to completion. Here we copy
the waker associated with the future to a static array of wakers that each map
to a specific interrupt line. We’ll come back to this in a moment when we look
at the actual interrupt handler.</p>

<div><pre><code><span>let</span> <span>imr</span> <span>=</span> <span>cpu_regs</span><span>()</span><span>.imr</span><span>(</span><span>0</span><span>)</span><span>.read</span><span>();</span>
<span>if</span> <span>!</span><span>imr</span><span>.line</span><span>(</span><span>self</span><span>.pin</span> <span>as</span> <span>_</span><span>)</span> <span>{</span>
    <span>Poll</span><span>::</span><span>Ready</span><span>(())</span>
<span>}</span> <span>else</span> <span>{</span>
    <span>Poll</span><span>::</span><span>Pending</span>
<span>}</span>
</code></pre></div>

<p>Here we check if the interrupt has fired, and return <code>Poll::Ready</code> if it has. If
it hasn’t fired yet, we return <code>Poll::Pending</code>. This is the pattern we
established earlier for futures. If there is still work to be done, in the case
of this future waiting for an interrupt, we return <code>Poll::Pending</code>.</p>

<div><pre><code><span>pub</span> <span>unsafe</span> <span>fn</span> <span>on_irq</span><span>()</span> <span>{</span>
    <span>#[cfg(not(any(exti_c0,</span> <span>exti_g0,</span> <span>exti_l5,</span> <span>exti_u5,</span> <span>exti_h5,</span> <span>exti_h50)))]</span>
    <span>let</span> <span>bits</span> <span>=</span> <span>EXTI</span><span>.pr</span><span>(</span><span>0</span><span>)</span><span>.read</span><span>()</span><span>.0</span><span>;</span>
    <span>#[cfg(any(exti_c0,</span> <span>exti_g0,</span> <span>exti_l5,</span> <span>exti_u5,</span> <span>exti_h5,</span> <span>exti_h50))]</span>
    <span>let</span> <span>bits</span> <span>=</span> <span>EXTI</span><span>.rpr</span><span>(</span><span>0</span><span>)</span><span>.read</span><span>()</span><span>.0</span> <span>|</span> <span>EXTI</span><span>.fpr</span><span>(</span><span>0</span><span>)</span><span>.read</span><span>()</span><span>.0</span><span>;</span>

    <span>// Mask all the channels that fired.</span>
    <span>cpu_regs</span><span>()</span><span>.imr</span><span>(</span><span>0</span><span>)</span><span>.modify</span><span>(|</span><span>w</span><span>|</span> <span>w</span><span>.0</span> <span>&amp;=</span> <span>!</span><span>bits</span><span>);</span>

    <span>// Wake the tasks</span>
    <span>for</span> <span>pin</span> <span>in</span> <span>BitIter</span><span>(</span><span>bits</span><span>)</span> <span>{</span>
        <span>EXTI_WAKERS</span><span>[</span><span>pin</span> <span>as</span> <span>usize</span><span>]</span><span>.wake</span><span>();</span>
    <span>}</span>

    <span>// Clear pending</span>
    <span>#[cfg(not(any(exti_c0,</span> <span>exti_g0,</span> <span>exti_l5,</span> <span>exti_u5,</span> <span>exti_h5,</span> <span>exti_h50)))]</span>
    <span>EXTI</span><span>.pr</span><span>(</span><span>0</span><span>)</span><span>.write_value</span><span>(</span><span>Lines</span><span>(</span><span>bits</span><span>));</span>
    <span>#[cfg(any(exti_c0,</span> <span>exti_g0,</span> <span>exti_l5,</span> <span>exti_u5,</span> <span>exti_h5,</span> <span>exti_h50))]</span>
    <span>{</span>
        <span>EXTI</span><span>.rpr</span><span>(</span><span>0</span><span>)</span><span>.write_value</span><span>(</span><span>Lines</span><span>(</span><span>bits</span><span>));</span>
        <span>EXTI</span><span>.fpr</span><span>(</span><span>0</span><span>)</span><span>.write_value</span><span>(</span><span>Lines</span><span>(</span><span>bits</span><span>));</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>The last piece of the puzzle is the interrupt handler. Again we won’t cover all
of the register configurations <sup id="fnref:5:2" role="doc-noteref"><a href="#fn:5" rel="footnote">4</a></sup> here. The important part is right in the
middle where we call <code>wake</code> on the waker associated with the interrupt line that
fired.</p>

<div><pre><code><span>for</span> <span>pin</span> <span>in</span> <span>BitIter</span><span>(</span><span>bits</span><span>)</span> <span>{</span>
    <span>EXTI_WAKERS</span><span>[</span><span>pin</span> <span>as</span> <span>usize</span><span>]</span><span>.wake</span><span>();</span>
<span>}</span>
</code></pre></div>

<p>This is the key to how our future is driven to completion. When any interrupt
registered fires, we’ll call the associated waker. Under the hood, this will
throw our <code>buttony</code> task back onto the executor’s task queue, and it will be
polled when it gets its next turn.</p>

<h2 id="benefits-of-async-rust-on-embedded-systems">Benefits of Async Rust on Embedded Systems</h2>

<p>One of the biggest benefits of async Rust is a much smaller RAM footprint. A
traditional RTOS will require a stack for each task. Rust futures only need to
keep track of variables that are used across await points. This means that the
memory needed for an async task can be much smaller than a traditional RTOS
task, as small as 10s of bytes for less complex tasks! This means we can spin up
more tasks that have smaller discrete jobs, instead of overloading single tasks
with multiple responsibilities.</p>

<p>Another benefit is the lack of context switching between tasks. Since we’re
operating in a cooperative scheduling environment, we don’t need to save and
restore the state of a task when it is pending. This means we can spend more
time doing useful work, and less time context-switching!</p>

<h2 id="downsides-of-async-rust-on-embedded-systems">Downsides of Async Rust on Embedded Systems</h2>

<p>There’s no such thing as a free lunch, and async Rust is no exception. There are
some downsides to using async Rust on embedded systems.</p>

<p>While async Rust feels ergonomic, there is still a lot of complexity under the
hood. This can make it difficult to debug issues when they arise. An overloaded
task that is starving others in the runtime can be difficult to diagnose. This
isn’t helped by the fact that debug symbols can get a little weird when using
async Rust. This is because stack unwinding isn’t quite what you’d
expect when looking because of the way futures are polled and scheduled.</p>

<p>Another downside is the lack of priorities introduced by the cooperative
scheduler. To its credit, Embassy has a pretty cool solution to this problem.
Using the priorities assigned to interrupts, an executor is run in each of these
interrupts<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">5</a></sup>. This allows for a form of priority-based scheduling, but it’s a
decent amount of added complexity compared to a traditional preemptive RTOS.</p>

<p>Additionally the ability to spawn more tasks than you would with a traditional
RTOS can be a double-edged sword. While it’s great to be able to spin up more
tasks, it can be easy to go overboard and create too many tasks. This can lead
to an overloaded task queue, with tasks spending more time waiting to be polled.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Async Rust is a powerful tool for embedded systems. It allows us to write
concurrent code that is easy to reason about, and has a small memory footprint.
It’s not without its downsides, but the benefits far outweigh the costs in my
opinion. I hope this article has helped you understand how async Rust works
under the hood, and how it can be used to write concurrent code on embedded
systems.</p>

<p>We’ve only scratched the surface of what’s possible with async Rust. I hope this
article serves as a jumping-off point for you to explore the world of async Rust
on embedded systems. I hope you have fun writing concurrent code in this
paradigm!</p>

<!-- Interrupt Keep START -->
<p>Like Interrupt? <a href="https://go.memfault.com/interrupt-subscribe" target="_blank">Subscribe</a> to get our latest posts straight to your mailbox.</p>

<p>See anything you'd like to change? Submit a pull request or open an issue at <a href="https://github.com/memfault/interrupt" target="_blank">GitHub</a></p>

<!-- Interrupt Keep END -->

<h2 id="further-reading">Further reading</h2>

<ul>
  <li>
<a href="https://github.com/cbiffle/lilos" target="_blank">lilos</a>: A lightweight async executor for
cortex-m devices.</li>
  <li>
<a href="https://tokio.rs/blog/2019-10-scheduler" target="_blank">Making the Tokio scheduler 10x Faster</a>:
An interesting blog post about a rewrite of the Tokio scheduler.</li>
  <li>
<a href="https://rust-lang.github.io/async-book/" target="_blank">The Rust Async Book</a>: The official
documentation for async Rust.</li>
</ul>

<h2 id="references">References</h2>

<!-- prettier-ignore-start -->
<!-- prettier-ignore-end -->



    </div><p><img src="https://interrupt.memfault.com/img/author/blake.jpg">
            
            <span>
                <a href="https://interrupt.memfault.com/authors/blake">Blake Hildebrand</a> is an embedded software engineer at Memfault. Blake previously worked on embedded software teams at Garmin and Amazon Robotics.<br>
                
<span>
    
    
    <a href="https://www.linkedin.com/in/blake-a-hildebrand/" target="_blank"><svg><use xlink:href="/img/social-icons.svg#linkedin"></use></svg></a>
    
    
    <a href="https://github.com/bahildebrand" target="_blank"><svg><use xlink:href="/img/social-icons.svg#github"></use></svg></a>
    
</span>

            </span>

        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Remote Code Execution in OpenSSH’s forwarded SSH-agent (245 pts)]]></title>
            <link>https://blog.qualys.com/vulnerabilities-threat-research/2023/07/19/cve-2023-38408-remote-code-execution-in-opensshs-forwarded-ssh-agent</link>
            <guid>36790196</guid>
            <pubDate>Wed, 19 Jul 2023 17:36:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.qualys.com/vulnerabilities-threat-research/2023/07/19/cve-2023-38408-remote-code-execution-in-opensshs-forwarded-ssh-agent">https://blog.qualys.com/vulnerabilities-threat-research/2023/07/19/cve-2023-38408-remote-code-execution-in-opensshs-forwarded-ssh-agent</a>, See on <a href="https://news.ycombinator.com/item?id=36790196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">

	<main id="primary">

		
<article id="post-33427">
	

	<div>
			
<p>The Qualys Threat Research Unit (TRU) has discovered a remote code execution vulnerability in OpenSSH’s forwarded ssh-agent. This vulnerability allows a remote attacker to potentially execute arbitrary commands on vulnerable OpenSSH’s forwarded ssh-agent. Given the widespread use of OpenSSH’s forwarded ssh-agent Qualys Research Unit recommends that security teams apply patches for this vulnerability on priority. </p>



<h2 id="about-opensshs-agent-forwarding"><strong>About OpenSSH’s Agent Forwarding</strong>&nbsp;</h2>



<p>The ssh-agent is a background program that caches private keys for SSH public key authentication, reducing the need for regular passphrase input. Initiated at the start of an X or login session, it operates by storing keys in memory and unloading only when the process ends.&nbsp;</p>



<p>It’s instrumental in automation scripts or tasks requiring frequent server connections, as it prevents the need for insecure password storage or constant passphrase input. The connections to ssh-agent may be forwarded from further remote to avoid the need for authentication data to be stored on other machines. Nonetheless, it’s still crucial to secure the keys with robust passphrases.&nbsp;&nbsp;</p>







<h2 id="potential-impact-of-opensshs-agent-forwarding"><strong>Potential Impact of OpenSSH’s Agent Forwarding</strong>&nbsp;</h2>



<p>Successful exploitation of this vulnerability allows a remote attacker to execute arbitrary commands on vulnerable OpenSSH forwarded ssh-agent. &nbsp;Qualys security researchers have been able to independently verify the vulnerability, develop a PoC exploit on installations of Ubuntu Desktop 22.04 and 21.10. Other Linux distributions are likely vulnerable and probably exploitable.&nbsp;</p>



<p>As soon as the Qualys research team confirmed the vulnerability, Qualys engaged in responsible vulnerability disclosure and coordinated with the vendor, OpenSSH, on this occasion to announce the vulnerability.&nbsp;</p>







<h2 id="disclosure-timeline">Disclosure Timeline&nbsp;</h2>



<p>2023-07-06: Draft advisory and initial patch sent to OpenSSH.&nbsp;</p>



<p>2023-07-07: OpenSSH sent refined patches.&nbsp;</p>



<p>2023-07-09: Feedback on patches sent to OpenSSH.&nbsp;</p>



<p>2023-07-11: Received final patches from OpenSSH; feedback sent.&nbsp;</p>



<p>2023-07-14: OpenSSH plans for security-only release on July 19th.&nbsp;</p>



<p>2023-07-19: Coordinated release.&nbsp;</p>







<h2 id="technical-details"><strong>Technical Details</strong></h2>



<p>You can find the technical details of these vulnerabilities at:&nbsp;&nbsp;</p>



<p><a rel="noreferrer noopener" href="https://www.qualys.com/2023/07/19/cve-2023-38408/rce-openssh-forwarded-ssh-agent.txt" target="_blank">https://www.qualys.com/2023/07/19/cve-2023-38408/rce-openssh-forwarded-ssh-agent.txt</a></p>



<p><a href="https://www.qualys.com/2023/07/19/cve-2023-38408/rce-openssh-forwarded-ssh-agent.tar.gz">https://www.qualys.com/2023/07/19/cve-2023-38408/rce-openssh-forwarded-ssh-agent.tar.gz</a></p>







<h2 id="conclusion"><strong>Conclusion</strong>&nbsp;</h2>



<p>This newly uncovered ssh-agent vulnerability underlines the continuous need for rigorous security measures and immediate response. Even robust systems can harbor hidden vulnerabilities, as demonstrated by the shortcomings of the ssh-agent. Proactively rectifying such vulnerabilities through actions such as implementing patches is critical to maintaining the integrity of digital assets. &nbsp;</p>

		</div>

	
</article>


<!-- #comments -->

	</main><!-- #main -->


</div></div>]]></description>
        </item>
    </channel>
</rss>