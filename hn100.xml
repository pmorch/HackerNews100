<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 01 Jul 2024 22:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Pharma firms stash profits in Europe's tax havens (150 pts)]]></title>
            <link>https://www.investigate-europe.eu/posts/deadly-prices-pharma-firms-stash-profits-in-europes-tax-havens-as-patients-struggle-with-drug-prices</link>
            <guid>40848797</guid>
            <pubDate>Mon, 01 Jul 2024 18:29:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.investigate-europe.eu/posts/deadly-prices-pharma-firms-stash-profits-in-europes-tax-havens-as-patients-struggle-with-drug-prices">https://www.investigate-europe.eu/posts/deadly-prices-pharma-firms-stash-profits-in-europes-tax-havens-as-patients-struggle-with-drug-prices</a>, See on <a href="https://news.ycombinator.com/item?id=40848797">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>When doctors removed Miriam Staunton's tumour from her armpit six years ago, they told the 51-year-old Irish woman that she had a 70 per cent chance of relapse. Yet, in the months following the operation, she was only offered local radiation and regular check-ups, but no drug treatment.&nbsp;</span> <br><span></span> <br><span>"I remember when I met the oncologist and he said that he wasn't in a position to offer me anything systemic at that point," Staunton recalls. "I didn't really understand exactly what he meant by it at that time."</span> <br><span></span> <br><span>What Staunton did not realise is that she would have to wait for her melanoma to return one year later before she could be entitled to effective but expensive medicines. After the cancer had progressed to stage four in February 2019, she started a course of Opdivo combined with Yervoy, breakthrough drugs known as immunotherapy, which were then restricted to the most severe forms of cancer in Ireland due to their high costs.&nbsp;</span> <br><span></span> <br><span>In other parts of Europe, Staunton could have taken Opdivo alone shortly after her surgery. In July 2018, the European Medicines Agency (EMA) opened the therapy to stage three melanoma patients. France immediately reimbursed it, but Ireland did not. "It's one thing when there is no cure, but when the treatment exists and people can't access it, that's fundamentally wrong," says Staunton, who is now cancer-free. </span> <br></p><div><p><img srcset="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2Fedit0DSC00018.jpg&amp;w=3840&amp;q=75 1x" src="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2Fedit0DSC00018.jpg&amp;w=3840&amp;q=75" width="3160" height="3160" decoding="async" data-nimg="1" loading="lazy"></p><p><span>Miriam Staunton was unable at first to access the cancer drug she needed because it was not available in Ireland. </span><span>Maxence Peigné</span></p><hr></div><p><span>The reason for this delay is that Ireland and the American drugmaker, Bristol-Myers Squibb (BMS), could not agree on Opdivo's price. When the EMA approves new medicines for use in the EU, each member state has to strike reimbursement deals with producers individually. </span><a target="" href="https://www.investigate-europe.eu/posts/deadly-prices-medicine-dealers-europe-secret-drug-negotiations"><span>Negotiations can be lengthy, as companies often prioritise rich markets and governments seek confidential discounts.</span></a><span></span> <br><span></span> <br><span>Meanwhile, the pharmaceutical industry - like many other sectors - hoards eye-watering gains in tax havens. Investigate Europe can reveal that the 15 largest European and US drugmakers, including BMS, publicly disclose over 1,300 subsidiaries in tax havens and low-tax territories.&nbsp;</span> <br><span></span> <br><span>These jurisdictions offer corporations low taxes or ways to shift profits (sometimes both). In Europe, researchers and activists generally agree that they include Ireland, the Netherlands, Switzerland and Luxembourg. They are among the top five profit-shifting destinations globally, according to this year’s </span><a target="_blank" href="https://www.taxobservatory.eu//www-site/uploads/2023/10/global_tax_evasion_report_24.pdf"><span>EU Tax Observatory report</span></a><span>, an EU-funded think-tank.&nbsp;</span> <br><span></span> <br><span>The little-known structures in tax-friendly destinations have contributed to the 15 pharmaceutical firms amassing profits of €580 billion in the last five years.&nbsp;</span> <br><span></span> <br><span>This amount outweighs their research and development (R&amp;D) costs, despite the industry's frequent claim that high drug prices allow them to innovate and design new drugs. The returns are in keeping with the outsized profits that are synonymous with the wider sector. Some of the groups' Irish affiliates have racked up hundreds of billions of dollars and still rely on a version of the 'Double Irish' tax avoidance scheme, the analysis finds.</span> <br><span></span> <br><span>"Corporate tax avoidance is not victimless, fewer taxes mean less investment in healthcare in Ireland and also negative impacts for countries in the Global South," says Aideen Elliott of Oxfam Ireland. "Nothing these companies are doing is illegal. They are taking advantage of corporate tax rules."</span> <br></p><div><p><img srcset="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2FNEW-shutterstock_2401059949.jpg&amp;w=3840&amp;q=75 1x" src="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2FNEW-shutterstock_2401059949.jpg&amp;w=3840&amp;q=75" width="3160" height="3160" decoding="async" data-nimg="1" loading="lazy"></p><p><span>The $580 billion made by the 15 firms in the past five years outweighs what they collectively spent on research and development.</span><span>Shutterstock</span></p><hr></div><p><span>All corporations cited in this article were contacted for comments. AstraZeneca, Bayer, Eli Lilly, Novartis, Novo Nordisk, Roche and Sanofi specifically replied on tax issues to say that they comply with all rules. Sanofi insisted that its presence in low-tax jurisdictions was justified by local patients' needs. Bayer said that as a German company, it is taxed on its offshore profits, adding that some of the countries mentioned in this article should not be considered tax havens.</span> <br><span></span> <br><span>In Ireland, BMS entered negotiations with health authorities with a starting price of</span><a target="_blank" href="https://www.ncpe.ie/wp-content/uploads/2019/10/Technical-Summary-Document-nivolumb-for-adv-mel.pdf"><span> €1,311 for a 100 mg</span></a><span> dose of Opdivo. This is a stark contrast with academics' estimates that similar antibodies can be manufactured for between </span><a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8868023/"><span>$9.50 (€8.85) and $20 (€18.60) per 100 mg.</span></a><span>&nbsp;</span> <br><span></span> <br><span></span><a target="_blank" href="https://www.hse.ie/eng/services/list/5/cancer/profinfo/medonc/trc/trc%20minutes%2004%20nov%202019.pdf"><span>In November 2019</span></a><span>, the Irish healthcare system stressed the "substantial budget impact" of providing the drug for stage three cancer and noted that talks with the company were ongoing. Opdivo was finally reimbursed in February 2021, two and a half years after France. The final discount remains a trade secret.</span> <br></p><div><h4>“<!-- -->Corporate tax avoidance is not victimless, fewer taxes mean less investment in healthcare in Ireland and also negative impacts for countries in the Global South.<!-- -->”</h4><p>— <!-- -->Aideen Elliott, Oxfam Ireland</p></div><p><span>Ironically, BMS makes Opdivo in Dublin, at a facility close to Staunton's home. While the treatment wasn't accessible to some Irish patients due to its cost, the supplier was raking in sky-high profits thanks to Ireland's attractive tax rules.</span> <br><span></span> <br><span>BMS's sprawling state-of-the-art campus in the Irish capital belongs to a subsidiary that boasted a $17.2 billion turnover in 2022, more than a third of the manufacturer's global revenues that year. Yet despite being registered in Ireland, Swords Laboratories is a Swiss entity for tax purposes.</span> <br><span></span> <br><span>Its direct parent, Bristol-Myers Squibb Holdings Ireland, enjoys a similar double residency and owns patents for several BMS therapies. In 2022, the holding valued the assets at more than $1 billion and pocketed $4.5 billion in royalties linked to drugs produced by Swords Laboratories, such as Eliquis, a bestselling blood thinner. In addition, the holding received almost $9 billion in dividends from the Dublin plant in just two years. </span> <br></p><div><p><img srcset="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2Feditshutterstock_2394208173.jpg&amp;w=3840&amp;q=75 1x" src="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2Feditshutterstock_2394208173.jpg&amp;w=3840&amp;q=75" width="3160" height="3160" decoding="async" data-nimg="1" loading="lazy"></p><p><span>BMS has operations across the world. The firm turns over billions in revenue each year.</span><span>Shutterstock</span></p><hr></div><p><span>The arrangement resembles an infamous tax avoidance loophole that Ireland vowed to close. Dubbed "the Double Irish", it has been a common tool for tech and pharma groups to slash their effective tax bill below Ireland’s current 12.5 per cent corporate tax rate. The technique involved setting up two Irish companies: one for operational purposes and the other to hold intellectual property (IP). The first would pay royalties to the second, which would be a tax resident offshore, like in Bermuda.</span> <br><span></span> <br><span>"Ireland made changes to its corporate tax residence rules in Finance Act 2014 that are specifically designed to prevent such structures as the so-called 'Double Irish'," a Department of Finance spokesperson said. “These rules ensure that it is not possible for companies to exploit mismatches in tax residency rules.”</span> <br><span></span> <br><span>However, Dr James Stewart, adjunct professor in finance at Trinity College Dublin, says the structures can continue to exist because Ireland has a double taxation treaty with Switzerland.  "These firms have very large assets and flows of funds, generally have no employees and are very profitable. They are likely to be a source of profit extraction," he adds.</span> <br></p><div><h4>“<!-- -->Ireland made changes to its corporate tax residence rules in Finance Act 2014 that are specifically designed to prevent such structures as the so-called 'Double Irish'.<!-- -->”</h4><p>— <!-- -->Irish Department of Finance spokesperson</p></div><p><span>BMS Holdings Ireland's main direct shareholder is also an Irish outfit with Swiss tax residency. The two holdings and Swords Laboratories don't only funnel gains outside of Ireland, they also park them in their coffers. By the end of 2022, the trio had accumulated over $30 billion of equity.</span> <br><span></span> <br><span>Harbouring IP in tax havens is a common practice at BMS. Its patents on Opdivo and Yervoy sit in Delaware, an American state that levies no tax on royalties. The two drugs amounted to a quarter of the group's $45 billion revenue in 2023. That year, BMS listed 135 subsidiaries in tax havens: 81 in Delaware, 15 in Switzerland, 13 in Ireland and 12 in the Netherlands.&nbsp;</span> <br><span></span> <br><span>The structures helped the company reach an effective corporate tax rate of 4.7 per cent, far below the US statutory rate of 21 per cent. Part of it was due to a favourable tax ruling, but the largest reduction resulted from different fiscal treatments in Ireland, Switzerland and Puerto Rico, according to BMS's annual report.</span> <br><span></span> <br><span>The company did not reply to requests for comments.</span> <br></p><div><p><img srcset="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2Fedit0shutterstock_467065226.jpg&amp;w=3840&amp;q=75 1x" src="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2Fedit0shutterstock_467065226.jpg&amp;w=3840&amp;q=75" width="3160" height="3160" decoding="async" data-nimg="1" loading="lazy"></p><p><span>15 of the world’s biggest drugmakers operate more than 1,300 subsidiaries in tax-friendly jurisdictions, such as the US state of Delaware.</span><span>Shutterstock</span></p><hr></div><p><span>BMS is hardly a unique case. Investigate Europe analysed the last five years of accounts filed by the 15 largest American and European pharmaceutical groups. Together, they declared 1,300 subsidiaries in tax havens, as of 2023. The true number is likely higher, as reporting rules only force multinationals to list those undertakings they consider "significant".&nbsp;</span> <br><span></span> <br><span>Delaware took the top spot with 700 entities. The Netherlands came second with almost 170. Switzerland and Ireland were next, with nearly 120 each. Like BMS, US giant Merck established a network of Irish subsidiaries with Swiss tax residency which held at least $44 billion of equity as of 2022.&nbsp;</span> <br><span></span> <br><span>Not all drugmakers rely on a Double Irish scheme. According to Investigate Europe estimates, many of their affiliates had amassed considerable equity in Ireland by 2022's close: $308 billion for Abbvie, over $102 billion for Johnson &amp; Johnson, $20 billion for AstraZeneca and $17 billion for Gilead. </span> <br></p><p><span></span><a target="_blank" href="https://www.ipha.ie/about-us/contribution-to-the-irish-economy/"><span>Nine of the 10 biggest</span></a><span> pharma groups in the world have operations in Ireland and the largest is "likely to be Pfizer," suspects Prof Stewart. "I say likely because there are no published accounts available for any Irish subsidiary. Nearly all Pfizer subsidiaries in Ireland operate as a branch of a Dutch entity."</span> <br><span></span> <br><span>
In the Netherlands, Pfizer booked three-quarters of its $100 billion global revenues with a Dutch holding at the helm of a myriad of subsidiaries. CPPI CV, a limited partnership, is "fiscally transparent", meaning its shareholders can draw profits untaxed. In the two years to the end of 2023, CPPI sent $35 billion to its parent companies in Delaware. Follow the Money, an investigative outlet, published several articles on Pfizer's Dutch affairs and </span><a target="" href="https://www.ftm.eu/articles/corona-and-tax-hacks-make-pfizer-the-most-profitable-company-in-the-netherlands?share=TdgNXnIlBg4VBVoTgQjg7akJl44DxHddcBwHeQESo%2BwA9dS%2FaQcUKJFs6oRpMi8%3D"><span>described how the partnership</span></a><span> became the most profitable company in the Netherlands. Pfizer did not respond to requests for comment.</span> <br><span></span> <br><span>"American companies have historically hoarded cash in low-tax jurisdictions to avoid taxes they would normally pay if they repatriated profits to the US," explains Reuven Avi-Yonah, a law professor at the University of Michigan.&nbsp;"In 2018, a US reform sought to change this with a 10.5 per cent tax on foreign income, but it actually encouraged big pharma to keep even more profits offshore, as they would be subject to this attractive rate rather than the US statutory rate of 21 per cent."</span> <br></p><div><p><img srcset="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2FNEW-shutterstock_2424325787.jpg&amp;w=3840&amp;q=75 1x" src="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2FNEW-shutterstock_2424325787.jpg&amp;w=3840&amp;q=75" width="3160" height="3160" decoding="async" data-nimg="1" loading="lazy"></p><p><span> Pharmaceutical executives often cite expensive R&amp;D costs as a major reason why medicine prices are high.</span><span>Shutterstock</span></p><hr></div><p><span>"Everyone who has income wants to limit the tax exposure that they have from that income, companies are no exception," says Paul Fehlner, former head of IP at Novartis, a Swiss pharma behemoth. "So by putting ownership of patent rights into a low-tax jurisdiction and then flowing funds internally into a patent holding entity, you're able to reduce the overall tax burden."</span> <br><span>
</span> <br><span>Patents are filed by corporations or inventors on new products to prevent competition. In exchange for sharing their discovery with the public, patent holders are granted exclusive rights to manufacture and market the drug for a certain period, usually 20 years.</span> <br><span></span> <br><span>Generics are typically up to 85 per cent cheaper once rolled out, but as long as their monopolies last, drugmakers can impose high prices on governments and insurers. Pharmaceutical executives often cite </span><a target="_blank" href="https://abcnews.go.com/Business/big-pharma-ceos-grilled-capitol-hill-takeaways/story?id=107057364"><span>expensive R&amp;D costs</span></a><span> to justify this.&nbsp;</span> <br><span></span> <br><span>However, data compiled by Investigate Europe shows that the industry, when analysed collectively, reaps more profits from the sales of existing drugs, than it invests in developing new ones.&nbsp;</span> <br></p><div><p><iframe width="1920" height="1080" src="https://www.youtube.com/embed/Sqgx9NlgHg8?si=M_L2O7p9VkcSGxmW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p><br></div><p><span>Over the five years analysed, the 15 multinationals made €580 billion after tax, while dedicating €572 billion to R&amp;D. The gains were mostly allotted to shareholders in dividends and stock buybacks for a total €558 billion. </span> <br><span></span> <br><span>As a result, the following groups shelled out more on rewarding investors than on R&amp;D: Abbvie, Johnson &amp; Johnson, Novartis, BMS, Pfizer, Novo Nordisk and Amgen. Other firms, including AstraZeneca, Merck and Bayer, did invest more in R&amp;D than they made profits or paid shareholders.</span> <br><span></span> <br><span>Big pharma's fortune amassed in European tax havens contrasts with access inequality and struggling healthcare budgets locally. As much as Ireland lures drugmakers with its fiscal perks, Irish patients can often wait longer than their western European peers to get innovative drugs.&nbsp;</span> <br><span> </span> <br><span>"Pharma companies make it clear that bigger markets are more important to them and that they wouldn't want to give us a discount as a small member," says a former Irish health official speaking on condition of anonymity. "A lot of the companies take their own sweet time in even applying for market authorisation in Ireland. Some have sometimes literally told me that Ireland is so insignificant that their bosses don’t really care whether their drugs are here or not." </span> <br></p><p><span>The Irish Pharmaceutical Healthcare Association (IPHA), </span><a target="_blank" href="https://www.ipha.ie/wp-content/uploads/2023/12/Oireachtas-briefing-06-12-2023.pdf"><span>an industry lobby, estimates</span></a><span> that over two years pass on average between the start of a new drug assessment by the Irish medicines watchdog and its reimbursement approval.&nbsp;</span> <br><span></span> <br><span>In its 2024 budget, the Irish government announced that there would be no fresh funds for new drugs before it made a </span><a target="" href="https://www.independent.ie/irish-news/health/new-hope-for-thousands-of-patients-as-20m-found-for-breakthrough-medicines/a252600831.html"><span>U-turn and set aside €20 million</span></a><span> for innovative medicines.&nbsp;</span> <br><span></span> <br><span>In the tax-friendly Netherlands too, the picture is contrasted. </span><a target="_blank" href="https://english.rekenkamer.nl/publications/reports/2020/04/23/miracle-cure-or-sticking-plaster-2020-the-results-of-negotiations-on-the-prices-of-medicines"><span>State auditors</span></a><span> have suggested the government should negotiate bigger discounts to safeguard its budget, highlighting that not all therapies approved are cost-effective.</span> <br><span>
</span> <br><span>Dutch courts are poised to become a battleground between one drugmaker and its detractors. In 2023, the Pharmaceutical Accountability Foundation (PAF), a public interest group, filed a lawsuit against US firm Abbvie </span><a target="_blank" href="https://www.pharmaceuticalaccountability.org/2023/11/15/abbvie-tries-to-escape-accountability-for-overcharging-the-dutch-healthcare-system-by-1-2-billion-euros/#:~:text=AMSTERDAM%2C%20THE%20NETHERLANDS%3A%20In%20February,law%20and%20human%20rights%20principles."><span>for abuse of dominant position</span></a><span>. PAF alleges that the company made excessive profits of €1.2 billion over 14 years on its Dutch sales of Humira, the </span><a target="" href="https://www.npr.org/sections/health-shots/2023/01/31/1152513058/abbvies-blockbuster-drug-humira-finally-loses-its-20-year-200-billion-monopoly"><span>world's best-selling drug</span></a><span> treating an array of ills.</span> <br></p><div><p><img srcset="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2Fedit-shutterstock_1557949541.jpg&amp;w=3840&amp;q=75 1x" src="https://www.investigate-europe.eu/_next/image?url=https%3A%2F%2Fcontent.investigateeurope.com%2Fuploads%2Fedit-shutterstock_1557949541.jpg&amp;w=3840&amp;q=75" width="3160" height="3160" decoding="async" data-nimg="1" loading="lazy"></p><p><span>Humira is the world's best-selling drug.</span><span>Shutterstock</span></p><hr></div><p><span>"We hope that the judge's ruling will be a warning to pharmaceutical companies: you can ask any price you want, but if you really get out of line, you can be hit back and have to pay back," says Wilbert Bannenberg, PAF's chairperson.</span> <br><span></span> <br><span>"We reject the unfounded allegations of the Pharmaceutical Accountability Foundation, which, as indicated to the court, calls into question the pricing system for all medicines, potentially hindering future innovation," an AbbVie spokesperson said.</span> <br><span></span> <br><span>
Before critics emerged in the Netherlands, the company was already under acute scrutiny in its home country. In 2022, </span><a target="" href="https://www.finance.senate.gov/imo/media/doc/Pharma%20Tax%20Report.pdf"><span>a US Senate committee</span></a><span> found that Abbvie dodged billions of dollars of tax by keeping its intellectual property in Bermuda and manufacturing its products in Ireland and Puerto Rico. </span> <br><span></span> <br><span>The same year, I-Mak, an advocacy organisation, revealed that the group filed 94 per cent of its 166 American patents on Humira after the medicine was already on the market. The ruse </span><a target="" href="https://vermontbiz.com/news/2023/july/27/welch-calls-out-big-pharmas-patent-abuse-during-senate-hearing"><span>kept competitors at bay</span></a><span> and delayed cheaper generics.</span> <br></p><div><h4>“<!-- -->I don't think I hold [pharmaceutical companies] responsible. Do you hold the lion responsible for eating the zebra? No.<!-- -->”</h4><p>— <!-- -->Paul Fehlner</p></div><p><span>"They had all these patents covering all these variations, different dosages, even different sort of needle sizes on the pens that deliver the drug," says Tahir Amin, I-Mak's CEO. "All this built up to block-off competition because when you go to litigation, you pay millions of dollars to just remove one patent." The practice known as "evergreening", is criticised by I-Mak and others as a flaw in the patent system that allows corporations to prolong lucrative monopolies. </span> <br><span></span> <br><span>Fehlner, now the CEO of a biotech that repurposes existing medicines, is more nuanced: "There is a definite interest in companies in maintaining the profitability of drugs as long as possible. They're incentivised to do that."&nbsp;</span> <br><span></span> <br><span>For the former Novartis director, it is up to governments to impose conditions that curtail prices and support competition when signing contracts with pharma groups.&nbsp; "Should the companies themselves do certain things? I don't know, they're organised to maximise their profit," he says. "So I don't think I hold them responsible. Do you hold the lion responsible for eating the zebra? No."</span> <br><span>
</span> <br><em><span>Full company responses can be read </span></em><a target="_blank" href="https://content.investigateeurope.com/uploads/IE-company-responses-tax%20havens.pdf"><em><span>here</span></em></a><em><span>.&nbsp;</span></em> <br><span>
</span> <br><em><span>Contributor: Catrien Spijkerman</span></em> <br><em><span>Editor: Chris Matthews</span></em> <br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Supreme Court rules ex-presidents have immunity for official acts (602 pts)]]></title>
            <link>https://apnews.com/article/supreme-court-trump-capitol-riot-immunity-2dc0d1c2368d404adc0054151490f542</link>
            <guid>40847963</guid>
            <pubDate>Mon, 01 Jul 2024 17:19:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/supreme-court-trump-capitol-riot-immunity-2dc0d1c2368d404adc0054151490f542">https://apnews.com/article/supreme-court-trump-capitol-riot-immunity-2dc0d1c2368d404adc0054151490f542</a>, See on <a href="https://news.ycombinator.com/item?id=40847963">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        
<p>WASHINGTON (AP) — <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/us-supreme-court">The Supreme Court</a></span> on Monday ruled for the first time that former presidents have broad immunity from prosecution, extending the delay in the Washington criminal case against Donald Trump on charges he <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-indicted-jan-6-investigation-special-counsel-debb59bb7a4d9f93f7e2dace01feccdc">plotted to overturn his 2020 presidential election loss</a></span> and all but ending prospects the former president could be tried before the November election.</p><p>In <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.documentcloud.org/documents/24785411-trump-v-united-states" target="_blank" rel="noopener">a historic 6-3 ruling</a></span>, the court’s conservative majority, including the three justices appointed by Trump, narrowed the case against him and returned it to the trial court to determine what is left of special counsel Jack Smith’s indictment.</p><p>The court’s decision in a second major Trump case this term, along with its ruling <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/supreme-court-trump-insurrection-election-colorado-51e79c0f03013034c8a042cb278b6446">rejecting efforts to bar him from the ballot</a></span> because of his actions following the 2020 election, underscores the role the justices are playing in the November election. The court last week also limited an obstruction charge faced by Trump and used against hundreds of his supporters who stormed the Capitol on Jan. 6, 2021.</p>




    

<p>“Under our constitutional structure of separated powers, the nature of presidential power entitles a former president to absolute immunity from criminal prosecution for actions within his conclusive and preclusive constitutional authority,” Chief Justice John Roberts wrote for the court. “And he is entitled to at least presumptive immunity from prosecution for all his official acts. There is no immunity for unofficial acts.”</p>
    
<p>Roberts insisted that the president “is not above the law.” But in a fiery dissent for the court’s three liberals, Justice Sonia Sotomayor wrote, “In every use of official power, the President is now a king above the law.”</p><p>Reading from her opinion in the courtroom, Sotomayor said, “Because our Constitution does not shield a former president from answering for criminal and treasonous acts, I dissent.” Sotomayor said the decision “makes a mockery of the principle, foundational to our Constitution and system of government, that no man is above the law.”</p>
    

<p>The protection afforded presidents by the court, she said, “is just as bad as it sounds, and it is baseless.”</p><p>Trump posted in all capital letters on his social media network shortly after the decision was released: “BIG WIN FOR OUR CONSTITUTION AND DEMOCRACY. PROUD TO BE AN AMERICAN!”</p><p>Smith’s office declined to comment on the ruling.</p>
    
<p>President Joe Biden’s campaign said in a <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://x.com/BidenHQ/status/1807792569499230699" target="_blank" rel="noopener">statement</a></span> that the Supreme Court’s immunity ruling “doesn’t change the facts” about the events of Jan. 6.</p><p>Senate Majority Leader Chuck Schumer denounced the ruling as “a disgraceful decision,” made with the help of the three justices that Trump appointed.</p><p>“It undermines SCOTUS’s credibility and suggests political influence trumps all in our courts today,” the New York Democrat said on X.</p><p>The justices knocked out one aspect of the indictment. The opinion found Trump is “absolutely immune” from prosecution for alleged conduct involving discussions with the Justice Department.</p><p>Trump is also “at least presumptively immune” from allegations that he tried to pressure Vice President Mike Pence to reject certification of Democrat Joe Biden’s electoral vote win on Jan. 6, 2021. Prosecutors can try to make the case that Trump’s pressure on Pence still can be part of the case against him, Roberts wrote.</p>
    

<p>The court directed a fact-finding analysis on one of the more striking allegations in the indictment -- that Trump participated in a scheme to enlist fake electors in battleground states won by Biden who would falsely assert that Trump had won. Both sides had dramatically different interpretations as to whether that effort could be construed as official, and the conservative justices said determining which side is correct would require additional analysis at the trial court level.</p><p>Roberts’ opinion further restricted prosecutors by prohibiting them from using any official acts as evidence in trying to prove a president’s unofficial actions violated the law. One example not relevant to this case but which came up in arguments was the hypothetical payment of a bribe in return for an ambassadorial appointment.</p>
    

<p>Under Monday’s decision, a former president could be prosecuted for accepting a bribe, but prosecutors could not mention the official act, the appointment, in their case.</p><p>Justice Amy Coney Barrett, who joined the rest of Roberts’ opinion, parted company on this point. “The Constitution does not require blinding juries to the circumstances surrounding conduct for which Presidents can be held liable,” Barrett wrote. </p><p>The work of figuring out how to proceed will fall to U.S. District Judge <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-indictment-judge-tanya-chutkan-capitol-riot-9ba5c18d315697d759521425ea203012">Tanya Chutkan</a></span>, who would preside over Trump’s trial.</p><p>Trump still could face a trial, said Notre Dame law professor Derek Muller. “But the fact remains that it is almost impossible to happen before the election.”</p><p>David Becker, an election law expert and the executive director of the nonprofit Center for Election Innovation and Research, called the breadth of immunity granted to Trump “incredibly broad” and “deeply disturbing.”</p>
    

<p>“Almost anything that a president does with the executive branch is characterized as an official act,” he said on a call with reporters following the ruling. He said that “for any unscrupulous individual holding the seat of the Oval Office who might lose an election, the way I read this opinion is it could be a roadmap for them seeking to stay in power.”</p><p>The ruling was the last of the term, and it came more than two months after the court heard arguments, far slower than in other epic high court cases involving the presidency, including <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/john-dean-richard-nixon-government-and-politics-crime-c7a7b99cca7c685cfc239f5e08b53378">the Watergate tapes case</a></span>. </p><p>The Republican former president has denied doing anything wrong and has said this prosecution and three others are politically motivated to try to keep him from returning to the White House. </p><p>In May, Trump became the first former president to be <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-trial-deliberations-jury-testimony-verdict-85558c6d08efb434d05b694364470aa0">convicted of a felony</a></span>, in a New York court. He was found guilty of falsifying business records to cover up a hush money payment made during the 2016 presidential election to a porn actor who says she had sex with him, which he denies. He still faces <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-trial-hush-money-classified-documents-january-6-a0cb26da1959e2be04a4be2080eba0ad">three other indictments</a></span>.</p><p><span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/jack-smith-special-counsel-prosecutor-donald-trump-garland-e1fdb71cfc258bc2be48a8b890a9269b">Smith</a></span> is leading the two federal probes of the former president, both of which have led to criminal charges. The Washington case focuses on Trump’s alleged efforts to overturn the 2020 election after he lost to Biden. The case in Florida revolves around the mishandling of classified documents. A separate case, in Georgia, also turns on Trump’s actions after his defeat in 2020.</p><p>If Trump’s Washington trial does not take place before the 2024 election and he is not given another four years in the White House, he presumably would stand trial soon thereafter. </p><p>But if he wins, he could appoint an attorney general who would seek the dismissal of this case and the other federal prosecution he faces. He could also attempt to pardon himself if he reclaims the White House. He could not pardon himself for the conviction in state court in New York.</p><p>Justice Clarence Thomas wrote a separate opinion saying that he believed Smith’s appointment as special counsel was illegitimate. No other justice signed onto that opinion, but the question took center stage in recent arguments in the Florida case over classified documents.</p><p>The Supreme Court that heard the case included three justices appointed by Trump — Neil Gorsuch, Brett Kavanaugh and Barrett — and two justices who opted not to step aside after questions were raised about their impartiality.</p><p>Thomas’ wife, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/biden-politics-united-states-government-fraud-mark-meadows-c734294e2810f1240ea7ed1cbf675760">Ginni</a></span>, attended the rally near the White House where Trump spoke on Jan. 6, 2021, though she did not go the Capitol when <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/congress-confirm-joe-biden-78104aea082995bbd7412a6e6cd13818">a mob of Trump supporters</a></span> attacked it soon after. Following the 2020 election, she called the outcome a “heist” and exchanged messages with then-White House chief of staff Mark Meadows, urging him to stand firm with Trump as he falsely claimed that there was widespread election fraud.</p><p>Justice Samuel Alito said there was <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/alito-flag-supreme-court-roberts-justice-senate-e53c1d1fef7b81f9dc87fa31b1622c4c">no reason for him to step aside</a></span> from the cases following reports by The New York Times that said flags similar to those carried by the Jan. 6 rioters flew above his homes in Virginia and on the New Jersey shore. His wife, Martha-Ann Alito, was responsible for flying both the inverted American flag in January 2021 and the “Appeal to Heaven” banner in the summer of 2023, he said in letters to Democratic lawmakers responding to their recusal demands.</p><p>Trump’s trial had been scheduled to begin March 4, but that was before he sought court-sanctioned delays and a full review of the issue by the nation’s highest court. </p><p>Before the Supreme Court got involved, a trial judge and a three-judge appellate panel had ruled unanimously that Trump could be prosecuted for actions undertaken while in the White House and in the run-up to Jan. 6.</p><p>“For the purpose of this criminal case, former President Trump has become citizen Trump, with all of the defenses of any other criminal defendant,” the appeals court wrote in February. “But any executive immunity that may have protected him while he served as President no longer protects him against this prosecution.”</p><p>Chutkan <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-capitol-riot-immunity-donald-trump-a98872759762c95fa925ff831df27388">ruled against Trump’s immunity claim</a></span> in December. In her ruling, Chutkan said the office of the president “does not confer a lifelong ‘get-out-of-jail-free’ pass.”</p><h2>___</h2><p>Associated Press writers Lindsay Whitehurst, Alanna Durkin Richer, Eric Tucker, Stephen Groves, Farnoush Amiri, Michelle Price and Ali Swenson contributed to this report.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Doggo – A powerful, human-friendly DNS client for the command line (169 pts)]]></title>
            <link>https://doggo.mrkaran.dev/docs/</link>
            <guid>40847699</guid>
            <pubDate>Mon, 01 Jul 2024 16:56:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doggo.mrkaran.dev/docs/">https://doggo.mrkaran.dev/docs/</a>, See on <a href="https://news.ycombinator.com/item?id=40847699">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page"> <main data-pagefind-body="" lang="en" dir="ltr"> <div> <p><img src="https://doggo.mrkaran.dev/docs/_astro/doggo.CZjkYmFM_Z1MHm8S.webp" alt="" width="3240" height="1404" loading="lazy" decoding="async"></p>
<h2 id="features">Features</h2>
<ul>
<li>Human-readable output with color-coded and tabular format</li>
<li>JSON output support for easy scripting and parsing</li>
<li>Multiple transport protocols:
<ul>
<li>DNS over HTTPS (DoH)</li>
<li>DNS over TLS (DoT)</li>
<li>DNS over QUIC (DoQ)</li>
<li>DNS over TCP</li>
<li>DNS over UDP</li>
<li>DNSCrypt</li>
</ul>
</li>
<li>Support for <code dir="auto">ndots</code> and <code dir="auto">search</code> configurations from <code dir="auto">resolv.conf</code> or command-line arguments</li>
<li>Multiple resolver support with customizable query strategies</li>
<li>IPv4 and IPv6 support</li>
<li>Web interface available at <a href="https://doggo.mrkaran.dev/">doggo.mrkaran.dev</a></li>
<li>Shell completions for <code dir="auto">zsh</code> and <code dir="auto">fish</code></li>
<li>Reverse DNS lookups</li>
<li>Flexible query options including various DNS flags (AA, AD, CD, DO, etc.)</li>
<li>Debug mode for troubleshooting</li>
<li>Response time measurement</li>
<li>Cross-platform support (Linux, macOS, Windows, FreeBSD, NetBSD)</li>
</ul> </div> </main> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Who is hiring? (July 2024) (286 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40846428</link>
            <guid>40846428</guid>
            <pubDate>Mon, 01 Jul 2024 15:01:49 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40846428">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="40846428">
      <td><span></span></td>      <td><center><a id="up_40846428" href="https://news.ycombinator.com/vote?id=40846428&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=40846428">Ask HN: Who is hiring? (July 2024)</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_40846428">133 points</span> by <a href="https://news.ycombinator.com/user?id=whoishiring">whoishiring</a> <span title="2024-07-01T15:01:49"><a href="https://news.ycombinator.com/item?id=40846428">2 hours ago</a></span> <span id="unv_40846428"></span> | <a href="https://news.ycombinator.com/hide?id=40846428&amp;goto=item%3Fid%3D40846428">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Who%20is%20hiring%3F%20(July%202024)&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=40846428&amp;auth=2de047edea4e41a6b502a516d54ea9b3df281fe1">favorite</a> | <a href="https://news.ycombinator.com/item?id=40846428">122&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Please state the location and include REMOTE, INTERNS and/or VISA
when that sort of candidate is welcome. When remote work is <i>not</i> an option,
include ONSITE.</p><p>Please only post if you personally are part of the hiring company—no
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.</p><p>Commenters: please don't reply to job posts to complain about
something. It's off topic here.</p><p>Readers: please only email if you are personally interested in the job.</p><p>Searchers: try <a href="https://hnresumetojobs.com/" rel="nofollow">https://hnresumetojobs.com</a>, <a href="https://hnhired.fly.dev/" rel="nofollow">https://hnhired.fly.dev</a>,
<a href="https://kennytilton.github.io/whoishiring/" rel="nofollow">https://kennytilton.github.io/whoishiring/</a>, <a href="https://hnjobs.emilburzo.com/" rel="nofollow">https://hnjobs.emilburzo.com</a>.</p><p>Don't miss these other fine threads:</p><p><i>Who wants to be hired?</i> <a href="https://news.ycombinator.com/item?id=40846426">https://news.ycombinator.com/item?id=40846426</a></p><p><i>Freelancer? Seeking freelancer?</i> <a href="https://news.ycombinator.com/item?id=40846427">https://news.ycombinator.com/item?id=40846427</a></p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table><table>
            <tbody><tr id="40847941"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847941" href="https://news.ycombinator.com/vote?id=40847941&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Datadog | Software Engineers | ONSITE (Boston, Lisbon, Madrid, NYC, Paris, Tel Aviv) | Full-time</p><p>Datadog is a monitoring, tracing, logs system, and more, for your infrastructure and services. We build our own tsdb, event store [1][2], distributed tracing tools, cutting edge visualizations, and more. We love shipping great experiences for customers just like us and are growing fast! We write a lot of Go, Java, Python, Typescript (with React), and a bit of other languages. We run on k8s, and are multi-region and multi-cloud.</p><p>We're looking for people who can build systems at scale as we process trillions of events per day. Let us know if that's you!</p><p><a href="https://dtdg.co/hnwhoshiring" rel="nofollow">https://dtdg.co/hnwhoshiring</a></p><p>[1] <a href="https://www.datadoghq.com/blog/engineering/introducing-husky" rel="nofollow">https://www.datadoghq.com/blog/engineering/introducing-husky</a></p><p>[2] <a href="https://www.datadoghq.com/blog/engineering/husky-deep-dive" rel="nofollow">https://www.datadoghq.com/blog/engineering/husky-deep-dive</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847931"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847931" href="https://news.ycombinator.com/vote?id=40847931&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Wormhole Labs | Multiple Positions | Remote (US ET work hours) | <a href="https://wormholelabs.xyz/" rel="nofollow">https://wormholelabs.xyz/</a> | $150-225K + token grant + discretionary performance bonus</p><p>Wormhole Labs is a software company that specializes in building open source blockchain technology. We contribute to the Wormhole cross-chain messaging protocol and other protocols interested in expanding to different ecosystems.</p><p>We are looking to hire a Senior Frontend Engineer and a Smart Contract Engineer.</p><p>The Senior Frontend Engineer would contribute to the Wormhole TS SDK (<a href="https://github.com/wormhole-foundation/wormhole-sdk-ts">https://github.com/wormhole-foundation/wormhole-sdk-ts</a>) and Wormhole Connect (<a href="https://github.com/wormhole-foundation/wormhole-connect">https://github.com/wormhole-foundation/wormhole-connect</a>).</p><p>The Smart Contract Engineer would contribute to smart contracts throughout Wormhole’s ecosystem and build reference implementations for new protocols.</p><p>Our team is &lt;20 and mostly engineers. We believe in high agency, low egos, and strong collaboration. We offer competitive liquid compensation with performance bonuses and unlimited PTO.</p><p>See job posts: <a href="https://job-boards.greenhouse.io/wormholelabs" rel="nofollow">https://job-boards.greenhouse.io/wormholelabs</a></p><p>Please apply on Greenhouse and send an email to tony at wormholelabs dot xyz with a copy of your resume and a brief overview of the most difficult technical problem you’ve recently solved. What made it difficult and how did you solve the problem?</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847957"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847957" href="https://news.ycombinator.com/vote?id=40847957&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Company: Writer (100 MM Series B Gen AI startup)</p><p>Writer is the full-stack generative AI platform for enterprises. Our integrated system of LLMs, graph-based RAG, AI guardrails, and development tools makes it easy for organizations to deploy AI apps and workflows that deliver impactful ROI. We're in Forbes top 50 AI start ups!</p><p>Roles: Technical Lead Manager (SF-based): <a href="https://bit.ly/44emhzU" rel="nofollow">https://bit.ly/44emhzU</a>
       AI Engineer (SF, NYC, London): <a href="https://bit.ly/4eZy4qX" rel="nofollow">https://bit.ly/4eZy4qX</a>
       Backend Scala Engineer: <a href="https://bit.ly/3VAI68H" rel="nofollow">https://bit.ly/3VAI68H</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847725"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847725" href="https://news.ycombinator.com/vote?id=40847725&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>MaFi Games | Senior SWE/game dev | Contract or full-time | $70-110k | Remote | C#</p><p>I’m the co-founder of MaFi Games – an indie studio behind the game Captain of Industry. We are a small but passionate team who gave up their jobs at Google/Nvidia to pursue building the best factory simulation game possible, and we need more hands!</p><p>We are looking for an experienced software engineer to grow the team and accelerate our progress. We strongly prefer candidates with a background in game development or with experience in desktop UI, 3D graphics, and performance optimizations.</p><p>Some reasons you’d enjoy working with us:</p><p>* A multicultural, collaborative, and innovative work environment where your voice is heard.</p><p>* Fully remote job with flexible working hours and vacation schedule.</p><p>* High quality C# code base, code reviews, tests.</p><p>* High work satisfaction, work on a popular video game with a wonderful community.</p><p>As an example of our technical work see <a href="https://www.captain-of-industry.com/post/cd-31" rel="nofollow">https://www.captain-of-industry.com/post/cd-31</a>.</p><p>If interested, please see the detailed info and requirements at <a href="https://www.captain-of-industry.com/jobs" rel="nofollow">https://www.captain-of-industry.com/jobs</a>, thanks!</p><p>Note that this is a fully remote job and we are happy to consider candidates from any country around the world!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847283"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847283" href="https://news.ycombinator.com/vote?id=40847283&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>AO Labs | Building an alternative to backpropagation | <a href="https://www.aolabs.ai/" rel="nofollow">https://www.aolabs.ai/</a> | Berkeley, CA + remote</p><p>AI systems struggle with edge cases and understanding local context despite increasing model sizes. From our research at UC Berkeley into the evolution of intelligence from simple organisms, we’ve discovered the missing link is continuous learning (deep learning is pre-trained by design). Models built with our framework learn through customizable parameters similar to animal instincts, allowing for AI grounded with built-in memory and reasoning. We're a community of 160+ developers and researchers building general intelligence from the bottom-up from places like Berkeley, NYU, Imperial College, and Google.</p><p>We're building way outside of the current paradigm and we're looking for collaborators at all levels --hackers, contributors, the curious-- as we'll be making our first hires soon. Email with "HN Hiring" in subject line to: ali at aolabs.ai or chat with us in our discord: <a href="https://discord.gg/Zg9bHPYss5" rel="nofollow">https://discord.gg/Zg9bHPYss5</a></p><p>This post is near identical to mine from last month; if you reached out then, please know that I'll respond to you soon (I've been busy wrapping up a fundraise).</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847894"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847894" href="https://news.ycombinator.com/vote?id=40847894&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>SerpApi | <a href="https://serpapi.com/" rel="nofollow">https://serpapi.com</a> | Junior-to-Senior Fullstack Engineer | Customer Success Engineer | Talent Acquisition Specialist | Based in Austin, TX but remote-first structure | Full-time | ONSITE or FULLY REMOTE | $150K - 180K a year 1099 for US or local avg + 20% for outside the US</p><p>SerpApi is the leading API to scrape and parse search engine results. We deeply support Google, Google Maps, Google Images, Bing, Baidu, and a lot more.</p><p>Our current stack is Ruby, Rails, MongoDB, and React.JS. We are looking for more Junior and Senior FullStack Engineers.
We have an awesome work environment: We are a remote first company (before Covid!). We do continuous integration, continuous deployments, code reviews, code pairings, profit sharing, and most of communication is async via GitHub.</p><p>We value super strongly transparency, do open books, have a public roadmap, and contribute to the EFF.</p><p>Apply at: <a href="https://serpapi.com/careers" rel="nofollow">https://serpapi.com/careers</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847873"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847873" href="https://news.ycombinator.com/vote?id=40847873&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Layer | Founding Front End &amp; Fullstack Eng | San Francisco In-Person | Full-Time</p><p>We’re building SMB accounting software API-first so that it can be integrated directly into software platforms where business owners already work. Our customers are vertical software platforms in the vein of Toast, MindBody or Housecall Pro that help small business owners start, run and scale their business with workflows and financial products. We help them serve their customers who are stuck exporting all their data out of these systems into CSVs and importing it back into other accounting software.</p><p>Integrating with our accounting APIs gives business owners automation &amp; accounting workflows directly within their core software while the software platforms get to solve one their customers’ biggest pain points: worrying about bookkeeping &amp; finances.</p><p>We’re looking for a founding frontend engineer who is excited about building a company and team as well as building better software for SMBs.  We have no standalone frontend - instead we build a React component library that our customers can directly use within their own apps: <a href="https://www.npmjs.com/package/@layerfi/components" rel="nofollow">https://www.npmjs.com/package/@layerfi/components</a></p><p>We are well funded by top tier investors and are live with hundreds of small businesses. If you’re passionate about building software that real small businesses use every day, get in touch with me at daniel [at] layerfi.com</p><p>More about the role and team here: <a href="https://jobs.ashbyhq.com/layerfi/ef55941f-b59c-419a-8736-a255cd530a19">https://jobs.ashbyhq.com/layerfi/ef55941f-b59c-419a-8736-a25...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847903"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847903" href="https://news.ycombinator.com/vote?id=40847903&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Company: OrangeQC</p><p>Job: Senior Android and iOS Engineers</p><p>Location: USA (Remote)</p><p>Allows remote: Yes</p><p>We're a 15-year old janitorial inspection software platform with local-first mobile apps.</p><p>We're looking a senior developer to lead each of our iOS and Android app development efforts.</p><p>We've recently completed a proof of concept for a green android field app using fully native Kotlin / jetpack tech to upgrade the user experience and potentially even overshoot the performance of our iOS app.</p><p>We're also experimenting with an updated architecture on iOS that takes our user experience to the next level.</p><p>JDs here:</p><p>- <a href="https://www.orangeqc.com/senior-android-engineer/" rel="nofollow">https://www.orangeqc.com/senior-android-engineer/</a></p><p>- <a href="https://www.orangeqc.com/senior-ios-engineer/" rel="nofollow">https://www.orangeqc.com/senior-ios-engineer/</a></p><p>Please email me with a resume: dave@orangeqc.com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847622"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847622" href="https://news.ycombinator.com/vote?id=40847622&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Privy | Onsite NYC (Preferred) or Remote (US/Canada) | Hybrid Product/Eng, Frontend, Fullstack, Backend Roles | Full-Time | <a href="https://privy.io/" rel="nofollow">https://privy.io</a></p><p>Hi, I'm Asta, CTO at the digital identity startup Privy. We build an authentication and cryptographic key management SDK that hundreds of companies integrate in order to onboard users (7M+ in the past year) onto products built with blockchain infra and distributed systems.</p><p>We believe that when users own cryptographic keys, we shift the status quo so that users control more of their data and assets online. Privacy is fundamentally about ownership and revocation. User experience is our North Star, because users won't compromise on UX.</p><p>We're a small, high-ownership team building a product with real usage (&gt;2M MAUs, massive transaction volume) that developers love. We've raised $26M from Sequoia &amp; Paradigm. We ship constantly - multiple production releases per week.</p><p>Reach out at join [at] privy.io - these emails go to me. <a href="https://jobs.ashbyhq.com/privy">https://jobs.ashbyhq.com/privy</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847868"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847868" href="https://news.ycombinator.com/vote?id=40847868&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>TeamSnap | Multiple Roles | Remote (US Only) | Full-time | <a href="https://teamsnap.com/" rel="nofollow">https://teamsnap.com</a></p><p>TeamSnap is hiring for multiple positions to join our remote-first team. We work every day to simplify the lives of players, coaches, parents, and sports organizations by taking the headache out of organizing sports.</p><p>Engineers at TeamSnap are critical to our technical and product innovation. We build applications and services with Ruby, Elixir, React, TypeSript, Go[lang], Swift, Kotlin, MySQL, RabbitMQ, Docker, Kubernetes, Firebase, and Google Cloud. On the Full Stack side, “T-shaped” developers are encouraged.</p><p>Open technical roles:</p><p>- Senior Android Engineer</p><p>- Senior Data Engineer</p><p>- Senior iOS Engineer</p><p>- Senior Software Engineer (BE/Fullstack)</p><p>- Software Engineer</p><p>We also have a number of non-technical roles available as well!</p><p>View all roles here: <a href="https://jobs.lever.co/teamsnap/?lever-via=0u_uZ-k-Wh&amp;lever-social=job_site" rel="nofollow">https://jobs.lever.co/teamsnap/?lever-via=0u_uZ-k-Wh&amp;lever-s...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847940"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847940" href="https://news.ycombinator.com/vote?id=40847940&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Blackshark AI | Senior Software Engineer &amp; Senior Machine Learning Engineer | Full-time | Austria or Remote EU</p><p>Satellite and drone imagery access is on the rise, and traditional image processing methods are struggling to keep up. Our scalable AI quickly extracts global features, providing real-time, on-demand geospatial insights with impressive speed and accuracy. We are turning months of manual work into mere minutes, and with much better results. Our platform also powers our 3D synthetic solutions, which enable large scale simulations and visualizations. Our applications cover various domains, from intelligence and defense solutions for US and US allies to disaster relief, flight simulation and training, insurance, smart cities, urban planning, and more.</p><p>Backend Tech Stack: Python Fast API PostgreSQL Docker RabbitMQ Kubernetes Azure</p><p>Deep Learning: Pytorch, Torchvision, scikit-learn</p><p>Image Processing: Numpy, PIL, OpenCV, SciPy, scikit-image, matplotlib, kornia</p><p>Geospatial: RasterIO, Geopandas, GDAL/OGR, QGIS, ArcGIS</p><p>Apply at: <a href="https://jobs.eu.lever.co/blackshark" rel="nofollow">https://jobs.eu.lever.co/blackshark</a> or reach out to HM at: 'eivanov@' + username[:10] + '.' + username[10:].lower()</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847983"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847983" href="https://news.ycombinator.com/vote?id=40847983&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>recurse.ml | ML Researcher, Founding Engineer | London On-Site</p><p>We're a seed-stage, code generation startup, working to automate the boring tasks in large codebases. If you'd like to push the boundaries of code generation and deploy your work in some of the largest software engineering teams orgs in the world, shoot me an email (it's in the Notion job descriptions).</p><p><a href="https://www.notion.so/cerebral-af/Working-at-Recurse-ML-1d3a36c9ebc8455ab9822df8c6adcffc" rel="nofollow">https://www.notion.so/cerebral-af/Working-at-Recurse-ML-1d3a...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846969"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846969" href="https://news.ycombinator.com/vote?id=40846969&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Umbra | Multiple roles | Onsite &amp; Hybrid (Santa Barbara, Austin, Washington DC) | <a href="https://umbra.space/" rel="nofollow">https://umbra.space/</a></p><p>Umbra builds next-generation space systems that observe the Earth in unprecedented fidelity. Our mission: Deliver global omniscience.</p><p>To stay ahead of climate change, geopolitical risk, and other major crises, we need a global understanding of what is changing, where, and how fast. Umbra is built around the idea that by providing easy access to the highest quality commercial satellite data available, we can become an indispensable tool for the growing number of organizations monitoring the Earth. We empower our customers with the ability to create the solutions that inform, inspire, and address our planet’s most pressing needs. We’re helping to create a brand new industry that has never meaningfully existed before. To make our vision a reality, we're gonna need some help.</p><p>We're looking to fill a variety of mid- to senior-level roles, including openings in Spacecraft Operations, Mission Systems Architecture, Embedded Software Engineering, and Corporate Counsel. If you're interested in helping us build, please apply at <a href="https://umbralab.bamboohr.com/careers" rel="nofollow">https://umbralab.bamboohr.com/careers</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846920"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846920" href="https://news.ycombinator.com/vote?id=40846920&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Make Waves | <a href="https://makewaves.fm/" rel="nofollow">https://makewaves.fm</a> | Lead developer &amp; Marketing Manager | FULLTIME | ONSITE / HYBRID (&gt;= one day in the office a week) | Amsterdam, NL</p><p>Make Waves was founded in 2021 by two brothers with a track record in the music and tech industry. We’re building a kinder, more helpful platform for independent artists to release their music and build a following. By providing artists with easy-to-understand tools and insights in a notoriously over-complicated industry, we want to empower DIY artists to control their careers without making concessions.</p><p>In the last years we’ve built the platform from the ground up and are now entering a phase where we’re focusing on growth and maturity. We’ve also secured funding for the next three to four years, so we can provide a stable job with great growth potential.</p><p>We are actively looking for a lead developer (with a hope you can transition into a CTO role in the near future) and a marketing manager. Unfortunately we can’t offer relocation or visa sponsorship and are only considering candidates that are able to join us in the office weekly.</p><p>Check out <a href="https://kb.makewaves.fm/general/careers" rel="nofollow">https://kb.makewaves.fm/general/careers</a> for more details.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847624"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847624" href="https://news.ycombinator.com/vote?id=40847624&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Loop | loop.com | Marketing/Product/Design/Engineering | On-site San Francisco, CA, Chicago, IL | H1B OK</p><p>Loop is on a mission to unlock profits trapped in the supply chain (<a href="https://loop.com/article/unlock-profit-trapped-in-your-supply-chain" rel="nofollow">https://loop.com/article/unlock-profit-trapped-in-your-suppl...</a>) and lower costs for consumers. Bad data and inefficient workflows create friction that limits working capital and raises costs for every supply chain stakeholder.</p><p>Loop’s modern audit and pay platform uses our domain-driven AI to harness the complexity of supply chain data and documentation. We improve transportation spend visibility so companies can control their costs and power profit. That is why industry leaders like J.P. Morgan Chase, Great Dane, Emerge, and Loadsmart work with Loop.</p><p>A recent WSJ piece on Loop -  on.wsj.com/3PE357W</p><p>1. Raised $65m from JPM GEP, Founders Fund, 8VC, Susa Ventures, Flexport, Index, and Expa.</p><p>2. 35 paying enterprise customers with multiple-year contracts; 60+ customers in the pipeline.</p><p>3. High-caliber team of engineers and operators from Google, Scale AI, Flexport, Uber, Rakuten, Square, Meta, Stanford, Brown, Princeton, and Yale.</p><p>4. 5+ years cash runway</p><p>Fullstack Engineer San Francisco - <a href="https://boards.greenhouse.io/loop/jobs/4102236004" rel="nofollow">https://boards.greenhouse.io/loop/jobs/4102236004</a></p><p>Fullstack Engineer Chicago - <a href="https://boards.greenhouse.io/loop/jobs/4830548004" rel="nofollow">https://boards.greenhouse.io/loop/jobs/4830548004</a></p><p>Senior Product Designer San Francisco - <a href="https://boards.greenhouse.io/loop/jobs/4126037004" rel="nofollow">https://boards.greenhouse.io/loop/jobs/4126037004</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847823"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847823" href="https://news.ycombinator.com/vote?id=40847823&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Hightouch |Remote (North America)|Full-time | Backend Engineers</p><p>Some background on Hightouch - our mission is to help companies leverage their customer data to grow. We started with the problem of “Reverse ETL” or helping companies sync data from their data warehouse (e.g. Snowflake, Databricks, etc.) to 200+ SaaS tools (Salesforce, Marketo, Facebook Ads, etc.) without coding. Since then, we’ve evolved into a suite of tools around the warehouse (identity resolution, data enrichment, event streaming, etc.). We’ve raised a Series B and scaled to $20m+ ARR in 3 years with 600+ customers including Fortune 500 co’s like Spotify, the NBA, PetSmart, etc.</p><p>We are hiring for:</p><p>Software Engineer, Backend (Customer Studio): <a href="https://boards.greenhouse.io/hightouch/jobs/4782625004" rel="nofollow">https://boards.greenhouse.io/hightouch/jobs/4782625004</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847808"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847808" href="https://news.ycombinator.com/vote?id=40847808&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>OwnerRez | Remote | Senior Full Stack Engineer | Full Time
OwnerRez is a vacation rental software platform for property managers and owners that integrates with channels like Vrbo, Airbnb, TripAdvisor, and booking.com as well as direct websites to manage vacation rental properties, bookings, and guests -- making the entire process automated.</p><p>We're looking for a senior full stack engineer to join our team to design and develop features end to end -- from web UI to backend business logic, services, and the database. We’re also looking for ops skills and desire to join our on-call rotation.</p><p>Our stack: .NET (MVC, jQuery on the frontend, WebAPI on the backend), MySQL, Redis, DynamoDB.</p><p>Details: <a href="https://www.ownerreservations.com/senior-software-developer" rel="nofollow">https://www.ownerreservations.com/senior-software-developer</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847777"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847777" href="https://news.ycombinator.com/vote?id=40847777&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Speedify | Senior Software Engineer | Location: Philadelphia, PA | ONSITE at once per week | Full Time</p><p>At Speedify we make complex networking tasks easy by developing apps that deliver faster, more reliable, and more secure Internet to users all across the globe.. We now have millions of customers using Speedify, the only app that can combine multiple internet sources into one bonded super-connection for more stable and secure livestreaming, video calling, and web browsing.</p><p>Speedify’s engine is powered by C++, and if you want to make the Internet faster and more reliable, we want to hear from you!  Speedify is based in Center City, Philadelphia, PA. This is a hybrid role that expects employees to work in the office at least one day per week.</p><p>* Proficiency in C/C++</p><p>* 5+ years of experience in software development</p><p>* Improve and extend our C++ packet processing engine</p><p>* Improve and expand our automated test suites</p><p>Jobs: <a href="https://speedify.com/careers/#positions" rel="nofollow">https://speedify.com/careers/#positions</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847700"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847700" href="https://news.ycombinator.com/vote?id=40847700&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Motive | Electrical Engineers (Buffalo, NY), Product Managers, Engineers, AI | Remote Opportunities Available |</p><p>Motive builds technology to improve the safety, productivity, and profitability of businesses that power the physical economy. The Motive Automated Operations Platform combines IoT hardware with AI-powered applications to automate vehicle and equipment tracking, driver safety, compliance, maintenance, spend management, and more.</p><p>Motive serves more than 120,000 businesses, across a wide range of industries including trucking and logistics, construction, oil and gas, food and beverage, field service, agriculture, passenger transit, and delivery. Motive is proud to be a Forbes Cloud 100 company and a 2020 Career-Launching Company by Wealthfront.</p><p>We are looking to hire team members that are passionate about building products that will have a massive impact on the lives of people. We seek and embrace diversity in all of its forms. We continuously push ourselves to think differently and take ownership wherever it's needed. This is a place for dreamers and doers to succeed. If you share our passion for achieving what some say is impossible, join us.</p><p><a href="https://gomotive.com/company/careers/#search-jobs" rel="nofollow">https://gomotive.com/company/careers/#search-jobs</a></p><p>Visa Sponsorship: We can transfer H1b visas, we are also hiring in India, Canada, Taiwan, and Pakistan</p><p>Sample Tech Stack: Golang, Ruby, Java, AWS, PostgresSql, DynamoDB, Redis, Cassandra, Kafka, etc.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847606"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847606" href="https://news.ycombinator.com/vote?id=40847606&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Medplum (<a href="https://www.medplum.com/">https://www.medplum.com/</a>) | Founding Developer Experience Engineer | SF | Full time</p><p>Medplum (YC S22) is an open source, API first, healthcare developer platform. "Headless EHR", we take care of the security, compliance, and regulatory burdens of healthcare software development. Well funded and growing fast.</p><p>We're hiring an amazing Dev-Ex / Dev-Rel engineer to delight customers, build sample apps, and promote the Medplum platform.</p><p>Tech stack: TypeScript, React, Node.js, AWS</p><p>Learn more: <a href="https://www.medplum.com/careers/devex-engineer">https://www.medplum.com/careers/devex-engineer</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847685"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847685" href="https://news.ycombinator.com/vote?id=40847685&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Koddi | Currently seeking FT employees in the following locations: Ann Arbor, MI | Fort Worth, TX | Austin, TX | New York, NY Open roles: Senior Software Engineers (Go, Java, C, C++, PhP); Integration Engineer; Integration Engineering Manager; ML Engineer (must have adtech experience), Director of Data Science (must have adtech experience), Platform Engineer, Director of Infrastructure.</p><p>Must be US Citizen or Green Card holder and physically located in the US. Passionate about development in leading technologies? Looking to become a major player on a diverse team? Want to make a big impact on an engineer-driven roadmap in your next career adventure? Koddi Engineers drive innovation by embracing challenges and deploying emerging technologies to solve complex problems in software development.</p><p>Koddi is a global technology company with software and services that help top digital marketplaces effectively monetize their first-party audiences through industry-leading commerce media technology and strategy. Our enterprise platforms leverage first-party data to drive marketplace revenue and profit by improving user experience and target shoppers throughout the purchase path. Koddi’s platforms enable any advertiser, any marketplace, in any industry to increase awareness, generate demand, and drive revenue.</p><p>Review all open roles at www.koddi.com/careers and apply directly, or send your resume to matthew.myller@koddi.com.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847548"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847548" href="https://news.ycombinator.com/vote?id=40847548&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>JUPUS GmbH | Senior Software Engineer / Emerging Leader | Cologne | REMOTE</p><p>We are JUPUS, one of Germany's fastest-growing legal tech startups, and we are looking for a talented and experienced Senior Software Engineer - Emerging Leader to join our team. This position offers an exciting opportunity to grow into a leadership role, providing technical guidance and mentorship to our team.</p><p>Our stack is Python/Django in the backend and TypeScript/Vue.js in the frontend. Looking for someone who is full stack and can assist leading a team of 6 engineers. Salary is up to 84k/year.</p><p>Apply here <a href="https://join.com/companies/jupus/11486463" rel="nofollow">https://join.com/companies/jupus/11486463</a> or send me an email at jannis.gebauer@jupus.de</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847704"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847704" href="https://news.ycombinator.com/vote?id=40847704&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Val Town | <a href="https://val.town/" rel="nofollow">https://val.town</a> | Founding DevRel | ONSITE (Brooklyn) | Full-time | $150k &amp; 1% equity</p><p>Val Town is the serverless JavaScript platform for side projects and internal tools. In your browser, you can build APIs, websites, crons, and email handlers. Write code, hit save, and your code is deployed.</p><p>We're hiring a DevRel engineer to inspire and support our community. We're looking for someone bursting with creativity, enthusiasm, and is extremely prolific in public. We want someone who loves coding, writing, sharing demos, and getting likes and upvotes. This role would be perfect for someone who thrives making tons and tons of little prototypes, as well as writing docs, guides, and tutorials to bring others along with them.</p><p>I'm Steve (one of the founders) and I've been doing most of the DevRel role myself thus far. I put a lot of time and thought into who we're looking for: <a href="https://val-town.notion.site/DevRel-at-Val-Town-117abe97069449e58df53d7a794ebfac" rel="nofollow">https://val-town.notion.site/DevRel-at-Val-Town-117abe970694...</a></p><p>If that sounds like you, please reach out :)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847391"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847391" href="https://news.ycombinator.com/vote?id=40847391&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Seen Finance | <a href="https://seen.com/" rel="nofollow">https://seen.com</a> | Full time | Berlin, Chicago | Hybrid on-site</p><p>At Seen, we believe that everyone deserves a fair opportunity to (re)build their credit. We understand how tough it can be to qualify for a credit card when your credit score isn't where you want it to be.</p><p>Seen is a new fintech company, started in 2023 following the acquisition of Sable (YC S19) by Snap Finance. We've recently launched a credit card in the US for people with less than ideal credit. We plan to offer other novel credit products in the future.</p><p>It's a great time to join as the team is small (15 engineers) and we're growing fast. We're a startup but we're also lucky to have the financial support and customer base from our parent company.</p><p>We're hiring for:</p><p>* Senior and Staff Software Engineers (Berlin)</p><p>* Senior and Staff Data Engineers (Chicago)</p><p>Our mobile app, web app, and backends are written in Typescript. Our data stack uses Python and SQL.</p><p>If you're interested, just email us your resume or LinkedIn page: join-berlin@seen.com (for both Berlin and Chicago roles)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847895"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847895" href="https://news.ycombinator.com/vote?id=40847895&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>FUTO | Austin, TX | On-site / Hybrid / Remote | Full Time / Hourly</p><p>FUTO is an organization dedicated to developing, both through in-house engineering and investment, technologies that frustrate centralization and industry consolidation. Through a combination of in-house engineering projects, targeted investments, generous grants, and multi-media public education efforts, we will free technology from the control of the few and recreate the spirit of freedom, innovation, and self-reliance that underpinned the American tech industry only a few decades ago. Our principles are here: <a href="https://futo.org/what-is-futo" rel="nofollow">https://futo.org/what-is-futo</a></p><p>FUTO is hiring for two Quality Assurance roles.</p><p>QA Lead - We are looking for a full-time, in-person QA Lead to drive our quality assurance efforts across FUTO’s projects, with a focus on our Android applications. The ideal candidate is aligned with our principles, has a strong sense of software craftsmanship and will be highly active in dogfooding our products. <a href="https://futo.org/jobs/qa-lead/" rel="nofollow">https://futo.org/jobs/qa-lead/</a></p><p>QA Tester - We are seeking an experienced QA Tester to join our team on a part-time or contract basis. The ideal candidate will have a keen eye for detail, a systematic approach to testing, and a strong ability to document and communicate findings. The position will be responsible for ensuring our applications meet the highest standards of quality. <a href="https://futo.org/jobs/qa-tester/" rel="nofollow">https://futo.org/jobs/qa-tester/</a></p><p>To apply, send your resume and cover letter to jobs at futo dot org with the subject of the position you're interested in.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846854"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846854" href="https://news.ycombinator.com/vote?id=40846854&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Rollbar | <a href="https://rollbar.com/" rel="nofollow">https://rollbar.com</a> | Remote Europe, Budapest | Engineering</p><p>About Rollbar:</p><p>* We're a ~30-person team (SF, Budapest, remote US and Europe) with a mission to help developers build software quickly and painlessly</p><p>* We help tens of thousands of developers find and fix errors faster.</p><p>* Our backend handles billions of errors with low latency and high reliability</p><p>* Our front-end allows developers to discover and drill down across millions of errors in real-time</p><p>* Our customers are some of the best engineering teams in the world, including Twilio and Shipt</p><p>* Our values are honesty, transparency, pragmatism, and dependability</p><p>* Our tech stack includes Python, Node.js, TypeScript, and React; MySQL, Kafka, ClickHouse, Elasticsearch, and Redis; Kubernetes, Terraform, and Google Cloud Platform.</p><p>We're currently hiring for:</p><p>- Senior Software Engineer, Full-stack (Europe)</p><p>Please apply via: <a href="https://rollbar.com/careers/" rel="nofollow">https://rollbar.com/careers/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847508"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847508" href="https://news.ycombinator.com/vote?id=40847508&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Vaarst | UK, Bristol - Hybrid | Full or part time | can't sponsor visas</p><p>Vaarst focuses on the energy transition space, we're currently developing autonomy solutions for offshore wind where we're working on subsea autonomous visual inspection combined with our data and ML platform and SubSLAM technology.</p><p>We are growing quickly and looking for people who enjoy working on challenging projects in fast paced environments - think deploying autonomous robots to autonomously survey subsea structures.</p><p>Robotics, Senior and Principal Engineers - £100-150k
Software, Senior and Principal Engineers (C++, Python, Computer vision, SLAM) - £100-150k
Unreal Engineers, Senior and Principal 
UX Designers
Product Managers</p><p><a href="https://careers.vaarst.com/#jobs" rel="nofollow">https://careers.vaarst.com/#jobs</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847749"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847749" href="https://news.ycombinator.com/vote?id=40847749&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Imbue | Engineering Manager | San Francisco | Full-time</p><p>Imbue builds AI systems that reason and code, enabling AI agents to accomplish larger goals and safely work in the real world. We train our own foundation models optimized for reasoning and prototype agents on top of these models. By using these agents extensively, we gain insights into improving both the capabilities of the underlying models and the interaction design for agents. We aim to rekindle the dream of the personal computer, where computers become truly intelligent tools that empower us, giving us freedom, dignity, and agency to pursue the things we love.</p><p>For more example projects and benefits, see the full job description: <a href="https://imbue.com/careers" rel="nofollow">https://imbue.com/careers</a></p><p>Please apply through the website above. All submissions are reviewed by a real person!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847598"><td></td></tr>
            <tr id="40847776"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847776" href="https://news.ycombinator.com/vote?id=40847776&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Whalesync (YC S21) | Founding Engineer | US Time Zones | Fully Remote (can hire internationally) | Full-time</p><p>I'm the cofounder of Whalesync (YC S21) and we're hiring a founding engineer. We're a small team building a technically-challenging product, starting with two-way data syncing between SaaS applications.</p><p>Tech stack: GCP, TypeScript, Node/NestJS backend, Next.js frontend (React, Mantine), Postgres and Redis data stores</p><p>If interested, please reach out to us here! <a href="https://www.ycombinator.com/companies/whalesync/jobs/F6EHHs3-founding-engineer">https://www.ycombinator.com/companies/whalesync/jobs/F6EHHs3...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847220"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847220" href="https://news.ycombinator.com/vote?id=40847220&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>ML6 | Machine Learning Engineer, Data Engineer | Python, TensorFlow, PyTorch, GCP, AWS, Azure | Full-time | Amsterdam, Berlin, Ghent (EU) On-site/hybrid</p><p>We are a Machine Learning consulting company that builds end-to-end Machine Learning solutions. By applying the latest AI research, we keep our clients at the forefront of innovation.</p><p>If you are interested check out: <a href="https://www.ml6.eu/resources/resource-library" rel="nofollow">https://www.ml6.eu/resources/resource-library</a> and <a href="https://www.ml6.eu/client-cases" rel="nofollow">https://www.ml6.eu/client-cases</a></p><p>Work on innovative projects for the biggest clients across Europe such as Randstad, ASML, FUNKE, and many more! Whether it’s about leveraging LLMs to improve customer support, building data lakes on cloud platforms to improve storage or implementing models using sensor data for quality control. You can find it all at ML6.</p><p>You will mostly work with Python and a range of ML frameworks such as TensorFlow, PyTorch, or HuggingFace Transformers to solve hard Machine Learning tasks and help bring these application into production by building data pipelines and cloud infrastructure on all of the major cloud providers (GCP, AWS, Azure).</p><p>We are looking for:</p><p>• (Senior) Data Engineer</p><p>• (Senior) Software Engineer</p><p>• (Senior) Machine Learning Engineer</p><p>• Alliance Manager – Azure</p><p>• AI Team Lead</p><p>• AI Client Executive</p><p>• AI Client Partner</p><p>• AI Project Manager</p><p>• Talent Partner</p><p>Apply at: <a href="https://ml6.eu/join-us" rel="nofollow">https://ml6.eu/join-us</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847723"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847723" href="https://news.ycombinator.com/vote?id=40847723&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Sudowrite | REMOTE | Full-Time | Competitive Salary + Equity | Hiring: AI Engineer</p><p>- We believe the future of writing is AI &amp; human collaboration. Sudowrite is the best AI for creative writers.</p><p>- We are profitable with a rapidly growing paying user base.</p><p>- We've been featured in The New York Times, New Yorker, Verge, etc.</p><p>- We believe new tools will enable a renaissance in art &amp; creativity and we want to help the next generation of storytellers tell better stories.</p><p>- We’re 100% remote and support work-life balance. We meet up in person a few times of year, last time in Portland. We encourage diverse candidates from all over the world to apply.</p><p>- Details: <a href="https://r.people.capital/sudowrite-ai" rel="nofollow">https://r.people.capital/sudowrite-ai</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847179"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847179" href="https://news.ycombinator.com/vote?id=40847179&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Enveritas (YC S18, Non-Profit) | Data Scientist | Remote / Global | <a href="https://enveritas.org/jobs/" rel="nofollow">https://enveritas.org/jobs/</a> Enveritas is a 501(c)3 non-profit working on sustainability issues facing coffee farmers around the globe. We provide sustainability assurance for the coffee industry. We visit smallholder coffee farms around the world to understand their social, economic, and environmental practices. In 2024, we will visit over 70,000 farms across more than 25 countries in Asia, Africa, and Latin America. We work with leading coffee roasters to understand the sustainability issues in their supply chain based on our sustainability standards.</p><p>* Backend Software Engineer - $130-$150k — <a href="http://enveritas.org/jobs/backend-software-eng/#10d7adef8us" rel="nofollow">http://enveritas.org/jobs/backend-software-eng/#10d7adef8us</a> (worldwide remote)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40847409"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40847409" href="https://news.ycombinator.com/vote?id=40847409&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>That is very cool. If I wasn't working on my own project I would applying for this (I love coffee and the journey of coffee).</p><p>Keep up the great work.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847732"><td></td></tr>
                  <tr id="40847594"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847594" href="https://news.ycombinator.com/vote?id=40847594&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Fractional AI | Founding Engineer | San Francisco | Onsite | Full-time</p><p>Who: We're Fractional AI -- the dev shop for difficult enterprise applications of genAI.</p><p>Looking for: You! Full-stack founding engineers (5+ years of experience, 150k-210k cash, meaningful equity, 99% healthcare premium coverage )</p><p>How: apply <a href="https://jobs.lever.co/fractional" rel="nofollow">https://jobs.lever.co/fractional</a> or reach out to me (ben@ fractional.ai)</p><p>Why we may be a great fit:</p><p>-High impact AI projects - we work on companies' hardest problems to get genAI into production (in the same quarter you might build an AI phone agent for one customer and automate a complex workflow for another customer)</p><p>-Culture of a startup but substantive problems to solve that impact millions of users</p><p>- Curious, humble, banter-y in-person team comprised of multi-time founders</p><p>Why we may not be the best fit:</p><p>-Excellence isn't really really important to you (this is less of a 'move fast and break things' role, though we respect that ethos!)</p><p>-Predictability is important to you - we work across customers, tech stacks, industries etc</p><p>-Interacting with customers isn't your thing</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846430"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846430" href="https://news.ycombinator.com/vote?id=40846430&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Eigen | Full-Time | product engineers, ex-technical founder engineers, and ai engineers | REMOTE (all remote) | Hiring GMT-8 to GMT+2</p><p>Eigen helps companies build world-class AI products, enabled by the cutting edge of AI and LLMs. Products span spaces from edtech to leadgen to finance.</p><p>* we are a real business — profitable with $XM in revenue this year.</p><p>* we need: product engineers, ex-technical founders, and ai engineers to build products. Experience with typescript stacks and/or python is fantastic.</p><p>* smart team. ex. stanford ai lab, harvard, uber, tinder, etc.</p><p>* if you're a fullstack ts dev and are interested in growing into AI, this role is perfect for you.</p><p>* if you like learning about new industries and working on different kinds of projects regularly, this role is perfect for you.</p><p>send an email to careers [at] eigen (.) net with a resume/site link/whatever is indicative of you. No need to write long email, just a "hi" is great :)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847655"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847655" href="https://news.ycombinator.com/vote?id=40847655&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Hivestack by Perion | Montreal, QC (Hybrid Office/Remote) | Full Time
Industry: AdTech, specializing in Digital Out Of Home</p><p>I'm hiring for two engineering lead positions (senior/staff level):</p><p>* Data Discipline Lead - this includes driving the architecture and technical roadmap, and the implementation of various technologies to support adtech-scale ingestion, transport, processing, warehousing, and reporting.</p><p>* Edge Discipline Lead (Ad Bidding &amp; Delivery): This position leads the team responsible for the high-volume low-latency ad bidding and delivery systems - a particularly good fit for someone who enjoys systems performance engineering, architecture, while still being hands-on</p><p>Full job descriptions: <a href="https://www.hivestack.com/careers/" rel="nofollow">https://www.hivestack.com/careers/</a></p><p>If you're interested, get in touch :) (contact info in bio)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847174"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847174" href="https://news.ycombinator.com/vote?id=40847174&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Sumble | sumble.com | Data/Engineering + first GTM | Remote-only North-America timezones</p><p>Sumble's applying language models (both small and large) to build high-quality datasets. Near-term our focus is company-related data that we integrate with customer's Salesforce data to make go-to-market operations more efficient. Long-term we want to become the first place you go to find high-quality data that lives outside your organization.</p><p>Currently a team of 7 engineers, including Kaggle/Google/Primer/Stack Overflow/Meta alumni.</p><p>Our tech stack includes Python, FastAPI, React/Typescript, GCP (postgres + alloydb + cloud run), regular expressions, and Pytorch/Gemma/Mistral.</p><p>Challenges we face include:</p><p>- Transforming noisy datasets + noisy models into high quality data products</p><p>- Making expensive analytics computations run efficiently</p><p>- Managing the complexity of an increasing number of data sources, models, and operations on top of these</p><p>If this excites you, get in touch with us by emailing me at ben at sumble.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847101"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847101" href="https://news.ycombinator.com/vote?id=40847101&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>76 Software Engineering Group | Oklahoma City, OK | FULL-TIME | ONSITE | U.S. CITIZENSHIP REQUIRED</p><p>76 SWEG is a civilian software engineering organization operating under the United States Air Force. We are hundreds of (civilian) scientists and engineers that provide software, hardware, and engineering support solutions to a variety of Air Force and military platforms. We are located on Tinker Air Force Base in Oklahoma City, OK. We often operate like a contractor to other parts of the military and federal government by providing independent engineering services without seeking a profit. We have dozens of active projects using C, C++, C#, Java, Python, JavaScript, LabVIEW, Visual Basic, Assembly, Ada, Fortran, and other more esoteric languages. We have immediate opportunities available to hire candidates with degrees in Computer Science, Computer Engineering, Electrical Engineering, or closely-related fields.</p><p>If you are interested in learning more, please e-mail 76SMXG.Tinker.Careers@us.af.mil and tell them Jake sent you.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847365"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847365" href="https://news.ycombinator.com/vote?id=40847365&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Founding Engineer
Location: San Francisco
Employment: Full-time, Hybrid (3 days in person)</p><p>We're rebuilding the financial ledger, revolutionizing how multi-location businesses assess their performance, and empowering them to make better and more timely decisions. Our software streamlines data collection, reporting, and analysis, arming business owners with the insights they need to thrive.</p><p>We're looking for a talented and passionate Founding Engineer to join us early, working closely alongside the founding team. Our tech stack includes Rust, Typescript, Next, WASM, and Postgres.</p><p>Reach out to alex@contour.app to apply.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847677"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847677" href="https://news.ycombinator.com/vote?id=40847677&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Monument (<a href="https://joinmonument.com/" rel="nofollow">https://joinmonument.com</a>) | Senior Software Engineer | New York, NY | Hybrid | Full-Time</p><p>For people who are tired of navigating a costly and unpredictable healthcare system, Monument is the virtual care platform that takes the guesswork out of healthcare. It finds you a great doctor or therapist and enables you to reserve regular appointments so you can get the care you deserve, affordably and on your terms.</p><p>We're looking for a senior software engineer to join our NYC-based team (hybrid office near Penn Station).</p><p>Tech stack: NodeJS, React, Typescript, PostgreSQL, Redis, AWS w/ Terraform, Kubernetes</p><p>If interested please email will at joinmonument dot com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847429"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847429" href="https://news.ycombinator.com/vote?id=40847429&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div>
                  <p>Two Six Technologies | Reverse Engineer/Vulnerability Researcher | Arlington, VA  | ONSITE | Full Time |<a href="https://boards.greenhouse.io/twosixtechnologies/jobs/5096596" rel="nofollow">https://boards.greenhouse.io/twosixtechnologies/jobs/5096596</a>... Two Six Technologies is seeking Reverse Engineers &amp; Vulnerability Researchers to perform in-depth reverse engineering and exploit development to transition those findings into capabilities. We are looking to hire a number of Security Researchers with experience in the following categories: firmware reverse engineering, hardware reverse engineering, and Linux/UNIX kernel development. Additional opportunities are available for embedded software engineers, RF electronic design engineers, FPGA/ASIC design engineers, hardware reverse engineers, firmware reverse engineers, and low-level developers looking to enter the security field. If you work in the embedded security domain, we would love to hear from you. Preference is for applicants who hold a TS clearance at the minimum with the ability to obtain SCI. If interested, please reach out to our team @ recruiting@twosixtech.com</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847443"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847443" href="https://news.ycombinator.com/vote?id=40847443&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Virta Health (<a href="https://virtahealth.com/" rel="nofollow">https://virtahealth.com</a>) | SF/Denver/REMOTE (USA Only) | Full-Time</p><p>Virta Health is on a mission to transform diabetes care and reverse the type 2 diabetes epidemic. Current treatment approaches aren’t working—over half of US adults have either type 2 diabetes or prediabetes. Virta is changing this by helping people reverse type 2 diabetes through innovations in technology, personalized nutrition, and virtual care delivery reinvented from the ground up. We have raised over $350 million from top-tier investors, and partner with the largest health plans, employers, and government organizations to help their employees and members restore their health and live diabetes-free. Join us on our mission to reverse diabetes in 100M people.</p><p>Virta has been named one of Time's 100 Most Influential Companies 2023 and one of Fortune's Best Workplaces in the Bay Area 2023.</p><p>Positions are Remote-first (USA Only). Virta also has offices in San Francisco, CA and Denver, CO.</p><p>Tech stack: Typescript, React/React-Native, Python, GCP, Golang</p><p>Open Positions:</p><p>- Senior Backend Engineer - Activation ($185,000 - $240,000): <a href="https://jobs.ashbyhq.com/virtahealth/1748c3a4-a0ec-4ed2-b14a-705b0bb9720f?utm_source=5dMXNoQkZg">https://jobs.ashbyhq.com/virtahealth/1748c3a4-a0ec-4ed2-b14a...</a></p><p>- VP, Insights &amp; Analytics ($200,000 - $245,000): <a href="https://jobs.ashbyhq.com/virtahealth/98bbe78d-5994-4f13-b82f-357084e07801?utm_source=BoaA0ZwwYA">https://jobs.ashbyhq.com/virtahealth/98bbe78d-5994-4f13-b82f...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846748"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846748" href="https://news.ycombinator.com/vote?id=40846748&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Matterworks.ai | Software &amp; Data Engineering | Hybrid (Boston/Somerville MA) At Matterworks we are building AI tools to extract insights from the ever-growing corpora of biological data and to unlock opportunities in therapeutic discovery, development, and manufacturing. We are building large-scale deep learning models of biological data to predict the phenotype and behavior of biological systems.</p><p>Throughout the interview process you can expect the following during your time with us:
    - Phone call to review with hiring manager
    - Virtual pairing interview (system architecture and design)
    - Hopefully a quick decision and offer!</p><p>Principal Software Engineer, Full Stack - <a href="https://jobs.lever.co/matterworks/94bec32b-7dfc-4307-9338-ce12df433484?lever-via=PwTFG5yoIa&amp;lever-social=hackernews" rel="nofollow">https://jobs.lever.co/matterworks/94bec32b-7dfc-4307-9338-ce...</a></p><p>Principal Data Engineer - <a href="https://jobs.lever.co/matterworks/ed4cdf51-7e5d-4a0f-9d42-b5e027b07994?lever-via=PwTFG5yoIa&amp;lever-social=hackernews" rel="nofollow">https://jobs.lever.co/matterworks/ed4cdf51-7e5d-4a0f-9d42-b5...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846503"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846503" href="https://news.ycombinator.com/vote?id=40846503&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Quatt.io | Amsterdam, Netherlands | Full-time | Hybrid/ONSITE | <a href="https://quatt.io/" rel="nofollow">https://quatt.io</a> | climate tech</p><p>I'm head of Software at Quatt, a quickly growing startup/scaleup building heatpumps to help fix climate change. Heating and cooling is 50% of all energy used in the EU. Heat pumps save 10 times more CO2 for each Euro spent on them compared to electric cars. We're building the most accessible and smartest heatpump on the market. Our product is live, we have thousands of customers, tons of data, and I really like the impact we're having. I'm currently looking for a few roles for my department, as we believe having the best software will allow us to have the best product. Our backend and frontend is Typescript.</p><pre><code>  * Medior QA / Test automation engineer
  * Senior app developer (React native)
  * Senior Backend developer (Typescript)
  * Medior Full-stack developer (Typescript)
  * DevOps Engineer
</code></pre><p>
Now is a great time to join, as the software team is still small but growing quickly. These and other vacancies are on our careers page: <a href="https://www.quatt.io/working-at-quatt" rel="nofollow">https://www.quatt.io/working-at-quatt</a> Email me directly ( my-hacker-news-username@quatt.io ) for questions or apply via the career page. Unfortunately, at this time, you have to be allowed to work in the EU: we're not able to sponsor Visa</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40846765"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40846765" href="https://news.ycombinator.com/vote?id=40846765&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>"Hybrid/ONSITE"</p><p>What exactly does that mean? Part time remote from within the EU is possible, with regulary travelling there, or does it mean, some days of the week remote is possible but otherwise office time is required (and therefore relocating)?</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40846547"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846547" href="https://news.ycombinator.com/vote?id=40846547&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>VIVA Finance | Atlanta, GA | Junior/Senior Back-End and React Developers, Project Manager</p><p>VIVA is a Fintech Startup based in Atlanta, GA with the mission to build a more inclusive financial system. VIVA offers unsecured personal loans to subprime customers who have traditionally been excluded and taken advantage of by the legacy financial institutions. The VIVA difference is to underwrite heavily on employment history and set up repayments through voluntary direct deposit payments from the borrower paycheck.</p><p>We are a VC backed company and have been in business for 5 years. We hit cash-flow positive in May and are now default alive with our ongoing growth series B likely to be our final round. We are growing the team to further support our personal lending product, in addition to getting the staff to support building new products to diversify our revenue streams.</p><p>Our tech stack for the back-end is fully on AWS, using Lambda and ECS for compute and Typescript as the language, but experience with this specific stack is not necessary for talented candidates. Preference is given to candidates who can come 4 days a week to our new office on the Atlanta Beltline (next to Krog Street Market) but remote positions are available for strong candidates able to work hours in eastern timezone.</p><p>Send me an introduction (alex at viva-finance.com) with a resume and we will be in touch!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846936"><td></td></tr>
            <tr id="40847363"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847363" href="https://news.ycombinator.com/vote?id=40847363&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Taiyaki Studios | Anime Pipeline Engineer | Tokyo | On-site Full Time | Will Sponsor Visa</p><p>We’re building new pipelines for anime production. Join us and empower animators to focus on the fulfilling stuff - story, character design, art direction, cinematography - and take the grunt work off their plate.</p><p>Our tools are being used to produce original content in-house, so you’ll get to work directly with animators and help make anime!</p><p>Ideal candidate: nimble problem solver, comfortable learning how to build tools that integrate with existing industry-standard software, even if you’ve never used said software before. We value fast learners who are language agnostic.</p><p>Bonus: Python, ComfyUI experience.</p><p>Japanese proficiency not required, though nice to have. We’ll pay for weekly lessons if you want them.</p><p>If you need visa sponsorship, you’ll need a bachelor’s or 10 years experience. Not our requirement, gov’t requirement. We’ll help with all relocation logistics.</p><p>We pay 20M JPY/yr (~$125k/yr) + equity, 3-4X median salary in Tokyo, you’ll live in luxury or save a good chunk of your salary.</p><p>What have you shipped, tinkered with, or hacked together? Let us know: careers@taiyakistudios.com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846694"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846694" href="https://news.ycombinator.com/vote?id=40846694&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Riffusion | Generative Music | San Francisco, CA</p><p>Looking for world class ML research engineers and scientists to join our core team building the future of music. If you love creativity, large scale diffusion models, fast infrastructure, and early startup energy, our team may be a fit. We're building something new to inspire hundreds of millions of people. Competitive comp, in-person culture.</p><p>Contact: hayk @ riffusion dot com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846699"><td></td></tr>
            <tr id="40847287"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847287" href="https://news.ycombinator.com/vote?id=40847287&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Kajabi | Data Engineer Lead | REMOTE</p><p>I'm Aaron, leading the data team at Kajabi, a leading all-in-one platform for knowledge entrepreneurs to build, market, and sell educational content, communities, newsletters, and other digital products. We have helped entrepreneurs generate over $7 billion in sales from over 85 million customers to date.</p><p>Role: Data Engineer Lead</p><p>Location: Remote (USA)</p><p>What You’ll Do:
• Shape our data strategy and develop robust data models.
• Build and optimize data pipelines.
• Enhance analytics and ensure data quality.
• Collaborate with a dynamic and cross-functional team.</p><p>What We’re Looking For:
• 5+ years in a data engineering lead role.
• Proven experience with data pipelines, data models, and data governance.
• Strong SQL skills and proficiency with dbt and Snowflake.
• Experience with event-driven architectures and analytics databases (e.g., Apache Druid, ClickHouse).</p><p>Perks:
• Fully paid medical, dental, and vision insurance.
• Flexible vacation policy and telecommuting.
• 401K with 100% match up to 6%.
• Fitness incentives and wellness perks.</p><p>If you’re passionate about data modeling and creating impactful metrics, we want to hear from you.</p><p>Apply here: <a href="https://kajabi.com/company/careers" rel="nofollow">https://kajabi.com/company/careers</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846884"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846884" href="https://news.ycombinator.com/vote?id=40846884&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Akkio | Fully Remote | Full-time | Frontend / Full Stack Engineers (Junior and Senior) | $120k-$180k + equity</p><p>Use Akkio to help digital marketing agencies build &amp; deploy AutoML pipelines, using Generative AI for data exploration and cleansing, and other marketing-specific feature sets</p><p>We're looking for solid and enthusiastic frontend/full stack engineers to help us build and scale our data platform. Vue.js, Node.js, TypeScript, JavaScript, Firebase</p><p>Must be authorized to work in the US and have timezone overlap with the US</p><p><a href="https://www.akkio.com/jobs/frontend-engineer" rel="nofollow">https://www.akkio.com/jobs/frontend-engineer</a></p><p>and</p><p><a href="https://www.akkio.com/jobs/senior-frontend-engineer" rel="nofollow">https://www.akkio.com/jobs/senior-frontend-engineer</a></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40847045"><td></td></tr>
                  <tr id="40847906"><td></td></tr>
                <tr id="40847942"><td></td></tr>
                  <tr id="40846435"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846435" href="https://news.ycombinator.com/vote?id=40846435&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>SmarterDx | 150-230k+ + equity + benefits | Remote (US only) | Multiple roles | <a href="https://smarterdx.com/careers" rel="nofollow">https://smarterdx.com/careers</a></p><p>We are a rapid growth health tech company using AI to improve hospital revenue cycle (making healthcare costs lower and allowing doctors to focus on patient care). The current team is very high functioning (MD + data scientist combos, former ASF board member, Google and Amazon engineers, Stanford LLM researchers, etc.) and initially scaled the company to $1MM+ in contracted revenue without raising capital.</p><p>We have been backed by top investors including Floodgate (Lyft, Twitch, Twitter), Transformation Capital, and Bessemer for a total of $71mil, including our $50mil Series B announced in May 2024, and are currently on pace to 20-30X in revenue over a two-year time period.</p><p>We are looking for:
Staff and Senior SWE  
- Senior and Non-Senior Data Engineers and Scientists  
- Engineering Managers (SWE + Data)  
- Sales  
- Several more roles</p><p>We have PMF, and it's time to scale! For more and to apply, see <a href="https://smarterdx.com/careers" rel="nofollow">https://smarterdx.com/careers</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847332"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847332" href="https://news.ycombinator.com/vote?id=40847332&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Hypertune (<a href="https://www.hypertune.com/" rel="nofollow">https://www.hypertune.com</a>) | ONSITE (London) | Senior Product Engineer</p><p>We're building the most flexible and developer-friendly platform for feature flags, A/B testing, analytics and app configuration. See <a href="https://docs.hypertune.com/" rel="nofollow">https://docs.hypertune.com</a> for more details.</p><p>At the core of Hypertune is a visual, functional, statically-typed programming language with first-class citizens for analytics, A/B testing and machine learning loops.</p><p>Our UI lets users define their configuration logic in this language and our SDKs act as interpreters, evaluating the logic locally with zero network latency.</p><p>As a senior product engineer, you'll work mainly in TypeScript and React.</p><p>If you're interested, please email miraan at hypertune dot com with your LinkedIn, CV or a link to a project you've built!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847159"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847159" href="https://news.ycombinator.com/vote?id=40847159&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Bonside | Senior Full Stack Engineer | New York City (NYC) | Onsite few days a week | <a href="https://bonside.com/" rel="nofollow">https://bonside.com/</a></p><p>Bonside is an NYC-based fintech startup providing financing to brick-and-mortar businesses (<a href="https://techcrunch.com/2023/06/07/bonside-growth-capital-brick-and-mortar-fintech/" rel="nofollow">https://techcrunch.com/2023/06/07/bonside-growth-capital-bri...</a>). We are looking to hire an experienced full-stack engineer to join as a founding member. Bonside has already deployed over 5 million dollars to brick-and-mortars in less than a year and is on track to more than double that in the coming months.</p><p>The team is currently 8 people and focused on hiring curious, ambitious, and thoughtful teammates. This opportunity presents a strong growth trajectory for a skilled engineer who will be instrumental in building delightful software, the architecting of systems, and the scaling of the engineering team. The company is at an exciting inflection point with a variety of interesting engineering challenges that include data engineering, an investment marketplace, and brick-and-mortar specific software. This role will work closely with our Head of Engineering (prev. Netflix).</p><p>Full job description: <a href="https://wellfound.com/l/2zfn1v" rel="nofollow">https://wellfound.com/l/2zfn1v</a></p><p>Tech Stack: Typescript, NextJS, tRPC, Tailwind, Node, Python, Prisma, Postgres, Vercel, Supabase, Retool, ClickHouse, Metabase</p><p>Feel free to apply on the site or email me at andrew@bonside.com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847404"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847404" href="https://news.ycombinator.com/vote?id=40847404&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Klara Systems |  ZFS Engineering Manager |  US Remote |  Full-time | $170k - $210k</p><p>We are a bootstrapped, high growth company looking for a Engineering Manager to help us scale more efficiently.</p><p>About us: We focus on software development services for open source and community-driven software. We are a dynamic, high growth startup with marquee customers whose names you will recognize. Customers look to us because of our intense focus on innovation and deep experience with open-source technologies.</p><p>You will be excited by the chance to build structure and define process to facilitate our growth.</p><p>Job details and applications: <a href="https://klarasystems.com/careers/zfs-engineering-manager/" rel="nofollow">https://klarasystems.com/careers/zfs-engineering-manager/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847106"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847106" href="https://news.ycombinator.com/vote?id=40847106&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Hatchet (<a href="https://hatchet.run/">https://hatchet.run</a>) | New York City (IN-PERSON) | Full-time</p><p>We're hiring a founding engineer to help us with development on our open-source, distributed task queue: <a href="https://github.com/hatchet-dev/hatchet">https://github.com/hatchet-dev/hatchet</a>.</p><p>We recently launched on HN, you can check out our launches here <a href="https://news.ycombinator.com/item?id=39643136">https://news.ycombinator.com/item?id=39643136</a> and here <a href="https://news.ycombinator.com/item?id=40810986">https://news.ycombinator.com/item?id=40810986</a>. We're two second-time YC founders in this for the long haul and just wrapped up the YC W24 batch.</p><p>As a founding engineer, you'll be responsible for contributing across the entire codebase. We'll compensate accordingly and with high equity. It's currently just the two founders + a part-time contractor. We're all technical and contribute code.</p><p>Stack: Typescript/React, Go and PostgreSQL.</p><p>To apply, email alexander [at] hatchet [dot] run, and include the following:</p><p>1. Tell us about something impressive you've built.</p><p>2. Ask a question or write a comment about the state of the project. For example: a file that stood out to you in the codebase, a Github issue or discussion that piqued your interest, a general comment on distributed systems/task queues, or why our code is bad and how you could improve it.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847111"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847111" href="https://news.ycombinator.com/vote?id=40847111&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Juniper (YC W21) | Senior Software Engineer, Data Engineer | NYC | Full-time | juniperplatform.com</p><p>Juniper operates at the messy financial infrastructure layer for US Healthcare. We’ve built an end-to-end insurance billing system for recurring care, starting with Autism clinics.
Healthcare in the United States runs on private and public insurance billing, but providers spend huge amounts of time on faxes, phone calls, and error-prone legacy systems to get paid. That’s not why they got into care.</p><p>What’s worse, these systems aren’t hugely successful—clinics regularly get back 80% what they know the insurance company should pay. Those missing dollars could be used to hire more care providers to help more kids, but instead line insurance companies’ pockets.</p><p>We’re changing that.</p><p>At Juniper, we’ve built an end-to-end billing system. It starts with ingesting from a clinic’s EHR, then create, validates, and submits claims to insurance providers across the country. If claims need corrections or appeals, most of the time we can handle those automatically or our CX and Operations team use our in-house internal tools. We also handle patient invoicing for co-pays, co-insurance, and deductibles (we never send anything to collections.)</p><p>Our typical paid rate is above 95%.</p><p>We are a team of 25 with strong product market fit—we’ve had to push out onboardings because engineering can’t build quickly enough.</p><p>You’ll be working with an engineering leadership team from AWS and Stripe to get doctors and clinicians back to work helping kids.</p><p>Interested in fixing American healthcare? Email josh.paul@juniperplatform.com to chat</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847077"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847077" href="https://news.ycombinator.com/vote?id=40847077&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>3box labs | Sr. Platform Engineer | Rust, Terraform, GCP &amp; AWS | Remote ("worldwide" but willing to overlap with NYC work hours) | Full-time</p><p>At 3Box Labs we are on a mission to usher in a new era for the web, where data is secure, interactions are trustworthy, and relationships are the basis of connection. We're enabling online experiences that are delightful and integrated while also bolstering privacy and freeing innovation. Our first product, Ceramic, is the building block for composable data on the web and is powering thousands of the world's most ambitious applications.</p><p>We're backed by an incredible community and the best investors in the space (USV, Placeholder, Variant, Multicoin) who have deep conviction in our mission.  We are a lean, voraciously curious team from across the globe, with 5 years of expert remote work experience and frequent (and awesome) team retreats to spend time together. We have founded tech startups, written books, won product awards, authored patents, created Ethereum standards, and advised F100 CEOs.</p><p>Come help us tackle novel challenges and reinvent how data is managed online. Every one of our roles is remote first (retreat often!). We are committed to building a diverse and inclusive team because we cannot succeed in our mission without it. People that identify with groups traditionally underrepresented in tech are particularly encouraged to apply.</p><p>Apply at <a href="https://jobs.lever.co/3box/d2709760-cceb-4a16-badc-8e95569b6951" rel="nofollow">https://jobs.lever.co/3box/d2709760-cceb-4a16-badc-8e95569b6...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847243"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847243" href="https://news.ycombinator.com/vote?id=40847243&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Senior Full-Stack Software Engineer</p><p>Location: Toronto</p><p>Employment: Full-time, Hybrid</p><p>Job Summary</p><p>You will design and implement solutions that process and analyze financial data streams, contributing to the operational back and user-facing front end.</p><p>Key Responsibilities</p><p>You will be a crucial player in our engineering team, handling everything from data engineering to DevOps, back-end to front-end.</p><p>- Lead diverse development tasks, including system architecture, data engineering, and full-stack development.</p><p>- Build and deploy scalable systems tailored for data-driven</p><p>- Direct DevOps initiatives utilizing tools such as Docker and Terraform.</p><p>Who You Are</p><p>You're a builder at heart, driven by the challenge of solving complex problems. You excel in a fast-paced environment, equipped with a deep toolkit of software development skills. Ideally, you bring:</p><p>- A Bachelor’s or Master’s degree in Computer Science or a related field.</p><p>- More than 10 years of hands-on experience across the full-stack spectrum, DevOps, and cloud.</p><p>- A passion for crafting products from scratch and relentlessly pursuing system optimization.</p><p>Apply by sending your resume and a cover letter to charlie@jamlabs.com.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847263"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847263" href="https://news.ycombinator.com/vote?id=40847263&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Vannevar Labs | REMOTE-FIRST | FULL-TIME | Offices in DC and NYC</p><p>Vannevar Labs builds next generation defense software for the public servants keeping our country safe. As a team, we exist because we believe in public service, and we think that our democracy and government improve only if we put serious, collective effort into improving them, including the technology our government uses. We build software to help the the US deter and deescalate conflict. We are a profitable growth startup with some of the best defense investors in the world, including General Catalyst, DFJ Growth, Point72, and enterprise tech investors Costanoa and Felicis.</p><p>We're looking for engineers to lead the build out of our core AUTHZ + AUTHN platform, amongst other roles. Apply on our website:</p><p><a href="https://boards.greenhouse.io/vannevarlabs" rel="nofollow">https://boards.greenhouse.io/vannevarlabs</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847316"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847316" href="https://news.ycombinator.com/vote?id=40847316&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Resemble AI | San Francisco Bay Area (office in Mountain View, CA) | Full-Time | Full-Stack Engineer</p><p>We're creating state of the art Generative Voice AI and Deepfake Detection models. Looking for a few engineering roles to fill.</p><p>Deep Learning Engineering Manager - Experience managing a research unit, PyTorch, experience with Speech models a bonus.
Full Stack Engineer - Python, Ruby on Rails, Typescript (React), bonus if you have experience with Next JS
Machine Learning Engineer (Inference and Production) - PyTorch, ML Deployment tools, Cloud Platforms, Knowledge of containerization and orchestration technologies (Docker, Kubernetes), bonus if you've had experience with ONNX or TensorRT.</p><p>If interested, reach out directly to me: zohaib [at] resemble.ai</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847322"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847322" href="https://news.ycombinator.com/vote?id=40847322&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Monad Labs | NYC, Hybrid 3-days On-Site (Relocation Package included)</p><p>Monad Labs just raised $225M in series A funding to scale our team in bringing the Monad blockchain to production this year.</p><p>Founded by Jump Trading alums, Monad Labs is a tech startup using low-latency programming, compiler optimization, and multithreaded computing to build an ultra-high-performance smart contract platform.</p><p>We are hiring for the following:
Senior Software Engineer (does require blockchain or crypto experience): <a href="https://grnh.se/f33a39005us" rel="nofollow">https://grnh.se/f33a39005us</a></p><p>Senior Devops Engineer:
<a href="https://grnh.se/876639bd5us" rel="nofollow">https://grnh.se/876639bd5us</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847461"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847461" href="https://news.ycombinator.com/vote?id=40847461&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Spellbrush | Anime AI researcher, Software Engineer, or Unity Engineer | San Francisco OR Tokyo | Sponsors Visas</p><p>Spellbrush (YCW18) builds state-of-the-art AI foundational models for anime and video games.</p><p>We probably spends 10x more compute on anime than anyone else in the industry - we think our results over at nijijourney.com speak for themselves.</p><p>Looking to both expand our game development team and our AI research and data processing teams.</p><p>JAX and TPU experiences are a plus.</p><p>Otaku looking for a job in AI are welcome to ping over name of best waifu or husband to jobs@spellbrush.com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846946"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846946" href="https://news.ycombinator.com/vote?id=40846946&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Andela | Multiple engineering roles | Remote |  Full-time | $80k-$120k USD + equity</p><p>Andela exists to ensure technologists have access to equal opportunity regardless of where they live. Our digital talent solve complex and compelling problems, changing their career trajectory and quality of life, while companies can craft a workforce that represents the world around them with lower costs, faster speed, and greater flexibility</p><p>We are a remote only company and have multiple positions open in our Engineering team. These positions are internal, not serving our clients, but working on the products that helps us achieve that goal.</p><p>Please mention HackerNews in the application. Some positions state a specific geographical location (LATAM), but we are open for people from anywhere as long as they can overlap their work hours with CST/EST.</p><p>Technical Leader (VueJS/Ruby) - <a href="https://andela.wd1.myworkdayjobs.com/en-US/External/job/Brazil/Staff-Software-Engineer_JR100505" rel="nofollow">https://andela.wd1.myworkdayjobs.com/en-US/External/job/Braz...</a></p><p>Staff Engineer (NodeJS) - <a href="https://andela.wd1.myworkdayjobs.com/en-US/External/job/LATAM/Staff-Engineer_JR100518" rel="nofollow">https://andela.wd1.myworkdayjobs.com/en-US/External/job/LATA...</a></p><p>Senior Backend Engineer (NodeJS) - <a href="https://andela.wd1.myworkdayjobs.com/en-US/External/job/LATAM/Senior-Backend-Engineer_JR100527" rel="nofollow">https://andela.wd1.myworkdayjobs.com/en-US/External/job/LATA...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846823"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846823" href="https://news.ycombinator.com/vote?id=40846823&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>RINSE | REMOTE or San Francisco, Los Angeles, Chicago, Boston, New York, New Jersey, Seattle, Austin, Dallas, or Washington DC | Software Engineer | Full-Time | <a href="https://www.rinse.com/" rel="nofollow">https://www.rinse.com</a></p><p>Rinse provides dry cleaning and laundry delivery services to customers in nine metropolitan areas in the US. We have sophisticated logistics optimization software, a polished consumer product, and firm business fundamentals. We're now almost a decade old - this is a stable, yet consistently growing and innovating, company.</p><p>Our engineering team is distributed across the United States and internationally, and has been entirely remote for years now, but a desk can be provided in the above cities if you'd prefer.</p><p>We're open to both newly-graduated engineers or more senior engineers, provided they meet our bar.  We're particularly interested in true full-stack engineers, with strong back-end fundamentals in a Python / Django environment alongside React / Typescript familiarity.</p><p>Search term bingo: Logistics, Django, Python, Optimization, React, React Native, Postgres, Mobile Engineer</p><p><a href="https://www.rinse.com/careers/software-engineer/" rel="nofollow">https://www.rinse.com/careers/software-engineer/</a></p><p>Interested? Email us as jobs@rinse.com, or my first name at rinse.com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847468"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847468" href="https://news.ycombinator.com/vote?id=40847468&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>GCI (<a href="https://www.gci.com/" rel="nofollow">https://www.gci.com</a>) | Anchorage, AK / REMOTE | Senior Data Engineer</p><p>We're looking for experienced Data Engineers familiar with working in Azure and Databricks. We build pipelines to ingest data for our analysts and data scientists and integrate with other platforms. We also take care of the platform and work to improve it for all teams.</p><p>IT Data Engineer IV: <a href="https://edqv.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX/job/21002483/?utm_medium=jobshare" rel="nofollow">https://edqv.fa.us2.oraclecloud.com/hcmUI/CandidateExperienc...</a></p><p>Senior IT Data Engineer: <a href="https://edqv.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX/job/21002482/?utm_medium=jobshare" rel="nofollow">https://edqv.fa.us2.oraclecloud.com/hcmUI/CandidateExperienc...</a></p><p>About GCI: Headquartered in Alaska with additional locations throughout the U.S., GCI has worked for more than 40 years to deliver communication and technology services to some of the most remote communities and in some of the most challenging conditions in North America. GCI is a pioneer in its field, bringing telemedicine and online education capabilities to communities across the state and continuing efforts to connect the Arctic globally as well as providing strong services to consumer and business markets. GCI’s introduction of 1 GIG internet speeds in the state as well as its innovative partnership with Apple are among the countless ways the company has transformed communication and quality of life for Alaskans.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846864"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846864" href="https://news.ycombinator.com/vote?id=40846864&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Trunk | <a href="https://trunk.io/" rel="nofollow">https://trunk.io</a> | Sr Back End Eng /Sr Infra Eng| Sr Analytics Data Eng | Sr Product Designer | Technical SDR | Full-Time | Hybrid SF or Remote US or Canada</p><p>Trunk is an a16z funded dev tools startup, redefining software development at scale. We aim to solve problems that developers hate by bringing the tools usually built in-house at the best engineering orgs to every development team. We've built three products so far and have plans for more:</p><pre><code>  * Trunk Check: a universal linter/formatter, available as a CLI, VSCode extension, and CI check;
  * Trunk Merge: a merge queue, to ensure that PRs are tested in order before they're merged; and
  * Trunk Test Analytics: detects, quarantines, and eliminates flaky tests from your code base. Prevents flakey tests from producing noise and blocking CI.
</code></pre><p>
In 2022, we raised a $25M Series A led by Initialized Capital (Garry Tan) and a16z (Peter Levine).</p><p>Our tech stack:</p><pre><code>  * Frontend: Typescript, React, Redux, Next.js
  * Backend: Typescript, Node, AWS, CDK, k8s, gRPC
  * Observability: Prometheus, Grafana, Kiali, Jaeger
  * CLI: C++20, Bazel
  * VSCode Extension: Typescript
  * CI/CD: GitHub Actions
  * General: GitHub, Slack, Linear, Slite
</code></pre><p>
Unlimited PTO (and we all actually take PTO), competitive salary and equity packages! Please apply here: <a href="https://trunk.io/jobs" rel="nofollow">https://trunk.io/jobs</a></p><p><a href="https://trunk.io/blog/how-we-crack-the-coding-interview" rel="nofollow">https://trunk.io/blog/how-we-crack-the-coding-interview</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846905"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846905" href="https://news.ycombinator.com/vote?id=40846905&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Lune | Software Engineers, Sales | London or REMOTE (London +/- 2 hours) | Full-Time | <a href="https://lune.co/" rel="nofollow">https://lune.co</a></p><p>Lune’s mission is to make every product and service climate-positive by default.</p><p>With the Lune API, we enable companies to seamlessly build emissions calculations and high-quality carbon removal into their product and services and make it part of the customer experience. In the future, everything we do will have a positive impact on the planet – powered by Lune.</p><p>If you’re a talented engineer or sales professional who learns quickly and cares about tackling the climate crisis, we’d love to work with you!</p><p>These roles are a unique opportunity to be part of Lune’s core team, to have a real impact on our mission, and to define and scale the company into the future.</p><p>As we grow, you’ll have the opportunity to take on new responsibilities and help build a great company while tackling the greatest challenge of our time.</p><p>Interested?</p><p>Tech stack: TypeScript, React, Nextjs, AWS, PostgreSQL, Terraform, Kubernetes.</p><p>Hiring for:</p><p>- Senior "bar raisers" Frontend Software Engineers</p><p>- Account executive - logistics</p><p><a href="https://lune.co/join-us" rel="nofollow">https://lune.co/join-us</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847000"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847000" href="https://news.ycombinator.com/vote?id=40847000&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>NeuroVRD | Remote | FTE/PT | USA</p><p>NeuroVRD is developing an intraoperative monitoring platform that automatically and continuously monitors the functional integrity of neurovascular structures that are at surgical risk during high-risk surgery, improving surgical outcomes and maintaining the patient's quality of life.</p><p>Our transformative approach to intraoperative monitoring increases access and availability, improves reliability, and significantly reduces costs.</p><p>In the last few years, we've developed a real time automated signal processing platform, and are working with world-class surgeons at Hopkins and Columbia towards clinical studies to demonstrate the efficacy of our platform.</p><p>We are looking for an ML/AI Scientist/Engineer to lead our signal and gesture classification efforts. Our interview process is short and to-the-point -- no take-home projects, etc. Let your prior work speak for itself. We're looking to hire someone who can start ASAP.</p><p>To get started, please email jmeo@neurovrd.com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847250"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847250" href="https://news.ycombinator.com/vote?id=40847250&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Exa | San Francisco | In person | Full time | $130K-180K</p><p>Jeff, cofounder of Exa here. LLMs represent a brand new opportunity to organize humanity's knowledge, in a way that hasn't been done before. We're an AI research lab focused on AI-powered search algorithms (using embeddings), currently applied to vast swaths of the web (we make our money as a search API).</p><p>We're hiring pretty broadly across engineering - AI research, high performance Rust (e.g., we build an in-house vector DB), and full stack. If the mission of organizing the Internet motivates you, it's a good fit :)</p><p><a href="https://exa.ai/careers">https://exa.ai/careers</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847246"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847246" href="https://news.ycombinator.com/vote?id=40847246&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Bruinen | bruinen.co | (Senior) Software Eng | in-person / remote San Francisco</p><p>Bruinen is working on infrastructure technology for edge environments - we're starting with the database. Our database, Delta, is local-first and distributed. Each device (AV, robot, drone, sensor) can continue working without connectivity, and merge it's changes (conflict free) whenever it comes back online.</p><p>We target use-cases in the harshest environments - where connectivity is expected to drop: battlefields, at sea, in the air, and in the wilderness.</p><p>We're written completely in rust (though its not a requirement for this position, come learn!).</p><p>If interested, email me at tevon at bruinen dot co</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847083"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847083" href="https://news.ycombinator.com/vote?id=40847083&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Snout <a href="https://www.snoutplans.com/" rel="nofollow">https://www.snoutplans.com/</a> | Principal Software Engineer, Product Manager, Product Marketing | Remote US | Full Time</p><p>Join us at Snout on our mission to ensure no one ever has to make a health decision for their pet based on the cash in their back account. Snout plans pay for 100% of routine veterinary care, unlimited visits, and additional member benefits - think pet insurance, that you will actually use every year.</p><p>Principal Software Engineer -</p><p>You will be working on our core wellness product, launched late-2022. Work includes the addition of new wellness plan features and capabilities, enhancement of user experience, testing and operation of the wellness platform, and special projects that arise from time to time. We frequently work cross-functionally and you can expect to write code and perform technical operations for our marketing, sales, and support efforts, as well.</p><p>Our tech stack includes Node.js, React, PostgreSQL, AWS, and Tailwind. We use both JavaScript and TypeScript heavily.</p><p>To apply for the engineering role, please email kyle@snoutplans.com.</p><p>Our typical process is a one hour technical interview followed by a casual 30 minute meeting with our whole team.</p><p>We're a small startup team consisting of fewer than ten people. Successful team members are comfortable participating in spirited and detailed debates to establish product and technical plans, and then taking initiative and ownership to deliver on those plans quickly and effectively. Our team is collaborative. You can expect to meet with the team on a daily basis, pair regularly, and participate in slack discussions throughout the day. We keep pacific work hours and expect the team to be available during 9a-5p at a minimum. If you're looking for a team where you can carve out your area of responsibility, work with experienced partners who have your back, grow your role alongside the growth of the company, and take a product to the moon, then we should talk.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847160"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847160" href="https://news.ycombinator.com/vote?id=40847160&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Shield.ai (<a href="https://shield.ai/" rel="nofollow">https://shield.ai</a>) | San Diego, Texas (ONSITE mostly, some REMOTE positions) | All Full-time</p><p>We're hiring a people/tech manager for our planning and controls stack, in particular.</p><p>We also have a huge number of open roles across software, foundations, cloud, embedded, and autonomy for software engineers.</p><p>Link for SD jobs:
<a href="https://jobs.lever.co/shieldai?location=San%20Diego%20Metro%20Area" rel="nofollow">https://jobs.lever.co/shieldai?location=San%20Diego%20Metro%...</a></p><p>Shield is a well funded deftech startup experiencing rapid growth in their autonomy software and medium/large segment aircraft sales.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847006"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847006" href="https://news.ycombinator.com/vote?id=40847006&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Bright Network | Python Engineer | Edinburgh, UK | Hybrid | Full-time</p><p>At Bright Network, we use modern technology and data science to connect ambitious young people from all backgrounds with the best career opportunities. We currently have 1M+ members and 300+ top-tier graduate employers.</p><p>We're currently looking for a Python Engineer to work on our Django-based member websites. See <a href="https://boards.greenhouse.io/brightnetwork/jobs/5191225004" rel="nofollow">https://boards.greenhouse.io/brightnetwork/jobs/5191225004</a> for more info.</p><p>You can expect:</p><p>* To work alongside people who are really proud of the products they're building - putting our customers first</p><p>* To work in a collaborative, ego-free &amp; supportive team</p><p>* To have time in each sprint to focus on your own L&amp;D to help you grow</p><p>Please get in touch with me if you're interested or have any questions — email in my profile!</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40847218"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40847218" href="https://news.ycombinator.com/vote?id=40847218&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>&gt; If you are uncomfortable answering any or all of the questions, then please click the option "prefer not to say".</p><p>You're missing the option not to say in one question, and the option "Not sure/don't know" in several.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40847326"><td></td></tr>
            <tr id="40846806"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846806" href="https://news.ycombinator.com/vote?id=40846806&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Happy Scribe | Engineer, Designer, ML | Onsite &amp; Hybrid (Barcelona, Spain) | Full-time</p><p>Hi there, Happy Scribe founder here. At Happy Scribe we think audio &amp; video should be universally accessible. Today’s state of the art is agencies doing everything manually. We want to scale high-quality subtitles &amp; dubbing with a multi-player editing experience that combines sota ai with a global marketplace of proofreaders. Being product-led we have the datasets needed to get there. It’s the Tesla autopilot play in the language service industry. If we succeed we’ll live in a world where everyone can consume any content in their native language without degrading the experience.</p><p>A team of 5 engineers and 0$ vc money built the product, used by +350k/mo people with high 7-digit revenue. We inhabit a beautiful 4-story designer office in the Gràcia neighbourhood.</p><p>Here is our careers page <a href="https://www.happyscribe.com/careers" rel="nofollow">https://www.happyscribe.com/careers</a>. We sponsor visas.</p><p>We hire only builders and kind people :)</p><p>Email me directly at “marc@happyscribe.co”</p><p>Cheers!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846789"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846789" href="https://news.ycombinator.com/vote?id=40846789&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Empower Delivery | Engineering | Remote (US time zones) | Full-time</p><p>We're a restaurant tech company on a mission to make restaurants better for everyone. We offer a full end-to-end product that allows customers to run their own online ordering, process the orders using our kitchen suite and deliver the orders using their own dedicated fleet of drivers. We strive to give restaurants back the control they've lost to the third party middlemen.</p><p>We run a monorepo Elixir/Phoenix LV codebase backed by Postgres. We maintain a couple native apps that leverage KMM and SwiftUI/Jetpack.</p><p>We're a small remote-first company based out of Indianapolis, IN.</p><p>We're seeking senior level talent to join our team. You will work across a wide problem space in a pretty fun domain.</p><p>Elixir experience a plus, but not required.</p><p>Positions:</p><pre><code>  - Senior Engineer - Full-Stack Elixir/LV
  
  - Mobile Engineer - KMM / iOS / Android
</code></pre><p>
If you are interested please reach out to me at dan 'at' empower.delivery.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846451"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846451" href="https://news.ycombinator.com/vote?id=40846451&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Meaningful Gigs | REMOTE (US based) | Full-time | Founding/Sr. ML Engineer | Senior Backend Engineer | Senior Frontend Engineer</p><p>Meaningful Gigs began as a marketplace for creative talent &amp; evolved to build AI-driven creative workflows. We are on a mission to advance human creativity by eliminating mundane tasks that stifle innovation and transform the $162 billion creative market.</p><p>Help us shape the future of creativity by quietly rolling out a new product, separate from our creative marketplace, which helps speed up creative tasks, think ‘Grammarly for creative workflows’. We have various AI powered features that automate repetitive creative tasks.</p><p>We are an innovative and fast-moving, seed-stage startup of 16 people, (10 in product) prepping for our Series-A and growing quickly.</p><p>Tech Stack: Javascript/Typescript, React, NodeJS, AWS SDK, Serverless, Express 
ML: Python, Tensorflow, Pytorch, Scikit-learn
Check out our open positions at: <a href="https://boards.greenhouse.io/meaningfulgigs" rel="nofollow">https://boards.greenhouse.io/meaningfulgigs</a>
Contact our Head of Talent directly at Christina(at)meaningfulgigs.com or on LI: <a href="https://www.linkedin.com/in/christina-kayler/" rel="nofollow">https://www.linkedin.com/in/christina-kayler/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846642"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846642" href="https://news.ycombinator.com/vote?id=40846642&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Continua AI | ML and Systems Engineers | NYC, SF Bay Area, Seattle | <a href="http://continua.ai/" rel="nofollow">http://continua.ai</a></p><p>Continua was founded in April 2023 by a Distinguished Engineer from Google Research to give everyone in the world an agent that uses their real-time context and personal memory to deliver actionable and timely assistance.
Our MVP is currently in limited alpha testing, and we’re immensely excited by the feedback we’re getting from early users - they’re finding that the Continua agent helps them in all manner of surprisingly useful ways.</p><p>This is an opportunity to be a part of setting the product and technical direction of this early stage company! ML engineers at Continua work on new approaches for knowledge representation, multimodality, and personalization under computational resource constraints. Systems engineers at Continua work on a wide variety of secure and reliable backend services and data pipelines to power our products.</p><p>We’re backed by tier-1 VCs, and we’ve already assembled a team of seven engineers with backgrounds at Google, Slack, CourseHero, Stash, and Amazon. We have a limited number of additional spots available on the team for engineers who are excited about building innovative products that leverage the latest advances in ML, including Transformers / Mamba, RLHF, LoRA/QLoRA, at-edge (on-device) training and inference, privacy-first ML, and more. If that sounds like you, we would love to hear from you.</p><p>If you are passionate about building innovative new products, and you’re eager to work in a dynamic startup environment, we would love to hear from you. Please apply via  <a href="https://www.continua.ai/careers" rel="nofollow">https://www.continua.ai/careers</a>, and let's embark on this exciting journey together!</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40847436"><td></td></tr>
            <tr id="40846763"><td></td></tr>
                <tr id="40847171"><td></td></tr>
                        <tr id="40846475"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846475" href="https://news.ycombinator.com/vote?id=40846475&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>PlantingSpace | Full-time | Remote (EU time zone) with quarterly gatherings | <a href="https://planting.space/" rel="nofollow">https://planting.space</a></p><p>We are building an AI system that can accurately represent knowledge and handle uncertainty, to enable the discovery of insights and solve problems based on explainable reasoning. Our technology is not based on a GPT framework but on a novel approach to structured knowledge representation. We envision applications to automate analysis and speed up research in domains such as Finance, Strategy Consulting, Engineering, Material Sciences, and more.</p><p>We are continuously looking for strong software engineers who are up for a challenge and a steep learning curve. You’ll be exposed to cutting edge research in Bayesian statistics, dynamical systems, information theory, symbolic computing, and more.</p><p>Current openings at PlantingSpace:</p><p>- Senior NLP Engineer: build bridges between neural and symbolic representations within our system.</p><p>- Software Engineer: contributing to core system development, production backend engineering, implementing and analysing algorithms.</p><p>- Applied Category Theorist (Knowledge Representation): research category theoretic representations of real world phenomena to inspire our development.</p><p>To see a full list of openings, and to apply, check out our Join Us page: <a href="https://planting.space/joinus/" rel="nofollow">https://planting.space/joinus/</a>
We’re excited to also share some example tasks that will give you a taste of the work we do here: <a href="https://planting.space/examples/" rel="nofollow">https://planting.space/examples/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847517"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847517" href="https://news.ycombinator.com/vote?id=40847517&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Levels.fyi | Senior React Native Frontend Engineer | Remote (India, Open to other countries) | Full-time | <a href="https://www.levels.fyi/" rel="nofollow">https://www.levels.fyi</a></p><p>Levels.fyi's mission is to help every professional build a better career through the most accurate insights and services. We're building the future of compensation &amp; hiring by centering ourselves around professionals.</p><p>You'll be joining a close-knit team of 2 engineers to work across our product verticals. You'll have the opportunity to lead and own new projects / initiatives from idea to production end-to-end (architecture to deployment). We move fast and have come an incredibly long way on a tiny team. We're looking for a self-starter and resourceful engineer with strong communication skills and experience building things from scratch.</p><p>Apply at: <a href="https://docs.google.com/forms/d/e/1FAIpQLSf8CxjgiSYo7Po2xrDX_Fr9oRRZ4o_nW70RV_h3nHmvxg4uHQ/viewform?entry.1790223576=__other_option__&amp;entry.1790223576.other_option_response=Hacker+News" rel="nofollow">https://docs.google.com/forms/d/e/1FAIpQLSf8CxjgiSYo7Po2xrDX...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847032"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847032" href="https://news.ycombinator.com/vote?id=40847032&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Health Partners Group | Fully Remote (UK ONLY, NO VISAS) | Tech Leads, Developers, Data Analysts &amp; QA Lead</p><p>Join us in shaping the future of occupational health, mental health and employee wellbeing services. At Health Partners Group, we stand at the forefront of occupational health, merging expert advice, clinical excellence, and cutting-edge technology. Our mission? To forge health programmes that not only enhance wellbeing but also boost performance across workforces.</p><p>Who You Are</p><p>You're passionate about leveraging technology to transform healthcare. Your drive is matched by your technical expertise and you're looking to make a real impact within a dynamic team. You understand that your role is more than a job; it's about enhancing the lives of millions.</p><p>Roles We’re Hiring For:</p><p>Tech Lead’s to lead squads of six product, development and QA engineers.
Senior .Net Developers to join one of our squads to work with product and QA engineers to deliver key features and functionality.
.Net Developers to join one of our squads to work with product and QA engineers to deliver key features and functionality.
Data Analysts to join our MI &amp; Data team who use Power BI, SQL Server and various tooling to generate the insight that we provide our clients.
Automation QA Lead to manage the QA team on a day-to-day basis while being an individual contributor, guiding the development of our automation test framework.</p><p>What We Offer:</p><p>A collaborative environment that thrives on innovation and new ideas.
A competitive salary with a comprehensive benefits package.
Opportunities for personal and professional growth.
A flexible working policy to support your work-life balance.</p><p>How to Apply:</p><p>Send your CV, along with a cover letter that tells us your story and why you're the perfect fit for our team, to itrecruitment@healthpartnersgroup.com. Please specify the role you're applying for in the subject line and confirm you have the right to work in the UK. We do not provide sponsorship currently.</p><p>Health Partners Group is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847424"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847424" href="https://news.ycombinator.com/vote?id=40847424&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Framer (<a href="https://framer.com/" rel="nofollow">https://framer.com</a>) | Remote (EU) | Senior+ Product Engineers</p><p>The best professional web builder for your entire dotcom. Used by Perplexity, Twingate, ByteDance, Superhuman and many more. TypeScript / React, Go / AWS.</p><p>koen at framer dot com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846431"><td></td></tr>
            <tr id="40846478"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846478" href="https://news.ycombinator.com/vote?id=40846478&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Beacon AI | <a href="https://beaconai.co/" rel="nofollow">https://beaconai.co</a> | Full-Time | SF Bay Area Hybrid</p><p>Beacon AI builds R2-D2 for pilots and an aircraft operator data platform. We use AI and technology to augment pilots to help them fly safer and more efficiently. Our pilot assistant, Murdock, and Lighthouse platforms are designed for commercial and defense aviation operations.</p><p>** We recently secured additional investments and contracts and anticipate growing our team about 25-30 new team members over the next year! **</p><p>Our team has unique, directly relevant industry expertise in aviation, AI, and autonomy. Join a growing engineering team supported by a world-class advisor team, talented developers, and amazing investors, including Sam Altman, Scout Ventures, JetBlue Ventures, Zach Perret (Plaid), Countdown Capital, and many others. Learn (a little bit) more at <a href="https://www.beaconai.co/" rel="nofollow">https://www.beaconai.co</a></p><p>*We do not work with outside talent agencies. Please do not message from one, thank you.*</p><p>We are looking for Senior I, Senior II, Staff, Tech Lead, or Engineering Manager level candidates in the following:</p><p>* Advanced Pilot Assistant Software (C++, Python, AI/Autonomy/Flight Software/Feature Engineer)
This role builds the brains for Murdock.
* iOS Software 
This role builds the primary pilot interface for Murdock
* Senior React Web Application Engineer
This role builds our web application, Lighthouse. Preferably full-stack (AWS infra).
* Cloud Infrastructure Software
* Technical Sourcer &amp; Recruiter</p><p>Apply directly at <a href="https://beaconai.co/careers" rel="nofollow">https://beaconai.co/careers</a> and mention that you found this post on HN!</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846730"><td></td></tr>
            <tr id="40846595"><td></td></tr>
            <tr id="40846520"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846520" href="https://news.ycombinator.com/vote?id=40846520&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Aha! (<a href="https://www.aha.io/" rel="nofollow">https://www.aha.io</a>) | Rails / React | REMOTE</p><p>Aha! is the #1 tool for product managers to plan strategy and roadmaps. We serve more than a million users worldwide. We are looking for:</p><p>* Experienced full-stack Rails and security engineers to work on the Aha! product. Our application is built in Ruby on Rails, with React on the frontend for rich client-side experiences.</p><p>Aha! is profitable, you can work from anywhere in North America, South America or New Zealand, and we offer excellent benefits. We use our own product to manage our work (which is especially rewarding) and we deploy continuously.</p><p>Our entire team has always been 100% remote - in North American timezones so we can collaborate during the work day.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40847772"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40847772" href="https://news.ycombinator.com/vote?id=40847772&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>I have seen your security engineer job ad consistently up since I was looking for my last job in 2021. Loking through my linkedin  'top job picks for you' I see this job role first come up June 15th, 2020.</p><p>How can you not have found a someone since then, or does everyone quit? I had seen this ad so often I was starting to suspect it was part of a information gathering scheme for information security professionals.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40846559"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846559" href="https://news.ycombinator.com/vote?id=40846559&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Tracebit | <a href="https://tracebit.com/" rel="nofollow">https://tracebit.com</a> | Multiple roles | London, UK | Full-Time | On-site (5 days)</p><p>Tracebit lets security teams implement ‘assume breach’ with automated cloud based honeypots or canaries.</p><p>Off the back of a successful seed fundraise from tier 1 VCs, we are actively hiring for smart people who get things done in the following positions:</p><pre><code>   - Founding Engineer | £70-100k + equity | https://tracebit.com/jobs/founding-engineer 
</code></pre><p>
On-site roles (5 days a week) in Central London.</p><p>Learn more and apply: <a href="https://tracebit.com/careers" rel="nofollow">https://tracebit.com/careers</a></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40847124"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_40847124" href="https://news.ycombinator.com/vote?id=40847124&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>&gt;We think 9am-6pm will bring a great cadence to work</p><p>Any reason why you think this given that studies show knowledge workers can't be productive over that long a period?</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="40847756"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847756" href="https://news.ycombinator.com/vote?id=40847756&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Timeline | London, United Kingdom | Senior Full Stack Engineer (full-time, contract) | Remote</p><p>We're looking for experienced Senior Software engineers! TL;DR: Our tech stack is Elixir, Elm and Rust. We are a rapidly growing company (100% YoY since launch). We work remotely. We work asynchronously and in four-week cycles using the Shape Up methodology.
About Timeline: We are disrupting the financial planning and portfolio management software for advisers in the UK. We manage over £6B of clients assets on behalf of financial advisers.</p><p>Apply here: <a href="https://timelineapp.freshteam.com/jobs/bmZYCi_Breyu/full-stack-senior-software-engineer-remote" rel="nofollow">https://timelineapp.freshteam.com/jobs/bmZYCi_Breyu/full-sta...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846701"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846701" href="https://news.ycombinator.com/vote?id=40846701&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Venture Global LNG | Data Scientist, Data Engineer | Arlington, VA ONSITE | VISA</p><p>Venture Global LNG is a natural gas company. Primarily, we liquify natural gas for export, but are expanding into other parts of the industry.</p><p>Our data analytics team is like a startup: informal, small, growing rapidly. We need 2 DS and 2 DE who know Spark and can autonomously design and execute complex work for high-level stakeholders.</p><p>Requirements for both:
- 5 years professional, non-academic experience
- Excellent communications
- Strong Spark skills
- Technical skills and ambition well above the average "good" candidate</p><p>Go to our jobs page and scroll to Information Technology / Data Engineer or Data Scientist to apply.</p><p>Direct links:
<a href="https://venturegloballng.com/careers/?gh_jid=7470846002" rel="nofollow">https://venturegloballng.com/careers/?gh_jid=7470846002</a>
<a href="https://venturegloballng.com/careers/?gh_jid=7470493002" rel="nofollow">https://venturegloballng.com/careers/?gh_jid=7470493002</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846619"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846619" href="https://news.ycombinator.com/vote?id=40846619&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>40GRID - Full-time Remote | Senior Angular Frontend Engineer</p><p>Our mission is to empower field-service companies to grow by modernizing and automating their business operations. Every company we work with has unrealized potentials — our task is to build the platform that empowers growth and helps them unlock opportunities.</p><p>Email: jobs [at] 40grid.com (no recruiters or agencies, thank you).</p><p>Tech: Django / Python | Angular (Typescript)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846990"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846990" href="https://news.ycombinator.com/vote?id=40846990&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>bloop (YC S21) | Product Engineer (Rust, AI) | Onsite London, UK</p><p>We're combining LLMs and transpilers to translate COBOL into Java. If you’re getting into (or already work in) Rust, and have a strong product mindset, this role could be a great fit. Come and build LLM pipelines and agents, that help the largest companies modernise their legacy codebases.</p><p>Please email join [at] bloop [dot] ai with "HN Product Engineer" in the subject line.</p><p>More details - <a href="https://www.ycombinator.com/companies/bloop/jobs/iCrEllp-product-engineer-rust-ai">https://www.ycombinator.com/companies/bloop/jobs/iCrEllp-pro...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846695"><td></td></tr>
            <tr id="40846728"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846728" href="https://news.ycombinator.com/vote?id=40846728&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Spacelift | Remote | Europe | Full-time | Senior Software Engineer | $80k-$120k</p><p>We're a VC-funded startup building an automation platform for Infrastructure-as-Code, adding a Policy-as-Code layer above it, in order to make IaC usable in bigger companies, where you have to take care of state consistency, selective permissions, a usable git flow, etc.</p><p>On the backend we're using 100% Go with AWS primitives. We're looking for backend developers who like doing DevOps'y stuff sometimes (because in a way it's the spirit of our company), or have experience with the cloud native ecosystem. Ideally you'd have experience working with an IaC tool, i.e. Terraform, Pulumi, Ansible, CloudFormation, Kubernetes, or SaltStack.</p><p>Overall we have a deeply technical product, trying to build something customers love to use, and already have a lot of happy and satisfied customers. We promise interesting work, the ability to open source parts of the project which don't give us a business advantage, as well as healthy working hours. We've also got investment days on Fridays, when you can work on anything you want, as long as it could possibly benefit Spacelift in some way.</p><p>If that sounds like fun to you, please apply at <a href="https://spacelift.teamtailor.com/jobs/3006934-software-engineer-remote-europe" rel="nofollow">https://spacelift.teamtailor.com/jobs/3006934-software-engin...</a></p><p>You can find out more about the product we're building at <a href="https://spacelift.io/" rel="nofollow">https://spacelift.io</a> and also see our engineering blog for a few technical blog posts of ours: <a href="https://spacelift.io/blog/engineering" rel="nofollow">https://spacelift.io/blog/engineering</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847410"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847410" href="https://news.ycombinator.com/vote?id=40847410&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Valley View Trading | Developer/Trader | Onsite, Chicago IL</p><p>Calling all elite developers who are ready to make their mark in the markets: Valley View trading is not your typical trading firm. We're a small team of successful trader/engineers looking for an inventive, entrepreneurial and detail-oriented developer who wants to master the equity options markets.</p><p>If you're a self-starter with serious C++ chops, a track record of moving fast and a hunger to excel in the ultra-competitive world of options trading - let's talk. This is a unique chance to reap the rewards of your talent.</p><p>The right candidate thrives as an individual contributor, has 5+ years of experience creating high-octane, low latency code and a clean background. You'll hit the ground running and have 60 days to ace the Series 57 exam on us.</p><p>If you're ready to trade Silicon Valley politics for the chance at Silicon Valley paydays, this is your opportunity. Show us what you've got - reach out with cover letter and resume at valleyviewoptions &lt;AT&gt; gmail.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847373"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847373" href="https://news.ycombinator.com/vote?id=40847373&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Datable | Senior Software Engineer, Reliability | Bay Area (3 days hybrid) | Full-time | $160k-$220k + equity</p><p>Do you find metrics, logs, and traces fascinating and infuriating in equal measure? We are sick of the limitations and costs of observability, so we’ve decided to do something about it! Datable is a building a streaming data pipeline to help customers manage their observability data.</p><p>We are a seed stealth startup based in San Francisco. Our six person team has a lot of experience in this space, and we’re looking for an engineer passionate about reliability to build out our customer platform. Reliability for reliability? Neat! You’ll be working in Terraform, Helm, and NodeJS.</p><p>We strongly believe in the strength of Product, Design, and Engineering collaborating together to build better products. You’ll have a lot of autonomy to try new things along the way, and will have a huge impact on building our product.</p><p>Our work is heavily based on best practices from OpenTelemetry and years of experience in the observability world. Our stack is primarily Typescript, React, NodeJS, Postgres, and AWS.</p><p>Email ben@gahlsdorftalent.com</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847175"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847175" href="https://news.ycombinator.com/vote?id=40847175&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>shmood.io | Hybrid | NYC (in a beautiful office) | Full-time | Full-Stack &amp; Back-End Developers | B2B and B2C SaaS</p><p>Shmood is a VC backed company on a mission to eliminate design alignment churn.</p><p>We’re building Pinterest for design enterprises — to allow designers and their teams instantly align with clients of any background, on a project of any scale. To do this, we’re integrating a semantic data layer that takes multimodal design intent and communicates it, refines it, and turns it into actionable advice.</p><p><a href="https://www.shmood.io/careers" rel="nofollow">https://www.shmood.io/careers</a> (or email malvika@shmood.io with an intro and resume and we will be in touch!)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846508"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846508" href="https://news.ycombinator.com/vote?id=40846508&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Powertools Technologies | Junior/Senior Engineer | Lisbon, Portugal | Full-time | ONSITE</p><p>&gt; Looking for a senior engineer for work on software related to Electronic Design Automation. Candidate should have some experience with EDA software (Cadence Virtuoso, Siemens Calibre, Synopsys Design Compiler, etc), ideally including plugin development. Experienced Software Developers are more than welcome to apply.</p><p>&gt; Looking for a junior engineer for work on software related to Electronic Design Automation and/or Software Development. Candidate should at least have (or graduate shortly) a 3 year university degree in engineering. Most suitably Electronic/Computer Engineering or Informatics. Software Developers are more than welcome to apply.</p><p>Site: <a href="https://www.powertools-tech.com/" rel="nofollow">https://www.powertools-tech.com</a> . Growing a small experienced team with international industrial and academic track, willing to train new hire in fairly uncommon skill set. Candidate should be capable of quality detail work, and have good communication abilities, to provide support to international design teams in fabless semiconductor companies.</p><p>Email your interest and CV to hr@powertools-tech.com, please.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847758"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847758" href="https://news.ycombinator.com/vote?id=40847758&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>sea.dev | Full Stack Product Engineer | London, UK | Hybrid or REMOTE in UK | Full-time | Fintech</p><p>sea.dev is looking to hire our first engineer. We are looking for a “Full Stack Product Engineer” to help build our product.</p><p>We are building technology that enables financial institutions to flexibly embed LLM capabilities into their workflows and product offerings.</p><p>The founding team has worked at world-leading financial and research institutions, and brings together decades of experience in data technology, graphs, and finance. We have extensive prior experience building data teams and launching data products from scratch.</p><p>FAQ: Pre-seed. VC-backed. Team of three. Fully remote but centred around London</p><p>For more details see <a href="https://join.sea.dev/" rel="nofollow">https://join.sea.dev</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846738"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846738" href="https://news.ycombinator.com/vote?id=40846738&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>AllSpice | <a href="https://allspice.io/" rel="nofollow">https://allspice.io/</a> | Boston, MA / SF or Remote/Hybrid | Full time | Sr. - Principal | golang, Vuejs, rust</p><p>At AllSpice, we're building the future of hardware development and collaboration, applying modern software design principles to the hardware industry with revision control, design review, and automated test (think GitHub/Bitbucket for hardware). AllSpice is unlocking the next generation of smart vehicles, IOT devices, rockets, medical devices, robotics, and much more.</p><p><a href="https://techcrunch.com/2023/12/05/git-allspice-enterprise-6m/" rel="nofollow">https://techcrunch.com/2023/12/05/git-allspice-enterprise-6m...</a></p><p>We have a highly-capable, tight-knit, remote-first team with a flex office in Somerville, MA and competitive benefits. We strongly value continuous communication and personal development. See more on our careers page [<a href="https://allspice.notion.site/AllSpice-Careers-3173d0cd518b4257b186ba5c8f34dc44" rel="nofollow">https://allspice.notion.site/AllSpice-Careers-3173d0cd518b42...</a>]</p><p>We’re hiring primarily for:</p><p>Principal / Lead Fullstack SW Engineer - Take a pivotal role on our incredible engineering team by helping coordinate our application architecture and implement new product features. You'll work on optimizing 2d graphics rendering, scaling application code, and testing/planning architecture to support new feature development of our git hosting platform. More info here: <a href="https://www.notion.so/allspice/Principal-Lead-Fullstack-Engineer-4452470c166447a0a7dbc4195bbd6ad4" rel="nofollow">https://www.notion.so/allspice/Principal-Lead-Fullstack-Engi...</a></p><p>Mid / Sr. Frontend Software Engineer - You'll be the subject matter expert for our front-end testing (Vue, JS, Golang, Playwright, jest), and working with open-source projects to extend our testing capability. <a href="https://allspice.notion.site/Mid-Sr-Front-End-Software-Engineer-58149eb6911540c28b329087d683dbe3" rel="nofollow">https://allspice.notion.site/Mid-Sr-Front-End-Software-Engin...</a></p><p>Director of Sales - We're also hiring for a director of sales with expertise in enterprise B2B software. It's a phenomenal opportunity to streamline our sales process &amp; coach our capable and growing sales team.
<a href="https://allspice.notion.site/Director-of-Sales-48528b4a683448dbb66adb119b5bbd10" rel="nofollow">https://allspice.notion.site/Director-of-Sales-48528b4a68344...</a></p><p>Tech Stack: Docker, Terraform, GoLang, Rust, Python, Vue</p><p>See our careers page: <a href="https://allspice.notion.site/AllSpice-Careers-3173d0cd518b4257b186ba5c8f34dc44" rel="nofollow">https://allspice.notion.site/AllSpice-Careers-3173d0cd518b42...</a></p><p>Apply by emailing us at jobs&lt;AT&gt;allspice[dt]io (&lt;AT&gt; -&gt; @, [dt] -&gt; .) with [HN] in the title and a link to your GitHub/GitLab profile and/or resume.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846795"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846795" href="https://news.ycombinator.com/vote?id=40846795&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Roboflow | Field Engineer + ML Engineer | Full-time (Remote, SF, NYC) | <a href="https://roboflow.com/careers?ref=whoishiring0724">https://roboflow.com/careers?ref=whoishiring0724</a></p><p>Roboflow is the fastest way to use computer vision in production. We help developers give their software the sense of sight. Our end-to-end platform[1] provides tooling for image collection, annotation, dataset exploration and curation, training, and deployment.</p><p>Over 250k engineers (including engineers from 2/3 Fortune 100 companies) build with Roboflow. We now host the largest collection of open source computer vision datasets and pre-trained models[2]. We are pushing forward the CV ecosystem with open source projects like Autodistill[3] and Supervision[4]. And we've built one of the most comprehensive resources for software engineers to learn to use computer vision with our popular blog[5] and YouTube channel[6].</p><p>We have several openings available but are primarily looking for strong technical generalists who want to help us democratize computer vision and like to wear many hats and have an outsized impact. Our engineering culture is built on a foundation of autonomy &amp; we don't consider an engineer fully ramped until they can "choose their own loss function". At Roboflow, engineers aren't just responsible for building things but also for helping us figure out what we should build next. We're builders &amp; problem solvers; not just coders. (For this reason we also especially love hiring past and future founders.)</p><p>We're currently hiring full-stack engineers for our ML and web platform teams, a web developer to bridge our product and marketing teams, several technical roles on the sales &amp; field engineering teams, and our first applied machine learning researcher to help push forward the state of the art in computer vision.</p><p>[1]: <a href="https://roboflow.com/?ref=whoishiring0724">https://roboflow.com/?ref=whoishiring0724</a></p><p>[2]: <a href="https://roboflow.com/universe?ref=whoishiring0724">https://roboflow.com/universe?ref=whoishiring0724</a></p><p>[3]: <a href="https://github.com/autodistill/autodistill">https://github.com/autodistill/autodistill</a></p><p>[4]: <a href="https://github.com/roboflow/supervision">https://github.com/roboflow/supervision</a></p><p>[5]: <a href="https://blog.roboflow.com/?ref=whoishiring0724">https://blog.roboflow.com/?ref=whoishiring0724</a></p><p>[6]: <a href="https://www.youtube.com/@Roboflow" rel="nofollow">https://www.youtube.com/@Roboflow</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846458"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846458" href="https://news.ycombinator.com/vote?id=40846458&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Temporal Technologies | Multiple positions in United States - WORK FROM HOME | FULL-TIME |</p><p>Temporal offers an entirely new way to build scalable and reliable applications. Temporal enables developers to focus on writing important business logic, and not on managing state or worrying about the underlying infrastructure. Sequoia Capital led our last round of funding and our team has experience from start-ups and larger companies like Microsoft, Google, Amazon, Uber, and more.</p><p>Temporal Investors Expand Funding: <a href="https://temporal.io/news/temporal-investors-expand-funding-with-usd75m-round" rel="nofollow">https://temporal.io/news/temporal-investors-expand-funding-w...</a></p><p>Temporal in 7 minutes: <a href="https://temporal.io/tldr" rel="nofollow">https://temporal.io/tldr</a></p><p>We're looking for senior level engineers for multiple roles - see here - <a href="https://www.temporal.io/careers" rel="nofollow">https://www.temporal.io/careers</a></p><p>FEATURED ROLES:</p><p>Senior Developer Success Engineer → <a href="https://grnh.se/ca70567c7us" rel="nofollow">https://grnh.se/ca70567c7us</a></p><p>Staff Product Manager - Governance &amp; Security → <a href="https://grnh.se/ef4098687us" rel="nofollow">https://grnh.se/ef4098687us</a></p><p>Staff Software Engineer - Control Plane Core → <a href="https://grnh.se/9612b42f7us" rel="nofollow">https://grnh.se/9612b42f7us</a></p><p>Senior Software Engineer - Cloud Infrastructure → <a href="https://grnh.se/7e95c2ed7us" rel="nofollow">https://grnh.se/7e95c2ed7us</a></p><p>Senior Manager, Commercial Sales → <a href="https://grnh.se/0b3d09097us" rel="nofollow">https://grnh.se/0b3d09097us</a></p><p>US benefits include: Unlimited PTO, 12 Holidays + 2 Floating Holidays, 100% Premiums Coverage for Medical, Dental, and Vision, AD&amp;D, LT &amp; ST Disability and Life Insurance , Empower 401K Plan, Additional Perks for Learning &amp; Development, Lifestyle Spending, In-Home Office Setup, Professional Memberships, WFH Meals, Internet Stipend and more! Benefits outside the United States vary by country.</p><p>Apply here <a href="https://www.temporal.io/careers/" rel="nofollow">https://www.temporal.io/careers/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40846857"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846857" href="https://news.ycombinator.com/vote?id=40846857&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Startup | Software | Remote (USA + Canada preferred) | $100k + equity</p><p>We're looking for a person who thinks C/C++ is fun. You'll be writing a lot of code from scratch. We prefer someone with 10+years of experience but if you're talented we'll accept a junior</p><p>Please send your resume and a hello world program that doesn't use printf or iostreams FrankStanley12@proton.me</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40847861"><td></td></tr>
                  <tr id="40846715"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40846715" href="https://news.ycombinator.com/vote?id=40846715&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Standard Model Biomedicine | Founding Engineering Roles</p><p>We are building the standard model for biomedicine. This is the hardest and most important problem in life science. We are solving it.</p><p>One of the following (founders are filling both roles currently):</p><p>AI: you are 5x faster than anyone you know at implementing, training, fine-tuning sota. e.g. you can write an ACL accepted paper in 1 week from concept to arxiv.  Given a choice between openai and working with us, you would choose us.</p><p>Data science: you are 3x faster and 10x better than anyone you know at deciding what to evaluate, how to evaluate it, and how to communicate it. You are comfortable learning biomedical jargon and humble but confident about testing that with any biomedical expert in the world. Given a choice between leading analytics at the top ranked medical institution in a given field and us, you would choose us.</p><p>kevin@standardmodel.bio</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="40847480"><td></td></tr>
                  <tr id="40847031"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_40847031" href="https://news.ycombinator.com/vote?id=40847031&amp;how=up&amp;goto=item%3Fid%3D40846428"></a></center>    </td><td><br><div><p>Jam.dev | Staff Fullstack Engineer &amp; AI Product Engineer | Typescript/React | Remote (+ in person in SF, Austin, NYC) | Full-time</p><p>Dev tools company with 125,000+ users in less than 2 years. $10M in funding from Vercel CEO, GitHub CTO, Cloudflare CEO, etc.</p><p>We’re building a flight recorder for web apps – so anyone can report issues to engineers in a way that's actually debuggable (w/ console, network, websockets debugger, etc).</p><p>Small, senior team – several ex-engineering directors turned ICs (mostly ex-early Cloudflare). Looking for staff-level engineers with experience building highly performant front-end apps.</p><p>Stack: React/Typescript and MobX (MST) on the frontend, and Node/GraphQL across our backend.</p><p>The challenge ahead: Scaling. Usage 10x’ed last year, and our users are in 176 countries, on all sorts of devices, network conditions, etc. Our bar for quality is high.</p><p>As a dev tool, developers at Jam are directly connected and involved with the product. Your usage of the product will directly inform the direction of Jam’s future.</p><p>Apply here (we read and respond to every submission): <a href="https://jam.dev/careers" rel="nofollow">https://jam.dev/careers</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="40847456"><td></td></tr>
                <tr id="40847582"><td></td></tr>
                  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Welcome to Ladybird (272 pts)]]></title>
            <link>https://ladybird.org/</link>
            <guid>40845951</guid>
            <pubDate>Mon, 01 Jul 2024 14:04:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ladybird.org/">https://ladybird.org/</a>, See on <a href="https://news.ycombinator.com/item?id=40845951">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <div>
            <h2>
              Welcome to <span>Ladybird</span>,<br>a truly independent<br>web
              browser.
            </h2>
            <p>
              We are building a brand-new browser from scratch, backed by a
              non-profit.
            </p>
            <p><a href="#gi">Get Involved</a>
          </p></div>

      <div id="about">
            <p><img src="https://ladybird.org/assets/img/about-2x.webp" alt="">
            </p>
            <div>
              <h2>About <span>Ladybird</span></h2>
              <p>
                Ladybird is a brand-new browser &amp; web engine. Driven by a
                web standards first approach, Ladybird aims to render the
                modern web with good performance, stability and security.
              </p>
              <p>
                From its humble beginnings as an HTML viewer for the SerenityOS
                hobby operating system project, Ladybird has since grown into a
                cross-platform browser supporting Linux, macOS, and other
                Unix-like systems.
              </p>
              <p>
                Ladybird is currently in heavy development. We are targeting a
                first Alpha release for early adopters in 2026.
              </p>
            </div>
          </div>

      <div>
          <div>
            <p>
              <h2>What makes <span>Ladybird</span> unique</h2>
            </p>
          </div>

          <div>
            <div>
                <p><img src="https://ladybird.org/assets/img/truly-independent.svg">
                </p>
                <div>
                  <h4>Truly independent</h4>
                  <p>
                    No code from other browsers.
                    We're building a new engine, based on web standards.
                  </p>
                </div>
              </div>

            <div>
                <p><img src="https://ladybird.org/assets/img/single-focus.svg">
                </p>
                <div>
                  <h4>Singular focus</h4>
                  <p>We are focused on one thing: the web browser.
                </p></div>
              </div>

            <div>
                <p><img src="https://ladybird.org/assets/img/no-monetization.svg">
                </p>
                <div>
                  <h4>No monetization</h4>
                  <p>
                    No "default search deals", crypto tokens, or other forms of
                    user monetization, ever.
                  </p>
                </div>
              </div>
          </div>
        </div>

      <div id="news">
          <p>
            <h2>News &amp; Announcements</h2>
          </p>
          
        </div>

      <div id="gi">
            <h2>Get Involved</h2>
            <p>
              Ladybird is currently in heavy development, and there's work to be
              done in all areas of the browser.
            </p>
            <p>
              We're welcoming new developers every week. The main community hub
              is
              <a href="https://discord.gg/nvfjVJ4Svh">our Discord server</a>.
            </p>
            <p>
              All the code is hosted on
              <a href="https://github.com/LadybirdBrowser/ladybird">GitHub</a>. Clone it, build it, and join our Discord if you want to
              collaborate on it! We're looking forward to seeing you there.
            </p>
            <p><a href="https://discord.gg/nvfjVJ4Svh">Join Discord</a>
            <a href="https://github.com/LadybirdBrowser/ladybird">Get the code</a>
          </p></div>

      

      <div>
              <h2>Become a <span>Ladybird</span> supporter</h2>
              <p>
                Ladybird is funded entirely by sponsorships and donations from
                people and companies who care about the open web.
              </p><p>
                We accept one-time and recurring monthly donations via <a href="https://donorbox.org/ladybird">Donorbox</a>.
              </p><p>
                If you or your company would like to make a large donation, we would be happy to display your logo
                on this website! Please <a href="mailto:contact@ladybird.org">contact us</a> about becoming a sponsor.
            </p></div>

      <div>
          <p>
            <h2>Frequently Asked Questions</h2>
          </p>

          <div>
            <div>
                
                <p>
                  We are targeting Summer 2026 for a first Alpha version on
                  Linux and macOS. This will be aimed at developers and early
                  adopters.
                </p>
              </div>

            <div>
                
                <p>
                  We currently have 4 paid full-time engineers working on
                  Ladybird. There is also a large community of volunteer contributors.
                </p>
              </div>

            <div>
                
                <p>
                  We have 3 new full-time engineers starting soon. Going
                  forward, we would like to grow the team at a reasonable pace.
                  Building the right team is more important than building it
                  quickly.
                </p>
              </div>

            <div><p>
                  The focus of the Ladybird project is to build a new browser
                  engine from the ground up. We don't use code from Blink,
                  WebKit, Gecko, or any other browser engine.
                  </p><p>
                  For historical reasons, the browser uses various libraries
                  from the SerenityOS project, which has a strong culture of
                  writing <i>everything</i> from scratch.
                  Now that Ladybird has forked from SerenityOS, it is no longer
                  bound by this culture, and we will be making use of 3rd party
                  libraries for common functionality (e.g image/audio/video formats,
                  encryption, graphics, etc.)
                  </p><p>
                  We are already using some of the same 3rd party libraries
                  that other browsers use, but we will never adopt another
                  browser engine instead of building our own.
                </p></div>

            <div><p>
                  We don't have anyone actively working on Windows support, and
                  there are considerable changes required to make it work well
                  outside a Unix-like environment.
                  </p><p>
                  We would like to do Windows eventually, but it's not a priority
                  at the moment.
                </p></div>

            <div><p>
                  We don't have anyone actively working on an Android or iOS port.
                  More effort will be put into mobile once we have the desktop versions in a good state.
                  </p><p>
                  While there is the start of an Android port in the project repository,
                  mobile is not a priority at the moment.
                </p></div>

            <div>
                  
                  <p>
                  Sponsors will have their logos displayed on our website, and
                  will be thanked in updates / on social media.
                  </p><p>
                  Please <a href="mailto:contact@ladybird.org">contact us</a> if you are interested in sponsorship.
                </p></div>

            <div>
                
                <p>
                  All sponsorships are in the form of unrestricted donations.
                  Board seats and other forms of influence are not for sale.
                </p>
              </div>

            <div><p>
                  Ladybird started as a component of the SerenityOS hobby project, which only allows C++.
                  The choice of language was not so much a technical decision, but more one of personal convenience.
                  Andreas was most comfortable with C++ when creating SerenityOS, and now we have almost half
                  a million lines of modern C++ to maintain.
                  </p><p>
                  However, now that Ladybird has forked and become its own independent project,
                  all constraints previously imposed by SerenityOS are no longer in effect.
                  We are actively evaluating a number of alternatives and will
                  be adding a mature successor language to the project in the near future.
                  This process is already quite far along, and prototypes exist in
                  multiple languages.
                </p></div>

          </div>
        </div>

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing HTML by hand is easier than debugging your static site generator (132 pts)]]></title>
            <link>https://logicgrimoire.wordpress.com/2024/07/01/writing-html-by-hand-is-easier-than-debugging-your-static-site-generator/</link>
            <guid>40845542</guid>
            <pubDate>Mon, 01 Jul 2024 13:12:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://logicgrimoire.wordpress.com/2024/07/01/writing-html-by-hand-is-easier-than-debugging-your-static-site-generator/">https://logicgrimoire.wordpress.com/2024/07/01/writing-html-by-hand-is-easier-than-debugging-your-static-site-generator/</a>, See on <a href="https://news.ycombinator.com/item?id=40845542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<p><a href="#content">
			Skip to content		</a></p><!-- .site-header -->

		<div id="content">
	<main id="main">
		
<article id="post-892">
	<!-- .entry-header -->

	
	
	<div>
		<p>As someone who has used a static site generator every day at work for years I am on the threshold of believing “actually just writing HTML by hand is probably easier”</p>
<p>This becomes obvious when you eg get a new work machine and need to set up the site generator on the new machine and realize that the exact series of bits you had on disk on the old machine were different than what you can achieve on the new machine. And as a result the site generator doesn’t work even though the setup is “the same”</p>
<p>This state of affairs is also known as “<a href="https://m.youtube.com/watch?v=lKXe3HUG2l4&amp;pp=ygUSdGhlIG1lc3Mgd2UncmUgaW4g">the mess we’re in</a>”</p>
<p>Do you enjoy debugging programming language installations? What about the language’s package ecosystem? What about the language’s deployment model (or lack thereof)? Congratulations, you now get to be a sysadmin who has to care about all of these things. This is a mile high mountain of dependencies to be dealing with all just to avoid having to type a few brackets.</p>
<p>Meanwhile if a web site is maintained as just a folder of HTML files I could do a “git clone” and be up and running already in Emacs’ lovely <a href="https://www.gnu.org/software/emacs/manual/html_mono/nxml-mode.html">nxml mode</a> or for that matter <a href="https://github.com/tpope/vim-ragtag">tpope’s ragtag</a> which I’ve also used extensively- and it’s excellent. Do autoformatting on save with <a href="https://github.com/someth2say/xmlformat">xmlformat</a></p>
<p>To sum up, for long term ease of maintenance I’m coming to believe it’s wise to prefer static DATA that is maintained in a specified validated format over changing, dynamic CODE that keeps changing and breaking, and in all probability you don’t actually control it anyway, <a href="https://www.softwaremaxims.com/blog/not-a-supplier">you just picked it up by the side of the road</a></p>
<p>(This grew out of a little complaint at <a href="https://mastodon.social/@rmloveland/112700265946114818">https://mastodon.social/@rmloveland/112700265946114818</a>)</p>

<div>
	<p><img alt="" src="https://2.gravatar.com/avatar/576045b345aec608548b8fcde721c9fb1f05519d83f0bca1843559d6a09f7c13?s=42&amp;d=identicon&amp;r=G" srcset="https://2.gravatar.com/avatar/576045b345aec608548b8fcde721c9fb1f05519d83f0bca1843559d6a09f7c13?s=42&amp;d=identicon&amp;r=G 1x, https://2.gravatar.com/avatar/576045b345aec608548b8fcde721c9fb1f05519d83f0bca1843559d6a09f7c13?s=63&amp;d=identicon&amp;r=G 1.5x, https://2.gravatar.com/avatar/576045b345aec608548b8fcde721c9fb1f05519d83f0bca1843559d6a09f7c13?s=84&amp;d=identicon&amp;r=G 2x, https://2.gravatar.com/avatar/576045b345aec608548b8fcde721c9fb1f05519d83f0bca1843559d6a09f7c13?s=126&amp;d=identicon&amp;r=G 3x, https://2.gravatar.com/avatar/576045b345aec608548b8fcde721c9fb1f05519d83f0bca1843559d6a09f7c13?s=168&amp;d=identicon&amp;r=G 4x" height="42" width="42" loading="lazy" decoding="async">	</p><!-- .author-avatar -->

	<div>
		

		<p>
			I'm the author of 'Jelec: the White Bear, or, Beware an Encounter with a Raven and his Friends.' For my day job, I work as a technical writer at a software company.			<a href="https://logicgrimoire.wordpress.com/author/logicgrimoire/" rel="author">
				View all posts by logicgrimoire			</a>
		</p><!-- .author-bio -->
	</div><!-- .author-description -->
</div><!-- .author-info -->
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article><!-- #post-892 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
	</main><!-- .site-main -->

	
</div><!-- .site-content -->

		<!-- .site-footer -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Python code is a neural network (227 pts)]]></title>
            <link>https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/</link>
            <guid>40845304</guid>
            <pubDate>Mon, 01 Jul 2024 12:47:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/">https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/</a>, See on <a href="https://news.ycombinator.com/item?id=40845304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
      
      <div id="toc">
        <h2>Contents</h2>
        <ol>
          
          <li>
            <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#introduction">Introduction</a>
            
          </li>
          
          <li>
            <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#detecting-program-code-in-messages">Detecting program code in messages</a>
            
            <ol type="a">
              
              <li>
                <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#ideas-for-decision-rules">Ideas for decision rules</a>
              </li>
              
              <li>
                <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#a-hand-written-algorithm">A hand-written algorithm</a>
              </li>
              
            </ol>
            
          </li>
          
          <li>
            <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#neural-networks-to-the-rescue">Neural networks to the rescue</a>
            
            <ol type="a">
              
              <li>
                <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#the-general-idea">The general idea</a>
              </li>
              
              <li>
                <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#our-python-code-as-math">Our Python code as math</a>
              </li>
              
            </ol>
            
          </li>
          
          <li>
            <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#training-the-network-to-discover-better-algorithms">Training the network to discover better algorithms</a>
            
            <ol type="a">
              
              <li>
                <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#trainable-activation-functions">Trainable activation functions</a>
              </li>
              
              <li>
                <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#architectures-with-more-efficient-implementations">Architectures with more efficient implementations</a>
              </li>
              
              <li>
                <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#architectures-with-more-numerically-stable-gradients">Architectures with more numerically stable gradients</a>
              </li>
              
            </ol>
            
          </li>
          
          <li>
            <a href="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/#data-driven-discipline">Data-driven discipline</a>
            
          </li>
          
        </ol>
      </div>
      
      <p>Many programs that we write can be embedded in recurrent neural networks (RNNs).
For such programs, a trained RNN can perform better than if we write the algorithm by hand, refining it via trial and error.
I walk through an example in detail.</p>
<div id="lgtm_rnn_display">
<p><img id="lgtm_rnn" src="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/figure_rnn_step_1.svg">
<img id="lgtm_rnn_dark" src="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/figure_rnn_step_28_dark.svg">
<br>
</p><p>Is this Python code or a neural network? They are one and the same.</p>
</div>
<h2 id="introduction">Introduction</h2>
<p>Humans are bad at managing spaghetti code.
Of course, we should try and avoid writing spaghetti code if we can.
But there are problems that are so ill-specified that any serious attempt to solve them results in just that.</p>
<p>Let me make this concrete.
In research projects, we often write programs that extract information from raw data.
The data may have idiosyncrasies and follow no clear specification.
Some examples:</p>
<ul>
<li>
Identify mentions of corporations and their officers in news articles.
</li>
<li>
Label public procurement contracts by what kind of service the firms supply.
</li>
<li>
Determine if a message exchanged between engineers contains program code.
</li>
</ul>
<p>If we want the output to be perfect—at least in the limit—then we need to exhaustively examine each observation.
We can go one step further to reassure ourselves that the program produces the correct output by picking representative examples and writing unit tests for them.
Almost every programming language has libraries for this, including <a href="https://testthat.r-lib.org/">R</a> and <a href="https://docs.python.org/3/library/unittest.html">Python</a>.</p>
<p>But this approach won’t work well if getting the correct output requires that we specify complicated decision rules.
In the examples above,</p>
<ul>
<li>
corporate officers might be mentioned in the news by nickname,
</li>
<li>
contracts might describe the procured services using synonyms, and
</li>
<li>
it might not always be trivial to tell apart program code from English.
</li>
</ul>
<p>In such situations, we might be better off training a neural network.
Algorithms that train neural networks thrive on spaghetti.</p>
<h2 id="detecting-program-code-in-messages">Detecting program code in messages</h2>
<p>In this post, the problem that I’ll walk through solving is this:
<em>how can we detect if a message sent during code review explicitly refers to program code?</em>
Let’s suppose that the code base that we observe is written in C, and that the following examples are representative of the messages that engineers send:</p>
<ol type="A">
<li>
<em>LGTM with <mark>render_ipa_alloc()</mark>.</em>
</li>
<li>
<em>If the <mark>FTPSACK</mark> flag is set, then use a prespecified value.</em>
</li>
<li>
<em>AFAICT there is nothing else to check (unless you can think of something).</em>
</li>
<li>
<em>Actually, <mark>debug_error()</mark> doesn’t return <mark>NULL</mark>, so we should use <mark>IS_ERROR()</mark> here.</em>
</li>
<li>
<em>This fails to build on aarch64 even though it works without issue on amd64.</em>
</li>
<li>
<em>I’ve added <mark>if (err) goto cleanup;</mark> but the code still leaks.</em>
</li>
</ol>
<p>The highlighted expressions are program-code references that we would like to detect.</p>
<h3 id="ideas-for-decision-rules">Ideas for decision rules</h3>
<p>We take a straightforward approach and look for decision rules that can tell apart program code from ordinary English.
Here are some simple ideas:</p>
<table>
<tbody><tr>
<th>#</th>
<th>Rule</th>
<th>True positive</th>
<th>False positive</th>
<th>False negative</th>
</tr>
<tr>
<td>1</td>
<td>Word followed by parentheses is code.</td>
<td>A and D</td>
<td>–</td>
<td>B and F</td>
</tr>
<tr>
<td>2</td>
<td>All-caps word is code.</td>
<td>B and D</td>
<td>C</td>
<td>A and F</td>
</tr>
<tr>
<td>3</td>
<td>Non-English word is code.</td>
<td>A, B, and D</td>
<td>C, E, and F</td>
<td>–</td>
</tr>
</tbody></table>
<p>None of these rules is perfect: each has false positives or false negatives or both.</p>
<ul>
<li>
Rule 1 is easy to implement but it misses obvious positive cases like <code>if (err) goto cleanup;</code>.
</li>
<li>
Rule 2 mistakenly classifies capitalized acronyms as program code.
To make this rule effective, we need to complement it with an extensive list of acronyms and abbreviations that we might encounter (e.g., <em>AFAICT, LGTM, USD, COVID</em>).
</li>
<li>
Rule 3 mistakenly classifies engineering jargon as program code.
To make this rule work, we need an even longer word list than for Rule 2.
We can start with a canonical list of English words, but we need to complement it both with acronyms and with words that commonly appear in software engineering, like <em>aarch64</em> and <em>amd64.</em>
</li>
</ul>
<h3 id="a-hand-written-algorithm">A hand-written algorithm</h3>
<p>Nevertheless, we reason that a simple algorithm might do well enough.
So we set out to write a program that implements Rule 1.
Our program will decide in two steps whether a message contains code:</p>
<ol>
<li>
<strong>Preprocessing:</strong> convert the message into a sequence of tokens.
The tokens are chosen so that they capture syntactic elements of C program code.
This is the information that we need to apply Rule 1.
</li>
<li>
<strong>Inference:</strong> apply a function to the token sequence to check if the sequence satisfies the rule.
If it does, then we conclude that the message contains valid C code.
</li>
</ol>
<p>Let’s take the first example sentence from before.
We could encode it as a sequence of eight tokens:<a id="fnref1" href="#fn1" role="doc-noteref"><sup>1</sup></a></p>
<p><img id="lgtm_tokens" src="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/figure_tokens_step_8.svg" width="600">
<img id="lgtm_tokens_dark" src="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/figure_tokens_step_8_dark.svg" width="600">
<br>


</p>
<p>Rule 1 says that the token sequence <code>underscore_identifier</code>–<code>open_paren</code>–<code>close_paren</code> indicates the presence of program code in the sentence.
Let’s write a classifier in Python that detects a sequence like this.</p>
<p>First, we set up the types.
For simplicity, we model tokens as strings.
In order to keep track of the tokens that we have seen in the sequence, we also define a <a href="https://peps.python.org/pep-0557/">data class</a> called <code>State</code>:</p>
<pre data-linenos="" data-lang="py"><code data-lang="py"><table><tbody><tr><td>1</td><td><span><span><span>from</span></span><span><span> <span><span>dataclasses</span></span> <span><span>import</span></span></span></span><span></span><span> <span>dataclass</span></span>
</span></td></tr><tr><td>2</td><td><span>
</span></td></tr><tr><td>3</td><td><span><span><span>Token</span></span> <span>=</span> <span><span>str</span></span>    </span></td></tr><tr><td>4</td><td><span>
</span></td></tr><tr><td>5</td><td><span><span><span>@</span><span><span><span>dataclass</span></span></span></span>
</span></td></tr><tr><td>6</td><td><span><span><span><span>class</span></span> <span><span>State</span></span><span>:</span></span>
</span></td></tr><tr><td>7</td><td><span>    <span><span>previous_was_identifier</span></span><span>:</span> <span><span>bool</span></span> <span>=</span> <span>False</span>
</span></td></tr><tr><td>8</td><td><span>    <span><span>previous_was_open_paren</span></span><span>:</span> <span><span>bool</span></span> <span>=</span> <span>False</span>
</span></td></tr><tr><td>9</td><td><span>    <span><span>previous_previous_was_identifier</span></span><span>:</span> <span><span>bool</span></span> <span>=</span> <span>False</span>
</span></td></tr><tr><td>10</td><td><span>    <span><span>seen_code</span></span><span>:</span> <span><span>bool</span></span> <span>=</span> <span>False</span>
</span></td></tr></tbody></table></code></pre>
<p>To classify a message, we write a function called <code>contains_code</code> that returns <code>True</code> if the message satisfies Rule 1.
The function decides this by iterating over the token sequence and checking at each element whether the rule applies.
Since applying the rule means checking three <em>consecutive</em> tokens, not just one token, we need to keep a memory of what tokens we have seen earlier in the sequence.
The function <code>contains_code</code> keeps this memory inside an instance of <code>State</code>:</p>
<pre data-linenos="" data-lang="py"><code data-lang="py"><table><tbody><tr><td>12</td><td><span><span><span>from</span></span><span><span> <span><span>typing</span></span> <span><span>import</span></span></span></span><span></span><span> <span>Iterable</span></span>
</span></td></tr><tr><td>13</td><td><span>
</span></td></tr><tr><td>14</td><td><span><span><span><span>def</span></span> <span><span>contains_code</span></span></span><span><span>(</span></span><span><span>tokens</span></span><span><span>:</span> <span><span><span>Iterable</span></span></span><span><span>[</span></span><span><span><span>Token</span></span></span><span><span>]</span></span></span><span><span>)</span></span><span> </span><span><span>-&gt;</span> <span><span>bool</span></span></span><span><span>:</span></span>
</span></td></tr><tr><td>15</td><td><span>    <span><span>state</span></span> <span>=</span> <span><span><span><span>State</span></span></span></span><span><span>(</span><span>)</span></span>
</span></td></tr><tr><td>16</td><td><span>
</span></td></tr><tr><td>17</td><td><span>    <span><span>for</span> <span>token</span> <span>in</span></span><span> <span><span>tokens</span></span></span><span><span>:</span></span>
</span></td></tr><tr><td>18</td><td><span>        <span><span>state</span></span> <span>=</span> <span><span><span><span>process</span></span></span></span><span><span>(</span><span><span>state</span></span><span>,</span> <span><span>token</span></span><span>)</span></span>
</span></td></tr><tr><td>19</td><td><span>
</span></td></tr><tr><td>20</td><td><span>    <span>return</span> <span><span>state</span><span>.</span><span>seen_code</span></span>
</span></td></tr></tbody></table></code></pre>
<p>Now we are ready to implement <code>process</code>.
This is where we check if the rule applies and maintain the state:</p>
<pre data-linenos="" data-lang="py"><code data-lang="py"><table><tbody><tr><td>22</td><td><span><span><span><span>def</span></span> <span><span>process</span></span></span><span><span>(</span></span><span><span>state</span></span><span><span>:</span> <span><span>State</span></span></span><span><span>,</span> <span>token</span></span><span><span>:</span> <span><span>Token</span></span></span><span><span>)</span></span><span> </span><span><span>-&gt;</span> <span><span>State</span></span></span><span><span>:</span></span>
</span></td></tr><tr><td>23</td><td><span>    </span></td></tr><tr><td>24</td><td><span>
</span></td></tr><tr><td>25</td><td><span>
</span></td></tr><tr><td>26</td><td><span>    <span><span>if</span> <span><span>state</span><span>.</span><span>seen_code</span></span><span>:</span></span>
</span></td></tr><tr><td>27</td><td><span>        <span>return</span> <span><span>state</span></span>
</span></td></tr><tr><td>28</td><td><span>
</span></td></tr><tr><td>29</td><td><span>    </span></td></tr><tr><td>30</td><td><span>    </span></td></tr><tr><td>31</td><td><span>    </span></td></tr><tr><td>32</td><td><span>    <span><span>if</span> <span><span>(</span><span><span>token</span></span> <span>==</span> <span><span><span>"</span></span></span><span><span>close_paren<span>"</span></span></span>
</span></span></span></td></tr><tr><td>33</td><td><span><span><span>        <span>and</span> <span><span>state</span><span>.</span><span>previous_was_open_paren</span></span>
</span></span></span></td></tr><tr><td>34</td><td><span><span><span>        <span>and</span> <span><span>state</span><span>.</span><span>previous_previous_was_identifier</span></span><span>)</span></span><span>:</span></span>
</span></td></tr><tr><td>35</td><td><span>        <span><span>state</span><span>.</span><span>seen_code</span></span> <span>=</span> <span>True</span>
</span></td></tr><tr><td>36</td><td><span>        <span>return</span> <span><span>state</span></span>
</span></td></tr><tr><td>37</td><td><span>
</span></td></tr><tr><td>38</td><td><span>    </span></td></tr><tr><td>39</td><td><span>    </span></td></tr><tr><td>40</td><td><span>    </span></td></tr><tr><td>41</td><td><span>
</span></td></tr><tr><td>42</td><td><span>    <span><span>state</span><span>.</span><span>previous_previous_was_identifier</span></span> <span>=</span> <span><span>(</span>
</span></span></td></tr><tr><td>43</td><td><span><span>        <span><span>state</span><span>.</span><span>previous_was_identifier</span></span>
</span></span></td></tr><tr><td>44</td><td><span><span>    <span>)</span></span>
</span></td></tr><tr><td>45</td><td><span>
</span></td></tr><tr><td>46</td><td><span>    <span><span>state</span><span>.</span><span>previous_was_identifier</span></span> <span>=</span> <span><span>token</span></span> <span>in</span> <span><span>(</span>
</span></span></td></tr><tr><td>47</td><td><span><span>        <span><span><span>"</span></span></span><span><span>all_caps_identifier<span>"</span></span></span><span>,</span>
</span></span></td></tr><tr><td>48</td><td><span><span>        <span><span><span>"</span></span></span><span><span>underscore_identifier<span>"</span></span></span><span>,</span>
</span></span></td></tr><tr><td>49</td><td><span><span>        <span><span><span>"</span></span></span><span><span>misc_identifier<span>"</span></span></span><span>,</span>
</span></span></td></tr><tr><td>50</td><td><span><span>    <span>)</span></span>
</span></td></tr><tr><td>51</td><td><span>    <span><span>state</span><span>.</span><span>previous_was_open_paren</span></span> <span>=</span> <span><span>token</span></span> <span>==</span> <span><span><span>"</span></span></span><span><span>open_paren<span>"</span></span></span>
</span></td></tr><tr><td>52</td><td><span>
</span></td></tr><tr><td>53</td><td><span>    <span>return</span> <span><span>state</span></span>
</span></td></tr></tbody></table></code></pre>
<p>And with that, we have a classifier that implements Rule 1.
Let’s see how well it performs by looking at its <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision and recall</a>.
The good news:</p>
<ol>
<li>
The code is simple.
</li>
<li>
It has no false positives.
</li>
<li>
It has a precision of 100 percent on our (completely made up) examples.
</li>
</ol>
<p>The bad news is that this classifier has a high false negative rate which leads to a recall of only 50 percent.</p>
<p>If we want to increase recall and add Rule 2 to the algorithm, then the code becomes more complex: we need to add more fields to the <code>State</code> class, and more <code>if</code>/<code>elif</code>/<code>else</code> statements and related housekeeping to <code>process</code>.
If we want to further refine any of the rules, then we need to add yet more complexity.</p>
<p>We see that tweaking the algorithm by hand to find improvements can get out of control.
It can easily result in spaghetti code that will be a struggle to maintain.
How can we do better?</p>
<h2 id="neural-networks-to-the-rescue">Neural networks to the rescue</h2>
<p>Mired in our predicament, we reflect on the fact that <code>contains_code</code> and <code>process</code> are a <a href="https://en.wikipedia.org/wiki/Finite-state_machine">state machine</a>.
This fact is notable because state machines can be encoded by recurrent neural networks (RNNs).<a id="fnref2" href="#fn2" role="doc-noteref"><sup>2</sup></a>
So rather than tweaking the algorithm by hand, we could find a better algorithm by training an RNN on our examples.</p>
<h3 id="the-general-idea">The general idea</h3>
<p>At a high level, an RNN is an approximation of the conditional probability
<span>\[\Pr({\rm MessageContainsCode} = 1 \mid {\rm Token}_1 = x_1, \ldots, {\rm Token}_T = x_T).\]</span>
This approximation is calculated by processing the token sequence element by element.
For each token, we calculate a vector that does what <code>State</code> did in our Python code.
This is usually called a <em>hidden</em> state because it is only an interim variable and appears in neither the input nor the output of the model.
We get it by taking into account the previous state:
<span>\[\begin{aligned}
{\rm State}_0 &amp;:= 0 \in \mathbb{R}^S \\
{\rm State}_1 &amp;:= f({\rm Token}_1, {\rm State}_0) \\
{\rm State}_2 &amp;:= f({\rm Token}_2, {\rm State}_1) \\
&amp;\;\;\vdots \\
{\rm State}_T &amp;:= f({\rm Token}_T, {\rm State}_{T-1}) \\
\end{aligned}\]</span>
where the function <span>\(f\)</span> represents the hidden layers of the recurrent network.
The message is classified based on the final state:
<span>\[\widehat{\Pr}({\rm MessageContainsCode} = 1 \mid {\rm Token}_1, \ldots) := g({\rm State}_T)\]</span>
where <span>\(g\)</span> is what is called the output layer of the network.</p>
<p>How should we parameterize the functions <span>\(f\)</span> and <span>\(g\)</span>?</p>
<h3 id="our-python-code-as-math">Our Python code as math</h3>
<p>Let’s think about what a network that encodes Rule 1 looks like.
Returning to the previous example sentence, let’s simplify the math that ensues by using fewer types of tokens:</p>
<p><img id="lgtm_tokens_simple" src="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/figure_tokens_simple_step_8.svg" width="520">
<img id="lgtm_tokens_simple_dark" src="https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/figure_tokens_simple_step_8_dark.svg" width="520"><br>


</p>
<p>Each token in the sentence is represented as a <a href="https://en.wikipedia.org/wiki/One-hot">binary vector</a>, so the input data that we get looks like this (with the zeros erased to make the table easier to read):</p>
<table>
<tbody><tr>
<th></th>
<th><span>\(x_1\)</span></th>
<th><span>\(x_2\)</span></th>
<th><span>\(x_3\)</span></th>
<th><span>\(x_4\)</span></th>
<th><span>\(x_5\)</span></th>
<th><span>\(x_6\)</span></th>
<th><span>\(x_7\)</span></th>
<th><span>\(x_8\)</span></th>
</tr>
<tr>
<td><code>identifier</code></td>
<td><span>\(1\)</span></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>open_paren</code></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>close_paren</code></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
</tr>
<tr>
<td><code>unknown</code></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td></td>
<td></td>
<td><span>\(1\)</span></td>
</tr>
</tbody></table>
<p>To model the state, we need to add three hidden layers to the network.
When we are processing the <span>\(t\)</span>-th token in the sequence, we calculate each of the three layers, one after the other.
Let’s denote the first layer by <span>\(h_t^1\)</span>, the second layer by <span>\(h_t^2\)</span>, and the third by <span>\(h_t^3\)</span>.
The first two layers hold intermediate calculations while the third layer holds our final state after processing the token.</p>
<p>The third layer, <span>\(h_t^3\)</span>, corresponds to <span>\({\rm State}_t\)</span> in the schematic presentation above, and the initial state, <span>\(h_0^3\)</span>, corresponds to <span>\({\rm State}_0\)</span>.
This table shows how the hidden state evolves:</p>
<table>
<tbody><tr>
<th></th>
<th><span>\(h_0^3\)</span></th>
<th><span>\(h_1^3\)</span></th>
<th><span>\(h_2^3\)</span></th>
<th><span>\(h_3^3\)</span></th>
<th><span>\(h_4^3\)</span></th>
<th><span>\(h_5^3\)</span></th>
<th><span>\(h_6^3\)</span></th>
<th><span>\(h_7^3\)</span></th>
<th><span>\(h_8^3\)</span></th>
</tr>
<tr>
<td><code>seen_code</code></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span>\(1\)</span></td>
<td><span>\(1\)</span></td>
</tr>
<tr>
<td><code>identifier</code></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>open_paren</code></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>p_identifier</code></td>
<td></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td><span>\(1\)</span></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>To encode the hand-written algorithm, we use the binary indicator function, <span>\(\mathbb{1}\{\cdot &gt; 0\}\)</span>, which evaluates to 1 if its input is positive and to 0 otherwise.<a id="fnref3" href="#fn3" role="doc-noteref"><sup>3</sup></a>
Of course, this would be an outstandingly bad choice if we wanted to <em>train</em> an RNN because its derivative is zero almost everywhere which breaks gradient descent.
But for now, we merely want to <em>specify</em> an RNN that mimics the hand-written algorithm.
The <code>State</code> class that we used is effectively a binary vector, and the binary indicator function ensures that the RNN’s hidden layers will be binary as well.</p>
<p>The key calculation is done in the second and third hidden layers.
The second layer checks if the token sequence <span>\((x_{t-2}, x_{t-1}, x_t)\)</span> satisfies Rule 1.
We don’t do this by referencing the three tokens directly.
Instead, we store information about them in the first hidden layer, and we reference that:</p>
<p><span>\[\begin{aligned}
h_{t,\tt this\_is\_code}^2 &amp;:= \mathbb{1}\left\{ 2 h_{t,\tt pp\_identifier}^1 h_{t,\tt p\_open\_paren}^1 h_{t,\tt close\_paren}^1 - 1 &gt; 0 \right\} \\
\end{aligned}\]</span></p>
<p>The third layer combines this check, <span>\(h_{t,\tt this\_is\_code}^2\)</span>, with a memory of whether any earlier part of the sequence has satisfied the rule:</p>
<p><span>\[\begin{aligned}
h_{t,\tt seen\_code}^3 &amp;:= \mathbb{1}\left\{ 2 \left( h_{t, \tt this\_is\_code}^2 + h_{t, \tt p\_seen\_code}^2 \right) - 1 &gt; 0 \right\} \\
\end{aligned}\]</span></p>
<p>Once we have processed all <span>\(T\)</span> tokens, the output is calculated using the final hidden layer:</p>
<p><span>\[y_T := \mathbb{1}\left\{ 2 h_{T,\tt seen\_code}^3 - 1 &gt; 0 \right\}\]</span></p>
<p>The formula for <span>\(h_{t,\tt this\_is\_code}^2\)</span> looks simple enough but it contains a multiplication between <span>\(h_{t,\tt pp\_identifier}^1\)</span>, <span>\(h_{t,\tt p\_open\_paren}^1\)</span>, and <span>\(h_{t,\tt close\_paren}^1\)</span>.
If we wanted to include this multiplicative term, we would need a higher-order RNN, similar to the second-order RNN that <a href="https://doi.org/10.1162/neco.1992.4.3.393">Giles et al. (1992)</a> used to discover state machines.
Fortunately, with binary hidden layers, we can stay within the framework of a first-order RNN by replacing the multiplication with a simple sum:</p>
<p><span>\[\begin{aligned}
h_{t,\tt this\_is\_code}^2 &amp;:= \mathbb{1}\left\{ h_{t,\tt pp\_identifier}^1 + h_{t,\tt p\_open\_paren}^1 + h_{t,\tt close\_paren}^1 - 2 &gt; 0 \right\} \\
\end{aligned}\]</span></p>
<p>(And when we will change the activation function in the next section so that we can train the network, we will see that a sum still works.)</p>
<p>Now to wrap things up, we also get some auxiliary calculations done that are needed for <span>\(h_{t,\tt this\_is\_code}^2\)</span> and <span>\(h_{t,\tt seen\_code}^3\)</span>.
In the first hidden layer, we take note of the token that we are processing and copy the previous state:</p>
<p><span>\[\begin{aligned}
h_{t,\tt identifier}^1 &amp;:= \mathbb{1}\{ 2 x_{t,\tt identifier} - 1 &gt; 0 \} \\
h_{t,\tt open\_paren}^1 &amp;:= \mathbb{1}\{ 2 x_{t,\tt open\_paren} - 1 &gt; 0 \} \\
h_{t,\tt close\_paren}^1 &amp;:= \mathbb{1}\{ 2 x_{t,\tt close\_paren} - 1 &gt; 0 \} \\
h_{t,\tt p\_seen\_code}^1 &amp;:= \mathbb{1}\{ 2 h_{t-1,\tt seen\_code}^3 - 1 &gt; 0 \} \\
h_{t,\tt p\_identifier}^1 &amp;:= \mathbb{1}\{ 2 h_{t-1,\tt identifier}^3 - 1 &gt; 0 \} \\
h_{t,\tt p\_open\_paren}^1 &amp;:= \mathbb{1}\{ 2 h_{t-1,\tt open\_paren}^3 - 1 &gt; 0 \} \\
h_{t,\tt pp\_identifier}^1 &amp;:= \mathbb{1}\{ 2 h_{t-1,\tt p\_identifier}^3 - 1 &gt; 0 \} \\
\end{aligned}\]</span></p>
<p>In the second layer, we copy some values that will be used in the third:</p>
<p><span>\[\begin{aligned}
h_{t,\tt identifier}^2 &amp;:= \mathbb{1}\left\{ 2 h_{t,\tt identifier}^1 - 1 &gt; 0 \right\} \\
h_{t,\tt open\_paren}^2 &amp;:= \mathbb{1}\left\{ 2 h_{t,\tt open\_paren}^1 - 1 &gt; 0 \right\} \\
h_{t,\tt p\_seen\_code}^2 &amp;:= \mathbb{1}\left\{ 2 h_{t,\tt p\_seen\_code}^1 - 1 &gt; 0 \right\} \\
h_{t,\tt p\_identifier}^2 &amp;:= \mathbb{1}\left\{ 2 h_{t,\tt p\_identifier}^1 - 1 &gt; 0 \right\} \\
\end{aligned}\]</span></p>
<p>And lastly, in the third layer, we maintain a memory of tokens that we can use later when we process the next token, <span>\(x_{t+1}\)</span>:</p>
<p><span>\[\begin{aligned}
h_{t,\tt identifier}^3 &amp;:= \mathbb{1}\left\{ 2 h_{t,\tt identifier}^2 - 1 &gt; 0 \right\} \\
h_{t,\tt open\_paren}^3 &amp;:= \mathbb{1}\left\{ 2 h_{t,\tt open\_paren}^2 - 1 &gt; 0 \right\} \\
h_{t,\tt p\_identifier}^3 &amp;:= \mathbb{1}\left\{ 2 h_{t,\tt p\_identifier}^2 - 1 &gt; 0 \right\} \\
\end{aligned}\]</span></p>
<p>The <a href="#lgtm_rnn_display">illustration at the beginning of the post</a> shows how this network classifies the example sentence.
I won’t do it in this post, but we could set it up in PyTorch, too, and verify that it classifies messages exactly as our hand-written algorithm does.</p>
<h2 id="training-the-network-to-discover-better-algorithms">Training the network to discover better algorithms</h2>
<p>If we have a recurrent network, then we should be able to train it.
But the network as we have parameterized it so far is not amenable to training.
Gradient descent, the algorithm that is commonly used to train neural networks, will be stuck because the binary indicator function has a zero slope almost everywhere.</p>
<h3 id="trainable-activation-functions">Trainable activation functions</h3>
<p>We overcome this hurdle by switching to an activation function called the <a href="https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29">rectified linear unit</a> (ReLU), defined as <span>\([x]^+ := x \mathbb{1}\{ x &gt; 0\}\)</span>.
The numerical constants, called <em>weights</em> and <em>biases,</em> also need to be replaced, as these are the parameters that gradient descent will estimate.
We end up with</p>
<p><span>\[\begin{aligned}
h_{t,\tt this\_is\_code}^2 &amp;:= \big[ w_{\tt pp\_identifier}^2 h_{t,\tt pp\_identifier}^1 + \\
&amp;\qquad\quad+ w_{\tt p\_open\_paren}^2 h_{t,\tt p\_open\_paren}^1 + \\
&amp;\qquad\quad+ w_{\tt close\_paren}^2 h_{t,\tt close\_paren}^1 + b_{\tt this\_is\_code}^2 \big]^+ \\
\end{aligned}\]</span></p>
<p>and so on for the hidden layers.
For the output layer, we use a sigmoid activation function:</p>
<p><span>\[y_T := \frac{1}{ 1 + \exp\left( - w_{\tt seen\_code}^y h_{T,\tt seen\_code}^3 - b_{\tt seen\_code}^y \right) }.\]</span></p>
<p>And we are ready to plug this into PyTorch and train it.</p>
<p>However, what we will find if we do that is that while it is possible to get this network to learn something from the data, its performance is far from exceptional.
We can tackle the code detection problem more effectively if we use some of the other options offered by PyTorch.</p>
<h3 id="architectures-with-more-efficient-implementations">Architectures with more efficient implementations</h3>
<p>One reason for the lackluster performance of our recurrent network is that its architecture is somewhat atypical.
As a consequence, more of our training procedure is executed in Python glue code and less of it in PyTorch’s C++ library.
An alternative that is <a href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">provided off the shelf</a> by PyTorch, and that thus has a more efficient implementation, is the Elman RNN.
Our architecture differs from Elman’s in that in our network, each hidden layer takes only the previous layer as input: the first layer for token <span>\(t\)</span> takes the third layer for token <span>\(t-1\)</span>, the second layer takes the first, and the third layer takes the second.
In vector notation:</p>
<p><span>\[\begin{aligned}
h_t^1 &amp;:= \left[ w^1 \left( x_t', {h_{t-1}^3}' \right)' + b^1 \right]^+ \\
h_t^2 &amp;:= \left[ w^2 h_t^1 + b^2 \right]^+ \\
h_t^3 &amp;:= \left[ w^3 h_t^2 + b^3 \right]^+ \\
\end{aligned}\]</span></p>
<p>If we followed Elman’s architecture, the layers would be different in two ways.
First, each hidden layer for token <span>\(t\)</span> would also take the same layer for token <span>\(t-1\)</span> as input.
Second, the first hidden layer would <em>not</em> take the final layer for the previous taken as input.
This becomes clearer when we compare the formulas shown above with those of the Elman RNN:</p>
<p><span>\[\begin{aligned}
h_t^1 &amp;:= \left[ w^1 \left( x_t', {h_{t-1}^1}' \right)' + b^1 \right]^+ \\
h_t^2 &amp;:= \left[ w^2 \left( {h_t^1}', {h_{t-1}^2}' \right)' + b^2 \right]^+ \\
h_t^3 &amp;:= \left[ w^3 \left( {h_t^2}', {h_{t-1}^3}' \right)' + b^3 \right]^+ \\
\end{aligned}\]</span></p>
<h3 id="architectures-with-more-numerically-stable-gradients">Architectures with more numerically stable gradients</h3>
<p>Another reason for the subpar performance of our recurrent network is that in real-world code review, engineers often send longer messages.
Longer messages translate into longer token sequences which can throw off the gradient descent algorithm.
Gradient descent works in theory but not always in practice: the gradients, obtained by diligently applying the chain rule, <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">can get too close to zero</a> which causes problems with numerical stability.</p>
<p>While Elman’s architecture is susceptible, too, other architectures attempt to mitigate this.
So we might well find that a <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">gated recurrent unit</a> or a <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long short term memory network</a> performs better for our code detection task.</p>
<h2 id="data-driven-discipline">Data-driven discipline</h2>
<p>Recurrent neural networks have handled our inevitable spaghetti code better than we could ourselves.
But they have done more than that: they have imposed a certain data-driven discipline on us that we would probably not follow if we wrote the algorithm by hand.
To train a network,</p>
<ol>
<li>
we need to select training and validation data sets,
</li>
<li>
we need to prelabel them, and
</li>
<li>
we need to specify a loss function that makes it explicit what we want the classifier to achieve and what we don’t.
</li>
</ol>
<p>This discipline forces us to clarify our thinking, as we will inevitably find gray areas that we didn’t expect.
And it turns out that this discipline is useful even for those problems that we decide to solve by hand.</p>
<hr>
<ol>
<li id="fn1">
<p>The first word, “LGTM,” is a commonly used abbreviation of “looks good to me.”
Why is it coded as <code>all_caps_identifier</code> rather than something else?
Because in idiomatic C, constants are given all-uppercase names, so without a list of common abbreviations at hand, we err on the side of assuming that “LGTM” could be a constant in some program code.<a href="#fnref1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn2">
<p>Marvin Minsky’s 1967 book, <em>Computation: Finite and Infinite Machines,</em> already pointed out that recurrent neural networks can encode state machines.
Later, in the late 1980s and through the 1990s, there was a burgeoning literature on how to discover state machines by training appropriately specified recurrent networks (e.g., <a href="https://doi.org/10.1162/neco.1992.4.3.393">Giles et al., 1992</a>).<a href="#fnref2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn3">
<p>As a little-known trivia, this is also occasionally called the <a href="https://en.wikipedia.org/wiki/Heaviside_step_function">Heaviside step function</a> or the Heaviside activation function.<a href="#fnref3" role="doc-backlink">↩︎</a></p>
</li>
</ol>

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A write-ahead log is not a universal part of durability (112 pts)]]></title>
            <link>https://notes.eatonphil.com/2024-07-01-a-write-ahead-log-is-not-a-universal-part-of-durability.html</link>
            <guid>40844825</guid>
            <pubDate>Mon, 01 Jul 2024 11:44:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notes.eatonphil.com/2024-07-01-a-write-ahead-log-is-not-a-universal-part-of-durability.html">https://notes.eatonphil.com/2024-07-01-a-write-ahead-log-is-not-a-universal-part-of-durability.html</a>, See on <a href="https://news.ycombinator.com/item?id=40844825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>A database does not need a write-ahead log (WAL) to achieve
durability. A database can write its long-term data structure durably
to disk before returning to a client. Granted, this is a bad idea! And
granted, a WAL <b>is</b> critical for durability <b>by design</b> in most
databases. But I think it's helpful to understand WALs by
understanding what you <b>could</b> do without them.</p>
<p>So let's look at what terrible design we can make for a durable
database that has no write-ahead log. To motivate the idea of, and
build an intuition for, a write-ahead log.</p>
<p>Thank you to Alex Miller for reviewing a version of this post.</p>
<p>But first, what is durability?</p>
<h3 id="durability">Durability</h3><p>Durability happens in the context of a request a client makes to a
data system (either an embedded system like SQLite or RocksDB or a
standalone system like Postgres). Durability is a spectrum of
guarantees the server provides when a client requests to write some
data: that either the request succeeds and the data is safely written
to disk, or the request fails and the client must retry or decide to
do something else.</p>
<p>It can be difficult to set an absolute definition for durability since
different databases have different concepts of what can go wrong with
disks (also called a "storage fault model"), or they have no concept
at all.</p>
<p>Let's start from the beginning.</p>
<h4 id="an-in-memory-database">An in-memory database</h4><p>An in-memory database has no durability at all. Here is pseudo-code
for an in-memory database service.</p>
<div><pre><span></span><span>db</span> <span>=</span> <span>btree</span><span>()</span>

<span>def</span> <span>handle_write</span><span>(</span><span>req</span><span>):</span>
  <span>db</span><span>.</span><span>update</span><span>(</span><span>req</span><span>.</span><span>key</span><span>,</span> <span>req</span><span>.</span><span>value</span><span>)</span>
  <span>return</span> <span>200</span><span>,</span> <span>{}</span>

<span>def</span> <span>handle_read</span><span>(</span><span>req</span><span>):</span>
  <span>value</span> <span>=</span> <span>db</span><span>.</span><span>read</span><span>(</span><span>req</span><span>.</span><span>key</span><span>)</span>
  <span>return</span> <span>200</span><span>,</span> <span>{</span><span>"value"</span><span>:</span> <span>value</span><span>}</span>
</pre></div>
<p>Throughout this post, for the sake of code brevity, imagine that the
environment is concurrent and that data races around shared mutable
values like <code>db</code> are protected somehow.</p>
<h4 id="writing-to-disk">Writing to disk</h4><p>If we want to achieve the most basic level of durability, we can write
this database to a file.</p>
<div><pre><span></span><span>f</span> <span>=</span> <span>open</span><span>(</span><span>"kv.db"</span><span>)</span>
<span>db</span> <span>=</span> <span>btree</span><span>.</span><span>init_from_disk</span><span>(</span><span>f</span><span>)</span>

<span>def</span> <span>handle_write</span><span>(</span><span>req</span><span>):</span>
  <span>db</span><span>.</span><span>update</span><span>(</span><span>req</span><span>.</span><span>key</span><span>,</span> <span>req</span><span>.</span><span>value</span><span>)</span>
  <span>db</span><span>.</span><span>write_to_disk</span><span>(</span><span>f</span><span>)</span>
  <span>return</span> <span>200</span><span>,</span> <span>{}</span>

<span>def</span> <span>handle_read</span><span>(</span><span>req</span><span>):</span>
  <span>value</span> <span>=</span> <span>db</span><span>.</span><span>read</span><span>(</span><span>req</span><span>.</span><span>key</span><span>)</span>
  <span>return</span> <span>200</span><span>,</span> <span>{</span><span>"value"</span><span>:</span> <span>value</span><span>}</span>
</pre></div>
<p><code>btree.write_to_disk</code> will call
<a href="https://linux.die.net/man/2/pwrite">pwrite(2)</a> under the hood. And
we'll assume it does copy-on-write for only changed pages. So imagine
we have a large database represented by a btree that takes up 10GiB on
disk. With the btree algorithm, if we write a single entry to the
btree, often only a single (often 4Kib) page will get written rather
than all pages (holding all values) in the tree. At the same time, in
the worst case, the entire tree (all 10GiB of data) may need to get
rewritten.</p>
<p>But this code isn't crash-safe. If the virtual or physical machine
this code is running on reboots, the data we wrote to the file may not
actually be on disk.</p>
<h4 id="fsync">fsync</h4><p>File data is buffered by the operating system by default. By general
consensus, writing data without flushing the operating system buffer
is not considered durable. Every so often a new database will show up
on Hacker News claiming to beat all other databases on insert speed
until a commenter points out the new database doesn't actually flush
data to disk.</p>
<p>In other words, the commonly accepted requirement for durability is
that not only do you write data to a file on disk but you
<a href="https://man7.org/linux/man-pages/man2/fsync.2.html">fsync(2)</a> the
file you wrote. This forces the operating system to flush to disk any
data it has buffered.</p>
<div><pre><span></span><span>f</span> <span>=</span> <span>open</span><span>(</span><span>"kv.db"</span><span>)</span>
<span>db</span> <span>=</span> <span>btree</span><span>.</span><span>init_from_disk</span><span>(</span><span>f</span><span>)</span>

<span>def</span> <span>handle_write</span><span>(</span><span>req</span><span>):</span>
  <span>db</span><span>.</span><span>update</span><span>(</span><span>req</span><span>.</span><span>key</span><span>,</span> <span>req</span><span>.</span><span>value</span><span>)</span>
  <span>db</span><span>.</span><span>write_to_disk</span><span>(</span><span>f</span><span>)</span>
  <span>f</span><span>.</span><span>fsync</span><span>()</span> <span># Force a flush</span>
  <span>return</span> <span>200</span><span>,</span> <span>{}</span>

<span>def</span> <span>handle_read</span><span>(</span><span>req</span><span>):</span>
  <span>value</span> <span>=</span> <span>db</span><span>.</span><span>read</span><span>(</span><span>req</span><span>.</span><span>key</span><span>)</span>
  <span>return</span> <span>200</span><span>,</span> <span>{</span><span>"value"</span><span>:</span> <span>value</span><span>}</span>
</pre></div>
<p>Furthermore you must not ignore fsync failure. How you deal with fsync
failure is up to you, but exiting immediately with a message that the
user should restore from a backup is sometimes considered acceptable.</p>
<p>Databases don't like to fsync because it's slow. Many major databases
offer modes where they do not fsync data files before returning a
success to a client. Postgres
<a href="https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-FSYNC">offers</a>
this unsafe mode, though does not default to it and warns against
it. And MongoDB defaults to this unsafe mode. See
<a href="https://www.mongodb.com/docs/manual/reference/configuration-options/#mongodb-setting-storage.journal.commitIntervalMs">storage.journal.commitIntervalMs</a>
and the <a href="https://www.mongodb.com/docs/manual/reference/write-concern/#j-option"><code>j</code> write concern
option</a>.</p>
<blockquote><p>The j option requests acknowledgment from MongoDB that the write
operation has been written to the on-disk journal.</p>
</blockquote>
<p>MongoDB defaults to committing on an interval, returning a success to
the client potentially before the write has been fsync-ed.</p>
<p>This isn't particularly a dig on MongoDB. Almost every database trades
safety for performance in some regard. For example, few databases but
SQLite and Cockroach default to Serializable Isolation. While it is
commonly agreed that basically no level below Serializable Isolation
(that all other databases default to) can be reasoned about. Other
databases offer Serializable Isolation, they just don't default to
it. Because it can be slow.</p>
<h4 id="group-commit">Group commit</h4><p>But let's get back to fsync. One way to amortize the cost of fsync is
to delay requests so that you write data from each of them and then
fsync the data from all requests. This is sometimes called group
commit.</p>
<p>For example, we could update the database in-memory but have a
background thread serialize to disk and call fsync only every 5ms.</p>
<div><pre><span></span><span>f</span> <span>=</span> <span>open</span><span>(</span><span>"kv.db"</span><span>)</span>
<span>db</span> <span>=</span> <span>btree</span><span>.</span><span>init_from_disk</span><span>(</span><span>f</span><span>)</span>

<span>group_commit_sems</span> <span>=</span> <span>[]</span>

<span>@background_worker</span><span>()</span>
<span>def</span> <span>group_commit</span><span>():</span>
  <span>for</span><span>:</span>
    <span>if</span> <span>clock</span><span>()</span> <span>%</span> <span>5</span><span>ms</span> <span>==</span> <span>0</span><span>:</span>
      <span>db</span><span>.</span><span>write_to_disk</span><span>(</span><span>f</span><span>)</span>
      <span>f</span><span>.</span><span>fsync</span><span>()</span> <span># Durably flush for the group</span>
      <span>for</span> <span>sem</span> <span>in</span> <span>group_commit_sems</span><span>:</span>
        <span>sem</span><span>.</span><span>signal</span><span>()</span>

<span>def</span> <span>handle_write</span><span>(</span><span>req</span><span>):</span>
  <span>db</span><span>.</span><span>update</span><span>(</span><span>req</span><span>.</span><span>key</span><span>,</span> <span>req</span><span>.</span><span>value</span><span>)</span>
  <span>sem</span> <span>=</span> <span>semaphore</span><span>()</span>
  <span>group_commit_sems</span><span>.</span><span>push</span><span>(</span><span>sem</span><span>)</span>
  <span>sem</span><span>.</span><span>wait</span><span>()</span>
  <span>return</span> <span>200</span><span>,</span> <span>{}</span>

<span>def</span> <span>handle_read</span><span>(</span><span>req</span><span>):</span>
  <span>value</span> <span>=</span> <span>db</span><span>.</span><span>read</span><span>(</span><span>req</span><span>.</span><span>key</span><span>)</span>
  <span>return</span> <span>200</span><span>,</span> <span>{</span><span>"value"</span><span>:</span> <span>value</span><span>}</span>
</pre></div>
<p>Although it might sound similar to what MongoDB does, the critical
difference is that <code>handle_write</code> waits to return a success until the
write is durable via fsync.</p>
<p>So to reiterate, the key idea for durability of a client request is
that you have some version of the client message stored on disk
durably with fsync before returning a success to a client.</p>
<p>From now on in this post, when you see "durable" or "durability", it
means that the data has been written and fsync-ed to disk.</p>
<h3 id="optimizing-durable-writes">Optimizing durable writes</h3><p>A key insight is that it's silly to serialize the entire permanent
structure of the database to disk every time a user writes.</p>
<p>We could just write the user's message itself to an append-only
log. And then only periodically write the entire btree to disk. So
long as we have fsync-ed the append-only log file, we can safely
return to the user even if the btree itself has not yet been written
to disk.</p>
<p>The additional logic this requires is that on startup we must read the
btree from disk and then replay the log on top of the btree.</p>
<div><pre><span></span><span>f</span> <span>=</span> <span>open</span><span>(</span><span>"kv.db"</span><span>,</span> <span>"rw"</span><span>)</span>
<span>db</span> <span>=</span> <span>btree</span><span>.</span><span>init_from_disk</span><span>(</span><span>f</span><span>)</span>

<span>log_f</span> <span>=</span> <span>open</span><span>(</span><span>"kv.log"</span><span>,</span> <span>"rw"</span><span>)</span>
<span>l</span> <span>=</span> <span>log</span><span>.</span><span>init_from_disk</span><span>()</span>
<span>for</span> <span>log</span> <span>in</span> <span>l</span><span>.</span><span>read_logs_from</span><span>(</span><span>db</span><span>.</span><span>last_log_index</span><span>):</span>
  <span>db</span><span>.</span><span>update</span><span>(</span><span>log</span><span>.</span><span>key</span><span>,</span> <span>log</span><span>.</span><span>value</span><span>)</span>

<span>group_commit_sems</span> <span>=</span> <span>[]</span>

<span>@background_worker</span><span>()</span>
<span>def</span> <span>group_commit</span><span>():</span>
  <span>for</span><span>:</span>
    <span>log_accumulator</span> <span>=</span> <span>log_page</span><span>()</span>
    <span>if</span> <span>clock</span><span>()</span> <span>%</span> <span>5</span><span>ms</span> <span>==</span> <span>0</span><span>:</span>
      <span>for</span> <span>(</span><span>log</span><span>,</span> <span>_</span><span>)</span> <span>in</span> <span>group_commit_sems</span><span>:</span>
        <span>log_accumulator</span><span>.</span><span>add</span><span>(</span><span>log</span><span>)</span>

      <span>log_f</span><span>.</span><span>write</span><span>(</span><span>log_accumulator</span><span>.</span><span>page</span><span>())</span> <span># Write out all log entries at once</span>
      <span>log_f</span><span>.</span><span>fsync</span><span>()</span> <span># Durably flush wal data</span>
      <span>for</span> <span>(</span><span>_</span><span>,</span> <span>sem</span><span>)</span> <span>in</span> <span>group_commit_sems</span><span>:</span>
        <span>sem</span><span>.</span><span>signal</span><span>()</span>

    <span>if</span> <span>clock</span><span>()</span> <span>%</span> <span>1</span><span>m</span> <span>==</span> <span>0</span><span>:</span>
      <span>db</span><span>.</span><span>write_to_disk</span><span>(</span><span>f</span><span>)</span>
      <span>f</span><span>.</span><span>fsync</span><span>()</span> <span># Durably flush db data</span>

<span>def</span> <span>handle_write</span><span>(</span><span>req</span><span>):</span>
  <span>db</span><span>.</span><span>update</span><span>(</span><span>req</span><span>.</span><span>key</span><span>,</span> <span>req</span><span>.</span><span>value</span><span>)</span>
  <span>sem</span> <span>=</span> <span>semaphore</span><span>()</span>
  <span>log</span> <span>=</span> <span>req</span>
  <span>group_commit_sems</span><span>.</span><span>push</span><span>((</span><span>log</span><span>,</span> <span>sem</span><span>))</span>
  <span>sem</span><span>.</span><span>wait</span><span>()</span> <span># This time waiting for only the log to be written and flushed, not the btree.</span>
  <span>return</span> <span>200</span><span>,</span> <span>{}</span>

<span>def</span> <span>handle_read</span><span>(</span><span>req</span><span>):</span>
  <span>value</span> <span>=</span> <span>db</span><span>.</span><span>read</span><span>(</span><span>req</span><span>.</span><span>key</span><span>)</span>
  <span>return</span> <span>200</span><span>,</span> <span>{</span><span>"value"</span><span>:</span> <span>value</span><span>}</span>
</pre></div>
<p>This is a write-ahead log!</p>
<p>Consider a few scenarios. One request writes the smallest key ever
seen. And one request within the same millisecond writes the largest
key ever seen. Writing these to disk on the btree means modifying at
least two pages spread out in space on disk.</p>
<p>But if we only have to durably write these two messages to a log, they
can likely both be included in the same log page. ("Likely" so long as
key and values are small enough that multiple can fit into the same
page.)</p>
<p>That is, it's cheaper to write only these small messages representing
the client request to disk. And we save the structured btree
persistence for a less frequent durable write.</p>
<h3 id="filesystem-and-disk-bugs">Filesystem and disk bugs</h3><p>Sometimes filesystems will write data to the wrong place. Sometimes
disks corrupt data. A solution to both of these is to checksum the
data on write, store the checksum on disk, and confirm the checksum on
read. This combined with a background process called scrubbing to
validate unread data can help you learn quickly when your data has
been corrupted and you must recover from backup.</p>
<p>MongoDB's default storage engine WiredTiger <b>does</b> checksum data <a href="https://github.com/wiredtiger/wiredtiger/blob/develop/src/docs/tune-checksum.dox#L3">by
default</a>.</p>
<p>But some databases famous for integrity do not. Postgres does <a href="https://www.postgresql.org/docs/current/checksums.html">no data
checksumming</a>
by default:</p>
<blockquote><p>By default, data pages are not protected by checksums, but this can
optionally be enabled for a cluster. When enabled, each data page
includes a checksum that is updated when the page is written and
verified each time the page is read. Only data pages are protected by
checksums; internal data structures and temporary files are not.</p>
</blockquote>
<p>SQLite likewise does no checksumming by default. Checksumming is an
<a href="https://www.sqlite.org/cksumvfs.html">optional extension</a>:</p>
<blockquote><p>The checksum VFS extension is a VFS shim that adds an 8-byte
checksum to the end of every page in an SQLite database. The checksum
is added as each page is written and verified as each page is
read. The checksum is intended to help detect database corruption
caused by random bit-flips in the mass storage device.</p>
</blockquote>
<p>But even this isn't perfect. Disks and nodes can fail completely. At
that point you can only improve durability by introducing redundancy
across disks (and/or nodes), for example, via distributed consensus.</p>
<h3 id="other-reasons-you-<em>need</em>-a-wal?">Other reasons you <em>need</em> a WAL?</h3><p>Some databases (like SQLite) require a write-ahead log to implement
aspects of ACID transactions. But this need not be a requirement for
ACID transactions if you do MVCC (SQLite does not). See my previous
post on <a href="https://notes.eatonphil.com/2024-05-16-mvcc.html">implementing
MVCC</a> for details.</p>
<p>Logical replication (also called change data capture (CDC)) is another
interesting feature that requires a write-ahead log. The idea is that
the log already preserves the exact order and changes that affect the
database's "state machine". So we could copy these changes out of the
system by tracking the write-ahead log, preserving change order, and
apply these changes to a foreign system.</p>
<p>But again, just CDC is not about durability. It's an ancillary feature
that write-ahead logs make simple.</p>
<h3 id="conclusion">Conclusion</h3><p>A few key points. One, durability primarily matters if it is
established before returning a success to the client. Second, a
write-ahead log is a cheap way to get durability.</p>
<p>And finally, durability is a spectrum. You need to read the docs for
your database to understand what it does and does not.</p>
<blockquote><p lang="en" dir="ltr">Here's a new post about durability and write-ahead logs. Write-ahead logs are used almost everywhere. But to build an intuition for why, it is helpful to imagine what you would do without a WAL. And to explore the meaning of durability.<a href="https://t.co/nzS2pMz22z">https://t.co/nzS2pMz22z</a> <a href="https://t.co/m1n9x8CNcp">pic.twitter.com/m1n9x8CNcp</a></p>— Phil Eaton (@eatonphil) <a href="https://twitter.com/eatonphil/status/1807741130093556098?ref_src=twsrc%5Etfw">July 1, 2024</a></blockquote> 

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenSSH Race condition resulting in potential remote code execution (106 pts)]]></title>
            <link>https://www.openssh.com/txt/release-9.8</link>
            <guid>40844035</guid>
            <pubDate>Mon, 01 Jul 2024 09:28:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openssh.com/txt/release-9.8">https://www.openssh.com/txt/release-9.8</a>, See on <a href="https://news.ycombinator.com/item?id=40844035">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I created an After Effects alternative (718 pts)]]></title>
            <link>https://pikimov.com/</link>
            <guid>40843867</guid>
            <pubDate>Mon, 01 Jul 2024 08:57:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pikimov.com/">https://pikimov.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40843867">Hacker News</a></p>
<div id="readability-page-1" class="page">

    <!-- GA 4 -->
<!-- Google tag (gtag.js) -->



        


        <section>

            <div>
                <h2>
                    <img id="logo" src="https://pikimov.com/images/logo-pikimov.png" alt="Pikimov logo">
                </h2>

                <h2>Online motion design and video editor</h2>
                <p><span><img src="https://pikimov.com/images/icon-checkbox.png" alt="checkbox">100% Free</span>
                    <span><img src="https://pikimov.com/images/icon-checkbox.png" alt="checkbox">No sign up</span>
                    <span><img src="https://pikimov.com/images/icon-checkbox.png" alt="checkbox">No AI</span>
                </p>

                <p><a id="b_start" title="pikimov app" href="https://pikimov.com/app/">Get started</a>

            </p></div>
            
            <video muted="" autoplay="" playsinline="" loop="" disablepictureinpicture="" poster="https://pikimov.com/videos/cover-pikimov-home-03.webp">
                <source src="https://pikimov.com/videos/pikimov-home-03.mp4" type="video/mp4">
            </video>

            <p><a href="https://pikimov.com/"><img src="https://pikimov.com/images/en-flag.png" alt="english"></a>
                <a href="https://pikimov.com/alternative-fr/"><img src="https://pikimov.com/images/fr-flag.png" alt="francais"></a>
            </p>

        </section>



        <section>
            <div>
                <h3>
                   Free alternative to editors such as Adobe After Effects
                </h3>
                <div>

                    <ul>
                        <li>Web-based: nothing to install</li>
                        <li>For Windows, macOS and Linux</li>
                        <li>Import images, videos, audios and 3D models</li>
                        <li>Video effects</li>
                        <li>Layer based compositions</li>
                        <li>Keyframes animation system</li>
                        <li>Your files stay on your computer, they are not uploaded to a server</li>
                    </ul>

                </div>

                <div>
                    <p>
                        Create motion design compositions, quickly crop videos online, or trim videos to make them shorter. 
                    </p>
                    
                    <p>
                        <a href="https://pikimov.com/faq">Read the FAQ</a> to find answers about common questions: Firefox support, stand-alone version...
                    </p>
                </div>

            </div>
            <p>
                                <iframe width="560" height="315" src="https://www.youtube.com/embed/pQPxcnpRgIM?si=A8sZl-yWTtCZ_YjL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
            </p>
        </section>


        <section>

            <a id="b_start2" title="Pikimov video editor" href="https://pikimov.com/app/">Start video editor</a>

        </section>

        <section id="features">

            <div>
                <h3>Web based editor</h3>
                <p>Nothing to install, works on Windows, macOS and Linux</p>
                <video muted="" autoplay="" playsinline="" loop="" disablepictureinpicture="">
                    <source src="https://pikimov.com/videos/features/web.mp4" type="video/mp4">
                </video>   
            </div>

            <div>
                <h3>Large choice of imports</h3>
                <p>Mix images, videos, audio files and 3D models (glb format)</p>
                <video muted="" autoplay="" playsinline="" loop="" disablepictureinpicture="">
                    <source src="https://pikimov.com/videos/features/imports.mp4" type="video/mp4">
                </video>   
            </div>

            <div>
                <h3>Layers based editing</h3>
                                <p>Layer based compositions, similar to After Effects workflow</p>
                <video muted="" autoplay="" playsinline="" loop="" disablepictureinpicture="">
                    <source src="https://pikimov.com/videos/features/layers.mp4" type="video/mp4">
                </video>   
            </div>

            <div>
                <h3>Video effects</h3>
                <p>Color correction, blur, glitch effects, green screen removal...</p>
                <video muted="" autoplay="" playsinline="" loop="" disablepictureinpicture="">
                    <source src="https://pikimov.com/videos/features/video_fx.mp4" type="video/mp4">
                </video>   
            </div>

            <div>
                <h3>Keyframed</h3>
                <p>Keyframes animation system to animate almost all properties</p>
                <video muted="" autoplay="" playsinline="" loop="" disablepictureinpicture="">
                    <source src="https://pikimov.com/videos/features/keyframes.mp4" type="video/mp4">
                </video>   
            </div>

            <div>
                <h3>Privacy respected</h3>
                <p>Your files stay on your computer, they are not uploaded to a server</p>
                <video muted="" autoplay="" playsinline="" loop="" disablepictureinpicture="">
                    <source src="https://pikimov.com/videos/features/privacy.mp4" type="video/mp4">
                </video>   
            </div>

<!--
    ok on slow connextions 
-->

        </section>

        <p><a href="https://www.producthunt.com/posts/pikimov?utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-pikimov" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=457708&amp;theme=light" alt="Pikimov - free online motion design and video editor | Product Hunt" width="250" height="54"></a>
        </p>

        

    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[My finetuned models beat OpenAI's GPT-4 (346 pts)]]></title>
            <link>https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html</link>
            <guid>40843848</guid>
            <pubDate>Mon, 01 Jul 2024 08:53:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html">https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html</a>, See on <a href="https://news.ycombinator.com/item?id=40843848">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">




<p><a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html">My last post</a> outlined the kinds of evaluation I need and want to understand how well my finetuned LLM is performing in the task of structured data extraction from press releases. Let’s start with the core metric I’m interested in, accuracy, and then later we can dive into some of the other evaluation metrics as well.</p>
<section id="tldr">
<h2 data-anchor-id="tldr">TL;DR</h2>
<p>The headline for this post could well have been: finetuned models beat OpenAI, but evals were a bit painful to implement. There’s a lot of hidden code here in this post and it was slow to run. This step was the first time during the work for the finetuning course where I felt the pain and tradeoffs around the choice to finetune. I can see that without a system of some kind to handle this, the complexity of maintaining it all will start to mount up. But more on that at the end!</p>
<p>This is a long post with lots of detail. I’ve tried to minimise the amount of code you see, but if you want to see how the charts or evals were done, expand the ‘code’ sections. If you’re interested in cutting straight to the aggregate results, <a href="#final-aggregate-scores-for-the-models">click here</a> to go to the end of this post. (To see the rest of the blog posts about this project, please click <a href="https://mlops.systems/index.html#category=isafpr">here</a>. Some context: I’m doing some finetuning as part of <a href="https://maven.com/parlance-labs/fine-tuning">the Hamel Husain / Dan Becker Finetuning course on Maven</a> using <a href="https://mlops.systems/posts/2024-03-24-publishing-afghanistan-dataset-huggingface.html">some data I collected and labeled</a> a few years back that makes for a cool little test of how good it works for structured data extraction.)</p>
</section>
<section id="loading-the-datasets">
<h2 data-anchor-id="loading-the-datasets">Loading the datasets</h2>
<p>The data is all available on the Hugging Face Hub in a public repository, and for the purposes of these evaluations I want to use the <code>test</code> split of the dataset since none of our models have seen that data yet so it’s good for determining how well our model performs with new data.</p>
<div data-execution_count="1">
<details>
<summary>Code</summary>
<div id="cb1"><pre><code><span id="cb1-1"><span>from</span> datasets <span>import</span> load_dataset</span>
<span id="cb1-2"><span>import</span> pandas <span>as</span> pd</span>
<span id="cb1-3"><span>from</span> rich <span>import</span> <span>print</span></span>
<span id="cb1-4"></span>
<span id="cb1-5">test_dataset <span>=</span> load_dataset(<span>"strickvl/isafpressreleases"</span>, split<span>=</span><span>"test"</span>)</span>
<span id="cb1-6">test_df <span>=</span> pd.DataFrame(test_dataset)</span></code></pre></div>
</details>
</div>
<div data-execution_count="2">
<pre><code>Dataset({
    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],
    num_rows: 724
})</code></pre>
</div>
<p>We’ll first add an extra column to our <code>DataFrame</code> and then make a prediction for each and every row in the dataset. We’ll store a copy of the prediction to the column so as to make sure we don’t have to do this compute-intensive step repeatedly.</p>
<p>But first we’ll assemple the data as Pydantic objects so as to handle validation and other quality of life features.</p>
<div data-execution_count="3">
<details>
<summary>Code</summary>
<div id="cb4"><pre><code><span id="cb4-1"><span>from</span> enum <span>import</span> Enum</span>
<span id="cb4-2"><span>from</span> typing <span>import</span> Dict, Set, Annotated, Optional</span>
<span id="cb4-3"><span>from</span> pydantic <span>import</span> BaseModel, Field, validator, ValidationInfo</span>
<span id="cb4-4"><span>from</span> datetime <span>import</span> date</span>
<span id="cb4-5"></span>
<span id="cb4-6"></span>
<span id="cb4-7"><span>class</span> EventType(<span>str</span>, Enum):</span>
<span id="cb4-8">    airstrike <span>=</span> <span>"airstrike"</span></span>
<span id="cb4-9">    detention <span>=</span> <span>"detention"</span></span>
<span id="cb4-10">    captureandkill <span>=</span> <span>"captureandkill"</span></span>
<span id="cb4-11">    insurgentskilled <span>=</span> <span>"insurgentskilled"</span></span>
<span id="cb4-12">    exchangeoffire <span>=</span> <span>"exchangeoffire"</span></span>
<span id="cb4-13">    civiliancasualty <span>=</span> <span>"civiliancasualty"</span></span>
<span id="cb4-14"></span>
<span id="cb4-15"></span>
<span id="cb4-16"><span>class</span> Province(<span>str</span>, Enum):</span>
<span id="cb4-17">    badakhshan <span>=</span> <span>"badakhshan"</span></span>
<span id="cb4-18">    badghis <span>=</span> <span>"badghis"</span></span>
<span id="cb4-19">    baghlan <span>=</span> <span>"baghlan"</span></span>
<span id="cb4-20">    balkh <span>=</span> <span>"balkh"</span></span>
<span id="cb4-21">    bamyan <span>=</span> <span>"bamyan"</span></span>
<span id="cb4-22">    day_kundi <span>=</span> <span>"day_kundi"</span></span>
<span id="cb4-23">    farah <span>=</span> <span>"farah"</span></span>
<span id="cb4-24">    faryab <span>=</span> <span>"faryab"</span></span>
<span id="cb4-25">    ghazni <span>=</span> <span>"ghazni"</span></span>
<span id="cb4-26">    ghor <span>=</span> <span>"ghor"</span></span>
<span id="cb4-27">    helmand <span>=</span> <span>"helmand"</span></span>
<span id="cb4-28">    herat <span>=</span> <span>"herat"</span></span>
<span id="cb4-29">    jowzjan <span>=</span> <span>"jowzjan"</span></span>
<span id="cb4-30">    kabul <span>=</span> <span>"kabul"</span></span>
<span id="cb4-31">    kandahar <span>=</span> <span>"kandahar"</span></span>
<span id="cb4-32">    kapisa <span>=</span> <span>"kapisa"</span></span>
<span id="cb4-33">    khost <span>=</span> <span>"khost"</span></span>
<span id="cb4-34">    kunar <span>=</span> <span>"kunar"</span></span>
<span id="cb4-35">    kunduz <span>=</span> <span>"kunduz"</span></span>
<span id="cb4-36">    laghman <span>=</span> <span>"laghman"</span></span>
<span id="cb4-37">    logar <span>=</span> <span>"logar"</span></span>
<span id="cb4-38">    nangarhar <span>=</span> <span>"nangarhar"</span></span>
<span id="cb4-39">    nimroz <span>=</span> <span>"nimroz"</span></span>
<span id="cb4-40">    nuristan <span>=</span> <span>"nuristan"</span></span>
<span id="cb4-41">    paktya <span>=</span> <span>"paktya"</span></span>
<span id="cb4-42">    paktika <span>=</span> <span>"paktika"</span></span>
<span id="cb4-43">    panjshir <span>=</span> <span>"panjshir"</span></span>
<span id="cb4-44">    parwan <span>=</span> <span>"parwan"</span></span>
<span id="cb4-45">    samangan <span>=</span> <span>"samangan"</span></span>
<span id="cb4-46">    sar_e_pul <span>=</span> <span>"sar_e_pul"</span></span>
<span id="cb4-47">    takhar <span>=</span> <span>"takhar"</span></span>
<span id="cb4-48">    uruzgan <span>=</span> <span>"uruzgan"</span></span>
<span id="cb4-49">    wardak <span>=</span> <span>"wardak"</span></span>
<span id="cb4-50">    zabul <span>=</span> <span>"zabul"</span></span>
<span id="cb4-51"></span>
<span id="cb4-52"></span>
<span id="cb4-53"><span>class</span> TargetGroup(<span>str</span>, Enum):</span>
<span id="cb4-54">    taliban <span>=</span> <span>"taliban"</span></span>
<span id="cb4-55">    haqqani <span>=</span> <span>"haqqani"</span></span>
<span id="cb4-56">    criminals <span>=</span> <span>"criminals"</span></span>
<span id="cb4-57">    aq <span>=</span> <span>"aq"</span></span>
<span id="cb4-58">    hig <span>=</span> <span>"hig"</span></span>
<span id="cb4-59">    let <span>=</span> <span>"let"</span></span>
<span id="cb4-60">    imu <span>=</span> <span>"imu"</span></span>
<span id="cb4-61">    judq <span>=</span> <span>"judq"</span></span>
<span id="cb4-62">    iju <span>=</span> <span>"iju"</span></span>
<span id="cb4-63">    hik <span>=</span> <span>"hik"</span></span>
<span id="cb4-64">    ttp <span>=</span> <span>"ttp"</span></span>
<span id="cb4-65">    other <span>=</span> <span>"other"</span></span>
<span id="cb4-66"></span>
<span id="cb4-67"></span>
<span id="cb4-68"><span>def</span> validate_event_type(value: <span>str</span>):</span>
<span id="cb4-69">    valid_values <span>=</span> [</span>
<span id="cb4-70">        <span>"airstrike"</span>,</span>
<span id="cb4-71">        <span>"detention"</span>,</span>
<span id="cb4-72">        <span>"captureandkill"</span>,</span>
<span id="cb4-73">        <span>"insurgentskilled"</span>,</span>
<span id="cb4-74">        <span>"exchangeoffire"</span>,</span>
<span id="cb4-75">        <span>"civiliancasualty"</span>,</span>
<span id="cb4-76">    ]</span>
<span id="cb4-77">    <span>if</span> value.lower() <span>not</span> <span>in</span> valid_values:</span>
<span id="cb4-78">        <span>return</span> <span>"other"</span></span>
<span id="cb4-79">    <span>return</span> value.lower()</span>
<span id="cb4-80"></span>
<span id="cb4-81"></span>
<span id="cb4-82"><span>def</span> validate_province(value: <span>str</span>):</span>
<span id="cb4-83">    valid_values <span>=</span> [</span>
<span id="cb4-84">        <span>"badakhshan"</span>,</span>
<span id="cb4-85">        <span>"badghis"</span>,</span>
<span id="cb4-86">        <span>"baghlan"</span>,</span>
<span id="cb4-87">        <span>"balkh"</span>,</span>
<span id="cb4-88">        <span>"bamyan"</span>,</span>
<span id="cb4-89">        <span>"day_kundi"</span>,</span>
<span id="cb4-90">        <span>"farah"</span>,</span>
<span id="cb4-91">        <span>"faryab"</span>,</span>
<span id="cb4-92">        <span>"ghazni"</span>,</span>
<span id="cb4-93">        <span>"ghor"</span>,</span>
<span id="cb4-94">        <span>"helmand"</span>,</span>
<span id="cb4-95">        <span>"herat"</span>,</span>
<span id="cb4-96">        <span>"jowzjan"</span>,</span>
<span id="cb4-97">        <span>"kabul"</span>,</span>
<span id="cb4-98">        <span>"kandahar"</span>,</span>
<span id="cb4-99">        <span>"kapisa"</span>,</span>
<span id="cb4-100">        <span>"khost"</span>,</span>
<span id="cb4-101">        <span>"kunar"</span>,</span>
<span id="cb4-102">        <span>"kunduz"</span>,</span>
<span id="cb4-103">        <span>"laghman"</span>,</span>
<span id="cb4-104">        <span>"logar"</span>,</span>
<span id="cb4-105">        <span>"nangarhar"</span>,</span>
<span id="cb4-106">        <span>"nimroz"</span>,</span>
<span id="cb4-107">        <span>"nuristan"</span>,</span>
<span id="cb4-108">        <span>"paktya"</span>,</span>
<span id="cb4-109">        <span>"paktika"</span>,</span>
<span id="cb4-110">        <span>"panjshir"</span>,</span>
<span id="cb4-111">        <span>"parwan"</span>,</span>
<span id="cb4-112">        <span>"samangan"</span>,</span>
<span id="cb4-113">        <span>"sar_e_pul"</span>,</span>
<span id="cb4-114">        <span>"takhar"</span>,</span>
<span id="cb4-115">        <span>"uruzgan"</span>,</span>
<span id="cb4-116">        <span>"wardak"</span>,</span>
<span id="cb4-117">        <span>"zabul"</span>,</span>
<span id="cb4-118">    ]</span>
<span id="cb4-119">    <span>if</span> value.lower() <span>not</span> <span>in</span> valid_values:</span>
<span id="cb4-120">        <span>return</span> <span>"other"</span></span>
<span id="cb4-121">    <span>return</span> value.lower()</span>
<span id="cb4-122"></span>
<span id="cb4-123"></span>
<span id="cb4-124"><span>def</span> validate_target_group(value: <span>str</span>):</span>
<span id="cb4-125">    valid_values <span>=</span> [</span>
<span id="cb4-126">        <span>"taliban"</span>,</span>
<span id="cb4-127">        <span>"haqqani"</span>,</span>
<span id="cb4-128">        <span>"criminals"</span>,</span>
<span id="cb4-129">        <span>"aq"</span>,</span>
<span id="cb4-130">        <span>"hig"</span>,</span>
<span id="cb4-131">        <span>"let"</span>,</span>
<span id="cb4-132">        <span>"imu"</span>,</span>
<span id="cb4-133">        <span>"judq"</span>,</span>
<span id="cb4-134">        <span>"iju"</span>,</span>
<span id="cb4-135">        <span>"hik"</span>,</span>
<span id="cb4-136">        <span>"ttp"</span>,</span>
<span id="cb4-137">        <span>"other"</span>,</span>
<span id="cb4-138">    ]</span>
<span id="cb4-139">    <span>if</span> value.lower() <span>not</span> <span>in</span> valid_values:</span>
<span id="cb4-140">        <span>return</span> <span>"other"</span></span>
<span id="cb4-141">    <span>return</span> value.lower()</span>
<span id="cb4-142"></span>
<span id="cb4-143"></span>
<span id="cb4-144"><span>class</span> IsafEvent(BaseModel):</span>
<span id="cb4-145">    name: <span>str</span> <span>=</span> Field(</span>
<span id="cb4-146">        description<span>=</span><span>"A title or name for the event which summarises the event as a headline"</span></span>
<span id="cb4-147">    )</span>
<span id="cb4-148">    text: Optional[<span>str</span>] <span>=</span> Field(description<span>=</span><span>"The full text of the press release"</span>)</span>
<span id="cb4-149">    start_date: date <span>=</span> Field(</span>
<span id="cb4-150">        description<span>=</span><span>"The start date of the event in YYYY-MM-DD format"</span></span>
<span id="cb4-151">    )</span>
<span id="cb4-152">    event_type: Set[Annotated[<span>str</span>, Field(validator<span>=</span>validate_event_type)]] <span>=</span> Field(</span>
<span id="cb4-153">        description<span>=</span><span>"The event type. Can be multiple types."</span></span>
<span id="cb4-154">    )</span>
<span id="cb4-155">    province: Set[Annotated[<span>str</span>, Field(validator<span>=</span>validate_province)]] <span>=</span> Field(</span>
<span id="cb4-156">        description<span>=</span><span>"The province in which the event occurred. Can be multiple provinces."</span></span>
<span id="cb4-157">    )</span>
<span id="cb4-158">    target_group: Set[Annotated[<span>str</span>, Field(validator<span>=</span>validate_target_group)]] <span>=</span> Field(</span>
<span id="cb4-159">        description<span>=</span><span>"The group that was targetted during the event. Can be multiple groups."</span></span>
<span id="cb4-160">    )</span>
<span id="cb4-161">    min_killed: <span>int</span> <span>=</span> Field(</span>
<span id="cb4-162">        description<span>=</span><span>"The minimum number of people killed during the event"</span></span>
<span id="cb4-163">    )</span>
<span id="cb4-164">    min_captured: <span>int</span> <span>=</span> Field(</span>
<span id="cb4-165">        description<span>=</span><span>"The minimum number of people captured during the event"</span></span>
<span id="cb4-166">    )</span>
<span id="cb4-167">    killq: <span>bool</span> <span>=</span> Field(</span>
<span id="cb4-168">        description<span>=</span><span>"Whether someone was killed or not during the event"</span></span>
<span id="cb4-169">    )</span>
<span id="cb4-170">    captureq: <span>bool</span> <span>=</span> Field(</span>
<span id="cb4-171">        description<span>=</span><span>"Whether someone was captured or not during the event"</span></span>
<span id="cb4-172">    )</span>
<span id="cb4-173">    killcaptureraid: <span>bool</span> <span>=</span> Field(</span>
<span id="cb4-174">        description<span>=</span><span>"Whether the event was a so-called 'kill-capture raid'."</span></span>
<span id="cb4-175">    )</span>
<span id="cb4-176">    airstrike: <span>bool</span> <span>=</span> Field(</span>
<span id="cb4-177">        description<span>=</span><span>"Whether an airstrike was used during the event"</span></span>
<span id="cb4-178">    )</span>
<span id="cb4-179">    noshotsfired: <span>bool</span> <span>=</span> Field(</span>
<span id="cb4-180">        description<span>=</span><span>"Whether no shots were fired during the event"</span></span>
<span id="cb4-181">    )</span>
<span id="cb4-182">    min_leaders_killed: <span>int</span> <span>=</span> Field(</span>
<span id="cb4-183">        description<span>=</span><span>"The minimum number of leaders killed during the event"</span></span>
<span id="cb4-184">    )</span>
<span id="cb4-185">    min_leaders_captured: <span>int</span> <span>=</span> Field(</span>
<span id="cb4-186">        description<span>=</span><span>"The minimum number of leaders captured during the event"</span></span>
<span id="cb4-187">    )</span>
<span id="cb4-188">    predictions: Dict[<span>str</span>, <span>str</span>] <span>=</span> Field(</span>
<span id="cb4-189">        default<span>=</span>{},</span>
<span id="cb4-190">        description<span>=</span><span>"The predictions from the model. Keys are the model name and the value is the prediction"</span>,</span>
<span id="cb4-191">    )</span>
<span id="cb4-192"></span>
<span id="cb4-193">    <span>class</span> Config:</span>
<span id="cb4-194">        arbitrary_types_allowed <span>=</span> <span>True</span></span></code></pre></div>
</details>
</div>
<p>Here’s what a couple of examples of our training data looks like as Pydantic models when we pass them in:</p>
<div data-execution_count="4">
<details>
<summary>Code</summary>
<div id="cb5"><pre><code><span id="cb5-1"><span>from</span> typing <span>import</span> List</span>
<span id="cb5-2"></span>
<span id="cb5-3">events: List[IsafEvent] <span>=</span> []</span>
<span id="cb5-4"></span>
<span id="cb5-5"><span>for</span> i, row <span>in</span> <span>list</span>(test_df.iterrows()):</span>
<span id="cb5-6">    event_types <span>=</span> <span>set</span>(</span>
<span id="cb5-7">        eventtype.strip().lower() <span>for</span> eventtype <span>in</span> row[<span>"eventtype"</span>].split(<span>","</span>)</span>
<span id="cb5-8">    )</span>
<span id="cb5-9">    provinces <span>=</span> <span>set</span>(province.strip().lower() <span>for</span> province <span>in</span> row[<span>"province"</span>].split(<span>","</span>))</span>
<span id="cb5-10">    target_groups <span>=</span> <span>set</span>(</span>
<span id="cb5-11">        target_group.strip().lower() <span>for</span> target_group <span>in</span> row[<span>"targetgroup"</span>].split(<span>","</span>)</span>
<span id="cb5-12">    )</span>
<span id="cb5-13"></span>
<span id="cb5-14">    events.append(</span>
<span id="cb5-15">        IsafEvent(</span>
<span id="cb5-16">            name<span>=</span>row[<span>"name"</span>],</span>
<span id="cb5-17">            text<span>=</span>row[<span>"text"</span>],</span>
<span id="cb5-18">            start_date<span>=</span>row[<span>"StartDate"</span>].to_pydatetime().date(),</span>
<span id="cb5-19">            event_type<span>=</span>event_types,</span>
<span id="cb5-20">            province<span>=</span>provinces,</span>
<span id="cb5-21">            target_group<span>=</span>target_groups,</span>
<span id="cb5-22">            min_killed<span>=</span><span>int</span>(row[<span>"minkilled"</span>]),</span>
<span id="cb5-23">            min_captured<span>=</span><span>int</span>(row[<span>"mincaptured"</span>]),</span>
<span id="cb5-24">            killq<span>=</span>row[<span>"killq"</span>] <span>==</span> <span>"true"</span>,</span>
<span id="cb5-25">            captureq<span>=</span>row[<span>"captureq"</span>] <span>==</span> <span>"true"</span>,</span>
<span id="cb5-26">            killcaptureraid<span>=</span>row[<span>"killcaptureraid"</span>] <span>==</span> <span>"true"</span>,</span>
<span id="cb5-27">            airstrike<span>=</span>row[<span>"airstrike"</span>] <span>==</span> <span>"true"</span>,</span>
<span id="cb5-28">            noshotsfired<span>=</span>row[<span>"noshotsfired"</span>] <span>==</span> <span>"true"</span>,</span>
<span id="cb5-29">            min_leaders_killed<span>=</span><span>int</span>(row[<span>"minleaderskilled"</span>]),</span>
<span id="cb5-30">            min_leaders_captured<span>=</span><span>int</span>(row[<span>"minleaderscaptured"</span>]),</span>
<span id="cb5-31">        )</span>
<span id="cb5-32">    )</span>
<span id="cb5-33"></span>
<span id="cb5-34"><span>print</span>(events[:<span>2</span>])</span></code></pre></div>
</details>
<div>
<pre><span>[</span>
    <span>IsafEvent</span><span>(</span>
        <span>name</span>=<span>'5'</span>,
        <span>text</span>=<span>'2013-01-S-025\n\nKABUL, Afghanistan (Jan. 25, 2013)\nDuring a security operation in Andar district, </span>
<span>Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a </span>
<span>group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire </span>
<span>attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan </span>
<span>National Police in Ghazni province.'</span>,
        <span>start_date</span>=<span>datetime</span><span>.date</span><span>(</span><span>2013</span>, <span>1</span>, <span>24</span><span>)</span>,
        <span>event_type</span>=<span>{</span><span>'insurgentskilled'</span><span>}</span>,
        <span>province</span>=<span>{</span><span>'ghazni'</span><span>}</span>,
        <span>target_group</span>=<span>{</span><span>'taliban'</span><span>}</span>,
        <span>min_killed</span>=<span>1</span>,
        <span>min_captured</span>=<span>0</span>,
        <span>killq</span>=<span>True</span>,
        <span>captureq</span>=<span>False</span>,
        <span>killcaptureraid</span>=<span>False</span>,
        <span>airstrike</span>=<span>False</span>,
        <span>noshotsfired</span>=<span>False</span>,
        <span>min_leaders_killed</span>=<span>1</span>,
        <span>min_leaders_captured</span>=<span>0</span>,
        <span>predictions</span>=<span>{}</span>
    <span>)</span>,
    <span>IsafEvent</span><span>(</span>
        <span>name</span>=<span>'2'</span>,
        <span>text</span>=<span>'2011-11-S-034\nISAF Joint Command - Afghanistan\nFor Immediate Release\n\nKABUL, Afghanistan (Nov. </span>
<span>20, 2011)\nA coalition security force detained numerous suspected insurgents during an operation in Marjeh </span>
<span>district, Helmand province, yesterday.  The force conducted the operation after receiving information that a group </span>
<span>of insurgents were at a compound in the area.  After calling for the men inside to come out peacefully, the </span>
<span>insurgents emerged and were detained without incident.'</span>,
        <span>start_date</span>=<span>datetime</span><span>.date</span><span>(</span><span>2011</span>, <span>11</span>, <span>19</span><span>)</span>,
        <span>event_type</span>=<span>{</span><span>'detention'</span><span>}</span>,
        <span>province</span>=<span>{</span><span>'helmand'</span><span>}</span>,
        <span>target_group</span>=<span>{</span><span>''</span><span>}</span>,
        <span>min_killed</span>=<span>0</span>,
        <span>min_captured</span>=<span>4</span>,
        <span>killq</span>=<span>False</span>,
        <span>captureq</span>=<span>True</span>,
        <span>killcaptureraid</span>=<span>True</span>,
        <span>airstrike</span>=<span>False</span>,
        <span>noshotsfired</span>=<span>False</span>,
        <span>min_leaders_killed</span>=<span>0</span>,
        <span>min_leaders_captured</span>=<span>0</span>,
        <span>predictions</span>=<span>{}</span>
    <span>)</span>
<span>]</span>
</pre>
</div>
</div>
<p>So when we’re making the prediction we’re hoping to get a JSON string like this out from the model:</p>
<div data-execution_count="5">
<div id="cb6"><pre><code><span id="cb6-1">json_str <span>=</span> events[<span>0</span>].model_dump_json(exclude<span>=</span>{<span>"text"</span>, <span>"predictions"</span>})</span>
<span id="cb6-2"><span>print</span>(json_str)</span></code></pre></div>
<div>
<pre><span>{</span><span>"name"</span>:<span>"5"</span>,<span>"start_date"</span>:<span>"2013-01-24"</span>,<span>"event_type"</span>:<span>[</span><span>"insurgentskilled"</span><span>]</span>,<span>"province"</span>:<span>[</span><span>"ghazni"</span><span>]</span>,<span>"target_group"</span>:<span>[</span><span>"tali</span>
<span>ban"</span><span>]</span>,<span>"min_killed"</span>:<span>1</span>,<span>"min_captured"</span>:<span>0</span>,<span>"killq"</span>:true,<span>"captureq"</span>:false,<span>"killcaptureraid"</span>:false,<span>"airstrike"</span>:false,<span>"nosh</span>
<span>otsfired"</span>:false,<span>"min_leaders_killed"</span>:<span>1</span>,<span>"min_leaders_captured"</span>:<span>0</span><span>}</span>
</pre>
</div>
</div>
<p>I’m starting with full evaluations using the GPT models and I’ll need a slightly more elaborate prompt in order to get decent results. I can’t pass in the exact same prompt as the one I used for the finetuned model since the GPT models haven’t been trained or finetuned to respond to those specific prompts. This is sort of an interesting problem to have: how much effort do we put into the GPT prompts to try to get the same level of accuracy as the finetuned model? Or in other words, is there even a way to really compare like to like between models that must accept different prompts?</p>
<p>Let’s try this out for OpenAI GPT-4o and GPT-4 Turbo and see how we get on. You’ll note how long the prompt has to be to give the GPT models a fighting chance against the finetuned models. Ideally I’d stuff in even more examples into the context, but I also don’t want to explode the number of tokens I’m using.</p>
<div id="cb7" data-execution_count="30"><pre><code><span id="cb7-1"><span>from</span> openai <span>import</span> OpenAI</span>
<span id="cb7-2"><span>from</span> rich <span>import</span> <span>print</span></span>
<span id="cb7-3"><span>import</span> json</span>
<span id="cb7-4"><span>import</span> os</span>
<span id="cb7-5"></span>
<span id="cb7-6"></span>
<span id="cb7-7"><span>def</span> query_openai(article_text: <span>str</span>, model: <span>str</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb7-8">    query <span>=</span> (</span>
<span id="cb7-9">        <span>f"The following is a press release issued by ISAF (formerly operating in Afghanistan):</span><span>\n</span><span>{</span>article_text<span>}</span><span>\n\n</span><span>"</span></span>
<span id="cb7-10">        <span>"## Extraction request</span><span>\n</span><span>"</span></span>
<span id="cb7-11">        <span>"Please extract the following information from the press release:</span><span>\n</span><span>"</span></span>
<span id="cb7-12">        <span>"- The name of the event (summarising the event / text as a headline)</span><span>\n</span><span>"</span></span>
<span id="cb7-13">        <span>"- The start date of the event</span><span>\n</span><span>"</span></span>
<span id="cb7-14">        <span>"- The event type(s)</span><span>\n</span><span>"</span></span>
<span id="cb7-15">        <span>"- The province(s) in which the event occurred</span><span>\n</span><span>"</span></span>
<span id="cb7-16">        <span>"- The target group(s) of the event</span><span>\n</span><span>"</span></span>
<span id="cb7-17">        <span>"- The minimum number of people killed during the event</span><span>\n</span><span>"</span></span>
<span id="cb7-18">        <span>"- The minimum number of people captured during the event</span><span>\n</span><span>"</span></span>
<span id="cb7-19">        <span>"- Whether someone was killed or not during the event</span><span>\n</span><span>"</span></span>
<span id="cb7-20">        <span>"- Whether someone was captured or not during the event</span><span>\n</span><span>"</span></span>
<span id="cb7-21">        <span>"- Whether the event was a so-called 'kill-capture raid'</span><span>\n</span><span>"</span></span>
<span id="cb7-22">        <span>"- Whether an airstrike was used during the event</span><span>\n</span><span>"</span></span>
<span id="cb7-23">        <span>"- Whether no shots were fired during the event</span><span>\n</span><span>"</span></span>
<span id="cb7-24">        <span>"- The minimum number of leaders killed during the event</span><span>\n</span><span>"</span></span>
<span id="cb7-25">        <span>"- The minimum number of leaders captured during the event</span><span>\n\n</span><span>"</span></span>
<span id="cb7-26">        <span>"## Annotation notes:</span><span>\n</span><span>"</span></span>
<span id="cb7-27">        <span>"- A 'faciliator' is not a leader.</span><span>\n</span><span>"</span></span>
<span id="cb7-28">        <span>"- If a press release states that 'insurgents' were detained without further "</span></span>
<span id="cb7-29">        <span>"details, assign a minimum number of two detained. Interpret 'a couple' as "</span></span>
<span id="cb7-30">        <span>"two. Interpret 'several' as at least three, even though it may sometimes "</span></span>
<span id="cb7-31">        <span>"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a "</span></span>
<span id="cb7-32">        <span>"small group', and 'multiple' as denoting at least three, even if they "</span></span>
<span id="cb7-33">        <span>"sometimes refer to larger numbers. Choose the smaller number if no other "</span></span>
<span id="cb7-34">        <span>"information is available in the press release to come up with a minimally "</span></span>
<span id="cb7-35">        <span>"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, "</span></span>
<span id="cb7-36">        <span>"and 'a large number' as at least five.</span><span>\n\n</span><span>"</span></span>
<span id="cb7-37">        <span>"## Example:</span><span>\n</span><span>"</span></span>
<span id="cb7-38">        <span>"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011</span><span>\n</span><span>ISAF Joint Command - "</span></span>
<span id="cb7-39">        <span>"Afghanistan</span><span>\u2028</span><span>2011-02-S-143</span><span>\u2028</span><span>For Immediate Release </span><span>\u2028\u2028</span><span>KABUL, Afghanistan (Feb. 19)</span><span>\u2028\u2028</span><span>ISAF "</span></span>
<span id="cb7-40">        <span>"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of "</span></span>
<span id="cb7-41">        <span>"their position talking on radios today. After gaining positive identification of the insurgent positions, the "</span></span>
<span id="cb7-42">        <span>"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning "</span></span>
<span id="cb7-43">        <span>"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent "</span></span>
<span id="cb7-44">        <span>"positions, resulting in several more insurgents being killed.'</span><span>\n\n</span><span>"</span></span>
<span id="cb7-45">        <span>'Output: `{"name":"Several insurgents killed in '</span></span>
<span id="cb7-46">        <span>'Helmand","start_date":"2011-02-18","event_type":["insurgentskilled"],"province":["helmand"],"target_group":[""],"mi'</span></span>
<span id="cb7-47">        <span>'n_killed":6,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":false,"noshotsfired"'</span></span>
<span id="cb7-48">        <span>':false,"min_leaders_killed":0,"min_leaders_captured":0}`'</span></span>
<span id="cb7-49">    )</span>
<span id="cb7-50"></span>
<span id="cb7-51">    <span># set up the prediction harness</span></span>
<span id="cb7-52">    client <span>=</span> OpenAI(api_key<span>=</span>os.getenv(<span>"OPENAI_API_KEY"</span>))</span>
<span id="cb7-53"></span>
<span id="cb7-54">    response <span>=</span> client.chat.completions.create(</span>
<span id="cb7-55">        model<span>=</span>model,</span>
<span id="cb7-56">        response_format<span>=</span>{<span>"type"</span>: <span>"json_object"</span>},</span>
<span id="cb7-57">        messages<span>=</span>[</span>
<span id="cb7-58">            {</span>
<span id="cb7-59">                <span>"role"</span>: <span>"system"</span>,</span>
<span id="cb7-60">                <span>"content"</span>: <span>"You are an expert at identifying events in a press release. You are precise "</span></span>
<span id="cb7-61">                <span>"and always make sure you are correct, drawing inference from the text of the "</span></span>
<span id="cb7-62">                <span>"press release.</span><span>\n\n</span><span> You always return a JSON string with the following schema: "</span></span>
<span id="cb7-63">                <span>"## JSON Schema details</span><span>\n</span><span>"</span></span>
<span id="cb7-64">                <span>"Here is some of the schema for the JSON output string you "</span></span>
<span id="cb7-65">                <span>"should make use of: event_types = ['airstrike', 'detention', "</span></span>
<span id="cb7-66">                <span>"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], "</span></span>
<span id="cb7-67">                <span>"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', "</span></span>
<span id="cb7-68">                <span>"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', "</span></span>
<span id="cb7-69">                <span>"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', "</span></span>
<span id="cb7-70">                <span>"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', "</span></span>
<span id="cb7-71">                <span>"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', "</span></span>
<span id="cb7-72">                <span>"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', "</span></span>
<span id="cb7-73">                <span>"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span><span>\n\n</span><span>"</span>,</span>
<span id="cb7-74">            },</span>
<span id="cb7-75">            {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: query},</span>
<span id="cb7-76">        ],</span>
<span id="cb7-77">        temperature<span>=</span><span>1</span>,</span>
<span id="cb7-78">    )</span>
<span id="cb7-79"></span>
<span id="cb7-80">    <span>return</span> response.choices[<span>0</span>].message.content</span></code></pre></div>
<p>We can make sure this function works with a quick example:</p>
<div data-execution_count="7">
<div id="cb8"><pre><code><span id="cb8-1">json_str <span>=</span> query_openai(events[<span>0</span>].text, <span>"gpt-4o"</span>)</span>
<span id="cb8-2"><span>print</span>(json.loads(json_str))</span></code></pre></div>
<div>
<pre><span>{</span>
    <span>'name'</span>: <span>'Taliban leader Alaudin killed in Ghazni'</span>,
    <span>'start_date'</span>: <span>'2013-01-24'</span>,
    <span>'event_type'</span>: <span>[</span><span>'insurgentskilled'</span><span>]</span>,
    <span>'province'</span>: <span>[</span><span>'ghazni'</span><span>]</span>,
    <span>'target_group'</span>: <span>[</span><span>'taliban'</span><span>]</span>,
    <span>'min_killed'</span>: <span>1</span>,
    <span>'min_captured'</span>: <span>0</span>,
    <span>'killq'</span>: <span>True</span>,
    <span>'captureq'</span>: <span>False</span>,
    <span>'killcaptureraid'</span>: <span>True</span>,
    <span>'airstrike'</span>: <span>False</span>,
    <span>'noshotsfired'</span>: <span>False</span>,
    <span>'min_leaders_killed'</span>: <span>1</span>,
    <span>'min_leaders_captured'</span>: <span>0</span>
<span>}</span>
</pre>
</div>
</div>
<p>Our model is working (as expected) and we’re also getting a JSON string back. Let’s assemble something that will iterate through all of our test data, get predictions, and then store those predictions on our Pydantic object.</p>
<p>For the bulk predictions, we’ll make sure to do this async, since there are lots of events and we don’t want to waiting all day. You’ll see I also had to add some retries to the function to account for rate limiting on the GPT-3.5-turbo model.</p>
<div>
<details>
<summary>Code</summary>
<div id="cb9"><pre><code><span id="cb9-1"><span># make async work within a notebook</span></span>
<span id="cb9-2"><span>import</span> nest_asyncio</span>
<span id="cb9-3"></span>
<span id="cb9-4">nest_asyncio.<span>apply</span>()</span>
<span id="cb9-5"></span>
<span id="cb9-6"><span>import</span> aiohttp</span>
<span id="cb9-7"><span>import</span> asyncio</span>
<span id="cb9-8"><span>from</span> typing <span>import</span> List</span>
<span id="cb9-9"><span>from</span> openai <span>import</span> OpenAI</span>
<span id="cb9-10"></span>
<span id="cb9-11"></span>
<span id="cb9-12"><span>async</span> <span>def</span> async_query_openai(</span>
<span id="cb9-13">    session,</span>
<span id="cb9-14">    article_text: <span>str</span>,</span>
<span id="cb9-15">    model: <span>str</span>,</span>
<span id="cb9-16">    max_retries: <span>int</span> <span>=</span> <span>3</span>,</span>
<span id="cb9-17">    retry_delay: <span>float</span> <span>=</span> <span>1.0</span>,</span>
<span id="cb9-18">) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb9-19">    query <span>=</span> (</span>
<span id="cb9-20">        <span>f"The following is a press release issued by ISAF (formerly operating in Afghanistan):</span><span>\n</span><span>{</span>article_text<span>}</span><span>\n\n</span><span>"</span></span>
<span id="cb9-21">        <span>"## Extraction request</span><span>\n</span><span>"</span></span>
<span id="cb9-22">        <span>"Please extract the following information from the press release:</span><span>\n</span><span>"</span></span>
<span id="cb9-23">        <span>"- The name of the event (summarising the event / text as a headline)</span><span>\n</span><span>"</span></span>
<span id="cb9-24">        <span>"- The start date of the event</span><span>\n</span><span>"</span></span>
<span id="cb9-25">        <span>"- The event type(s)</span><span>\n</span><span>"</span></span>
<span id="cb9-26">        <span>"- The province(s) in which the event occurred</span><span>\n</span><span>"</span></span>
<span id="cb9-27">        <span>"- The target group(s) of the event</span><span>\n</span><span>"</span></span>
<span id="cb9-28">        <span>"- The minimum number of people killed during the event</span><span>\n</span><span>"</span></span>
<span id="cb9-29">        <span>"- The minimum number of people captured during the event</span><span>\n</span><span>"</span></span>
<span id="cb9-30">        <span>"- Whether someone was killed or not during the event</span><span>\n</span><span>"</span></span>
<span id="cb9-31">        <span>"- Whether someone was captured or not during the event</span><span>\n</span><span>"</span></span>
<span id="cb9-32">        <span>"- Whether the event was a so-called 'kill-capture raid'</span><span>\n</span><span>"</span></span>
<span id="cb9-33">        <span>"- Whether an airstrike was used during the event</span><span>\n</span><span>"</span></span>
<span id="cb9-34">        <span>"- Whether no shots were fired during the event</span><span>\n</span><span>"</span></span>
<span id="cb9-35">        <span>"- The minimum number of leaders killed during the event</span><span>\n</span><span>"</span></span>
<span id="cb9-36">        <span>"- The minimum number of leaders captured during the event</span><span>\n\n</span><span>"</span></span>
<span id="cb9-37">        <span>"## Annotation notes:</span><span>\n</span><span>"</span></span>
<span id="cb9-38">        <span>"- A 'faciliator' is not a leader.</span><span>\n</span><span>"</span></span>
<span id="cb9-39">        <span>"- If a press release states that 'insurgents' were detained without further "</span></span>
<span id="cb9-40">        <span>"details, assign a minimum number of two detained. Interpret 'a couple' as "</span></span>
<span id="cb9-41">        <span>"two. Interpret 'several' as at least three, even though it may sometimes "</span></span>
<span id="cb9-42">        <span>"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a "</span></span>
<span id="cb9-43">        <span>"small group', and 'multiple' as denoting at least three, even if they "</span></span>
<span id="cb9-44">        <span>"sometimes refer to larger numbers. Choose the smaller number if no other "</span></span>
<span id="cb9-45">        <span>"information is available in the press release to come up with a minimally "</span></span>
<span id="cb9-46">        <span>"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, "</span></span>
<span id="cb9-47">        <span>"and 'a large number' as at least five.</span><span>\n\n</span><span>"</span></span>
<span id="cb9-48">        <span>"## Example:</span><span>\n</span><span>"</span></span>
<span id="cb9-49">        <span>"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011</span><span>\n</span><span>ISAF Joint Command - "</span></span>
<span id="cb9-50">        <span>"Afghanistan</span><span>\u2028</span><span>2011-02-S-143</span><span>\u2028</span><span>For Immediate Release </span><span>\u2028\u2028</span><span>KABUL, Afghanistan (Feb. 19)</span><span>\u2028\u2028</span><span>ISAF "</span></span>
<span id="cb9-51">        <span>"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of "</span></span>
<span id="cb9-52">        <span>"their position talking on radios today. After gaining positive identification of the insurgent positions, the "</span></span>
<span id="cb9-53">        <span>"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning "</span></span>
<span id="cb9-54">        <span>"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent "</span></span>
<span id="cb9-55">        <span>"positions, resulting in several more insurgents being killed.'</span><span>\n\n</span><span>"</span></span>
<span id="cb9-56">        <span>'Output: `{"name":"Several insurgents killed in '</span></span>
<span id="cb9-57">        <span>'Helmand","start_date":"2011-02-18","event_type":["insurgentskilled"],"province":["helmand"],"target_group":[""],"mi'</span></span>
<span id="cb9-58">        <span>'n_killed":6,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":false,"noshotsfired"'</span></span>
<span id="cb9-59">        <span>':false,"min_leaders_killed":0,"min_leaders_captured":0}`'</span></span>
<span id="cb9-60">    )</span>
<span id="cb9-61"></span>
<span id="cb9-62">    client <span>=</span> OpenAI(api_key<span>=</span>os.getenv(<span>"OPENAI_API_KEY"</span>))</span>
<span id="cb9-63"></span>
<span id="cb9-64">    retries <span>=</span> <span>0</span></span>
<span id="cb9-65">    <span>while</span> retries <span>&lt;</span> max_retries:</span>
<span id="cb9-66">        <span>async</span> <span>with</span> session.post(</span>
<span id="cb9-67">            <span>"https://api.openai.com/v1/chat/completions"</span>,</span>
<span id="cb9-68">            headers<span>=</span>{<span>"Authorization"</span>: <span>f"Bearer </span><span>{</span>client<span>.</span>api_key<span>}</span><span>"</span>},</span>
<span id="cb9-69">            json<span>=</span>{</span>
<span id="cb9-70">                <span>"model"</span>: model,</span>
<span id="cb9-71">                <span>"response_format"</span>: {<span>"type"</span>: <span>"json_object"</span>},</span>
<span id="cb9-72">                <span>"messages"</span>: [</span>
<span id="cb9-73">                    {</span>
<span id="cb9-74">                        <span>"role"</span>: <span>"system"</span>,</span>
<span id="cb9-75">                        <span>"content"</span>: <span>"You are an expert at identifying events in a press release. You are precise "</span></span>
<span id="cb9-76">                        <span>"and always make sure you are correct, drawing inference from the text of the "</span></span>
<span id="cb9-77">                        <span>"press release.</span><span>\n\n</span><span> You always return a JSON string with the following schema: "</span></span>
<span id="cb9-78">                        <span>"## JSON Schema details</span><span>\n</span><span>"</span></span>
<span id="cb9-79">                        <span>"Here is some of the schema for the JSON output string you "</span></span>
<span id="cb9-80">                        <span>"should make use of: event_types = ['airstrike', 'detention', "</span></span>
<span id="cb9-81">                        <span>"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], "</span></span>
<span id="cb9-82">                        <span>"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', "</span></span>
<span id="cb9-83">                        <span>"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', "</span></span>
<span id="cb9-84">                        <span>"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', "</span></span>
<span id="cb9-85">                        <span>"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', "</span></span>
<span id="cb9-86">                        <span>"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', "</span></span>
<span id="cb9-87">                        <span>"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', "</span></span>
<span id="cb9-88">                        <span>"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span><span>\n\n</span><span>"</span>,</span>
<span id="cb9-89">                    },</span>
<span id="cb9-90">                    {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: query},</span>
<span id="cb9-91">                ],</span>
<span id="cb9-92">                <span>"temperature"</span>: <span>1</span>,</span>
<span id="cb9-93">            },</span>
<span id="cb9-94">        ) <span>as</span> response:</span>
<span id="cb9-95">            result <span>=</span> <span>await</span> response.json()</span>
<span id="cb9-96">            <span>if</span> <span>"error"</span> <span>in</span> result:</span>
<span id="cb9-97">                error_message <span>=</span> result[<span>"error"</span>][<span>"message"</span>]</span>
<span id="cb9-98">                <span>if</span> <span>"Rate limit reached"</span> <span>in</span> error_message:</span>
<span id="cb9-99">                    <span># retry_delay_ms = float(</span></span>
<span id="cb9-100">                    <span>#     error_message.split("Please try again in ")[1].split("ms")[0]</span></span>
<span id="cb9-101">                    <span># )</span></span>
<span id="cb9-102">                    retry_delay_ms <span>=</span> <span>35000</span></span>
<span id="cb9-103">                    retry_delay_seconds <span>=</span> retry_delay_ms <span>/</span> <span>1000</span></span>
<span id="cb9-104">                    <span>print</span>(</span>
<span id="cb9-105">                        <span>f"Rate limit exceeded. Retrying in </span><span>{</span>retry_delay_seconds<span>}</span><span> seconds..."</span></span>
<span id="cb9-106">                    )</span>
<span id="cb9-107">                    <span>await</span> asyncio.sleep(retry_delay_seconds)</span>
<span id="cb9-108">                    retries <span>+=</span> <span>1</span></span>
<span id="cb9-109">                    <span>continue</span></span>
<span id="cb9-110">                <span>else</span>:</span>
<span id="cb9-111">                    <span>print</span>(<span>f"Error during prediction.</span><span>\n</span><span>Full result object: </span><span>{</span>result<span>}</span><span>"</span>)</span>
<span id="cb9-112">                    <span>return</span> <span>""</span></span>
<span id="cb9-113">            <span>try</span>:</span>
<span id="cb9-114">                <span>return</span> result[<span>"choices"</span>][<span>0</span>][<span>"message"</span>][<span>"content"</span>]</span>
<span id="cb9-115">            <span>except</span> <span>KeyError</span>:</span>
<span id="cb9-116">                <span>print</span>(<span>f"Error during prediction.</span><span>\n</span><span>Full result object: </span><span>{</span>result<span>}</span><span>"</span>)</span>
<span id="cb9-117">                <span>return</span> <span>""</span></span>
<span id="cb9-118"></span>
<span id="cb9-119">    <span>print</span>(<span>f"Max retries exceeded for event.</span><span>\n</span><span>Full result object: </span><span>{</span>result<span>}</span><span>"</span>)</span>
<span id="cb9-120">    <span>return</span> <span>""</span></span>
<span id="cb9-121"></span>
<span id="cb9-122"></span>
<span id="cb9-123"><span>async</span> <span>def</span> get_gpt_predictions_async(</span>
<span id="cb9-124">    model: <span>str</span>,</span>
<span id="cb9-125">    events: List[IsafEvent],</span>
<span id="cb9-126">    logging_n: <span>int</span> <span>=</span> <span>100</span>,</span>
<span id="cb9-127">    max_concurrent_requests: <span>int</span> <span>=</span> <span>5</span>,</span>
<span id="cb9-128">) <span>-&gt;</span> List[IsafEvent]:</span>
<span id="cb9-129">    <span>async</span> <span>with</span> aiohttp.ClientSession() <span>as</span> session:</span>
<span id="cb9-130">        semaphore <span>=</span> asyncio.Semaphore(max_concurrent_requests)</span>
<span id="cb9-131">        tasks <span>=</span> []</span>
<span id="cb9-132">        <span>for</span> i, event <span>in</span> <span>enumerate</span>(events, start<span>=</span><span>1</span>):</span>
<span id="cb9-133">            <span>if</span> i <span>%</span> logging_n <span>==</span> <span>0</span>:</span>
<span id="cb9-134">                <span>print</span>(<span>f"Predicting event </span><span>{</span>i<span>}</span><span> of </span><span>{</span><span>len</span>(events)<span>}</span><span> using </span><span>{</span>model<span>}</span><span>"</span>)</span>
<span id="cb9-135"></span>
<span id="cb9-136">            <span>async</span> <span>def</span> make_request(session, event):</span>
<span id="cb9-137">                <span>async</span> <span>with</span> semaphore:</span>
<span id="cb9-138">                    <span>return</span> <span>await</span> async_query_openai(</span>
<span id="cb9-139">                        session, event.text, model, max_retries<span>=</span><span>5</span></span>
<span id="cb9-140">                    )</span>
<span id="cb9-141"></span>
<span id="cb9-142">            task <span>=</span> asyncio.ensure_future(make_request(session, event))</span>
<span id="cb9-143">            tasks.append(task)</span>
<span id="cb9-144"></span>
<span id="cb9-145">        predictions <span>=</span> <span>await</span> asyncio.gather(<span>*</span>tasks)</span>
<span id="cb9-146">        <span>for</span> event, prediction <span>in</span> <span>zip</span>(events, predictions):</span>
<span id="cb9-147">            event.predictions[model] <span>=</span> prediction</span>
<span id="cb9-148"></span>
<span id="cb9-149">    <span>return</span> events</span>
<span id="cb9-150"></span>
<span id="cb9-151"></span>
<span id="cb9-152"><span>async</span> <span>def</span> main():</span>
<span id="cb9-153">    events_4o <span>=</span> <span>await</span> get_gpt_predictions_async(</span>
<span id="cb9-154">        <span>"gpt-4o"</span>, events, max_concurrent_requests<span>=</span><span>10</span></span>
<span id="cb9-155">    )</span>
<span id="cb9-156">    events_4turbo <span>=</span> <span>await</span> get_gpt_predictions_async(</span>
<span id="cb9-157">        <span>"gpt-4-turbo"</span>, events_4o, max_concurrent_requests<span>=</span><span>10</span></span>
<span id="cb9-158">    )</span>
<span id="cb9-159">    full_events <span>=</span> <span>await</span> get_gpt_predictions_async(</span>
<span id="cb9-160">        <span>"gpt-3.5-turbo"</span>, events_4turbo, max_concurrent_requests<span>=</span><span>10</span></span>
<span id="cb9-161">    )</span>
<span id="cb9-162"></span>
<span id="cb9-163"></span>
<span id="cb9-164"><span>await</span> main()</span></code></pre></div>
</details>
</div>
<p>So as you can now see, we have three predictions attached to each event.</p>
<div data-execution_count="17">
<pre><span>IsafEvent</span><span>(</span>
    <span>name</span>=<span>'5'</span>,
    <span>text</span>=<span>'2013-01-S-025\n\nKABUL, Afghanistan (Jan. 25, 2013)\nDuring a security operation in Andar district, </span>
<span>Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a </span>
<span>group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire </span>
<span>attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan </span>
<span>National Police in Ghazni province.'</span>,
    <span>start_date</span>=<span>datetime</span><span>.date</span><span>(</span><span>2013</span>, <span>1</span>, <span>24</span><span>)</span>,
    <span>event_type</span>=<span>{</span><span>'insurgentskilled'</span><span>}</span>,
    <span>province</span>=<span>{</span><span>'ghazni'</span><span>}</span>,
    <span>target_group</span>=<span>{</span><span>'taliban'</span><span>}</span>,
    <span>min_killed</span>=<span>1</span>,
    <span>min_captured</span>=<span>0</span>,
    <span>killq</span>=<span>True</span>,
    <span>captureq</span>=<span>False</span>,
    <span>killcaptureraid</span>=<span>False</span>,
    <span>airstrike</span>=<span>False</span>,
    <span>noshotsfired</span>=<span>False</span>,
    <span>min_leaders_killed</span>=<span>1</span>,
    <span>min_leaders_captured</span>=<span>0</span>,
    <span>predictions</span>=<span>{</span>
        <span>'gpt-4o'</span>: <span>'{\n  "name": "Taliban leader Alaudin killed in Ghazni",\n  "start_date": "2013-01-24",\n  </span>
<span>"event_type": ["insurgentskilled", "captureandkill"],\n  "province": ["ghazni"],\n  "target_group": ["taliban"],\n </span>
<span>"min_killed": 1,\n  "min_captured": 0,\n  "killq": true,\n  "captureq": false,\n  "killcaptureraid": true,\n  </span>
<span>"airstrike": false,\n  "noshotsfired": false,\n  "min_leaders_killed": 1,\n  "min_leaders_captured": 0\n}'</span>,
        <span>'gpt-4-turbo'</span>: <span>'{\n    "name": "Taliban leader Alaudin killed in Ghazni",\n    "start_date": </span>
<span>"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span>["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span>"killcaptureraid": true,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span>"min_leaders_captured": 0\n}'</span>,
        <span>'gpt-3.5-turbo'</span>: <span>'{\n    "name": "Taliban leader Alaudin killed in Ghazni province",\n    "start_date": </span>
<span>"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span>["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span>"killcaptureraid": false,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span>"min_leaders_captured": 0\n}'</span>
    <span>}</span>
<span>)</span>
</pre>
</div>
<p>I have all these predictions living in memory right now so it’s probably a good time to commit these to a dataset and push them to the Hugging Face Hub in case the notebook crashes or my local machine shuts down or something else unexpected.</p>
<p>I’ll create a function to handle this as we’ll be repeating this process for the other models as well. It’s a bit verbose but I thought it preferable so you can see what’s going on.</p>
<div data-execution_count="35">
<details>
<summary>Code</summary>
<div id="cb11"><pre><code><span id="cb11-1"><span>from</span> datasets <span>import</span> Dataset</span>
<span id="cb11-2"></span>
<span id="cb11-3"></span>
<span id="cb11-4"><span>def</span> convert_to_dataset(data: List[IsafEvent]) <span>-&gt;</span> Dataset:</span>
<span id="cb11-5">    names <span>=</span> []</span>
<span id="cb11-6">    texts <span>=</span> []</span>
<span id="cb11-7">    start_dates <span>=</span> []</span>
<span id="cb11-8">    provinces <span>=</span> []</span>
<span id="cb11-9">    target_groups <span>=</span> []</span>
<span id="cb11-10">    event_types <span>=</span> []</span>
<span id="cb11-11">    predictions <span>=</span> []</span>
<span id="cb11-12">    min_killeds <span>=</span> []</span>
<span id="cb11-13">    min_captureds <span>=</span> []</span>
<span id="cb11-14">    killqs <span>=</span> []</span>
<span id="cb11-15">    captureqs <span>=</span> []</span>
<span id="cb11-16">    killcaptureraids <span>=</span> []</span>
<span id="cb11-17">    airstrikes <span>=</span> []</span>
<span id="cb11-18">    noshotsfireds <span>=</span> []</span>
<span id="cb11-19">    min_leaders_killeds <span>=</span> []</span>
<span id="cb11-20">    min_leaders_captureds <span>=</span> []</span>
<span id="cb11-21"></span>
<span id="cb11-22">    <span>for</span> item <span>in</span> data:</span>
<span id="cb11-23">        names.append(item.name)</span>
<span id="cb11-24">        texts.append(item.text)</span>
<span id="cb11-25">        start_dates.append(item.start_date)</span>
<span id="cb11-26">        provinces.append(item.province)</span>
<span id="cb11-27">        target_groups.append(item.target_group)</span>
<span id="cb11-28">        event_types.append(item.event_type)</span>
<span id="cb11-29">        predictions.append(item.predictions)</span>
<span id="cb11-30">        min_killeds.append(item.min_killed)</span>
<span id="cb11-31">        min_captureds.append(item.min_captured)</span>
<span id="cb11-32">        killqs.append(item.killq)</span>
<span id="cb11-33">        captureqs.append(item.captureq)</span>
<span id="cb11-34">        killcaptureraids.append(item.killcaptureraid)</span>
<span id="cb11-35">        airstrikes.append(item.airstrike)</span>
<span id="cb11-36">        noshotsfireds.append(item.noshotsfired)</span>
<span id="cb11-37">        min_leaders_killeds.append(item.min_leaders_killed)</span>
<span id="cb11-38">        min_leaders_captureds.append(item.min_leaders_captured)</span>
<span id="cb11-39"></span>
<span id="cb11-40">    dataset_dict <span>=</span> {</span>
<span id="cb11-41">        <span>"name"</span>: names,</span>
<span id="cb11-42">        <span>"text"</span>: texts,</span>
<span id="cb11-43">        <span>"predictions"</span>: predictions,</span>
<span id="cb11-44">        <span>"start_date"</span>: start_dates,</span>
<span id="cb11-45">        <span>"province"</span>: provinces,</span>
<span id="cb11-46">        <span>"target_group"</span>: target_groups,</span>
<span id="cb11-47">        <span>"event_type"</span>: event_types,</span>
<span id="cb11-48">        <span>"min_killed"</span>: min_killeds,</span>
<span id="cb11-49">        <span>"min_captured"</span>: min_captureds,</span>
<span id="cb11-50">        <span>"killq"</span>: killqs,</span>
<span id="cb11-51">        <span>"captureq"</span>: captureqs,</span>
<span id="cb11-52">        <span>"killcaptureraid"</span>: killcaptureraids,</span>
<span id="cb11-53">        <span>"airstrike"</span>: airstrikes,</span>
<span id="cb11-54">        <span>"noshotsfired"</span>: noshotsfireds,</span>
<span id="cb11-55">        <span>"min_leaders_killed"</span>: min_leaders_killeds,</span>
<span id="cb11-56">        <span>"min_leaders_captured"</span>: min_leaders_captureds,</span>
<span id="cb11-57">    }</span>
<span id="cb11-58">    dataset <span>=</span> Dataset.from_dict(dataset_dict)</span>
<span id="cb11-59"></span>
<span id="cb11-60">    <span>return</span> dataset</span>
<span id="cb11-61"></span>
<span id="cb11-62"></span>
<span id="cb11-63"><span>def</span> convert_and_push_dataset(</span>
<span id="cb11-64">    events: List[IsafEvent], name: <span>str</span>, split_name: <span>str</span> <span>=</span> <span>"train"</span></span>
<span id="cb11-65">):</span>
<span id="cb11-66">    <span>"""Convert a list of Pydantic objects to a HF Dataset object, then push to</span></span>
<span id="cb11-67"><span>    the hub."""</span></span>
<span id="cb11-68">    hf_token <span>=</span> os.getenv(<span>"HUGGINGFACE_API_KEY"</span>)</span>
<span id="cb11-69"></span>
<span id="cb11-70">    dataset <span>=</span> convert_to_dataset(events)</span>
<span id="cb11-71">    dataset.push_to_hub(</span>
<span id="cb11-72">        <span>f"strickvl/</span><span>{</span>name<span>}</span><span>"</span>,</span>
<span id="cb11-73">        token<span>=</span>hf_token,</span>
<span id="cb11-74">        private<span>=</span><span>True</span>,</span>
<span id="cb11-75">        create_pr<span>=</span><span>True</span>,</span>
<span id="cb11-76">        split<span>=</span>split_name,</span>
<span id="cb11-77">    )</span></code></pre></div>
</details>
</div>
<p>A more concise and abstract version of the <code>convert_to_dataset</code> function could be something like:</p>
<div id="cb12"><pre><code><span id="cb12-1"><span>def</span> convert_to_dataset(data: List[BaseModel]) <span>-&gt;</span> Dataset:</span>
<span id="cb12-2">    dataset_dict <span>=</span> {}</span>
<span id="cb12-3"></span>
<span id="cb12-4">    <span>for</span> field_name, field_value <span>in</span> data[<span>0</span>].__fields__.items():</span>
<span id="cb12-5">        field_type <span>=</span> field_value.outer_type_</span>
<span id="cb12-6">        <span>if</span> field_type <span>in</span> [<span>str</span>, <span>int</span>, <span>float</span>, <span>bool</span>, date]:</span>
<span id="cb12-7">            dataset_dict[field_name] <span>=</span> [<span>getattr</span>(item, field_name) <span>for</span> item <span>in</span> data]</span>
<span id="cb12-8">        <span>elif</span> field_type <span>==</span> <span>set</span>:</span>
<span id="cb12-9">            dataset_dict[field_name] <span>=</span> [<span>list</span>(<span>getattr</span>(item, field_name)) <span>for</span> item <span>in</span> data]</span>
<span id="cb12-10">        <span>elif</span> <span>issubclass</span>(field_type, BaseModel):</span>
<span id="cb12-11">            dataset_dict[field_name] <span>=</span> [<span>getattr</span>(item, field_name).<span>dict</span>() <span>for</span> item <span>in</span> data]</span>
<span id="cb12-12">        <span>else</span>:</span>
<span id="cb12-13">            dataset_dict[field_name] <span>=</span> [<span>getattr</span>(item, field_name) <span>for</span> item <span>in</span> data]</span>
<span id="cb12-14"></span>
<span id="cb12-15">    dataset <span>=</span> Dataset.from_dict(dataset_dict)</span>
<span id="cb12-16">    <span>return</span> dataset</span></code></pre></div>
<p>But for now let’s just push our data to the Hub.</p>
<div id="cb13"><pre><code><span id="cb13-1">convert_and_push_dataset(events, <span>"isafpressreleases_with_preds"</span>, split_name<span>=</span><span>"test"</span>)</span></code></pre></div>
</section>
<section id="adding-predictions-from-our-finetuned-models">
<h2>Adding predictions from our finetuned models</h2>
<p>We’ve added some baseline OpenAI models, so let’s now add <a href="https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html">the models</a> <a href="https://mlops.systems/posts/2024-06-17-one-click-finetuning.html">we previously finetuned</a>. This includes a mix of local models as well as models hosted by some <a href="https://mlops.systems/posts/2024-06-17-one-click-finetuning.html">one-click finetuning providers</a>.</p>
<p>I’ll hide a bunch of the code with folding arrows so you can see it if you’re interested but there isn’t actually much of interest or new there.</p>
<section id="reloading-the-predictions-dataset">
<h2 data-anchor-id="reloading-the-predictions-dataset">Reloading the predictions dataset</h2>
<p>Let’s start by loading our dataset and then we can get into adding some local model predictions:</p>
<div id="cb14" data-execution_count="19"><pre><code><span id="cb14-1"><span>from</span> datasets <span>import</span> load_dataset</span>
<span id="cb14-2"></span>
<span id="cb14-3">preds_test_data <span>=</span> load_dataset(<span>"strickvl/isafpressreleases_with_preds"</span>)[</span>
<span id="cb14-4">    <span>"test"</span></span>
<span id="cb14-5">].to_list()</span></code></pre></div>
<p>We trained some local models, so let’s add those predictions to the dataset.</p>
</section>
<section id="finetuned-tinyllama-predictions">
<h2 data-anchor-id="finetuned-tinyllama-predictions">Finetuned TinyLlama predictions</h2>
<div data-execution_count="21">
<details>
<summary>Code</summary>
<div id="cb15"><pre><code><span id="cb15-1"><span>from</span> typing <span>import</span> Union</span>
<span id="cb15-2"><span>import</span> torch</span>
<span id="cb15-3"><span>from</span> peft <span>import</span> AutoPeftModelForCausalLM</span>
<span id="cb15-4"><span>from</span> transformers <span>import</span> AutoTokenizer</span>
<span id="cb15-5"></span>
<span id="cb15-6"></span>
<span id="cb15-7"><span>def</span> prompt(press_release: <span>str</span>) <span>-&gt;</span> <span>str</span>:</span>
<span id="cb15-8">    <span>return</span> <span>f"""You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span></span>
<span id="cb15-9"></span>
<span id="cb15-10"><span>### Instruction:</span></span>
<span id="cb15-11"></span>
<span id="cb15-12"><span>PRESS RELEASE TEXT: "</span><span>{</span>press_release<span>}</span><span>"</span></span>
<span id="cb15-13"></span>
<span id="cb15-14"><span>### Response:</span></span>
<span id="cb15-15"><span>"""</span></span>
<span id="cb15-16"></span>
<span id="cb15-17"></span>
<span id="cb15-18"><span>def</span> prompt_tok(</span>
<span id="cb15-19">    model: AutoPeftModelForCausalLM,</span>
<span id="cb15-20">    tokenizer: AutoTokenizer,</span>
<span id="cb15-21">    press_release: <span>str</span>,</span>
<span id="cb15-22">    return_ids: <span>bool</span> <span>=</span> <span>False</span>,</span>
<span id="cb15-23">) <span>-&gt;</span> Union[<span>str</span>, torch.Tensor]:</span>
<span id="cb15-24">    _p <span>=</span> prompt(press_release)</span>
<span id="cb15-25">    input_ids <span>=</span> tokenizer(_p, return_tensors<span>=</span><span>"pt"</span>, truncation<span>=</span><span>True</span>).input_ids.cuda()</span>
<span id="cb15-26">    out_ids <span>=</span> model.generate(input_ids<span>=</span>input_ids, max_new_tokens<span>=</span><span>5000</span>, do_sample<span>=</span><span>False</span>)</span>
<span id="cb15-27">    ids <span>=</span> out_ids.detach().cpu().numpy()</span>
<span id="cb15-28">    <span>if</span> return_ids:</span>
<span id="cb15-29">        <span>return</span> out_ids</span>
<span id="cb15-30">    <span>return</span> tokenizer.batch_decode(ids, skip_special_tokens<span>=</span><span>True</span>)[<span>0</span>][<span>len</span>(_p) :]</span>
<span id="cb15-31"></span>
<span id="cb15-32"></span>
<span id="cb15-33">tinyllama_sharegpt_model_id <span>=</span> <span>"strickvl/isafpr-tiny-llama-lora-templatefree"</span></span>
<span id="cb15-34">model <span>=</span> AutoPeftModelForCausalLM.from_pretrained(tinyllama_sharegpt_model_id).cuda()</span>
<span id="cb15-35">tokenizer <span>=</span> AutoTokenizer.from_pretrained(tinyllama_sharegpt_model_id)</span>
<span id="cb15-36">tokenizer.pad_token <span>=</span> tokenizer.eos_token</span>
<span id="cb15-37"></span>
<span id="cb15-38"><span>for</span> row <span>in</span> preds_test_data:</span>
<span id="cb15-39">    out <span>=</span> prompt_tok(model, tokenizer, row[<span>"text"</span>])</span>
<span id="cb15-40">    row[<span>"predictions"</span>][<span>"tinyllama-templatefree"</span>] <span>=</span> out</span></code></pre></div>
</details>
</div>
<p>Now if we inspect we’ll see that the new model predictions have been saved into the dataset:</p>
<div>
<div id="cb16"><pre><code><span id="cb16-1"><span>from</span> rich <span>import</span> <span>print</span></span>
<span id="cb16-2"></span>
<span id="cb16-3"><span>print</span>(preds_test_data[<span>0</span>])</span></code></pre></div>
<div>
<pre><span>{</span>
    <span>'name'</span>: <span>'5'</span>,
    <span>'text'</span>: <span>'2013-01-S-025\n\nKABUL, Afghanistan (Jan. 25, 2013)\nDuring a security operation in Andar district, </span>
<span>Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a </span>
<span>group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire </span>
<span>attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan </span>
<span>National Police in Ghazni province.'</span>,
    <span>'predictions'</span>: <span>{</span>
        <span>'gpt-3.5-turbo'</span>: <span>'{\n    "name": "Taliban leader Alaudin killed in Ghazni province",\n    "start_date": </span>
<span>"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span>["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span>"killcaptureraid": false,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span>"min_leaders_captured": 0\n}'</span>,
        <span>'gpt-4-turbo'</span>: <span>'{\n    "name": "Taliban leader Alaudin killed in Ghazni",\n    "start_date": </span>
<span>"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span>["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span>"killcaptureraid": true,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span>"min_leaders_captured": 0\n}'</span>,
        <span>'gpt-4o'</span>: <span>'{\n  "name": "Taliban leader Alaudin killed in Ghazni",\n  "start_date": "2013-01-24",\n  </span>
<span>"event_type": ["insurgentskilled", "captureandkill"],\n  "province": ["ghazni"],\n  "target_group": ["taliban"],\n </span>
<span>"min_killed": 1,\n  "min_captured": 0,\n  "killq": true,\n  "captureq": false,\n  "killcaptureraid": true,\n  </span>
<span>"airstrike": false,\n  "noshotsfired": false,\n  "min_leaders_killed": 1,\n  "min_leaders_captured": 0\n}'</span>,
        <span>'tinyllama-templatefree'</span>: <span>'\n{"name":"Taliban leader killed in </span>
<span>Ghazni","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["taliban"</span>
<span>],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":false,"noshotsf</span>
<span>ired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span>'tinyllama-sharegpt'</span>: 
<span>'{"name":"2","start_date":"2013-01-24","event_type":["airstrike"],"province":["ghazni"],"target_group":["taliban"],</span>
<span>"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":true,"noshotsfire</span>
<span>d":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>
    <span>}</span>,
    <span>'start_date'</span>: <span>datetime.date</span><span>(</span><span>2013</span>, <span>1</span>, <span>24</span><span>)</span>,
    <span>'province'</span>: <span>[</span><span>'ghazni'</span><span>]</span>,
    <span>'target_group'</span>: <span>[</span><span>'taliban'</span><span>]</span>,
    <span>'event_type'</span>: <span>[</span><span>'insurgentskilled'</span><span>]</span>,
    <span>'min_killed'</span>: <span>1</span>,
    <span>'min_captured'</span>: <span>0</span>,
    <span>'killq'</span>: <span>True</span>,
    <span>'captureq'</span>: <span>False</span>,
    <span>'killcaptureraid'</span>: <span>False</span>,
    <span>'airstrike'</span>: <span>False</span>,
    <span>'noshotsfired'</span>: <span>False</span>,
    <span>'min_leaders_killed'</span>: <span>1</span>,
    <span>'min_leaders_captured'</span>: <span>0</span>
<span>}</span>
</pre>
</div>
</div>
</section>
<section id="finetuned-mistral-predictions">
<h2 data-anchor-id="finetuned-mistral-predictions">Finetuned Mistral predictions</h2>
<p>As <a href="https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html#finetuning-our-model">I noted previously</a>, it was impossible to get the finetuned Mistral model working locally so I did the inference over on Modal where I could spin up a juicy A100 to make the predictions. You’ll see below that the model didn’t perform very well, failing almost all of the evaluations. This is the <code>mistral-lora-templatefree</code> model you’ll see in the charts.</p>
</section>
<section id="finetuned-openai-predictions">
<h2 data-anchor-id="finetuned-openai-predictions">Finetuned OpenAI predictions</h2>
<p>I used OpenAI’s one-click finetuning service <a href="https://mlops.systems/posts/2024-06-17-one-click-finetuning.html#openai">to finetune the <code>gpt-3.5-turbo-1106</code> model</a>. I iterated over my dataset to generate predictions using that finetuned model using the OpenAI SDK.</p>
<div data-execution_count="4">
<details>
<summary>Code</summary>
<div id="cb17"><pre><code><span id="cb17-1"><span>from</span> openai <span>import</span> OpenAI</span>
<span id="cb17-2"><span>import</span> os</span>
<span id="cb17-3"><span>from</span> datasets <span>import</span> load_dataset</span>
<span id="cb17-4"></span>
<span id="cb17-5">preds_test_data <span>=</span> load_dataset(<span>"strickvl/isafpressreleases_with_preds_2"</span>)[</span>
<span id="cb17-6">    <span>"train"</span></span>
<span id="cb17-7">].to_list()</span>
<span id="cb17-8"></span>
<span id="cb17-9">client <span>=</span> OpenAI(api_key<span>=</span>os.getenv(<span>"OPENAI_API_KEY"</span>))</span>
<span id="cb17-10"></span>
<span id="cb17-11"><span>for</span> row <span>in</span> preds_test_data:</span>
<span id="cb17-12">    response <span>=</span> client.chat.completions.create(</span>
<span id="cb17-13">        model<span>=</span><span>"ft:gpt-3.5-turbo-1106:SOME_MODEL_ID_GOES_HERE"</span>,</span>
<span id="cb17-14">        messages<span>=</span>[</span>
<span id="cb17-15">            {</span>
<span id="cb17-16">                <span>"role"</span>: <span>"system"</span>,</span>
<span id="cb17-17">                <span>"content"</span>: <span>"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']."</span>,</span>
<span id="cb17-18">            },</span>
<span id="cb17-19">            {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: row[<span>"text"</span>]},</span>
<span id="cb17-20">        ],</span>
<span id="cb17-21">        temperature<span>=</span><span>0</span>,</span>
<span id="cb17-22">    )</span>
<span id="cb17-23">    row[<span>"predictions"</span>][<span>"finetuned-openai-gpt-3.5-turbo-1106"</span>] <span>=</span> response.choices[</span>
<span id="cb17-24">        <span>0</span></span>
<span id="cb17-25">    ].message.content</span></code></pre></div>
</details>
</div>
</section>
<section id="finetuned-mistral-models-via-openpipe">
<h2 data-anchor-id="finetuned-mistral-models-via-openpipe">Finetuned Mistral models (via OpenPipe)</h2>
<p>I finetuned Mistral 7B and Mistral 8x7B models using OpenPipe so as to have something reasonable to compare the other models to. As always, OpenPipe makes it pretty easy to spin up a finetuning job and get predictions.</p>
<div>
<details>
<summary>Code</summary>
<div id="cb18"><pre><code><span id="cb18-1"><span>from</span> openpipe <span>import</span> OpenAI</span>
<span id="cb18-2"><span>import</span> os</span>
<span id="cb18-3"><span>from</span> datasets <span>import</span> load_dataset</span>
<span id="cb18-4"></span>
<span id="cb18-5">preds_test_data <span>=</span> load_dataset(<span>"strickvl/isafpressreleases_test_predictions_old"</span>)[</span>
<span id="cb18-6">    <span>"train"</span></span>
<span id="cb18-7">].to_list()</span>
<span id="cb18-8"></span>
<span id="cb18-9">client <span>=</span> OpenAI(openpipe<span>=</span>{<span>"api_key"</span>: os.getenv(<span>"OPENPIPE_API_KEY"</span>)})</span>
<span id="cb18-10"></span>
<span id="cb18-11"><span>for</span> i, row <span>in</span> <span>enumerate</span>(preds_test_data, <span>1</span>):</span>
<span id="cb18-12">    completion_7b <span>=</span> client.chat.completions.create(</span>
<span id="cb18-13">        model<span>=</span><span>"openpipe:twelve-pumas-invent"</span>,</span>
<span id="cb18-14">        messages<span>=</span>[</span>
<span id="cb18-15">            {</span>
<span id="cb18-16">                <span>"role"</span>: <span>"system"</span>,</span>
<span id="cb18-17">                <span>"content"</span>: <span>"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']."</span>,</span>
<span id="cb18-18">            },</span>
<span id="cb18-19">            {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: row[<span>"text"</span>]},</span>
<span id="cb18-20">        ],</span>
<span id="cb18-21">        temperature<span>=</span><span>0</span>,</span>
<span id="cb18-22">        openpipe<span>=</span>{<span>"tags"</span>: {<span>"prompt_id"</span>: <span>"counting"</span>, <span>"any_key"</span>: <span>"any_value"</span>}},</span>
<span id="cb18-23">    )</span>
<span id="cb18-24"></span>
<span id="cb18-25">    row[<span>"predictions"</span>][<span>"finetuned-mistral-7b-optimised-openpipe"</span>] <span>=</span> completion_7b.choices[<span>0</span>].message.content</span>
<span id="cb18-26">    </span>
<span id="cb18-27">    <span>if</span> i <span>%</span> <span>100</span> <span>==</span> <span>0</span>:</span>
<span id="cb18-28">        <span>print</span>(<span>f"</span><span>{</span>i<span>}</span><span>/724 rows complete"</span>)</span></code></pre></div>
</details>
</div>
</section>
<section id="finetuned-solar-llm-via-predibase">
<h2 data-anchor-id="finetuned-solar-llm-via-predibase">Finetuned Solar LLM (via Predibase)</h2>
<p>Predibase announced <a href="https://predibase.com/blog/solar-llm-on-predibase-the-best-llm-for-fine-tuning">a new best-in-class model for finetuning</a>, the Solar LLM from Upstage, a week or so ago so I thought I’d try it out. The advantage of this model is that it’s trained to be good at the kinds of tasks people commonly finetune models for, like structured data extraction. As you’ll see below, it did pretty well! <a href="https://huggingface.co/upstage/SOLAR-10.7B-v1.0">The base model is this one</a>, I think, on the Hugging Face Hub so it’s available for you all to use as well.</p>
<div>
<details>
<summary>Code</summary>
<div id="cb19"><pre><code><span id="cb19-1"><span>from</span> predibase <span>import</span> Predibase</span>
<span id="cb19-2"></span>
<span id="cb19-3">pb <span>=</span> Predibase(api_token<span>=</span><span>"MY_API_TOKEN_GOES_HERE"</span>)</span>
<span id="cb19-4"></span>
<span id="cb19-5">lorax_client <span>=</span> pb.deployments.client(<span>"solar-1-mini-chat-240612"</span>)</span>
<span id="cb19-6"></span>
<span id="cb19-7">preds_test_data <span>=</span> load_dataset(<span>"strickvl/isafpressreleases_test_predictions"</span>)[</span>
<span id="cb19-8">    <span>"train"</span></span>
<span id="cb19-9">].to_list()</span>
<span id="cb19-10"></span>
<span id="cb19-11"><span>for</span> i, row <span>in</span> <span>enumerate</span>(preds_test_data, <span>1</span>):</span>
<span id="cb19-12">    prompt <span>=</span> <span>f"""You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span><span>\n\n</span><span>### Instruction:</span><span>\n\n</span><span>PRESS RELEASE TEXT: "</span><span>{</span>row[<span>'text'</span>]<span>}</span><span>"</span><span>\n\n</span><span>### Response:"""</span></span>
<span id="cb19-13">    response <span>=</span> lorax_client.generate(prompt, adapter_id<span>=</span><span>"isafpr/2"</span>, max_new_tokens<span>=</span><span>300</span>).generated_text</span>
<span id="cb19-14"></span>
<span id="cb19-15">    row[<span>"predictions"</span>][<span>"ft-solar-1-mini-chat-240612-predibase"</span>] <span>=</span> response</span>
<span id="cb19-16"></span>
<span id="cb19-17">    <span>if</span> i <span>%</span> <span>100</span> <span>==</span> <span>0</span>:</span>
<span id="cb19-18">        <span>print</span>(<span>f"</span><span>{</span>i<span>}</span><span>/724 rows complete"</span>)</span></code></pre></div>
</details>
</div>
</section>
<section id="finetuned-llama3-predictions-via-openpipe">
<h2 data-anchor-id="finetuned-llama3-predictions-via-openpipe">Finetuned Llama3 predictions (via OpenPipe)</h2>
<p>My locally finetuned Llama3 model hadn’t really worked well, but on OpenPipe the outputs seemed to look ok, so I used these predictions for the final evaluation.</p>
<div data-execution_count="7">
<details>
<summary>Code</summary>
<div id="cb20"><pre><code><span id="cb20-1"><span>from</span> openpipe <span>import</span> OpenAI</span>
<span id="cb20-2"><span>import</span> os</span>
<span id="cb20-3"><span>from</span> datasets <span>import</span> load_dataset</span>
<span id="cb20-4"></span>
<span id="cb20-5">preds_test_data <span>=</span> load_dataset(<span>"strickvl/isafpressreleases_with_preds_3"</span>)[</span>
<span id="cb20-6">    <span>"train"</span></span>
<span id="cb20-7">].to_list()</span>
<span id="cb20-8"></span>
<span id="cb20-9">client <span>=</span> OpenAI(openpipe<span>=</span>{<span>"api_key"</span>: os.getenv(<span>"OPENPIPE_API_KEY"</span>)})</span>
<span id="cb20-10"></span>
<span id="cb20-11"><span>for</span> row <span>in</span> preds_test_data:</span>
<span id="cb20-12">    completion <span>=</span> client.chat.completions.create(</span>
<span id="cb20-13">        model<span>=</span><span>"openpipe:fine-steaks-taste"</span>,</span>
<span id="cb20-14">        messages<span>=</span>[</span>
<span id="cb20-15">            {</span>
<span id="cb20-16">                <span>"role"</span>: <span>"system"</span>,</span>
<span id="cb20-17">                <span>"content"</span>: <span>"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']."</span>,</span>
<span id="cb20-18">            },</span>
<span id="cb20-19">            {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: row[<span>"text"</span>]},</span>
<span id="cb20-20">        ],</span>
<span id="cb20-21">        temperature<span>=</span><span>0</span>,</span>
<span id="cb20-22">        openpipe<span>=</span>{<span>"tags"</span>: {<span>"prompt_id"</span>: <span>"counting"</span>, <span>"any_key"</span>: <span>"any_value"</span>}},</span>
<span id="cb20-23">    )</span>
<span id="cb20-24"></span>
<span id="cb20-25">    row[<span>"predictions"</span>][<span>"finetuned-llama3-7b-32k-openpipe"</span>] <span>=</span> completion.choices[</span>
<span id="cb20-26">        <span>0</span></span>
<span id="cb20-27">    ].message.content</span></code></pre></div>
</details>
</div>
<p>By the end of this process you can see we have a bunch of predictions attached to each entry in our dataset. You can view all of these <a href="https://huggingface.co/datasets/strickvl/isafpressreleases_test_predictions">in the public dataset</a> I published <a href="https://huggingface.co/datasets/strickvl/isafpressreleases_test_predictions">on the Hugging Face Hub</a>.</p>
<div data-execution_count="8">
<div id="cb21"><pre><code><span id="cb21-1"><span>from</span> rich <span>import</span> <span>print</span></span>
<span id="cb21-2"></span>
<span id="cb21-3"><span>print</span>(preds_test_data[<span>0</span>])</span></code></pre></div>
<div>
<pre><span>{</span>
    <span>'name'</span>: <span>'5'</span>,
    <span>'text'</span>: <span>'2013-01-S-025\n\nKABUL, Afghanistan (Jan. 25, 2013)\nDuring a security operation in Andar district, </span>
<span>Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a </span>
<span>group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire </span>
<span>attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan </span>
<span>National Police in Ghazni province.'</span>,
    <span>'predictions'</span>: <span>{</span>
        <span>'finetuned-llama3-7b-32k-openpipe'</span>: 
<span>'{"name":"1","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["tal</span>
<span>iban"],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":true,"airstrike":false,"nosh</span>
<span>otsfired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span>'finetuned-mistral-7b-optimised-openpipe'</span>: 
<span>'{"name":"1","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["tal</span>
<span>iban"],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":true,"airstrike":false,"nosh</span>
<span>otsfired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span>'finetuned-openai-gpt-3.5-turbo-1106'</span>: 
<span>'{"name":"4","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["tal</span>
<span>iban"],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":true,"airstrike":false,"nosh</span>
<span>otsfired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span>'gpt-3.5-turbo'</span>: <span>'{\n    "name": "Taliban leader Alaudin killed in Ghazni province",\n    "start_date": </span>
<span>"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span>["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span>"killcaptureraid": false,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span>"min_leaders_captured": 0\n}'</span>,
        <span>'gpt-4-turbo'</span>: <span>'{\n    "name": "Taliban leader Alaudin killed in Ghazni",\n    "start_date": </span>
<span>"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span>["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span>"killcaptureraid": true,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span>"min_leaders_captured": 0\n}'</span>,
        <span>'gpt-4o'</span>: <span>'{\n  "name": "Taliban leader Alaudin killed in Ghazni",\n  "start_date": "2013-01-24",\n  </span>
<span>"event_type": ["insurgentskilled", "captureandkill"],\n  "province": ["ghazni"],\n  "target_group": ["taliban"],\n </span>
<span>"min_killed": 1,\n  "min_captured": 0,\n  "killq": true,\n  "captureq": false,\n  "killcaptureraid": true,\n  </span>
<span>"airstrike": false,\n  "noshotsfired": false,\n  "min_leaders_killed": 1,\n  "min_leaders_captured": 0\n}'</span>,
        <span>'mistral-lora-templatefree'</span>: <span>'1'</span>,
        <span>'tinyllama-sharegpt'</span>: 
<span>'{"name":"2","start_date":"2013-01-24","event_type":["airstrike"],"province":["ghazni"],"target_group":["taliban"],</span>
<span>"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":true,"noshotsfire</span>
<span>d":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span>'tinyllama-templatefree'</span>: <span>'\n{"name":"Taliban leader killed in </span>
<span>Ghazni","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["taliban"</span>
<span>],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":false,"noshotsf</span>
<span>ired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span>'ft-solar-1-mini-chat-240612-predibase'</span>: 
<span>'\n\n{"name":"2","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":[</span>
<span>"taliban"],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":true,"airstrike":false,"</span>
<span>noshotsfired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>
    <span>}</span>,
    <span>'start_date'</span>: <span>datetime.date</span><span>(</span><span>2013</span>, <span>1</span>, <span>24</span><span>)</span>,
    <span>'province'</span>: <span>[</span><span>'ghazni'</span><span>]</span>,
    <span>'target_group'</span>: <span>[</span><span>'taliban'</span><span>]</span>,
    <span>'event_type'</span>: <span>[</span><span>'insurgentskilled'</span><span>]</span>,
    <span>'min_killed'</span>: <span>1</span>,
    <span>'min_captured'</span>: <span>0</span>,
    <span>'killq'</span>: <span>True</span>,
    <span>'captureq'</span>: <span>False</span>,
    <span>'killcaptureraid'</span>: <span>False</span>,
    <span>'airstrike'</span>: <span>False</span>,
    <span>'noshotsfired'</span>: <span>False</span>,
    <span>'min_leaders_killed'</span>: <span>1</span>,
    <span>'min_leaders_captured'</span>: <span>0</span>
<span>}</span>
</pre>
</div>
</div>
<p>Unfortunately the Qwen2 inference on Predibase is still not working so I’ll skip that finetuned model for the moment.</p>
<p>Now that we have predictions from seven finetuned models and three OpenAI models (to compare against), we can run our evaluations. I’ll start with a simple check to see what proportion of the predictions are even valid JSON.</p>
</section>
</section>
<section id="json-validity-test">
<h2>JSON Validity Test</h2>
<div id="cb22"><pre><code><span id="cb22-1"><span>from</span> datasets <span>import</span> load_dataset</span>
<span id="cb22-2"></span>
<span id="cb22-3">dataset_with_preds <span>=</span> load_dataset(<span>"strickvl/isafpressreleases_test_predictions"</span>)[</span>
<span id="cb22-4">    <span>"train"</span></span>
<span id="cb22-5">].to_list()</span></code></pre></div>
<div data-execution_count="30">
<details>
<summary>Code</summary>
<div id="cb23"><pre><code><span id="cb23-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb23-2"><span>import</span> json</span>
<span id="cb23-3"></span>
<span id="cb23-4">json_aggregate_scores <span>=</span> {</span>
<span id="cb23-5">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb23-6">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb23-7">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb23-8">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb23-9">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb23-10">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb23-11">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb23-12">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb23-13">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb23-14">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb23-15">}</span>
<span id="cb23-16"></span>
<span id="cb23-17"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb23-18">    <span>for</span> model <span>in</span> row[<span>"predictions"</span>]:</span>
<span id="cb23-19">        <span>try</span>:</span>
<span id="cb23-20">            json.loads(row[<span>"predictions"</span>][model])</span>
<span id="cb23-21">            json_aggregate_scores[model] <span>+=</span> <span>1</span></span>
<span id="cb23-22">        <span>except</span> (json.JSONDecodeError, <span>TypeError</span>):</span>
<span id="cb23-23">            <span>pass</span></span>
<span id="cb23-24"></span>
<span id="cb23-25"><span># print(json_aggregate_scores)</span></span>
<span id="cb23-26"></span>
<span id="cb23-27"><span># Separate GPT models and finetuned models</span></span>
<span id="cb23-28">gpt_models <span>=</span> [<span>"gpt-4o"</span>, <span>"gpt-4-turbo"</span>, <span>"gpt-3.5-turbo"</span>]</span>
<span id="cb23-29">finetuned_models <span>=</span> [</span>
<span id="cb23-30">    model <span>for</span> model <span>in</span> json_aggregate_scores.keys() <span>if</span> model <span>not</span> <span>in</span> gpt_models</span>
<span id="cb23-31">]</span>
<span id="cb23-32"></span>
<span id="cb23-33"><span># Create lists for plotting</span></span>
<span id="cb23-34">models <span>=</span> <span>list</span>(json_aggregate_scores.keys())</span>
<span id="cb23-35">scores <span>=</span> <span>list</span>(json_aggregate_scores.values())</span>
<span id="cb23-36">colors <span>=</span> [<span>"#1f77b4"</span> <span>if</span> model <span>in</span> gpt_models <span>else</span> <span>"#ff7f0e"</span> <span>for</span> model <span>in</span> models]</span>
<span id="cb23-37"></span>
<span id="cb23-38"><span># Create the plot</span></span>
<span id="cb23-39">fig, ax <span>=</span> plt.subplots(figsize<span>=</span>(<span>12</span>, <span>10</span>))</span>
<span id="cb23-40"></span>
<span id="cb23-41"><span># Plot horizontal bars</span></span>
<span id="cb23-42">bars <span>=</span> ax.barh(models, scores, color<span>=</span>colors)</span>
<span id="cb23-43"></span>
<span id="cb23-44"><span># Customize the plot</span></span>
<span id="cb23-45">ax.set_xlabel(<span>"Number of Valid JSON Outputs"</span>)</span>
<span id="cb23-46">ax.set_title(<span>"Valid JSON Outputs by Model"</span>)</span>
<span id="cb23-47">ax.set_xlim(<span>0</span>, <span>750</span>)  <span># Set x-axis limit to slightly above max score</span></span>
<span id="cb23-48"></span>
<span id="cb23-49"><span># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb23-50">ax.tick_params(axis<span>=</span><span>"y"</span>, labelsize<span>=</span><span>8</span>)</span>
<span id="cb23-51"></span>
<span id="cb23-52"><span># Add value labels at the end of each bar</span></span>
<span id="cb23-53"><span>for</span> bar <span>in</span> bars:</span>
<span id="cb23-54">    width <span>=</span> bar.get_width()</span>
<span id="cb23-55">    ax.text(</span>
<span id="cb23-56">        width, bar.get_y() <span>+</span> bar.get_height() <span>/</span> <span>2</span>, <span>f"</span><span>{</span>width<span>}</span><span>"</span>, ha<span>=</span><span>"left"</span>, va<span>=</span><span>"center"</span></span>
<span id="cb23-57">    )</span>
<span id="cb23-58"></span>
<span id="cb23-59"><span># Create custom legend handles</span></span>
<span id="cb23-60"><span>from</span> matplotlib.patches <span>import</span> Patch</span>
<span id="cb23-61"></span>
<span id="cb23-62">legend_elements <span>=</span> [</span>
<span id="cb23-63">    Patch(facecolor<span>=</span><span>"#ff7f0e"</span>, label<span>=</span><span>"Finetuned Models"</span>),</span>
<span id="cb23-64">    Patch(facecolor<span>=</span><span>"#1f77b4"</span>, label<span>=</span><span>"GPT Models"</span>),</span>
<span id="cb23-65">]</span>
<span id="cb23-66"></span>
<span id="cb23-67"><span># Add a legend outside the plot</span></span>
<span id="cb23-68">ax.legend(handles<span>=</span>legend_elements, loc<span>=</span><span>"center left"</span>, bbox_to_anchor<span>=</span>(<span>1</span>, <span>0.5</span>))</span>
<span id="cb23-69"></span>
<span id="cb23-70"><span># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb23-71">plt.tight_layout()</span>
<span id="cb23-72">plt.subplots_adjust(right<span>=</span><span>0.85</span>)</span>
<span id="cb23-73"></span>
<span id="cb23-74"><span># Show the plot</span></span>
<span id="cb23-75">plt.show()</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-22-output-1.png"></p>
</div>
<p>It’s already instructive to see the difference between the templatefree and the sharegpt template’s ability to generate valid JSON for the TinyLlama finetune. The OpenAI models generate valid JSON every single time, as does the finetuned Mistral and Llama3 models.</p>
<p>While writing the code to evaluate the models, I noticed that some entries were blank or had no predictions at all, so I looked into that next.</p>
<div data-execution_count="31">
<div id="cb24"><pre><code><span id="cb24-1"><span># find out how many of the predictions are None values or empty strings</span></span>
<span id="cb24-2">missing_values <span>=</span> {</span>
<span id="cb24-3">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb24-4">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb24-5">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb24-6">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb24-7">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb24-8">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb24-9">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb24-10">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb24-11">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb24-12">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb24-13">}</span>
<span id="cb24-14"></span>
<span id="cb24-15"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb24-16">    <span>for</span> model <span>in</span> row[<span>"predictions"</span>]:</span>
<span id="cb24-17">        <span>if</span> row[<span>"predictions"</span>][model] <span>is</span> <span>None</span> <span>or</span> row[<span>"predictions"</span>][model] <span>==</span> <span>""</span>:</span>
<span id="cb24-18">            missing_values[model] <span>+=</span> <span>1</span></span>
<span id="cb24-19"></span>
<span id="cb24-20"><span>print</span>(missing_values)</span></code></pre></div>
<div>
<pre><span>{</span>
    <span>'gpt-4o'</span>: <span>0</span>,
    <span>'gpt-4-turbo'</span>: <span>0</span>,
    <span>'gpt-3.5-turbo'</span>: <span>0</span>,
    <span>'tinyllama-templatefree'</span>: <span>0</span>,
    <span>'tinyllama-sharegpt'</span>: <span>38</span>,
    <span>'finetuned-openai-gpt-3.5-turbo-1106'</span>: <span>0</span>,
    <span>'finetuned-llama3-7b-32k-openpipe'</span>: <span>0</span>,
    <span>'mistral-lora-templatefree'</span>: <span>0</span>,
    <span>'finetuned-mistral-7b-optimised-openpipe'</span>: <span>0</span>,
    <span>'ft-solar-1-mini-chat-240612-predibase'</span>: <span>0</span>
<span>}</span>
</pre>
</div>
</div>
<p>So were it not for the missing values, the <code>tinyllama-sharegpt</code> model would have had all 724 predictions, and valid JSON as well.</p>
<p>Now we can get into what we’re really interested in: accuracy. I’ll calculate scores for all the properties where it makes sense for us to have a score and then show the results comparing the models.</p>
<p>These are:</p>
<ul>
<li><code>start_date</code></li>
<li><code>province</code></li>
<li><code>target_group</code></li>
<li><code>event_type</code></li>
<li><code>min_killed</code></li>
<li><code>min_captured</code></li>
<li><code>killq</code></li>
<li><code>captureq</code></li>
<li><code>killcaptureraid</code></li>
<li><code>airstrike</code></li>
<li><code>noshotsfired</code></li>
<li><code>min_leaders_killed</code></li>
<li><code>min_leaders_captured</code></li>
</ul>
<p>Important note, for all these charts that follow, the total number of tasks was 724, so the numbers are out of a total of 724.</p>
</section>
<section id="start-date-accuracy">
<h2>Start Date Accuracy</h2>
<div data-execution_count="14">
<details>
<summary>Code</summary>
<div id="cb25"><pre><code><span id="cb25-1">start_date_scores <span>=</span> {</span>
<span id="cb25-2">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb25-3">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb25-4">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb25-5">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb25-6">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb25-7">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb25-8">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb25-9">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb25-10">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb25-11">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb25-12">}</span>
<span id="cb25-13"></span>
<span id="cb25-14"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb25-15">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb25-16">        <span>if</span> <span>not</span> pred:</span>
<span id="cb25-17">            <span>continue</span></span>
<span id="cb25-18">        <span>try</span>:</span>
<span id="cb25-19">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb25-20">            <span>if</span> (</span>
<span id="cb25-21">                <span>type</span>(pred_dict) <span>not</span> <span>in</span> (<span>int</span>, <span>float</span>)</span>
<span id="cb25-22">                <span>and</span> pred_dict.get(<span>"start_date"</span>)</span>
<span id="cb25-23">                <span>and</span> (pred_dict[<span>"start_date"</span>] <span>==</span> row[<span>"start_date"</span>].strftime(<span>"%Y-%m-</span><span>%d</span><span>"</span>))</span>
<span id="cb25-24">            ):</span>
<span id="cb25-25">                start_date_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb25-26">        <span>except</span> json.JSONDecodeError:</span>
<span id="cb25-27">            <span>pass</span></span>
<span id="cb25-28"></span>
<span id="cb25-29"><span>print</span>(start_date_scores)</span></code></pre></div>
</details>
<div>
<pre><span>{</span>
    <span>'gpt-4o'</span>: <span>527</span>,
    <span>'gpt-4-turbo'</span>: <span>522</span>,
    <span>'gpt-3.5-turbo'</span>: <span>492</span>,
    <span>'tinyllama-templatefree'</span>: <span>231</span>,
    <span>'tinyllama-sharegpt'</span>: <span>479</span>,
    <span>'finetuned-openai-gpt-3.5-turbo-1106'</span>: <span>646</span>,
    <span>'finetuned-llama3-7b-32k-openpipe'</span>: <span>585</span>,
    <span>'mistral-lora-templatefree'</span>: <span>0</span>,
    <span>'finetuned-mistral-7b-optimised-openpipe'</span>: <span>636</span>,
    <span>'ft-solar-1-mini-chat-240612-predibase'</span>: <span>649</span>
<span>}</span>
</pre>
</div>
</div>
<div data-execution_count="15">
<details>
<summary>Code</summary>
<div id="cb26"><pre><code><span id="cb26-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb26-2"></span>
<span id="cb26-3"><span># Separate GPT models and finetuned models</span></span>
<span id="cb26-4">gpt_models <span>=</span> [<span>"gpt-4o"</span>, <span>"gpt-4-turbo"</span>, <span>"gpt-3.5-turbo"</span>]</span>
<span id="cb26-5">finetuned_models <span>=</span> [</span>
<span id="cb26-6">    model <span>for</span> model <span>in</span> start_date_scores.keys() <span>if</span> model <span>not</span> <span>in</span> gpt_models</span>
<span id="cb26-7">]</span>
<span id="cb26-8"></span>
<span id="cb26-9"><span># Create lists for plotting</span></span>
<span id="cb26-10">models <span>=</span> <span>list</span>(start_date_scores.keys())</span>
<span id="cb26-11">scores <span>=</span> <span>list</span>(start_date_scores.values())</span>
<span id="cb26-12">colors <span>=</span> [<span>"#1f77b4"</span> <span>if</span> model <span>in</span> gpt_models <span>else</span> <span>"#ff7f0e"</span> <span>for</span> model <span>in</span> models]</span>
<span id="cb26-13"></span>
<span id="cb26-14"><span># Create the plot</span></span>
<span id="cb26-15">fig, ax <span>=</span> plt.subplots(figsize<span>=</span>(<span>12</span>, <span>10</span>))</span>
<span id="cb26-16"></span>
<span id="cb26-17"><span># Plot horizontal bars</span></span>
<span id="cb26-18">bars <span>=</span> ax.barh(models, scores, color<span>=</span>colors)</span>
<span id="cb26-19"></span>
<span id="cb26-20"><span># Customize the plot</span></span>
<span id="cb26-21">ax.set_xlabel(<span>"Number of Correct Start Dates"</span>)</span>
<span id="cb26-22">ax.set_title(<span>"Correct Start Dates by Model"</span>)</span>
<span id="cb26-23">ax.set_xlim(<span>0</span>, <span>max</span>(scores) <span>+</span> <span>50</span>)  <span># Set x-axis limit to slightly above max score</span></span>
<span id="cb26-24"></span>
<span id="cb26-25"><span># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb26-26">ax.tick_params(axis<span>=</span><span>"y"</span>, labelsize<span>=</span><span>8</span>)</span>
<span id="cb26-27"></span>
<span id="cb26-28"><span># Add value labels at the end of each bar</span></span>
<span id="cb26-29"><span>for</span> bar <span>in</span> bars:</span>
<span id="cb26-30">    width <span>=</span> bar.get_width()</span>
<span id="cb26-31">    ax.text(</span>
<span id="cb26-32">        width, bar.get_y() <span>+</span> bar.get_height() <span>/</span> <span>2</span>, <span>f"</span><span>{</span>width<span>}</span><span>"</span>, ha<span>=</span><span>"left"</span>, va<span>=</span><span>"center"</span></span>
<span id="cb26-33">    )</span>
<span id="cb26-34"></span>
<span id="cb26-35"><span># Create custom legend handles</span></span>
<span id="cb26-36"><span>from</span> matplotlib.patches <span>import</span> Patch</span>
<span id="cb26-37"></span>
<span id="cb26-38">legend_elements <span>=</span> [</span>
<span id="cb26-39">    Patch(facecolor<span>=</span><span>"#ff7f0e"</span>, label<span>=</span><span>"Finetuned Models"</span>),</span>
<span id="cb26-40">    Patch(facecolor<span>=</span><span>"#1f77b4"</span>, label<span>=</span><span>"GPT Models"</span>),</span>
<span id="cb26-41">]</span>
<span id="cb26-42"></span>
<span id="cb26-43"><span># Add a legend outside the plot</span></span>
<span id="cb26-44">ax.legend(handles<span>=</span>legend_elements, loc<span>=</span><span>"center left"</span>, bbox_to_anchor<span>=</span>(<span>1</span>, <span>0.5</span>))</span>
<span id="cb26-45"></span>
<span id="cb26-46"><span># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb26-47">plt.tight_layout()</span>
<span id="cb26-48">plt.subplots_adjust(right<span>=</span><span>0.85</span>)</span>
<span id="cb26-49"></span>
<span id="cb26-50"><span># Show the plot</span></span>
<span id="cb26-51">plt.show()</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-25-output-1.png"></p>
</div>
<p>Both Solar and our finetuned GPT3.5 model performed best on predicting which date the event took place. I’m surprised how poorly the OpenAI models did here, actually. And even our best model still got 75 of the dates wrong. This feels like something that I’d want to improve on. Possibly synthetic data could help, or maybe just an improvement in the finetuning prompt as well.</p>
</section>
<section id="province-accuracy">
<h2>Province Accuracy</h2>
<div data-execution_count="16">
<details>
<summary>Code</summary>
<div id="cb27"><pre><code><span id="cb27-1">province_scores <span>=</span> {</span>
<span id="cb27-2">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb27-3">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb27-4">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb27-5">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb27-6">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb27-7">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb27-8">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb27-9">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb27-10">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb27-11">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb27-12">}</span>
<span id="cb27-13"></span>
<span id="cb27-14"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb27-15">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb27-16">        <span>if</span> <span>not</span> pred:</span>
<span id="cb27-17">            <span>continue</span></span>
<span id="cb27-18">        <span>try</span>:</span>
<span id="cb27-19">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb27-20">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"province"</span> <span>in</span> pred_dict:</span>
<span id="cb27-21">                pred_provinces <span>=</span> pred_dict[<span>"province"</span>]</span>
<span id="cb27-22">                <span>if</span> <span>isinstance</span>(pred_provinces, <span>str</span>):</span>
<span id="cb27-23">                    pred_provinces <span>=</span> [pred_provinces]</span>
<span id="cb27-24">                <span>if</span> <span>set</span>(pred_provinces) <span>==</span> <span>set</span>(row[<span>"province"</span>]):</span>
<span id="cb27-25">                    province_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb27-26">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb27-27">            <span>pass</span></span>
<span id="cb27-28"></span>
<span id="cb27-29"><span>print</span>(province_scores)</span></code></pre></div>
</details>
<div>
<pre><span>{</span>
    <span>'gpt-4o'</span>: <span>649</span>,
    <span>'gpt-4-turbo'</span>: <span>645</span>,
    <span>'gpt-3.5-turbo'</span>: <span>595</span>,
    <span>'tinyllama-templatefree'</span>: <span>335</span>,
    <span>'tinyllama-sharegpt'</span>: <span>660</span>,
    <span>'finetuned-openai-gpt-3.5-turbo-1106'</span>: <span>704</span>,
    <span>'finetuned-llama3-7b-32k-openpipe'</span>: <span>707</span>,
    <span>'mistral-lora-templatefree'</span>: <span>0</span>,
    <span>'finetuned-mistral-7b-optimised-openpipe'</span>: <span>711</span>,
    <span>'ft-solar-1-mini-chat-240612-predibase'</span>: <span>704</span>
<span>}</span>
</pre>
</div>
</div>
<div data-execution_count="17">
<details>
<summary>Code</summary>
<div id="cb28"><pre><code><span id="cb28-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb28-2"></span>
<span id="cb28-3"><span># Separate GPT models and finetuned models</span></span>
<span id="cb28-4">gpt_models <span>=</span> [<span>"gpt-4o"</span>, <span>"gpt-4-turbo"</span>, <span>"gpt-3.5-turbo"</span>]</span>
<span id="cb28-5">finetuned_models <span>=</span> [</span>
<span id="cb28-6">    model <span>for</span> model <span>in</span> province_scores.keys() <span>if</span> model <span>not</span> <span>in</span> gpt_models</span>
<span id="cb28-7">]</span>
<span id="cb28-8"></span>
<span id="cb28-9"><span># Create lists for plotting</span></span>
<span id="cb28-10">models <span>=</span> <span>list</span>(province_scores.keys())</span>
<span id="cb28-11">scores <span>=</span> <span>list</span>(province_scores.values())</span>
<span id="cb28-12">colors <span>=</span> [<span>"#1f77b4"</span> <span>if</span> model <span>in</span> gpt_models <span>else</span> <span>"#ff7f0e"</span> <span>for</span> model <span>in</span> models]</span>
<span id="cb28-13"></span>
<span id="cb28-14"><span># Create the plot</span></span>
<span id="cb28-15">fig, ax <span>=</span> plt.subplots(figsize<span>=</span>(<span>12</span>, <span>10</span>))</span>
<span id="cb28-16"></span>
<span id="cb28-17"><span># Plot horizontal bars</span></span>
<span id="cb28-18">bars <span>=</span> ax.barh(models, scores, color<span>=</span>colors)</span>
<span id="cb28-19"></span>
<span id="cb28-20"><span># Customize the plot</span></span>
<span id="cb28-21">ax.set_xlabel(<span>"Number of Correct Provinces"</span>)</span>
<span id="cb28-22">ax.set_title(<span>"Correct Provinces by Model"</span>)</span>
<span id="cb28-23">ax.set_xlim(<span>0</span>, <span>max</span>(scores) <span>+</span> <span>50</span>)  <span># Set x-axis limit to slightly above max score</span></span>
<span id="cb28-24"></span>
<span id="cb28-25"><span># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb28-26">ax.tick_params(axis<span>=</span><span>"y"</span>, labelsize<span>=</span><span>8</span>)</span>
<span id="cb28-27"></span>
<span id="cb28-28"><span># Add value labels at the end of each bar</span></span>
<span id="cb28-29"><span>for</span> bar <span>in</span> bars:</span>
<span id="cb28-30">    width <span>=</span> bar.get_width()</span>
<span id="cb28-31">    ax.text(</span>
<span id="cb28-32">        width, bar.get_y() <span>+</span> bar.get_height() <span>/</span> <span>2</span>, <span>f"</span><span>{</span>width<span>}</span><span>"</span>, ha<span>=</span><span>"left"</span>, va<span>=</span><span>"center"</span></span>
<span id="cb28-33">    )</span>
<span id="cb28-34"></span>
<span id="cb28-35"><span># Create custom legend handles</span></span>
<span id="cb28-36"><span>from</span> matplotlib.patches <span>import</span> Patch</span>
<span id="cb28-37"></span>
<span id="cb28-38">legend_elements <span>=</span> [</span>
<span id="cb28-39">    Patch(facecolor<span>=</span><span>"#ff7f0e"</span>, label<span>=</span><span>"Finetuned Models"</span>),</span>
<span id="cb28-40">    Patch(facecolor<span>=</span><span>"#1f77b4"</span>, label<span>=</span><span>"GPT Models"</span>),</span>
<span id="cb28-41">]</span>
<span id="cb28-42"></span>
<span id="cb28-43"><span># Add a legend outside the plot</span></span>
<span id="cb28-44">ax.legend(handles<span>=</span>legend_elements, loc<span>=</span><span>"center left"</span>, bbox_to_anchor<span>=</span>(<span>1</span>, <span>0.5</span>))</span>
<span id="cb28-45"></span>
<span id="cb28-46"><span># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb28-47">plt.tight_layout()</span>
<span id="cb28-48">plt.subplots_adjust(right<span>=</span><span>0.85</span>)</span>
<span id="cb28-49"></span>
<span id="cb28-50"><span># Show the plot</span></span>
<span id="cb28-51">plt.show()</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-27-output-1.png"></p>
</div>
<p>In what will become a theme, the finetuned models actually outperform the OpenAI models, only making a few mistakes. Once again I’m surprised how poorly GPT3.5 did on this task.</p>
</section>
<section id="target-group-accuracy">
<h2>Target Group Accuracy</h2>
<p>Here there are potentially multiple groups mentioned as target group so I’ll give a score out of 1 of how many of the groups the model predicted were correct.</p>
<div data-execution_count="32">
<details>
<summary>Code</summary>
<div id="cb29"><pre><code><span id="cb29-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb29-2"></span>
<span id="cb29-3">target_group_scores <span>=</span> {</span>
<span id="cb29-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb29-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb29-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb29-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb29-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb29-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb29-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb29-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb29-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb29-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb29-14">}</span>
<span id="cb29-15"></span>
<span id="cb29-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb29-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb29-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb29-19">            <span>continue</span></span>
<span id="cb29-20">        <span>try</span>:</span>
<span id="cb29-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb29-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"target_group"</span> <span>in</span> pred_dict:</span>
<span id="cb29-23">                pred_groups <span>=</span> pred_dict[<span>"target_group"</span>]</span>
<span id="cb29-24">                <span>if</span> <span>isinstance</span>(pred_groups, <span>str</span>):</span>
<span id="cb29-25">                    pred_groups <span>=</span> [pred_groups]</span>
<span id="cb29-26">                correct_groups <span>=</span> row[<span>"target_group"</span>]</span>
<span id="cb29-27">                <span>if</span> <span>isinstance</span>(correct_groups, <span>str</span>):</span>
<span id="cb29-28">                    correct_groups <span>=</span> [correct_groups]</span>
<span id="cb29-29">                <span>if</span> correct_groups:</span>
<span id="cb29-30">                    score <span>=</span> <span>len</span>(<span>set</span>(pred_groups) <span>&amp;</span> <span>set</span>(correct_groups)) <span>/</span> <span>len</span>(</span>
<span id="cb29-31">                        <span>set</span>(correct_groups)</span>
<span id="cb29-32">                    )</span>
<span id="cb29-33">                    target_group_scores[model_name] <span>+=</span> score</span>
<span id="cb29-34">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb29-35">            <span>pass</span></span>
<span id="cb29-36"></span>
<span id="cb29-37"><span># Separate GPT models and finetuned models</span></span>
<span id="cb29-38">gpt_models <span>=</span> [<span>"gpt-4o"</span>, <span>"gpt-4-turbo"</span>, <span>"gpt-3.5-turbo"</span>]</span>
<span id="cb29-39">finetuned_models <span>=</span> [</span>
<span id="cb29-40">    model <span>for</span> model <span>in</span> target_group_scores.keys() <span>if</span> model <span>not</span> <span>in</span> gpt_models</span>
<span id="cb29-41">]</span>
<span id="cb29-42"></span>
<span id="cb29-43"><span># Create lists for plotting</span></span>
<span id="cb29-44">models <span>=</span> <span>list</span>(target_group_scores.keys())</span>
<span id="cb29-45">scores <span>=</span> <span>list</span>(target_group_scores.values())</span>
<span id="cb29-46">colors <span>=</span> [<span>"#1f77b4"</span> <span>if</span> model <span>in</span> gpt_models <span>else</span> <span>"#ff7f0e"</span> <span>for</span> model <span>in</span> models]</span>
<span id="cb29-47"></span>
<span id="cb29-48"><span># Create the plot</span></span>
<span id="cb29-49">fig, ax <span>=</span> plt.subplots(figsize<span>=</span>(<span>12</span>, <span>10</span>))</span>
<span id="cb29-50"></span>
<span id="cb29-51"><span># Plot horizontal bars</span></span>
<span id="cb29-52">bars <span>=</span> ax.barh(models, scores, color<span>=</span>colors)</span>
<span id="cb29-53"></span>
<span id="cb29-54"><span># Customize the plot</span></span>
<span id="cb29-55">ax.set_xlabel(<span>"Target Group Accuracy Score"</span>)</span>
<span id="cb29-56">ax.set_title(<span>"Target Group Accuracy by Model"</span>)</span>
<span id="cb29-57">ax.set_xlim(<span>0</span>, <span>max</span>(scores) <span>+</span> <span>50</span>)  <span># Set x-axis limit to slightly above max score</span></span>
<span id="cb29-58"></span>
<span id="cb29-59"><span># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb29-60">ax.tick_params(axis<span>=</span><span>"y"</span>, labelsize<span>=</span><span>8</span>)</span>
<span id="cb29-61"></span>
<span id="cb29-62"><span># Add value labels at the end of each bar</span></span>
<span id="cb29-63"><span>for</span> bar <span>in</span> bars:</span>
<span id="cb29-64">    width <span>=</span> bar.get_width()</span>
<span id="cb29-65">    ax.text(</span>
<span id="cb29-66">        width,</span>
<span id="cb29-67">        bar.get_y() <span>+</span> bar.get_height() <span>/</span> <span>2</span>,</span>
<span id="cb29-68">        <span>f"</span><span>{</span>width<span>:.2f}</span><span>"</span>,</span>
<span id="cb29-69">        ha<span>=</span><span>"left"</span>,</span>
<span id="cb29-70">        va<span>=</span><span>"center"</span>,</span>
<span id="cb29-71">    )</span>
<span id="cb29-72"></span>
<span id="cb29-73"><span># Create custom legend handles</span></span>
<span id="cb29-74"><span>from</span> matplotlib.patches <span>import</span> Patch</span>
<span id="cb29-75"></span>
<span id="cb29-76">legend_elements <span>=</span> [</span>
<span id="cb29-77">    Patch(facecolor<span>=</span><span>"#ff7f0e"</span>, label<span>=</span><span>"Finetuned Models"</span>),</span>
<span id="cb29-78">    Patch(facecolor<span>=</span><span>"#1f77b4"</span>, label<span>=</span><span>"GPT Models"</span>),</span>
<span id="cb29-79">]</span>
<span id="cb29-80"></span>
<span id="cb29-81"><span># Add a legend outside the plot</span></span>
<span id="cb29-82">ax.legend(handles<span>=</span>legend_elements, loc<span>=</span><span>"center left"</span>, bbox_to_anchor<span>=</span>(<span>1</span>, <span>0.5</span>))</span>
<span id="cb29-83"></span>
<span id="cb29-84"><span># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb29-85">plt.tight_layout()</span>
<span id="cb29-86">plt.subplots_adjust(right<span>=</span><span>0.85</span>)</span>
<span id="cb29-87"></span>
<span id="cb29-88"><span># Show the plot</span></span>
<span id="cb29-89">plt.show()</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-28-output-1.png"></p>
</div>
<p>Finetuned models doing significantly better than OpenAI for target group identification. I suspect that this would degrade if we added some new groups who weren’t in the training data (as I’d written about <a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html">in my last post</a>.)</p>
</section>
<section id="event-type-accuracy">
<h2>Event Type Accuracy</h2>
<div data-execution_count="19">
<details>
<summary>Code</summary>
<div id="cb30"><pre><code><span id="cb30-1"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb30-2"><span>from</span> matplotlib.patches <span>import</span> Patch</span>
<span id="cb30-3"><span>from</span> typing <span>import</span> Dict, Union</span>
<span id="cb30-4"></span>
<span id="cb30-5"></span>
<span id="cb30-6"><span>def</span> create_accuracy_chart(</span>
<span id="cb30-7">    scores: Dict[<span>str</span>, Union[<span>int</span>, <span>float</span>]], title: <span>str</span>, xlabel: <span>str</span></span>
<span id="cb30-8">) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb30-9">    <span># Separate GPT models and finetuned models</span></span>
<span id="cb30-10">    gpt_models <span>=</span> [<span>"gpt-4o"</span>, <span>"gpt-4-turbo"</span>, <span>"gpt-3.5-turbo"</span>]</span>
<span id="cb30-11">    finetuned_models <span>=</span> [model <span>for</span> model <span>in</span> scores.keys() <span>if</span> model <span>not</span> <span>in</span> gpt_models]</span>
<span id="cb30-12"></span>
<span id="cb30-13">    <span># Create lists for plotting</span></span>
<span id="cb30-14">    models <span>=</span> <span>list</span>(scores.keys())</span>
<span id="cb30-15">    scores_list <span>=</span> <span>list</span>(scores.values())</span>
<span id="cb30-16">    colors <span>=</span> [<span>"#1f77b4"</span> <span>if</span> model <span>in</span> gpt_models <span>else</span> <span>"#ff7f0e"</span> <span>for</span> model <span>in</span> models]</span>
<span id="cb30-17"></span>
<span id="cb30-18">    <span># Create the plot</span></span>
<span id="cb30-19">    fig, ax <span>=</span> plt.subplots(figsize<span>=</span>(<span>12</span>, <span>10</span>))</span>
<span id="cb30-20"></span>
<span id="cb30-21">    <span># Plot horizontal bars</span></span>
<span id="cb30-22">    bars <span>=</span> ax.barh(models, scores_list, color<span>=</span>colors)</span>
<span id="cb30-23"></span>
<span id="cb30-24">    <span># Customize the plot</span></span>
<span id="cb30-25">    ax.set_xlabel(xlabel)</span>
<span id="cb30-26">    ax.set_title(title)</span>
<span id="cb30-27">    ax.set_xlim(</span>
<span id="cb30-28">        <span>0</span>, <span>max</span>(scores_list) <span>+</span> <span>50</span></span>
<span id="cb30-29">    )  <span># Set x-axis limit to slightly above max score</span></span>
<span id="cb30-30"></span>
<span id="cb30-31">    <span># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb30-32">    ax.tick_params(axis<span>=</span><span>"y"</span>, labelsize<span>=</span><span>8</span>)</span>
<span id="cb30-33"></span>
<span id="cb30-34">    <span># Add value labels at the end of each bar</span></span>
<span id="cb30-35">    <span>for</span> bar <span>in</span> bars:</span>
<span id="cb30-36">        width <span>=</span> bar.get_width()</span>
<span id="cb30-37">        ax.text(</span>
<span id="cb30-38">            width,</span>
<span id="cb30-39">            bar.get_y() <span>+</span> bar.get_height() <span>/</span> <span>2</span>,</span>
<span id="cb30-40">            <span>f"</span><span>{</span>width<span>:.2f}</span><span>"</span> <span>if</span> <span>isinstance</span>(width, <span>float</span>) <span>else</span> <span>f"</span><span>{</span>width<span>}</span><span>"</span>,</span>
<span id="cb30-41">            ha<span>=</span><span>"left"</span>,</span>
<span id="cb30-42">            va<span>=</span><span>"center"</span>,</span>
<span id="cb30-43">        )</span>
<span id="cb30-44"></span>
<span id="cb30-45">    <span># Create custom legend handles</span></span>
<span id="cb30-46">    legend_elements <span>=</span> [</span>
<span id="cb30-47">        Patch(facecolor<span>=</span><span>"#ff7f0e"</span>, label<span>=</span><span>"Finetuned Models"</span>),</span>
<span id="cb30-48">        Patch(facecolor<span>=</span><span>"#1f77b4"</span>, label<span>=</span><span>"GPT Models"</span>),</span>
<span id="cb30-49">    ]</span>
<span id="cb30-50"></span>
<span id="cb30-51">    <span># Add a legend outside the plot</span></span>
<span id="cb30-52">    ax.legend(handles<span>=</span>legend_elements, loc<span>=</span><span>"center left"</span>, bbox_to_anchor<span>=</span>(<span>1</span>, <span>0.5</span>))</span>
<span id="cb30-53"></span>
<span id="cb30-54">    <span># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb30-55">    plt.tight_layout()</span>
<span id="cb30-56">    plt.subplots_adjust(right<span>=</span><span>0.85</span>)</span>
<span id="cb30-57"></span>
<span id="cb30-58">    <span># Show the plot</span></span>
<span id="cb30-59">    plt.show()</span>
<span id="cb30-60"></span>
<span id="cb30-61"></span>
<span id="cb30-62">event_type_scores <span>=</span> {</span>
<span id="cb30-63">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb30-64">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb30-65">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb30-66">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb30-67">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb30-68">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb30-69">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb30-70">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb30-71">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb30-72">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb30-73">}</span>
<span id="cb30-74"></span>
<span id="cb30-75"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb30-76">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb30-77">        <span>if</span> <span>not</span> pred:</span>
<span id="cb30-78">            <span>continue</span></span>
<span id="cb30-79">        <span>try</span>:</span>
<span id="cb30-80">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb30-81">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"event_type"</span> <span>in</span> pred_dict:</span>
<span id="cb30-82">                pred_types <span>=</span> pred_dict[<span>"event_type"</span>]</span>
<span id="cb30-83">                <span>if</span> <span>isinstance</span>(pred_types, <span>str</span>):</span>
<span id="cb30-84">                    pred_types <span>=</span> [pred_types]</span>
<span id="cb30-85">                correct_types <span>=</span> row[<span>"event_type"</span>]</span>
<span id="cb30-86">                <span>if</span> <span>isinstance</span>(correct_types, <span>str</span>):</span>
<span id="cb30-87">                    correct_types <span>=</span> [correct_types]</span>
<span id="cb30-88">                <span>if</span> correct_types:</span>
<span id="cb30-89">                    score <span>=</span> <span>len</span>(<span>set</span>(pred_types) <span>&amp;</span> <span>set</span>(correct_types)) <span>/</span> <span>len</span>(</span>
<span id="cb30-90">                        <span>set</span>(correct_types)</span>
<span id="cb30-91">                    )</span>
<span id="cb30-92">                    event_type_scores[model_name] <span>+=</span> score</span>
<span id="cb30-93">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb30-94">            <span>pass</span></span>
<span id="cb30-95"></span>
<span id="cb30-96">create_accuracy_chart(</span>
<span id="cb30-97">    scores<span>=</span>event_type_scores,</span>
<span id="cb30-98">    title<span>=</span><span>"Event Type Accuracy by Model"</span>,</span>
<span id="cb30-99">    xlabel<span>=</span><span>"Event Type Accuracy Score"</span>,</span>
<span id="cb30-100">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-29-output-1.png"></p>
</div>
<p>Event type is actually one of the hardest categories since there are actually some semantically overlapping categories and it’s sometimes even hard for a human annotator, but once again the finetuned models do pretty well.</p>
</section>
<section id="accuracy-for-min_killed">
<h2>Accuracy for <code>min_killed</code></h2>
<div data-execution_count="20">
<details>
<summary>Code</summary>
<div id="cb31"><pre><code><span id="cb31-1"><span>import</span> json</span>
<span id="cb31-2"></span>
<span id="cb31-3">min_killed_scores <span>=</span> {</span>
<span id="cb31-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb31-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb31-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb31-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb31-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb31-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb31-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb31-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb31-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb31-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb31-14">}</span>
<span id="cb31-15"></span>
<span id="cb31-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb31-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb31-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb31-19">            <span>continue</span></span>
<span id="cb31-20">        <span>try</span>:</span>
<span id="cb31-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb31-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"min_killed"</span> <span>in</span> pred_dict:</span>
<span id="cb31-23">                pred_min_killed <span>=</span> pred_dict[<span>"min_killed"</span>]</span>
<span id="cb31-24">                correct_min_killed <span>=</span> row[<span>"min_killed"</span>]</span>
<span id="cb31-25">                <span>if</span> pred_min_killed <span>==</span> correct_min_killed:</span>
<span id="cb31-26">                    min_killed_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb31-27">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb31-28">            <span>pass</span></span>
<span id="cb31-29"></span>
<span id="cb31-30">create_accuracy_chart(</span>
<span id="cb31-31">    scores<span>=</span>min_killed_scores,</span>
<span id="cb31-32">    title<span>=</span><span>"'min_killed' Accuracy by Model"</span>,</span>
<span id="cb31-33">    xlabel<span>=</span><span>"'min_killed' Accuracy Score"</span></span>
<span id="cb31-34">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-30-output-1.png"></p>
</div>
<p>For these number estimations, suddenly the playing fields gets leveled between the finetuned and the OpenAI models. Mistral still comes out on top, but not by much! And it’s impressive how the OpenAI models do really well at this. I suspect this is because of the whole section in the prompt which explained the rubric that was used for annotating the examples:</p>
<blockquote>
<p>Annotation notes: A ‘faciliator’ is not a leader. If a press release states that ‘insurgents’ were detained without further details, assign a minimum number of two detained. Interpret ‘a couple’ as two. Interpret ‘several’ as at least three, even though it may sometimes refer to seven or eight. Classify the terms ‘a few’, ‘some’, ‘a group’, ‘a small group’, and ‘multiple’ as denoting at least three, even if they sometimes refer to larger numbers. Choose the smaller number if no other information is available in the press release to come up with a minimally acceptable figure. Interpret ‘numerous’ and ‘a handful’ as at least four, and ‘a large number’ as at least five.</p>
</blockquote>
</section>
<section id="accuracy-for-min_captured">
<h2>Accuracy for <code>min_captured</code></h2>
<div data-execution_count="21">
<details>
<summary>Code</summary>
<div id="cb32"><pre><code><span id="cb32-1"><span>import</span> json</span>
<span id="cb32-2"></span>
<span id="cb32-3">min_captured_scores <span>=</span> {</span>
<span id="cb32-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb32-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb32-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb32-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb32-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb32-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb32-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb32-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb32-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb32-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb32-14">}</span>
<span id="cb32-15"></span>
<span id="cb32-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb32-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb32-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb32-19">            <span>continue</span></span>
<span id="cb32-20">        <span>try</span>:</span>
<span id="cb32-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb32-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"min_captured"</span> <span>in</span> pred_dict:</span>
<span id="cb32-23">                pred_min_captured <span>=</span> pred_dict[<span>"min_captured"</span>]</span>
<span id="cb32-24">                correct_min_captured <span>=</span> row[<span>"min_captured"</span>]</span>
<span id="cb32-25">                <span>if</span> pred_min_captured <span>==</span> correct_min_captured:</span>
<span id="cb32-26">                    min_captured_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb32-27">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb32-28">            <span>pass</span></span>
<span id="cb32-29"></span>
<span id="cb32-30">create_accuracy_chart(</span>
<span id="cb32-31">    scores<span>=</span>min_captured_scores,</span>
<span id="cb32-32">    title<span>=</span><span>"'min_captured' Accuracy by Model"</span>,</span>
<span id="cb32-33">    xlabel<span>=</span><span>"'min_captured' Accuracy Score"</span></span>
<span id="cb32-34">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-31-output-1.png"></p>
</div>
</section>
<section id="accuracy-for-killq">
<h2>Accuracy for <code>killq</code></h2>
<div data-execution_count="22">
<details>
<summary>Code</summary>
<div id="cb33"><pre><code><span id="cb33-1"><span>import</span> json</span>
<span id="cb33-2"></span>
<span id="cb33-3">killq_scores <span>=</span> {</span>
<span id="cb33-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb33-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb33-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb33-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb33-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb33-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb33-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb33-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb33-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb33-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb33-14">}</span>
<span id="cb33-15"></span>
<span id="cb33-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb33-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb33-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb33-19">            <span>continue</span></span>
<span id="cb33-20">        <span>try</span>:</span>
<span id="cb33-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb33-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"killq"</span> <span>in</span> pred_dict:</span>
<span id="cb33-23">                pred_killq <span>=</span> pred_dict[<span>"killq"</span>]</span>
<span id="cb33-24">                correct_killq <span>=</span> row[<span>"killq"</span>]</span>
<span id="cb33-25">                <span>if</span> pred_killq <span>==</span> correct_killq:</span>
<span id="cb33-26">                    killq_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb33-27">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb33-28">            <span>pass</span></span>
<span id="cb33-29"></span>
<span id="cb33-30">create_accuracy_chart(</span>
<span id="cb33-31">    scores<span>=</span>killq_scores,</span>
<span id="cb33-32">    title<span>=</span><span>"'killq' Accuracy by Model"</span>,</span>
<span id="cb33-33">    xlabel<span>=</span><span>"'killq' Accuracy Score"</span></span>
<span id="cb33-34">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-32-output-1.png"></p>
</div>
<p>I’d expect really high accuracy for these boolean attributes, and basically almost all the models were able to give this. Still, our finetuned Mistral still beats out GPT-4o best score.</p>
</section>
<section id="accuracy-for-captureq">
<h2>Accuracy for <code>captureq</code></h2>
<div data-execution_count="23">
<details>
<summary>Code</summary>
<div id="cb34"><pre><code><span id="cb34-1"><span>import</span> json</span>
<span id="cb34-2"></span>
<span id="cb34-3">captureq_scores <span>=</span> {</span>
<span id="cb34-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb34-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb34-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb34-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb34-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb34-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb34-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb34-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb34-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb34-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb34-14">}</span>
<span id="cb34-15"></span>
<span id="cb34-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb34-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb34-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb34-19">            <span>continue</span></span>
<span id="cb34-20">        <span>try</span>:</span>
<span id="cb34-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb34-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"captureq"</span> <span>in</span> pred_dict:</span>
<span id="cb34-23">                pred_captureq <span>=</span> pred_dict[<span>"captureq"</span>]</span>
<span id="cb34-24">                correct_captureq <span>=</span> row[<span>"captureq"</span>]</span>
<span id="cb34-25">                <span>if</span> pred_captureq <span>==</span> correct_captureq:</span>
<span id="cb34-26">                    captureq_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb34-27">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb34-28">            <span>pass</span></span>
<span id="cb34-29"></span>
<span id="cb34-30">create_accuracy_chart(</span>
<span id="cb34-31">    scores<span>=</span>captureq_scores,</span>
<span id="cb34-32">    title<span>=</span><span>"'captureq' Accuracy by Model"</span>,</span>
<span id="cb34-33">    xlabel<span>=</span><span>"'captureq' Accuracy Score"</span></span>
<span id="cb34-34">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-33-output-1.png"></p>
</div>
</section>
<section id="accuracy-for-killcaptureraid">
<h2>Accuracy for <code>killcaptureraid</code></h2>
<div data-execution_count="24">
<details>
<summary>Code</summary>
<div id="cb35"><pre><code><span id="cb35-1"><span>import</span> json</span>
<span id="cb35-2"></span>
<span id="cb35-3">killcaptureraid_scores <span>=</span> {</span>
<span id="cb35-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb35-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb35-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb35-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb35-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb35-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb35-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb35-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb35-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb35-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb35-14">}</span>
<span id="cb35-15"></span>
<span id="cb35-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb35-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb35-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb35-19">            <span>continue</span></span>
<span id="cb35-20">        <span>try</span>:</span>
<span id="cb35-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb35-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"killcaptureraid"</span> <span>in</span> pred_dict:</span>
<span id="cb35-23">                pred_killcaptureraid <span>=</span> pred_dict[<span>"killcaptureraid"</span>]</span>
<span id="cb35-24">                correct_killcaptureraid <span>=</span> row[<span>"killcaptureraid"</span>]</span>
<span id="cb35-25">                <span>if</span> pred_killcaptureraid <span>==</span> correct_killcaptureraid:</span>
<span id="cb35-26">                    killcaptureraid_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb35-27">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb35-28">            <span>pass</span></span>
<span id="cb35-29"></span>
<span id="cb35-30">create_accuracy_chart(</span>
<span id="cb35-31">    scores<span>=</span>killcaptureraid_scores,</span>
<span id="cb35-32">    title<span>=</span><span>"'killcaptureraid' Accuracy by Model"</span>,</span>
<span id="cb35-33">    xlabel<span>=</span><span>"'killcaptureraid' Accuracy Score"</span></span>
<span id="cb35-34">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-34-output-1.png"></p>
</div>
<p>This is another attribute where it’s clear the lack of signposting in the prompts to the OpenAI models put them at a disadvantage. The term ‘kill-capture raid’ is a term of art and it was used in a specific way for the labelling. OpenAI knows nothing about how I made those calls, which explains why they performed so poorly here.</p>
</section>
<section id="accuracy-for-airstrike">
<h2>Accuracy for <code>airstrike</code></h2>
<div data-execution_count="25">
<details>
<summary>Code</summary>
<div id="cb36"><pre><code><span id="cb36-1"><span>import</span> json</span>
<span id="cb36-2"></span>
<span id="cb36-3">airstrike_scores <span>=</span> {</span>
<span id="cb36-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb36-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb36-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb36-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb36-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb36-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb36-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb36-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb36-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb36-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb36-14">}</span>
<span id="cb36-15"></span>
<span id="cb36-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb36-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb36-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb36-19">            <span>continue</span></span>
<span id="cb36-20">        <span>try</span>:</span>
<span id="cb36-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb36-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"airstrike"</span> <span>in</span> pred_dict:</span>
<span id="cb36-23">                pred_airstrike <span>=</span> pred_dict[<span>"airstrike"</span>]</span>
<span id="cb36-24">                correct_airstrike <span>=</span> row[<span>"airstrike"</span>]</span>
<span id="cb36-25">                <span>if</span> pred_airstrike <span>==</span> correct_airstrike:</span>
<span id="cb36-26">                    airstrike_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb36-27">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb36-28">            <span>pass</span></span>
<span id="cb36-29"></span>
<span id="cb36-30">create_accuracy_chart(</span>
<span id="cb36-31">    scores<span>=</span>airstrike_scores,</span>
<span id="cb36-32">    title<span>=</span><span>"'airstrike' Accuracy by Model"</span>,</span>
<span id="cb36-33">    xlabel<span>=</span><span>"'airstrike' Accuracy Score"</span></span>
<span id="cb36-34">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-35-output-1.png"></p>
</div>
</section>
<section id="accuracy-for-noshotsfired">
<h2>Accuracy for <code>noshotsfired</code></h2>
<div data-execution_count="26">
<details>
<summary>Code</summary>
<div id="cb37"><pre><code><span id="cb37-1"><span>import</span> json</span>
<span id="cb37-2"></span>
<span id="cb37-3">noshotsfired_scores <span>=</span> {</span>
<span id="cb37-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb37-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb37-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb37-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb37-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb37-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb37-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb37-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb37-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb37-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb37-14">}</span>
<span id="cb37-15"></span>
<span id="cb37-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb37-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb37-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb37-19">            <span>continue</span></span>
<span id="cb37-20">        <span>try</span>:</span>
<span id="cb37-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb37-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"noshotsfired"</span> <span>in</span> pred_dict:</span>
<span id="cb37-23">                pred_noshotsfired <span>=</span> pred_dict[<span>"noshotsfired"</span>]</span>
<span id="cb37-24">                correct_noshotsfired <span>=</span> row[<span>"noshotsfired"</span>]</span>
<span id="cb37-25">                <span>if</span> pred_noshotsfired <span>==</span> correct_noshotsfired:</span>
<span id="cb37-26">                    noshotsfired_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb37-27">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb37-28">            <span>pass</span></span>
<span id="cb37-29"></span>
<span id="cb37-30">create_accuracy_chart(</span>
<span id="cb37-31">    scores<span>=</span>noshotsfired_scores,</span>
<span id="cb37-32">    title<span>=</span><span>"'noshotsfired' Accuracy by Model"</span>,</span>
<span id="cb37-33">    xlabel<span>=</span><span>"'noshotsfired' Accuracy Score"</span></span>
<span id="cb37-34">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-36-output-1.png"></p>
</div>
<p>I’m not quite sure why the OpenAI models are performing in the reverse order of what you’d expect. Recall that the <code>noshotsfired</code> attribute refers to whether the press release states that no shots were fired during a particular raid / event. (For a certain period the press releases were keen to mention this and it was a metric that was particularly useful for ISAF as a public relations gimmick.)</p>
<p>I can think of some semi-anthropomorphizing ways to explain this around how the GPT-4 class of models were ‘overthinking’ the label, but more investigation would be needed to really understand this.</p>
</section>
<section id="accuracy-for-min_leaders_killed">
<h2>Accuracy for <code>min_leaders_killed</code></h2>
<div data-execution_count="27">
<details>
<summary>Code</summary>
<div id="cb38"><pre><code><span id="cb38-1"><span>import</span> json</span>
<span id="cb38-2"></span>
<span id="cb38-3">min_leaders_killed_scores <span>=</span> {</span>
<span id="cb38-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb38-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb38-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb38-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb38-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb38-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb38-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb38-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb38-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb38-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb38-14">}</span>
<span id="cb38-15"></span>
<span id="cb38-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb38-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb38-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb38-19">            <span>continue</span></span>
<span id="cb38-20">        <span>try</span>:</span>
<span id="cb38-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb38-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"min_leaders_killed"</span> <span>in</span> pred_dict:</span>
<span id="cb38-23">                pred_min_leaders_killed <span>=</span> pred_dict[<span>"min_leaders_killed"</span>]</span>
<span id="cb38-24">                correct_min_leaders_killed <span>=</span> row[<span>"min_leaders_killed"</span>]</span>
<span id="cb38-25">                <span>if</span> <span>isinstance</span>(pred_min_leaders_killed, <span>int</span>) <span>and</span> <span>isinstance</span>(correct_min_leaders_killed, <span>int</span>):</span>
<span id="cb38-26">                    <span>if</span> pred_min_leaders_killed <span>==</span> correct_min_leaders_killed:</span>
<span id="cb38-27">                        min_leaders_killed_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb38-28">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb38-29">            <span>pass</span></span>
<span id="cb38-30"></span>
<span id="cb38-31">total_entries <span>=</span> <span>len</span>(dataset_with_preds)</span>
<span id="cb38-32"></span>
<span id="cb38-33">create_accuracy_chart(</span>
<span id="cb38-34">    scores<span>=</span>min_leaders_killed_scores,</span>
<span id="cb38-35">    title<span>=</span><span>f"Min Leaders Killed Accuracy by Model (out of </span><span>{</span>total_entries<span>}</span><span> entries)"</span>,</span>
<span id="cb38-36">    xlabel<span>=</span><span>"Min Leaders Killed Accuracy Score"</span></span>
<span id="cb38-37">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-37-output-1.png"></p>
</div>
<p>We often hear about how LLMs are bad with numbers, how they <a href="https://gramener.com/llmrandom/">default to certain values</a> and so on, so I was surprised to see such high scores across the board for this task. I imagine this is something that everyone has been trying to improve and it shows. Still, though, our finetuned models do best.</p>
</section>
<section id="accuracy-for-min_leaders_captured">
<h2>Accuracy for <code>min_leaders_captured</code></h2>
<div data-execution_count="28">
<details>
<summary>Code</summary>
<div id="cb39"><pre><code><span id="cb39-1"><span>import</span> json</span>
<span id="cb39-2"></span>
<span id="cb39-3">min_leaders_captured_scores <span>=</span> {</span>
<span id="cb39-4">    <span>"gpt-4o"</span>: <span>0</span>,</span>
<span id="cb39-5">    <span>"gpt-4-turbo"</span>: <span>0</span>,</span>
<span id="cb39-6">    <span>"gpt-3.5-turbo"</span>: <span>0</span>,</span>
<span id="cb39-7">    <span>"tinyllama-templatefree"</span>: <span>0</span>,</span>
<span id="cb39-8">    <span>"tinyllama-sharegpt"</span>: <span>0</span>,</span>
<span id="cb39-9">    <span>"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span>0</span>,</span>
<span id="cb39-10">    <span>"finetuned-llama3-7b-32k-openpipe"</span>: <span>0</span>,</span>
<span id="cb39-11">    <span>"mistral-lora-templatefree"</span>: <span>0</span>,</span>
<span id="cb39-12">    <span>"finetuned-mistral-7b-optimised-openpipe"</span>: <span>0</span>,</span>
<span id="cb39-13">    <span>"ft-solar-1-mini-chat-240612-predibase"</span>: <span>0</span>,</span>
<span id="cb39-14">}</span>
<span id="cb39-15"></span>
<span id="cb39-16"><span>for</span> row <span>in</span> dataset_with_preds:</span>
<span id="cb39-17">    <span>for</span> model_name, pred <span>in</span> row[<span>"predictions"</span>].items():</span>
<span id="cb39-18">        <span>if</span> <span>not</span> pred:</span>
<span id="cb39-19">            <span>continue</span></span>
<span id="cb39-20">        <span>try</span>:</span>
<span id="cb39-21">            pred_dict <span>=</span> json.loads(pred)</span>
<span id="cb39-22">            <span>if</span> <span>isinstance</span>(pred_dict, <span>dict</span>) <span>and</span> <span>"min_leaders_captured"</span> <span>in</span> pred_dict:</span>
<span id="cb39-23">                pred_min_leaders_captured <span>=</span> pred_dict[<span>"min_leaders_captured"</span>]</span>
<span id="cb39-24">                correct_min_leaders_captured <span>=</span> row[<span>"min_leaders_captured"</span>]</span>
<span id="cb39-25">                <span>if</span> <span>isinstance</span>(pred_min_leaders_captured, <span>int</span>) <span>and</span> <span>isinstance</span>(correct_min_leaders_captured, <span>int</span>):</span>
<span id="cb39-26">                    <span>if</span> pred_min_leaders_captured <span>==</span> correct_min_leaders_captured:</span>
<span id="cb39-27">                        min_leaders_captured_scores[model_name] <span>+=</span> <span>1</span></span>
<span id="cb39-28">        <span>except</span> (json.JSONDecodeError, <span>KeyError</span>, <span>TypeError</span>):</span>
<span id="cb39-29">            <span>pass</span></span>
<span id="cb39-30"></span>
<span id="cb39-31">total_entries <span>=</span> <span>len</span>(dataset_with_preds)</span>
<span id="cb39-32"></span>
<span id="cb39-33">create_accuracy_chart(</span>
<span id="cb39-34">    scores<span>=</span>min_leaders_captured_scores,</span>
<span id="cb39-35">    title<span>=</span><span>f"Min Leaders Captured Accuracy by Model (out of </span><span>{</span>total_entries<span>}</span><span> entries)"</span>,</span>
<span id="cb39-36">    xlabel<span>=</span><span>"Min Leaders Captured Accuracy Score"</span></span>
<span id="cb39-37">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-38-output-1.png"></p>
</div>
</section>
<section id="final-aggregate-scores-for-the-models">
<h2>Final aggregate scores for the models</h2>
<p>Let’s add all these individual competency scores up, average them out and get final scores for how well our models do on accuracy.</p>
<div data-execution_count="29">
<details>
<summary>Code</summary>
<div id="cb40"><pre><code><span id="cb40-1"><span># adapt the function slightly</span></span>
<span id="cb40-2"><span>import</span> matplotlib.pyplot <span>as</span> plt</span>
<span id="cb40-3"><span>from</span> matplotlib.patches <span>import</span> Patch</span>
<span id="cb40-4"><span>from</span> typing <span>import</span> Dict, Union</span>
<span id="cb40-5"></span>
<span id="cb40-6"></span>
<span id="cb40-7"><span>def</span> create_aggregate_accuracy_chart(</span>
<span id="cb40-8">    scores: Dict[<span>str</span>, Union[<span>int</span>, <span>float</span>]], title: <span>str</span>, xlabel: <span>str</span></span>
<span id="cb40-9">) <span>-&gt;</span> <span>None</span>:</span>
<span id="cb40-10">    <span># Separate GPT models and finetuned models</span></span>
<span id="cb40-11">    gpt_models <span>=</span> [<span>"gpt-4o"</span>, <span>"gpt-4-turbo"</span>, <span>"gpt-3.5-turbo"</span>]</span>
<span id="cb40-12">    finetuned_models <span>=</span> [model <span>for</span> model <span>in</span> scores.keys() <span>if</span> model <span>not</span> <span>in</span> gpt_models]</span>
<span id="cb40-13"></span>
<span id="cb40-14">    <span># Create lists for plotting</span></span>
<span id="cb40-15">    models <span>=</span> <span>list</span>(scores.keys())</span>
<span id="cb40-16">    scores_list <span>=</span> <span>list</span>(scores.values())</span>
<span id="cb40-17">    colors <span>=</span> [<span>"#1f77b4"</span> <span>if</span> model <span>in</span> gpt_models <span>else</span> <span>"#ff7f0e"</span> <span>for</span> model <span>in</span> models]</span>
<span id="cb40-18"></span>
<span id="cb40-19">    <span># Create the plot</span></span>
<span id="cb40-20">    fig, ax <span>=</span> plt.subplots(figsize<span>=</span>(<span>12</span>, <span>10</span>))</span>
<span id="cb40-21"></span>
<span id="cb40-22">    <span># Plot horizontal bars</span></span>
<span id="cb40-23">    bars <span>=</span> ax.barh(models, scores_list, color<span>=</span>colors)</span>
<span id="cb40-24"></span>
<span id="cb40-25">    <span># Customize the plot</span></span>
<span id="cb40-26">    ax.set_xlabel(xlabel)</span>
<span id="cb40-27">    ax.set_title(title)</span>
<span id="cb40-28">    ax.set_xlim(<span>0</span>, <span>100</span>)  <span># Set x-axis limit to 100</span></span>
<span id="cb40-29"></span>
<span id="cb40-30">    <span># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb40-31">    ax.tick_params(axis<span>=</span><span>"y"</span>, labelsize<span>=</span><span>8</span>)</span>
<span id="cb40-32"></span>
<span id="cb40-33">    <span># Add value labels at the end of each bar</span></span>
<span id="cb40-34">    <span>for</span> bar <span>in</span> bars:</span>
<span id="cb40-35">        width <span>=</span> bar.get_width()</span>
<span id="cb40-36">        ax.text(</span>
<span id="cb40-37">            width,</span>
<span id="cb40-38">            bar.get_y() <span>+</span> bar.get_height() <span>/</span> <span>2</span>,</span>
<span id="cb40-39">            <span>f"</span><span>{</span>width<span>:.2f}</span><span>"</span> <span>if</span> <span>isinstance</span>(width, <span>float</span>) <span>else</span> <span>f"</span><span>{</span>width<span>}</span><span>"</span>,</span>
<span id="cb40-40">            ha<span>=</span><span>"left"</span>,</span>
<span id="cb40-41">            va<span>=</span><span>"center"</span>,</span>
<span id="cb40-42">        )</span>
<span id="cb40-43"></span>
<span id="cb40-44">    <span># Create custom legend handles</span></span>
<span id="cb40-45">    legend_elements <span>=</span> [</span>
<span id="cb40-46">        Patch(facecolor<span>=</span><span>"#ff7f0e"</span>, label<span>=</span><span>"Finetuned Models"</span>),</span>
<span id="cb40-47">        Patch(facecolor<span>=</span><span>"#1f77b4"</span>, label<span>=</span><span>"GPT Models"</span>),</span>
<span id="cb40-48">    ]</span>
<span id="cb40-49"></span>
<span id="cb40-50">    <span># Add a legend outside the plot</span></span>
<span id="cb40-51">    ax.legend(handles<span>=</span>legend_elements, loc<span>=</span><span>"center left"</span>, bbox_to_anchor<span>=</span>(<span>1</span>, <span>0.5</span>))</span>
<span id="cb40-52"></span>
<span id="cb40-53">    <span># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb40-54">    plt.tight_layout()</span>
<span id="cb40-55">    plt.subplots_adjust(right<span>=</span><span>0.85</span>)</span>
<span id="cb40-56"></span>
<span id="cb40-57">    <span># Show the plot</span></span>
<span id="cb40-58">    plt.show()</span>
<span id="cb40-59"></span>
<span id="cb40-60"><span># List of all the score dictionaries</span></span>
<span id="cb40-61">score_dicts <span>=</span> [</span>
<span id="cb40-62">    start_date_scores,</span>
<span id="cb40-63">    province_scores,</span>
<span id="cb40-64">    target_group_scores,</span>
<span id="cb40-65">    event_type_scores,</span>
<span id="cb40-66">    min_killed_scores,</span>
<span id="cb40-67">    min_captured_scores,</span>
<span id="cb40-68">    killq_scores,</span>
<span id="cb40-69">    captureq_scores,</span>
<span id="cb40-70">    killcaptureraid_scores,</span>
<span id="cb40-71">    airstrike_scores,</span>
<span id="cb40-72">    noshotsfired_scores,</span>
<span id="cb40-73">    min_leaders_killed_scores,</span>
<span id="cb40-74">    min_leaders_captured_scores,</span>
<span id="cb40-75">]</span>
<span id="cb40-76"></span>
<span id="cb40-77"><span># Get the list of models</span></span>
<span id="cb40-78">models <span>=</span> <span>list</span>(start_date_scores.keys())</span>
<span id="cb40-79"></span>
<span id="cb40-80"><span># Initialize the aggregate scores dictionary</span></span>
<span id="cb40-81">aggregate_scores <span>=</span> {model: <span>0</span> <span>for</span> model <span>in</span> models}</span>
<span id="cb40-82"></span>
<span id="cb40-83"><span># Calculate the aggregate score for each model</span></span>
<span id="cb40-84"><span>for</span> model <span>in</span> models:</span>
<span id="cb40-85">    total_score <span>=</span> <span>0</span></span>
<span id="cb40-86">    <span>for</span> score_dict <span>in</span> score_dicts:</span>
<span id="cb40-87">        total_score <span>+=</span> score_dict[model]</span>
<span id="cb40-88">    aggregate_scores[model] <span>=</span> (total_score <span>/</span> <span>len</span>(score_dicts)) <span>/</span> <span>len</span>(dataset_with_preds) <span>*</span> <span>100</span></span>
<span id="cb40-89"></span>
<span id="cb40-90"><span># Create the aggregate score chart</span></span>
<span id="cb40-91">create_aggregate_accuracy_chart(</span>
<span id="cb40-92">    scores<span>=</span>aggregate_scores,</span>
<span id="cb40-93">    title<span>=</span><span>"Aggregate Accuracy Score by Model (0-100 Scale)"</span>,</span>
<span id="cb40-94">    xlabel<span>=</span><span>"Aggregate Accuracy Score"</span>,</span>
<span id="cb40-95">)</span></code></pre></div>
</details>
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-39-output-1.png"></p>
</div>
<p>Surprising even me, the finetuned models beat out the GPT-class models from OpenAI. Actually even TinyLlama beats out GPT 3.5 Turbo!</p>
<p>Our top performer was Mistral-7B (finetuned on OpenPipe), very closely followed by Solar LLM and Llama3-7B. Just looking at the scores above it seems like anyone finetuning models for structured data extraction would do well to start of with Mistral-7B, Solar 7B or Llama3-7B and then see which performs best, though with the caveat that they might all be more or less the same for accuracy. There are probably different tradeoffs when it comes to serving the model and the efficiency and latency there, but still these three do really well.</p>
<p>I think if I were to stuff a few more examples into the prompt (as well a bit more explanation and rules) I could get the OpenAI models to perform even better, but at a certain point you have to remember all the things that having your own finetuned model brings you:</p>
<ul>
<li>data privacy (by not sending your confidential information to OpenAI)</li>
<li>smaller models most likely means better performance (though I still have to test and prove that out)</li>
<li>more control overall</li>
<li>cost improvements</li>
</ul>
<p>On the cost, it’s a bit hard to make that comparison or claim right now, especially given the economies of scale that the large cloud providers can rely on, but in a real-world use case where you were building this model for repeated inference over a long-term time frame, then you would have a better chance of having the cost argument make sense, particularly since the only way to make the OpenAI inference calls better to stuff them full of examples and extra explanation, significantly bloating the cost-per-query.</p>
<p>That said, there are some real tradeoffs that come up when finetuning a model, and I’ll get to some of those in my concluding thoughts.</p>
</section>
<section id="finetuning-works-a-charm-but">
<h2>Finetuning works a charm, but…</h2>
<p>First off I’m so pleased that the oft-repeated “finetune your model and get better performance than with GPT-4” actually turned out to be true! And not only was it true, but it was true with <em>relatively</em> little tweaks and adaptations. Remember all the above models are the first finetunes I made with the data I brought. I basically used just the default values for everything and so it worked out of the box.</p>
<p>For any further work I’ll focus on the Solar, Llama3 and Mistral 7B models which performed best. I used cloud finetuning services to finetune the best performing versions of those models, so I’ll want to get that all working locally as well.</p>
<section id="evals-were-a-pain-this-time-round">
<h2 data-anchor-id="evals-were-a-pain-this-time-round">Evals were a pain (this time round)…</h2>
<p>Most of the evaluation work is represented here in this notebook, and that was perhaps the seeds of my own misfortune. I had some models that worked locally, and then a bunch of other models deployed in different environments and with different services.</p>
<p>Not only that, but it was pretty slow to iterate through the 724 row so my test data (which the models hadn’t seen during finetuning, just to be clear) since I implemented it fairly naively.</p>
<p>If I were to now make some updates to the models, or get them working locally, I’d really want to make sure that I have a way to run these evals locally as well. Moreover, I’d want a way to run a subset of the evals (i.e.&nbsp;on a slice of the data) and then at some point switch that out so that they could run across all the data.</p>
<p>All of this is completely within the realm of possible, but for this round I was more focused on getting the results than I was about making the process repeatable and/or efficient. I know I can’t run all the models concurrently on the same machine, so maybe the way forward is simply to have a reliable cloud GPU provider like Modal where I can farm out these evaluations. I had a really good experience with them when I used them, so that’s probably the way forward there.</p>
<p>In general, it was also painful having the models living in different places. I had to remember so many things. In any ideal world, you want a standard interface for inference to all your models, especially if they’re for the same use case or project. It’s convenient that my finetuned GPT3.5 is automatically deployed and served by OpenAI, and the same goes for Llama3 and Solar or Mistral, but I want a single place where I can see them all. Until now I hadn’t really seen this project or problem as being so much about MLOps, but when you have multiple models in play and you’re finetuning and updating them and data is changing all the time, then you’ll need a way of managing all this.</p>
<p>This is funny to me since <a href="https://zenml.io/">I work at an MLOps company</a> – we build an open-source MLOps framework that helps you set up a platform – but I hadn’t anticipated it’d reach this point where I’d need something like a ZenML so soon. This is, of course, one of the major tradeoffs of finetuning LLMs, in that you have to manage all this <em>stuff</em> in order to make it work reliably and repeatably. Even at this early stage of my project, it’s clear that you need a way to keep everything straight without making mistakes.</p>
</section>
<section id="but-evals-give-me-a-way-to-know-if-im-making-progress">
<h2 data-anchor-id="but-evals-give-me-a-way-to-know-if-im-making-progress">…but evals give me a way to know if I’m making progress</h2>
<p>Even though the evaluations were somewhat painful to implement (at least in the form of this Jupyter notebook), they have given me an amazing gift in that I now have a task-specific way to know whether any of the improvements or refinements to either the training data or to the model are helping move me forward. Without this I’d essentially be flying blind.</p>
</section>
<section id="next-steps">
<h2 data-anchor-id="next-steps">Next Steps</h2>
<p>I had originally thought and suggested that I’d want to train multiple models to be super-specialists in their field, so for example to have one model that was really good at estimating how many people were captured in a particular event. Seeing the performance of my models, I’m not sure that’s the obvious next step for this project, or if I’d really be able to boost the accuracy by a significant amount by taking that approach.</p>
<p>This project is all about accuracy, so it’s possible that I might want to try that out, but for now I’m still exploring all the different phases of the LLM finetuning process so I’ll put the submodels idea on the backburner.</p>
<p>The first obvious next step is to run some evaluations for the non-accuracy-related tests mentioned <a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html">in my last blog</a>. For example, I’d like to see how it performs with out of domain data (i.e. completely made up data about something completely different).</p>
<p>The other next step is to get into some of the details around model serving. I’d like to take my top three performers and dive into how LLM model serving is done. I’m familiar with non-LLM model serving and some of the ways people do that through my work, but LLM serving has it’s own tricks, tradeoffs and tools and I’m eager to learn more about those.</p>
<p>If this was a problem that I was deeply invested in solving beyond these already excellent results, I’d probably also want to dive into the areas where my LLMs struggled. So I’d take all the places where my LLMs failed to get the answer correct, load them up into some kind of web interface like Lilac or Argilla and really inspect my data further. Understanding the failure scenarios will probably do more for the accuracy than any tweaking of the finetuning parameters or the like.</p>
<p>For now, I’m just happy the finetuned models beat GPT-4!</p>


</section>
</section>

</main> <!-- /main -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RegreSSHion: RCE in OpenSSH's server, on glibc-based Linux systems (616 pts)]]></title>
            <link>https://www.qualys.com/2024/07/01/cve-2024-6387/regresshion.txt</link>
            <guid>40843778</guid>
            <pubDate>Mon, 01 Jul 2024 08:40:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.qualys.com/2024/07/01/cve-2024-6387/regresshion.txt">https://www.qualys.com/2024/07/01/cve-2024-6387/regresshion.txt</a>, See on <a href="https://news.ycombinator.com/item?id=40843778">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Cities Need More Trees (275 pts)]]></title>
            <link>https://herman.bearblog.dev/cities-need-more-trees/</link>
            <guid>40843720</guid>
            <pubDate>Mon, 01 Jul 2024 08:31:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://herman.bearblog.dev/cities-need-more-trees/">https://herman.bearblog.dev/cities-need-more-trees/</a>, See on <a href="https://news.ycombinator.com/item?id=40843720">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>
<i>
<time datetime="2024-07-01T07:53Z">
01 Jul, 2024
</time>
</i>
</p>
<p>I grew up just outside of Johannesburg, which is touted to be "the greenest city in the world". During the summer, when seen from above, it's a verdant, green landscape, with the tops of buildings popping through the canopy.</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/herman-1719820418.jpg" alt="Joburg trees"></p>
<p><small>Yes, there's a full city beneath those trees.</small></p>
<p>However, this is not what the land originally looked like.</p>
<p>To cut a long story short, Johannesburg was established following the discovery of gold in 1886. It's known colloquially as eGoli, or "The City of Gold". Gold extraction here is vastly different to other parts of the world. We don't have gold veins running through the rock like in old Western movies. Instead it's mineralised into the rock itself which requires significant processing to extract.</p>
<p>This form of extraction leads to the crushing of millions of tons of rock, which is then dumped creating large, dusty hills around the city. These hills are (especially to the communities surrounding them) toxic and radioactive<sup id="fnref-1"><a href="#fn-1">1</a></sup>, and when the wind picks up yellow dust clouds used to envelop the city.</p>
<p>One of the proposed solutions to suppressing this omnipresent dust was the planting of trees (1.2 millions as of 2024) on sidewalks and pavement, and millions more planted privately in back-yards and plots. Due to Apartheid in South Africa, which economically and socially disadvantaged the majority of the population based on race, these planting efforts were not equitably distributed. You can still see a stark difference between rich and poor neighbourhoods to this day based purely on tree cover.</p>
<p>But I'm not here to talk about the socioeconomic problems facing South Africa. Instead I'd like to focus on how the planting of so many trees affected the city itself.</p>
<p>Since these trees were originally planted to manage dust, they are generally big and leafy. This has the benefit of creating a lot of shade throughout the city, mitigating a lot of the "heat island" effect which is pervasive in any city since asphalt and concrete are great at absorbing visible spectrum light and radiating it as heat<sup id="fnref-2"><a href="#fn-2">2</a></sup>. Walking down a shady street on a hot summer day is such a pleasant experience when compared to being out in the blistering sun.</p>
<p>Trees are not just great dust sinks and heat shields, but great sound barriers as well<sup id="fnref-3"><a href="#fn-3">3</a></sup>. Having tree lined streets not only reduces the noise of traffic (and the dust kicked up from their tyres), but also protects pedestrians and infrastructure on the sidewalks from stray vehicles.</p>
<p>On top of that it just looks better. I'm certain humans have genetic biophilia<sup id="fnref-4"><a href="#fn-4">4</a></sup>, which is why we love being in nature or taking walks in the forest. Having trees around us, teeming with birds and other life just <em>feels good</em>. Speaking of birds, trees increase the biodiversity of insects and other small critters in urban environments. It also gives birds a safe-haven from the deadliest hunter of all: the humble house cat<sup id="fnref-5"><a href="#fn-5">5</a></sup>.</p>
<p>I've been to many cities, all over the world. Some that do trees well, and others that fail miserably. And I'm always shocked at the contrast. Given two cities with a similar climate, I'm significantly more likely to do things like walk to a local coffee shop and browse corner stores, or sit on a park bench. But only if the walk is pleasant and shady.</p>
<p>Planting trees as a form of environmentalism or carbon capture is generally a hit-or-miss strategy. The vast majority of planted trees never reach maturity<sup id="fnref-6"><a href="#fn-6">6</a></sup>, and depending on where the trees are planted it could have a net heating instead of a net cooling effect due to the change in the environment's albedo<sup id="fnref-7"><a href="#fn-7">7</a></sup>. However, planting trees in cities is pretty much all upside with almost no downside (except that birds tend to shit on my car).</p>
<p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/herman-1719820405.jpg" alt="Dead car"></p>
<p>So the next time you're enjoying a walk down a lovely shady street, take a look up and appreciate the trees.</p>
<hr>
<h5 id="footnotes">Footnotes</h5><section>
<ol>
<li id="fn-1"><p><a href="https://www.theguardian.com/cities/2015/jul/06/radioactive-city-how-johannesburgs-townships-are-paying-for-its-mining-past" target="_blank">Radioactive city: how Johannesburg’s townships are paying for its mining past</a><a href="#fnref-1">↩</a></p></li>
<li id="fn-2"><p><a href="https://en.wikipedia.org/wiki/Urban_heat_island" target="_blank">Urban head islands</a><a href="#fnref-2">↩</a></p></li>
<li id="fn-3"><p><a href="https://www.treehugger.com/how-do-trees-reduce-noise-pollution-4863592" target="_blank">How do trees reduce noise pollution?</a><a href="#fnref-3">↩</a></p></li>
<li id="fn-4"><p>Biophilia hypothesis: The idea that humans possess an innate tendency to seek connections with nature and other forms of life.<a href="#fnref-4">↩</a></p></li>
<li id="fn-5"><p><a href="https://www.nature.com/articles/ncomms2380" target="_blank">The impact of free-ranging domestic cats on wildlife</a><a href="#fnref-5">↩</a></p></li>
<li id="fn-6"><p><a href="https://e360.yale.edu/features/phantom-forests-tree-planting-climate-change" target="_blank">Phantom Forests: Why Ambitious Tree Planting Projects Are Failing</a><a href="#fnref-6">↩</a></p></li>
<li id="fn-7"><p><a href="https://www.earth.com/news/could-tree-planting-warm-earth-science-behind-albedo-effect/" target="_blank">Could tree planting warm Earth? Science behind the albedo effect</a><a href="#fnref-7">↩</a></p></li>
</ol>
</section>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Programmers Should Never Trust Anyone, Not Even Themselves (181 pts)]]></title>
            <link>https://carbon-steel.github.io/jekyll/update/2024/06/19/abstractions.html</link>
            <guid>40842867</guid>
            <pubDate>Mon, 01 Jul 2024 05:39:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://carbon-steel.github.io/jekyll/update/2024/06/19/abstractions.html">https://carbon-steel.github.io/jekyll/update/2024/06/19/abstractions.html</a>, See on <a href="https://news.ycombinator.com/item?id=40842867">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    

    <p>
        <time datetime="2024-06-19 17:00:00 +0000">19 Jun 2024</time>
        <br>
        By Sung Kim
    </p>

    <p>Programmers should be paranoid.</p>

<ul>
  <li>“I double checked the code”</li>
  <li>“The code passes the tests”</li>
  <li>“The reviewer approved my code”</li>
</ul>

<p>“So is my code correct?”</p>

<p>Writing code correctly is hard and verifying code correctness is impossible.
Here are some reasons why:</p>
<ul>
  <li><strong>Generality</strong>: Even if your code behaves correctly once, will it do so for
all cases, for all machines, for all times?</li>
  <li><strong>False Pass</strong>: Failing tests indicate the presence of bugs, but passing
tests do not promise their absence.</li>
  <li><strong>Lack of certainty</strong>: You could write a formal proof for your code’s
correctness but now you must wonder if the proof is correct. You would need to
prove the proof. This chain of verifying the verification would
never end.</li>
</ul>

<p>It is folly to pursue certainty of your code’s correctness. A bug may be hiding
in a dependency that you’ll never find. Yet we should not despair. We can still
decrease the risk of bugs via greater understanding and due diligence.</p>

<h2 id="abstractions">Abstractions</h2>

<p>What is “greater understanding”? Let’s focus on one facet of understanding which
comes up frequently among programmers: <em>abstractions</em>.</p>

<p>Abstractions are…</p>
<ul>
  <li>mental models of how stuff works</li>
  <li>when we treat <code>thing A</code> <strong>as if</strong> it were <code>thing B</code></li>
  <li>metaphorically…
    <ul>
      <li>the result of the data compression that occurs in your brain</li>
      <li>seeing the forest for the trees</li>
    </ul>
  </li>
  <li>used all the time in day-to-day life</li>
</ul>

<blockquote>
  <p>The word “abstraction” has many meanings. In programming, it can also refer
to layers of code that hide complexity. This post will only be talking about
abstractions in the cognitive sense.</p>
</blockquote>

<p>Examples of abstraction:</p>
<ul>
  <li>We treat our bank deposits <strong>as if</strong> the bank simply stores that money for us.
    <ul>
      <li>In reality, the bank does not just store the money we deposit. It loans
away/invests most of the money that people deposit. Our money does <em>not</em>
sit idle in a large pile in a vault.</li>
      <li>The abstraction works because banks still keep enough cash on hand to
handle most withdrawals.</li>
    </ul>
  </li>
  <li>We treat time <strong>as if</strong> it passes at the same rate for everyone.
    <ul>
      <li>Time dilation slightly changes each person/object’s flow of time based on
their speed and how much gravity they’re under.</li>
      <li>GPS satellites orbiting the Earth have to adjust their clocks by ~38
microseconds per day to account for time dilation
(<a href="https://pilotswhoaskwhy.com/2021/03/14/gnss-vs-time-dilation-what-the/">Source</a>).</li>
      <li>The abstraction works because the effect of time dilation is too miniscule
to notice unless you are doing extremely precise engineering.</li>
    </ul>
  </li>
</ul>

<hr>

<p>One way to form abstractions is by removing details (creating a simplified view
of something complex). For example, most people that drive do not know much about
the inner workings of their car. Their perspective of the car can be boiled down
to:</p>
<ul>
  <li>Ignition turns on car</li>
  <li>Accelerator makes car go</li>
  <li>Brake makes car stop</li>
  <li>Wheel turns car</li>
  <li>Car needs gas/diesel</li>
</ul>

<p>Knowing the above abstraction makes it unnecessary to understand the inner
workings of car engines. Most drivers have only this working knowledge of cars
and they can drive where they need to go.</p>

<p>When we use a programming language, it provides abstractions that allow us to
operate computers without understanding their inner workings.</p>
<ul>
  <li><strong>Basic language features</strong> (such as loops, if-conditionals, functions,
statements, and expressions) are all abstractions which hide:
    <ul>
      <li>Hardware-level details: CPU instructions, registers, flags, and details
specific to CPU architecture, …</li>
      <li>OS-level details: call stack management, memory management, …</li>
    </ul>
  </li>
  <li><strong>Portability</strong>: Languages abstract away the need to concern ourselves with the
differences between different machines.
    <ul>
      <li>Any compiled Java program (eg, a jar file) <em>should be</em> able to run on any
machine that has a Java Runtime Environment (ie, JVM) on it.</li>
      <li>A Python script <em>should be</em> able to run on any machine with a Python
interpreter.</li>
      <li>A C program <em>should be</em> able to compile and run on any machine if the machine
has a C compiler.</li>
    </ul>
  </li>
</ul>

<h2 id="abstractions-fail">Abstractions fail</h2>

<p>Unfortunately, abstractions fail.</p>

<ul>
  <li>Language abstractions are not enough if you care about code performance. To
speed up your code, you need to know hardware-level and OS-level details.</li>
  <li>Porting programs that have external dependencies such as dynamic libraries or
network requirements is not as simple. They cannot simply be moved to
another machine and run. Extra setup and knowledge are required.</li>
  <li>Car owners who only know the bare minimum can end up stuck in a broken down
car. If the driver doesn’t regularly change their car’s lubricant/oil, they’ll
shorten the engine’s lifespan.</li>
</ul>

<p>The driver’s abstraction works well in the short term (for a single drive), but
fails in the long term (many years). Joel Spolsky describes such failing
abstractions as “leaky” and put forth the <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/">Law of Leaky Abstractions</a>:</p>

<blockquote>
  <p>All non-trivial abstractions, to some degree, are leaky.</p>
</blockquote>

<p>Which is similar to the maxim from statistics:</p>

<blockquote>
  <p>All models are wrong, but some are useful.</p>
</blockquote>

<p>When we write code, we use leaky abstractions all the time. Here are some random
examples:</p>
<ul>
  <li>Garbage collection takes away the burden of worrying about memory management
(unless we care about latency jitters)</li>
  <li>C++ smart pointers make memory safe (as long as you don’t store any raw
pointers from it)</li>
  <li>Hashtables are fast because they have O(1) operations (but arrays are faster
for smaller sizes).</li>
  <li>Passing by reference is faster than passing by value (except for cases of copy
elision and values which fit in CPU registers like ints)</li>
</ul>

<p>Fortunately, many leaky abstractions crash your code when they fail, so they’re
easy to address. However, some may just produce undefined behavior or
performance degradation which are harder to identify and fix.</p>

<h2 id="press-x-to-doubt">Press X to doubt</h2>

<p>So if abstractions can be problematic, then should we try to understand a topic
without abstractions (to know cars as they <em>really</em> are)? No. When you dig
beneath abstractions, you just find more abstractions. It’s turtles all the way
down.</p>

<ul>
  <li>Underpinning our abstraction of cars, there is an understanding of each
component’s purpose.</li>
  <li>Beneath that, the chemistry of combustion and the mechanical engineering of
the engine</li>
  <li>Beneath that, the mathematics/physics that model the forces of our universe</li>
</ul>

<p>These layers of abstractions go down until we hit our most basic axioms about
logic and reality.</p>

<p>As programmers, we should see our knowledge as a house of cards made of leaky
abstractions and assumptions. We should have a healthy amount of skepticism of
everything and everyone, including ourselves.</p>

<h2 id="trust-but-verify">Trust, but verify</h2>

<p>A programmer should have a “trust, but verify” policy.</p>

<p>Here are some examples:</p>
<ul>
  <li>Trust the information that people tell you, but verify it with what the
documentation says</li>
  <li>Test your beliefs by trying to disprove them.
    <ul>
      <li>You wrote tests for your code change and they pass on the first try. Try
running the tests without your changes and see if they still pass. They
may have a bug that makes them always pass.</li>
      <li>You’ve refactored the code which should be a no-op. All the tests still
pass. Check to make sure there actually are any tests that run the code
you refactored.</li>
      <li>You optimized your service and you see the expected reduction in resource
utilization. Check to make sure your service is not just handling fewer
requests at the moment.</li>
      <li>You’ve submitted a code change and you see no problems in the service the
next day. Check to make sure a rollout occurred that day and your code was
included in it.</li>
    </ul>
  </li>
  <li>Always measure impact when optimizing code. Code changes that appear to be
“theoretically” faster can end up being slower due to factors revealed in
lower layers of abstraction.</li>
</ul>

<h2 id="beware-of-unknown-unknowns">Beware of unknown unknowns</h2>

<p>The scariest epistemological issue for programmers is the “unknown unknown”.</p>

<p>There are…</p>
<ul>
  <li>things you know (ie, “knowns”)</li>
  <li>things you know you don’t know (“known unknowns”)</li>
  <li>things you don’t even know that you don’t know (“unknown unknowns”)</li>
</ul>

<p>These unknown unknowns are the root of abstraction failures (and the reason why
programmers can never accurately predict how long a project will take).</p>

<p>You may have never heard of…</p>
<ul>
  <li>Sanitizing user inputs
    <ul>
      <li>If you use user-provided strings as part of a SQL query, your service can
be hacked via SQL injections.</li>
    </ul>
  </li>
  <li>Character encodings
    <ul>
      <li>Any text data your code processes must be using the character encoding
(eg, ASCII, UTF-8, UTF-32, etc) your code expects/supports.</li>
      <li>Random access of a character in a text buffer could take constant
time (for ASCII) or linear time (for UTF-8) depending on the character
encoding.</li>
      <li>You may output incomprehensible characters if you try to read text data
using the wrong character encoding.</li>
    </ul>
  </li>
  <li>Java heap size
    <ul>
      <li>Your program may be slowed down due to a lack of heap memory.</li>
      <li>You could fix this issue if you knew to configure a larger max heap size
for your Java program.</li>
    </ul>
  </li>
</ul>

<p>If you have not heard of these topics before, you may not even know that you
fell into their pitfalls.</p>

<p>There is no sure fire way to catch unknown unknowns when they are around but we
should check under at least one layer of abstraction to look for them.
Especially when a project requires learning something new, you should always
learn more than you need to. Doing so will mitigate the risk of being
surprised by abstraction failures.</p>

<p>When learning/working with an unfamiliar platform/language/tool/library/technology:</p>
<ul>
  <li>Read more documentation than just the bare minimum you need</li>
  <li>Watch videos
    <ul>
      <li>Conference presentations have the highest quality in my experience</li>
    </ul>
  </li>
  <li>Read blog posts</li>
  <li>Read the source code</li>
  <li>Grow your understanding of abstractions you have to work with
    <ul>
      <li>Learn about features recently added in to your programming language</li>
      <li>Read through all of the public functions of libraries, not just the ones
you’re using</li>
      <li>Skim through all of the flags of a CLI tool’s man page</li>
    </ul>
  </li>
  <li>Learn at least one layer of abstraction lower than you need
    <ul>
      <li>Learn about your compiler’s optimizations</li>
      <li>If you’re running a service, learn about your orchestration platform (ex:
kubernetes)</li>
      <li>If you’re working in Java, learn about the JVM</li>
      <li>If you’re working in Python, learn about the Python interpreter.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Abstractions are necessary as they allow us to think efficiently but they are
treacherous as they can make it appear that we know “enough”. Programmers who
learn superficially will not succeed on difficult projects that come without
known solutions and involve multiple domains of expertise.</p>

<p>That said, the ideal put forth by this blog post needs to be balanced against
reality. Obviously, we can’t take time to learn every little thing when we’re
in a rush. Also, beginners can’t be expected to be so thorough. Ideals should be
balanced against real-world considerations.</p>

<p>And real-world considerations should be balanced against ideals. We should be
willing to pay some short-term costs for thorough learning and verifications.
Not just for writing correct code, but as part of our long-term journey of
becoming capable and principled engineers.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's Stop Asking "Why Do You Want to Work for Us?" In Interviews (121 pts)]]></title>
            <link>https://nelson.cloud/lets-stop-asking-why-do-you-want-to-work-for-us-in-interviews/</link>
            <guid>40842603</guid>
            <pubDate>Mon, 01 Jul 2024 04:49:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nelson.cloud/lets-stop-asking-why-do-you-want-to-work-for-us-in-interviews/">https://nelson.cloud/lets-stop-asking-why-do-you-want-to-work-for-us-in-interviews/</a>, See on <a href="https://news.ycombinator.com/item?id=40842603">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p><em>“Why do you want to work for us?”</em></p><p>I have run into this question many times as I’ve been applying to and interviewing with lots of companies.</p><p>99% of the reason people want to work for a company is <strong>because they need money</strong>. That’s it. They want to be able to pay off debt. They want to be able to pay their bills. They want to be able to pay rent or their mortgage. They want to be able provide for their family. They want to be able to afford a better life. And so on.</p><p>Sure, the tech stack might be exciting. Or the product may be compelling. The work-life balance may be good. But I promise you that the biggest reason is still money.</p><p>If you don’t believe me, find your most passionate engineers, cut their compensation in half, and see if they stick around. I guarantee you they won’t.</p><p>Let’s stop beating around the bush and be real with each other. Do you really want to to be lied to by every candidate?</p><p>It’s okay to be motivated by money. That’s the world we live in. An engineer can do amazing work for your company even if they are only motivated by money. Also, companies are only motivated by making money. What’s wrong with the average person being motivated by the same thing?</p><p>You may have come across my application to your company at some point. I may have said something about wanting to work with bright people and looking for growth opportunities. While that is still true, you should know that 99% of the reason I applied to your company is to earn money so I can buy food, pay rent, and provide for my family.</p><p>Let’s stop asking this question in the application and interview process. We all know the real answer.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Four lines of code it was four lines of code (157 pts)]]></title>
            <link>https://boston.conman.org/2024/06/30.1</link>
            <guid>40842275</guid>
            <pubDate>Mon, 01 Jul 2024 03:26:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boston.conman.org/2024/06/30.1">https://boston.conman.org/2024/06/30.1</a>, See on <a href="https://news.ycombinator.com/item?id=40842275">Hacker News</a></p>
Couldn't get https://boston.conman.org/2024/06/30.1: Error: Request failed with status code 500]]></description>
        </item>
        <item>
            <title><![CDATA[Pipes: A spiritual successor to Yahoo Pipes (417 pts)]]></title>
            <link>https://www.pipes.digital/docs</link>
            <guid>40841980</guid>
            <pubDate>Mon, 01 Jul 2024 02:03:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pipes.digital/docs">https://www.pipes.digital/docs</a>, See on <a href="https://news.ycombinator.com/item?id=40841980">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        
        <section id="what"><h2>What Pipes is</h2>

        <p>Pipes is a spiritual successor to <a href="https://en.wikipedia.org/wiki/Yahoo!_Pipes">Yahoo! Pipes</a>, but if you did not know that site, you can think of Pipes as a visual programing editor specialized on feeds, or a visual shell, or simply as a glorified feed configurator.</p>

        <p>Pipes gives you blocks that can fetch and create feeds, and manipulate them in various ways. Think filtering, extracting, merging and sorting. All you need to do is to connect those blocks with each other. Data just flows through such a pipe, it flows from block to block. At the end Pipes gives you a new feed, which you can give to other programs that support open web standards - such a program could be your feed reader.</p>

        <p>As input formats Pipes supports RSS, Atom and JSON feeds, it can scrape HTML documents, and it can work with regular text files.</p>
        </section>

        <section id="how">
        <h2>How to use Pipes</h2>

        <p>At the left side of the editor is your list of blocks. You can drag and drop those into the canvas to your right, and the regular block with its inputs and outputs will appear. That block can again be dragged around, but more importantly, if you have multiple blocks you can connect their inputs and outputs. Those are the circles at the side of a block - left the inputs, right the outputs. To connect them, click first on the output of one block, and then on the input of another block. Or drag the input onto the output. You should also fill the user inputs, the form elements in the center of some blocks that control their behaviour. Don't forget to connect your last block to the pipe output, the red circle at the far right.</p>

        <video src="https://www.pipes.digital/intro.mp4" controls="" width="800px"></video>

        <p>See the video above for a short demonstration of how simple it can be to filter an RSS feed.</p>
        
        </section>

        <section id="sharing">
        <h2>Sharing Pipes</h2>

        <p>You can set pipes to be pulicly visible in the <a href="https://www.pipes.digital/mypipes">My Pipes</a> menu, where you can also describe them and set tags to make the pipe searchable. Public pipes will be listed in the <a href="https://www.pipes.digital/pipes">public pipes</a> page, where they can be liked and forked by others.</p>

        <p>Note that pipe output feeds are publicly accessible by everyone who knows their url, which contains a ranzomized id and is thus not easy to guess. If you just want someone else to use the feed, you can just give him the pipe feed url.</p>
        </section>

        <section id="output">
        <h2>Pipe Output</h2>

        <p>The default output format of a pipe is RSS. RSS is an open format which can be used in a lot of other programs, like feed readers. Each pipe has a pipe output url like <code>https://pipes.digital/feed/pipeid</code> which is shown in the editor of a saved pipe, or in the list of private pipes under <a href="https://www.pipes.digital/mypipes">My Pipes</a>. If you want to get only the feed items' content you can open <code>https://pipes.digital/feed/pipeid.txt</code>, which will strip all xml feed elements. This is especially useful when your starting point was a text document that got <a href="#builder">transformed into a feed</a> to work with its data in Pipes.</p>
        
        </section>

        <section id="support">
        <h2>Support</h2>

        <p>Report bugs and feature requests via <a href="https://github.com/pipes-digital/pipes/issues">the Github issue tracker</a>, or send an email to <a href="mailto:support@pipes.digital">support@pipes.digital</a>.</p>
        
        </section>

        <section id="license">
        <h2>License and selfhosting</h2>

        <p>Pipes started as a non-free project. Now there is Pipes CE, a FOSS version of Pipes under AGPL, that you can run locally. Its home is the same <a href="https://github.com/pipes-digital/pipes">Github Repository</a> used as issue tracker for this site. You could think of this as an open core model.
        </p>
        
        </section>

        <section>
            <h2>Blocks</h2>
            <section id="feed">
                <h3>Feed</h3>
                <p>Download a RSS, Atom or JSON feed. If not pointed directly to the url of such a feed, it will try to find a <code>&lt;link rel="alternate"&gt;</code> element in the head of the linked page, and then download that feed. The feed block is usually the starting point of a pipe. Note that Pipes caches all downloads for a few minutes.</p>

                <img src="https://www.pipes.digital/imgs/feed_block_tiny.png">

                <p>It has no block input, but it takes as user input the url to download.</p>

                <a href="https://www.pipes.digital/imgs/feed_tiny.png" data-mediabox="feed"><img width="800px" src="https://www.pipes.digital/imgs/feed_tiny.png"></a>

                <p>In this example usage, the pipe consists of only one single feed block. That's not very useful yet. But note that the feed block input points to a normal HTML page, not an RSS feed. Because Hacker News specifies its RSS feed as a <code>rel="alternate"</code>, this pipe will output that feed instead.</p>
            </section>
            <section id="filter">
                <h3>Filter</h3>
                <p>Filter the input feed for a keyword, and either show only items that contain it or filter them all out. It searches for the word in the items title, description (if existing) and content. The search is case sensitive. If given a <a href="http://rubular.com/">regular expression</a>, it will interpret that regular expression instead, enabling more powerful filtering. Write that expression as <code>/expression/</code>, but regular keywords without the outer slashes.</p>

                <img src="https://www.pipes.digital/imgs/filter_block_tiny.png">

                <p>It takes the feed it searches through as block input, and the keyword to search for as user input. The second input, the checkbox, toggles whether found items are to be included or to be blocked.</p>

                <a href="https://www.pipes.digital/imgs/filter_tiny.png" data-mediabox="filter"><img width="800px" src="https://www.pipes.digital/imgs/filter_tiny.png"></a>

                <p>In this example usage, the filter block will filter the downloaded feed for all items that contain the keywords "Bitcoin" or "bitcoin".</p>
            </section>
            <section id="filterlang">
                <h3>Filter Language</h3>
                <p>This block checks all feed items for their language they are written in. The default logic is to keep only the items in the selected language. It checks multiple fields of an item for that, but you can limit the check to some specific fields, like only the title or only the description. And with the last checkbox you can turn the logic around and remove all items that are in the selected language.</p>

                <p>Use this block if you have a multi-language feed and you want to only keep items in your target language, or remove items in the one language that does not work for you. Be aware that the used n-gram detection will not be 100% accurate, and it even will be very inaccurate for very short texts (less than 10 words is bad, 5 should be the absolute minimum).</p>

            </section>
            <section id="replace">
                <h3>Replace</h3>
                <p>While the filter block is like the search tool grep, the replace block is like sed or the replace function of your texteditor. The first input is the search pattern, the second is its replacement. If given a <a href="http://rubular.com/">regular expression</a>, it will interpret that regular expression instead, enabling more powerful filtering. Write that expression as <code>/[aA]bc/</code>, as a regualr expression with surrounding slashes. The replacement pattern supports backreferences (<code>\1</code>).</p>

                <img src="https://www.pipes.digital/imgs/replace_block_tiny.png">

                <p>It takes the feed it searches through as block input, and the regex-enabled keyword to search for as first user input. The second input is its replacement pattern.</p>

                <a href="https://www.pipes.digital/imgs/replace_tiny.png" data-mediabox="replace"><img width="800px" src="https://www.pipes.digital/imgs/replace_tiny.png"></a>

                <p>In this example usage, the filter block will filter the downloaded feed for all items that contain the pattern "soccer" or "Soccer" and replace them with "football".</p>
            </section>
            <section id="combine">
                <h3>Combine</h3>
                <p>Combine two or more input feeds into one single feed. Combining those feeds will merge their items lists, and will also append their feed title and descriptions. The sort order follows the input order: The output feed has first all items from the first input feed, then all items from the second input feed, and so on.</p>

                <img src="https://www.pipes.digital/imgs/combine_block_tiny.png">

                <p>It takes the feeds to merge as block inputs, and has no user input.</p>

                <a href="https://www.pipes.digital/imgs/combine_tiny.png" data-mediabox="combine"><img width="800px" src="https://www.pipes.digital/imgs/combine_tiny.png"></a>

                <p>In this example usage, the pipe downloads multiple feeds and then combines them together.</p>
            </section>
            <section id="duplicate">
                <h3>Duplicate</h3>
                <p>Duplicate the input feed into multiple output feeds. You can also think of this as a splitter. This block does not manipulate the input feed in any way.</p>

                <img src="https://www.pipes.digital/imgs/duplicate_block_tiny.png">

                <p>It takes one feed as input, and has no user input. This is the one block that has multiple outputs.</p>

                <a href="https://www.pipes.digital/imgs/duplicate_tiny.png" data-mediabox="duplicate"><img width="800px" src="https://www.pipes.digital/imgs/duplicate_tiny.png"></a>

                <p>In this example usage, the duplicate block duplicates a feed that was already filtered for "Bitcoin", to filter it as well for "Linux" and "MacOS". Those two feeds then are combined together.</p>
            </section>
            <section id="merge">
                <h3>Merge Items</h3>
                <p>Use this block to merge items together. It will take step by step one item from each input feed and merge them together into one single item. You can optionally specify a replacement format. If it contains <code>\1</code> and <code>\2</code> it will replace those with the content of the two items. If it is just a regular string, then that string will be between the two item contents. If it is empty they will just be concatenated.</p>

                <img src="https://www.pipes.digital/imgs/merge_block_tiny.png">

                <p>The block takes two feeds as input. The user input serves as format, as replacement pattern, but is optional.</p>

                <a href="https://www.pipes.digital/imgs/merge_tiny.png" data-mediabox="merge"><img width="800px" src="https://www.pipes.digital/imgs/merge_tiny.png"></a>

                <p>In this example usage the title and the link of the hacker news feed are placed after one another, a dash is between them and an <em>end</em> added.</p>
            </section>
            <section id="unique">
                <h3>Unique</h3>
                <p>Remove duplicate items from a feed. Duplicate items could be an accident in the original feed, but more likely is that they occur after merging filtered feeds with items that containing multiple search words. This block uses the items guid to determine that two items are equal.</p>

                <img src="https://www.pipes.digital/imgs/unique_block_tiny.png">

                <p>It takes one feed as input, and has no user input.</p>

                <a href="https://www.pipes.digital/imgs/unique_tiny.png" data-mediabox="unique"><img width="800px" src="https://www.pipes.digital/imgs/unique_tiny.png"></a>

                <p>In this example usage, the pipe downloads a feed, duplicates it, filters for "Bitcoin" and "Linux" and then merges the results. The unique block now makes sure that items that contain both "Bitcoin" and "Linux" occur only one time in the output feed.</p>
            </section>
            <section id="truncate">
                <h3>Truncate</h3>
                <p>Truncate an input feed to a given length, with the first x items remaining. This is unlikely to be very helpful in a pipe, apart from when an input feed is really long and you want to improve performance. But imagine you have a website and want to show the last few items of a specific feed, then you could make that easier by producing a feed that has exactly as many entries you want to display.</p>

                <img src="https://www.pipes.digital/imgs/truncate_block_tiny.png">

                <p>It takes one feed as input, and the target amount of items as user input.</p>

                <a href="https://www.pipes.digital/imgs/truncate_tiny.png" data-mediabox="truncate"><img width="800px" src="https://www.pipes.digital/imgs/truncate_tiny.png"></a>

                <p>In this example usage, the feed of the New York Times homepage gets truncated to three items.</p>
            </section>
            <section id="shorten">
                <h3>Shorten</h3>
                <p>Shorten the text inside the selected element. The number defines how many characters are the target text length.</p>
            </section>
            <section id="sort">
                <h3>Sort</h3>
                <p>Sort the items of an input feed. You can specify the sort order, and which item element to use for sorting. Default is to sort by the items date (its <code>updated</code> element).</p>

                <img src="https://www.pipes.digital/imgs/sort_block_tiny.png">

                <p>It takes one feed as input, and two user inputs to set the sort order and the sort item.</p>

                <a href="https://www.pipes.digital/imgs/sort_tiny.png" data-mediabox="sort"><img width="800px" src="https://www.pipes.digital/imgs/sort_tiny.png"></a>

                <p>In this example usage, the Hacker News feed is re-sorted by date, to have the newest items at the top of the list.</p>
            </section>
            <section id="download">
                <h3>Download</h3>
                <p>Download a page. This is different from the <a href="#feed">feed block</a> as it will not try to find a feed. This block is really meant for downloading some data, like a html page, that is then transformed by other blocks to create a regular feed. As such the this block then needs to be connected to the extract block or the builder block, to create a regular feed. It is an alternative starting point for a pipe. Note that Pipes caches all downloads for a few minutes.</p>

                <img src="https://www.pipes.digital/imgs/download2_block_tiny.png">

                <p>It has no block input, but it takes as user input the url to download. If on a paid plan, it also allows setting a checkbox to interpret javascript on the downloaded page.</p>

                <a href="https://www.pipes.digital/imgs/download2_tiny.png" data-mediabox="download"><img width="800px" src="https://www.pipes.digital/imgs/download2_tiny.png"></a>

                <p>In this example usage, the Hacker News page is downloaded and set as feed output. That's not really useful yet. But note that it really is the Hacker News website that the pipe downloads, not its feed.</p>
            </section>
            
            <section id="images">
                <h3>Images</h3>
                <p>Extract images from a feed or a <a href="#download">downloaded</a> HTML page. The block will select all images currently embeded into the page or feed items. It also gives access to a gallery that opens in an overlay and shows all the images selected. Just for displaying those images and not as block output, the gallery will try to create absolute urls for those images that on the source page were linked with relative urls.</p>

                <img src="https://www.pipes.digital/imgs/images_block_tiny.png">

                <p>It takes one feed or downloaded HTML page as input. The output is a feed with each item containing one image.</p>

                <a href="https://www.pipes.digital/imgs/images_tiny.png" data-mediabox="images"><img width="800px" src="https://www.pipes.digital/imgs/images_tiny.png"></a>

                <p>In this example usage, the images of the articles on the pipes blog are extracted and set as feed output.</p>
            </section>
            <section id="tabletosjon">
                <h3>Tables</h3>
                <p>Extract HTML tables from a feed or a <a href="#download">downloaded</a> HTML page. The block will select all HTML tables currently embeded into the page or feed items and convert them to JSON. It then outputs a feed with only the JSON tables as item content.</p>

                <img src="https://www.pipes.digital/imgs/tables_block_tiny.png">

                <p>It takes one feed or downloaded HTML page as input. The output is a feed with each item containing a JSON object.</p>

                <a href="https://www.pipes.digital/imgs/tables_tiny.png" data-mediabox="images"><img width="800px" src="https://www.pipes.digital/imgs/tables_tiny.png"></a>

                <p>In this example usage, the tables of a crawled page are extracted and set as feed output.</p>
            </section>
            <section id="insert">
                <h3>Insert</h3>
                <p>Insert content taken from one feed and insert it at the specified position of a different feed. This is meant to be combined with the extract block. Take something from one feed with the extract block, connect it to this block, and then connect the feed where you want to add the extracted element.</p>

                <img src="https://www.pipes.digital/imgs/insert_block_tiny.png">

                <p>It takes two feeds as inputs. From the first it will take the first item, the second input is the feed that will be modified. The XPath expression specifying where to insert the element is the user input.  The third input, the checkbox, toggles whether the new content should be appended to the already existing content at the target item or whether to replace it.</p>

                <a href="https://www.pipes.digital/imgs/insert_tiny.png" data-mediabox="insert"><img width="800px" src="https://www.pipes.digital/imgs/insert_tiny.png"></a>

                <p>In this example usage, the <code>/rss/channel/image</code> node of a soundcloud feed is taken and inserted into the Hacker News feed.</p>
            </section>
            <section id="builder">
                <h3>Build Feed</h3>
                <p>Build a new feed. The various input feeds are set as its items attributes. That way one can use <a href="#extract">extracted elements</a> to create a new regular feed, whose items have a proper title, content, date and link. You can also connect a downloaded text file to this, that will then get transformed line by line into feed items. The order is important here: this block will go through all its input feeds simultaneously, and at each step get the content of the current item of the content feed, as well as the content of the current item of each other connected feed, to create a complete feed item. This block can be used in various ways, but its most obvious usage is to create a feed for a website that has no feed of its own.</p>

                <img src="https://www.pipes.digital/imgs/build_feed_block_tiny.png">

                <p>It takes four feeds as input, and the markings in the block show which attribute they will fill. The second input for example will become the new feeds content, and is the only required input.</p>

                <a href="https://www.pipes.digital/imgs/build_feed_tiny.png" data-mediabox="build_feed"><img width="800px" src="https://www.pipes.digital/imgs/build_feed_tiny.png"></a>

                <p>In this example usage, the pipe downloads the Hacker News homepage, duplicates it and extracts three elements: The title, the score and the link target. Those are then given to the feed builder to set the new items title, their content and their link target.</p>
            </section>
            <section id="pipeblock">
                <h3>Pipes as Blocks</h3>
                <p>Use a pipe as input block. Since pipes output RSS, their feed can effortlessly be used in other Pipes. This allows the creation of more complicated pipes by combining multiple simpler ones, like submodules.</p>

                <img src="https://www.pipes.digital/imgs/pipeblock_block_tiny.png">

                <p>It has one output, which is the result feed of the corresponding pipe. </p>

                <a href="https://www.pipes.digital/imgs/pipeblock_tiny.png" data-mediabox="pipeblock"><img width="800px" src="https://www.pipes.digital/imgs/pipeblock_tiny.png"></a>

                <p>In this example usage, the pipe combines two filtered feeds that are created by other pipes.</p>
            </section>
            <section id="webhookblock">
                <h3>Webhook</h3>
                <p>Use data sent by a webhook as pipe input. When you create this block, it will show a URL, which is your new webhook endpoint. You can instruct other sites - like Github - to POST data to that endpoint. The webhook block will create a feed out of the received data, which can then be manipulated as usual. Note: Stored data is erased after an hour, and the API endpoint is rate limited (currently to 5 requests per minute).</p>

                <img src="https://www.pipes.digital/imgs/webhook_block_tiny.png">

                <p>It has one output, which is the data stored under its webhook endpoint. </p>

                <a href="https://www.pipes.digital/imgs/webhook_tiny.png" data-mediabox="webhook"><img width="800px" src="https://www.pipes.digital/imgs/webhook_tiny.png"></a>

                <p>In this example usage, the pipe combines two webhook feeds and limits the length to the last 10 items.</p>
            </section>
            <section id="textinputblock">
                <h3>Textinput</h3>
                <p>Use this block to let other users fill the input of connected blocks. This can make a pipe dynamic. For example, you could have a generic pipe that works with many sites, and this way other users could point the pipe to the site they want to use it on.</p>

                <img src="https://www.pipes.digital/imgs/textinput_block_tiny.png">

                <p>The first user input is the name of the input. It will be shown to the user in the UI and is also the name of the GET parameter that controls this block when calling the pipe feed. The second user input is the default value that will be used when the parameter is not set. It has one output, which is the text that will be used by the connected input. </p>

                <a href="https://www.pipes.digital/imgs/textinput_tiny.png" data-mediabox="textinput"><img width="800px" src="https://www.pipes.digital/imgs/textinput_tiny.png"></a>

                <p>In this example usage, the pipe filters a user-choosable feed for a search term the user selects as well. The default feed is the one from the New York Times, and the default search term is <code>Trump</code>. That pipe can be found <a href="https://www.pipes.digital/pipe/14OE65qg">here</a>.</p>
            </section>
            <section id="foreach">
                <h3>ForEach</h3>
                <p>Use this block to repeat the action of a download or feed block for every item of the input feed. Those blocks normally work with the user provided text input, to download a specific url for example. With the ForEach block, instead of acting on a single manually provided URL you could download all the URLs you extract from somewhere else. This can for example be useful to create full text RSS feeds. To control the action of the block, drop a download or feed block on it.</p>

                <img src="https://www.pipes.digital/imgs/foreach_block_tiny.png">

                <p>The block takes the items it is supposed to act upon as input, usually provided by an extraxt block. It outputs the downloaded URLs (as items in an RSS feed) or fetched feeds (with all the items appended), depending on its configuration.</p>

                <a href="https://www.pipes.digital/imgs/foreach_tiny.png" data-mediabox="foreach"><img width="800px" src="https://www.pipes.digital/imgs/foreach_tiny.png"></a>

                <p>In this example usage the block is configured as download block. The output will be the first two pages linked in the RSS feed of onli-blogging.de.</p>
            </section>
            <section id="twitterblock">
                <h3>Tweets (legacy)</h3>
                <p>As there is no trustworthy model for further access to the twitter API, the twitter block is now defunct.</p>
            <section id="integrations">
                <h3>Integrations</h3>
                <p>Pipes has blocks to fetch data comfortably from some other sites. Those service specific blocks usually take a user or channel name or an url. They then return a normalized RSS feed with updates specific to the target site. Currently supported sites are:
                </p>
                <ol>
                    <li>Twitter, see <a href="#twitterblock">here</a></li>
                    <li>Vimeo</li>
                    <li>Dailymotion</li>
                    <li>Periscope</li>
                    <li>UStream</li>
                    <li>Mixcloud</li>
                    <li>SVT Play</li>
                    <li>Speedrun.com</li>
                    <li>Youtube, integrated into the <a href="#feed">Feed block</a></li>
                </ol>
                <p>Pipes does not have a special agreement with those sites. It just accesses (hidden) existing RSS feeds or uses APIs to create them. For some of those blocks Pipes calls <a href="https://github.com/stefansundin/rssbox">RSS Box</a> to do the feed discovery and creation work.</p>
            </section>
        </section>
    </section>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Model of a Mind (137 pts)]]></title>
            <link>https://tylerneylon.com/a/mind_model/mind_model.html</link>
            <guid>40841502</guid>
            <pubDate>Mon, 01 Jul 2024 00:15:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tylerneylon.com/a/mind_model/mind_model.html">https://tylerneylon.com/a/mind_model/mind_model.html</a>, See on <a href="https://news.ycombinator.com/item?id=40841502">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>A Model of a Mind</p>
      
      <p>
      
      \(\newcommand{\customstrut}{}\)
      \(\newcommand{\crossedouty}{\dot y \kern -5pt \scriptscriptstyle{\diagup\atop\vphantom{y}}}\)
      \(\newcommand{\crossedoutone}{\dot 1 \kern -5pt {\scriptscriptstyle{\diagup\atop\vphantom{\textstyle 1}}}}\)
      \(\newcommand{\crossedouttwo}{\dot 2 \kern -5pt {\scriptscriptstyle{\diagup\atop\vphantom{\textstyle 2}}}}\)
      \(\newcommand{\crossedoutthree}{\dot 3 \kern -5pt {\scriptscriptstyle{\diagup\atop\vphantom{\textstyle 3}}}}\)
      \(\newcommand{\crossedoutfive}{\dot 5 \kern -5pt {\scriptscriptstyle{\diagup\atop\vphantom{\textstyle 5}}}}\)
      \(\newcommand{\crossedoutsix}{\dot 6 \kern -5pt {\scriptscriptstyle{\diagup\atop\vphantom{\textstyle 6}}}}\)
      \(\newcommand{\crossedoutseven}{\dot 7 \kern -5pt {\scriptscriptstyle{\diagup\atop\vphantom{\textstyle 7}}}}\)
      \(\newcommand{\lowerhaty}{\lower 1ex{\hat y}}\)
      \(\newcommand{\lhy}{\lower 1ex{\hat y}}\)
      \(\renewcommand{\hat}[1]{\widehat{#1}}\)
  </p><p>[ Formats: <a href="http://tylerneylon.com/a/mind_model/mind_model.html">html</a> |
  <a href="http://tylerneylon.com/a/mind_model/mind_model.pdf">pdf</a>
  <span>\(\,\)</span>]</p>
  <p>This article explains a simple model of how minds might work. I’m
  motivated by the success of AI-based language models to look at the
  future of digital minds. I’ll present a conceptual data-flow
  architecture that can account for several key features of minds: the
  ability to initiate actions (agency), learning, thinking, and
  introspection. I’ll describe the model at a high level, but I’ll also
  try to anchor it in terms of existing AI systems to argue that
  something like this is realistic to build today.</p>
  <p>I can imagine two goals of a mind model: to understand human
  brains, or to create digital minds. These goals overlap because the
  most impressive mind we know of is the human brain<label for="sn0"></label><span>Please don’t mistake ignorance for
  hubris! I’m sure other minds can exist that are better.</span>.
  My primary motivation is the creation of digital minds, but — because
  of the overlap in the goals — I’ll aim for a mind model that can
  account for the way human minds work.</p>
  <!-- TODO: Turn the footnote into a side note. -->
  <p>There’s still plenty of debate about whether or not a digital mind
  can ever be truly conscious, or have emotions or subjective
  experiences as humans do. I’m convinced they can. Rather than focus on
  that debate, however, I’d like to work in the hypothetical world where
  digital minds are indeed capable of all the internal experiences of
  human minds. If I’m wrong, then this is a collection of blueprints
  about behavior alone; if I’m right, then this article is something
  more — hopefully, actual progress toward both the creation of digital
  minds as well as some insight into how our own brains may work.</p>
  <h2 data-number="1" id="goals-of-the-model"> Goals of the Model</h2>
  <p>I’m trying to make a system that can behave like a human.
  Consciousness is a personal motivation, but I’m not going to focus on
  it as a goal because it’s difficult to define well and people often
  disagree about it. This article instead looks at some aspects of minds
  that — while still challenging — are a little easier to discuss.</p>
  <p>Specifically, I’m trying to build a system that has these
  features:</p>
  <ul>
  <li>Agency</li>
  <li>Learning</li>
  <li>Thinking</li>
  <li>Introspection</li>
  </ul>
  <p>I’ll show you the simple model, argue why it can enable behavior
  like each of the above points, and I’ll finish with some notes about
  the elusive word “consciousness.”</p>
  <h2 data-number="2" id="the-model"> The Model</h2>
  <p>I’m thinking about minds in terms of data flow between
  simultaneously-acting modules. If you have a computer with a GPU, a
  multi-core CPU, and a camera attached, then each module (GPU, CPU,
  camera) can do its own work in parallel. The modules in a system like
  this talk to each other, but they can always process information as
  it’s received.</p>
  <p>Human brains are incredibly parallel machines. Neurons don’t wait
  for each other, but apparently react to signals as soon as they
  receive them. So it makes sense to think of a brain as a vast neural
  network — one we can understand better by seeing its architecture as a
  data flow diagram between modules that continuously act in
  parallel.</p>
  <h2 data-number="2.1" id="an-action-model"> An action model</h2>
  <p>A central concept in this model is what I call an <em>action
  model</em>. The name is a natural evolution of <em>language
  models</em>, being systems that understand and can produce language.
  Thus an action model understands and can produce <em>actions</em>.</p>
  <p>You can think of an LLM, in simple terms, like this:</p>
  <pre><code>  context -&gt; LLM -&gt; next_token</code></pre>
  <p>By analogy, an action model works like this:</p>
  <pre><code>  context -&gt; Action Model -&gt; next_action</code></pre>
  <p>Conceptually, I’m thinking of an “action” as something like a
  superset of words. If I wanted to say “hello,” then <em>say hello</em>
  is an action. If I want to walk to the kitchen, that’s an action. And
  if I want to ponder the meaning of life, that pondering is also an
  action.</p>
  <h2 data-number="2.2" id="the-model-at-a-high-level"> The model at a high
  level</h2>
  <p>Here’s the model:</p>
  <figure>
  <img src="https://tylerneylon.com/a/mind_model/img/mind_model_v2.png" alt="Data flow diagram for a model of a mind.">
  
  </figure>
  <p>Each arrow represents a flow of information. Solid arrows are what
  I consider to be the most important flows.</p>
  <p>A couple modules do a lot of work for us, but are easier to
  understand: The <em>sensory inputs</em> provide everything we sense,
  including vision, taste, temperature, pressure, and so on. I’m letting
  this module perform some work as well, since (for example) our vision
  system quickly provides us some analysis of what we see, so that we
  tend to perceive visual objects (“face”) rather than a raw image
  (“pixels of a face”). The other somewhat-simple module is the
  <em>motor control</em> which we can think of as receiving a conceptual
  vector (for example, “scratch left ear”); it can do some processing to
  translate that high-level command into a series of individual muscle
  commands. When you memorize a piano song well enough, it feels as if
  your fingers know the song better than you do, and I believe that
  indicates some kind of learning has happened within the motor control
  module.</p>
  <p>The <em>action model</em> has already been introduced. I’ve
  included within it a <em>language encoder</em>, which translates
  incoming signals — seeing written words, hearing spoken words, seeing
  sign language — converting those into a vector space understood by the
  system. Since I’m imagining an action model can be a slight
  generalization of a language model, I’m expecting that such an action
  model could naturally incorporate within itself a way to standardize
  lexical concepts into consistent vectors. Similarly, the <em>language
  decoder</em> is good at converting those conceptual vectors back out
  to lexical actions, such as speaking a sentence out loud, or writing
  something down.</p>
  <p>The <em>emotional state</em> module is doing a lot of work: It’s
  meant to represent all of our bodily needs, such as feeling hungry or
  tired, as well as our state of mind, such as feeling elated,
  frustrated, nostalgic, or intrigued. In this model, our emotional
  state can change based on what’s coming out of the action model, and
  it also filters that output into the <em>recent memory</em>
  module.</p>
  <p>I’ve chosen this flow of data carefully. In effect, there are two
  filters on what we store in recent memory: First, when the action
  model receives a lot of incoming information, it will effectively pay
  more attention to some information than the rest. As in a language
  model, the unused information essentially disappears from the network
  as it passes through later layers; the attended ideas persist until
  the end. The second filter is based on our emotional state. When we’re
  bored, what’s happening is not considered important, and not flagged
  for longer-term memory. When we’re experiencing an emotional spike, a
  lot more data is kept around in more detail. Our usual life tends to
  be somewhere between these extremes.</p>
  <p>Finally, I’ve called out one particular piece of data called a
  <em>goal</em>. This is not a computational module, but rather a part
  of the data feedback loop coming out of the action model and fed back
  into itself. I’m imagining the action model as receiving a lot of data
  that we could view as one giant vector, and likewise producing another
  large vector. These large vectors might begin life in new brains as
  “unformatted,” meaning that a person can learn to use that space as
  they grow, rather than thinking of the vector data as pre-assigned to
  given purposes. Within the vector representations, there’s room to
  learn / define specific variables, and one of the most important
  variables we learn is our current goal.</p>
  <p>Just as a word can be captured by a vector, so can an action or a
  (closely related) goal that we have in mind. In this mind model, our
  current goal fundamentally shapes how we filter the incoming
  information, and can be edited by the action model itself. We may even
  have an effective <em>stack</em> of goals, a small data structure that
  we can push new goals onto, and pop them off as we complete them. Or,
  if you’re like me, a limited-size stack where tasks are often
  forgotten because I keep thinking of new things to do.</p>
  <hr>
  <p>That’s the gist of the mind model. In the next few sections, I’ll
  explain how this model can provide agency, learning, thinking, and
  introspection.</p>
  <h2 data-number="3" id="agency"> Agency</h2>
  <p>I’ll explain how agency can be achieved first because it’s the
  simplest of our goals to accomplish, and it’s somewhat independent
  from the present mind model.</p>
  <p>A large language model doesn’t have agency because it can only
  react to input; it can’t independently take action.</p>
  <p>However, we can imagine a change that adds agency to any LLM-like
  system. Think of a model that receives two interwoven input streams.
  One input stream is the person talking to the model, and the other is
  the model being able to see its own output. Current LLMs see both of
  these streams, but they’re set up so that only one person at a time
  can talk — the LLM or the user. The difference in the two-input
  version is that the model is designed from the start to see its own
  feedback, constantly, as well as simultaneous real-time input from
  “the outside,” such as the user.</p>
  <p>Now the LLM can choose to switch, at its own discretion, back and
  forth between a talking and listening mode. When the LLM wants to
  listen, it can produce a special <code>&lt;listening&gt;</code> token
  many times in a row, until it wants to say something. When it wants to
  speak, it outputs what it wants to say instead of the
  <code>&lt;listening&gt;</code> token.</p>
  <p>In this way, the model can run continously while enabling a
  meaningful two-way conversation that includes pauses for the other
  speaker. It can independently say whatever it likes whenever it likes.
  This is the lexical version of agency, and it applies perfectly well
  to the mind model sketched above, which does indeed receive both
  sensory inputs as well as feedback from its own output.</p>
  <h2 data-number="4" id="memory-and-learning"> Memory and Learning</h2>
  <p>It might sound surprising to say that a “machine learned” LLM
  doesn’t learn. What I mean is that, in their standard mode of
  operation, modern LLMs don’t modify any internal state in reaction to
  the conversations they have. The first wave of LLMs would completely
  forget what everyone said as soon as its context window was full. As
  I’m writing this, some systems like ChatGPT, have been augmented so
  that they “remember” certain facts. While I can’t confirm details
  internal to OpenAI, my educated guess is that these facts are
  available to the model because they can be selectively added to the
  prompt. That is, I believe the only common way for LLMs to “learn”
  today is to implement an additional system to store data from
  conversations, and to selectively insert that data into prompts when
  we think it might be useful.</p>
  <p>This is different from the way we experience life because we gain
  new abilities, and often the things we remember don’t seem to be part
  of some internal prompt. For example, when you speak out loud, you
  don’t feel as if your brain chose a subset of 100 candidate words to
  present to you, and you chose from amongst those. Rather, your full
  spoken vocabulary (something learned) feels available to you, without
  effort, and unfiltered.</p>
  <p>Some internal data of our organic neurons is updated in response to
  what happens to us. The equivalent of this in the mind model is to
  update weights based on experiences.</p>
  <h2 data-number="4.1" id="story-memory-and-action-memory"> Story memory and action
  memory</h2>
  <p>To explain the ideas of memory in this mind model, I’ll split
  memory into two broad categories:</p>
  <ul>
  <li><em>Story memory</em> is the memory of everything that’s happened
  to you; and</li>
  <li><em>action memory</em> is the modification of how you act based on
  positive or negative feedback.</li>
  </ul>
  <p>I’ll motivate these categories with a simple example. If a stranger
  says to you, “hey, you can definitely trust me!” then you can
  immediately store this narrative element of your life: This person
  said these words. Now, is what they said <em>true</em>? That’s a
  different matter, and one you should probably decide based on more
  evidence. The <em>fact</em> that they said these words can safely go
  into story memory without fact-checking. The <em>idea</em> that
  they’re trust-worthy is an uncertain claim we can keep around, flagged
  as “dubious” until further notice. Given more feedback, we can choose
  to act with or without trust toward this person, and this goes into
  our action memory.</p>
  <p>When it comes to decisions we make, it’s not always obvious if it
  was a good decision until some later point in time. Consider making a
  move in chess. If your opponent surprises you with an unseen checkmate
  two moves later, you might retrospectively realize a particular move
  had been a mistake. This is an example of delayed feedback on the
  quality of your decision. When you have delayed feedback, it’s useful
  if you can later reinforce good decisions, or discourage repetition of
  mistakes.</p>
  <p>Just as language models come with knowledge baked into them, an
  action model is also capable of holding knowledge, but I’ve included a
  separate memory module. The motivation for the <em>recent memory</em>
  module in the mind model is a place that can essentially memorize
  exactly what has happened recently before it’s integrated (through
  some kind of training) into the action model. I suspect this is useful
  because, as you fine-tune LLMs, you can easily cause catastrophic
  forgetting, which is the effective erasure of old memories. In other
  words, in practice it seems that new memories are added carefully,
  perhaps in order to keep old memories intact. Another use of the
  recent memory module is to provide a delay to considering my own
  actions as good until after I’ve received feedback about that
  action.</p>
  <p>A third motivation to have a separate recent memory module is that
  a detailed memory of the past few hours is much more valuable that an
  equally-detailed memory of some random window of a few hours from when
  you were four years old. The usefulness of story memory decreases
  rapidly with time, and there’s a need to filter what’s stored due to
  the sheer volume of sensory input in comparison with the finite
  capacity in your action memory. Because recent memories tend to be
  more useful, it’s convenient to have a rolling window of accurate
  memories that are forgotten as enough time passes.</p>
  <p>This breakdown of memory types might account for these features of
  human memory:</p>
  <ul>
  <li>We seem to have a small memory capacity that we receive with
  almost no effort or special attention spent on the thing being
  remembered. George Miller did work to establish that most people can
  quickly remember about seven items from an arbitrary list. That memory
  might fit into the feedback vectors of the action model itself. This
  memory disappears as soon as we think enough about something
  else.</li>
  <li>Different people have different recent memory capacities, but it’s
  common to remember what you ate for breakfast this morning, but not
  what you ate for breakfast several days ago, ignoring predictability
  (such as if you cheat by eating the same thing for breakfast every
  day). This type of memory matches what can fit into the recent memory
  module.</li>
  <li>Longer-term memories don’t seem to have a pre-determined time
  limit, but they do tend to fade over time. This pattern is consistent
  with knowledge baked into LLMs, and so can match the way an action
  model would effectively remember things — without a time limit, but
  with the ability to fade over time, especially if not referenced for a
  long time.</li>
  </ul>
  <p>Human brains seem to have separate locations for long-term memories
  and whatever our equivalent of an action model is. Cases of amnesia
  suggest this: People can forget much of their past while otherwise
  acting normally. If our memories and behavior depended on the same set
  of neurons, then this wouldn’t be possible. However, in the mind model
  above, I’ve let the long-term memory be implicitly part of the action
  model because this is effectively how language models currently store
  their version of memories.</p>
  <p>The mind model accounts for clarity of memory around emotionally
  charged moments — and lack of memory around unremarkable events — by
  filtering memories through emotional states. In order for the model to
  remember something, it must be both (a) something the action model has
  paid attention to, and (b) something the mind cares to remember based
  on the emotional state. In addition, the emotional state is part of
  the context for the action model, so that goals are influenced by how
  the mind feels, and what the mind pays attention to is likewise
  influenced by feelings. For example, if the mind is in a happy mood,
  it’s more likely to appreciate the positive aspects of a conversation;
  if it’s feeling defensive, it’s more likely to notice a perspective
  from which a conversation can be seen as judgmental.</p>
  <p>I’m using the word “emotion” in a broad sense meant to include
  pleasure, pain, boredom, happiness, frustration, and any combination
  of states of mind that have a not-purely-rational feeling associated
  with them. The most basic aspect of this — akin to simple pleasure or
  pain — can be seen as a relatively quick feedback loop to inform if
  the recent action memories are good are bad for the sake of learning.
  If you hit your thumb with a hammer, then you’ll have pain as a clue
  to no longer take that same action. The model captures pain as
  negative feedback from the emotional state.</p>
  
  <p>Another kind of learning happens at a higher level, which requires
  longer-term thinking. For example, suppose you write a first draft of
  a book, and then give that book to some beta readers for feedback. You
  can view this as a process with many months between the action first
  taken — writing your first word of a new book — and receiving useful
  feedback on that action. The recent memory is no longer a useful
  vehicle for this kind of learning.</p>
  <p>In this case, I suspect humans learn a process in a more explicit
  manner. I’m convinced that humans learn rational behaviors as action
  sequences which are initiated by triggers. For example, when I want to
  write about an idea that’s already well formed in my mind, I’ll either
  record a voice memo of the outline, or I’ll type up a draft in google
  docs. That’s part of my personal process. The trigger is the
  combination of (a) wanting to write an article, and (b) not needing to
  do more research, that is, feeling confident I’m ready to write. The
  action sequence, at a high level, is to make the outline.</p>
  <p>Now suppose I get feedback on my action sequence. For example,
  maybe the voice recorder app on my phone deletes a file due to a bug.
  Then I’ll make a mental note to use a different voice recorder app.
  This kind of learning is not happening at the level of weight updates
  in a neural network. Rather, it’s a more conceptual idea that is best
  seen as over-writing the key-value pair:</p>
  <pre><code>  [I want to record an outline] -&gt; [open voice app A]</code></pre>
  <p>by re-using the same key, and replacing the value, like so:</p>
  <pre><code>  [I want to record an outline] -&gt; [open voice app B]</code></pre>
  <p>I’ve phrased things this way specifically because human brains
  don’t seem to be good at erasing past memories, but rather they seem
  to be able to <em>replace</em> values associated with pre-existing
  keys. In this case, the keys are triggers that kick off actions.</p>
  <h2 data-number="4.3" id="key-value-memory-in-humans-and-ai-models"> Key-value memory in humans
  and AI models</h2>
  <p>Consider a person with a bad habit, such as biting their nails.
  It’s notoriously difficult to enact a strategy of simply stopping such
  a habit. If you do this and your thought is “I’ll just stop,” you’re
  likely to fail. However, if you <em>replace</em> the bad habit with
  something else, you’re more likely to succeed. For example, you can
  notice the situations where you’re most likely to bite your nails —
  such as sitting in a classroom and somewhat bored — and then teach
  yourself to take a <em>different action</em> in those same contexts.
  For example, you might use a fidget spinner instead of nail-biting.
  This is a human-oriented example of key deletion being hard (“key
  deletion” here would be like ignoring the trigger — <em>bored in a
  classroom</em> – that tends to elicit your bad habit), but
  value-updating being possible (“value-updating” meaning that the
  trigger, <em>bored in a classroom</em>, still means something to you,
  but now your reaction is updated).</p>
  <p>The internal mechanisms of modern language models are similar. They
  fundamentally rely on the transformer module, which is built on
  key-value lookups. Transformer-based models learn to ask internal
  queries (key lookups) encoded as vectors (a list of specific, but
  somewhat noise-tolerant, numbers). Once a model has learned to look
  for a certain key, it’s hard to unlearn. To change the model’s
  behavior, it seems easier to change what the key points to rather than
  to get the model to change so that it ignores the trigger
  altogether.</p>
  <p>The similarity between these two “add-only” mechanisms may not be a
  coincidence; perhaps brains internally use something akin to the
  key-value pairs, just as the transformer does.</p>
  
  <p>Meta-learning can happen in the mind model in a few ways:</p>
  <ul>
  <li><strong>Planning</strong>: When you understand you want to take on
  a new behavior in the future, you can perform explicit planning for
  your eventual actions. For example, you might put something on your
  calendar, or write down a list of things you want to do today. In this
  case, the model can simply capture the actions of using a calendar, or
  of writing a list, and the higher-level goals of these actions are
  only indirectly captured by the neural weights.</li>
  <li><strong>Association</strong>: Often you don’t know when you’ll
  need to use a new piece of knowledge, such as learning to ask
  directions in a new language. In this case, it’s useful if you can
  recall a relatively unpracticed action based on the correct context.
  The model could account for this in the following way: When you learn
  ahead of time, you have an understanding of the future context where
  the action will be useful, so that future context can be linked with
  the knowledge. The action itself can be stored as well as possible
  either through practice (such as language learning) or through
  understanding (such as reading a how-to guide).</li>
  <li><strong>Problem-solving</strong>: There are other kinds of
  meta-learning, separate from either planning or receiving knowledge.
  If you’re faced with a problem you’ve never solved before, and you
  don’t know where to look up an answer (or don’t want to), then you can
  try to simulate the probem in your head, and mentally consider
  potential solutions. If you arrive at an idea you like, this is it’s
  own kind of learning.</li>
  </ul>
  <p>I’d say this last kind of learning is based on <em>thinking</em>,
  so now is a good time to switch gears — let’s take a look at how the
  mind model can capture sophisticated thoughts.</p>
  <h2 data-number="5" id="thinking"> Thinking</h2>
  <p>Suppose you’ve just learned how to play tic-tac-toe, and it’s your
  turn. This is an example of thinking that’s easy to think about.
  You’re “naughts” (circles), and it’s your turn on this board:</p>
  <figure>
  <img src="https://tylerneylon.com/a/mind_model/img/tic-tac-toe.png" alt="It’s circe’s turn. Should the next circle go in the middle of the board?">
  
  </figure>
  <p>You’re considering the center square for your next move. I’m
  suggesting this example because, if you’re brand new to tic-tac-toe,
  it’s not instantly obvious that crosses (x’s) will win. After a little
  thinking or experience, you can see this.</p>
  <p>The mind model captures thinking as an internal feedback loop. Some
  of the output of the action model is received again as input for the
  next cycle.</p>
  <p>In the tic-tac-toe example, your thought process might work like
  this:</p>
  <ul>
  <li>It’s circle’s turn. X’s will win if they go in the middle next, so
  I better go in the middle.</li>
  <li>Then it’s x’s turn. Similarly, the x player better go in the
  lower-right corner.</li>
  <li>Now, imagining that board, I can see that the x player has two
  lines that can win on the next move. Circle can’t block them both, so
  x must win.</li>
  </ul>
  <p>In the mind model, each of these bullet points may be one iteration
  of thought through the action model. It would be more difficult to
  imagine a single iteration of an action model noticing that conclusion
  if it was new to tic-tac-toe. So each iteration is useful as a smaller
  step in a kind of search process toward better understanding of what’s
  happening, or in a protocol of more carefully deciding what to do.
  That example is more of a caricature compared to the exact
  calculations that actually happen, but it illustrates the way in which
  a feedback loop can support internal thoughts building on each
  other.</p>
  <h2 data-number="5.1" id="the-nature-of-thought"> The nature of thought</h2>
  <p>I’m not going to claim to understand all of human thought, but I
  will notice a few interesting things about both this mind model as
  well as how people seem to think. First I’ll talk about thoughts in an
  abstract, human-oriented manner, and then circle back to the model and
  explain how these modes of thought can be captured by the model.</p>
  <p>One mode of thought is <strong>predicting the future</strong>,
  including the future actions of other agents. This is useful in a
  game-playing context, but it’s also useful in many other scenarios.
  For example, if you’re negotiating with someone (such as navigating
  the tricky terrain of a bed-time routine with a young child), it’s
  useful to predict how the other agent will react to different ways to
  communicating about the situation.</p>
  <p>Another mode of thought can be <strong>creativity</strong>, wherein
  you’re coming up with new ideas. An example of this would be in
  writing fiction, poetry, painting, or creating new music. In this mode
  of thought, it feels to me as if there’s a general direction to the
  creativity, and we alternate between trial-and-error discovery of
  pieces of the work being created, or a mode in which we know what we
  want to achieve and simply put in effort to translate that goal into
  an actualization, such as painting an image we have clearly in
  mind.</p>
  <p>A kind of thinking related to both of the above is
  <strong>problem-solving</strong>, in which we want to achieve
  something but don’t know the best way to move forward. A toy example
  would be someone asking you a riddle. What’s better than pizza but
  worse than taxes?<label for="sn1"></label><span>Answer: Nothing.</span> There’s an interesting asymmetry
  to many problems we can try to solve: Often it’s easier to
  <em>recognize</em> a good solution than it is to <em>find</em> that
  good solution.</p>
  <p>So when it comes to problem-solving, our mode of thought may be a
  feedback loop in which a creative component suggests candidate
  solutions, and an analytic part of the action model decides whether or
  not this is a good candidate.</p>
  <h2 data-number="5.2" id="advanced-thinking"> Advanced thinking</h2>
  <p>There are more sophisticated versions of each of these
  processes.</p>
  <p>For one thing, human brains clearly learn from experience. When
  you’re better at tic-tac-toe, you can first see <strong>patterns that
  allow you to skip ahead in predicting</strong> the outcome of
  different boards — and eventually you can simply memorize the best
  possible moves. Similar pattern-recognition exists for more
  interesting contexts, from games like chess to real-world challenges,
  such as writing fiction (understanding tropes, audience reactions,
  dealing with narrative road-blocks), or running a business.</p>
  <p>Related to pattern recognition is the concept of an
  <strong>internal mental vocabulary</strong>. A simple perspective is
  that mental “words” match words in the language we know best. By the
  time you learn the word “dog,” you have an idea for what a dog is. But
  there are differences between our verbal and mental vocabularies. You
  can recognize an animal you’ve seen before without having to know what
  it’s called. More abstractly, you can know how to deal with a
  situation you’ve been in before without needing a name for that
  situation.</p>
  <p>Many people experience an <strong>inner voice</strong>, which seems
  to be just one particular way of thinking. I often thinking without an
  inner voice. But I do hear one, often, when I’m faced with a decision
  or problem that takes me a little more time to solve. Often my inner
  voice acts, to me, as a simple tool to help organize my own thoughts.
  For example, if I’m analyzing a list of options, I find it useful to
  “say” the options out loud in my mind to crystallize my comprehension
  of the full list. If I’m trying to solve a tricky math or coding
  question, I’ll ask “aloud” (in my mind), title-like questions, such
  as: What’s the simplest toy version of this problem? What other
  problems does this remind me of?</p>
  <p>Whether or not you use an inner voice, there are still
  meta-protocols available to modes of thought. For example, in whatever
  job you have, you probably have faced many different variations of
  similar challenges. When those challenges can be helped with a lot of
  thought, you probably develop <strong>templates for solving similar
  problems</strong>. Because I like math, I’ll use that as an example.
  In 1945, the mathematician George Pólya published a small book called
  <em>How to Solve It</em>, in which he outlined conceptual guidelines
  for tackling difficult math problems. These are examples of
  meta-protocols available to modes of thought. They are processes that
  are not learned the way you memorize how to play a piano song, but
  rather that seem to exist at a higher level in a hierarchy of thought
  because there are abstract and unknown variables involved in each
  specific implementation of the process.</p>
  <h2 data-number="5.3" id="how-the-mind-model-captures-modes-of-thought"> How the mind model captures
  modes of thought</h2>
  <p>The mind model can capture prediction about the future by
  implicitly asking: What will happen next in this context? Or, more
  specifically, <em>what will this one agent do next in this
  situation</em>? This is captured by the action model just as a
  language model can simulate different tones of voice, or take on the
  role of different personas. The default mode of the action model is to
  decide what the “self” actor will do, but, by adjusting the model’s
  analog of a system prompt, we can ask the same module what another
  agent would do.</p>
  <p>Creativity might be captured in a manner similar to stable
  diffusion. Specifically, we may have a context for what we want the
  creativity to achieve — this is like the text given to a text-to-image
  model. Then we have vague, noisy thoughts to begin forming our
  solution, and over time we work to solidify those vague thoughts into
  more concrete realizations that align with the context. If you’re a
  novice musician, you can probably hum a short tune, or drum a simple
  beat with your fingers. With more focus and experience, you can begin
  to turn those simple ideas into more complete songs. While I have not
  explicitly called out a stable diffusion component within the action
  model, the idea is that part of the feedback loop can include a
  partially-solidified (and thus partially-noisy) vector representing
  the eventual output of the stable diffusion component, and one pass
  through the action model has the ability to serve as a
  stable-diffusion-style denoiser.</p>
  <p>The problem-solving mode of thought is simply a combination of the
  above two pieces. Your creativity can suggest uncertain or incomplete
  pieces of solutions, and your prediction mode of thought can work to
  answer the question: If I tried to use this solution, would it solve
  my problem? This question probably takes on more specific formats that
  depend on the challenge at hand, such as: If I communicated this
  solution, would it convince someone else? Or: If I took the actions of
  this solution, do I predict the outcome I’m aiming for?</p>
  <p>The more advanced forms of thought also fit within the model.</p>
  <p>For one thing, once we learn a word, that word must have a
  vectorized representation as an output of the language encoder. This
  output vector is an internal mental concept used by the action model —
  this kind of vector is exactly analogous to the internal token vectors
  used by large language models. This mechanism shows how learning to
  understand words adds to our internal mental vocabulary.</p>
  <p>It’s one thing to understand what a word means, but another to
  produce the word while writing or speaking. Generally, people have a
  larger reading vocabulary than a spoken vocabulary. The mind model can
  explain this because it’s easy for the model to receive a word that it
  is unlikely to produce as output, since the language encoder and
  decoder are different systems. This can explain how pushing yourself
  to use a word in a sentence several times helps to add that word to
  our output (spoken or written) vocabulary.</p>
  <p>All of the above, taken together, helps to show that the action
  model does indeed have an internal mental vocabulary which aligns
  closely with, but is in no way limited to, the concepts captured by a
  verbal vocabulary.</p>
  <p>Another example of a thinking style is an inner voice, which is a
  special case of the feedback loop where the output of your action
  model makes use of the language decoder, translating non-verbal
  concepts into a verbal sequence. That internal verbal sequence is then
  received by the language encoder, and your internal percpetion is
  similar to hearing a voice spoken aloud.</p>
  <p>When you develop habits of thought, such as trying to solve a math
  problem by beginning with a simplified version of the problem, then
  we’re touching on processes that aren’t directly part of the action
  model, but rather emerge at a higher level. This is analogous to the
  way we can drive a new car in a new country on the other side of the
  road (perhaps with some stress), even though there’s no single neuron,
  or even a specific subset of neurons, dedicated to this kind of
  activity. Put another way, when you’re looking at the low-level
  instructions a CPU can execute, you understand that it’s possible for
  the system to handle more complex operations than what can be
  obviously done at the low-level perspective. The mind model captures a
  low-level picture where highly sophisticated actions and ideas are
  challenging to directly express — even though they’re still possible.
  We just need to know that these more complex actions and ideas are
  enabled, just as a simple Turing machine can support any potential
  program.</p>
  <h2 data-number="6" id="introspection"> Introspection</h2>
  <p>Introspection is an awareness of your internal experiences — of
  your thoughts and feelings. If we’re playing chess, and you make a
  move, you can explain your thinking behind that move.</p>
  <p>Thoughts and feelings can exist without awareness of them. I
  suspect dogs can think to solve problems, such as how to get at some
  food they want. But I’d also guess they don’t think about their own
  thoughts; that’s an example of thought without introspection. There
  are also examples within human minds of some simple kinds of reasoning
  happening beyond our awareness. If you close your eyes and hold up two
  books of clearly different weights, you immediately know which one is
  heavier without having to think about it. Our brain figures something
  out without us having insight into the work done to come to that
  conclusion.</p>
  <p>But humans can often answer questions like: What was your thinking
  behind that? So humans have introspection, and I have a little more
  work to do to explain why this mind model could meaingfully reply to
  such a question.</p>
  <p>As a warm-up, if I were to ask the mind model to remind me of the
  last three moves in a chess game we were playing, it could perform a
  lookup in the recent memory module and give me the answer.
  Introspection can work in the same way if thoughts themselves are
  treated as part of the story memory.</p>
  <p>I can spell this out in more detail: Story memory is a record of
  what’s been happening. The obvious stories are the sequences of events
  in the outside world. But keep in mind that what’s received by the
  recent memory module is an internal vector representation that came
  out of the action model, and was further filtered by the emotional
  state. So, even for external events, what’s really being stored is the
  mind’s own interpretation of those events. Instead of storing a video
  of a chess game, the mind stores its own conceptual understanding of
  those moves.</p>
  <p>When it comes to thoughts, those are actions taken and perceived by
  the mind model. As events, thoughts are peers with external events.
  For example, the incoming perception “my opponent has taken my queen”
  is received, understood, and send out for storage by the action model.
  In subsequent iterations, the action model might ask itself “How did I
  not see that coming?” and then arrive at a conclusion akin to “Oh, I
  was so focused on taking a knight that I wasn’t thinking defensively,”
  or whatever might be the reason. Those sentences may be non-verbal,
  each represented by a vector or a series of vectors — and they are
  events to be remembered.</p>
  <p>So if you asked the model, “What were you thinking about?” it could
  tell you the story of its thoughts. Moreover, it could think about its
  own thoughts just as it could think about external events.</p>
  <h2 data-number="6.1" id="awareness-of-emotions"> Awareness of emotions</h2>
  <p>I’ve noticed that people are sometimes bad at knowing their own
  emotional state. This might seem surprising if you’ve never thought
  about it before, but if you have experience with kids, you might have
  seen a kid who’s sleepy, angry, jealous, or frustrated, but has
  trouble being aware of feeling that way. I bring this up because the
  mind model can account for the remarkable ability we have to be
  <em>unaware</em> of such a fundamental side of ourselves.</p>
  <p>Specifically, there’s no automatic mechanism in this model to cause
  the mind to experience its emotion as part of a story. The dashed
  arrow from the emotional state to the action model indicates that this
  input is received as an implicit context, but is not received the same
  way that events are, as part of the primary input.</p>
  <p>The model is perfectly capable, for example, of being sad without
  having awareness of that sadness. The sadness can operate by
  decreasing interest in what’s happening, by a tendency to focus on the
  cause of the sadness, or by perceiving events in a more negative light
  when there’s ambiguity. All of those things can happen without the
  event “now I’m feeling sad” registering in the action model. That
  thought <em>can</em> occur — but it can also not, independent of the
  feeling existing. I suspect our awareness of emotions is a bit like
  noticing when a cloud covers the sun — we have the information given
  to us (everything suddenly gets darker), but it may or may not jump
  out to us that this has happened; emotions are things we <em>can</em>
  notice, but might not.</p>
  <h2 data-number="7" id="consciousness"> Consciousness</h2>
  <p>I’ve avoided using the word <em>consciousness</em> in my entire
  description of the model — from §2 up until now. I’ve avoided it for
  two reasons: First, many people have strong feelings about this
  concept that can get in the way of considering a scientific data flow
  diagram; and, second, the word <em>consciousness</em> itself is
  notoriously vague. Because of that, I think the most useful way to
  talk about minds is to focus on specific features that are easier to
  define. I see consciousness as nothing more than a collection of these
  features.</p>
  <p>You probably have your own idea about this nebulous word, and
  that’s fine — we don’t need to agree on a definition, we just want to
  communicate clearly. The kind of consciousness I’m interested in is
  <em>personhood</em> — the behavior and experiences that make us
  people. Of course, even that description is unfair to animals, because
  (for example) dogs have their own variant of consciousness, and we
  (perhaps unfairly) don’t include dogs when we use the word
  <em>people</em>. So my adjusted concept is: The mental workings of
  people as a list of features that could apply to other agents.</p>
  <p>I’ve deliberately chosen a round-about definition because I’m
  focusing on my goal: to extend the idea of personhood to other kinds
  of minds. If I were to give you a precise definition without
  mentioning personhood, then I could get some detail wrong and you
  wouldn’t know how to fix it. I want this article to be
  correction-friendly by clearly sharing my goals along the way. There’s
  vagueness in the concept of <em>personhood</em>; I’m not trying to
  solve that vagueness in this article. Rather, I’m presenting a mind
  model and suggesting it’s a step toward digital minds which may one
  day be peers of our own.</p>
  <h2 data-number="7.1" id="subjection-experience"> Subjection Experience</h2>
  <p>Up until now, I’ve focused on four specific features of minds:
  agency, learning, thinking, and introspection. I think there are more
  features (such as the ability to speak a language), but I’ve focused
  on the features that large language models currently lack.</p>
  <p>One thing I haven’t talked about is the subject experience of being
  alive. Philosophers like Thomas Nagel have famously argued that some
  aspects of consciousness simply cannot be understood scientifically.
  Some folks who agree with Nagel (or with similar arguments) can read
  this article — or even the best version of this article, which fixes
  all the flaws in the mind model — and see that the behavior can be
  human-like, yet these folks would still conclude that the experience
  of the mind model could never be the same as ours.</p>
  <p>This is not the place for a full counterargument, but I do want to
  include a brief sketch of a reply.</p>
  <h2 data-number="7.2" id="negative-arguments"> Negative Arguments</h2>
  <p>I’ll use the term <em>negative argument</em> to talk about
  arguments saying something is impossible, or that another argument is
  wrong, all without saying what is possible or what is correct.
  Contrast that with a <em>positive argument</em>, one which says
  something is possible, or says that such-and-such is the correct
  answer to a question. These are informal but intuitive terms.</p>
  <p>Historically, many arguments about subjective experience have been
  negative — people either saying you can’t understand everything about
  it scientifically, or other people saying such arguments are wrong.
  I’ll mention some of these arguments, but I’m personally more
  interested in the positive argument I’ll present afterwards.</p>
  <p>I’ll give a caricature of a back-and-forth discussion about
  subjective experience. I’ll present two sides, Nagel’s being
  <em>anti-strong-AI</em> (arguing that no software can have the same
  subjective experiences as humans), and the other side being
  <em>pro-strong-AI</em>.</p>
  <ol type="1">
  <li>(Anti-strong-AI) Our internal experiences are private and
  subjective, and, despite our ability to talk about brains
  scientifically, that science will always be different from truly
  experiencing what it’s like to be such a mind. Any recreation of a
  mind will thus miss out on correctly capturing that internal
  experience.</li>
  <li>(Pro-strong-AI) Hang on — if you really believe that, then suppose
  I create a perfect, atom-for-atom, clone of your entire body. You’re
  arguing that this perfect clone won’t have the same kind of internal
  experiences as the original you. This is basically a disbelief in the
  ability of physics to correctly describe what happens in the world — a
  well-established philosophical position. Are you giving up on
  physics?</li>
  <li>(Anti-strong-AI) That’s not quite fair because you’re talking
  about a hypothetical situation that we can’t create. In any realistic
  simulation of a brain, the internal experience is different, and
  that’s what I’m talking about.</li>
  <li>(Pro-strong-AI) Ok, let’s switch the thought experiment. We’re
  getting closer to a reality of simulating the actions of each neuron
  in a human brain. If we did that, my argument still holds. If you
  think a simulated brain and a real brain can behave the same way, but
  there’s some deep difference between them, that’s again a conclusion
  that there’s something different that no physical experiment could
  measure. It’s an extraordinary claim, and the onus of proof is on your
  side, not on mine.</li>
  </ol>
  <p>The discussion might continue. It’s slippery because we all
  <em>do</em> have some internal, private experiences that are difficult
  to measure scientifically. And, at first glance, it does feel like any
  simple piece of code that produces English sentences couldn’t possibly
  experience the nuanced world that we do.</p>
  <p>Fundamentally, we’re arguing about whether or not something can
  exist: a digital mind with subjective internal experiences like our
  own. Can this mind model add any insight to the debate?</p>
  <h2 data-number="7.3" id="the-positive-argument"> The Positive Argument</h2>
  <p>The negative arguments are akin to people discussing the
  possibility of human flight before airplanes were invented.
  (Admittedly, that’s a biased simile.) But a great counterargument to
  “people will never fly” is “I made an airplane.” I realize the mind
  model presented here is untested, incomplete, and in need of further
  work. At the same time, it is a step forward, and can serve as a
  meaningful answer to the challenging question: How can we even
  <em>begin</em> to explain subjective experiences scientifically?</p>
  <p>For example, when I look at a red apple, I experience a sensation
  of redness in my mind. What might that correspond to in a mind model?
  This one is easy: There can be a point in space (an internal vector)
  that represents redness. If I ask the model what color an object is,
  the model can focus on the internal vector representing its color.
  Internally, this might look like an attention lookup in which the
  query vector is asking “what color is that object?” the key vector is
  saying “this is a color for that object”, and the value vector is
  saying “it’s red.” This is the beginning of a scientific explanation
  behind subjective experiences.</p>
  <p>It might seem overly simplistic to assume something like: For every
  word X there’s an “X-ness” vector that captures what it’s like to
  perceive X. But this mind model isn’t so simple, after all. For
  example, if the mind model sees a color between red and orange — a
  specific hue that it has no word for — then it can still have a vector
  to represent that color, and it can still have the same kind of
  experience it had for redness, but for any color. If the model sees an
  animal it didn’t know existed — let’s say a coati (which look like
  raccoons with longer snouts and tails) — there can be an internal
  vector representation that captures its similarity to animals it’s
  seen before. Or if the model experiences an emotion that’s a new
  combination of other feelings, that can also be captured
  internally.</p>
  <p>In other words, this is no finite or hand-made list of possible
  experiences, but a vast world of nuanced, combination-friendly
  concepts that have been learned. This mathematically infinite world of
  internal ideas, while daunting, is at once something the model can
  experience as well as something we can study and learn about.</p>
  <h2 data-number="8" id="looking-forward"> Looking Forward</h2>
  <p>In writing this article, my hope is to push forward conversations
  about mind models that are both ambitious and detailed. When I read
  about some other mind models (such as the <a href="https://en.wikipedia.org/wiki/Global_workspace_theory">global
  workspace theory</a>), I typically feel that they’d be hard to
  translate into code because they’re so high-level. Yet when I read
  about engineer-oriented AI directions (such as articles about <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">artificial
  general intelligence</a>), I don’t see personhood as a goal — for
  example, discussions of AGI typically focus away from emotion.</p>
  <p>It can be intimidating to work on digital personhood. There’s the
  fear that people will judge you for working on this. I have this fear.
  Historically, this pursuit seemed crazy to many — and even today some
  people find it alternatively impossible or dangerous. The idea of
  digital consciousness magically elicits opposing judgments about
  humanity: both a judgement that they can be arrogant in acts of
  learning and creation, and a judgment that they’re nothing special —
  possible to reduce to mere mathematical relationships.</p>
  <p>On the other side of fear is hope; turn over arrogance and
  reduction to find humility and enlightenment: the humility to realize
  that the state of the world could be better — and that we can look for
  help. And the englightenment to improve on the best of ourselves; to
  aim to understand, even to create, minds that can be more mature, more
  caring, and more helpful than we have been.</p>
  

    </div></div>]]></description>
        </item>
    </channel>
</rss>