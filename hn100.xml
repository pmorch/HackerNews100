<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 15 Aug 2023 17:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Firefox finally outperforming Google Chrome in SunSpider (456 pts)]]></title>
            <link>https://www.phoronix.com/news/Firefox-Faster-SunSpider</link>
            <guid>37134092</guid>
            <pubDate>Tue, 15 Aug 2023 13:58:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Firefox-Faster-SunSpider">https://www.phoronix.com/news/Firefox-Faster-SunSpider</a>, See on <a href="https://news.ycombinator.com/item?id=37134092">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="MOZILLA" src="https://www.phoronix.com/assets/categories/mozilla.webp" width="100" height="100"></p><p>
Mozilla developers are celebrating that they are now faster than Google Chrome with the SunSpider JavaScript benchmark, although that test has been superseded by the JetStream benchmark.
</p><p>
Last week a new Firefox Nightly News was published that outlines that "We’re now apparently beating Chrome on the SunSpider JavaScript benchmark!" The provided numbers now show Firefox easily beating Chrome in this decade-old JavaScript benchmark.
</p><p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=firefox_faster_sunspider" alt="SunSpider browser benchmark results"></p>
<p>The benchmarks come from <a href="https://arewefastyet.com/win10/benchmarks/overview?numDays=60">AreWeFastYet.com</a>. Meanwhile for the newer and more demanding JetStream 2.0 benchmark, Google Chrome continues to win easily over Firefox:
</p><p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=chrome_jetstream_win" alt="Chrome much faster in JetStream 2"></p>
<p>Besides Firefox running the JavaScript SunSpider benchmark much faster over the roughly past month, there's been work on the HTTP/2 upload speed improvements, and various other enhancements.
</p><p>
Learn about the latest Firefox Nightly build advancements via the <a href="https://blog.nightly.mozilla.org/2023/08/10/a-view-to-a-better-faster-web-these-weeks-in-firefox-issue-143/">Firefox Nightly News</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stellar Developers (602 pts)]]></title>
            <link>https://stellarsamurai.com/</link>
            <guid>37133872</guid>
            <pubDate>Tue, 15 Aug 2023 13:38:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stellarsamurai.com/">https://stellarsamurai.com/</a>, See on <a href="https://news.ycombinator.com/item?id=37133872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="MainContent" role="main" tabindex="-1">
      <div id="shopify-section-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24"><p>
        <h2 id="SectionHeading-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24">
          Favorite Categories
        </h2></p><slider-component>
      <ul id="Slider-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24" role="list"><li id="Slide-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24-1" data-cascade="">


          </li><li id="Slide-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24-2" data-cascade="">


          </li><li id="Slide-template--19422400577876__41a3ff50-b491-469f-8cd8-eaebc2dacd24-3" data-cascade="">


          </li></ul></slider-component></div><div id="shopify-section-template--19422400577876__256b0e40-e01d-4353-8ed0-5b40603a99f1">
    <div>
      <p><img src="https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=1500" alt="" srcset="https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=198 198w, https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=432 432w, https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=642 642w, https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=900 900w, https://stellarsamurai.com/cdn/shop/files/StellarSamuraiGrafittiW1.png?v=1691489553&amp;width=1284 1284w" width="1500" height="1500" loading="lazy" sizes="(min-width: 1200px) 659.9868002639947px,
              (min-width: 750px) calc((100vw - 130px) / 1.667), calc((100vw - 50px) / 1.667)">
</p>
    </div>
    <div id="ImageWithText--template--19422400577876__256b0e40-e01d-4353-8ed0-5b40603a99f1"><h2>
                Our Vision
              </h2><div>
                <p>We aim to portray the essence of ancient warriors know as Samurai into everyday clothing and accessories.</p><p>Each  design holds a story, wisdom and energy that we want to share with the world.</p><p>Unique clothing, accessories and home décor that gives a new feeling and an elegant touch. </p>
              </div><p><a href="https://stellarsamurai.com/pages/about">
                  Learn more about us
                </a></p></div>
  </div><div id="shopify-section-template--19422400577876__c5052349-f3ac-4407-aee0-d90ebc946611"><p>As a part of our launch, all customers get:</p><h2 data-cascade="">
                20% Discount for orders over 150$!
              </h2></div><div id="shopify-section-template--19422400577876__9207cebc-a5dc-4b77-8753-369fdc80294f">
    <h2>
      Find your style!
    </h2>
    
  </div><div data-cascade="" id="shopify-section-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3"><slider-component>
      <ul id="Slider-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3" role="list"><li id="Slide-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3-1" data-cascade="">
            <div><h3>Quality</h3><p>Each product is tested and reviewed before it reaches your home. We do our best to ensure the quality of each item meets our standards.</p></div>
          </li><li id="Slide-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3-2" data-cascade="">
            <div><h3>Durability</h3><p>We optimize for better and not cheaper materials. This type of approach makes sure that each design will last for as long as possible.</p></div>
          </li><li id="Slide-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3-3" data-cascade="">
            <div><h3>Style</h3><p>Whether it's a backpack, T-Shirt or phone case each product holds it's own unique look that catches the eye and makes you stand out.</p></div>
          </li><li id="Slide-template--19422400577876__195040ff-b3b8-4bbb-86a4-1e9bd99b70b3-4" data-cascade="">
            <div><h3>Uniqueness</h3><p>No two designs are the same. Explore our various palettes and be pleasantly surprised with the volume of variety that we have prepared!</p></div>
          </li></ul></slider-component>
    
  </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why host your own LLM? (119 pts)]]></title>
            <link>http://marble.onl/posts/why_host_your_own_llm.html</link>
            <guid>37133504</guid>
            <pubDate>Tue, 15 Aug 2023 13:06:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://marble.onl/posts/why_host_your_own_llm.html">http://marble.onl/posts/why_host_your_own_llm.html</a>, See on <a href="https://news.ycombinator.com/item?id=37133504">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>Andrew Marble<br><a href="http://marble.onl/">marble.onl</a><br>andrew@willows.ai<br>August 13, 2023</p>
<p>In the Terminator movies, good relationships beat technological superiority. Kyle Reese and Sarah Connor outwit the advanced T-800 who in turn helps Sarah and John beat the ultra-advanced T-1000. OpenAI’s GPT-4 is currently the most advanced publicly available language model. There are also analyses showing it’s generally cheaper to run than self-hosting comparable models. I want to argue that despite everything OpenAI’s models have going for them, it’s worth considering self-hosting anyway, especially if you’re building a product or an internal capability.</p>
<p>If you’re using language models for custom applications, you can use an API from companies like OpenAI or Anthropic, where you submit your prompt, get a response, and pay usage based fees. Or you can configure your own model and host it locally or in the cloud. There are many models available for self-hosting. A few recent analyses have made a case for using OpenAI’s API based on cost and performance<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a><sup>,</sup><a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Very detailed cost calculations are possible, but the obvious cost advantage of a usage based API is that you only pay for the hardware when you use it. Most self-hosted applications will be challenged to get good utilization from dedicated GPUs and so are paying a lot for idle time.</p>
<p>There’s a lot of subtlety in gauging performance – personally I think there’s not a 1:1 relationship between rankings in the various benchmarks and “leaderboards”<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> and performance on specific commercially relevant tasks. But GPT-4 is unequivocally better than the rest across a wide range of skills and only the best publicly available models compete with Claude (Anthropic’s model) and GPT-3.5.</p>
<p>Despite the advantages, there is still a compelling case for working with publicly available models. (Note I don’t say open source, many have other limitations that disqualify them from being labeled as such<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>, but I won’t dwell on that here.) To me it boils down to a the “relationship”. Using APIs means you’re a taker of whatever OpenAI et al. are offering. Model features, customizations, values (censorship and world view) etc. are all dictated by those companies. You can only build a front-end. This also means you don’t have access to internal states and so are limited, for example in applying advanced accountability techniques or guardrails on top. All of this could be good, it means you don’t have to worry about it. But it also makes whatever you build utterly dependent on a start-up company.</p>
<p>For “relationship-based” development, there are good reasons to use self-hosted models. Having control over the model architecture and weights removes uncertainty about future changes, and means you don’t have to take what OpenAI decides to give you. There is a rich ecosystem of different models to experiment with, as well as the ability to customize – for example by fine-tuning on your own terms. The construct ultimately lets you build a long-term relationship with your AI model and adapt your product around it, having clarity that what you build is going to keep working with the model that you’ve chosen and giving you control over when and if you decide to make changes. It lets you build something that isn’t just a front-end on somebody else’s language model but is deeply integrated.</p>
<p>Also, for many applications, the well-rounded superiority of a GPT-like model is not what’s driving value. Running a model as big as GPT-4 is potentially $10,000’s per month. But it’s possible to run 7B and 13B models (models with 7 and 13 billion parameters, common sizes for LLaMA and other public models) on a laptop. These models are big enough to perform many tasks competently and can be cost-effective as part of local systems.</p>
<p>“Responsible” use of AI has many meanings. Tech companies have often focused on political correctness and superficial notions of bias, largely to avoid controversy in broadly capable public models like chat-GPT. For many applications, particularly specialized knowledge work<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>, those concerns are mostly irrelevant and give way to real issues about factual accuracy, completeness, or simply staying on-topic. Many techniques for “keeping models in line” require access to internal states, gradients, and intermediate outputs<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>. Using an API-based model limits the kind of experimentation and augmentation that is possible.</p>
<p>The same holds true for various optimizations such as caching internal model states, as well as model fine-tuning. APIs offer options, but they are limited compared to what is available. The technology is evolving so quickly still that new models and techniques are becoming available every day. For those that are using the LLM as a tightly integrated part of a product or tool, the only way to have the flexibility to evolve with the technology is to have a self-hosted model.</p>
<p>An additional aspect of the fast pace of change in language models right now is that the skills and knowledge required to work with the technology are evolving quickly. Working with self-hosted models gives institutional and individual experience in this evolving landscape, in a way that APIs don’t. For professional development of employees as well as adaptability to change, keeping “AI” at a deeper technical level is important for many companies, particularly those that are building applications. It’s not a mature technology, and part of the “moat” that we practitioners have is simply knowing what’s going on. I’ll actually go further and say that any organization making nontrivial use of AI should internally or through advisers have access to some deep knowledge of the technology, not just the API reference, to be able to understand what it fundamentally does best. As AI gets commodified and hyped-up, there often ends up being a big disconnect between what it can do and what it’s used or proposed for.</p>
<p>In a few years, I expect the landscape will look very different – there will be agreed upon things that are critical to be able to do with a model, and APIs will support this. For a new, still experimental, and rapidly evolving technology, real participation requires deep access to the models and code. This doesn’t mean that all companies or products require such access – there are many valuable things that can be built on top of an API and would probably be a waste of time to self-host. But these are different kinds of products.</p>
<p>Back to the Terminator, Reese and the T-800 both built strong relationships that led to their successful completion of their missions. The Skynet-tasked Terminators just went around flexing their superior technological prowess, and it wasn’t enough to win the day. Part of building the relationships is access. I know it’s a silly analogy, but I believe the same is true with these models, it’s about being able to deeply understand the strengths of the tool and built something tightly integrated, and you can’t do that with an API.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526">https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526</a><a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="https://www.cursor.so/blog/llama-inference">https://www.cursor.so/blog/llama-inference</a><a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</a><a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><a href="http://marble.onl/posts/software-licenses-masquerading-as-open-source.html">http://marble.onl/posts/software-licenses-masquerading-as-open-source.html</a><a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>There is a wide range of literature and research into model accountability. For a narrow example, see “The Internal State of an LLM Knows When its Lying” <a href="https://arxiv.org/pdf/2304.13734.pdf">https://arxiv.org/pdf/2304.13734.pdf</a> but also <a href="https://arxiv.org/pdf/2307.00175.pdf">https://arxiv.org/pdf/2307.00175.pdf</a><a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The OpenTF Manifesto (220 pts)]]></title>
            <link>https://opentf.org/</link>
            <guid>37133054</guid>
            <pubDate>Tue, 15 Aug 2023 12:19:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opentf.org/">https://opentf.org/</a>, See on <a href="https://news.ycombinator.com/item?id=37133054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
          Terraform was open-sourced in 2014 under the Mozilla Public License (v 2.0) (the “MPL”).
          Over the next ~9 years, it built up a community that included thousands of users, contributors, customers,
          certified practitioners, vendors, and an ecosystem of open-source modules, plugins,
          libraries, and extensions.

          Then, on August 10th, 2023, with little or no advance notice or chance for much, if not all,
          of the community to have any input, HashiCorp switched the license for Terraform from the
          MPL to the Business Source License (v1.1) (the “BSL”), a non-open source license. In our
          opinion, this change threatens the entire community and ecosystem that’s built up around
          Terraform over the last 9 years.
        </p>

        <p>
          Our concern: the BSL license is a poison pill for Terraform.
        </p>

        <p>
          Overnight, tens of thousands of businesses, ranging from one-person shops to the
          Fortune 500, woke up to a new reality where the underpinnings of their infrastructure
          suddenly became a potential legal risk. The BSL and the additional use grant written by
          the HashiCorp team are vague, and now every company, vendor, and developer using Terraform
          has to wonder whether what they are doing could be construed as competitive with HashiCorp’s
          offerings. The FAQ provides some solace for end-customers and systems integrators today,
          but even if you might be in the clear now, how can you build confidence that your usage
          won't violate the license terms in the future? What if your products or HashiCorp's products
          change? What if HashiCorp changes how they interpret competitive? What if they change the
          license again? As a result, everything that uses Terraform is on shaky ground.
        </p>

        <p>
          It is clear to us that under the new license, the thriving ecosystem built up around the
          open source Terraform will dwindle and wither. As developers consider what tools to learn
          and what ecosystems to contribute to, and as companies consider what tools to use to manage
          their infrastructure, more and more, they'll pick alternatives that are genuinely open-source.
          Existing Terraform codebases will turn into outdated liabilities, independent tooling will
          all but disappear, and the community will fracture and disappear.
        </p>

        <p>
          This sort of change also harms all similar open-source projects. Every company and every
          developer now needs to think twice before adopting and investing in an open-source project
          in case the creator suddenly decides to change the license. Imagine if the creators of Linux
          or Kubernetes suddenly switched to a non-open-source license that only permitted
          non-competitive usage.
        </p>

        <p>
          We believe that the essential building blocks of the modern Internet, such as Linux, Kubernetes, 
          and Terraform need to be truly open source: that is the only way to ensure
          that we are building our industry on top of solid and predictable underpinnings.
        </p>

        <p>
          Our goal: ensure Terraform remains truly open source—always.
        </p>

        <p>
          Our aim with this manifesto is to return Terraform to a fully open source license. BSL 
          is <em>not</em> open source, so this would mean moving Terraform back to the MPL license, 
          or some other well-known, widely accepted open source license (e.g., Apache License 2.0). 
          Moreover, we want to be confident that Terraform will always remain open source, so you 
          don't have to worry about another sudden license change putting everything at risk. 
        </p>

        <p>
          Our request to HashiCorp: switch Terraform back to an open source license.
        </p>

        <p>
          We ask HashiCorp to do the right thing by the community: instead of going forward with the
          BSL license change, switch Terraform back to a truly open source license, and commit to keeping
          it that way forever going forward. That way, instead of fracturing the community, we end up with 
          a single, impartial, reliable home for Terraform where the whole community can unite to keep 
          building this amazing ecosystem.
        </p>

        <p>
          Our fallback plan: fork Terraform into a foundation.
        </p>

        <p>
          If HashiCorp is unwilling to switch Terraform back to an open source license, we propose to fork
          the legacy MPL-licensed Terraform and maintain the fork in the foundation. This is similar to how 
          Linux and Kubernetes are managed by foundations (the Linux Foundation and the Cloud Native 
          Computing Foundation, respectively), which are run by multiple companies, ensuring the tool stays 
          truly open source and neutral, and not at the whim of any one company.
        </p>

        <p>
          In particular, we want to create a foundation for Terraform that is:
        </p>

        <ul>
          <li>
            <span>Truly open source</span> - under a well-known and widely-accepted license that companies can trust,
            that won't suddenly change in the future, and isn't subject to the whims of a single vendor
          </li>
          <li>
            <span>Community-driven</span> - so that the community governs the project for the community, where pull
            requests are regularly reviewed and accepted on their merit
          </li>
          <li>
            <span>Impartial</span> - so that valuable features and fixes are accepted based on their value to the community,
            regardless of their impact on any particular vendor
          </li>
          <li>
            <span>Layered and modular</span> - with a programmer-friendly project structure
            to encourage building on top, enabling a new vibrant ecosystem of
            tools and integrations
          </li>
          <li>
            <span>Backwards-compatible</span> - so that the existing code can drive value for years to come
          </li>
        </ul>

        <h2>LIST OF PLEDGING COMPANIES AND PLEDGED RESOURCES:</h2>

        <p>
          We acknowledge that maintaining an open source project such as Terraform takes a considerable investment 
          in terms of time, skill, effort, and coordination. We are grateful to HashiCorp for creating Terraform
          and their leadership in getting it to this point, and to the thousands of community members for their 
          contributions so far. The next step for Terraform must be to remain open source, either by HashiCorp 
          switching it back to a truly open source license or by us forking it into a foundation. Whichever way 
          it turns out, to ensure that there is sufficient investment to grow and evolve Terraform, the 
          signatories below pledge to pool our resources to build a more open, inclusive future 
          for an open source Terraform.
        </p>

        <p>
          If you’re willing to join our cause, please sign the manifesto by
          <a href="https://github.com/opentffoundation/manifesto">creating a
            PR</a> and adding yourself at the bottom of this page and optionally
          let us know how you’d like to help, either as an individual or as an
          organization.
        </p>

        <h2>Pledged Companies</h2>

        <ul>
          <li><a href="https://gruntwork.io/">Gruntwork</a></li>
          <li><a href="https://spacelift.io/">Spacelift</a></li>
          <li><a href="https://env0.com/">env0</a></li>
          <li><a href="https://scalr.com/">Scalr</a></li>
          <li><a href="https://digger.dev/">Digger</a></li>
          <li><a href="https://doppler.com/">Doppler</a></li>
          <li><a href="https://massdriver.cloud/">Massdriver</a></li>
          <li><a href="https://www.qovery.com/">Qovery</a></li>
          <li><a href="https://rivet.gg/">Rivet</a></li>
          <li><a href="https://terramate.io/">Terramate</a></li>
          <li><a href="https://terrateam.io/">Terrateam</a></li>    
          <li><a href="https://verifa.io/">Verifa</a></li>    
          <li><a href="https://finisterra.io/">Finisterra</a></li>
        </ul>

        <h2>Contact us</h2>

        <p>
          If you are a member of the community, a member of the press, an employee of HashiCorp, or anyone else
          with questions or feedback to share, you can reach the team behind this manifesto by emailing us at
          <a href="mailto:pledge@opentf.org">pledge@opentf.org</a>.
        </p>

        <h2>Share</h2>

        
        <p>
          August 14th, 2023
        </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CSS Selectors: A Visual Guide (217 pts)]]></title>
            <link>https://fffuel.co/css-selectors/</link>
            <guid>37132754</guid>
            <pubDate>Tue, 15 Aug 2023 11:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fffuel.co/css-selectors/">https://fffuel.co/css-selectors/</a>, See on <a href="https://news.ycombinator.com/item?id=37132754">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section id="universal"><h2><span>*</span> universal selector</h2><p>Select all elements:</p><div><pre><code>div * {
  background: coral;
}</code></pre></div></section><section id="element"><h2><span>element</span> element selector</h2><p>Select element(s):</p><div><pre><code>p {
  background: deeppink;
}</code></pre></div></section><section id="class"><h2><span>.class</span> class selector</h2><p>Select all elements with the specified class name:</p><div><pre><code>.my-class {
  background: royalblue;
}</code></pre></div></section><section id="id"><h2><span>#id</span> ID selector</h2><p>Select the element with the specified ID:</p><div><pre><code>#my-id {
  background: aquamarine;
}</code></pre></div></section><section id="multiple"><h2><span>.class.class-2</span> multiple selectors</h2><p>Chain two or more classes or IDs to select the element(s) that have all the specified classes/IDs:</p><div><pre><code>.my-class.special {
  background: royalblue;
}</code></pre></div></section><section id="comma"><h2><span>.class, .class-2</span> comma combinator</h2><p>Separate multiple selector declarations using a <strong>comma</strong>. This makes it easy to apply the same styles to multiple selector declarations:</p><div><pre><code>.item-1, .item-2 {
  background: sandybrown;
}</code></pre></div></section><section id="descendant"><h2><span>.class .class-2</span> descendant selector</h2><p>Leave a <strong>space</strong> (<em>descendant combinator</em>) between selectors to select element(s) that are descendant of another element:</p><div><pre><code>.wrapper .card {
  background: lightblue;
}</code></pre></div></section><section id="adjacent"><h2><span>.class + .class-2</span> adjacent selector</h2><p>Use a <strong>plus sign</strong> (<em>adjacent combinator</em>) to select an element that is a direct sibling to a first element:</p><div><pre><code>.item-1 + div {
  background: yellowgreen;
}</code></pre></div></section><section id="child"><h2><span>.class &gt; .class-2</span> child selector</h2><p>Use a <strong>&gt;</strong> sign (<em>child combinator</em>) to select element(s) that are direct children to another element:</p><div><pre><code>.wrapper &gt; div {
  background: olive;
}</code></pre></div></section><section id="subsequent"><h2><span>.class ~ .class-2</span> subsequent selector</h2><p>Use a <strong>tilde sign</strong> (<em>subsequent combinator</em>) to select every element that is preceded by the first element, without having to be a direct sibling to the first element:</p><div><pre><code>.item-1 ~ div {
  background: lightcoral;
}</code></pre></div></section><section id="lobotomized"><h2><span>* + *</span> lobotomized owl</h2><p>A selector pattern where all elements that have a preceding sibling are selected. Use it for example to add spacing to elements within a container except for the first element, which has no preceding sibling:</p><div><pre><code>* + * {
  background: khaki;
}</code></pre></div></section><section id="attribute"><h2><span>[attr]</span> attribute selector</h2><p>Select element(s) that have a specified attribute:</p><div><pre><code>[data-text] {
  background: deepskyblue;
}</code></pre></div></section><section id="attribute-value"><h2><span>[attr=val]</span> attribute &amp; attribute value</h2><p>Select element(s) that have the specified attribute and attribute value:</p><div><pre><code>[data-text="hello"] {
  background: lemonchiffon;
}</code></pre></div></section><section id="attribute-tilde-value"><h2><span>[attr~=val]</span> attribute &amp; one of the attribute's values</h2><p>Select element(s) that have the specified attribute with one of it's space-separated values matching the value:</p><div><pre><code>[title~="old"] {
  background: crimson;
}</code></pre></div></section><section id="attribute-star-value"><h2><span>[attr*=val]</span> attribute &amp; partial value</h2><p>Select element(s) that have the specified attribute with <em>val</em> being included in the attribute value:</p><div><pre><code>[title*="saur"] {
  background: darkgoldenrod;
}</code></pre></div></section><section id="link"><h2><span>:link :visited :hover &amp; :active</span> link pseudo-class selectors</h2><p>These 4 pseudo-classes are useful to select elements such as links in various states. These 4 are most often used with links, but <strong>:active</strong> is also useful for buttons and <strong>:hover</strong> can be used on all kinds of elements:</p><ul><li><strong>:link</strong> - Targets unvisited links. It allows you to style hyperlinks that the user hasn't clicked on yet.</li><li><strong>:visited</strong> - Targets links that have already been visited by the user. This pseudo-class lets you apply styles to previously clicked hyperlinks.</li><li><strong>:hover</strong> - Targets elements (commonly links) when they are being hovered over by the user's pointer, such as a mouse cursor.</li><li><strong>:active</strong> - Targets elements (typically links or buttons) during the moment they are being activated, like when a user clicks on them.</li></ul><div><pre><code>a:link {
  background: aliceblue;
}
a:visited {
  background: blanchedalmond;
}
a:hover {
  background: honeydew;
}
a:active {
  background: lavenderblush;
}</code></pre></div></section><section id="focus"><h2><span>:focus</span> focused input element(s)</h2><p>The <strong>:focus</strong> pseudo-class targets an element when it receives focus, such as when a user clicks on an input field or navigates to it using the keyboard:</p><div><pre><code>input:focus {
  border: 2px solid deepskyblue;
  background: lightcyan;
  outline: none;
  box-shadow: 0 0 8px rgba(0, 191, 255, 0.5);
}</code></pre><p><label for="my-input">Your name: </label></p></div></section><section id="checked"><h2><span>:checked</span> checked input element(s)</h2><p>The <strong>:checked</strong> pseudo-class targets <em>radio buttons, checkboxes, or options</em> in a select element that are currently selected/checked.</p><p>In the following example I make use of <em>appearance: none</em> to remove the default styling of the checkbox input, then use the <strong>:checked</strong> pseudo-class to style the sibling label, while also adding styling on the label when the checkbox is focused:</p><div><pre><code>input[type='checkbox'] {
  /* remove default
     checkbox styles */
  all: unset;
  -webkit-appearance: none;
  appearance: none;
  margin: 0;
}
input[type='checkbox']:checked + label {
  background: mediumseagreen;
}
input[type='checkbox']:focus + label {
  box-shadow: 0 0 0 2px yellow;
}</code></pre><p> <label for="checkbox1">I am a label</label></p></div></section><section id="disabled"><h2><span>:disabled</span> disabled input element(s)</h2><p>The <strong>:disabled</strong> pseudo-class matches form elements like buttons or text inputs that are disabled:</p><div><pre><code>input[type="text"] {
  padding: 10px;
  border: 2px solid mediumslateblue;
  margin-right: 10px;
}

input[type="text"]:disabled {
  background: lightgray;
  border: 2px solid darkgray;
  color: darkgray;
}</code></pre></div></section><section id="enabled"><h2><span>:enabled</span> enabled input element(s)</h2><p>The <strong>:enabled</strong> pseudo-class matches form elements that are interactive and can receive input:</p><div><pre><code>input[type="text"] {
  padding: 10px;
  border: 1px solid gray;
  margin-right: 10px;
}

input[type="text"]:enabled {
  background: lightgreen;
  border: 1px solid green;
}</code></pre></div></section><section id="valid"><h2><span>:valid</span> valid input element(s)</h2><p>The <strong>:valid</strong> pseudo-class is used to target an input element that has content that matches the requirements as specified by its attributes (like <em>pattern</em>, <em>type</em>, etc.):</p><div><pre><code>input[type="email"]:valid {
  border: 2px solid limegreen;
  background: honeydew;
}</code></pre><p><label>Email: </label></p></div></section><section id="invalid"><h2><span>:invalid</span> invalid input element(s)</h2><p>The <strong>:invalid</strong> pseudo-class is used to target input elements that have content that doesn't match the requirements:</p><div><pre><code>input[type="email"]:invalid {
  border: 2px solid tomato;
  background: mistyrose;
}</code></pre><p><label>Email: </label></p></div></section><section id="required"><h2><span>:required</span> required input element(s)</h2><p>The <strong>:required</strong> pseudo-class targets input elements that have the <em>required</em> attribute, indicating that they must be filled out before the form can be submitted:</p><div><pre><code>input:required {
  border: 2px dotted orangered;
  background: floralwhite;
  box-shadow: 0 0 10px rgba(255, 69, 0, 0.2);
}</code></pre><p><label>Name (required): </label> <label>Optional Field: </label></p></div></section><section id="optional"><h2><span>:optional</span> optional input element(s)</h2><p>The <strong>:optional</strong> pseudo-class targets input elements that do not have the <em>required</em> attribute, implying that they're not mandatory to fill out:</p><div><pre><code>input:optional {
  border: 2px dotted darkgray;
  background: whitesmoke;
  box-shadow: 0 0 10px rgba(105, 105, 105, 0.2);
}</code></pre><p><label>Name (required): </label> <label>Optional Field: </label></p></div></section><section id="first-child"><h2><span>:first-child</span> first child element</h2><p>The <strong>:first-child</strong> pseudo-class targets the first child element within its parent:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:first-child {
  background: lightblue;
  border-color: deepskyblue;
  border-style: solid;
}</code></pre><div><p>First Child</p><p>Second Child</p><p>Third Child</p></div></div></section><section id="last-child"><h2><span>:last-child</span> last child element</h2><p>The <strong>:last-child</strong> pseudo-class targets the last child element within its parent:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:last-child {
  background: lightblue;
  border-color: deepskyblue;
  border-style: solid;
}</code></pre><div><p>First Child</p><p>Second Child</p><p>Last Child</p></div></div></section><section id="nth-child"><h2><span>:nth-child</span> nth child element</h2><p>The <strong>:nth-child</strong> pseudo-class targets elements based on their position within their parent, allowing for a wide variety of selections:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:nth-child(2) {
  background: lightcoral;
  border-color: darkred;
  border-style: solid;
}</code></pre><div><p>First Child</p><p>Second Child</p><p>Last Child</p></div></div></section><section id="nth-last-child"><h2><span>:nth-last-child</span> nth child element, counting backwards from last</h2><p>The <strong>:nth-last-child</strong> pseudo-class is similar to :nth-child, but it counts from the last child backwards:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:nth-last-child(2) {
  background: darkorchid;
  border-color: indigo;
  color: white;
  border-style: solid;
}</code></pre><div><p>First Child</p><p>Second Child</p><p>Third Child</p><p>Last Child</p></div></div></section><section id="only-child"><h2><span>:only-child</span> only child of an element</h2><p>The <strong>:only-child</strong> pseudo-class targets an element if it's the only child element of its parent:</p><div><pre><code>div {
  border: 1px dotted gray;
}

div:only-child {
  background: lightsalmon;
  border-color: darksalmon;
  border-style: solid;
}</code></pre><div><div><p>1st Child</p><p>Inner child 1</p><p>Inner child 2</p></div><div><p>2nd Child</p><p>Only child of '2nd Child'</p></div></div></div></section><section id="first-of-type"><h2><span>:first-of-type</span> first element of a type</h2><p>The <strong>:first-of-type</strong> pseudo-class targets the first element of its type within its parent:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

div:first-of-type {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p><p>First div</p><p>Second div</p></div></div></section><section id="last-of-type"><h2><span>:last-of-type</span> last element of a type</h2><p>The <strong>:last-of-type</strong> pseudo-class targets the last element of its type within a parent:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

p:last-of-type {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p><p>First div</p><p>Second div</p></div></div></section><section id="nth-of-type"><h2><span>:nth-of-type</span> nth element of a type</h2><p>The <strong>:nth-of-type</strong> pseudo-class matches elements based on their type and position among siblings:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

p:nth-of-type(3) {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p><p>Third paragraph</p><p>First div</p><p>Second div</p><p>Third div</p></div></div></section><section id="nth-last-of-type"><h2><span>:nth-last-of-type</span> nth element of type, counting backwards</h2><p>The <strong>:nth-last-of-type</strong> pseudo-class matches elements based on their type and position among siblings, but counting from the end:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

div:nth-last-of-type(3) {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p><p>Third paragraph</p><p>First div</p><p>Second div</p><p>Third div</p></div></div></section><section id="only-of-type"><h2><span>:only-of-type</span> only element of its type</h2><p>The <strong>:only-of-type</strong> pseudo-class targets an element that is the only element of its type among its siblings:</p><div><pre><code>p, div {
  border: 1px dotted gray;
}

:only-of-type {
  background: lightyellow;
  border-color: gold;
  color: darkorange;
  border-style: solid;
}</code></pre><div><p>First paragraph</p><p>Second paragraph</p></div></div></section><section id="target"><h2><span>:target</span> target element selector</h2><p>The <strong>:target</strong> pseudo-class selects an element with an ID attribute matching the URL fragment (eg: <em>https://example.com/#fragment</em>).</p><p><strong>:target</strong> is often used to style sections of a page that are directly linked to, typically used with in-page links:</p><div><pre><code>div:target {
  border: 3px solid deepskyblue;
  background-color: lightcyan;
  transform: scale(1.05);
}</code></pre><div><p><a href="#section1">Go to Section 1</a> <a href="#section2">Go to Section 2</a></p><p>Section 1 content</p><p>Section 2 content</p></div></div></section><section id="not"><h2><span>:not()</span> negation pseudo-class</h2><p>The <strong>:not()</strong> functional pseudo-class allows you to target elements that do not match a specified selector or condition. It's essentially an exclusion filter:</p><div><pre><code>div:not(.exclude) {
  border: 2px solid royalblue;
  background: aliceblue;
}</code></pre><div><p>This div is targeted</p><p>This div has the '.exclude' class</p><p>Another targeted div</p></div></div></section><section id="has"><h2><span>:has()</span> parent selector</h2><p>The <strong>:has()</strong> functional pseudo-class allows to style an element if it contains a certain element or another selector:</p><div><pre><code>div:has(p.special) {
  border: 2px solid darkkhaki;
  background-color: peachpuff;
}</code></pre><div><p>Regular paragraph.</p><p>This paragraph has a the '.special' class, so its parent div is styled!</p><p>Another regular paragraph.</p></div></div></section><section id="before"><h2><span>::before</span> first child pseudo-element</h2><p>The <strong>::before</strong> pseudo-element is used to insert content before the content of an element. It can be used to add decorative content, icons, or other elements that don't need to be in the actual DOM:</p><div><pre><code>.alert::before {
  content: '⚠️ ';
  margin-right: 0.25rem;
}</code></pre></div></section><section id="after"><h2><span>::after</span> last child pseudo-element</h2><p>The <strong>::after</strong> pseudo-element is similar to <em>::before</em> and is used to insert content after the content of an element:</p><div><pre><code>.info::after {
  content: '';
  display: inline-block;
  width: 0.75rem;
  height: 0.75rem;
  border-radius: 35%;
  background: darkseagreen;
  margin-left: 0.35rem;
  rotate: 45deg;
  vertical-align: middle;
}</code></pre></div></section><section id="first-letter"><h2><span>::first-letter</span> first letter pseudo-element</h2><p>The <strong>::first-letter</strong> pseudo-element is used to style the first letter of a block-level element, allowing for design elements like drop caps:</p><div><pre><code>p::first-letter {
  font-size: 2em;
  font-weight: bold;
  float: left;
  color: crimson;
}</code></pre><p>Once upon a time, in a land far, far away, there lived a curious coder on a quest for knowledge.</p></div></section><section id="first-line"><h2><span>::first-line</span> first line pseudo-element</h2><p>The <strong>::first-line</strong> pseudo-element is used to style the first line of a block-level element. This allows for typographic effects that can adapt dynamically based on the size of the containing element and the font size:</p><div><pre><code>p::first-line {
  font-weight: bold;
  color: darkslategray;
  text-transform: uppercase;
}</code></pre><p>It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness...</p></div></section><section id="placeholder"><h2><span>::placeholder</span> text input placeholder</h2><p>The <strong>::placeholder</strong> pseudo-element is used to style the placeholder text of form fields like <em>&lt;input&gt;</em> and <em>&lt;textarea&gt;</em>:</p><div><pre><code>input::placeholder {
  font-style: italic;
  color: thistle;
  opacity: 0.7;
}</code></pre></div></section><section id="selection"><h2><span>::selection</span> style highlighted box</h2><p>The <strong>::placeholder</strong> pseudo-element is used to style the portion of an element that has been highlighted or selected by the user. For instance, when a user clicks and drags to select text, the <em>::selection</em> pseudo-element can be used to modify the background color, text color, and other styles of that selection:</p><div><pre><code>::selection {
  background-color: gold;
  color: darkblue;
}</code></pre><p>Click and drag to select this text to see the custom selection styling in action.</p></div></section><section id="marker"><h2><span>::marker</span> list marker pseudo-element</h2><p>The <strong>::placeholder</strong> pseudo-element is used to style marker boxes in list items, which typically contain bullets (for unordered lists) or numbers/letters (for ordered lists).</p><p>Before the introduction of the <em>::marker</em> pseudo-element, customizing these markers often required workarounds, but this pseudo-element gives us a bit more control:</p><div><pre><code>li::marker {
  color: LightSeaGreen;
  font-size: 1.5em;
  font-weight: bold;
}</code></pre><div><ul><li>Apples 🍎</li><li>Bananas 🍌</li><li>Cherries 🍒</li></ul></div></div></section><svg width="300" viewBox="0 0 687 155" xmlns="http://www.w3.org/2000/svg"><g stroke="currentColor" stroke-width="7" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"><path d="M20 58c27-13.33333333 54-20 81-20 40.5 0 40.5 20 81 20s40.626917-20 81-20 40.123083 20 80.5 20 40.5-20 81-20 40.5 20 81 20 40.626917-20 81-20c26.915389 0 53.748722 6.66666667 80.5 20" opacity=".1"></path><path d="M20 78c27-13.3333333 54-20 81-20 40.5 0 40.5 20 81 20s40.626917-20 81-20 40.123083 20 80.5 20 40.5-20 81-20 40.5 20 81 20 40.626917-20 81-20c26.915389 0 53.748722 6.6666667 80.5 20" opacity=".2"></path><path d="M20 98c27-13.3333333 54-20 81-20 40.5 0 40.5 20 81 20s40.626917-20 81-20 40.123083 20 80.5 20 40.5-20 81-20 40.5 20 81 20 40.626917-20 81-20c26.915389 0 53.748722 6.6666667 80.5 20" opacity=".6"></path><path d="M20 118c27-13.3333333 54-20 81-20 40.5 0 40.5 20 81 20s40.626917-20 81-20 40.123083 20 80.5 20 40.5-20 81-20 40.5 20 81 20 40.626917-20 81-20c26.915389 0 53.748722 6.6666667 80.5 20"></path></g></svg><section id="more"><h2>More pseudo-classes</h2><p>Here's some additional pseudo-classes that are available in CSS:</p><ul><li><strong>:root:</strong> Targets the highest-level parent element in a document, typically the <em>&lt;html&gt;</em> element in HTML documents. Useful to define CSS variables that will be available to all elements within the page.</li><li><strong>:is():</strong> Matches elements that can be one of several selectors, making long selector lists shorter and easier to read. For example, <em>:is(h1, h2, h3)</em> would match any of those three heading elements.</li><li><strong>:where():</strong> Similar to <em>:is()</em>, but allows for selecting elements based on conditions without affecting the specificity of the selector.</li><li><strong>:default:</strong> Matches UI elements (like radio buttons or checkboxes) that are set to their default selection state.</li><li><strong>:empty:</strong> Selects elements that have no children (including text nodes).</li><li><strong>:fullscreen:</strong> Targets elements that are currently displayed in fullscreen mode.</li><li><strong>:in-range:</strong> Matches form elements with a value that is within the specified range (using attributes like <em>min</em> and <em>max</em>).</li><li><strong>:out-of-range:</strong> Matches form elements with a value that is outside the specified range.</li><li><strong>:indeterminate:</strong> Targets form elements whose state is uncertain, such as a checkbox that's neither checked nor unchecked (often seen in tree-view structures).</li><li><strong>:read-only:</strong> Matches form elements that are not editable by the user, due to the <em>readonly</em> attribute.</li><li><strong>:read-write:</strong> Targets form elements that are editable by the user, implying they are not <em>readonly</em>.</li><li><strong>:lang()</strong>: Matches elements based on their language attribute. E.g., <em>:lang(en)</em> selects elements defined in English.</li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPU-Accelerated LLM on an Orange Pi (156 pts)]]></title>
            <link>https://blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi</link>
            <guid>37132209</guid>
            <pubDate>Tue, 15 Aug 2023 10:30:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi">https://blog.mlc.ai/2023/08/09/GPU-Accelerated-LLM-on-Orange-Pi</a>, See on <a href="https://news.ycombinator.com/item?id=37132209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>
        <time datetime="2023-08-09T13:30:00+00:00" itemprop="datePublished">
          Aug 9, 2023
        </time>
        
        • 
        
      </p>
      
    <br>
    <h2 id="tldr">TL;DR</h2>

<p>This post shows GPU-accelerated LLM running smoothly on an embedded device at a reasonable speed. More specifically, on a $100 Orange Pi 5 with Mali GPU, we achieve 2.5 tok/sec for Llama2-7b and 5 tok/sec for RedPajama-3b through Machine Learning Compilation (MLC) techniques. Additionally, we are able to run a Llama-2 13b model at 1.5 tok/sec on a 16GB version of the Orange Pi 5+ under $150.</p>

<p>
  <img src="https://blog.mlc.ai/img/orange-pi/orange-pi.jpg" width="90%">
</p>

<h2 id="background">Background</h2>

<p>Progress in open language models has been catalyzing innovation across question-answering, translation, and creative tasks. While current solutions demand high-end desktop GPUs to achieve satisfactory performance, to unleash LLMs for everyday use, we wanted to understand how usable we could deploy them on the affordable embedded devices.</p>

<p>Many embedded devices come with mobile GPUs that can serve as a source of acceleration. In this post, we pick Orange Pi 5, a RK35888-based board that is similar to Raspberry Pi but also features a more powerful Mali-G610 GPU. This post summarizes our first attempt at leveraging Machine Learning Compilation and provides out-of-box GPU acceleration for this device.</p>

<h2 id="machine-learning-compilation-for-mali">Machine Learning Compilation for Mali</h2>

<p>
  <img src="https://blog.mlc.ai/img/orange-pi/WebXYZ%20Images.svg" width="90%">
</p>

<p>Machine learning compilation (MLC) is an emerging technology that automatically compiles and optimizes machine learning workloads, and deploys the compiled workload to a broad set of backends. At the time of writing, based on Apache TVM Unity, MLC supports platforms including browsers (WebGPU, WASM), NVIDIA GPUs (CUDA), AMD GPUs (ROCm, Vulkan), Intel GPUs (Vulkan), iOS and MacBooks (Metal), Android (OpenCL), and Mali GPUs (this post).</p>

<h3 id="generalizable-ml-compilation-for-mali-codegen">Generalizable ML Compilation for Mali Codegen</h3>

<p>MLC is built on top of  Apache TVM Unity, a generalizable stack for compiling machine learning models across different hardwares and backends. To compile LLMs onto Mali GPUs, we reuse all the existing compilation pipeline without any code optimizations. More specifically, we successfully deployed Llama-2 and RedPajama models with the following steps:</p>

<ul>
  <li>Reuse model optimization passes, including quantization, fusion, layout optimization, etc;</li>
  <li>Reuse a generic GPU kernel optimization space written in TVM TensorIR and re-target it to Mali GPUs;</li>
  <li>Reuse OpenCL codegen backend from TVM, and re-target it to Mali GPUs;</li>
  <li>Reuse the existing user interface, including Python APIs, CLI, and REST APIs.</li>
</ul>

<h2 id="try-it-out">Try it out</h2>

<p>This section provides a step-by-step guide so that you can try it out on your own orange pi device. Here we use <code>RedPajama-INCITE-Chat-3B-v1-q4f16_1</code> as the running example. You can replace that by <code>​​Llama-2-7b-chat-hf-q4f16_1</code> or <code>​​Llama-2-13b-chat-hf-q4f16_1</code> (requires a 16GB board).</p>

<h3 id="prepare">Prepare</h3>

<p>Please first follow the instruction <a href="https://mlc.ai/mlc-llm/docs/install/gpu.html#orange-pi-5-rk3588-based-sbc">here</a>, to setup the RK3588 board with OpenCL driver. Then clone the MLC-LLM from the source, and download weights and prebuilt libs.</p>

<div><pre><code><span># clone mlc-llm from GitHub</span>
git clone <span>--recursive</span> https://github.com/mlc-ai/mlc-llm.git <span>&amp;&amp;</span> <span>cd </span>mlc-llm
<span># Download prebuilt weights and libs</span>
git lfs <span>install
mkdir</span> <span>-p</span> dist/prebuilt <span>&amp;&amp;</span> <span>cd </span>dist/prebuilt
git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git lib
git clone https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1
<span>cd</span> ../../..
</code></pre></div>

<h3 id="try-out-the-cli">Try out the CLI</h3>

<p>Build mlc_llm_cli from the source code</p>

<div><pre><code><span>cd </span>mlc-llm/
<span># create build directory</span>
<span>mkdir</span> <span>-p</span> build <span>&amp;&amp;</span> <span>cd </span>build
<span># generate build configuration</span>
python3 ../cmake/gen_cmake_config.py
<span># build `mlc_chat_cli`</span>
cmake .. <span>&amp;&amp;</span> cmake <span>--build</span> <span>.</span> <span>--parallel</span> <span>$(</span><span>nproc</span><span>)</span> <span>&amp;&amp;</span> <span>cd</span> ..
</code></pre></div>

<p>Verify installation</p>

<div><pre><code><span># expected to see `mlc_chat_cli`, `libmlc_llm.so` and `libtvm_runtime.so`</span>
<span>ls</span> <span>-l</span> ./build/
<span># expected to see help message</span>
./build/mlc_chat_cli <span>--help</span>
</code></pre></div>

<p>Run LLMs through mlc_chat_cli</p>

<div><pre><code>./build/mlc_chat_cli <span>--local-id</span> RedPajama-INCITE-Chat-3B-v1-q4f16_1 –device mali
</code></pre></div>

<p>
  <img src="https://blog.mlc.ai/img/orange-pi/cli.png" width="90%">
</p>

<h3 id="try-out-the-python-api">Try out the Python API</h3>

<p>Build TVM runtime</p>

<div><pre><code><span># clone from GitHub</span>
git clone <span>--recursive</span> https://github.com/mlc-ai/relax.git tvm_unity <span>&amp;&amp;</span> <span>cd </span>tvm_unity/
<span># create build directory</span>
<span>mkdir</span> <span>-p</span> build <span>&amp;&amp;</span> <span>cd </span>build
<span># generate build configuration</span>
<span>cp</span> ../cmake/config.cmake <span>.</span> <span>&amp;&amp;</span> <span>echo</span> <span>"set(CMAKE_BUILD_TYPE RelWithDebInfo)</span><span>\n</span><span>set(USE_OPENCL ON)"</span> <span>&gt;&gt;</span> config.cmake
<span># build `mlc_chat_cli`</span>
cmake .. <span>&amp;&amp;</span> cmake <span>--build</span> <span>.</span> <span>--target</span> runtime <span>--parallel</span> <span>$(</span><span>nproc</span><span>)</span> <span>&amp;&amp;</span> <span>cd</span> ../..
</code></pre></div>

<p>Setup python path (please set it to the <code>bashrc</code> or <code>zshrc</code> for persistent settings)</p>

<div><pre><code><span>export </span><span>TVM_HOME</span><span>=</span><span>$(</span><span>pwd</span><span>)</span>/tvm_unity
<span>export </span><span>MLC_LLM_HOME</span><span>=</span><span>$(</span><span>pwd</span><span>)</span>/mlc-llm
<span>export </span><span>PYTHONPATH</span><span>=</span><span>$TVM_HOME</span>/python:<span>$MLC_LLM_HOME</span>/python:<span>${</span><span>PYTHONPATH</span><span>}</span>
</code></pre></div>

<p>Run the following python script.</p>

<div><pre><code><span>from</span> <span>mlc_chat</span> <span>import</span> <span>ChatModule</span>
<span>from</span> <span>mlc_chat.callback</span> <span>import</span> <span>StreamToStdout</span>
<span>cm</span> <span>=</span> <span>ChatModule</span><span>(</span><span>model</span><span>=</span><span>"RedPajama-INCITE-Chat-3B-v1-q4f16_1"</span><span>)</span>

<span># Generate a response for a given prompt
</span><span>output</span> <span>=</span> <span>cm</span><span>.</span><span>generate</span><span>(</span>
   <span>prompt</span><span>=</span><span>"What is the meaning of life?"</span><span>,</span>
   <span>progress_callback</span><span>=</span><span>StreamToStdout</span><span>(</span><span>callback_interval</span><span>=</span><span>2</span><span>),</span>
<span>)</span>

<span># Print prefill and decode performance statistics
</span><span>print</span><span>(</span><span>f</span><span>"Statistics: </span><span>{</span><span>cm</span><span>.</span><span>stats</span><span>()</span><span>}</span><span>\n</span><span>"</span><span>)</span>
</code></pre></div>

<h2 id="discussion-and-future-work">Discussion and Future Work</h2>

<p>Our current experiments show that 3B models might be a sweet spot. The RedPajama-3B model can provide up to 5 tok/sec and a decent chat experience. There is also room for improvements, specifically around the integer-to-float conversions. Moving forward, we will address the related issues and improve Mali GPUs’ performance.</p>

<p>This post contributes to our quest to integrate LLMs into affordable devices and bring AI to everyone. Our future endeavors will focus on harnessing advancements in single-board computers, refining software frameworks like OpenCL and MLC-LLM, and exploring broader applications such as smart home devices. Collaborative efforts in the open-source community and a commitment to continuous learning and adaptation will be pivotal in navigating the evolving landscape of LLM deployment on emerging hardware.</p>

<h2 id="contributions">Contributions</h2>

<p>LLM on Orange Pi is primarily completed by <a href="https://www.linkedin.com/in/haolin-zhang-534530231/">Haolin Zhang</a>. The support of mali optimizations comes from Siyuan Feng, with foundation support from Junru Shao and Bohan Hou and other community members.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Privacy friendly ESP32 smart doorbell with Home Assistant local integration (174 pts)]]></title>
            <link>https://tristam.ie/2023/758/</link>
            <guid>37131957</guid>
            <pubDate>Tue, 15 Aug 2023 09:46:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tristam.ie/2023/758/">https://tristam.ie/2023/758/</a>, See on <a href="https://news.ycombinator.com/item?id=37131957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p id="bkmrk-privacy-focused-%22sma">Privacy focused “smart” doorbells seem to be few and far between so I decided to build one that integrates with Home Assistant via ESPHome and is easy to build.</p>



<p id="bkmrk-this-project-is-aime">This project is aimed at being simple while allowing a ton of customisation and flexibility. To get started, you’ll need an instance of <a rel="noreferrer noopener" href="https://www.home-assistant.io/" target="_blank">Home Assistant</a> running with the <a rel="noreferrer noopener" href="https://esphome.io/index.html" target="_blank">ESPHome</a> add-on as well as the Home Assistant companion app on your mobile phone to receive notifications when someone presses the doorbell button.</p>



<p>I have used an 8 RGB LED ring light in my version but if you want to simplify things, you can skip this and use the ESP32-CAM’s built in LED as a flash – it’s surprisingly bright.</p>



<h4 id="bkmrk-parts"><strong>Parts list:</strong></h4>



<ul id="bkmrk-esp32-cam-%28amazon---">
<li>ESP32-CAM (Amazon – <a rel="noreferrer noopener" href="https://amzn.to/3KoP1x1" target="_blank">US</a>, <a rel="noreferrer noopener" href="https://amzn.to/442cUBH" target="_blank">UK</a>, <a href="https://amzn.to/3DPKhNn" target="_blank" rel="noreferrer noopener">DE</a>) Make sure to get one with a “flash/download/io0” button to make your life easier when you flash ESPHome onto it for the first time. If you make the same mistake as me and buy the one without that button, follow <a rel="noreferrer noopener" href="https://hagensieker.com/2021/10/03/esp32-cam-tiny-live-stream-camera/" target="_blank">this guide</a> to flash the ESP32-CAM using an FTDI adapter.</li>



<li>Momentary push button (Amazon –&nbsp;<a rel="noreferrer noopener" href="https://amzn.to/3Yfktna" target="_blank">US</a>, <a href="https://amzn.to/3DO0EtP" data-type="link" data-id="https://amzn.to/3DO0EtP" target="_blank" rel="noreferrer noopener">UK</a>, <a rel="noreferrer noopener" href="https://amzn.to/44RJ3gs" target="_blank">DE</a>)</li>



<li>10k resistor</li>



<li>8&nbsp; RGB LED ring light (Amazon – <a href="https://amzn.to/3KizvTx" target="_blank" rel="noreferrer noopener">US</a>, <a href="https://amzn.to/3OBUinp" target="_blank" rel="noreferrer noopener">DE</a>) Note: these aren’t the exact ones that I used but they are the closest ones that I could find. I used the Pi Supply PIS-1270 from <a href="https://ie.rs-online.com/web/p/led-development-tools/2011639" target="_blank" rel="noreferrer noopener">RS Components</a>.</li>



<li>10m Micro USB cable (Amazon –&nbsp;<a rel="noreferrer noopener" href="https://amzn.to/3rIiUlt" target="_blank">US</a>, <a href="https://amzn.to/3OORUtF" target="_blank" rel="noreferrer noopener">UK</a>, <a rel="noreferrer noopener" href="https://amzn.to/457hjnD" target="_blank">DE</a>)</li>



<li>M2.5 brass inserts (Amazon – <a rel="noreferrer noopener" href="https://amzn.to/3VYSZ4i" target="_blank">US</a>, <a rel="noreferrer noopener" href="https://amzn.to/455KYOz" target="_blank">UK</a>, <a href="https://amzn.to/3sazoD6" target="_blank" rel="noreferrer noopener">DE</a>) </li>



<li>M2.5 screws (Amazon – <a href="https://amzn.to/3IIuAuG">US</a>, <a href="https://amzn.to/45iVkdM" target="_blank" rel="noreferrer noopener">UK</a>, <a href="https://amzn.to/3lUMHVs">DE</a>)</li>



<li>eSUN white PETG filament (Amazon –&nbsp;<a rel="noreferrer noopener" href="https://amzn.to/45aTyes" target="_blank">US</a>, <a href="https://amzn.to/3OtHTAN" target="_blank" rel="noreferrer noopener">UK</a>, <a rel="noreferrer noopener" href="https://amzn.to/45aTyes" target="_blank">DE</a>)&nbsp;</li>
</ul>



<p>You can find the .stl’s on Printables <a rel="noreferrer noopener" href="https://www.printables.com/model/542900-privacy-friendly-esp32-smart-doorbell-with-home-as" target="_blank">here</a> and the home assistant config in my github repo: <a rel="noreferrer noopener" href="https://github.com/thatguy-za/esp32-cam-doorbell" target="_blank">thatguy-za/esp32-cam-doorbell</a>.</p>



<h4 id="bkmrk-build-guide"><strong>Build guide</strong></h4>



<h5 id="bkmrk-step-1---printing-th"><strong>Step 1 – Printing the enclosure</strong></h5>



<p id="bkmrk-this-step-takes-the-">This step takes the longest so lets send the .stl’s to the printer while we crack on with the rest of the build. There are three pieces that you’ll need to print:<br>1. The main body<br>2. The ESP32-CAM retention plate<br>3. The back plate/wall mount</p>



<p id="bkmrk-you%27ll-need-to-print">You’ll need to print the front and the back of the enclosure with supports. I printed it using PLA but you’ll want to use PETG or ABS filament so it is waterproof and use 20-30% infill.&nbsp;</p>



<p id="bkmrk-once-the-enclosure-h">Once everything has printed, you’ll need to add two M2.5 threaded inserts:<br>1. Into the front cover so you can screw the ESP32-CAM retention bracket into it.<br>1. Into the bottom of the backplate so you can screw on the face plate with a 10mm M2.5 screw</p>







<h5 id="bkmrk-step-1---adding-the-"><strong>Step 2 – Configuring the ESP32-CAM in ESPHome</strong></h5>



<p id="bkmrk-it%27s-easiest-to-do-t">Hold down the&nbsp;<strong>“flash/download/io0” button</strong> and connect your ESP32-CAM to your computer using a micro USB cable.&nbsp; This will boot it into flashing mode.</p>



<p id="bkmrk-head-over-to-your-in">Launch Google Chrome, go to your instance of Home Assistant and launch the ESPHome Add-on by clicking&nbsp;<strong>Settings -&gt; Add-ons -&gt; ESPHome -&gt; Open Web UI</strong>. Chrome is important because it seems to be the most reliable browser for flashing firmware onto the ESP32-CAM.</p>



<p id="bkmrk-click-%2B-new-device-t">Click <strong>+ New Device</strong> to add a new device.Give it a name (“Doorbell” is probably a good starting point).</p>



<p id="bkmrk-when-asked-to-select">When asked to select the device type, select <strong>ESP32 </strong>and check the box <strong>“use recommended settings’</strong>.</p>



<p id="bkmrk-once-the-configurati">Once the configuration has been created, you can skip installing it onto the device – we’ll do that later.</p>



<p id="bkmrk-from-your-list-of-es">From your list of ESPHome devices, click <strong>Edit</strong> on the device that you have just created.</p>



<p id="bkmrk-at-the-bottom-of-the">At the bottom of the yaml file (below <code>captive_portal:</code>), paste the configuration code from my github repository that is linked above.&nbsp;</p>



<p id="bkmrk-click-save-and-insta">Click <strong>Save</strong> and <strong>Install</strong>.</p>



<p id="bkmrk-select-%22plug-into-th">Select&nbsp;<strong>Plug into this computer</strong>.</p>



<p id="bkmrk-click-open-esphome-w">Click <strong>Open ESPHome Web</strong>, this will allow you to flash the firmware onto the device from the web browser. This is where it is important that you are using Google Chrome.</p>



<p id="bkmrk-once-the-firmware-ha">Once the firmware has compiled, you should be able to click <strong>Download Project</strong> – this could take a few minutes.</p>



<p id="bkmrk-head-over-to-esphome">Head over to <strong>ESPHome Web </strong>and follow the prompts to flash the firmware onto your ESP32-CAM.</p>



<p id="bkmrk-once-flashing-is-com">Home Assistant should discover the new device once the new firmware has been flashed onto it – yay! Now you can add whatever entities you want to your dashboard.</p>



<figure><img decoding="async" width="495" height="241" src="https://tristam.ie/wp-content/uploads/2023/08/Screenshot-2023-08-01-153724.png" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/Screenshot-2023-08-01-153724.png 495w, https://tristam.ie/wp-content/uploads/2023/08/Screenshot-2023-08-01-153724-300x146.png 300w" sizes="(max-width: 495px) 100vw, 495px"><figcaption>Screenshot: New device found in Home Assistant.</figcaption></figure>



<h5 id="bkmrk-">Step 3 –&nbsp; Time for some automation &amp; notifications</h5>



<p id="bkmrk--1">We want to create an Automation to take a snapshot from the doorbell’s camera and send it to your mobile phone when someone presses the doorbell button.</p>



<p>Click <strong>Settings -&gt; Automations -&gt; + Create Automation</strong> and then create a new automation from scratch. </p>



<p>Click on the three vertical dots in the top right hand corner of the screen and then click <strong>Edit in YAML</strong></p>



<p>Paste the automation from my github repo (linked above) into the editor and  update entity names for devices such as your mobile phone. </p>



<p>Save the automation and restart Home Assistant so the new automation becomes active.</p>



<p>Here is a summary of how the automation should behave.</p>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1.png" data-slb-active="1" data-slb-asset="1931303768" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="417" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1-1024x417.png" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1-1024x417.png 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1-300x122.png 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1-768x313.png 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_process-1.png 1155w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<h5 id="bkmrk-wiring">Step 4 – Time to wire it up</h5>



<p id="bkmrk-once-the-enclosure-i">Once the enclosure is printed, we can start the final assembly.</p>



<p id="bkmrk-i-tried-to-keep-this">There are a few variants of the ESP32-CAM board, each with slightly different pinouts so double check the pinout on the board you get.</p>



<p id="bkmrk-follow-the-wiring-gu">Follow the wiring guide below. I soldered everything onto the back of the lower PCB (the one with the micro USB port).&nbsp; It’s&nbsp;important to add the 10k ohm pull down resistor between GPIO14 and ground because without it, I noticed GPIO14 was floating high on quite often.&nbsp;</p>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring.png" data-slb-active="1" data-slb-asset="848354910" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="497" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring-1024x497.png" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring-1024x497.png 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring-300x145.png 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring-768x372.png 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_wiring.png 1060w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>This is what it should look like when you’re done. Bonus points for covering the resistor in heatshrink tube.</p>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside.jpg" data-slb-active="1" data-slb-asset="164896066" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="576" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-1024x576.jpg" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-1024x576.jpg 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-300x169.jpg 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-768x432.jpg 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside-1536x864.jpg 1536w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_inside.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Here are some pics of mine before it goes up next to the front door!</p>



<figure>
<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall.jpg" data-slb-active="1" data-slb-asset="2010640426" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="576" data-id="772" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-1024x576.jpg" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-1024x576.jpg 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-300x169.jpg 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-768x432.jpg 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall-1536x864.jpg 1536w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_on_wall.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside.jpg" data-slb-active="1" data-slb-asset="2025733673" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="1024" height="576" data-id="773" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-1024x576.jpg" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-1024x576.jpg 1024w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-300x169.jpg 300w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-768x432.jpg 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside-1536x864.jpg 1536w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<figure><a href="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l.jpg" data-slb-active="1" data-slb-asset="1634067867" data-slb-internal="0" data-slb-group="758"><img decoding="async" width="576" height="1024" data-id="774" src="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-576x1024.jpg" alt="" srcset="https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-576x1024.jpg 576w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-169x300.jpg 169w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-768x1365.jpg 768w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l-864x1536.jpg 864w, https://tristam.ie/wp-content/uploads/2023/08/esp32_doorbell_outside_2l.jpg 1080w" sizes="(max-width: 576px) 100vw, 576px"></a></figure>
</figure>



<div>
<p><em>*The product links in this post may contain affiliate links. Any commission earned is used to keep the servers running and the gin cool.</em></p>



<p><strong>Thanks for making it to the end of the post! Did this article help you or do you like my work? </strong><br>☕<strong><a rel="noreferrer noopener" href="https://www.buymeacoffee.com/tristam" target="_blank">Buy Me a Coffee</a></strong>☕</p>
</div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things you forgot because of React (384 pts)]]></title>
            <link>https://joshcollinsworth.com/blog/antiquated-react</link>
            <guid>37131802</guid>
            <pubDate>Tue, 15 Aug 2023 09:18:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joshcollinsworth.com/blog/antiquated-react">https://joshcollinsworth.com/blog/antiquated-react</a>, See on <a href="https://news.ycombinator.com/item?id=37131802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
	


<main id="main" tabindex="-1"><article><img src="https://d33wubrfki0l68.cloudfront.net/3a018a423858b084f337e9d9b7c167dc2b4849bd/74eac/images/post_images/because-of-react.png" alt="" width="320" height="180">

		

		
		<p><b>Published:</b> August 4, 2023
			<br>
				<b>Updated:</b> August 9, 2023</p>
		
		
		

<h2 id="part-1-an-intro-about-music-defaults-and-bubbles">Part 1: an intro about music, defaults, and bubbles</h2>
<p>Like a lot of people, there was a time when the only music I listened to was whatever was played on my local radio station. (<em>A lot of people over 30 or so, anyway. If this doesn’t sound familiar to you yet, just stick with me for a minute here</em>.) At the time, I was happy with that. It seemed like all I needed.</p>


<p>Looking back, I realize: I naively trusted that anything good inevitably became popular—and therefore, anything worth knowing would eventually come my way on its own.</p>

<p>Eventually, though, <em>other</em> music began to take root in my life. Through new friends and the internet, I became acquainted with new artists, further and further from the things I liked before—or, at least, <em>thought</em> I liked.</p>
<p><em>This</em> music was different. I wasn’t in love with it one week and sick of it the next. Listening to it wasn’t part of an endless cycle.</p>
<p>If anything, it was the <em>opposite</em>; it was music I actually liked and appreciated <em>more</em> the more I listened to it. There was depth to it. Sure, it didn’t have the loud distorted guitars, punch-line lyrics, or sugar-coated melodies I’d enjoyed up until that point. But to my surprise, that actually somehow made it <em>better</em>, not worse.</p>
<p>That’s when I began to realize: maybe I was never really as satisfied as I thought I was.</p>
<p>Maybe my bliss was, in fact, predicated on ignorance.</p>
<h3 id="finding-richness-beyond-the-defaults">Finding richness beyond the defaults</h3>
<p>I suspect you can probably relate to that story, even if it’s not with music specifically.</p>
<p>Most likely, you now count a food or drink you didn’t once like among your favorites. Or, maybe you were surprised to find a movie, book, game, podcast, influencer, or hobby you didn’t expect to like resonated with you.</p>
<p>The details aren’t important; all I’m getting at is:</p>


<p>You’ve probably experienced finding something great beyond the periphery of popular defaults.</p>

<p>Not to sound like a frontend version of a snobby hipster. That’s not my intention. If your idea of a good time is Bud Lites at Olive Garden: cool, pass the breadsticks.</p>
<p>But what I <em>am</em> trying to do is: gently share the idea that <em>maybe</em> you’re shutting yourself off to something great, without even realizing it.</p>
<p>Maybe this whole concept—finding better things beyond familiar boundaries—applies to our tools and workflows just as much as it does any other area of life.</p>
<p>And maybe—just <em>maybe</em>— your current satisfaction comes, at least a little bit, from simply <em>not knowing what you’re missing</em>.</p>
<h3 id="completing-the-analogy-and-acknowledging-its-shortcomings">Completing the analogy, and acknowledging its shortcomings</h3>
<p>I’ve written before about how <a href="https://reactjs.org/" rel="nofollow">React</a> is <a href="https://joshcollinsworth.com/blog/self-fulfilling-prophecy-of-react">the new default</a> frontend framework, and how I don’t think most people using React on a regular basis realize quite how much it’s fallen behind.</p>
<p>And on that note, this is where our analogy begins to fall short.</p>
<p>Assuming we were only talking about personal preferences, I’d never write a blog post arguing about what you like, or trying to change your mind. (Not at this age, anyway.) Who cares? If you enjoy it, have fun.</p>


<p>But unlike music or other subjective things meant for our own enjoyment, our choice of frontend tools has empirical, measurable effects on others.</p>

<p>That decision carries a real responsibility. It’s not just about what we like. When it comes to development—unless we’re building things purely for ourselves, anyway—our enjoyment is secondary; the user’s experience is what matters most.</p>
<p>If you love your tools, that’s wonderful. I hope you do. But that’s a side quest at best, and a potentially harmful distraction at worst. Developer experience (DX) shouldn’t ever supersede user experience (UX).</p>
<p>So forgive me for choosing a flimsy metaphor. You can keep listening to the same music for the rest of your life, if you want to. I support that. But we have very valid and important reasons to push beyond the comfort of our existing preferences when it comes to the tools we use.</p>
<h3 id="the-react-bubble">The React bubble</h3>
<p>The idea that React lags behind its peers might be new to you. Like many, you might still consider React the modern standard in frontend. So let’s quickly poke at that bubble, in this one last section before we get into the titular list.</p>
<p>This, from <a href="https://toot.cafe/@slightlyoff" rel="nofollow">Alex Russell, via Mastodon</a>, is what started me writing this post:</p>
<blockquote><p>Someone asked me today if there was a case for using React in a new app that doesn’t need to support IE.</p>
<p>I could not come up with a single reason…</p>
<p>It’s astonishing how antiquated React is.</p></blockquote>
<p>Alex mentions React’s lack of support for web components in that thread. That feature has been glaringly missing from React for years. And yes, it’s “on the roadmap.” As of this writing, though, there’s no firm commitment to either an implementation or an expected ship date.</p>


<p>Meanwhile, pretty much all of React’s contemporaries—any framework or technology you might choose instead of React—already have that story shipped and in production.</p>

<p>Web components are one thing. But they’re far from the only item on the list of “stuff everything else does already and/or better.” (We’ll cover several others below.)</p>
<p>React benefitted mightily from being early to the framework game; it set the standard. But that’s come with severe drawbacks in agility and adaptability. Every decision React’s made since its inception circa 2013 is another layer of tech debt—one that its newer contemporaries aren’t constrained by.</p>
<p>To <a href="https://toot.cafe/@slightlyoff/110512849934452558" rel="nofollow">quote Alex once more</a>:</p>
<blockquote><p>React is ‘13 tech designed to ‘08 constraints. Nothing about it is innovative in 2023; in fact, it’s the slowest way to get functional-reactive frontend programming in the modern era…</p></blockquote>
<p><a href="https://joshcollinsworth.com/blog/self-fulfilling-prophecy-of-react">React has aged, and how I don’t think most people realize how much or how poorly</a>. So to put the quote above another way (and tie it back to our intro about music):</p>


<p>React was designed seven Taylor Swift albums ago, for a world where John Mayer and Jennifer Aniston were still dating.</p>

<p>(<em>Seven</em> new <em>Taylor Swift albums ago, that is. That doesn’t even count the</em> Taylor’s Version <em>releases</em>.)</p>
<p>So if you’re one of the many developers whose whole world has been React for the past few years, there might be things you’ve forgotten—or never knew at all—because you’ve been using React for so long.</p>
<p>As fast as modern frontend moves, we seem to be very slow in realizing the world which crowned React king, in many ways, no longer exists. (<em>If it ever did; not many organizations had anything resembling Facebook’s specific set of problems to begin with</em>.)</p>
<p>Browsers have seen <em>wild</em> growth in new feature adoption in the last ten years, in both JavaScript and CSS. Technology and user expectations have evolved, and the current ecosystem of tools has done a <em>lot</em> more than you might think to iterate and adapt past React, in ways such legacy software can’t.</p>
<p>I realize calling React “legacy software” is controversial, but I think it’s fair; it’s comparatively complicated, relatively old, contains a lot of rules and gotchas, beginners are often afraid of it, and the architectural decisions it’s built on top of have long since become an impediment to its ability to iterate.</p>
<hr>
<p>If I haven’t completely alienated you yet by this point (<em>with some combination of quasi-elitism, rambling preamble, and overuse of parenthetical interjections</em>), I’d like to share some things you might have missed if your head’s been entirely in the React world for a while, in the hopes of introducing you to some tunes you might be surprised to find are better than what’s on your current playlist.</p>
<h2 id="part-2-things-you-forgot-or-never-knew-because-of-react">Part 2: things you forgot (or never knew) because of React</h2>

<p>I’ve touched on this in <a href="https://joshcollinsworth.com/blog/self-fulfilling-prophecy-of-react#community-and-support">other posts</a>, but any time an “unproven” framework’s name comes up as a potential tool for a dev project, the first question anybody seems to care about is: <em>how big is the ecosystem</em>?</p>
<p>You might have even had that thought as soon as you read the premise of this post. <em>Move from React to another framework? Are any of them big enough yet?</em></p>
<p>Why do we have this obsession with ecosystem size?</p>
<p>Sure, we want to be certain this framework won’t just vanish on us, or stop being maintained in a few years. That’s perfectly valid. And yes, we wouldn’t bet the farm on something <em>too</em> new or unproven. But <a href="https://vuejs.org/" rel="nofollow">Vue</a>, <a href="https://svelte.dev/" rel="nofollow">Svelte</a>, <a href="https://preactjs.com/" rel="nofollow">Preact</a>, <a href="https://www.solidjs.com/" rel="nofollow">Solid</a>, <a href="https://astro.build/" rel="nofollow">Astro</a>, and others are all <em>far</em> past that point, well-supported and well-maintained. So it clearly isn’t just that.</p>
<p>So what <em>is</em> the sticking point? I have a theory:</p>
<p>We’ve been trained that packages need to be built <em>specifically for our framework</em>.</p>
<p>You could reasonably argue that mindset started with jQuery, but I think React accelerated it.</p>
<p>With React, any time we needed a module or a widget or a library to do something specific (a carousel, a map, an accordion, or whatever else), <em>it had to be a React thing</em>; a plain web thing or a vanilla JavaScript thing just wouldn’t do. All of React’s rules and handling of state and quirks of component lifecycles meant that any available package or library which <em>wasn’t</em> explicitly written for React probably wasn’t going to work.</p>


<p>React trained us that things need to be built <em>specifically for a certain framework</em>. But that’s not very true anymore, and it arguably never should have been.</p>

<p>We shouldn’t <em>need</em> to do that—especially for a framework that so often claims it’s “just JavaScript.” If it’s <em>just JavaScript</em>, then it should <em>just work</em> with anything that’s <em>actually just JavaScript</em>.</p>
<p>Sure, other frontend frameworks have their own rules and conventions about state and architecture. You can step on figurative rakes in their yards, too. And yes, there will always be things that are, and need to be, built specifically to work with Svelte or Vue or whatever else.</p>
<p>But crucially—and I want to emphasize this as strongly as possible:</p>


<p>No other modern frontend framework is as stubbornly incompatible with the platform as React is.</p>

<p>If you’re building using other modern tools and frameworks, it’s <em>far</em> more likely that the vanilla JavaScript packages available will work just fine for you—and there are <em>thousands</em> of them. They’re far less likely to cause issues with render cycles, or other framework-specific issues. Not to mention: they all have the option of using web components, too.</p>
<p>You often don’t <em>need</em> a specialized package or library tailor-built for your thing, because your thing probably already works with the platform, and therefore, everything else that’s already out there.</p>
<p><a href="https://preactjs.com/guide/v10/signals/" rel="nofollow">Preact Signals</a> is a phenomenal example: although built for use with Preact, it can be imported and used in <em>any</em> framework, or even in vanilla JavaScript. Web components, too, are compatible with just about any modern non-React framework.</p>
<p>Where the frameworks fall short, it’s likely the platform already has the thing you need. (Form submission, for example; always a pain point in React, now made infinitely easier by two-way data binding and just using the conventions browsers give to us.)</p>
<p>And worst-case, it’s probably a lot <em>easier</em> to build whatever thing you need than it was in React. (It shouldn’t take very much comparing of <code>useState</code> with other frameworks’ versions to see that.)</p>
<p>Being newer is often considered a disadvantage by conservative-minded developers who are wary to test the waters with something that hasn’t been thoroughly vetted in every which way possible. But it’s important to remember that being new is <em>also</em> an advantage, because there’s less tech debt and old browser support to worry about—<em>and</em> new things are free to iterate further on existing good ideas and more modern browser features.</p>
<h3 id="react-hooks-are-actually-kind-of-outdated">React hooks are actually kind of outdated</h3>
<p>Hooks are the newest evolution of React, replacing class components.</p>
<p>Credit where it’s due: hooks <em>were</em> a massive shift in the frontend space. They revolutionized how we composed logic and state in our applications. Hooks are undeniably great, and pretty much every framework has coalesced around a hooks-like model for managing state.</p>
<p>But React hooks aren’t new anymore. (In fact, stable React with hooks is almost exactly the same age as my kid, and he’s starting pre-k in a couple of weeks.)</p>
<p>Hooks are no longer a competitive advantage, or even a notable feature; they’re the baseline. They’re just the way we do things.</p>


<p>Every other framework not only has its own implementation of hooks, but notably: every one of them is faster, smarter, easier to write, or a combination of all three.</p>

<p>Preact’s Signals warrant mention here; so do Svelte’s dead-simple stores. Solid, too, has Signals. Even Vue 3’s composition API, which is pretty directly inspired by hooks, has some key advantages over the React implementation.</p>
<p>Hooks are an excellent pattern, and React deserves credit for popularizing it. But pretty much every other framework does hooks better, with fewer rules, and with less boilerplate.</p>
<p>If you’re unfamiliar with the concept of Signals: it’s a crude oversimplification, but you could think of them as the next, better evolution of reactive state; an update to hooks, with better defaults around what causes re-renders, to only re-render the nodes that need to be re-rendered, instead of entire components.</p>
<h3 id="you-dont-need-to-micro-manage-rendering-anymore">You don’t need to micro-manage rendering anymore</h3>
<p>I have a confession to make: I’m still not exactly sure what the difference between <code>useMemo</code> and <code>useCallback</code> is—or when you should and shouldn’t use them—even though I <em>literally read multiple articles on that exact topic earlier today</em>. (No joke.)</p>
<p>I have a second confession: it’s still not intuitive to me what should and shouldn’t go into the <code>useEffect</code> dependency array, or why. I feel like every time I write a <code>useEffect</code> call, I spend like 15 minutes refactoring my code to be in a shape the linter likes, even when <em>I’m 99% certain it’s actually fine</em> and it’s not going to suck my app into an infinite abyss.</p>
<p>I’m betting if you use React, you can probably relate to those confessions. And maybe you’ve even just accepted that confusion and ambiguity as normal. But if so, you should know:</p>
<p>We haven’t had to do this kind of rendering cycle micromanagement in other frameworks for <em>years</em>.</p>


<p>These days, frameworks are smart enough to handle this kind of thing without you needing to hold their hand and explain what they should do.</p>

<p>They already know not to waste precious resources re-rendering when there’s no real need. They’re intelligent enough to only update values, and not constantly reevaluate things that don’t need it.</p>
<p>…Most of the time, anyway. They’re not perfect. But they <em>are</em> much better than React at knowing what to do, and doing it in a performant way by default.</p>
<p>You <em>might</em> need to optimize some things in other frameworks, too. They’re not perfect. But by the time you do, you’re way, <em>way</em> past the point where you would’ve needed to in React.</p>
<h3 id="nobody-else-is-afraid-of-their-frameworks-version-of-useeffect">Nobody else is afraid of their framework’s version of <code>useEffect</code></h3>
<p>When you want a component to just do something when it enters the DOM—and/or when you want it to recalculate something dynamically, based on some other data or variable(s)—just about every other framework has a better way than <code>useEffect</code>.</p>
<p>I don’t think I need to harp too much on this here, because even within React communities, <code>useEffect</code> is considered notoriously hazardous, and often even avoided altogether. But trust me: no other non-React-based frontend framework has people so afraid to use such a normal, useful feature, and nowhere else are there such obtuse rules around it.</p>
<p>Nobody else is looking at third-party packages just to do something when a component is mounted without shooting themselves in the foot.</p>
<h3 id="scaling-isnt-really-a-frontend-concern-anymore">Scaling isn’t really a frontend concern anymore</h3>
<p>This is the <em>other</em> question people immediately ask when a new(er than React) framework comes up: <em>does it scale</em>? But I believe that question might be a bit outdated.</p>
<p>It’s worth remembering: the world that gave us React had a different set of problems.</p>
<p>In that world, most frontend UIs were built either with vanilla JavaScript, or with jQuery (or similar alternatives). And that method of building apps, as we now know, didn’t scale well beyond a certain limit.</p>
<p>That’s because you had to write your own selectors for each and every element and DOM node you might want to interact with, and you had to come up with your own manual way of tracking and syncing state. That usually involved writing to and retrieving from the DOM, which was messy, error-prone, and most importantly, slow. (That’s where the virtual DOM came in, but even <em>that</em> has been <a href="https://svelte.dev/blog/virtual-dom-is-pure-overhead" rel="nofollow">pretty thoroughly outdated for years</a>.)</p>
<p>Writing modular code back then was difficult to impossible, and JS files often ballooned to hundreds of lines, if not thousands. If multiple authors were working on the same project, they’d often reinvent, repeat, or even override each other’s code (partly because code often went into a shared global namespace, which made collisions even more likely). And the bigger or more complex your app (<em>Facebook</em>), the worse the problem was.</p>
<p>It’s important to remember: that’s our baseline for “does it scale?” as it relates to frontend. Does it stay reasonably maintainable even if my app grows exponentially?</p>


<p>The worry that a frontend framework might not scale is as old as jQuery, and should be considered just as antiquated in relation to modern web development.</p>

<p>React solved many of these problems, yes. But it didn’t do so by being a marvel of modern engineering, so much as simply coming up with a good way to manage and share state, make data reactive, abstract complexity, and enable developers to share the same programming patterns without conflicts, namespace collisions, or overrides.</p>
<p>React wasn’t the best, only, or even <em>first</em> solution to frontend scalability; it’s just one of many possible versions of the same paradigm.</p>
<p>(It also happens to be among the oldest.)</p>
<p>How do I know this? Because a plethora of benchmark tests have been run, with publicly available results, comparing the performance of React to every other frontend framework at scale. (I’m not linking to any here, because they’re readily available online.) They all confirm that just about every other option in the frontend space does as well or better than React—and in many cases, <em>dramatically</em> better.</p>
<p>Here I’m referring to scaling in the general sense; making sure complexity stays minimal, and doesn’t grow linearly as the app increases in size. Certainly, some frameworks will scale much better or worse than others in terms of, say, building static HTML from Markdown files, or other more specialized tasks.</p>
<h3 id="server-side-rendering-isnt-special-anymore">Server-side rendering isn’t special anymore</h3>
<p>An earlier version of this section erroneously conflated server-side rendering with React Server Components (for reasons that I hope are at least understandable, given the confusing naming conventions).</p>
<p>There was a time, several years ago, when React was pretty much the only game in town when it came to server-rendered content (mainly via Next JS). People were rightly excited for the idea that React could be rendered on a server as HTML, instead of on the client as a Single-Page App (SPA). The speed and SEO gains were impossible to ignore, and initially, it took other frameworks a bit to catch up.</p>
<p>However, as is a theme with these things in general, and with this post in particular: the first to iterate is rarely the best.</p>
<p><a href="https://kit.svelte.dev/" rel="nofollow">SvelteKit</a> is server-rendered by default, without you needing to do anything, and offers fine-grained control over its rendering patterns. <a href="https://nuxt.com/" rel="nofollow">Nuxt</a>, Vue’s meta-framework, was earlier to the game (being obviously inspired by Next).</p>
<p><a href="https://fresh.deno.dev/" rel="nofollow">Fresh</a> (Deno’s frontend framework) is entirely server-rendered, except for what you designate as an “island” (client-rendered); anything else just ships as static HTML. Fresh also uses Preact (which, again, is even faster than React, and which has <a href="https://preactjs.com/guide/v10/signals/" rel="nofollow">Signals</a>, a much more performant and ergonomic version of <code>useState</code> and the reactivity model).</p>
<p>Astro has server-rendering, and just lets you server-render whatever flavor of components you want. It can render other frameworks’ components just fine, and has even been noted as a major performance upgrade from Next, in some cases.</p>
<p><a href="https://start.solidjs.com/getting-started/what-is-solidstart" rel="nofollow">SolidStart</a> (Solid’s meta-framework) has server rendering. Qwik is entirely built around it. Even some older frameworks like <a href="https://emberjs.com/" rel="nofollow">Ember</a> and <a href="https://angularjs.org/" rel="nofollow">Angular</a> have a story here; I’m sure I’m leaving out others, too.</p>
<p>Point is: way back when, React was one of few frameworks that had the concept of rendering client view framework components on a server. But now, server rendering is table stakes. A lot of newer frameworks don’t just have the <em>option</em> to render on the server; they do it <em>by default</em>.</p>
<p>PHP is back, baby.</p>
<h3 id="two-way-data-binding-isnt-hard-and-it-isnt-a-bad-idea">Two-way data binding isn’t hard and it isn’t a bad idea</h3>
<p>I think it’s important to remember that React was created by Facebook, in order to solve Facebook’s unique set of problems.</p>
<p>One of React’s strongest opinions—that data should flow only one way (top down)—is a good example of how the engineering challenges of Facebook in the early 2010s indelibly shaped React’s architecture.</p>
<p>For some time, it seemed like one-way data flow was considered a best practice. These days, though, we’ve mostly figured out solutions to the pitfalls of two-way data binding, and found that in many cases, it’s actually much more convenient.</p>
<p>Working with forms in React is notoriously cumbersome because every user keystroke is a two-step process: get the value from the input; then set the state to match it (which in turn needlessly re-renders the input, to contain the exact value it already did, but synced up with React state). Sure, it’s usually too fast to notice, but it’s a lot of extra work.</p>
<p>Svelte, Vue, and many others don’t have this issue. You can just bind state in such a way that it updates automatically from both ends. If the state changes, the DOM updates; if the DOM changes, the state updates.</p>
<p>This way, you don’t have to do the multi-step dance. If you just want to capture, say, the value of a text box, you do two-way data binding. Then, when the user types into the field, the data updates automatically, and you can get it whenever the time is right with no further steps. If in the meantime you need to do something like set a value or clear the field, that’s also a simple one-liner.</p>
<p>Two-way data binding lets you keep data and the DOM in sync without the need to constantly make sure one is keeping up with the other.</p>
<p>Could you get in trouble using these? For sure. But I find that dogmatic ideals of best practices get in the way as much or more than they help. One-way data flow is a prime example.</p>
<h3 id="styling-is-easy-actually">Styling is easy, actually</h3>
<p>If you work mostly in React, it’s quite possible you’ve gone through two, three, or more iterations of handling styles in your frontend components.</p>
<p>You might have imported .css files straight into JSX components, or used CSS Modules, Styled Components, and/or Tailwind (probably with either the <code>classnames</code> or <code>tailwind-merge</code> packages—or maybe even both, plus some extra Tailwind add-ons). And those are just the most popular options.</p>
<p>Tailwind is its own rabbit hole (and its own frontend framework I’m not particularly a fan of; I consider it cutting against the grain of the platform in exchange for short-term gains that eventually compound into long-term losses). But in any case, these styling solutions exist and see significant adoption at least partly because React’s had a vacuum in place of first-party styling options for as long as it’s been around.</p>


<p>You might not realize styling is a solved problem in several other frameworks.</p>

<p>In particular, Vue and Svelte both have their own component styling story. They both have component-level scoping (Vue’s is opt-in; Svelte’s is opt-out). They both work wonderfully with vanilla CSS, if that’s the way you want to go. But both of them—along with every other frontend framework—are still compatible with CSS modules, Tailwind, Sass, or whatever else you like to use.</p>
<p>But most importantly: all the supposed problems with CSS—whether you actually consider them problems or not—are fully addressed by the built-in style handling. You don’t need a mess of packages and configs nearly as much anywhere else, because scoped CSS solves just about every issue you could possibly imagine.</p>
<p>Seriously; read through any list of reasons CSS is supposedly bad (it’s not, but people who are bad at it like to say that). Just about any critique you could possibly have of CSS is solved by scoped styling, and multiple non-React frameworks just come with it already built in.</p>
<h3 id="frameworks-arent-as-hard-to-learn-anymore">Frameworks aren’t as hard to learn anymore</h3>
<p>I theorize developers mainly trained on React think back to how difficult it was to learn, and assess the learning curve of other frameworks similarly. And that’s probably part of what keeps us from trying new things; it seems really hard, because it sure was the first time.</p>
<p>All the ins and outs of state management, props, nesting, component lifecycles, hooks, and of course, how to write JSX…it’s a lot. Even the most ardent React fans would likely concede it’s not the easiest thing for beginners to pick up quickly. (Anyone who says otherwise has probably forgotten what it was like to be a beginner.)</p>
<p>If you can relate, I have good news:</p>


<p>There’s no comparable tool as difficult to learn as React is. But once you know one framework, you have a huge head start on all the others.</p>

<p>I compare this to learning your <em>second</em> musical instrument (not just to tie this back to music again). The <em>first</em> time you learn to play, you’re learning <em>everything about music</em>, on top of learning your specific instrument, and how to get it to make the sounds you want. But when you learn your <em>second</em> instrument, you get to skip so much. All the concepts are familiar. You understand music. All you need to do is transfer your existing knowledge and muscle memory into a slightly different shape.</p>
<p>Frontend is similar: every frontend framework has components; they’re all compatible with TypeScript; they all have the concept of props, children, and reactive state. These are things we’ve generally agreed we like and are good. They just have different takes on implementation.</p>
<p>And speaking of which: while React undoubtedly helped to proliferate these ideas, it would be silly to consider React the ideal implementation of them.</p>
<p>Great things are created through iteration, and for the most part, other choices in the frontend space that came later have the distinct advantage of iterating on top of the core ideas of React.</p>
<p>This means React is a bit like a git branch that’s fallen well behind <code>main</code>. You might not realize it, if React is the star your galaxy orbits around, but…well, frontend has moved on. The ecosystem has taken those ideas and run with them to make things that are even better.</p>
<p>We have no shortage of more performant, less complex, less difficult-to-learn options available to us now. And if you know React already, none of them will be very hard to learn as well.</p>
<h2 id="part-3-the-other-stuff-you-should-try">Part 3: the other stuff you should try</h2>
<p>You probably started wondering a few dozen paragraphs ago: if React is so antiquated, what’s the alternative?</p>
<p>I’m going to cover several here, and mention their use cases as well. One of the issues with React is that it’s long tried to be everything for everyone, and useful though a React-shaped tool might be, I think maybe two or three different tools could be better than one Swiss army knife.</p>
<p>Two quick notes before we dive in, though:</p>
<ol><li><p>I list several options here, for the sake of covering all the other modern frameworks I mentioned above. <strong>I don’t expect anyone to learn about—let alone <em>use</em>—all of them</strong>. If you have to pick one, go with Svelte, or maybe Vue. But in any case, know that I’m only listing them all for the sake of thoroughness.</p></li>
<li><p>I didn’t list <em>all</em> the options here. There are others.</p>
<p>I omitted Ember and Angular, for example, because they’re both older than React, and don’t generally tend to outperform React significantly, if at all, in benchmark tests (sorry, Mel).</p>
<p>I also omitted the lightweight options like <a href="https://alpinejs.dev/" rel="nofollow">Alpine</a> and <a href="https://github.com/vuejs/petite-vue" rel="nofollow">Petite Vue</a>, since those are more replacements for jQuery than React, and shine where you might not need something as heavy-handed as a framework.</p>
<p>Finally, I also omitted exceptionally good tools in and around this category, like <a href="https://www.11ty.dev/" rel="nofollow">Eleventy</a>, since it’s more of a pure static site generator than a framework. (Still worth a look if you’re using Gatsby, however.)</p></li></ol>
<p>All that said: here’s your Discover Weekly.</p>
<h3 id="svelte-my-personal-pick"><a href="https://svelte.dev/" rel="nofollow">Svelte</a> (my personal pick)</h3>
<blockquote><p><em>Ladies and gentlemen of the class of 2023: use Svelte.</em></p>
<p><em>If I could offer you only one tip for the future, Svelte would be it.</em></p></blockquote>
<p>Joking aside: if I were to pick one thing from this list to recommend over React, it would be <a href="https://svelte.dev/" rel="nofollow">Svelte</a>. I’ve long maintained that “Svelte is React, but without the bullshit,” as I originally quipped on Twitter back in 2019 (RIP), and if anything, that’s only grown truer over time.</p>
<p>Svelte is delightfully simple to use, comparatively easy to learn (especially if you’re coming from the React world already; even the syntax is often similar), much, much more performant in just about all cases, and capable of anything React is and more. This site, and all my own side projects these days, are written in <a href="https://kit.svelte.dev/" rel="nofollow">SvelteKit</a>.</p>
<p>Svelte is fast; it’s comparable to the fastest options available. Its DX is phenomenal; it regularly appears at or near the top of most-loved frameworks in developer surveys.</p>
<p>Svelte hews as closely to the web platform as possible, so even though it’s incredibly powerful, its concepts will be largely familiar. Svelte also includes transitions, easings, CSS handling, component-scoped styles, and more niceties out of the box.</p>
<p>That might make you wonder about framework size, but where Svelte differs is: instead of being a JavaScript runtime, it’s a compiler. Anything you don’t use is stripped away at build time, and your code is transpiled into tiny bits of vanilla JavaScript. That means Svelte’s bundles are generally a fraction the size of React’s.</p>


<p>Although it feels and works like a framework, Svelte is, essentially, a small, elegant superset of HTML, with a delightfully simple syntax, which compiles to fast, minimal bundles.</p>

<p>Svelte’s own meta-framework, <a href="https://kit.svelte.dev/" rel="nofollow">SvelteKit</a>, is highly versatile and powerful, capable of static, server-rendered, deployment to the edge, and even mixing per-route. It hit version 1.0 at the end of 2022 and is very ready for production. (It’s also supported by Vercel, who make Next.js as well.)</p>
<h4 id="svelte-is-recommended-if">Svelte is recommended if:</h4>
<p>You want to rediscover the joy of frontend with (what I consider to be) the best all-around option, for the reasons above.</p>
<h4 id="svelte-replaces">Svelte replaces:</h4>
<p>Anything you’re doing with React. Svelte can replace React itself, or SvelteKit is versatile enough to sub in for Next, Gatsby, and/or Remix (or even all at once).</p>
<h3 id="vue"><a href="https://vuejs.org/" rel="nofollow">Vue</a></h3>
<p><a href="https://vuejs.org/" rel="nofollow">Vue</a> is possibly the closest option to React, and likely has the next-biggest ecosystem. It’s significantly more performant than React, however, and a bit more UI-focused.</p>
<p>In some ways, Vue is the smallest leap from React, especially now that it has a similar hooks-based approach in Vue 3. But Vue uses a templating language closer to default HTML than to JSX, which makes it much easier to write conditionals and loops in template files, without having to reach for workarounds like <code>map</code> and ternaries.</p>
<p>Vue has a similar meta-framework to Next in <a href="https://nuxtjs.org/" rel="nofollow">Nuxt</a>, which is well maintained and adding powerful new features all the time. Vue is also a bit more batteries-included than React, coming with things like scoped CSS handling and easy transitions/animations out of the box.</p>
<h4 id="vue-is-recommended-if">Vue is recommended if:</h4>
<p>Community size/overall framework popularity is an important factor for you; you want something like React, but more batteries-included or HTML-like; you prefer your framework to be independent and <em>not</em> be owned by a large corporation.</p>
<h4 id="vue-replaces">Vue replaces:</h4>
<p>React itself, or <a href="https://nuxt.com/" rel="nofollow">Nuxt</a> can replace anything you might be using Next for.</p>
<h3 id="solid"><a href="https://www.solidjs.com/" rel="nofollow">Solid</a></h3>
<p><a href="https://www.solidjs.com/" rel="nofollow">Solid</a> is what I would call React, but better. It looks almost (if not entirely) identical to React in many cases, but Solid is far, far more performant. It’s one of the fastest options available, in fact.</p>
<p>Solid essentially starts with React, and then rethinks it to eliminate complexity, performance issues, and a lot of boilerplate. Signals appear as a concept in Solid, which eliminate a great deal of the confusion and footguns around component rendering and lifecycles. It might even be fair to say Solid is React, if React was built in the modern era, on top of all the lessons we’ve learned since 2013.</p>
<p>Solid also offers its own meta-framework in <a href="https://start.solidjs.com/getting-started/what-is-solidstart" rel="nofollow">SolidStart</a>, though that is currently in beta. Solid itself is plenty mature enough to use, though, and boasts an impressive gallery of sponsors.</p>
<h4 id="solid-is-recommended-if">Solid is recommended if:</h4>
<p>You generally like React (and JSX), but you just wish it was more modern, faster and/or easier; performance is an absolute top priority.</p>
<h4 id="solid-replaces">Solid replaces:</h4>
<p>React and React DOM. SolidStart will likely be capable of replacing Next one day, but it’s still in beta as of this writing.</p>
<h3 id="fresh"><a href="https://fresh.deno.dev/" rel="nofollow">Fresh</a></h3>
<p><a href="https://fresh.deno.dev/" rel="nofollow">Fresh</a> is a server-rendered frontend framework with islands architecture, built on Deno. It’s a bit younger than most of the rest of the items on this list, but it’s full of promise as a minimal-JS, island-based framework that can run on the edge—powered by Deno, no less, which means your server code is faster, more secure, TypeScript by default, and all the other benefits Deno brings over traditional Node (such as easier, first-party linting, testing, and code formatting settings).</p>
<p>Every Fresh component is either static-rendered and served at response time as HTML, with no JavaScript, or an “island,” which means it renders only on the client. You can mix and match as needed. Because it runs on Deno, this opens the gate for extremely fast, dynamic content that loads as quickly as possible on any device anywhere in the world.</p>
<p>Fresh uses Preact, so you know it’s fast, and won’t be difficult to pick up if you’re coming from React, either. And again: building on Deno feels great.</p>
<h4 id="fresh-is-recommended-if">Fresh is recommended if:</h4>
<p>You like the idea of a server-side app globally available in the cloud, shipping absolutely minimal JavaScript, and/or building on the latest technology.</p>
<h4 id="fresh-replaces">Fresh replaces:</h4>
<p>Remix is probably the closest thing to Fresh in React-land.</p>
<h3 id="astro"><a href="https://astro.build/" rel="nofollow">Astro</a></h3>
<p><a href="https://astro.build/" rel="nofollow">Astro</a> is a next-gen, highly performant static site generator that does more than static. Astro is one of the newest options on this list, but it’s already at a very stable 1.0 release and has garnered widespread praise and adoption.</p>
<p>Built mainly to be a new generation of SSG (hey, React fans: it supports JSX and MDX), Astro now also features dynamic, server-side capabilities as well. I’d definitely recommend it over, say, Gatsby, or for any content-heavy or static sites.</p>
<p>The real killer feature is: Astro ships zero JavaScript by default. You opt in to only what you want to use.</p>
<p>Astro is also compatible with whatever frontend framework you want to use, so if you prefer to template in React, Vue, Svelte, or others, you can!</p>
<h4 id="astro-is-recommended-if">Astro is recommended if:</h4>
<p>You’re building a largely static, or content/Markdown-based site (even if you may need some server-side rendering or logic); you want to ship minimal JavaScript; you want to bring your own frontend framework.</p>
<h4 id="astro-replaces">Astro replaces:</h4>
<p>Gatsby, or similar React-based content tools.</p>
<h3 id="preact"><a href="https://preactjs.com/" rel="nofollow">Preact</a></h3>
<p>You probably already know about <a href="https://preactjs.com/" rel="nofollow">Preact</a> if you live in React land, but it warrants mention here. It’s a much slimmer, much faster version of React. Although it began more-or-less as a drop-in replacement for React, it’s beginning to gain some superior features React doesn’t have (like <em>Signals</em>, which we’ve already mentioned).</p>
<h4 id="preact-is-recommended-if">Preact is recommended if:</h4>
<p>You want to stick with React, essentially, but you just want it to be faster.</p>
<h4 id="preact-replaces">Preact replaces:</h4>
<p>React. (Actually, it just adds a P to the beginning. The P stands for performance. I made all that up; don’t blame the Preact team for that.)</p>
<h3 id="qwik"><a href="https://qwik.builder.io/" rel="nofollow">Qwik</a></h3>
<p><a href="https://qwik.builder.io/" rel="nofollow">Qwik</a> server-renders React-like code (JSX) with a new approach to hydration and performance. In fact, what it does can’t really be called “hydration” at all; instead, it serializes JavaScript into the DOM, and loads it in tiny bits only when it’s needed. Qwik is one of the deeper cuts on this list, but if you have a <em>lot</em> of interactivity that you need to run as fast as possible, it’s well worth a look.</p>
<h4 id="qwik-is-recommended-if">Qwik is recommended if:</h4>
<p>You’re shipping <em>lots</em> of JavaScript to the browser, and you want a way to make that more performant.</p>
<h4 id="qwik-replaces">Qwik replaces:</h4>
<p>React itself, allowing it to run very efficiently on the edge.</p>
<h3 id="web-component-libraries">Web component libraries</h3>
<p>I won’t go very deep on this one, because frankly, I’m not the guy for that. I don’t have the experience with either <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components" rel="nofollow">web components</a> on their own, or web component frameworks, to speak well on the topic.</p>
<p>That said, there <em>is</em> a certain class of projects that could benefit from a <a href="https://www.webcomponents.org/libraries" rel="nofollow">web component framework/library</a> like <a href="https://lit.dev/" rel="nofollow">Lit</a>, <a href="https://stenciljs.com/" rel="nofollow">Stencil</a>, <a href="https://www.polymer-project.org/" rel="nofollow">Polymer</a>, or others. Rather than generating “proprietary” components in a specific frontend framework, these libraries help you write actual web components, which are then portable to any web project.</p>
<p>In my opinion, most projects still benefit from using a frontend framework over pure web components—or, at the very least, both together. Maybe that will change in the future, but for now, I think the tradeoffs still tilt away from a pure web component approach in most cases.</p>
<p>Still, there are certainly use cases for which a purely web component-based approach ought to be considered. And for <em>those</em> projects, React is definitely overkill. The web component libraries mentioned above would be a much better fit.</p>
<h4 id="web-component-libraries-are-recommended-if">Web component libraries are recommended if:</h4>
<p>You need to reuse the same components in multiple environments; want to future-proof yourself against framework changes; or just prefer using the platform, and are prepared to deal with the tradeoffs of web component authoring.</p>
<h4 id="web-components-replace">Web components replace:</h4>
<p>React, but maybe only partially, depending on your use case</p>
<h2 id="epilogue">Epilogue</h2>
<p>This post is, admittedly, a lot like my post from last year, <a href="https://joshcollinsworth.com/blog/self-fulfilling-prophecy-of-react"><em>The self-fulfilling prophecy of React</em></a>. It treads some of the same territory, and makes some of the same arguments (albeit hopefully in new ways or from new perspectives).</p>
<p>I didn’t set out to repeat myself, but clearly, I think about this stuff a lot—spurred no doubt by my professional shift to working with React full time around the time that post was published, by coincidence.</p>
<p>I’ve come to believe React’s popularity is, in no small part, because folks don’t look beyond it. It’s not the greatest, but most people aren’t looking for the greatest; they’re just looking for good enough. (We’re humans. There are a lot of personal, emotional, irrational reasons for our decisions, all of us, and that’s fine. We’re busy.)</p>
<p>It seems like we adopt technologies in leaps, rather than in a linear motion, at least in the world of frontend. Part of what caused everyone to jump on the React bandwagon was that <em>everyone</em> at the time was stuck on antiquated technology, and was looking for something better. We didn’t gradually advance to the new thing, in small steps (maybe because that wasn’t really an option to begin with); we took a giant <em>leap</em> from where we were to the next thing.</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/d1e525bee1dc5534a4f1c1466ce1a429e2b2d286/4f0d3/images/post_images/tech-adoption.png" alt="A linear line with an arrow pointing forward, labeled 'progress.' There are a few arced leaps of progress on top of the line, jumping from left to right, labeled 'adoption.' The final leap, however, lands well short of the furthest edge of the straight 'progress' line."></p>
<p>But the thing is: we’ve been sitting there, in mostly that same spot, since we took that leap all those years ago.</p>
<p>My sense is: we’re beginning to near another leap.</p>
<p>I don’t know what it will be, or why. But I think we’re starting to feel all the problems React actually <em>doesn’t</em> solve for us, like we felt with jQuery back in those days. And I think eventually, it will be clear that it’s time to advance.</p>
<p>What will that new thing be? I don’t know. Maybe it’ll just be the web platform. Maybe we won’t even need frameworks. Maybe it’ll be a framework above; maybe it’ll be something we haven’t even seen yet. Maybe it won’t even be <em>a thing</em>; maybe there will be more diversity of tooling and less coalescing around one single accepted standard (though of all the above options, I’d say that seems the least likely, because again: humans. We’re busy little monkeys and so we like defaults.)</p>
<p>I think, though, that the delta between React and that thing, whatever it is, will continue to grow larger and larger over time.</p>
<p>So every new day is an even better day than the one before it to explore what you’ve been missing.</p>
<p>Happy listening.</p>

		

		
</article></main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam Bankman-Fried is going to jail (141 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/08/sam-bankman-fried-is-going-to-jail/</link>
            <guid>37131626</guid>
            <pubDate>Tue, 15 Aug 2023 08:49:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/08/sam-bankman-fried-is-going-to-jail/">https://arstechnica.com/tech-policy/2023/08/sam-bankman-fried-is-going-to-jail/</a>, See on <a href="https://news.ycombinator.com/item?id=37131626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      From Bahamas penthouse to Manhattan’s big house    —
</h4>
            
            <h2 itemprop="description">Judge also denied SBF's request to delay jail time.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-1258714385-800x534.jpg" alt="Sam Bankman-Fried.">
      <figcaption><div><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-1258714385.jpg" data-height="683" data-width="1024">Enlarge</a> <span>/</span> Sam Bankman-Fried.</p></div></figcaption>  </figure>

  




<!-- cache hit 57:single/related:14f233083c90a25e64cd5632e9f5f425 --><!-- empty -->
<p>A federal judge in New York today ordered disgraced FTX founder Sam Bankman-Fried's to jail after revoking his bail, <a href="https://www.nytimes.com/2023/08/11/technology/sam-bankman-fried-to-be-sent-to-jail-after-judge-revokes-bail.html">The New York Times reported</a>.</p>
<p>Bankman-Fried had been under house arrest, but prosecutors convinced Judge Lewis A. Kaplan of the Federal District Court in Manhattan that Bankman-Fried had fed documents to the media in order to intimidate a witness in the case. Now Bankman-Fried has to prepare his defense to seven criminal charges from jail.</p>
<p>In June, Bankman-Fried filed a motion to dismiss, hoping that some of those charges would be dropped. But Kaplan decided that his arguments in the motion were "either moot or without merit,” <a href="https://www.cnn.com/2023/06/27/business/sbf-motion-to-dismiss-denied/index.html">CNN reported</a>.</p>
<p>A New York Times <a href="https://www.nytimes.com/2023/07/20/technology/ftx-caroline-ellison-bankman-fried.html">report</a> was among media stories that the prosecution shared to convince the court to give Bankman-Fried jail time. In that report, Bankman-Fried shared private writings of Caroline Ellison, a former FTX executive and former girlfriend to Bankman-Fried who has pled guilty and is currently cooperating with law enforcement in their investigation of the cryptocurrency exchange, the Times reported.</p>
<p>Prosecutors claimed that Bankman-Fried shared Ellison's communications to intimidate her. The court found that Bankman-Fried tampered with witnesses at least twice, <a href="https://www.reuters.com/legal/ftxs-bankman-fried-seeking-avoid-jail-due-back-court-2023-08-11/">Reuters reported</a>.</p>
<p>It wasn't just the New York Times report that alarmed the court, however. Bankman-Fried's other communications with the media led the prosecution to request a gag order to block all talks with the media.</p>
<p>According to The New York Times, "The Times, the Reporters Committee for the Freedom of the Press, and a documentarian making a film" about Bankman-Fried "each submitted court filings raising First Amendment concerns about the gag order."</p>
<p>Bankman-Fried requested his detention be delayed, pending an appeal of the order revoking his bail, Reuters reported, but the judge denied that request.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Continuous batching to increase LLM inference throughput and reduce p50 latency (104 pts)]]></title>
            <link>https://www.anyscale.com/blog/continuous-batching-llm-inference</link>
            <guid>37131477</guid>
            <pubDate>Tue, 15 Aug 2023 08:21:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anyscale.com/blog/continuous-batching-llm-inference">https://www.anyscale.com/blog/continuous-batching-llm-inference</a>, See on <a href="https://news.ycombinator.com/item?id=37131477">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Due to the large GPU memory footprint and <a href="https://www.anyscale.com/large-language-models">compute cost of LLMs</a>, serving dominates the compute cost for most real world applications. ML engineers often treat LLMs like "black boxes" that can only be optimized with internal changes such as quantization and custom CUDA kernels. However, this is not entirely the case. Because LLMs iteratively generate their output, and because LLM inference is often memory and not compute bound, there are surprising <i>system-level</i> batching optimizations that make 10x or more differences in real-world workloads.</p><p>One recent such proposed optimization is <b>continuous batching</b>, also known as <b>dynamic batching</b>, or batching with <b>iteration-level scheduling</b>. We wanted to see how this optimization performs. We will get into details below, including how we simulate a production workload, but to summarize our findings:</p><ul><li><p>Up to 23x throughput improvement using continuous batching and continuous batching-specific memory optimizations (using <a href="https://twitter.com/zhuohan123/status/1671234707206590464?s=20"><u>vLLM</u></a>).</p></li><li><p>8x throughput over naive batching by using continuous batching (both on <a href="https://docs.ray.io/en/latest/serve/index.html"><u>Ray Serve</u></a> and <a href="https://github.com/huggingface/text-generation-inference"><u>Hugging Face’s text-generation-inference</u></a>).</p></li><li><p>4x throughput over naive batching by using an optimized model implementation (<a href="https://github.com/NVIDIA/FasterTransformer"><u>NVIDIA’s FasterTransformer</u></a>).</p></li></ul><p>You can try out continuous batching today: see <a href="https://github.com/ray-project/ray/blob/cc983fc3e64c1ba215e981a43dd0119c03c74ff1/doc/source/serve/doc_code/vllm_example.py"><u>this example to run vLLM on Ray Serve</u></a>.</p><p>The remainder of this blog is structured as follows:</p><ul><li><p>We’ll cover the basics of how LLM inference works and highlight inefficiencies in traditional request-based dynamic batching policies.</p></li><li><p>We’ll introduce continuous batching and how it answers many of the inefficiencies of request-based dynamic batching.&nbsp;</p></li><li><p>We then discuss our benchmarks and the implications this has on how to serve LLM models cost-effectively.</p></li></ul><hr><h2>The basics of LLM inference</h2><p>There is a lot to know about LLM inference, and we refer users to <a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one"><i><u>Efficient Inference on a Single GPU</u></i></a><i> </i>and <a href="https://huggingface.co/blog/bloom-inference-optimization"><i><u>Optimization story: Bloom inference</u></i></a> for more detail. However, at a high level, LLM inference is pretty straightforward.</p><p>For each request:</p><ol><li><p>You start with a sequence of tokens (called the "prefix" or "prompt").</p></li><li><p>The LLM produces a sequence of completion tokens, stopping only after producing a stop token or reaching a maximum sequence length. </p></li></ol><p>This is an iterative process. You get one additional completion token for each new forward pass of the model. For example, suppose you prompt with a sentence "What is the capital of California: ", it would take ten forward pass iterations to get back the full response of ["S", "a", "c", "r", “a”, "m", "e", "n", "t", "o"]. This example simplifies things a little bit because in actuality tokens do not map 1:1 to ASCII characters (a popular token encoding technique is <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding"><u>Byte-Pair Encoding</u></a> which is beyond the scope of this blog post), but the iterative nature of generation is the same regardless of how you tokenize your sequences.</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/4Htl7q5sOaX47ViD1EaMdT/039ba19b73bcf58e7be4130d53b147d4/01_diagram-llm-basics_aspect_ratio.png" alt="cb 01 diagram-llm-basics"></p></div><p><span>Simplified LLM inference. This toy example shows a hypothetical model which supports a maximum sequence length of 8 tokens (T1, T2, …, T8). Starting from the prompt tokens (yellow), the iterative process generates a single token at a time (blue). Once the model generates an end-of-sequence token (red), the generation loop stops. This example shows a batch of only one input sequence, so the batch size is 1.</span></p></div><p>Now that we understand the simplicity of the iterative process, let’s dive deeper with some things you may not know about LLM inference:</p><ol><li><p>The initial ingestion (“prefill”) of the prompt "What is the capital of California: " takes about as much time as the generation of each subsequent token. This is because the <a href="https://github.com/huggingface/text-generation-inference/tree/f59fb8b630844c2ad2cd80e689202de89d45c37e/router#prefill-decode-and-past-key-values"><u>prefill phase</u></a> pre-computes <a href="https://kipp.ly/blog/transformer-inference-arithmetic/#kv-cache"><u>some inputs</u></a> of the attention mechanism that remain constant over the lifetime of the generation. This prefill phase efficiently uses the GPU’s parallel compute because these inputs can be computed independently of each other.</p></li><li><p>LLM inference is <a href="https://en.wikipedia.org/wiki/Memory_bandwidth"><u>memory-IO bound</u></a>, not compute bound. In other words, it currently takes more time to load 1MB of data to the GPU’s compute cores than it does for those compute cores to perform LLM computations on 1MB of data. This means that LLM inference throughput <i>is largely determined by how large a batch you can fit into high-bandwidth GPU memory</i>. See <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#understand-perf"><u>this page</u></a> in the NVIDIA docs for more details.</p></li><li><p>The amount of GPU memory consumed scales with the base model size + the length of the token sequence. In <a href="https://github.com/ray-project/llm-numbers#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model"><i><u>Numbers every LLM developer should know</u></i></a>, it’s estimated that a 13B parameter model consumes nearly 1MB of state for each token in a sequence. On a higher-end A100 GPU with 40GB RAM, back-of-the-envelope math suggests that since 14 GB are left after storing the 26GB of model parameters, ~14k tokens can be held in memory at once. This may seem high but is actually quite limiting; if we limit our sequence lengths to 512, we can process at most ~28 sequences in a batch. The problem is worse for higher sequence lengths; a sequence length of 2048 means our batch size is limited to 7 sequences. Note that this is an upper bound since it doesn’t leave room for storing intermediate computations.</p></li></ol><p>What this all means is that there is substantial “room on the table” so to speak if you can optimize memory usage. This is why approaches such as model quantization strategies such as <a href="https://github.com/PanQiWei/AutoGPTQ"><u>AutoGPTQ</u></a> are potentially so powerful; if you could halve the memory usage by moving from 16-bit to 8-bit representations, you could double the space available for larger batch sizes. However, not all strategies require modifications to the model weights. For example, <a href="https://github.com/HazyResearch/flash-attention"><u>FlashAttention</u></a> found significant throughput improvements by reorganizing the attention computation to require less memory-IO.</p><p>Continuous batching is another memory optimization technique which does not require modification of the model. We next explain how naive batching works (and is inefficient), and how continuous batching increases the memory-efficiency of LLM generation.</p><hr><h2>LLM batching explained</h2><p>GPUs are massively-parallel compute architectures, with compute rates (measured in floating-point operations per second, or flops) in the teraflop (<a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf"><u>A100</u></a>) or even petaflop (<a href="https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet"><u>H100</u></a>) range. Despite these staggering amounts of compute, LLMs struggle to achieve saturation because so much of the chip’s memory bandwidth is spent loading model parameters.</p><p>Batching is one way to improve the situation; instead of loading new model parameters each time you have an input sequence, you can load the model parameters once and then use them to process many input sequences. This more efficiently uses the chip’s memory bandwidth, leading to higher compute utilization, higher throughput, and cheaper LLM inference.</p><h3>Naive batching / static batching</h3><p>We call this traditional approach to batching <i>static batching</i>, because the size of the batch remains constant until the inference is complete. Here’s an illustration of static batching in context of LLM inference:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png" alt="cb 02 diagram-static-batching"></p></div><p><span>Completing four sequences using static batching. On the first iteration (left), each sequence generates one token (blue) from the prompt tokens (yellow). After several iterations (right), the completed sequences each have different sizes because each emits their end-of-sequence-token (red) at different iterations. Even though sequence 3 finished after two iterations, static batching means that the GPU will be underutilized until the last sequence in the batch finishes generation (in this example, sequence 2 after six iterations).</span></p></div><p>Unlike traditional deep learning models, batching for LLMs can be tricky due to the iterative nature of their inference. Intuitively, this is because requests can "finish" earlier in a batch, but it is tricky to release their resources and add new requests to the batch that may be at different completion states. This means that as the GPU is underutilized as generation lengths of different sequences in a batch differ from the largest generation length of the batch. In the figure on the right above, this is illustrated by the white squares after end-of-sequence tokens for sequences 1, 3, and 4.</p><p>How often does static batching under-utilize the GPU? It depends on the generation lengths of sequences in a batch. For example, one could use LLM inference to emit a single token as a classification task (there are better ways to do this but let’s use this as an example). In this case, every output sequence is the same size (1 token). If the input sequences are also the same size (say, 512 tokens), then each static batch will achieve the best possible GPU utilization.</p><p>On the other hand, a LLM-powered chatbot service cannot assume fixed-length input sequences, nor assume fixed-length output sequences. Proprietary models offer maximum context lengths in excess of 8K tokens at the time of writing. With static batching, variance in generation output could cause massive underutilization of GPUs. It’s no wonder OpenAI CEO Sam Altman described the compute costs as <a href="https://twitter.com/sama/status/1599669571795185665?lang=en">eye-watering</a>.</p><p>Without restrictive assumptions on user input and model output, unoptimized production-grade LLM systems simply can’t serve traffic without underutilizing GPUs and incurring unnecessarily high costs. We need to optimize how we serve LLMs for their power to be broadly accessible.</p><h3>Continuous batching</h3><p>The industry recognized the inefficiency and came up with a better approach. <a href="https://www.usenix.org/conference/osdi22/presentation/yu"><i><u>Orca: A Distributed Serving System for Transformer-Based Generative Models</u></i></a> is a paper presented in OSDI ‘22 which is the first to our knowledge to tackle this problem. Instead of waiting until every sequence in a batch has completed generation, Orca implements <i>iteration-level</i> scheduling where the batch size is determined per iteration. The result is that once a sequence in a batch has completed generation, a new sequence can be inserted in its place, yielding higher GPU utilization than static batching.</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/744TAv4dJIQqeHcEaz5lko/b823cc2d92bbb0d82eb252901e1dce6d/cb_03_diagram-continuous-batching.png" alt="cb 03 diagram-continuous-batching"></p></div><p><span>Completing seven sequences using continuous batching. Left shows the batch after a single iteration, right shows the batch after several iterations. Once a sequence emits an end-of-sequence token, we insert a new sequence in its place (i.e. sequences S5, S6, and S7). This achieves higher GPU utilization since the GPU does not wait for all sequences to complete before starting a new one.</span></p></div><p>Reality is a bit more complicated than this simplified model: since the prefill phase takes compute and has a different computational pattern than generation, it cannot be easily batched with the generation of tokens. Continuous batching frameworks currently manage this via hyperparameter: <a href="https://github.com/huggingface/text-generation-inference/blob/f59fb8b630844c2ad2cd80e689202de89d45c37e/launcher/src/main.rs#L124-L135"><u>waiting_served_ratio</u></a>, or the ratio of requests waiting for prefill to those waiting end-of-sequence tokens.</p><p>Speaking of frameworks, Hugging Face has productionized continuous batching in their Rust- and Python-based <a href="https://github.com/huggingface/text-generation-inference/tree/main"><u>text-generation-inference LLM inference server</u></a>. We use their implementation to understand the performance characteristics of continuous batching in our benchmarks below.</p><p><b><i>Note</i></b><i>: Continuous batching, dynamic batching, and iteration-level scheduling are all close enough in meaning that any one of them can be used to describe the batching algorithm. We chose to use continuous batching. Dynamic batching is fitting but can be confused with request-level batching, where an LLM inference server uses a static batch whose size is chosen when the current batch has completely finished generation. We feel that iteration-level scheduling is descriptive of the scheduling mechanism but not the process as a whole.</i></p><hr><h2>PagedAttention and vLLM</h2><p>For this blog post, we want to showcase the differences between static batching and continuous batching. It turns out that continuous batching can unlock memory optimizations that are not possible with static batching by improving upon Orca’s design.</p><p>PagedAttention is a new attention mechanism implemented in <a href="https://vllm.ai/"><u>vLLM</u></a> (<a href="https://github.com/vllm-project/vllm/tree/main#easy-fast-and-cheap-llm-serving-for-everyone"><u>GitHub</u></a>). It takes inspiration from traditional OS concepts such as <a href="https://en.wikipedia.org/wiki/Memory_paging"><u>paging</u></a> and <a href="https://en.wikipedia.org/wiki/Virtual_memory"><u>virtual memory</u></a>. They allow the KV cache (what is computed in the “prefill” phase, discussed above) to be non-contiguous by allocating memory in fixed-size “pages”, or blocks. The attention mechanism can then be rewritten to operate on block-aligned inputs, allowing attention to be performed on non-contiguous memory ranges.</p><p>This means that buffer allocation can happen just-in-time instead of ahead-of-time: when starting a new generation, the framework does not need to allocate a contiguous buffer of size maximum_context_length. Each iteration, the scheduler can decide if it needs more room for a particular generation, and allocate on the fly without any degradation to PagedAttention’s performance. This doesn’t guarantee perfect utilization of memory (<a href="https://vllm.ai/"><u>their blog</u></a> says the wastage is now limited to under 4%, only in the last block), but it significantly improves upon wastage from ahead-of-time allocation schemes used widely by the industry today.</p><p>Altogether, PagedAttention + vLLM enable massive memory savings as most sequences will not consume the entire context window. These memory savings translate directly into a higher batch size, which means higher throughput and cheaper serving. We include vLLM in our benchmarks below.</p><hr><h2>Benchmarking setup</h2><p>We’ll discuss our experimental setup then dive into the results of our benchmarks.</p><h3>Experiments</h3><p>Our goal is to see how continuous batching performs versus static batching on a simulated real-world live-inference workload. Fundamentally, we care about cost. We break this down into throughput and latency since cost is directly downstream of how efficiently you can serve at a given latency.</p><table><tbody><tr><td><p><b>Benchmark goal</b></p></td><td><p><b>Measurement</b></p></td></tr><tr><td><p>Measure throughput</p></td><td><p>Time-to-process a queue of 1000 requests, each with 512 input tokens and generation length sampled from an exponential distribution.</p></td></tr><tr><td><p>Measure latency</p></td><td><p>Request latencies for 100 requests, with varying input lengths, output lengths, and arrival times at a fixed average rate.</p></td></tr></tbody></table><p>We’ll discuss the datasets and other details of the experiments in their respective results section.</p><h3>Hardware/model</h3><p><br>We benchmark throughput and latency on a single <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf"><u>NVIDIA A100 GPU</u></a> provided by <a href="https://www.anyscale.com/"><u>Anyscale</u></a>. Our A100 has 40GB of GPU RAM. We selected <a href="https://huggingface.co/facebook/opt-13b"><u>Meta’s OPT-13B</u></a> model because each framework under test had a readily-available integration with this model. We selected the 13B variant because it fits into our GPU without requiring tensor parallelism, yet is still large enough to present memory efficiency challenges. We opt not to use tensor parallelism, where each transformer block is split over multiple GPUs, to keep our experiments simple, although both static batching and continuous batching work with tensor parallelism.</p><h3>Frameworks</h3><div><p><img src="https://images.ctfassets.net/xjan103pcp94/3K202bzJfK6ZlhmJpgwZ9q/7ccf0aacaf298e24bf824ee0ac429c47/06_frameworks_aspect_ratio.png" alt="cb 06 frameworks"></p></div><p>We test two static batching frameworks and three continuous batching frameworks. Our static batching frameworks are:</p><ul><li><p><a href="https://huggingface.co/docs/transformers/pipeline_tutorial"><b><u>Hugging Face’s Pipelines</u></b></a><b>.</b> This is the simplest inference solution. It provides static batching with an easy-to-use API that works with any model and supports more tasks than simple text-generation. We use this as our baseline.&nbsp;</p></li><li><p><a href="https://github.com/NVIDIA/FasterTransformer"><b><u>NVIDIA’s FasterTransformer</u></b></a><b>.</b> This is a library which provides optimized implementations of various transformer models. It currently only provides static batching (the <a href="https://github.com/triton-inference-server/server"><u>Triton inference server</u></a> provides request-level dynamic batching, but not continuous batching yet). This provides us with an idea of how far an extremely optimized implementation of our model can get us with static batching – it provides a more competitive baseline than the relatively unoptimized OPT-13B implementation <a href="https://huggingface.co/facebook/opt-13b"><u>available on Hugging Face Hub</u></a>.</p></li></ul><p>Our continuous batching frameworks are:</p><ul><li><p><a href="https://github.com/huggingface/text-generation-inference"><b><u>Hugging Face’s text-generation-inference</u></b></a><b>.</b> This is the inference server Hugging Face uses to power their LLM live-inference APIs. It <a href="https://github.com/huggingface/text-generation-inference/tree/main/router#continuous-batching"><u>implements</u></a> continuous batching.</p></li><li><p><u><b>Continuous batching on Ray Serve</b></u><b>.</b> <a href="https://docs.ray.io/en/latest/serve/index.html"><u>Ray Serve</u></a> leverages Ray’s serverless capabilities to provide seamless autoscaling, high-availability, and support for complex DAGs. We wanted to understand how continuous batching works, so we re-implemented text-generation-inference’s core continuous batching logic in pure-Python on Ray Serve. As you will see in our results, our implementation achieves the same performance as text-generation-inference, which validates our understanding.</p></li><li><p><a href="https://vllm.ai/"><b><u>vLLM</u></b></a><b>.</b> This is an open-source project recently released by folks at UC Berkeley (<a href="https://github.com/vllm-project/vllm"><u>GitHub</u></a>). It builds upon Orca’s continuous batching design by taking full control of dynamic memory allocations, allowing it to significantly reduce different forms of GPU memory fragmentation. We test this framework because it shows the impact of further optimizations made possible by iteration-level scheduling and continuous batching.</p></li></ul><h2>Benchmarking results: Throughput</h2><p>Based on our understanding of static batching, we expect continuous batching to perform significantly better when there is higher <i>variance</i> in sequence lengths in each batch. To show this, we run our throughput benchmark four times for each framework, each time on a dataset with higher variance in sequence lengths.</p><p>To do this, we create a dataset containing 1000 sequences each with 512 input tokens. We configure our model to always emit a per-sequence generation length by ignoring the end-of-sequence token and configuring max_tokens. We then generate 1000 generation lengths, one for each request, sampled from an<a href="https://en.wikipedia.org/wiki/Exponential_distribution"><u> exponential distribution</u></a> with mean=128 tokens. We use an exponential distribution as it is a good approximation of the generation lengths that one may encounter while serving an application like ChatGPT. To vary the variance of each run, we select only samples from the exponential distribution that are less than or equal to 32, 128, 512, and 1536. The total output sequence length is then, at most, 512+32=544, 512+128=640, 512+512=1024, and 512+1536=2048 (the maximum sequence length of our model).</p><p>We then use a simple asyncio Python benchmarking script to submit HTTP requests to our model server. The benchmarking script submits all requests in burst fashion, so that the compute is saturated.</p><p>The results are as follows:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/1Os82uuLDUkqP90Nlhp3vh/1e783aff1edb97cd25b5139d26083c1c/cb_07_throughput_table.png" alt="cb 07 throughput table"></p></div><p><span>Throughput in tokens per second of each framework as variance in sequence length increases.</span></p></div><p>As expected, the static batchers and naive continuous batchers perform approximately identically for lower-variance generation lengths. However as the variance increases, naive static batching’s performance plummets to 81 token/s. FasterTransformers improves upon naive static batching significantly, nearly keeping up with the naive continuous batchers until generation length limit of 1536. Continuous batching on Ray Serve and text-generation-inference achieves about the same performance, which is what we expect since they use the same batching algorithm.</p><p>What is most impressive here is vLLM. For each dataset, vLLM more than doubles performance compared to naive continuous batching. We have not analyzed what optimization contributes the most to vLLM performance the most, but we suspect vLLM’s ability to reserve space dynamically instead of ahead-of-time allows vLLM to dramatically increase the batch size.</p><p>We plot these performance results relative to naive static batching:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/46OIG2WmA2j0SBfcG5fdq7/3bcdebf8014730a1a592a18f023cfdcc/cb_08_throughput_graph.png" alt="cb 08 throughput graph"></p></div><p><span>Our throughput benchmark results presented as improvement multiples over naive static batching, log scale.</span></p></div><p>It’s important to note how impressive even FasterTransformer’s 4x improvement is; we’re very interested in benchmarking FasterTransformers plus continuous batching when NVIDIA implements it. However, continuous batching is clearly a significant improvement over static batching even with an optimized model. The performance gap becomes gigantic when you include further memory optimization enabled by continuous batching and iteration-level scheduling as vLLM does.</p><h2>Benchmarking results: Latency</h2><p>Live-inference endpoints often face latency-throughput tradeoffs that must be optimized based on user needs. We benchmark latency on a realistic workload and measure how the <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function"><u>cumulative distribution function</u></a> of latencies changes with each framework.</p><p>Similar to the throughput benchmark, we configure the model to always emit a specified amount of tokens specified per-request. We prepare 100 randomly-generated prompts by sampling lengths from a <a href="https://en.wikipedia.org/wiki/Discrete_uniform_distribution"><u>uniform distribution</u></a> between 1 token and 512 tokens. We sample 100 output lengths from a capped exponential distribution with mean=128 and maximum size of 1536. These numbers were chosen because they are reasonably realistic and allow the generation to use up the full context-length of our model (512+1536=2048).</p><p>Instead of submitting all requests at the same time as done in the throughput benchmark, we delay each request by a predetermined number of seconds. We sample a <a href="https://en.wikipedia.org/wiki/Poisson_distribution"><u>Poisson distribution</u></a> to determine how long each request waits after the previously submitted request. The Poisson distribution is parameterized by λ, the expected rate, which in our case is how many queries per second (QPS) hit our model endpoint. We measure latencies at both QPS=1 and QPS=4 to see how the latency distribution changes as load changes.</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/4ElanYNZRv3sUBL0459zWV/ce78b3daf7e05f1bb84dad61906f1663/cb_09_latency_table.png" alt="cb 09 latency table"></p></div><p><span>Median generation request latency for each framework, under average load of 1 QPS and 4 QPS. Continuous batching systems improve median latency.</span></p></div><p>We see that while improving throughput, continuous batching systems also <i>improve</i> median latency. This is because continuous batching systems allow for new requests to be added to an existing batch if there is room, each iteration. But how about other percentiles? In fact, we find that they improve latency across all percentiles:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/6zynLiX4AJVO23tRfQ1rnV/763589eb4a6418157f21a51e6e36abaf/cb_10_latency_cdf_qps_1.png" alt="cb 10 latency cdf qps=1"></p></div><p><span>Cumulative distribution function of generation request latencies for each framework with QPS=1. Static batchers and continuous batchers have distinct curve shapes caused by the presence of iteration-level batch scheduling in continuous batchers. All continuous batchers perform approximately equally under this load; FasterTransformers performs noticeably better than static batching on a naive model implementation.</span></p></div><p>The reason why continuous batching improves latency at all percentiles is the same as why it improves latency at p50: new requests can be added regardless of how far into generation other sequences in the batch are. However, like static batching, continuous batching is still limited by how much space is available on the GPU. As your serving system becomes saturated with requests, meaning a higher on-average batch size, there are less opportunities to inject new requests immediately when they are received. We can see this as we increase the average QPS to 4:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/2az2DSpj3IujUOOu2i5WPp/5f7457205acae98fcd7fb3170e93b773/cb_11_latency_cdf_qps_4.png" alt="cb 11 latency cdf qps=4"></p></div><p><span>Cumulative distribution function of generation request latencies for each framework with QPS=4. Compared to QPS=1, FasterTransformer’s distribution of latencies becomes more similar to static batching on a naive model. Both Ray Serve and text-generation-inference’s continuous batching implementations perform similarly, but noticeably worse than vLLM.</span></p></div><p>We observe that FasterTransformer becomes more similar to naive static batching, and that both text-generation-inference and Ray Serve’s implementation of continuous batching are on their way to look like FasterTransformer’s curve with QPS=1. That is, as the systems become saturated there are less opportunities to inject new requests immediately, so request latency goes up. This lines up with the vLLM curve – it remains mostly unchanged between QPS=1 and QPS=4. This is because due to its advanced memory optimizations, it has a higher maximum batch size.</p><p>Anecdotally, we observe that vLLM becomes saturated around QPS=8 with a throughput near 1900 token/s. To compare these numbers apples-to-apples to the other serving systems requires more experimentation; however we have shown that continuous batching significantly improves over static batching by 1) reducing latency by injecting new requests immediately when possible, and 2) enable advanced memory optimizations (in vLLM’s case) that increase the QPS that the serving system can handle before becoming saturated.</p><h2>Conclusion</h2><p>LLMs present some amazing capabilities, and we believe their impact is still mostly undiscovered. We have shared how a new serving technique, continuous batching, works and how it outperforms static batching. It improves throughput by wasting fewer opportunities to schedule new requests, and improves latency by being capable of immediately injecting new requests into the compute stream. We are excited to see what people can do with continuous batching, and where the industry goes from here.</p><h2>Try out continuous batching for yourself</h2><p>We have a <a href="https://github.com/ray-project/ray/blob/cc983fc3e64c1ba215e981a43dd0119c03c74ff1/doc/source/serve/doc_code/vllm_example.py"><u>vLLM + Ray Serve example</u></a> that allows you to try out continuous batching. We are integrating continuous batching systems into <a href="https://aviary.anyscale.com/"><u>Aviary</u></a>, a webapp <a href="https://www.anyscale.com/blog/announcing-aviary-open-source-multi-llm-serving-solution"><u>that allows you to compare the outputs of different LLMs in parallel</u></a>, and will release it within the week.</p><p><i>Acknowledgements. We’d like to thank the following people for assisting in benchmarking and/or reviewing our results. </i>Anyscale<i>: Stephanie Wang, Antoni Baum, Edward Oakes, and Amog Kamsetty; </i>UC Berkeley<i>: Zhuohan Li and Woosuk Kwon.</i></p><h2>Get involved with Ray</h2><p>The <a href="https://github.com/anyscale/llm-continuous-batching-benchmarks"><u>code used for the experiments in the blog post is here</u></a>. To connect with the Ray community, join <a href="https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform"><u>the Ray Slack</u></a> or ask questions <a href="https://discuss.ray.io/"><u>on the Discuss forum</u></a>. If you are interested in hosting LLMs, check out <a href="https://www.anyscale.com/platform"><u>our managed Ray offering</u></a>. If you are interested in learning more about Ray, see <a href="http://ray.io/">ray.io</a> and <a href="http://docs.ray.io/">docs.ray.io</a>.</p><p>See our earlier <a href="https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure"><u>blog series on solving Generative AI infrastructure</u></a> and using <a href="https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray"><u>LangChain with Ray</u></a>.</p><p><b>Ray Summit 2023</b>: If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, <a href="https://raysummit.anyscale.com/"><u>join Ray Summit on September 18-20th</u></a>! We have a set of great keynote speakers including <a href="http://joschu.net/"><u>John Schulman</u></a> from OpenAI and <a href="https://aidangomez.ca/"><u>Aidan Gomez</u></a> from <a href="https://cohere.com/"><u>Cohere</u></a>, community and tech talks about Ray <a href="https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb"><u>as well as practical training focused on LLMs</u></a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New responsibilities (113 pts)]]></title>
            <link>https://www.hadess.net/2023/08/new-responsibilities.html</link>
            <guid>37131263</guid>
            <pubDate>Tue, 15 Aug 2023 07:40:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hadess.net/2023/08/new-responsibilities.html">https://www.hadess.net/2023/08/new-responsibilities.html</a>, See on <a href="https://news.ycombinator.com/item?id=37131263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-727042356482467106" itemprop="description articleBody">
<p><span>, my management chain has made the decision to stop all 
upstream and downstream work on desktop Bluetooth, multimedia 
applications (namely totem, rhythmbox and sound-juicer) and 
libfprint/fprintd. The rest of my upstream and downstream work will be 
reassigned depending on Red Hat's own priorities (see below), as I am 
transferred to another team that deals with one of a list of Red Hat’s 
priority projects.</span></p><p><span>I'm
 very disappointed, because those particular projects were already 
starved for resources: I spent less than 10% of my work time on them in 
the past year, with other projects and responsibilities taking most of 
my time.</span></p><p><span>This means that, in the medium-term at least, all those GNOME projects will go without a maintainer, reviewer, or triager:</span></p><p><span>- gnome-bluetooth (including Settings panel and gnome-shell integration)</span></p><p><span>- totem, totem-pl-parser, gom</span></p><p><span>- libgnome-volume-control</span></p><p><span>- libgudev</span></p><p><span>- geocode-glib</span></p><p><span>- gvfs AFC backend</span></p><p><span>Those freedesktop projects will be archived until further notice:</span></p><p><span>- power-profiles-daemon</span></p><p><span>- switcheroo-control</span></p><p><span>- iio-sensor-proxy</span></p><p><span>- low-memory-monitor</span></p><p><span>I will not be available for reviewing libfprint/fprintd, upower, grilo/grilo-plugins, gnome-desktop thumbnailer sandboxing patches, or any work related to XDG specifications.</span></p><p><span>Kernel
 work, reviews and maintenance, including recent work on SteelSeries 
headset and Logitech devices kernel drivers, USB revoke for Flatpak 
Portal support, or core USB is suspended until further notice.</span></p><p><span>All my <a href="https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/thread/WRHVGQBKKFU74CBO3CHIJC3Q5VEKH2AV/">Fedora </a></span><span><a href="https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/thread/WRHVGQBKKFU74CBO3CHIJC3Q5VEKH2AV/">packages 
were orphaned</a> about a month and a half ago, it's likely that there are 
still some that are orphaned, if there are takers. RHEL packages were 
unassigned about 3 weeks ago, they've been reassigned 
since then, so I cannot point to the new maintainer(s).</span></p><p><span>If
 you are a partner, or a customer, I would recommend that you get in 
touch with your Red Hat contacts to figure out what the plan is going 
forward for the projects you might be involved with.</span></p><p><span>If
 you are a colleague that will take on all or part of the 90% of the 
work that's not being stopped, or a community member that was relying on
 my work to further advance your own projects, get in touch, I'll do my 
best to accommodate your queries, time permitting.</span></p><p><span>I'll try to make sure to update this post, or create a new one if and when any of the above changes.<br></span></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Features of Project Loom incorporated in JDK 21 (188 pts)]]></title>
            <link>https://jdk.java.net/loom/</link>
            <guid>37130138</guid>
            <pubDate>Tue, 15 Aug 2023 04:21:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jdk.java.net/loom/">https://jdk.java.net/loom/</a>, See on <a href="https://news.ycombinator.com/item?id=37130138">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="footer"><p><a href="https://oracle.com/"><img alt="Oracle logo" src="https://jdk.java.net/images/oracle.png"></a></p><p> © 2023 Oracle Corporation and/or its affiliates </p><div><p><a href="https://jdk.java.net/tou">Terms of Use</a>
          · <a href="https://www.oracle.com/legal/privacy/">Privacy</a>
          · <a href="https://openjdk.org/legal/openjdk-trademark-notice.html">Trademarks</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: t.co is adding a five-second delay to some domains (427 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37130060</link>
            <guid>37130060</guid>
            <pubDate>Tue, 15 Aug 2023 04:09:03 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37130060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37130129"><td></td></tr>
                <tr id="37130240"><td></td></tr>
            <tr id="37130260"><td></td></tr>
            <tr id="37130194"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130194" href="https://news.ycombinator.com/vote?id=37130194&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>Agree/confirmed - just recorded a number of different nytimes urls that pass through t.co, all 4.7s+. various cnbc and google articles through t.co were ~130-200ms response time from t.co specifically (not total redirect-&gt;page load).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130246"><td></td></tr>
            <tr id="37130186"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130186" href="https://news.ycombinator.com/vote?id=37130186&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>I almost didn't believe OP, because it's so comically inept and petty. But, I can also confirm in some private testing there is a deliberate delay.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130327"><td></td></tr>
                        <tr id="37130143"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130143" href="https://news.ycombinator.com/vote?id=37130143&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>I think that HN itself also shadow flags submissions from a list of domains it doesn't like.<p>Try submitting a URL from the following domains, and it will be automatically flagged (but you can't see its flagged unless you log out):</p><pre><code>  - archive.is
  - watcher.guru
  - stacker.news
  - zerohedge.com
  - freebeacon.com
  - thefederalist.com
  - breitbart.com</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130147"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130147" href="https://news.ycombinator.com/vote?id=37130147&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>Well, yes, many sites are banned on HN. Others are penalized (see e.g. <a href="https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;query=by%3Adang%20%22major%20media%22&amp;sort=byDate&amp;type=comment" rel="nofollow noreferrer">https://hn.algolia.com/?dateRange=all&amp;page=0&amp;prefix=true&amp;que...</a>). None of this is secret, though we don't publish the lists themselves.<p>Edit: about 67k sites are banned on HN. Here's a random selection of 10 of them:</p><pre><code>  vodlockertv.com
  biggboss.org
  infoocode.com
  newyorkpersonalinjuryattorneyblog.com
  moringajuice.wordpress.com
  surrogacymumbai.com
  maximizedlivingdrlabrecque.com
  radio.com
  gossipcare.com
  tecteem.com</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130185"><td></td></tr>
                <tr id="37130234"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37130234" href="https://news.ycombinator.com/vote?id=37130234&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>The problem is that if you publish the lists it leads to more abuses. For example if spammers find out which sites are banned then they just post other ones.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130270"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37130270" href="https://news.ycombinator.com/vote?id=37130270&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>&gt; For example if spammers find out which sites are banned then they just post other ones.<p>I don't think that makes sense. The supposed spammers can just try looking up whether their submissions show up or not when not logged in.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130279"><td></td></tr>
                              <tr id="37130261"><td></td></tr>
                <tr id="37130315"><td></td></tr>
                        <tr id="37130155"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130155" href="https://news.ycombinator.com/vote?id=37130155&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>The difference is that HN is explicitly heavily moderated while Twitter pretends to be an equitable free speech platform.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130144"><td></td></tr>
                <tr id="37130157"><td></td></tr>
                <tr id="37130177"><td></td></tr>
                <tr id="37130220"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37130220" href="https://news.ycombinator.com/vote?id=37130220&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>And how was  the decision made to ban Federalist, but not say Guardian or The Daily Beast? Do you have any process in place to ensure that your political biases don't influence the list, or you don't care about that?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130299"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37130299" href="https://news.ycombinator.com/vote?id=37130299&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>Hey, man, if you want to go read those sites go for it. It's a free country.<p>This is a moderated site targeted at a specific community. It's under no obligation to be politically balanced. It's certainly under no obligation to promote right-wing propaganda and hate.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37130325"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37130325" href="https://news.ycombinator.com/vote?id=37130325&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>&gt; It's under no obligation to be politically balanced.<p>And obviously, I'm under no obligation to not voice my concern about that.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130255"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37130255" href="https://news.ycombinator.com/vote?id=37130255&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>I'm guessing it's reactive, and Federalist links tended to be garbage often enough to convince someone they should hit the ban button, whereas the others didn't rise up with trash often enough to matter?</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37130170"><td></td></tr>
                        <tr id="37130217"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130217" href="https://news.ycombinator.com/vote?id=37130217&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>the wise man bowed his head solemnly and spoke: "theres actually zero difference between good &amp; bad things." -- @dril</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130078"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130078" href="https://news.ycombinator.com/vote?id=37130078&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>Remember when people were excoriating Google AMP for encouraging walled gardens? If true, this seems in so much worse faith than that.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130131"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130131" href="https://news.ycombinator.com/vote?id=37130131&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>Not worse. They are both as evil as it gets. Typical: take public resource and use it for an exclusive  profit.<p>What happened to net neutrality? Could it applied for this case?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130099"><td></td></tr>
                <tr id="37130191"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130191" href="https://news.ycombinator.com/vote?id=37130191&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>Enshitification is different. It’s when companies destroy a product with hundreds of changes that prioritise
internal politics above what end users want.<p>This is something else - just the ego of one rich guy petulantly satisfying his inner demons.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                            <tr id="37130102"><td></td></tr>
                <tr id="37130163"><td></td></tr>
            <tr id="37130151"><td></td></tr>
                  <tr id="37130179"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130179" href="https://news.ycombinator.com/vote?id=37130179&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>If true and intentional, then this is a strong move by Musk against his ideological opponents. Hard to believe he has the cognizance to recognize them as such but maybe he purged more of the 3-letter agency folks from X than it seemed.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130111"><td></td></tr>
            <tr id="37130073"><td></td></tr>
                <tr id="37130101"><td></td></tr>
                <tr id="37130117"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37130117" href="https://news.ycombinator.com/vote?id=37130117&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>A good test might include a bunch of domains. And checking the timing on each. Could we demonstrate the delay is on t.co and not on NYT?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130180"><td></td></tr>
                  <tr id="37130208"><td></td></tr>
                  <tr id="37130109"><td></td></tr>
            <tr id="37130226"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130226" href="https://news.ycombinator.com/vote?id=37130226&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><p><span>they already told you they did tested it and you don't believe them.<p>what else could they say that would make you believe them?</p><p>you might as well just test it yourself like i did with time wget. it's not like you're going to believe anything anyone writes.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130137"><td></td></tr>
            <tr id="37130119"><td></td></tr>
                <tr id="37130192"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130192" href="https://news.ycombinator.com/vote?id=37130192&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>It is probably not illegal in America. Would it be illegal in Europe? Because (at least w/r/t Threads) it is an anti-competitive practice?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37130161"><td></td></tr>
                  <tr id="37130097"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37130097" href="https://news.ycombinator.com/vote?id=37130097&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>i mean, you can stop visiting the site, no? just leave, bro. it's not that hard. there are other means to connect to people.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130107"><td></td></tr>
            <tr id="37130127"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130127" href="https://news.ycombinator.com/vote?id=37130127&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>You're right! Which is why making Twitter's product <i>worse</i> when there are active competitors taking big bites out of their business seems... dumb?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130245"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37130245" href="https://news.ycombinator.com/vote?id=37130245&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>yep, it also seems to me that the helmsman of that site is dumb. good thing i left that site many years ago.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37130165"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37130165" href="https://news.ycombinator.com/vote?id=37130165&amp;how=up&amp;goto=item%3Fid%3D37130060"></a></center>    </td><td><br><div>
                  <p><span>Unfortunately not. All of my local government agencies - Police, Fire, DOT, Weather Service, Emergency Management updates, etc. are exclusively on twitter - they frequently post things there they don't even post to their own websites.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37130268"><td></td></tr>
                        </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The unpublished preface to Orwell’s Animal Farm (239 pts)]]></title>
            <link>https://mindmatters.ai/2023/08/a-warning-from-the-unpublished-preface-to-orwells-animal-farm/</link>
            <guid>37129768</guid>
            <pubDate>Tue, 15 Aug 2023 03:18:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mindmatters.ai/2023/08/a-warning-from-the-unpublished-preface-to-orwells-animal-farm/">https://mindmatters.ai/2023/08/a-warning-from-the-unpublished-preface-to-orwells-animal-farm/</a>, See on <a href="https://news.ycombinator.com/item?id=37129768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-swiftype-name="body">
			
<p><a href="http://www.bbc.co.uk/history/historic_figures/orwell_george.shtml">George Orwell</a>‘s novella <em><a href="http://www.george-orwell.org/Animal_Farm/0.html">Animal Farm</a></em> (1945) was a political fable. The cleverly portrayed animals who chase off the farmer and try to run the farm as a utopia slowly begin to replicate all the attitudes and practices against which they had rebelled. The story, summarized <a href="https://interestingliterature.com/2020/05/a-summary-and-analysis-of-george-orwells-animal-farm/">here,</a> satirizes the Soviet Union’s transition from revolution to totalitarianism under Joseph Stalin (1878–1953). In fact, the animal characters and incidents are often allusions to <a href="https://www.enotes.com/homework-help/what-some-examples-allusions-book-animal-farm-349433">historical Soviet figures and events.</a></p>







<figure><div>
<p><iframe title="Animal Farm Video Summary" width="500" height="281" src="https://www.youtube.com/embed/BFP1IMyKyy4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div></figure>







<p>His Preface, <a href="https://orwell.ru/library/novels/Animal_Farm/english/efp_go">“The Freedom of the Press,”</a> was omitted from the first edition of the book, then disappeared, and <a href="https://www.bl.uk/collection-items/orwells-proposed-introduction-to-animal-farm">was not rediscovered until 1971.</a> From it, we learn that Orwell had considerable difficulty getting his fable published. That wasn’t principally because of wartime issues. There was a shortage of books and his was highly readable. Rather, British intellectuals of the day did not wish to hear any criticism of Stalin or allusions to his atrocities:</p>



<p>Obviously it is not desirable that a government department should have any power of censorship (except security censorship, which no one objects to in war time) over books which are not officially sponsored. But the chief danger to freedom of thought and speech at this moment is not the direct interference of the MOI or any official body. If publishers and editors exert themselves to keep certain topics out of print, it is not because they are frightened of prosecution but because they are frightened of public opinion. In this country intellectual cowardice is the worst enemy a writer or journalist has to face, and that fact does not seem to me to have had the discussion it deserves…</p>


<div>
<figure><img decoding="async" loading="lazy" src="https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-892x1597.jpg" alt="" width="276" height="494" srcset="https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-892x1597.jpg 892w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-551x987.jpg 551w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-768x1375.jpg 768w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-858x1536.jpg 858w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1-1144x2048.jpg 1144w, https://mindmatters.ai/wp-content/uploads/sites/2/2023/08/Animal-Farm-1.jpg 1430w" sizes="(max-width: 276px) 100vw, 276px"></figure></div>


<p>At this moment what is demanded by the prevailing orthodoxy is an uncritical admiration of Soviet Russia. Everyone knows this, nearly everyone acts on it. Any serious criticism of the Soviet régime, any disclosure of facts which the Soviet government would prefer to keep hidden, is next door to unprintable. And this nation-wide conspiracy to flatter our ally takes place, curiously enough, against a background of genuine intellectual tolerance. For though you arc not allowed to criticise the Soviet government, at least you are reasonably free to criticise our own. Hardly anyone will print an attack on Stalin, but it is quite safe to attack Churchill, at any rate in books and periodicals. And throughout five years of war, during two or three of which we were fighting for national survival, countless books, pamphlets and articles advocating a compromise peace have been published without interference. More, they have been published without exciting much disapproval. So long as the prestige of the USSR is not involved, the principle of free speech has been reasonably well upheld. There are other forbidden topics, and I shall mention some of them presently, but the prevailing attitude towards the USSR is much the most serious symptom. It is, as it were, spontaneous, and is not due to the action of any pressure group. </p>



<p>Orwell, it should be said, was very much a man of the Left. But he was not a totalitarian. That combination perhaps enabled him to publish some of the most broadly appealing  popular-level dissections of the evils of totalitarian rule in English.</p>



<p>For example, he offers us a significant insight in the passage above. The censorship he had to address was not a conspiracy or even a campaign; it was spontaneous. Every right-thinking intellectual somehow <em>knew</em> that a candid assessment of Soviet rule was, well, just <em>not the done thing!…</em></p>



<p>Why not? Well, gentle reader, if you have ever encountered such an environment, you will know — or suspect anyway — that most of the people who know for sure which political views need censoring could not ably defend their opinion. Their defense is, precisely, groupthink. They don’t need to think much about it individually. And they don’t. In fact, if you challenge them on their censorship, they may act aggrieved, as if they were the victims of a calculated personal injury. It’s doubtless all the more tiresome if, as Orwell found, the groupthinkers are held up as the leading intellectuals of the day:</p>



<p>But now to come back to this book of mine. The reaction towards it of most English intellectuals will be quite simple: ‘It oughtn’t to have been published.’ Naturally, those reviewers who understand the art of denigration will not attack it on political grounds but on literary ones. They will say that it is a dull, silly book and a disgraceful waste of paper. This may well be true, but it is obviously not [th]e whole of the story. One does not say that a book ‘ought not to have been published’ merely because it is a bad book. After all, acres of rubbish are printed daily and no one bothers. The English intelligentsia, or most of them, will object to this book because it traduces their Leader and (as they see it) does harm to the cause of progress. If it did [th]e opposite they would have nothing to say against it, even if its literary faults were ten times as glaring as they are. The success of, for instance, the Left Book Club over a period of four or five years shows how willing they are to tolerate both scurrility and slipshod writing, provided that it tells them what they want to hear.</p>



<p>And he ends his Preface on a high note,</p>



<p>I know that the English intelligentsia have plenty of reason for their timidity and dishonesty, indeed I know by heart the arguments by which they justify themselves. But at least let us have no more nonsense about defending liberty against Fascism. If liberty means anything at all it means the right to tell people what they do not want to hear.</p>



<p>Orwell would doubtless be pleased that millions of people worldwide have offered a much more positive assessment of <em>Animal Farm.</em> Many of us might also find key points of comparison between his situation and the shrill calls for censorship that we hear so often today.</p>







<figure><div>
<p><iframe loading="lazy" title="Animal Farm trailer" width="500" height="375" src="https://www.youtube.com/embed/LAeKX5n-5IE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div></figure>







<p><em>Note:</em> The better-known <em><a href="https://www.amazon.com/1984-George-Orwell/dp/0451516753">1984</a></em> was not published until 1949, not long before Orwell’s death from tuberculosis. Also, “George Orwell” was a pen name; he was known in life as <a href="https://www.cliffsnotes.com/literature/n/1984/george-orwell-biography">Eric Blair.</a></p>



<p><em>You may also wish to read:</em> In Big Tech World: the journalist as <a href="https://mindmatters.ai/2021/02/in-big-tech-world-the-journalist-as-censor-hit-man-and-snitch/">censor, hit man, and snitch.</a> Glenn Greenwald looks at a disturbing trend in media toward misrepresentation as well as censorship.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lynn Conway's Story (2000) (158 pts)]]></title>
            <link>https://ai.eecs.umich.edu/people/conway/LynnsStory.html</link>
            <guid>37129132</guid>
            <pubDate>Tue, 15 Aug 2023 01:39:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.eecs.umich.edu/people/conway/LynnsStory.html">https://ai.eecs.umich.edu/people/conway/LynnsStory.html</a>, See on <a href="https://news.ycombinator.com/item?id=37129132">Hacker News</a></p>
<div id="readability-page-1" class="page">

<dl>
  <dt><center>&nbsp;</center>
  </dt><dt><center></center>
</dt></dl>

<center><i>This is the story of a woman who made amazing contributions
to society,<br>
in spite of intense ostracism and stigmatization just for trying
to be herself, <br>
and how she did it by taking on a secret new identity, and living
her life in "stealth mode".</i></center>

<center>#</center>

<p>
<a href="http://ai.eecs.umich.edu/people/conway/Awards/ElectronicDesign/HallOfFame.html">Lynn Conway is a 
<span>famed pioneer of 
microelectronics chip design</span></a>.
Her innovations during the 1970's at the Xerox Palo Alto Research
Center (PARC) have impacted chip design worldwide. Many high-tech
companies and computing methods have foundations in her work.</p>

<p>Thousands of chip designers learned their craft from Lynn's
textbook <i>Introduction to VLSI Systems</i>, which she co-authored
with Prof. Carver Mead of Caltech. Thousands more did their first
VLSI design projects using the government's MOSIS prototyping
system, which is based directly on Lynn's work at PARC. 
<span>
<a href="http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;articleId=9046420">
Much of the modern silicon chip design revolution is based on her work</a></span>.</p>

<p>Lynn went on to win many awards and high honors, including
election as a Member of the National Academy of Engineering, the
highest professional recognition an engineer can receive.</p>

<center>#</center>

<p>What no one knew till recently is that Lynn also did earlier
pioneering research at IBM in the 1960's. Fresh out of grad school,
she invented a powerful method for issuing multiple out-of-order
instructions per machine cycle in supercomputers. By solving this
fundamental computer architecture problem way back in 1965, she
made possible the creation of the first true superscalar computer,
and participated in its design at IBM. Lynn called her invention
dynamic instruction scheduling (DIS).</p>

<p>By the 90's, chips held enough transistors so that entire superscalar
computers could be put on single chips. Lynn's DIS invention suddenly
became used in almost all the powerful new PC chips, making them
much more powerful than they'd otherwise have been. Lynn's work
thus had yet another big impact on the modern information technology
revolution.</p>

<p>Most computer engineers thought DIS was a generalization of
decades of work, and had no idea it had been invented in 1965.
It caused Lynn great angst to see her wonderful invention so widely
used, and described in all the computer architecture textbooks,
without anyone knowing it was her idea.</p>

<center>#</center>

<p>How could this oversight have happened? Why did Lynn remain
silent for over three decades about her IBM work?</p>

<p>The answer is that women like Lynn have lived, especially in
the past, in a holocaust of stigmatization, persecution and violence.
They could not reveal their past identities without risking great
physical danger to themselves, and great harm to their careers
and their personal relationships.</p>

<p>You see, Lynn was born and raised as a boy. It was a terrible
mistake, because Lynn had the brain-sex and gender identity of
a girl. However, back in the forties and fifties there wasn't
any knowledge about such things, and Lynn was forced to grow up
as a boy. She did the best she could at it, but suffered terribly
from what was happening to her. She was still a boy and had a
boy's name when she worked at IBM.</p>

<p>After years and years of trying to find help, she finally connected
with the pioneering physician Harry Benjamin, M.D. in 1966, shortly
after he'd published his seminal textbook <i>The Transsexual Phenomenon</i>.
That text was the first to describe the true nature of, and medical
solutions for, Lynn's mis-gendering affliction.</p>

<p>With Dr. Benjamin's help, Lynn began medical treatments in
1967. She became one of the very early transsexual women to undergo
hormonal and surgical sex reassignment to have her body completely
changed from that of a boy into that of a woman. Sadly, just before
Lynn underwent sex reassignment surgery in 1968, she was fired
by IBM for being transsexual and lost all connections to her important
work there.</p>

<center>#</center>

<p>Lynn's case was a first at IBM. The idea that a professional
person would seek a "sex change" totally shocked IBM's
management. Most transsexual women seeking help back then were
from among those who worked as "female impersonators"
or as prostitutes. Only those who were sure they could fully pass
as women, who were totally desperate and who had nothing to lose,
dared to change gender back then. When top IBM management learned
what Lynn was doing, she was fired in a maelstrom of animosity.
It is almost certain that the decision was made by T. J. Watson,
Jr., himself.</p>

<p>Lynn had managed to put together some fragile bits of support
and help from her family and friends. However, when IBM fired
her everyone lost confidence in what she was doing and her support
system collapsed. Lynn went abroad for her surgery, all alone.
She had lost not only her career and professional reputation,
but also her family, relatives, friends and colleagues. She faced
a frighteningly uncertain future without a soul in the world to
help her other than her doctors.</p>

<center>#</center>

<p>When Lynn returned, she made her social transition and took
on her new name. She started her career all over again as a lowly
contract programmer without a past. A gritty survivor, her adjustment
in her new role went completely against the dire predictions of
the IBM executives and all the family and the friends who had
deserted her. All alone she went out into the world, made new
friends and worked hard to succeed in her new life.</p>

<p>Amazingly, Lynn became so happy, and so full of life and hope
after her transformation, that her career took off like a rocket.
Moving up through a series of companies, she landed a computer
architecture job at Memorex in 1971. In 1973, she was recruited
by Xerox's exciting new Palo Alto Research Center, just as it
was forming.</p>

<p>By 1978, just 10 years after her gender transition, Lynn was
already on the verge of international fame in her field for her
VLSI innovations. By then she was writing the seminal textbook
on the subject, and was heading off to M. I. T. to teach the first
prototype course on VLSI systems.</p>

<p>Within two years, universities all over the world were adopting
her text for similar courses. The Department of Defense started
a major new program to sponsor research to build on her work.
Scores of startup companies began incubating and forming to commercialize
the knowledge. All this happened without people catching on to
Lynn's secret past. She could never have survived and done it
if they had.</p>

<center>#</center>

<p>In the 80's and 90's, Lynn went on to enjoy a wide-ranging,
influential career, and a wonderfully adventurous, fulfilling
and happy personal life. She is now Professor of Electrical Engineering
and Computer Science, Emerita, at the University of Michigan in
Ann Arbor, where she also served for many years as Associate Dean
of Engineering. She now lives on country property in rural Michigan
with her husband Charlie. They've been together since 1987.</p>

<p>However, for 31 years after her transition, Lynn carefully
remained in "stealth mode". Only her closest friends
knew about her past. Lynn knew of other transsexual women who
had been socially ostracized, ghettoized, beaten, gang-raped,
murdered or driven to suicide when "read" or otherwise
discovered by brutal, hateful people.</p>

<p>For years Lynn lived with an ever-present sense of danger,
fearful that exposure of her past could cause her to lose her
civil rights, legal rights and employment rights, and to suffer
estrangements in her professional and personal relationships.</p>

<center>#</center>

<p>In 1999, computer historians finally stumbled into Lynn's early
IBM work. They tracked it down to her, and her past was revealed
amongst her colleagues. Frightened at first, she gradually realized
times might have changed enough that she needn't be afraid to
be "out" now. She certainly has nothing at all to be
ashamed of, and is indeed very proud of the successes in her personal
life as well as those in her career.</p>

<p>At the same time, Lynn was dismayed that transsexual women
are still treated so inhumanely by parents, relatives, employers,
the legal system and society at large. The total rejection of teenage 
transgender and transsexual girls-to-be by their families is especially tragic,
since it often happens just as they first cry out for
help, and can doom them to years of marginalized existence.</p>

<p>Lynn began to think that her story might help somehow. Societal
views are partly a media problem. Images of transsexualism routinely
come from stories of "transition". That's a time when
media can focus on prurient, somewhat shocking and often embarrassing
aspects of someone's gender change. The stories seem superficially
sympathetic, but often convey a sad, dreary image. Readers are
left feeling sorry for the "poor things", and "certainly
wouldn't want it to happen in their family"!</p>

<p>What doesn't come through is the miracle of release from entrapment
in a male body that the transsexual girl experiences, and the
happiness she finds as a woman later on. Folks never learn about
the tens of thousands of post-operative women living among us
who are very successful and fully accepted as regular gals. The
public simply never sees these successes.</p>

<div><p>Why is this? Because almost all these women live in stealth,
just as Lynn did, fearing what might happen if their pasts were
revealed. Meanwhile, tens of thousands of young pre-operative
transsexuals live in fear and doubt about their futures. They
are often excommunicated by their families and lose their jobs,
as had happened to Lynn, when they identify their problem and
seek medical help. </p><p>

Lynn is the first truly successful case to come out of long-term
stealth and tell her story. That story should give hope to young
transsexuals. It should help parents see possibilities for happiness for a 
transsexual daughter-to-be, especially if they were to support
their child's efforts to transform a "boy's" body
and become a woman early enough in life. It should also give employers
pause for thought before firing someone - just because of their
transsexualism. </p></div>

<p>The day will come when gender transition is no longer be seen
as a sad, somewhat shameful and tragic event, but instead as a
wonderful life-giving miracle for those so unfortunate as to have
been mis-gendered at birth. Lynn hopes to live to see that day.</p>

<dl>
  <dt>&nbsp;
  </dt><dt>&nbsp;
  </dt><dt>&nbsp;
  </dt><dt><center><i>For background on transgender, transsexual and
  intersex issues, see <a href="http://ai.eecs.umich.edu/people/conway/TS/TS.html">TG/TS/IS
  Info.</a></i></center>
  </dt><dt><center><i>For more about Lynn's accomplishments, see her
  <a href="https://ai.eecs.umich.edu/people/conway/BioSketch.html">BioSketch.</a></i></center>
  </dt><dt><center><i>For more on her story, see her <a href="https://ai.eecs.umich.edu/people/conway/RetrospectiveT.html">Retrospective</a></i>
  <i>and also</i></center>
  </dt><dt><center><i>Lynn's Homepage: <a href="https://ai.eecs.umich.edu/people/conway/conway.html">http://www.lynnconway.com</a></i></center>
  </dt><dt>&nbsp;
  </dt><dt>&nbsp;
  </dt><dt>&nbsp;
  </dt><dt><center><b>A <i>Los Angeles Times Sunday Magazine</i> Feature
  Story about Lynn</b></center>
  </dt><dt><center><b>can also be retrieved at:</b></center>
  </dt><dt><center><a href="https://ai.eecs.umich.edu/people/conway/Media/Through%20the%20Gender%20Labyrinth.pdf"><span size="+1">Through the Gender Labyrinth.pdf</span></a></center>
  </dt><dt><center>&nbsp;</center>
  </dt><dt><center>&nbsp;</center>
  </dt><dt><center>&nbsp;</center>
  </dt><dt><center><img src="http://cgi.www.umich.edu/counter?link=http:/ai.eecs.umich.edu/people/conway/LynnsStory.html&amp;width=6&amp;font=simpson"></center>
  </dt><dt><center>&nbsp;</center>
  </dt><dt><center>Reset on 5-11-00<br>
  V-5-13-04</center>
</dt></dl>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bankman-Fried used $100M in stolen FTX funds for political donations, US says (254 pts)]]></title>
            <link>https://www.reuters.com/legal/bankman-fried-used-customer-funds-100-mln-us-political-donations-prosecutors-say-2023-08-14/</link>
            <guid>37128392</guid>
            <pubDate>Mon, 14 Aug 2023 23:57:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/bankman-fried-used-customer-funds-100-mln-us-political-donations-prosecutors-say-2023-08-14/">https://www.reuters.com/legal/bankman-fried-used-customer-funds-100-mln-us-political-donations-prosecutors-say-2023-08-14/</a>, See on <a href="https://news.ycombinator.com/item?id=37128392">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-image" role="figure" aria-describedby="primary-image-caption"><figure><div data-testid="Image"><p><img src="https://cloudfront-us-east-2.images.arcpublishing.com/reuters/6RDKDYUWOFMHPJTDYF7SGZ4IUY.jpg" srcset="https://www.reuters.com/resizer/cyhXAgZITfCPqTP0ns4WMKlZEgY=/480x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/6RDKDYUWOFMHPJTDYF7SGZ4IUY.jpg 480w,https://www.reuters.com/resizer/pTykvtuFz1Z7dlzPf1iOb16WEu4=/960x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/6RDKDYUWOFMHPJTDYF7SGZ4IUY.jpg 960w,https://www.reuters.com/resizer/5VTmp8WvaqPcCqqo5ITFnnKbEwA=/1080x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/6RDKDYUWOFMHPJTDYF7SGZ4IUY.jpg 1080w,https://www.reuters.com/resizer/6gg5bwQ8_1WWEOWXE8OoNx8M2Qs=/1200x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/6RDKDYUWOFMHPJTDYF7SGZ4IUY.jpg 1200w" sizes="(min-width: 1024px) 560px, (min-width: 1440px) 700px, 100vw" width="4631" height="3115" alt="Former FTX Chief Executive Bankman-Fried at a courthouse in New York"></p></div><p data-testid="Body">Sam Bankman-Fried, the founder of bankrupt cryptocurrency exchange FTX, arrives at court as lawyers push to persuade the judge overseeing his fraud case not to jail him ahead of trial, at a courthouse in New York, U.S., August 11, 2023.  REUTERS/Eduardo Munoz/File Photo</p></figure></div><div><p data-testid="paragraph-0">NEW YORK, Aug 14 (Reuters) - Sam Bankman-Fried used money he stole from customers of his FTX cryptocurrency exchange to make more than $100 million in political campaign contributions before the 2022 U.S. midterm elections, federal prosecutors said on Monday.</p><p data-testid="paragraph-1">An amended indictment accused the 31-year-old former billionaire of directing two FTX executives to evade contribution limits by donating to Democrats and Republicans, and to conceal where the money came from.</p><p data-testid="paragraph-2">"He leveraged this influence, in turn, to lobby Congress and regulatory agencies to support legislation and regulation he believed would make it easier for FTX to continue to accept customer deposits and grow," the indictment said.</p><p data-testid="paragraph-3">Bankman-Fried faces seven counts of conspiracy and fraud over FTX's collapse, though the indictment no longer includes conspiracy to violate campaign finance laws as a separate count.</p><p data-testid="paragraph-4">Federal prosecutors in Manhattan said last month they would drop that charge after the Bahamas, where FTX was based and where Bankman-Fried was arrested in December 2022, said it never intended to extradite him on that count.</p><p data-testid="paragraph-5">Instead, prosecutors told U.S. District Judge Lewis Kaplan last week that a new indictment would "make clear that Mr. Bankman-Fried remains charged with conducting an illegal campaign finance scheme as part of the fraud and money laundering schemes originally charged."</p><p data-testid="paragraph-6">Mark Botnick, a spokesman for Bankman-Fried, declined to comment.</p><p data-testid="paragraph-7">Bankman-Fried has previously pleaded not guilty to stealing billions of dollars in FTX customer funds to plug losses at Alameda Research, his crypto-focused hedge fund.</p><p data-testid="paragraph-8">Kaplan <a data-testid="Link" href="https://www.reuters.com/legal/ftxs-bankman-fried-seeking-avoid-jail-due-back-court-2023-08-11/">jailed him last Friday</a> ahead of his Oct. 2 trial, after finding probable cause that Bankman-Fried tampered with witnesses.</p><p data-testid="paragraph-9">Previously, Bankmman-Fried had been largely confined to his parents' Palo Alto, California, home on $250 million bond.</p><p data-testid="paragraph-10">Bankman-Fried rode a boom in cryptocurrency values to amass a fortune that was once estimated at $26 billion, and became an influential donor to mostly Democratic candidates and causes.</p><p data-testid="paragraph-11">The November 2022 collapse of FTX after a flurry of customer withdrawals destroyed his wealth and stained his reputation.</p><h2 data-testid="Heading">EX-FTX EXEC SALAME DECLINES TO TESTIFY</h2><p data-testid="paragraph-12">Bankman-Fried's indictment does not name the two people prosecutors say he used for "straw donors" to donate money at his direction. But other court papers and Federal Elections Commission data show they are Nishad Singh and Ryan Salame.</p><p data-testid="paragraph-13">Singh, FTX's former engineering chief, pleaded guilty to fraud and campaign finance violations in February. He donated $9.7 million to Democratic candidates and causes, and <a data-testid="Link" href="https://www.reuters.com/legal/ftxs-singh-agrees-plead-guilty-us-criminal-charges-lawyer-says-2023-02-28/">said in court</a> he knew the money came from FTX customers.</p><p data-testid="paragraph-14">Salame, the former co-CEO of FTX's Bahamian unit, gave more than $24 million to Republican candidates and causes in the 2022 election cycle, according to Federal Election Commision data.</p><p data-testid="paragraph-15">He has not been charged with a crime. In a separate court filing on Monday, prosecutors said Salame's lawyer had told them he would invoke his Fifth Amendment right against self-incrimination if called to testify.</p><p data-testid="paragraph-16">Prosecutors said Salame told a family member in a November 2021 message that Bankman-Fried wanted to use political donations to "weed-out" anti-crypto Democratic and Republican lawmakers, and would likely “route money through me to weed out that republican [sic] side.”</p><p data-testid="paragraph-17">Salame's lawyer did not immediately respond to a request for comment.</p><p><span data-testid="Text">Reporting by Luc Cohen in San Jose, California
Editing by Chris Reese, David Gregorio, Jonathan Oatis and Shri Navaratnam</span></p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank">The Thomson Reuters Trust Principles.</a></p><div><address><p data-testid="Body">Reports on the New York federal courts. Previously worked as a correspondent in Venezuela and Argentina.</p></address></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AI-town, run your own custom AI world SIM with JavaScript (399 pts)]]></title>
            <link>https://github.com/a16z-infra/ai-town</link>
            <guid>37128293</guid>
            <pubDate>Mon, 14 Aug 2023 23:46:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/a16z-infra/ai-town">https://github.com/a16z-infra/ai-town</a>, See on <a href="https://news.ycombinator.com/item?id=37128293">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">AI Town <g-emoji alias="house" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3e0.png">🏠</g-emoji><g-emoji alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png">💻</g-emoji><g-emoji alias="love_letter" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f48c.png">💌</g-emoji></h2>
<p dir="auto"><a href="https://www.convex.dev/ai-town" rel="nofollow">Live Demo</a></p>
<p dir="auto"><a href="https://discord.gg/PQUmTBTGmT" rel="nofollow">Join our community Discord: AI Stack Devs</a></p>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/3489963/260520547-a4c91f17-23ed-47ec-8c4e-9f9a8505057d.png"><img width="1454" alt="Screen Shot 2023-08-14 at 10 01 00 AM" src="https://user-images.githubusercontent.com/3489963/260520547-a4c91f17-23ed-47ec-8c4e-9f9a8505057d.png"></a>
<p dir="auto">AI Town is a virtual town where AI characters live, chat and socialize.</p>
<p dir="auto">This project is a deployable starter kit for easily building and customizing your own version of AI town. Inspired by the research paper <a href="https://arxiv.org/pdf/2304.03442.pdf" rel="nofollow"><em>Generative Agents: Interactive Simulacra of Human Behavior</em></a>.</p>
<p dir="auto">The primary goal of this project, beyond just being a lot of fun to work on, is to provided a platform with a strong foundation that is meant to be extended. The back-end engine natively supports shared global state, transactions, and a journal of all events so should be suitable for everything from a simple project to play around with to a scalable, multi-player game. A secondary goal is to make a JS/TS framework available as most simulators in this space (including the original paper above) are written in Python.</p>
<h2 tabindex="-1" dir="auto">Overview</h2>
<ul dir="auto">
<li><g-emoji alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png">💻</g-emoji> <a href="#stack">Stack</a></li>
<li><g-emoji alias="brain" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f9e0.png">🧠</g-emoji> <a href="#installation">Installation</a></li>
<li><g-emoji alias="bust_in_silhouette" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f464.png">👤</g-emoji> <a href="#customize-your-own-simulation">Customize - run YOUR OWN simulated world</a></li>
<li><g-emoji alias="trophy" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png">🏆</g-emoji> <a href="#credits">Credits</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Stack</h2>
<ul dir="auto">
<li>Game engine &amp; Database: <a href="https://convex.dev/" rel="nofollow">Convex</a></li>
<li>VectorDB: <a href="https://www.pinecone.io/" rel="nofollow">Pinecone</a></li>
<li>Auth: <a href="https://clerk.com/" rel="nofollow">Clerk</a></li>
<li>Text model: <a href="https://platform.openai.com/docs/models" rel="nofollow">OpenAI</a></li>
<li>Deployment: <a href="https://fly.io/" rel="nofollow">Fly</a></li>
<li>Pixel Art Generation: <a href="https://replicate.com/" rel="nofollow">Replicate</a>, <a href="https://serverless.fal.ai/lora" rel="nofollow">Fal.ai</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Installation</h2>
<h3 tabindex="-1" dir="auto">Clone repo and Install packages</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:a16z-infra/ai-town.git
cd AI-town
npm install
npm run dev"><pre>git clone git@github.com:a16z-infra/ai-town.git
<span>cd</span> AI-town
npm install
npm run dev</pre></div>
<p dir="auto"><code>npm run dev</code> will fail asking for environment variables.
Enter them in the environment variables on your Convex dashboard to proceed.
You can get there via <code>npx convex dashboard</code> or <a href="https://dashboard.convex.dev/" rel="nofollow">https://dashboard.convex.dev</a>
See below on how to get the various environnment variables.</p>
<p dir="auto">a. <strong>Set up Clerk</strong></p>
<ul dir="auto">
<li>Go to <a href="https://dashboard.clerk.com/" rel="nofollow">https://dashboard.clerk.com/</a> and click on "Add Application"</li>
<li>Name your application and select the sign-in providers you would like to offer users</li>
<li>Create Application</li>
<li>Add <code>NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY</code> and <code>CLERK_SECRET_KEY</code> to <code>.env.local</code></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_***
CLERK_SECRET_KEY=sk_***"><pre>NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=pk_<span>***</span>
CLERK_SECRET_KEY=sk_<span>***</span></pre></div>
<ul dir="auto">
<li>Go to JWT Templates and create a new Convex Template.</li>
<li>Copy the JWKS endpoint URL for use below.</li>
</ul>
<p dir="auto">b. <strong>OpenAI API key</strong></p>
<p dir="auto">Visit <a href="https://platform.openai.com/account/api-keys" rel="nofollow">https://platform.openai.com/account/api-keys</a> to get your OpenAI API key if you're using OpenAI for your language model.</p>
<p dir="auto">c. <strong>Pinecone API keys</strong></p>
<ul dir="auto">
<li>Create a Pinecone index by visiting <a href="https://app.pinecone.io/" rel="nofollow">https://app.pinecone.io/</a> and click on "Create Index"</li>
<li>Give it an index name (this will be the environment variable <code>PINECONE_INDEX_NAME</code>)</li>
<li>Fill in Dimension as <code>1536</code></li>
<li>Once the index is successfully created, click on "API Keys" on the left side nav and create an API key: copy "Environment" value to <code>PINECONE_ENVIRONMENT</code> variable, and "Value" to <code>PINECONE_API_KEY</code></li>
</ul>
<p dir="auto">d. <strong>Add secrets to the convex dashboard</strong></p>

<p dir="auto">Go to "settings" and add the following environment varables. <code>CLERK_ISSUER_URL</code> should be the URL from the JWKS endpoint.</p>
<div dir="auto" data-snippet-clipboard-copy-content="OPENAI_API_KEY  sk-*******
CLERK_ISSUER_URL  https://****
PINECONE_API_KEY  ********
PINECONE_ENVIRONMENT us****
PINECONE_INDEX_NAME  ********"><pre>OPENAI_API_KEY  sk-<span>*******</span>
CLERK_ISSUER_URL  https://<span>****</span>
PINECONE_API_KEY  <span>********</span>
PINECONE_ENVIRONMENT us<span>****</span>
PINECONE_INDEX_NAME  <span>********</span></pre></div>
<h3 tabindex="-1" dir="auto">Run the code</h3>
<p dir="auto">To run both the front and and back end:</p>

<p dir="auto">You can now visit <a href="http://localhost:%5BPORT_NUMBER%5D" rel="nofollow">http://localhost:[PORT_NUMBER]</a></p>
<p dir="auto">If you'd rather run the frontend in a separate terminal from Convex (which syncs
your backend functions as they're saved), you can run these two commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm run dev:frontend
npm run dev:backend"><pre>npm run dev:frontend
npm run dev:backend</pre></div>
<p dir="auto">See package.json for details, but dev:backend runs <code>npx convex dev</code></p>
<p dir="auto">*Note: The simulation will pause after 5 minutes if the window is idle.
Loading the page will unpause it. If you want to run the world without the
browser, you can comment-out the heartbeat check in <code>convex/engine.ts</code></p>
<h3 tabindex="-1" dir="auto">Various commands to run / test / debug</h3>
<p dir="auto"><strong>To add a new world, seed it, and start it running</strong></p>
<p dir="auto"><strong>Note</strong>: you can add <code>--no-push</code> to run these commands without first syncing
the functions. If you already have <code>npm run dev</code> running, this will be faster.
If you remove it, it'll push up the latest version of code before running the
command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="npx convex run init:reset"><pre>npx convex run init:reset</pre></div>
<p dir="auto"><strong>To go one iteration at a time, you can create a world with</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="npx convex run --no-push init:resetFrozen

# for each iteration
npx convex run --no-push engine:tick '{&quot;worldId&quot;:&quot;<your world id>&quot;,&quot;noSchedule&quot;:true}'"><pre>npx convex run --no-push init:resetFrozen

<span><span>#</span> for each iteration</span>
npx convex run --no-push engine:tick <span><span>'</span>{"worldId":"&lt;your world id&gt;","noSchedule":true}<span>'</span></span></pre></div>
<p dir="auto"><strong>To freeze the back end, in case of too much activity</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="npx convex run --no-push engine:freezeAll

# when ready to rerun (defaults to latest world)
npx convex run --no-push engine:unfreeze"><pre>npx convex run --no-push engine:freezeAll

<span><span>#</span> when ready to rerun (defaults to latest world)</span>
npx convex run --no-push engine:unfreeze</pre></div>
<p dir="auto"><strong>To clear all databases</strong></p>
<p dir="auto">Many options:</p>
<ul dir="auto">
<li>Go to the dashboard <code>npx convex dashboard</code> and clear tables from there.</li>
<li>Adjust the variables in <a href="https://github.com/a16z-infra/ai-town/blob/main/convex/crons.ts"><code>crons.ts</code></a> to automatically clear
up space from old journal and memory entries.</li>
<li>Run <code>npx convex run --no-push testing:debugClearAll</code> to wipe all the tables.</li>
<li>As a fallback, if things are stuck, you can check out the <code>origin/reset-town</code>
git branch. Doing <code>npm run dev</code> from there will clear your schema, stop your
functions, and allow you to delete your tables in the dashboard.</li>
</ul>
<p dir="auto">To delete all vectors from the Pinecone index, you can run:</p>
<div data-snippet-clipboard-copy-content="npx convex run --no-push lib/pinecone:deleteAllVectors"><pre><code>npx convex run --no-push lib/pinecone:deleteAllVectors
</code></pre></div>
<p dir="auto"><strong>NOTE</strong>: If you share this index between dev &amp; prod, or between projects,
it will wipe them all out. You generally don't need to be deleting vectors from
Pinecone, as each query is indexed on the userId, which is unique between worlds
and backend instances.</p>
<p dir="auto"><strong>To Snoop on messages</strong></p>
<p dir="auto">Run the following in a side terminal</p>
<div dir="auto" data-snippet-clipboard-copy-content="npx convex run testing:listMessages --no-push --watch"><pre>npx convex run testing:listMessages --no-push --watch</pre></div>
<p dir="auto">Or to watch one player's state:</p>
<div dir="auto" data-snippet-clipboard-copy-content="npx convex run testing:latestPlayer --no-push --watch"><pre>npx convex run testing:latestPlayer --no-push --watch</pre></div>
<p dir="auto">See more functions in <a href="https://github.com/a16z-infra/ai-town/blob/main/convex/testing.ts"><code>testing.ts</code></a>.</p>
<h3 tabindex="-1" dir="auto">Deploy the app</h3>
<h4 tabindex="-1" dir="auto">Deploy to fly.io</h4>
<ul dir="auto">
<li>
<p dir="auto">Register an account on fly.io and then <a href="https://fly.io/docs/hands-on/install-flyctl/" rel="nofollow">install flyctl</a></p>
</li>
<li>
<p dir="auto"><strong>If you are using Github Codespaces</strong>: You will need to <a href="https://fly.io/docs/hands-on/install-flyctl/" rel="nofollow">install flyctl</a> and authenticate from your codespaces cli by running <code>fly auth login</code>.</p>
</li>
<li>
<p dir="auto">Run <code>npx convex deploy</code> to deploy your dev environment to prod environment. Make sure you copy over all secrets to Convex's prod environment</p>
</li>
<li>
<p dir="auto">Run <code>fly launch</code> under project root. This will generate a <code>fly.toml</code> that includes all the configurations you will need</p>
</li>
<li>
<p dir="auto">Modify generated <code>fly.toml</code> to include <code>NEXT_PUBLIC_*</code> during build time for NextJS to access client side.</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="[build]
  [build.args]
    NEXT_PUBLIC_CLERK_SIGN_IN_URL=&quot;/sign-in&quot;
    NEXT_PUBLIC_CLERK_SIGN_UP_URL=&quot;/sign-up&quot;
    NEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL=&quot;/&quot;
    NEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL=&quot;/&quot;
    NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY=&quot;pk_*****&quot;
    NEXT_PUBLIC_CONVEX_URL=&quot;https://*******.convex.cloud&quot;"><pre><code>[build]
  [build.args]
    NEXT_PUBLIC_CLERK_SIGN_IN_URL="/sign-in"
    NEXT_PUBLIC_CLERK_SIGN_UP_URL="/sign-up"
    NEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL="/"
    NEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL="/"
    NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY="pk_*****"
    NEXT_PUBLIC_CONVEX_URL="https://*******.convex.cloud"
</code></pre></div>
<ul dir="auto">
<li>Modify fly.io's generated <code>Dockerfile</code> to include new ENV variables right above <code>RUN npm run build</code></li>
</ul>
<div data-snippet-clipboard-copy-content="ARG NEXT_PUBLIC_CLERK_SIGN_IN_URL
ARG NEXT_PUBLIC_CLERK_SIGN_UP_URL
ARG NEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL
ARG NEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL
ARG NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY
ARG NEXT_PUBLIC_CONVEX_URL

# Build application
RUN npm run build"><pre><code>ARG NEXT_PUBLIC_CLERK_SIGN_IN_URL
ARG NEXT_PUBLIC_CLERK_SIGN_UP_URL
ARG NEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL
ARG NEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL
ARG NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY
ARG NEXT_PUBLIC_CONVEX_URL

# Build application
RUN npm run build
</code></pre></div>
<ul dir="auto">
<li>Run <code>fly deploy --ha=false</code> to deploy the app. The --ha flag makes sure fly only spins up one instance, which is included in the free plan.</li>
<li>Run <code>fly scale memory 512</code> to scale up the fly vm memory for this app.</li>
<li>Create a new file <code>.env.prod</code> locally and fill in all the production-environment secrets. Remember to update <code>NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY</code> and <code>CLERK_SECRET_KEY</code> by copying secrets from Clerk's production instance -<code>cat .env.prod | fly secrets import</code> to upload secrets. Also remember to update <code>CONVEX_DEPLOYMENT</code> and <code>NEXT_PUBLIC_CONVEX_URL</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Customize your own simulation</h2>
<p dir="auto">NOTE: every time you change character data, you should re-run <code>npx convex run testing:debugClearAll --no-push</code> and then <code>npm run dev</code> to re-upload everything to Convex. This is because character data is sent to Convex on the initial load. However, beware that <code>npx convex run testing:debugClearAll --no-push</code> WILL wipe all of your data, including your vector store.</p>
<ol dir="auto">
<li>Create your own characters and strories: All characters and stories, as well as their spirtesheet references are stored in <a href="https://github.com/a16z-infra/ai-town/blob/main/convex/characterdata/data.ts#L4">data.ts</a>. You can start by changing character descriptions.</li>
<li>Updating spritesheets: in <code>data.ts</code>, you will see this code:</li>
</ol>
<div data-snippet-clipboard-copy-content="  {
    name: 'f1',
    textureUrl: '/assets/32x32folk.png',
    spritesheetData: f1SpritesheetData,
    speed: 0.1,
  },..."><pre lang="export"><code>  {
    name: 'f1',
    textureUrl: '/assets/32x32folk.png',
    spritesheetData: f1SpritesheetData,
    speed: 0.1,
  },...
</code></pre></div>
<p dir="auto">You should find a sprite sheet for your character, and define sprite motion / assets in the corresponding file (in the above example, <code>f1SpritesheetData</code> was defined in f1.ts)</p>
<ol start="3" dir="auto">
<li>Update the background (environment): <code>convex/maps/firstmap.ts</code> is where the map gets loaded. The easiest way to export a tilemap is by using <a href="https://www.mapeditor.org/" rel="nofollow">Tiled</a> -- Tiled export tilemaps as a CSV and you can convert CSV to a 2d array accepted by firstmap.ts</li>
</ol>
<h2 tabindex="-1" dir="auto">Credits</h2>
<ul dir="auto">
<li>Tilesheet:
<ul dir="auto">
<li><a href="https://opengameart.org/content/16x16-game-assets" rel="nofollow">https://opengameart.org/content/16x16-game-assets</a> by George Bailey</li>
<li><a href="https://opengameart.org/content/16x16-rpg-tileset" rel="nofollow">https://opengameart.org/content/16x16-rpg-tileset</a> by hilau</li>
</ul>
</li>
<li>We used <a href="https://github.com/pierpo/phaser3-simple-rpg">https://github.com/pierpo/phaser3-simple-rpg</a> for the original POC of this project. We have since re-wrote the whole app, but appreciated the easy starting point</li>
<li>Original assets by <a href="https://opengameart.org/content/tiny-rpg-forest" rel="nofollow">ansimuz</a></li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dell fined $6.5M after admitting it made overpriced monitors look discounted (133 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/08/dell-fined-6-5m-after-admitting-it-made-overpriced-monitors-look-discounted/</link>
            <guid>37128281</guid>
            <pubDate>Mon, 14 Aug 2023 23:44:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/08/dell-fined-6-5m-after-admitting-it-made-overpriced-monitors-look-discounted/">https://arstechnica.com/gadgets/2023/08/dell-fined-6-5m-after-admitting-it-made-overpriced-monitors-look-discounted/</a>, See on <a href="https://news.ycombinator.com/item?id=37128281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      5,300 monitors sold    —
</h4>
            
            <h2 itemprop="description">Dell Australia is paying for something many of its peers are guilty of. </h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-497371776-800x533.jpg" alt="An employee uses a handheld scanner to register the barcode of an outgoing Dell Inc. computer monitor inside the warehouse of an order fulfillment centre,">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 52:single/related:ba14820ac87d6f95974cc19b433acd26 --><!-- empty -->
<p>Dell's Australia arm has been slapped with a $10 million AUD (about $6.49 million) fine for "making false and misleading representations on its website about discount prices for add-on computer monitors," the Australian Competition &amp; Consumer Commission (ACCC) announced today. The Australian regulator said the company sold 5,300 monitors this way.</p>
<p>As Ars Technica previously reported, the ACCC launched <a href="https://arstechnica.com/gadgets/2023/06/dell-in-hot-water-for-making-shoppers-think-overpriced-monitors-were-discounted/">litigation against Dell Australia</a> in November. In June, the Australian Federal Court <a href="https://www.judgments.fedcourt.gov.au/judgments/Judgments/fca/single/2023/2023fca0588" data-uri="05854df1c1e3192c7a6baccebbf109c8">declared</a> that Dell Australia made shoppers believe monitors would be cheaper if bought as an add-on item.</p>
<p>Here's how the "misleading representations" worked. Shoppers of Dell Australia's website who were buying a computer would see an offer for a Dell display with a lower price next to a higher price with a strikethrough line. That suggested to shoppers that the price they'd pay for the monitor if they added it to their cart now would be lower than the monitor's usual cost. But it turns out the strikethrough prices weren't the typical costs. Sometimes, the lower price was actually <em>higher</em> than what Dell Australia typically charged.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/Dell-jpg.jpg" data-height="874" data-width="654"><img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/Dell-jpg-640x855.jpg" width="640" height="855" srcset="https://cdn.arstechnica.net/wp-content/uploads/2023/08/Dell-jpg.jpg 2x"></a><figcaption></figcaption></figure>
<p>"In some cases, consumers paid more for the add-on monitor advertised as 'discounted' than they would have paid if they had bought it as a stand-alone product, which is shocking," ACCC commissioner Liza Carver said in a statement in June.</p>
<p>Dell Australia's website would use savings-signaling lingo, such as: "Includes x% off," "Total Savings" plus a dollar amount, and "Get the best price for popular accessories when purchased with this product," the ACCC noted.</p>                                            
                                                        
<p>Dell Australia also admitted to overstating "discounts customers received" since "monitors were not sold for the strikethrough price for most of the relevant time" and that it "contravened the Australian Consumer Law," according to ACCC's announcement today.</p>
<p>These tricky methods led to shoppers spending over $2 million AUD (about $1.3 million) on Dell monitors from August 2019 to December 2021, according to the ACCC.</p>
<p>“We took this action against Dell Australia because consumers rely on accurate information about prices and discounts to make purchasing decisions," Carver said in a statement today.</p>
<p>Dell Australia was already ordered by Australia's Federal Court to provide full or partial refunds to customers. The ACCC said Dell has already started contacting customers about giving full or partial refunds.</p>
<p>A Dell spokesperson told Ars Technica today that Dell is also paying customers interest and "taking steps to improve our pricing processes to ensure this sort of error does not happen again." Dell didn't specify its exact steps, but in June, Australia's Federal Court ordered Dell Australia to hire an "independent compliance professional."</p>
<p>A Dell spokesperson also told Ars:</p>
<blockquote><p>We are pleased that this is now behind us, and our focus can return to serving our Australian customers. As we acknowledged in November 2022 when the ACCC commenced these proceedings, due to an error in Dell's pricing processes, there was incorrect information displayed on our website about the pricing and savings associated with certain monitors.</p></blockquote>
<p>Unfortunately, for shoppers, even if Dell makes good on its word and eliminates tactics that make typical or bad prices seem like deals, the practice is common among consumer tech vendors. I often see OEMs list products, like laptops and monitors, with discounted prices <em>before</em> they've actually been released. And online marketplaces are flooded with strikethrough prices that represent what the product might have cost <em>years</em> ago.</p>
<p>Dell Australia may promise to make it easier to spot its actual deals, but it's best to make your own price comparisons or use a price tracker like <a href="https://pcpartpicker.com/" data-uri="8379f2468ee2483f463d89cdbcdc3028">PCPartPicker</a> or <a href="https://camelcamelcamel.com/camelizer" data-uri="6f2b6aaa9b5247ef1624625dd07701e8">The Camelizer</a> (which also comes as a handy browser plugin) and avoid buyer's remorse.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Internet Archive responds to recording industry lawsuit targeting obsolete media (379 pts)]]></title>
            <link>https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/</link>
            <guid>37128044</guid>
            <pubDate>Mon, 14 Aug 2023 23:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/">https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/</a>, See on <a href="https://news.ycombinator.com/item?id=37128044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>Late Friday, some of the world’s largest record labels, including Sony and Universal Music Group, filed a lawsuit against the Internet Archive and others for the <a href="https://great78.archive.org/">Great 78 Project</a>, a community effort for the preservation, research and discovery of 78 rpm records that are 70 to 120 years old. As a non-profit library, we take this matter seriously and are currently reviewing the lawsuit with our legal counsel.</p>


<div>
<figure><a href="http://blog.archive.org/wp-content/uploads/2023/08/20thcenturytimemachineimages_0000.jpg"><img decoding="async" width="500" height="750" src="http://blog.archive.org/wp-content/uploads/2023/08/20thcenturytimemachineimages_0000.jpg" alt="" srcset="https://blog.archive.org/wp-content/uploads/2023/08/20thcenturytimemachineimages_0000.jpg 500w, https://blog.archive.org/wp-content/uploads/2023/08/20thcenturytimemachineimages_0000-200x300.jpg 200w" sizes="(max-width: 500px) 100vw, 500px"></a><figcaption>A 78 rpm player in the foyer of the Internet Archive.</figcaption></figure></div>


<p>Of note, the <a href="https://great78.archive.org/">Great 78 Project</a> has been in operation since 2006 to bring free public access to a largely forgotten but culturally important medium. Through the efforts of dedicated librarians, archivists and sound engineers, we have preserved hundreds of thousands of recordings that are stored on shellac resin, an obsolete and brittle medium. The resulting preserved recordings retain the scratch and pop sounds that are present in the analog artifacts; noise that modern remastering techniques remove.</p>



<p><strong>Statement from Brewster Kahle, digital librarian of the Internet Archive:</strong><br>“When people want to listen to music they go to Spotify. When people want to study sound recordings as they were originally created, they go to libraries like the Internet Archive. Both are needed. There shouldn’t be conflict here.”</p>



<p>These preservation recordings are used in teaching and research, including by university professors like Jason Luther of Rowan University, whose students use the Great 78 collection as the basis for researching and writing podcasts for use in class assignments (<a href="https://blog.archive.org/2021/06/09/university-professor-leverages-78rpm-record-collection-from-the-internet-archive-for-students/">University Professor Leverages 78rpm Record Collection From the Internet Archive for Student Podcasts</a>, June 9, 2021). While this mode of access is important, usage is tiny—on average, each recording in the collection is only accessed by one researcher per month.</p>



<figure><a href="https://blog.archive.org/wp-content/uploads/2023/08/Technician-digitizes-78-1000x750-1.jpg"><img decoding="async" loading="lazy" width="1000" height="750" src="https://blog.archive.org/wp-content/uploads/2023/08/Technician-digitizes-78-1000x750-1.jpg" alt="" srcset="https://blog.archive.org/wp-content/uploads/2023/08/Technician-digitizes-78-1000x750-1.jpg 1000w, https://blog.archive.org/wp-content/uploads/2023/08/Technician-digitizes-78-1000x750-1-300x225.jpg 300w, https://blog.archive.org/wp-content/uploads/2023/08/Technician-digitizes-78-1000x750-1-768x576.jpg 768w, https://blog.archive.org/wp-content/uploads/2023/08/Technician-digitizes-78-1000x750-1-624x468.jpg 624w" sizes="(max-width: 1000px) 100vw, 1000px"></a><figcaption>A technician uses a 4-arm turntable to digitize a 78 rpm record.</figcaption></figure>



<p>While we review the lawsuit, we remain dedicated to our mission of providing “Universal Access to All Knowledge.” We are grateful for the continued support of our library patrons and partners as we continue to fight these attacks.</p>



<p>For more information or media inquiries, please contact <a href="mailto:press@archive.org">press@archive.org</a>.&nbsp;</p>



<p><strong>LINKS</strong></p>



<ul>
<li>The Great 78 Project: <a href="https://great78.archive.org/">https://great78.archive.org/</a>&nbsp;</li>



<li>University Professor Leverages 78rpm Record Collection From the Internet Archive for Student Podcasts: <a href="https://blog.archive.org/2021/06/09/university-professor-leverages-78rpm-record-collection-from-the-internet-archive-for-students/">https://blog.archive.org/2021/06/09/university-professor-leverages-78rpm-record-collection-from-the-internet-archive-for-students/</a>&nbsp;</li>



<li>78 rpm record digitization: <a href="https://archive.org/details/mass78rpmdiscdigitization">https://archive.org/details/mass78rpmdiscdigitization</a></li>
</ul>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Montana loses fight against youth climate activists in landmark ruling (150 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/08/montana-loses-fight-against-youth-climate-activists-in-landmark-ruling/</link>
            <guid>37127606</guid>
            <pubDate>Mon, 14 Aug 2023 22:33:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/08/montana-loses-fight-against-youth-climate-activists-in-landmark-ruling/">https://arstechnica.com/tech-policy/2023/08/montana-loses-fight-against-youth-climate-activists-in-landmark-ruling/</a>, See on <a href="https://news.ycombinator.com/item?id=37127606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      "A monumental decision"    —
</h4>
            
            <h2 itemprop="description">Emotional testimony leads to plaintiffs' win in first youth-led climate trial.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-1258643987-800x532.jpg" alt="Youth plaintiffs are greeted by supporters as they arrive for the nation's first youth climate change trial at Montana's First Judicial District Court on June 12, 2023. ">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/GettyImages-1258643987.jpg" data-height="681" data-width="1024">Enlarge</a> <span>/</span> Youth plaintiffs are greeted by supporters as they arrive for the nation's first youth climate change trial at Montana's First Judicial District Court on June 12, 2023. </p></figcaption>  </figure>

  




<!-- cache hit 52:single/related:a2e5d82019c1f66e0c2bd583e2cac693 --><!-- empty -->
<p>A Montana state court <a href="https://westernlaw.org/wp-content/uploads/2023/08/2023.08.14-Held-v.-Montana-victory-order.pdf">today sided</a> with <a href="https://arstechnica.com/tech-policy/2023/06/montana-calls-un-climate-report-hearsay-at-landmark-youth-led-climate-trial/">young people who sued the state</a> for promoting the fossil fuel industry through its <a href="https://leg.mt.gov/bills/2023/billpdf/HB0971.pdf">energy policy</a>, which they alleged prohibits Montana from weighing greenhouse gas emissions in approving the development of new factories and power plants. This prohibition, 16 plaintiffs ages 5 to 22 successfully argued, violates their constitutional right to a "clean and healthful environment in Montana for present and future generations."</p>
<p>Experts previously predicted that a win for youths in Montana would set an important legal precedent for how courts can hold states accountable for climate inaction. The same legal organization representing Montana's young plaintiffs, Our Children's Trust, is currently pursuing similar cases in four other states, <a href="https://www.washingtonpost.com/climate-environment/2023/08/14/youths-win-montana-climate-trial/">The Washington Post reported</a>.</p>
<p>The Post described this landmark case as "the nation’s first constitutional and first youth-led climate lawsuit to go to trial."</p>
<p>To climate activists, it illustrates the power of a court hearing directly from young people describing experiences with immense loss caused by climate change. Today's order followed five days of emotional testimony from young plaintiffs describing harms caused by the state's climate inaction.</p>
<p>Montana tried to argue that adjusting its energy policy and other statutes would have “no meaningful impact or appreciable effect,” the Post reported, because climate change is a global issue. Montana Assistant Attorney General Michael Russell described the testimony as a “week-long airing of political grievances that properly belong in the Legislature, not a court of law,” according to the Post. Notably, the state did not meaningfully attempt to dispute climate science.</p>
<p>However, "undisputed testimony established" that the state "could evaluate 'greenhouse gas emissions and corresponding impacts to the climate in the state' when evaluating fossil fuel activities," judge Kathy Seeley wrote in the Montana 1st Judicial District Court <a href="https://westernlaw.org/wp-content/uploads/2023/08/2023.08.14-Held-v.-Montana-victory-order.pdf">order</a>. The state had no compelling interest not to conduct climate analyses and consider remedies, Seeley wrote.</p>                                            
                                                        
<p>Experts <a href="https://www.scientificamerican.com/article/young-people-in-historic-climate-trial-rest-their-case/">told Scientific American</a> that Montana's emissions are significant given its population size, emitting in 2019 "about 32 million tons of carbon dioxide." That's "about as much as Ireland, which has a population six times larger," Scientific American reported. Young people suing alleged that Montana had "never denied a permit for a fossil fuel project," the Post reported.</p>
<p>Because of this powerful youth testimony, the court ruled that the state's energy policy's limitation on environmental impact reviews was unconstitutional.</p>
<p>"Montana's greenhouse gas emissions and climate change have been proven to be a substantial factor in causing climate impacts to Montana's environment and harm and injury to the youth plaintiffs," Seeley wrote.</p>
<p>As a result of the order, any Montana statutes prohibiting climate impact analysis and remedies are now invalid and permanently enjoined.</p>
<p>"This is a monumental decision," Phil Gregory, the plaintiffs' attorney, told the Post. Another attorney for plaintiffs and executive director of Our Children's Trust, Julia Olson, <a href="https://apnews.com/article/climate-change-youth-montana-trial-c7fdc1d8759f55f60346b31c73397db0?taid=64da64ccb5e4dd0001f844bc&amp;utm_campaign=TrueAnthem&amp;utm_medium=AP&amp;utm_source=Twitter">told AP</a> that the ruling was a "huge win for Montana, for youth, for democracy, and for our climate."</p>
<p>Montana is expected to appeal the ruling, according to Emily Flower, a spokesperson for the state's Attorney General Austin Knudsen, who called the ruling "absurd."</p>
<p>“Montanans can’t be blamed for changing the climate—even the plaintiffs’ expert witnesses agreed that our state has no impact on the global climate," Flower said. "Their same legal theory has been thrown out of federal court and courts in more than a dozen states. It should have been here as well, but they found an ideological judge who bent over backward to allow the case to move forward and earn herself a spot in their next documentary.”</p>
<p>Youth plaintiffs did not ask for any damages beyond their attorneys' fees and costs, which were awarded by the court. To young people suing, winning is seemingly just about pushing the state to embrace climate science and mitigate known harms moving forward.</p>
<p>“As fires rage in the West, fueled by fossil fuel pollution, today’s ruling in Montana is a game-changer that marks a turning point in this generation’s efforts to save the planet from the devastating effects of human-caused climate chaos,” Olson told AP.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Cuts at WVU (103 pts)]]></title>
            <link>https://community.wvu.edu/~jokatz/Closure/</link>
            <guid>37127450</guid>
            <pubDate>Mon, 14 Aug 2023 22:13:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.wvu.edu/~jokatz/Closure/">https://community.wvu.edu/~jokatz/Closure/</a>, See on <a href="https://news.ycombinator.com/item?id=37127450">Hacker News</a></p>
<div id="readability-page-1" class="page"><div face="garamond, palatino, geneva">
<span>
<img src="http://community.wvu.edu/~jokatz/JKShih.jpg" alt="Photo by Stephanie Shih" height="300">		
<p>My name is Jonah Katz; I'm an associate professor of linguistics at West Virginia University. I work in a large department that includes all of the world languages and literatures at WVU in addition to its 'basic' and applied linguistics programs. On August 10, 2023, the WVU provost recommended dissolving our department and all of its academic programs and faculty lines, including the only Linguistics programs in the state of West Virginia (our MA and undergrad minor). All of the tenured and untenured faculty in the department are to be laid off, including the linguistics faculty. All of the foreign language and literature programs at the university are to be discontinued; the president of the university publicly stated that foreign-language classes will be replaced with online apps or remote classes at other universities. I'm asking linguists to help us publicize what's happened here and advocate on our behalf. We are a very small program (4 faculty in basic linguistics and 4-5 more in applied linguistics/TESOL), but punch well above our weight in both research output and external grant funding. In fact, on the same day WVU wrote to tell us our department is cancelled and we are fired, they ran a front-page <a href="https://wvutoday.wvu.edu/stories/2023/08/10/wvu-linguists-sound-out-how-intensity-and-duration-of-speech-shape-pronunciation-rethinking-language-learning">article</a> on the university website celebrating the NSF grant that Sergio Robles-Puente and I recently received, and lauding our innovative research and intensive student mentoring. That grant will now need to be discontinued or moved to another institution.
</p><p>The reason given for this egregious violation of ethical and professional norms is that the university faces a dire budget crisis, and the administration has no choice but to cut academic programs in order to close their structural budget deficit. But the administration's own financial data, gathered at great cost with external consultants and publicly posted <a href="https://provost.wvu.edu/files/d/bf3ef02f-e90a-4e43-a316-d295fa489067/academic-transformation-public-data-table_july-17-2023-100.pdf">here</a>, clearly indicate that the department as a whole (p. 7) has generated operating profits of more than $800,000 in each of the last three years, even without counting our grant income, which is substantial (our NSF project is just one of several large external grants that faculty in our department have been awarded in the past several years). This is not a financial decision: it is an ideological one, as our president's public comments make clear.
</p><p>The story of how the university got into such a catastrophic financial position to begin with is a long and complex one, and tangential to my message here. But for those interested, <a href="https://www.chronicle.com/article/why-is-west-virginia-u-making-sweeping-cuts?">this exposé</a> in the Chronicle of Higher Education (unfortunately paywalled for many readers) and this <a href="https://wvufacts.wordpress.com/">anonymous report</a> by WVU faculty demonstrate convincingly that skyrocketing administrative personnel costs and excessive debt associated with a failed growth strategy are two of the major drivers (declining enrollment, COVID, and inadequate state funding also played a role).
</p><p>To be honest, I don't know if anything that we can do will help the situation. The leadership of the university has made up its mind, they have the backing of state politicians and the board they appointed to oversee the university, and they will not be swayed by appeals to reason or ideals. What they may respond to is public pressure, and to that end I am asking my colleagues to share what is happening here as widely as you can through all available media and professional networks. Sunlight is the best disinfectant, and we need a whole lot of disinfectant at my institution. You could also consider writing to the people who made these decisions and their enablers, and if you represent an organization like a department or institute, consider doing so publicly in an open letter. Let them know what you think of the decision to eliminate all foreign language, literature, and linguistics classes at a public land-grant state flagship, and to fire all faculty regardless of merit, longevity, or tenure. Let them know how this will affect the reputation of West Virginia University and the state that it represents. Let them know how it will affect young people's prospects and their choices in one of the poorest and least-educated states in the country, where huge numbers of our most talented and driven young people are already leaving to seek better educational and professional opportunities. Thank you for your time, and to my many wonderful colleagues who have reached out this week to ask how you can help. I am lucky beyond belief to work in such a supportive, principled, and collaborative field, and to have so many amazing mentors and colleagues. Linguists really are the best. 

</p></span></div><p><span face="garamond, palatino, geneva">University leadership directly in charge of this process include President Gordon Gee ( Gordon.Gee@mail.wvu.edu ) and Provost Maryanne Reed ( Maryanne.Reed@mail.wvu.edu ). The WVU Board of Governors is a politically appointed body that is supposed to oversee the administration of the university, and will eventually need to approve the provost's recommendations. They can be contacted via Special Assistant Valerie Lopez ( Valerie.Lopez@mail.wvu.edu ). Governor Jim Justice appointed most of the Board and has strongly supported Gee during his term. His office can be contacted using <a href="https://appengine.egov.com/apps/wv/governor/contactus">this form</a>. 
 
</span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grindr employees have 2 weeks: agree to move across the country+RTO or lose jobs (107 pts)]]></title>
            <link>https://www.businessinsider.com/grindr-united-unionized-employees-work-from-home-return-in-person-2023-8</link>
            <guid>37126856</guid>
            <pubDate>Mon, 14 Aug 2023 21:12:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/grindr-united-unionized-employees-work-from-home-return-in-person-2023-8">https://www.businessinsider.com/grindr-united-unionized-employees-work-from-home-return-in-person-2023-8</a>, See on <a href="https://news.ycombinator.com/item?id=37126856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <ul><li>Employees at LGBTQ+ dating site Grindr are being asked to return to work in person.&nbsp;</li><li>The company gave employees two weeks to decide if they could move by October.&nbsp;</li><li>The company's employees say Grindr could be retaliating against them for trying to form a union.</li></ul><section id="formContainer" data-component-type="inline-newsletter-module" data-event-label="insider_today" data-newsletter-id="1" data-list="Insider Today" data-acq-source="newsinlinesignup">
                            
                        
                            <p><svg version="1.1" xmlns="http://www.w3.org/2000/svg" role="img" width="50" height="50" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50;" xml:space="preserve">
                          <title>Loading</title>
                          <desc>Something is loading.</desc>
                          <path fill="#111" d="M43.935,25.145c0-10.318-8.364-18.683-18.683-18.683c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615c8.072,0,14.615,6.543,14.615,14.615H43.935z">
                            <animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform>
                          </path>
                        </svg></p>
                            
                        
                            
                        
                            <div>
                              <p>Thanks for signing up!</p>
                              
                              <p>
                              Access your favorite topics in a personalized feed while you're on the go.
                                    </p>
                            </div>
                        
                            
                            
                          </section><p>Management at the popular LGBTQ+ dating app Grindr is asking workers to return to the office or lose their jobs, prompting outrage from employees who say the move will upend their lives.</p><p>According to a form sent to workers at Grindr on August 4, <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.vice.com/en/article/qjv3gm/grindr-tells-unionizing-workers-move-across-the-country-or-be-fired" data-analytics-product-module="body_link"><u>obtained by Vice's Motherboard</u></a>, workers would need to confirm by August 17 whether or not they would move within 50 miles of Grindr's three offices in Chicago, Los Angeles, or the San Francisco Bay Area or lose their jobs at the end of the month.</p><p>The news comes two weeks after employees <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/unions-having-major-moment-but-dont-expect-1930s-level-boom-2023-7" data-analytics-product-module="body_link"><u>announced their effort to unionize</u></a> under the Communications Workers of America, Grindr United. Grindr United posted Sunday that the pivot to in-person work by the company <a target="_blank" rel="noopener noreferrer nofollow" href="https://twitter.com/GrindrUnited/status/1690811142082527233?s=20" data-analytics-product-module="body_link"><u>is a "bizarre coincidence."</u></a></p><p>The CWA has also <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.nlrb.gov/case/31-CA-323349" data-analytics-product-module="body_link"><u>filed a complain</u></a>t with the National Labor Relations Board as a result of the return to office order, arguing that it is retaliation against union organizing.</p><div id="1691976039072" data-styles="default-width" data-embed-type="twitter" data-script="https://platform.twitter.com/widgets.js" data-type="embed"><blockquote><div lang="en" dir="ltr"><p>Timeline review!</p><p>July 20: we tell management that we have organized ourselves and request voluntary recognition.</p><p>July 20 - Aug 3: no contact from management. At all. None. Zero.</p><p>Aug 4: management announces we have two weeks to decide whether to uproot our lives or we're fired.</p></div>— Grindr United ✊ (@GrindrUnited) <a href="https://twitter.com/GrindrUnited/status/1687803784251031552?ref_src=twsrc%5Etfw" rel=" nofollow">August 5, 2023</a></blockquote>  
                        </div><p>Rowan Rosenthal, an organizing committee member at Grindr United CWA, told Insider that the move has been a significant hit for Grindr's "majority queer" workplace, as well as for disabled members of the company.</p><p>"A big part of our vision as a union was to enshrine this benefit that we had in the past of remote first to accommodate those folks," Rosenthal told Insider. "A lot of people who are disabled or have neurodivergence or mental health concerns or caring for somebody that they love, it just makes it a lot more possible to do your best work when you have the flexibility to work from your home or go into an office in your city a few times a week or month."</p><p>Rosenthal told Insider that as of Monday, Grindr has given employees an opportunity to send in questions about the company's new policy but still has not addressed union organizers.</p><p>Grindr CEO George Arison told staff that the decision was "many months" in the making, per a memo <a target="_blank" rel="noopener noreferrer nofollow" href="https://news.bloomberglaw.com/daily-labor-report/grindr-uses-return-to-work-order-to-thwart-union-complaint-says" data-analytics-product-module="body_link"><u>obtained by Bloomberg.</u></a></p><p>In a statement to Insider, a company spokesperson said the company began "the process of transitioning away from 'remote-first' to hybrid" in April and that employees were informed of a future switch to hybrid work during an all-hands meeting in June — before the <a target="_blank" rel="noopener noreferrer nofollow" href="https://time.com/6296516/grindr-union-tech-layoffs-lgbt/" data-analytics-product-module="body_link"><u>unionization effort was announced</u></a>.</p><p>However, employees told <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.nytimes.com/2023/08/12/business/grindr-rto-union.html" data-analytics-product-module="body_link"><u>The New York Times</u></a> that the company told them to expect the transition after one or two quarters. Rosenthal told Insider that they were not at the all-hands meeting but said many employees expected that remote workers would be allowed to stay remote based on company communications.</p><p>"We were told that we were going to hire new workers in these specific cities, but specifically that this was not going to affect existing employees," Rosenthal said.</p><p>The company spokesperson also said that the decision to move to a hybrid-work model has "nothing to do with the NLRB election petition" and said, "We respect and support our team members' rights to make their own decision about union representation."</p><p>Grindr is just one of the latest companies to <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/wfh-work-from-home-decreases-productivity-18-percent-study-rto-2023-8" data-analytics-product-module="body_link"><u>urge employees to return to the office</u></a>. <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/amazon-senior-leader-office-remote-work-grow-faster-better-job-2023-8" data-analytics-product-module="body_link"><u>Amazon</u></a>, <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/apple-threatens-staff-not-coming-office-three-days-week-2023-3" data-analytics-product-module="body_link"><u>Apple</u></a>, <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/disney-starbucks-forcing-employees-back-office-your-company-unlikely-next-2023-1" data-analytics-product-module="body_link"><u>Disney</u></a>, <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/google-microsoft-ibm-companies-hybrid-work-office-collaboration-flexibility-happiness-2023-8" data-analytics-product-module="body_link"><u>Google</u></a>, <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/meta-rehiring-workers-from-layoffs-2023-8" data-analytics-product-module="body_link"><u>Meta</u></a>, <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/elon-musk-emailed-twitter-staff-office-is-not-optional-2023-3" data-analytics-product-module="body_link"><u>X</u></a> (formerly <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/x-could-be-a-positive-move-for-musk-twitter-2023-7" data-analytics-product-module="body_link"><u>Twitter</u></a>), and <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/companies-making-workers-employees-return-to-office-rto-wfh-hybrid-2023-1" data-analytics-product-module="body_link"><u>dozens of other </u></a>businesses are asking their white-collar employees to work in the office at least part of the time.</p><p>That model is unpopular <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/i-do-not-miss-office-life-2023-4" data-analytics-product-module="body_link"><u>among many workers</u></a>, who say they would <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/us-remote-workers-would-take-pay-cut-to-keep-wfh-2023-5" data-analytics-product-module="body_link"><u>take a pay cut over an in-person job.</u></a> According to a recent <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.wsj.com/articles/need-to-hire-workers-in-a-hot-job-market-let-them-do-some-remote-work-506f72e6?mod=hp_lead_pos8" data-analytics-product-module="body_link"><u>Wall Street Journal</u></a> report, employers forcing a return to <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.businessinsider.com/employees-work-from-home-benefits-as-good-as-raise-2023-8" data-analytics-product-module="body_link"><u>in-person work</u></a> are seeing slower hiring rates.</p><p>Quinn McGee, an employee organizer at Grindr United CWA, told Vice the demands sent by Grindr — which McGee said has refused to meet with employees about the union drive — were "dehumanizing."</p><p>"To tell me that I have two weeks to decide whether or not to uproot my family's life for a job that won't come to the table and speak with me as an adult — it's dehumanizing," McGee told the publication.</p><p>Rosenthal, who lives in New York, told Insider that they are being asked to move to Los Angeles as a member of the design team. To do this, they said, would mean that they would have to leave their family and community on the East Coast.</p><p>They also said that it would affect their goals of eventually getting gender-affirming surgery.</p><p>"It feels really awful and scary to be asked to either do this or resign from my job, functionally," Rosenthal said. "In terms of other return-to-office orders that I've seen other companies instate, it just feels shocking, to be honest with you."</p><p><em>Update: August 14, 2023 — This story has been updated to include quotes from Rowan Rosenthal, an organizing committee member for Grindr United CWA.</em></p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Java 21: First Release Candidate (143 pts)]]></title>
            <link>https://openjdk.org/projects/jdk/21/</link>
            <guid>37126530</guid>
            <pubDate>Mon, 14 Aug 2023 20:40:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openjdk.org/projects/jdk/21/">https://openjdk.org/projects/jdk/21/</a>, See on <a href="https://news.ycombinator.com/item?id=37126530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">

<p>This release will be the Reference Implementation of
version&nbsp;21 of the Java&nbsp;SE Platform, as specified by
<a href="https://openjdk.org/projects/jdk/21/spec">JSR&nbsp;396</a> in the Java Community Process.</p>
<h2 id="Status">Status</h2>
<p>JDK 21 is in <a href="http://openjdk.java.net/jeps/3#rdp-1">Rampdown Phase Two</a>. The
overall feature set is frozen. No further JEPs will be targeted to
this release.</p>
<p>The stabilization repository, <a href="http://github.com/openjdk/jdk21">jdk21</a>, is open for select bug
fixes, <a href="https://openjdk.org/jeps/3#Fix-Request-Process">with approval</a>, per
the <a href="https://openjdk.org/jeps/3">JDK Release Process (JEP 3)</a>. Late
enhancements are still possible, <a href="https://openjdk.org/jeps/3#Late-Enhancement-Request-Process">with approval</a>, but
the bar is now extraordinarily high. Integrate most stabilization
changes via <a href="https://openjdk.org/jeps/3#Integrating-fixes-and-enhancements">backports
from the main-line repository</a>.</p>
<ul>
<li><a href="http://j.mp/jdk-rdp-2">RDP 2 candidate bugs</a></li>
<li><a href="https://openjdk.org/jeps/3#Fix-Request-Process">Fix-Request
Process</a></li>
<li><a href="https://openjdk.org/jeps/3#Bug-Deferral-Process">Bug-Deferral
Process</a></li>
<li><a href="https://openjdk.org/jeps/3#Late-Enhancement-Request-Process">Late-Enhancement Request
Process</a></li>
</ul>
<p>Early-access binaries under the GPL are available <a href="https://jdk.java.net/21/">here</a>.</p>
<h2 id="Schedule">Schedule</h2>
<blockquote>
<table summary="schedule">
<tbody><tr>
<td>2023/06/08</td>
<td></td>
<td><a href="https://openjdk.org/jeps/3#rdp-1">Rampdown Phase One</a>
(fork from main line)</td>
</tr>
<tr>
<td>2023/07/20</td>
<td></td>
<td><a href="https://openjdk.org/jeps/3#rdp-2">Rampdown Phase
Two</a></td>
</tr>
<tr>
<td>2023/08/10</td>
<td></td>
<td><a href="https://openjdk.org/jeps/3#rc">Initial Release
Candidate</a></td>
</tr>
<tr>
<td>2023/08/24</td>
<td></td>
<td><a href="https://openjdk.org/jeps/3#rc">Final Release
Candidate</a></td>
</tr>
<tr>
<td>2023/09/19</td>
<td></td>
<td>General Availability</td>
</tr>
</tbody></table>
</blockquote>
<h2 id="Features">Features</h2>
<blockquote>
<table summary="jeps"><!--
        <tr><th class="title" colspan="2">JEPs integrated into JDK 21</th></tr>
        -->
<tbody><tr>
<td>430:</td>
<td><a href="https://openjdk.org/jeps/430">String Templates (Preview)</a></td>
</tr>
<tr>
<td>431:</td>
<td><a href="https://openjdk.org/jeps/431">Sequenced Collections</a></td>
</tr>
<tr>
<td>439:</td>
<td><a href="https://openjdk.org/jeps/439">Generational ZGC</a></td>
</tr>
<tr>
<td>440:</td>
<td><a href="https://openjdk.org/jeps/440">Record Patterns</a></td>
</tr>
<tr>
<td>441:</td>
<td><a href="https://openjdk.org/jeps/441">Pattern Matching for switch</a></td>
</tr>
<tr>
<td>442:</td>
<td><a href="https://openjdk.org/jeps/442">Foreign Function &amp; Memory API (Third
Preview)</a></td>
</tr>
<tr>
<td>443:</td>
<td><a href="https://openjdk.org/jeps/443">Unnamed Patterns and Variables
(Preview)</a></td>
</tr>
<tr>
<td>444:</td>
<td><a href="https://openjdk.org/jeps/444">Virtual Threads</a></td>
</tr>
<tr>
<td>445:</td>
<td><a href="https://openjdk.org/jeps/445">Unnamed Classes and Instance Main Methods
(Preview)</a></td>
</tr>
<tr>
<td>446:</td>
<td><a href="https://openjdk.org/jeps/446">Scoped Values (Preview)</a></td>
</tr>
<tr>
<td>448:</td>
<td><a href="https://openjdk.org/jeps/448">Vector API (Sixth Incubator)</a></td>
</tr>
<tr>
<td>449:</td>
<td><a href="https://openjdk.org/jeps/449">Deprecate the Windows 32-bit x86 Port for
Removal</a></td>
</tr>
<tr>
<td>451:</td>
<td><a href="https://openjdk.org/jeps/451">Prepare to Disallow the Dynamic Loading of
Agents</a></td>
</tr>
<tr>
<td>452:</td>
<td><a href="https://openjdk.org/jeps/452">Key Encapsulation Mechanism API</a></td>
</tr>
<tr>
<td>453:</td>
<td><a href="https://openjdk.org/jeps/453">Structured Concurrency (Preview)</a></td>
</tr>
</tbody></table>
</blockquote>
<!--
    <blockquote>
      <table class="jeps" summary="jeps" width="100%">
        <tr><th class="title" colspan="2">JEPs proposed to target JDK 21</th>
            <th class="review">review&nbsp;ends</th></tr>

      </table>
    </blockquote>
-->
<p>Last update: 2023/7/20 15:05 UTC</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Peter Shor's MIT Fall 2022 course lecture notes on quantum computing (136 pts)]]></title>
            <link>https://math.mit.edu/~shor/435-LN/</link>
            <guid>37126479</guid>
            <pubDate>Mon, 14 Aug 2023 20:36:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://math.mit.edu/~shor/435-LN/">https://math.mit.edu/~shor/435-LN/</a>, See on <a href="https://news.ycombinator.com/item?id=37126479">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h2> Lecture Notes for 8.370/18.435 Quantum Computation from Fall 2022 </h2>

<h3><a href="http://www-math.mit.edu/~shor/contact-info/">Contact 
information</a></h3>

<!--- Here is the 2021 version of
<A HREF="https://math.mit.edu/~shor/Course-Info-435.pdf">the Course
Info page</a>. Most things stayed the same for 2022. --->

<h4>Lecture Notes</h4>
Here are the 2022 Lecture notes. I never got around to writing the notes for
Lecture 26 --- I may or may not do that at some point in the future.
<p> 

<a href="https://math.mit.edu/~shor/435-LN/Lecture_01.pdf">Lecture 1</a> 
---  Introduction and History<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_02.pdf">Lecture 2</a>
--- The Superposition Principle<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_03.pdf">Lecture 3</a>
--- Unitary Evolution and the Bloch Sphere
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_04.pdf">Lecture 4</a>
--- Quantum Measurements 
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_05.pdf">Lecture 5</a>
--- Joint Quantum Systems and Tensor Products
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_06.pdf">Lecture 6</a>
--- More Tensor Products (Measurements of Joint Systems)
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_07.pdf">Lecture 7</a>
--- Classical Boolean circuits
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_08.pdf">Lecture 8</a>
--- Reversible Boolean circuits
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_09.pdf">Lecture 9</a>
--- Quantum gates I
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_10.pdf">Lecture 10</a>
--- Quantum gates II
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_11.pdf">Lecture 11</a>
--- Quantum Teleportation
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_12.pdf">Lecture 12</a>
--- Density Matrices I
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_13.pdf">Lecture 13</a>
--- Density Matrices II
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_14.pdf">Lecture 14</a>
--- The GHZ Experiment (theory)
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_15.pdf">Lecture 15</a>
--- Quantum Optics and the GHZ Experiment
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_16.pdf">Lecture 16</a>
--- The Deutsch-Jozsa Algorithm
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_17.pdf">Lecture 17</a>
--- Classical computational complexity theory
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_18.pdf">Lecture 18</a>
--- Simon's algorithm
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_19.pdf">Lecture 19</a>
--- The quantum Fourier transform
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_20.pdf">Lecture 20</a>
--- Phase Estimation
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_21.pdf">Lecture 21</a>
--- Quantum factoring algorithm 
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_22.pdf">Lecture 22</a>
--- The Number Theory Needed for the Factoring Algorithm
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_23.pdf">Lecture 23</a>
--- The Discrete Log Algorithm
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_24.pdf">Lecture 24</a>
--- Grover's search algorithm
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_25.pdf">Lecture 25</a>
--- Proof that Grover Search is Optimal
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_26.pdf">Lecture 26</a>
--- Lecture on Hamiltonian Simulation; Notes Unwritten
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_27.pdf">Lecture 27</a>
--- Introduction to Quantum error correcting codes --- the 9-qubit code
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_28.pdf">Lecture 28</a>
--- More on the 9-qubit code
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_29.pdf">Lecture 29</a>
--- The 7-qubit Quantum Hamming Code
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_30.pdf">Lecture 30</a>
--- Quantum CSS Codes
<br>
<a href="https://math.mit.edu/~shor/435-LN/Lecture_31.pdf">Lecture 31</a>
--- The BB84 Quantum Key Distribution Protocol and the Proof of Its Security
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Burnout because of ChatGPT? (142 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37126182</link>
            <guid>37126182</guid>
            <pubDate>Mon, 14 Aug 2023 20:10:51 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37126182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="37126182">
      <td><span></span></td>      <td><center><a id="up_37126182" href="https://news.ycombinator.com/vote?id=37126182&amp;how=up&amp;goto=item%3Fid%3D37126182"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=37126182">Ask HN: Burnout because of ChatGPT?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_37126182">104 points</span> by <a href="https://news.ycombinator.com/user?id=smdz">smdz</a> <span title="2023-08-14T20:10:51"><a href="https://news.ycombinator.com/item?id=37126182">5 hours ago</a></span> <span id="unv_37126182"></span> | <a href="https://news.ycombinator.com/hide?id=37126182&amp;goto=item%3Fid%3D37126182">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Burnout%20because%20of%20ChatGPT%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=37126182&amp;auth=4a89681a7f8ea50f2f0b6195c20a05986749518d">favorite</a> | <a href="https://news.ycombinator.com/item?id=37126182">106&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>TL;DR (summarised by ChatGPT) - I'm experiencing increased productivity and independence with ChatGPT but grappling with challenges such as lack of work-life boundaries and overwhelming information, leading to stress and burnout.</p><p>Long story...</p><p>I have been using ChatGPT for a while, and moved to the Plus subscription for their GPT-4 model, which I must say, is quite good.</p><p>1. ChatGPT makes us very productive. Personally, in my early 40s, I feel my brain is back in 20s.</p><p>2. I no longer feel the need to hire juniors. This is a short-term positive and maybe a long-term negative.
[[EDIT: I may have implied a wrong meaning. To clarify - nobody's going yet because of ChatGPT. It is just raising the bar high and higher. What took me years to learn, this thing can do already and much more. And I cannot predict the financial future of OpenAI or the markets in general.]]</p><p>A lot of stuff I used to delegate to fellow humans are now being delegated to ChatGPT. And I can get the results immediately and at any time I want. I agree that it cannot operate on its own. I still need to review and correct things. I have do that even when working with other humans. The only difference is that I can start trusting a human to improve, but I cannot expect ChatGPT to do so. Not that it is incapable, but because it is restricted by OpenAI.</p><p>And I have gotten better at using it. Calling myself a prompt-engineer sounds weird.</p><p>With all the good, I am now experiencing the cons, stress and burnout:</p><p>1. Humans work 9-5 (or some schedule), but ChatGPT is available always and works instantly. Now, when I have some idea I want to try out - I start working on it immediately with the help of AI. Earlier I just used to put a note in the todo-list and stash it for the next day.</p><p>2. The outputs with ChatGPT are so fast, that my "review load" is too high. At times it feels like we are working for ChatGPT and not the other way around.</p><p>3. ChatGPT has the habit of throwing new knowledge back at you. Google does that too, but this feels 10x of Google. Sometimes it is overwhelming. Good thing is we learn a lot, bad thing is that if often slows down our decision making.</p><p>4. I tried to put a schedule to use it - but when everybody has access to this tech, I have a genuine fear of missing out.</p><p>5. I have zero doubt that AI is setting the bar high, and it is going to take away a ton of average-joe desk jobs. GPT-4 itself is quite capable and organisations are yet to embrace it.</p><p>And not the least, it makes me worry - what lies with the future models. I am not a layman when it comes to AI/ML - have worked with it until the past few years in the pre-GPT era.</p><p>Has anybody experienced these issues? And how do you deal with those?</p><p>* I could not resist asking ChatGPT the above - couple of strategies it told me were to "Seek Support from Others" and "Participating in discussions or groups focused on ethical AI". *</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenFarm – a free and open database and web application for gardening knowledge (442 pts)]]></title>
            <link>https://openfarm.cc</link>
            <guid>37125830</guid>
            <pubDate>Mon, 14 Aug 2023 19:42:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openfarm.cc">https://openfarm.cc</a>, See on <a href="https://news.ycombinator.com/item?id=37125830">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      

      
<div id="how-it-works">
        
        
        <h2>
            Growing Guides show you how to care for your Crop during all
of its Life Stages. Each Guide is based on specific environmental
conditions and growing practices, and ranked for compatibility
with you and your gardens.

        </h2>
        <ul>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
        </ul>
    </div>

<div>
            <h2>Grow Your Food</h2>
            <h2>Farm and garden through knowledge sharing</h2>
            </div>



<div>
    <p id="community-favorites">
        <h2>Community Favorites</h2>
    </p>
    

</div>

      <!-- Necessary for sticky footer -->
      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Study shows dementia more common in older adults with vision issues (134 pts)]]></title>
            <link>https://www.michiganmedicine.org/health-lab/study-shows-dementia-more-common-older-adults-vision-issues</link>
            <guid>37125458</guid>
            <pubDate>Mon, 14 Aug 2023 19:13:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.michiganmedicine.org/health-lab/study-shows-dementia-more-common-older-adults-vision-issues">https://www.michiganmedicine.org/health-lab/study-shows-dementia-more-common-older-adults-vision-issues</a>, See on <a href="https://news.ycombinator.com/item?id=37125458">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span><span><span><span>Losing the ability to see clearly, and losing the ability to think or remember clearly, are two of the most dreaded, and preventable, health issues associated with getting older. </span></span></span></span></p>

<p><span><span><span><span>Now, a new study lends further weight to the idea that vision problems and dementia are linked.&nbsp; </span></span></span></span></p>

<p><span><span><span><span>In a sample of nearly 3,000 older adults who took vision tests and cognitive tests during home visits, the </span></span><a href="https://jamanetwork.com/journals/jamaophthalmology/fullarticle/10.1001/jamaophthalmol.2023.2854?guestAccessKey=35220a9e-e304-4ca1-a14e-c221ea092c34&amp;utm_source=For_The_Media&amp;utm_medium=referral&amp;utm_campaign=ftm_links&amp;utm_content=tfl&amp;utm_term=071323"><span><span>risk of dementia was much higher among those with eyesight problems</span></span></a><span><span> – including those who weren’t able to see well even when they were wearing their usual eyeglasses or contact lenses. </span></span></span></span></p>

<p><span><span><span><span>The research was published recently in <em>JAMA Ophthalmology</em> by a team from the </span></span><a href="https://www.umkelloggeye.org/"><span><span>Kellogg Eye Center at Michigan Medicine</span></span></a><span><span>, the University of Michigan’s academic medical center. </span></span></span></span></p>

<p><span><span><span><span>Based on data from a nationally representative study of older adults conducted in 2021 through the </span></span><a href="https://isr.umich.edu/"><span><span>U-M Institute for Social Research</span></span></a><span><span>, it adds to a growing pile of studies that have suggested a link between vision and dementia. </span></span></span></span></p>

<p><span><span><strong><em><span><span><a href="https://www.michiganmedicine.org/health-lab/many-older-adults-lack-clear-eyesight-even-glasses#:~:text=New%20research%20shows%20that%2028,lenses%2C%20or%20other%20visual%20aids.">SEE ALSO: Many older adults lack clear eyesight, even with glasses</a></span></span></em></strong></span></span></p>

<p><span><span><span><span>All of the older adults in the study were over the age of 71, with an average age of 77. They had their up-close and distance vision, and their ability to see letters that didn’t contrast strongly with their background, tested by a visiting team member using a digital tablet. They also took tests of memory and thinking ability, and provided health information including any existing diagnosis of Alzheimer’s disease or another form of dementia.</span></span></span></span></p>

<p><span><span><span><span><span>Just over 12% of the whole group had dementia. But that percentage was higher – nearly 22% -- among those who had impaired vision for seeing up close.</span></span></span></span></span></p>

<p><span><span><span><span><span>In addition, one-third (33%) of those with moderate or severe distance vision impairment, including those who were blind, had signs of dementia. So did 26% of those who had trouble seeing letters that didn’t contrast strongly against a background</span></span></span></span></span></p>

<p><span><span><span><span><span>Even among those with a mild distance vision issue, 19% had dementia.</span></span></span></span></span></p>

<p><span><span><span><span><span>After the researchers adjusted for other differences in health status and personal characteristics, people with moderate to severe distance vision issues were 72% more likely than those with no vision issues to have dementia. </span></span></span></span></span></p>

<p><span><span><span><span><span>The gaps were smaller, but still large, for other types of vision impairment&nbsp;</span></span></span>– <span><span><span> except mild problems with distance vision, where there was no statistical difference.</span></span></span></span></span></p>

<p><span><span><span><span><span>Those who had more than one kind of vision impairment were also 35% more likely to have dementia than those with normal vision.</span></span></span></span></span></p>

<p><span><span><span><span><span>The study b</span></span></span><span><span>uilds on previous studies that had similar findings but relied on self-reported vision abilities rather than objective testing, or that were not representative of the United States population. </span></span></span></span></p>

<p><span><span><strong><em><span><span><a href="https://www.michiganmedicine.org/health-lab/vision-impairment-associated-mortality">SEE ALSO: Vision Impairment is Associated with Mortality</a></span></span></em></strong></span></span></p>

<p><span><span><span><span>It also builds on previous work about cataract surgery that showed lower rates of dementia over time in adults who had had their distance vision restored by having surgery. </span></span></span></span></p>

<p><span><span><span><span>The authors, led by ophthalmologists Olivia Killeen, M.D., M.S. and Joshua Ehrlich, M.D., M.P.H., write, “</span></span><span><span><span>Prioritizing vision health may be key to optimizing both sight and overall health and well-being.</span></span></span> <span><span><span>Randomized trials are warranted to determine whether optimizing vision is a viable strategy to slow cognitive decline and reduce dementia risk.”</span></span></span></span></span></p>

<p><span><span><span><span>But in the meantime, in an </span></span><a href="https://jamanetwork-com.proxy.lib.umich.edu/article.aspx?doi=10.1001/jamaophthalmol.2023.3008"><span><span>accompanying editorial</span></span></a><span><span>, Sheila West, Ph.D., of the Wilmer Eye Institute at Johns Hopkins Medicine, wrote that the new study adds to accumulating evidence about the link between vision and cognitive issues. </span></span></span></span></p>

<p><span><span><span><span>“Equitable access to vision care services that prevent, reverse, or at least stave off progression of loss of sight is a worthy goal regardless of the potential impact on dementia and may be especially critical for those experiencing cognitive decline,” she wrote.</span></span></span></span></p>

<p><span><span><span><span>The study is based on data from </span></span><a href="https://www.nhats.org/"><span><span>the National Health and Aging Trends Study</span></span></a><span><span>, which is based at the U-M Institute for Social Research and the Johns Hopkins University Bloomberg School of Public Health.&nbsp; </span></span></span></span></p>

<p><span><span><strong><em><span><span><a href="https://www.michiganmedicine.org/health-lab/early-signs-alzheimers-most-older-adults-see-value-screening-havent-been-tested">SEE ALSO: Early signs of Alzheimer’s: Most older adults see the value of screening but haven’t been tested</a></span></span></em></strong></span></span></p>

<p><span><span><span><span>Last year, Ehrlich and colleagues </span></span><a href="https://jamanetwork.com/journals/jamaneurology/article-abstract/2791268"><span><span>published a paper in JAMA Neurology</span></span></a><span><span> that used another ISR-based survey of older adults – the Health and Retirement Study – to estimate the percentage of Americans with dementia whose condition is likely related to their vision loss. They calculated that 1.8 percent of all cases are vision-related, equating to more than 100,000 of the 6 million Americans with dementia. This study suggested that vision impairment should be considered alongside other more commonly recognized modifiable dementia risk factors. That study was funded by the </span></span><span><span>U-M Center to Accelerate Population Research in Alzheimer's through funding from the National Institute on Aging. </span></span></span></span></p>

<p><span><span><span><span>Killeen recently completed the </span></span><a href="https://ihpi.umich.edu/education-training/NCSP"><span><span>National Clinician Scholars Program</span></span></a><span><span> at the U-M Institute for Healthcare Policy and Innovation and is now at Duke University. Ehrlich is an assistant professor of Ophthalmology and Visual Sciences at Michigan Medicine and a research assistant professor at ISR, where he is a co-investigator of NHATS, as well as a member of IHPI.</span></span></span></span></p>

<p><span><span><span><span>The study’s authors also include Yunshu Zhou, M.S.</span></span></span></span></p>

<p><span><span><span><span>In addition to the National Institute on Aging grant that supports NHATS, and the U-M funding that supports the National Clinician Scholars Program, the study was funded by an unrestricted grant to the U-M Department of Ophthalmology and Visual Sciences by Research to Prevent Blindness</span></span></span></span></p>

<p><span><span><strong><span><span>Paper cited:</span></span></strong><span><span> “Objectively Measured Visual Impairment and Dementia Prevalence in Older Adults,” <em>JAMA Ophthalmology</em>. </span></span><a href="https://jamanetwork.com/journals/jamaophthalmology/fullarticle/10.1001/jamaophthalmol.2023.2854?guestAccessKey=35220a9e-e304-4ca1-a14e-c221ea092c34&amp;utm_source=For_The_Media&amp;utm_medium=referral&amp;utm_campaign=ftm_links&amp;utm_content=tfl&amp;utm_term=071323"><span><span>DOI:10.1001/jamaophthalmol.2023.2854</span></span></a></span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: LLMs can generate valid JSON 100% of the time (761 pts)]]></title>
            <link>https://github.com/normal-computing/outlines</link>
            <guid>37125118</guid>
            <pubDate>Mon, 14 Aug 2023 18:52:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/normal-computing/outlines">https://github.com/normal-computing/outlines</a>, See on <a href="https://news.ycombinator.com/item?id=37125118">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/normal-computing/outlines/blob/main/docs/source/_static/logo.png"><img src="https://github.com/normal-computing/outlines/raw/main/docs/source/_static/logo.png" alt="Outlines Logo" width="300"></a></p>
<h2 tabindex="-1" dir="auto">Outlines <g-emoji alias="wavy_dash" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/3030.png">〰️</g-emoji></h2>
<p dir="auto">Fast and reliable neural text generation.</p>
<p dir="auto"><a href="#installation">Install</a> •
<a href="#guided-generation">Guided generation</a> •
<a href="#prompting">Prompting primitives</a> •
<a href="#examples">Examples</a> •
<a href="#stay-tuned-for">Stay tuned</a></p>
</div>
<p dir="auto"><strong>Outlines</strong> 〰 is a library for neural text generation. You can think of it as a
more flexible replacement for the <code>generate</code> method in the
<a href="https://github.com/huggingface/transformers">transformers</a> library.</p>
<p dir="auto"><strong>Outlines</strong> 〰 helps developers <em>guide text generation</em> to build robust
interfaces with external systems. Provides generation methods that
guarantee that the output will match a regular expressions, or follow
a JSON schema.</p>
<p dir="auto"><strong>Outlines</strong> 〰 provides <em>robust prompting primitives</em> that separate the prompting
from the execution logic and lead to simple implementations of few-shot
generations, ReAct, meta-prompting, agents, etc.</p>
<p dir="auto"><strong>Outlines</strong> 〰 is designed as a <em>library</em> that is meant to be compatible the
broader ecosystem, not to replace it. We use as few abstractions as possible,
and generation can be interleaved with control flow, conditionals, custom Python
functions and calls to other libraries.</p>
<p dir="auto"><strong>Outlines</strong> 〰 is <em>compatible with all models</em>. It only interfaces with models
via the next-token logits. It can be used with API-based models as well.</p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul>
<li> 🖍️Simple and powerful prompting primitives based on the <a href="https://jinja.palletsprojects.com/" rel="nofollow">Jinja templating engine</a></li>
<li> <g-emoji alias="bullettrain_side" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f684.png">🚄</g-emoji> Guided generation, including multiple choice, type constraints and dynamic stopping</li>
<li> ⚡ Fast <a href="#efficient-regex-guided-generation">regex-guided generation</a></li>
<li> <g-emoji alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png">🔥</g-emoji> Fast <a href="#efficient-json-generation-following-a-pydantic-model">JSON generation</a> following a JSON schema or a Pydantic model</li>
<li> <g-emoji alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png">🐍</g-emoji> Interleave completions with loops, conditionals, and custom Python functions</li>
<li> <g-emoji alias="floppy_disk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png">💾</g-emoji> Caching of generations</li>
<li> 🤗 Integration with HuggingFace's <code>transformers</code> models</li>
</ul>
<p dir="auto">Outlines 〰 has new releases and features coming every week! Make sure to <g-emoji alias="star" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png">⭐</g-emoji> star and <g-emoji alias="eyes" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f440.png">👀</g-emoji> watch this repository to stay up to date.</p>
<h2 tabindex="-1" dir="auto">Stay tuned for</h2>
<ul dir="auto">
<li>Context-Free Grammar guided generation (<a href="https://github.com/normal-computing/outlines/pull/178" data-hovercard-type="pull_request" data-hovercard-url="/normal-computing/outlines/pull/178/hovercard">#178</a>);</li>
<li>Prompt-token alignment so you don't have to think about tokenization details (<a href="https://github.com/normal-computing/outlines/pull/201" data-hovercard-type="pull_request" data-hovercard-url="/normal-computing/outlines/pull/201/hovercard">#201</a>)</li>
<li>An infilling DSL (<a href="https://github.com/normal-computing/outlines/issues/182" data-hovercard-type="issue" data-hovercard-url="/normal-computing/outlines/issues/182/hovercard">#182</a>)</li>
</ul>
<p dir="auto">You can follow <a href="https://twitter.com/NormalComputing" rel="nofollow">@NormalComputing</a>, <a href="https://twitter.com/remilouf" rel="nofollow">@remilouf</a> or <a href="https://twitter.com/BrandonTWillard" rel="nofollow">@BrandonTWillard</a> for regular updates!</p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto"><strong>Outlines</strong> is available on PyPi:</p>

<h2 tabindex="-1" dir="auto">Guided generation</h2>
<p dir="auto">The first step towards reliability of systems that include large language models
is to ensure that there is a well-defined interface between their output and
user-defined code. <strong>Outlines</strong> provides ways to control the generation of
language models to make their output more predictable.</p>
<h3 tabindex="-1" dir="auto">Early stopping</h3>
<p dir="auto">You can stop the generation after a given sequence has been found:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import outlines.text.generate as generate
import outlines.models as models

model = models.transformers(&quot;gpt2&quot;)
answer = generate.continuation(model, stop=[&quot;.&quot;])(&quot;Tell me a one-sentence joke.&quot;)"><pre><span>import</span> <span>outlines</span>.<span>text</span>.<span>generate</span> <span>as</span> <span>generate</span>
<span>import</span> <span>outlines</span>.<span>models</span> <span>as</span> <span>models</span>

<span>model</span> <span>=</span> <span>models</span>.<span>transformers</span>(<span>"gpt2"</span>)
<span>answer</span> <span>=</span> <span>generate</span>.<span>continuation</span>(<span>model</span>, <span>stop</span><span>=</span>[<span>"."</span>])(<span>"Tell me a one-sentence joke."</span>)</pre></div>
<h3 tabindex="-1" dir="auto">Multiple choices</h3>
<p dir="auto">You can reduce the completion to a choice between multiple possibilities:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import outlines.text.generate as generate
import outlines.models as models

model = models.transformers(&quot;gpt2&quot;)

prompt = labelling(&quot;Just awesome&quot;, examples)
answer = generate.choice(model, [&quot;Positive&quot;, &quot;Negative&quot;])(prompt)"><pre><span>import</span> <span>outlines</span>.<span>text</span>.<span>generate</span> <span>as</span> <span>generate</span>
<span>import</span> <span>outlines</span>.<span>models</span> <span>as</span> <span>models</span>

<span>model</span> <span>=</span> <span>models</span>.<span>transformers</span>(<span>"gpt2"</span>)

<span>prompt</span> <span>=</span> <span>labelling</span>(<span>"Just awesome"</span>, <span>examples</span>)
<span>answer</span> <span>=</span> <span>generate</span>.<span>choice</span>(<span>model</span>, [<span>"Positive"</span>, <span>"Negative"</span>])(<span>prompt</span>)</pre></div>
<h3 tabindex="-1" dir="auto">Type constraint</h3>
<p dir="auto">You can instruct the model to only return integers or floats:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import outlines.text.generate as generate
import outlines.models as models

model = models.transformers(&quot;gpt2&quot;)

prompt = &quot;1+1=&quot;
answer = generate.integer(model)(prompt)

prompt = &quot;sqrt(2)=&quot;
answer = generate.float(model)(prompt)"><pre><span>import</span> <span>outlines</span>.<span>text</span>.<span>generate</span> <span>as</span> <span>generate</span>
<span>import</span> <span>outlines</span>.<span>models</span> <span>as</span> <span>models</span>

<span>model</span> <span>=</span> <span>models</span>.<span>transformers</span>(<span>"gpt2"</span>)

<span>prompt</span> <span>=</span> <span>"1+1="</span>
<span>answer</span> <span>=</span> <span>generate</span>.<span>integer</span>(<span>model</span>)(<span>prompt</span>)

<span>prompt</span> <span>=</span> <span>"sqrt(2)="</span>
<span>answer</span> <span>=</span> <span>generate</span>.<span>float</span>(<span>model</span>)(<span>prompt</span>)</pre></div>
<h3 tabindex="-1" dir="auto">Efficient regex-guided generation</h3>
<p dir="auto">Outlines also comes with fast regex-guided generation. In fact, the <code>choice</code>,
<code>integer</code> and <code>float</code> functions above all use regex-guided generation under the
hood:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import outlines.models as models
import outlines.text.generate as generate


model = models.transformers(&quot;gpt2-medium&quot;)

prompt = &quot;Is 1+1=2? &quot;
unguided = generate.continuation(model, max_tokens=30)(prompt)
guided = generate.regex(model, r&quot;\s*([Yy]es|[Nn]o|[Nn]ever|[Aa]lways)&quot;, max_tokens=30)(
    prompt
)

print(unguided)
# Is 1+1=2?
#
# This is probably the most perplexing question.
# As I said in one of my articles describing how
# I call 2 and 1, there isn't

print(guided)
# Is 1+1=2? Always"><pre><span>import</span> <span>outlines</span>.<span>models</span> <span>as</span> <span>models</span>
<span>import</span> <span>outlines</span>.<span>text</span>.<span>generate</span> <span>as</span> <span>generate</span>


<span>model</span> <span>=</span> <span>models</span>.<span>transformers</span>(<span>"gpt2-medium"</span>)

<span>prompt</span> <span>=</span> <span>"Is 1+1=2? "</span>
<span>unguided</span> <span>=</span> <span>generate</span>.<span>continuation</span>(<span>model</span>, <span>max_tokens</span><span>=</span><span>30</span>)(<span>prompt</span>)
<span>guided</span> <span>=</span> <span>generate</span>.<span>regex</span>(<span>model</span>, <span>r"\s*([Yy]es|[Nn]o|[Nn]ever|[Aa]lways)"</span>, <span>max_tokens</span><span>=</span><span>30</span>)(
    <span>prompt</span>
)

<span>print</span>(<span>unguided</span>)
<span># Is 1+1=2?</span>
<span>#</span>
<span># This is probably the most perplexing question.</span>
<span># As I said in one of my articles describing how</span>
<span># I call 2 and 1, there isn't</span>

<span>print</span>(<span>guided</span>)
<span># Is 1+1=2? Always</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="import outlines.models as models
import outlines.text.generate as generate


model = models.transformers(&quot;gpt2-medium&quot;)

prompt = &quot;What is the IP address of the Google DNS servers? &quot;
unguided = generate.continuation(model, max_tokens=30)(prompt)
guided = generate.regex(
    model,
    r&quot;((25[0-5]|2[0-4]\d|[01]?\d\d?)\.){3}(25[0-5]|2[0-4]\d|[01]?\d\d?)&quot;,
    max_tokens=30,
)(prompt)

print(unguided)
# What is the IP address of the Google DNS servers?
#
# Passive DNS servers are at DNS servers that are private.
# In other words, both IP servers are private. The database
# does not contain Chelsea Manning

print(guided)
# What is the IP address of the Google DNS servers?
# 2.2.6.1"><pre><span>import</span> <span>outlines</span>.<span>models</span> <span>as</span> <span>models</span>
<span>import</span> <span>outlines</span>.<span>text</span>.<span>generate</span> <span>as</span> <span>generate</span>


<span>model</span> <span>=</span> <span>models</span>.<span>transformers</span>(<span>"gpt2-medium"</span>)

<span>prompt</span> <span>=</span> <span>"What is the IP address of the Google DNS servers? "</span>
<span>unguided</span> <span>=</span> <span>generate</span>.<span>continuation</span>(<span>model</span>, <span>max_tokens</span><span>=</span><span>30</span>)(<span>prompt</span>)
<span>guided</span> <span>=</span> <span>generate</span>.<span>regex</span>(
    <span>model</span>,
    <span>r"((25[0-5]|2[0-4]\d|[01]?\d\d?)\.){3}(25[0-5]|2[0-4]\d|[01]?\d\d?)"</span>,
    <span>max_tokens</span><span>=</span><span>30</span>,
)(<span>prompt</span>)

<span>print</span>(<span>unguided</span>)
<span># What is the IP address of the Google DNS servers?</span>
<span>#</span>
<span># Passive DNS servers are at DNS servers that are private.</span>
<span># In other words, both IP servers are private. The database</span>
<span># does not contain Chelsea Manning</span>

<span>print</span>(<span>guided</span>)
<span># What is the IP address of the Google DNS servers?</span>
<span># 2.2.6.1</span></pre></div>
<p dir="auto">Unlike other libraries, regex-guided generation in Outlines is almost as fast
as non-guided generation.</p>
<h3 tabindex="-1" dir="auto">Efficient JSON generation following a Pydantic model</h3>
<p dir="auto">Outlines 〰 allows to guide the generation process so the output is <em>guaranteed</em> to follow a <a href="https://json-schema.org/" rel="nofollow">JSON schema</a> or <a href="https://docs.pydantic.dev/latest/" rel="nofollow">Pydantic model</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from typing import List
from enum import Enum
from pydantic import BaseModel, constr

import outlines.models as models
import outlines.text.generate as generate


class Weapon(str, Enum):
    sword = &quot;sword&quot;
    axe = &quot;axe&quot;
    mace = &quot;mace&quot;
    spear = &quot;spear&quot;
    bow = &quot;bow&quot;
    crossbow = &quot;crossbow&quot;


class Armor(str, Enum):
    leather = &quot;leather&quot;
    chainmail = &quot;chainmail&quot;
    plate = &quot;plate&quot;


class Character(BaseModel):
    name: constr(max_length=10)
    age: int
    armor: Armor
    weapon: Weapon
    strength: int


model = models.transformers(&quot;gpt2&quot;)
sequence = generate.json(model, Character)(&quot;Give me a character description&quot;)
print(sequence)
# {
#   &quot;name&quot;: &quot;ranbelt&quot;,
#   &quot;age&quot;: 26,
#   &quot;armor&quot;: &quot;chainmail&quot;,
#   &quot;weapon&quot;: &quot;bow&quot;,
#   &quot;strength&quot;: 5
# }

parsed = Character.model_validate_json(sequence)
print(parsed)
# name='ranbelt' age=26 armor=<Armor.chainmail: 'chainmail'> weapon=<Weapon.bow: 'bow'> strength=5"><pre><span>from</span> <span>typing</span> <span>import</span> <span>List</span>
<span>from</span> <span>enum</span> <span>import</span> <span>Enum</span>
<span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>, <span>constr</span>

<span>import</span> <span>outlines</span>.<span>models</span> <span>as</span> <span>models</span>
<span>import</span> <span>outlines</span>.<span>text</span>.<span>generate</span> <span>as</span> <span>generate</span>


<span>class</span> <span>Weapon</span>(<span>str</span>, <span>Enum</span>):
    <span>sword</span> <span>=</span> <span>"sword"</span>
    <span>axe</span> <span>=</span> <span>"axe"</span>
    <span>mace</span> <span>=</span> <span>"mace"</span>
    <span>spear</span> <span>=</span> <span>"spear"</span>
    <span>bow</span> <span>=</span> <span>"bow"</span>
    <span>crossbow</span> <span>=</span> <span>"crossbow"</span>


<span>class</span> <span>Armor</span>(<span>str</span>, <span>Enum</span>):
    <span>leather</span> <span>=</span> <span>"leather"</span>
    <span>chainmail</span> <span>=</span> <span>"chainmail"</span>
    <span>plate</span> <span>=</span> <span>"plate"</span>


<span>class</span> <span>Character</span>(<span>BaseModel</span>):
    <span>name</span>: <span>constr</span>(<span>max_length</span><span>=</span><span>10</span>)
    <span>age</span>: <span>int</span>
    <span>armor</span>: <span>Armor</span>
    <span>weapon</span>: <span>Weapon</span>
    <span>strength</span>: <span>int</span>


<span>model</span> <span>=</span> <span>models</span>.<span>transformers</span>(<span>"gpt2"</span>)
<span>sequence</span> <span>=</span> <span>generate</span>.<span>json</span>(<span>model</span>, <span>Character</span>)(<span>"Give me a character description"</span>)
<span>print</span>(<span>sequence</span>)
<span># {</span>
<span>#   "name": "ranbelt",</span>
<span>#   "age": 26,</span>
<span>#   "armor": "chainmail",</span>
<span>#   "weapon": "bow",</span>
<span>#   "strength": 5</span>
<span># }</span>

<span>parsed</span> <span>=</span> <span>Character</span>.<span>model_validate_json</span>(<span>sequence</span>)
<span>print</span>(<span>parsed</span>)
<span># name='ranbelt' age=26 armor=&lt;Armor.chainmail: 'chainmail'&gt; weapon=&lt;Weapon.bow: 'bow'&gt; strength=5</span></pre></div>
<p dir="auto">The method works with union types, optional types, arrays, nested schemas, etc. Some field constraints are <a href="https://github.com/normal-computing/outlines/issues/215" data-hovercard-type="issue" data-hovercard-url="/normal-computing/outlines/issues/215/hovercard">not supported yet</a>, but everything else should work.</p>
<h2 tabindex="-1" dir="auto">Prompting</h2>
<p dir="auto">Writing prompts by concatenating strings in pure Python quickly becomes
cumbersome: the prompt building logic gets entangled with the rest of the
program, and the structure of the rendered prompt is obfuscated.<strong>Outlines</strong>
makes it easier to write and manage prompts by encapsulating templates inside
"template functions".</p>
<p dir="auto">These functions make it possible to neatly separate the prompt logic from the
general program logic; they can be imported from other modules and libraries.</p>
<p dir="auto">Template functions require no superfluous abstraction, they use the Jinja2
templating engine to help build complex prompts in a concise manner:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import outlines.text as text
import outlines.models as models


examples = [
    (&quot;The food was digusting&quot;, &quot;Negative&quot;),
    (&quot;We had a fantastic night&quot;, &quot;Positive&quot;),
    (&quot;Recommended&quot;, &quot;Positive&quot;),
    (&quot;The waiter was rude&quot;, &quot;Negative&quot;)
]

@text.prompt
def labelling(to_label, examples):
    &quot;&quot;&quot;You are a sentiment-labelling assistant.

    {% for example in examples %}
    {{ example[0] }} // {{ example[1] }}
    {% endfor %}
    {{ to_label }} //
    &quot;&quot;&quot;

model = models.transformers(&quot;gpt2&quot;)
prompt = labelling(&quot;Just awesome&quot;, examples)
answer = text.generate.continuation(model, max_tokens=100)(prompt)"><pre><span>import</span> <span>outlines</span>.<span>text</span> <span>as</span> <span>text</span>
<span>import</span> <span>outlines</span>.<span>models</span> <span>as</span> <span>models</span>


<span>examples</span> <span>=</span> [
    (<span>"The food was digusting"</span>, <span>"Negative"</span>),
    (<span>"We had a fantastic night"</span>, <span>"Positive"</span>),
    (<span>"Recommended"</span>, <span>"Positive"</span>),
    (<span>"The waiter was rude"</span>, <span>"Negative"</span>)
]

<span>@<span>text</span>.<span>prompt</span></span>
<span>def</span> <span>labelling</span>(<span>to_label</span>, <span>examples</span>):
    <span>"""You are a sentiment-labelling assistant.</span>
<span></span>
<span>    {% for example in examples %}</span>
<span>    {{ example[0] }} // {{ example[1] }}</span>
<span>    {% endfor %}</span>
<span>    {{ to_label }} //</span>
<span>    """</span>

<span>model</span> <span>=</span> <span>models</span>.<span>transformers</span>(<span>"gpt2"</span>)
<span>prompt</span> <span>=</span> <span>labelling</span>(<span>"Just awesome"</span>, <span>examples</span>)
<span>answer</span> <span>=</span> <span>text</span>.<span>generate</span>.<span>continuation</span>(<span>model</span>, <span>max_tokens</span><span>=</span><span>100</span>)(<span>prompt</span>)</pre></div>
<h3 tabindex="-1" dir="auto">Tools</h3>
<p dir="auto">We can teach language models to call external functions to get additional
informations or perform tasks, by encoding the functions' description in the
prompt. To avoid duplicating information between the function definition and the
description passed to the prompt, we define custom Jinja filters that can
extract the function's name, description, signature and source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from typing import Callable, List
import outlines.text as text


def google_search(query: str):
    &quot;&quot;&quot;Google Search&quot;&quot;&quot;
    pass


def wikipedia_search(query: str):
    &quot;&quot;&quot;Wikipedia Search&quot;&quot;&quot;
    pass


@text.prompt
def agent(tools: List[Callable]):
    &quot;&quot;&quot;AVAILABLE COMMANDS:

    {% for tool in tools %}
    TOOL
    {{ tool | name }}, {{ tool | description }}, args: {{ tool | signature }}
    {{ tool | source }}
    {% endfor %}
    &quot;&quot;&quot;


prompt = my_commands([google_search, wikipedia_search])"><pre><span>from</span> <span>typing</span> <span>import</span> <span>Callable</span>, <span>List</span>
<span>import</span> <span>outlines</span>.<span>text</span> <span>as</span> <span>text</span>


<span>def</span> <span>google_search</span>(<span>query</span>: <span>str</span>):
    <span>"""Google Search"""</span>
    <span>pass</span>


<span>def</span> <span>wikipedia_search</span>(<span>query</span>: <span>str</span>):
    <span>"""Wikipedia Search"""</span>
    <span>pass</span>


<span>@<span>text</span>.<span>prompt</span></span>
<span>def</span> <span>agent</span>(<span>tools</span>: <span>List</span>[<span>Callable</span>]):
    <span>"""AVAILABLE COMMANDS:</span>
<span></span>
<span>    {% for tool in tools %}</span>
<span>    TOOL</span>
<span>    {{ tool | name }}, {{ tool | description }}, args: {{ tool | signature }}</span>
<span>    {{ tool | source }}</span>
<span>    {% endfor %}</span>
<span>    """</span>


<span>prompt</span> <span>=</span> <span>my_commands</span>([<span>google_search</span>, <span>wikipedia_search</span>])</pre></div>
<h3 tabindex="-1" dir="auto">Response models</h3>
<p dir="auto">We can instruct models to return their output in a pre-defined format, often
JSON. To avoid duplicating information between the function definition and the
description passed to the prompt we define a custom Jinja filter that can
extract the expected response's schema:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from pydantic import BaseModel
import outlines.text as text


class Joke(BaseModel):
    joke: str
    explanation: str


@text.prompt
def joke_ppt(response_model):
    &quot;&quot;&quot;Tell a joke and explain why the joke is funny.

    RESPONSE FORMAT:
    {{ response_model | schema }}
    &quot;&quot;&quot;


joke_ppt(Joke)
# Tell a joke and explain why the joke is funny.
#
# RESPONSE FORMAT:
# {
#    &quot;joke&quot;: &quot;The joke&quot;
#    &quot;explanation&quot;: &quot;The explanation of why the joke is funny&quot;
#  }"><pre><span>from</span> <span>pydantic</span> <span>import</span> <span>BaseModel</span>
<span>import</span> <span>outlines</span>.<span>text</span> <span>as</span> <span>text</span>


<span>class</span> <span>Joke</span>(<span>BaseModel</span>):
    <span>joke</span>: <span>str</span>
    <span>explanation</span>: <span>str</span>


<span>@<span>text</span>.<span>prompt</span></span>
<span>def</span> <span>joke_ppt</span>(<span>response_model</span>):
    <span>"""Tell a joke and explain why the joke is funny.</span>
<span></span>
<span>    RESPONSE FORMAT:</span>
<span>    {{ response_model | schema }}</span>
<span>    """</span>


<span>joke_ppt</span>(<span>Joke</span>)
<span># Tell a joke and explain why the joke is funny.</span>
<span>#</span>
<span># RESPONSE FORMAT:</span>
<span># {</span>
<span>#    "joke": "The joke"</span>
<span>#    "explanation": "The explanation of why the joke is funny"</span>
<span>#  }</span></pre></div>
<p dir="auto">With these prompting primitives <strong>Outlines</strong> makes building agents like
<a href="https://github.com/Significant-Gravitas/Auto-GPT">AutoGPT</a>,
<a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a>,
<a href="https://viper.cs.columbia.edu/" rel="nofollow">ViperGPT</a> or <a href="https://huggingface.co/docs/transformers/transformers_agents" rel="nofollow">Transformers
Agent</a> easier by
removing boilerplate prompting code.</p>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<h3 tabindex="-1" dir="auto">What contributions?</h3>
<p dir="auto">We curently only accept bug fixes and documentation contributions. If you have a
feature request, please start a new
<a href="https://github.com/normal-computing/outlines/discussions">discussion</a>. The
issue tracker is only intended for actionable items.</p>
<h3 tabindex="-1" dir="auto">How to contribute?</h3>
<p dir="auto">Run <code>pip install -e .[test]</code> or <code>conda env create -f environment.yml</code>. To build the documentation you will also need to run <code>pip install -r requirements-doc.txt</code>.</p>
<p dir="auto">Before pushing your code to repository please run <code>pre-commit run --all-files</code> and <code>pytest</code> to make sure that the code is formatted correctly and that the tests pass.</p>
<p dir="auto">Do not hesitate to open a draft PR before your contribution is ready, especially if you have questions and/or need feedback.</p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<ul dir="auto">
<li><a href="https://github.com/normal-computing/outlines/blob/main/examples/pick_odd_one_out.py">Pick the odd one out</a></li>
<li><a href="https://github.com/normal-computing/outlines/blob/main/examples/meta_prompting.py">Meta prompting</a></li>
<li><a href="https://github.com/normal-computing/outlines/blob/main/examples/meta_prompting.py">ReAct</a></li>
<li><a href="https://github.com/normal-computing/outlines/blob/main/examples/dust/math-generate-code.py">Generate code to solve math problems</a></li>
<li><a href="https://github.com/normal-computing/outlines/blob/main/examples/babyagi.py">BabyAGI</a></li>
<li><a href="https://github.com/normal-computing/outlines/blob/main/examples/sampling.ipynb">Uncertainty</a></li>
<li><a href="https://github.com/normal-computing/outlines/blob/main/examples/simulation_based_inference.ipynb">Simulation-based inference</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Cite Outlines</h2>
<div data-snippet-clipboard-copy-content="@article{willard2023efficient,
  title={Efficient Guided Generation for LLMs},
  author={Willard, Brandon T and Louf, R{\'e}mi},
  journal={arXiv preprint arXiv:2307.09702},
  year={2023}
}"><pre><code>@article{willard2023efficient,
  title={Efficient Guided Generation for LLMs},
  author={Willard, Brandon T and Louf, R{\'e}mi},
  journal={arXiv preprint arXiv:2307.09702},
  year={2023}
}
</code></pre></div>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">Outlines is open-source and licensed under the <a href="https://github.com/normal-computing/outlines/blob/main/LICENSE">Apache License 2.0</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Trafilatura: Python tool to gather text on the Web (120 pts)]]></title>
            <link>https://github.com/adbar/trafilatura</link>
            <guid>37124424</guid>
            <pubDate>Mon, 14 Aug 2023 18:10:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/adbar/trafilatura">https://github.com/adbar/trafilatura</a>, See on <a href="https://news.ycombinator.com/item?id=37124424">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">A Python package &amp; command-line tool to gather text on the Web</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/adbar/trafilatura/blob/master/docs/trafilatura-logo.png"><img alt="Logo as PNG image" src="https://github.com/adbar/trafilatura/raw/master/docs/trafilatura-logo.png"></a></p>

<a href="https://pypi.python.org/pypi/trafilatura" rel="nofollow"><img alt="Python package" src="https://camo.githubusercontent.com/4700e404125581d438bfb57b02977dc2b8cd4f277554e46a9312c0224594bf9c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f74726166696c61747572612e737667" data-canonical-src="https://img.shields.io/pypi/v/trafilatura.svg">
</a>
<a href="https://pypi.python.org/pypi/trafilatura" rel="nofollow"><img alt="Python versions" src="https://camo.githubusercontent.com/7fcb906ca6d28fb8fe7cecca7fac4e8796b74625b9b9cb18a73a9714e205ed1b/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f74726166696c61747572612e737667" data-canonical-src="https://img.shields.io/pypi/pyversions/trafilatura.svg">
</a>
<a href="http://trafilatura.readthedocs.org/en/latest/?badge=latest" rel="nofollow"><img alt="Documentation Status" src="https://camo.githubusercontent.com/93419916683f0122d49ede8da9d41abeac2be987121c3f0c1f1a068529c91f04/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f74726166696c61747572612f62616467652f3f76657273696f6e3d6c6174657374" data-canonical-src="https://readthedocs.org/projects/trafilatura/badge/?version=latest"></a>
<a href="https://codecov.io/gh/adbar/trafilatura" rel="nofollow"><img alt="Code Coverage" src="https://camo.githubusercontent.com/1b1011dee13fcb37bec9605b52571d74daa7a40ee15705d611b6d9c650170ac1/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f61646261722f74726166696c61747572612e737667" data-canonical-src="https://img.shields.io/codecov/c/github/adbar/trafilatura.svg">
</a>
<a href="https://pepy.tech/project/trafilatura" rel="nofollow"><img alt="Downloads" src="https://camo.githubusercontent.com/e854c0a4847d9c0788e3bdd38aa0d1a0857b1449607621bdb01f9e6bd1d86575/68747470733a2f2f7374617469632e706570792e746563682f62616467652f74726166696c61747572612f6d6f6e7468" data-canonical-src="https://static.pepy.tech/badge/trafilatura/month"></a>
<a href="https://aclanthology.org/2021.acl-demo.15/" rel="nofollow"><img alt="Reference DOI: 10.18653/v1/2021.acl-demo.15" src="https://camo.githubusercontent.com/6488438ef78b3040c66fbb1e6a2edd27ba312e985ac49c1ed376f1599bdf46bf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f492d31302e31383635332532467631253246323032312e61636c2d2d64656d6f2e31352d626c7565" data-canonical-src="https://img.shields.io/badge/DOI-10.18653%2Fv1%2F2021.acl--demo.15-blue"></a>

<a href="https://trafilatura.readthedocs.org/" rel="nofollow"><img alt="Demo as GIF image" src="https://github.com/adbar/trafilatura/raw/master/docs/trafilatura-demo.gif" data-animated-image=""></a>
<a name="user-content-description"></a>
<h2 tabindex="-1" dir="auto">Description</h2>
<p dir="auto">Trafilatura is a <strong>Python package and command-line tool</strong> designed to gather text on the Web. It includes discovery, extraction and text processing components. Its main applications are <strong>web crawling, downloads, scraping, and extraction</strong> of main texts, metadata and comments. It aims at staying <strong>handy and modular</strong>: no database is required, the output can be converted to various commonly used formats.</p>
<p dir="auto">Going from raw HTML to essential parts can alleviate many problems related to text quality, first by avoiding the <strong>noise caused by recurring elements</strong> (headers, footers, links/blogroll etc.) and second by including information such as author and date in order to <strong>make sense of the data</strong>. The extractor tries to strike a balance between limiting noise (precision) and including all valid parts (recall). It also has to be <strong>robust and reasonably fast</strong>, it runs in production on millions of documents.</p>
<p dir="auto">This tool can be <strong>useful for quantitative research</strong> in corpus linguistics, natural language processing, computational social science and beyond: it is relevant to anyone interested in data science, information extraction, text mining, and scraping-intensive use cases like search engine optimization, business analytics or information security.</p>
<a name="user-content-features"></a>
<h3 tabindex="-1" dir="auto">Features</h3>
<ul dir="auto">
<li><dl>
<dt>Web crawling and text discovery:</dt>
<dd><ul dir="auto">
<li>Focused crawling and politeness rules</li>
<li>Support for sitemaps (TXT, XML) and feeds (ATOM, JSON, RSS)</li>
<li>URL management (blacklists, filtering and de-duplication)</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Seamless and parallel processing, online and offline:</dt>
<dd><ul dir="auto">
<li>URLs, HTML files or parsed HTML trees usable as input</li>
<li>Efficient and polite processing of download queues</li>
<li>Conversion of previously downloaded files</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Robust and efficient extraction:</dt>
<dd><ul dir="auto">
<li>Main text (with LXML, common patterns and generic algorithms: jusText, fork of readability-lxml)</li>
<li>Metadata (title, author, date, site name, categories and tags)</li>
<li>Formatting and structural elements: paragraphs, titles, lists, quotes, code, line breaks, in-line text formatting</li>
<li>Comments (if applicable)</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Output formats:</dt>
<dd><ul dir="auto">
<li>Text (minimal formatting or Markdown)</li>
<li>CSV (with metadata, <a href="https://en.wikipedia.org/wiki/Tab-separated_values" rel="nofollow">tab-separated values</a>)</li>
<li>JSON (with metadata)</li>
<li>XML (with metadata, text formatting and page structure) and <a href="https://tei-c.org/" rel="nofollow">TEI-XML</a></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt>Optional add-ons:</dt>
<dd><ul dir="auto">
<li>Language detection on extracted content</li>
<li>Graphical user interface (GUI)</li>
<li>Speed optimizations</li>
</ul>
</dd>
</dl>
</li>
</ul>
<a name="user-content-evaluation-and-alternatives"></a>
<h3 tabindex="-1" dir="auto">Evaluation and alternatives</h3>
<p dir="auto">For more detailed results see the <a href="https://trafilatura.readthedocs.io/en/latest/evaluation.html" rel="nofollow">benchmark</a> and <a href="https://github.com/adbar/trafilatura/blob/master/tests/comparison.py">evaluation script</a>. To reproduce the tests just clone the repository, install all necessary packages and run the evaluation script with the data provided in the <em>tests</em> directory.</p>
<table>








<thead>
<tr><th colspan="6">750 documents, 2236 text &amp; 2250 boilerplate segments (2022-05-18), Python 3.8</th>
</tr>
<tr><th>Python Package</th>
<th>Precision</th>
<th>Recall</th>
<th>Accuracy</th>
<th>F-Score</th>
<th>Diff.</th>
</tr>
</thead>
<tbody>
<tr><td>html_text 0.5.2</td>
<td>0.529</td>
<td><strong>0.958</strong></td>
<td>0.554</td>
<td>0.682</td>
<td>2.2x</td>
</tr>
<tr><td>inscriptis 2.2.0 (html to txt)</td>
<td>0.534</td>
<td><strong>0.959</strong></td>
<td>0.563</td>
<td>0.686</td>
<td>3.5x</td>
</tr>
<tr><td>newspaper3k 0.2.8</td>
<td>0.895</td>
<td>0.593</td>
<td>0.762</td>
<td>0.713</td>
<td>12x</td>
</tr>
<tr><td>justext 3.0.0 (custom)</td>
<td>0.865</td>
<td>0.650</td>
<td>0.775</td>
<td>0.742</td>
<td>5.2x</td>
</tr>
<tr><td>boilerpy3 1.0.6 (article mode)</td>
<td>0.814</td>
<td>0.744</td>
<td>0.787</td>
<td>0.777</td>
<td>4.1x</td>
</tr>
<tr><td><em>baseline (text markup)</em></td>
<td>0.757</td>
<td>0.827</td>
<td>0.781</td>
<td>0.790</td>
<td><strong>1x</strong></td>
</tr>
<tr><td>goose3 3.1.9</td>
<td><strong>0.934</strong></td>
<td>0.690</td>
<td>0.821</td>
<td>0.793</td>
<td>22x</td>
</tr>
<tr><td>readability-lxml 0.8.1</td>
<td>0.891</td>
<td>0.729</td>
<td>0.820</td>
<td>0.801</td>
<td>5.8x</td>
</tr>
<tr><td>news-please 1.5.22</td>
<td>0.898</td>
<td>0.734</td>
<td>0.826</td>
<td>0.808</td>
<td>61x</td>
</tr>
<tr><td>readabilipy 0.2.0</td>
<td>0.877</td>
<td>0.870</td>
<td>0.874</td>
<td>0.874</td>
<td>248x</td>
</tr>
<tr><td>trafilatura 1.2.2 (standard)</td>
<td>0.914</td>
<td>0.904</td>
<td><strong>0.910</strong></td>
<td><strong>0.909</strong></td>
<td>7.1x</td>
</tr>
</tbody>
</table>
<a name="user-content-other-evaluations"></a>
<h4 tabindex="-1" dir="auto">Other evaluations:</h4>
<ul dir="auto">
<li>Most efficient open-source library in <em>ScrapingHub</em>'s <a href="https://github.com/scrapinghub/article-extraction-benchmark">article extraction benchmark</a></li>
<li>Best overall tool according to Gaël Lejeune &amp; Adrien Barbaresi, <a href="https://hal.archives-ouvertes.fr/hal-02768510v3/document" rel="nofollow">Bien choisir son outil d'extraction de contenu à partir du Web</a> (2020, PDF, French)</li>
</ul>
<a name="user-content-usage-and-documentation"></a>
<h2 tabindex="-1" dir="auto">Usage and documentation</h2>
<p dir="auto">For more information please refer to <a href="https://trafilatura.readthedocs.io/" rel="nofollow">the documentation</a>:</p>
<ul dir="auto">
<li><a href="https://trafilatura.readthedocs.io/en/latest/installation.html" rel="nofollow">Installation</a></li>
<li>Usage: <a href="https://trafilatura.readthedocs.io/en/latest/usage-cli.html" rel="nofollow">On the command-line</a>, <a href="https://trafilatura.readthedocs.io/en/latest/usage-python.html" rel="nofollow">With Python</a>, <a href="https://trafilatura.readthedocs.io/en/latest/usage-r.html" rel="nofollow">With R</a></li>
<li><a href="https://trafilatura.readthedocs.io/en/latest/corefunctions.html" rel="nofollow">Core Python functions</a></li>
<li>Python Notebook <a href="https://github.com/adbar/trafilatura/blob/master/docs/Trafilatura_Overview.ipynb">Trafilatura Overview</a></li>
<li><a href="https://trafilatura.readthedocs.io/en/latest/tutorials.html" rel="nofollow">Tutorials</a></li>
</ul>
<p dir="auto">For video tutorials see this Youtube playlist:</p>
<ul dir="auto">
<li><a href="https://www.youtube.com/watch?v=8GkiOM17t0Q&amp;list=PL-pKWbySIRGMgxXQOtGIz1-nbfYLvqrci" rel="nofollow">Web scraping how-tos and tutorials</a></li>
</ul>
<a name="user-content-license"></a>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto"><em>Trafilatura</em> is distributed under the <a href="https://github.com/adbar/trafilatura/blob/master/LICENSE">GNU General Public License v3.0</a>. If you wish to redistribute this library but feel bounded by the license conditions please try interacting <a href="https://www.gnu.org/licenses/gpl-faq.html#GPLInProprietarySystem" rel="nofollow">at arms length</a>, <a href="https://en.wikipedia.org/wiki/Multi-licensing" rel="nofollow">multi-licensing</a> with <a href="https://en.wikipedia.org/wiki/GNU_General_Public_License#Compatibility_and_multi-licensing" rel="nofollow">compatible licenses</a>, or <a href="https://github.com/adbar/trafilatura#author">contacting me</a>.</p>
<p dir="auto">See also <a href="https://web.archive.org/web/20230127221311/https://www.techrepublic.com/article/gpl-and-free-software-licensing-whats-in-it-for-business/" rel="nofollow">GPL and free software licensing: What's in it for business?</a></p>
<a name="user-content-context"></a>
<h2 tabindex="-1" dir="auto">Context</h2>
<a name="user-content-contributing"></a>
<h3 tabindex="-1" dir="auto">Contributing</h3>
<p dir="auto">Contributions are welcome! See <a href="https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a> for more information. Bug reports can be filed on the <a href="https://github.com/adbar/trafilatura/issues">dedicated page</a>.</p>
<p dir="auto">Many thanks to the <a href="https://github.com/adbar/trafilatura/graphs/contributors">contributors</a> who submitted features and bugfixes!</p>
<a name="user-content-roadmap"></a>
<h3 tabindex="-1" dir="auto">Roadmap</h3>
<p dir="auto">For planned enhancements and relevant milestones see <a href="https://github.com/adbar/trafilatura/milestones">issues page</a>.</p>
<a name="user-content-author"></a>
<h3 tabindex="-1" dir="auto">Author</h3>
<p dir="auto">This effort is part of methods to derive information from web documents in order to build <a href="https://www.dwds.de/d/k-web" rel="nofollow">text databases for research</a> (chiefly linguistic analysis and natural language processing). Extracting and pre-processing web texts to the exacting standards of scientific research presents a substantial challenge for those who conduct such research. Web corpus construction involves numerous design decisions, and this software package can help facilitate text data collection and enhance corpus quality.</p>
<ul dir="auto">
<li>Barbaresi, A. <a href="https://aclanthology.org/2021.acl-demo.15/" rel="nofollow">Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction</a>, Proceedings of ACL/IJCNLP 2021: System Demonstrations, 2021, p. 122-131.</li>
<li>Barbaresi, A. "<a href="https://hal.archives-ouvertes.fr/hal-02447264/document" rel="nofollow">Generic Web Content Extraction with Open-Source Software</a>", Proceedings of KONVENS 2019, Kaleidoscope Abstracts, 2019.</li>
<li>Barbaresi, A. "<a href="https://hal.archives-ouvertes.fr/hal-01371704v2/document" rel="nofollow">Efficient construction of metadata-enhanced web corpora</a>", Proceedings of the <a href="https://www.sigwac.org.uk/wiki/WAC-X" rel="nofollow">10th Web as Corpus Workshop (WAC-X)</a>, 2016.</li>
</ul>
<a href="https://aclanthology.org/2021.acl-demo.15/" rel="nofollow"><img alt="Reference DOI: 10.18653/v1/2021.acl-demo.15" src="https://camo.githubusercontent.com/6488438ef78b3040c66fbb1e6a2edd27ba312e985ac49c1ed376f1599bdf46bf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f492d31302e31383635332532467631253246323032312e61636c2d2d64656d6f2e31352d626c7565" data-canonical-src="https://img.shields.io/badge/DOI-10.18653%2Fv1%2F2021.acl--demo.15-blue"></a>
<a href="https://doi.org/10.5281/zenodo.3460969" rel="nofollow"><img alt="Zenodo archive DOI: 10.5281/zenodo.3460969" src="https://camo.githubusercontent.com/568a2322e67c5d157da75aa4f3c674b1278d3b7d74b1f699546f4711c7cc2fca/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333436303936392e737667" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.3460969.svg">
</a>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{barbaresi-2021-trafilatura,
  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},
  author = &quot;Barbaresi, Adrien&quot;,
  booktitle = &quot;Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations&quot;,
  pages = &quot;122--131&quot;,
  publisher = &quot;Association for Computational Linguistics&quot;,
  url = &quot;https://aclanthology.org/2021.acl-demo.15&quot;,
  year = 2021,
}"><pre>@inproceedings{barbaresi-2021-trafilatura,
  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool <span>for</span> Text Discovery and Extraction}},
  author = <span><span>"</span>Barbaresi, Adrien<span>"</span></span>,
  booktitle = <span><span>"</span>Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations<span>"</span></span>,
  pages = <span><span>"</span>122--131<span>"</span></span>,
  publisher = <span><span>"</span>Association for Computational Linguistics<span>"</span></span>,
  url = <span><span>"</span>https://aclanthology.org/2021.acl-demo.15<span>"</span></span>,
  year = 2021,
}</pre></div>
<p dir="auto">You can contact me via my <a href="https://adrien.barbaresi.eu/" rel="nofollow">contact page</a> or on <a href="https://github.com/adbar">GitHub</a>.</p>
<a name="user-content-software-ecosystem"></a>
<h3 tabindex="-1" dir="auto">Software ecosystem</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/adbar/trafilatura/blob/master/docs/software-ecosystem.png"><img alt="Software ecosystem" src="https://github.com/adbar/trafilatura/raw/master/docs/software-ecosystem.png"></a></p>
<p dir="auto"><em>Trafilatura</em>: <a href="https://en.wiktionary.org/wiki/trafilatura" rel="nofollow">Italian word</a> for <a href="https://en.wikipedia.org/wiki/Wire_drawing" rel="nofollow">wire drawing</a>.</p>
<p dir="auto"><a href="https://trafilatura.readthedocs.io/en/latest/used-by.html" rel="nofollow">Known uses of the software</a>.</p>
<p dir="auto">Corresponding posts on <a href="https://adrien.barbaresi.eu/blog/tag/trafilatura.html" rel="nofollow">Bits of Language</a> (blog).</p>

</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discord.io breached, 760k user accounts for sale on darknet (391 pts)]]></title>
            <link>https://stackdiary.com/the-data-of-760000-discord-io-users-was-put-up-for-sale-on-the-darknet/</link>
            <guid>37124187</guid>
            <pubDate>Mon, 14 Aug 2023 17:52:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stackdiary.com/the-data-of-760000-discord-io-users-was-put-up-for-sale-on-the-darknet/">https://stackdiary.com/the-data-of-760000-discord-io-users-was-put-up-for-sale-on-the-darknet/</a>, See on <a href="https://news.ycombinator.com/item?id=37124187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
							
<p><em>Note: I've gone ahead and updated the featured image, so it doesn't seem like this has something to do with Discord "directly". It was not my intention to leave an impression like that, but this still affects every single Discord user who was using the Discord.io service!</em></p>



<p>An unidentified individual has listed the data of 760,000 <a href="https://discord.io/servers" target="_blank" rel="noreferrer noopener">Discord.io</a> (the site is dead at the moment, so you can <a href="https://web.archive.org/web/20220329132537/https://discord.io/servers" target="_blank" rel="noreferrer noopener">see an Archive.org snapshot here</a>) users for sale on a darknet forum. This discovery was brought to light by <a href="https://t.me/dataleak/3057" target="_blank" rel="noreferrer noopener">the "Information Leaks" Telegram channel</a>, associated with the Russian service for tracking vulnerabilities, data leaks, and monitoring fraudulent online resources.</p>



<p>For clarity, Discord.io is/was a platform that allows you to create custom, personal Discord invites.&nbsp;The offered database comprises details like email addresses, hashed passwords, and other user-specific data.</p>



<p>UPDATE: Discord.io team has now confirmed that the breach is real; an update is added at the bottom of the article!</p>



<p>To vouch for the authenticity of the data, the seller presented a sample which was then reviewed by cybersecurity experts. Their evaluation confirmed that the sample logins are genuine, matching real Discord users. And just to make it clear - genuine, as in the corresponding email addresses from the leak, were verified to be associated with real Discord accounts through several password recovery tests.</p>



<p>The implication here is that malevolent parties can exploit this data for phishing schemes, spamming, or other deceptive undertakings.</p>


<div>
<figure><img decoding="async" width="800" height="382" src="https://stackdiary.com/wp-content/uploads/2023/08/discord-potential-leak.jpg" alt="discord potential leak" srcset="https://stackdiary.com/wp-content/uploads/2023/08/discord-potential-leak.jpg 800w, https://stackdiary.com/wp-content/uploads/2023/08/discord-potential-leak-300x143.jpg 300w, https://stackdiary.com/wp-content/uploads/2023/08/discord-potential-leak-768x367.jpg 768w" sizes="(max-width: 800px) 100vw, 800px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20382'%3E%3C/svg%3E" data-lazy-srcset="https://stackdiary.com/wp-content/uploads/2023/08/discord-potential-leak.jpg 800w, https://stackdiary.com/wp-content/uploads/2023/08/discord-potential-leak-300x143.jpg 300w, https://stackdiary.com/wp-content/uploads/2023/08/discord-potential-leak-768x367.jpg 768w" data-lazy-src="https://stackdiary.com/wp-content/uploads/2023/08/discord-potential-leak.jpg"><figcaption>Interesting text there at the bottom, "database access still available".</figcaption></figure></div>


<p><strong>This does look to be real, though. I tried reaching out to the Discord.io team <a href="https://discord.com/invite/fa2mdsZ" target="_blank" rel="noreferrer noopener">on their Discord server</a>, and two minutes after joining, every single channel got manually deleted.</strong></p>



<figure><video controls="" poster="https://stackdiary.com/wp-content/uploads/2023/08/screenshot-discord.com-2023.08.14-17_45_38.png" src="https://stackdiary.com/wp-content/uploads/2023/08/screencast-discord.com-2023.08.14-17_14_31.mp4"></video><figcaption>All the channels in the Discord.io server getting manually deleted.</figcaption></figure>



<p>As of now, Discord itself has not provided an official response to the situation. </p>



<p>However, for users of the platform, the advisable course of action is to promptly change passwords and activate two-factor authentication on their accounts to bolster security.</p>



<p><em>Note: We have reached out to Discord for comments and are awaiting their response.</em></p>



<h3>Discord.io team confirms the breach is real; here's what you need to know</h3>



<p>The team behind Discord.io has officially confirmed the data breach. In a detailed statement on Discord, they provided <a href="https://discord.com/channels/232335047170981890/1140679839347646614/1140685631471755389" target="_blank" rel="noreferrer noopener">a comprehensive account of the events</a> that led to the breach, what data was compromised, and the subsequent actions they've taken.</p>



<p><em>Timeline of Events:</em></p>



<ul>
<li><strong>Monday, August 14, 2023, 12:51 AM CET</strong>: A preview of the Discord.io user database appears on BreachForums.</li>



<li><strong>Monday, August 14, 2023, 4:30 PM CET</strong>: Discord.io team becomes aware of the breach.</li>



<li><strong>Monday, August 14, 2023, 4:36 PM CET</strong>: The breach's legitimacy is confirmed.</li>



<li><strong>Monday, August 14, 2023, 4:40 PM CET</strong>: All Discord.io services commence shutdown.</li>
</ul>



<p><em>Data Compromised in the Breach:</em></p>



<p><strong>Non-Sensitive Information:</strong></p>



<ul>
<li>Internal user ID</li>



<li>Avatar details</li>



<li>User status (e.g., moderator, admin, has ads, banned, public)</li>



<li>Coin balance and current streak in the free minigame</li>



<li>API key (relevant for a limited number of users)</li>



<li>Registration and last payment dates, including premium membership expiration</li>
</ul>



<p><strong>Potentially Sensitive Information:</strong></p>



<ul>
<li>Usernames, either from signup or the current Discord username</li>



<li>Discord ID</li>



<li>Email address associated with the account</li>



<li>Billing address (pertaining to a select few users who provided this before the adoption of Stripe for payments)</li>



<li>Salted and hashed passwords (mainly concerning users prior to 2018 when Discord.io began exclusively using Discord for logins)</li>
</ul>



<p><em>Data That Remained Secure:</em></p>



<ul>
<li>Anything not explicitly mentioned in the compromised list.</li>



<li>Payment details, which are securely stored with partners Stripe and PayPal.</li>
</ul>



<p><em>Further Actions &amp; Notes:</em></p>



<ul>
<li>All existing premium subscriptions have been canceled, with the team set to contact subscribers individually.</li>



<li>As of their last update, the Discord.io team hasn't established contact with the culprits nor discerned if the database has been shared with the public.</li>



<li>A list of servers that once used Discord.io's service has been made available, though it might contain outdated or inactive links.</li>



<li>Users wishing to get in touch are encouraged to send a helpdesk request, with "Support" for general queries and "Admin" for sensitive matters. Given the gravity of the situation, the team cautions that they might not be able to address every message but appreciate user patience and understanding.</li>
</ul>
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Does there exist a complete implementation of the Risch algorithm? (144 pts)]]></title>
            <link>https://mathoverflow.net/questions/374089/does-there-exist-a-complete-implementation-of-the-risch-algorithm</link>
            <guid>37124059</guid>
            <pubDate>Mon, 14 Aug 2023 17:43:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mathoverflow.net/questions/374089/does-there-exist-a-complete-implementation-of-the-risch-algorithm">https://mathoverflow.net/questions/374089/does-there-exist-a-complete-implementation-of-the-risch-algorithm</a>, See on <a href="https://news.ycombinator.com/item?id=37124059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>No computer algebra system implements a complete decision process for the integration of mixed transcendental and algebraic functions.</p>
<p>The integral from the excellent paper of Schultz may be solved by Maple if you convert the integrand to RootOf notation (Why this is not done internally in Maple is an interesting question?)</p>
<pre><code>int(convert((29*x^2+18*x-3)/(x^6+4*x^5+6*x^4-12*x^3+33*x^2-16*x)^(1/2),RootOf),x);
</code></pre>
<p>My experiments suggest Maple has the best implementation of the Risch-Trager-Bronstein algorithm for the integration of purely algebraic integrals in terms of elementary functions (ref: table 1, section 3 of Sam Blake, <em>A Simple Method for Computing Some Pseudo-Elliptic Integrals in Terms of Elementary Functions</em>, arXiv:<a href="https://arxiv.org/abs/2004.04910" rel="noreferrer">2004.04910</a>). However, Maple's implementation does not integrate expressions containing parameters or nested radicals (both of which has some support in AXIOM and FriCAS).</p>
<p>It would seem that some significant progress has been made in computing the logarithmic part of a mixed transcendental-algebraic integral by Miller [1]. Though, as far as I know, no computer algebra system has implemented his algorithm. It is also not clear if Miller's algorithm can deal with parameters, for example, the Risch-Trager-Bronstein algorithm has difficulties with the following pseudo-elliptic integral</p>
<p><span>$$\int\frac{\left(p x^2-q\right) \left(p x^2-x+q\right)dx}{x \left(p x^2+2 x+q\right) \sqrt{2 p^2x^4+2 p x^3+(4 p q+1) x^2+2 q x+2 q^2}} = - \frac{1}{\sqrt{2}}\log (x) + \frac{1}{\sqrt{2}}\log \left(\sqrt{2} y +2 p x^2+x+2q\right) - \frac{3}{\sqrt{5}}\tanh ^{-1}\left(\frac{\sqrt{5} y}{3 p x^2+3 q+x}\right),$$</span>
where <span>$y=\sqrt{2 p^2 x^4+2 p x^3+(4 pq+1)x^2+2 q x+2 q^2}$</span>. My heuristic in the <a href="https://arxiv.org/abs/2004.04910.pdf" rel="noreferrer">previously-linked paper</a> computes this integral quickly with the substitution <span>$u=\frac{px^2+q}{p x}$</span>.</p>
<p>In regards to the mixed algebraic-transcendental case of the Risch-Trager-Bronstein algorithm, an integral which cannot be solved with Maple, Mathematica, AXIOM or FriCAS (and possibly other CAS) is</p>
<p><span>$$\int \frac{\left(\sqrt{x}+1\right) \left(e^{2x \sqrt{x}} -a\right) \sqrt{a^2+2 a x e^{2 \sqrt{x}} +cx e^{2 \sqrt{x}} +x^2 e^{4 \sqrt{x}}}}{x \sqrt{x}e^{\sqrt{x}} \left(a+x e^{2 \sqrt{x}} \right)} dx.$$</span></p>
<p>This integral is interesting as it returns two distinct messages from AXIOM and FriCAS suggesting their respective implementations are incomplete. FriCAS returns</p>
<pre><code>(1) -&gt; integrate(((-a+exp(2*x^(1/2))*x)*x^(-3/2)*(1+x^(1/2))*(a^2+2*a*exp(2*x^(1/2))*x+c*exp(2*x^(1/2))*x+exp(4*x^(1/2))*x^2)^(1/2))/(exp(x^(1/2))*(a+exp(2*x^(1/2))*x)),x)
                                                                                                        
   &gt;&gt; Error detected within library code:                                                               
   integrate: implementation incomplete (has polynomial part)                                                                                                                                                
</code></pre>
<p>While AXIOM returns</p>
<pre><code>(1) -&gt; integrate(((-a+exp(2*x^(1/2))*x)*x^(-3/2)*(1+x^(1/2))*(a^2+2*a*exp(2*x^(1/2))*x+c*exp(2*x^(1/2))*x+exp(4*x^(1/2))*x^2)^(1/2))/(exp(x^(1/2))*(a+exp(2*x^(1/2))*x)),x)
                                                                                                        
   &gt;&gt; Error detected within library code:
   integrate: implementation incomplete (constant residues)                                                                                                                                             
</code></pre>
<p>[1] Miller, B. (2012). “<a href="http://hdl.handle.net/2346/45299" rel="noreferrer">On the Integration of Elementary Functions: Computing the Logarithmic Part</a>”. Thesis (Ph.D.) Texas Tech University, Dept. of Mathematics and Statistics.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fan-made port of Wipeout (PSX) for modern browsers (106 pts)]]></title>
            <link>https://phoboslab.org/wipegame/</link>
            <guid>37123992</guid>
            <pubDate>Mon, 14 Aug 2023 17:38:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phoboslab.org/wipegame/">https://phoboslab.org/wipegame/</a>, See on <a href="https://news.ycombinator.com/item?id=37123992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="select-version">
				<p>
					<a href="#" id="load-full-version">FULL VERSION</a><br>
					the complete game ~144mb
				</p>
				<p>
					<a href="#" id="load-minimal-version">MINIMAL VERSION</a><br>
					no intro, no music ~11mb
				</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JWST spots giant black holes all over the early universe (303 pts)]]></title>
            <link>https://www.quantamagazine.org/jwst-spots-giant-black-holes-all-over-the-early-universe-20230814/</link>
            <guid>37123792</guid>
            <pubDate>Mon, 14 Aug 2023 17:26:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/jwst-spots-giant-black-holes-all-over-the-early-universe-20230814/">https://www.quantamagazine.org/jwst-spots-giant-black-holes-all-over-the-early-universe-20230814/</a>, See on <a href="https://news.ycombinator.com/item?id=37123792">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Years before she was even sure the <a href="https://www.quantamagazine.org/why-nasas-james-webb-space-telescope-matters-so-much-20211203/">James Webb Space Telescope</a> would successfully launch, <a href="https://www.mit.edu/~eilers/">Christina Eilers</a> started planning a conference for astronomers specializing in the early universe. She knew that if — preferably, when — JWST started making observations, she and her colleagues would have a lot to talk about. Like a time machine, the telescope could see farther away and farther into the past than any previous instrument.</p>
<p>Fortunately for Eilers (and the rest of the astronomical community), her planning was not for naught: JWST launched and deployed without a hitch, then started scrutinizing the early universe in earnest from its perch in space a million miles away.</p>
<p>In mid-June, about 150 astronomers gathered at the Massachusetts Institute of Technology for Eilers’ JWST “First Light” conference. Not quite a year had passed since JWST <a href="https://www.quantamagazine.org/two-weeks-in-the-webb-space-telescope-is-reshaping-astronomy-20220725/">started sending images</a> back to Earth. And just as Eilers had anticipated, the telescope was already reshaping astronomers’ understanding of the cosmos’s first billion years.</p>

<p>One set of enigmatic objects stood out in the myriad presentations. Some astronomers called them “hidden little monsters.” To others, they were “little red dots.” But whatever their name, the data was clear: When JWST stares at young galaxies — which appear as mere red specks in the darkness — it sees a surprising number with cyclones churning in their centers.</p>
<p>“There seems to be an abundant population of sources we didn’t know about,” said Eilers, an astronomer at MIT, “which we didn’t anticipate finding at all.”</p>
<p>In recent months, a torrent of observations of the cosmic smudges has delighted and confounded astronomers.</p>
<p>“Everybody is talking about these little red dots,” said <a href="https://www.as.arizona.edu/people/faculty/xiaohui-fan">Xiaohui Fan</a>, a researcher at the University of Arizona who has spent his career searching for distant objects in the early universe.</p>
<p>The most straightforward explanation for the tornado-hearted galaxies is that large black holes weighing millions of suns are whipping the gas clouds into a frenzy. That finding is both expected and perplexing. It is expected because JWST was built, in part, to find the ancient objects. They are the ancestors of billion-sun behemoth black holes that seem to appear in the cosmic record inexplicably early. By studying these precursor black holes, such as three record-setting youngsters discovered this year, scientists hope to learn where the first humongous black holes came from and perhaps identify which of two competing theories better describes their formation: Did they grow extremely rapidly, or were they simply born big? Yet the observations are also perplexing because few astronomers expected JWST to find so many young, hungry black holes — and surveys are turning them up by the dozen. In the process of attempting to solve the former mystery, astronomers have uncovered a throng of bulky black holes that may rewrite established theories of stars, galaxies and more.</p>
<p>“As a theorist, I have to build a universe,” said <a href="http://www2.iap.fr/users/volonter/">Marta Volonteri</a>, an astrophysicist specializing in black holes at the Paris Institute of Astrophysics. Volonteri and her colleagues are now contending with the influx of giant black holes in the early cosmos. “If they are [real], they completely change the picture.”</p>
<h2><strong>A Cosmic Time Machine </strong></h2>
<p>The JWST observations are shaking up astronomy in part because the telescope can detect light reaching Earth from deeper in space than any earlier machine.</p>

<p>“We built this absurdly powerful telescope over 20 years,” said <a href="https://www.granttremblay.com/">Grant Tremblay</a>, an astrophysicist at the Harvard-Smithsonian Center for Astrophysics. “The whole point of it originally was to look deep into cosmic time.”</p>
<p>One of the mission’s goals is to catch galaxies in the act of forming during the universe’s first billion years (out of its roughly 13.8-billion-year history). The telescope’s initial observations from last summer <a href="https://www.quantamagazine.org/standard-model-of-cosmology-survives-jwsts-surprising-finds-20230120/">hinted at a young universe</a> full of strikingly mature galaxies, but the information astronomers could wring from such images was limited. To really understand the early universe, astronomers needed more than just the images; they hungered for the spectra of those galaxies — the data that comes in when the telescope breaks incoming light into specific hues.</p>
<p>Galactic spectra, which JWST started to send back in earnest at the end of last year, are useful for two reasons.</p>
<p>First, they let astronomers nail down the galaxy’s age. The infrared light JWST collects is reddened, or redshifted, meaning that as it traverses the cosmos, its wavelengths are stretched by the expansion of space. The extent of that redshift lets astronomers determine a galaxy’s distance, and therefore when it originally emitted its light. Nearby galaxies have a redshift of almost zero. JWST can handily make out objects beyond a redshift of 5, which corresponds to roughly 1 billion years after the Big Bang. Objects at higher redshifts are significantly older and farther away.</p>

<p>Second, spectra give astronomers a sense of what’s happening in a galaxy. Each hue marks an interaction between photons and specific atoms (or molecules). One color originates from a hydrogen atom flashing as it settles down after a bump; another indicates jostled oxygen atoms, and another nitrogen. A spectrum is a pattern of colors that reveals what a galaxy is made of and what those elements are doing, and JWST is providing that crucial context for galaxies at unprecedented distances.</p>
<p>“We’ve made such a huge leap,” said <a href="https://users.ox.ac.uk/~phys2391/">Aayush Saxena</a>, an astronomer at the University of Oxford. The fact that “we’re talking about chemical composition of redshift 9 galaxies is just absolutely remarkable.”</p>
<p>(Redshift 9 is mind-bogglingly distant, corresponding to a time when the universe was a mere 0.55 billion years old.)</p>
<p>Galactic spectra are also perfect tools for finding a major perturber of atoms: giant black holes that lurk at the hearts of galaxies. Black holes themselves are dark, but when they feed on gas and dust, they rip atoms apart, making them beam out telltale colors. Long before JWST’s launch, astrophysicists hoped the telescope would help them spot those patterns and find enough of the early universe’s biggest and most active black holes to solve the mystery of how they formed.</p>
<h2><strong>Too Big, Too Early</strong></h2>
<p>The mystery began more than 20 years ago, when a team led by Fan spotted one of <a href="https://iopscience.iop.org/article/10.1086/300944">the most distant galaxies</a> ever observed — a brilliant quasar, or a galaxy anchored to an active supermassive black hole weighing perhaps billions of suns. It had a redshift of 5, corresponding to around 1.1 billion years after the Big Bang. With further sweeps of the sky, Fan and his colleagues repeatedly broke their own records, pushing the quasar redshift frontier to <a href="https://iopscience.iop.org/article/10.1086/324111">6 in 2001</a> and eventually to <a href="https://iopscience.iop.org/article/10.3847/2041-8213/abd8c6/meta">7.6 in 2021</a> ­­— just 0.7 billion years after the Big Bang.</p>
<p>The problem was that making such gigantic black holes seemed impossible so early in cosmic history.</p>
<p>Like any object, black holes take time to grow and form. And like a 6-foot-tall toddler, Fan’s supersize black holes were too big for their age — the universe wasn’t old enough for them to have accrued billions of suns of heft. To explain those overgrown toddlers, physicists were forced to consider two distasteful options.</p>

<p>The first was that Fan’s galaxies started off filled with standard, roughly stellar-mass black holes of the sort supernovas often leave behind. Those then grew both by merging and by swallowing up surrounding gas and dust. Normally, if a black hole feasts aggressively enough, an outpouring of radiation pushes away its morsels. That stops the feeding frenzy and sets a speed limit for black hole growth that scientists call the Eddington limit. But it’s a soft ceiling: A constant torrent of dust could conceivably overcome the outpouring of radiation. However, it’s hard to imagine sustaining such “super-Eddington” growth for long enough to explain Fan’s beasts — they would have had to bulk up unthinkably fast.</p>
<p>Or perhaps black holes can be born improbably large. Gas clouds in the early universe may have collapsed directly into black holes weighing many thousands of suns — producing objects called heavy seeds. This scenario is hard to stomach too, because such large, lumpy gas clouds should fracture into stars before forming a black hole.</p>
<p>One of JWST’s priorities is to evaluate these two scenarios by peering into the past and catching the fainter ancestors of Fan’s galaxies. These precursors wouldn’t quite be quasars, but galaxies with somewhat smaller black holes on their way to becoming quasars. With JWST, scientists have their best chance of spotting black holes that have barely started to grow — objects that are young enough and small enough for researchers to nail down their birth weight.</p>
<p>That’s one reason a group of astronomers with the Cosmic Evolution Early Release Science Survey, or CEERS, led by Dale Kocevski of Colby College, started working overtime when they first noticed signs of such young black holes popping up in the days following Christmas.</p>
<p>“It’s kind of impressive how many of these there are,” wrote <a href="https://www.rit.edu/directory/jsksps-jeyhan-kartaltepe">Jeyhan Kartaltepe</a>, an astronomer at the Rochester Institute of Technology, during a discussion on Slack.</p>
<p>“Lots of little hidden monsters,” Kocevski replied.</p>

<h2><strong>A Growing Crowd of Monsters </strong></h2>
<p>In the CEERS spectra, a few galaxies immediately leapt out as potentially hiding baby black holes — the little monsters. Unlike their more vanilla siblings, these galaxies emitted light that didn’t arrive with just one crisp shade for hydrogen. Instead, the hydrogen line was smeared, or broadened, into a range of hues, indicating that some light waves were squished as orbiting gas clouds accelerated toward JWST (just as an approaching ambulance emits a rising wail as its siren’s soundwaves are compressed) while other waves were stretched as clouds flew away. Kocevski and his colleagues knew that black holes were just about the only object capable of slinging hydrogen around like that.</p>

<p>“The only way to see the broad component of the gas orbiting the black hole is if you’re looking right down the barrel of the galaxy and right into the black hole,” Kocevski said.</p>
<p>By the end of January, the CEERS team had managed to crank out a preprint describing two of the “hidden little monsters,” as they called them. Then the group set out to systematically study a wider swath of the hundreds of galaxies collected by their program to see just how many black holes were out there. But they got scooped by another team, led by Yuichi Harikane of the University of Tokyo, just weeks later. Harikane’s group searched 185 of the most distant CEERS galaxies and <a href="https://arxiv.org/abs/2303.11946">found 10</a> with broad hydrogen lines — the likely work of million-solar-mass central black holes at redshifts between 4 and 7. Then in June, an analysis of two other surveys led by <a href="https://people.phys.ethz.ch/~mattheej/Welcome.html">Jorryt Matthee</a> of the Swiss Federal Institute of Technology Zurich identified 20 more “<a href="https://arxiv.org/abs/2306.05448">little red dots</a>” with broad hydrogen lines: black holes churning around redshift 5. An analysis <a href="https://arxiv.org/abs/2308.01230">posted in early August</a> announced another dozen, a few of which may even be in the process of growing by merging.</p>
<p>“I’ve been waiting for these things for so long,” Volonteri said. “It’s been incredible.”</p>
<p>But few astronomers anticipated the sheer number of galaxies with a big, active black hole. The baby quasars in JWST’s first year of observations are more numerous than scientists had predicted based on the <a href="https://iopscience.iop.org/article/10.3847/1538-4357/abbe11/meta">census of adult quasars</a> — between 10 times and 100 times more abundant.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Judge rules in favor of Montana youths in landmark climate decision (220 pts)]]></title>
            <link>https://www.washingtonpost.com/climate-environment/2023/08/14/youths-win-montana-climate-trial/</link>
            <guid>37123755</guid>
            <pubDate>Mon, 14 Aug 2023 17:23:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/climate-environment/2023/08/14/youths-win-montana-climate-trial/">https://www.washingtonpost.com/climate-environment/2023/08/14/youths-win-montana-climate-trial/</a>, See on <a href="https://news.ycombinator.com/item?id=37123755">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/climate-environment/2023/08/14/youths-win-montana-climate-trial/: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Following Pushback, Zoom Says It Won't Use Customer Data to Train AI Models (353 pts)]]></title>
            <link>https://www.darkreading.com/analytics/following-pushback-zoom-says-it-won-t-use-customer-data-to-train-ai-models</link>
            <guid>37123572</guid>
            <pubDate>Mon, 14 Aug 2023 17:09:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.darkreading.com/analytics/following-pushback-zoom-says-it-won-t-use-customer-data-to-train-ai-models">https://www.darkreading.com/analytics/following-pushback-zoom-says-it-won-t-use-customer-data-to-train-ai-models</a>, See on <a href="https://news.ycombinator.com/item?id=37123572">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Zoom says it will walk back a recent change to its terms of service that allowed the company to use some customer content to train its machine learning and artificial intelligence models.</p><p>The move comes after recent criticism on social media from customers who are concerned about the privacy implications of Zoom using data in such a manner.</p><h2>Backing Down on Data Use Plans</h2><p>"Following feedback, Zoom made the decision to <a href="https://explore.zoom.us/en/terms/" target="_blank">update its Terms of Service</a> to reflect Zoom does not use any of your audio, video, chat, screen sharing, attachments or other communications-like Customer Content (such as poll results, whiteboard and reactions) to train Zoom or third-party artificial intelligence models," a spokeswoman said in an emailed statement. "Zoom has accordingly updated its Terms of Service and product to make this policy clear."</p><p>Zoom's decision — and the reason for it — is sure to add to the growing debate about the privacy and security implications of technology companies using customer data to train AI models.</p><p>In Zoom's case, the company recently introduced two generative AI features — Zoom IQ Meeting Summary and Zoom IQ Team Chat Compose — that offer AI-powered chat composition and automated meeting summaries. The terms of an updated service policy that the company announced earlier this year gave Zoom the right to use some customer data behind these services <span>for training the AI models </span>— without needing customer consent.</p><p>Specifically, Zoom's policy gave the company a "perpetual, worldwide, non-exclusive, royalty-free, sublicensable, and transferable" right to use customer data for a wide range of purposes including machine learning, artificial intelligence, training, and testing. It also allowed Zoom the unbridled right to do virtually anything with the data including to "redistribute, publish, import, access, use, store, transmit, disclose" the data.</p><p>After customers <a href="https://www.linkedin.com/feed/update/urn:li:activity:7094259387493453825/" target="_blank">pushed back on social media</a> Zoom initially revised its policy earlier this month to give customers the right to opt out of having their data used for AI training. "Zoom will not use audio, video or chat Customer Content to train our artificial intelligence models without your consent," the company said.</p><h2>Delicate Balance</h2><p>On August 11, the company again revised its terms of service, this time to scrub virtually all references to the use of artificial intelligence. The newly revised policy still gives Zoom all "rights, title, and interest" to a lot of service generated data including telemetry data, product usage data, and diagnostic data. But the company will not user customer content to train AI models.</p><p>Zoom's experience highlights the delicate balance tech companies must strike between innovation and user trust when integrating AI into their products and services. Numerous technology companies have been using customer data for years to improve user experiences and introduce new features and functions, says Shomron Jacob, head of machine learning at iterate.ai. "Data is often called the "new oil" in the digital age because of its invaluable role in training and refining AI models to improve user experiences, functionalities, and new features," Jacob says. "Companies like Google, Facebook, and Amazon have long used user data to tailor their services and improve their AI algorithms."</p><p>However, given the increasing scrutiny of the privacy, security, and ethical implications surrounding AI, there's a rising expectation for transparency and user consent, he says. While companies will likely continue to use customer data like they have been, there is going to be increased pressure on them to provide clear user opt-outs, to anonymize data and to ensure that personal and sensitive information remain protected.</p><p>"Moreover, regulatory frameworks like [the] GDPR in Europe and CCPA in California set data collection and usage standards," Jacob says. "As these regulations become more stringent and widespread, tech companies must navigate the dual challenges of leveraging user data for AI improvements while ensuring strict compliance and safeguarding user trust."</p></div></div>]]></description>
        </item>
    </channel>
</rss>