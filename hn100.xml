<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 07 Nov 2023 06:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Inshellisense – IDE style shell autocomplete (102 pts)]]></title>
            <link>https://github.com/microsoft/inshellisense</link>
            <guid>38167363</guid>
            <pubDate>Mon, 06 Nov 2023 19:15:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/inshellisense">https://github.com/microsoft/inshellisense</a>, See on <a href="https://news.ycombinator.com/item?id=38167363">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-inshellisense" dir="auto"><a href="#inshellisense">inshellisense</a></h2>
<p dir="auto"><code>inshellisense</code> provides IDE style autocomplete for shells. It's a terminal native runtime for <a href="https://github.com/withfig/autocomplete">autocomplete</a> which has support for 600+ command line tools. <code>inshellisense</code> supports Windows, Linux, &amp; MacOS.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/inshellisense/blob/main/docs/demo.gif"><img alt="demo of inshellisense working" src="https://github.com/microsoft/inshellisense/raw/main/docs/demo.gif" height="450px" data-animated-image=""></a></p>
<h2 tabindex="-1" id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting Started</a></h2>
<h3 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="npm install -g @microsoft/inshellisense"><pre>npm install -g @microsoft/inshellisense</pre></div>
<h3 tabindex="-1" id="user-content-quickstart" dir="auto"><a href="#quickstart">Quickstart</a></h3>
<p dir="auto">After completing the installation, you can already run <code>inshellisense --shell &lt;shell&gt;</code> to start the autocomplete session for your desired shell. Additionally, you can bind <code>inshellisense</code> to a keybinding of <code>CTRL+a</code> by running the below command. This brings the added advantages of automatically starting the autocomplete session with your current shell and injecting any accepted command into your shell's history.</p>

<p dir="auto">Additionally, inshellisense is also aliased under <code>is</code> after install for convenience.</p>
<h2 tabindex="-1" id="user-content-integrations" dir="auto"><a href="#integrations">Integrations</a></h2>
<p dir="auto">inshellisense supports the following shells:</p>
<ul dir="auto">
<li><a href="https://www.gnu.org/software/bash/" rel="nofollow">bash</a></li>
<li><a href="https://www.zsh.org/" rel="nofollow">zsh</a></li>
<li><a href="https://github.com/fish-shell/fish-shell">fish</a></li>
<li><a href="https://github.com/PowerShell/PowerShell">pwsh</a></li>
<li><a href="https://learn.microsoft.com/en-us/powershell/scripting/windows-powershell/starting-windows-powershell" rel="nofollow">powershell</a> (Windows Powershell)</li>
</ul>
<h2 tabindex="-1" id="user-content-contributing" dir="auto"><a href="#contributing">Contributing</a></h2>
<p dir="auto">This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com/" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<h2 tabindex="-1" id="user-content-trademarks" dir="auto"><a href="#trademarks">Trademarks</a></h2>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPTs: Custom versions of ChatGPT (302 pts)]]></title>
            <link>https://openai.com/blog/introducing-gpts</link>
            <guid>38166431</guid>
            <pubDate>Mon, 06 Nov 2023 18:18:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/blog/introducing-gpts">https://openai.com/blog/introducing-gpts</a>, See on <a href="https://news.ycombinator.com/item?id=38166431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><!--[--><!--[--><div><p>We’re rolling out custom versions of ChatGPT that you can create for a specific purpose—called GPTs. GPTs are a new way for anyone to create a tailored version of ChatGPT to be more helpful in their daily life, at specific tasks, at work, or at home—and then share that creation with others. For example, GPTs can help you <a href="https://openai.com/chatgpt#do-more-with-gpts" rel="noopener noreferrer">learn the rules to any board game, help teach your kids math, or design stickers</a>.</p><p>Anyone can easily build their own GPT—no coding is required. You can make them for yourself, just for your company’s internal use, or for everyone. Creating one is as easy as starting a conversation, giving it instructions and extra knowledge, and picking what it can do, like searching the web, making images or analyzing data.</p><p>Example GPTs are available today for ChatGPT Plus and Enterprise users to try out including <a href="https://chat.openai.com/g/g-alKfVrz9K-canva" rel="noopener noreferrer" target="_blank">Canva</a> and <a href="https://zapier.com/blog/gpt-assistant/" rel="noopener noreferrer" target="_blank">Zapier AI Actions</a>. We plan to offer GPTs to more users soon.</p><p><em><br>Learn more about our </em><a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday" rel="noopener noreferrer" target="_blank"><em>OpenAI DevDay announcements for new models and developer products</em></a><em>.</em><br></p></div><!--]--><!--[--><div id="gpts-let-you-customize-chatgpt-for-a-specific-purpose" data-heading=""><p><h3>GPTs let you customize ChatGPT for a specific purpose</h3></p></div><!--]--><!--[--><div><p>Since launching ChatGPT people have been asking for ways to customize ChatGPT to fit specific ways that they use it. We launched Custom Instructions in July that let you set some preferences, but requests for more control kept coming. Many power users maintain a list of carefully crafted prompts and instruction sets, manually copying them into ChatGPT. GPTs now do all of that for you.<br></p></div><!--]--><!--[--><!--]--><!--[--><div><p>We believe the most incredible GPTs will come from builders in the community. Whether you’re an educator, coach, or just someone who loves to build helpful tools, you don’t need to know coding to make one and share your expertise. <br></p></div><!--]--><!--[--><div id="the-gpt-store-is-rolling-out-later-this-month" data-heading=""><p><h3>The GPT Store is rolling out later this month</h3></p></div><!--]--><!--[--><div><p>Starting today, you can create GPTs and share them publicly. Later this month, we’re launching the GPT Store, featuring creations by verified builders. Once in the store, GPTs become searchable and may climb the leaderboards. We will also spotlight the most useful and delightful GPTs we come across in categories like productivity, education, and “just for fun”. In the coming months, you’ll also be able to earn money based on how many people are using your GPT.<br></p></div><!--]--><!--[--><div id="we-built-gpts-with-privacy-and-safety-in-mind" data-heading=""><p><h3>We built GPTs with privacy and safety in mind</h3></p></div><!--]--><!--[--><div><p>As always, you are in control of your data with ChatGPT. Your chats with GPTs are not shared with builders. If a GPT uses third party APIs, you choose whether data can be sent to that API. When builders customize their own GPT with actions or knowledge, the builder can choose if user chats with that GPT can be used to improve and train our models. These choices build upon the existing <a href="https://help.openai.com/en/articles/7730893-data-controls-faq" rel="noopener noreferrer" target="_blank">privacy controls</a> users have, including the option to opt your entire account out of model training.&nbsp;</p><p>We’ve set up new systems to help review GPTs against our <a href="https://openai.com/policies/usage-policies" rel="noopener noreferrer" target="_blank">usage policies</a>. These systems stack on top of our existing mitigations and aim to prevent users from sharing harmful GPTs, including those that involve fraudulent activity, hateful content, or adult themes. We’ve also taken steps to build user trust by allowing builders to verify their identity. We'll continue to monitor and learn how people use GPTs and update and strengthen our safety mitigations. If you have concerns with a specific GPT, you can also use our reporting feature on the GPT shared page to notify our team.</p><p>GPTs will continue to get more useful and smarter, and you’ll eventually be able to let them take on real tasks in the real world. In the field of AI, these systems are often discussed as “agents”. We think it’s important to move incrementally towards this future, as it will require careful technical and safety work—and time for society to adapt. We have been thinking deeply about the societal implications and will have more analysis to share soon.<br></p></div><!--]--><!--[--><div id="developers-can-connect-gpts-to-the-real-world" data-heading=""><p><h3>Developers can connect GPTs to the real world</h3></p></div><!--]--><!--[--><div><p>In addition to using our built-in capabilities, you can also define custom actions by making one or more APIs available to the GPT. Like plugins, actions allow GPTs to integrate external data or interact with the real-world. Connect GPTs to databases, plug them into emails, or make them your shopping assistant. For example, you could integrate a travel listings database, connect a user’s email inbox, or facilitate e-commerce orders.</p><p>The design of actions builds upon insights from our plugins beta, granting developers greater control over the model and how their APIs are called. Migrating from the plugins beta is easy with the ability to use your existing plugin manifest to define actions for your GPT.<br></p></div><!--]--><!--[--><div id="enterprise-customers-can-deploy-internal-only-gpts" data-heading=""><p><h3>Enterprise customers can deploy internal-only GPTs</h3></p></div><!--]--><!--[--><div><p>Since we launched ChatGPT Enterprise a few months ago, early customers have expressed the desire for even more customization that aligns with their business. GPTs answer this call by allowing you to create versions of ChatGPT for specific use cases, departments, or proprietary datasets. Early customers like Amgen, Bain, and Square are already leveraging internal GPTs to do things like craft marketing materials embodying their brand, aid support staff with answering customer questions, or help new software engineers with onboarding.</p><p>Enterprises can get started with GPTs on Wednesday. You can now empower users inside your company to design internal-only GPTs without code and securely publish them to your workspace. The admin console lets you choose how GPTs are shared and whether external GPTs may be used inside your business. Like all usage on ChatGPT Enterprise, we do not use your conversations with GPTs to improve our models.<br></p></div><!--]--><!--[--><div id="we-want-more-people-to-shape-how-ai-behaves" data-heading=""><p><h3>We want more people to shape how AI behaves</h3></p></div><!--]--><!--[--><div><p>We designed GPTs so more people can build with us. Involving the community is critical to our mission of building safe AGI that benefits humanity. It allows everyone to see a wide and varied range of useful GPTs and get a more concrete sense of what’s ahead. And by broadening the group of people who decide 'what to build' beyond just those with access to advanced technology it's likely we'll have safer and better aligned AI. The same desire to build with people, not just for them, drove us to launch the OpenAI API and to research methods for incorporating democratic input into AI behavior, which we plan to share more about soon.<br></p></div><!--]--><!--[--><div id="we-ve-made-chatgpt-plus-fresher-and-simpler-to-use" data-heading=""><p><h3>We’ve made ChatGPT Plus fresher and simpler to use</h3></p></div><!--]--><!--[--><div><p>Finally, ChatGPT Plus now includes fresh information up to April 2023. We’ve also heard your feedback about how the model picker is a pain. Starting today, no more hopping between models; everything you need is in one place. You can access DALL·E, browsing, and data analysis all without switching. You can also attach files to let ChatGPT search PDFs and other document types. Find us at <a href="https://chatgpt.com/" rel="noopener noreferrer" target="_blank">chatgpt.com</a>.<br></p></div><!--]--><!--[--><!--]--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New models and developer products (331 pts)]]></title>
            <link>https://openai.com/blog/new-models-and-developer-products-announced-at-devday</link>
            <guid>38166420</guid>
            <pubDate>Mon, 06 Nov 2023 18:17:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">https://openai.com/blog/new-models-and-developer-products-announced-at-devday</a>, See on <a href="https://news.ycombinator.com/item?id=38166420">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><!--[--><!--[--><div><p>Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:</p><ul><li>New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window</li><li>New Assistants API that makes it easier for developers to build their own assistive AI apps that have goals and can call models and tools</li><li>New multimodal capabilities in the platform, including vision, image creation (DALL·E 3), and text-to-speech (TTS)</li></ul><p>We’ll begin rolling out new features to OpenAI customers starting at 1pm PT today.</p><p><em><br>Learn more about </em><a href="https://openai.com/blog/introducing-gpts" rel="noopener noreferrer" target="_blank"><em>OpenAI DevDay announcements for ChatGPT</em></a><em>.</em><br></p></div><!--]--><!--[--><div id="gpt-4-turbo-with-128k-context" data-heading=""><p><h2>GPT-4 Turbo with 128K context</h2></p></div><!--]--><!--[--><div><p>We released the first version of GPT-4 in March and made GPT-4 generally available to all developers in July. Today we’re launching a preview of the next generation of this model, <a href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" rel="noopener noreferrer" target="_blank">GPT-4 Turbo</a>.&nbsp;</p><p>GPT-4 Turbo is more capable and has knowledge of world events up to April 2023. It has a 128k context window so it can fit the equivalent of more than 300 pages of text in a single prompt. We also optimized its performance so we are able to offer GPT-4 Turbo at a <a href="https://openai.com/pricing#gpt-4-turbo" rel="noopener noreferrer" target="_blank">3x cheaper</a> price for input tokens and a 2x cheaper price for output tokens compared to GPT-4.</p><p>GPT-4 Turbo is available for all paying developers to try by passing <code>gpt-4-1106-preview</code> in the API and we plan to release the stable production-ready model in the coming weeks.<br></p></div><!--]--><!--[--><div id="function-calling-updates" data-heading=""><p><h3>Function calling updates</h3></p></div><!--]--><!--[--><div><p><a href="https://platform.openai.com/docs/guides/function-calling" rel="noopener noreferrer" target="_blank">Function calling</a> lets you describe functions of your app or external APIs to models, and have the model intelligently choose to output a JSON object containing arguments to call those functions. We’re releasing several improvements today, including the ability to call multiple functions in a single message: users can send one message requesting multiple actions, such as “open the car window and turn off the A/C”,&nbsp;which would previously require multiple roundtrips with the model (<a href="https://platform.openai.com/docs/guides/function-calling/parallel-function-calling" rel="noopener noreferrer" target="_blank">learn more</a>). We are also improving function calling accuracy: GPT-4 Turbo is more likely to return the right function parameters.<br></p></div><!--]--><!--[--><div id="improved-instruction-following-and-json-mode" data-heading=""><p><h3>Improved instruction following and JSON mode</h3></p></div><!--]--><!--[--><div><p>GPT-4 Turbo performs better than our previous models on tasks that require the careful following of instructions, such as generating specific formats (e.g., “always respond in XML”). It also supports our new <a href="https://platform.openai.com/docs/guides/text-generation/json-mode" rel="noopener noreferrer" target="_blank">JSON mode</a>, which ensures the model will respond with valid JSON. The new API parameter <code>response_format</code> enables the model to constrain its output to generate a syntactically correct JSON object. JSON mode is useful for developers generating JSON in the Chat Completions API outside of function calling.<br></p></div><!--]--><!--[--><div id="reproducible-outputs-and-log-probabilities" data-heading=""><p><h3>Reproducible outputs and log probabilities</h3></p></div><!--]--><!--[--><div><p>The new <code>seed</code> parameter enables <strong>reproducible outputs</strong> by making the model return consistent completions most of the time. This beta feature is useful for use cases such as replaying requests for debugging, writing more comprehensive unit tests, and generally having a higher degree of control over the model behavior. We at OpenAI have been using this feature internally for our own unit tests and have found it invaluable. We’re excited to see how developers will use it. <a href="https://platform.openai.com/docs/guides/text-generation/reproducible-outputs" rel="noopener noreferrer" target="_blank">Learn more</a>.<br></p><p>We’re also launching a feature to return the <strong>log probabilities</strong> for the most likely output tokens generated by GPT-4 Turbo and GPT-3.5 Turbo in the next few weeks, which will be useful for building features such as autocomplete in a search experience.<br></p></div><!--]--><!--[--><div id="updated-gpt-3-5-turbo" data-heading=""><p><h3>Updated GPT-3.5 Turbo</h3></p></div><!--]--><!--[--><div><p>In addition to GPT-4 Turbo, we are also releasing a new version of GPT-3.5 Turbo that supports a 16K context window by default. The new 3.5 Turbo supports improved instruction following, JSON mode, and parallel function calling. For instance, our internal evals show a 38% improvement on format following tasks such as generating JSON, XML and YAML. Developers can access this new model by calling <code>gpt-3.5-turbo-1106</code> in the API. Applications using the <code>gpt-3.5-turbo</code> name will automatically be upgraded to the new model on December 11. Older models will continue to be accessible by passing <code>gpt-3.5-turbo-0613</code> in the API until June 13, 2024. <a href="https://platform.openai.com/docs/models/gpt-3-5" rel="noopener noreferrer" target="_blank">Learn more</a>.<br></p></div><!--]--><!--[--><div id="assistants-api-retrieval-and-code-interpreter" data-heading=""><p><h2>Assistants API, Retrieval, and Code Interpreter</h2></p></div><!--]--><!--[--><div><p>Today, we’re releasing the <a href="https://platform.openai.com/docs/assistants/overview" rel="noopener noreferrer" target="_blank">Assistants API</a>, our first step towards helping developers build agent-like experiences within their own applications. An assistant is a purpose-built AI that has specific instructions, leverages extra knowledge, and can call models and tools to perform tasks. The new Assistants API provides new capabilities such as Code Interpreter and Retrieval as well as function calling to handle a lot of the heavy lifting that you previously had to do yourself and enable you to build high-quality AI apps.</p><p>This API is designed for flexibility; use cases range from a natural language-based data analysis app, a coding assistant, an AI-powered vacation planner, a voice-controlled DJ, a smart visual canvas—the list goes on. The Assistants API is built on the same capabilities that enable <a href="http://openai.com/blog/introducing-gpts" rel="noopener noreferrer" target="_blank">our new GPTs product</a>: custom instructions and tools such as Code interpreter, Retrieval, and function calling.</p><p>A key change introduced by this API is <strong>persistent and infinitely long threads</strong>, which allow developers to hand off thread state management to OpenAI and work around context window constraints. With the Assistants API, you simply add each new message to an existing <code>thread</code>.</p><p>Assistants also have access to call new tools as needed, including:</p><ul><li><strong>Code Interpreter</strong>: writes and runs Python code in a sandboxed execution environment, and can generate graphs and charts, and process files with diverse data and formatting. It allows your assistants to run code iteratively to solve challenging code and math problems, and more.</li><li><strong>Retrieval</strong>: augments the assistant with knowledge from outside our models, such as proprietary domain data, product information or documents provided by your users. This means you don’t need to compute and store embeddings for your documents, or implement chunking and search algorithms. The Assistants API optimizes what retrieval technique to use based on our experience building knowledge retrieval in ChatGPT.</li><li><strong>Function calling</strong>: enables assistants to invoke functions you define and incorporate the function response in their messages.</li></ul><p>As with the rest of the platform, data and files passed to the OpenAI API are <a href="https://openai.com/enterprise-privacy" rel="noopener noreferrer" target="_blank">never used to train our models</a> and developers can delete the data when they see fit.</p><p>You can try the Assistants API beta without writing any code by heading to the <a href="https://platform.openai.com/playground?mode=assistant" rel="noopener noreferrer" target="_blank">Assistants playground</a>.<br></p></div><!--]--><!--[--><div><video autoplay="" loop="" muted="" playsinline="true" src="https://cdn.openai.com/new-models-and-developer-products-announced-at-devday/assistants-playground.mp4" poster="https://cdn.openai.com/new-models-and-developer-products-announced-at-devday/assistants-playground-poster.jpg"></video><p>Use the Assistants playground to create high quality assistants without code.</p></div><!--]--><!--[--><div><p>The Assistants API is in beta and available to all developers starting today. Please share what you build with us (<a href="https://twitter.com/OpenAI" rel="noopener noreferrer" target="_blank">@OpenAI</a>) along with your feedback which we will incorporate as we continue building over the coming weeks. Pricing for the Assistants APIs and its tools is available <a href="https://openai.com/pricing" rel="noopener noreferrer" target="_blank">on our pricing page</a>.<br></p></div><!--]--><!--[--><div id="new-modalities-in-the-api" data-heading=""><p><h2>New modalities in the API</h2></p></div><!--]--><!--[--><div id="gpt-4-turbo-with-vision" data-heading=""><p><h3>GPT-4 Turbo with vision</h3></p></div><!--]--><!--[--><div><p>GPT-4 Turbo can accept images as inputs in the Chat Completions API, enabling use cases such as generating captions, analyzing real world images in detail, and reading documents with figures. For example, BeMyEyes uses this technology to help people who are blind or have low vision with daily tasks like identifying a product or navigating a store. Developers can access this feature by using <code>gpt-4-vision-preview</code> in the API. We plan to roll out vision support to the main GPT-4 Turbo model as part of its stable release. <a href="https://openai.com/pricing" rel="noopener noreferrer" target="_blank">Pricing</a> depends on the input image size. For instance, passing an image with 1080×1080 pixels to GPT-4 Turbo costs $0.00765. Check out <a href="https://platform.openai.com/docs/guides/vision" rel="noopener noreferrer" target="_blank">our vision guide</a>.<br></p></div><!--]--><!--[--><div id="dall-e-3" data-heading=""><p><h3>DALL·E 3</h3></p></div><!--]--><!--[--><div><p>Developers can integrate DALL·E 3, which we <a href="https://openai.com/blog/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise" rel="noopener noreferrer" target="_blank">recently launched</a> to ChatGPT Plus and Enterprise users, directly into their apps and products through our Images API by specifying <code>dall-e-3</code> as the model. Companies like Snap, Coca-Cola, and Shutterstock have used DALL·E 3 to programmatically generate images and designs for their customers and campaigns. Similar to the previous version of DALL·E, the API incorporates built-in moderation to help developers protect their applications against misuse. We offer different format and quality options, with prices starting at $0.04 per image generated. Check out our <a href="https://platform.openai.com/docs/guides/images" rel="noopener noreferrer" target="_blank">guide to getting started</a> with DALL·E 3 in the API.<br></p></div><!--]--><!--[--><div id="text-to-speech-tts" data-heading=""><p><h3>Text-to-speech (TTS)</h3></p></div><!--]--><!--[--><div><p>Developers can now generate human-quality speech from text via the text-to-speech API. Our new TTS model offers six preset voices to choose from and two model variants, <code>tts-1</code> and <code>tts-1-hd</code>. <code>tts</code> is optimized for real-time use cases and <code>tts-1-hd</code> is optimized for quality. Pricing starts at $0.015 per input 1,000 characters. Check out our <a href="https://platform.openai.com/docs/guides/text-to-speech" rel="noopener noreferrer" target="_blank">TTS guide</a> to get started.<br></p></div><!--]--><!--[--><div><h4>Listen to voice samples</h4><p><label><span>Select text</span></label></p><blockquote><p><span>As the golden sun dips below the horizon, casting long shadows across the tranquil meadow, the world seems to hush, and a sense of calmness envelops the Earth, promising a peaceful night’s rest for all living beings.</span></p></blockquote><p><label><span>Select voice</span></label><audio src="https://cdn.openai.com/new-models-and-developer-products-announced-at-devday/audio/scenic-alloy.mp3" controls=""></audio></p></div><!--]--><!--[--><div id="model-customization" data-heading=""><p><h2>Model customization</h2></p></div><!--]--><!--[--><div id="gpt-4-fine-tuning-experimental-access" data-heading=""><p><h3>GPT-4 fine tuning experimental access</h3></p></div><!--]--><!--[--><div><p>We’re creating an experimental access program for <strong>GPT-4 fine-tuning</strong>. Preliminary results indicate that GPT-4 fine-tuning requires more work to achieve meaningful improvements over the base model compared to the substantial gains realized with GPT-3.5 fine-tuning. As quality and safety for GPT-4 fine-tuning improves, developers actively using GPT-3.5 fine-tuning will be presented with an option to apply to the GPT-4 program within their <a href="https://platform.openai.com/finetune" rel="noopener noreferrer" target="_blank">fine-tuning console</a>.<br></p></div><!--]--><!--[--><div id="custom-models" data-heading=""><p><h3>Custom models</h3></p></div><!--]--><!--[--><div><p>For organizations that need even more customization than fine-tuning can provide (particularly applicable to domains with extremely large proprietary datasets—billions of tokens at minimum), we’re also launching a <strong>Custom Models program</strong>, giving selected organizations an opportunity to work with a dedicated group of OpenAI researchers to train custom GPT-4 to their specific domain. This includes modifying every step of the model training process, from doing additional domain specific pre-training, to running a custom RL post-training process tailored for the specific domain. Organizations will have exclusive access to their custom models. In keeping with our existing enterprise privacy policies, custom models will not be served to or shared with other customers or used to train other models.&nbsp;Also, proprietary data provided to OpenAI to train custom models will not be reused in any other context. This will be a very limited (and expensive) program to start—interested orgs can <a href="https://openai.com/form/custom-models" rel="noopener noreferrer" target="_blank">apply here</a>.<br></p></div><!--]--><!--[--><div id="lower-prices-and-higher-rate-limits" data-heading=""><p><h2>Lower prices and higher rate limits</h2></p></div><!--]--><!--[--><div id="lower-prices" data-heading=""><p><h3>Lower prices</h3></p></div><!--]--><!--[--><div><p>We’re <a href="https://openai.com/pricing" rel="noopener noreferrer" target="_blank">decreasing several prices</a> across the platform to pass on savings to developers (all prices below are expressed per 1,000 tokens):</p><ul><li>GPT-4 Turbo input tokens are 3x cheaper than GPT-4 at $0.01 and output tokens are 2x cheaper at $0.03.</li><li>GPT-3.5 Turbo input tokens are 3x cheaper than the previous 16K model at $0.001 and output tokens are 2x cheaper at $0.002. Developers previously using GPT-3.5 Turbo 4K benefit from a 33% reduction on input tokens at $0.001. Those lower prices only apply to the new GPT-3.5 Turbo introduced today.</li><li>Fine-tuned GPT-3.5 Turbo 4K model input tokens are reduced by 4x at $0.003 and output tokens are 2.7x cheaper at $0.006. Fine-tuning also supports 16K context at the same price as 4K with the new GPT-3.5 Turbo model. These new prices also apply to fine-tuned <code>gpt-3.5-turbo-0613</code> models.<br></li></ul></div><!--]--><!--[--><div><table><tbody><!--[--><!--[--><tr><!--[--><td><!----><!----></td><td><span>Older models</span><!----></td><td><span>New models</span><!----></td><!--]--></tr><!--]--><!--[--><tr><!--[--><td><span>GPT-4 Turbo</span><!----></td><td><span>GPT-4 8K
<span>Input: $0.03
Output: $0.06</span>

GPT-4 32K
<span>Input: $0.06
Output: $0.012</span></span><!----></td><td><span>GPT-4 Turbo 128K
<span>Input: $0.01
Output: $0.03</span></span><!----></td><!--]--></tr><!--]--><!--[--><tr><!--[--><td><span>GPT-3.5 Turbo</span><!----></td><td><span>GPT-3.5 Turbo 4K
<span>Input: $0.0015
Output: $0.002</span>

GPT-3.5 Turbo 16K
<span>Input: $0.003
Output: $0.004</span></span><!----></td><td><span>GPT-3.5 Turbo 16K
<span>Input: $0.001
Output: $0.002</span></span><!----></td><!--]--></tr><!--]--><!--[--><tr><!--[--><td><span>GPT-3.5 Turbo fine-tuning</span><!----></td><td><span>GPT-3.5 Turbo 4K fine-tuning
<span>Training: $0.008
Input: $0.012
Output: $0.016</span></span><!----></td><td><span>GPT-3.5 Turbo 4K and 16K fine-tuning
<span>Training: $0.008
Input: $0.003
Output: $0.006</span></span><!----></td><!--]--></tr><!--]--><!--]--></tbody></table></div><!--]--><!--[--><div id="higher-rate-limits" data-heading=""><p><h3>Higher rate limits</h3></p></div><!--]--><!--[--><div><p>To help you scale your applications, we’re doubling the tokens per minute limit for all our paying GPT-4 customers. You can view your new rate limits in your <a href="https://platform.openai.com/account/rate-limits" rel="noopener noreferrer" target="_blank">rate limit page</a>. We’ve also published our <a href="https://platform.openai.com/docs/guides/rate-limits/usage-tiers" rel="noopener noreferrer" target="_blank">usage tiers</a> that determine automatic rate limits increases, so you know what to expect in how your usage limits will automatically scale. You can now request increases to usage limits from your <a href="https://platform.openai.com/account/rate-limits" rel="noopener noreferrer" target="_blank">account settings</a>.<br></p></div><!--]--><!--[--><div id="copyright-shield" data-heading=""><p><h2>Copyright Shield</h2></p></div><!--]--><!--[--><div><p>OpenAI is committed to protecting our customers with built-in copyright safeguards in our systems. Today, we’re going one step further and introducing Copyright Shield—we will now step in and defend our customers, and pay the costs incurred, if you face legal claims around copyright infringement. This applies to generally available features of ChatGPT Enterprise and our developer platform.<br></p></div><!--]--><!--[--><div id="whisper-v3-and-consistency-decoder" data-heading=""><p><h2>Whisper v3 and Consistency Decoder</h2></p></div><!--]--><!--[--><div><p>We are releasing <a href="https://github.com/openai/whisper" rel="noopener noreferrer" target="_blank">Whisper large-v3,</a> the next version of our open source automatic speech recognition model (ASR) which features improved performance across languages. We also plan to support Whisper v3 in our API in the near future.</p><p>We are also open sourcing the <a href="https://github.com/openai/consistencydecoder" rel="noopener noreferrer" target="_blank">Consistency Decoder</a>, a drop in replacement for the Stable Diffusion VAE decoder. This decoder improves all images compatible with the by Stable Diffusion 1.0+ VAE, with significant improvements in text, faces and straight lines.<br></p></div><!--]--><!--[--><!--]--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI DevDay, Opening Keynote Livestream [video] (129 pts)]]></title>
            <link>https://www.youtube.com/watch?v=U9mJuUkhUzk</link>
            <guid>38165090</guid>
            <pubDate>Mon, 06 Nov 2023 16:55:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=U9mJuUkhUzk">https://www.youtube.com/watch?v=U9mJuUkhUzk</a>, See on <a href="https://news.ycombinator.com/item?id=38165090">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[XAI PromptIDE (130 pts)]]></title>
            <link>https://x.ai/prompt-ide/</link>
            <guid>38164886</guid>
            <pubDate>Mon, 06 Nov 2023 16:42:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x.ai/prompt-ide/">https://x.ai/prompt-ide/</a>, See on <a href="https://news.ycombinator.com/item?id=38164886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Integrated development environment for prompt engineering and interpretability research</p></div><div class="page"><p><small>November 6, 2023</small></p><p><strong>The xAI PromptIDE is an integrated development environment for prompt engineering and interpretability research. It accelerates prompt engineering through an SDK that allows implementing complex prompting techniques and rich analytics that visualize the network's outputs. We use it heavily in our continuous development of <a href="https://x.ai/">Grok</a>.</strong></p><p>We developed the PromptIDE to give transparent access to Grok-1, the model that powers <a href="https://x.ai/">Grok</a>, to engineers and researchers in the community. The IDE is designed to empower users and help them explore the capabilities of our large language models (LLMs) at pace. At the heart of the IDE is a Python code editor that - combined with a new <a href="https://x.ai/ide/docs.html">SDK</a> - allows implementing complex prompting techniques. While executing prompts in the IDE, users see helpful analytics such as the precise tokenization, sampling probabilities, alternative tokens, and aggregated attention masks.</p><p>The IDE also offers quality of life features. It automatically saves all prompts and has built-in versioning. The analytics generated by running a prompt can be stored permanently allowing users to compare the outputs of different prompting techniques. Finally, users can upload small files such as CSV files and read them using a single Python function from the SDK. When combined with the SDK's concurrency features, even somewhat large files can be processed quickly.</p><p>We also hope to build a community around the PromptIDE. Any prompt can be shared publicly at the click of a button. Users can decide if they only want to share a single version of the prompt or the entire tree. It's also possible to include any stored analytics when sharing a prompt.</p><p>The PromptIDE is available to members of our early <a href="https://ide.x.ai/">access program</a>. Below, you find a walkthrough of the main features of the IDE.</p><p>Thank you,<br> the xAI Team</p><h2 id="code-editor-sdk">Code editor &amp; SDK</h2><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_42.png"></p><p>At the heart of the PromptIDE is a code editor and a <a href="https://x.ai/ide/docs.html">Python SDK</a>. The SDK provides a new programming paradigm that allows implementing complex prompting techniques elegantly. All Python functions are executed in an implicit context, which is a sequence of tokens. You can manually add tokens to the context using the <code>prompt()</code> function or you can use our models to generate tokens based on the context using the <code>sample()</code> function. When sampling from the model, you have various configuration options that are passed as argument to the function:</p><pre data-lang="python"><code data-lang="python"><span>async def </span><span>sample</span><span>(
</span><span>    </span><span>self</span><span>,
</span><span>    </span><span>max_len</span><span>: int = </span><span>256</span><span>,
</span><span>    </span><span>temperature</span><span>: float = </span><span>1.0</span><span>,
</span><span>    </span><span>nucleus_p</span><span>: float = </span><span>0.7</span><span>,
</span><span>    </span><span>stop_tokens</span><span>: Optional[list[str]] = </span><span>None</span><span>,
</span><span>    </span><span>stop_strings</span><span>: Optional[list[str]] = </span><span>None</span><span>,
</span><span>    </span><span>rng_seed</span><span>: Optional[int] = </span><span>None</span><span>,
</span><span>    </span><span>add_to_context</span><span>: bool = </span><span>True</span><span>,
</span><span>    </span><span>return_attention</span><span>: bool = </span><span>False</span><span>,
</span><span>    </span><span>allowed_tokens</span><span>: Optional[Sequence[Union[int, str]]] = </span><span>None</span><span>,
</span><span>    </span><span>disallowed_tokens</span><span>: Optional[Sequence[Union[int, str]]] = </span><span>None</span><span>,
</span><span>    </span><span>augment_tokens</span><span>: bool = </span><span>True</span><span>,
</span><span>) -&gt; SampleResult:
</span><span>    </span><span>"""Generates a model response based on the current prompt.
</span><span>
</span><span>    The current prompt consists of all text that has been added to the prompt either since the
</span><span>    beginning of the program or since the last call to `clear_prompt`.
</span><span>
</span><span>    Args:
</span><span>        max_len: Maximum number of tokens to generate.
</span><span>        temperature: Temperature of the final softmax operation. The lower the temperature, the
</span><span>            lower the variance of the token distribution. In the limit, the distribution collapses
</span><span>            onto the single token with the highest probability.
</span><span>        nucleus_p: Threshold of the Top-P sampling technique: We rank all tokens by their
</span><span>            probability and then only actually sample from the set of tokens that ranks in the
</span><span>            Top-P percentile of the distribution.
</span><span>        stop_tokens: A list of strings, each of which will be mapped independently to a single
</span><span>            token. If a string does not map cleanly to one token, it will be silently ignored.
</span><span>            If the network samples one of these tokens, sampling is stopped and the stop token
</span><span>            *is not* included in the response.
</span><span>        stop_strings: A list of strings. If any of these strings occurs in the network output,
</span><span>            sampling is stopped but the string that triggered the stop *will be* included in the
</span><span>            response. Note that the response may be longer than the stop string. For example, if
</span><span>            the stop string is "Hel" and the network predicts the single-token response "Hello",
</span><span>            sampling will be stopped but the response will still read "Hello".
</span><span>        rng_seed: See of the random number generator used to sample from the model outputs.
</span><span>        add_to_context: If true, the generated tokens will be added to the context.
</span><span>        return_attention: If true, returns the attention mask. Note that this can significantly
</span><span>            increase the response size for long sequences.
</span><span>        allowed_tokens: If set, only these tokens can be sampled. Invalid input tokens are
</span><span>            ignored. Only one of `allowed_tokens` and `disallowed_tokens` must be set.
</span><span>        disallowed_tokens: If set, these tokens cannot be sampled. Invalid input tokens are
</span><span>            ignored. Only one of `allowed_tokens` and `disallowed_tokens` must be set.
</span><span>        augment_tokens: If true, strings passed to `stop_tokens`, `allowed_tokens` and
</span><span>            `disallowed_tokens` will be augmented to include both the passed token and the
</span><span>            version with leading whitespace. This is useful because most words have two
</span><span>            corresponding vocabulary entries: one with leading whitespace and one without.
</span><span>
</span><span>    Returns:
</span><span>        The generated text.
</span><span>    """
</span></code></pre><p>The code is executed locally using an in-browser Python interpreter that runs in a separate web worker. Multiple web workers can run at the same time, which means you can execute many prompts in parallel.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_42_completion.png"></p><p>Complex prompting techniques can be implemented using multiple contexts within the same program. If a function is annotated with the <code>@prompt_fn</code> decorator, it is executed in its own, fresh context. The function can perform some operations independently of its parent context and pass the results back to the caller using the <code>return</code> statement. This programming paradigm enables recursive and iterative prompts with arbitrarily nested sub-contexts.</p><h2 id="concurrency">Concurrency</h2><p>The SDK uses Python coroutines that enable processing multiple <code>@prompt_fn</code>-annotated Python functions concurrently. This can significantly speed up the time to completion - especially when working with CSV files.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_concurrency.png"></p><h2 id="user-inputs">User inputs</h2><p>Prompts can be made interactive through the <code>user_input()</code> function, which blocks execution until the user has entered a string into a textbox in the UI. The <code>user_input()</code> function returns the string entered by the user, which cen then, for example, be added to the context via the <code>prompt()</code> function. Using these APIs, a chatbot can be implemented in just four lines of code:</p><pre data-lang="python"><code data-lang="python"><span>await </span><span>prompt</span><span>(</span><span>PREAMBLE</span><span>)
</span><span>while </span><span>text := </span><span>await </span><span>user_input</span><span>("</span><span>Write a message</span><span>"):
</span><span>    </span><span>await </span><span>prompt</span><span>(</span><span>f</span><span>"</span><span>&lt;|separator|&gt;</span><span>\n\n</span><span>Human: </span><span>{text}</span><span>&lt;|separator|&gt;</span><span>\n\n</span><span>Assistant:</span><span>")
</span><span>    </span><span>await </span><span>sample</span><span>(</span><span>max_len</span><span>=</span><span>1024</span><span>, </span><span>stop_tokens</span><span>=["</span><span>&lt;|separator|&gt;</span><span>"], </span><span>return_attention</span><span>=</span><span>True</span><span>)
</span></code></pre><h2 id="files">Files</h2><p>Developers can upload small files to the PromptIDE (up to 5 MiB per file. At most 50 MiB total) and use their uploaded files in the prompt. The <code>read_file()</code> function returns any uploaded file as a byte array. When combined with the concurrency feature mentioned above, this can be used to implement batch processing prompts to evaluate a prompting technique on a variety of problems. The screenshot below shows a prompt that calculates the MMLU evaluation score.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_mmlu2.png"></p><h2 id="analytics">Analytics</h2><p>While executing a prompt, users see detailed per-token analytics to help them better understand the model's output. The completion window shows the precise tokenization of the context alongside the numeric identifiers of each token. When clicking on a token, users also see the top-K tokens after applying top-P thresholding and the aggregated attention mask at the token.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_42_completion.png"></p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_42_token.png"></p><p>When using the <code>user_input()</code> function, a textbox shows up in the window while the prompt is running that users can enter their response into. The below screenshot shows the result of executing the chatbot code snippet listed above.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_chat_completion.png"></p><p>Finally, the context can also be rendered in markdown to improve legibility when the token visualization features are not required.</p><p><img alt="Sampling probabilities in the PromptIDE" src="https://x.ai/screenshot_completion_markdown.png"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple developer boycott of Feedback Assistant (254 pts)]]></title>
            <link>https://lapcatsoftware.com/articles/2023/11/2.html</link>
            <guid>38164735</guid>
            <pubDate>Mon, 06 Nov 2023 16:30:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lapcatsoftware.com/articles/2023/11/2.html">https://lapcatsoftware.com/articles/2023/11/2.html</a>, See on <a href="https://news.ycombinator.com/item?id=38164735">Hacker News</a></p>
<div id="readability-page-1" class="page">
<nav>
Previous: <a href="https://lapcatsoftware.com/articles/2023/11/1.html">This Feedback will no longer be monitored, and incoming messages will not be reviewed</a>
<br><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a></nav>
<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>

<h3>November 6 2023</h3>

<p>I'm organizing a boycott of <a href="https://feedbackassistant.apple.com/">Apple's Feedback Assistant</a>, starting immediately, and I encourage all Apple developers to join me. Here's how I propose that each of us can effectively participate in the boycott and let Apple know that we're boycotting Feedback Assistant:</p>
<ol>
<li>File a new Feedback about Feedback Assistant (in Developer Tools &amp; Resources) that lists the issues below and states that you're boycotting Feedback Assistant until the issues are addressed.</li>
<li>Don't file any other new Feedbacks unless and until Apple addresses the issues.</li>
<li>If Apple requests a response to a previously filed Feedback, respond only by saying that you're boycotting Feedback Assistant, and refer to your Feedback number from step 1.</li>
</ol>
<p>Ideally, I think you should make your Feedback from step 1 as unique as possible. The point is to flood Apple with new Feedbacks about the boycott and force Apple to do some work to handle them, to take notice of the boycott, and to recognize that we're serious about it.</p>
<p>Boycotting Feedback Assistant does not preclude talking about your bugs on social media, on your blogs, and on your podcasts. Nor does it preclude filing reports with Apple's other public bug reporting systems, such as those for <a href="https://bugs.webkit.org/">WebKit</a> and various open source projects on <a href="https://github.com/apple">GitHub</a>. Those other bug reporting systems are superior to Feedback Assistant in a number of ways. The primary goal of the boycott is to bring about changes specifically in Feedback Assistant, the most <em>hostile</em> bug reporter I've ever seen.</p>
<p>After consulting with fellow developers, I've composed a list of issues with Feedback Assistant that Apple needs to address in order to end the boycott. I'll number the issues for ease of reference, but the order doesn't necessarily reflect their relative importance.</p>
<ol>
<li>Apple neglects or refuses to say whether or not they can reproduce reported bugs, even when we give them precise steps to reproduce and sample Xcode projects. This is crucial for us to determine whether Apple is taking our Feedbacks seriously or just lazily, bureaucratically stringing us along.</li>
<li>Apple closes Feedbacks with the status "Investigation complete - Unable to diagnose with current information" without asking us for more information or even notifying us that the Feedback has been closed.</li>
<li>Apple closes Feedbacks without the agreement of the person who filed the Feedback, and apparently it's now a "feature" of their bug reporting system that closed Feedbacks cannot be reopened, even by Apple employees. (It wasn't always this way, I believe.)</li>
<li>When Apple mistakenly closes a Feedback for a bug that isn't fixed, Apple demands that we open a new Feedback for the same bug, instead of just opening a new one themselves and giving us the new Feedback number.</li>
<li>Apple demands that developers "verify" Feedbacks with the latest betas despite the fact that Apple has not fixed the bugs, attempted to fix the bugs, or even attempted to reproduce the bugs with the steps given by us. This is a giant waste of our time. And Apple closes the Feedbacks if we don't "verify" them.</li>
<li>Apple doesn't always notify us of changes to the status of the original Feedback when our Feedbacks are closed as duplicates.</li>
<li>Apple constantly demands invasive sysdiagnose reports, often unnecessarily, and refuses to look at Feedbacks without them. Many developers work on their own personal devices, and sysdiagnoses are gross violations of our privacy, which Apple claims is a fundamental human right. Apple has avoided or abandoned creating smaller, more targeted and less intrusive methods of collecting information and diagnosing bugs.</li>
<li>Feedbacks can no longer be filed from the web. Apple now requires that all Feedbacks be filed from the native Feedback Assistant app on macOS or iOS. This is a very recent setback: I've been filing Feedbacks via the web app for years, the last one on October 26. Note the passive-aggressive question "Where would you like to start your feedback?" and the "recommendation" to use the native app, as if there were a choice.<br>
<img src="https://lapcatsoftware.com/articles/2023/11/2.png" width="414" height="300"></li>
<li>We can't search Feedback Assistant for bugs. Apple employees can search the database, but I can see only the Feedbacks that I personally filed. Of course we acknowledge that some Feedbacks need to remain secret, especially for products that haven't yet been announced by Apple, but countless Feedbacks require no such protection, and an opt-in searchable bug database would help external developers immensely, improving the overall quality of the software on Apple's platforms, to the benefit of Apple, developers, and users alike.</li>
</ol>
<p>Below is a screenshot of one of my old reports that epitomizes the absurdity of Apple's bug reporting system. Apple claimed in their response that "much has changed", but to this day, nothing has really changed. I've seen no evidence that Apple sincerely appreciates our input. Apple's Feedback Assistant, formerly known as Radar, has remained unreasonably terrible for a very long time, much too long, so now we're demanding change.</p>
<p><img src="https://lapcatsoftware.com/articles/images/Safari-extension-issues.jpeg" width="638" height="421" alt="Safari-extension-issues"></p>
<p>In defense of Apple, some people assert that Apple doesn't have the time to properly respond to Feedbacks. I don't find this argument convincing, because Apple's priorities, schedules, and staffing are determined by Apple itself, via the decisions of the company's leadership. Apple values its own time over the time of external developers and seems to have no guilt over wasting endless amounts of our time. We are not happy, though, to sacrifice ourselves for a corporation worth trillions of dollars. Needless to say, my net worth and income are microscopic in comparison. If Apple can decide that it doesn't have the time to respond to our Feedbacks, then we can decide that we don't have the time to file them; Apple's problems with lack of time are thereby solved. Frankly, as a longtime Apple user, I could do without the relentless annual OS updates, and many of us look back fondly to the era of Mac OS X Snow Leopard when the updates were around two years apart, leaving more time for bug fixes.</p>
<p>This is not a boycott against individual Apple engineers, many of whom also want Feedback Assistant to be improved. Indeed, the improvement of Feedback Assistant would enhance rather than detract from the relationship between Apple engineers and external developers. This is a boycott against the bug reporting <em>system</em>, intended to force Apple leadership to recognize and respond to the persistent problems with the system.</p>
<p>Although I call it a boycott, it could also be termed a labor strike. Apple utilizes developers for vast amounts of unpaid QA labor. Both Apple and developers know the crucial role that developers play in testing and refining Apple's software and products. Apple <em>needs</em> our bug reports, our labor, often hours or even days of labor for a single Feedback. Nonetheless, Apple acts as if it were entitled to our Feedbacks, treating developers kind of like indentured servants. No respect or basic human courtesy is afforded by Apple to developers in the bug reporting system. We've been indoctrinated into believing that it's simply our <em>duty</em> to file Feedbacks, for the sake of the platforms. However, Apple's platforms are not charity cases. To the contrary, they've made Apple the most profitable  company in the world. We developers are not Apple employees, and our unpaid labor should not be taken for granted. Henceforth, it will not be taken for granted.</p>
<p>In my view the boycott, or strike, has two goals. First, obviously, is to pressure Apple into improving Feedback Assistant by showing Apple that it needs us to file bug reports and would suffer without them. The second goal is to show ourselves that we don't actually need to file bug reports with Apple. My feeling is that Apple has a lot more to lose here than we do. After all, the majority of bugs that I file never get fixed anyway, and even the fixes usually come later rather than sooner, not in time to avoid the consequences of the bug. Do Apple bugs affect our apps? Yes, of course. But we typically have to ship workarounds for the bugs in our apps, because we can't count on Apple to fix our reported bugs in a timely manner. With a workaround for a bug in place, we no longer <em>need</em> Apple to fix the bug, so reporting the bug becomes more an act of charity than urgency.</p>
<p>This situation is often misunderstood by the public and even by Apple employees. Feedback Assistant does not provide customer service to developers. We developers are the ones who are providing the service to Feedback Assistant, and now we're choosing to withhold our services until the system is improved. I hope that Apple chooses to address the problems with Feedback Assistant, but if Apple happens to choose otherwise, and improvements never arrive, then my intention is to boycott forever. Regardless of whether Apple responds positively, I will consider the boycott to be a success if many developers participate, and we show ourselves that Feedback Assistant is not essential to our work and our livelihood.</p>

<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>
<nav><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a><br>
Previous: <a href="https://lapcatsoftware.com/articles/2023/11/1.html">This Feedback will no longer be monitored, and incoming messages will not be reviewed</a>
</nav>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckDB performance improvements with the latest release (173 pts)]]></title>
            <link>https://duckdb.org/2023/11/03/db-benchmark-update.html</link>
            <guid>38164189</guid>
            <pubDate>Mon, 06 Nov 2023 15:51:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duckdb.org/2023/11/03/db-benchmark-update.html">https://duckdb.org/2023/11/03/db-benchmark-update.html</a>, See on <a href="https://news.ycombinator.com/item?id=38164189">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<p><span>2023-11-03</span><span>Tom Ebergen</span></p>
							
							<p><em>TL;DR: the H2O.ai db-benchmark has been updated with new results. In addition, the AWS EC2 instance used for benchmarking has been changed to a c6id.metal for improved repeatability and fairness across libraries. DuckDB is the fastest library for both join and group by queries at almost every data size.</em></p>

<p><a href="#results">Skip directly to the results</a></p>
      <h2 id="the-benchmark-has-been-updated">
        
        <a href="#the-benchmark-has-been-updated">The Benchmark Has Been Updated!</a>
        
      </h2>
    

<p>In April, DuckDB Labs published a <a href="https://duckdb.org/2023/04/14/h2oai.html">blog post reporting updated H2O.ai db-benchmark results</a>. Since then, the results haven’t been updated. The original plan was to update the results with every DuckDB release. DuckDB 0.9.1 was recently released, and DuckDB Labs has updated the benchmark. While updating the benchmark, however, we noticed that our initial setup did not lend itself to being fair to all solutions. The machine used had network storage and could suffer from noisy neighbors. To avoid these issues, the whole benchmark was re-run on a c6id.metal machine.</p>
      
    

<p>Initially, updating the results to the benchmark showed strange results. Even using the same library versions from the prior update, some solutions regressed and others improved. We believe this variance came from the AWS EC2 instance we chose: an m4.10xlarge. The m4.10xlarge has 40 virtual CPUs and EBS storage. EBS storage is highly available network block storage for EC2 instances. When running compute-heavy benchmarks, a machine like the m4.10xlarge can suffer from the following issues:</p>

<ul>
  <li>
    <p><strong>Network storage</strong> is an issue for benchmarking solutions that interact with storage frequently. For the 500MB and 5GB workloads, network storage was not an issue on the m4.10xlarge since all solutions could execute the queries in memory. For the 50GB workload, however, network storage was an issue for the solutions that could not execute queries in memory. While the m4.10xlarge has dedicated EBS bandwidth, any read/write from storage is still happening over the network, which is usually slower than physically mounted storage. Solutions that frequently read and write to storage for the 50GB queries end up doing this over the network. This network time becomes a chunk of the execution time of the query. If the network has variable performance, the query performance is then also variable.</p>
  </li>
  <li>
    <p><strong>Noisy neighbors</strong> is a common issue when benchmarking on virtual CPUs. The previous machine most likely shared its compute hardware with other (neighboring) AWS EC2 instances. If these neighbors are also running compute heavy workloads, the physical CPU caches are repeatedly invalidated/flushed by the neighboring instance and the benchmark instance. When the CPU cache is shared between two workloads on two instances, both workloads require extra reads from memory for data that would already be in CPU cache on a non-virtual machine.</p>
  </li>
</ul>

<p>In order to be fair to all solutions, we decided to change the instance type to a metal instance with local storage. Metal instance types negate any noisy neighbor problems because the hardware is physical and not shared with any other AWS users/instances. Network storage problems are also fixed because solutions can read and write data to the local instance storage, which is physically mounted on the hardware.</p>

<p>Another benefit of the the c6id.metal box is that it stresses parallel performance. There are 128 cores on the c6id.metal. Performance differences between solutions that can effectively use every core and solutions that cannot are clearly visible.</p>

<p>See the <a href="#updated-settings">updated settings</a> section on how settings were change for each solution when run on the new machine.</p>
      <h2 id="updating-the-benchmark">
        
        <a href="#updating-the-benchmark">Updating the Benchmark</a>
        
      </h2>
    

<p>Moving forward we will update the benchmark when PRs with new performance numbers are provided. The PR should include a description of the changes to a solution script or a version update and new entries in the <code>time.csv</code> and <code>logs.csv</code> files. These entries will be verified using a different c6id.metal instance, and if there is limited variance, the PR will be merged and the results will be updated!</p>
      <h3 id="updated-settings">
        
        <a href="#updated-settings">Updated Settings</a>
        
      </h3>
    

<ol>
  <li>ClickHouse
    <ul>
      <li>Storage: Any data this gets spilled to disk also needs to be on the NVMe drive. This has been changed in the new <code>format_and_mount.sh</code> script and the <code>clickhouse/clickhouse-mount-config.xml</code> file.</li>
    </ul>
  </li>
  <li>Julia (juliadf &amp; juliads)
    <ul>
      <li>Threads: The threads were hardcoded for juliadf/juliads to 20/40 threads. Now the max number of threads are used. No option was given to spill to disk, so this was not changed/researched.</li>
    </ul>
  </li>
  <li>DuckDB
    <ul>
      <li>Storage: The DuckDB database file was specified to run on the NVMe mount.</li>
    </ul>
  </li>
  <li>Spark
    <ul>
      <li>Storage: There is an option to spill to disk. I was unsure of how to modify the storage location so that it was on the NVMe drive. Open to a PR with storage location changes and improved results!</li>
    </ul>
  </li>
</ol>

<p>Many solutions do not spill to disk, so they did not require any modification to use the instance storage. Other solutions use <code>parallel::ncores()</code> or default to a maximum number of cores for parallelism. Solution scripts were run in their current form on <a href="https://github.com/duckdblabs/db-benchmark">github.com/duckdblabs/db-benchmark</a>. Please read the <a href="https://github.com/duckdblabs/db-benchmark#updating-the-benchmark">Updating the Benchmark</a> section on how to re-run your solution.</p>
      <h3 id="results">
        
        <a href="#results">Results</a>
        
      </h3>
    

<p>The first results you see are the 50GB group by results. The benchmark runs every query twice per solution, and both runtimes are reported. The “first time” can be considered a cold run, and the “second time” can be considered a hot run. DuckDB and DuckDB-latest perform very well among all dataset sizes and variations.</p>

<p>The team at DuckDB Labs has been hard at work improving the performance of the out-of-core hash aggregates and joins. The most notable improvement is the performance of query 5 in the advanced group by queries. The cold run is almost an order of magnitude better than every other solution! DuckDB is also one of only two solutions to finish the 50GB join query. Some solutions are experiencing timeouts on the 50GB datasets. Solutions running the 50GB group by queries are killed after running for 180 minutes, meaning all 10 group by queries need to finish within the 180 minutes. Solutions running the 50GB join queries are killed after running for 360 minutes.</p>

<p><a href="https://duckdblabs.github.io/db-benchmark/">Link to result page</a></p>


							<p><a href="https://duckdb.org/news/">back to news archive <span></span></a>
						</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Create a shortcut for even lower phone brightness (154 pts)]]></title>
            <link>https://practicalbetterments.com/create-a-shortcut-for-even-lower-phone-brightness/</link>
            <guid>38164127</guid>
            <pubDate>Mon, 06 Nov 2023 15:47:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practicalbetterments.com/create-a-shortcut-for-even-lower-phone-brightness/">https://practicalbetterments.com/create-a-shortcut-for-even-lower-phone-brightness/</a>, See on <a href="https://news.ycombinator.com/item?id=38164127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><nav></nav><p>This practical betterment presupposes that…</p><ul><li>You use your phone in bed</li><li>You know staying up until 3am scrolling through contextless rage fuel is bad for your sleep, your mental health, and possibly even your eye balls</li><li>You're going to do it anyway</li></ul><p>We're all addicted to our phones and many of us go to sleep with them in our hands<sup><a href="#footnote-1" id="footnote-reference-1">1</a></sup> — so it's odd that manufacturers choose the default minimum brightness to be brighter than the sun.</p><figure><svg viewBox="0 0 566 186" stroke-linejoin="round" stroke-miterlimit="2"><title>A brightness slider ranging from maximum with a dilated pupil to minimum with a less dilated pupil, and an arrow indicating a lower, more comfortable brightness level with a drowsy eye.</title><path d="m57 186 1-2h7l2 1h1l1-1h6l17-2 2 1h7l5-2h3l1 1h1l1-1a143 143 0 0 1 14-2l1 1h2l5-2h5l2 1h3l21-1h7v-1l-5-2-6 2-1-1-1 1-1-1h-7l-1 1v-1h-1l-2 1h-8l-2-1-7 2h-4l-1-1h-2l-9 2-9 1a1130 1130 0 0 1-30 2h-6l-2-1-14 3h-4l-1-1-3 1-2-1H33l-9-2-2-5-3-1h-2l-2-5h-1l-2-4-1-1-3-8-1-2-1-3v-1l2-3v-3l3-3v-3l1-4v-1l-1-1v-1l2-5v-3l-2-3v-2l3-9v-5l-1-2 1-2v-2l1-2v-1l-1-1 1-1h1l2 2h-1l1 1h3l1-1-1-1 1-1v-2l-3-2h-2l-1-3-2-3v-3l1-1 2-1-1-2-1-1-2 2v-1l-2 3-2 2H8l-1 2-2 4-3 9 1 1h1l1-2 2 2 2-1 2 1v4l-1 3 1 2v9l-3 7 2 4v2l-3 8-2 2 1 4-1 6v6l1 2 3 8 2 2v4l1 3 5 3h1l2 1 2 3 3 3 15 2 2-1h6l2 1h4v-1l1 1m186-3 2-1v-1l4-1 1-3 2-1 2-1 4 1 1-1h1l2 2 4-2 2 1 1-1 3 2v-1l3 1h3v-1l1 1 1-1 2 1 1 1 1-1h15l3 1h11l7 1h3l1-1h7l2 1 2-1h5l2 1 9-1h5l4-1 8 1h16l8-1 2 1h4l2-1 2 1 2-1h1l3 1 2-1h4l5 1 8-2 1 1h6l1-1h18l2 1 1-1h3l6 1 44 4 1-1h5l1 1h5l2-1 2-1 2 1h1l4-2 1 1 3-2v-4l-1-1h-8l-19-1-53-2h-4l-2 1h-30v-1h-10l-14 2h-1l-2-1h-11l-1 1h-6l-1-1-2 1h-34l-85-4-3-3-3-1-1-2h-8l-2 2-4 5h-3l-1-1h-6l-7 2-3 2v2l1 1 3-1 3 2 9-1 1-1c1-1 0-1-1-1h-6v-2c3 0 7-2 9 0s1 5 2 8l2 3 5 3m280-6h-3l-23-1-38-3h-22l-15 1h-64l-22 1-42-2h-7l-2 1-3-1h-5l-5-1h-4l-1 1h-4l-6-2v-1l1-1h33l6 1a1445 1445 0 0 0 38 1h1l10 1h27l39-1 5-1h3l14 1h74l9 2h3l2-1h1l1 1 1-1h3l3 1h6l1 1h-1l-2 2-1-1h-2l-2 1-1 1-1-1h-2m-277 1-7-1-3-2-1-2v-2l1-3 1-2 1-1h2l2-1c2-1 3 0 4 1h1l1 1 2 2-1 1h1l1 2v1h-3l-1 1h-1v1l3 1-2 2m-29-5h-1l1-2 2 2M8 138v3-1m228-14h1l1-1 1-1-1-2v-3l1-12v-6h2l3-3-1-1-1-4-2-1 1-1h-1l-1-1-1-1-3-1h-1l-3 5-2 2-3 2v1l1 1h3l1 1 2-1 1-1h1v19l-1 2 1 2-1 1v2m298-5v6h2l2-9v-12l1-8 1-2h4l1-1v-2l-3-3-2-4-1-1-1-4-1 1-8 8-2 1-2 2v1l2 1h9v4l-1 4m-298-5v-1l1-1v1m-2-1h-1l1-1v-1l1 1M12 93h-1v-1l-2-1H6l1-4 2-3c1-1 2-3 3-2 2 2 3 6 3 9l-2 2m518-2h-1l-2-1v-1l2 1 1-1c1-1 0-5 2-4 2 0 1 3 2 4l-1 1M225 46l1-2v-3l2 1 1-1h2l1 1c1-1 2-3 3-2l1 3 2 1 2-2v-2l2-2h2l1 1v-1l3-4v-1h-1l-2 2-3 1-6 1-2 2-3-2-1 2h-2l-3-2h-2l-1 2-5-1-3-3-2-2h-2l-2-1h-1v1l1 3 2 2h1v1l-1 2 1 1h1l1-4h1l2 2 4 1h3l1 5m296-2 2-7v-2l-1 1-2 5v3m11-2v-3l1-1h-2v5h1M19 42l1-3h3l2 2 1-2 3-2 1 1 1 1h3v-2l2-1v-1l2-2v-1l-2-1-1 1a40 40 0 0 1-14 4l-2-1-1 1-2-2-2-2-2 1c-1-1-1-2-3-2H8v3l1 3h1l4 2h3l2 3m521-1v-3l-1-1h-1l-1 1h1l1 3m-27-2 2-4-1-1h-1l-2 4v1h2m36-1 1 1 1-1-2-1 1-2-1-2-1 1v4m-13-3 2-1h2l4-1h3l2-2h3l2-4v-2l-2 2h-2l-2 2h-3l-2 2-3-1h-1l-2 1h-7l-4-1h-1l-7-3h-3l-1 1 1 3 4 3 7 1 1 1 3-2 1 1 1-1 2 2m23-2 1-1-1-1-1 1m-338 1h1v-1c-1-3 0-6-2-8l-1 2v1l1 6M22 34l3-2 1-4 1-1 2 1h3l1 2 3-1 1 1 4-1 2-3-1-1h-2l-1-1-3-2 1-2-1-1-2 1h-1l-2-2v-1l1-4v-2l-1 1-2 5h-2v-2l1-1-1-1h-1v1l-2 2h-1l-2-1h-3l-2 1h-1v-3l1-2-1-1-2 6-3 1-2-3H7v3l-3 2v2l1 1h2l2-2 1-1h7l2 1-1 1-11 2c-2 0-4 1-5 3l1 2c2-1 3-3 5-3l7-2h2v1l-2 5 2 3 1 1 2 1m212-1h2l2-4v-2l-1-3h-1l-3-2h-2l-2 2-3 3 2 2 1 3 2 1m14-1 1-2v-2l-2 3v1m282-3v-1 1m17-1 1-3 2-5-1-1-2 5-2 3v2m-16-3-1-3 2-6v-1l2-2v-1h-1l-3 3-1 3v4l1 3v1m-279 0 2-1-1-4h-1v4m-16 0h1v1m28-2 6-1 1-1-1-2-3-2-1 1-2-1 3-2-1-1h-2l-1 1-1 1h-1l-3-2 1-1 2-2-1-1h-1l-2 3h-3l-1-1v-1l1-2v-1c-2 0-2 3-3 3l-2-1-3-2V9l2-2V5l-4 5h-2l-3-2V6l1-1-1-1V3h-1l-1 7-2-1h-2l-1 1-3 1V3h-1l-1 7-2 2h-2l-2-4h-1v2l2 2-1 1-2 2v1l1 1h1l1-2h2l2-1 5-2 2 1h3l10 4 4 1 4 3h2v1l5 2 2 1m-40-2 1 1v-1l2-4 2-2v-1h-1l-2 2-2 5M3 23v-1l-2-4H0v3l2 2m556-1 7-2 1-1-1-2-2-1-2 1-4-1 2-2v-1l-1-1v-1l-3 4-4-3 1-1 1-1-1-1h-1l-2 1-2-1-2-1 2-3V4l-1-1c-1 1-1 4-3 4-2 1-3-1-5-2h-1l1-2-1-2-1 3-4 1h-2l-1-5-1 1v3c0 1-1 2-3 2h-3l-1-1h1V3l-2-2v1h-1v1l1 1v2l1 1-4 2-1-1-2-3-1-1-1 2c1 1 3 2 2 3 0 2-2 3-3 3h-1l-2-1-1-4-1 1v1l2 4h1l2 3 1-1 2-3 4-1 8-2 3 1 2-1h2l2 2 4 1 2 1h2l6 5h1l1 2M21 22l-1-1h4l-1 1m510-1v-2l-2-2-2 3v1h3m10-3v-3h-2m-334 2 1-1v-2l-1-1h-2l-2-2v3l1 1 3 2M20 14l1-1v-1l-1-3-1 1v4" fill-rule="nonzero"></path></svg></figure><p>It turns out you can go lower than the minimum brightness.</p><p>Save your eyes from strain, increase your phone's battery life<sup><a href="#footnote-2" id="footnote-reference-2">2</a></sup>, and better ease yourself into a slumber by <strong>creating a shortcut to turn your phone brightness lower than the minimum.</strong></p><p>When you go to bed, or are trying not to disturb others on a sleeper train, or just want to save battery, you can quickly enable this shortcut.</p><p>Here's how…</p><h2>How to create a extra low brightness shortcut on iPhone</h2><p>On iOS this setting is called <code>Reduce White Point</code>.</p><p>Here's how you enable it:</p><ol><li>Open <code>Settings</code></li><li>Navigate to <code>Accessibility</code></li><li>Navigate to <code>Accessibility shortcut</code></li><li>From the menu select <code>Reduce White Point</code></li></ol><p>That's it. Once you've set this up you can <strong>triple-click the side button</strong> and your phone will be significantly dimmer.</p><h2>How to create an extra dim shortcut on Android</h2><p>On android phones this feature is called <code>extra dim</code>.</p><p>Android phones are all different from one another so the instructions are not consistent —&nbsp;but you can search for "extra dim shortcut" in settings and follow the instructions from there.</p><section><h3>Footnotes</h3><ol><li id="footnote-1"><a href="#footnote-reference-1">1</a><p>There are lots of studies on this from all over the world. I couldn't pick which one to cite. I do this.</p></li><li id="footnote-2"><a href="#footnote-reference-2">2</a><p><a href="https://endtimes.dev/actually-dark-mode-can-save-the-world/">Actually, dark mode can save the planet.</a> I wrote this</p></li></ol></section><dl><dt>Tags</dt><dd></dd><dt>Published</dt><dd><time datetime="2023-11-06">6 Nov 2023</time></dd><dt>Updated</dt><dd><time datetime="2023-11-06">6 Nov 2023</time></dd></dl></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Git rebase, what can go wrong (191 pts)]]></title>
            <link>https://jvns.ca/blog/2023/11/06/rebasing-what-can-go-wrong-/</link>
            <guid>38164046</guid>
            <pubDate>Mon, 06 Nov 2023 15:42:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jvns.ca/blog/2023/11/06/rebasing-what-can-go-wrong-/">https://jvns.ca/blog/2023/11/06/rebasing-what-can-go-wrong-/</a>, See on <a href="https://news.ycombinator.com/item?id=38164046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
     

<p>Hello! While talking with folks about Git, I’ve been seeing a comment over and
over to the effect of “I hate rebase”. People seemed to feel pretty strongly
about this, and I was really surprised because I don’t run into a lot of
problems with rebase and I use it all the time.</p>

<p>I’ve found that if many people have a very strong opinion that’s different from
mine, usually it’s because they have different experiences around that thing
from me.</p>

<p>So I asked on <a href="https://social.jvns.ca/@b0rk/111342083852635579">Mastodon</a>:</p>

<blockquote>
<p>today I’m thinking about the tradeoffs of using <code>git rebase</code> a bit. I think
the goal of rebase is to have a nice linear commit history, which is something
I like.</p>

<p>but what are the <em>costs</em> of using rebase? what problems has it caused for you
in practice? I’m really only interested in specific bad experiences you’ve had
here – not opinions or general statements like “rewriting history is bad”</p>
</blockquote>

<p>I got a huge number of incredible answers to this, and I’m going to do my best
to summarize them here. I’ll also mention solutions or workarounds to those
problems in cases where I know of a solution. Here’s the list:</p>

<ul>
<li><a href="#fixing-the-same-conflict-repeatedly-is-annoying">fixing the same conflict repeatedly is annoying</a></li>
<li><a href="#undoing-a-rebase-is-hard">undoing a rebase is hard</a></li>
<li><a href="#force-pushing-to-shared-branches-can-cause-lost-work">force pushing to shared branches can cause lost work</a></li>
<li><a href="#force-pushing-makes-code-reviews-harder">force pushing makes code reviews harder</a></li>
<li><a href="#losing-commit-metadata">losing commit metadata</a></li>
<li><a href="#rebasing-can-break-intermediate-commits">rebasing can break intermediate commits</a></li>
<li><a href="#accidentally-run-git-commit-amend-instead-of-git-rebase-continue">accidentally run git commit –amend instead of git rebase –continue</a></li>
<li><a href="#splitting-commits-in-an-interactive-rebase-is-hard">splitting commits in an interactive rebase is hard</a></li>
<li><a href="#complex-rebases-are-hard">complex rebases are hard</a></li>
<li><a href="#rebasing-long-lived-branches-can-be-annoying">rebasing long lived branches can be annoying</a></li>
<li><a href="#miscellaneous-problems">miscellaneous problems</a></li>
<li><a href="#what-could-a-merge-only-workflow-look-like">what could a merge-only workflow look like?</a></li>
</ul>

<p>My goal with this isn’t to convince anyone that rebase is bad and you shouldn’t
use it (I’m certainly going to keep using rebase!). But seeing all these
problems made me want to be more cautious about recommending rebase to
newcomers without explaining how to use it safely. It also makes me wonder if
there’s an easier workflow for cleaning up your commit history that’s harder to
accidentally mess up.</p>

<h3 id="my-git-workflow-assumptions">my git workflow assumptions</h3>

<p>First, I know that people use a lot of different Git workflows. I’m going to be
talking about the workflow I’m used to when working on a team, which is:</p>

<ul>
<li>the team uses a central Github/Gitlab repo to coordinate</li>
<li>there’s one central <code>main</code> branch. It’s protected from force pushes.</li>
<li>people write code in feature branches and make pull requests to <code>main</code></li>
<li>The web service is deployed from <code>main</code> every time a pull request is merged.</li>
<li>the only way to make a change to <code>main</code> is by making a pull request on Github/Gitlab and merging it</li>
</ul>

<p>This is not the only “correct” git workflow (it’s a very “we run a web service”
workflow and open source project or desktop software with releases generally
use a slightly different workflow). But it’s what I know so that’s what I’ll
talk about.</p>

<h3 id="two-kinds-of-rebase">two kinds of rebase</h3>

<p>Also before we start: one big thing I noticed is that there were 2 different kinds of rebase that kept coming up, and only one of them requires you to deal with merge conflicts.</p>

<ol>
<li><strong>rebasing on an ancestor</strong>, like <code>git rebase -i HEAD^^^^^^^</code> to squash many
small commits into one. As long as you’re just squashing commits, you’ll
never have to resolve a merge conflict while doing this.</li>
<li><strong>rebasing onto a branch that has diverged</strong>, like <code>git rebase main</code>. This can cause merge conflicts.</li>
</ol>

<p>I think it’s useful to make this distinction because sometimes I’m thinking
about rebase type 1 (which is a lot less likely to cause problems), but people
who are struggling with it are thinking about rebase type 2.</p>

<p>Now let’s move on to all the problems!</p>

<h3 id="fixing-the-same-conflict-repeatedly-is-annoying">fixing the same conflict repeatedly is annoying</h3>

<p>If you make many tiny commits, sometimes you end up in a hellish loop where you
have to fix the same merge conflict 10 times.  You can also end up fixing merge
conflicts totally unnecessarily (like dealing with a merge conflict in code
that a future commit deletes).</p>

<p>There are a few ways to make this better:</p>

<ul>
<li>first do a <code>git rebase -i HEAD^^^^^^^^^^^</code> to squash all of the tiny commits
into 1 big commit and then a <code>git rebase main</code> to rebase onto a different
branch. Then you only have to fix the conflicts once.</li>
<li>use <code>git rerere</code> to automate repeatedly resolving the same merge conflicts
(“rerere” stands for “reuse recorded resolution”, it’ll record your previous merge conflict resolutions and replay them).
I’ve never tried this but I think you need to set <code>git config rerere.enabled
true</code> and then it’ll automatically help you.</li>
</ul>

<p>Also if I find myself resolving merge conflicts more than once in a rebase,
I’ll usually run <code>git rebase --abort</code> to stop it and then squash my commits into
one and try again.</p>

<h3 id="undoing-a-rebase-is-hard">undoing a rebase is hard</h3>

<p>I heard from several people that when they were new to rebase, they messed up a
rebase and permanently lost a week of work that they then had to redo.</p>

<p>The problem here is that undoing a rebase that went wrong is <strong>much</strong> more complicated
than undoing a merge that went wrong (you can undo a bad merge with something like <code>git reset --hard HEAD^</code>).
Many newcomers to rebase don’t even realize that undoing a rebase is even
possible, and I think it’s pretty easy to understand why.</p>

<p>That said, it is possible to undo a rebase that went wrong. Here’s an example of how to undo a rebase using <code>git reflog</code>.</p>

<p><strong>step 1</strong>: Do a bad rebase (for example run <code>git rebase -I HEAD^^^^^</code> and just delete 3 commits)</p>

<p><strong>step 2</strong>:  Run <code>git reflog</code>. You should see something like this:</p>

<pre><code>ee244c4 (HEAD -&gt; main) HEAD@{0}: rebase (finish): returning to refs/heads/main
ee244c4 (HEAD -&gt; main) HEAD@{1}: rebase (pick): test
fdb8d73 HEAD@{2}: rebase (start): checkout HEAD^^^^^^^
ca7fe25 HEAD@{3}: commit: 16 bits by default
073bc72 HEAD@{4}: commit: only show tooltips on desktop
</code></pre>

<p><strong>step 3</strong>: Find the entry immediately before <code>rebase (start)</code>. In my case that’s <code>ca7fe25</code></p>

<p><strong>step 4</strong>:  Run <code>git reset --hard ca7fe25</code></p>

<p>Another solution folks mentioned to “undoing a rebase is hard” that avoids having
to use the reflog is to make a “backup branch” with <code>git switch -c backup</code>
before rebasing, so you can easily get back to the old commit.</p>

<h3 id="force-pushing-to-shared-branches-can-cause-lost-work">force pushing to shared branches can cause lost work</h3>

<p>A few people mentioned the following situation:</p>

<ol>
<li>You’re collaborating on a branch with someone</li>
<li>You push some changes</li>
<li>They rebase the branch and run <code>git push --force</code> (maybe by accident)</li>
<li>Now when you run <code>git pull</code>, it’s a mess – you get the a <code>fatal: Need to specify how to reconcile divergent branches</code> error</li>
<li>While trying to deal with the fallout you might lose some commits, especially if some of the people are involved aren’t very comfortable with git</li>
</ol>

<p>This is an even worse situation than the “undoing a rebase is hard” situation
because the missing commits might be split across many different people’s and
the only worse thing than having to hunt through the reflog is multiple
different people having to hunt through the reflog.</p>

<p>This has never happened to me because the only branch I’ve ever collaborated on
is <code>main</code>, and <code>main</code> has always been protected from force pushing (in my
experience the only way you can get something into <code>main</code> is through a pull
request). So I’ve never even really been in a situation where this <em>could</em>
happen. But I can definitely see how this would cause problems.</p>

<p>The main tools I know to avoid this are:</p>

<ul>
<li>don’t rebase on shared branches</li>
<li>use <code>--force-with-lease</code> when force pushing, to make sure that nobody else has pushed to the branch since you last push</li>
</ul>

<p>I was curious about why people would run <code>git push --force</code> on a shared branch. Some reasons people gave were:</p>

<ul>
<li>they’re working on a collaborative feature branch, and the feature branch needs to be rebased onto <code>main</code>. The idea here is that you’re just really careful about coordinating the rebase so nothing gets lost.</li>
<li>as an open source maintainer, sometimes they need to rebase a contributor’s branch to fix a merge conflict</li>
<li>they’re new to git, read some instructions online that suggested <code>git rebase</code> and <code>git push --force</code> as a solution, and followed them without understanding the consequences</li>
<li>they’re used to doing <code>git push --force</code> on a personal branch and ran it on a shared branch by accident</li>
</ul>

<h3 id="force-pushing-makes-code-reviews-harder">force pushing makes code reviews harder</h3>

<p>The situation here is:</p>

<ul>
<li>You make a pull request on GitHub</li>
<li>People leave some comments</li>
<li>You update the code to address the comments, rebase to clean up your commits, and force push</li>
<li>Now when the reviewer comes back, it’s hard for them to tell what you changed since the last time you saw it – all the commits show up as “new”.</li>
</ul>

<p>One way to avoid this is to push new commits addressing the review comments,
and then after the PR is approved do a rebase to reorganize everything.</p>

<p>I think some reviewers are more annoyed by this problem than others, it’s kind
of a personal preference. Also this might be a Github-specific issue, other
code review tools might have better tools for managing this.</p>

<h3 id="losing-commit-metadata">losing commit metadata</h3>

<p>If you’re rebasing to squash commits, you can lose important commit metadata
like <code>Co-Authored-By</code>. Also if you GPG sign your commits, rebase loses the
signatures.</p>

<p>There’s probably other commit metadata that you can lose that I’m not thinking of.</p>

<p>I haven’t run into this one so I’m not sure how to avoid it. I think GPG
signing commits isn’t as popular as it used to be.</p>

<h3 id="rebasing-can-break-intermediate-commits">rebasing can break intermediate commits</h3>

<p>If you’re trying to have a very clean commit history where the tests pass on
every commit (very admirable!), rebasing can result in some intermediate
commits that are broken and don’t pass the tests, even if the final commit
passes the tests.</p>

<p>Apparently you can avoid this by using <code>git rebase -x</code> to run the test suite at
every step of the rebase and make sure that the tests are still passing. I’ve
never done that though.</p>

<h3 id="accidentally-run-git-commit-amend-instead-of-git-rebase-continue">accidentally run <code>git commit --amend</code> instead of <code>git rebase --continue</code></h3>

<p>A couple of people mentioned issues with running <code>git commit --amend</code> instead of <code>git rebase --continue</code> when resolving a merge conflict.</p>

<p>The reason this is confusing is that there are two reasons when you might want to edit files during a rebase:</p>

<ol>
<li>editing a commit (by using <code>edit</code> in <code>git rebase -i</code>), where you need to write <code>git commit --amend</code> when you’re done</li>
<li>a merge conflict, where you need to run <code>git rebase --continue</code> when you’re done</li>
</ol>

<p>It’s very easy to get these two cases mixed up because they feel very similar. I think what goes wrong here is that you:</p>

<ul>
<li>Start a rebase</li>
<li>Run into a merge conflict</li>
<li>Resolve the merge conflict, and run <code>git add file.txt</code></li>
<li>Run <code>git commit</code> because that’s what you’re used to doing after you run <code>git add</code></li>
<li>But you were supposed to run <code>git rebase --continue</code>! Now you have a weird extra commit, and maybe it has the wrong commit message and/or author</li>
</ul>

<h3 id="splitting-commits-in-an-interactive-rebase-is-hard">splitting commits in an interactive rebase is hard</h3>

<p>The whole point of rebase is to clean up your commit history, and <strong>combining</strong>
commits with rebase is pretty easy. But what if you want to split up a commit into 2
smaller commits? It’s not as easy, especially if the commit you want to split
is a few commits back! I actually don’t really know how to do it even though I
feel very comfortable with rebase. I’d probably just do <code>git reset HEAD^^^</code>  or
something and use <code>git add -p</code> to redo all my commits from scratch.</p>

<p>One person shared <a href="https://github.com/kimgr/git-rewrite-guide#split-a-commit">their workflow for splitting commits with rebase</a>.</p>

<h3 id="complex-rebases-are-hard">complex rebases are hard</h3>

<p>If you try to do too many things in a single <code>git rebase -i</code> (reorder commits
AND combine commits AND modify a commit), it can get really confusing.</p>

<p>To avoid this, I personally prefer to only do 1 thing per rebase, and if I want
to do 2 different things I’ll do 2 rebases.</p>

<h3 id="rebasing-long-lived-branches-can-be-annoying">rebasing long lived branches can be annoying</h3>

<p>If your branch is long-lived (like for 1 month), having to rebase repeatedly
gets painful. It might be easier to just do 1 merge at the end and only resolve
the conflicts once.</p>

<p>The dream is to avoid this problem by not having long-lived branches but it
doesn’t always work out that way in practice.</p>

<h3 id="miscellaneous-problems">miscellaneous problems</h3>

<p>A few more issues that I think are not that common:</p>

<ul>
<li><strong>Stopping a rebase wrong</strong>: If you try to abort a rebase that’s going badly with
<code>git reset --hard</code> instead of <code>git rebase --abort</code>, things will behave
weirdly until you stop it properly</li>
<li><strong>Weird interactions with merge commits</strong>: A couple of quotes about this: “If you
rebase your working copy to keep a clean history for a branch, but the
underlying project uses merges, the result can be ugly. If you do rebase -i
HEAD~4 and the fourth commit back is a merge, you can see dozens of commits
in the interactive editor.“, “I’ve learned the hard way to <em>never</em> rebase if
I’ve merged anything from another branch”</li>
</ul>

<h3 id="should-we-teach-squash-and-merge-before-rebase">should we teach “squash and merge” before rebase?</h3>

<p>Seeing all of these issues made me wonder if there’s something better we can
recommend to git newcomers before rebase. Like – if someone has 1 million crappy
commits on their branch (wip wip wip fix ugh) and they don’t know how to clean
them up, is rebase really the best place to start? Would “squash and merge” be
easier?</p>

<p>I think an alternate workflow that avoids rebase could be:</p>

<ul>
<li>make commits</li>
<li>Run <code>git merge main</code> to merge main into my branch periodically</li>
<li>To look at the log of changes made on my branch, run <code>git diff main</code> or <code>git log main..mybranch</code>. That will look something like this:</li>
</ul>

<pre><code>$ git log main..mybranch
756d4af (HEAD -&gt; mybranch) Merge branch 'main' into mybranch
20106fd Merge branch 'main' into mybranch
d7da423 some commit on my branch
85a5d7d some other commit on my branch
</code></pre>

<ul>
<li>When you’re done, use GitHub’s “squash and merge” feature (which is the equivalent of
running <code>git checkout main; git merge --squash mybranch</code>) to squash all of my
changes into 1 commit. This gets rid of all the “ugly” merge commits.</li>
</ul>

<p>I think the main things you’d lose with this merge-only workflow (relative to
using rebase) are:</p>

<ul>
<li>you have to look at some “merge” commits when you run <code>git log main..mybranch</code></li>
<li>you have to squash all your commits, so you can’t have 2 separate commits</li>
<li>The “clean up with <code>git reset</code>” workflow is dangerous in a different way than using rebase</li>
</ul>

<p>I haven’t tried this, this is just me thinking out loud. I’d like to hear if
anyone uses a similar workflow in practice though.</p>

<h3 id="there-are-more-problems-than-i-expected">there are more problems than I expected</h3>

<p>I went into this really feeling like “rebase is fine, what could go wrong?” But
many of these problems actually have happened to me in the past, it’s just that
over the years I’ve learned how to avoid or fix all of them.</p>

<p>And I’ve never really seen anyone share best practices for rebase, other than
“never force push to a shared branch”. All of these honestly make me a lot more
reluctant to recommend using rebase.</p>

<p>To recap, I think these are my personal rebase rules I follow:</p>

<ul>
<li>stop a rebase if it’s going badly instead of letting it finish (with <code>git rebase --abort</code>)</li>
<li>know how to use <code>git reflog</code> to undo a bad rebase</li>
<li>don’t rebase a million tiny commits (instead do it in 2 steps: <code>git rebase -i HEAD^^^^</code> and then <code>git rebase main</code>)</li>
<li>don’t do more than one thing in a <code>git rebase -i</code>. Keep it simple.</li>
<li>never force push to a shared branch</li>
<li>never rebase commits that have already been pushed to <code>main</code></li>
</ul>

<p><small>
Thanks to Marco Rogers for encouraging me to think about the problems people
have with rebase, and to everyone on Mastodon who helped with this.
</small></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Regarding Proposed US Restrictions on RISC-V (161 pts)]]></title>
            <link>https://www.bunniestudios.com/blog/?p=6862</link>
            <guid>38163412</guid>
            <pubDate>Mon, 06 Nov 2023 14:54:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bunniestudios.com/blog/?p=6862">https://www.bunniestudios.com/blog/?p=6862</a>, See on <a href="https://news.ycombinator.com/item?id=38163412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-6862">
				<p>A bipartisan group of 18 lawmakers in the US Congress have recently amplified a request to the White House and the Secretary of Commerce to <a href="https://www.reuters.com/technology/us-lawmakers-press-biden-plans-chinese-use-open-chip-technology-2023-11-02/">place restrictions on Americans working with RISC-V</a> (see also <a href="https://www.reuters.com/technology/us-china-tech-war-risc-v-chip-technology-emerges-new-battleground-2023-10-06/">the initial request from the Senate</a>) in order to prevent China from gaining dominance in CPU technology.</p>
<p>The request is facially misguided; any restrictions would only serve to reduce American participation in an important emerging technology, while bolstering ARM’s position as an incumbent near-monopoly provider of embedded CPUs.</p>
<p>When the first report came out, I hoped it was just a blip that would go away, but with the broader bi-partisan group asking for restrictions, I felt I could no longer just stand by and watch: I am an active participant in the RISC-V ecosystem. I’m also subject to US law.</p>
<p>I did the one thing any American can do, which is write a letter summarizing my thoughts on the issue, and sending it to the White House, Department of Commerce, and the relevant members of Congress. Unfortunately, I don’t have a PAC, lobbyists or any sort of high-level connections to US politicians, so I don’t have much hope the letter will be received in time.</p>
<p>However, I do have a blog. I’m posting a copy of the letter I sent to the White House here, in far-flung hopes that maybe someone with more political connections than I might pick it up and send it on.</p>
<p>Finally, if you disagree with my stance or have a different perspective, I also encourage you to send a letter expressing your thoughts to various government officials. It doesn’t have to be “my way”, but a show of broad public interest in the topic may at least encourage policymakers to think a bit more carefully about the issue, and to hear out more perspectives.</p>
<h3>The Letter</h3>
<p>To President Biden and the White House staff:</p>
<p>Recently, a letter was sent to the White House and the Secretary of Commerce by 18 lawmakers asking how the US plans to prevent China “from achieving dominance in … RISC-V technology and leveraging that dominance at the expense of US national and economic security”.</p>
<p>I am a Michigan-born American with a PhD from MIT in electrical engineering. I’m also a small business owner who designs and manufactures electronics. I am writing to urge you to not place any restrictions on the sharing of RISC-V technology.</p>
<p>My products’ CPUs are based on the open source RISC-V standard. RISC-V’s openness specifically benefits small businesses such as mine. I get tools and designs from the open source community, and I contribute my improvements back to the pool. Barrier-free participation in this vibrant open source ecosystem keeps overhead low, allowing me to be competitive in the cutthroat hardware business.</p>
<p>Like the Internet, RISC-V is already a global phenomenon. There are already prolific contributions from the EU, India, China, and more [1]; the US is not the sole proprietor of RISC-V implementations. I use an implementation of RISC-V called the VexRiscv, which is developed in the EU. Any barrier for US persons’ participation will only slow American progress in developing and adopting this technology. It will have an effect opposite of that intended by lawmakers.</p>
<p>A further subtlety is that RISC-V is simply a standard. It defines a set of words used to tell a chip to do something, similar to how we rely on a dictionary to define the meaning of English words. Just as one can write secret documents using openly defined words, designs using the RISC-V standard can be proprietary, even if the standard is open. The benefits of open standards are so well established that the US has an entire agency – NIST – to promote American innovation and industrial competitiveness by publishing open standards.</p>
<p>Furthermore, it is not practical to police the use of an established standard: once a book is published, it is impractical to ensure that none of America’s enemies obtain a copy of it. This has long been a trade-off of American innovation philosophy: we can freely exercise our First Amendment rights to share ideas, creating a vibrant intellectual exchange, even at the risk of others benefiting from reading our textbooks, journals and patents.</p>
<p>I believe this trade-off has been in our favor. With every exchange – even with potential competitors – we learn more. Chilling our freedom of expression to achieve administrative outcomes is a page out of other more oppressive regimes’ playbooks: it is fundamentally un-American to restrict the flow of ideas.</p>
<p>In summary, any restrictions placed on US persons sharing RISC-V technology would only serve to diminish America’s role as a technological leader. Over-broad restrictions could deprive educators of a popular tool used to teach students about computers on American campuses, for fear of also accidentally teaching to an embargoed entity. And even narrow restrictions on RISC-V could deprive US tech companies with any potential exposure to the Chinese market of access to a cost-effective, high-performance CPU technology, forcing them to pay royalties to the incumbent near-monopoly provider, ARM Holdings plc – a company that isn’t American. This weakens American competitiveness and ultimately harms the US’s best interests.</p>
<p>If the administration agrees that RISC-V is a technology so critical to US economic and military interests that it deserves special attention, instead of trying to restrict its expression with a federally-mandated licensing regime, it should invest in programs to develop more home-grown American RISC-V chip maker success stories. It is already within the four corners of existing US legal framework, and the RISC-V contractual framework, for companies to choose to develop proprietary implementations of RISC-V CPUs. The US has strong precedents for companies navigating the boundaries of open standards and finding success without the need for federal guidance: Intel and AMD are American industrial juggernauts built around proprietary implementations of an otherwise openly documented “x86” computer standard. What the US needs is an American answer to ARM Holdings plc’s monopoly, and that answer comes from investing in US companies that embrace RISC-V.</p>
<p>President Biden, I urge you: have faith in American innovation. Have faith in American values. Do not place any restrictions on the sharing of RISC-V technology. We can work together to build more US chip maker success stories, while embracing the American value of freedom of expression!</p>
<p>Very truly yours,</p>
<p>Andrew ‘bunnie’ Huang<br>
An American Hacker, Maker, and Author</p>
<p>[1] https://github.com/riscvarchive/riscv-cores-list</p>

								
				<p>
					<small>
												This entry was posted on Monday, November 6th, 2023 at 10:30 pm and is filed under <a href="https://www.bunniestudios.com/blog/?cat=5" rel="category">Ponderings</a>, <a href="https://www.bunniestudios.com/blog/?cat=3" rel="category">Social</a>.						You can follow any responses to this entry through the <a href="https://www.bunniestudios.com/blog/?feed=rss2&amp;p=6862">RSS 2.0</a> feed. 

													You can <a href="#respond">leave a response</a>, or <a href="https://www.bunniestudios.com/wordpress/wp-trackback.php?p=6862" rel="trackback">trackback</a> from your own site.
						
					</small>
				</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[USB-C cable with the bandwidth and USB type imprinted on the connector (233 pts)]]></title>
            <link>https://www.theverge.com/2023/11/6/23948486/usb-c-cables-marking-speed-power-delivery-elgato</link>
            <guid>38162837</guid>
            <pubDate>Mon, 06 Nov 2023 14:15:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/11/6/23948486/usb-c-cables-marking-speed-power-delivery-elgato">https://www.theverge.com/2023/11/6/23948486/usb-c-cables-marking-speed-power-delivery-elgato</a>, See on <a href="https://news.ycombinator.com/item?id=38162837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Elgato hasn’t just made an <a href="https://www.theverge.com/2023/10/17/23920498/elgato-prompter-teleprompter-monitor-release-date-price">excellent teleprompter</a>, it’s also made a great USB-C cable that ships with it. Professional audio engineer <a href="https://twitter.com/JustSpike_/status/1721247993230250387">Matt “Spike” McWilliams spotted</a> that Elgato’s latest USB-C cable has the bandwidth and USB type imprinted on the connector, and now I wish all manufacturers did this.</p><p>I recently spent too many hours sorting my USB-C cables into ones that are high speed, ones that can deliver fast charging, and ones that can do both. None of them had any marker to let me know the speed or type of USB-C cable without me having to test them. It’s a common issue for <a href="https://www.theverge.com/2023/9/16/23872237/apple-iphone-15-usb-c-switch-guide">people switching to USB-C</a> right now, and even a small indicator like Elgato’s can certainly help. The writing on Elgato’s cable tells me it’s USB 3.0 compatible and can support up to 5Gbps in bandwidth.</p><p><a href="https://www.usb.org/logo-license">The USB Implementers Forum has a logo</a> that manufacturers can use on packaging, but that’s often useless later on when you’re searching for a specific USB-C cable in your box of cables. I’ve also purchased plenty of USB-C cables that have claimed, on the box, to deliver a certain bandwidth and failed to do so in reality.</p><p>Elgato’s solution is so simple I didn’t even notice it when I unboxed its <a href="https://www.theverge.com/2023/10/17/23920498/elgato-prompter-teleprompter-monitor-release-date-price">Prompter hardware recently</a>, but it’s so useful that I wish it was more widespread. “You’ll see spec data on all Elgato USB and HDMI cables going forward,” explains Julian Fest, senior vice president at Elgato, in a <a href="https://twitter.com/JFest/status/1721264675193553025">post on X (Twitter)</a>. “No more guessing if the cable is the culprit of a tech issue.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who's Behind the Swat USA Reshipping Service? (142 pts)]]></title>
            <link>https://krebsonsecurity.com/2023/11/whos-behind-the-swat-usa-reshipping-service/</link>
            <guid>38162597</guid>
            <pubDate>Mon, 06 Nov 2023 13:55:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2023/11/whos-behind-the-swat-usa-reshipping-service/">https://krebsonsecurity.com/2023/11/whos-behind-the-swat-usa-reshipping-service/</a>, See on <a href="https://news.ycombinator.com/item?id=38162597">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>Last week, KrebsOnSecurity broke the news that one of the largest cybercrime services for laundering stolen merchandise was hacked recently, exposing its internal operations, finances and organizational structure. In today’s Part II, we’ll examine clues about the real-life identity of “<strong>Fearlless</strong>,” the nickname chosen by the proprietor of the <strong>SWAT USA Drops</strong> service.</p>
<p><img decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2022/01/wbrkb.jpg" alt="" width="750" height="501"></p>
<p>Based in Russia, SWAT USA recruits people in the United States to reship packages containing pricey electronics that are purchased with stolen credit cards. As detailed in <a href="https://krebsonsecurity.com/2023/11/russian-reshipping-service-swat-usa-drop-exposed/" target="_blank" rel="noopener">this Nov. 2 story</a>, SWAT currently employs more than 1,200 U.S. residents, all of whom will be cut loose without a promised payday at the end of their first month reshipping stolen goods.</p>
<p>The current co-owner of SWAT, a cybercriminal who uses the nickname “Fearlless,” operates primarily on the cybercrime forum <strong>Verified</strong>. This Russian-language forum has tens of thousands of members, and it has suffered several hacks that exposed more than a decade’s worth of user data and direct messages.</p>
<p>January 2021 posts on Verified show that Fearlless and his partner <strong>Universalo</strong> purchased the SWAT reshipping business from a Verified member named SWAT, who’d been operating the service for years. SWAT agreed to transfer the business in exchange for 30 percent of the net profit over the ensuing six months.</p>
<p>Cyber intelligence firm <a href="https://www.intel471.com/" target="_blank" rel="noopener">Intel 471</a> says Fearlless first registered on Verified in February 2013. The email address Fearlless used on Verified leads nowhere, but a review of Fearlless’ direct messages on Verified indicates this user originally registered on Verified a year earlier as a reshipping vendor, under the alias “<strong>Apathyp</strong>.”</p>
<p>There are two clues supporting the conclusion that Apathyp and Fearlless are the same person. First, the Verified administrators warned Apathyp he had violated the forum’s rules barring the use of multiple accounts by the same person, and that Verified’s automated systems had detected that Apathyp and Fearlless were logging in from the same device.&nbsp; Second, in his earliest private messages on Verified, Fearlless told others to contact him on an instant messenger address that Apathyp had claimed as his.<span id="more-65541"></span></p>
<p>Intel 471 says Apathyp registered on Verified using the email address <strong>triploo@mail.ru</strong>. A search on that email address at the breach intelligence service <a href="https://www.constellaintelligence.com/" target="_blank" rel="noopener">Constella Intelligence</a> found that a password commonly associated with it was “<strong>niceone</strong>.” But the triploo@mail.ru account isn’t connected to much else that’s interesting except a now-deleted account at <strong>Vkontakte</strong>, the Russian answer to Facebook.</p>
<p>However, in Sept. 2020, Apathyp sent a private message on Verified to the owner of a stolen credit card shop, saying his credentials no longer worked. Apathyp told the proprietor that his chosen password on the service was “<strong>12Apathy</strong>.”</p>
<p>A search on that password at Constella reveals it was used by just four different email addresses, two of which are particularly interesting: <strong>gezze@yandex.ru </strong>and <strong>gezze@mail.ru</strong>. Constella discovered that both of these addresses were previously associated with the same password as triploo@mail.ru — “niceone,” or some variation thereof.</p>
<p>Constella found that years ago gezze@mail.ru was used to create a Vkontakte account under the name <strong>Ivan Sherban</strong> (former password: “<strong>12niceone</strong>“) from Magnitogorsk, an industrial city in the southern region of Russia. That same email address is now tied to a Vkontakte account for an Ivan Sherban who lists his home as Saint Petersburg, Russia. Sherban’s profile photo <a href="https://krebsonsecurity.com/wp-content/uploads/2023/11/gezze.png" target="_blank" rel="noopener">shows</a> a heavily tattooed, muscular and <a href="https://krebsonsecurity.com/wp-content/uploads/2023/11/gezze-car.png" target="_blank" rel="noopener">recently married</a> individual with his beautiful new bride getting ready to drive off in a convertible sports car.</p>
<p>A pivotal clue for validating the research into Apathyp/Fearlless came from the identity intelligence firm <a href="https://mynetwatchman.com/" target="_blank" rel="noopener">myNetWatchman</a>, which found that gezze@mail.ru at one time used the passwords “<strong>геззи1991</strong>” (gezze1991) and “<strong>gezze18081991</strong>.”</p>
<p>Care to place a wager on when Vkontakte says is Mr. Sherban’s birthday? Ten points if you answered <strong>August 18</strong> (18081991).</p>
<p>Mr. Sherban did not respond to multiple requests for comment.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mastering DOM manipulation with vanilla JavaScript (349 pts)]]></title>
            <link>https://phuoc.ng/collection/html-dom/</link>
            <guid>38162435</guid>
            <pubDate>Mon, 06 Nov 2023 13:41:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phuoc.ng/collection/html-dom/">https://phuoc.ng/collection/html-dom/</a>, See on <a href="https://news.ycombinator.com/item?id=38162435">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Mastering DOM manipulation with vanilla JavaScript</h2><p><a href="https://github.com/phuocng/html-dom">Star me on GitHub <!-- -->→ 5400<!-- --> ⭐</a></p><div><p>Web development moves at lightning speed. I still remember when I first started using libraries like <a href="https://jquery.com/">jQuery</a>, <a href="http://prototypejs.org/">Prototype</a>, <a href="https://script.aculo.us/">script.aculo.us</a>,
<a href="https://zeptojs.com/">Zepto</a>, and many more. Even with modern tools like <a href="https://angular.io/">Angular</a>, <a href="https://vuejs.org/">VueJS</a>, <a href="https://react.dev/">React</a>, <a href="https://www.solidjs.com/">Solid</a> and <a href="https://svelte.dev/">Svelte</a>,
we still have to deal with the Document Object Model (DOM). While these frameworks encapsulate and hide direct DOM management, they still give us access to work with the DOM via <em>refs</em> and <em>event handlers</em>.</p>
<p>Whether you're developing or using a web component in any framework, you need to work with the DOM at a certain level.
Knowing the browser DOM APIs and how to use them is crucial to web development. A website that introduces the APIs, highlights common problems,
and provides answers to popular questions can be incredibly useful.</p>
<p>That's why I've put together this collection of resources:</p>
<ul>
<li>No external libraries, just native browser APIs</li>
<li>Small, easy-to-understand examples</li>
<li>Live demos</li>
<li>Tips and best practices included</li>
<li>Real-life use cases</li>
<li>Works with modern browsers and <del>even supports Internet Explorer</del></li>
</ul>
<p>Get ready to master DOM manipulation with vanilla JavaScript.</p></div><section><h2>Level 1 — Basic</h2></section><section><h2>Level 2 — Intermediate</h2></section><section><h2>Level 3 — Advanced</h2></section><section><h2>Tip</h2></section><section><h2>Recent posts ⚡</h2></section><section><h2>Newsletter 🔔</h2><p>If you're into front-end technologies and you want to see more of the content I'm creating, then you might want to consider subscribing to my newsletter.</p><p>By subscribing, you'll be the first to know about new articles, products, and exclusive promotions.</p><p>Don't worry, I won't spam you. And if you ever change your mind, you can unsubscribe at any time.</p><p>Phước Nguyễn</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A new home and license (AGPL) for Synapse and friends (113 pts)]]></title>
            <link>https://element.io/blog/element-to-adopt-agplv3/</link>
            <guid>38162275</guid>
            <pubDate>Mon, 06 Nov 2023 13:25:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://element.io/blog/element-to-adopt-agplv3/">https://element.io/blog/element-to-adopt-agplv3/</a>, See on <a href="https://news.ycombinator.com/item?id=38162275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>We believe in open source because it encourages innovation, ensures transparency and puts end-users in control. It’s why we created Matrix as an open source project, and ensured a <a href="https://matrix.org/foundation">well-defined open governance model</a> around it through the Matrix.org Foundation, to ensure the protocol will always be guided by its original manifesto.</p><p>As founders of Matrix, we created Element as a for-profit open source company to hire the core Matrix team to be able to work on Matrix, develop a flagship Matrix-based product, bootstrap the Matrix ecosystem, and help fund the underlying core Matrix projects. As a commercial entity, Element has driven the bulk (more than 95%) of core Matrix development for the last seven years, and maintains the largest Matrix homeserver (matrix.org) on behalf of the Matrix.org Foundation. That has helped drive Matrix adoption, and stimulate a wonderfully vibrant community.</p><p>Over the last year or two Matrix has evolved from ‘explosive growth’ to being a ‘category’ in its own right. In other words, ‘Matrix-based’ is now specified as a requirement in massive public and private sector tenders - in which multinationals compete to provide Matrix-based products and services.</p><p>That’s fantastic, and a huge achievement. A competitive open source ecosystem is a powerful multiplier. It triggers more innovation, encourages transparency and gives end-users more independence.</p><h2 id="the-road-ahead">The road ahead</h2><p>Element has always put the growth and success of Matrix first. We have daily discussions about which choices best ensure that Matrix thrives, with Element simply needing to stay on a path to success.</p><p>Today we have arrived at a crossroads. We have succeeded in making Matrix wildly successful, but Element is losing its ability to compete in the very ecosystem it has created. It is hard for Element to innovate and adapt as quickly as companies whose business model is developing proprietary Matrix-based products and services without the responsibility and costs of maintaining the bulk of Matrix. In order to be fair to our customers, we need to be able to put more focus on them and their specific requirements.</p><p>So it’s time for us to get back in the game by establishing a level playing field and ensuring we can continue to support Matrix, whilst delivering the services our customers are requesting. This took us to reconsider how we license the open source code we develop.</p><p>After considerable thought, and taking particular <a href="https://grafana.com/blog/2021/04/20/grafana-loki-tempo-relicensing-to-agplv3/">inspiration from Grafana</a>, we’ve chosen to pursue future development of Synapse (the main Matrix server), Dendrite (our second generation Matrix server) and associated server-side projects (e.g. sydent, sygnal) under the terms of the <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">Affero General Public License (AGPL) v3</a> - maintaining the code in new repositories in the Element GitHub org, forked from the Apache-licensed repositories in the Matrix.org GitHub org (originally donated by Element). This is still the same team of developers who have been working on Matrix since it began in 2014, still developing and releasing Synapse as open source - and in fact, arguably even more Free and Libre than before thanks to the AGPL. Client-side code developed by Element, including projects donated to the Foundation, is not affected.</p><p>The benefit of switching to AGPLv3 is that it obliges downstream developers to contribute back to the core project - either by releasing their modifications as open source for the benefit of the whole Matrix ecosystem, or by contacting Element for an alternative license. Future code contributors to Synapse will need to sign a contributor license agreement (CLA) based on the <a href="https://www.apache.org/licenses/contributor-agreements.html#clas">Apache Software Foundation’s CLA</a>, giving Element the right to distribute the contribution commercially so we can use it to help fund Matrix core development in future.</p><p>We believe this is the fairest approach possible: preserving the Free and Open Source nature of these Matrix implementations under an OSI-approved open source license (AGPLv3), while encouraging proprietary forks to contribute to the development costs of the underlying project.</p><h2 id="what-s-the-impact-on-me">What’s the impact on me?</h2><p>For Element users and others who don’t run their own server, there’s no difference at all.</p><p>For those running unmodified free-standing open source instances of <a href="https://github.com/matrix-org/synapse">Synapse</a>, <a href="https://github.com/matrix-org/dendrite">Dendrite</a>, <a href="https://github.com/matrix-org/sygnal">Sygnal</a>, <a href="https://github.com/matrix-org/sydent">Sydent</a> and <a href="https://github.com/matrix-org/matrix-authentication-service">MAS</a>, there is no change beyond needing to point your deployments at the new repositories in the Element rather than Matrix GitHub organization. These will be available in the coming days. </p><p>Please see <a href="https://matrix.org/blog/2023/11/06/future-of-synapse-dendrite">here</a> for The Matrix.org Foundation's position. Contributors to these new repositories will now need to agree to a standard <a href="https://www.apache.org/licenses/contributor-agreements.html#clas">Apache CLA</a> with Element before their PRs can be merged - this replaces the previous <a href="https://developercertificate.org/">DCO</a> sign-off mechanism.</p><p>Those running <a href="https://element.io/enterprise-functionality">Element Server Suite (ESS)</a>, Element’s enterprise server distribution of Synapse and supporting projects, are not affected - ESS comes with its own enterprise software license which supplants the AGPL requirements.</p><p>Organizations and individuals who want to create or run modified versions of these projects or incorporate them within non-AGPL projects <strong>are</strong> impacted, and they have two options:</p><ol><li>If they are happy to publish their modifications as open source (under the terms of the AGPLv3), they can do so and carry on as normal.</li><li>If they want to keep their modifications and surrounding code proprietary, and distribute the software or run it as a service<strong>,</strong> they will need to contact Element at <a href="https://element.io/cdn-cgi/l/email-protection#2945404a4c475a40474e694c454c444c475d074046"><span data-cfemail="9bf7f2f8fef5e8f2f5fcdbfef7fef6fef5efb5f2f4">[email&nbsp;protected]</span></a> to arrange an alternative license.</li></ol><h2 id="a-catalyst-for-the-ecosystem">A catalyst for the ecosystem</h2><p>We think of this license change as a catalyst that will help Element increase the momentum of Matrix even more; and drive full scale adoption of interoperable, data sovereign and secure real time communication. It will speed innovation and accelerate the entire Matrix economy.</p><p>Across 2023 Element has laid the groundwork for <a href="https://matrix.org/blog/2023/09/matrix-2-0/">Matrix 2.0</a>, enabling a step change in performance built off the back of OIDC, Sliding Sync and Native VoIP. AGPLv3 means all those building on Element’s Matrix 2.0 implementations will now contribute to further development.</p><p>Thanks in large part to our work with BWI, the IT-Systemhouse of the German Armed Forces, <a href="https://element.io/labs/element-x">Element X</a> exists as a <a href="https://element.io/blog/element-x-ignition/">flagship Matrix 2.0 client</a> which will spark a new era of client-side innovation.</p><p>We’ve built out <a href="https://element.io/enterprise-functionality">Element Server Suite</a>, providing a scalable, robust, and professionally supported Matrix server distribution based on our work deploying Matrix for the largest and most demanding organizations in the world - powering both Element deployments, and giving Matrix client developers an enterprise-grade backend to use as they extend the Matrix ecosystem.</p><p>And finally, to help get up and running on Element Server Suite, we’ve launched <a href="https://element.io/blog/element-starter-open-source-meets-on-premise-collaboration/">Element Starter</a> - a free self-hosted version of ESS that supports organizations of up to 200 people.</p><p>Combined with the creation of <a href="https://matrix.org/blog/2023/06/membership-program/">The Matrix.org Foundation Membership program</a>, and <a href="https://matrix.org/blog/2023/09/introducing-josh-simmons-managing-director/">Josh Simmons’ appointment as Managing Director</a>, the Matrix ecosystem has never been so big, mature or held more potential.</p><p>While licensing changes always mark an era of change, we believe the shift to AGPL for Synapse and friends will ensure the entire ecosystem is operating to create a virtuous cycle: helping Element support Matrix more than ever, and so benefiting the entire Matrix industry.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Video Game That Pays: Lessons Learned from Working Remotely (161 pts)]]></title>
            <link>https://dtransposed.github.io/blog/2023/11/02/Remote-SWE/</link>
            <guid>38161997</guid>
            <pubDate>Mon, 06 Nov 2023 13:00:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dtransposed.github.io/blog/2023/11/02/Remote-SWE/">https://dtransposed.github.io/blog/2023/11/02/Remote-SWE/</a>, See on <a href="https://news.ycombinator.com/item?id=38161997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  

  <article>
  <div>
<p><img src="https://dtransposed.github.io/assets/17/1.png" width="50%"></p><p>The original tweet by Sahil Lavingia</p></div>

<p>Some time ago, I stumbled upon this amusing tweet from <a href="https://twitter.com/shl">@shl</a>. I love how true this statement is, although it feels very wrong to admit it. Let’s think about the day in the life of a software engineer:</p>

<blockquote>

  <ul>
    <li>You interact with people from all corners of the world.</li>
    <li>You complete tasks on different online platforms (Slack, GitHub, VS Code, Google Docs, etc.).</li>
    <li>You have a list of your main quests to complete (you can find them in your journ… Kanban board!)</li>
    <li>There are also side-quests to take care of (“Hey Damian, could you take a look at this bug?”).</li>
    <li>Sometimes you can gang up with your teammates to slay a big beast (“Hey, I have this nasty bug that I have been working on for the past few days. I know you have better knowledge of this particular part of the repository, could we jump on a pair programming session?”).</li>
    <li>And you get to level up once in a while!</li>
  </ul>
</blockquote>

<p>Playing a game that pays you money is cool for one important reason. Nobody can give you a hard time playing - after all, the game pays money.</p>

<p>I enjoy exploring the similarities between video games and remote software engineering. However, on a serious note, remote work has its own set of pros and cons, which those experienced in this field are well aware of. Having worked remotely for almost two years, I’ve gained some valuable insights and lessons that I want to share in this blog post. I believe these insights could be helpful to others, especially as more people are considering remote work after the Covid era.</p>

<h3 id="trust---the-currency-of-success-in-remote-work">Trust - The Currency of Success in Remote Work</h3>

<p>It is said (source: “So Good They Can’t Ignore You” by Cal Newport) that there are three traits that characterize work done with passion: creativity, impact, and control.</p>

<p>This last trait, control, especially control over our schedule, is what makes remote work so liberating. As long as you deliver quality work, you are (to a large degree) free to plan your day according to your preferences. You have the flexibility to blend your work schedule with your personal life. You are trusted to find your best individual way to contribute to the company. I find this very motivating.</p>

<p>However, working remotely often demands much more discipline and self-organization than an onsite position. In the office, it is often easy to confuse physical presence with productivity. I am fairly convinced that merely showing up to work and producing limited output is enough for people to “survive” for years in some organizations. Even though they do not produce meaningful output, this is not important, as long as they “show up”. <em>Remote work, on the contrary, is more about building trust through continuously producing quality output, in the absence of the physical presence.</em></p>

<p>Do you remember the infamous <a href="https://www.youtube.com/watch?v=6pLLVqqF8VA">Elon Musk ‘laptop class la-la land’ interview</a>? While I disagree with his claim that remote work is immoral, I understand his concern about productivity. At the same time, I also have strong reasons to believe that in many cases, Elon would have no problem hiring a remote software engineer if he could trust them enough.</p>

<p><u>If you are working in a remote setting, you should ensure that your colleagues trust you and can rely on you. Even if you are not online at the moment, they should know that you eventually will be there and not leave them hanging. The goal of building trust is to eliminate potential friction that arises due to the lack of physical presence.</u></p>

<p>There are a few simple rules that I try to enforce every day to make my remote colleagues’ lives easier:</p>

<ul>
  <li>Always try to end your day in a way that allows the people you interact with to seamlessly pick up where you left off.</li>
  <li>Communicate and work asynchronously. If you want to work fully remotely, it is extremely important to learn how to work asynchronously.</li>
  <li>Do not leave people hanging. If someone asks for your help, either help them or communicate that you cannot do so.</li>
</ul>

<p>Being a trustworthy remote colleague means that your team and managers can rely on you. They understand that your good work helps them do good work, even if they do not physically see you doing it.</p>

<div>
<p><img src="https://dtransposed.github.io/assets/17/2.png" width="50%"></p><p>@shl makes some great points here. He often shares lessons learned from running his company, Gumroad, which is 100% remote and async </p></div>

<h3 id="clarity-and-precision-slow-is-smooth-and-smooth-is-fast">Clarity and Precision: Slow is Smooth and Smooth is Fast</h3>

<p>Imagine you’re sitting at your office desk, fiddling with the idea of getting a cup of coffee. Despite this, you manage to quickly send a message to your friend, notifying her that you encountered a bug after her changes. Then you go grab a coffee. If she did not understand your message, you can quickly visit her desk on the way back from the kitchen and explain in more depth what your perhaps somewhat hasty and imprecise message was all about.</p>

<p>However, when you work remotely, you never know if your colleague is online. Even worse, by the time she comes back and reads your message, you may no longer be online. This brings me to my main point:</p>

<p><u>You want to leave your messages in a state where your colleagues can quickly act on them without needing additional clarification. This ties back to the previous point about asynchronous communication. Your messages should leave no room for ambiguity and enable your colleagues to promptly understand what they need to work on.</u></p>

<p>Let me provide a few examples:</p>

<blockquote>
  <p>You are blocked by an error introduced by your colleague’s pull request. You currently can’t fix it, so you want to give her a heads-up that her diff broke the functionality. What should you do?</p>

  <p><strong>Bad</strong>: Leave an imprecise, sloppy message like, *“Hey, X, have you seen this before: [insert the last line from the stack trace]? Please help!”</p>

  <p><strong>Good</strong>: Leave a detailed description of the problem, including the full stack trace (not just the last few lines), along with an explanation of what you were trying to achieve, what your environment is, and how to reproduce the problem.</p>
</blockquote>

<p>or</p>

<blockquote>
  <p>You have a proposal for a feature or a design solution that you want to share with the team.</p>

  <p><strong>Bad</strong>: Leave a convoluted, unstructured, hasty explanation that, ironically, raises more questions.</p>

  <p><strong>Good</strong>: Provide a short, high-level description of what you would like to implement and include a piece of pseudocode, a flowchart, or a diagram. It doesn’t matter whether you use a formal language like UML or any other visual representation. The key is to efficiently compress your ideas into a format that will enable your colleagues to accurately reconstruct your mental model.</p>
</blockquote>

<p>Yes, following this advice may take an extra 10 minutes of your time, but it will save you twice that time by avoiding back-and-forth exchanges with your team. It will also provide nicely documented material for future reference and show that you are a good communicator who is mindful of the time of your teammates. Remember: <u>slow is smooth, and smooth is fast</u>.</p>

<h3 id="embrace-the-small-time-zone-overlap">Embrace the Small Time Zone Overlap</h3>
<p>Very often, in a remote setting, there is a time zone difference between you and your colleagues. If you are working remotely from Brazil and interacting with the European team, your colleagues are most likely finished before your afternoon. Whether you like it or not, they most likely will not be available for a good part of your evening. For some, this may be frustrating. What happens if I get stuck working on a task? Do I need to wait until the next day for my colleagues to step in?</p>

<p><u>From my experience, most people in their work environment struggle with the opposite problem - the lack of time to work on their own</u>. We often have difficulty “blocking off” the “focus time”. We are in a perpetual context-switching loop. In our calendars, we try to schedule time blocks in advance, hoping to muster up half an hour of concentration time. Very often, we go to some isolated part of the office so that we can work alone on the problem at hand. Serendipity at the workplace is cool, but many companies embrace it so much that employees cannot focus on meaningful deep work. I remember suffering from the same problem when working in the office. I was trying all sorts of creative ways to shield myself from distractions. All the attempts at downloading calendar-blocking apps or openly communicating my focus-time hours were quite futile. The final solution was also suboptimal - showing up in the office at 7 a.m. to get some tasks done before most of the employees arrived at 9 a.m.</p>

<p>Most remote workers are blessed with a few hours when they are not being pinged and have no need to check their email/Teams/Slack/etc. They have the comfort of knowing that every day there are at least a few hours free from the beehive buzz. So their schedule is naturally divided between “deep work” done in seclusion and then back-to-back meetings, discussions, and other attention-grabbing interactions. <u>I find the natural division of the daily schedule productive. It allows for enforcing more structure and thus maintaining better control of your time.</u></p>

<blockquote>
  <p>A 40-hour time-blocked work week, I estimate, produces the same amount of output as a 60+ hour work week pursued without structure. - Cal Newport</p>
</blockquote>

<h3 id="meetings-less-is-more">Meetings: Less is More</h3>

<p>One could naturally flip my argument and say that working across time zones (and thus a limited overlap window) may be harmful to the productivity of an organization. There is only a limited time window in which all team members or the larger part of the organization is available for synchronous meetings. I can recognize how this may be a disadvantage: if there are so few opportunities for all people to meet at the same time, how does the information flow through the company, and how can any sensible decisions be made?</p>

<p><u>But then again, have you ever worked in an organization that has suffered from too few meetings? It is mostly too many meetings!</u> Any company, especially one that grows fast, is very fertile soil for unnecessary meetings to grow like weeds. Recently, <a href="https://blog.johnqian.com/startup-spark">@johnlqian wrote a short and nice blog post about how startups lose the spark as they scale</a>. He points out that the proliferation of unnecessary meetings is one of the main factors.</p>

<p>During my time at Tesla, despite the company’s size, I think we did an okay job of protecting our schedules. This was the result of the rules established by Elon: <a href="https://medium.com/@matthew.edgar.ritchie/want-to-spend-less-time-in-meetings-elon-musk-has-the-answer-9ec1d0d5032f">“Excessive meetings are the blight of big companies and almost always get worse over time”</a>. Tobias Lütke, the CEO of Shopify, is also very critical of excessive meetings. Critical to the point where Shopify conducts periodic <a href="https://www.theguardian.com/money/2023/jan/06/work-meetings-shopify-isolation">“calendar purges”</a> to avoid trudging through the swamp of meetings.</p>

<p><u>Excessive meetings are like an anchor that serves its purpose until it no longer does, resulting in slowing down the whole organization. So if you are a remote-first company, embrace the blessing and the curse of a limited time overlap.</u> You most likely will not be able to spin out multiple series of all-hands, dailies, stand-ups, syncs, or war council meetings. <u> Your meeting budget is fixed and modest, so you need to be mindful of how to spend it. This restriction should be positive in the long run. It should coerce organizations to build a sensible and efficient approach toward the quality and quantity of meetings.</u></p>

<h3 id="conclusion">Conclusion</h3>

<p><u>For me, going remote was very rewarding</u>. My commute has decreased by 2 hours daily. I have returned to running marathons, take much better care of myself, and spend much more time with my loved ones. I am also much more productive, and able to disentangle deep work in isolation from times when I need to be available for collaboration and context-switching.</p>

<p><u>Let's also be clear, remote work is not all unicorns and rainbows</u>. The lack of physical contact with other human beings is, personally, my biggest issue. You can no longer take the act of randomly bumping into one another and spontaneous discussions for granted, as you would in the office. I realized how much I miss meeting with my colleagues at 8 a.m. for an objectively mediocre yet delicious office coffee that gains its value from the social context. In the remote setting, socializing among colleagues needs to be actively managed. While some may think that working remotely may be perfect for introverts, it may be worthwhile to be the force that brings the group together. Putting more direct effort into building relationships, setting up pair-programming sessions, or initiating watercooler talk may benefit everyone.</p>

<p>After the first onsite with Neural Magic, where I got to finally meet the people in the flesh, my team’s productivity significantly boosted. <u>Getting to know people and realizing that they are interesting, passionate individuals rather than pixels on a screen improved our communication and collaboration immensely</u>.</p>

<p>I hope that this write-up was useful for you. Remote work is like playing a video game that not only makes money but also hopefully allows you to grow and have a positive impact on the world.</p>


  



  </article>

  <!-- mathjax -->
  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ladder, open source alternative to 12ft.io and 1ft.io (279 pts)]]></title>
            <link>https://github.com/kubero-dev/ladder</link>
            <guid>38161452</guid>
            <pubDate>Mon, 06 Nov 2023 12:04:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kubero-dev/ladder">https://github.com/kubero-dev/ladder</a>, See on <a href="https://news.ycombinator.com/item?id=38161452">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/kubero-dev/ladder/blob/main/assets/pigeon.svg"><img src="https://github.com/kubero-dev/ladder/raw/main/assets/pigeon.svg" width="100px"></a>
</p>
<h2 tabindex="-1" id="user-content-ladder" dir="auto"><a href="#ladder">Ladder</a></h2>
<p dir="auto"><em>Ladder is a web proxy to help bypass paywalls.</em> This is a selfhosted version of <a href="https://1ft.io/" rel="nofollow">1ft.io</a> and <a href="https://12ft.io/" rel="nofollow">12ft.io</a>. It is inspired by <a href="https://github.com/wasi-master/13ft">13ft</a>.</p>
<h3 tabindex="-1" id="user-content-why" dir="auto"><a href="#why">Why</a></h3>
<p dir="auto">Freedom of information is an essential pillar of democracy and informed decision-making. While media organizations have legitimate financial interests, it is crucial to strike a balance between profitability and the public's right to access information. The proliferation of paywalls raises concerns about the erosion of this fundamental freedom, and it is imperative for society to find innovative ways to preserve access to vital information without compromising the sustainability of journalism. In a world where knowledge should be shared and not commodified, paywalls should be critically examined to ensure that they do not undermine the principles of an open and informed society.</p>
<blockquote>
<p dir="auto"><strong>Disclaimer:</strong> This project is intended for educational purposes only. The author does not endorse or encourage any unethical or illegal activity. Use this tool at your own risk.</p>
</blockquote>
<h3 tabindex="-1" id="user-content-features" dir="auto"><a href="#features">Features</a></h3>
<ul>
<li> Bypass Paywalls</li>
<li> Remove CORS headers from responses, assets, and images ...</li>
<li> Apply domain based ruleset/code to modify response</li>
<li> Keep site browsable</li>
<li> API</li>
<li> Fetch RAW HTML</li>
<li> Custom User Agent</li>
<li> Custom X-Forwarded-For IP</li>
<li> <a href="https://github.com/kubero-dev/ladder/pkgs/container/ladder">Docker container</a> (amd64, arm64)</li>
<li> Linux binary</li>
<li> Mac OS binary</li>
<li> Windows binary (untested)</li>
<li> Removes most of the ads (unexpected side effect ¯_(ツ)_/¯ )</li>
<li> Basic Auth</li>
<li> Disable logs</li>
<li> No Tracking</li>
<li> Limit the proxy to a list of domains</li>
<li> Expose Ruleset to other ladders</li>
<li> Optional TOR proxy</li>
<li> A key to share only one URL</li>
<li> Fetch from Google Cache if not available</li>
</ul>
<h3 tabindex="-1" id="user-content-limitations" dir="auto"><a href="#limitations">Limitations</a></h3>
<p dir="auto">Certain sites may display missing images or encounter formatting issues. This can be attributed to the site's reliance on JavaScript or CSS for image and resource loading, which presents a limitation when accessed through this proxy. If you prefer a full experience, please consider buying a subscription for the site.</p>
<p dir="auto">Some sites do not expose their content to search engines, which means that the proxy cannot access the content. A future version will try to fetch the content from Google Cache.</p>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h2>
<blockquote>
<p dir="auto"><strong>Warning:</strong> If your instance will be publicly accessible, make sure to enable Basic Auth. This will prevent unauthorized users from using your proxy. If you do not enable Basic Auth, anyone can use your proxy to browse nasty/illegal stuff. And you will be responsible for it.</p>
</blockquote>
<h3 tabindex="-1" id="user-content-binary" dir="auto"><a href="#binary">Binary</a></h3>
<ol dir="auto">
<li>Download binary <a href="https://github.com/kubero-dev/ladder/releases/latest">here</a></li>
<li>Unpack and run the binary <code>./ladder</code></li>
<li>Open Browser (Default: <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a>)</li>
</ol>
<h3 tabindex="-1" id="user-content-docker" dir="auto"><a href="#docker">Docker</a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8080:8080 -d --name ladder ghcr.io/kubero-dev/ladder:latest"><pre>docker run -p 8080:8080 -d --name ladder ghcr.io/kubero-dev/ladder:latest</pre></div>
<h3 tabindex="-1" id="user-content-docker-compose" dir="auto"><a href="#docker-compose">Docker Compose</a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="curl https://raw.githubusercontent.com/kubero-dev/ladder/main/docker-compose.yaml --output docker-compose.yaml
docker-compose up -d"><pre>curl https://raw.githubusercontent.com/kubero-dev/ladder/main/docker-compose.yaml --output docker-compose.yaml
docker-compose up -d</pre></div>
<h2 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage</a></h2>
<h3 tabindex="-1" id="user-content-browser" dir="auto"><a href="#browser">Browser</a></h3>
<ol dir="auto">
<li>Open Browser (Default: <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a>)</li>
<li>Enter URL</li>
<li>Press Enter</li>
</ol>
<p dir="auto">Or direct by appending the URL to the end of the proxy URL:
<a href="http://localhost:8080/https://www.example.com" rel="nofollow">http://localhost:8080/https://www.example.com</a></p>
<h3 tabindex="-1" id="user-content-api" dir="auto"><a href="#api">API</a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X GET &quot;http://localhost:8080/api/https://www.example.com&quot;"><pre>curl -X GET <span><span>"</span>http://localhost:8080/api/https://www.example.com<span>"</span></span></pre></div>
<h3 tabindex="-1" id="user-content-raw" dir="auto"><a href="#raw">RAW</a></h3>
<p dir="auto"><a href="http://localhost:8080/raw/https://www.example.com" rel="nofollow">http://localhost:8080/raw/https://www.example.com</a></p>
<h3 tabindex="-1" id="user-content-running-ruleset" dir="auto"><a href="#running-ruleset">Running Ruleset</a></h3>
<p dir="auto"><a href="http://localhost:8080/ruleset" rel="nofollow">http://localhost:8080/ruleset</a></p>
<h2 tabindex="-1" id="user-content-configuration" dir="auto"><a href="#configuration">Configuration</a></h2>
<h3 tabindex="-1" id="user-content-environment-variables" dir="auto"><a href="#environment-variables">Environment Variables</a></h3>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>PORT</code></td>
<td>Port to listen on</td>
<td><code>8080</code></td>
</tr>
<tr>
<td><code>PREFORK</code></td>
<td>Spawn multiple server instances</td>
<td><code>false</code></td>
</tr>
<tr>
<td><code>USER_AGENT</code></td>
<td>User agent to emulate</td>
<td><code>Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)</code></td>
</tr>
<tr>
<td><code>X_FORWARDED_FOR</code></td>
<td>IP forwarder address</td>
<td><code>66.249.66.1</code></td>
</tr>
<tr>
<td><code>USERPASS</code></td>
<td>Enables Basic Auth, format <code>admin:123456</code></td>
<td>``</td>
</tr>
<tr>
<td><code>LOG_URLS</code></td>
<td>Log fetched URL's</td>
<td><code>true</code></td>
</tr>
<tr>
<td><code>DISABLE_FORM</code></td>
<td>Disables URL Form Frontpage</td>
<td><code>false</code></td>
</tr>
<tr>
<td><code>FORM_PATH</code></td>
<td>Path to custom Form HTML</td>
<td>``</td>
</tr>
<tr>
<td><code>RULESET</code></td>
<td>URL to a ruleset file</td>
<td><code>https://raw.githubusercontent.com/kubero-dev/ladder/main/ruleset.yaml</code> or <code>/path/to/my/rules.yaml</code></td>
</tr>
<tr>
<td><code>EXPOSE_RULESET</code></td>
<td>Make your Ruleset available to other ladders</td>
<td><code>true</code></td>
</tr>
<tr>
<td><code>ALLOWED_DOMAINS</code></td>
<td>Comma separated list of allowed domains. Empty = no limitations</td>
<td>``</td>
</tr>
<tr>
<td><code>ALLOWED_DOMAINS_RULESET</code></td>
<td>Allow Domains from Ruleset. false = no limitations</td>
<td><code>false</code></td>
</tr>
</tbody>
</table>
<p dir="auto"><code>ALLOWED_DOMAINS</code> and <code>ALLOWED_DOMAINS_RULESET</code> are joined together. If both are empty, no limitations are applied.</p>
<h3 tabindex="-1" id="user-content-ruleset" dir="auto"><a href="#ruleset">Ruleset</a></h3>
<p dir="auto">It is possible to apply custom rules to modify the response. This can be used to remove unwanted or modify elements from the page. The ruleset is a YAML file that contains a list of rules for each domain and is loaded on startup</p>
<p dir="auto">See in <a href="https://github.com/kubero-dev/ladder/blob/main/ruleset.yaml">ruleset.yaml</a> for an example.</p>
<div dir="auto" data-snippet-clipboard-copy-content="- domain: www.example.com
  regexRules:
    - match: <script\s+([^>]*\s+)?src=&quot;(/)([^&quot;]*)&quot;
      replace: <script $1 script=&quot;/https://www.example.com/$3&quot;
  injections:
    - position: head # Position where to inject the code
      append: | 
        <script>
          window.localStorage.clear();
          console.log(&quot;test&quot;);
          alert(&quot;Hello!&quot;);
        </script>
- domain: www.anotherdomain.com # Domain where the rule applies
  paths:                        # Paths where the rule applies
    - /article
  googleCache: false            # Search also in Google Cache
  regexRules:                   # Regex rules to apply
    - match: <script\s+([^>]*\s+)?src=&quot;(/)([^&quot;]*)&quot;
      replace: <script $1 script=&quot;/https://www.example.com/$3&quot;
  injections:
    - position: .left-content article .post-title # Position where to inject the code into DOM
      replace: | 
        <h1>My Custom Title</h1>
    - position: .left-content article # Position where to inject the code into DOM
      prepend: | 
        <h2>Suptitle</h2>"><pre>- <span>domain</span>: <span>www.example.com</span>
  <span>regexRules</span>:
    - <span>match</span>: <span>&lt;script\s+([^&gt;]*\s+)?src="(/)([^"]*)"</span>
      <span>replace</span>: <span>&lt;script $1 script="/https://www.example.com/$3"</span>
  <span>injections</span>:
    - <span>position</span>: <span>head </span><span><span>#</span> Position where to inject the code</span>
      <span>append</span>: <span>| </span>
<span>        &lt;script&gt;</span>
<span>          window.localStorage.clear();</span>
<span>          console.log("test");</span>
<span>          alert("Hello!");</span>
<span>        &lt;/script&gt;</span>
<span></span>- <span>domain</span>: <span>www.anotherdomain.com </span><span><span>#</span> Domain where the rule applies</span>
  <span>paths</span>:                        <span><span>#</span> Paths where the rule applies</span>
    - <span>/article</span>
  <span>googleCache</span>: <span>false            </span><span><span>#</span> Search also in Google Cache</span>
  <span>regexRules</span>:                   <span><span>#</span> Regex rules to apply</span>
    - <span>match</span>: <span>&lt;script\s+([^&gt;]*\s+)?src="(/)([^"]*)"</span>
      <span>replace</span>: <span>&lt;script $1 script="/https://www.example.com/$3"</span>
  <span>injections</span>:
    - <span>position</span>: <span>.left-content article .post-title </span><span><span>#</span> Position where to inject the code into DOM</span>
      <span>replace</span>: <span>| </span>
<span>        &lt;h1&gt;My Custom Title&lt;/h1&gt;</span>
<span></span>    - <span>position</span>: <span>.left-content article </span><span><span>#</span> Position where to inject the code into DOM</span>
      <span>prepend</span>: <span>| </span>
<span>        &lt;h2&gt;Suptitle&lt;/h2&gt;</span></pre></div>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Was Atlantis a Minoan Civilization on Santorini Island? (121 pts)]]></title>
            <link>https://greekreporter.com/2023/10/29/atlantis-minoan-civilisation-santorini/</link>
            <guid>38161286</guid>
            <pubDate>Mon, 06 Nov 2023 11:44:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://greekreporter.com/2023/10/29/atlantis-minoan-civilisation-santorini/">https://greekreporter.com/2023/10/29/atlantis-minoan-civilisation-santorini/</a>, See on <a href="https://news.ycombinator.com/item?id=38161286">Hacker News</a></p>
Couldn't get https://greekreporter.com/2023/10/29/atlantis-minoan-civilisation-santorini/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[FPGA Dev Boards for $150 or Less (161 pts)]]></title>
            <link>https://www.fpgajobs.com/blog/fpga-boards-under-150-dollars/</link>
            <guid>38161215</guid>
            <pubDate>Mon, 06 Nov 2023 11:33:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fpgajobs.com/blog/fpga-boards-under-150-dollars/">https://www.fpgajobs.com/blog/fpga-boards-under-150-dollars/</a>, See on <a href="https://news.ycombinator.com/item?id=38161215">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	  <article>
		  
		  <p>Posted today</p>
		  <p>In the dynamic world of FPGA design, breaking the bank shouldn't be a prerequisite to diving into hands-on experimentation and learning. Fortunately, a plethora of FPGA development boards have emerged that are not only feature-rich, but also surprisingly affordable. For budding enthusiasts, students, or even seasoned developers on a budget, boards priced at $150 or less offer an incredible gateway into the intricate realm of digital design. In this blog post, we'll explore some of the most noteworthy and cost-effective FPGA boards on the market, ensuring that your journey into the FPGA universe is both enriching and wallet-friendly. Join us as we delve into the world of affordable FPGA development boards!</p><p><em>Publisher's Note: if you choose to purchase one of these boards, we would really appreciate it if you purchased using one of the links below. FPGAjobs earns a commission if you purchase through one of those links, which, in turn, helps support the site and its ongoing operation. Thanks a bunch!</em></p><h2>The Sub-$100 Contenders</h2><p>Many of our top picks for FPGA boards are well under $100. Here's four you can consider as a home development platform for RTL design. </p><h2>Lattice iCEstick</h2><p><img src="https://www.mouser.com/images/lattice/lrg/icestick-reva-front-2400.jpg" alt="ICE40HX1K-STICK-EVN Lattice | Mouser"></p><p>The Lattice iCEstick FPGA development board stands out as a compact and user-friendly platform tailored for both hobbyists and professionals venturing into digital design. At the heart of this board lies the Lattice iCE40 FPGA, which offers a versatile environment for a myriad of applications. A significant advantage of the iCEstick is its compatibility with the YoSys open-source FPGA toolchain, allowing developers to leverage a powerful, community-driven suite of tools for synthesis and design exploration. (Unfortunately, that also makes these boards quite popular - and, as a result, hard to get a hold of at times!) This broadens the possibilities for innovation and experimentation: instead of being confined to a hardware description language, you can program your iCEstick's FPGA in Python! Outfitted with an onboard USB programmer, user LEDs, and connectors for external components,  and bundled into a USB flash drive form factor, the iCEstick provides a holistic development experience, making FPGA design more accessible and streamlined for all.</p><p><strong><u>Price:</u></strong> $58.62 </p><p><a href="https://www.amazon.com/Programmable-Logic-Development-Ice40-Hx1K-Icestick/dp/B0131UQVVW/ref=sr_1_2?crid=OT7RW8ZGEKT4&amp;amp;keywords=ice+stick+fpga&amp;amp;qid=1698669354&amp;amp;sprefix=icestick+fpga%252Caps%252C62&amp;amp;sr=8-2&amp;_encoding=UTF8&amp;tag=fpgajobs-20&amp;linkCode=ur2&amp;linkId=dfdf5d1d6048ae55e608c1c466728890&amp;camp=1789&amp;creative=9325" rel="noopener noreferrer" target="_blank">Click Here to buy from Amazon</a></p><p><a href="https://www.newark.com/07X1682?CMP=AFF-CJ" rel="noopener noreferrer" target="_blank">Click Here to buy from Newark</a></p><h2>TinyFPGA BX</h2><p><img src="https://cdn.sparkfun.com//assets/parts/1/3/0/9/8/14829-TinyFPGA_BX_Board-01.jpg" alt="TinyFPGA BX Board - DEV-14829 - SparkFun Electronics"></p><p>The TinyFPGA BX development board is a compact yet powerful platform crafted for hobbyists, educators, and professionals eager to explore the realm of FPGA-based projects. Underpinning this diminutive board is the Lattice iCE40 FPGA, which provides a flexible environment conducive to a range of digital designs. What sets the TinyFPGA BX apart is its emphasis on simplicity and accessibility. It features an onboard USB interface, eliminating the need for external programming tools and ensuring a hassle-free setup. Moreover, its compatibility with open-source toolchains like nextpnr and YoSys enables a community-driven approach to design and development. A standout feature of the TinyFPGA BX is its breadboard-compatible form factor, allowing for easy integration into prototyping environments and facilitating hands-on experimentation.</p><p><strong><u>Price:</u></strong> $49.00 </p><p><a href="https://www.crowdsupply.com/tinyfpga/tinyfpga-ax-bx#products" rel="noopener noreferrer" target="_blank">Click Here to buy from CrowdSupply</a></p><h2>NANDLand Go Board</h2><p><img src="https://nandland.com/wp-content/uploads/2022/12/goboard_solo.jpg" width="635"></p><p>The NANDLand Go Board is an FPGA development board tailored for beginners eager to dive into the realm of digital design. Centered around the Lattice ICE40 HX1K FPGA, the Go Board offers a balanced combination of simplicity and capability, making it an ideal choice for those embarking on their first FPGA journey. One of its standout features is the onboard programmer, eliminating the need for external programming devices and ensuring a smooth user experience. In addition to this, the board is equipped with essential components such as LEDs, push-buttons, and GPIO pins, which facilitate a wide array of beginner to intermediate projects. The Go Board's affordability, paired with a supportive online community and a range of tutorials available on <a href="https://www.nandland.com/" rel="noopener noreferrer" target="_blank">NANDLand's website</a>, ensures that anyone will have the resources they need to kickstart their FPGA adventures.</p><p><strong><u>Price:</u></strong> $70.00 </p><p><a href="https://nandland.com/the-go-board/" rel="noopener noreferrer" target="_blank">Click Here to buy from NANDLand</a></p><h2>Digilent Cmod S7</h2><p><img src="https://uk.farnell.com/productimages/large/en_GB/3050773-40.jpg" alt="410-376 Digilent, Development Board, Cmod S7 Module, Spartan-7 FPGA |  Farnell UK"></p><p>The Digilent Cmod S7 FPGA board is a compact and feature-rich platform, tailor-made for enthusiasts and professionals venturing into FPGA-based digital designs. At its core, the board is powered by the Xilinx Spartan-7 FPGA, striking a balance between high performance and efficient power consumption. A standout feature of the Cmod S7 is its inclusion of a PMOD connector, allowing users to seamlessly integrate a myriad of modular components and expand the board's capabilities. Despite its diminutive size, it also offers an extensive set of I/O pins for versatile interfacing. With a design that's breadboard-compatible, it effortlessly fits into diverse prototyping scenarios. Digilent's dedication to fostering a supportive community and providing robust documentation further amplifies the appeal of the Cmod S7, making it a top choice for both FPGA newcomers and seasoned developers in search of a dynamic, yet compact development tool.</p><p><strong><u>Price:</u></strong> $99.00 - 137.61 </p><p><a href="https://www.amazon.com/Digilent-Cmod-S7-Breadboardable-Spartan-7/dp/B07FRV4JNX/ref=sr_1_1?crid=82NK3GMDWFPY&amp;amp;keywords=digilent+cmod+s7&amp;amp;qid=1698668198&amp;amp;sprefix=digilent+cmod+s7%252Caps%252C77&amp;amp;sr=8-1&amp;amp;ufe=app_do%253Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc&amp;_encoding=UTF8&amp;tag=fpgajobs-20&amp;linkCode=ur2&amp;linkId=fe48c7406b05857f9feaa90df12b9be3&amp;camp=1789&amp;creative=9325" rel="noopener noreferrer" target="_blank">Click Here to buy from Amazon</a></p><p><a href="https://www.newark.com/40AH1677?CMP=AFF-CJ" rel="noopener noreferrer" target="_blank">Click Here to buy from Newark</a></p><h2>Lattice iCE40UP5K-B-EVN</h2><p><img src="https://www.newark.com/productimages/large/en_US/55AJ2586-40.jpg?01AD=3y6_YUXWXej0aJEKVSaDZK9EAtlG6z0hmiGHp0dmwY7H6l4VQ0ynYuw&amp;01RI=2302DB7BBCEEB95&amp;01NA=na" alt="https://www.newark.com/productimages/large/en_US/55AJ2586-40.jpg?01AD=3y6_YUXWXej0aJEKVSaDZK9EAtlG6z0hmiGHp0dmwY7H6l4VQ0ynYuw&amp;01RI=2302DB7BBCEEB95&amp;01NA=na" height="1245" width="1243"></p><p>The Lattice ICE40UP5K-B-EVN evaluation board serves as a pivotal tool for developers keen on exploring the capabilities of the ICE40 UltraPlus FPGA. Designed by Lattice Semiconductor, this board harnesses the power of the ICE40UP5K FPGA, renowned for its low power consumption and high functionality. One of its salient features is its rich array of I/O options and onboard resources, allowing for a comprehensive development and testing environment. Additionally, the board's layout is meticulously crafted to facilitate easy interfacing with various peripherals, ensuring a seamless design experience. Backed by Lattice's robust software tools and extensive documentation, the ICE40UP5K-B-EVN stands out as an invaluable asset for both beginners looking to understand FPGA intricacies and seasoned professionals aiming for advanced experimentation and prototyping. This is a great choice if you're interested in designing a board that needs a very, very small footprint FPGA - the iCE40UP5K is one of the only FPGAs out there with a WLCSP-30 footprint!</p><p><strong><u>Price:</u></strong> $64.28 </p><p><a href="https://www.amazon.com/LATTICE-SEMICONDUCTOR-ICE40UP5K-B-EVN-UltraPlus-Evaluation/dp/B06Y2SS8PP/ref=sr_1_1_mod_primary_new?crid=4PT8RD4ZWP7W&amp;amp;keywords=iCE40UP5K-B-EVN&amp;amp;qid=1698669064&amp;amp;sbo=RZvfv%252F%252FHxDF%252BO5021pAnSA%253D%253D&amp;amp;sprefix=ice40up5k-b-evn%252Caps%252C138&amp;amp;sr=8-1&amp;_encoding=UTF8&amp;tag=fpgajobs-20&amp;linkCode=ur2&amp;linkId=ceb59ef6d702b4282075a82acf5fc49b&amp;camp=1789&amp;creative=9325" rel="noopener noreferrer" target="_blank">Click Here to buy from Amazon</a></p><p><a href="https://www.newark.com/lattice-semiconductor/ice40up5k-b-evn/breakout-board-ice40-ultraplus/dp/55AJ2586" rel="noopener noreferrer" target="_blank">Click Here to buy from Newark</a></p><h2>For a Few Dollars More</h2><p>We originally wanted this post to be about FPGA boards that were $100 or less, but upon some research, it became clear that a few important options to consider in the $100 to $150 range. Here's a few  boards you may want to consider if you can spare another $50 in your search for an FPGA evaluation board. </p><h2>Terasic DE0-Nano</h2><p><img src="https://4.bp.blogspot.com/-03HcoVVaBU4/ViQhC5jwB9I/AAAAAAAADaY/5rEPb6tpQzg/s1600/DE0nano.jpg" alt="https://4.bp.blogspot.com/-03HcoVVaBU4/ViQhC5jwB9I/AAAAAAAADaY/5rEPb6tpQzg/s1600/DE0nano.jpg"></p><p>If you are dead set on your development platform running Intel silicon, then the Terasic DE0-Nano is for you. The DE0-Nano FPGA board is a compact and user-friendly platform designed to provide students, hobbyists, and FPGA enthusiasts with a cost-effective introduction to digital logic and hardware design. Built around Altera's Cyclone IV E FPGA, the DE0-Nano offers a rich set of features that make it suitable for a wide range of applications, from simple educational projects to more advanced prototyping. The board is equipped with a plethora of I/O options, including GPIO headers, an accelerometer, LEDs, and buttons. Its lightweight nature and onboard USB-Blaster for programming and debugging make it especially convenient for on-the-go development. Accompanied by robust documentation and a supportive community, the DE0-Nano stands as a popular choice for those diving into the world of FPGAs. </p><p><strong><u>Price:</u></strong> $117.26</p><p><a href="https://www.newark.com/78T4479?CMP=AFF-CJ" rel="noopener noreferrer" target="_blank">Click Here to buy from Newark</a></p><h2>Digilent Arty S7</h2><p><img src="https://m.media-amazon.com/images/I/71gYR80OU6L.jpg" alt="Amazon.com: Digilent Arty S7: Spartan-7 FPGA Board for Makers and Hobbyists  (Arty S7-25) : Electronics"></p><p>The Arty S7 FPGA development board is a modern and adaptable platform tailored for designers and enthusiasts eager to delve into FPGA-centric digital projects. At its core, the Xilinx Spartan-7 FPGA powers the board, offering a harmonious blend of performance and energy efficiency, suitable for diverse applications from embedded systems to intricate digital signal processing. One of the standout features of the Arty S7 is its compatibility with both Arduino shields and PMOD evaluation boards, allowing developers to seamlessly integrate a wide array of modules and expand functionality. Additionally, the board comes equipped with DDR3 memory, a USB-UART bridge, and an expansive set of I/O connectors. The seamless integration with the Vivado Design Suite further enhances the development experience, facilitating efficient hardware synthesis and debugging. Given its multifaceted compatibility and robust support resources, the Arty S7 emerges as a top-tier choice for FPGA enthusiasts.</p><p><strong><u>Price:</u></strong> $119.00 </p><p><a href="https://www.amazon.com/Digilent-Arty-S7-Spartan-7-Hobbyists/dp/B07C24TD86/ref=sr_1_fkmr0_1?crid=BNCFMQOP1VW2&amp;amp;keywords=digilent%252Barty%252Bs7&amp;amp;qid=1698669265&amp;amp;sprefix=digilent%252Baraty%252Caps%252C67&amp;amp;sr=8-1-fkmr0&amp;amp;ufe=app_do%253Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc&amp;amp;th=1&amp;_encoding=UTF8&amp;tag=fpgajobs-20&amp;linkCode=ur2&amp;linkId=6e30410f4c5b617020e1e0a077a58689&amp;camp=1789&amp;creative=9325" rel="noopener noreferrer" target="_blank">Click Here to buy from Amazon</a></p><p><a href="https://digilent.com/shop/arty-s7-spartan-7-fpga-development-board/" rel="noopener noreferrer" target="_blank">Click Here to buy from Digilent</a></p><h2>Digilent Cora Z7</h2><p><img src="https://kamami.pl/32665-large_default/digilent-cora-z7-10-kit-410-370-1-.jpg" alt="Digilent Cora Z7-10 Kit (410-370-1) - Kamami on-line store"></p><p>The Digilent Cora Z7 development board stands out as a dual-purpose platform meticulously crafted for both FPGA design and embedded Linux development. At its heart is the Xilinx Zynq-7000 series SoC, a powerhouse that seamlessly marries ARM Cortex processors with programmable logic, facilitating a unique environment where software programmability intersects with hardware optimization. This versatility ensures that the Cora Z7 caters to a broad spectrum of developers, from those focused on FPGA-centric projects to those diving into the nuances of embedded Linux applications. In addition to its core capabilities, the board is replete with features such as DDR3 memory and an array of I/O options. Digilent's commitment to creating user-friendly tools further enhances the Cora Z7's appeal, positioning it as an invaluable asset for both newcomers and seasoned professionals in the realms of FPGA and embedded Linux development.</p><p><strong><u>Price:</u></strong> $149.00</p><p><a href="https://www.amazon.com/Digilent-Cora-Z7-Zynq-7000-Development/dp/B07FP1VXRS/ref=sr_1_1?crid=27ESYHECK4Q2E&amp;amp;keywords=digilent+cora+z7&amp;amp;qid=1698669185&amp;amp;sprefix=digilent+cora+z7%252Caps%252C46&amp;amp;sr=8-1&amp;amp;ufe=app_do%253Aamzn1.fos.f5122f16-c3e8-4386-bf32-63e904010ad0&amp;_encoding=UTF8&amp;tag=fpgajobs-20&amp;linkCode=ur2&amp;linkId=f8193f95f8b55ae732ac8629150cb7e2&amp;camp=1789&amp;creative=9325" rel="noopener noreferrer" target="_blank">Click Here to buy from Amazon</a></p><p><a href="https://www.newark.com/digilent/410-370/cora-z7-07s-zynq-7000-single-core/dp/40AH1674" rel="noopener noreferrer" target="_blank">Click Here to buy from Newark</a></p>
	  </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Pays $21B for Search Monopoly: How "Free" Tech Markets Repress (128 pts)]]></title>
            <link>https://tutanota.com/blog/google-search-monopoly</link>
            <guid>38161198</guid>
            <pubDate>Mon, 06 Nov 2023 11:30:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tutanota.com/blog/google-search-monopoly">https://tutanota.com/blog/google-search-monopoly</a>, See on <a href="https://news.ycombinator.com/item?id=38161198">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Google has built a huge monopoly - one that is finally being investigated at by the US Department of Justice. It
now became public that Google paid $21 Billion in 2021 alone to keep its search engine market share of staggering 90%. This is just a glimpse
of how much your data and attention is worth to Google. It's about time that we take back our data!
</p><div><h2 id="google-search-monopoly-about-to-end">Google Search Monopoly About to End?</h2>
<p>If you have purchased a new technological device with an internet browser, it may come as no surprise when you first open a
browser to search and are greeted with the well-known multicolored G logo over a white background.</p>
<p><strong>Recent reports have shown that Google search
holds a startling 90% market share against all other search engines.</strong></p>
<p>This status has landed them in hot water and an ongoing
Department of Justice investigation into Google's business practices revealed that in 2021 <strong>Google paid Apple alone over
$18 Billion USD to be the default search engine in iOS</strong>, and a total of $21 Billion USD to other companies to make sure that
it came pre-installed across multiple browsers and devices.</p>
<p>That first initial search, even if it is for an alternative search
engine like DuckDuckGo, if launched through Google Search displays ads and in turn generates profits for Alphabet. This may
not seem like a big deal, but if every initial search generates revenue, the global market for that first single search is
immense. With margins like this at stake, it is clear why Google stoops to anti-competitive business practices in order
to stack the deck in their favor. In addition, most people will never change the pre-installed search engine.</p>
<p>This kind of practice isn't new to tech and in the early 1990's the <a href="https://en.wikipedia.org/wiki/United_States_v._Microsoft_Corp.">US government took Microsoft to trial</a> with particular
emphasis on their antitrust behaviour in bundling Internet Explorer in their Windows operating system, claiming that this took steps to prevent
competition in the growing "Browser Wars". If this practice warranted major litigation against Microsoft, then there is
no reason that Google's paid positioning of their product on third-party devices should justify the same type of inquiry.</p>
<p>Luckily, the <a href="https://en.wikipedia.org/wiki/United_States_v._Google_LLC_(2023)">United States vs Google LLC.</a>
case was opened in October 2020 and after three years we are finally seeing the
heads of Google having to testify in court and how they attempt to justify their business practices.</p>
<p>This is not the first investigation for Google, of course. In the EU, there's an ongoing debate whether
<a href="https://tutanota.com/blog/posts/is-google-analytics-illegal">Google Analytics can be legally used by EU companies</a>; here it is not about
monopolistic issues, but whether using Google Analytics violates the European privacy regulation GDPR.</p>
<p>
		<picture>
   			<source type="image/webp" srcset="https://tutanota.com/blog/images/google-monopoly-game.webp">
    		<img height="771" width="1280" loading="lazy" alt="Google holds the monopoly for search - and it pays huge amounts to keep it." src="https://tutanota.com/blog/images/google-monopoly-game.jpg">
		</picture>
<em>Everyone knows that Google is the No 1 search engine. But now we will find out that its not due to its great results, but simply because Alphabet pays to be the default.</em></p>
<h3 id="anti-competition-hurts-markets">Anti-Competition Hurts Markets</h3>
<p>Google operates primarily from its Googleplex epicenter in Mountain View, California. The business mantra of the United
States is that free markets are the only way the economy can continue to generate profits and allow for continued growth.
Unfortunately, Big Tech along with major companies operating within the Finance, Industry, and Real Estate (FIRE) industries
have discovered <strong>the advantages of abandoning free market competition in favor of pushing for shared monopoly markets, and so has Google</strong>. This is easy
to see in the field of smart phones. Which operating system would you like to purchase? <strong>You of course have choices as a consumer,
between two parties Apple or Android. Are you thirsty? Why not choose between a Pepsi or a Coke?!</strong> Yes, there are other great
alternatives like GrapheneOS (built on Android) but they can only operate on a donation model and, due to the dominant presence
of Google's Android, stand no chance to make headway against the tech giant.</p>
<p>There is a long standing practice in the tech industry of <strong>buying out competition</strong> instead of allowing them to grow. This
practice effectively snubs out market threats while not requiring the larger company to make any major changes in offering a
better product, major innovation, or lowering their prices. Google's monopoly style approach to business is not new, but
the step of actively bribing other hardware and software manufacturers to pushing their product stifles innovation and
prevents alternative search engines which may not have as deep pockets from being offered a seat at the table.</p>
<p>This moves beyond pushing to influence the behavior of other for-profit companies, even <strong>Firefox which is developed and
supported by the Mozilla Foundation, a non-profit, ships with Google - the monopolist - as the default search provider</strong>.</p>
<p>You can of course
change this under the settings options - even when using Android or Google Chrome. But let's be honest: Who changes the default
search engine after
purchasing a new device?</p>
<p>However, if you think the fact that these options are available makes this court case
moot, you need to honestly ask yourself: would your less-technically capable family or friends know how to access the settings to
look for alternative search engine and replace Google with something else?</p>
<p><strong>There are also many alternative ringtones for iOS calls and message notifications, but
how often do you hear those default pings when your out and about?</strong></p>
<p>Just having the option to change the deaulft is something else entirely than offering a different default or making people
pick their preferences upon launching a new device.</p>
<p>
		<picture>
   			<source type="image/webp" srcset="https://tutanota.com/blog/images/screenshot-change-default-search-engine-break-google-monopoly.webp">
    		<img height="324" width="700" loading="lazy" alt="Screenshot of changing the default search engine from Google to another" src="https://tutanota.com/blog/images/screenshot-change-default-search-engine-break-google-monopoly.png">
		</picture></p>
<h3 id="anti-competition-hurts-customers">Anti-Competition Hurts Customers</h3>
<p>Not only do these monopolistic practices harm the natural development of innovation within the tech industry, but
they also shift corporate goals with many startups not seeking to become their own large business themselves, but become
noticable enough that Google buys them out. In these cases Google has positioned themselves as a major customer whose
interests far outweigh those of the average personal customer who may be looking to purchase a product.</p>
<p><strong>Beyond providing a search function Google Search has turned you the customer into a product for advertising.</strong></p>
<p>Your data
does not stay with Google. You receive a nice list of potential dinner recipes while Google sells your personal data
to advertising agencies. This can include your sex, gender, age, location, not to mention any information gathered
from your search history. The internet should not be this way.</p>
<h3 id="freedom-of-competition-is-necessary-for-freedom-of-speech">Freedom of Competition is Necessary for Freedom of Speech</h3>
<p>Google is a business, and its search function operates in a way that generates revenue, which can lead to users
becoming digitally isolated in what Eli Pariser termed a "Filter Bubble". If you are logged into a Google account
while searching, the search results will be catered to your personal interest which can have devastating consequences.
This kind of algorithm can effectively split a populus and result in parallel echo chambers which prevent proper dialogue
over shared sources of information.</p>
<p>These practices not only put the customer at a disadvantage due to the vendor lock-in that they find themselves living in,
but it also has major impacts on the freedom of speech for internet users. If Google Search accounts for 90% of web searches
worldwide, than these users are subject to the content policies which best fit the philosophy and ultimate profit goals of
Alphabet. <strong>If a single corporate entity is solely responsible for the information that the public receives, there is a fine
line to how this information is presented without having negative consequences in the function of democratic societies.</strong></p>
<p>If
Google were to decide one day that some kind of story shouldn't be presented in search queries, that would mean 90% of
internet searches would not provide links to this information. This isn't a dystopian paranoia either, when presented
with such requests at the behest of the Chinese Communist Party (CCP), Google willingly bends over backwards to remove
historical information from searches that have been deemed harmful to the image of the CCP.</p>
<p>Not only does this type of practice and Google's monopoly harm our freedom of speech, even our language itself cannot escape the massive
presence of Google with the name itself becoming synonymous with "to look up" or "search".</p>
<p>In the end, these issues would not magically go away if the US courts actually break up Google's monopolistic power. But at least
less-problematic alternative search engines would finally get a chance of becoming mainstream.</p>
<h3 id="break-free-of-big-tech">Break Free of Big Tech</h3>
<p>If you are looking to break free of Google's monopoly as a search engine, there are some great alternatives to
choose from. To name a quick few, we recommend you check out DuckDuckGo and Ecosia.</p>
<h4 id="duckduckgo">DuckDuckGo</h4>
<p><img height="237" width="300" alt="Logo of DuckDuckGo search engine - a popular Google alternative" src="https://tutanota.com/blog/images/DuckDuckGo_logo.png"></p><p>DuckDuckGo (DDG) has built itself to be a major player in the search engine space. They are now offered in the settings menus from many browsers by default
and can be set as your primary search engine. At DuckDuckGo your privacy matters and they cater their searches with this in mind. They also offer
their own browser with great built-in privacy tools and can also provide @duck.com email address aliases which play well with your secure Tutanota account.
I, myself, use DDG as my primary search engine and I can full-heartedly recommend them.</p>
<h4 id="ecosia">Ecosia</h4>
<p><img height="101" width="400" alt="Logo of Ecosia - a Google search alternative from Germany focussing on sustainability" src="https://tutanota.com/blog/images/Logo_Ecosia_2022.png"></p><p>Ecosia is a German-based search engine which rose to fame with their eco-friendly image. Your searches are funded by ad revenue, but a chunk
of this revenue is spent planting trees, over 185 million at the time of writing, and promoting other climate friendly causes. The registered
B-Corp combines search results from Yahoo, Bing, and Wikipedia to provide you with accurate results while supporting a great cause.
They have also made their way to the status of being a major player and can be manually added as a default search engine in Chrome, Safari, and Brave Browser.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Only time will tell how the US judicial inquiry into Google's monopoly in the search market plays out, but we as consumers can
already take steps to push back against Google
and other Big Tech monopolies by utilizing some of the great privacy-first alternatives. Check out some of our search
engine recommendations, why not use LibreOffice instead of <a href="https://tutanota.com/blog/posts/microsoft-office-365-email-alternative">Microsoft's telemetry ridden office suite</a>, and of course
you can always exchange the <a href="https://tutanota.com/blog/posts/google-calendar-alternative">Google calendar with Tutanota</a> and drop Gmail by <a href="https://tutanota.com/">creating your secure Tutanota account</a>!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The first stable release of a memory safe sudo implementation (294 pts)]]></title>
            <link>https://www.memorysafety.org/blog/sudo-first-stable-release/</link>
            <guid>38161016</guid>
            <pubDate>Mon, 06 Nov 2023 10:58:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.memorysafety.org/blog/sudo-first-stable-release/">https://www.memorysafety.org/blog/sudo-first-stable-release/</a>, See on <a href="https://news.ycombinator.com/item?id=38161016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><p>Josh Aas<br>Aug 29, 2023</p></header><article><p>Prossimo is pleased to announce the <a href="https://crates.io/crates/sudo-rs">first stable release</a> of <a href="https://github.com/memorysafety/sudo-rs">sudo-rs</a>, our Rust rewrite of the critical sudo utility.</p><p>The sudo utility is one of the most common ways for engineers to cross the privacy boundary between user and administrative accounts in the ubiquitous Linux operating system. As such, its security is of the utmost importance.</p><p>The <a href="https://www.memorysafety.org/initiative/sudo-su/">sudo-rs project</a> improves on the security of the original sudo by:</p><ul><li><p>Using a memory safe language (Rust), as it's estimated that one out of three security bugs in the original sudo have been memory management issues</p></li><li><p>Leaving out less commonly used features so as to reduce attack surface</p></li><li><p>Developing an extensive test suite which even managed to <a href="https://ferrous-systems.com/blog/testing-sudo-rs/">find bugs in the original sudo</a></p></li></ul><p>The Wolfi Linux OS already includes sudo-rs and we hope that others will follow their lead. "When we first set out to build Wolfi, making sure it was memory safe was always a top priority," said Dan Lorenc, CEO and Co-founder at Chainguard. "The sudo utility is a perfect example of a security-critical tool that's both pervasive and under-appreciated. Security improvements to tools like this will have an outsized impact on the entire industry. The work that went into building the first sudo-rs release is a great step forward in eliminating potential security issues by adopting memory safe languages like Rust. This is critical for upholding and maintaining Wolfi as the secure-by-default foundation for developers who want to address most modern supply chain threats."</p><p>A joint team from <a href="https://tweedegolf.nl/">Tweede Golf</a> and <a href="https://ferrous-systems.com/">Ferrous Systems</a> built sudo-rs under contract with Prossimo. We're pleased with how much progress they've made since <a href="https://www.memorysafety.org/blog/sudo-and-su/">starting this project</a> in December, 2022. An external security audit of the sudo-rs code is scheduled to start in September 2023. After that, the team will start on Milestone 4 of our <a href="https://www.memorysafety.org/initiative/sudo-su/sudo-su-work-plan/">work plan</a>, which focuses on enterprise features.</p><p>The original <a href="https://www.sudo.ws/">C-based sudo utility</a> has been maintained by Todd C. Miller for many years now, and we're grateful to him for taking on this huge and important task. We're also grateful that Todd has made time to offer us excellent advice on implementing sudo-rs.</p><p>Prossimo is able to take on the challenging work of rewriting critical components of the Internet thanks to our community of funders from around the world. We’d like to thank the NLnet Foundation for their funding of the audit of Sudo-rs. We'd also like to thank Amazon Web Services for supporting this work and supporting the transition to memory safe software.</p><p>ISRG is a 501(c)(3) nonprofit organization that is 100% supported through the generosity of those who share our vision for ubiquitous, open Internet security. If you'd like to support our work, please consider <a href="https://www.abetterinternet.org/getinvolved/">getting involved</a>, <a href="https://www.abetterinternet.org/donate/">donating</a>, or encouraging your company to <a href="https://www.abetterinternet.org/sponsor/">become a sponsor</a>.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg is getting better with multithreaded transcoding pipelines (273 pts)]]></title>
            <link>https://twitter.com/FFmpeg/status/1721275669336707152</link>
            <guid>38160703</guid>
            <pubDate>Mon, 06 Nov 2023 10:12:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/FFmpeg/status/1721275669336707152">https://twitter.com/FFmpeg/status/1721275669336707152</a>, See on <a href="https://news.ycombinator.com/item?id=38160703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p><br></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia admin unmasks self as sockpuppet of other admin banned in 2015 (165 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-11-06/Arbitration_report</link>
            <guid>38160425</guid>
            <pubDate>Mon, 06 Nov 2023 09:29:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-11-06/Arbitration_report">https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2023-11-06/Arbitration_report</a>, See on <a href="https://news.ycombinator.com/item?id=38160425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>A <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2015-02-18/Arbitration_report" title="Wikipedia:Wikipedia Signpost/2015-02-18/Arbitration report">2015 arbitration report</a> in this very periodical said "it was a matter of deep concern" that an abusive editor who had obtained administrator privileges "was able to fool the community for so long". At that time, they were banned by the Arbitration Committee following a long case. We are sad to report that, not only did the abuse not stop in 2015, but the same person managed to obtain a second administrator account, and was just discovered a few days ago.
</p>
<h3><span id="November_1_case_request_and_startling_admission" data-mw-thread-id="h-November_1_case_request_and_startling_admission-signpost-article-title"><span data-mw-comment-start="" id="h-November_1_case_request_and_startling_admission-signpost-article-title"></span>November 1 case request and startling admission<span data-mw-comment-end="h-November_1_case_request_and_startling_admission-signpost-article-title"></span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/User:Beeblebrox" title="User:Beeblebrox">Beeblebrox</a> opened a request for arbitration against administrator <a href="https://en.wikipedia.org/wiki/User:Lourdes" title="User:Lourdes">Lourdes</a> on 1 November, claiming misdeeds including <q>administrative blackmail</q> — bullying other less-privileged editors over their votes during a recent request for adminship. With the case request around one day old, on 2 November, the respondent suddenly <a href="https://en.wikipedia.org/wiki/Special:Diff/1183105704" title="Special:Diff/1183105704">stated</a> that they are the site-banned former admin, Wifione. The case request was <a href="https://en.wikipedia.org/wiki/Special:Diff/1183170493" title="Special:Diff/1183170493">closed as moot</a> following Lourdes' admission. 
</p><p>One of the contributors to the case, <a href="https://en.wikipedia.org/wiki/User:Kurtis" title="User:Kurtis">Kurtis</a>, asked "Is this an ArbCom case request or an M. Night Shyamalan movie?" Others, like arbitrator Moneytrees in the quote above, were more to-the-point.
</p>
<h3><span id="Wifione_background" data-mw-thread-id="h-Wifione_background-signpost-article-title"><span data-mw-comment-start="" id="h-Wifione_background-signpost-article-title"></span>Wifione background<span data-mw-comment-end="h-Wifione_background-signpost-article-title"></span></span></h3>
<p>If you have read our <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2015-02-18/Arbitration_report#In_depth:_Wifione" title="Wikipedia:Wikipedia Signpost/2015-02-18/Arbitration report">prior coverage</a> of how the Wifione siteban came to be, amidst allegations of paid editing while holding the admin bit, you can probably skip over this section. 
</p><p>According to the 2015 Arbcom case, the <a href="https://en.wikipedia.org/wiki/Wikipedia:Arbitration/Requests/Case/Wifione#Sock_puppetry" title="Wikipedia:Arbitration/Requests/Case/Wifione">oldest known account</a> used by the individual also known as Wifione was created in 2006. They created dozens of sock accounts, which were revealed by a <a href="https://en.wikipedia.org/wiki/Wikipedia:Requests_for_checkuser/Case/Mrinal_Pandey" title="Wikipedia:Requests for checkuser/Case/Mrinal Pandey">2008 checkuser request</a>.
</p><p>That prior account was later linked to another account called Wifione, which was created in 2009 and that had become a Wikipedia admin in 2010. The Arbitration committee case found that Wifione was engaging in search engine optimization related to an Indian educational firm. Wifione was <a href="https://en.wikipedia.org/wiki/Wikipedia:SBAN" title="Wikipedia:SBAN">sitebanned</a> as part of the case resolution.
</p>
<h3><span id="An_admin_called_.22Lourdes.22"></span><span id="An_admin_called_&quot;Lourdes&quot;" data-mw-thread-id="h-An_admin_called_&quot;Lourdes&quot;-signpost-article-title"><span data-mw-comment-start="" id="h-An_admin_called_&quot;Lourdes&quot;-signpost-article-title"></span>An admin called "Lourdes"<span data-mw-comment-end="h-An_admin_called_&quot;Lourdes&quot;-signpost-article-title"></span></span></h3>
<p>This <a href="https://en.wikipedia.org/wiki/Wikipedia:LTA" title="Wikipedia:LTA">long-term abuser</a> created the Lourdes account in late 2015, initially under a different name. In 2016 they renamed the account. They were most active in 2016–17, and ran an unsuccessful, self-nominated request for adminship in early 2017; a second attempt in 2018 <a href="https://en.wikipedia.org/wiki/Special:PermanentLink/827679457" title="Special:PermanentLink/827679457">was successful</a> with 207 in favor and 3 opposed. The account went mostly unused for 2020 through 2022, with many months of total editorial inactivity, although it continued to perform admin actions. In 2023, they returned to regularly editing the English Wikipedia.
</p><p>Throughout their tenure, they made 2,282 admin actions, according to <a href="https://en.wikipedia.org/wiki/User:JamesR/AdminStats" title="User:JamesR/AdminStats">User:JamesR/AdminStats</a>.
</p><p>The <a href="https://en.wikipedia.org/wiki/Special:PermanentLink/1183169230" title="Special:PermanentLink/1183169230">arbitration case request</a> filed this month alleged that Lourdes engaged in egregious abuse of their administrator status during a recent request for adminship, including the following (described by one of the contributors to the request as "the kind of thinly veiled threat you'd expect to hear in <i>The Godfather</i>"):
</p>
<blockquote><p>Because I remember having acted on your complaints at ANI a few times, and on the basis of that connect and support that I gave you, I am requesting you to reconsider your stand
<br><span>—&nbsp;Lourdes, at <a href="https://en.wikipedia.org/wiki/Special:PermanentLink/1181782012" title="Special:PermanentLink/1181782012">the case request</a></span></p></blockquote>
<p>In response, they gave an admission nobody expected:
</p>
<blockquote><p>I am <a href="https://en.wikipedia.org/wiki/User:Wifione" title="User:Wifione">User:Wifione</a>, the admin who got blocked years ago. 
</p><p>My RL identity has nothing to do with any celebrity or anyone like that. I am not writing this to have any final laugh. It's just that I feel it appropriate to place it here specially for Beeblebrox, who I almost emotionally traumatised over the years with the aforementioned double sleight -- aka, pulling him around for revealing my so-called identity. It also required double-doxxing myself on at least one external project, namely Wikipediocracy, which even placed mentions of my name in the private section to protect my identity.
</p>
<p><br><span>—&nbsp;Lourdes, at <a href="https://en.wikipedia.org/wiki/Special:Diff/1183105704" title="Special:Diff/1183105704">the case request</a></span></p></blockquote>
<p>And blocked themselves indefinitely:
</p>
<blockquote><p><span><a href="https://en.wikipedia.org/w/index.php?title=Special:Log&amp;logid=154542088">2023-11-01T22:47:55</a> <a href="https://en.wikipedia.org/wiki/User:Lourdes" title="User:Lourdes">User:Lourdes</a> (<a href="https://en.wikipedia.org/wiki/User_talk:Lourdes" title="User talk:Lourdes">talk</a> | <a href="https://en.wikipedia.org/wiki/Special:Contributions/Lourdes" title="Special:Contributions/Lourdes">contribs</a>) blocked <a href="https://en.wikipedia.org/wiki/User:Lourdes" title="User:Lourdes">User:Lourdes</a> (<a href="https://en.wikipedia.org/wiki/User_talk:Lourdes" title="User talk:Lourdes">talk</a> | <a href="https://en.wikipedia.org/wiki/Special:Contributions/Lourdes" title="Special:Contributions/Lourdes">contribs</a>) with an expiration time of indefinite (account creation blocked, email disabled) (Abusing <a href="https://en.wikipedia.org/wiki/Wikipedia:Sockpuppetry" title="Wikipedia:Sockpuppetry">multiple accounts</a>)</span>
</p></blockquote>
<p>All of the details of the request and the statements made there — which arbitrators voted to decline as pointless soon after the revelations and the self-block — can be seen at <a href="https://en.wikipedia.org/wiki/Special:PermanentLink/1183169230" title="Special:PermanentLink/1183169230">its last revision link</a>.
</p>
<h3><span id="Aftermath" data-mw-thread-id="h-Aftermath-signpost-article-title"><span data-mw-comment-start="" id="h-Aftermath-signpost-article-title"></span>Aftermath<span data-mw-comment-end="h-Aftermath-signpost-article-title"></span></span></h3>
<p>Nobody is quite sure what to make of this. How did they get away with this for so long? How did they conceal it this well? How did nobody notice? What was the point of spending years as a productive administrator, making tens of thousands of edits and logging thousands of actions, to implode the whole thing over a pointless argument on an RfA talk page?
</p><p><i>The Signpost</i>'s sources have confirmed that the particular <a href="https://en.wikipedia.org/wiki/Wikipedia:BADSITES" title="Wikipedia:BADSITES">BADSITE</a> mentioned in Lourdes' final message has indeed discussed this issue, and that both Beeblebrox and the disgraced LTA have posted more about the events, but the <a rel="nofollow" href="https://wikipediocracy.com/forum/viewtopic.php?f=38&amp;t=13187">thread over there</a> doesn't make a whole lot of sense either.
</p><p>In short: what?
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debian discusses vendoring again (2021) (109 pts)]]></title>
            <link>https://lwn.net/Articles/842319/</link>
            <guid>38160382</guid>
            <pubDate>Mon, 06 Nov 2023 09:22:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/842319/">https://lwn.net/Articles/842319/</a>, See on <a href="https://news.ycombinator.com/item?id=38160382">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
           <div><b>LWN.net needs you!</b><p>Without subscribers, LWN would simply not exist.  Please consider
       <a href="https://lwn.net/subscribe/">signing up for a subscription</a> and helping
       to keep LWN publishing</p></div>
           </center>
           
<p>
The problems with "vendoring" in packages—bundling dependencies rather than
getting them from other packages—seems to crop up frequently these days.
We looked at Debian's <a href="https://lwn.net/Articles/835599/">concerns</a> about
packaging <a href="https://kubernetes.io/">Kubernetes</a> and its myriad of Go
dependencies back in October. A more recent discussion in that
distribution's community looks at another famously dependency-heavy
ecosystem: JavaScript libraries from the <a href="https://www.npmjs.com/">npm</a> repository.  Even C-based ecosystems
are not immune to the problem, as we <a href="https://lwn.net/Articles/836911/">saw with
iproute2 and libbpf</a> back in November;  the discussion of vendoring seems
likely to recur over the coming years. 
</p>

<p>
Many application projects, particularly those written in languages like
JavaScript, PHP, and Go, tend to have a rather large pile of
dependencies.  These projects typically simply download specific
versions of the needed dependencies at build time.  This works well for fast-moving
projects using collections of fast-moving libraries and frameworks, but it
works rather less well for traditional Linux distributions.  So
distribution projects have been trying to figure out how best to incorporate these
types of applications.
</p>

<p>
This time around, Raphaël Hertzog <a href="https://lwn.net/ml/debian-devel/X9pReHUtn9LWBu+B@home.ouaza.com/">raised the
issue</a> with regard to the <a href="https://docs.greenbone.net/src/gsa/7.0/index.html">Greenbone Security
Assistant</a> (gsa), which provides a web front-end to the <a href="https://www.openvas.org/">OpenVAS</a> vulnerability scanner (which is
now known as Greenbone Vulnerability Management or gvm).
</p><div><p>
[...]  the
version currently in Debian no longer works with the latest gvm
so we have to update it to the latest upstream release... but the
latest upstream release has significant changes, in particular
it now relies on yarn or npm from the node ecosystem to download
all the node modules that it needs (and there are many of them,
and there's no way that we will package them individually).
</p><p>
The Debian policy forbids download during the build so we can't
run the upstream build system as is.
</p></div>


<p>
Hertzog suggested three possible solutions: collecting all of
the dependencies into the Debian source package (though there would be
problems creating the <a href="https://www.debian.org/doc/packaging-manuals/copyright-format/1.0/"><tt>copyright</tt>
file</a>), moving the package to the
<a href="https://www.debian.org/doc/debian-policy/ch-archive#s-contrib">contrib
repository</a> and adding a post-install step to download the dependencies, or
removing gsa from
Debian entirely.  He is working on updating gsa as part of his work on
<a href="https://www.kali.org/">Kali Linux</a>, which is a Debian
derivative that is focused on penetration testing and security auditing.
Kali Linux does not have the same restrictions on downloading during builds
that Debian has, so the Kali gsa package can simply use the upstream build
process.
</p>

<p>
He would prefer to keep gsa in Debian, "<q>but there's only so
much busy-work that I'm willing to do to achieve this goal</q>".  He
wondered if it made more sense for Debian to consider relaxing its
requirements.  But Jonas Smedegaard <a href="https://lwn.net/ml/debian-devel/160814520549.3822905.17688764528316962056@auryn.jones.dk/">offered</a>
another possible approach: analyzing what packages are needed by gsa and
then either using existing Debian packages for those dependencies or
creating new ones for those that are not available.  Hertzog was convinced
that wouldn't be done, but Smedegaard said that the
JavaScript team is already <a href="https://wiki.debian.org/Javascript/Nodejs/Tasks">working on that
process</a> for multiple projects. 
</p>

<p>
Hertzog ran the analysis script described on that page and <a href="https://lwn.net/ml/debian-devel/X9tMDrI7ybtA/8md@home.ouaza.com/">pointed to</a>
the <a href="https://wiki.debian.org/Javascript/Nodejs/Tasks/gsa">output</a> from
the <tt>package.json</tt> file of gsa.  He said that it confirmed his
belief that there are too many dependencies to package;
"<q>Even if you package everything, you will never ever have the right
combination of version of the various packages.</q>" 
</p>

<p>
To many, that list looks daunting at best, impossible at worst, but
Smedegaard <a href="https://lwn.net/ml/debian-devel/160820908322.3822905.3803292725902751282@auryn.jones.dk/">seemed
unfazed</a>,  noting several reasons to believe that those dependencies can
be handled.  But
Hertzog <a href="https://lwn.net/ml/debian-devel/X9tjP0LAE31Errik@home.ouaza.com/">pointed
out</a> that the work is not of any real benefit, at least in his mind.  He
cannot justify spending lots of time packaging those npm modules (then
maintaining them) for a single package "<q>when said package was updated in Kali in a matter of
hours</q>".  He thinks the distribution should focus its efforts
elsewhere:
</p><p>
By trying to shoehorn node/go modules into Debian packages we are creating
busy work with almost no value. We must go back to what is the value
added by Debian and find ways to continue to provide this value while
accepting the changed paradigm that some applications/ecosystems have
embraced.
</p>


<p>
He said that Debian is failing to keep up with the paradigm change in these other
ecosystems, which means that "<q>many useful things</q>" are not
being packaged.  Pirate Praveen agreed that there are useful things going
unpackaged, but <a href="https://lwn.net/ml/debian-devel/35MHLQ.PEWX0JI5RYHD@onenetbeyond.org/">disagreed
with Hertzog's approach</a> of simply using the upstream download-and-build
process.  Praveen thinks that a mix of vendoring (bundling) for
ultra-specific dependencies and creating
packages for more generally useful modules is the right way forward. It
comes down to distributions continuing to provide a particular service for their users:
</p><p>
All the current trends are making it easy for developers to ship code
directly to users. Which encourages more isolation instead of 
collaboration between projects. It also makes it easy for shipping more
proprietary code, duplication of security tracking or lack of it. 
Debian and other distributions have provided an important buffer between
developers and users as we did not necessarily follow the priorities or
choices of upstream developers exactly always.
</p>


<p>
One of the reasons Smedegaard felt that the dependencies for gsa could be
handled via Debian packages is that gsa (and other large projects) tend to
overspecify the versions required; in many cases, other versions (which
might already be packaged for Debian) work just fine.  But figuring that
out is "<q>a substantial amount of work</q>", Josh Triplett <a href="https://lwn.net/ml/debian-devel/X91Ib/s8eQ4bt0NE@localhost/">said</a> in a lengthy
message. He
cautioned against the "<q>standard tangent</q>" where complaints
about the number of dependencies for these types of projects are aired.
</p><p>
[...] people
will still use fine-grained packages and dependencies per the standard
best-practices of those communities, no matter the number or content of
mails in this thread suggesting otherwise. The extremes of "package for
a one-line function" are not the primary issue here; not every
fine-grained dependency is that small, and the issues raised in this
mail still apply whether you have 200 dependencies or 600. So let's take
it as a given that packages *will* have hundreds of library
dependencies, and try to make that more feasible.
</p>


<p>
He said that disregarding a project's guidance on the versions for its
dependencies is fraught, especially for dynamically typed languages where
problems may only be detected at run time.   For those ecosystems, the
normal Debian practice of having only one version of a given library
available may be getting in the way.  Relaxing that requirement somewhat
could be beneficial:
</p><p>
 I'm not suggesting there should be 50 versions of a given
library in the archive, but allowing 2-4 versions would greatly simplify
packaging, and would allow such unification efforts to take place
incrementally, via transitions *in the archive* and *in collaboration
with upstream*, rather than *all at once before a new package can be
uploaded*.
</p>


<p>
Triplett outlined the problems that developers encounter when trying to
package a project of this sort.  They can either try to make it work
with the older libraries available in Debian, upgrade the libraries in
Debian and fix all the resulting problems in every package that uses them,
or simply bundle the required libraries.  The first two are enormously
difficult in most cases, so folks settle for bundling, which is undesirable
but unavoidable:
</p><p>
Right now, Debian pushes back heavily on bundling, and *also* pushes
back heavily on all of the things that would solve the problems with
unbundled dependencies. That isn't sustainable. If we continue to push
back on bundling, we need to improve our tools and processes and
policies to make it feasible to maintain unbundled packages. Otherwise,
we need to build tools and processes and policies around bundled
dependencies. (Those processes could still include occasional
requirements for unbundling, such as for security-sensitive libraries.)
</p>


<p>
Adrian Bunk is <a href="https://lwn.net/ml/debian-devel/20201222220100.GA6924@localhost/">concerned</a> with
handling security problems in a world with multiple library versions.  He
said that these ecosystems seem to not be interested in supporting
stable packages for three to five years, as needed by distributions such
as Debian stable or Ubuntu LTS.  More library proliferation (version-wise)
just means more work for Debian when the inevitable CVE comes along, he said.
</p>

<p>
But Triplett <a href="https://lwn.net/ml/debian-devel/X+e/Vc/FsURXNuta@localhost/">said</a> that he is
not expecting there to be a lot of different library versions, but that at
times it might make sense to have more than one:
</p><div><p>
I'm talking about packaging xyz 1.3.1 and 2.0.1, as separate xyz-1 and
xyz-2 packages, and allowing the use of both in build dependencies.
Then, a package using xyz-1 can work with upstream to migrate to xyz-2,
and when we have no more packages in the archive using xyz-1 we can drop
it.
</p><p>
That's different from requiring *exactly one* version of xyz, forcing
all packages to transition immediately, and preventing people from
uploading packages because they don't fork upstream and port to
different versions of dependencies.
</p></div>


<p>
It seems safe to say that few minds were changed in the course of the
discussion.  Bunk and Triplett seemed to talk past each other a fair bit.
And no one spoke up with some wild new solution to these problems.  But the
problems are not going to disappear anytime soon—or ever.   Without some
kind of shift, bundling will likely be the path of least resistance, at
least until some hideous security problem has to be fixed in enough
different packages that bundling is further restricted or prohibited.  That
would, of course, then require a different solution.
</p>

<p>
The approach currently
being taken by Smedegaard, Praveen, and others to tease out the
dependencies into their own packages has its attractions,
but scalability and feasibility within a volunteer-driven organization like Debian are not
among them.  The size and scope of the open-source-creating community is
vastly larger than Debian or any of its language-specific teams, so it
should not come as a surprise that the distribution is not keeping up.
Debian is hardly alone with this problem either, of course; it is a problem
that the Linux distribution community will continue to grapple with.
</p><br clear="all"><hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/842319/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox Development Is Moving from Mercurial to Git (206 pts)]]></title>
            <link>https://groups.google.com/a/mozilla.org/g/firefox-dev/c/QnfydsDj48o/m/8WadV0_dBQAJ</link>
            <guid>38160161</guid>
            <pubDate>Mon, 06 Nov 2023 08:50:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://groups.google.com/a/mozilla.org/g/firefox-dev/c/QnfydsDj48o/m/8WadV0_dBQAJ">https://groups.google.com/a/mozilla.org/g/firefox-dev/c/QnfydsDj48o/m/8WadV0_dBQAJ</a>, See on <a href="https://news.ycombinator.com/item?id=38160161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="yjbGtf" aria-labelledby="i4" role="region"><p>FIREFOX DEVELOPMENT IS MOVING FROM MERCURIAL TO GIT</p><p>For a long time Firefox Desktop development has supported both Mercurial and</p><p>Git users. This dual SCM requirement places a significant burden on teams which</p><p>are already stretched thin in parts. We have made the decision to move Firefox</p><p>development to Git.</p><p>- We will continue to use Bugzilla, moz-phab, Phabricator, and Lando</p><p>- Although we'll be hosting the repository on GitHub, our contribution workflow</p><p>&nbsp; will remain unchanged and we will not be accepting Pull Requests at this time</p><p>- We're still working through the planning stages, but we're expecting at least</p><p>&nbsp; six months before the migration begins</p><p>APPROACH</p><p>In order to deliver gains into the hands of our engineers as early as possible,</p><p>the work will be split into two components: developer-facing first, followed by</p><p>piecemeal migration of backend infrastructure.</p><p>Phase One - Developer Facing</p><p>&nbsp; &nbsp; We'll switch the primary repository from Mercurial to Git, at the same time</p><p>&nbsp; &nbsp; removing support for Mercurial on developers' workstations. &nbsp;At this point</p><p>&nbsp; &nbsp; you'll need to use Git locally, and will continue to use moz-phab to submit</p><p>&nbsp; &nbsp; patches for review.</p><p>&nbsp; &nbsp; All changes will land on the Git repository, which will be unidirectionally</p><p>&nbsp; &nbsp; synchronised into our existing Mercurial infrastructure.</p><p>Phase Two - Infrastructure</p><p>&nbsp; &nbsp; Respective teams will work on migrating infrastructure that sits atop</p><p>&nbsp; &nbsp; Mercurial to Git. &nbsp;This will happen in an incremental manner rather than</p><p>&nbsp; &nbsp; all at once.</p><p>&nbsp; &nbsp; By the end of this phase we will have completely removed support of</p><p>&nbsp; &nbsp; Mercurial from our infrastructure.</p><div dir="auto"><p>glob ∙&nbsp;moz://a</p><p>Senior Engineering Manager ∙ Engineering Workflow &amp; Release Management</p></div>
<br></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[01-AI/Yi: A series of large language models trained from scratch (129 pts)]]></title>
            <link>https://github.com/01-ai/Yi</link>
            <guid>38159927</guid>
            <pubDate>Mon, 06 Nov 2023 08:03:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/01-ai/Yi">https://github.com/01-ai/Yi</a>, See on <a href="https://news.ycombinator.com/item?id=38159927">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h2 tabindex="-1" id="user-content-introduction" dir="auto"><a href="#introduction">Introduction</a></h2>
<p dir="auto">The <strong>Yi</strong> series models are large language models trained from scratch by
developers at <a href="https://01.ai/" rel="nofollow">01.AI</a>. The first public release contains two
bilingual (English/Chinese) base models with the parameter sizes of 6B and 34B.
Both of them are trained with 4K sequence length and can be extended to 32K
during inference time.</p>
<h2 tabindex="-1" id="user-content-news" dir="auto"><a href="#news">News</a></h2>
<ul dir="auto">
<li>🎯 <strong>2023/11/05</strong>: The base model of <code>Yi-6B-200K</code> and <code>Yi-34B-200K</code> with 200K context length.</li>
<li>🎯 <strong>2023/11/02</strong>: The base model of <code>Yi-6B</code> and <code>Yi-34B</code>.</li>
</ul>
<h2 tabindex="-1" id="user-content-model-performance" dir="auto"><a href="#model-performance">Model Performance</a></h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>MMLU</th>
<th>CMMLU</th>
<th>C-Eval</th>
<th>GAOKAO</th>
<th>BBH</th>
<th>Common-sense Reasoning</th>
<th>Reading Comprehension</th>
<th>Math &amp; Code</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>5-shot</td>
<td>5-shot</td>
<td>5-shot</td>
<td>0-shot</td>
<td>3-shot@1</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>LLaMA2-34B</td>
<td>62.6</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>44.1</td>
<td>69.9</td>
<td>68.0</td>
<td>26.0</td>
</tr>
<tr>
<td>LLaMA2-70B</td>
<td>68.9</td>
<td>53.3</td>
<td>-</td>
<td>49.8</td>
<td>51.2</td>
<td>71.9</td>
<td>69.4</td>
<td>36.8</td>
</tr>
<tr>
<td>Baichuan2-13B</td>
<td>59.2</td>
<td>62.0</td>
<td>58.1</td>
<td>54.3</td>
<td>48.8</td>
<td>64.3</td>
<td>62.4</td>
<td>23.0</td>
</tr>
<tr>
<td>Qwen-14B</td>
<td>66.3</td>
<td>71.0</td>
<td>72.1</td>
<td>62.5</td>
<td>53.4</td>
<td>73.3</td>
<td>72.5</td>
<td><strong>39.8</strong></td>
</tr>
<tr>
<td>Skywork-13B</td>
<td>62.1</td>
<td>61.8</td>
<td>60.6</td>
<td>68.1</td>
<td>41.7</td>
<td>72.4</td>
<td>61.4</td>
<td>24.9</td>
</tr>
<tr>
<td>InternLM-20B</td>
<td>62.1</td>
<td>59.0</td>
<td>58.8</td>
<td>45.5</td>
<td>52.5</td>
<td>78.3</td>
<td>-</td>
<td>30.4</td>
</tr>
<tr>
<td>Aquila-34B</td>
<td>67.8</td>
<td>71.4</td>
<td>63.1</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Falcon-180B</td>
<td>70.4</td>
<td>58.0</td>
<td>57.8</td>
<td>59.0</td>
<td>54.0</td>
<td>77.3</td>
<td>68.8</td>
<td>34.0</td>
</tr>
<tr>
<td>Yi-6B</td>
<td>63.2</td>
<td>75.5</td>
<td>72.0</td>
<td>72.2</td>
<td>42.8</td>
<td>72.3</td>
<td>68.7</td>
<td>19.8</td>
</tr>
<tr>
<td>Yi-6B-200K</td>
<td>64.0</td>
<td>75.3</td>
<td>73.5</td>
<td>73.9</td>
<td>42.0</td>
<td>72.0</td>
<td>69.1</td>
<td>19.0</td>
</tr>
<tr>
<td><strong>Yi-34B</strong></td>
<td><strong>76.3</strong></td>
<td><strong>83.7</strong></td>
<td>81.4</td>
<td>82.8</td>
<td><strong>54.3</strong></td>
<td><strong>80.1</strong></td>
<td>76.4</td>
<td>37.1</td>
</tr>
<tr>
<td>Yi-34B-200K</td>
<td>76.1</td>
<td>83.6</td>
<td><strong>81.9</strong></td>
<td><strong>83.4</strong></td>
<td>52.7</td>
<td>79.7</td>
<td><strong>76.6</strong></td>
<td>36.3</td>
</tr>
</tbody>
</table>
<p dir="auto">While benchmarking open-source models, we have observed a disparity between the
results generated by our pipeline and those reported in public sources (e.g.
OpenCompass). Upon conducting a more in-depth investigation of this difference,
we have discovered that various models may employ different prompts,
post-processing strategies, and sampling techniques, potentially resulting in
significant variations in the outcomes. Our prompt and post-processing strategy
remains consistent with the original benchmark, and greedy decoding is employed
during evaluation without any post-processing for the generated content. For
scores that were not reported by the original authors (including scores reported
with different settings), we try to get results with our pipeline.</p>
<p dir="auto">To evaluate the model's capability extensively, we adopted the methodology
outlined in Llama2. Specifically, we included PIQA, SIQA, HellaSwag, WinoGrande,
ARC, OBQA, and CSQA to assess common sense reasoning. SquAD, QuAC, and BoolQ
were incorporated to evaluate reading comprehension. CSQA was exclusively tested
using a 7-shot setup, while all other tests were conducted with a 0-shot
configuration. Additionally, we introduced GSM8K (8-shot@1), MATH (4-shot@1),
HumanEval (0-shot@1), and MBPP (3-shot@1) under the category "Math &amp; Code". Due
to technical constraints, we did not test Falcon-180 on QuAC and OBQA; the score
is derived by averaging the scores on the remaining tasks. Since the scores for
these two tasks are generally lower than the average, we believe that
Falcon-180B's performance was not underestimated.</p>
<h2 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage</a></h2>
<p dir="auto">Feel free to <a href="https://github.com/01-ai/Yi/issues/new">create an issue</a> if you
encounter any problem when using the <strong>Yi</strong> series models.</p>
<h3 tabindex="-1" id="user-content-1-prepare-development-environment" dir="auto"><a href="#1-prepare-development-environment">1. Prepare development environment</a></h3>
<p dir="auto">The best approach to try the <strong>Yi</strong> series models is through Docker with GPUs. We
provide the following docker images to help you get started.</p>


<p dir="auto">Note that the <code>latest</code> tag always points to the latest code in the <code>main</code>
branch. To test a stable version, please replace it with a specific
<a href="https://github.com/01-ai/Yi/tags">tag</a>.</p>
<p dir="auto">If you prefer trying out with your local development environment. First, create
a virtual environment and clone this repo. Then install the dependencies with
<code>pip install -r requirements.txt</code>. For the best performance, we recommend you
also install the latest version (<code>&gt;=2.3.3</code>) of
<a href="https://github.com/Dao-AILab/flash-attention#installation-and-features">flash-attention</a>.</p>
<h3 tabindex="-1" id="user-content-2-download-the-model-optional" dir="auto"><a href="#2-download-the-model-optional">2. Download the model (optional)</a></h3>
<p dir="auto">By default the model weights and tokenizer will be downloaded from
<a href="https://huggingface.co/01-ai" rel="nofollow">HuggingFace</a> automatically in the next step. You
can also download them manually from the following places:</p>
<ul dir="auto">
<li><a href="https://www.modelscope.cn/organization/01ai/" rel="nofollow">ModelScope</a></li>
<li><a href="https://wisemodel.cn/models" rel="nofollow">WiseModel</a> (Search for <code>Yi</code>)</li>
<li>Mirror site (remember to extract the content with <code>tar</code>)
<ul dir="auto">
<li><a href="https://storage.lingyiwanwu.com/yi/models/Yi-6B.tar" rel="nofollow">Yi-6B.tar</a></li>
<li><a href="https://storage.lingyiwanwu.com/yi/models/Yi-6B-200K.tar" rel="nofollow">Yi-6B-200K.tar</a></li>
<li><a href="https://storage.lingyiwanwu.com/yi/models/Yi-34B.tar" rel="nofollow">Yi-34B.tar</a></li>
<li><a href="https://storage.lingyiwanwu.com/yi/models/Yi-34B-200K.tar" rel="nofollow">Yi-34B-200K.tar</a></li>
</ul>
</li>
</ul>
<h3 tabindex="-1" id="user-content-3-examples" dir="auto"><a href="#3-examples">3. Examples</a></h3>
<h4 tabindex="-1" id="user-content-31-use-the-base-model" dir="auto"><a href="#31-use-the-base-model">3.1 Use the base model</a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="python demo/text_generation.py"><pre>python demo/text_generation.py</pre></div>
<p dir="auto">To reuse the downloaded models in the previous step, you can provide the extra
<code>--model</code> argument:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python demo/text_generation.py  --model /path/to/model"><pre>python demo/text_generation.py  --model /path/to/model</pre></div>
<p dir="auto">Or if you'd like to get your hands dirty:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(&quot;01-ai/Yi-34B&quot;, device_map=&quot;auto&quot;, torch_dtype=&quot;auto&quot;, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(&quot;01-ai/Yi-34B&quot;, trust_remote_code=True)
inputs = tokenizer(&quot;There's a place where time stands still. A place of breath taking wonder, but also&quot;, return_tensors=&quot;pt&quot;)
outputs = model.generate(inputs.input_ids.cuda(), max_new_tokens=256)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>"01-ai/Yi-34B"</span>, <span>device_map</span><span>=</span><span>"auto"</span>, <span>torch_dtype</span><span>=</span><span>"auto"</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>"01-ai/Yi-34B"</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)
<span>inputs</span> <span>=</span> <span>tokenizer</span>(<span>"There's a place where time stands still. A place of breath taking wonder, but also"</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>)
<span>outputs</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>inputs</span>.<span>input_ids</span>.<span>cuda</span>(), <span>max_new_tokens</span><span>=</span><span>256</span>)
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>outputs</span>[<span>0</span>], <span>skip_special_tokens</span><span>=</span><span>True</span>))</pre></div>
<details>
<summary>Output</summary>
<p dir="auto"><strong>Prompt</strong>: There's a place where time stands still. A place of breath taking wonder, but also</p>
<p dir="auto"><strong>Generation</strong>: There's a place where time stands still. A place of breath taking wonder, but also of great danger. A place where the very air you breathe could kill you. A place where the only way to survive is to be prepared.
The place is called the Arctic.
The Arctic is a vast, frozen wilderness. It is a place of extremes. The temperatures can drop to -40 degrees Celsius. The winds can reach speeds of 100 kilometers per hour. The sun can shine for 24 hours a day, or not at all for weeks on end.
The Arctic is also a place of great beauty. The ice and snow are a pristine white. The sky is a deep blue. The sunsets are spectacular.
But the Arctic is also a place of great danger. The ice can be treacherous. The winds can be deadly. The sun can be blinding.
The Arctic is a place where the only way to survive is to be prepared.
The Arctic is a place of extremes. The temperatures can drop to -40 degrees Celsius. The winds can reach speeds of 100 kilometers per hour. The sun can shine for 24 hours a day, or not at all for weeks on end.
The Arctic is a place of great beauty. The ice and snow are a</p>
</details>
<p dir="auto">For more advanced usage, please refer the
<a href="https://github.com/01-ai/Yi/tree/main/demo">doc</a>.</p>
<h4 tabindex="-1" id="user-content-32-finetuning-from-the-base-model" dir="auto"><a href="#32-finetuning-from-the-base-model">3.2 Finetuning from the base model:</a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="bash finetune/scripts/run_sft_Yi_6b.sh"><pre>bash finetune/scripts/run_sft_Yi_6b.sh</pre></div>
<p dir="auto">Once finished, you can compare the finetuned model and the base model with the
following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bash finetune/scripts/run_eval.sh"><pre>bash finetune/scripts/run_eval.sh</pre></div>
<p dir="auto">For more advanced usage like fine-tuning based on your custom data, please refer
the <a href="https://github.com/01-ai/Yi/tree/main/finetune">doc</a>.</p>

<h2 tabindex="-1" id="user-content-disclaimer" dir="auto"><a href="#disclaimer">Disclaimer</a></h2>
<p dir="auto">We use data compliance checking algorithms during the training process, to
ensure the compliance of the trained model to the best of our ability. Due to
complex data and the diversity of language model usage scenarios, we cannot
guarantee that the model will generate correct, and reasonable output in all
scenarios. Please be aware that there is still a risk of the model producing
problematic outputs. We will not be responsible for any risks and issues
resulting from misuse, misguidance, illegal usage, and related misinformation,
as well as any associated data security concerns.</p>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h2>
<p dir="auto">The source code in this repo is licensed under the <a href="https://github.com/01-ai/Yi/blob/main/LICENSE">Apache 2.0
license</a>. The Yi series models
are fully open for academic research and free commercial usage with permission
via applications. All usage must adhere to the <a href="https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt">Model License
Agreement 2.0</a>.
To apply for the official commercial license, please contact us
(<a href="mailto:yi@01.ai">yi@01.ai</a>).</p>
</article>
          </div></div>]]></description>
        </item>
    </channel>
</rss>