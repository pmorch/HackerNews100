<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 15 Apr 2025 06:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Palestinian activist arrested by ICE while expecting U.S. citizenship interview (132 pts)]]></title>
            <link>https://www.cbc.ca/lite/story/1.7510325</link>
            <guid>43688069</guid>
            <pubDate>Tue, 15 Apr 2025 01:18:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbc.ca/lite/story/1.7510325">https://www.cbc.ca/lite/story/1.7510325</a>, See on <a href="https://news.ycombinator.com/item?id=43688069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><main><div><article id="article"><p>The Associated Press | Posted: April 15, 2025 12:16 AM | Last Updated: 4 hours ago</p><p>Mohsen Mahdawi, a legal permanent resident, had led protests against the war in Gaza</p><div role="figure" data-pw="imageEmbed"><p>Image  |  Immigration Palestinian Student Detained</p><p>Caption: This image taken from a video shows Mohsen Mahdawi being detained at the U.S. Citizenship and Immigration Services office in Colchester, Vt., on Monday. The video was taken by Christopher Helali, a friend who accompanied Mahdawi to a meeting at the office.  (Christopher Helali via AP)</p></div><p>A Palestinian man who led protests against the war in Gaza as a student at Columbia University was arrested Monday at a Vermont immigration office where he expected to be interviewed about finalizing his U.S. citizenship, his attorneys said.</p><p>Mohsen Mahdawi, a legal permanent resident who has held a green card since 2015, was detained at the U.S. Citizenship and Immigration Services office in Colchester, Vt., by Immigration and Customs Enforcement agents, his lawyers said.</p><p>The attorneys said they do not know where he is and have filed a petition in federal court seeking an order barring the government from removing him from the state or country.</p><p>"The Trump administration detained Mohsen Mahdawi in direct retaliation for his advocacy on behalf of Palestinians and because of his identity as a Palestinian. His detention is an attempt to silence those who speak out against the atrocities in Gaza. It is also unconstitutional," attorney Luna Droubi said in an email.</p><p>According to the court filing, Mahdawi was born in a refugee camp in the West Bank and moved to the United States in 2014.</p><p>He recently completed coursework at Columbia in New York and was expected to graduate in May before beginning a master's degree program there in the fall.</p><p>The petition describes him as a committed Buddhist who believes in "non-violence and empathy as a central tenet of his religion."</p><h2>Arrest 'immoral, inhumane, and illegal'</h2><p>As a student, Mahdawi was an outspoken critic of Israel's military campaign in Gaza and organized campus protests until March 2024.</p><p>He co-founded the Palestinian Student Union at Columbia with Mahmoud Khalil, another Palestinian permanent resident of the U.S. and graduate student who recently was detained by ICE.</p><p>Khalil was the first person arrested under President Donald Trump's promised crackdown on students who joined campus protests against the war in Gaza.</p><p>On Friday, an immigration judge in Louisiana ruled that Khalil can be deported as a national security risk.</p><p>Christopher Helali, a friend of Mahdawi who lives near him in Vermont, was present outside the immigration office when Mahdawi was detained and recorded a video of Mahdawi being led away by authorities.</p><p>In the video, which Helali released on social media Monday, Mahdawi is shown giving a peace sign with his hands and being led away to a car.</p><p>Helali described Mahdawi as a peaceful demonstrator who has worked to foster dialogue about the struggle of Palestinians in his homeland.</p><p>Helali said he and Mahdawi were aware that Mahdawi could be detained today and that his friend went forward with the appointment anyway.</p><p>"And rightfully so, he was nervous for what was going on around him. But he was very much resolute in coming to this interview and coming today because he didn't do anything wrong and was a law-abiding citizen, or soon-to-be citizen," Helali said.</p><p>Vermont's congressional delegation issued a statement condemning Mahdawi's arrest, saying that instead of taking one of the final steps in his citizenship process, he was handcuffed by armed officers with their faces covered.</p><p>"This is immoral, inhumane and illegal. Mr. Mahdawi, a legal resident of the United States, must be afforded due process under the law and immediately released from detention," said the statement from Sen. Bernie Sanders, Sen. Peter Welch and Rep. Becca Balint.</p></article></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tomb Engine (118 pts)]]></title>
            <link>https://tombengine.com/</link>
            <guid>43686936</guid>
            <pubDate>Mon, 14 Apr 2025 22:22:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tombengine.com/">https://tombengine.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43686936">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="6f3696b0" data-element_type="container" data-widget_type="text-editor.default" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}" data-elementor-type="footer" data-elementor-id="1421" data-elementor-post-type="elementor_library">
				<p><span>This is a community project which is not affiliated with Core Design, Eidos Interactive, or Embracer Group AB. Tomb Raider is a registered trademark of Embracer Group AB. TombEngine is not be sold. The code is open-source to encourage contributions and to be used for study purposes. We are not responsible for illegal uses of this source code. This source code is released as-is and continues to be maintained by non-paid contributors in their free time.</span></p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel sells 51% stake in Altera to private equity firm on a $8.75B valuation (243 pts)]]></title>
            <link>https://newsroom.intel.com/corporate/intel-partner-deal-news-april2025</link>
            <guid>43686773</guid>
            <pubDate>Mon, 14 Apr 2025 21:59:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsroom.intel.com/corporate/intel-partner-deal-news-april2025">https://newsroom.intel.com/corporate/intel-partner-deal-news-april2025</a>, See on <a href="https://news.ycombinator.com/item?id=43686773">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Raghib Hussain appointed chief executive officer of Altera.</p><div nonce="ijCpSEMuRnL490hOs/kXNA==">

					
						

					<!--?xml encoding="utf-8" ?--><p>SANTA CLARA, Calif.; SAN JOSE, Calif.; and MENLO PARK, Calif., April 14, 2025 – Intel Corporation today announced that it has entered into a definitive agreement to sell 51% of its Altera business to Silver Lake, a global leader in technology investing.</p><p>The transaction, which values Altera at $8.75 billion, establishes Altera’s operational independence and makes it the largest pure-play FPGA (field programmable gate array) semiconductor solutions company. Altera offers a proven and highly scalable architecture and tool chain and is focused on driving growth and FPGA innovation to meet the demands and opportunities of an AI-driven market.</p><p>Intel will own the remaining 49% of the Altera business, enabling it to participate in Altera’s future success while focusing on its core business.</p><p>Intel also announced that Raghib Hussain will succeed Sandra Rivera as chief executive officer of Altera, effective May 5, 2025. Hussain is a highly accomplished and visionary technology executive with strong business acumen and engineering credentials. He joins Altera from his previous role as president of Products and Technologies at Marvell. Prior to joining Marvell in 2018, Hussain served as chief operating officer of Cavium, a company he co-founded. Prior to Cavium, Hussain held engineering roles at both Cisco and Cadence and helped found VPNet, an enterprise security company.</p><p>“Today’s announcement reflects our commitment to sharpening our focus, lowering our expense structure and strengthening our balance sheet,” said Lip-Bu Tan, chief executive officer of Intel. “Altera continues to make progress repositioning its product portfolio to participate in the fastest growing and most profitable segments of the FPGA market. We are grateful for Sandra’s strong leadership and lasting impact throughout her 25-year Intel career and wish her continued success as she begins a new chapter. Raghib is a superb executive we selected to lead the business forward based on his vast industry experience and proven track record of success. We look forward to partnering with Silver Lake upon closing of the transaction, as their industry expertise will help to accelerate Altera's efforts and unlock additional economic value for Intel.”</p><p>“This investment represents a once-in-a-generation opportunity to invest in a scale leader in advanced semiconductors. Together with Raghib, we will be focused on strengthening Altera’s technology leadership position and investing in emerging AI-driven markets such as edge computing and robotics,” said Kenneth Hao, chairman and managing partner of Silver Lake. “We look forward to working closely with Intel as a strategic partner who will continue to provide U.S.-based foundry services and complementary engagement with customers.”</p><p>“I am excited to lead Altera in its next chapter, and this milestone with Silver Lake furthers Altera’s journey to be the world's No. 1 FPGA solutions provider,” said Hussain. “Backed by Silver Lake’s strong track record and now with clarity of focus as an independent company, Altera is well-positioned to build on its momentum and deliver breakthrough FPGA-based solutions that are shaping the future of compute driven by AI. I am grateful for the impact Sandra has made and the team she has built as we begin Altera’s next phase of growth.”</p><p>Altera has been at the forefront of driving FPGA innovations for more than 40 years. The company provides leading programmable solutions that are easy-to-use and deploy in a range of strategically important segments such as industrial, communications, data center and military, aerospace, and government, as well as emerging markets such as AI/edge and robotics. Its broad portfolio of programmable semiconductor solutions, software and development tools deliver the reliability and flexibility needed to accelerate customer technology innovation.</p><p>The transaction is expected to close in the second half of 2025, subject to customary closing conditions.</p><p>Upon closing, Intel expects to deconsolidate Altera’s financial results from Intel’s consolidated financial statements. In Fiscal Year 2024, Altera generated revenues of $1.54 billion, GAAP gross margin of $361 million and GAAP operating loss of $615 million. Altera’s Fiscal Year 2024 non-GAAP gross margin was $769 million and non-GAAP operating income was $35&nbsp;million. Reconciliations between the GAAP and non-GAAP measures are provided below.</p><p>Morgan Stanley &amp; Co. LLC acted as financial advisor to Intel.</p><p><strong>Forward-Looking Statements</strong></p><p>This release contains forward-looking statements that involve a number of risks and uncertainties, including with respect to the terms and anticipated timing of closing the agreed upon sale of a controlling interest in Altera and the potential benefits of such sale to Intel and Altera. Such statements involve risks and uncertainties that could cause actual results to differ materially from those expressed or implied, including:&nbsp; the risk that the transaction may not be completed in a timely manner or at all, including as a result of a failure to receive regulatory approvals; the occurrence of any event, change or other circumstance that could give rise to the termination of the transaction; the risk that the expected benefits of the transaction, including as a result of the increased independence of Altera, may not be realized; the risk of future loss of the Altera business by Intel as a result of the sale of a controlling interest in Altera; disputes or potential litigation related to the transaction or the ownership, control and operation of the Altera business, including as it relates to Intel; unanticipated costs related to the transaction or the Altera business that may be incurred; risks as to the retention of key Altera personnel and customers; risks related to the diversion of management’s attention during the pendency of the transaction; potential adverse reactions or changes to business relationships resulting from the announcement or completion of the transaction; changes in demand for Altera’s semiconductor products; the high level of competition and rapid technological change in the semiconductor industry; and other risks and uncertainties described in Intel’s 2024 Form 10-K and our other filings with the SEC.</p><p>Given these risks and uncertainties, readers are cautioned not to place undue reliance on such forward-looking statements. Readers are urged to carefully review and consider the various disclosures made in this release and in other documents we file from time to time with the SEC that disclose risks and uncertainties that may affect our business.</p><p>All information in this press release reflects Intel management views as of the date hereof unless an earlier date is specified. Intel does not undertake, and expressly disclaims any duty, to update such statements, whether as a result of new information, new developments, or otherwise, except to the extent that disclosure may be required by law.</p><p><strong>Non-GAAP Financial Measures</strong></p><p>This release contains references to non-GAAP financial measures: Altera non-GAAP gross margin and Altera non-GAAP operating income / (loss) measures. Set out below are reconciliations of these measures to the most directly comparable GAAP financial measures. The non-GAAP financial measures disclosed herein should not be considered a substitute for, or superior to, the financial measures prepared in accordance with GAAP. Please refer to “Explanation of Non-GAAP Measures” in Intel’s earnings release dated Jan. 30, 2025 for a detailed explanation of the adjustments made to the comparable GAAP measures, the ways management uses the non-GAAP measures, and the reasons why management believes the non-GAAP measures provide investors with useful supplemental information.</p><table nonce="ijCpSEMuRnL490hOs/kXNA==">
<tbody>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Twelve Months Ended</td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">(in Millions; Unaudited)</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Dec 28, 2024</td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">GAAP gross margin</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="><strong>$ 361</strong></td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Acquisition-related adjustments</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">402</td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Share-based compensation</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">6</td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Non-GAAP gross margin</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">$ 769</td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">GAAP operating income / (loss)</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="><strong>$ (615)</strong></td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Acquisition-related adjustments</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">491</td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Share-based compensation</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">122</td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Restructuring and other charges</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">37</td>
</tr>
<tr>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">Non-GAAP operating income / (loss)</td>
<td nonce="ijCpSEMuRnL490hOs/kXNA=="></td>
<td nonce="ijCpSEMuRnL490hOs/kXNA==">$ 35</td>
</tr>
</tbody>
</table><p><strong>About Intel</strong></p><p>Intel (Nasdaq: INTC) is an industry leader, creating world-changing technology that enables global progress and enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and manufacturing of semiconductors to help address our customers’ greatest challenges. By embedding intelligence in the cloud, network, edge and every kind of computing device, we unleash the potential of data to transform business and society for the better. To learn more about Intel’s innovations, go to <a href="https://newsroom.intel.com/">newsroom.intel.com</a> and <a href="https://intel.com/">intel.com</a>.</p><p><strong>About Altera</strong><br>
Altera is a leading supplier of programmable hardware, software, and development tools that empower designers of electronic systems to innovate, differentiate, and succeed in their markets. With a broad portfolio of industry-leading FPGAs, SoCs, and design solutions, Altera enables customers to achieve faster time-to-market and unmatched performance in applications spanning data centers, communications, industrial, automotive, and more. For more information, visit&nbsp;<a href="http://www.altera.com./">www.altera.com.</a></p><p><strong>About Silver Lake</strong><br>
Silver Lake is a global technology investment firm, with approximately $104 billion in combined assets under management and committed capital and a team of professionals based in North America, Europe and Asia. Silver Lake’s portfolio companies collectively generate nearly $252 billion of revenue annually and employ approximately 433,000 people globally.</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Is Entropy? (181 pts)]]></title>
            <link>https://jasonfantl.com/posts/What-is-Entropy/</link>
            <guid>43684560</guid>
            <pubDate>Mon, 14 Apr 2025 18:32:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jasonfantl.com/posts/What-is-Entropy/">https://jasonfantl.com/posts/What-is-Entropy/</a>, See on <a href="https://news.ycombinator.com/item?id=43684560">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>People say many things about entropy: entropy increases with time, entropy is disorder, entropy increases with energy, entropy determines the arrow of time, etc.. But I have no idea what entropy is, and from what I find, neither do most other people. This is the introduction I wish I had when first told about entropy, so hopefully you find it helpful. My goal is that by the end of this long post we will have a rigorous and intuitive understanding of those statements, and in particular, why the universe looks different when moving forward through time versus when traveling backward through time.</p><p>This journey begins with defining and understanding entropy. There are multiple formal definitions of entropy across disciplines—thermodynamics, statistical mechanics, information theory—but they all share a central idea: <strong>entropy quantifies uncertainty</strong>. The easiest introduction to entropy is through Information Theory, which will lead to entropy in physical systems, and then finally to the relationship between entropy and time.</p><h2 id="information-theory"><span>Information Theory</span><a href="#information-theory"><i></i></a></h2><p>Imagine you want to communicate to your friend the outcome of some random events, like the outcome of a dice roll or the winner of a lottery, but you want to do it with the fewest number of bits (only 1s and 0s) as possible. How few bits could you use?</p><p>The creator of Information Theory, Claude Shannon, was trying to answer questions such as these during his time at Bell labs. He was developing the mathematical foundations of communication and compression, and eventually he discovered that the minimum number of bits required for a message was directly related to the uncertainty of the message. He was able to then formulate an equation to quantify the uncertainty of a message. When he shared it with his physicist colleague at Bell Labs, John von Neumann, von Neumann suggested calling it <em>entropy</em> for two reasons:</p><blockquote><p>Von Neumann, Shannon reports, suggested that there were two good reasons for calling the function “entropy”. “It is already in use under that name,” he is reported to have said, “and besides, it will give you a great edge in debates because nobody really knows what entropy is anyway.” Shannon called the function “entropy” and used it as a measure of “uncertainty,” interchanging the two words in his writings without discrimination.<br> — <em>Harold A. Johnson (ed.),</em> <em>Heat Transfer, Thermodynamics and Education: Boelter Anniversary Volume</em> (New York: McGraw-Hill, 1964), p. 354.</p></blockquote><p>Later we will see that the relationship between Shannon’s entropy and the pre-existing definition of entropy was more than coincidental, they are deeply intertwined.</p><p>But now let us see how Shannon found definitions for these usually vague terms of “information” and “uncertainty”.</p><p>In Information Theory, the information of an observed state is formally defined as the number of bits needed to communicate that state (at least for a system with equally likely outcomes with powers of two, we’ll see shortly how to generalize this). Here are some examples of information:</p><ul><li>If I flip a fair coin, it will take one bit of information to tell you the outcome: I use a <code>0</code> for head and a <code>1</code> for tails.</li><li>If I roll a fair 8-sided dice, I can represent the outcome with 3 bits: I use <code>000</code> for a 1, <code>001</code> for 2, <code>010</code> for 3, etc.</li></ul><p>The more outcomes a system can have, the more bits (information) it will require to represent its outcome. If a system has $N$ equally likely outcomes, then it will take $\text{log}_2(N)$ bits of information to represent an outcome of that system.</p><p>Entropy is defined as the expected number of bits of information needed to represent the state of a system (this is a lie, but it’s the most useful definition for the moment, we’ll fix it later). So the entropy of a coin is 1 since on average we expect it to take 1 bit of information to represent the outcome of the coin. An 8-sided dice will have an entropy of 3 bits, since we expect it to take an average of 3 bits to represent the outcome.</p><p>It initially seems that entropy is an unnecessary definition since we can just look at how many bits it takes to represent the outcome of our system and use that value, but this is only true when the chance of the outcomes are all equally likely.</p><p>Imagine now that I have a weighted 8-sided dice, so the number 7 comes up $50$% of the time while the rest of the faces come up $\approx 7.14$% of the time. Now, if we are clever, we can reduce the expected number of bits needed to communicate the outcome of the dice. We can decide to represent a 7 with a <code>0</code>, and all the other numbers will be represented with <code>1XXX</code> where the <code>X</code>s are some unique bits. This would mean that $50$% percent of the time we only have to use 1 bit of information to represent the outcome, and the other $50$% of the time we use 4 bits, so the expected number of bits (the entropy of the dice) is 2.5. This is lower than the 3 bits of entropy for the fair 8-sided dice.</p><p>Fortunately, we don’t need to come up with a clever encoding scheme for every possible system, there exists a pattern to how many bits of information it takes to represent a state with probability $p$. We know if $p=0.5$ such as in the case of a coin landing on heads, then it takes 1 bit of information to represent that outcome. If $p=0.125$ such as in the case of a fair 8-sided dice landing on the number 5, it takes 3 bits of information to represent that outcome. If $p=0.5$ such as in the case of our unfair 8-sided dice landing on the number 7, then it takes 1 bit of information, just like the coin, which shows us that all that matters is the probability of the outcome. With this, we can discover an equation for the number of bits of information needed for a state with probability $p$.</p><p>\[I(p) = -\text{log}_2(p)\]</p><p>This value $I$ is usually called <em>information content</em> or <em>surprise</em>, since the lower the probability of a state occurring, the higher the surprise when it does occur.</p><p>When the probability is low, the surprise is high, and when the probability is high, the surprise is low. This is a more general formula then “the number of bits needed” since it allows for states that are exceptionally likely (such as $99$% likely) to have surprise less then 1, which would make less sense if we tried to interpret the value as “the number of needed bits to represent the outcome”.</p><p>And now we can fix our definition of entropy (the lie I told earlier). Entropy is not necessarily the expected number of bits used to represent a system (although it is when you use an optimal encoding scheme), but more generally the entropy is the expected <em>surprise</em> of the system.</p><p>And now we can calculate the entropy of systems like a dice or a coin or any system with known probabilities for its outcomes. The expected surprise (entropy) of a system with $N$ possible outcomes each with probability $p_i$ (all adding up to 1) can be calculated as</p><p>\[\begin{align} \sum_{i=1}^{N} p_i \cdot I(p_i) = - \sum_{i=1}^{N} p_i \cdot \text{log}_2(p_i)\label{shannon_entropy}\tag{Shannon entropy}\\ \end{align}\]</p><p>And notice that if all the $N$ probabilities are the same (so $p_i = \frac{1}{N}$), then the entropy equation can simplify to</p><p>\[- \sum_{i=1}^{N} p_i \cdot \text{log}_2(p_i) \Rightarrow \text{log}_2(N)\]</p><p>Here are some basic examples using $\eqref{shannon_entropy}$.</p><ul><li>The entropy of a fair coin is</li></ul><p>\[- ( 0.5 \cdot \text{log}_2(0.5) + 0.5 \cdot \text{log}_2(0.5)) = \text{log}_2(2) = 1\]</p><ul><li>The entropy of a fair 8-sided dice is</li></ul><p>\[- \sum_{i=1}^{8} 0.125 \cdot \text{log}_2(0.125) = \text{log}_2(8) = 3\]</p><ul><li>The entropy of an unfair 8-sided dice, where the dice lands on one face $99$% of the time and lands on the other faces the remaining $1$% of the time with equal probability (about $0.14$% each), is</li></ul><p>\[- (0.99 \cdot \text{log}_2(0.99) + \sum_{i=1}^{7} 0.0014 \cdot \text{log}_2(0.0014)) = 0.10886668511648723\]</p><p>Hopefully it is a bit more intuitive now that entropy represents uncertainty. An 8-sided dice would have higher entropy than a coin since we are more uncertain about the outcome of the 8-sided dice than we are about the coin (8 equally likely outcomes are more uncertain than only 2 equally likely outcomes). But a highly unfair 8-sided dice has less entropy than even a coin since we have very high certainty about the outcome of the unfair dice. Now we have an actual equation to quantify that uncertainty (entropy) about a system.</p><p>It is not clear right now how this definition of entropy has anything to do with disorder, heat, or time, but this idea of entropy as uncertainty is fundamental to understanding the entropy of the universe which we will explore shortly. For reference, this definition of entropy is called Shannon entropy.</p><p>We will move on now, but I recommend looking further into Information Theory. It has many important direct implications for data compression, error correction, cryptography, and even linguistics, and touches nearly any field that deals with uncertainty, signals, or knowledge.</p><h2 id="physical-entropy"><span>Physical Entropy</span><a href="#physical-entropy"><i></i></a></h2><p>Now we will see entropy from a very different lens, that of Statistical Mechanics. We begin with the tried-and-true introduction to entropy which every student is given.</p><h3 id="balls-in-a-box"><span>Balls in a box</span><a href="#balls-in-a-box"><i></i></a></h3><p>I shall give you a box with 10 balls in it, $p_0$ through $p_9$, and we will count how many balls are on the left side of the box and on the right side of the box. Assume every ball is equally likely to be on either side. Immediately we can see it is highly unlikely that we count all the balls are on the left side of the box, and more likely that we count an equal number of balls on each side. Why is that?</p><p>Well, there is only one state in which we count all the balls on the left, and that is if every ball is on the left (truly astounding, but stay with me). But there are many ways in which the box is balanced: We could have $p_0$ through $p_4$ one side and the rest on the other, or the same groups but flipped from left to right, or we could have all the even balls on one side and the odd on the other, or again flipped, or any of the other many possible combinations.</p><p>This box is a system that we can measure the entropy of, at least once I tell you how many balls are counted on each side. It can take a moment to see, but imagine the box with our left and right counts as a system where the outcome will be finding out where all the individual balls are in the box, similar to rolling a dice and seeing which face it lands on.</p><p>This would mean that the box where we count all the balls on the left side only has one possible outcome: all the balls are on the left side. We would take this to mean that this system has $0$ entropy (no expected surprise) since we already know where we will find each individual ball.</p><p>The box with balanced sides (5 on each) has many possible equally likely outcomes, and in fact, we can count them. A famous equation in combinatorics is the N-choose-k equation, which calculates exactly this scenario. It tells us that there are 252 possible ways in which we can place 5 balls on each side. The entropy for this system would then be $- \sum_{i=1}^{252} \frac{1}{252} \cdot \text{log}_2(\frac{1}{252}) = \text{log}_2(252) = 7.9772799235$. This is the same as calculating the entropy of a 252-sided dice.</p><p>And if we were to increase the number of balls, the entropy of the balanced box would increase since there would then be even more possible combinations that could make up a balanced box.</p><p>We should interpret these results as: The larger the number of ways there are to satisfy the large-scale measurement (counting the number of balls on each side), the higher the entropy of the system. When all the balls are on the left, there is only one way to satisfy that measurement and so it has a low entropy. When there are many ways to balance it on both sides, it has high entropy.</p><p>Here we see 1000 balls bouncing around in a box. They will all start on the left, so the box would have 0 entropy, but once the balls start crossing to the right and changing the count on each side, the entropy will increase.</p><p><a href="https://jasonfantl.com/assets/img/posts/Entropy/2_cell_box.gif"><img data-src="/assets/img/posts/Entropy/2_cell_box.gif" alt=" balls in a box with its entropy " width="300" data-proofer-ignore=""></a></p><p>In Statistical Mechanics, the formal term for the large-scale measurement is the <em>macrostate</em>, and the specific states that can satisfy that measurement are <em>microstates</em>. We would call the measurement of the number of balls on each side of the box the macrostate, and the different combinations of positions of individual balls the microstates. So rephrasing the above: There is only one microstate representing the macrostate of all balls being counted on one side, and there are many microstates representing the macrostate of a balanced box.</p><p>But why did we decide to measure the number of balls on the left and right? We could have measured a different macrostate, and the entropy would be different.</p><h3 id="macrostates"><span>Macrostates</span><a href="#macrostates"><i></i></a></h3><p>Imagine instead of selecting the left and right halves of the box to count the number of balls, we instead count how many balls are in each pixel of the box. In this scenario, the entropy would almost always be maximized, as the balls rarely share a pixel. Even if all the balls were on the left side of the box, they would likely still each occupy a different pixel, and the measured entropy would be the same as if the balls were evenly distributed in the box.</p><p>If we use an expensive instrument to measure the box and track the balls with high precision, then the entropy would rarely change and would be very high. If we instead use an inexpensive instrument that can only tell if a ball is on the left or right of the box, then the entropy will be low and could very easily fluctuate if some of the balls temporarily end up on the same side of the box.</p><p>Let’s run exactly the same simulation of 1000 balls in the box again, still starting with the balls on the left. But, this time we count how many balls are in each cell in a 50x50 grid, as opposed to the previous two cells (the left and right cells). The entropy will be high since there are many microstates that represent a bunch of cells with only 1 ball in it, and the entropy won’t change much since two balls rarely share the same cell. Recall that if two balls share the same cell, the count would go up, and there are fewer microstates that satisfy a cell with a count of 2 compared to two cells with a count of 1 in each.</p><p><a href="https://jasonfantl.com/assets/img/posts/Entropy/50_cell_box.gif"><img data-src="/assets/img/posts/Entropy/50_cell_box.gif" alt=" balls in a box with its entropy " width="300" data-proofer-ignore=""></a></p><p>Entropy is not intrinsic to the physical system alone, but rather to our description of it as well — i.e., the macrostate we’re measuring, and the resolution at which we observe it.</p><p>This process of measuring a lower-resolution version of our system (like counting how many balls are on the left or right side of a box) is called <em>coarse-graining</em>.</p><p>How we choose/measure the macrostate, that is, how we coarse-grain the system, is dependent on the problem we are solving.</p><ul><li>Imagine you have a box of gas (like our balls in a box, but at the scale of $10^{25}$ balls in the box), and we place a temperature-reader on the left and right side of the box. This gives us a macrostate of two counts of the average ball speed on the left and right sides of the box. We can then calculate the entropy by comparing when the temperature-readers are equal to when they are different by $T$ degrees. Once we learn how time and entropy interact, we will use this model to show that the two temperature-readers are expected to converge to the same value over time.</li><li>Imagine you sequence the genome of many different people in a population, you could choose many different macrostates based on what you care about. You could count how many of each nucleotide there are in all the sequences, allowing you to quantify how variable the four nucleotides are in DNA. You could calculate the entropy of every individual position in the DNA sequence by counting how many nucleotide types are used in that position across the population, allowing you to identify portions of DNA that are constant across individuals or vary across individuals.</li></ul><p>How you choose to measure the macrostate can come in many forms for the same system, depending on what you are capable of measuring and/or what you care about measuring.</p><p>But once we have a macrostate, we need a way to identify all the microstates and assign probabilities to them.</p><h3 id="microstates"><span>Microstates</span><a href="#microstates"><i></i></a></h3><p>When we were looking at the positions of balls in a box in equally sized cells, it was easy to see that every ball was equally likely to be in any of the cells, so each microstate was equally likely. This made calculating the entropy very simple, we just used the simplified version of $\eqref{shannon_entropy}$ to find that for $W$ microstates that satisfy a given macrostate, the entropy of the system is $\text{log}_{2}(W)$. It isn’t too hard to extend this idea to microstates that are not equally likely.</p><p>For example, let’s calculate the entropy of a box with 5 balls on the left and 5 balls on the right, but we replace one of the balls in the box with a metal ball that is pulled by a magnet to the left. In this case, the probability of each microstate is no longer equally likely. If we assume there is an $80$% chance that the metal ball is on the left side instead of the right side, then the entropy of the box can be calculated as follows: For all of the 252 microstates, 126 of them have the metal ball on the left, which has a $0.8$ chance of being true, and the other 126 have the metal ball on the right with a $0.2$ chance. This means using the $\eqref{shannon_entropy}$ we get an entropy of</p><p>\[- \sum_{i=1}^{126} \frac{0.2}{126} \cdot \text{log}_2(\frac{0.2}{126}) - \sum_{i=1}^{126} \frac{0.8}{126} \cdot \text{log}_2(\frac{0.8}{126}) = 7.69921\]</p><p>This is a little less than the box with normal balls which had $7.9772799235$ entropy. This is exactly what we should expect, we are a bit more certain about the outcome of this system since we knew where one of the balls was more likely to be.</p><p>But this raises a subtle question: why did we choose this particular set of microstates? For example, if we have the macrostate of 5 balls on the left and 5 balls on the right, but we decide to use the 50x50 grid of cells to describe the microstates, then there are far more microstates that satisfy the macrostate compared to when we were using the 2x1 grid of left and right.</p><p>Let’s calculate the entropy for those two examples. Keep in mind they both have the same macrostate: 5 balls on the left and 5 balls on the right.</p><ul><li>If we choose to use the microstates of looking at the position of individual balls between two cells splitting the box in half, then we can use n-choose-k to calculate that there are 252 possible combinations of balls across the two cells. This gives us an entropy of $\text{log}_2(252) = 7.977279923$.</li><li>If we choose to use the microstates of looking at the position of individual balls between 50x50 (2500) cells splitting the box into a grid, then we can use n-choose-k to calculate that there are 252 possible combinations of balls across the two halves of the box, for each of which every ball could be in any of 50x25 (1250) cells. This gives us an entropy of $\text{log}_2(252*1250^{10}) = 110.8544037$.</li></ul><p>This result lines up very well with our Information-theoretic understanding of entropy: when we allow more microstates to represent the same macrostate, we are more uncertain about the microstate our system is in. But this result does raise some concerns.</p><p>If different microstates give different entropy, how do we choose the right microstates for our problem? Unlike the macrostate, this decision of which microstates to use is not determined by our instruments or the scope of the problem, it has to be determined by the person making the calculation. Often for physical systems people will use the set of microstates that capture all the relevant information related to the macrostate. For example, if our macrostate is about balls on the left or right side of a box, then we probably don’t care about the ball’s velocity or mass or anything else but the ball position.</p><p>Another concern is that it feels wrong that the same physical system with the same macrostate can have different entropies depending on the microstate representation we use. Usually, we expect physical systems to have invariant measurements regardless of the internal representation we decide to use for our measurement. But this is incorrect for entropy. We need to recall that entropy is the uncertainty of a system and that the definition of entropy is completely dependent on what we are uncertain about, which for physical systems are the microstates. This would be similar to someone asking “How many parts make up that machine?”, to which we should respond “How do you define a ‘part’?”. When we ask “What is the entropy of this macrostate?”, we need to respond with “What microstates are we using?”.</p><p>With all that said, there is some small truth to what our intuition is telling us, although it doesn’t apply to the general case. While the entropy of the system changes when we change the microstates, the relative differences in entropy across macrostates will be equal <em>if</em> the new microstates uniformly multiply the old microstates. That is, if each original microstate is split into the same number of refined microstates, then the entropy of every macrostate increases by a constant. We’re getting lost in the terminology, an example will demonstrate.</p><p>Let us again take the 10 balls in a box, and we will calculate the entropy of the system for a few different macrostates and microstate representations. We indicate the number of balls on each side of the box with <code>(L, R)</code>, where <code>L</code> is the number of balls on the left and <code>R</code> is the number of balls on the right. Then we calculate the entropy using the microstate of a 2x1 grid of cells (just the left and right halves of the box) and for the 50x50 grid of cells.</p><div><table><thead><tr><th>&nbsp;</th><th>(10,0)</th><th>(9,1)</th><th>(8,2)</th><th>(7,3)</th><th>(6,4)</th><th>(5,5)</th><th>(4,6)</th><th>(3,7)</th><th>(2,8)</th><th>(1,9)</th><th>(0,10)</th></tr></thead><tbody><tr><td>2x1</td><td>0.00000</td><td>3.32193</td><td>5.49185</td><td>6.90689</td><td>7.71425</td><td>7.97728</td><td>7.71425</td><td>6.90689</td><td>5.49185</td><td>3.32193</td><td>0.00000</td></tr><tr><td>50x50</td><td>102.87712</td><td>106.19905</td><td>108.36898</td><td>109.78401</td><td>110.59137</td><td>110.85440</td><td>110.59137</td><td>109.78401</td><td>108.36898</td><td>106.19905</td><td>102.87712</td></tr></tbody></table></div><p>And if we look, we will see that the entropy in the 50x50 grid microstate values is just the 2x1 grid values plus a constant. The relative entropy in both cases would be identical. This is even more clear if we mathematically show how the entropy is calculated. For the 2x1 grid we use the equation $\text{log}_2({10 \choose L})$, and for the 50x50 grid we use $\text{log}_2(1250^{10} {10 \choose L}) = \text{log}_2(1250^{10}) + \text{log}_2({10 \choose L})$. Mathematically we can see that it is the same as the entropy of the 2x1 grid offset by $\text{log}_2(1250^{10})$.</p><p>You can imagine if we added another dimension along the microstates that we would increase the entropy again by a constant. For example, if each of the 10 balls could be one of 3 colors, then the number of microstates would grow by a factor of $3^{10}$, and so the entropy of the whole system would increase by $\text{log}_2(3^{10})$.</p><p>Our intuition was correct when we used different microstates that are multiples of each other, but that intuition fails if the microstates are not so neatly multiples of each other. An easy example of this is if we represent the left side of the box as one cell and the right as a 50x25 grid of cells, then the entropy looks very different. Below is the table again, but with the added row of our non-homogenous microstates. An example of how we calculate the entropy of macrostate $(3, 7)$ is: there are 120 equally likely ways to place 3 balls on the left and 7 balls on the right, but the balls on the right can also be in $1250^7$ different states, so the entropy is $\text{log}_2(120 \cdot 1250^7) = 78.920877252$.</p><div><table><thead><tr><th>&nbsp;</th><th>(10,0)</th><th>(9,1)</th><th>(8,2)</th><th>(7,3)</th><th>(6,4)</th><th>(5,5)</th><th>(4,6)</th><th>(3,7)</th><th>(2,8)</th><th>(1,9)</th><th>(0,10)</th></tr></thead><tbody><tr><td>2x1</td><td>0.00000</td><td>3.32193</td><td>5.49185</td><td>6.90689</td><td>7.71425</td><td>7.97728</td><td>7.71425</td><td>6.90689</td><td>5.49185</td><td>3.32193</td><td>0.00000</td></tr><tr><td>50x50</td><td>102.87712</td><td>106.19905</td><td>108.36898</td><td>109.78401</td><td>110.59137</td><td>110.85440</td><td>110.59137</td><td>109.78401</td><td>108.36898</td><td>106.19905</td><td>102.87712</td></tr><tr><td>mixed</td><td>0.00000</td><td>13.60964</td><td>26.06728</td><td>37.77003</td><td>48.86510</td><td>59.41584</td><td>69.44052</td><td>78.92088</td><td>87.79355</td><td>95.91134</td><td>102.87712</td></tr></tbody></table></div><p>A funny thing to note is that when all the balls are on the left, the entropy is zero, but when all the balls are on the right, the entropy is maximized. And again, hopefully, this makes sense from our understanding of entropy, that it measures uncertainty relative to our microstates. If we know all the balls are on the left, then we know they must be in the single left cell, so no uncertainty. If we know the balls are all on the right, then they could be in any of $1250^{10}$ microstates, so high uncertainty.</p><p>Clearly, we need to be careful and aware of what microstates we are choosing when measuring the entropy of a system. Fortunately, for most physical systems we use the standard microstates of a uniform grid of positions and momentums of the balls (particles) in the system. Another standard microstate to use is the continuous space of position and momentum.</p><h3 id="continuous-microstates"><span>Continuous Microstates</span><a href="#continuous-microstates"><i></i></a></h3><p>So far, we’ve looked at discrete sets of microstates — such as balls in cells. But in physical systems, microstates are often continuous: positions and momenta can vary over a continuum. How do we compute entropy in this setting? This is not related to the rest of the explanation, but it is an interesting tangent to explore.</p><p>Let’s return to our 10 balls in a 2D box. If each ball can occupy any position in the square, then the microstate of the system is a point in a $20$-dimensional space (2 dimensions per ball). The number of possible microstates is infinite — and each individual one has infinitesimal probability.</p><p>In this setting, we use a probability density function $\rho(x)$, and entropy becomes a continuous integral:</p><p>\[S = - \int_X \rho(x) \log_2 \rho(x) \, dx\]</p><p>This is called differential entropy. It generalizes Shannon entropy to continuous systems, though it has some subtleties — it can be negative, and it’s not invariant under coordinate transformations.</p><p>If the density is uniform, say $\rho(x) = \frac{1}{V}$ over a region of volume $V$, then the entropy becomes:</p><p>\[S = - \int_X \frac{1}{V} \log_2 \left( \frac{1}{V} \right) dx = \log_2(V)\]</p><p>So entropy still grows with the logarithm of the accessible state volume, just as in the discrete case.</p><p>This formalism is particularly natural in quantum mechanics, where the wavefunction $\psi(x)$ defines a probability density $\rho(x) = |\psi(x)|^2$. Consider a 1D Gaussian wavefunction:</p><p>\[\psi(x) = \left( \frac{1}{\pi \sigma^2} \right)^{1/4} e^{-x^2 / (2 \sigma^2)}\]</p><p>Its entropy (in bits) is:</p><p>\[S = - \int_{-\infty}^{\infty} \rho(x) \log_2 \rho(x) \, dx = \frac{1}{2} \log_2(2 \pi e \sigma^2)\]</p><p>This shows that wider distributions have higher entropy, as expected: a more spread-out wavefunction indicates more uncertainty in the particle’s location.</p><p>For instance:</p><ul><li>If $\sigma = 1$, then $S \approx 2.047$</li><li>If $\sigma = 3$, then $S \approx 3.600$</li></ul><p>Which again should make sense: When we are less certain about a system, like where a particle will be when measured, the more entropy it has.</p><p>And a quick issue to address: If the state space is unbounded, like momentum in classical mechanics, then the entropy can diverge. This isn’t a problem in practice because physical systems typically have probability distributions (like Gaussians) that decay quickly enough at infinity to keep the entropy finite. When that’s not the case, we either limit the system to a finite region or focus on entropy differences, which remain well-defined even when absolute entropy diverges.</p><p>But let’s get back to our main topic, and we’ll get back into it with a historical overview.</p><h3 id="standard-usage-of-entropy"><span>Standard Usage of Entropy</span><a href="#standard-usage-of-entropy"><i></i></a></h3><p>Eighty years before Claude Shannon developed Information Theory, <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann">Ludwig Boltzmann</a> formulated a statistical definition of entropy for an ideal gas. He proposed that the entropy $S$ of a system is proportional to the logarithm of the number of microstates $W$ consistent with a given macrostate:</p><p>\[\begin{align} S = k_{B} \ln(W) \label{boltzmann_entropy}\tag{Boltzmann entropy} \end{align}\]</p><p>This equation should look familiar: it’s the equal-probability special case of the Shannon entropy we’ve been using, just with a change of base (from $\log_2$ to $\ln$) and a scaling factor $k_B$ (Boltzmann’s constant). The connection between Boltzmann’s statistical mechanics and Shannon’s information theory is more than historical coincidence—both quantify uncertainty, whether in physical states or messages.</p><p>A few years later, <a href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">Josiah Willard Gibbs</a> generalized Boltzmann’s definition to cases where microstates are not equally likely. His formulation remains the standard definition of entropy in modern physics:</p><p>\[\begin{align} S = -k_B \sum_{i} p_i \ln(p_i) \label{gibbs_entropy}\tag{Gibbs entropy} \end{align}\]</p><p>This is formally identical to Shannon entropy, again differing only in logarithm base and physical units. But Gibbs’s generalization was a profound leap: it enabled thermodynamics to describe systems in contact with heat baths, particle reservoirs, and other environments where probability distributions over microstates are non-uniform. This made entropy applicable far beyond ideal gases—covering chemical reactions, phase transitions, and statistical ensembles of all kinds.</p><p>Now that we have a formal understanding of entropy with some historical background, let’s try to understand how entropy relates to our universe and in particular to time.</p><h3 id="time"><span>Time</span><a href="#time"><i></i></a></h3><p>How does time play a role in all of this?</p><p>When you drop a spot of milk into tea, it always spreads and mixes, and yet you never see the reverse where the milk molecules spontaneously separate and return to a neat droplet. When ocean waves crash into the shore, the spray and foam disperse, but we never see that chaos reassemble into a coherent wave that launches back into the sea. These examples are drawn from this <a href="https://www.youtube.com/watch?v=ROrovyJXSnM">lecture on entropy</a> by Richard Feynman. If you were shown a reversed video of these events, you’d immediately recognize something was off. This sounds obvious at first, but it actually isn’t clear this should be true if we just look at the laws of physics. All the known laws of physics are time-reversible (the wave function collapse seems to be debatable), which just means that they <em>do</em> look the same playing forward and backward. The individual molecules all obey these time-reversible laws, and yet the cup of tea gets murky from the milk always mixing in.</p><p>This highlights a fundamental paradox: the microscopic laws of physics are time-reversible, but the macroscopic world is not. If you took a video of two atoms bouncing off each other and played it backward, it would still look physically valid, but play a video of milk mixing into coffee backward, and it looks obviously wrong.</p><p>We want to build a simplified model of time in a way that reflects both the time-reversibility of microscopic laws and the time-asymmetry of macroscopic behavior. Let’s imagine the complete state of a physical system, like a box of particles, as a single point in a high-dimensional space called phase space, with each dimension corresponding to a particle’s position and momentum. As time evolves, the system traces out a continuous trajectory through this space.</p><p>The laws of physics, such as Newton’s equations, Hamiltonian mechanics, or Schrödinger’s equation, all govern this trajectory. They are deterministic and time-reversible. That means if you reverse the momenta of all particles at any moment, the system will retrace its path backward through state space.</p><p>So far everything is time-reversible, including this view of how the universe moves through time. But we will see that even in this toy model, time appears to have a preferred direction, an <em>arrow of time</em>.</p><p>The key lies in coarse-graining. When we observe the world, we don’t see every microscopic detail. Instead, we measure macrostates: aggregate properties like temperature, pressure, position of an object, or color distribution in a cup of tea. Each macrostate corresponds to many underlying microstates — and not all macrostates are created equal.</p><p>For example, consider a box sliding across the floor and coming to rest due to friction. At the microscopic level, the system is just particles exchanging momentum, and all time-reversible. But we certainly would not call this action time-reversible, we never see a box spontaneously start speeding up from stand-still. But, if we took the moment after the box comes to a rest due to friction, and you reversed the velocities of all the particles (including those in the floor that absorbed the box’s kinetic energy as heat), the box <em>would</em> spontaneously start moving and slide back to its original position. This would obey Newton’s laws, but it’s astronomically unlikely. Why?</p><p>The number of microstates where the energy is spread out as heat (the box is at rest, and the molecules in the floor are jiggling) vastly outnumber the microstates where all that energy is coordinated to move the box. The stand-still macrostate has high entropy while the spontaneous-movement macrostate has low entropy. When the system evolves randomly or deterministically from low entropy, it is overwhelmingly likely to move toward higher entropy simply because there are more such microstates.</p><p>If you had perfect knowledge of all particles in the universe (i.e., you lived at the level of microstates), time wouldn’t seem to have a direction. But from the perspective of a coarse-grained observer, like us, entropy tends to increase. And that’s why a movie of tea mixing looks natural, but the reverse looks fake. At the level of physical laws, both are valid. But one is typical, and one is astronomically rare, all because we coarse-grained.</p><p>To drive the point home, let’s again look at the balls in a box. We’ll define macrostates by dividing the box into a grid of cells and counting how many balls are in each bin.</p><p>Now suppose the balls move via random small jitters (our toy model of microscopic dynamics). Over time, the system will naturally tend to explore the most probable macrostates, as the most probable macrostates have far more microstates for you to wander into. That is, entropy increases over time, not because of any fundamental irreversibility in the laws, but because high-entropy macrostates are far more typical.</p><p>If we started the simulation with all the balls packed on the left, that’s a very specific (low entropy) macrostate. As they spread out, the number of compatible microstates grows, and so does the entropy.</p><p>This leads to a crucial realization: Entropy increases because we started in a low-entropy state. This is often called the <a href="https://en.wikipedia.org/wiki/Past_hypothesis">Past Hypothesis</a>, the postulate that the universe began in an extremely low-entropy state. Given that, the Second Law of Thermodynamics follows naturally. The arrow of time emerges not from the dynamics themselves, but from the statistical unlikelihood of reversing them after coarse-graining, and the fact that we began in a low-entropy state.</p><p>You could imagine once a system reaches near-maximum entropy that it no longer looks time-irreversible. The entropy of such a system would <a href="https://en.wikipedia.org/wiki/Fluctuation_theorem">fluctuate a tiny bit</a> since entropy is an inherently statistical measure, but they would be small enough not to notice. For example, while it is clear when a video of milk being poured into tea (a low-entropy macrostate) is playing forward as opposed to backward, you couldn’t tell if a video of already-combined milk and tea (a high-entropy macrostate) being swirled around is playing forward or backward.</p><p>While there are tiny fluctuations in entropy, they are not enough to explain the large-scale phenomena that sometimes seem to violate this principle that we just established of entropy always increasing with time.</p><h3 id="violations-of-the-second-law"><span>Violations of the Second Law?</span><a href="#violations-of-the-second-law"><i></i></a></h3><p>Some real-world examples seem to contradict the claim that entropy always increases. For instance, oil and water separate after mixing, dust clumps into stars and planets, and we build machines like filters and refrigerators that separate mixed substances. Aren’t these violations?</p><p>The issue is we have only been considering the position of molecules, while physical systems have many different properties which allow for more microstates. For example, if we start considering both the position and velocity of balls in a box, then the entropy can be high even while all the balls are on the left side of the box since every ball could have a different velocity. If the balls were all on the left <em>and</em> the velocities were all the same, then the entropy would be low. Once we consider velocity as well, entropy can increase both from more spread out positions and more spread out velocities.</p><p>When water and oil separate, the positions of the molecules separate into top and bottom, which appears to decrease positional entropy. However, this separation actually increases the total entropy of the system. Why? Water molecules strongly prefer to form hydrogen bonds with other water molecules rather than interact with oil molecules. When water molecules are forced to be near oil molecules in a mixed state, they must adopt more constrained arrangements to minimize unfavorable interactions, reducing the number of available microstates. When water and oil separate, water molecules can interact freely with other water molecules in more configurations, and oil molecules can interact with other oil molecules more freely. This increase in available microstates for molecular arrangements and interactions more than compensates for the decrease in positional mixing entropy. So, while the entropy decreases if we only consider the general positions of molecules (mixed versus separated), the total entropy increases when we account for all the molecular interactions, orientations, and local arrangements. This demonstrates why we need to consider all properties of a system when calculating its entropy.</p><p>When stars or planets form together from dust particles floating around in space and clump together from gravity, it would seem that even when we consider position and velocity of the particles that the entropy might be decreasing. Even though the particles speed up to clump together, they slow down after they collide, seemingly decreasing entropy. This is because we are again failing to consider the entire system. When particles collide with each other, their speed decreases a bit by turning that kinetic energy into radiation, causing photons to get sent out into space. If we considered a system where radiation isn’t allowed, then the kinetic energy would just get transferred from one particle to another through changes in velocity, and the entropy of the system would still be increasing because of the faster velocities. Once we start considering the entropy of the position, velocity, and <em>all</em> particles in a system, we can consider <em>all</em> the microstates that are equally likely and calculate the correct entropy.</p><p>Similarly, once we consider the entire system around a refrigerator, the decrease in entropy disappears. The entropy from the power generated to run the refrigerator and the heat moved from the inside to the outside of the refrigerator will offset the decrease in entropy caused by cooling the inside of the refrigerator. Local decreases in entropy <em>can</em> be generated, as long as the entropy of the entire system is still increasing.</p><p>Ensure that the entire system is being considered when analyzing the entropy of a system, with the position, velocity, other interactions of particles, that all particles are included, and that the entire system is actually being analyzed.</p><h3 id="disorder"><span>Disorder</span><a href="#disorder"><i></i></a></h3><p>Entropy is sometimes described as “disorder,” but this analogy is imprecise and often misleading. In statistical mechanics, entropy has a rigorous definition: it quantifies the number of microstates compatible with a given macrostate. That is, entropy measures our uncertainty about the exact microscopic configuration of a system given some coarse-grained, macroscopic description.</p><p>So where does the idea of “disorder” come from?</p><p>Empirically, macrostates we label as “disordered” often correspond to a vastly larger number of microstates than those we consider “ordered”. For example, in a child’s room, there are many more configurations where toys are scattered randomly than ones where everything is neatly shelved. Since the scattered room corresponds to more microstates, it has higher entropy.</p><p>But this connection between entropy and disorder is not fundamental. The problem is that “disorder” is subjective—it depends on human perception, context, and labeling. For instance, in our earlier example of 1000 balls bouncing around a box, a perfectly uniform grid of balls would have high entropy due to the huge number of possible microstates realizing it. And yet to a human observer, such a grid might appear highly “ordered.”</p><p>The key point is: entropy is objective and well-defined given a macrostate and a set of microstates, while “disorder” is a human-centric heuristic concept that sometimes, but not always, tracks entropy. Relying on “disorder” to explain entropy risks confusion, especially in systems where visual symmetry or regularity masks the underlying statistical structure.</p><h2 id="conclusion"><span>Conclusion</span><a href="#conclusion"><i></i></a></h2><p>So here are some thoughts in regard to some common statements made about entropy:</p><ul><li>Entropy is a measure of disorder.<ul><li>“disorder” is a subjective term for states of a system that humans don’t find useful/nice, and usually has much higher entropy than the “ordered” macrostate that humans create. Because of this, when entropy increases, it is more likely that we end up in disordered state, although not guaranteed.</li></ul></li><li>Entropy always increases in a closed system.<ul><li>This is a statistical statement that for all practical purposes is true, but is not guaranteed and can fail when you look at very small isolated systems or measure down to the smallest details of a system. It also assumes you started in a low-entropy state, giving your system space to increase in entropy. This has the neat implication that since our universe has been observed to be increasing in entropy, it must have begun in a low-entropy state.</li></ul></li><li>Heat flows from hot to cold because of entropy.<ul><li>Heat flows from hot to cold because the number of ways in which the system can be non-uniform in temperature is much lower than the number of ways it can be uniform in temperature, and so as the system “randomly” moves to new states, it will statistically end up in states that are more uniform.</li></ul></li><li>Entropy is the only time-irreversible law of physics.<ul><li>All the fundamental laws of physics are time-reversible, but by coarse-graining and starting from a lower-entropy state, a system will statistically move to a higher-entropy state. This means if a system is already in a near-maximum entropy state (either because of its configuration or because of the choice for coarse-graining) or we don’t coarse-grain, then entropy will not look time-irreversible.</li></ul></li></ul><p>And here is some further reading, all of which I found supremely helpful in learning about entropy.</p><ul><li><a href="https://www.youtube.com/watch?v=ROrovyJXSnM">Lecture on entropy by Richard Feynman</a></li><li><a href="https://scholar.harvard.edu/files/schwartz/files/6-entropy.pdf">Lecture notes on entropy from the Statistical Mechanics course at Harvard taught by Matthew Schwartz</a></li><li><a href="https://math.ucr.edu/home/baez/what_is_entropy.pdf">A both friendly and rigorous textbook on entropy by John C. Baez</a></li><li><a href="https://www.youtube.com/watch?v=VCXqELB3UPg">A youtube video on entropy using actual balls bouncing in a box</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard's response to federal government letter demanding changes (1029 pts)]]></title>
            <link>https://www.harvard.edu/president/news/2025/the-promise-of-american-higher-education/</link>
            <guid>43684536</guid>
            <pubDate>Mon, 14 Apr 2025 18:28:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.harvard.edu/president/news/2025/the-promise-of-american-higher-education/">https://www.harvard.edu/president/news/2025/the-promise-of-american-higher-education/</a>, See on <a href="https://news.ycombinator.com/item?id=43684536">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

			<div>
				
<div><p>Dear Members of the Harvard Community,</p><p>&nbsp;For three-quarters of a century, the federal government has awarded grants and contracts to Harvard and other universities to help pay for work that, along with investments by the universities themselves, has led to groundbreaking innovations across a wide range of medical, engineering, and scientific fields. These innovations have made countless people in our country and throughout the world healthier and safer. In recent weeks, the federal government has threatened its partnerships with several universities, including Harvard, over accusations of antisemitism on our campuses. These partnerships are among the most productive and beneficial in American history. New frontiers beckon us with the prospect of life-changing advances—from treatments for diseases such as Alzheimer’s, Parkinson’s, and diabetes, to breakthroughs in artificial intelligence, quantum science and engineering, and numerous other areas of possibility. For the government to retreat from these partnerships now risks not only the health and well-being of millions of individuals but also the economic security and vitality of our nation.</p><p>&nbsp;Late Friday night, the administration issued an updated and expanded list of demands, warning that Harvard must comply if we intend to “maintain [our] financial relationship with the federal government.” It makes clear that the intention is not to work with us to address antisemitism in a cooperative and constructive manner. Although some of the demands outlined by the government are aimed at combating antisemitism, the majority represent direct governmental regulation of the “intellectual conditions” at Harvard.</p><p>&nbsp;I encourage you to&nbsp;<a href="https://www.harvard.edu/research-funding/wp-content/uploads/sites/16/2025/04/Letter-Sent-to-Harvard-2025-04-11.pdf" target="_blank" rel="noreferrer noopener">read the letter</a>&nbsp;to gain a fuller understanding of the unprecedented demands being made by the federal government to control the Harvard community. They include requirements to “audit” the viewpoints of our student body, faculty, staff, and to “reduc[e] the power” of certain students, faculty, and administrators targeted because of their ideological views. We have informed the administration through our legal counsel that&nbsp;<a href="https://www.harvard.edu/research-funding/wp-content/uploads/sites/16/2025/04/Harvard-Response-2025-04-14.pdf" target="_blank" rel="noreferrer noopener">we will not accept their proposed agreement</a>. The University will not surrender its independence or relinquish its constitutional rights.</p><p>&nbsp;The administration’s prescription goes beyond the power of the federal government. It violates Harvard’s First Amendment rights and exceeds the statutory limits of the government’s authority under Title VI. And it threatens our values as a private institution devoted to the pursuit, production, and dissemination of knowledge. No government—regardless of which party is in power—should dictate what private universities can teach, whom they can admit and hire, and which areas of study and inquiry they can pursue.</p><p>&nbsp;Our motto—Veritas, or truth—guides us as we navigate the challenging path ahead. Seeking truth is a journey without end. It requires us to be open to new information and different perspectives, to subject our beliefs to ongoing scrutiny, and to be ready to change our minds. It compels us to take up the difficult work of acknowledging our flaws so that we might realize the full promise of the University, especially when that promise is threatened.</p><p>&nbsp;We have made it abundantly clear that we do not take lightly our moral duty to fight antisemitism. Over the past fifteen months, we have taken many steps to address antisemitism on our campus. We plan to do much more. As we defend Harvard, we will continue to:&nbsp;</p></div>



<ul>
<li>nurture a thriving culture of open inquiry on our campus; develop the tools, skills, and practices needed to engage constructively with one another; and broaden the intellectual and viewpoint diversity within our community;&nbsp;</li>



<li>affirm the rights and responsibilities we share; respect free speech and dissent while also ensuring that protest occurs in a time, place, and manner that does not interfere with teaching, learning, and research; and enhance the consistency and fairness of disciplinary processes; and&nbsp;</li>



<li>work together to find ways, consistent with law, to foster and support a vibrant community that exemplifies, respects, and embraces difference. As we do, we will also continue to comply with&nbsp;<em>Students For Fair Admissions v. Harvard</em>, which ruled that Title VI of the Civil Rights Act makes it unlawful for universities to make decisions “on the basis of race.”&nbsp;</li>
</ul>



<div><p>These ends will not be achieved by assertions of power, unmoored from the law, to control teaching and learning at Harvard and to dictate how we operate. The work of addressing our shortcomings, fulfilling our commitments, and embodying our values is ours to define and undertake as a community. Freedom of thought and inquiry, along with the government’s longstanding commitment to respect and protect it, has enabled universities to contribute in vital ways to a free society and to healthier, more prosperous lives for people everywhere. All of us share a stake in safeguarding that freedom. We proceed now, as always, with the conviction that the fearless and unfettered pursuit of truth liberates humanity—and with faith in the enduring promise that America’s colleges and universities hold for our country and our world.</p><p>&nbsp;Sincerely,<br>Alan M. Garber</p></div>
			</div>

			

			
		</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Federal Government's letter to Harvard demanding changes [pdf] (132 pts)]]></title>
            <link>https://www.harvard.edu/research-funding/wp-content/uploads/sites/16/2025/04/Letter-Sent-to-Harvard-2025-04-11.pdf</link>
            <guid>43684386</guid>
            <pubDate>Mon, 14 Apr 2025 18:13:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.harvard.edu/research-funding/wp-content/uploads/sites/16/2025/04/Letter-Sent-to-Harvard-2025-04-11.pdf">https://www.harvard.edu/research-funding/wp-content/uploads/sites/16/2025/04/Letter-Sent-to-Harvard-2025-04-11.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43684386">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[AudioX: Diffusion Transformer for Anything-to-Audio Generation (125 pts)]]></title>
            <link>https://zeyuet.github.io/AudioX/</link>
            <guid>43683907</guid>
            <pubDate>Mon, 14 Apr 2025 17:35:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zeyuet.github.io/AudioX/">https://zeyuet.github.io/AudioX/</a>, See on <a href="https://news.ycombinator.com/item?id=43683907">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <!-- <section class="hero"> -->
  <section>
    <!-- <div class="hero-body"> -->
    <div>
          
          

          <p><span><sup>1</sup>HKUST</span>
          </p>

          <p><span><sup>†</sup>Corresponding authors
            </span>
          </p>
          
        </div>
    
    <!-- </section> -->

    <div>
        <!-- Abstract. -->
        <div>
            <h2>Abstract</h2>
            <p>
                Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. 
              </p>
          </div>
        <!--/ Abstract. -->

        <!-- Paper video. -->
        <div>
            <h2>Demo Video</h2>
    
            
    
          </div>

        <!--/ Paper video. -->
      </div>

    <div>
    

            

            
  <!-- 单个任务示例：Text-to-Audio Generation -->
  <div>
    <!-- 外层容器，带边框 -->


      <!-- 任务标题 -->
      <h2>Text-to-Audio Generation</h2>

      <!-- 预览区域：2-3个示例 -->
      <div>
          <!-- 示例3 -->
          <div>
            <p><strong>Prompt:</strong>
              <br>  
              Thunder and rain during a sad piano solo</p>
            </div>          
          <!-- 示例1 -->
          <div>
            <p><strong>Prompt:</strong>
              <br>  
              Typing on a keyboard</p>
            </div>
          <!-- 示例2 -->
          <div>
            <p><strong>Prompt:</strong> 
              <br>  
              Ocean waves crashing</p>
            </div>           
     

        </div>

      <!-- 折叠按钮：点击展开更多样本 -->
      <!-- 折叠内容：更多 sample，带 carousel -->
      <div>

            <div>
              <p><strong>Prompt:</strong>
                A person is snoring</p>
              </div>              
            <div>
              <p><strong>Prompt:</strong> A toilet flushing</p>
              </div>

            <div>
              <p><strong>Prompt:</strong> Rain falling on a rooftop</p>
              </div>
            <div>
              <p><strong>Prompt:</strong> An airplane is taking flight</p>
              </div>                                                   
            <div>
              <p><strong>Prompt:</strong> An explosion and crackling</p>
              </div>
            
            <div>
              <p><strong>Prompt:</strong> Footsteps in snow</p>
              </div>

            <div>
              <p><strong>Prompt:</strong> A cat meowing repeatedly</p>
              </div>

            <div>
              <p><strong>Prompt:</strong> Food and oil sizzling</p>
              </div>
            <!-- 继续添加更多 sample-item ... -->
          </div>

  </div>


  <!-- 单个任务示例：Text-to-Music Generation -->
  <div>
    <!-- 外层容器，带边框 -->

      <!-- 任务标题 -->
      <h2>Text-to-Music Generation</h2>

      <!-- 预览区域：2-3个示例 -->
      <div>
          <!-- 示例1 -->
          <div>
            <p><strong>Prompt:</strong> 
              <br>  
              Orchestral, epic, with drums, strings, 
              <br>  
              and brass</p>
            </div>
          <!-- 示例2 -->
          <div>
            <p><strong>Prompt:</strong> 
              <br>  
              Electronic dance music with synthesizers, bass, drums, and a slow build-up</p>
            </div>
          <!-- 示例3 -->
          <div>
            <p><strong>Prompt:</strong> 
              <br>  
              Sad emotional soundtrack with ambient textures and solo cello</p>
            </div>
        </div>

      <!-- 折叠按钮：点击展开更多样本 -->
      <!-- 折叠按钮：点击展开更多样本 -->
      <!-- 折叠内容：更多 sample，带 carousel -->
      <div>
            <div>
              <p><strong>Prompt:</strong> 
                A suspenseful scene in a haunted mansion</p>
              </div>
            <div>
              <p><strong>Prompt:</strong> 
                An orchestral music piece for a fantasy world</p>
              </div>
            <div>
              <p><strong>Prompt:</strong> 
                
                Uplifting ukulele tune for a travel vlog</p>
              </div>
            <div>
              <p><strong>Prompt:</strong> 
                
                Romantic acoustic guitar music for a sunset scene</p>
              </div>                                                
            <div>
              <p><strong>Prompt:</strong> 
                Smooth urban R&amp;B beat with a mellow groove</p>
              </div>

            <div>
              <p><strong>Prompt:</strong> 
                Produce upbeat electronic music for a dance party</p>
              </div>
            
            <div>
              <p><strong>Prompt:</strong> 
                Playful 8-bit chiptune music for a retro platformer game</p>
              </div> 
            
            <div>
              <p><strong>Prompt:</strong> 
                Ambient synth music in a deep space setting</p>
              </div>             
            <!-- 继续添加更多 sample-item ... -->
          </div>


  </div>




    
            <!-- Video-to-Audio Generation -->
            <div>

                <h2>Video-to-Audio Generation</h2>
                

      <!-- 折叠按钮：点击展开更多样本 -->
      <!-- 折叠内容：更多 sample，带 carousel -->
      

            </div>
    
            <!-- Video-to-Music Generation -->
            <div>

                <h2>Video-to-Music Generation</h2>
                
      <!-- 折叠按钮：点击展开更多样本 -->
      <!-- 折叠内容：更多 sample，带 carousel -->
      
          </div>
    




    </div>
    
    
    
    
    

    
    <div>
            <h2>Teaser</h2>
            <!-- <div class="columns is-centered"> -->
            <p><img src="https://zeyuet.github.io/AudioX/static/images/teaser.png" alt="Teaser." height="100%" width="100%"></p><p>
                (a) Overview of AudioX, illustrating its capabilities across various tasks. (b) Radar chart comparing the performance of different methods across multiple benchmarks. AudioX demonstrates superior Inception Scores (IS) across a diverse set of datasets in audio and music generation tasks.
              </p>
          </div>

    <div>
            <h2>Method</h2>
            <!-- <div class="columns is-centered"> -->
            <p><img src="https://zeyuet.github.io/AudioX/static/images/method-.png" alt="Method." height="100%" width="100%"></p><p>
                The AudioX Framework.
              </p>
          </div>


    <div id="BibTeX">
        <h2>BibTeX</h2>
        <p>
          If you find our work useful, please consider citing:</p>
        <pre><code>@article{tian2025audiox,
          title={AudioX: Diffusion Transformer for Anything-to-Audio Generation},
          author={Tian, Zeyue and Jin, Yizhu and Liu, Zhaoyang and Yuan, Ruibin and Tan, Xu and Chen, Qifeng and Xue, Wei and Guo, Yike},
          journal={arXiv preprint arXiv:2503.10522},
          year={2025}
        }</code></pre>
      </div>



    
    
    
    

    






</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Podman Quadlets with Podman Desktop (130 pts)]]></title>
            <link>https://podman-desktop.io/blog/podman-quadlet</link>
            <guid>43683641</guid>
            <pubDate>Mon, 14 Apr 2025 17:16:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://podman-desktop.io/blog/podman-quadlet">https://podman-desktop.io/blog/podman-quadlet</a>, See on <a href="https://news.ycombinator.com/item?id=43683641">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container"><p><img decoding="async" loading="lazy" alt="banner" src="https://podman-desktop.io/assets/images/banner-b4811c66dc40efde426f577bd07fc7fd.png" width="1216" height="832"></p>
<p>Containers are typically deployed in Kubernetes clusters.
However, for smaller-scale use cases such as on a single-node server or during development, Kubernetes can be overkill.</p>
<p>What’s a more lightweight solution for running autonomous applications with multiple interacting containers?</p>
<p>In this blog, we'll dive into what Quadlets are, their benefits, and how to use them within Podman Desktop.</p>
<h2 id="what-are-quadlets">What Are Quadlets?<a href="#what-are-quadlets" aria-label="Direct link to What Are Quadlets?" title="Direct link to What Are Quadlets?">​</a></h2>
<p>Podman Quadlets allow you to manage containers declaratively using systemd<sup><a href="#user-content-fn-1-4453df" id="user-content-fnref-1-4453df" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>.
Since version 4.4, Podman can create, start, and manage containers (including pulling images, creating volumes, and managing pods) through systemd.</p>
<p>Quadlets are simplified configuration files—recognized by their specific extensions,
such as <code>*.container</code>, <code>*.pod</code>, or <code>*.image</code> that are processed during startup or when you reload the daemon using the <code>systemctl daemon-reload</code> command.</p>
<p>Quadlets generate the equivalent systemd unit files, streamlining the container management process.</p>
<h3 id="why-use-quadlets">Why Use Quadlets?<a href="#why-use-quadlets" aria-label="Direct link to Why Use Quadlets?" title="Direct link to Why Use Quadlets?">​</a></h3>
<ul>
<li><strong>Declarative Configuration</strong>: Similar to Compose or Kubernetes manifests, Quadlets allow you to declare what you want to run, simplifying the workload setup.</li>
<li><strong>Tight System Integration</strong>: Quadlets align with Podman’s philosophy of integrating seamlessly with Linux, leveraging systemd’s process management capabilities.</li>
<li><strong>Ease of Automation</strong>: Quadlets make it simple to configure containers to start at boot, restart on failure, and more.</li>
</ul>
<h3 id="example-a-quadlet-file-for-nginx">Example: A Quadlet File for Nginx<a href="#example-a-quadlet-file-for-nginx" aria-label="Direct link to Example: A Quadlet File for Nginx" title="Direct link to Example: A Quadlet File for Nginx">​</a></h3>
<p>Below is an example of an <code>nginx.container</code> Quadlet file, which starts an nginx container at boot:</p>
<div><p>~/.config/containers/systemd/nginx.container</p><div><pre tabindex="0"><code><span><span># nginx.container</span><br></span><span><span>[Container]</span><br></span><span><span>ContainerName=nginx</span><br></span><span><span>Image=nginx</span><br></span><span><span>PublishPort=80:8080</span><br></span><span><span></span><br></span><span><span>[Service]</span><br></span><span><span>Restart=always</span><br></span></code></pre></div></div>
<p>This configuration ensures the container restarts automatically if stopped, and exposes port 8080.</p>
<h2 id="using-the-podman-quadlet-extension-in-podman-desktop">Using the Podman Quadlet Extension in Podman Desktop<a href="#using-the-podman-quadlet-extension-in-podman-desktop" aria-label="Direct link to Using the Podman Quadlet Extension in Podman Desktop" title="Direct link to Using the Podman Quadlet Extension in Podman Desktop">​</a></h2>
<p>Managing Quadlets directly on non-Linux platforms can be challenging due to virtualized environments (e.g., WSL or Hyper-V).
Fortunately, the Podman Desktop extension Podman Quadlet simplifies this process, enabling you to list, generate, and edit Quadlets visually.</p>
<h3 id="key-features-of-the-extension">Key Features of the Extension<a href="#key-features-of-the-extension" aria-label="Direct link to Key Features of the Extension" title="Direct link to Key Features of the Extension">​</a></h3>
<ul>
<li><strong>Integration with Podlet</strong>: Generates Quadlets from existing Podman objects<sup><a href="#user-content-fn-2-4453df" id="user-content-fnref-2-4453df" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>.</li>
<li><strong>Quadlet Management UI</strong>: Provides a dedicated interface to list, edit, delete, start, and stop Quadlets.</li>
<li><strong>Logs Viewer</strong>: Fetches and displays systemd logs using journalctl for troubleshooting.</li>
</ul>
<h3 id="installation">Installation<a href="#installation" aria-label="Direct link to Installation" title="Direct link to Installation">​</a></h3>
<p>If you already have the latest version of Podman Desktop, you can <a href="podman-desktop:extension/podman-desktop.quadlet"><strong>click here to install the Podman Quadlet extension</strong></a>.</p>
<p>Alternatively, navigate to the Extensions page within Podman Desktop to install it.</p>
<h3 id="list-quadlets-clipboard">List Quadlets <!-- -->📋<a href="#list-quadlets-clipboard" aria-label="Direct link to list-quadlets-clipboard" title="Direct link to list-quadlets-clipboard">​</a></h3>
<p>On the Podman Quadlet page, you can view all the Quadlets available across your Podman machines. To update the list, click <strong>Refresh</strong>.</p>
<p><img src="https://podman-desktop.io/assets/images/podman-quadlet-home-light-dd43d64eaf29d2f9b622f501dff42ffb.png" alt="Quadlets List"><img src="https://podman-desktop.io/assets/images/podman-quadlet-home-dark-c9e82c34f0facb36b072159c54b56ce9.png" alt="Quadlets List"></p><p>In Podman Desktop, you can see that a dedicated icon is used for the containers managed by a Quadlet.</p>
<p><img src="https://podman-desktop.io/assets/images/container-icon-quadlet-light-b86c5a14b2a3f6583a0efa67e6b87b28.png" alt="Container Quadlet Icon"><img src="https://podman-desktop.io/assets/images/container-icon-quadlet-dark-813e9532612012aad3a46278d8db8f2d.png" alt="Container Quadlet Icon"></p><h3 id="generate-quadlets-hammer">Generate Quadlets <!-- -->🔨<a href="#generate-quadlets-hammer" aria-label="Direct link to generate-quadlets-hammer" title="Direct link to generate-quadlets-hammer">​</a></h3>
<p>To generate a Quadlet from an existing container, you’ll need to install <a href="https://github.com/containers/podlet" target="_blank" rel="noopener noreferrer">Podlet</a>. The extension simplifies installation.</p>
<p>Use one of the following ways to install Podlet:</p>
<ul>
<li>Go to <strong> Settings &gt; CLI Tools</strong> and install Podlet using the Podman Quadlet extension.</li>
<li>Download Podlet manually from its <a href="https://github.com/containers/podlet/releases" target="_blank" rel="noopener noreferrer">GitHub release page</a>.</li>
</ul>
<p><img src="https://podman-desktop.io/assets/images/cli-podlet-light-c3a1435d0d7961b4a62a38e29ffd63fc.png" alt="Podlet Installation"><img src="https://podman-desktop.io/assets/images/cli-podlet-dark-58159bbeed8fc3dda26e0e88dfe891ca.png" alt="Podlet Installation"></p><h4 id="example-generate-a-container-quadlet">Example: Generate a Container Quadlet<a href="#example-generate-a-container-quadlet" aria-label="Direct link to Example: Generate a Container Quadlet" title="Direct link to Example: Generate a Container Quadlet">​</a></h4>
<ol>
<li>Start a container using Podman:</li>
</ol>
<div><pre tabindex="0"><code><span><span>podman</span><span> run </span><span>--name</span><span> nginx-demo </span><span>-d</span><span> </span><span>-p</span><span> </span><span>80</span><span>:8080 nginx</span><br></span></code></pre></div>
<ol start="2">
<li>In Podman Desktop, find your container on the Containers page.</li>
<li>Click the <strong>overflow menu</strong> icon and select <strong>Generate Quadlet</strong>.</li>
</ol>
<p><img src="https://podman-desktop.io/assets/images/generate-quadlet-action-light-9d7bc4b162aa72adeb6d1bf91f72acd7.png" alt="Container actions"><img src="https://podman-desktop.io/assets/images/generate-quadlet-action-dark-76b7699d969dd5f80fd28c6b3acd1fe2.png" alt="Container actions"></p><ol start="4">
<li>Click <strong>Generate</strong> to finalize the Quadlet.</li>
</ol>
<p><img src="https://podman-desktop.io/assets/images/generate-form-options-light-e09f8de6226947ec69a1548ff8624a0e.png" alt="Quadlet Generate Form"><img src="https://podman-desktop.io/assets/images/generate-form-options-dark-53ccd0966f5fbee1dbd9f9e5223d80d1.png" alt="Quadlet Generate Form"></p><ol start="5">
<li>Optional: Edit the Quadlet configuration details.</li>
<li>Click <strong>Load into machine</strong>.</li>
</ol>
<p><img src="https://podman-desktop.io/assets/images/generate-form-edit-light-a63038e484f23e452c00c67c2f4ea2ff.png" alt="Quadlet Generate Form"><img src="https://podman-desktop.io/assets/images/generate-form-edit-dark-ebea360be56e8a803c6e5fdd677ca98c.png" alt="Quadlet Generate Form"></p><p>Congrats 🎉 you created your first Quadlet!</p>
<h3 id="edit-quadlets-pen">Edit Quadlets <!-- -->🖊<a href="#edit-quadlets-pen" aria-label="Direct link to edit-quadlets-pen" title="Direct link to edit-quadlets-pen">​</a></h3>
<p>Click the Quadlet <strong>STATUS</strong> icon to view its details page, which has three tabs:</p>
<ul>
<li><strong>Generated</strong>: View the systemd unit generated by Podman (read-only).</li>
<li><strong>Source</strong>: Edit the Quadlet file directly.</li>
<li><strong>Logs</strong>: Monitor logs for the service using journalctl.</li>
</ul>
<p>You can make changes to the Quadlet’s source file and apply updates as needed.</p>
<p><img src="https://podman-desktop.io/assets/images/quadlet-details-source-light-c4da0f3f8b8a56d3cc4c3dff0fa5c796.png" alt="Quadlet Details Source"><img src="https://podman-desktop.io/assets/images/quadlet-details-source-dark-b1496cbe3e4692864ad17d1675a71e6d.png" alt="Quadlet Details Source"></p><h3 id="view-quadlet-logs-scroll">View Quadlet Logs <!-- -->📜<a href="#view-quadlet-logs-scroll" aria-label="Direct link to view-quadlet-logs-scroll" title="Direct link to view-quadlet-logs-scroll">​</a></h3>
<p>Since a Quadlet's corresponding resource is managed by systemd we can access corresponding unit's logs using journalctl.</p>
<p><img src="https://podman-desktop.io/assets/images/quadlet-details-logs-light-017268786f1d799f5e324c1fae9b7989.png" alt="Quadlet Details Logs"><img src="https://podman-desktop.io/assets/images/quadlet-details-logs-dark-ddad799f50f58aa48e7e931de1c052b1.png" alt="Quadlet Details Logs"></p><h2 id="conclusion">Conclusion<a href="#conclusion" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Podman Quadlets provide a powerful way to manage containers declaratively with systemd, bridging the gap between lightweight container management and full orchestration tools like Kubernetes.</p>
<p>With the Podman Quadlet extension in Podman Desktop, users gain a convenient interface to manage Quadlets visually, reducing complexity and saving time.</p>
<p>Try it today and streamline your container workflows!</p>
<!-- -->
<section data-footnotes="true">
<ol>
<li id="user-content-fn-1-4453df">
<p><a href="https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html" target="_blank" rel="noopener noreferrer">https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html</a> <a href="#user-content-fnref-1-4453df" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2-4453df">
<p><a href="https://github.com/containers/podlet" target="_blank" rel="noopener noreferrer">https://github.com/containers/podlet</a> <a href="#user-content-fnref-2-4453df" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-4.1 in the API (529 pts)]]></title>
            <link>https://openai.com/index/gpt-4-1/</link>
            <guid>43683410</guid>
            <pubDate>Mon, 14 Apr 2025 17:01:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/gpt-4-1/">https://openai.com/index/gpt-4-1/</a>, See on <a href="https://news.ycombinator.com/item?id=43683410">Hacker News</a></p>
Couldn't get https://openai.com/index/gpt-4-1/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. and El Salvador Say They Won't Return Man Who Was Mistakenly Deported (243 pts)]]></title>
            <link>https://www.nytimes.com/live/2025/04/14/us/trump-news-tariffs</link>
            <guid>43683405</guid>
            <pubDate>Mon, 14 Apr 2025 17:01:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/live/2025/04/14/us/trump-news-tariffs">https://www.nytimes.com/live/2025/04/14/us/trump-news-tariffs</a>, See on <a href="https://news.ycombinator.com/item?id=43683405">Hacker News</a></p>
Couldn't get https://www.nytimes.com/live/2025/04/14/us/trump-news-tariffs: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Is a Systemic Risk to the Tech Industry (110 pts)]]></title>
            <link>https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/</link>
            <guid>43683071</guid>
            <pubDate>Mon, 14 Apr 2025 16:28:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/">https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/</a>, See on <a href="https://news.ycombinator.com/item?id=43683071">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <p><strong><em>Before we go any further: I hate to ask you to do this, but I need your help — I'm up for this year's Webbys for the best business podcast award. I know it's a pain in the ass, but</em></strong><a href="https://vote.webbyawards.com/PublicVoting?ref=wheresyoured.at#/2025/podcasts/individual-episode/business"><strong><em> <u>can you sign up and vote for Better Offline</u></em></strong></a><strong><em>? I have never won an award in my life, so help me win this one.</em></strong></p><hr><p><strong><em>Soundtrack: </em></strong><a href="https://www.youtube.com/watch?v=L4PztrhXkXohttps%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DL4PztrhXkXo&amp;ref=wheresyoured.at"><strong><em><u>Mastodon - High Road</u></em></strong></a></p><hr><p>I wanted to start this newsletter with a pithy anecdote about chaos, both that caused by Donald Trump's tariffs and the brittle state of the generative AI bubble.</p><p>Instead, I am going to write down some questions, and make an attempt to answer them.</p><h2 id="how-much-cash-does-openai-have"><strong>How Much Cash Does OpenAI Have?</strong></h2><p>Last week, OpenAI closed "<a href="https://www.cnbc.com/2025/03/31/openai-closes-40-billion-in-funding-the-largest-private-fundraise-in-history-softbank-chatgpt.html?ref=wheresyoured.at"><u>the largest private tech funding round in history</u></a>," where it "raised"&nbsp; an astonishing "$40 billion," and the reason that I've put quotation marks around it is that OpenAI has only raised $10 billion of the $40 billion, with the rest arriving by "the end of the year."&nbsp;</p><p>The remaining $30 billion — $20 billion of which will (allegedly) be provided by SoftBank — is partially contingent on OpenAI's conversion from a non-profit to a for-profit by the end of 2025, and if it fails,<a href="https://www.cnbc.com/2025/03/31/openai-funding-could-be-cut-by-10-billion-if-for-profit-move-lags.html?ref=wheresyoured.at"> <u>SoftBank will only give OpenAI a further $20 billion</u></a>. The round also valued OpenAI at $300 billion.</p><p>To put that in context, OpenAI had revenues of $4bn in 2024. This deal <em>values OpenAI at 75 times its revenue</em>. That’s a bigger gulf than Tesla at its peak market cap — a company that was, in fact, worth more than all other legacy car manufacturers combined, despite making far less than them, and shipping a fraction of their vehicles.&nbsp;</p><p>I also want to add that, as of writing this sentence,<strong> this money is yet to arrive.</strong><a href="https://group.softbank/en/news/press/20250401?ref=wheresyoured.at"><strong> </strong><u>SoftBank's filings</u></a> say that the money will arrive mid-April — and that SoftBank would be borrowing as much as $10 billion to finance the round, with the option to syndicate part of it to other investors. For the sake of argument, I'm going to assume this money actually arrives.</p><p>Filings also suggest that "in certain circumstances" the second ($30 billion) tranche could arrive "in early 2026." This isn't great. <strong>It also seems that SoftBank's $10 billion commitment is contingent on getting a loan, "...financed through borrowings from Mizuho Bank, Ltd., among other financial institutions."</strong></p><p><a href="https://www.theverge.com/openai/640894/chatgpt-has-hit-20-million-paid-subscribers?ref=wheresyoured.at"><u>OpenAI also revealed it now has 20 million paying subscribers</u></a> and over 500 million weekly active users. If you're wondering why it doesn’t talk about <em>monthly</em> active users, it's because they'd likely be much higher than 500 million, which would<a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=It%20would%20also%20suggest%20a%20conversion%20rate%20of%202.583%25%20from%20free%20to%20paid%20users%20on%20ChatGPT%20%E2%80%94%20an%20astonishingly%20bad%20number%2C%20one%20made%20worse%20by%20the%20fact%20that%20every%20single%20user%20of%20ChatGPT%2C%20regardless%20of%20whether%20they%20pay%2C%20loses%20the%20company%20money."> <u>reveal exactly how poorly OpenAI converts free ChatGPT users to paying ones</u></a>, and how few people use ChatGPT in their day-to-day lives.</p><p>The Information reported back in January that<a href="https://www.theinformation.com/articles/openai-tightens-grip-on-high-end-of-app-market-muratis-startup-gets-a-name?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>OpenAI was generating $25 million in revenue a month from its $200-a-month "Pro" subscribers</u></a> (<a href="https://techcrunch.com/2025/01/05/openai-is-losing-money-on-its-pricey-chatgpt-pro-plan-ceo-sam-altman-says/?ref=wheresyoured.at"><u>it still loses money on every one of them</u></a>), suggesting around 125,000 ChatGPT Pro subscribers. Assuming the other 19,875,000 users are paying $20 a month, that puts its revenue at about $423 million a month, or about $5 billion a year, from ChatGPT subscriptions.&nbsp;</p><p>This is what reporters mean when they say "annualized revenue" by the way — it's literally the monthly revenue multiplied by 12.</p><p><a href="https://www.bloomberg.com/news/articles/2025-03-26/openai-expects-revenue-will-triple-to-12-7-billion-this-year?ref=wheresyoured.at"><u>Bloomberg reported recently that OpenAI expects its 2025 revenue to "triple" to $12.7 billion this year</u></a>.<a href="https://www.wheresyoured.at/oai-business/#:~:text=Licensing%20Access%20To%20Models%20And%20Services%20%E2%80%94%2027%25%20of%20revenue%20(approximately%20%241%20billion)."> <u>Assuming a similar split of revenue to 2024</u></a>, this would require OpenAI to nearly double its annualized subscription revenue from Q1 2025 (from $5 billion to around $9.27 billion) <strong>and nearly quadruple API revenue </strong>(from 2024's revenue of $1 billion, which includes Microsoft's 20% payment for access to OpenAI's models, to $3.43 billion).</p><p>While these are messy numbers, it's unclear how OpenAI intends to pull this off.</p><p>The Information reported in February<a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>that it planned to do so by making $3 billion a year selling "agents,"</u></a> with ChatGPT subscriptions ($7.9 billion) and API calls ($1.8 billion) making up the rest. This, of course, is utter bollocks.<a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=Counterpoint%3A%20OpenAI%20has%20a%20new%20series%20of%20products%20that%20could%20open%20up%20new%20revenue%20streams%20such%20as%20Operator%2C%20its%20%22agent%22%20product%2C%20and%20%22Deep%20Research%2C%22%20their%20research%20product."> <u>OpenAI's "agents" can't do even the simplest tasks,</u></a> and<a href="https://www.cnbc.com/2025/02/03/softbank-commits-to-joint-venture-with-openai.html?ref=wheresyoured.at"> <u>three billion dollars of the $12.7 billion figure appears to be a commitment made by SoftBank to purchase</u></a> OpenAI's tech for its various subsidiaries and business units.&nbsp;</p><p>Let's say out the numbers precisely:</p><ul><li><strong>Incoming monthly revenue: </strong>roughly $425 million, give or take.</li><li><strong>Theoretical revenue from Softbank:</strong> $250 million a month. However, I can find no proof that SoftBank has begun to make these payments or, indeed, that it intends to make them.</li><li><strong>Liquidity:</strong><ul><li>$10 billion <strong>that it is yet to receive</strong> from SoftBank and a syndicate of investors including Microsoft, <strong>potentially.</strong></li><li><a href="https://openai.com/index/new-credit-facility-enhances-financial-flexibility/?ref=wheresyoured.at"><u>An indeterminate amount of remaining capital on the $4 billion credit facility provided by multiple banks</u></a> back in October 2024, raised alongside a funding round that valued the company at $157 billion.<ul><li>As a note, this announcement stated that OpenAI had "access to over $10 billion in liquidity."</li></ul></li><li><strong>Based on reports, OpenAI will not have access to the rest of its $40bn funding until "the end of the year," and it's unclear what part of the end of the year.</strong></li></ul></li></ul><p>We can assume, in this case, that OpenAI likely has, in the best case scenario, <strong>access to roughly $16 billion in liquidity at any given time. </strong>It's reasonable to believe that OpenAI will raise more <em>debt</em> this year, and I'd estimate it does so to the tune of around $5 billion or $6 billion. Without it, I am not sure what it’s going to do.</p><p><strong>As a reminder: OpenAI loses money on every single user.</strong></p><h2 id="what-are-openais-obligations"><strong>What Are OpenAI's Obligations?</strong></h2><p>When I wrote "<a href="https://www.wheresyoured.at/to-serve-altman/"><u>How Does OpenAI Survive</u></a>?" and "<a href="https://www.wheresyoured.at/oai-business/"><u>OpenAI Is A Bad Business</u></a>," I used reported information to explain how this company was, at its core, unsustainable.</p><p>Let's refresh our memories.</p><h3 id="compute-costs-at-least-13-billion-in-2025-with-microsoft-alone-and-as-much-as-594-million-to-coreweave"><strong>Compute Costs: at least $13 billion in 2025 <em>with Microsoft alone</em>, and as much as $594 million to CoreWeave.</strong></h3><ul><li><strong>In 2024,</strong><a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=As%20a%20note,run%20this%20company."><strong> <u>OpenAI spent $9 billion to lose $5 billion</u></strong></a><strong>.</strong><ul><li>This figure includes the $3 billion spent on training new models and $2 billion on running them.</li></ul></li></ul><p>It seems, from even a cursory glance, that OpenAI's costs are increasing dramatically. The Information reported earlier in the year that<a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at#:~:text=more%20than%20doubling%20from%20%2413%20billion%20this%20year"> <u>OpenAI projects to spend <strong>$13 billion on compute with Microsoft alone in 2025</strong></u></a><strong>,</strong><a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at#:~:text=more%20than%20doubling%20from%20%2413%20billion%20this%20year"><strong> </strong><u>nearly <em>tripling</em> what it spent in total on compute in 2024 ($5 billion)</u></a>.</p><p>This suggests that OpenAI's costs are skyrocketing, and that was before<a href="https://techcrunch.com/2025/03/31/openais-new-image-generator-is-now-available-to-all-users/?ref=wheresyoured.at"> <u>the launch of its new image generator</u></a> which led to multiple complaints from Altman<a href="https://x.com/sama/status/1905296867145154688?ref=wheresyoured.at"> <u>about a lack of available GPUs</u></a>,<a href="https://x.com/sama/status/1907098207467032632?ref=wheresyoured.at"> <u>leading to OpenAI's CEO saying to expect "stuff to break" and delays in new products</u></a>. Nevertheless, even if we assume OpenAI factored in the compute increases into its projections, <strong>it still expects to pay Microsoft $13 billion for compute this year.</strong></p><p>This number, however, doesn't include the $12.9 billion five-year-long compute deal signed with CoreWeave,<a href="https://www.semafor.com/article/03/20/2025/microsoft-chose-not-to-exercise-12-billion-coreweave-option?ref=wheresyoured.at"> <u>a deal that was a result of Microsoft declining to pick up the option to buy said compute itself</u></a>.<a href="https://www.theinformation.com/articles/coreweave-faces-reality-check-bullish-growth-forecasts?rc=kz8jh3&amp;ref=wheresyoured.at#:~:text=The%20recent%20analyst%20forecasts%20took%20into%20account%20its%20largest%20ever%20single%20contract%2C%20a%20%2411.9%20billion%20deal%20with%20OpenAI%20that%20CoreWeave%20said%20would%20begin%20in%20October.%20That%20means%20it%20will%20only%20be%20able%20to%20book%20those%20revenues%20toward%20the%20tail%20end%20of%20this%20year."> <u>Payments for this deal, according to The Information, start in October 2025</u></a>, and assuming that it's evenly paid (the terms of these contracts are generally secret, even in the case of public companies), this would <strong>still amount to roughly $2.38 billion a year.</strong></p><p>However, for the sake of argument, let's consider the payments are around $198 million a month, though there are scenarios — such as, say,<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Number%20Four%20%2D%20CoreWeave%20Is%20Using%20A%20Suspicious%20and%20Unproven%20Partner%20To%20Build%20its%20Entire%20Infrastructure"> <u>CoreWeave's buildout partner not being able to build the data centers</u></a> or<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Number%20Three%3A%20CoreWeave%20Does%20Not%20Have%20Access%20To%20The%20Capital%20Necessary%20To%20Meet%20Its%20Obligations"> <u>CoreWeave not having the money to pay to build them</u></a> — where OpenAI might pay less.</p><p>To be clear, and I’ll explain in greater detail later, this wouldn’t be a good thing, either. While it would be off the hook for some of its payments, it would also be without the compute that’s essential for it to continue growing, serving existing customers, and building new AI models. Cash and compute are <em>both</em> essential to OpenAI’s survival.&nbsp;&nbsp;</p><h3 id="stargate-1-billion"><strong>Stargate: $1 Billion+</strong></h3><p><a href="https://www.reuters.com/technology/openai-softbank-each-commit-19-bln-stargate-data-center-venture-information-2025-01-23/?ref=wheresyoured.at"><u>OpenAI has dedicated somewhere in the region of $19 billion to the Stargate data center project</u></a>, along with another $19 billion provided by SoftBank and an indeterminate amount by other providers.</p><p><a href="https://www.datacenterdynamics.com/en/news/openai-and-oracle-to-deploy-64000-gb200-gpus-at-stargate-abilene-data-center-by-2026-report/?ref=wheresyoured.at"><u>Based on reporting from Bloomberg</u></a>, OpenAI plans to have 64,000 Blackwell GPUs running "by the end of 2026," or roughly $3.84 billion worth of them. I should also note that Bloomberg said that 16,000 of these chips would be operational by Summer 2025, though it's unclear if that will actually happen.</p><p>Though it's unclear who actually pays for what parts of Stargate, it's safe to assume that OpenAI will have to, <strong>at the very least, put a billion dollars into a project that is meant to be up and running by the end of 2026,</strong> if not more.</p><p>As of now, Stargate has exactly one data center under development in Abilene, Texas, and as above, it's unclear how that's going, though <a href="https://www.theinformation.com/articles/pressure-rises-oracle-finish-openai-data-center?rc=kz8jh3&amp;ref=wheresyoured.at"><u>a recent piece from The Information reported</u></a> that it was currently "empty and incomplete," and that if it stays that way, "OpenAI could walk away from the deal, which would cost Oracle billions of dollars." Though the article takes pains to assure the reader that won't be likely, even an <em>inkling</em> of such a possibility is a bad sign.</p><p><a href="https://www.businessinsider.com/texas-stargate-data-center-build-cost-2025-1?ref=wheresyoured.at"><u>Business Insider's reporting on the site in Abilene calls it a</u></a> "$3.4 billion data center development" (<a href="https://crusoe.ai/newsroom/crusoe-blue-owl-capital-primary-digital-joint-venture/?ref=wheresyoured.at"><u>as did the press release from site developer Crusoe</u></a>), though these numbers don't include GPUs, hardware, or the labor necessary to run them. Right now, Crusoe is (according to Business Insider) building "six new data centers, each with a minimum square footage...[which will] join the two it is already constructing for Oracle." Oracle has signed, according to The Information, a 15-year-long lease with Crusoe for its data centers, all of which will be rented to OpenAI.</p><p>In any case, OpenAI’s exposure could be much, much higher than the $1bn posited at the start of this section (and I’ll explain in greater depth how I reached that figure at the bottom of this section). If OpenAI has to contribute significantly to the costs associated with building Stargate, it could be on the hook for <em>billions</em>.<a href="https://www.datacenterdynamics.com/en/news/crusoe-begins-construction-on-second-phase-of-abilene-texas-data-center-campus-will-add-six-buildings/?ref=wheresyoured.at">&nbsp;</a></p><p><a href="https://www.datacenterdynamics.com/en/news/crusoe-begins-construction-on-second-phase-of-abilene-texas-data-center-campus-will-add-six-buildings/?ref=wheresyoured.at"><u>Data Center Dynamics reports that the Abilene site is meant to have 200MW of compute capacity in the first half of 2025, and then as much as 1.2GW by "mid-2026."</u></a><u> To give you a sense of total costs for this project, </u><a href="https://www.latitudemedia.com/news/catalyst-explaining-the-watt-bit-spread/?ref=wheresyoured.at#:~:text=But%20I%20think%20that,the%20CapEx%20deployment%20opportunity."><u>former Microsoft VP of Energy Brian Janous said in January</u></a> that it costs about $25 million a megawatt (or $25 billion a gigawatt), meaning that the initial capital expenditures for Stargate to spin up its first 200MW data center will be around $5 billion, spiraling to $30 billion for the entire project.&nbsp;</p><p>Or perhaps even more. The Information has reported that the site, which could be "...potentially one of the world's biggest AI data centers," could cost "$50 billion to $100 billion in the coming years."&nbsp;</p><p><strong>Assuming we stick with the lower end of the cost estimates, it’s likely that OpenAI is on the hook for over $5 billion for the Abilene site based on the $19 billion it has agreed to contribute to the <em>entire </em>Stargate project, the (often disagreeing) cost projections of the facility), and the contributions of other partners.&nbsp;</strong></p><p>This expenditure won’t come all at once, and will be spread across several years. Still, assuming even the rosiest numbers, it's hard to see how OpenAI doesn't have to pony up $1 billion in 2025, with similar annual payments going forward until its completion, and that is likely because the development of this site is going to be heavily delayed by both tariffs, labor shortages, and Oracle's (as reported by The Information) trust in "scrappy but unproven startups to develop the project."</p><h3 id="other-costs-at-least-35-billion"><strong>Other costs: at least $3.5 billion</strong></h3><p><a href="https://www.theinformation.com/articles/openai-projections-imply-losses-tripling-to-14-billion-in-2026?rc=kz8jh3&amp;ref=wheresyoured.at"><u>Based on reporting from The Information last year</u></a>, OpenAI will spend <em>at least $2.5 billion</em> across salaries, "data" (referring to buying data from other companies), hosting and other cost of sales, and sales and marketing, and then another billion on what infrastructure OpenAI owns.</p><p>I expect the latter cost to balloon with OpenAI's investment in physical infrastructure for Stargate.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdTLWNPUZ-Y3kviQD7Sn0ojL9bEazTw2r7G-JOiNtwC8ac3-d_DsBNExz6VYKwjyo6C2Tp1K6h1-4KeAT0bY89YsF7HFhQHZI7l-ok8rNK1AuoeynifiUxsvRPbVSQUYUPxs0l1?key=MFOPt-R0auIdYEYVFbbAfdWf" alt="" loading="lazy" width="624" height="564"></figure><h2 id="how-does-openai-meet-its-obligations"><strong>How Does OpenAI Meet Its Obligations?</strong></h2><h3 id="openai-could-spend-28-billion-or-more-in-2025-and-lose-over-14-billion-while-having-an-absolute-maximum-of-20-billion-in-liquidity"><strong>OpenAI Could Spend $28 Billion Or More In 2025, and Lose over $14 Billion while having an absolute maximum of $20 billion in liquidity</strong></h3><p><a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=The%20New%20York%20Times%20reports%20that%20OpenAI%20projects%20it%27ll%20make%20%2411.6%20billion%20in%202025%2C%20and%20assuming%20that%20OpenAI%20burns%20at%20the%20same%20rate%20it%20did%20in%202024%20%E2%80%94%20spending%20%242.25%20to%20make%20%241"><u>Based on previous estimates, OpenAI spends about $2.25 to make $1.</u></a> At that rate, it's likely that OpenAI's costs <em>in its rosiest revenue projections of $12.7 billion </em>are at least $28 billion — <strong>meaning that it’s on course to burn at least $14 billion in 2025.</strong></p><p>Assuming that OpenAI has <strong>all of its liquidity from last year </strong>(it doesn't, but for sake of argument, let’s pretend it still has the full $10 billion), <strong>as well as the $10 billion from SoftBank</strong>, it is <em>still</em> unclear how it meets its obligations.</p><p>While OpenAI likely has preferential payment structures with all vendors, such as its discounted rates with Microsoft for Azure cloud services, it will still have to pay them, especially in the case of costs related to Stargate, many of which will be up-front costs. In the event that its costs are as severe as reporting suggests, it’s likely the company will find itself needing to raise more capital — whether through equity (or the weird sort-of equity that it issues) or through debt.&nbsp;</p><p>And yes, while OpenAI has some revenue, it comes at a terrible cost, and anything that isn’t committed to paying for salaries and construction fees will likely be immediately funnelled directly into funding the obscene costs behind inference and training models like GPT 4.5 — a "<a href="https://www.wheresyoured.at/power-cut/#:~:text=The%20bad%20news%20was%20that%2C%20and%20I%20quote%2C%20GPT%204.5%20is%20%E2%80%9C...a%20giant%2C%20expensive%20model%2C%E2%80%9D"><u>giant expensive model</u></a>" to run that<a href="https://help.openai.com/en/articles/10658365-gpt-4-5-in-chatgpt?ref=wheresyoured.at"> <u>the company has nevertheless pushed to every user</u></a>.</p><p>Worse still, OpenAI has, while delaying its next model (GPT-5),<a href="https://techcrunch.com/2025/04/04/openai-says-itll-release-o3-after-all-delays-gpt-5/?ref=wheresyoured.at"> <u>promised to launch its o3 reasoning model after saying it wouldn't do so</u></a>, which is strange, because it turns out that o3 is actually <em>way</em> more expensive to run than people thought.&nbsp;</p><p>Reasoning models are almost always more expensive to operate, as they involve the model “checking” its work, which, in turn, requires more calculations and more computation. Still, o3 is ludicrously expensive even for this category, with the Arc Prize Foundation (a non-profit that makes the ARC-AGI test for benchmarking models) estimating that it will cost<a href="https://techcrunch.com/2025/04/02/openais-o3-model-might-be-costlier-to-run-than-originally-estimated/?ref=wheresyoured.at"> <em><u>$30,000 a task.</u></em></a></p><h3 id="softbank-has-to-borrow-money-to-meet-its-openai-and-stargate-obligations-leading-to-softbanks-financial-condition-likely-deteriorating"><strong>SoftBank Has To Borrow Money To Meet Its OpenAI and Stargate Obligations, leading to SoftBank's  "...financial condition likely deteriorating."</strong></h3><p>As of right now, SoftBank has committed to the following:</p><ul><li>At least $30 billion ($7.5 of the initial $10 billion, and $22.5 billion of the remaining $30 billion) in funding as part of OpenAI's recent $40bn funding round.<ul><li>This assumes that SoftBank finds others to invest with it. <a href="https://group.softbank/en/news/press/20250401?ref=wheresyoured.at"><u>SoftBank's filings surrounding OpenAI's funding</u></a> also suggest that SoftBank is, ultimately, on the hook for the entire $40 billion, but <em>can</em> syndicate with other investors.<a href="https://www.cnbc.com/2025/03/31/openai-closes-40-billion-in-funding-the-largest-private-fundraise-in-history-softbank-chatgpt.html?ref=wheresyoured.at"> <u>Reporting suggests that syndication will happen with Coatue, Microsoft and other investors</u></a>.</li><li>If OpenAI fails to convert to a for-profit, that $40bn figure is slashed to $30, although, again, SoftBank’s share of the final sum is contingent upon whether it finds other investors to join the deal.&nbsp;</li></ul></li><li><a href="https://www.cnbc.com/2025/02/03/softbank-commits-to-joint-venture-with-openai.html?ref=wheresyoured.at"><u>$3 billion in spend on OpenAI "tech."</u></a></li><li>$19 billion for the Stargate data center project,<a href="https://www.theinformation.com/articles/softbanks-son-goes-on-a-new-borrowing-binge-to-fund-ai?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>which SoftBank takes financial responsibility for</u></a>.<ul><li><strong>Total: $52 billion or $62 billion, with at least $20 billion due by the end of 2025.</strong></li></ul></li></ul><p>SoftBank's exposure to OpenAI is materially harming the company.<a href="https://www.wsj.com/business/deals/openai-softbank-investment-debt-51b4a130?ref=wheresyoured.at"> <u>To quote the Wall Street Journal:</u></a></p><blockquote>Ratings agency S&amp;P Global said last week that SoftBank’s “financial condition will likely deteriorate” as a result of the OpenAI investment and that its plans to add debt could lead the agency to consider downgrading SoftBank’s ratings.&nbsp;</blockquote><p>While one might argue that SoftBank has a good amount of cash, the Journal also adds that it’s&nbsp; somewhat hamstrung in its use as a result of CEO Masayoshi Son's reckless gambles:</p><blockquote>SoftBank had a decent buffer of $31 billion of cash as of Dec. 31, but the company has also pledged to hold much of that in reserve to quell worried investors. SoftBank has committed not to borrow more than 25% of the value of all of its holdings, which means it will likely need to sell some of the other parts of its empire to pay for the rest of the OpenAI deal.</blockquote><p>Worse still, it seems, as mentioned before, that SoftBank will be financing the entirety of the first $10 billion — or $7.5 billion, assuming it finds investors to syndicate the first tranche, and they follow through right until the moment Masayoshi Son hits ‘send’ on the wire transfer .</p><p>As a result, SoftBank will likely have to start selling off parts of its valuable holdings in companies like Alibaba and ARM, or, worse still,<a href="https://www.wheresyoured.at/power-cut/#:~:text=On%20the%20subject%20of%20Softbank%E2%80%99s%20holdings"> <u>parts of its ailing investments from its Vision Fund</u></a>, resulting in a material loss on its underwater deals.</p><p>This is an untenable strategy, and I'll explain why.</p><h3 id="openai-needs-at-least-40-billion-a-year-to-survive-and-its-costs-are-increasing"><strong>OpenAI Needs At Least $40 billion A Year To Survive, And Its Costs Are Increasing</strong></h3><p>While we do not have much transparency into OpenAI's actual day-to-day finances, we can make the educated guess that its costs are <em>increasing</em> based on the amount of capital it’s raising. If OpenAI’s costs were flat, or only mildly increasing, we’d expect to see raises roughly the same size as previous ones. Its $40bn raise is nearly <em>six</em> times the previous funding round.&nbsp;</p><p>Admittedly, multiples like that aren’t particularly unusual. If a company raises $300,000 in a pre-seed round, and $3m in a Series A round, that’s a tenfold increase. But we’re not talking about hundreds of thousands of dollars, or even millions of dollars. We’re talking about <em>billions</em> of dollars. If OpenAI’s funding round with Softbank goes as planned, it’ll raise the equivalent of the entire GDP of Estonia — a fairly wealthy country itself, and one that’s also a member of Nato and the European Union. That alone should give you a sense of the truly insane scale of this.&nbsp;</p><p>Insane, sure, but undoubtedly necessary. <a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at#:~:text=this%20year%20to-,%2428%20billion%20in%202028,-.%20The%20spending%20forecast"><u>Per The Information</u></a>, OpenAI expects to spend as much as $28 billion in compute on Microsoft's Azure cloud in 2028. Over a third of OpenAI's revenue, per the same article, will come from SoftBank's (alleged) spend.It's reasonable to believe that OpenAI will, as a result, need to raise in excess of $40 billion in funding a year, though it's reasonable to believe that it will need to raise more along the lines of $50 billion or more a year until it reaches profitability. This is due to both its growing cost of business, as well as its various infrastructure commitments, both in terms of Stargate, as well as with third-party suppliers like CoreWeave and Microsoft.&nbsp;</p><blockquote><strong>Counterpoint: OpenAI could reduce costs:</strong> While this is <u>theoretically</u> possible, there is no proof that this is taking place.<a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>The Information claims that</u></a> "...OpenAI would turn profitable by the end of the decade after the buildout of Stargate," but there is no suggestion as to how it might do so, or how building more data centers would somehow reduce its costs.This is especially questionable when you realize that<a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=OpenAI%20pays%20just%20over%2025%25%20of%20the%20cost%20of%20Azure%E2%80%99s%20GPU%20compute%20as%20part%20of%20its%20deal%20with%20Microsoft%20%E2%80%94%20around%20%241.30%2Dper%2DGPU%2Dper%2Dhour%20versus%20the%20regular%20Azure%20cost%20of%20%243.40%20to%20%244."> <u>Microsoft is already providing discounted pricing on Azure compute</u></a>. We don’t know if these discounts are below Microsoft’s break-even point — which it wouldn’t, nor would any other company offer, if they didn’t have something else to incentivize it, such as equity or a profit-sharing program. Microsoft, for what it’s worth, has both of those things.&nbsp;</blockquote><p>OpenAI CEO Sam Altman's statements around costs also suggest that they're increasing. In late February,<a href="https://techcrunch.com/2025/02/27/openai-ceo-sam-altman-says-the-company-is-out-of-gpus/?ref=wheresyoured.at"> <u>Altman claimed that OpenAI was "out of GPUs</u></a>." While this suggests that there’s demand for some products — like its image-generating tech, which enjoyed a viral day in the sun in March — it also means that to meet the demand it needs to spend more. And, at the risk of repeating myself, that demand doesn’t necessarily translate into profitability.&nbsp;</p><h3 id="softbank-cannot-fund-openai-long-term-as-openais-costs-are-projected-to-be-320-billion-in-the-next-five-years"><strong>SoftBank Cannot Fund OpenAI Long-Term, as OpenAI's costs are projected to be $320 billion in the next five years</strong></h3><p>As discussed above, SoftBank has to overcome significant challenges to fund both OpenAI and Stargate, and when I say "fund," I mean <strong>fund the current state of both projects, assuming no further obligations.</strong></p><p>The Information reports that<a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>OpenAI forecasts that it will spend $28 billion on compute with Microsoft alone in 2028</u></a>. The same article also reports that OpenAI "would turn profitable by the end of the decade after the buildout of Stargate," suggesting that OpenAI's operating expenses will grow exponentially year-over-year.</p><p>These costs, per The Information, are astronomical:</p><blockquote>The reason for the expanding cash burn is simple: OpenAI is spending whatever revenue comes in on computing needs for operating its existing models and developing new models. The company expects those costs to surpass $320 billion overall between 2025 and 2030.<p>The company expects more than half of that spending through the end of the decade to fund research-intensive compute for model training and development. That spending will rise nearly sixfold from current rates to around $40 billion per year starting in 2028. OpenAI projects its spending on running AI models will surpass its training costs in 2030.</p></blockquote><p>SoftBank has had to (and will continue having to) go to remarkable lengths to fund OpenAI's current ($40 billion) round, lengths so significant that it may lead to its credit rating being further downgraded.</p><p>Even if we assume the best case scenario — OpenAI successfully converts to a for-profit entity by the end of the year, and receives the full $30 billion — it seems unlikely (if not impossible) for it to continue raising the amount of capital they need to continue operations. As I’ve argued in previous newsletters, there are only a few entities that can provide the kinds of funding that OpenAI needs. These include big tech-focused investment firms like Softbank, sovereign wealth funds (like those of Saudi Arabia and the United Emirates), and perhaps the largest tech companies.</p><p>These entities can meet OpenAI’s needs, but not all the time. It’s not realistic to expect Softbank, or Microsoft, or the Saudis, or Oracle, or whoever, to provide $40bn <em>every year</em> for the foreseeable future.&nbsp;</p><p>This is especially true for Softbank. Based on its current promise to not borrow more than 25% of its holdings, it is near-impossible that SoftBank will be able to continue funding OpenAI at this rate ($40 billion a year), and $40 billion a year may not actually be enough.</p><p>Based on<a href="https://group.softbank/en/ir/stock/sotp?ref=wheresyoured.at"> <u>its last reported equity value of holdings</u></a>, SoftBank's investments and other assets are worth around $229 billion, meaning that it can borrow just over $57bn while remaining compliant with these guidelines.</p><p>In any case, it is unclear how SoftBank can fund OpenAI, but it's far clearer that <em>nobody else is willing to.</em></p><h3 id="openai-is-running-into-capacity-issues-suggesting-material-instability-in-its-business-or-infrastructure-%E2%80%94-and-its-unclear-how-it-expands-further"><strong>OpenAI Is Running Into Capacity Issues, Suggesting Material Instability In Its Business or Infrastructure — And It's Unclear How It Expands Further</strong></h3><p>Before we go any further, it's important to note that OpenAI does not really have its own compute infrastructure. The majority of its compute is provided by Microsoft, though, as mentioned above,<a href="https://www.semafor.com/article/03/20/2025/microsoft-chose-not-to-exercise-12-billion-coreweave-option?ref=wheresyoured.at"> <u>OpenAI now has a deal with CoreWeave to take over Microsoft's future options for more capacity</u></a>.</p><p>Anyway, in the last 90 days, Sam Altman has complained about a lack of GPUs and pressure on OpenAI's servers multiple times. Forgive me for repeating stuff from above, but this is necessary.</p><ul><li><a href="https://x.com/sama/status/1895203654103351462?ref=wheresyoured.at"><u>On February 27,</u></a> he lamented how GPT 4.5 was a "giant, expensive model," adding that it was "hard to perfectly predict growth surges that lead to GPU shortages." He also added that they would be adding tens of thousands of GPUs in the following week, then hundreds of thousands of GPUs "soon."</li><li><a href="https://x.com/sama/status/1905000759336620238?ref=wheresyoured.at"><u>On March 26</u></a>, he said that "images in chatgpt are wayyyy more popular than [OpenAI] expected," delaying the free tier launch as a result.</li><li><a href="https://x.com/sama/status/1905296867145154688?ref=wheresyoured.at"><u>On March 27</u></a>, he said that OpenAI's "GPUs [were] melting," adding that it was "going to introduce some temporary rate limits" while it worked out how to "make it more efficient."</li><li><a href="https://x.com/rohanjamin/status/1905721967216599199?ref=wheresyoured.at"><u>On March 28</u></a>, he retweeted Rohan Sahai, the product team lead on OpenAI's Sora video generation model, who said "The 4o image gen demand has been absolutely incredible. Been super fun to watch the Sora feed fill up with great content...GPUs are also melting in Sora land unfortunately so you may see longer wait times / capacity issues over coming days."</li><li><a href="https://x.com/sama/status/1906210479695126886?ref=wheresyoured.at"><u>On March 30</u></a>, he said "can yall please chill on generating images this is insane our team needs sleep."</li><li><a href="https://x.com/sama/status/1907098207467032632?ref=wheresyoured.at"><u>On April 1</u></a>, he said that "we are getting things under control, but you should expect new releases from openai [sic] to be delayed, stuff to break, and for service to sometimes be slow as we deal with capacity challenges." He also added that OpenAI is "working as fast we can to really get stuff humming; if anyone has GPU capacity in 100k chunks we can get asap please call!"</li></ul><p>These statements, in a bubble, seem either harmless or like OpenAI's growth is skyrocketing — the latter of which might indeed be true, but bodes ill for a company that burns money on every single user.</p><p>Any mention of rate limits or performance issues suggests that OpenAI is having significant capacity issues, and at this point it's unclear what further capacity it can actually expand to outside of that currently available. Remember,<a href="https://www.datacenterdynamics.com/en/news/microsoft-cancels-up-to-2gw-of-data-center-projects-says-td-cowen/?ref=wheresyoured.at"> <u>Microsoft has now pulled out of as much as 2GW of data center projects</u></a>,<a href="https://www.datacenterdynamics.com/en/news/microsoft-backs-away-from-1bn-data-center-plans-in-licking-county-ohio/?ref=wheresyoured.at"> <u>walked away from a $1 billion data center development in Ohio</u></a>, and<a href="https://www.semafor.com/article/03/20/2025/microsoft-chose-not-to-exercise-12-billion-coreweave-option?ref=wheresyoured.at"> <u>declined the option on $12bn of compute from CoreWeave that OpenAI had to pick up</u></a> — meaning that it may be pushing up against the limits of what is physically available.</p><p>While the total available capacity of GPUs at many providers like Lambda and Crusoe is unknown, we know that CoreWeave has approximately 360MWavailable,<a href="https://www.wheresyoured.at/power-cut/#:~:text=For%20some%20context,at%20this%20time."> <u>compared to Microsoft's 6.5 to 7.5 Gigawatts</u></a>, a large chunk of which already powers OpenAI.</p><p>If OpenAI is running into capacity issues, it could be one of the following:</p><ul><li>OpenAI is running up against the limit of what Microsoft has available, or is willing to offer the company.<a href="https://www.theinformation.com/articles/openai-eases-away-from-microsoft-data-centers?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>The Information reported in October 2024</u></a> that OpenAI was frustrated with Microsoft, which said it wasn’t moving fast enough to supply it with servers.</li><li>While OpenAI's capacity is sufficient, It does not have the resources available to easily handle bursts in user growth in a stable manner.</li></ul><p>Per The Information's reporting, Microsoft "promised OpenAI 300,000 NVIDIA GB200 (Blackwell) chips by the end of 2025," or roughly $18 billion of chips. It's unclear if this has changed<a href="https://techcrunch.com/2025/01/21/microsoft-is-no-longer-openais-exclusive-cloud-provider/?ref=wheresyoured.at"> <u>since Microsoft allowed OpenAI to seek other compute in late January 2025</u></a>.</p><p>I also don't believe that OpenAI has any other viable options for <em>existing compute infrastructure outside of Microsoft.</em><a href="https://www.cnbc.com/2025/03/26/the-concern-with-coreweaves-250000-nvidia-chips-ahead-of-its-ipo.html?ref=wheresyoured.at"><em> </em><u>CoreWeave's current data centers mostly feature NVIDIA's aging "Hopper" GPUs</u></a>, and while it could — and likely is! — retrofitting its current infrastructure with Blackwell chips, doing so is not easy. Blackwell chips require far more powerful cooling and server infrastructure to make them run smoothly (<a href="https://www.theinformation.com/articles/nvidias-top-customers-face-delays-from-glitchy-ai-chip-racks?rc=kz8jh3&amp;ref=wheresyoured.at"><u>a problem which led to a delay in their delivery to most customers</u></a>), and even if CoreWeave was able to replace every last Hopper GPU with Blackwell (it won't), it still wouldn't match what OpenAI needs to expand.</p><p>One might argue that it simply needs to wait for the construction of the Stargate data center, or for CoreWeave to finish the gigawatt or so of construction it’s working on.</p><p><a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Per%20its%20S,its%20current%20commitments."><u>As I've previously written</u></a>, I have serious concerns over the viability of CoreWeave ever completing its (alleged) contracted 1.3 Gigawatts of capacity.</p><p>Per my article:</p><blockquote>Per its S-1, CoreWeave has contracted for around 1.3 Gigawatts of capacity, which it expects to roll out over the coming years, and based on NextPlatform's math, <strong>CoreWeave will have to spend in excess of $39 billion to build its contracted compute. It is unclear how it will fund doing so, and it's fair to assume that CoreWeave does not currently have the capacity to cover its current commitments.</strong></blockquote><p>However, even if I were to humour the idea, it is impossible that any of this project is done by the end of the year, or even in 2026. I can find no commitments to any timescale, other than the fact that OpenAI will allegedly start paying CoreWeave in October (<a href="https://www.theinformation.com/articles/coreweave-faces-reality-check-bullish-growth-forecasts?rc=kz8jh3&amp;ref=wheresyoured.at#:~:text=The%20recent%20analyst%20forecasts%20took%20into%20account%20its%20largest%20ever%20single%20contract%2C%20a%20%2411.9%20billion%20deal%20with%20OpenAI%20that%20CoreWeave%20said%20would%20begin%20in%20October.%20That%20means%20it%20will%20only%20be%20able%20to%20book%20those%20revenues%20toward%20the%20tail%20end%20of%20this%20year."><u>per The Information</u></a>), which could very well be using current capacity.</p><p>I can also find no evidence that Crusoe, the company building the Stargate data center, has <em>any</em> compute available. Lambda,<a href="https://lambda.ai/blog/lambda-raises-320m-to-build-a-gpu-cloud-for-ai?srsltid=AfmBOopeWiVs7rtdEu5gHfszLlR2caTYw9u_avGGz6Go2D-izzkM5CBL&amp;ref=wheresyoured.at"> <u>a GPU compute company that raised $320 million earlier in this year</u></a>, and<a href="https://www.datacenterdynamics.com/en/analysis/true-believers-lambda-labs-ai-cloud-dreams/?ref=wheresyoured.at"> <u>according to Data Center Dynamics</u></a> "operates out of colocation data centers in San Francisco, California, and Allen, Texas, and is backed by more than $820 million in funds raised just this year," suggesting that it may not have their own data centers at all. Its ability to scale is entirely contingent on the availability of whatever data center providers it has relationships with.&nbsp;</p><p>In any case, this means that OpenAI's only real choice for GPUs is CoreWeave or Microsoft. While it's hard to calculate precisely,<a href="https://www.datacenterdynamics.com/en/news/openai-and-oracle-to-deploy-64000-gb200-gpus-at-stargate-abilene-data-center-by-2026-report/?ref=wheresyoured.at"> <u>OpenAI's best case scenario is that 16,000 GPUs come online in the summer of 2025</u></a> as part of the Stargate data center project.</p><p>That's a drop in the bucket compared to the 300,000 Blackwell GPUs that Microsoft had previously promised.</p><h3 id="any-capacity-or-expansion-issues-threaten-to-kneecap-openai"><strong>Any capacity or expansion issues threaten to kneecap OpenAI</strong></h3><p>OpenAI is, regardless of how you or I may feel about generative AI, one of the fastest-growing companies of all time. It currently has, according to its own statements, 500 million weekly active users. Putting aside that each user is unprofitable, such remarkable growth — especially as it's partially a result of its extremely resource-intensive image generator — is also a strain on its infrastructure.</p><p>The vast majority of OpenAI's users are free customers using ChatGPT, with only around 20 million paying subscribers, and the vast majority on the cheapest $20 plan. OpenAI's services — even in the case of image generation — are relatively commoditized, meaning that users can, if they really care, go and use any number of other different Large Language Model services. They can switch to Bing Image Creator, or Grok, or Stable Diffusion, or whatever.</p><p>Free users are also a burden on the company — especially with such a piss-poor conversion rate — losing it money with each prompt (which is also the case with paying customers), and the remarkable popularity of its image generation service only threatens to bring more burdensome one-off customers that will generate a few abominable Studio Ghibli pictures and then never return.</p><p>If OpenAI's growth continues at this rate, it will run into capacity issues, and it does not have much room to expand. While we do not know how much capacity it’s taking up with Microsoft, or indeed whether Microsoft is approaching capacity or otherwise limiting how much of it OpenAI can take, we do know that OpenAI has seen reason to beg for access to more GPUs.</p><p>In simpler terms, even if OpenAI wasn’t running out of money, even if OpenAI wasn’t horrifyingly unprofitable, it also may not have enough GPUs to continue providing its services in a reliable manner.</p><p>If that's the case, there really isn't much that can be done to fix it other than:</p><ul><li>Significantly limiting free users' activity on the platform, which is OpenAI's primary mechanism for revenue growth and customer acquisition.</li><li>Limiting activity or changing the economics behind its paid product, to quote Sam Altman, "<a href="https://x.com/sama/status/1889679681047482730?ref=wheresyoured.at"><u>find[ing] some way to let people to pay for compute they want to use more dynamically.</u></a>"<ul><li><a href="https://x.com/sama/status/1897036361506689206?ref=wheresyoured.at"><u>On March 4th</u></a>, Altman solicited feedback on "...an idea for paid plans: your $20 plus subscription converts to credits you can use across features like deep research, o1, gpt-4.5, sora, etc...no fixed limits per feature and you choose what you want; if you run out of credits you can buy more."</li><li><a href="https://x.com/sama/status/1876104315296968813?ref=wheresyoured.at"><u>On January 5th</u></a>, Sam Altman revealed that OpenAI is currently losing money on every paid subscription, including its $200-a-month "pro" subscription.</li><li><a href="https://www.theinformation.com/articles/openai-plots-charging-20-000-a-month-for-phd-level-agents?rc=kz8jh3&amp;ref=wheresyoured.at#:~:text=OpenAI%20CEO%20Sam,200%20a%20month.%E2%80%9D"><u>Buried in an article from The Information from March 5</u></a> is a comment that suggests it’s considering measures like changing its pricing model, with "...Sam Altman reportedly [telling] developers in London [in February] that OpenAI is primed to charge 20% or 30% of Pro customers a higher price because of how many research queries they’re doing, but he suggested an “a la carte” or pay-as-you-go approach. When it comes to agents, though, “we have to charge much more than $200 a month.”</li></ul></li></ul><p>The problem is that these measures, even if they succeed in generating more money for the company, <strong>also need to reduce the burden on OpenAI's available infrastructure.</strong></p><p><a href="https://www.wheresyoured.at/power-cut/#:~:text=Data%20center%20buildouts,a%20year%20ago."><u>Remember: data centers can take three to six years to build</u></a>, and even with the Stargate's accelerated (and I'd argue unrealistic) timelines, OpenAI isn't even unlocking a tenth of Microsoft's promised compute (16,000 GPUs online this year versus the 300,000 GPUs promised by Microsoft).</p><h3 id="what-might-capacity-issues-look-like-and-what-are-the-consequences"><strong>What Might Capacity Issues Look Like? And What Are The Consequences?</strong></h3><p>Though downtime might be an obvious choice, capacity issues at OpenAI will likely manifest in hard limits on what free users can do, some of which I've documented above. Nevertheless, I believe the real pale horses of capacity issues come from <strong>arbitrary limits on any given user group,</strong> meaning both free and paid users. Sudden limits on what a user can do — a reduction in the number of generations of images of videos for paid users, any introduction of "peak hours," or any increases in prices are a sign that OpenAI is running out of GPUs, which it has already publicly said is happening.</p><p>However, the really obvious one would be <em>service degradation</em> — delays in generations of any kind,<a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/http-500-internal-server-error.html?ref=wheresyoured.at#:~:text=An%20HTTP%20500%20status%20code,it%20from%20fulfilling%20the%20request."> <u>500 status code errors</u></a>, or ChatGPT failing to fully produce an answer. OpenAI has, up until this point, had fairly impressive uptime. Still, if it is running up against a wall, this streak will end.</p><p>The consequences depend on how often these issues occur, and to whom they occur. If free users face service degradation, they will bounce off the product, as their use is likely far more fleeting than a paid user, which will begin to erode OpenAI's growth. Ironically, rapid (and especially unprecedented) growth in one of OpenAI’s competitors, like xAI or Anthropic, could also represent a pale horse for OpenAI.&nbsp;</p><p>If <em>paid</em> users face service degradation, it's likely this will cause the most harm to the company, as while paid users still lose OpenAI money in the end, <em>it at least receives some money in exchange.</em></p><p>OpenAI has effectively one choice here: getting more GPUs from Microsoft, and its future depends heavily both on its generosity <em>and</em> there being enough of them at a time when Microsoft<a href="https://www.wheresyoured.at/power-cut/"> <u>has pulled back</u></a><a href="https://sherwood.news/tech/microsoft-cancels-2-gigawatts-worth-of-data-centers-analysts-say/?ref=wheresyoured.at"> <u>from two gigawatts of data centers</u></a><a href="https://www.reuters.com/technology/microsoft-pulls-back-more-data-center-leases-us-europe-analysts-say-2025-03-26/?ref=wheresyoured.at"> <em><u>specifically because of it moving away from providing compute for OpenAI</u></em></a><em>.</em></p><p>Admittedly, OpenAI has previously spent more on training models than inference (actually running them) and the company might be able to smooth downtime issues by shifting capacity. This would, of course, have a knock-on effect on its ability to continue developing new models, and the company is already losing ground, particularly when it comes to Chinese rivals like DeepSeek.</p><h3 id="openai-must-convert-to-a-for-profit-entity-by-the-end-of-2025-or-it-loses-10-billion-in-funding-and-doing-so-may-be-impossible"><strong>OpenAI Must Convert To A For-Profit Entity By The End of 2025 Or It Loses $10 Billion In Funding, And Doing So May Be Impossible</strong></h3><p>As part of its deal with SoftBank, OpenAI must convert its bizarre non-profit structure into a for-profit entity by December 2025, or it’ll lose $10 billion from its promised funding.&nbsp;</p><p>Furthermore, in the event that OpenAI fails to convert to a for-profit by October 2026,<a href="https://www.wsj.com/tech/ai/open-ai-division-for-profit-da26c24b?st=yvhGAx&amp;ref=wheresyoured.at"> <u>investors in its previous $6.6 billion round can claw back their investment</u></a>, with it converting into a loan with an attached interest rate. Naturally, this represents a nightmare scenario for the company, as it’ll increase both its costs and its outgoings.</p><p>This is a complex situation that almost warrants its own newsletter, but the long and short of it is that OpenAI would have to effectively dissolve itself, <a href="https://www.upcounsel.com/converting-non-profit-to-for-profit?ref=wheresyoured.at#:~:text=Converting%20a%20nonprofit%20to%20a,a%20new%20for%2Dprofit%20entity."><u>start the process of forming an entirely new entity</u></a>, and distribute its assets to other nonprofits (or sell/license them to the for-profit company at fair market rates). It would require valuing OpenAI's assets, which in and of itself would be a difficult task, as well as getting past the necessary state regulators, the IRS, state revenue agencies, and<a href="https://www.inc.com/reuters/legal-battle-between-musk-and-openai-heads-to-trial-in-2026/91172454?ref=wheresyoured.at"> <u>the upcoming trial with Elon Musk only adds further problems</u></a>.</p><p>I’ve simplified things here, and that’s because (as I said) this stuff is complex. Suffice to say, this isn’t as simple as liquidating a company and starting afresh, or submitting a couple of legal filings. It’s a long, fraught process and one that will be — and has been — subject to legal challenges, both from OpenAI’s business rivals, as well as from civil society organizations in California.</p><p>Based on discussions with experts in the field and my own research, I simply do not know how OpenAI pulls this off <em>by October 2026,</em> let alone by the end of the year.</p><h2 id="openai-has-become-a-systemic-risk-to-the-tech-industry"><strong>OpenAI Has Become A Systemic Risk To The Tech Industry</strong></h2><p>OpenAI has become a load-bearing company for the tech industry, both as a narrative —<a href="https://www.wheresyoured.at/wheres-the-money/"> <u>as previously discussed, ChatGPT is the only Large Language Model company with any meaningful userbase</u></a> — and as a financial entity.&nbsp;</p><p>Its ability to meet its obligations and its future expansion plans are critical to the future health — or, in some cases, survival — of multiple large companies, and that's before the after-effects that will affect its customers as a result of any financial collapse.&nbsp;</p><p>The parallels to the 2007-2008 financial crisis are startling. Lehman Brothers wasn’t the largest investment bank in the world (although it was certainly big), just like OpenAI isn’t the largest tech company (though, again, it’s certainly large in terms of market cap and expenditure). Lehman Brothers’ collapse sparked a contagion that would later spread throughout the global financial services industry, and consequently, the global economy.&nbsp;</p><p>I can see OpenAI’s failure having a similar systemic effect. While there is a vast difference between OpenAI’s involvement in people’s lives compared to the millions of subprime loans issued to real people, the stock market’s dependence on the value of the Magnificent 7 stocks (Apple, Microsoft, Amazon, Alphabet, NVIDIA and Tesla), and in turn the Magnificent 7’s reliance on the stability of the AI boom narrative still threatens material harm to millions of people, and that’s before the ensuing layoffs.&nbsp;</p><p>And as I’ve said before, this entire narrative is based off of OpenAI’s success, because OpenAI <em>is</em> the generative AI industry.&nbsp;</p><p>I want to lay out the direct result of any kind of financial crisis at OpenAI, because I don't think anybody is taking this seriously.</p><h3 id="oracle-will-lose-at-least-1-billion-if-openai-doesnt-fulfil-its-obligations"><strong>Oracle Will Lose At Least $1 Billion If OpenAI Doesn't Fulfil Its Obligations</strong></h3><p><a href="https://www.theinformation.com/articles/pressure-rises-oracle-finish-openai-data-center?rc=kz8jh3&amp;ref=wheresyoured.at"><u>Per The Information</u></a>, Oracle, which has taken responsibility for organizing the construction of the Stargate data centers with unproven data center builder Crusoe, "...may need to raise more capital to fund its data center ambitions."</p><p>Oracle has signed a 15-year lease with Crusoe, and, to quote The Information, "...is on the hook for $1 billion in payments to that firm."</p><p>To further quote The Information:</p><blockquote>...while that’s a standard deal length, the unprecedented size of the facility Oracle is building for just one customer makes it riskier than a standard cloud data center used by lots of interchangeable customers with more predictable needs, according to half a dozen people familiar with these types of deals.</blockquote><p>In simpler terms, Oracle is building a giant data center for one customer — OpenAI — and has taken on the financial burden associated with it. If OpenAI fails to expand, or lacks the capital to actually pay for its share of the Stargate project, Oracle is on the hook for at least a billion dollars, and, based on The Information's reporting, is also on the hook to buy the GPUs for the site.</p><blockquote>Even before the Stargate announcement, Oracle and OpenAI had agreed to expand their Abilene deal from two to eight data center buildings, which can hold 400,000 Nvidia Blackwell GPUs, adding tens of billions of dollars to the total cost of the facility.</blockquote><p>In reality, this development will likely cost tens of billions of dollars, $19 billion of which is due from OpenAI, which does not have the money until it receives its second tranche of funding in December 2025, which is contingent partially on its ability to convert into a for-profit entity, which, as mentioned, is a difficult and unlikely proposition.</p><p>It's unclear how many of the Blackwell GPUs that Oracle has had to purchase in advance, but in the event of any kind of financial collapse at OpenAI, Oracle would likely <strong>take a loss of at least a billion dollars, if not several billion dollars.</strong></p><h3 id="coreweaves-expansion-is-likely-driven-entirely-by-openai-and-it-cannot-survive-without-openai-fulfilling-its-obligations-and-may-not-anyway"><strong>CoreWeave's Expansion Is Likely Driven Entirely By OpenAI, And It Cannot Survive Without OpenAI Fulfilling Its Obligations (And May Not Anyway)</strong></h3><p><a href="https://www.wheresyoured.at/core-incompetency/"><u>I have written a lot about publicly-traded AI compute firm CoreWeave</u></a>, and it would be my greatest pleasure to never mention it again.</p><p>Nevertheless, I have to.</p><p>The Financial Times revealed a few weeks ago that<a href="https://www.ft.com/content/163c6927-2032-4346-857e-8e3787e4babc?ref=wheresyoured.at"> <u>CoreWeave's debt payments could balloon to over $2.4 billion a year by the end of 2025</u></a>, far outstripping its cash reserves, and<a href="https://www.theinformation.com/articles/coreweave-faces-reality-check-bullish-growth-forecasts?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>The Information reported that its cash burn would increase to $15 billion in 2025</u></a>.</p><p>As per its IPO filings, 62% of CoreWeave's 2024 revenue (a little under $2 billion, with losses of $863 million) was Microsoft compute, and based on conversations with sources, a good amount of this was Microsoft running compute for OpenAI.</p><p>Starting October 2025,<a href="https://www.semafor.com/article/03/20/2025/microsoft-chose-not-to-exercise-12-billion-coreweave-option?ref=wheresyoured.at"> <u>OpenAI will start paying Coreweave as part of its five-year-long $12 billion contract</u></a>, picking up the option that Microsoft declined.<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Starting%20from%20October%202025"> <u>This is also when</u></a> CoreWeave will have to start making payments on<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Problem%20Loan%20Number%202%3A%20DDTL%202.0"> <u>its massive, multi-billion dollar DDTL 2.0 loan</u></a>, which likely makes these payments critical to CoreWeave's future.</p><p>This deal also suggests that OpenAI will become CoreWeave's largest customer. Microsoft had previously committed to spending $10 billion on CoreWeave's services "<a href="https://www.datacenterdynamics.com/en/news/microsoft-to-invest-10bn-in-coreweave-by-end-of-decade/?ref=wheresyoured.at"><u>by the end of the decade</u></a>," but CEO Satya Nadella added a few months later on a podcast that its relationship with CoreWeave was a "<a href="https://www.youtube.com/watch?v=9NtsnzRFJ_o&amp;ref=wheresyoured.at"><u>one-time thing</u></a>." Assuming Microsoft keeps spending at its previous rate — something that isn't guaranteed — it would still be only half of OpenAI's potential revenue.</p><p>CoreWeave's expansion, at this point, is entirely driven by OpenAI. 77% of its 2024 revenue came from two customers — Microsoft being the largest, and using CoreWeave as an auxiliary supplier of compute for OpenAI. As a result, the future expansion efforts —<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Per%20its%20S,its%20current%20commitments."> <u>the theoretical 1.3 gigawatts of contracted (translation: does not exist yet) compute</u></a> — are largely (if not entirely) for the benefit of OpenAI.</p><p><strong>In the event that OpenAI cannot fulfil its obligations, CoreWeave will collapse.</strong> It is that simple.&nbsp;</p><h3 id="nvidia-relies-on-coreweave-for-more-than-6-of-its-revenue-and-coreweaves-future-creditworthiness-to-continue-receiving-it-%E2%80%94-much-of-which-is-dependent-on-openai"><strong>NVIDIA Relies On CoreWeave For More Than 6% Of Its Revenue, And CoreWeave's Future Creditworthiness To Continue Receiving It — Much Of Which Is Dependent On OpenAI</strong></h3><p>I’m basing this on a comment I received from Gil Luria, Managing Director and Head of Technology Research at analyst D.A. Davidson &amp; Co:</p><blockquote>Since CRWV bought 200,000 GPUs last year and those systems are around $40,000 we believe CRWV spent $8 billion on NVDA last year. That represents more than 6% of NVDA’s revenue last year.&nbsp;</blockquote><p>CoreWeave receives preferential access to NVIDIA's GPUs, and makes up billions of dollars of its revenue.<a href="https://www.ft.com/content/41bfacb8-4d1e-4f25-bc60-75bf557f1f21?ref=wheresyoured.at"> <u>CoreWeave then takes those GPUs and raises debt using them as collateral</u></a>, then proceeds to buy more of those GPUs from NVIDIA. NVIDIA was the anchor for CoreWeave's IPO, and CEO Michael Intrator said that the IPO "wouldn't have closed" without NVIDIA buying $250 million worth of shares.<a href="https://www.theinformation.com/articles/project-osprey-how-nvidia-seeded-coreweaves-rise?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>NVIDIA invested $100 million in the early days of CoreWeave</u></a>, and, for reasons I cannot understand, also agreed to spend $1.3 billion over four years to, and I quote The Information, "rent its own chips from CoreWeave."</p><p><a href="https://www.wheresyoured.at/core-incompetency/#:~:text=potentially%20fatal%20%E2%80%94%20vulnerability.-,Sidenote,-%3A%20On%20the%20subject"><u>Buried in CoreWeave's S-1 — the document every company publishes before going public —&nbsp; was a warning about counterparty credit risk</u></a>, which is when one party provides services or goods to another with specific repayment terms, and the other party not meeting their side of the deal. While this was written as a theoretical (as it could, in theoretically, come from any company to which CoreWeave acts as a creditor) it only named one company: OpenAI.&nbsp;</p><p>As discussed previously, CoreWeave is saying that, should a customer — any customer, but really, it means OpenAI — fail to pay its bills for infrastructure built on their behalf, or for services rendered, it could have a material risk to the business.</p><blockquote><strong>Aside:</strong><a href="https://www.theinformation.com/articles/google-advanced-talks-rent-nvidia-ai-servers-coreweave?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>The Information reported that Google is in "advanced talks" to rent GPUs from CoreWeave</u></a>. It also, when compared to Microsoft and OpenAI's deals with CoreWeave, noted that "...Google's potential deal with CoreWeave is "significantly smaller than those commitments, according to one of the people briefed on it, but could potentially expand in future years."</blockquote><p>CoreWeave's continued ability to do business hinges heavily on its ability to raise further debt (<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Number%20Two%20%E2%80%94%20CoreWeave%20Has%20Taken%20On%20A%20Fatal%20Amount%20of%20Debt"><u>which I have previously called into question</u></a>), and its ability to raise further debt is,<a href="https://www.ft.com/content/163c6927-2032-4346-857e-8e3787e4babc?ref=wheresyoured.at"> <u>to quote the Financial Times</u></a>, "secured against its more than 250,000 Nvidia chips and its contracts with customers, such as Microsoft." Any future debt that CoreWeave raises would be based upon its contract with OpenAI (you know, the counterparty credit risk threat that represents a disproportionate share of its revenue) and whatever GPUs it still has to collateralize.</p><p>As a result, a chunk of NVIDIA's future revenue is dependent on OpenAI's ability to fulfil its obligations to CoreWeave, both in its ability to pay them and their timeliness in doing so. If OpenAI fails, then CoreWeave fails, which then hurts NVIDIA.&nbsp;</p><p>Contagion.&nbsp;</p><h3 id="openais-expansion-is-dependent-on-two-unproven-startups-who-are-also-dependent-on-openai-to-live"><strong>OpenAI's Expansion Is Dependent On Two Unproven Startups, Who Are Also Dependent on OpenAI To Live</strong></h3><p>With Microsoft's data center pullback and OpenAI's intent to become independent from Redmond, future data center expansion is based on two partners supporting CoreWeave and Oracle: Crusoe and Core Scientific, neither of which appear to have ever built an AI data center.</p><p>I also must explain how <em>difficult</em> building a data center is, and how said difficulty increases when you're building an AI-focused data center. For example,<a href="https://www.theinformation.com/articles/nvidia-customers-worry-about-snag-with-new-ai-chip-servers?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>NVIDIA had to delay the launch of its Blackwell GPUs because of how finicky the associated infrastructure</u></a> (the accompanying servers and cooling them) is. <em>For customers that already had experience handling GPUs, and therefore likely know how to manage the extreme temperatures created by them.</em></p><p><em>As another reminder,</em> OpenAI is on the hook for $19 billion of funding behind Stargate, money that neither it nor SoftBank has right now.</p><p>Imagine if you didn't have any experience, and effectively had to learn from scratch? How do you think that would go?</p><p>We're about to find out!</p><h3 id="crusoestargateabilene-texas"><strong>Crusoe - Stargate - Abilene Texas</strong></h3><p><strong>Crusoe </strong>is a former cryptocurrency mining company that<a href="https://techcrunch.com/2024/11/21/crusoe-a-rumored-openai-data-center-supplier-has-secured-686m-in-new-funds-filing-shows/?ref=wheresyoured.at"> <u>has now raised hundreds of millions of dollars</u></a> to build data centers for AI companies, starting with<a href="https://www.theinformation.com/briefings/crusoe-in-talks-to-raise-several-billion-dollars-for-oracle-openai-data-center?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>a $3.4 billion data center financing deal with asset manager Blue Owl Capital</u></a>. This (yet-to-be-completed) data center has now been leased by Oracle, which will, allegedly, fill it full of GPUs for OpenAI.</p><p>Despite calling itself "the industry’s first vertically integrated AI infrastructure provider," with the company using <a href="https://www.datacenterdynamics.com/en/news/crusoe-to-deploy-gas-flare-data-centers-in-utah/?ref=wheresyoured.at"><u>flared gas (a waste byproduct of oil production) to power IT infrastructure</u></a>, Crusoe does not appear to have built an AI data center, and is now being tasked with<a href="https://crusoe.ai/blog/crusoe-expands-ai-data-center-campus-in-abilene-to-1-2-gigawatts/?ref=wheresyoured.at"> <u>building a 1.2 Gigawatt data center campus for OpenAI</u></a>.</p><p>Crusoe is the sole developer and operator of the Abilene site, meaning, according to<a href="https://www.theinformation.com/articles/why-openai-and-oracles-ai-data-center-plan-hinges-on-a-little-known-startup?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>The Information</u></a>, "...is in charge of contracting with construction contractors and data center customers, as well as running the data center after it is built."</p><p>Oracle, it seems, will be responsible for<a href="https://www.theinformation.com/articles/why-openai-and-oracles-ai-data-center-plan-hinges-on-a-little-known-startup?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>filling said data center with GPUs and the associated hardware</u></a>.</p><p>Nevertheless, the project appears to be behind schedule.</p><p>The Information reported in October 2024 that Abeline was meant to have "...<a href="https://www.theinformation.com/articles/why-openai-and-oracles-ai-data-center-plan-hinges-on-a-little-known-startup?rc=kz8jh3&amp;ref=wheresyoured.at"><u>50,000 of NVIDIA's [Blackwell] AI chips...in the first quarter of [2025</u></a>]," and also suggested that the site was projected to have 100,000 Blackwell chips by the end of 2025.</p><p>Here in reality,<a href="https://www.datacenterdynamics.com/en/news/openai-and-oracle-to-deploy-64000-gb200-gpus-at-stargate-abilene-data-center-by-2026-report/?ref=wheresyoured.at"> <u>a report from Bloomberg in March 2025</u></a> (that I cited previously) said that OpenAI and Oracle were expected to have <em>16,000 GPUs</em> available <em>by the Summer of 2025, </em>with "...OpenAI and oracle are expected to deploy 64,000 NVIDIA GB200s at the Stargate data center...by the end of 2026."</p><p>As discussed above, OpenAI <em>needs this capacity</em>.<a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>According to The Information</u></a>, OpenAI expects Stargate to handle three-quarters of its compute by 2030, and these delays call into question at the very least whether this schedule is reasonable, if not whether Stargate, as a project, is actually possible.</p><h3 id="core-scientificcoreweavedenton-texas"><strong>Core Scientific - CoreWeave - Denton Texas</strong></h3><p>I've written a great deal about CoreWeave in the past,<a href="https://www.wheresyoured.at/optimistic-cowardice/"> <u>and specifically about its buildout partner Core Scientific</u></a>, a cryptocurrency mining company (yes, <em>another one</em>) that has exactly one customer for AI data centers — CoreWeave.</p><p>A few notes:</p><ul><li>Core Scientific was bankrupt last year.</li><li>Core Scientific has never built an AI data center, and its cryptocurrency mining operations were built around ASICs — specialist computers for mining Bitcoin —<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Needham%20analysts%20wrote%20in%20a%20report%20in%20May%20that%20almost%20all%20infrastructure%20that%20miners%20currently%20have%20would%20%E2%80%9Cneed%20to%20be%20bulldozed%20and%20built%20from%20the%20ground%20up%20to%20accommodate%20HPC%2C%E2%80%9D%20or%20high%2Dperformance%20computing.%C2%A0"> <u>which led an analyst to tell CNBC</u></a> that said data centers would "<a href="https://www.cnbc.com/2024/08/06/bitcoin-miner-core-scientific-expands-coreweave-deal-to-6point7-billion.html?ref=wheresyoured.at#:~:text=Needham%20analysts%20wrote%20in%20a%20report%20in%20May%20that%20almost%20all%20infrastructure%20that%20miners%20currently%20have%20would%20%E2%80%9Cneed%20to%20be%20bulldozed%20and%20built%20from%20the%20ground%20up%20to%20accommodate%20HPC%2C%E2%80%9D%20or%20high%2Dperformance%20computing."><u>need to be bulldozed and built up from the ground up</u></a>" to accommodate AI compute.</li><li>Core Scientific does not appear to have any meaningful AI compute of any kind. Its AI/HPC (high-performance computing) revenue represents a tiny, tiny percentage of its overall revenue, which still comes primarily from mining crypto, both for itself and for third-parties.&nbsp;</li><li><a href="https://investors.corescientific.com/news-events/press-releases/detail/110/core-scientific-and-coreweave-announce-1-2-billion-expansion-at-denton-tx-site?ref=wheresyoured.at"><u>CoreWeave's entire 1.3 Gigawatt buildout appears to be being handled by Core Scientific</u></a>.</li></ul><p>Core Scientific is also, it seems, taking on $1.14 billion of capital expenditures to build out these data centers,<a href="https://www.wheresyoured.at/core-incompetency/#:~:text=Core%20Scientific%2C%20according%20to%20its%2010%2DK%20form%3A"> <u>with CoreWeave promising to reimburse $899.3 million of these costs</u></a>.</p><p>It's also unclear how Core Scientific intends to do this. While it’s taken on a good amount of debt in the past —<a href="https://investors.corescientific.com/news-events/press-releases/detail/102/core-scientific-prices-upsized-550-million-convertible-senior-notes-offering?ref=wheresyoured.at"> <u>$550 million in a convertible note toward the end of 2024</u></a> — this would be more debt than it’s ever taken on.</p><p>It also, as with Crusoe, does not appear to have any experience building AI data centers, except unlike Crusoe, Core Scientific is a barely-functioning recently-bankrupted bitcoin miner pretending to be a data center company.</p><p>How important is CoreWeave to OpenAI exactly?<a href="https://www.semafor.com/article/03/20/2025/microsoft-chose-not-to-exercise-12-billion-coreweave-option?ref=wheresyoured.at#:~:text=%E2%80%9CCoreWeave%20has%20been,very%2C%20very%20quickly.%E2%80%9D"> <u>From Semafor</u></a>:</p><blockquote>“CoreWeave has been one of our earliest and largest compute partners,” OpenAI chief Sam Altman said in CoreWeave’s roadshow <a href="https://www.netroadshow.com/custom/IPO/CoreWeave/retail/disclaimer.html?ref=wheresyoured.at"><u>video</u></a>, adding that CoreWeave’s computing power “led to the creation of some of the models that we’re best known for.”<p>“Coreweave figured out how to innovate on hardware, to innovate on data center construction, and to deliver results very, very quickly.”</p></blockquote><p>But will it survive long term?</p><p>Going back to the point of contagion: If OpenAI fails, and CoreWeave fails, so too does Core Scientific. And I don’t fancy Crusoe’s chances, either. At least Crusoe isn’t public.</p><h3 id="an-open-question-does-microsoft-book-openais-compute-as-revenue"><strong>An Open Question: Does Microsoft Book OpenAI's Compute As Revenue?</strong></h3><p>Up until fairly recently, Microsoft has been the entire infrastructural backbone of OpenAI, but recently (to free OpenAI up to work with Oracle)<a href="https://techcrunch.com/2025/01/21/microsoft-is-no-longer-openais-exclusive-cloud-provider/?ref=wheresyoured.at"> <u>released it from its exclusive cloud compute deal</u></a>. Nevertheless,<a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at#:~:text=more%20than%20doubling%20from%20%2413%20billion%20this%20year"> <u>per The Information</u></a>, OpenAI still intends to spend $13 billion on compute on Microsoft Azure this year.</p><p>What's confusing, however, is whether any of this is booked as <em>revenue.</em> Microsoft claimed earlier in this year that it surpassed $13 billion in annual recurring revenue — by which it means its last month multiplied by 12 —<a href="https://www.marketwatch.com/livecoverage/microsoft-earnings-stock-results-azure-cloud-ai-deepseek/card/microsoft-says-ai-revenue-has-surpassed-13-billion-annual-run-rate-4d7JEBh564bN1pCGbGI6?ref=wheresyoured.at"> <u>from artificial intelligence</u></a>.<a href="https://www.theinformation.com/articles/openai-projections-imply-losses-tripling-to-14-billion-in-2026?ref=wheresyoured.at&amp;rc=kz8jh3"> <u>OpenAI's compute costs in 2024 were $5 billion</u></a>, at a<a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=OpenAI%20pays%20just%20over%2025%25%20of%20the%20cost%20of%20Azure%E2%80%99s%20GPU%20compute%20as%20part%20of%20its%20deal%20with%20Microsoft%20%E2%80%94%20around%20%241.30%2Dper%2DGPU%2Dper%2Dhour%20versus%20the%20regular%20Azure%20cost%20of%20%243.40%20to%20%244."> <u>discounted Azure rate</u></a>, which, on an annualized basis, would be around $416 million in revenue a month for Microsoft.</p><p>It isn't, however, clear whether Microsoft counts OpenAI's compute spend as revenue.</p><p>Microsoft's earnings do not include an "artificial intelligence" section, but<a href="https://www.microsoft.com/en-us/investor/segment-information?ref=wheresyoured.at"> <u>three separate segments</u></a>:</p><ul><li>Productivity and Business Processes, which includes things like Microsoft 365, LinkedIn, Dynamics 365 and other business processing software.</li><li>More Personal Computing, which includes Windows and Gaming Products</li><li>Intelligent Cloud, Including server products and cloud services like Azure, which is likely where OpenAI's compute is included.</li></ul><p>As a result, it's hard to say specifically where OpenAI's revenue sits, but based on an analysis of Microsoft's Intelligent Cloud segment from FY23 Q1 (note, financial years don’t always correspond with the calendar year,<a href="https://www.microsoft.com/en-us/Investor/earnings/FY-2025-Q2/press-release-webcast?os=httpwww.szxlp.xyz&amp;ref=app"> <u>so we just finished FY25 Q2 in January</u></a>) through to its most recent earnings, and found that there was a spike in revenue from FY23 Q1 to FY24 Q1.&nbsp;</p><p>In FY23 Q1 (which ended on <a href="https://www.microsoft.com/en-us/investor/earnings/fy-2023-q1/press-release-webcast?ref=wheresyoured.at"><u>September 30, 2022</u></a>, a month before ChatGPT's launch),&nbsp; the segment made $20.3 billion. The following year, in FY24 Q1, it made $24.3 billion — a 19.7% year-over-year (or roughly $4 billion) increase.</p><p>This could represent the massive increase in training and inference costs associated with hosting ChatGPT,<a href="https://www.microsoft.com/en-us/investor/earnings/fy-2024-q4/press-release-webcast?ref=wheresyoured.at"> <u>peaking at $28.5 billion in revenue in FY24 Q4</u></a> — before dropping dramatically to $24.1 billion in<a href="https://www.microsoft.com/en-us/investor/earnings/fy-2025-q1/press-release-webcast?ref=wheresyoured.at"> <u>FY25 Q1</u></a> and raising a little to $25.5 billion in<a href="https://www.microsoft.com/en-us/Investor/earnings/FY-2025-Q2/press-release-webcast?os=httpwww.szxlp.xyz&amp;ref=app"> <u>FY25 Q2</u></a>.</p><p>OpenAI spent 2023 training its GPT-4o model before transitioning to its massive, expensive "Orion" model which would eventually become GPT 4.5, as well as its video generation model "Sora."<a href="https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693?ref=wheresyoured.at"> <u>According to the Wall Street Journal</u></a>, training GPT 4.5 involved at least one training run costing "around half a billion dollars in computing costs alone."</p><p>These are huge sums, but it’s worth noting a couple of things. First, Microsoft licenses OpenAI’s models to third parties, so some of this revenue could be from other companies using GPT on Azure. And there’s also other companies running their own models on Azure. We’ve seen a lot of companies launch AI products, and not all of them are based on LLMs.</p><p>Muddling things further, Microsoft provides OpenAI access to Azure cloud services at a discounted rate. And so, there’s a giant question mark over OpenAI’s contribution to the various spikes in revenue for Microsoft’s Intelligent Cloud segment, or whether other third-parties played a significant role.&nbsp;</p><p>Furthermore, Microsoft’s investment in OpenAI isn’t entirely in cold, hard cash. Rather, it has provided the company with credits to be redeemed on Azure services. I’m not entirely sure how this would be represented on accounting terms, and if anyone can shed light on this, please get in touch.&nbsp;</p><p>Would it be noted as revenue, or something else? OpenAI isn’t paying Microsoft, but rather doing the tech equivalent of redeeming some airmiles, or spending a gift card.&nbsp;</p><p>Additionally, while equity is often treated as income for tax purposes — as is the case when an employee receives RSUs as part of their compensation package — under the existing OpenAI structure, Microsoft isn’t a shareholder but rather the owner of profit-sharing units. This is a distinction worth noting.&nbsp;&nbsp;</p><p>These profit-sharing units are treated as analogous to equity, at least in terms of OpenAI’s ability to raise capital, but in practice they aren’t the same thing. They don’t represent ownership in the company as directly as, for example, a normal share unit would. They lack the liquidity of a share, and the upside they provide — namely, dividends — is purely theoretical.&nbsp;</p><p>Another key difference: when a company goes bankrupt and enters liquidation, shareholders can potentially receive a share of the proceeds (after other creditors, employees, etc are paid). While that often doesn’t happen (as in, the liabilities far exceed the assets of the company), it’s at least theoretically possible. Given that profit-sharing units aren’t actually shares, where does that leave Microsoft?</p><p>This stuff is confusing, and I’m not ashamed to say that complicated accounting questions like these are far beyond my understanding. If anyone can shed some light, drop me an email, or a message on Twitter or BlueSky, or post on the Better Offline subreddit.&nbsp;</p><h2 id="the-future-of-generative-ai-rests-on-openai-and-openais-future-rests-on-near-impossible-financial-requirements"><strong>The Future of Generative AI Rests On OpenAI, And OpenAI's Future Rests On Near-Impossible Financial Requirements</strong></h2><p>I have done my best to write this piece in as objective a tone as possible, regardless of my feelings about the generative AI bubble and its associated boosters.</p><p>OpenAI,<a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=Is%20Generative%20AI%20A%20Real%20Industry%3F"> <u>as I've written before</u></a>, is effectively the entire generative AI industry, with its nearest competitor being less than five percent of its 500 million weekly active users.</p><p>Its future is dependent — and this is not an opinion, but objective fact — on effectively infinite resources.</p><h3 id="financial-resources"><strong>Financial Resources</strong></h3><p>If it required $40 billion to continue operations this year, it is reasonable to believe it will need at least another $40 billion next year, and based on its internal projections, will need at least that every single other year until 2030, when it claims, somehow, it will be profitable "with the completion of the Stargate data center."</p><h3 id="compute-resources-and-expansion"><strong>Compute Resources and Expansion</strong></h3><p>OpenAI requires more compute resources than anyone has ever needed, and will continue to do so in perpetuity. Building these resources is now dependent on two partners — Core Scientific and Crusoe — that have never built a data center, as Microsoft has materially pulled back on data center development, which have (as well as the aforementioned pullback on 2GW of data centers)<a href="https://www.linkedin.com/posts/noelle-walsh-b29356108_microsoftcloud-datacenters-activity-7315439628562423808-W67e/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAACNp-u0B8oyS6pLtatitYCwLv1mvyLXiCvk"> <u>"slowed or paused" some of its "early stage" data center projects</u></a>. This shift is directly linked to Microsoft’s relationship with OpenAI, withTD Cowen's recent analyst report saying that data center pullbacks were, and I quote its March 26 2025 data center channel checks letter, "...driven by the decision to not support incremental OpenAI training workloads."</p><p>In simpler terms, OpenAI needs more compute at a time when its lead backer,<a href="https://www.datacenterdynamics.com/en/news/microsoft-bought-twice-as-many-nvidia-hopper-gpus-as-other-big-tech-companies-report/?ref=wheresyoured.at"> <u>which has the most GPUs in the world</u></a>, has specifically walked away from building it.</p><p>Even in my most optimistic frame of mind, it isn't realistic to believe that Crusoe or Core Scientific can build the data centers necessary for OpenAI's expansion.</p><p>Even if SoftBank and OpenAI had the money to invest in Stargate <em>today</em>, dollars do not change the fabric of reality. Data centers take time to build, requiring concrete, wood, steel and other materials to be manufactured and placed, and that's after the permitting required to get these deals done. Even if that succeeds, getting the power necessary is a challenge unto itself, to the point that even Oracle, an established and storied cloud compute company,<a href="https://www.theinformation.com/articles/pressure-rises-oracle-finish-openai-data-center?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>to quote The Information</u></a>, "...has less experience than its larger rivals in dealing with utilities to secure power and working with powerful and demanding cloud customers whose plans change frequently."</p><p>A partner like Crusoe or Core Scientific simply doesn't have the muscle memory or domain expertise that Microsoft has when it comes to building and operating data centers. As a result, it's hard to imagine even in the <em>best case scenario</em> that they're able to match the hunger for compute that OpenAI has.</p><p>Now, I want to be clear — I believe OpenAI will still continue to use Microsoft's compute, and even expand further into whatever remaining compute Microsoft may have. However, there is now a hard limit on how much of it there's going to be, both literally (in what's physically available) and in what Microsoft itself will actually OpenAI them to use, especially given how unprofitable GPU compute might be.</p><h2 id="how-does-this-end"><strong>How Does This End?</strong></h2><p>Last week, a truly offensive piece of fan fiction — framed as a "report" —<a href="https://ai-2027.com/?ref=wheresyoured.at"> <u>called AI 2027 went viral</u></a>, garnering press coverage with<a href="https://www.dwarkesh.com/p/scott-daniel?ref=wheresyoured.at"> <u>the Dwarkesh Podcast</u></a> and<a href="https://www.nytimes.com/2025/04/03/technology/ai-futures-project-ai-2027.html?ref=wheresyoured.at"> <u>gormless, child-like wonder from the New York Times' Kevin Roose</u></a>. Its predictions vaguely suggest a theoretical company called OpenBrain will invent a self-teaching agent of some sort.</p><p>It's bullshit, but it captured the hearts and minds of AI boosters because it vaguely suggests that somehow Large Language Models and their associated technology will become something entirely different.</p><p>I don't like making predictions like these because the future — especially in our current political climate — is so chaotic, but I will say that I do not see, and I say this with complete objectivity, how any of this continues.</p><p>I want to be <strong>extremely blunt</strong> with the following points, as I feel like both members of the media and tech analysts have failed to express how ridiculous things have become. I will be repeating myself, but it's necessary, as I <strong>need you to understand how untenable things are.</strong></p><ul><li>SoftBank is putting itself in dire straits <em>simply to fund OpenAI once. </em>This deal threatens its credit rating, with SoftBank having to take on what will be multiple loans <strong>to fund OpenAI's $40 billion round.<u> OpenAI will need at least another $40 billion in the next year.</u></strong><ul><li>This is before you consider the other $19 billion that SoftBank has agreed to contribute to the Stargate data center project, money that it does not currently have available.</li></ul></li><li>OpenAI has promised $19 billion to the Stargate data center project, money it <strong>does not have</strong> and <strong>cannot get without SoftBank's funds.</strong><ul><li><strong><u>Again, neither SoftBank nor OpenAI has the money for Stargate right now.</u></strong></li></ul></li><li>OpenAI <em>needs Stargate to get built to grow much further.</em></li></ul><p>I see no way in which OpenAI can continue to raise money at this rate, <em>even if OpenAI somehow actually receives the $40 billion, which will require it becoming a for-profit entity. </em>While it could theoretically stretch that $40 billion to last multiple years,<a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>projections say it’ll burn $320 billion in the next five years</u></a>.</p><p>Or, more likely, I can’t see a realistic way in which OpenAI gets the resources it needs to survive. It’ll need a streak of unlikely good fortune, the kind of which you only ever hear about in Greek epic poems:&nbsp;</p><ul><li>SoftBank somehow gets the resources (and loses the constraints) required to bankroll it indefinitely.&nbsp;</li><li>The world’s wealthiest entities — those sovereign wealth funds mentioned earlier, the Saudis and so on&nbsp; — pick up the slack each year until OpenAI reaches productivity (assuming it does).</li><li>It has enough of those mega-wealthy benefactors to provide the $320bn it needs before it reaches profitability.</li><li>Crusoe and CoreScientific turn out to be really good at building AI infrastructure — something they’ve never done before.&nbsp;</li><li>Microsoft walks-back its walk-back on building new AI infrastructure and recommits to the tens of billions of dollars of capex spending it previously floated.&nbsp;</li><li>Stargate construction happens faster than expected, and there are no supply chain issues (in terms of labor, building materials, GPUs, and so on).</li></ul><p>If those things happen, I’ll obviously find myself eating crow. But I’m not worried.&nbsp;</p><p>In the present conditions, OpenAI is on course to run out of money or compute capacity, and it's unclear which will happen first.</p><h2 id="its-time-to-wake-up"><strong>It's Time To Wake Up</strong></h2><p>Even in a hysterical bubble where everybody is agreeing that this is the future, OpenAI currently requires more money and more compute than is reasonable to acquire. <em>Nobody</em> has ever raised as much as OpenAI needs to, and based on the sheer amount of difficulty that SoftBank is having in raising the funds to meet <em>the lower tranche ($10bn) of its commitment, </em>it may simply not be possible for this company to continue.</p><p>Even with <em>extremely</em> preferential payment terms — months-long deferred payments, for example — at some point somebody is going to need to get paid.</p><p>I will give Sam Altman credit. He's found many partners to shoulder the burden of the rotten economics of OpenAI, with Microsoft, Oracle, Crusoe and CoreWeave handling the up-front costs of building the infrastructure, SoftBank finding the investors for its monstrous round, and the tech media mostly handling his marketing for him.</p><p>He is, however, over-leveraged. OpenAI has never been forced to stand on its own two feet or focus on efficiency, and I believe the constant enabling of its ugly, nonsensical burnrate has doomed this company. OpenAI has acted like it’ll always have more money and compute, and that people will always believe its bullshit, mostly because up until recently <em>everybody has.</em></p><p>OpenAI cannot "make things cheaper" at this point, because the money has always been there to make things more expensive, as has the compute to make larger language models that burn billions of dollars a year. This company is not built to reduce its footprint in any way, nor is it built for a future in which it wouldn't have access to, as I've said before, infinite resources.</p><p>Worse still, investors and the media have run cover for the fact that these models don't really do much more than they did a year ago and for<a href="https://www.wheresyoured.at/godot-isnt-making-it/"> <u>the overall diminishing returns of Large Language Models</u></a>.</p><p>I have had many people <em>attack</em> my work about OpenAI, but none have provided any real counterpoint to<a href="https://www.wheresyoured.at/to-serve-altman/"> <u>the underlying economic argument I've made since July of last year</u></a> that OpenAI is unsustainable. This is likely because there really isn't one, other than "OpenAI will continue to raise more money than anybody has ever raised in history, in perpetuity, and will somehow turn from the least-profitable company of all time to a profitable one."</p><p>This isn’t a rational argument. It’s a religious one. It’s a call for faith.&nbsp;</p><p>And I see no greater pale horse of the apocalypse than Microsoft's material pullback on data centers. While the argument might be that Microsoft wants OpenAI to have an independent future, that's laughable when you consider Microsoft's deeply monopolistic tendencies — and, for that matter, it owns a massive proportion of OpenAI’s pseudo-equity. At one point, Microsoft’s portion was valued at 49 percent. And while additional fundraising has likely diluted Microsoft’s stake, it still “owns” a massive proportion of what is (at least) the most valuable private startup of all time.</p><p>And we’re supposed to believe that Microsoft’s pullback — which limits OpenAI’s access to the infrastructure it needs to train and run its models, and thus (as mentioned) represents an existential threat to the company — is because of some paternal desire to see OpenAI leave the childhood bedroom, spread its wings, and enter the real world? Behave.&nbsp;</p><p>More likely, Microsoft got what it needed out of OpenAI, which has reached the limit of the models it can develop, and which Microsoft already retains the IP of. There’s probably no reason to make any further significant investments, though they allegedly may be part of the initial $10 billion tranche of OpenAI’s next round.</p><p>It's also important to note that absolutely nobody <em>other than NVIDIA </em>is making any money from generative AI. CoreWeave loses billions of dollars, OpenAI loses billions of dollars, Anthropic loses billions of dollars, and I can't find a single company providing generative AI-powered software that's making a profit. The only companies even <em>close</em> to doing so are consultancies providing services to train and create data for models like Turing and Scale AI — and<a href="https://www.bloomberg.com/news/articles/2025-04-02/scale-ai-expects-to-more-than-double-sales-to-2-billion-in-2025?ref=wheresyoured.at"> <u>Scale isn't even profitable</u></a>.</p><p>The knock-on effects of OpenAI's collapse will be wide-ranging. Neither CoreWeave nor Crusoe will have tenants for their massive, unsustainable operations, and Oracle will have nobody to sell the compute it’s leased from Crusoe for the next 15 years. CoreWeave will likely collapse under the weight of its abominable debt, which will lead to a 7%+ revenue drop for NVIDIA at a time when revenue growth has already begun to slow.</p><p>On a philosophical level, OpenAI's health is what keeps this industry alive.<a href="https://www.wheresyoured.at/wheres-the-money/"> <u>OpenAI has the only meaningful userbase in generative AI</u></a>, and this entire hype-cycle has been driven by its success, meaning any deterioration (or collapse) of OpenAI will tell the market what I've been saying for over a year: that generative AI is not the next hyper-growth market, and its underlying economics do not make sense.</p><p>I am not writing this to be "right" or "be a hater."</p><p>If something changes, and I am wrong somehow, I will write exactly how, and why, and what mistakes I made to come to the conclusions I have in this piece.</p><p>I do not believe that my peers in the media will do the same when this collapses, but I promise you that they will be held accountable, because all of this abominable waste could have been avoided.</p><p>Large Language Models are not, on their own, the problem. They're tools, capable of some outcomes, doing some things, but the problem, ultimately, are the extrapolations made about their abilities, and the unnecessary drive to make them larger, even if said largeness never amounted to much.</p><p>Everything that I'm describing is the result of a tech industry — including media and analysts — that refuses to do business with reality, trafficking in ideas and ideology, celebrating victories that have yet to take place, applauding those who have yet to create the things they're talking about, cheering on men lying about what's possible so that they can continue to burn billions of dollars and increase their wealth and influence.</p><p>I understand why others might not have written this piece. What I am describing is a systemic failure, one at a scale hereto unseen, one that has involved so many rich and powerful and influential people agreeing to ignore reality, and that’ll have crushing impacts for the wider tech ecosystem when it happens.</p><p>Don't say I didn't warn you.</p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists: Protein IL-17 fights infection, acts on the brain, inducing anxiety (104 pts)]]></title>
            <link>https://medicalxpress.com/news/2025-04-scientists-protein-il-infection-brain.html</link>
            <guid>43682686</guid>
            <pubDate>Mon, 14 Apr 2025 15:54:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2025-04-scientists-protein-il-infection-brain.html">https://medicalxpress.com/news/2025-04-scientists-protein-il-infection-brain.html</a>, See on <a href="https://news.ycombinator.com/item?id=43682686">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2025/anxiety.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2025/anxiety.jpg" data-sub-html="Credit: Andrew Neel from Pexels">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2025/anxiety.jpg" alt="anxiety" title="Credit: Andrew Neel from Pexels" width="800" height="530">
             <figcaption>
                Credit: Andrew Neel from Pexels
            </figcaption>        </figure>
    </div><p>Immune molecules called cytokines play important roles in the body's defense against infection, helping to control inflammation and coordinating the responses of other immune cells. A growing body of evidence suggests that some of these molecules also influence the brain, leading to behavioral changes during illness.</p>

                                        
                                                                                  
                                         

                                                                                                                                    <p>Two new studies from MIT and Harvard Medical School, focusing on a cytokine called IL-17, now add to that evidence. The researchers found that IL-17 acts on two distinct brain regions—the amygdala and the somatosensory cortex—to exert two divergent effects. In the amygdala, IL-17 can elicit feelings of anxiety, while in the cortex it promotes sociable behavior.</p>
<p>These findings suggest that the immune and nervous systems are tightly interconnected, says Gloria Choi, an associate professor of brain and cognitive sciences, a member of MIT's Picower Institute for Learning and Memory, and one of the senior authors of the studies.</p>
<p>"If you're sick, there's so many more things that are happening to your internal states, your mood, and your behavioral states, and that's not simply you being fatigued physically. It has something to do with the brain," she says.</p>
<p>Jun Huh, an associate professor of immunology at Harvard Medical School, is also a senior author of both studies, which appear in <i>Cell</i>. One of the papers was led by Picower Institute Research Scientist Byeongjun Lee and former Picower Institute research scientist Jeong-Tae Kwon, and the other was led by Harvard Medical School postdoc Yunjin Lee and Picower Institute postdoc Tomoe Ishikawa.</p>
<h2>Behavioral effects</h2>
<p>Choi and Huh became interested in IL-17 several years ago, when they found it was involved in a phenomenon known as the fever effect. Large-scale studies of autistic children have found that for many of them, their behavioral symptoms temporarily diminish when they have a fever.</p>

                                                                                                                                                         
                                                                                                                                                                                                <p>In a <a href="https://medicalxpress.com/news/2019-12-infections-autism-symptoms.html">2019 study</a> in mice, Choi and Huh showed that in some cases of infection, IL-17 is released and suppresses a small region of the brain's cortex known as S1DZ. Overactivation of neurons in this region can lead to autism-like behavioral symptoms in mice, including repetitive behaviors and reduced sociability.</p>
<p>"This molecule became a link that connects immune system activation, manifested as a fever, to changes in <a href="https://medicalxpress.com/tags/brain+function/" rel="tag">brain function</a> and changes in the animals' behavior," Choi says.</p>
<p>IL-17 comes in six different forms, and there are five different receptors that can bind to it.</p>
<p>In their two new papers, the researchers set out to map which of these receptors are expressed in different parts of the brain. This mapping revealed that a pair of receptors known as IL-17RA and IL-17RB is found in the cortex, including in the S1DZ region that the researchers had previously identified. The receptors are located in a population of neurons that receive proprioceptive input and are involved in controlling behavior.</p>
<p>When a type of IL-17 known as IL-17E binds to these receptors, the neurons become less excitable, which leads to the behavioral effects seen in the 2019 study.</p>
<p>"IL-17E, which we've shown to be necessary for behavioral mitigation, actually does act almost exactly like a neuromodulator in that it will immediately reduce these neurons' excitability," Choi says.</p>
<p>"So, there is an immune molecule that's acting as a neuromodulator in the brain, and its main function is to regulate excitability of neurons."</p>
<p>Choi hypothesizes that IL-17 may have originally evolved as a neuromodulator, and later on was appropriated by the immune system to play a role in promoting inflammation.</p>
<p>That idea is consistent with previous work showing that in the worm C. elegans, IL-17 has no role in the immune system but instead acts on neurons. Among its effects in worms, IL-17 promotes aggregation, a form of social behavior. Additionally, in mammals, IL-17E is actually made by neurons in the cortex, including S1DZ.</p>
<p>"There's a possibility that a couple of forms of IL-17 perhaps evolved first and foremost to act as a neuromodulator in the brain, and maybe later were hijacked by the immune system also to act as immune modulators," Choi says.</p>

                                                                                                                                            <h2>Provoking anxiety</h2>
<p>In the other <i>Cell</i> paper, the researchers explored another brain location where they found IL-17 receptors—the amygdala. This almond-shaped structure plays an important role in processing emotions, including fear and anxiety.</p>
<p>That study revealed that in a region known as the basolateral amygdala (BLA), the IL-17RA and IL-17RE receptors, which work as a pair, are expressed in a discrete population of neurons. When these receptors bind to IL-17A and IL-17C, the neurons become more excitable, leading to an increase in anxiety.</p>
<p>The researchers also found that, counterintuitively, if animals are treated with antibodies that block IL-17 receptors, it actually increases the amount of IL-17C circulating in the body. This finding may help to explain unexpected outcomes observed in a clinical trial of a drug targeting the IL-17-RA receptor for psoriasis treatment, particularly regarding its potential adverse effects on mental health.</p>
<p>"We hypothesize that there's a possibility that the IL-17 ligand that is upregulated in this patient cohort might act on the brain to induce suicide ideation, while in animals there is an anxiogenic phenotype," Choi says.</p>
<p>During infections, this anxiety may be a beneficial response, keeping the sick individual away from others to whom the infection could spread, Choi hypothesizes.</p>
<p>"Other than its main function of fighting pathogens, one of the ways that the immune system works is to control the host behavior, to protect the host itself and also protect the community the host belongs to," she says. "One of the ways the immune system is doing that is to use cytokines, secreted factors, to go to the brain as communication tools."</p>
<p>The researchers found that the same BLA neurons that have receptors for IL-17 also have receptors for IL-10, a cytokine that suppresses inflammation. This molecule counteracts the excitability generated by IL-17, giving the body a way to shut off anxiety once it's no longer useful.</p>

                                                                                                                                                                                                                                                                                                    <h2>Distinctive behaviors</h2>
<p>Together, the two studies suggest that the immune system, and even a single family of cytokines, can exert a variety of effects in the brain.</p>
<p>"We have now different combinations of IL-17 receptors being expressed in different populations of neurons, in two different brain regions, that regulate very distinct behaviors. One is actually somewhat positive and enhances social behaviors, and another is somewhat negative and induces anxiogenic phenotypes," Choi says.</p>
<p>Her lab is now working on additional mapping of IL-17 receptor locations, as well as the IL-17 molecules that bind to them, focusing on the S1DZ region. Eventually, a better understanding of these neuro-immune interactions may help researchers develop new treatments for neurological conditions such as autism or depression.</p>
<p>"The fact that these molecules are made by the immune system gives us a novel approach to influence brain function as a means of therapeutics," Choi says. "Instead of thinking about directly going for the brain, can we think about doing something to the immune system?"</p>

                                                                                                                                                                            
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    Inflammatory and anti-inflammatory cytokines bidirectionally modulate amygdala circuits regulating anxiety, <i>Cell</i> (2025). <a data-doi="1" href="https://dx.doi.org/10.1016/j.cell.2025.03.005" target="_blank">DOI: 10.1016/j.cell.2025.03.005</a>. <a href="https://www.cell.com/cell/fulltext/S0092-8674(25)00278-8" target="_blank">www.cell.com/cell/fulltext/S0092-8674(25)00278-8</a>
</p><p>Brain-wide mapping of immune receptors uncovers a neuro-modulatory role of interleukin-17E and the receptor IL-17RB., <i>Cell</i> (2025). <a data-doi="1" href="https://dx.doi.org/10.1016/j.cell.2025.03.006" target="_blank">DOI: 10.1016/j.cell.2025.03.006</a>. <a href="https://www.cell.com/cell/fulltext/S0092-8674(25)00279-X" target="_blank">www.cell.com/cell/fulltext/S0092-8674(25)00279-X</a></p>

																								
																								<div>
													<p><strong>Journal information:</strong>
																											<a href="https://medicalxpress.com/journals/cell/"><cite>Cell</cite></a></p><a href="http://www.cell.com/" target="_blank" rel="nofollow">
															<svg>
																<use href="https://medx.b-cdn.net/tmpl/v6/img/svg/sprite.svg#icon_open" x="0" y="0"></use>
															</svg>
														</a> 
																									</div>
																							</div>
                                        											
																					
                                                                                                                            <p>
                                                <i>This story is republished courtesy of MIT News (<a href="http://web.mit.edu/newsoffice/" target="_blank">web.mit.edu/newsoffice/</a>), a popular site that covers news about MIT research, innovation and teaching.</i>
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Scientists discover the protein IL-17 that fights infection also acts on the brain, inducing anxiety or sociability (2025, April 7)
                                                 retrieved 14 April 2025
                                                 from https://medicalxpress.com/news/2025-04-scientists-protein-il-infection-brain.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Zero-codegen, no-compile TypeScript type inference from Protobufs (107 pts)]]></title>
            <link>https://github.com/nathanhleung/protobuf-ts-types</link>
            <guid>43682547</guid>
            <pubDate>Mon, 14 Apr 2025 15:41:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nathanhleung/protobuf-ts-types">https://github.com/nathanhleung/protobuf-ts-types</a>, See on <a href="https://news.ycombinator.com/item?id=43682547">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">protobuf-ts-types</h2><a id="user-content-protobuf-ts-types" aria-label="Permalink: protobuf-ts-types" href="#protobuf-ts-types"></a></p>
<blockquote>
<p dir="auto">Zero-codegen, no-compile TypeScript <code>type</code> inference from protobuf <code>message</code>s.</p>
</blockquote>
<p dir="auto"><code>protobuf-ts-types</code> lets you define language-agnostic <code>message</code> types in <code>proto</code> format, then infers TypeScript types from them with no additional codegen.</p>
<p dir="auto"><a href="https://github.dev/nathanhleung/protobuf-ts-types/blob/main/examples/basic/index.ts" rel="nofollow">Try on github.dev</a> | <a href="https://codesandbox.io/p/github/nathanhleung/protobuf-ts-types/main?import=true&amp;embed=1&amp;file=%2Fexamples%2Fbasic%2Findex.ts" rel="nofollow">View on CodeSandbox</a></p>
<div dir="auto"><p dir="auto">Warning</p><p dir="auto">Proof of concept, not production ready. See <a href="#limitations">Limitations</a> below for more details.</p>
</div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/nathanhleung/protobuf-ts-types/blob/main/screenshot.png"><img src="https://github.com/nathanhleung/protobuf-ts-types/raw/main/screenshot.png" width="400px" alt="Screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it Works" href="#how-it-works"></a></p>
<p dir="auto">In short, aggressive use of TypeScript's <a href="https://www.typescriptlang.org/docs/handbook/2/template-literal-types.html" rel="nofollow">template literal types</a>. Annotated example from the source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Pass the proto string you want to infer `message` names from as a generic parameter
type MessageNames<Proto extends string> =
  // Infer `message` parts using template literal type
  WrapWithNewlines<Proto> extends `${string}${Whitespace}message${Whitespace}${infer MessageName}${OptionalWhitespace}{${string}}${infer Rest}`
    ? // Recursively infer remaining message names
      [MessageName, ...MessageNames<Rest>]
    : [];"><pre><span>// Pass the proto string you want to infer `message` names from as a generic parameter</span>
<span>type</span> <span>MessageNames</span><span>&lt;</span><span>Proto</span> <span>extends</span> <span>string</span><span>&gt;</span> <span>=</span>
  <span>// Infer `message` parts using template literal type</span>
  <span>WrapWithNewlines</span><span>&lt;</span><span>Proto</span><span>&gt;</span> <span>extends</span> `${<span>string</span><span>}</span>${<span>Whitespace</span><span>}</span>message${<span>Whitespace</span><span>}</span>${infer <span>MessageName</span><span>}</span>${<span>OptionalWhitespace</span><span>}</span>{${<span>string</span><span>}</span>}${infer <span>Rest</span><span>}</span>`
    ? <span>// Recursively infer remaining message names</span>
      <span>[</span><span>MessageName</span><span>,</span> ...<span>MessageNames</span><span>&lt;</span><span>Rest</span><span>&gt;</span><span>]</span>
    : <span>[</span><span>]</span><span>;</span></pre></div>
<p dir="auto">See more in <a href="https://github.com/nathanhleung/protobuf-ts-types/blob/main/src/proto.ts"><code>src/proto.ts</code></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">First, install the package.</p>
<div data-snippet-clipboard-copy-content="npm install https://github.com/nathanhleung/protobuf-ts-types"><pre><code>npm install https://github.com/nathanhleung/protobuf-ts-types
</code></pre></div>
<p dir="auto">Then, use it in TypeScript.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import { pbt } from &quot;protobuf-ts-types&quot;;

const proto = `
    syntax = &quot;proto3&quot;;

    message Person {
      string name = 1;
      int32 id = 2;
      bool is_ceo = 3;
      optional string description = 4;
    }

    message Group {
        string name = 1;
        repeated Person people = 2;
    }
`;

// `Proto` is a mapping of message names to message types, inferred from the
// `proto` source string above.
type Proto = pbt.infer<typeof proto>;

type Person = Proto[&quot;Person&quot;];
type Person2 = pbt.infer<typeof proto, &quot;Person&quot;>;

// `Person` and `Person2` are the same type:
// ```
// {
//     name: string;
//     id: number;
//     is_ceo: boolean;
//     description?: string;
// }
// ```

type Group = pbt.infer<typeof proto, &quot;Group&quot;>;

function greetPerson(person: Person) {
  console.log(`Hello, ${person.name}!`);

  if (person.description) {
    console.log(`${person.description}`);
  } else {
    console.log(&quot;(no description)&quot;);
  }
}

function greetGroup(group: Group) {
  console.log(`=========${&quot;=&quot;.repeat(group.name.length)}===`);
  console.log(`= Hello, ${group.name}! =`);
  console.log(`=========${&quot;=&quot;.repeat(group.name.length)}===`);

  for (const person of group.people) {
    greetPerson(person);
    console.log();
  }
}

// If the structure of the `Group` or any of the individual `Person`s does not
// match the type, TypeScript will show an error.
greetGroup({
  name: &quot;Hooli&quot;,
  people: [
    {
      name: &quot;Gavin Belson&quot;,
      id: 0,
      is_ceo: true,
      description: &quot;CEO of Hooli&quot;,
    },
    {
      name: &quot;Richard Hendricks&quot;,
      id: 1,
      is_ceo: true,
      description: &quot;CEO of Pied Piper&quot;,
    },
    {
      name: &quot;Dinesh Chugtai&quot;,
      id: 2,
      is_ceo: false,
      description: &quot;Software Engineer&quot;,
    },
    {
      name: &quot;Jared Dunn&quot;,
      id: 3,
      is_ceo: false,
    },
  ],
});

// Output:
// ```
// =================
// = Hello, Hooli! =
// =================
// Hello, Gavin Belson!
// CEO of Hooli

// Hello, Richard Hendricks!
// CEO of Pied Piper

// Hello, Dinesh Chugtai!
// Software Engineer

// Hello, Jared Dunn!
// (no description)
// ```"><pre><span>import</span> <span>{</span> <span>pbt</span> <span>}</span> <span>from</span> <span>"protobuf-ts-types"</span><span>;</span>

<span>const</span> <span>proto</span> <span>=</span> <span>`</span>
<span>    syntax = "proto3";</span>
<span></span>
<span>    message Person {</span>
<span>      string name = 1;</span>
<span>      int32 id = 2;</span>
<span>      bool is_ceo = 3;</span>
<span>      optional string description = 4;</span>
<span>    }</span>
<span></span>
<span>    message Group {</span>
<span>        string name = 1;</span>
<span>        repeated Person people = 2;</span>
<span>    }</span>
<span>`</span><span>;</span>

<span>// `Proto` is a mapping of message names to message types, inferred from the</span>
<span>// `proto` source string above.</span>
<span>type</span> <span>Proto</span> <span>=</span> <span>pbt</span><span>.</span><span>infer</span><span>&lt;</span><span>typeof</span> <span>proto</span><span>&gt;</span><span>;</span>

<span>type</span> <span>Person</span> <span>=</span> <span>Proto</span><span>[</span><span>"Person"</span><span>]</span><span>;</span>
<span>type</span> <span>Person2</span> <span>=</span> <span>pbt</span><span>.</span><span>infer</span><span>&lt;</span><span>typeof</span> <span>proto</span><span>,</span> <span>"Person"</span><span>&gt;</span><span>;</span>

<span>// `Person` and `Person2` are the same type:</span>
<span>// ```</span>
<span>// {</span>
<span>//     name: string;</span>
<span>//     id: number;</span>
<span>//     is_ceo: boolean;</span>
<span>//     description?: string;</span>
<span>// }</span>
<span>// ```</span>

<span>type</span> <span>Group</span> <span>=</span> <span>pbt</span><span>.</span><span>infer</span><span>&lt;</span><span>typeof</span> <span>proto</span><span>,</span> <span>"Group"</span><span>&gt;</span><span>;</span>

<span>function</span> <span>greetPerson</span><span>(</span><span>person</span>: <span>Person</span><span>)</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>`Hello, <span><span>${</span><span>person</span><span>.</span><span>name</span><span>}</span></span>!`</span><span>)</span><span>;</span>

  <span>if</span> <span>(</span><span>person</span><span>.</span><span>description</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>`<span><span>${</span><span>person</span><span>.</span><span>description</span><span>}</span></span>`</span><span>)</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>"(no description)"</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span>

<span>function</span> <span>greetGroup</span><span>(</span><span>group</span>: <span>Group</span><span>)</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>`=========<span><span>${</span><span>"="</span><span>.</span><span>repeat</span><span>(</span><span>group</span><span>.</span><span>name</span><span>.</span><span>length</span><span>)</span><span>}</span></span>===`</span><span>)</span><span>;</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>`= Hello, <span><span>${</span><span>group</span><span>.</span><span>name</span><span>}</span></span>! =`</span><span>)</span><span>;</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>`=========<span><span>${</span><span>"="</span><span>.</span><span>repeat</span><span>(</span><span>group</span><span>.</span><span>name</span><span>.</span><span>length</span><span>)</span><span>}</span></span>===`</span><span>)</span><span>;</span>

  <span>for</span> <span>(</span><span>const</span> <span>person</span> <span>of</span> <span>group</span><span>.</span><span>people</span><span>)</span> <span>{</span>
    <span>greetPerson</span><span>(</span><span>person</span><span>)</span><span>;</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span>

<span>// If the structure of the `Group` or any of the individual `Person`s does not</span>
<span>// match the type, TypeScript will show an error.</span>
<span>greetGroup</span><span>(</span><span>{</span>
  <span>name</span>: <span>"Hooli"</span><span>,</span>
  <span>people</span>: <span>[</span>
    <span>{</span>
      <span>name</span>: <span>"Gavin Belson"</span><span>,</span>
      <span>id</span>: <span>0</span><span>,</span>
      <span>is_ceo</span>: <span>true</span><span>,</span>
      <span>description</span>: <span>"CEO of Hooli"</span><span>,</span>
    <span>}</span><span>,</span>
    <span>{</span>
      <span>name</span>: <span>"Richard Hendricks"</span><span>,</span>
      <span>id</span>: <span>1</span><span>,</span>
      <span>is_ceo</span>: <span>true</span><span>,</span>
      <span>description</span>: <span>"CEO of Pied Piper"</span><span>,</span>
    <span>}</span><span>,</span>
    <span>{</span>
      <span>name</span>: <span>"Dinesh Chugtai"</span><span>,</span>
      <span>id</span>: <span>2</span><span>,</span>
      <span>is_ceo</span>: <span>false</span><span>,</span>
      <span>description</span>: <span>"Software Engineer"</span><span>,</span>
    <span>}</span><span>,</span>
    <span>{</span>
      <span>name</span>: <span>"Jared Dunn"</span><span>,</span>
      <span>id</span>: <span>3</span><span>,</span>
      <span>is_ceo</span>: <span>false</span><span>,</span>
    <span>}</span><span>,</span>
  <span>]</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// Output:</span>
<span>// ```</span>
<span>// =================</span>
<span>// = Hello, Hooli! =</span>
<span>// =================</span>
<span>// Hello, Gavin Belson!</span>
<span>// CEO of Hooli</span>

<span>// Hello, Richard Hendricks!</span>
<span>// CEO of Pied Piper</span>

<span>// Hello, Dinesh Chugtai!</span>
<span>// Software Engineer</span>

<span>// Hello, Jared Dunn!</span>
<span>// (no description)</span>
<span>// ```</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Limitations</h2><a id="user-content-limitations" aria-label="Permalink: Limitations" href="#limitations"></a></p>
<ul dir="auto">
<li>If not using inline (i.e., literals in TypeScript) proto <code>string</code>s <code>as const</code>, probably requires a <a href="https://github.com/nonara/ts-patch"><code>ts-patch</code></a> compiler patch to import <code>.proto</code> files until <a data-error-text="Failed to load title" data-id="779392318" data-permission-text="Title is private" data-url="https://github.com/microsoft/TypeScript/issues/42219" data-hovercard-type="issue" data-hovercard-url="/microsoft/TypeScript/issues/42219/hovercard" href="https://github.com/microsoft/TypeScript/issues/42219">microsoft/TypeScript#42219</a> is resolved</li>
<li><code>service</code>s and <code>rpc</code>s are not supported (only <code>message</code>s)</li>
<li><code>oneof</code> and <code>map</code> fields are not supported</li>
<li><code>import</code>s are not supported (for now, concatenate)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">API</h2><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>pbt</code></h3><a id="user-content-pbt" aria-label="Permalink: pbt" href="#pbt"></a></p>
<p dir="auto">Top-level exported namespace.</p>
<div data-snippet-clipboard-copy-content="import { pbt } from &quot;protobuf-ts-types&quot;;"><pre><code>import { pbt } from "protobuf-ts-types";
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>pbt.infer&lt;Proto extends string, MessageName extends string = ""&gt;</code></h3><a id="user-content-pbtinferproto-extends-string-messagename-extends-string--" aria-label="Permalink: pbt.infer<Proto extends string, MessageName extends string = &quot;&quot;>" href="#pbtinferproto-extends-string-messagename-extends-string--"></a></p>
<p dir="auto">Given a proto source string, infers the types of the <code>message</code>s in the source.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Returns</h4><a id="user-content-returns" aria-label="Permalink: Returns" href="#returns"></a></p>
<ul dir="auto">
<li>If <code>MessageName</code> is an empty string, the returned type is a mapping from message names to message types.</li>
<li>If <code>MessageName</code> is a known <code>message</code>, the returned type is the inferred type of the given <code>MessageName</code>.</li>
<li>If <code>MessageName</code> is not a known <code>message</code>, the returned type is <code>never</code>.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Path to Open-Sourcing the DeepSeek Inference Engine (430 pts)]]></title>
            <link>https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine</link>
            <guid>43682088</guid>
            <pubDate>Mon, 14 Apr 2025 15:03:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine">https://github.com/deepseek-ai/open-infra-index/tree/main/OpenSourcing_DeepSeek_Inference_Engine</a>, See on <a href="https://news.ycombinator.com/item?id=43682088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">The Path to Open-Sourcing the DeepSeek Inference Engine</h2><a id="user-content-the-path-to-open-sourcing-the-deepseek-inference-engine" aria-label="Permalink: The Path to Open-Sourcing the DeepSeek Inference Engine" href="#the-path-to-open-sourcing-the-deepseek-inference-engine"></a></p>
<p dir="auto">A few weeks ago,
during <a href="https://github.com/deepseek-ai/open-infra-index?tab=readme-ov-file#202502-open-source-week">Open Source Week</a>,
we open-sourced several libraries.
The response from the community has been incredibly positive - sparking inspiring collaborations, productive
discussions, and valuable bug fixes.
Encouraged by this, we’ve decided to take another step forward: contributing our internal inference engine back to the
open-source community.</p>
<p dir="auto">We are deeply grateful for the open-source ecosystem, without which our progress toward AGI would not be possible.
Our training framework relies on <a href="https://github.com/pytorch/pytorch">PyTorch</a>, and our inference engine is built
upon <a href="https://github.com/vllm-project/vllm">vLLM</a>,
both of which have been instrumental in accelerating the training and deployment of DeepSeek models.</p>
<p dir="auto">Given the growing demand for deploying models like <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a>
and <a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek-R1</a>, we want to give back to the community as much as we can.
While we initially considered open-sourcing our full internal inference engine, we identified several challenges:</p>
<ul dir="auto">
<li><strong>Codebase Divergence</strong>: Our engine is based on an early fork of vLLM from over a year ago. Although structurally
similar, we’ve heavily customized it for DeepSeek models, making it difficult to extend for broader use cases.</li>
<li><strong>Infrastructure Dependencies</strong>: The engine is tightly coupled with our internal infrastructure, including cluster
management tools, making it impractical for public deployment without significant modifications.</li>
<li><strong>Limited Maintenance Bandwidth</strong>: As a small research team focused on developing better models, we lack bandwidth to
maintain a large open-source project.</li>
</ul>
<p dir="auto">Considering these challenges, we’ve decided to collaborate with existing open-source projects as more sustainable alternatives.</p>
<p dir="auto">Moving forward, we will work closely with existing open-source projects to:</p>
<ul dir="auto">
<li><strong>Extract Standalone Features</strong>: Modularize and contribute reusable components as independent libraries.</li>
<li><strong>Share Optimizations</strong>: Contribute design improvements and implementation details directly.</li>
</ul>
<p dir="auto">We are profoundly grateful for the open-source movement - from operating systems and programming languages to machine
learning frameworks and inference engines. It’s an honor to contribute to this thriving ecosystem and to see our models
and code embraced by the community. Together, let’s push the boundaries of AGI and ensure its benefits serve all of
humanity.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto"><strong>To clarify, this article outlines our approach to open-sourcing of our DeepSeek-Inference-Engine codebase only.
Regarding future model releases, we maintain an open and collaborative stance towards both the open-source community
and hardware partners.
We commit to proactively synchronizing inference-related engineering efforts prior to new model launches, with the
goal of enabling the community to achieve state-of-the-art (SOTA) support from Day-0. Our ultimate aim is to foster a
synchronized ecosystem where cutting-edge AI capabilities can be seamlessly implemented across diverse hardware
platforms upon official model releases.</strong></p>
</div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite File Format Viewer (199 pts)]]></title>
            <link>https://sqlite-internal.pages.dev</link>
            <guid>43682006</guid>
            <pubDate>Mon, 14 Apr 2025 14:55:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sqlite-internal.pages.dev">https://sqlite-internal.pages.dev</a>, See on <a href="https://news.ycombinator.com/item?id=43682006">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How to Bike Across the Country (169 pts)]]></title>
            <link>https://www.brooks.team/posts/how-to-bike-across-the-country/</link>
            <guid>43681936</guid>
            <pubDate>Mon, 14 Apr 2025 14:49:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.brooks.team/posts/how-to-bike-across-the-country/">https://www.brooks.team/posts/how-to-bike-across-the-country/</a>, See on <a href="https://news.ycombinator.com/item?id=43681936">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  
  <p>Loading...</p>

  
  
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tariff: A Python package that imposes tariffs on Python imports (114 pts)]]></title>
            <link>https://pypi.org/project/tariff/</link>
            <guid>43681752</guid>
            <pubDate>Mon, 14 Apr 2025 14:32:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pypi.org/project/tariff/">https://pypi.org/project/tariff/</a>, See on <a href="https://news.ycombinator.com/item?id=43681752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="description" data-project-tabs-target="content" role="tabpanel" aria-labelledby="description-tab mobile-description-tab" tabindex="-1">
          <h2>Project description</h2>
          <div>
            
<p>The GREATEST, most TREMENDOUS Python package that makes importing great again!</p>
<h2>About</h2>
<p>TARIFF is a fantastic tool that lets you impose import tariffs on Python packages. We're going to bring manufacturing BACK to your codebase by making foreign imports more EXPENSIVE!</p>
<h2>Installation</h2>
<pre lang="bash">pip<span> </span>install<span> </span>tariff
</pre>
<h2>Usage</h2>
<pre lang="python3"><span>import</span><span> </span><span>tariff</span>

<span># Set your tariff rates (package_name: percentage)</span>
<span>tariff</span><span>.</span><span>set</span><span>({</span>
    <span>"numpy"</span><span>:</span> <span>50</span><span>,</span>     <span># 50% tariff on numpy</span>
    <span>"pandas"</span><span>:</span> <span>200</span><span>,</span>   <span># 200% tariff on pandas</span>
    <span>"requests"</span><span>:</span> <span>150</span>  <span># 150% tariff on requests</span>
<span>})</span>

<span># Now when you import these packages, they'll be TARIFFED!</span>
<span>import</span><span> </span><span>numpy</span>   <span># This will be 50% slower</span>
<span>import</span><span> </span><span>pandas</span>  <span># This will be 200% slower</span>
</pre>
<h2>How It Works</h2>
<p>When you import a package that has a tariff:</p>
<ol>
<li>TARIFF measures how long the original import takes</li>
<li>TARIFF makes the import take longer based on your tariff percentage</li>
<li>TARIFF announces the tariff with a TREMENDOUS message</li>
</ol>
<h2>Example Output</h2>
<pre><code>JUST IMPOSED a 50% TARIFF on numpy! Original import took 45000 us, now takes 67500 us. American packages are WINNING AGAIN! #MIPA
</code></pre>
<h2>Why TARIFF?</h2>
<p>Because foreign packages have been STEALING our CPU cycles for TOO LONG! It's time to put AMERICA FIRST and make importing FAIR and BALANCED again!</p>
<h2>License</h2>
<p>This is a parody package. Use at your own risk. MAKE IMPORTING GREAT AGAIN!</p>

          </div>
        </div><div id="files" data-project-tabs-target="content" role="tabpanel" aria-labelledby="files-tab mobile-files-tab" tabindex="-1">
            <h2>Download files</h2>
            <p>Download the file for your platform. If you're not sure which to choose, learn more about <a href="https://packaging.python.org/tutorials/installing-packages/" title="External link" target="_blank" rel="noopener">installing packages</a>.</p>

            <h3>
Source Distribution            </h3>

                  


            <h3>
Built Distribution            </h3>

                

          </div><div id="tariff-1.0.0.tar.gz" data-project-tabs-target="content" role="tabpanel" aria-labelledby="file-tab mobile-file-tab" tabindex="-1">
  <h2>File details</h2>
  <p>Details for the file <code>tariff-1.0.0.tar.gz</code>.</p>

  <h3>File metadata</h3>
  <div>
    <ul>
      <li>
        Download URL: <a href="https://files.pythonhosted.org/packages/6e/86/8a6d8b6c88cbfa42a7f20d7a0bf166ec621e4b0d4f89b2910a539452d587/tariff-1.0.0.tar.gz">
          tariff-1.0.0.tar.gz
        </a>
      </li>
      <li>Upload date: <time datetime="2025-04-10T19:33:18+0000" data-controller="localized-time" data-localized-time-relative="true" data-localized-time-show-time="false">
  Apr 10, 2025
</time></li>
      <li>Size: 4.0 kB</li>
      <li>Tags: Source</li>
      <li>
Uploaded using Trusted Publishing? No      </li>
      <li>Uploaded via: twine/6.1.0 CPython/3.10.16</li>
    </ul>
  </div>

  <h3>File hashes</h3>
  <div>
    <table>
      <caption>Hashes for tariff-1.0.0.tar.gz</caption>
      <thead>
        <tr>
          <th scope="col">Algorithm</th>
          <th scope="col">Hash digest</th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr data-controller="clipboard">
          <th scope="row">SHA256</th>
          <td><code data-clipboard-target="source">24a8d49034398a7820d6f9eb1d345476f0d0bfcb67f75ce266284943208596cd</code></td>
          <td>
            
          </td>
        </tr>
        <tr data-controller="clipboard">
          <th scope="row">MD5</th>
          <td><code data-clipboard-target="source">46117ae6651d1a90855555f1cb629b38</code></td>
          <td>
            
          </td>
        </tr>
        <tr data-controller="clipboard">
          <th scope="row">BLAKE2b-256</th>
          <td><code data-clipboard-target="source">6e868a6d8b6c88cbfa42a7f20d7a0bf166ec621e4b0d4f89b2910a539452d587</code></td>
          <td>
            
          </td>
        </tr>
      </tbody>
    </table>
    <p>
<a href="https://pip.pypa.io/en/stable/topics/secure-installs/#hash-checking-mode" title="External link" target="_blank" rel="noopener">See more details on using hashes here.</a>    </p>
  </div>

</div><div id="tariff-1.0.0-py3-none-any.whl" data-project-tabs-target="content" role="tabpanel" aria-labelledby="file-tab mobile-file-tab" tabindex="-1">
  <h2>File details</h2>
  <p>Details for the file <code>tariff-1.0.0-py3-none-any.whl</code>.</p>

  <h3>File metadata</h3>
  <div>
    <ul>
      <li>
        Download URL: <a href="https://files.pythonhosted.org/packages/8e/ae/6f57db1138cfce911d19113e8d931a739858e0116ca3a2a3e391748cb5ff/tariff-1.0.0-py3-none-any.whl">
          tariff-1.0.0-py3-none-any.whl
        </a>
      </li>
      <li>Upload date: <time datetime="2025-04-10T19:33:17+0000" data-controller="localized-time" data-localized-time-relative="true" data-localized-time-show-time="false">
  Apr 10, 2025
</time></li>
      <li>Size: 4.5 kB</li>
      <li>Tags: Python 3</li>
      <li>
Uploaded using Trusted Publishing? No      </li>
      <li>Uploaded via: twine/6.1.0 CPython/3.10.16</li>
    </ul>
  </div>

  <h3>File hashes</h3>
  <div>
    <table>
      <caption>Hashes for tariff-1.0.0-py3-none-any.whl</caption>
      <thead>
        <tr>
          <th scope="col">Algorithm</th>
          <th scope="col">Hash digest</th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr data-controller="clipboard">
          <th scope="row">SHA256</th>
          <td><code data-clipboard-target="source">20d738a789f96146ae49de4fffc000ea9942bd4335f0cdc336d2c824e6aa063b</code></td>
          <td>
            
          </td>
        </tr>
        <tr data-controller="clipboard">
          <th scope="row">MD5</th>
          <td><code data-clipboard-target="source">eb211c83b4fd4814b8a15f0a92ed3dab</code></td>
          <td>
            
          </td>
        </tr>
        <tr data-controller="clipboard">
          <th scope="row">BLAKE2b-256</th>
          <td><code data-clipboard-target="source">8eae6f57db1138cfce911d19113e8d931a739858e0116ca3a2a3e391748cb5ff</code></td>
          <td>
            
          </td>
        </tr>
      </tbody>
    </table>
    <p>
<a href="https://pip.pypa.io/en/stable/topics/secure-installs/#hash-checking-mode" title="External link" target="_blank" rel="noopener">See more details on using hashes here.</a>    </p>
  </div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A hackable AI assistant using a single SQLite table and a handful of cron jobs (568 pts)]]></title>
            <link>https://www.geoffreylitt.com/2025/04/12/how-i-made-a-useful-ai-assistant-with-one-sqlite-table-and-a-handful-of-cron-jobs</link>
            <guid>43681287</guid>
            <pubDate>Mon, 14 Apr 2025 13:52:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geoffreylitt.com/2025/04/12/how-i-made-a-useful-ai-assistant-with-one-sqlite-table-and-a-handful-of-cron-jobs">https://www.geoffreylitt.com/2025/04/12/how-i-made-a-useful-ai-assistant-with-one-sqlite-table-and-a-handful-of-cron-jobs</a>, See on <a href="https://news.ycombinator.com/item?id=43681287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There’s a lot of hype these days around patterns for building with AI. Agents, memory, RAG, assistants—so many buzzwords! But the reality is, <strong>you don’t need fancy techniques or libraries to build useful personal tools with LLMs.</strong></p>

<p>In this short post, I’ll show you how I built a useful AI assistant for my family using a dead simple architecture: a single SQLite table of memories, and a handful of cron jobs for ingesting memories and sending updates, all hosted on <a href="https://www.val.town/">Val.town</a>. The whole thing is so simple that you can easily copy and extend it yourself.</p>

<h2 id="meet-stevens">Meet Stevens</h2>

<p>The assistant is called Stevens, named after the butler in the great Ishiguro novel <a href="https://en.wikipedia.org/wiki/The_Remains_of_the_Day">Remains of the Day</a>. Every morning it sends a brief to me and my wife via Telegram, including our calendar schedules for the day, a preview of the weather forecast, any postal mail or packages we’re expected to receive, and any reminders we’ve asked it to keep track of. All written up nice and formally, just like you’d expect from a proper butler.</p>

<p>Here’s an example. (I’ll use fake data throughout this post, beacuse our actual updates contain private information.)</p>

<p><img src="https://www.geoffreylitt.com/images/article_images/stevens/telegram.png?1744560139" alt=""></p>

<p>Beyond the daily brief, we can communicate with Stevens on-demand—we can forward an email with some important info, or just leave a reminder or ask a question via Telegram chat.</p>

<p><img src="https://www.geoffreylitt.com/images/article_images/stevens/coffee.png?1744560139" alt=""></p>

<p>That’s Stevens. It’s rudimentary, but already more useful to me than Siri!</p>

<h2 id="behind-the-scenes">Behind the scenes</h2>

<p>Let’s break down the simple architecture behind Stevens. The whole thing is hosted on <a href="https://www.val.town/">Val.town</a>, a lovely platform that offers SQLite storage, HTTP request handling, scheduled cron jobs, and inbound/outbound email: a perfect set of capabilities for this project.</p>

<p>First, how does Stevens know what goes in the morning brief? The key is the butler’s notebook, a log of everything that Stevens knows. There’s an admin view where we can see the notebook contents—let’s peek and see what’s in there:</p>

<p><img src="https://www.geoffreylitt.com/images/article_images/stevens/notebook.png?1744560139" alt=""></p>

<p>You can see some of the entries that fed into the morning brief above—for example, the parent-teacher conference has a log entry.</p>

<p>In addition to some text, entries can have a <em>date</em> when they are expected to be relevant.  There are also entries with no date that serve as general background info, and are always included. You can see these particular background memories came from a Telegram chat, because Stevens does an intake interview via Telegram when you first get started:</p>

<p><img src="https://www.geoffreylitt.com/images/article_images/stevens/background.png?1744560139" alt=""></p>

<p><strong>With this notebook in hand, sending the morning brief is easy</strong>: just run a cron job which makes a call to the Claude API to write the update, and then sends the text to a Telegram thread. As context for the model, we include any log entries dated for the coming week, as well as the undated background entries.</p>

<p>Under the hood, the “notebook” is just a single SQLite table with a few columns. Here’s a more boring view of things:</p>

<p><img src="https://www.geoffreylitt.com/images/article_images/stevens/db.png?1744560139" alt=""></p>

<p>But wait: how did the various log entries get there in the first place? In the admin view, we can watch Stevens buzzing around entering things into the log from various sources:</p>

<video width="100%" controls="">
  <source src="https://www.geoffreylitt.com/images/article_images/stevens/cron.mp4" type="video/mp4">
</video>

<p>This is just some data importers populating the table:</p>

<ul>
<li>An hourly data pull from the Google Calendar API</li>
<li>An hourly check of the local weather forecast using a weather API</li>
<li>I forward <a href="https://www.usps.com/manage/informed-delivery.htm">USPS Informed Delivery</a> containing scans of our postal mail, and Stevens OCRs them using Claude</li>
<li>Inbound Telegram and email messages can also result in log entries</li>
<li>Every week, some “fun facts” get added into the log, as a way of adding some color to future daily updates.</li>
</ul>

<p><strong>This system is easily extensible with new importers.</strong> An importer is just any process that adds/edits memories in the log. The memory contents can be any arbitrary text, since they’ll just be fed back into an LLM later anyways.</p>

<h2 id="reflections">Reflections</h2>

<p>A few quick reflections on this project:</p>

<p><strong>It’s very useful for personal AI tools to have access to broader context from other information sources.</strong> Awareness of things like my calendar and the weather forecast turns a dumb chatbot into a useful assistant. ChatGPT recently added memory of past conversations, but there’s lots of information not stored within that silo. I’ve <a href="https://x.com/geoffreylitt/status/1810442615264796864">written before</a> about how the endgame for AI-driven personal software isn’t more app silos, it’s small tools operating on a shared pool of context about our lives.</p>

<p><strong>“Memory” can start simple.</strong> In this case, the use cases of the assistant are limited, and its information is inherently time-bounded, so it’s fairly easy to query for the relevant context to give to the LLM. It also helps that some modern models have long context windows. As the available information grows in size, RAG and <a href="https://x.com/sjwhitmore/status/1910439061615239520">fancier</a> <a href="https://arxiv.org/abs/2304.03442">approaches</a> to memory may be needed, but you can start simple.</p>

<p><strong>Vibe coding enables sillier projects.</strong> Initially, Stevens spoke with a dry tone, like you might expect from a generic Apple or Google product. But it turned out it was just more <em>fun</em> to have the assistant speak like a formal butler. This was trivial to do, just a couple lines in a prompt. Similarly, I decided to make the admin dashboard views feel like a video game, because why not? I generated the image assets in ChatGPT, and vibe coded the whole UI in Cursor + Claude 3.7 Sonnet; it took a tiny bit of extra effort in exchange for a lot more fun.</p>

<h2 id="try-it-yourself">Try it yourself</h2>

<p>Stevens isn’t a product you can run out of the box, it’s just a personal project I made for myself.</p>

<p>But if you’re curious, you can check out the code and fork the project <a href="https://www.val.town/x/geoffreylitt/stevensDemo">here</a>. You should be able to apply this basic pattern—a single memories table and an extensible constellation of cron jobs—to do lots of other useful things.</p>

<p>I recommend editing the code using your AI editor of choice with the <a href="https://github.com/pomdtr/vt">Valtown CLI</a> to sync to local filesystem.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta antitrust trial kicks off in federal court (316 pts)]]></title>
            <link>https://www.axios.com/pro/tech-policy/2025/04/14/ftc-meta-antitrust-trial-kicks-off-in-federal-court</link>
            <guid>43680957</guid>
            <pubDate>Mon, 14 Apr 2025 13:18:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/pro/tech-policy/2025/04/14/ftc-meta-antitrust-trial-kicks-off-in-federal-court">https://www.axios.com/pro/tech-policy/2025/04/14/ftc-meta-antitrust-trial-kicks-off-in-federal-court</a>, See on <a href="https://news.ycombinator.com/item?id=43680957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><div data-theme="pro"><div data-vars-content-id="cff1ef1d-a5d1-4ba0-b14e-f2c9500f3c62" data-vars-event-name="story_view" data-vars-deprecated-content-id="cff1ef1d-a5d1-4ba0-b14e-f2c9500f3c62" data-vars-deprecated-headline="Meta antitrust trial kicks off in federal court" data-vars-deprecated-category="story" data-vars-deprecated-sub-category="story" data-vars-headline="Meta antitrust trial kicks off in federal court" data-vars-latitude="47.23" data-vars-longitude="8.84" data-vars-postal-code="8645"><div><p><span>Axios Pro Exclusive Content</span></p><div><p><img alt="" loading="lazy" width="52" height="52" decoding="async" data-nimg="1" srcset="https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2FC_Z_frNtxeJxkMDSsXJSCdXj6w0%3D%2F52x0%2Fsmart%2F2023%2F03%2F24%2F1679666155723.jpg&amp;w=64&amp;q=75 1x, https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2FC_Z_frNtxeJxkMDSsXJSCdXj6w0%3D%2F52x0%2Fsmart%2F2023%2F03%2F24%2F1679666155723.jpg&amp;w=128&amp;q=75 2x" src="https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2FC_Z_frNtxeJxkMDSsXJSCdXj6w0%3D%2F52x0%2Fsmart%2F2023%2F03%2F24%2F1679666155723.jpg&amp;w=128&amp;q=75"></p></div></div><figure data-cy="au-image" data-chromatic="ignore"><img data-cy="StoryImage" alt="Mark Zuckerberg taking an oath in Congress with a black backdrop" fetchpriority="high" width="1920" height="1080" decoding="async" data-nimg="1" sizes="100vw" srcset="https://images.axios.com/dZ2sVpFnmwMBZaPT7ZQc8qfwTSw=/0x0:7555x4250/640x360/2025/04/11/1744384586720.jpg?w=640 640w, https://images.axios.com/mIftE3zydXa1hmCjzI7bWDEyCOc=/0x0:7555x4250/1920x1080/2025/04/11/1744384586720.jpg?w=750 750w, https://images.axios.com/mIftE3zydXa1hmCjzI7bWDEyCOc=/0x0:7555x4250/1920x1080/2025/04/11/1744384586720.jpg?w=828 828w, https://images.axios.com/mIftE3zydXa1hmCjzI7bWDEyCOc=/0x0:7555x4250/1920x1080/2025/04/11/1744384586720.jpg?w=1080 1080w, https://images.axios.com/mIftE3zydXa1hmCjzI7bWDEyCOc=/0x0:7555x4250/1920x1080/2025/04/11/1744384586720.jpg?w=1200 1200w, https://images.axios.com/mIftE3zydXa1hmCjzI7bWDEyCOc=/0x0:7555x4250/1920x1080/2025/04/11/1744384586720.jpg?w=1920 1920w, https://images.axios.com/mIftE3zydXa1hmCjzI7bWDEyCOc=/0x0:7555x4250/1920x1080/2025/04/11/1744384586720.jpg?w=2048 2048w, https://images.axios.com/mIftE3zydXa1hmCjzI7bWDEyCOc=/0x0:7555x4250/1920x1080/2025/04/11/1744384586720.jpg?w=3840 3840w" src="https://images.axios.com/mIftE3zydXa1hmCjzI7bWDEyCOc=/0x0:7555x4250/1920x1080/2025/04/11/1744384586720.jpg?w=3840"><figcaption data-cy="image-caption"><p>Mark Zuckerberg on Jan. 31, 2024 on Capitol Hill. Photo: Tom Williams/CQ-Roll Call, Inc via Getty Images</p></figcaption></figure><div data-chromatic="ignore"><p><span data-schema="smart-brevity"><p>The Federal Trade Commission and Meta will square off in a long-awaited antitrust trial on Monday over the tech giant's past acquisitions of WhatsApp and Instagram.</p><p><strong>Why it matters: </strong>The trial will be a major test of the FTC's ability to take on tech behemoths for<strong> </strong>allegedly breaking antitrust law and comes as Meta CEO Mark Zuckerberg<strong> </strong>tries to <a data-vars-link-text="cozy up" data-vars-click-url="https://www.axios.com/pro/tech-policy/2025/04/03/zuckerberg-gets-closer-to-dc" data-vars-content-id="cff1ef1d-a5d1-4ba0-b14e-f2c9500f3c62" data-vars-headline="Meta antitrust trial kicks off in federal court" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/pro/tech-policy/2025/04/03/zuckerberg-gets-closer-to-dc" target="_self">cozy up</a> to President Trump.</p></span></p><ul><li>The case could result in Meta having to spin off WhatsApp and Instagram. </li><li>If Meta wins, the company would be vindicated in its longtime argument that the two apps couldn't have thrived without the company's backing and that Meta has plenty of competition in the social networking space.</li><li>The lawsuit's main question is whether Meta acted illegally in its WhatsApp and Instagram acquisitions, done in 2014 and 2012.</li></ul><p><strong>Federal judge <a data-vars-link-text="James Boasberg" data-vars-click-url="https://www.axios.com/2025/03/18/judge-trump-impeachment-james-boasberg" data-vars-content-id="cff1ef1d-a5d1-4ba0-b14e-f2c9500f3c62" data-vars-headline="Meta antitrust trial kicks off in federal court" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2025/03/18/judge-trump-impeachment-james-boasberg" target="_self">James Boasberg</a> </strong>will hear the case, which was <a data-vars-link-text="first filed" data-vars-click-url="https://www.axios.com/2020/12/09/ftc-sues-facebookstate-ags-sue-facebook" data-vars-content-id="cff1ef1d-a5d1-4ba0-b14e-f2c9500f3c62" data-vars-headline="Meta antitrust trial kicks off in federal court" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2020/12/09/ftc-sues-facebookstate-ags-sue-facebook" target="_self">first filed</a> in December 2020 under Trump's first administration.</p><ul><li>A judge <a data-vars-link-text="dismissed" data-vars-click-url="https://www.axios.com/2021/06/28/judge-dismisses-ftcs-antitrust-complaint-against-facebook" data-vars-content-id="cff1ef1d-a5d1-4ba0-b14e-f2c9500f3c62" data-vars-headline="Meta antitrust trial kicks off in federal court" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.axios.com/2021/06/28/judge-dismisses-ftcs-antitrust-complaint-against-facebook" target="_self">dismissed</a> that original lawsuit in June 2021 for lacking sufficient evidence of Meta's market power.</li><li>Under Lina Khan, FTC chair under President Biden, the case was re-filed and expanded in August 2021. </li><li>Boasberg allowed that case to proceed in January 2022, and rejected a bid from Meta<strong> </strong>last year to have the case dismissed, paving way for this trial.</li></ul><p><strong>What they're saying: </strong>The FTC says Meta has illegally monopolized the market for "personal social networking services" through those acquisitions, in a bid to "neutralize" its rivals, per legal filings. </p><ul><li>"Acquiring these competitive threats has enabled Facebook to sustain its dominance—to the detriment of competition and users—not by competing on the merits, but by avoiding competition," the FTC wrote in a filing.</li><li>Meta could have chosen to compete with then-upstart photo sharing app Instagram in 2012, a senior FTC official said on a call with reporters ahead of the trial, but instead it bought it, and did the same with WhatsApp. </li></ul><p><strong>The other side: </strong>"The FTC's lawsuit against Meta defies reality. The evidence at trial will show what every 17-year-old in the world knows: Instagram, Facebook and WhatsApp compete with Chinese-owned TikTok, YouTube, X, iMessage and many others," Meta spokesperson Chris Sgro said in a statement.</p><ul><li>"More than 10 years after the FTC reviewed and cleared our acquisitions, the Commission's action in this case sends the message that no deal is ever truly final."</li><li>"Regulators should be supporting American innovation, rather than seeking to break up a great American company and further advantaging China on critical issues like AI."</li></ul><p><strong>What we're watching:</strong> The case could take eight weeks or more. There'll be a slew of high-profile witnesses, including Zuckerberg.</p><ul><li>Former COO Sheryl Sandberg, chief technology officer Andrew Bosworth, and WhatsApp and Instagram leadership past and present, will also testify, per court filings.</li><li>Representatives from Snap, TikTok and Pinterest are expected to testify as well.</li></ul><p><strong>Our thought bubble: </strong>Tech firms have gotten much closer with Trump in his second term.</p><ul><li>But unless Trump tells the FTC to shut the whole trial down, Meta's overtures may not do the company any good here.</li></ul></div></div><h5>Go deeper</h5></div><div data-theme="pro" data-cy="pro-paywall" data-vars-event-name="paywall_view" data-vars-content-id="cff1ef1d-a5d1-4ba0-b14e-f2c9500f3c62" data-vars-latitude="47.23" data-vars-longitude="8.84" data-vars-postal-code="8645" data-vars-deprecated-category="cta" data-vars-deprecated-experiment="pro-paywall" data-vars-deprecated-experiment-variant="tech-policy"><p>This article is currently free.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DolphinGemma: How Google AI is helping decode dolphin communication (272 pts)]]></title>
            <link>https://blog.google/technology/ai/dolphingemma/</link>
            <guid>43680899</guid>
            <pubDate>Mon, 14 Apr 2025 13:12:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/dolphingemma/">https://blog.google/technology/ai/dolphingemma/</a>, See on <a href="https://news.ycombinator.com/item?id=43680899">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
  }">
  
  <div>
      <div>
          
            <p>Apr 14, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          DolphinGemma, a large language model developed by Google, is helping scientists study how dolphins communicate — and hopefully find out what they're saying, too.
        </p>
      
    </div>
  
  <div>
      
  
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="122px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-122x92.format-webp.webp 122w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="244px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-244x184.format-webp.webp 244w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-244x184.format-webp.webp" alt="thad headshot" sizes=" 122px,  244px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-122x92.format-webp.webp 122w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1517757973355.max-244x184.format-webp.webp 244w" data-target="image" loading="lazy">
    


        </picture>
    </figure>



<div>
  <p>Dr. Thad Starner</p>
  
    <p>
      Google DeepMind Research Scientist and Georgia Tech Professor
    </p>
  
  
</div>

    </div>
</div>

    

    
      


  <uni-youtube-player-hero index="0" thumbnail-alt="DolphinGemma text over a picture of dolphins" component-title="DolphinGemma: How Google AI is helping decode dolphin communication" video-id="T8GdEVVvXyE" video-type="video" image="DolphinGemma_SocialExplainers_16x9_DolphinGemma" video-image-url-lazy="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_SocialExplainers_16x.width-100.format-webp.webp" video-image-url-mobile="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_SocialExplainers_16x.width-700.format-webp.webp" video-image-url-desktop="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_SocialExplainers_16.width-1000.format-webp.webp">
  </uni-youtube-player-hero>


    

    
    <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="DolphinGemma: How Google AI is helping decode dolphin communication" listen-to-article="Listen to article" data-date-modified="2025-04-14T17:08:29.525540+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }"><p data-block-key="6q2c0">For decades, understanding the clicks, whistles and burst pulses of dolphins has been a scientific frontier. What if we could not only listen to dolphins, but also understand the patterns of their complex communication well enough to generate realistic responses?</p><p data-block-key="4o0o1">Today, on National Dolphin Day, Google, in collaboration with researchers at Georgia Tech and the field research of the <a href="https://www.wilddolphinproject.org/">Wild Dolphin Project</a> (WDP), is announcing progress on DolphinGemma: a foundational AI model trained to learn the structure of dolphin vocalizations and generate novel dolphin-like sound sequences. This approach in the quest for interspecies communication pushes the boundaries of AI and our potential connection with the marine world.</p></div>
  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }"><h2 data-block-key="6q2c0">Researching dolphin society for decades</h2><p data-block-key="ae8k9">Understanding any species requires deep context, and that's one of the many things the WDP provides. Since 1985, WDP has conducted the world's longest-running underwater dolphin research project, studying a specific community of wild Atlantic spotted dolphins (Stenella frontalis) in the Bahamas across generations. This non-invasive, "In Their World, on Their Terms" approach yields a rich, unique dataset: decades of underwater video and audio meticulously paired with individual dolphin identities, life histories and observed behaviors.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Dolphins swimming in the water" external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="DolphinGemma: How Google AI is helping decode dolphin communication" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="27106">A pod of Atlantic spotted dolphins, Stenella frontalis</p>
    </div>
  
  
    <p><img alt="Dolphins swimming in the water" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/dolphins.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/dolphins.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/dolphins.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }"><p data-block-key="iocbw">A primary focus for WDP is observing and analyzing the dolphins' natural communication and social interactions. Working underwater allows researchers to directly link sounds to specific behaviors in ways surface observation cannot. For decades, they have correlated sound types with behavioral contexts. Here are some examples:</p><ul><li data-block-key="fu0nf">Signature whistles (unique names) that can be used by mothers and calves to reunite</li><li data-block-key="b63q5">Burst-pulse "squawks" often seen during fights</li><li data-block-key="bseip">Click "buzzes" often used during courtship or chasing sharks</li></ul><p data-block-key="2ar36">Knowing the individual dolphins involved is crucial for accurate interpretation. The ultimate goal of this observational work is to understand the structure and potential meaning within these natural sound sequences — seeking patterns and rules that might indicate language. This long-term analysis of natural communication forms the bedrock of WDP's research and provides essential context for any AI analysis.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="A split image: left, a dolphin touching the sandy seabed underwater; right, a spectrogram with bright vertical streaks indicating high-frequency sounds." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="DolphinGemma: How Google AI is helping decode dolphin communication" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="s704z">Left: A mother spotted dolphin observes her calf while foraging. She will use her unique signature whistle to call the calf back after he is finished. Right: Spectrogram to visualize the whistle.</p>
    </div>
  
  
    <p><img alt="A split image: left, a dolphin touching the sandy seabed underwater; right, a spectrogram with bright vertical streaks indicating high-frequency sounds." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword1_RD3_V02.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword1_RD3_V02.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword1_RD3_V02.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }"><h2 data-block-key="iocbw">Introducing DolphinGemma</h2><p data-block-key="2m8ks">Analyzing dolphins' natural, complex communication is a monumental task, and WDP's vast, labeled dataset provides a unique opportunity for cutting-edge AI.</p><p data-block-key="bgerj">Enter DolphinGemma. Developed by Google, this AI model makes use of specific Google audio technologies: the SoundStream tokenizer efficiently represents dolphin sounds, which are then processed by a model architecture suited for complex sequences. This ~400M parameter model is optimally-sized to run directly on the Pixel phones WDP uses in the field.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Two spectrograms: left shows three arching sound patterns; right shows a more uniform sound pattern." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="DolphinGemma: How Google AI is helping decode dolphin communication" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="30sb2">Left: Whistles (left) and burst pulses (right) generated during early testing of DolphinGemma.</p>
    </div>
  
  
    <p><img alt="Two spectrograms: left shows three arching sound patterns; right shows a more uniform sound pattern." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword2_RD3_V01.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword2_RD3_V01.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Keyword2_RD3_V01.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }"><p data-block-key="3p158">This model builds upon insights from <a href="https://ai.google.dev/gemma">Gemma</a>, Google’s collection of lightweight, state-of-the-art open models that are built from the same research and technology that powers our Gemini models. Trained extensively on WDP’s acoustic database of wild Atlantic spotted dolphins, DolphinGemma functions as an audio-in, audio-out model, processes sequences of natural dolphin sounds to identify patterns, structure and ultimately predict the likely subsequent sounds in a sequence, much like how large language models for human language predict the next word or token in a sentence.</p><p data-block-key="a754e">WDP is beginning to deploy DolphinGemma this field season with immediate potential benefits. By identifying recurring sound patterns, clusters and reliable sequences, the model can help researchers uncover hidden structures and potential meanings within the dolphins' natural communication — a task previously requiring immense human effort. Eventually, these patterns, augmented with synthetic sounds created by the researchers to refer to objects with which the dolphins like to play, may establish a shared vocabulary with the dolphins for interactive communication.</p></div>
  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }"><h2 data-block-key="3p158">Using Pixel phones to listen to and analyze dolphin sounds</h2><p data-block-key="7vd3s">In addition to analyzing natural communication, WDP is also pursuing a distinct, parallel path: exploring potential two-way interaction using technology in the ocean. This effort led to the development of the <a href="https://www.wilddolphinproject.org/our-research/chat-research/">CHAT</a> (Cetacean Hearing Augmentation Telemetry) system, in partnership with the Georgia Institute of Technology. CHAT is an underwater computer designed not to directly decipher the dolphins' complex natural language, but to establish a simpler, shared vocabulary.</p><p data-block-key="6avcn">The concept first relies on associating novel, synthetic whistles (created by CHAT, distinct from natural dolphin sounds) with specific objects the dolphins enjoy, like sargassum, seagrass or scarves the researchers use. By demonstrating the system between humans, researchers hope the naturally curious dolphins will learn to mimic the whistles to request these items. Eventually, as more of the dolphins’ natural sounds are understood, they can also be added to the system.</p></div>
  

  
    
  
    


  <uni-youtube-player-article index="10" thumbnail-alt="CHAT explainer video" video-id="YhopeQKbpZA" video-type="video">
  </uni-youtube-player-article>


  


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }"><p data-block-key="3p158">To enable two-way interaction, the CHAT system first needs to:</p><ol><li data-block-key="9is9o">Hear the mimic accurately amid ocean noise.</li><li data-block-key="5qsu4">Identify which whistle was mimicked in real-time.</li><li data-block-key="48nl7">Inform the researcher (via bone-conducting headphones that work underwater) which object the dolphin "requested."</li><li data-block-key="2f5o">Enable the researcher to respond quickly by offering the correct object, reinforcing the connection.</li></ol><p data-block-key="c9b4">A Google Pixel 6 handled the high-fidelity analysis of dolphin sounds in real time. The upcoming generation, centered around a Google Pixel 9 (research slated for summer 2025), builds on this effort by integrating speaker/microphone functions and using the phone's advanced processing to run both deep learning models and template matching algorithms simultaneously.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Two portraits: left, a woman on a boat holding a device; right, a man indoors wearing headphones and holding a similar device." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="DolphinGemma: How Google AI is helping decode dolphin communication" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="pdphj">Left: Dr. Denise Herzing wearing “Chat Senior, 2012”, Right: Georgia Tech PhD Student Charles Ramey wearing “Chat Junior, 2025”</p>
    </div>
  
  
    <p><img alt="Two portraits: left, a woman on a boat holding a device; right, a man indoors wearing headphones and holding a similar device." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Researchers_RD2_V01.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Researchers_RD2_V01.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/DolphinGemma_Researchers_RD2_V01.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }">
        <p data-block-key="su54v">Using Pixel smartphones dramatically reduces the need for custom hardware, improves system maintainability, lowers power consumption and shrinks the device's cost and size — crucial advantages for field research in the open ocean. Meanwhile, DolphinGemma’s predictive power can help CHAT anticipate and identify potential mimics earlier in the vocalization sequence, increasing the speed at which researchers can react to the dolphins and making interactions more fluid and reinforcing.</p>
      </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Pixel phone inside a case hooked up to cables" external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="DolphinGemma: How Google AI is helping decode dolphin communication" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    <div slot="caption-slot">
      <p data-block-key="qfo9j">A Google Pixel 9 inside the latest CHAT system hardware.</p>
    </div>
  
  
    <p><img alt="Pixel phone inside a case hooked up to cables" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CHAT_Pixel.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CHAT_Pixel.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CHAT_Pixel.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;DolphinGemma: How Google AI is helping decode dolphin communication&quot;
         }"><h2 data-block-key="su54v">Sharing DolphinGemma with the research community</h2><p data-block-key="2ad01">Recognizing the value of collaboration in scientific discovery, we’re planning to share DolphinGemma as an open model this summer. While trained on Atlantic spotted dolphin sounds, we anticipate its potential utility for researchers studying other cetacean species, like bottlenose or spinner dolphins. Fine-tuning may be required for different species' vocalizations, and the open nature of the model facilitates this adaptation.</p><p data-block-key="40nps">By providing tools like DolphinGemma, we hope to give researchers worldwide the tools to mine their own acoustic datasets, accelerate the search for patterns and collectively deepen our understanding of these intelligent marine mammals.</p><p data-block-key="e2jq7">The journey to understanding dolphin communication is long, but the combination of dedicated field research by WDP, engineering expertise from Georgia Tech and the power of Google's technology is opening exciting new possibilities. We're not just listening anymore. We're beginning to understand the patterns within the sounds, paving the way for a future where the gap between human and dolphin communication might just get a little smaller.</p><p data-block-key="esrl0">You can learn more about the<a href="https://www.wilddolphinproject.org/"> Wild Dolphin Project</a> on their website.</p></div>
  


            
            

            
              




            
          </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meilisearch – search engine API bringing AI-powered hybrid search (131 pts)]]></title>
            <link>https://github.com/meilisearch/meilisearch</link>
            <guid>43680699</guid>
            <pubDate>Mon, 14 Apr 2025 12:46:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/meilisearch/meilisearch">https://github.com/meilisearch/meilisearch</a>, See on <a href="https://news.ycombinator.com/item?id=43680699">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://www.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=logo#gh-light-mode-only" rel="nofollow">
    <img src="https://github.com/meilisearch/meilisearch/raw/main/assets/meilisearch-logo-light.svg?sanitize=true#gh-light-mode-only">
  </a>
  <a href="https://www.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=logo#gh-dark-mode-only" rel="nofollow">
    <img src="https://github.com/meilisearch/meilisearch/raw/main/assets/meilisearch-logo-dark.svg?sanitize=true#gh-dark-mode-only">
  </a>
</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">
  <a href="https://www.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav" rel="nofollow">Website</a> |
  <a href="https://roadmap.meilisearch.com/tabs/1-under-consideration" rel="nofollow">Roadmap</a> |
  <a href="https://www.meilisearch.com/pricing?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav" rel="nofollow">Meilisearch Cloud</a> |
  <a href="https://blog.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav" rel="nofollow">Blog</a> |
  <a href="https://www.meilisearch.com/docs?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav" rel="nofollow">Documentation</a> |
  <a href="https://www.meilisearch.com/docs/faq?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav" rel="nofollow">FAQ</a> |
  <a href="https://discord.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=nav" rel="nofollow">Discord</a>
</h4><a id="user-content---website---roadmap---meilisearch-cloud---blog---documentation---faq---discord" aria-label="Permalink: Website |
  Roadmap |
  Meilisearch Cloud |
  Blog |
  Documentation |
  FAQ |
  Discord" href="#--website---roadmap---meilisearch-cloud---blog---documentation---faq---discord"></a></p>
<p dir="auto">
  <a href="https://deps.rs/repo/github/meilisearch/meilisearch" rel="nofollow"><img src="https://camo.githubusercontent.com/0b51de54cdba053bdde0478c1ffc91dbc30d279d73e71c78088e77e98f41735e/68747470733a2f2f646570732e72732f7265706f2f6769746875622f6d65696c697365617263682f6d65696c697365617263682f7374617475732e737667" alt="Dependency status" data-canonical-src="https://deps.rs/repo/github/meilisearch/meilisearch/status.svg"></a>
  <a href="https://github.com/meilisearch/meilisearch/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/60e4fe2b4b86adf9d06832e9dcbbe27eddf7f46bc4af612f3dda01c9907a7a07/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d696e666f726d6174696f6e616c" alt="License" data-canonical-src="https://img.shields.io/badge/license-MIT-informational"></a>
  <a href="https://github.com/meilisearch/meilisearch/queue"><img alt="Merge Queues enabled" src="https://camo.githubusercontent.com/4fc74073767004f08ce185305a7295b9e062871fb144ca0fa35592b02a3fcac0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d657267655f5175657565732d656e61626c65642d2532333537636636303f6c6f676f3d676974687562" data-canonical-src="https://img.shields.io/badge/Merge_Queues-enabled-%2357cf60?logo=github"></a>
</p>
<p name="user-content-ph-banner" dir="auto">
  <a href="https://www.producthunt.com/posts/meilisearch-ai" rel="nofollow">
    <img src="https://github.com/meilisearch/meilisearch/raw/main/assets/ph-banner.png" alt="Meilisearch AI-powered search general availability announcement on ProductHunt">
  </a>
</p>
<p dir="auto">⚡ A lightning-fast search engine that fits effortlessly into your apps, websites, and workflow 🔍</p>
<p dir="auto"><a href="https://www.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=intro" rel="nofollow">Meilisearch</a> helps you shape a delightful search experience in a snap, offering features that work out of the box to speed up your workflow.</p>
<p name="user-content-demo" dir="auto">
  <a href="https://where2watch.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demo-gif#gh-light-mode-only" rel="nofollow">
    <img src="https://github.com/meilisearch/meilisearch/raw/main/assets/demo-light.gif#gh-light-mode-only" alt="A bright colored application for finding movies screening near the user" data-animated-image="">
  </a>
  <a href="https://where2watch.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demo-gif#gh-dark-mode-only" rel="nofollow">
    <img src="https://github.com/meilisearch/meilisearch/raw/main/assets/demo-dark.gif#gh-dark-mode-only" alt="A dark colored application for finding movies screening near the user" data-animated-image="">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🖥 Examples</h2><a id="user-content--examples" aria-label="Permalink: 🖥 Examples" href="#-examples"></a></p>
<ul dir="auto">
<li><a href="https://where2watch.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=organization" rel="nofollow"><strong>Movies</strong></a> — An application to help you find streaming platforms to watch movies using <a href="https://www.meilisearch.com/solutions/hybrid-search?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos" rel="nofollow">hybrid search</a>.</li>
<li><a href="https://ecommerce.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos" rel="nofollow"><strong>Ecommerce</strong></a> — Ecommerce website using disjunctive <a href="https://www.meilisearch.com/docs/learn/fine_tuning_results/faceted_search?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos" rel="nofollow">facets</a>, range and rating filtering, and pagination.</li>
<li><a href="https://music.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos" rel="nofollow"><strong>Songs</strong></a> — Search through 47 million of songs.</li>
<li><a href="https://saas.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos" rel="nofollow"><strong>SaaS</strong></a> —&nbsp;Search for contacts, deals, and companies in this <a href="https://www.meilisearch.com/docs/learn/security/multitenancy_tenant_tokens?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=demos" rel="nofollow">multi-tenant</a> CRM application.</li>
</ul>
<p dir="auto">See the list of all our example apps in our <a href="https://github.com/meilisearch/demos">demos repository</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Features</h2><a id="user-content--features" aria-label="Permalink: ✨ Features" href="#-features"></a></p>
<ul dir="auto">
<li><strong>Hybrid search:</strong> Combine the best of both <a href="https://www.meilisearch.com/docs/learn/experimental/vector_search?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">semantic</a> &amp; full-text search to get the most relevant results</li>
<li><strong>Search-as-you-type:</strong> Find &amp; display results in less than 50 milliseconds to provide an intuitive experience</li>
<li><strong><a href="https://www.meilisearch.com/docs/learn/relevancy/typo_tolerance_settings?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">Typo tolerance</a>:</strong> get relevant matches even when queries contain typos and misspellings</li>
<li><strong><a href="https://www.meilisearch.com/docs/learn/fine_tuning_results/filtering?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">Filtering</a> and <a href="https://www.meilisearch.com/docs/learn/fine_tuning_results/faceted_search?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">faceted search</a>:</strong> enhance your users' search experience with custom filters and build a faceted search interface in a few lines of code</li>
<li><strong><a href="https://www.meilisearch.com/docs/learn/fine_tuning_results/sorting?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">Sorting</a>:</strong> sort results based on price, date, or pretty much anything else your users need</li>
<li><strong><a href="https://www.meilisearch.com/docs/learn/relevancy/synonyms?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">Synonym support</a>:</strong> configure synonyms to include more relevant content in your search results</li>
<li><strong><a href="https://www.meilisearch.com/docs/learn/fine_tuning_results/geosearch?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">Geosearch</a>:</strong> filter and sort documents based on geographic data</li>
<li><strong><a href="https://www.meilisearch.com/docs/learn/what_is_meilisearch/language?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">Extensive language support</a>:</strong> search datasets in any language, with optimized support for Chinese, Japanese, Hebrew, and languages using the Latin alphabet</li>
<li><strong><a href="https://www.meilisearch.com/docs/learn/security/master_api_keys?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">Security management</a>:</strong> control which users can access what data with API keys that allow fine-grained permissions handling</li>
<li><strong><a href="https://www.meilisearch.com/docs/learn/security/multitenancy_tenant_tokens?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">Multi-Tenancy</a>:</strong> personalize search results for any number of application tenants</li>
<li><strong>Highly Customizable:</strong> customize Meilisearch to your specific needs or use our out-of-the-box and hassle-free presets</li>
<li><strong><a href="https://www.meilisearch.com/docs/reference/api/overview?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=features" rel="nofollow">RESTful API</a>:</strong> integrate Meilisearch in your technical stack with our plugins and SDKs</li>
<li><strong>AI-ready:</strong> works out of the box with <a href="https://www.meilisearch.com/with/langchain" rel="nofollow">langchain</a> and the <a href="https://github.com/meilisearch/meilisearch-mcp">model context protocol</a></li>
<li><strong>Easy to install, deploy, and maintain</strong></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📖 Documentation</h2><a id="user-content--documentation" aria-label="Permalink: 📖 Documentation" href="#-documentation"></a></p>
<p dir="auto">You can consult Meilisearch's documentation at <a href="https://www.meilisearch.com/docs/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=docs" rel="nofollow">meilisearch.com/docs</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Getting started</h2><a id="user-content--getting-started" aria-label="Permalink: 🚀 Getting started" href="#-getting-started"></a></p>
<p dir="auto">For basic instructions on how to set up Meilisearch, add documents to an index, and search for documents, take a look at our <a href="https://www.meilisearch.com/docs?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=get-started" rel="nofollow">documentation</a> guide.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🌍 Supercharge your Meilisearch experience</h2><a id="user-content--supercharge-your-meilisearch-experience" aria-label="Permalink: 🌍 Supercharge your Meilisearch experience" href="#-supercharge-your-meilisearch-experience"></a></p>
<p dir="auto">Say goodbye to server deployment and manual updates with <a href="https://www.meilisearch.com/cloud?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch" rel="nofollow">Meilisearch Cloud</a>. Additional features include analytics &amp; monitoring in many regions around the world. No credit card is required.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🧰 SDKs &amp; integration tools</h2><a id="user-content--sdks--integration-tools" aria-label="Permalink: 🧰 SDKs &amp; integration tools" href="#-sdks--integration-tools"></a></p>
<p dir="auto">Install one of our SDKs in your project for seamless integration between Meilisearch and your favorite language or framework!</p>
<p dir="auto">Take a look at the complete <a href="https://www.meilisearch.com/docs/learn/what_is_meilisearch/sdks?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=sdks-link" rel="nofollow">Meilisearch integration list</a>.</p>
<p dir="auto"><a href="https://www.meilisearch.com/docs/learn/what_is_meilisearch/sdks?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=sdks-logos" rel="nofollow"><img src="https://github.com/meilisearch/meilisearch/raw/main/assets/integrations.png" alt="Logos belonging to different languages and frameworks supported by Meilisearch, including React, Ruby on Rails, Go, Rust, and PHP"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚙️ Advanced usage</h2><a id="user-content-️-advanced-usage" aria-label="Permalink: ⚙️ Advanced usage" href="#️-advanced-usage"></a></p>
<p dir="auto">Experienced users will want to keep our <a href="https://www.meilisearch.com/docs/reference/api/overview?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced" rel="nofollow">API Reference</a> close at hand.</p>
<p dir="auto">We also offer a wide range of dedicated guides to all Meilisearch features, such as <a href="https://www.meilisearch.com/docs/learn/fine_tuning_results/filtering?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced" rel="nofollow">filtering</a>, <a href="https://www.meilisearch.com/docs/learn/fine_tuning_results/sorting?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced" rel="nofollow">sorting</a>, <a href="https://www.meilisearch.com/docs/learn/fine_tuning_results/geosearch?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced" rel="nofollow">geosearch</a>, <a href="https://www.meilisearch.com/docs/learn/security/master_api_keys?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced" rel="nofollow">API keys</a>, and <a href="https://www.meilisearch.com/docs/learn/security/tenant_tokens?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced" rel="nofollow">tenant tokens</a>.</p>
<p dir="auto">Finally, for more in-depth information, refer to our articles explaining fundamental Meilisearch concepts such as <a href="https://www.meilisearch.com/docs/learn/core_concepts/documents?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced" rel="nofollow">documents</a> and <a href="https://www.meilisearch.com/docs/learn/core_concepts/indexes?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=advanced" rel="nofollow">indexes</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📊 Telemetry</h2><a id="user-content--telemetry" aria-label="Permalink: 📊 Telemetry" href="#-telemetry"></a></p>
<p dir="auto">Meilisearch collects <strong>anonymized</strong> user data to help us improve our product. You can <a href="https://www.meilisearch.com/docs/learn/what_is_meilisearch/telemetry?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=telemetry#how-to-disable-data-collection" rel="nofollow">deactivate this</a> whenever you want.</p>
<p dir="auto">To request deletion of collected data, please write to us at <a href="mailto:privacy@meilisearch.com">privacy@meilisearch.com</a>. Remember to include your <code>Instance UID</code> in the message, as this helps us quickly find and delete your data.</p>
<p dir="auto">If you want to know more about the kind of data we collect and what we use it for, check the <a href="https://www.meilisearch.com/docs/learn/what_is_meilisearch/telemetry?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=telemetry#how-to-disable-data-collection" rel="nofollow">telemetry section</a> of our documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📫 Get in touch!</h2><a id="user-content--get-in-touch" aria-label="Permalink: 📫 Get in touch!" href="#-get-in-touch"></a></p>
<p dir="auto">Meilisearch is a search engine created by Meili, a software development company headquartered in France and with team members all over the world. Want to know more about us? <a href="https://blog.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=contact" rel="nofollow">Check out our blog!</a></p>
<p dir="auto">🗞 <a href="https://meilisearch.us2.list-manage.com/subscribe?u=27870f7b71c908a8b359599fb&amp;id=79582d828e" rel="nofollow">Subscribe to our newsletter</a> if you don't want to miss any updates! We promise we won't clutter your mailbox: we only send one edition every two months.</p>
<p dir="auto">💌 Want to make a suggestion or give feedback? Here are some of the channels where you can reach us:</p>
<ul dir="auto">
<li>For feature requests, please visit our <a href="https://github.com/meilisearch/product/discussions">product repository</a></li>
<li>Found a bug? Open an <a href="https://github.com/meilisearch/meilisearch/issues">issue</a>!</li>
<li>Want to be part of our Discord community? <a href="https://discord.meilisearch.com/?utm_campaign=oss&amp;utm_source=github&amp;utm_medium=meilisearch&amp;utm_content=contact" rel="nofollow">Join us!</a></li>
</ul>
<p dir="auto">Thank you for your support!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">👩‍💻 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 👩‍💻 Contributing" href="#-contributing"></a></p>
<p dir="auto">Meilisearch is, and will always be, open-source! If you want to contribute to the project, please look at <a href="https://github.com/meilisearch/meilisearch/blob/main/CONTRIBUTING.md">our contribution guidelines</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 Versioning</h2><a id="user-content--versioning" aria-label="Permalink: 📦 Versioning" href="#-versioning"></a></p>
<p dir="auto">Meilisearch releases and their associated binaries are available on the project's <a href="https://github.com/meilisearch/meilisearch/releases">releases page</a>.</p>
<p dir="auto">The binaries are versioned following <a href="https://semver.org/" rel="nofollow">SemVer conventions</a>. To know more, read our <a href="https://github.com/meilisearch/engine-team/blob/main/resources/versioning-policy.md">versioning policy</a>.</p>
<p dir="auto">Differently from the binaries, crates in this repository are not currently available on <a href="https://crates.io/" rel="nofollow">crates.io</a> and do not follow <a href="https://semver.org/" rel="nofollow">SemVer conventions</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Omnom: Self-hosted bookmarking with searchable, wysiwyg snapshots [showcase] (146 pts)]]></title>
            <link>https://omnom.zone/?src=hn</link>
            <guid>43680232</guid>
            <pubDate>Mon, 14 Apr 2025 11:42:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://omnom.zone/?src=hn">https://omnom.zone/?src=hn</a>, See on <a href="https://news.ycombinator.com/item?id=43680232">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
  <p>Warning</p>
  <p>This is a read-only demo instance - check out our <a href="https://github.com/asciimoo/omnom">GitHub</a> for more details</p>
</article>

            <h3>Download extension</h3>
            <p>
                Browser extensions are required to create bookmarks &amp; snapshots. Install the extension to your browser and enjoy Omnoming.
            </p>
            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacktical C: practical hacker's guide to the C programming language (126 pts)]]></title>
            <link>https://github.com/codr7/hacktical-c</link>
            <guid>43679781</guid>
            <pubDate>Mon, 14 Apr 2025 10:20:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/codr7/hacktical-c">https://github.com/codr7/hacktical-c</a>, See on <a href="https://news.ycombinator.com/item?id=43679781">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Hacktical C</h2><a id="user-content-hacktical-c" aria-label="Permalink: Hacktical C" href="#hacktical-c"></a></p>
<p dir="auto">A practical hacker's guide to the C programming language.</p>
<p dir="auto"><em>In memory of <a href="https://en.wikipedia.org/wiki/Dennis_Ritchie" rel="nofollow">Dennis Ritchie</a>,
one of the greatest hackers this world has known.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About the book</h2><a id="user-content-about-the-book" aria-label="Permalink: About the book" href="#about-the-book"></a></p>
<p dir="auto">This book assumes basic programming knowledge. We're not going to spend a lot of time and space on explaining basic features, except where they behave differently in important ways compared to other mainstream languages. Instead we're going to focus on practical techniques for making the most out of the power and flexibility C offers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">About the author</h2><a id="user-content-about-the-author" aria-label="Permalink: About the author" href="#about-the-author"></a></p>
<p dir="auto">You could say that there are two kinds of programmers, with very different motivations; academics and hackers. I've always identified as a hacker. I like solving tricky problems, and I prefer using powerful tools that don't get in my way. To me; software is all about practical application, about making a change in the real world.</p>
<p dir="auto">I've been writing code for fun on a mostly daily basis since I got a Commodore 64 for Christmas in 1985, professionally in different roles/companies since 1998.</p>
<p dir="auto">I started out with Basic on the Commodore 64, went on to learn Assembler on an Amiga 500, Pascal on PC; C++, Modula-3, Prolog, Ruby, Python, Perl, JavaScript, Common Lisp, Forth, Haskell, SmallTalk, Go, Swift.</p>
<p dir="auto">For a long time, I didn't care much about C at all, it felt very primitive compared to other languages. But gradually over time, I learned that the worst enemy in software is complexity, and started taking C more seriously.</p>
<p dir="auto">Since then I've written a ton of C; and along the way I've picked up many interesting, lesser known techniques that helped me make the most out of the language and appreciate it for its strengths.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donations</h2><a id="user-content-donations" aria-label="Permalink: Donations" href="#donations"></a></p>
<p dir="auto">If you would like to see this project succeed, all contributions are welcome.</p>
<p dir="auto">I've decided to release the project using an open license to benefit as many as possible, because I believe knowledge should be shared freely. But I also believe in compensation for creators; and the less economic pressure I have to deal with, the more time and energy I can put into the project.</p>
<p dir="auto">The repository is set up for sponsoring via Stripe and Liberapay, alternatively you may use BTC (bitcoin:18k7kMcvPSSSzQtJ6hY5xxCt5U5p45rbuh) or ETH (0x776001F33F6Fc07ce9FF70187D5c034DCb429811).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why C?</h2><a id="user-content-why-c" aria-label="Permalink: Why C?" href="#why-c"></a></p>
<p dir="auto">The reason I believe C is and always will be important is that it stands in a class of its own as a mostly portable assembler language, offering similar levels of freedom.</p>
<p dir="auto">C doesn't try very hard to prevent you from making mistakes. It has very few opinions about your code and happily assumes that you know exactly what you're doing. Freedom with responsibility.</p>
<p dir="auto">These days; many programmers will recommend choosing a stricter language, regardless of the problem being solved. Most of those programmers wouldn't trust themselves with the kind of freedom C offers, many haven't even bothered to learn the language properly.</p>
<p dir="auto">Since most of the foundation of the digital revolution, including the Internet was built using C; it gets the blame for many problems that are more due to our immaturity in designing and building complicated software than about programming languages.</p>
<p dir="auto">The truth is that any reasonably complicated software system created by humans will have bugs, regardless of what technology was used to create it. Using a stricter language helps with reducing some classes of bugs, at the cost of reduced flexibility in expressing a solution and increased effort creating the software.</p>
<p dir="auto">Programmers like to say that you should pick 'the right tool for the job'; what many fail to grasp is that the only people who have the capability to decide which tools are right, are the people creating the software. Much effort has been wasted on arguing and bullying programmers into picking tools other people prefer.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">The makefile requires <code>gcc</code>, <code>ccache</code> and <code>valgrind</code> to do its thing.</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/codr7/hacktical-c.git
cd hacktical-c
mkdir build
make"><pre><code>git clone https://github.com/codr7/hacktical-c.git
cd hacktical-c
mkdir build
make
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Platforms</h2><a id="user-content-platforms" aria-label="Permalink: Platforms" href="#platforms"></a></p>
<p dir="auto">Since Unix is all about C, and Linux is currently the best supported Unix out there; Linux is the platform I would recommend for writing C. Just having access to <code>valgrind</code> is priceless. Microsoft has unfortunately chosen to neglect C for a long time, its compilers dragging far behind the rest of the pack. Windows does however offer a way of running Linux in the form of WSL2, which works very well from my experience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extensions</h2><a id="user-content-extensions" aria-label="Permalink: Extensions" href="#extensions"></a></p>
<p dir="auto">The code in this book uses several GNU extensions that are not yet in the C standard. Cleanup attributes, multi-line expressions and nested functions specifically.</p>
<p dir="auto">Some developers avoid extensions like the plague, some are happy to use them for everything and anything. I fall somewhere in the middle of the spectrum; comfortable with using extensions when there are no good standard alternatives, especially if they're supported by both <code>gcc</code> and <code>clang</code>. All of the extensions used in this book except nested functions (which is currently only supported by <code>gcc</code>) fall in that category.</p>
<p dir="auto">I can think of one feature, <code>hc_defer()</code>, which would currently be absolutely impossible to do without extensions. In other cases, alternative solutions are simply less convenient.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks</h2><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">Some chapters come with benchmarks, <code>make build/benchmark</code> builds and runs all of them.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chapters</h2><a id="user-content-chapters" aria-label="Permalink: Chapters" href="#chapters"></a></p>
<p dir="auto">The content is arranged to form a natural progression, where later chapters build on concepts that have already been introduced. That being said; feel free to skip around, just be prepared to backtrack to fill in blanks.</p>
<ul dir="auto">
<li><a href="https://github.com/codr7/hacktical-c/tree/main/macro">Macros</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/fix">Fixed-Point Arithmetic</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/list">Intrusive Doubly Linked Lists</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/task">Lightweight Concurrent Tasks</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/malloc1">Composable Memory Allocators - Part 1</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/vector">Vectors</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/error">Exceptions</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/set">Ordered Sets and Maps</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/malloc2">Composable Memory Allocators - Part 2</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/dynamic">Dynamic Compilation</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/stream1">Extensible Streams - Part 1</a></li>
<li><a href="https://github.com/codr7/hacktical-c/tree/main/slog">Structured Logs</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig's new LinkedList API (it's time to learn fieldParentPtr) (168 pts)]]></title>
            <link>https://www.openmymind.net/Zigs-New-LinkedList-API/</link>
            <guid>43679707</guid>
            <pubDate>Mon, 14 Apr 2025 10:06:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openmymind.net/Zigs-New-LinkedList-API/">https://www.openmymind.net/Zigs-New-LinkedList-API/</a>, See on <a href="https://news.ycombinator.com/item?id=43679707">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  
  
<p>In a recent, post-Zig 0.14 commit, Zig's <code>SinglyLinkedList</code> and <code>DoublyLinkedList</code> saw <a href="https://github.com/ziglang/zig/commit/1639fcea43549853f1fded32aa1d711d21771e1c">significant changes</a>.</p>

<p>The previous version was a generic and, with all the methods removed, looked like:</p>

<pre><code><span>pub</span> <span>fn</span> <span>SinglyLinkedList</span><span>(</span><span>comptime</span> T<span>:</span> <span><span>type</span></span><span>)</span> <span><span>type</span></span> <span>{</span>
  <span>return</span> <span>struct</span> <span>{</span>
    first<span>:</span> <span><span>?</span><span>*</span>Node</span> <span>=</span> <span>null</span><span>,</span>

    <span>pub</span> <span>const</span> <span>Node</span> <span>=</span> <span>struct</span> <span>{</span>
      next<span>:</span> <span><span>?</span><span>*</span>Node</span> <span>=</span> <span>null</span><span>,</span>
      data<span>:</span> <span>T</span><span>,</span>
    <span>}</span><span>;</span>
  <span>}</span><span>;</span>
<span>}</span></code></pre>

<p>The new version isn't generic. Rather, you embed the linked list node with your data. This is known as an intrusive linked list and tends to perform better and require fewer allocations. Except in trivial examples, the data that we store in a linked list is typically stored on the heap. Because an intrusive linked list has the linked list node embedded in the data, it doesn't need its own allocation. Before we jump into an example, this is what the new structure looks like, again, with all methods removed:</p>

<pre><code><span>pub</span> <span>const</span> <span>SinglyLinkedList</span> <span>=</span> <span>struct</span> <span>{</span>
  first<span>:</span> <span><span>?</span><span>*</span>Node</span> <span>=</span> <span>null</span><span>,</span>

  <span>pub</span> <span>const</span> <span>Node</span> <span>=</span> <span>struct</span> <span>{</span>
    next<span>:</span> <span><span>?</span><span>*</span>Node</span> <span>=</span> <span>null</span><span>,</span>
  <span>}</span><span>;</span>
<span>}</span><span>;</span></code></pre>

<p>Much simpler, and, notice that this has no link or reference to any of our data. Here's a working example that shows how you'd use it:</p>

<pre><code><span>const</span> std <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>)</span><span>;</span>
<span>const</span> SinglyLinkedList <span>=</span> std<span>.</span>SinglyLinkedList<span>;</span>

<span>pub</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span><span>!</span><span>void</span></span> <span>{</span>
    <span>// GeneralPurposeAllocator is being renamed</span>
    <span>// to DebugAllocator. Let's get used to that name</span>
    <span>var</span> gpa<span>:</span> std<span>.</span>heap<span>.</span><span>DebugAllocator</span><span>(</span><span>.</span><span>{</span><span>}</span><span>)</span> <span>=</span> <span>.</span>init<span>;</span>
    <span>const</span> allocator <span>=</span> gpa<span>.</span><span>allocator</span><span>(</span><span>)</span><span>;</span>

    <span>var</span> list<span>:</span> <span>SinglyLinkedList</span> <span>=</span> <span>.</span><span>{</span><span>}</span><span>;</span>

    <span>const</span> user1 <span>=</span> <span>try</span> allocator<span>.</span><span>create</span><span>(</span>User<span>)</span><span>;</span>
    <span>defer</span> allocator<span>.</span><span>destroy</span><span>(</span>user1<span>)</span><span>;</span>
    user1<span>.*</span> <span>=</span> <span>.</span><span>{</span>
        <span>.</span>id <span>=</span> <span>1</span><span>,</span>
        <span>.</span>power <span>=</span> <span>9000</span><span>,</span>
        <span>.</span>node <span>=</span> <span>.</span><span>{</span><span>}</span><span>,</span>
    <span>}</span><span>;</span>
    list<span>.</span><span>prepend</span><span>(</span><span>&amp;</span>user1<span>.</span>node<span>)</span><span>;</span>

    <span>const</span> user2 <span>=</span> <span>try</span> allocator<span>.</span><span>create</span><span>(</span>User<span>)</span><span>;</span>
    <span>defer</span> allocator<span>.</span><span>destroy</span><span>(</span>user2<span>)</span><span>;</span>
    user2<span>.*</span> <span>=</span> <span>.</span><span>{</span>
        <span>.</span>id <span>=</span> <span>2</span><span>,</span>
        <span>.</span>power <span>=</span> <span>9001</span><span>,</span>
        <span>.</span>node <span>=</span> <span>.</span><span>{</span><span>}</span><span>,</span>
    <span>}</span><span>;</span>
    list<span>.</span><span>prepend</span><span>(</span><span>&amp;</span>user2<span>.</span>node<span>)</span><span>;</span>

    <span>var</span> node <span>=</span> list<span>.</span>first<span>;</span>
    <span>while</span> <span>(</span>node<span>)</span> <span>|</span>n<span>|</span> <span>{</span>
        std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"{any}\n"</span><span>,</span> <span>.</span><span>{</span>n<span>}</span><span>)</span><span>;</span>
        node <span>=</span> n<span>.</span>next<span>;</span>
    <span>}</span>
<span>}</span>

<span>const</span> <span>User</span> <span>=</span> <span>struct</span> <span>{</span>
    id<span>:</span> <span><span>i64</span></span><span>,</span>
    power<span>:</span> <span><span>u32</span></span><span>,</span>
    node<span>:</span> <span>SinglyLinkedList<span>.</span>Node</span><span>,</span>
<span>}</span><span>;</span></code></pre>

<p>To run this code, you'll need a nightly release from within the last week. What do you think the output will be? You should see something like:</p>

<pre><code>SinglyLinkedList.Node{ .next = SinglyLinkedList.Node{ .next = null } }
SinglyLinkedList.Node{ .next = null }</code></pre>

<p>We're only getting the nodes, and, as we can see here and from the above skeleton structure of the new <code>SinglyLinkedList</code>, there's nothing about our users. Users have nodes, but there's seemingly nothing that links a node back to its containing user. Or is there?</p>

<p>In the past, we've described how <a href="https://www.openmymind.net/learning_zig/pointers/">the compiler uses the type information</a> to figure out how to access fields. For example, when we execute <code>user1.power</code>, the compiler knows that:</p>

<ol>
  <li><code>id</code> is +0 bytes from the start of the structure,
  </li><li><code>power</code> is +8 bytes from the start of the structure (because id is an i64), and
  </li><li><code>power</code> is an i32
</li></ol>

<p>With this information, the compiler knows how to access <code>power</code> from <code>user1</code> (i.e. jump forward 8 bytes, read 4 bytes and treat it as an i32). But if you think about it, that logic is simple to reverse. If we know the address of <code>power</code>, then the address of <code>user</code> has to be <code>address_of_power - 8</code>. We can prove this:</p>

<pre><code><span>const</span> std <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>)</span><span>;</span>

<span>pub</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> <span><span>!</span><span>void</span></span> <span>{</span>
    <span>var</span> user <span>=</span> <span>User</span><span>{</span>
        <span>.</span>id <span>=</span> <span>1</span><span>,</span>
        <span>.</span>power <span>=</span> <span>9000</span><span>,</span>
    <span>}</span><span>;</span>
    std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"address of user: {*}\n"</span><span>,</span> <span>.</span><span>{</span><span>&amp;</span>user<span>}</span><span>)</span><span>;</span>

    <span>const</span> address_of_power <span>=</span> <span>&amp;</span>user<span>.</span>power<span>;</span>
    std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"address of power: {*}\n"</span><span>,</span> <span>.</span><span>{</span>address_of_power<span>}</span><span>)</span><span>;</span>

    <span>const</span> power_offset <span>=</span> <span>8</span><span>;</span>
    <span>const</span> also_user<span>:</span> <span><span>*</span>User</span> <span>=</span> <span>@ptrFromInt</span><span>(</span><span>@intFromPtr</span><span>(</span>address_of_power<span>)</span> <span>-</span> power_offset<span>)</span><span>;</span>
    std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"address of also_user: {*}\n"</span><span>,</span> <span>.</span><span>{</span>also_user<span>}</span><span>)</span><span>;</span>

    std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"also_user: {}\n"</span><span>,</span> <span>.</span><span>{</span>also_user<span>}</span><span>)</span><span>;</span>
<span>}</span>

<span>const</span> <span>User</span> <span>=</span> <span>struct</span> <span>{</span>
    id<span>:</span> <span><span>i64</span></span><span>,</span>
    power<span>:</span> <span><span>u32</span></span><span>,</span>
<span>}</span><span>;</span></code></pre>

<p>The magic happens here:</p>

<pre><code><span>const</span> power_offset <span>=</span> <span>8</span><span>;</span>
<span>const</span> also_user<span>:</span> <span><span>*</span>User</span> <span>=</span> <span>@ptrFromInt</span><span>(</span><span>@intFromPtr</span><span>(</span>address_of_power<span>)</span> <span>-</span> power_offset<span>)</span><span>;</span></code></pre>

<p>We're turning the address of our user's power field, <code>&amp;user.power</code> into an integer, subtracting 8 (8 bytes, 64 bits), and telling the compiler that it should treat that memory as a <code>*User</code>. This code will <em>probably</em> work for you, but it isn't safe. Specifically, unless we're using a packed or extern struct, Zig makes no guarantees about the layout of a structure. It could put <code>power</code> BEFORE <code>id</code>, in which case our <code>power_offset</code> should be 0. It could add padding after every field. It can do anything it wants. To make this code safer, we use the <code>@offsetOf</code> builtin to get the actual byte-offset of a field with respect to its struct:</p>

<pre><code><span>const</span> power_offset <span>=</span> <span>@offsetOf</span><span>(</span>User<span>,</span> <span>"power"</span><span>)</span><span>;</span></code></pre>

<p>Back to our linked list, given that we have the address of a <code>node</code> and we know that it is part of the <code>User</code> structure, we <em>are</em> able to get the <code>User</code> from a node. Rather than use the above code though, we'll use the <em>slightly</em> friendlier <code>@fieldParentPtr</code> builtin. Our <code>while</code> loop changes to:</p>

<pre><code><span>while</span> <span>(</span>node<span>)</span> <span>|</span>n<span>|</span> <span>{</span>
  <span>const</span> user<span>:</span> <span><span>*</span>User</span> <span>=</span> <span>@fieldParentPtr</span><span>(</span><span>"node"</span><span>,</span> n<span>)</span><span>;</span>
  std<span>.</span>debug<span>.</span><span>print</span><span>(</span><span>"{any}\n"</span><span>,</span> <span>.</span><span>{</span>user<span>}</span><span>)</span><span>;</span>
  node <span>=</span> n<span>.</span>next<span>;</span>
<span>}</span></code></pre>

<p>We give <code>@fieldParentPtr</code> the name of the field, a pointer to that field as well as a return type (which is inferred above by the assignment to a <code>*User</code> variable), and it gives us back the instance that contains that field.</p>

<p>Performance aside, I have mixed feelings about the new API. My initial reaction is that I dislike exposing, what I consider, a complicated builtin like <code>@fieldParentPtr</code> for something as trivial as using a linked list. However, while <code>@fieldParentPtr</code> seems esoteric, it's quite useful and developers should be familiar with it because it can help solve problems which are otherwise problematic.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kezurou-Kai #39 (261 pts)]]></title>
            <link>https://www.bigsandwoodworking.com/kezurou-kai-39/</link>
            <guid>43679004</guid>
            <pubDate>Mon, 14 Apr 2025 07:47:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bigsandwoodworking.com/kezurou-kai-39/">https://www.bigsandwoodworking.com/kezurou-kai-39/</a>, See on <a href="https://news.ycombinator.com/item?id=43679004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Last weekend I went to the 39th annual Kezurou-kai event in Itoigawa, Niigata.  It was my first time going to the event here in Japan, and it was such a blast.  For those who are unfamiliar with kezurou-kai, it’s an event where people compete to take the thinnest shavings of wood using Japanese planes.  But more than that it’s really a gathering of people who are passionate about woodworking and carpentry, sharpening and hand tools, who are pushing their skills to the absolute limits of what is possible.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0301.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0301.jpg?resize=1024%2C683&amp;ssl=1" alt="70mm kanna kezuroukai preliminary" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0301.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0301.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0301.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0301.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0301.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0301.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>The event takes place over two days, with preliminary planing running all through the first day, and ending around mid-day on day 2.  Throughout that time competitors have three chances each day to bring a plane shaving up for official measurement.  5 individuals with the thinnest shavings then go on to the final planing contest toward the end of the day on day 2.  </p><p>The main contest required using 70 mm kanna, and the material was limited to hinoki at 55 mm wide by 1800 mm long.   Hinoki has become the standard wood for thin planing, since it cuts beautifully and can be planed down to an extreme level without breaking up.  For preliminary planing each competitor or group was required to bring their own material for planing.  The final contest however involved planing material selected by the event organizers, with the final 5 competitors all planing the same board.  </p><p>The event took place in a gymnasium which was filled with planing benches shared by teams and individuals.  When I arrived on day 1 I met up with my friends from Somakosha and we pretty much started taking shavings right away.  Here’s Yamamoto-san getting things started.</p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0304.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0304.jpg?resize=1024%2C683&amp;ssl=1" alt="Team Somakosha taking thin shavings at Kezuroukai" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0304.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0304.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0304.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0304.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0304.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0304.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>We all came with a few different planes, and myself I brought 2 kanna, an old Ishido blue steel blade and another from an unknown maker which I’m pretty confident is some type of white steel.  We also had a Mitutoyo digital micrometer for measuring our shavings.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0436.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0436.jpg?resize=1024%2C683&amp;ssl=1" alt="Ishido and Unknown kanna used at kezuroukai" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0436.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0436.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0436.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0436.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0436.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0436.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0438.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0438.jpg?resize=1024%2C683&amp;ssl=1" alt="Mitutoyo digital micrometee" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0438.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0438.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0438.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0438.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0438.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0438.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Given than none of us had been doing any kind of practice our shavings on day one were pretty decent.  We were all able to take really clean and consistent shavings in the 10-12 micron range without too much trouble.  It was getting under 10 microns that was the real challenge.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0302.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0302.jpg?resize=1024%2C683&amp;ssl=1" alt="70mm kanna taking thin shaving of hinoki" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0302.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0302.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0302.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0302.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0302.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0302.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>This is something that I’ve faced before when having “kezurou-kai nights” with friends.  With careful sharpening and tuning of the dai, it’s fairly straightforward to get really clean consistent shavings in the 10-15 micron range.  But pushing past 10 microns requires a whole other level a fastidiousness when it comes to every aspect of planing.  In any case, on that first day at Kezuroukai we struggled a bit, but we kept sharpening and adjusting out planes trying to break the sub-10 micron barrier.  </p><p>Once you had a good shaving you could take it up for official measurement.  The shaving needed to be full length and free of tears, splits, etc.  Simple jigs were provided which allowed you to clamp a 1 meter section of the shaving for the purpose of bringing it up for official measurement.  Here’s a line of people waiting to get their shavings measured on day 1.  You can see everyone holding a the jig with their shavings clamped.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0309.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0309.jpg?resize=1024%2C683&amp;ssl=1" alt="waiting to get plane shaving thickness measured" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0309.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0309.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0309.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0309.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0309.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0309.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>And here is the official measuring device; three digital calipers which were pneumatically controlled to measure each shaving with a consistent pressure.  When you brought your shaving up, you had to carefully set it below the calipers, and when everything was set the operator would push a button and all three calipers simultaneously plunged down.  The calipers were offset along the length of the shaving, but also across the width, giving measurements which revealed the overall consistency.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0331.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0331.jpg?resize=1024%2C683&amp;ssl=1" alt="" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0331.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0331.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0331.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0331.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0331.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0331.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>If the measurement was satisfactory you could then take it over and paste it on the boards seen below.  Shavings on the far right were all 5 microns and less.  The other two boards were for the remainder of the shavings, most of which were between 6-12 microns.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0325-1.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0325-1.jpg?resize=1024%2C683&amp;ssl=1" alt="kezuroukai shaving board on day 1" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0325-1.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0325-1.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0325-1.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0325-1.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0325-1.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0325-1.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Outside the venue was a space setup for sharpening.  There was a good mix of people using synthetic and natural stones.  I personally stuck with a variation on my usual routine, 1000 grit Hibiki, an 8000 King or 8000 Hibiki, and a 12000 grit Kagayaki stone, doing a micro-bevel on the 8000 and 12000 stones.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0333.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0333.jpg?resize=1024%2C683&amp;ssl=1" alt="kezuroukai sharpening area" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0333.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0333.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0333.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0333.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0333.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0333.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0335.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0335.jpg?resize=1024%2C683&amp;ssl=1" alt="sharpening kanna at kezuroukai" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0335.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0335.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0335.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0335.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0335.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0335.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Day 1 went fast.  I planed a lot but I also spent a fair amount of time catching up with old friends.  In terms of shaving I wasn’t able to break through the 10 micron barrier with a consistent shaving.  It’s easy enough to have parts of a shaving break below that barrier, but getting a consistent shaving for the full length and width of the board is really difficult.  On one hand it’s frustrating but it’s also becomes an interesting puzzle figuring out how to improve things.  At the Izakaya that night pretty much all we talked about was sharpening and how to improve our results.</p><hr><p>Day 2 was a fair amount busier, with more people showing up to plane.  All of us from team Somakosha experimented with some different sharpening techniques to see if we could get thinner shavings.  Some things seemed to work better than others, but more than our sharpening technique or dai adjustments, it became clear that our material was a big limiting factor.  As you approach ultra thin sub-10 micron shavings the quality of the material becomes a huge factor in how thin you can go.  The evenness and density of the grain, and especially the moisture content of the wood are really important factors.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0342.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0342.jpg?resize=1024%2C683&amp;ssl=1" alt="kezuroukai day 2 planing" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0342.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0342.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0342.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0342.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0342.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0342.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Overall we had really nice material, with nice even straight grain, but it was definitely on the drier side.  It was really interesting to see how much other competitors cared for and maintained their material.  Most people had their planing blanks wrapped in plastic to prevent moisture loss, and many went to great lengths to protect the wood when not planing by protecting it with blankets or foam packing.  </p><p>The two guys who we shared a bench with were Kezurou-kai veterans, having started some 20 years ago, and they had 2 planing beams that they were rotating in and out as they planed.  Whenever they set aside a board they would cover it will moist towels to maintain a high moisture content in the wood.  In another case Yamamoto-san went over to a friend’s bench and was able to take some shavings from their hinoki which was definitely higher quality and well maintained.  He had been pulling shavings in the 10-12 micron range on our board, but taking the same plane, without resharpening to the his friend’s higher quality board, he was able to plane down to 6 microns.  Pretty amazing how much of a difference the quality of material and moisture content makes.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0311.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0311.jpg?resize=1024%2C683&amp;ssl=1" alt="hinoki wrapped and protected in preparation for planinng" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0311.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0311.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0311.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0311.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0311.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0311.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>As day 2 went on you could sense the energy level rising as everyone worked to take ultra-thin shaving before time was up.  About an hour before the deadline for preliminary planing and the leaderboards really started to fill up.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0343.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0343.jpg?resize=1024%2C683&amp;ssl=1" alt="thin hinoki plane shavings entered into the competion" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0343.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0343.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0343.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0343.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0343.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0343.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0347.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0347.jpg?resize=1024%2C683&amp;ssl=1" alt="planing hinoki kezuroukai 39" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0347.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0347.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0347.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0347.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0347.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0347.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0346.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0346.jpg?resize=1024%2C683&amp;ssl=1" alt="group of people watching at kezuroukai" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0346.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0346.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0346.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0346.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0346.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0346.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0345.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0345.jpg?resize=1024%2C683&amp;ssl=1" alt="kezuroukai planing benches and venue" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0345.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0345.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0345.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0345.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0345.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0345.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><hr><p>Back at our bench we started to try every possible trick we could think of to improve our results.  What seemed to work best was simply wiping the board with a lightly damp rag prior to planing.  It would definitley be better to have the wood “pre-soaked” rather than wiping the wood before hand, since exceess moisture on the surface of the wood can cause the dai to move, but given the situation and with time running out we did what we needed to do.  And it did help, a lot.  The quality of shaving between really dry wood and moist wood is completely different.  </p><p>In the end one of my last shavings turned out to be my best.  With a freshly sharpened blade, and a touch of moisture on the wood, I was able to pull a really clean shaving.  I took it up to the judges for measurement and the results were 10, 6, and 9 microns.  I’m pretty happy with that result.  It’d be great if the whole thing came out around 6, but I’m glad to have gotten a really clean full length/width shaving at that level.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0349.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0349.jpg?resize=1024%2C683&amp;ssl=1" alt="Jon Billing hinoki shaving at kezuroukai" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0349.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0349.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0349.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0349.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0349.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0349.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Here are the top 5 winners from the preliminary contest and their numbers.  Insanity!  Crazy thin and consistent.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0350.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0350.jpg?resize=1024%2C683&amp;ssl=1" alt="thinnest shavings from preliminary competition" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0350.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0350.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0350.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0350.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0350.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0350.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>With the preliminary contest over, the top 5 went on to the final challenge which was planing a 3 meter quartersawn piece of sugi (Japanese cedar).  Compared to hinoki, sugi is not an easy wood to plane, especially thin.  This time the rules for the final round also changed, and each person had just a few minutes (I think it was 3-4) for both setting their planes and planing.  In otherwords, before the timer started your blade had to be loose in the dai.  Then once the clock started ticking you could begin setting the blade in dai and start planing.  Kind of intense given the time allotted and overall pressure of the situation.  </p><p>Here’s the first person up, taking a fairly thick shaving.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0357.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0357.jpg?resize=1024%2C683&amp;ssl=1" alt="final kezuroukai competition planing sugi" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0357.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0357.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0357.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0357.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0357.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0357.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p> With sugi theres a fine line between planing too thick and too thin.  Too thin and the shaving just falls apart.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0370.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0370.jpg?resize=1024%2C683&amp;ssl=1" alt="difficult to plane sugi from final competition" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0370.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0370.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0370.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0370.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0370.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0370.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0371.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0371.jpg?resize=1024%2C683&amp;ssl=1" alt="competitor focusing on getting a clean sugi shaving" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0371.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0371.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0371.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0371.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0371.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0371.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Each person only had one chance to have a complete shaving measured, which means you have to really gauge the material and your capabilities.  It’s all about taking the thinnest shaving you can manage and knowing when to stop.   Spend too much time trying to get a thin shaving and you risk running out of time.  But it’s also tricky to gauge the thickness of the shaving until you ask the judges to measure it.  In reality it may look thinner than it actually is.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0380.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0380.jpg?resize=1024%2C683&amp;ssl=1" alt="measuring the thickness of a sugi shaving" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0380.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0380.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0380.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0380.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0380.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0380.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0384.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0384.jpg?resize=1024%2C683&amp;ssl=1" alt="final competitor shaving sugi" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0384.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0384.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0384.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0384.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0384.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0384.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0385.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0385.jpg?resize=1024%2C683&amp;ssl=1" alt="measuring the last sugi shaving from final at kezuroukai 39" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0385.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0385.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0385.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0385.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0385.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0385.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>The winning shaving from the final round of 5 competitors was somewhere around 50 microns (it may have been 48), which just goes to show you how different sugi is from hinoki.  It also reveals how different it is to plane material that is of unknown quality versus planing your own moisture controlled material.  </p><p>I love the challenge of ultra-thin planing, and it’s fascinating to see the skill and dedication it takes to plane at a this level.  But planing in the sub-10 micron range really requires a high level of control over the material (not to mention the kanna), which as a woodworker/carpenter is pretty far from the reality of day-to-day work.  So I like the idea of a contest which requires people to plane an unknown piece of wood, which is more or less how the final competition here goes.  I’d also love to see some sort of tear-out challenge, where the goal is to plane a really gnarly piece of wood with knots or difficult grain, and try to perfect the surface.  A challenge like that would be really beneficial for folks looking to use kanna for real work.  </p><hr><p>Throughout the event I was pretty focused on visiting with friends and planing, but I did take a quick lap towards the end of day 2 to snap some photos of other some of the other things taking place.  </p><p>In one corner of the venue a craftsman was demonstrating carving a sumitsubo.  (I didn’t realize until later when I edited these photos that he also had carved wooden shoes in the foreground!)</p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0315.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0315.jpg?resize=1024%2C683&amp;ssl=1" alt="kezuroukai sumitsubo carving demonstration" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0315.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0315.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0315.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0315.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0315.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0315.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Next to him was a guy demonstrating how to cut a new kanna dai.  If you search for Kezuroukai videos you can find a good video of this same person chopping a dai at a previous event.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0316.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0316.jpg?resize=1024%2C683&amp;ssl=1" alt="kezuroukai kanna dai fabrication demonstration" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0316.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0316.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0316.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0316.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0316.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0316.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0317.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0317.jpg?resize=1024%2C683&amp;ssl=1" alt="chopping a kanna dai demo at kezuroukai" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0317.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0317.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0317.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0317.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0317.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0317.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Outside near the sharpening area were several people demonstrating hewing, and brave spectators could also give it a go with a bit of supervision.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0340.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0340.jpg?resize=1024%2C683&amp;ssl=1" alt="hewing demonstration at kezuroukai" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0340.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0340.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0340.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0340.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0340.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0340.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>Back inside the venue were also plenty of vendors selling anything and everything related to planes and handtools.  Here was one of the natural sharpening stone vendors.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0320.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0320.jpg?resize=1024%2C683&amp;ssl=1" alt="natural stone shop at kezuroukai" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0320.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0320.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0320.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0320.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0320.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0320.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>The NSK company who are making a new variety of diamond sharpening stones were also present.  They made their stones available to try for anyone who was interested.  </p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0319.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0319.jpg?resize=1024%2C683&amp;ssl=1" alt="nsk diamond stones for sale at kezuroukai 39" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0319.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0319.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0319.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0319.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0319.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0319.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><p>And of course there were plenty of kanna for sale…</p><figure><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0318.jpg?resize=1024%2C683&amp;ssl=1" data-src="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0318.jpg?resize=1024%2C683&amp;ssl=1" alt="kanna for sale at kezuroukai 39" data-srcset="https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0318.jpg?w=1024&amp;ssl=1 1024w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0318.jpg?resize=300%2C200&amp;ssl=1 300w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0318.jpg?resize=18%2C12&amp;ssl=1 18w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0318.jpg?resize=930%2C620&amp;ssl=1 930w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0318.jpg?resize=325%2C217&amp;ssl=1 325w, https://i0.wp.com/www.bigsandwoodworking.com/wp-content/uploads/2023/11/DSCF0318.jpg?resize=600%2C400&amp;ssl=1 600w" data-sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure><hr><p>There’s a lot I wasn’t able to cover but that’s the quick story behind Kezuroukai #39.  It really was a busy couple of days, and hard to take everything in.  I’d love to go back and try my hand at planing again, but I’d also love to just go as a spectator and spend more time watching.  There’s so much you can learn at Kezuroukai, and also so many really passionate and inspired people to meet.  I highly recommend a visit to anyone who can make the trip to Japan, but if not then definitely seek out a more local event or start one up!  In the US now we have Kezuroua-kai USA along with a few other kez events like Jason Fox’s Maine event.  So go, plane wood, and help spread the joy of hand tools and craft!</p></div></div>]]></description>
        </item>
    </channel>
</rss>