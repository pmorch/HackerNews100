<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 26 Jul 2023 17:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Community Note correcting Musk’s anti-vax tweet mysteriously disappears (105 pts)]]></title>
            <link>https://old.reddit.com/r/EnoughMuskSpam/comments/15a086f/community_note_correcting_musks_antivax_tweet/</link>
            <guid>36878068</guid>
            <pubDate>Wed, 26 Jul 2023 14:14:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/EnoughMuskSpam/comments/15a086f/community_note_correcting_musks_antivax_tweet/">https://old.reddit.com/r/EnoughMuskSpam/comments/15a086f/community_note_correcting_musks_antivax_tweet/</a>, See on <a href="https://news.ycombinator.com/item?id=36878068">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For those that have had enough of the Elon Musk worship on Reddit and beyond.  </p>

<ul>
<li><p>No flaming, baiting, etc. This subreddit is intended for those opposed to the influx of Elon Musk-related advertising on Reddit. Coming here to defend Musk or his companies will not get you banned, but it likely will result in downvotes. Please use the reporting feature if you see a rule violation.</p></li>
<li><p>Opinions from all sides of the political spectrum are welcome here. However, we kindly ask that off-topic political discussion be kept to a minimum, so as to focus on the goal of this sub. This sub is minimally moderated, so discussion and the power of upvotes/downvotes are allowed, provided Reddit rules are not broken.</p></li>
<li><p>Post links to instances of obvious Elon Musk fanboy brigading in default subreddits, astroturfing from Tesla/SpaceX/etc., or any articles critical of Musk, his ideas, unrealistic promises and timelines, or the working conditions at his companies.</p></li>
</ul>

<p>Tesla-specific discussion can be posted here as well as our sister subreddit <a href="https://old.reddit.com/r/RealTesla">/r/RealTesla</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mold 2.0 (223 pts)]]></title>
            <link>https://github.com/rui314/mold/releases/tag/v2.0.0</link>
            <guid>36876783</guid>
            <pubDate>Wed, 26 Jul 2023 12:48:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rui314/mold/releases/tag/v2.0.0">https://github.com/rui314/mold/releases/tag/v2.0.0</a>, See on <a href="https://news.ycombinator.com/item?id=36876783">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pjax="true" data-test-selector="body-content" data-view-component="true"><p>Mold 2.0.0 is a new major release of our high-speed linker. With this release, we've transitioned our license from AGPL to MIT, aiming to expand the user base of our linker. This was not an easy decision, as those who have been following our progress know that we've been attempting to monetize our product through an AGPL/commercial license dual-licensing scheme. Unfortunately, this approach didn't meet our expectations. The license change represents our acceptance of this reality. We don't want to persist with a strategy that didn't work well.</p>
<p>As always, we welcome new GitHub sponsors. If you are happy with the license change, please consider <a href="https://github.com/sponsors/rui314">becoming a sponsor</a>.</p>
<p>In addition to the license change, here is a list of updates we have made in this release:</p>
<ul>
<li>Previously, mold could not produce an object file with more than 65520 sections using the <code>--relocatable</code> option. Now the bug has been fixed. (<a data-hovercard-type="commit" data-hovercard-url="https://github.com/rui314/mold/commit/2e8bd0b7a6d78e1bb2a08249bf83f7603a245174/hovercard" href="https://github.com/rui314/mold/commit/2e8bd0b7a6d78e1bb2a08249bf83f7603a245174"><tt>2e8bd0b</tt></a>)</li>
<li>mold now interprets <code>-undefined</code> as a synonym for <code>--undefined</code> instead of <code>-u ndefined</code>. This seems inconsistent, as <code>-ufoo</code> is generally treated as <code>-u foo</code> (which is an alias for <code>--undefined foo</code>), but this is the behavior of the GNU linkers and LLVM lld, so we prioritize compatibility over consistency.</li>
<li><code>-nopie</code> is now handled as a synonym for <code>--no-pie</code>.</li>
<li>[RISC-V] <code>R_RISCV_SET_ULEB128</code> and <code>R_RISCV_SUB_ULEB128</code> relocation types are now supported (<a data-hovercard-type="commit" data-hovercard-url="https://github.com/rui314/mold/commit/4bffe26bc0167f26c2bad0778f0f95e6be4809f2/hovercard" href="https://github.com/rui314/mold/commit/4bffe26bc0167f26c2bad0778f0f95e6be4809f2"><tt>4bffe26</tt></a>, <a data-hovercard-type="commit" data-hovercard-url="https://github.com/rui314/mold/commit/1ac5fe7c1e9678dca3c7e0bb0f96e1d20be969e5/hovercard" href="https://github.com/rui314/mold/commit/1ac5fe7c1e9678dca3c7e0bb0f96e1d20be969e5"><tt>1ac5fe7</tt></a>)</li>
<li>[PPC64] <code>R_PPC64_REL32</code> relocation type is now supported. (<a data-hovercard-type="commit" data-hovercard-url="https://github.com/rui314/mold/commit/ebd780e8baf8722283be4d157a713d751bed0f81/hovercard" href="https://github.com/rui314/mold/commit/ebd780e8baf8722283be4d157a713d751bed0f81"><tt>ebd780e</tt></a>)</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google is already pushing WEI (web DRM) into Chromium (853 pts)]]></title>
            <link>https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd</link>
            <guid>36876301</guid>
            <pubDate>Wed, 26 Jul 2023 12:05:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd">https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd</a>, See on <a href="https://news.ycombinator.com/item?id=36876301">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  

    
    

    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
  <p>
  <h2>Commit</h2>
</p>

<p><a href="https://github.com/chromium/chromium/commit/6f47a22906b2899412e79a2727355efa9cc8f5bd" data-hotkey="y">Permalink</a></p>


<div>
  <p><a id="browse-at-time-link" href="https://github.com/chromium/chromium/tree/6f47a22906b2899412e79a2727355efa9cc8f5bd" rel="nofollow">Browse files</a></p><tool-tip id="tooltip-1297872c-3785-4260-b922-f8c0a1d7f452" for="browse-at-time-link" popover="manual" data-direction="ne" data-type="description" data-view-component="true">Browse the repository at this point in the history</tool-tip>
    <p>
      [wei] Ensure Origin Trial enables full feature
    </p>

    <div><pre>This CL moves the base::Feature from content_features.h to
a generated feature from runtime_enabled_features.json5.

This means that the base::Feature can be default-enabled
while the web API is controlled by the RuntimeFeature, which will
still be default-disabled.

An origin trial can enable the RuntimeFeature, which will
allow full access to the API, provided the base::Feature is also
enabled (see change to origin_trial_context.cc).

Meanwhile, the base::Feature can be disabled through Finch as a
kill-switch for the whole feature, and prevent origin trials
from turning the feature on.

Tests have been added to WebView test, as it allowed for easy
spoofing of responses on a known origin.

Bug: <a data-hovercard-type="commit" data-hovercard-url="https://github.com/chromium/chromium/commit/1439945750e581212f3cb89144e39349ac1a51d3/hovercard" href="https://github.com/chromium/chromium/commit/1439945750e581212f3cb89144e39349ac1a51d3"><tt>1439945</tt></a>
Change-Id: Ifa0f5d4f5e0a0bf882dd1b0207698dddd6f71420
Fixed: b/278701736
Reviewed-on: <a href="https://chromium-review.googlesource.com/c/chromium/src/+/4681552" rel="nofollow">https://chromium-review.googlesource.com/c/chromium/src/+/4681552</a>
Reviewed-by: Rayan Kanso &lt;rayankans@chromium.org&gt;
Commit-Queue: Peter Pakkenberg &lt;pbirk@chromium.org&gt;
Reviewed-by: Dmitry Gozman &lt;dgozman@chromium.org&gt;
Reviewed-by: Richard Coles &lt;torne@chromium.org&gt;
Reviewed-by: Kinuko Yasuda &lt;kinuko@chromium.org&gt;
Cr-Commit-Position: refs/heads/main@{#1173344}</pre></div>

  <div>
  <include-fragment src="/chromium/chromium/branch_commits/6f47a22906b2899412e79a2727355efa9cc8f5bd" id="async-branches-list">
    
    <ul>
      <li>Loading branch information<span></span></li>
    </ul>
</include-fragment></div>


  
</div>


  


  <diff-layout>
    
        </diff-layout>


</div>

</turbo-frame>


    </main>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unesco calls for global ban on smartphones in schools (185 pts)]]></title>
            <link>https://www.theguardian.com/world/2023/jul/26/put-learners-first-unesco-calls-for-global-ban-on-smartphones-in-schools</link>
            <guid>36875862</guid>
            <pubDate>Wed, 26 Jul 2023 11:23:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2023/jul/26/put-learners-first-unesco-calls-for-global-ban-on-smartphones-in-schools">https://www.theguardian.com/world/2023/jul/26/put-learners-first-unesco-calls-for-global-ban-on-smartphones-in-schools</a>, See on <a href="https://news.ycombinator.com/item?id=36875862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Smartphones should be banned from schools to tackle classroom disruption, improve learning and help protect children from cyberbullying, a UN report has recommended.</p><p>Unesco, the UN’s education, science and culture agency, said there was evidence that excessive mobile phone use was linked to reduced educational performance and that high levels of screen time had a negative effect on children’s emotional stability.</p><p>It said its call for a smartphone ban sent a clear message that digital technology as a whole, including artificial intelligence, should always be subservient to a “human-centred vision” of education, and never supplant face-to-face interaction with teachers.</p><p>Unesco warned policymakers against an unthinking embrace of digital technology, arguing that its positive impact on learning outcomes and economic efficiency could be overstated, and new was not always better. “Not all change constitutes progress. Just because something can be done does not mean it should be done,” it concluded.</p><p>With more learning moving online, especially in universities, it urged policymakers not to neglect the “social dimension” of education where students receive face-to-face teaching. “Those urging increasing individualisation may be missing the point of what education is about,” it said.</p><p>“The digital revolution holds immeasurable potential but, just as warnings have been voiced for how it should be regulated in society, similar attention must be paid to the way it is used in education,” said Unesco’s director general, Audrey Azoulay.</p><p>She added: <em>“</em>Its use must be for enhanced learning experiences and for the wellbeing of students and teachers, not to their detriment. Keep the needs of the learner first and support teachers. Online connections are no substitute for human interaction.”</p><p>Unesco said in the <a href="https://unesdoc.unesco.org/ark:/48223/pf0000385723" data-link-name="in body link">report</a> that countries needed to ensure they had clear objectives and principles in place to ensure digital technology in education was beneficial and avoided harm, both to individual students’ health, and more widely to democracy and human rights, for instance through invasion of privacy and stoking of online hatred.</p><p>Excessive or inappropriate student use of technology in the classroom and at home, whether smartphones, tablets or laptops, could be distracting, disruptive and result in a detrimental impact on learning, it said. It cited large-scale international assessment data that indicated a “negative link” between excessive use of digital technology and student performance.</p><p>Although technology could potentially open up opportunities for learning for millions, the benefits were unequally spread, with many poorer people from around the world effectively excluded, it said. A digital educational infrastructure was expensive, and its environmental costs often underestimated.</p><p>There was little robust research to demonstrate digital technology inherently added value to education, Unesco said in its 2023 Global Education Monitor report. Much of the evidence was funded by private education companies trying to sell digital learning products. Their growing influence on education policy around the world was “a cause for concern”, it added.</p><p>Countries were “waking up to the importance of putting learners first” when it came to digital technology, said Unesco. It cited China, which it said has set boundaries for the use of digital devices as teaching tools, limiting them to 30% of all teaching time, with students expected to take regular screen breaks.</p><p>It accepted that online learning “stopped education melting down” when schools and universities closed during Covid-19 lockdowns. It estimated that more than a billion students globally moved to online learning during the pandemic – but added that millions of poorer students without internet access were left out.</p><p>Based on its analysis of 200 education systems around the world, Unesco estimated one in four countries had banned smartphones in school, either through law or guidance. These included <a href="https://www.theguardian.com/world/2017/dec/11/france-to-ban-mobile-phones-in-schools-from-september" data-link-name="in body link">France</a>, which introduced its policy in 2018, and the Netherlands, which will bring in restrictions from 2024.</p><p><a href="https://www.theguardian.com/world/2023/jul/04/mobile-phones-other-devices-to-be-banned-from-dutch-classrooms" data-link-name="in body link">Announcing the ban</a> this month, the Dutch education minister, Robbert Dijkgraaf, said: “Students need to be able to concentrate and need to be given the opportunity to study well. Mobile phones are a disturbance, scientific research shows. We need to protect students against this.”</p><p>In the UK, the former education secretary Gavin Williamson called for a <a href="https://www.theguardian.com/education/2021/jun/29/education-secretary-wants-ban-on-mobile-phones-in-english-schools#:~:text=The%20education%20secretary%2C%20Gavin%20Williamson,the%20impact%20of%20the%20pandemic." data-link-name="in body link">mobile phone ban in schools </a>in 2021 as part of a crackdown on poor student discipline but this was dismissed as a “distraction” by education unions who said schools had had smartphone use policies in place for years.</p><p>UK secondary schools’ smartphone policies vary as they are a decision for individual headteachers. They typically ensure phones are switched off and not visible while on the school site, and can be used in the classroom only with the permission of the teacher. Misuse of phones or other digital devices on school premises can lead to confiscation and sanctions such as detention.</p><p>Geoff Barton, general secretary of the Association of School and College Leaders, said: “The majority of schools will already have robust policies about mobile phones in place. In most cases pupils will either be prohibited entirely from using them during the school day or restricted to only using them in certain circumstances.</p><p>“Banning mobile phones entirely from school premises would raise some practical concerns, for example for parents wanting to contact their children while travelling between school and home. Some pupils will also use phones as payment methods on public transport.”</p><p>He added: “We completely understand the legitimate concerns around the use of mobile phones, including cyberbullying, the impact of extended screen time on mental health, and the lack of regulation of big technology companies. The fact is though that the widespread use of smartphones is a societal issue and problems that result from this are more likely to arise outside of the school gates.”</p><p>The Department for Education was approached for comment.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A world where people pay for software (141 pts)]]></title>
            <link>https://1sub.dev/</link>
            <guid>36875513</guid>
            <pubDate>Wed, 26 Jul 2023 10:47:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://1sub.dev/">https://1sub.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=36875513">Hacker News</a></p>
<div id="readability-page-1" class="page">

<header>
<p><a href="https://1sub.dev/"><big>1Sub.dev</big></a></p>
<nav>
<p>
<a href="https://1sub.dev/about/">About</a> ·
<a href="https://1sub.dev/list">Find developer</a> ·
<a href="https://1sub.dev/login">Log in</a>
</p>
</nav>
</header>




<p>Software development has to be funded somehow.
Unfortunately, it is hard to sell software like physical products because it does not have the built-in copy protection that physical products have.
Any artificial copy protections added to software are ineffective if users should have full control of their own computers.
Therefore funding has to be done in a different way.</p>

<p>The existing methods of funding software have their problems:</p>
<ul>
<li>Pay to download or for other services: Not worth it; users can find the software somewhere else and they don't need your other services.</li>
<li>Accepting donations: Why would you pay if you don't get anything for your money?</li>
</ul>

<p><strong>1Sub.dev</strong> tries to solve this by letting developers cooperate to give users more for their money.
The user subscribes to a developer of their choice and in return, all developers (and everyone else who wants to) can give that user some kind of benefit, like giving them access to downloads or other resources.
Simply, from the effort of subscribing to one, you get the benefit of subscribing to all.</p>

<p><a href="https://1sub.dev/about/how-it-works">Read more about how it works.</a></p>

<h2>Why this is better than the alternatives</h2>

<ul>
<li>You don't have to make separate payments for each program you use. Just subscribe to one developer and you get access to everything.</li>
<li>Do you need to use some software but you don't want to or can't give that developer money? Don't worry, you choose who you want to subscribe to. Nobody can see who you are subscribing to.</li>
<li>You are anonymous and this system doesn't handle any money, just the information about who is subscribing to who. Users are identified by an account number.</li>
</ul>

<h2>Are you a developer?</h2>

<p>We are looking for developers (companies or individuals) of Free and Open Source software who want to join this cooperation.
Are you one of them?
Do you believe in this solution?
You can contact me at 1sub@robalni.org and we can try to make it happen.</p>

<h2>Try it out</h2>

<p><a href="https://1sub.dev/link?u=https://1sub.dev/^.txt&amp;s=AyQGmdu0Lc3HfKbY3_w&amp;k=&amp;n=JYGSlzF1n4BBizER&amp;a=a.18">Payment check example</a></p>




</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google engineers want to make ad-blocking (near) impossible (442 pts)]]></title>
            <link>https://stackdiary.com/web-environment-integrity/</link>
            <guid>36875226</guid>
            <pubDate>Wed, 26 Jul 2023 10:04:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stackdiary.com/web-environment-integrity/">https://stackdiary.com/web-environment-integrity/</a>, See on <a href="https://news.ycombinator.com/item?id=36875226">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
							
<p>In recent news, Google has put forth a proposal known as the "<a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/blob/main/explainer.md" target="_blank" rel="noreferrer noopener">Web Environment Integrity Explainer</a>", authored by four of its engineers. </p>



<p>On the surface, it appears to be a comprehensive effort to enhance trust and security in the digital landscape. However, as with many sweeping technological proposals, it's not without controversy. </p>



<p>The tech community, <a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/issues" target="_blank" rel="noreferrer noopener">especially on GitHub</a>, has raised several eyebrows and voiced significant criticism. </p>



<p>Mozilla has just come out to say that <a href="https://github.com/mozilla/standards-positions/issues/852#issuecomment-1648820747" target="_blank" rel="noreferrer noopener">they oppose this proposal</a>, <em>"Detecting fraud and invalid traffic is a challenging problem that we're interested in helping address. However this proposal does not explain how it will make practical progress on the listed use cases, and there are clear downsides to adopting it."</em></p>



<h3>The Core Proposal: A Trust-Privacy Trade-off?</h3>



<p>Google's proposal pivots on a key premise: enhancing trust in the client environment. It introduces a new API that allows websites to request a token, providing evidence about the client code's environment. Google's engineers elaborate, <em>"Websites funded by ads require proof that their users are human and not bots...Social websites need to differentiate between real user engagement and fake engagement...Users playing online games want assurance that other players are adhering to the game's rules."</em> </p>



<p>However, the critics argue that the quest for trust may come at the expense of privacy. While Google ensures that the tokens will not include unique identifiers, critics fear that this system, if misused, could lead to unwarranted surveillance and control.</p>



<h3>Veiled DRM and the Threat to Open Web</h3>



<p>The proposed API, while framed as a tool for fostering trust, could potentially be used to control user behavior on the web. Some critics fear it could be a covert introduction of Digital Rights Management (DRM) into web pages, making ad-blocking near impossible. </p>



<p>This not only impacts user experience but also raises concerns about net neutrality and the open nature of the web. As one critic aptly questioned, "Could this be a veiled attempt at introducing DRMs for web pages, making ad-blocking near-impossible in the browser?"</p>



<h3>Monopolization Fears: Who Controls the Attesters?</h3>



<p>A significant concern stemming from the tech community is the potential for monopolistic control. By controlling the "attesters" that verify client environments, Google, or any other big tech company, could potentially manipulate the trust scores, thereby deciding which websites are deemed trustworthy. This opens up a can of worms regarding the democratic nature of the web. </p>



<p>As one GitHub user commented, "This raises a red flag for the open nature of the web, potentially paving the way for a digital hierarchy dominated by a few tech giants."</p>



<h3>What About Browser Modifications and Extensions?</h3>



<p>Google's proposal remains ambiguous about its impact on browser modifications and extensions. It attests to the legitimacy of the underlying hardware and software stack without restricting the application’s functionality. </p>



<p>However, how this plays out with browsers that allow extensions or are modified remains a grey area. As the proposal vaguely mentions, "Web Environment Integrity attests the legitimacy of the underlying hardware and software stack, it does not restrict the indicated application’s functionality."</p>



<h3>Unanswered Questions and The Path Forward</h3>



<p>The proposal leaves several questions unanswered and open for discussion. For instance, it doesn't clearly address how it will prevent the signal from being used to exclude vendors. Google's engineers write, <em>"Attesters will be required to offer their service under the same conditions to any browser who wishes to use it and meets certain baseline requirements."</em> </p>



<p>However, it's unclear how these baseline requirements will be set and who will enforce them.</p>



<p>In conclusion, while Google's proposal is a technically sophisticated attempt to enhance trust on the web, its potential implications for user privacy and the open nature of the web cannot be ignored. The tech community's concerns highlight the need for a balanced approach that doesn't compromise on either trust or privacy. </p>



<p>It's crucial that the tech community continues to engage in these debates to ensure that the future of the web is shaped by openness, privacy, and freedom rather than control and surveillance.</p>
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google vs. the Open Web (453 pts)]]></title>
            <link>https://interpeer.io/blog/2023/07/google-vs-the-open-web/</link>
            <guid>36875164</guid>
            <pubDate>Wed, 26 Jul 2023 09:52:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://interpeer.io/blog/2023/07/google-vs-the-open-web/">https://interpeer.io/blog/2023/07/google-vs-the-open-web/</a>, See on <a href="https://news.ycombinator.com/item?id=36875164">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<section>
<article>
<p>A few days ago, I made <a href="https://chaos.social/@interpeer/110740096258668222" title="a social media post" rel="external">a social media post</a> about Google vs.
the Open Web. It received some responses, so I’ll reproduce it below with some
additional comments.</p>
<figure><a href="https://www.flickr.com/photos/35034356597@N01/2787595632" target="\_blank" rel="external"><img sizes="(min-width: 35em) 720px, 100vw" srcset="https://interpeer.io/blog/2023/07/google-vs-the-open-web/xopen_web_hu9b2049b3d2de937c06820417b96c6b9c_954312_500x0_resize_q75_box.jpg.pagespeed.ic.S8rrGRfHbZ.jpg 500w, https://interpeer.io/blog/2023/07/google-vs-the-open-web/xopen_web_hu9b2049b3d2de937c06820417b96c6b9c_954312_800x0_resize_q75_box.jpg.pagespeed.ic.TMcc1yxNKS.jpg 800w, https://interpeer.io/blog/2023/07/google-vs-the-open-web/xopen_web_hu9b2049b3d2de937c06820417b96c6b9c_954312_800x0_resize_q75_box.jpg.pagespeed.ic.TMcc1yxNKS.jpg 1200w" src="https://interpeer.io/blog/2023/07/google-vs-the-open-web/xopen_web.jpg.pagespeed.ic.ToV_CzhF_r.jpg" alt="Open Web - Gnomedex 2008" width="100%">
</a><figcaption>
<p><a href="https://www.flickr.com/photos/35034356597@N01/2787595632">“Open Web - Gnomedex 2008”</a> by <a href="https://www.flickr.com/photos/35034356597@N01">Randy Stewart</a> is licensed under <a href="https://creativecommons.org/licenses/by-sa/2.0/">CC BY-SA 2.0</a></p>
</figcaption>
</figure>
<hr>
<h2 id="google-is-trying-to-kill-the-open-web">Google is trying to kill the Open Web.</h2>
<p>Using the proposed “Web Environment Integrity” means websites can select on
which devices (browsers) they wish to be displayed, and can refuse service
to other devices. It binds client side software to a website, creating a
silo’d app.</p>
<a href="https://github.com/RupertBenWiser/Web-Environment-Integrity/blob/main/explainer.md" title="Web Environment Integrity on GitHub" rel="external">Web Environment Integrity on GitHub</a>
<p>This penalizes platforms on which the preferred client side software is not
available.</p>
<p>This is an issue for accessibility and inclusion, in particular when the reason
the software is not available is tied to the needs of marginalized groups, such
as when poverty makes it impossible to own sufficiently modern devices, etc.</p>
<p>“Web Environment Integrity” is a deeply #antisocial proposal, and goes counter
to the design principles of the web.</p>
<p>In all honesty, this move by Google is hardly surprising. They’ve been trying to
capture the web as their own platform for a long time. This is just the latest
battle.</p>
<p>But it also marks a particularly perverse point, in how the proposal admits it
exists primarily to extract value from people. People mining at its worst.</p>
<p>Remember when their motto was “Don’t be Evil?”</p>
<hr>
<h2 id="analysis-of-the-proposal">Analysis of the Proposal</h2>
<p>Some details on the proposal may help here.</p>
<p>The proposal suggests that websites should be able to request an attestation
from the browser about its “integrity”. Such attestations are to be provided
by external agents, which – presumably – examine the browser and its plugins,
and issue an approval only if those checks pass.</p>
<p>The attestation is sent back to the website, which can now decide to deny
service if the agent did not give approval.</p>
<p>Ostensibly, this is to ensure <em>for the user</em> that the environment has not been
tampered with in any way. The described use cases, however, make fairly clear
that it is <em>for the business</em> that this feature exists.</p>
<p>In particular, the proposal suggests that “Google Play” could provide such
attestations, and also provides an example case which intends to ensure that
ads are served only to legitimate users, not to automated processes.</p>
<p>These two points are not raised together. But put them together, and you find
the following underlying problem:</p>
<ol>
<li>Advertisers want to reduce costs.</li>
<li>Website owner wishes to display ads.</li>
<li>Google’s ad network charges per <em>impression</em>.</li>
<li>Bots create impressions.</li>
</ol>
<p>The proposal effectively provides a solution for Google’s advertising problem,
and tries to couch it in more user friendly terms. The above scenario is the
closest to a problem they describe outright.</p>
<p>The solution, expressed in the proposal, is to exclude bots via attestations,
such that ads generate impressions only with logged-in Google Play users.</p>
<p>However…</p>
<p>In general, bots are pretty easy to exclude. They usually advertise themselves
by a user agent string. Yes, that can be faked – but it seems highly unlikely
that bots using faked user agents create such a large number of impressions that
Google has to use this route against them. If I look at my own webserver logs,
it’s very clear which are bot requests <em>just from the log information</em>.</p>
<p>If bots are not the actual problem, then what is?</p>
<p>The agent providing the attestation is free to use whichever means to approve
or disapprove of a browser. That includes examining whether the browser runs
ad blockers.</p>
<p>Given how advertising networks track users, and user tracking is a practice
that comes under increasing criticism, ad blockers are also gaining in
popularity. Security experts regularly recommend the use of ad blockers as
a cyber security measure – as ads can be used to side-load malware into an
otherwise legitimate website.</p>
<p>What Google is really after is ad blockers.</p>
<h3 id="problems">Problems</h3>
<p>The downside of this approach is that it opens up a door for arbitrary abuse.
Websites can refuse service unless you install their proprietary data collection
agent. Websites can refuse service if you use the wrong browser – we’d enter
the <a href="https://en.wikipedia.org/wiki/Browser_wars" title="browser wars" rel="external">browser wars</a>
of the late 90s, with renewed effort.</p>
<p>The day this proposal gets accepted is the day the Open Web is set back by
decades.</p>
<p>In <a href="https://yalebooks.yale.edu/book/9780300151244/the-future-of-the-internet-and-how-to-stop-it/" title="The Future of the Internet -- And How to Stop It" rel="external">The Future of the Internet -- And How to Stop It</a>,
Jonathan Zittrain lays out, starting with the telephone network, that there
exist “appliances” and “generative systems”.</p>
<p>An “appliance” works like any other household appliance, like a toaster. It
has one primary function, and all other functions it may offer are at best
mild variations of the primary one. It toasts.</p>
<p>Zittrain lists the PC and the internet as examples of generative systems.
Generative systems are not necessarily as complete in functionality as an
appliance – they provide some basic functionality, but with no specific
primary purpose. The purpose is left to the user. Another way of phrasing
this is to call these things tools, or crafting materials.</p>
<p>Maybe it’s worth pointing out the text of the above image at this point:</p>
<blockquote>
<p>The Open Web is a collection of user-developed, non-proprietary frameworks
for building services, emphasizing interoperability and the balance of data
access and ownership between providers and end-users.</p>
</blockquote>
<p>Generative systems are significantly more impactful than appliances precisely
because they leverage the user’s imagination to address their own needs. They
crowdsource meaning at global scale. This is what makes the internet so
powerful.</p>
<p>Attestations from an agent doing arbitrary things effectively turns the web
into an appliance – or, to be more specific, it turns the browser into an
extension of an appliance website.</p>
<p>Of course website owners are free to build appliances. They already are doing
so. But this reduces the usefulness of “the web” step by step, until the
generative open web is lost. We’re already seeing the negative effects of this,
and technology like the proposed would only accelerate the trend.</p>
<p>Google does not need to mind. The inexorable logic of capitalism means that
businesses that managed to build upon a generative system to rise, now have
to turn that same system into an appliance for their own needs, or risk being
open to competition.</p>
<h2 id="reactions">Reactions</h2>
<p>The reactions to the post were diverse, and it’s worth addressing a few.</p>
<ol>
<li>
<p><strong>This does not imply accessibility or inclusion issues!</strong> – Yes and no.
No, in principle this technology does not <em>cause</em> accessibility issues. But
the <a href="https://en.wikipedia.org/wiki/Pareto_principle" title="pareto principle" rel="external">pareto principle</a> implies that effort
should be spent on 20% of the browser market because that captures 80% of the
users – and cost effectiveness then mandates that the remaining 20% of users
should be ignored, because they’ll cost too much to support.
<br>
That is exactly the worry here. Marginalized groups which need specialized
browser – for example with good screen reader capability, or capable of
running on cheaper/older devices – will effectively be excluded by rational
business logic.</p>
</li>
<li>
<p><strong>Worry about access, not about technology!</strong> – The argument is that good
regulatory frameworks will legally mandate access, so that should be the
focus.
<br>
This is true, but not enough. The two problems with this line of thinking are
that first, good regulatory frameworks are rare. And part of the reason for
that is the second problem, namely that technology moves faster than the law.
<br>
Which means that worrying about access <em>instead</em> of technology will still
exclude marginalized groups in practice. What is required instead is to
worry about technology in the short term, and regulation in the long term.</p>
</li>
<li>
<p><strong>It is legitimate for businesses to wish to protect their interests.</strong> –
That is a debatable point. Businesses “protecting their interests” to the
detriment of people is not legitimate. But within the bounds of that, sure,
why not.
<br>
Here’s the problem, though: the internet and open web are <em>generative
systems</em>, which means the reason they have a positive impact is because
people can decide how to use them. The moment this decision making power
is curtailed, the system shifts towards an <em>appliance</em>.
<br>
If businesses protect their interests by reducing a former generative
system to an appliance, by definition this is to the detriment of people,
and no longer legitimate.</p>
</li>
</ol>
<h2 id="updates">Updates</h2>
<ul>
<li>
<p><strong>2023-07-21</strong>: After raising <a href="https://github.com/w3c/PWETF/issues/306">a code of conduct violation</a>
for the proposal with the W3C’s group responsible for said code, I was
rightly told that they are not responsible (TL;DR, see the link). I then
sent an email to the ombudspeople at W3C which I’ll reproduce here:</p>
<pre tabindex="0"><code>From jens@OBFUSCATED Fri Jul 21 17:23:38 2023
Date: Fri, 21 Jul 2023 17:23:38 +0200
From: "Jens Finkhaeuser" &lt;jens@OBFUSCATED&gt;
To: ombuds@w3.org
Subject: Web Environment Integrity proposal

Dear Ombudspeople of the W3C,

I wish to raise concerns about the behaviour of the people working on
the Web Environment Integrity proposal, as well as the proposal
itself.

https://github.com/RupertBenWiser/Web-Environment-Integrity/

In particular, I would like to draw your attention to isuse #131 in
their working repository:

https://github.com/RupertBenWiser/Web-Environment-Integrity/issues/131

The group working on this claims to adhere to the W3C Code of Ethics
and Professional Conduct. However, as documented in this issue, they
violate said code.

As a bit of background, WEI is a deeply unethical proposal that
suggests to use cryptographic means to permit websites to deny
services to users based on arbitrary user metadata. Such metadata
is to be processed by agents running on the user's machine, which
provide attestations about the browser environment.

One such proposed service is Google Play, which has access to personal
identifiable information (PII). This turns the proposal into a
technological mechanism for discrimination.

The community has raised and is raising issues about the ethics of
such a proposal, which led me to find the W3C code of ethics.
Unfortunately, as was pointed out to me, the code of ethics does not
concern the content of proposals - merely the conduct of participants.

Unfortunately, some maintainers of the repository have taken to
closing issues raised by the community -- the fourth bullet point in
the "participant" explanation of the code ("Anyone from the Public
partaking in the W3C work environment (e.g. commenting on our specs,
(...)"). This violates several points in section 3.1 of the same
document, whereby use of reasonable means to process diverse views are
required.

It seems to be the case that this proposal has not yet made it to a
W3C group. However, its maintainers already violate the W3C code of
ethics in practice in the run-up to such an activity. In the meantime,
even though the code is not directly applicable to the proposal
contents, it nonetheless violates said code in spirit.

It seems appropriate that W3C does not permit this proposal to go ahead
in any formal fashion.

Kind regards,
  Jens Finkhaeuser
</code></pre></li>
<li>
<p><strong>2023-07-22</strong>: Google has now closed the ability to contribute to the
repository, including by raising or commenting on issues.</p>
</li>
<li>
<p><strong>2023-07-26 #1</strong>: Apple has already shipped a similar API for about a year.
As described on the Apple developer blog, <a href="https://developer.apple.com/news/?id=huqjyh7k">Private Access Tokens</a>
implement pretty much the same mechanism as Google’s WEI.</p>
<p>There are a few notable differences in <em>tone</em>, however. The first is a direct
quote from the above blog post:</p>
<blockquote>
<p>Note: When you send token challenges, don’t block the main page load.
Make sure that any clients that don’t support tokens still can access
your website!</p>
</blockquote>
<p>This note is not doing anything in itself, but it <em>does</em> stand in stark
contrast to the motivations documented in WEI. In particular, the proposal
suggests that private access tokens should be used instead of e.g. CAPTCHAs
or other, more disruptive authentication mechanisms.</p>
<p>The second important difference is in the actual details of the proposal.
It states that the token issuer is an external web service rather than some
opaque process running on the user’s machine. Suggested are some CDN
providers’ services. The clear message of intent here is that this is
supposed to be a mechanism by which CDNs authenticate a request to the
source.</p>
<p>The protocol by which this is to be done is defined by the <a href="https://datatracker.ietf.org/wg/privacypass/about/">IETF PrivacyPass Working Group</a>.
Reading through <a href="https://www.ietf.org/archive/id/draft-ietf-privacypass-protocol-11.html">the protocol draft</a>,
it furthermore becomes clear that the data the client is supposed to send
to the issuer is… nothing but the challenge sent by the server, in an
obfuscated (hashed) manner.</p>
<p>This leads to two conclusions.</p>
<ol>
<li>No personal data is being leaked.</li>
<li>There is no checking of the “environment”, aka the browser and its plugins,
that can prevent some browsers from receiving an attestation.</li>
</ol>
<p>While Private Access Token attestations can <em>still</em> be used to undermine the
open web simply by forcing browsers to start adopting this or risk exclusion,
this attestation approach <em>could not be more different in its fundamental
function</em> from Google’s WEI if it tried.</p>
<p>Is it still a bad idea? Maybe. But if so, it’s an entirely different, far
friendlier ball game.</p>
</li>
<li>
<p><strong>2023-07-26 #2</strong>: <a href="https://github.com/mozilla/standards-positions/issues/852#issuecomment-1648820747">Mozilla has taken a stance against WEI</a>
writing:</p>
<blockquote>
<p>Mozilla opposes this proposal because it contradicts our principles and
vision for the Web.</p>
</blockquote>
<p>That’s something, at least.</p>
</li>
<li>
<p><strong>2023-07-26 #3</strong>: As a honourable mention, the maintainer of the Google
repository has published a <a href="https://blog.yoav.ws/posts/web_platform_change_you_do_not_like/">personal blog post about their experience</a>,
which contains some fair and some unfair bits.</p>
<p>However, one of the points bears commenting on:</p>
<blockquote>
<p>Don’t assume a hidden agenda</p>
<p>When thinking about a new proposal, it’s often safe to assume that Occam’s
razor is applicable and the reason it is being proposed is that the team
proposing it is trying to tackle the use cases the proposal handles. While
the full set of organizational motivations behind supporting certain use
cases may not always public (e.g. a new device type not yet announced,
legal obligations, etc), the use cases themselves should give a clear enough
picture of what is being solved.</p>
</blockquote>
<p>There are a few comments to this:</p>
<ol>
<li>Given that Apple’s mechanism is undergoing IETF standardization, the only
reason for an opposing mechanism is that the existing approach does not
fulfil Google’s needs.</li>
<li>Google <em>clearly</em> states its needs in its use cases. There is no hidden
agenda that people complain about, but rather the agenda as it is stated
clearly in plain text.</li>
</ol>
<p>This comment actually <em>confirms</em> the community’s worst fears.</p>
</li>
</ul>
<hr>
<p>The Interpeer Project’s mission is to build next-generation internet technology
that re-focuses on people over businesses. You can support us by
<a href="https://interpeer.io/donations/">donating today</a>.</p>
</article>
</section>
<hr>
<p>Published on July 21, 2023</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When the Music Industry Waged War on the Cassette Tape During the 1980s (111 pts)]]></title>
            <link>https://www.openculture.com/2023/07/home-taping-is-killing-music-when-the-music-industry-waged-war-on-the-cassette-tape.html</link>
            <guid>36874275</guid>
            <pubDate>Wed, 26 Jul 2023 07:41:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2023/07/home-taping-is-killing-music-when-the-music-industry-waged-war-on-the-cassette-tape.html">https://www.openculture.com/2023/07/home-taping-is-killing-music-when-the-music-industry-waged-war-on-the-cassette-tape.html</a>, See on <a href="https://news.ycombinator.com/item?id=36874275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2023/07/24205918/tometaping.jpg" alt="" width="500" height="324" srcset="https://cdn8.openculture.com/2023/07/24205918/tometaping.jpg 500w, https://cdn8.openculture.com/2023/07/24205918/tometaping-360x233.jpg 360w, https://cdn8.openculture.com/2023/07/24205918/tometaping-240x156.jpg 240w" sizes="(max-width: 500px) 100vw, 500px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2023/07/24205918/tometaping.jpg" data-srcset="https://cdn8.openculture.com/2023/07/24205918/tometaping.jpg 500w, https://cdn8.openculture.com/2023/07/24205918/tometaping-360x233.jpg 360w, https://cdn8.openculture.com/2023/07/24205918/tometaping-240x156.jpg 240w"></p>
<p>The first time I saw the infamous Skullcassette-and-Bones logo was on holiday in the UK and purchased the very un-punky <em>Chariots of Fire</em> soundtrack. It was on the inner sleeve. “Home Taping Is Killing Music” it proclaimed. <em>It was?</em> I asked myself. “And it’s illegal” a subhead added. <em>It is?</em> I also asked myself. (Ironically, this was a few months before I came into possession of my first combination turntable-cassette deck.)</p>
<p>Ten years and racks and racks of homemade cassette dubs on my shelves later, music seemed to be doing very well. (Later, by going digital, the music industry killed itself, and I had absolutely nothing to do with it.)</p>


<p><a href="https://diffuser.fm/home-taping-is-killing-music-uk/">British record collectors will no doubt remember this campaign that started in 1981</a>, another business-backed “moral” panic. And funnily enough it had nothing to do with dubbing vinyl.</p>
<p>Instead, the British Phonographic Industry (BPI) were taking aim at people who were recording songs off the radio instead of purchasing records. With the rise of the cassette tape in popularity, the BPI saw pounds and pence leaving their pockets.</p>
<div>
<p><span><iframe loading="lazy" title="YouTube video player" type="text/html" width="640" height="505" src="//www.youtube.com/embed/WkZvtu4W748?wmode=transparent&amp;fs=1&amp;hl=en&amp;showsearch=0&amp;rel=0&amp;theme=dark" frameborder="0" allowfullscreen=""></iframe></span>
	</p>
</div>

<p>Now, figuring out lost profits from home taping could be a fools’ errand, but let’s focus on the “illegal” part. Technically, this is true. Radio stations pay licensing fees to play music, so a consumer taping that song off the radio is infringing on the song’s copyright. Britain has very different “fair use” laws than America. In addition, digital radio and clearer signals have complicated matters over the years.</p>
<p>In practice, however, the whole thing was bunkum. Radio recordings are historic. Mixtapes are culture. I have my tapes of John Peel’s BBC shows, which I recorded for the music. Now, I listen to them for Peel’s intros and outros.</p>
<p>Seriously, the Napalm Death Peel Sessions *only* make sense with his commentary. Whoever taped this is an unknown legend:</p>
<div>
<p><span><iframe loading="lazy" title="YouTube video player" type="text/html" width="640" height="505" src="//www.youtube.com/embed/VIBGlshimQ0?wmode=transparent&amp;fs=1&amp;hl=en&amp;showsearch=0&amp;rel=0&amp;theme=dark" frameborder="0" allowfullscreen=""></iframe></span>
	</p>
</div>

<p>The post-punk crowd knew the campaign was bunkum too. Malcolm McLaren, always the provocateur, released Bow Wow Wow’s cassette-only-single <em>C-30 C-60 C-90 Go</em> with a blank B-side that urged consumers to record their own music. EMI quickly dropped the band.</p>
<p>The Dead Kennedys also repeated the black b-side gimmick with <em>In God We Trust, Inc.</em> (I would be interested in anybody who picks up a copy used of either to see what *is* on the b-side).</p>
<p>And then there were the parodies. The metal group Venom used “Home Taping Is Killing Music; So Are Venom” on an album; Peter Principle offered “Home Taping Is Making Music”: Billy Bragg kept it Marxist: “Capitalism is killing music – pay no more than £4.99 for this record”. For the industry, music was the product; for the regular folks, music was communication, it was art, it was a language.</p>
<p>The campaign never did much damage. Attempts to levy a tax on blank cassettes didn’t get traction in the UK. And BPI’s director general John Deacon was frustrated that record companies didn’t want to splash the Jolly Roger on inner sleeves. The logo lives on, however, as part of torrent site Pirate Bay’s sails:</p>
<p>Just after the hysteria died down, compact discs began their rise, planting the seeds for the digital revolution, the mp3, file sharing, and now streaming.</p>
<p>(Wait, is it possible to record internet streams? <a href="http://www.highcriteria.com/">Why, yes.</a>)</p>
<p>If you have any stories about how you helped “kill music” by recording your favorite DJs, confess your crimes in the comments.</p>
<p>Note: An earlier version of this post appeared on our site in 2019.</p>
<p>If you would like to sign up for Open Culture’s free email newsletter,&nbsp;<a href="https://www.openculture.com/newsletter-signup">please find it here</a>.</p>
<p><i><span>If you would like to support the mission of Open Culture, consider <a href="https://bit.ly/3EBHjtX">making&nbsp;a donation to our site</a>. It’s hard to rely 100% on ads, and your <a href="https://bit.ly/3EBHjtX">contributions</a> will help us continue providing the best free cultural and educational materials to learners everywhere. You can contribute through <a href="https://www.openculture.com/help-fund-open-culture">PayPal</a>, <a href="https://bit.ly/3eB2GRB">Patreon</a>, Venmo (@openculture) and <a href="https://bit.ly/3JtLPOK">Crypto</a>. Thanks!</span></i><i></i></p>

<p><strong>Related Content:</strong></p>
<p><a title="Permanent Link to Frank Zappa Debates Whether the Government Should Censor Music in a Heated Episode of <i>Crossfire</i>: Why Are People Afraid of Words? (1986)" href="https://www.openculture.com/2018/04/frank-zappa-debates-whether-the-government-should-censor-music.html" rel="bookmark">Frank Zappa Debates Whether the Government Should Censor Music in a Heated Episode of&nbsp;Crossfire: Why Are People Afraid of Words? (1986)</a></p>
<p><a title="Permanent Link to The Devilish History of the 1980s Parental Advisory Sticker: When Heavy Metal &amp; Satanic Lyrics Collided with the Religious Right" href="https://www.openculture.com/2019/04/the-devilish-history-of-the-1980s-parental-advisory-sticker.html" rel="bookmark">The Devilish History of the 1980s Parental Advisory Sticker: When Heavy Metal &amp; Satanic Lyrics Collided with the Religious Right</a></p>
<p><a title="Permanent Link to 75 Post-Punk and Hardcore Concerts from the 1980s Have Been Digitized &amp; Put Online: Fugazi, GWAR, Lemonheads, Dain Bramage (with Dave Grohl) &amp; More" href="https://www.openculture.com/2022/01/75-post-punk-and-hardcore-concerts-from-the-1980s-have-digitized-put-online.html" rel="bookmark">75 Post-Punk and Hardcore Concerts from the 1980s Have Been Digitized &amp; Put Online: Fugazi, GWAR, Lemonheads, Dain Bramage (with Dave Grohl) &amp; More</a></p>
<p><em> Ted Mills is a freelance writer on the arts who currently hosts the artist interview-based <a href="http://www.funkzonepodcast.com/">FunkZone Podcast</a> and is the producer of KCRW’s <a href="http://curious.kcrw.com/category/curious-coast">Curious Coast</a>. You can also follow him on Twitter at <a href="http://www.twitter.com/tedmills">@tedmills</a>, read his other arts writing at <a href="http://www.tedmills.com/">tedmills.com</a> and/or watch his films <a href="https://vimeo.com/user749601">here</a>.</em></p>
<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists Say Recycling Has Backfired Spectacularly (111 pts)]]></title>
            <link>https://futurism.com/the-byte/scientists-recycling-backfired</link>
            <guid>36873516</guid>
            <pubDate>Wed, 26 Jul 2023 05:38:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://futurism.com/the-byte/scientists-recycling-backfired">https://futurism.com/the-byte/scientists-recycling-backfired</a>, See on <a href="https://news.ycombinator.com/item?id=36873516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="incArticle"><h2>We thought we were doing the right thing!</h2><h2>Reduce, Reuse, Repudiate</h2><p>While recycling campaigns can help limit what heads to the landfill, scientists are now saying that it's masked the glaring problem of over-production and de-emphasized other waste reduction strategies that are far more sustainable.</p><p>In a <a href="https://theconversation.com/decades-of-public-messages-about-recycling-in-the-us-have-crowded-out-more-sustainable-ways-to-manage-waste-208924">new essay for <em>The Conversation</em></a>, an interdisciplinary group of researchers out of the University of Virginia that's been studying the psychology of waste found that many people over-emphasize the recycling aspect of the waste management industry's "Reduce, Reuse, Recycle" slogan. The result, they say, is a major backfiring as the public has come to mistakenly consider recycling a get-out-of-jail-free card, confusing which goods are actually recyclable in the first place and ignoring the growing waste production catastrophe.</p><p>In a series of experiments, the UV researchers first asked participants first to list "reduce," "reuse," and "recycle" by order of efficacy —&nbsp;the correct answer&nbsp;being the same one in the old slogan — finding that a whopping 78 percent got it wrong. In a second experiment, the researchers had participants use a computer program to virtually "sort" waste into recycling, compost, and landfill bins.&nbsp;Unfortunately, the outcome of that survey was even more stark, with many incorrectly putting non-recyclable waste, such as plastic bags and lightbulbs, into the virtual recycle bin.</p><h2>Cause and Effect</h2><p>While over-emphasizing or getting the recycling protocol wrong is an <a href="https://futurism.com/plastic-recycling-landfill-greenpeace">issue on its own</a>, its downstream effects have been devastating as <a href="https://futurism.com/microplastics-bottled-water">microplastics from consumer waste</a> continue to <a href="https://www.nytimes.com/2022/04/03/science/ocean-plastic-animals.html">pollute our oceans</a>, <a href="https://www.unep.org/news-and-stories/story/plastic-planet-how-tiny-plastic-particles-are-polluting-our-soil#:~:text=Microplastics%20can%20also%20interact%20with,Science%20Daily%20about%20the%20research.">land masses</a>, and <a href="https://futurism.com/neoscope/scientists-microplastics-human-lungs">bodies</a> — and as <a href="https://www.nature.com/articles/s41561-021-00690-8">greenhouse gases from the production of all this stuff</a> keep <a href="https://education.nationalgeographic.org/resource/greenhouse-effect/">throttling our planet</a>.</p><p>While lots of governmental bodies are, as the researchers note, <a href="https://www.plantswitch.com/single-use-plastic-ban/">attempting to stem</a> and <a href="https://www.seasidesustainability.org/post/the-u-s-progress-with-single-use-plastic-bans#:%7E:text=Currently%2C%20the%20U.S.%20has%20not,placed%20bans%20on%20plastic%20bags.">even ban</a> the proliferation of single-use plastic goods such as plastic straws and bags, the industries responsible for creating those landfill-bound items keep making more and more of them, and even their own <a href="https://theconversation.com/designing-batteries-for-easier-recycling-could-avert-a-looming-e-waste-crisis-146065">mitigation strategies</a> are voluntary.</p><p>The onus to reduce, reuse, and recycle ends up falling on consumers — who, as the aforementioned studies show, aren't as well-trained on how to do them as we should be.&nbsp;It's a status quo that does little to tackle the global waste crisis and ends up using a lot of <a href="https://www.inverse.com/science/plastic-recycling-report">logistical and worker power to boot</a>.</p><p><strong>More on waste: </strong><a href="https://futurism.com/the-byte/oceans-plastic-pollution-unprecedented-levels"><em>The Ocean's Plastic Pollution Has Spiked to "Unprecedented" Levels</em></a></p><br></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Where do you discuss computer related stuff now? (123 pts)]]></title>
            <link>https://lobste.rs/s/ih3cwj/where_do_you_discuss_computer_related</link>
            <guid>36873131</guid>
            <pubDate>Wed, 26 Jul 2023 04:34:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lobste.rs/s/ih3cwj/where_do_you_discuss_computer_related">https://lobste.rs/s/ih3cwj/where_do_you_discuss_computer_related</a>, See on <a href="https://news.ycombinator.com/item?id=36873131">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>With so many twitter alternatives, the death of forums, the explosion of chats / video platforms and the reddit shenanigans, we are once again facing this decades old question.</p>
<p>So far, only Hacker News has stood the test of time, and you cannot really discuss random topics, it’s more about link sharing. “Ask HN” is not the main dish, so to speak, and the heavy curating that keeps the quality up also limit the volume of interactions by design. Lobster is basically a mini HN.</p>
<p>So where do you go when you want to geek out?</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Netlify account has been suspended, I don’t know why? (143 pts)]]></title>
            <link>https://answers.netlify.com/t/my-netlify-account-ha-been-suspended-i-dont-know-why/97537</link>
            <guid>36872876</guid>
            <pubDate>Wed, 26 Jul 2023 03:54:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://answers.netlify.com/t/my-netlify-account-ha-been-suspended-i-dont-know-why/97537">https://answers.netlify.com/t/my-netlify-account-ha-been-suspended-i-dont-know-why/97537</a>, See on <a href="https://news.ycombinator.com/item?id=36872876">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting" id="main-outlet" role="main">
      <meta itemprop="headline" content="My Netlify account ha been suspended, I don't know why?">
        <meta itemprop="articleSection" content="Support">
      <meta itemprop="keywords" content="authentication">
      

          <div itemprop="articleBody" id="post_1">
              <h2><a name="authentication-error-1" href="#authentication-error-1"></a>Authentication Error</h2>
<p>Authenticating failed due to the following error: <strong>We already have a registered user with this email address. Log in to connect your GitHub account.</strong></p>
<p>Since yesterday I am getting this message as I try to login into my netlify account and due to this I am unable to access my account and also my some of important projects which I have deployed on netlify, failed to work properly which contains my portfolio and some of major projects which I had mentioned them in my resume.<br>
Can anyone please help me with this.</p>
            </div>
          <div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>You’re not the only one. Seems like its happening to a lot of people. Including myself.</p>

            

            

          </div>
          <div id="post_3" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Mine is suspended too, this is very bad out of nowhere without any explanation</p>

            

            

          </div>
          <div id="post_4" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Because of this issue I am unable to access all my deployments and they were not working properly. This will affect my candidature in various companies where I have applied for job profiles.</p>

            

            

          </div>
          <div itemprop="comment" id="post_5" itemscope="" itemtype="http://schema.org/Comment">
              <p>Those of you experiencing this:</p>
<p>Do you have free or paid accounts?</p>
<p>Do you host the kind of content that you may expect to be kicked off the system…</p>
<ul>
<li>Copyrighted material</li>
<li>Clone websites using the logos of the real businesses (e.g. Netflix, YouTube etc)</li>
<li>Cryptocurrency related</li>
</ul>
<p>From the posts I’ve seen, it appears related to logging in with two different providers, both git and email?<br>
Was that your experience?</p>
<p>It could be they’re legitimate account locks, but seeing so many occur with no response from Netlify has left me a little spooked to access my own paid account lest it happen to our business.</p>
            </div>
          <div itemprop="comment" id="post_6" itemscope="" itemtype="http://schema.org/Comment">
              <p>Its free account</p>
<p>For me its just app landing page nothing copyrighted, i built from scratch whole website</p>
            </div>
          <div id="post_7" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>It’s my personal site and no all content on the website is mine. I also have a free account.</p>

            

            

          </div>
          <div itemprop="comment" id="post_8" itemscope="" itemtype="http://schema.org/Comment">
              <p>Are these newly created accounts or have you had them for some time?</p>
<p>(Just trying to take a wild guess at what metrics the seemingly automated system is using to flag and lock your accounts.)</p>
            </div>
          <div id="post_9" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I’ve had the account for about 2-3 months I believe. I noticed I got the suspension roght after I signed in with my email. I I usually sign in with GitHub but then created an account with my email and that’s when it suspended right after.</p>

            

            

          </div>
          <div itemprop="comment" id="post_10" itemscope="" itemtype="http://schema.org/Comment">
              <p>This post mentions being told about “suspicious login behavior”:</p>

<p>This post mentions originally having a ‘git account’ and trying to use their ‘email’:</p>

<p>This post also mentions “suspicious login behavior”:</p>

<p>As does this post:</p>

<p>So anecdotally it certainly seems related to logging in, or logging in with both GitHub &amp; Email.</p>
            </div>
          <div id="post_11" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>How do you think we can get their attention so we can get our accounts back because this sucks lol.</p>

            

            

          </div>
          <div id="post_12" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Yes i was trying to login in , i entered my password 4-5 times wrong but i was not aware it will suspend my account</p>

            

            

          </div>
          <div itemprop="comment" id="post_13" itemscope="" itemtype="http://schema.org/Comment">
              
<p>You cannot, on a free account support is only administered by this forum.</p>
<p>That said, in my own experience support wasn’t faster via the “Pro” &amp; “Business” plans, as everything below an Enterprise account is considered “self-serve”.</p>
<p>Direct quotes from their sales team:</p>
<blockquote>
<p>“Pro” and “Business” are both self-serve tier plans the support offering there is the same.<br>
Currently the support offering is email support with no guaranteed response time.</p>
</blockquote>
<blockquote>
<p>Our self-serve tier is for prototyping, hobbyist or smaller projects and therefore we don’t expect projects with urgent needs or requirements that go beyond self-serve on our self-serve tier. Our enterprise plans are suited for for projects that require more timely responses and more specific attention.</p>
</blockquote>
<p>I can’t advise what the current response times are, but historically we encountered 72+ hour response times via email, with the forum being closer to 24-48 hours.</p>
<p>I anticipate someone at Netlify will spot this in the next few hours though, the question will be do they have to wait until their office hours tomorrow morning to be able to action anything.</p>
            </div>
          <div id="post_14" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I only use github for login in and as soon as I logged in to see my staging branch deploy the account lost authorization. Later did a sign up using the same email address and the account was flagged for “fraudulent behavior”.</p>

            

            

          </div>
          <div itemprop="comment" id="post_15" itemscope="" itemtype="http://schema.org/Comment">
              
<p>Were you trying to login via a git provider or via email directly with Netlify?</p>
<p>I may be misunderstanding this, but if the security measure for multiple failed password attempts “locks an account” (taking the sites down), it sounds like a great vector for attacking peoples sites on Netlify, all you would need to know is what email address they had their account under.</p>
            </div>
          <div id="post_16" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>By email, i am trying to move all my websites to <a href="http://render.com/" rel="noopener nofollow ugc">render.com</a> now …</p>

            

            

          </div>
          <div id="post_17" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I’ve never heard of render! we are moving to a Pro License on Vercel but I might just give render a try thanks! Any idea how is the support there?</p>

            

            

          </div>
          <div itemprop="comment" id="post_18" itemscope="" itemtype="http://schema.org/Comment">
              <p>Netlify will respond eventually, and I’m not sure what you’re building with, but other options for “static sites” are:</p>


<p><a href="https://firebase.google.com/products/hosting" target="_blank" rel="noopener nofollow ugc">https://firebase.google.com/products/hosting</a></p>
            </div>
          <div id="post_19" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Its look good but i am trying first time.</p>

            

            

          </div>
          <div itemprop="comment" id="post_20" itemscope="" itemtype="http://schema.org/Comment">
              <p>Other recent related posts:</p>




<p>More:</p>




            </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Which GPU(s) to Get for Deep Learning (155 pts)]]></title>
            <link>https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/</link>
            <guid>36872514</guid>
            <pubDate>Wed, 26 Jul 2023 02:41:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/">https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/</a>, See on <a href="https://news.ycombinator.com/item?id=36872514">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/CreativeWork"><div itemprop="text"><p>Deep learning is a field with intense computational requirements, and your choice of GPU will fundamentally determine your deep learning experience. But what features are important if you want to buy a new GPU? GPU RAM, cores, tensor cores, caches? How to make a cost-efficient choice? This blog post will delve into these questions, tackle common misconceptions, give you an intuitive understanding of how to think about GPUs, and will lend you advice, which will help you to make a choice that is right for you.</p> <p>This blog post is designed to give you different levels of understanding of GPUs and the new Ampere series GPUs from NVIDIA. You have the choice: (1) If you are not interested in the details of how GPUs work, what makes a GPU fast compared to a CPU, and what is unique about the new NVIDIA RTX 40 Ampere series, you can skip right to the performance and performance per dollar charts and the recommendation section. The cost/performance numbers form the core of the blog post and the content surrounding it explains the details of what makes up GPU performance.</p><p>(2) If you worry about specific questions, I have answered and addressed the most common questions and misconceptions in the later part of the blog post.</p><p>(3) If you want to get an in-depth understanding of how GPUs, caches, and Tensor Cores work, the best is to read the blog post from start to finish. You might want to skip a section or two based on your understanding of the presented topics.</p><h2><span id="Overview"><strong>Overview</strong></span></h2><p>This blog post is structured in the following way. First, I will explain what makes a GPU fast. I will discuss CPUs vs GPUs, Tensor Cores, memory bandwidth, and the memory hierarchy of GPUs and how these relate to deep learning performance. These explanations might help you get a more intuitive sense of what to look for in a GPU. I discuss the unique features of the new NVIDIA RTX 40 Ampere GPU series that are worth considering if you buy a GPU. From there, I make GPU recommendations for different scenarios. After that follows a Q&amp;A section of common questions posed to me in Twitter threads; in that section, I will also address common misconceptions and some miscellaneous issues, such as cloud vs desktop, cooling, AMD vs NVIDIA, and others.&nbsp;</p><h2><span id="How_do_GPUs_work">How do GPUs work?</span></h2><p>If you use GPUs frequently, it is useful to understand how they work. This knowledge will help you to undstand cases where are GPUs fast or slow. In turn, you might be able to understand better why you need a GPU in the first place and how other future hardware options might be able to compete. You can skip this section if you just want the useful performance numbers and arguments to help you decide which GPU to buy. The best high-level explanation for the question of how GPUs work is my following Quora answer:</p> <p><span data-name="Why-are-GPUs-well-suited-to-deep-learning/answer/Tim-Dettmers-1">Read <a data-width="560" data-height="260" href="https://www.quora.com/Why-are-GPUs-well-suited-to-deep-learning/answer/Tim-Dettmers-1" data-type="answer" data-id="21379913" data-key="bbb3732f88834d75dfa98d816eb9eccd" load-full-answer="False" data-embed="jqubkoa"></a><a href="https://www.quora.com/Tim-Dettmers-1">Tim Dettmers</a>‘ <a href="https://timdettmers.com/Why-are-GPUs-well-suited-to-deep-learning?top_ans=21379913">answer</a> to <a href="https://timdettmers.com/Why-are-GPUs-well-suited-to-deep-learning" ref="canonical"><span>Why are GPUs well-suited to deep learning?</span></a> on <a href="https://www.quora.com/">Quora</a></span></p><p>This is a high-level explanation that explains quite well why GPUs are better than CPUs for deep learning. If we look at the details, we can understand what makes one GPU better than another.</p><h2><span id="The_Most_Important_GPU_Specs_for_Deep_Learning_Processing_Speed">The Most Important GPU Specs for Deep Learning Processing Speed</span></h2><p>This section can help you build a more intuitive understanding of how to think about deep learning performance. This understanding will help you to evaluate future GPUs by yourself. This section is sorted by the importance of each component. Tensor Cores are most important, followed by memory bandwidth of a GPU, the cache hierachy, and only then FLOPS of a GPU.</p><h3><span id="Tensor_Cores">Tensor Cores</span></h3><p>Tensor Cores are tiny cores that perform very efficient matrix multiplication. Since the most expensive part of any deep neural network is matrix multiplication Tensor Cores are very useful. In fast, they are so powerful, that I do not recommend any GPUs that do not have Tensor Cores.</p><p>It is helpful to understand how they work to appreciate the importance of these computational units specialized for matrix multiplication. Here I will show you a simple example of A*B=C matrix multiplication, where all matrices have a size of 32×32, what a computational pattern looks like with and without Tensor Cores. This is a simplified example, and not the exact way how a high performing matrix multiplication kernel would be written, but it has all the basics. A CUDA programmer would take this as a first “draft” and then optimize it step-by-step with concepts like double buffering, register optimization, occupancy optimization, instruction-level parallelism, and many others, which I will not discuss at this point.</p><p>To understand this example fully, you have to understand the concepts of cycles. If a processor runs at 1GHz, it can do 10^9 cycles per second. Each cycle represents an opportunity for computation. However, most of the time, operations take longer than one cycle. Thus we essentially have a queue where the next operations needs to wait for the next operation to finish. This is also called the latency of the operation.</p><p>Here are some important latency cycle timings for operations. These times can change from GPU generation to GPU generation. <a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s33322/">These numbers are for Ampere GPUs</a>, which have relatively slow caches.</p><ul><li>Global memory access (up to 80GB): ~380 cycles</li><li>L2 cache: ~200 cycles</li><li>L1 cache or Shared memory access (up to 128 kb per Streaming Multiprocessor): ~34 cycles</li><li>Fused multiplication and addition, a*b+c (FFMA): 4 cycles</li><li>Tensor Core matrix multiply: 1 cycle</li></ul><p>Each operation is always performed by a pack of 32 threads. This pack is termed a warp of threads. Warps usually operate in a synchronous pattern — threads within a warp have to wait for each other. All memory operations on the GPU are optimized for warps. For example, loading from global memory happens at a granularity of 32*4 bytes, exactly 32 floats, exactly one float for each thread in a warp. We can have up to 32 warps = 1024 threads in a streaming multiprocessor (SM), the GPU-equivalent of a CPU core. The resources of an SM are divided up among all active warps. This means that sometimes we want to run fewer warps to have more registers/shared memory/Tensor Core resources per warp.</p><p>For both of the following examples, we assume we have the same computational resources. For this small example of a 32×32 matrix multiply, we use 8 SMs (about 10% of an RTX 3090) and 8 warps per SM.</p><p>To understand how the cycle latencies play together with resources like threads per SM and shared memory per SM, we now look at examples of matrix multiplication. While the following example roughly follows the sequence of computational steps of matrix multiplication for both with and without Tensor Cores, please note that these are very simplified examples. Real cases of matrix multiplication involve much larger shared memory tiles and slightly different computational patterns.</p><h4><span id="Matrix_multiplication_without_Tensor_Cores">Matrix multiplication without Tensor Cores</span></h4><p>If we want to do an A*B=C matrix multiply, where each matrix is of size 32×32, then we want to load memory that we repeatedly access into shared memory because its latency is about five times lower (200 cycles vs 34 cycles). A memory block in shared memory is often referred to as a memory tile or just a tile. Loading two 32×32 floats into a shared memory tile can happen in parallel by using 2*32 warps. We have 8 SMs with 8 warps each, so due to parallelization, we only need to do a single sequential load from global to shared memory, which takes 200 cycles.</p><p>To do the matrix multiplication, we now need to load a vector of 32 numbers from shared memory A and shared memory B and perform a fused multiply-and-accumulate (FFMA). Then store the outputs in registers C. We divide the work so that each SM does 8x dot products (32×32) to compute 8 outputs of C. Why this is exactly 8 (4 in older algorithms) is very technical. I recommend Scott Gray’s blog post on <a href="https://github.com/NervanaSystems/maxas/wiki/SGEMM">matrix multiplication</a> to understand this. This means we have 8x shared memory accesses at the cost of 34 cycles each and 8 FFMA operations (32 in parallel), which cost 4 cycles each. In total, we thus have a cost of:</p><p>200 cycles (global memory) + 8*34 cycles (shared memory) + 8*4 cycles (FFMA) = 504 cycles</p><p>Let’s look at the cycle cost of using Tensor Cores.</p><h4><span id="Matrix_multiplication_with_Tensor_Cores">Matrix multiplication with Tensor Cores</span></h4><p>With Tensor Cores, we can perform a 4×4 matrix multiplication in one cycle. To do that, we first need to get memory into the Tensor Core. Similarly to the above, we need to read from global memory (200 cycles) and store in shared memory. To do a 32×32 matrix multiply, we need to do 8×8=64 Tensor Cores operations. A single SM has 8 Tensor Cores. So with 8 SMs, we have 64 Tensor Cores — just the number that we need! We can transfer the data from shared memory to the Tensor Cores with 1 memory transfers (34 cycles) and then do those 64 parallel Tensor Core operations (1 cycle). This means the total cost for Tensor Cores matrix multiplication, in this case, is:</p><p>200 cycles (global memory) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 235 cycles.</p><p>Thus we reduce the matrix multiplication cost significantly from 504 cycles to 235 cycles via Tensor Cores. In this simplified case, the Tensor Cores reduced the cost of both shared memory access and FFMA operations.&nbsp;</p><p>This example is simplified, for example, usually each thread needs to calculate which memory to read and write to as you transfer data from global memory to shared memory. With the new Hooper (H100) architectures we additionally have the Tensor Memory Accelerator (TMA) compute these indices in hardware and thus help each thread to focus on more computation rather than computing indices.</p><h4><span id="Matrix_multiplication_with_Tensor_Cores_and_Asynchronous_copies_RTX_30RTX_40_and_TMA_H100">Matrix multiplication with Tensor Cores and Asynchronous copies (RTX 30/RTX 40) and TMA (H100)</span></h4><p>The RTX 30 Ampere and RTX 40 Ada series GPUs additionally have support to perform asynchronous transfers between global and shared memory. The H100 Hopper GPU extends this further by introducing the Tensor Memory Accelerator (TMA) unit. the TMA unit combines asynchronous copies and index calculation for read and writes simultaneously — so each thread no longer needs to calculate which is the next element to read and each thread can focus on doing more matrix multiplication calculations. This looks as follows.</p><p>The TMA unit fetches memory from global to shared memory (200 cycles). Once the data arrives, the TMA unit fetches the next block of data asynchronously from global memory. While this is happening, the threads load data from shared memory and perform the matrix multiplication via the tensor core. Once the threads are finished they wait for the TMA unit to finish the next data transfer, and the sequence repeats.</p><p>As such, due to the asynchronous nature, the second global memory read by the TMA unit is already progressing as the threads process the current shared memory tile.  This means, the second read takes only 200 – 34 – 1 = 165 cycles.</p><p>Since we do many reads, only the first memory access will be slow and all other memory accesses will be partially overlapped with the TMA unit. Thus on average, we reduce the time by 35 cycles.</p><p>165 cycles (wait for async copy to finish) + 34 cycles (shared memory) + 1 cycle (Tensor Core) = 200 cycles.</p><p>Which accelerates the matrix multiplication by another 15%.</p><p>From these examples, it becomes clear why the next attribute, memory bandwidth, is so crucial for Tensor-Core-equipped GPUs. Since global memory is the by far the largest cycle cost for matrix multiplication with Tensor Cores, we would even have faster GPUs if the global memory latency could be reduced. We can do this by either increasing the clock frequency of the memory (more cycles per second, but also more heat and higher energy requirements) or by increasing the number of elements that can be transferred at any one time (bus width).</p><h3><span id="Memory_Bandwidth">Memory Bandwidth</span></h3><p>From the previous section, we have seen that Tensor Cores are very fast. So fast, in fact, that they are idle most of the time as they are waiting for memory to arrive from global memory. For example, during GPT-3-sized training, which uses huge matrices — the larger, the better for Tensor Cores — we have a Tensor Core TFLOPS utilization of about 45-65%, meaning that even for the large neural networks about 50% of the time, Tensor Cores are idle.</p><p>This means that when comparing two GPUs with Tensor Cores, one of the single best indicators for each GPU’s performance is their memory bandwidth. For example, The A100 GPU has 1,555 GB/s memory bandwidth vs the 900 GB/s of the V100. As such, a basic estimate of speedup of an A100 vs V100 is 1555/900 = 1.73x.</p><h3><span id="L2_Cache_Shared_Memory_L1_Cache_Registers">L2 Cache / Shared Memory / L1 Cache / Registers</span></h3><p>Since memory transfers to the Tensor Cores are the limiting factor in performance, we are looking for other GPU attributes that enable faster memory transfer to Tensor Cores. L2 cache, shared memory, L1 cache,  and amount of registers used are all related. To understand how a memory hierarchy enables faster memory transfers, it helps to understand how matrix multiplication is performed on a GPU.</p><p>To perform matrix multiplication, we exploit the memory hierarchy of a GPU that goes from slow global memory, to faster L2 memory, to fast local shared memory, to lightning-fast registers. However, the faster the memory, the smaller it is.</p><p>While logically, L2 and L1 memory are the same, L2 cache is larger and thus the average physical distance that need to be traversed to retrieve a cache line is larger. You can see the L1 and L2 caches as organized warehouses where you want to retrieve an item. You know where the item is, but to go there takes on average much longer for the larger warehouse. This is the essential difference between L1 and L2 caches. Large = slow, small = fast.</p><p>For matrix multiplication we can use this hierarchical separate into smaller and smaller and thus faster and faster chunks of memory to perform very fast matrix multiplications. For that, we need to chunk the big matrix multiplication into smaller sub-matrix multiplications. These chunks are called memory tiles, or often for short just tiles.</p><p>We perform matrix multiplication across these smaller tiles in local shared memory that is fast and close to the streaming multiprocessor (SM) — the equivalent of a CPU core. With Tensor Cores, we go a step further: We take each tile and load a part of these tiles into Tensor Cores which is directly addressed by registers. A matrix memory tile in L2 cache is 3-5x faster than global GPU memory (GPU RAM), shared memory is ~7-10x faster than the global GPU memory, whereas the Tensor Cores’ registers are ~200x faster than the global GPU memory.&nbsp;</p><p>Having larger tiles means we can reuse more memory. I wrote about this in detail in my <a href="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/">TPU vs GPU</a> blog post. In fact, you can see TPUs as having very, very, large tiles for each Tensor Core. As such, TPUs can reuse much more memory with each transfer from global memory, which makes them a little bit more efficient at matrix multiplications than GPUs.</p><p>Each tile size is determined by how much memory we have per streaming multiprocessor (SM) and how much we L2 cache we have across all SMs. We have the following shared memory sizes on the following architectures:</p><ul><li>Volta (Titan V): 128kb shared memory / 6 MB L2</li><li>Turing (RTX 20s series): 96 kb shared memory / 5.5 MB L2</li><li>Ampere (RTX 30s series): 128 kb shared memory / 6 MB L2</li><li>Ada (RTX 40s series): 128 kb shared memory / 72 MB L2</li></ul><p>We see that Ada has a much larger L2 cache allowing for larger tile sizes, which reduces global memory access. For example, for BERT large during training, the input and weight matrix of any matrix multiplication fit neatly into the L2 cache of Ada (but not other Us). As such, data needs to be loaded from global memory only once and then data is available throught the L2 cache, making matrix multiplication about 1.5 – 2.0x faster for this architecture for Ada. For larger models the speedups are lower during training but certain sweetspots exist which may make certain models much faster. Inference, with a batch size larger than 8 can also benefit immensely from the larger L2 caches.</p><h2><span id="Estimating_Ada_Hopper_Deep_Learning_Performance">Estimating Ada / Hopper Deep Learning Performance</span></h2><p>This section is for those who want to understand the more technical details of how I derive the performance estimates for Ampere GPUs. If you do not care about these technical aspects, it is safe to skip this section.</p><h3><span id="Practical_Ada_Hopper_Speed_Estimates">Practical Ada / Hopper Speed Estimates</span></h3><p>Suppose we have an estimate for one GPU of a GPU-architecture like Hopper, Ada, Ampere, Turing, or Volta. It is easy to extrapolate these results to other GPUs from the same architecture/series. Luckily, NVIDIA already <a href="https://developer.nvidia.com/deep-learning-performance-training-inference">benchmarked the A100 vs V100 vs H100</a> across a wide range of computer vision and natural language understanding tasks. Unfortunately, NVIDIA made sure that these numbers are not directly comparable by using different batch sizes and the number of GPUs whenever possible to favor results for the H100 GPU. So in a sense, the benchmark numbers are partially honest, partially marketing numbers. In general, you could argue that using larger batch sizes is fair, as the H100/A100 GPU has more memory. Still, to compare GPU architectures, we should evaluate unbiased memory performance with the same batch size.</p><p>To get an unbiased estimate, we can scale the data center GPU results in two ways: (1) account for the differences in batch size, (2) account for the differences in using 1 vs 8 GPUs. We are lucky that we can find such an estimate for both biases in the data that NVIDIA provides.&nbsp;</p><p>Doubling the batch size increases throughput in terms of images/s (CNNs) by 13.6%. I benchmarked the same problem for transformers on my RTX Titan and found, surprisingly, the very same result: 13.5% — it appears that this is a robust estimate.</p><p>As we parallelize networks across more and more GPUs, we lose performance due to some networking overhead. The A100 8x GPU system has better networking (NVLink 3.0) than the V100 8x GPU system (NVLink 2.0) — this is another confounding factor. Looking directly at the data from NVIDIA, we can find that for CNNs, a system with 8x A100 has a 5% lower overhead than a system of 8x V100. This means if going from 1x A100 to 8x A100 gives you a speedup of, say, 7.00x, then going from 1x V100 to 8x V100 only gives you a speedup of 6.67x.&nbsp; For transformers, the figure is 7%.&nbsp;</p><p>Using these figures, we can estimate the speedup for a few specific deep learning architectures from the direct data that NVIDIA provides. The Tesla A100 offers the following speedup over the Tesla V100:</p><ul><li>SE-ResNeXt101: 1.43x</li><li>Masked-R-CNN: 1.47x</li><li>Transformer (12 layer, Machine Translation, WMT14 en-de): 1.70x</li></ul><p>Thus, the figures are a bit lower than the theoretical estimate for computer vision. This might be due to smaller tensor dimensions, overhead from operations that are needed to prepare the matrix multiplication like img2col or Fast Fourier Transform (FFT), or operations that cannot saturate the GPU (final layers are often relatively small). It could also be artifacts of the specific architectures (grouped convolution).</p><p>The practical transformer estimate is very close to the theoretical estimate. This is probably because algorithms for huge matrices are very straightforward. I will use these practical estimates to calculate the cost efficiency of GPUs.</p><h3><span id="Possible_Biases_in_Estimates">Possible Biases in Estimates</span></h3><p>The estimates above are for H100, A100 , and V100 GPUs. In the past, NVIDIA sneaked unannounced performance degradations into the “gaming” RTX GPUs: (1) Decreased Tensor Core utilization, (2) gaming fans for cooling, (3) disabled peer-to-peer GPU transfers. It might be possible that there are unannounced performance degradations in the RTX 40 series compared to the full Hopper H100.</p><p>As of now, one of these degradations was found for Ampere GPUs: Tensor Core performance was decreased so that RTX 30 series GPUs are not as good as Quadro cards for deep learning purposes. This was also done for the RTX 20 series, so it is nothing new, but this time it was also done for the Titan equivalent card, the RTX 3090. The RTX Titan did not have performance degradation enabled.</p><p>Currently, no degradation for Ada GPUs are known, but I update this post with news on this and let my followers on <a href="https://twitter.com/Tim_Dettmers">twitter</a> know.</p><h2><span id="Advantages_and_Problems_for_RTX40_and_RTX_30_Series"><strong>Advantages and Problems for RTX40 and RTX 30 Series</strong></span></h2><p>The new NVIDIA Ampere RTX 30 series has additional benefits over the NVIDIA Turing RTX 20 series, such as sparse network training and inference. Other features, such as the new data types, should be seen more as an ease-of-use-feature as they provide the same performance boost as Turing does but without any extra programming required.</p><p>The Ada RTX 40 series has even further advances like 8-bit Float (FP8) tensor cores. The RTX 40 series also has similar power and temperature issues compared to the RTX 30. The issue of melting power connector cables in the RTX 40 can be easily prevented by connecting the power cable correctly.</p><h3><span id="Sparse_Network_Training">Sparse Network Training</span></h3><p>Ampere allows for fine-grained structure automatic sparse matrix multiplication at dense speeds. How does this work? Take a weight matrix and slice it into pieces of 4 elements. Now imagine 2 elements of these 4 to be zero. Figure 1 shows how this could look like.</p><p>When you multiply this sparse weight matrix with some dense inputs, the sparse matrix tensor core feature in Ampere automatically compresses the sparse matrix to a dense representation that is half the size as can be seen in Figure 2. After this compression, the densely compressed matrix tile is fed into the tensor core which computes a matrix multiplication of twice the usual size. This effectively yields a 2x speedup since the bandwidth requirements during matrix multiplication from shared memory are halved.</p><figure><img data-lazy-fallback="1" data-attachment-id="934" data-permalink="https://timdettmers.com/sparse_matmul/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?fit=1055%2C638&amp;ssl=1" data-orig-size="1055,638" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Sparse Matrix Multiplication in Ampere" data-image-description="<p>Figure X: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. Figure is taken from Jeff Pool’s GTC 2020 presentation on  <a href=&quot;https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;>Accelerating Sparsity in the NVIDIA Ampere Architecture</a> by the courtesy of NVIDIA.</p>
" data-image-caption="<p>Figure X: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. Figure is taken from Jeff Pool’s GTC 2020 presentation on  <a href=&quot;https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;>Accelerating Sparsity in the NVIDIA Ampere Architecture</a> by the courtesy of NVIDIA.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?fit=300%2C181&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?fit=1024%2C619&amp;ssl=1" width="1024" height="619" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=1024%2C619&amp;ssl=1" alt="Figure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. " srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=1024%2C619&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=300%2C181&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=768%2C464&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?w=1055&amp;ssl=1 1055w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=1024%2C619&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=300%2C181&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=768%2C464&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?w=1055&amp;ssl=1 1055w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/sparse_matmul.png?resize=1024%2C619&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Figure 2: The sparse matrix is compressed to a dense representation before the matrix multiplication is performed. The figure is taken from Jeff Pool’s GTC 2020 presentation on <a href="https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s22085-accelerating-sparsity-in-the-nvidia-ampere-architecture%E2%80%8B.pdf" rel="noopener noreferrer" target="_blank">Accelerating Sparsity in the NVIDIA Ampere Architecture</a> by the courtesy of NVIDIA.</figcaption></figure><p>I was working on <a href="https://arxiv.org/abs/1907.04840">sparse network training</a> in my research and I also wrote a <a href="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/">blog post about sparse training</a>. One criticism of my work was that “You reduce the FLOPS required for the network, but it does not yield speedups because GPUs cannot do fast sparse matrix multiplication.” Well, with the addition of the sparse matrix multiplication feature for Tensor Cores, my algorithm, or <a href="https://arxiv.org/abs/2002.03231" rel="nofollow">other</a> <a href="https://arxiv.org/abs/2002.07376" rel="nofollow">sparse</a> <a href="https://arxiv.org/abs/1911.11134" rel="nofollow">training</a> <a href="https://arxiv.org/abs/1902.05967" rel="nofollow">algorithms</a>, now actually provide speedups of up to 2x during training.</p><figure><a href="https://arxiv.org/abs/1907.04840"><img data-lazy-fallback="1" data-attachment-id="779" data-permalink="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/sparse_momentum/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=1096%2C528&amp;ssl=1" data-orig-size="1096,528" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Sparse Momentum Dettmers &amp; Zettlemoyer 2019" data-image-description="<p>Figure X: The sparse training algorithm developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layers.</p>
" data-image-caption="<p>Figure X: The sparse training algorithm developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layers.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=300%2C145&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?fit=1024%2C493&amp;ssl=1" width="1024" height="493" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=1024%2C493&amp;ssl=1" alt="Figure 3: The sparse training algorithm that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my sparse training blog post." srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=1024%2C493&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=300%2C145&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=768%2C370&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?w=1096&amp;ssl=1 1096w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=1024%2C493&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=300%2C145&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=768%2C370&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?w=1096&amp;ssl=1 1096w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2019/07/sparse_momentum.png?resize=1024%2C493&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Figure 3: The <a href="https://arxiv.org/abs/1907.04840">sparse training algorithm</a> that I developed has three stages: (1) Determine the importance of each layer. (2) Remove the smallest, unimportant weights. (3) Grow new weights proportional to the importance of each layer. Read more about my work in my <a href="https://timdettmers.com/2019/07/11/sparse-networks-from-scratch/">sparse training blog post</a>.</figcaption></figure><p>While this feature is still experimental and training sparse networks are not commonplace yet, having this feature on your GPU means you are ready for the future of sparse training.</p><h3><span id="Low-precision_Computation">Low-precision Computation</span></h3><p>In my work, I’ve previously shown that new data types can improve stability during <a href="https://arxiv.org/abs/1511.04561">low-precision backpropagation</a>.</p><figure><img data-lazy-fallback="1" data-attachment-id="941" data-permalink="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/8-bit_data_types/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?fit=869%2C268&amp;ssl=1" data-orig-size="869,268" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="8-bit_data_types" data-image-description="<p>Figure X: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent large numbers and small numbers with high precision.</p>
" data-image-caption="<p>Figure X: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent large numbers and small numbers with high precision.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?fit=300%2C93&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?fit=869%2C268&amp;ssl=1" width="869" height="268" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=869%2C268&amp;ssl=1" alt="Figure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision." srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?w=869&amp;ssl=1 869w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=300%2C93&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=768%2C237&amp;ssl=1 768w" sizes="(max-width: 869px) 100vw, 869px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?w=869&amp;ssl=1 869w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=300%2C93&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=768%2C237&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/8-bit_data_types.png?resize=869%2C268&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Figure 4: Low-precision deep learning 8-bit datatypes that I developed. Deep learning training benefits from highly specialized data types. My dynamic tree datatype uses a dynamic bit that indicates the beginning of a binary bisection tree that quantized the range [0, 0.9] while all previous bits are used for the exponent. This allows to dynamically represent numbers that are both large and small with high precision.</figcaption></figure><p>Currently, if you want to have stable backpropagation with 16-bit floating-point numbers (FP16), the big problem is that ordinary FP16 data types only support numbers in the range [-65,504, 65,504]. If your gradient slips past this range, your gradients explode into NaN values. To prevent this during FP16 training, we usually perform loss scaling where you multiply the loss by a small number before backpropagating to prevent this gradient explosion.&nbsp;</p><p>The BrainFloat 16 format (BF16) uses more bits for the exponent such that the range of possible numbers is the same as for FP32: [-3*10^38, 3*10^38]. BF16 has less precision, that is significant digits, but gradient precision is not that important for learning. So what BF16 does is that you no longer need to do any loss scaling or worry about the gradient blowing up quickly. As such, we should see an increase in training stability by using the BF16 format as a slight loss of precision.</p><p>What this means for you: With BF16 precision, training might be more stable than with FP16 precision while providing the same speedups. With 32-bit TensorFloat (TF32) precision, you get near FP32 stability while giving the speedups close to FP16. The good thing is, to use these data types, you can just replace FP32 with TF32 and FP16 with BF16 — no code changes required!</p><p>Overall, though, these new data types can be seen as lazy data types in the sense that you could have gotten all the benefits with the old data types with some additional programming efforts (proper loss scaling, initialization, normalization, using Apex). As such, these data types do not provide speedups but rather improve ease of use of low precision for training.</p><h3><span id="Fan_Designs_and_GPUs_Temperature_Issues">Fan Designs  and GPUs Temperature Issues</span></h3><p>While the new fan design of the RTX 30 series performs very well to cool the GPU, different fan designs of non-founders edition GPUs might be more problematic. If your GPU heats up beyond 80C, it will throttle itself and slow down its computational speed / power. This overheating can happen in particular if you stack multiple GPUs next to each other. A solution to this is to use PCIe extenders to create space between GPUs.</p><p>Spreading GPUs with PCIe extenders is very effective for cooling, and other fellow PhD students at the University of Washington and I use this setup with great success. It does not look pretty, but it keeps your GPUs cool! This has been running with no problems at all for 4 years now. It can also help if you do not have enough space to fit all GPUs in the PCIe slots. For example, if you can find the space within a desktop computer case, it might be possible to buy standard 3-slot-width RTX 4090 and spread them with PCIe extenders within the case. With this, you might solve both the space issue and cooling issue for a 4x RTX 4090 setup with a single simple solution.</p><div><figure><img data-lazy-fallback="1" data-attachment-id="861" data-permalink="https://timdettmers.com/4x_rtx2080ti_desktop_extenders/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?fit=1920%2C2560&amp;ssl=1" data-orig-size="1920,2560" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.9&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Redmi Note 5&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1557156443&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;3.94&quot;,&quot;iso&quot;:&quot;1250&quot;,&quot;shutter_speed&quot;:&quot;0.05&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="4x_RTX2080Ti_desktop_extenders" data-image-description="<p>4x GPUs with PCIe extenders</p>
" data-image-caption="<p>4x GPUs with PCIe extenders</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?fit=225%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?fit=768%2C1024&amp;ssl=1" width="768" height="1024" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders.jpg?resize=768%2C1024&amp;ssl=1" alt="Figure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 2 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs." srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=1152%2C1536&amp;ssl=1 1152w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=1536%2C2048&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?w=1920&amp;ssl=1 1920w" sizes="(max-width: 768px) 100vw, 768px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=768%2C1024&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=225%2C300&amp;ssl=1 225w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=1152%2C1536&amp;ssl=1 1152w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?resize=1536%2C2048&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders-scaled.jpg?w=1920&amp;ssl=1 1920w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/4x_RTX2080Ti_desktop_extenders.jpg?resize=768%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Figure 5: 4x GPUs with PCIe extenders. It looks like a mess, but it is very effective for cooling. I used this rig for 4 years and cooling is excellent despite problematic RTX 2080 Ti Founders Edition GPUs.</figcaption></figure></div><h3><span id="3-slot_Design_and_Power_Issues">3-slot Design and Power Issues</span></h3><p>The RTX 3090 and RTX 4090 are 3-slot GPUs, so one will not be able to use it in a 4x setup with the default fan design from NVIDIA. This is kind of justified because it runs at over 350W TDP, and it will be difficult to cool in a multi-GPU 2-slot setting. The RTX 3080 is only slightly better at 320W TDP, and cooling a 4x RTX 3080 setup will also be very difficult.</p><p>It is also difficult to power a 4x 350W = 1400W or 4x 450W = 1800W system in the 4x RTX 3090 or 4x RTX 4090 case. Power supply units (PSUs) of 1600W are readily available, but having only 200W to power the <a href="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">CPU and motherboard</a> can be too tight. The components’ maximum power is only used if the components are fully utilized, and in deep learning, the CPU is usually only under weak load. With that, a 1600W PSU might work quite well with a 4x RTX 3080 build, but for a 4x RTX 3090 build, it is better to look for high wattage PSUs (+1700W). Some of my followers have had great success with cryptomining PSUs — have a look in the comment section for more info about that. Otherwise, it is important to note that not all outlets support PSUs above 1600W, especially in the US. This is the reason why in the US, there are currently few standard desktop PSUs above 1600W on the market. If you get a server or cryptomining PSUs, beware of the form factor — make sure it fits into your computer case.</p><h3><span id="Power_Limiting_An_Elegant_Solution_to_Solve_the_Power_Problem">Power Limiting: An Elegant Solution to Solve the Power Problem?</span></h3><p>It is possible to set a power limit on your GPUs. So you would be able to programmatically set the power limit of an RTX 3090 to 300W instead of their standard 350W. In a 4x GPU system, that is a saving of 200W, which might just be enough to build a 4x RTX 3090 system with a 1600W PSU feasible. It also helps to keep the GPUs cool. So setting a power limit can solve the two major problems of a 4x RTX 3080 or 4x RTX 3090 setups, cooling, and power, at the same time. For a 4x setup, you still need effective blower GPUs (and the standard design may prove adequate for this), but this resolves the PSU problem.</p><figure><img data-lazy-fallback="1" data-attachment-id="933" data-permalink="https://timdettmers.com/power_limit_nvidia_smi/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?fit=1187%2C1195&amp;ssl=1" data-orig-size="1187,1195" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Power Limit Cooling Effect NVIDIA SMI" data-image-description="<p>Figure X: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.</p>
" data-image-caption="<p>Figure X: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?fit=298%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?fit=1017%2C1024&amp;ssl=1" width="1017" height="1024" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=1017%2C1024&amp;ssl=1" alt="Figure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent." srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=1017%2C1024&amp;ssl=1 1017w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=298%2C300&amp;ssl=1 298w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=768%2C773&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?w=1187&amp;ssl=1 1187w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=1017%2C1024&amp;ssl=1 1017w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=298%2C300&amp;ssl=1 298w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=768%2C773&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?w=1187&amp;ssl=1 1187w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2020/09/power_limit_nvidia_smi.png?resize=1017%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Figure 6: Reducing the power limit has a slight cooling effect. Reducing the RTX 2080 Ti power limit by 50-60 W decreases temperatures slightly and fans run more silent.</figcaption></figure><p>You might ask, “Doesn’t this slow down the GPU?” Yes, it does, but the question is by how much. I benchmarked the 4x RTX 2080 Ti system shown in Figure 5 under different power limits to test this. I benchmarked the time for 500 mini-batches for BERT Large during inference (excluding the softmax layer). I choose BERT Large inference since, from my experience, this is the deep learning model that stresses the GPU the most. As such, I would expect power limiting to have the most massive slowdown for this model. As such, the slowdowns reported here are probably close to the maximum slowdowns that you can expect. The results are shown in Figure 7.</p><figure><img data-lazy-fallback="1" data-attachment-id="939" data-permalink="https://timdettmers.com/rtx-2080-ti-slowdown-vs-power-limit/" data-orig-file="https://timdettmers.com/wp-content/uploads/2020/09/RTX-2080-Ti-Slowdown-vs-Power-Limit.svg" data-orig-size="853,703" data-comments-opened="1" data-image-meta="[]" data-image-title="RTX 2080 Ti Slowdown vs Power Limit" data-image-description="<p>Figure 6: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).</p>
" data-image-caption="<p>Figure 6: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).</p>
" data-medium-file="https://timdettmers.com/wp-content/uploads/2020/09/RTX-2080-Ti-Slowdown-vs-Power-Limit.svg" data-large-file="https://timdettmers.com/wp-content/uploads/2020/09/RTX-2080-Ti-Slowdown-vs-Power-Limit.svg" width="853" height="703" src="https://timdettmers.com/wp-content/uploads/2020/09/RTX-2080-Ti-Slowdown-vs-Power-Limit.svg" alt="Figure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer)." srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Figure 7: Measured slowdown for a given power limit on an RTX 2080 Ti. Measurements taken are mean processing times for 500 mini-batches of BERT Large during inference (excluding softmax layer).</figcaption></figure><p>As we can see, setting the power limit does not seriously affect performance. Limiting the power by 50W — more than enough to handle 4x RTX 3090 — decreases performance by only 7%.</p><h3><span id="RTX_4090s_and_Melting_Power_Connectors_How_to_Prevent_Problems">RTX 4090s and Melting Power Connectors: How to Prevent Problems</span></h3><p>There was a misconception that RTX 4090 power cables melt because they were bent. However, it was found that only 0.1% of users had this problem and the problem occured due to user error. Here a video that shows that the main problem is that <a href="https://www.youtube.com/watch?v=ig2px7ofKhQ&amp;t=1065s">cables were not inserted correctly</a>.</p><p>So using RTX 4090 cards is perfectly safe if you follow the following install instructions:</p><ol><li>If you use an old cable or old GPU make sure the contacts are free of debri / dust.</li><li>Use the power connector and stick it into the socket until you hear a *click* — this is the most important part.</li><li>Test for good fit by wiggling the power cable left to right. The cable should not move.</li><li>Check the contact with the socket visually, there should be no gap between cable and socket.</li></ol><h3><span id="8-bit_Float_Support_in_H100_and_RTX_40_series_GPUs">8-bit Float Support in H100 and RTX 40 series GPUs</span></h3><p>The support of the 8-bit Float (FP8) is a huge advantage for the RTX 40 series and H100 GPUs. With 8-bit inputs it allows you to load the data for matrix multiplication twice as fast, you can store twice as much matrix elements in your caches which in the Ada and Hopper architecture are very large, and now with FP8 tensor cores you get 0.66 PFLOPS of compute for a RTX 4090 — this is more FLOPS then the entirety of the worlds fastest supercomputer in year 2007. 4x RTX 4090 with FP8 compute rival the faster supercomputer in the world in year 2010 (deep learning started to work just in 2009).</p><p>The main problem with using 8-bit precision is that transformers can get very unstable with so few bits and crash during training or generate non-sense during inference. I have written a <a href="https://arxiv.org/abs/2208.07339">paper about the emergence of instabilities in large language models</a> and I also written a more accessible <a href="https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/">blog post</a>.</p><p>The main take-way is this: Using 8-bit instead of 16-bit makes things very unstable, but if you keep a couple of dimensions in high precision everything works just fine.</p><figure><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="1146" data-permalink="https://timdettmers.com/llm_int8_zeroshot_emergence/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?fit=1808%2C1462&amp;ssl=1" data-orig-size="1808,1462" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="LLM_int8_zeroshot_emergence" data-image-description="" data-image-caption="<p>Main results from my work on 8-bit matrix multiplication for Large Language Models (LLMs). We can see that the best 8-bit baseline fails to deliver good zero-shot performance. The method that I developed, LLM.int8(), can perform Int8 matrix multiplication with the same results as the 16-bit baseline.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?fit=300%2C243&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?fit=1024%2C828&amp;ssl=1" width="1024" height="828" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=1024%2C828&amp;ssl=1" alt="" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=1024%2C828&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=300%2C243&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=768%2C621&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=1536%2C1242&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?w=1808&amp;ssl=1 1808w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=1024%2C828&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=300%2C243&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=768%2C621&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=1536%2C1242&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?w=1808&amp;ssl=1 1808w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/LLM_int8_zeroshot_emergence.png?resize=1024%2C828&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Main results from my work on 8-bit matrix multiplication for Large Language Models (LLMs). We can see that the best 8-bit baseline fails to deliver good zero-shot performance. The method that I developed, LLM.int8(), can perform Int8 matrix multiplication with the same results as the 16-bit baseline.</figcaption></figure><p>But Int8 was already supported by the RTX 30 / A100 / Ampere generation GPUs, why is FP8 in the RTX 40 another big upgrade? The FP8 data type is much more stable than the Int8 data type and its easy to use it in functions like layer norm or non-linear functions, which are difficult to do with Integer data types. This will make it very straightforward to use it in training and inference. I think this will make FP8 training and inference relatively common in a couple of months.</p><p>If you want to read more about the advantages of Float vs Integer data types you can read my recent paper about <a href="https://arxiv.org/abs/2212.09720">k-bit inference scaling laws</a>. Below you can see one relevant main result for Float vs Integer data types from this paper. We can see that bit-by-bit, the FP4 data type preserve more information than Int4 data type and thus improves the mean LLM zeroshot accuracy across 4 tasks.</p><figure><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="1151" data-permalink="https://timdettmers.com/pythia_4bit_datatypes2/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?fit=1159%2C875&amp;ssl=1" data-orig-size="1159,875" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pythia_4bit_datatypes2" data-image-description="" data-image-caption="<p>4-bit Inference scaling laws for Pythia Large Language Models for different data types. We see that bit-by-bit, 4-bit float data types have better zeroshot accuracy compared to the Int4 data types.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?fit=300%2C226&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?fit=1024%2C773&amp;ssl=1" width="1024" height="773" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?resize=1024%2C773&amp;ssl=1" alt="" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?resize=1024%2C773&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?resize=768%2C580&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?w=1159&amp;ssl=1 1159w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?resize=1024%2C773&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?resize=300%2C226&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?resize=768%2C580&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?w=1159&amp;ssl=1 1159w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/pythia_4bit_datatypes2.png?resize=1024%2C773&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>4-bit Inference scaling laws for Pythia Large Language Models for different data types. We see that bit-by-bit, 4-bit float data types have better zeroshot accuracy compared to the Int4 data types.</figcaption></figure><h2><span id="Raw_Performance_Ranking_of_GPUs">Raw Performance Ranking of GPUs</span></h2><p>Below we see a chart of raw relevative performance across all GPUs. We see that there is a gigantic gap in 8-bit performance of H100 GPUs and old cards that are optimized for 16-bit performance.</p><figure><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="1160" data-permalink="https://timdettmers.com/gpus_ada_raw_performance3/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?fit=1703%2C1673&amp;ssl=1" data-orig-size="1703,1673" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="GPUS_Ada_raw_performance3" data-image-description="" data-image-caption="<p>Shown is raw relative performance of GPUs. For example, an RTX 4090 has about 0.33x performance of a H100 SMX for 8-bit inference. In other words, a H100 SMX is three times faster for 8-bit inference compared to a RTX 4090.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?fit=300%2C295&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?fit=1024%2C1006&amp;ssl=1" width="1024" height="1006" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=1024%2C1006&amp;ssl=1" alt="" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=1024%2C1006&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=300%2C295&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=768%2C754&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=1536%2C1509&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?w=1703&amp;ssl=1 1703w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=1024%2C1006&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=300%2C295&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=768%2C754&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=1536%2C1509&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?w=1703&amp;ssl=1 1703w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUS_Ada_raw_performance3.png?resize=1024%2C1006&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Shown is raw relative transformer performance of GPUs. For example, an RTX 4090 has about 0.33x performance of a H100 SMX for 8-bit inference. In other words, a H100 SMX is three times faster for 8-bit inference compared to a RTX 4090.</figcaption></figure><p>For this data, I did not model 8-bit compute for older GPUs.  I did so, because 8-bit Inference and training are much more effective on Ada/Hopper GPUs because of the 8-bit Float data type and Tensor Memory Accelerator (TMA) which saves the overhead of computing read/write indices which is particularly helpful for 8-bit matrix multiplication. Ada/Hopper also have FP8 support, which makes in particular 8-bit training much more effective.</p><p>I did not model numbers for 8-bit training because to model that I need to know the latency of L1 and L2 caches on Hopper/Ada GPUs, and they are unknown and I do not have access to such GPUs. On Hopper/Ada, 8-bit training performance can well be 3-4x of 16-bit training performance if the caches are as fast as rumored.</p><p>But even with the new FP8 tensor cores there are some additional issues which are difficult to take into account when modeling GPU performance. For example, FP8 tensor cores do not support transposed matrix multiplication which means backpropagation needs either a separate transpose before multiplication or one needs to hold two sets of weights — one transposed and one non-transposed — in memory. I used two sets of weight when I experimented with Int8 training in my <a href="https://arxiv.org/abs/2208.07339">LLM.int8()</a> project and this reduced the overall speedups quite significantly. I think one can do better with the right algorithms/software, but this shows that missing features like a transposed matrix multiplication for tensor cores can affect performance.</p><p>For old GPUs, Int8 inference performance is close to the 16-bit inference performance for models below 13B parameters. Int8 performance on old GPUs is only relevant if you have relatively large models with 175B parameters or more. If you are interested in 8-bit performance of older GPUs, you can read the Appendix D of my <a href="https://arxiv.org/abs/2208.07339">LLM.int8() paper</a> where I benchmark Int8 performance.</p><h2><span id="GPU_Deep_Learning_Performance_per_Dollar">GPU Deep Learning Performance per Dollar</span></h2><p>Below we see the chart for the performance per US dollar for all GPUs sorted by 8-bit inference performance. How to use the chart to find a suitable GPU for you is as follows:</p><ol><li>Determine the amount of GPU memory that you need (rough heuristic: at least 12 GB for image generation; at least 24 GB for work with transformers)</li><li>While 8-bit inference and training is experimental, it will become standard within 6 months. You might need to do some extra difficult coding to work with 8-bit in the meantime. Is that OK for you? If not, select for 16-bit performance.</li><li>Using the metric determined in (2), find the GPU with the highest relative performance/dollar  that has the amount of memory you need.</li></ol><p>We can see that the RTX 4070 Ti is most cost-effective for 8-bit and 16-bit inference while the RTX 3080 remains most cost-effective for 16-bit training. While these GPUs are most cost-effective, they are not necessarily recommended as they do not have sufficient memory for many use-cases. However, it might be the ideal cards to get started on your deep learning journey. Some of these GPUs are excellent for Kaggle competition where one can often rely on smaller models. Since to do well in  Kaggle competitions the method of how you work is more important than the models size, many of these smaller GPUs are excellent for Kaggle competitions.</p><p>The best GPUs for academic and startup servers seem to be A6000 Ada GPUs (not to be confused with A6000 Turing). The H100 SXM GPU is also very cost effective and has high memory and very strong performance. If I would build a small cluster for a company/academic lab, I would use 66-80% A6000 GPUs and 20-33% H100 SXM GPUs. If I get a good deal on L40 GPUs, I would also pick them instead of A6000, so you can always ask for a quote on these.</p><figure><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="1192" data-permalink="https://timdettmers.com/gpus_ada_performance_per_dollar6/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?fit=1703%2C1673&amp;ssl=1" data-orig-size="1703,1673" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="GPUs_Ada_performance_per_dollar6" data-image-description="" data-image-caption="<p>Shown is relative performance per US Dollar of GPUs normalized by the cost for a desktop computer and the average Amazon and eBay price for each GPU. Additionally, the electricity cost of ownership for 5 years is added with an electricity price of 0.175 USD per kWh and a 15% GPU utilization rate. The electricity cost for a RTX 4090 is about $100 per year. How to read and interpret the chart: a desktop computer with RTX 4070 Ti cards owned for 5 years yields about 2x more 8-bit inference performance per dollar compared to a RTX 3090 GPU.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?fit=300%2C295&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?fit=1024%2C1006&amp;ssl=1" width="1024" height="1006" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=1024%2C1006&amp;ssl=1" alt="" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=1024%2C1006&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=300%2C295&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=768%2C754&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=1536%2C1509&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?w=1703&amp;ssl=1 1703w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=1024%2C1006&amp;ssl=1 1024w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=300%2C295&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=768%2C754&amp;ssl=1 768w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=1536%2C1509&amp;ssl=1 1536w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?w=1703&amp;ssl=1 1703w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png?resize=1024%2C1006&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>Shown is relative performance per US Dollar of GPUs normalized by the cost for a desktop computer and the average Amazon and eBay price for each GPU. Additionally, the electricity cost of ownership for 5 years is added with an electricity price of 0.175 USD per kWh and a 15% GPU utilization rate. The electricity cost for a RTX 4090 is about $100 per year. How to read and interpret the chart: a desktop computer with RTX 4070 Ti cards owned for 5 years yields about 2x more 8-bit inference performance per dollar compared to a RTX 3090 GPU.</figcaption></figure><h2><span id="GPU_Recommendations">GPU Recommendations</span></h2><p>I have a create a recommendation flow-chart that you can see below (click here for <a href="https://nanx.me/gpu/">interactive app</a> from Nan Xiao). While this chart will help you in 80% of cases, it might not quite work for you because the options might be too expensive. In that case, try to look at the benchmarks above and pick the most cost effective GPU that still has enough GPU memory for your use-case. You can estimate the GPU memory needed by running your problem in the vast.ai or Lambda Cloud for a while so you know what you need. The vast.ai or Lambda Cloud might also work well if you only need a GPU very sporadically (every couple of days for a few hours) and you do not need to download and process large dataset to get started. However, cloud GPUs are usually not a good option if you use your GPU for many months with a high usage rate each day (12 hours each day). You can use the example in the “When is it better to use the cloud vs a dedicated GPU desktop/server?” section below to determine if cloud GPUs are good for you.</p><figure><a href="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?ssl=1"><img data-lazy-fallback="1" data-attachment-id="1173" data-permalink="https://timdettmers.com/gpu_recommendations/" data-orig-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?fit=845%2C686&amp;ssl=1" data-orig-size="845,686" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gpu_recommendations" data-image-description="" data-image-caption="<p>GPU recommendation chart for Ada/Hopper GPUs. Follow the answers to the Yes/No questions to find the GPU that is most suitable for you. While this chart works well in about 80% of cases, you might end up with a GPU that is too expensive. Use the cost/performance charts above to make a selection instead.</p>
" data-medium-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?fit=300%2C244&amp;ssl=1" data-large-file="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?fit=845%2C686&amp;ssl=1" width="845" height="686" src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?resize=845%2C686&amp;ssl=1" alt="" srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?w=845&amp;ssl=1 845w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?resize=300%2C244&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?resize=768%2C623&amp;ssl=1 768w" sizes="(max-width: 845px) 100vw, 845px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?w=845&amp;ssl=1 845w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?resize=300%2C244&amp;ssl=1 300w, https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?resize=768%2C623&amp;ssl=1 768w" data-lazy-src="https://i0.wp.com/timdettmers.com/wp-content/uploads/2023/01/gpu_recommendations.png?resize=845%2C686&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a><figcaption>GPU recommendation chart for Ada/Hopper GPUs. Follow the answers to the Yes/No questions to find the GPU that is most suitable for you. While this chart works well in about 80% of cases, you might end up with a GPU that is too expensive. Use the cost/performance charts above to make a selection instead. [<a href="https://nanx.me/gpu/">interactive app</a>]</figcaption></figure><h3><span id="Is_it_better_to_wait_for_future_GPUs_for_an_upgrade_The_future_of_GPUs">Is it better to wait for future GPUs for an upgrade? The future of GPUs.</span></h3><p>To understand if it makes sense to skip this generation and buy the next generation of GPUs, it makes sense to talk a bit about what improvements in the future will look like.</p><p>In the past it was possible to shrink the size of transistors to improve speed of a processor. This is coming to an end now. For example, while shrinking SRAM increased its speed (smaller distance, faster memory access), this is no longer the case. Current improvements in SRAM do not improve its performance anymore and might even be negative. While logic such as Tensor Cores get smaller, this does not necessarily make GPU faster since the main problem for matrix multiplication is to get memory to the tensor cores which is dictated by SRAM and GPU RAM speed and size. GPU RAM still increases in speed if we stack memory modules into high-bandwidth modules (HBM3+), but these are too expensive to manufacture for consumer applications. The main way to improve raw speed of GPUs is to use more power and more cooling as we have seen in the RTX 30s and 40s series. But this cannot go on for much longer.</p><p>Chiplets such as used by AMD CPUs are another straightforward way forward. AMD beat Intel by developing CPU chiplets. Chiplets are small chips that are fused together with a high speed on-chip network. You can think about them as two GPUs that are so physically close together that you can almost consider them a single big GPU. They are cheaper to manufacture, but more difficult to combine into one big chip. So you need know-how and fast connectivity between chiplets. AMD has a lot of experience with chiplet design. AMD’s next generation GPUs are going to be chiplet designs, while NVIDIA currently has no public plans for such designs. This may mean that the next generation of AMD GPUs might be better in terms of cost/performance compared to NVIDIA GPUs.</p><p>However, the main performance boost for GPUs is currently specialized logic. For example, the asynchronous copy hardware units on the Ampere generation (RTX 30 / A100 / RTX 40) or the extension, the Tensor Memory Accelerator (TMA), both reduce the overhead of copying memory from the slow global memory to fast shared memory (caches) through specialized hardware and so each thread can do more computation. The TMA also reduces overhead by performing automatic calculations of read/write indices which is particularly important for 8-bit computation where one has double the elements for the same amount of memory compared to 16-bit computation. So specialized hardware logic can accelerate matrix multiplication further.<br>Low-bit precision is another straightforward way forward for a couple of years. We will see widespread adoption of 8-bit inference and training in the next months. We will see widespread 4-bit inference in the next year. Currently, the technology for 4-bit training does not exists, but research looks promising and I expect the first high performance FP4 Large Language Model (LLM) with competitive predictive performance to be trained in 1-2 years time.</p><p>Going to 2-bit precision for training currently looks pretty impossible, but it is a much easier problem than shrinking transistors further. So progress in hardware mostly depends on software and algorithms that make it possible to use specialized features offered by the hardware.</p><p>We will probably be able to still improve the combination of algorithms + hardware to the year 2032, but after that will hit the end of GPU improvements (similar to smartphones). The wave of performance improvements after 2032 will come from better networking algorithms and mass hardware. It is uncertain if consumer GPUs will be relevant at this point. It might be that you need an RTX 9090 to run run Super HyperStableDiffusion Ultra Plus 9000 Extra or OpenChatGPT 5.0, but it might also be that some company will offer a high-quality API that is cheaper than the electricity cost for a RTX 9090 and you want to use a laptop + API for image generation and other tasks.</p><p>Overall, I think investing into a 8-bit capable GPU will be a very solid investment for the next 9 years. Improvements at 4-bit and 2-bit are likely small and other features like Sort Cores would only become relevant once sparse matrix multiplication can be leveraged well. We will probably see some kind of other advancement in 2-3 years which will make it into the next GPU 4 years from now, but we are running out of steam if we keep relying on matrix multiplication. This makes investments into new GPUs last longer.</p><h2><span id="Question_Answers_Misconceptions">Question &amp; Answers &amp; Misconceptions</span></h2><h3><span id="Do_I_need_PCIe_40_or_PCIe_50">Do I need PCIe 4.0 or PCIe 5.0?</span></h3><p>Generally, no. PCIe 5.0 or 4.0 is great if you have a GPU cluster. It is okay if you have an 8x GPU machine, but otherwise, it does not yield many benefits. It allows better parallelization and a bit faster data transfer. Data transfers are not a bottleneck in any application. In computer vision, in the data transfer pipeline, the data storage can be a bottleneck, but not the PCIe transfer from CPU to GPU. So there is no real reason to get a PCIe 5.0 or 4.0 setup for most people. The benefits will be maybe 1-7% better parallelization in a 4 GPU setup.</p><h3><span id="Do_I_need_8x16x_PCIe_lanes">Do I need 8x/16x PCIe lanes?&nbsp;</span></h3><p>Same as with PCIe 4.0 — generally, no. <a href="https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/">PCIe lanes</a> are needed for parallelization and fast data transfers, which are seldom a bottleneck. Operating GPUs on 4x lanes is fine, especially if you only have 2 GPUs. For a 4 GPU setup, I would prefer 8x lanes per GPU, but running them at 4x lanes will probably only decrease performance by around 5-10% if you parallelize across all 4 GPUs.</p><h3><span id="How_do_I_fit_4x_RTX_4090_or_3090_if_they_take_up_3_PCIe_slots_each">How do I fit 4x RTX 4090 or 3090 if they take up 3 PCIe slots each?</span></h3><p>You need to get one of the two-slot variants, or you can try to spread them out with PCIe extenders. Besides space, you should also immediately think about cooling and a suitable PSU.</p><p>PCIe extenders might also solve both space and cooling issues, but you need to make sure that you have enough space in your case to spread out the GPUs. Make sure your PCIe extenders are long enough!</p><h3><span id="How_do_I_cool_4x_RTX_3090_or_4x_RTX_3080">How do I cool 4x RTX 3090 or 4x RTX 3080?</span></h3><p>See the previous section.</p><h3><span id="Can_I_use_multiple_GPUs_of_different_GPU_types">Can I use multiple GPUs of different GPU types?</span></h3><p>Yes, you can! But you cannot parallelize efficiently across GPUs of different types since you will often go at the speed of the slowest GPU (data and fully sharded parallelism).  So different GPUs work just fine, but parallelization across those GPUs will be inefficient since the fastest GPU will wait for the slowest GPU to catch up to a synchronization point (usually gradient update).</p><h3><span id="What_is_NVLink_and_is_it_useful">What is NVLink, and is it useful?</span></h3><p>Generally, NVLink is not useful. NVLink is a high speed interconnect between GPUs. It is useful if you have a GPU cluster with +128 GPUs. Otherwise, it yields almost no benefits over standard PCIe transfers.</p><h3><span id="I_do_not_have_enough_money_even_for_the_cheapest_GPUs_you_recommend_What_can_I_do">I do not have enough money, even for the cheapest GPUs you recommend. What can I do?</span></h3><p>Definitely buy used GPUs. You can buy a small cheap GPU for prototyping and testing and then roll out for full experiments to the cloud like vast.ai or Lambda Cloud. This can be cheap if you train/fine-tune/inference on large models only every now and then and spent more time protoyping on smaller models.</p><h3><span id="What_is_the_carbon_footprint_of_GPUs_How_can_I_use_GPUs_without_polluting_the_environment">What is the carbon footprint of GPUs? How can I use GPUs without polluting the environment?</span></h3><p>I built a <a href="https://github.com/TimDettmers/carbonneutral">carbon calculator</a> for calculating your carbon footprint for academics (carbon from flights to conferences + GPU time). The calculator can also be used to calculate a pure GPU carbon footprint. You will find that GPUs produce much, much more carbon than international flights. As such, you should make sure you have a green source of energy if you do not want to have an astronomical carbon footprint. If no electricity provider in our area provides green energy, the best way is to buy carbon offsets. Many people are skeptical about carbon offsets. Do they work? Are they scams?</p><p>I believe skepticism just hurts in this case, because not doing anything would be more harmful than risking the probability of getting scammed. If you worry about scams, just invest in a portfolio of offsets to minimize risk.</p><p>I worked on a project that produced carbon offsets about ten years ago. The carbon offsets were generated by burning leaking methane from mines in China. UN officials tracked the process, and they required clean digital data and physical inspections of the project site. In that case, the carbon offsets that were produced were highly reliable. I believe many other projects have similar quality standards.</p><h3><span id="What_do_I_need_to_parallelize_across_two_machines">What do I need to parallelize across two machines?</span></h3><p>If you want to be on the safe side, you should get at least +50Gbits/s network cards to gain speedups if you want to parallelize across machines. I recommend having at least an EDR Infiniband setup, meaning a network card with at least 50 GBit/s bandwidth. Two EDR cards with cable are about $500 on eBay.</p><p>In some cases, you might be able to get away with 10 Gbit/s Ethernet, but this is usually only the case for special networks (certain convolutional networks) or if you use certain algorithms (Microsoft DeepSpeed).</p><h3><span id="Is_the_sparse_matrix_multiplication_features_suitable_for_sparse_matrices_in_general">Is the sparse matrix multiplication features suitable for sparse matrices in general?</span></h3><p>It does not seem so. Since the granularity of the sparse matrix needs to have 2 zero-valued elements, every 4 elements, the sparse matrices need to be quite structured. It might be possible to adjust the algorithm slightly, which involves that you pool 4 values into a compressed representation of 2 values, but this also means that precise arbitrary sparse matrix multiplication is not possible with Ampere GPUs.</p><h3><span id="Do_I_need_an_Intel_CPU_to_power_a_multi-GPU_setup">Do I need an Intel CPU to power a multi-GPU setup?</span></h3><p>I do not recommend Intel CPUs unless you heavily use CPUs in Kaggle competitions (heavy linear algebra on the CPU). Even for Kaggle competitions AMD CPUs are still great, though. AMD CPUs are cheaper and better than Intel CPUs in general for deep learning. For a 4x GPU built, my go-to CPU would be a Threadripper. We built dozens of systems at our university with Threadrippers, and they all work great — no complaints yet. For 8x GPU systems, I would usually go with CPUs that your vendor has experience with. CPU and PCIe/system reliability is more important in 8x systems than straight performance or straight cost-effectiveness.</p><h3><span id="Does_computer_case_design_matter_for_cooling">Does computer case design matter for cooling?</span></h3><p>No. GPUs are usually perfectly cooled if there is at least a small gap between GPUs. Case design will give you 1-3 C better temperatures, space between GPUs will provide you with 10-30 C improvements. The bottom line, if you have space between GPUs, cooling does not matter. If you have no space between GPUs, you need the right cooler design (blower fan) or another solution (water cooling, PCIe extenders), but in either case, case design and case fans do not matter.</p><h3><span id="Will_AMD_GPUs_ROCm_ever_catch_up_with_NVIDIA_GPUs_CUDA">Will AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA?</span></h3><p>Not in the next 1-2 years. It is a three-way problem: Tensor Cores, software, and community.&nbsp;</p><p>AMD GPUs are great in terms of pure silicon: Great FP16 performance, great memory bandwidth. However, their lack of Tensor Cores or the equivalent makes their deep learning performance poor compared to NVIDIA GPUs. Packed low-precision math does not cut it. Without this hardware feature, AMD GPUs will never be competitive. Rumors show that <a href="https://wccftech.com/amd-cdna-architecture-radeon-instinct-arcturus-gpu-120-cu-7680-cores/">some data center card</a> with Tensor Core equivalent is planned for 2020, but no new data emerged since then. Just having data center cards with a Tensor Core equivalent would also mean that few would be able to afford such AMD GPUs, which would give NVIDIA a competitive advantage.</p><p>Let’s say AMD introduces a Tensor-Core-like-hardware feature in the future. Then many people would say, “But there is no software that works for AMD GPUs! How am I supposed to use them?” This is mostly a misconception. The AMD software via ROCm has come to a long way, and support via PyTorch is excellent. While I have not seen many experience reports for AMD GPUs + PyTorch, all the software features are integrated. It seems, if you pick any network, you will be just fine running it on AMD GPUs. So here AMD has come a long way, and this issue is more or less solved.</p><p>However, if you solve software and the lack of Tensor Cores, AMD still has a problem: the lack of community. If you have a problem with NVIDIA GPUs, you can Google the problem and find a solution. That builds a lot of trust in NVIDIA GPUs. You have the infrastructure that makes using NVIDIA GPUs easy (any deep learning framework works, any scientific problem is well supported). You have the hacks and tricks that make usage of NVIDIA GPUs a breeze (e.g., apex). You can find experts on NVIDIA GPUs and programming around every other corner while I knew much less AMD GPU experts.</p><p>In the community aspect, AMD is a bit like Julia vs Python. Julia has a lot of potential, and many would say, and rightly so, that it is the superior programming language for scientific computing. Yet, Julia is barely used compared to Python. This is because the Python community is very strong. Numpy, SciPy, Pandas are powerful software packages that a large number of people congregate around. This is very similar to the NVIDIA vs AMD issue.</p><p>Thus, it is likely that AMD will not catch up until Tensor Core equivalent is introduced (1/2 to 1 year?) and a strong community is built around ROCm (2 years?). AMD will always snatch a part of the market share in specific subgroups (e.g., cryptocurrency mining, data centers). Still, in deep learning, NVIDIA will likely keep its monopoly for at least a couple more years.</p><h3><span id="When_is_it_better_to_use_the_cloud_vs_a_dedicated_GPU_desktopserver">When is it better to use the cloud vs a dedicated GPU desktop/server?</span></h3><p>Rule-of-thumb: If you expect to do deep learning for longer than a year, it is cheaper to get a desktop GPU. Otherwise, cloud instances are preferable unless you have extensive cloud computing skills and want the benefits of scaling the number of GPUs up and down at will.</p><p>Numbers in the following paragraphs are going to change, but it serves as a scenario that helps you to understand the rough costs. You can use similar math to determine if cloud GPUs are the best solution for you.</p><p>For the exact point in time when a cloud GPU is more expensive than a desktop depends highly on the service that you are using, and it is best to do a little math on this yourself. Below I do an example calculation for an AWS V100 spot instance with 1x V100 and compare it to the price of a desktop with a single RTX 3090 (similar performance). The desktop with RTX 3090 costs $2,200 (<a ref="https://pcpartpicker.com/user/tim_dettmers/saved/#view=mZ2rD3">2-GPU barebone</a> + RTX 3090). Additionally, assuming you are in the US, there is an additional $0.12 per kWh for electricity. This compares to $2.14 per hour for the AWS on-demand instance.</p><p>At 15% utilization per year, the desktop uses:&nbsp;</p><p>(350 W (GPU) + 100 W (CPU))*0.15 (utilization) * 24 hours * 365 days = 591 kWh per year</p><p>So 591 kWh of electricity per year, that is an additional $71.</p><p>The break-even point for a desktop vs a cloud instance at 15% utilization (you use the cloud instance 15% of time during the day), would be about 300 days ($2,311 vs $2,270):</p><p>$2.14/h * 0.15 (utilization) * 24 hours * 300 days = $2,311</p><p>So if you expect to run deep learning models after 300 days, it is better to buy a desktop instead of using AWS on-demand instances.</p><p>You can do similar calculations for any cloud service to make the decision if you go for a cloud service or a desktop.</p><p>Common utilization rates are the following:</p><ul><li>PhD student personal desktop: &lt; 15%</li><li>PhD student slurm GPU cluster: &gt; 35%</li><li>Company-wide slurm research cluster: &gt; 60%</li></ul><p>In general, utilization rates are lower for professions where thinking about cutting edge ideas is more important than developing practical products. Some areas have low utilization rates (interpretability research), while other areas have much higher rates (machine translation, language modeling). In general, the utilization of personal machines is almost always overestimated. Commonly, most personal systems have a utilization rate between 5-10%. This is why I would highly recommend slurm GPU clusters for research groups and companies instead of individual desktop GPU machines.</p><h2><span id="Version_History">Version History</span></h2><ul><li>2023-01-30: Improved font and recommendation chart. Added 5 years cost of ownership electricity perf/USD chart. Updated Async copy and TMA functionality. Slight update to FP8 training. General improvements.</li><li>2023-01-16: Added Hopper and Ada GPUs. Added GPU recommendation chart. Added information about the TMA unit and L2 cache.</li><li>2020-09-20: Added discussion of using power limiting to run 4x RTX 3090 systems. Added older GPUs to the performance and cost/performance charts. Added figures for sparse matrix multiplication.</li><li>2020-09-07: Added NVIDIA Ampere series GPUs. Included lots of good-to-know GPU details.</li><li>2019-04-03: Added RTX Titan and GTX 1660 Ti. Updated TPU section. Added startup hardware discussion.</li><li>2018-11-26: Added discussion of overheating issues of RTX cards.</li><li>2018-11-05: Added RTX 2070 and updated recommendations. Updated charts with hard performance data. Updated TPU section.</li><li>2018-08-21: Added RTX 2080 and RTX 2080 Ti; reworked performance analysis</li><li>2017-04-09: Added cost-efficiency analysis; updated recommendation with NVIDIA Titan Xp</li><li>2017-03-19: Cleaned up blog post; added GTX 1080 Ti</li><li>2016-07-23: Added Titan X Pascal and GTX 1060; updated recommendations</li><li>2016-06-25: Reworked multi-GPU section; removed simple neural network memory section as no longer relevant; expanded convolutional memory section; truncated AWS section due to not being efficient anymore; added my opinion about the Xeon Phi; added updates for the GTX 1000 series</li><li>2015-08-20: Added section for AWS GPU instances; added GTX 980 Ti to the comparison relation</li><li>2015-04-22: GTX 580 no longer recommended; added performance relationships between cards</li><li>2015-03-16: Updated GPU recommendations: GTX 970 and GTX 580</li><li>2015-02-23: Updated GPU recommendations and memory calculations</li><li>2014-09-28: Added emphasis for memory requirement of CNNs</li></ul><h2><span id="Acknowledgments">Acknowledgments</span></h2><p>I thank Suhail for making me aware of outdated prices on H100 GPUs, Gjorgji Kjosev for pointing out font issues, Anonymous for pointing out that the TMA unit does not exist on Ada GPUs, Scott Gray for pointing out that FP8 tensor cores have no transposed matrix multiplication, and reddit and HackerNews users for pointing out many other improvements.</p><p>For past updates of this blog post, I want to thank Mat Kelcey for helping me to debug and test custom code for the GTX 970; I want to thank Sander Dieleman for making me aware of the shortcomings of my GPU memory advice for convolutional nets; I want to thank Hannes Bretschneider for pointing out software dependency problems for the GTX 580; and I want to thank Oliver Griesel for pointing out notebook solutions for AWS instances. I want to thank Brad Nemire for providing me with an RTX Titan for benchmarking purposes. I want to thank Agrin Hilmkil, Ari Holtzman, Gabriel Ilharco, Nam Pho for their excellent feedback on the previous version of this blog post.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google abandons work to move Assistant smart speakers to Fuchsia (191 pts)]]></title>
            <link>https://9to5google.com/2023/07/25/google-abandons-assistant-speakers-fuchsia/</link>
            <guid>36871673</guid>
            <pubDate>Wed, 26 Jul 2023 00:39:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5google.com/2023/07/25/google-abandons-assistant-speakers-fuchsia/">https://9to5google.com/2023/07/25/google-abandons-assistant-speakers-fuchsia/</a>, See on <a href="https://news.ycombinator.com/item?id=36871673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://9to5google.com/wp-content/uploads/sites/4/2020/10/google_nest_audio_sand_2.jpg?quality=82&amp;strip=all&amp;w=1600" srcset="https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2020/10/google_nest_audio_sand_2.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2020/10/google_nest_audio_sand_2.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2020/10/google_nest_audio_sand_2.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2020/10/google_nest_audio_sand_2.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1600" height="800" alt="Nest Audio review" fetchpriority="high">
	
	</figure>

<p>Less than a year after the work was first discovered, it seems Google has abandoned its plans to upgrade its line of Assistant smart speakers to the Fuchsia operating system.</p>



<p>Since 2017, we’ve been closely following the development of Fuchsia, Google’s in-house operating system. In that time, it went from an early prototype to being the underlying software that powers all three of Google’s Nest Hub smart displays. Along the way, Google has also worked on supporting other hardware on Fuchsia, including the Pixelbook series, developer boards, and more.</p>



<p>Last year, we reported that Google’s Fuchsia team had <a href="https://9to5google.com/2022/10/21/google-nest-speakers-fuchsia-2023/">renewed its efforts</a> to support smart speakers. Long story short, the team had experimented with a single speaker, ditched that effort, then “restored” it later on. More importantly, the Fuchsia team was found to be working on multiple speakers, the most notable of which was an <a href="https://9to5google.com/2022/11/15/nest-speaker-uwb-handoff-apple-homepod/">as-yet-unreleased speaker equipped with UWB</a>.</p>



<p>This, along with the direct involvement of the SoC manufacturer Amlogic, signaled to us that Fuchsia was on track to replace the underlying “Cast OS” of speakers <a href="https://9to5google.com/2022/12/07/google-nest-audio-fuchsia-upgrade/">like the Nest Audio</a> after accomplishing the same feat for the Nest Hub series. However, it seems that this will no longer be the case.</p>



<p>In a newly posted <a href="https://fuchsia-review.googlesource.com/c/fuchsia/+/888834">code change</a>, the Fuchsia team formally marked all of its speaker hardware as “unsupported” and altogether removed the related code. Among the hardware now unsupported by Fuchsia, you’ll find the underlying SoCs for the Nest Mini, Nest Audio, Nest Wifi point, a potentially upcoming Nest speaker, and some Android Things-based smart speakers.</p>



<p>The Fuchsia team hasn’t shared a reason why its smart speaker efforts were discontinued. One issue that potentially played a role is that the Amlogic A113L chip used in “Clover” – an unknown device that we suspect may be the Pixel Tablet dock – <a href="https://fuchsia-review.googlesource.com/c/fuchsia/+/808670">does not meet</a> Fuchsia’s strict CPU requirements. Amlogic’s engineers <a href="https://fuchsia-review.googlesource.com/c/fuchsia/+/789422?usp=search">attempted to work around</a> this issue, seemingly to no avail.</p>




	<p>Another factor may be the sweeping layoffs that Google enacted at the beginning of the year. Early estimates suggested at least 16% of the Fuchsia team’s approximately 400 members were laid off, while a reliable source tells <em>9to5Google</em> that the final number, after international layoffs, was upwards of 20%.</p>



<p><strong>Read more:</strong> <a href="https://9to5google.com/2023/01/21/fuchsia-area-120-google-layoffs/">Google’s Fuchsia and Area 120 see significant cuts in layoffs</a></p>



<p>Whatever the reasoning, it’s disappointing to us to see the door close on Fuchsia’s most obvious next step after smart displays. To a certain degree, it seems some Googlers on the team share in that sentiment. One engineer commented on the code change to salute the departing hardware (“🫡”), while another metaphorically poured one out (“🫗”) for the outgoing speakers.</p>



<p>Importantly, the Nest Hub series of smart displays are entirely unaffected by this change. Those devices will continue to run Fuchsia under the hood and will continue to receive updates as normal.</p>



<h2 id="h-more-on-fuchsia">More on Fuchsia:</h2>



<ul>
<li><a href="https://9to5google.com/2023/05/02/nest-hub-2nd-gen-fuchsia-update/">Nest Hub 2nd Gen updates to Google’s Fuchsia operating system</a></li>



<li><a href="https://9to5google.com/2023/01/10/google-fuchsia-launch-upcoming-device/">Google accidentally reveals an upcoming device will launch with Fuchsia</a></li>



<li><a href="https://9to5google.com/2022/08/30/fuchsia-director-interview-chris-mckillop/">Interview: Fuchsia’s past, present, and future, as told by ex-director Chris McKillop</a></li>
</ul>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMMqA-Qow-c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Google to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3QcZvke"><img src="https://9to5google.com/wp-content/uploads/sites/6/2023/03/DECLUTTR-9TO5GOOGLE23-Generic-Version-2-1.jpg?quality=82&amp;strip=all" alt="" width="1024" height="205"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Expo – open-source platform for making universal apps for Android, iOS, and web (214 pts)]]></title>
            <link>https://github.com/expo/expo</link>
            <guid>36871651</guid>
            <pubDate>Wed, 26 Jul 2023 00:35:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/expo/expo">https://github.com/expo/expo</a>, See on <a href="https://news.ycombinator.com/item?id=36871651">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">
  <a href="https://expo.dev/" rel="nofollow">
    <img alt="expo sdk" height="128" src="https://github.com/expo/expo/raw/main/.github/resources/banner.png">
    </a></p><h2 tabindex="-1" dir="auto">Expo</h2>
  

<p dir="auto">
   <a aria-label="SDK version" href="https://www.npmjs.com/package/expo" rel="nofollow">
    <img alt="Expo SDK version" src="https://camo.githubusercontent.com/85403c89fe67bba1dfd90f41e7b66ce58ed12216193288013300f692d4f19630/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f6578706f2e7376673f7374796c653d666c61742d737175617265266c6162656c3d53444b266c6162656c436f6c6f723d30303030303026636f6c6f723d343633304542" data-canonical-src="https://img.shields.io/npm/v/expo.svg?style=flat-square&amp;label=SDK&amp;labelColor=000000&amp;color=4630EB">
  </a>
  <a aria-label="Join our forums" href="https://forums.expo.dev/" rel="nofollow">
    <img alt="Forums" src="https://camo.githubusercontent.com/1636511dd917f668099cf54d1af4861a0dd3c66f18758f90550b1b7437afba62/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41736b2532305175657374696f6e732532302d626c75652e7376673f7374796c653d666c61742d737175617265266c6f676f3d646973636f75727365266c6f676f57696474683d3135266c6162656c436f6c6f723d30303030303026636f6c6f723d343633304542" data-canonical-src="https://img.shields.io/badge/Ask%20Questions%20-blue.svg?style=flat-square&amp;logo=discourse&amp;logoWidth=15&amp;labelColor=000000&amp;color=4630EB">
  </a>
  <a aria-label="Join our Discord" href="https://chat.expo.dev/" rel="nofollow">
    <img alt="Discord" src="https://camo.githubusercontent.com/b0d99e96346c7053dc7be51e042e7823cfede4bf4a424a03a5aa35c141d076ac/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3639353431313233323835363939373936382e7376673f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d30303030303026636f6c6f723d343633304542266c6f676f3d646973636f7264266c6f676f436f6c6f723d464646464646266c6162656c3d" data-canonical-src="https://img.shields.io/discord/695411232856997968.svg?style=flat-square&amp;labelColor=000000&amp;color=4630EB&amp;logo=discord&amp;logoColor=FFFFFF&amp;label=">
  </a>
  <a aria-label="Expo is free to use" href="https://github.com/expo/expo/blob/main/LICENSE">
    <img alt="License: MIT" src="https://camo.githubusercontent.com/e5f7b5dfbbf65c46e30f930c21cd82f8db4dc97813a54bcf6664b0ceb2f88ceb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d737563636573732e7376673f7374796c653d666c61742d73717561726526636f6c6f723d333343433132" data-canonical-src="https://img.shields.io/badge/License-MIT-success.svg?style=flat-square&amp;color=33CC12">
  </a>
  <a aria-label="expo downloads" href="http://www.npmtrends.com/expo" rel="nofollow">
    <img alt="Downloads" src="https://camo.githubusercontent.com/49608498c612afb7041c92bbf7a7e7357c0cb014a4dbef324d930c6931e0454a/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f6578706f2e7376673f7374796c653d666c61742d737175617265266c6162656c436f6c6f723d6772617926636f6c6f723d333343433132266c6162656c3d446f776e6c6f616473" data-canonical-src="https://img.shields.io/npm/dm/expo.svg?style=flat-square&amp;labelColor=gray&amp;color=33CC12&amp;label=Downloads">
  </a>
</p>
<p dir="auto">
  <a aria-label="try expo with snack" href="https://snack.expo.dev/" rel="nofollow"><b>Try Expo in the Browser</b></a>
 |
  <a aria-label="expo documentation" href="https://docs.expo.dev/" rel="nofollow">Read the Documentation 📚</a>
</p>
<p dir="auto">
  <a aria-label="Follow @expo on Twitter" href="https://twitter.com/intent/follow?screen_name=expo" rel="nofollow">
    <img alt="Twitter: expo" src="https://camo.githubusercontent.com/614b49a1f64ced540abf7b47d5c13ed882e769bef98d8b8b1b05c3182d0c48ae/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6578706f2e7376673f7374796c653d666c61742d737175617265266c6162656c3d466f6c6c6f772532302534306578706f266c6f676f3d54574954544552266c6f676f436f6c6f723d464646464646266c6162656c436f6c6f723d303061636564266c6f676f57696474683d313526636f6c6f723d6c6967687467726179" data-canonical-src="https://img.shields.io/twitter/follow/expo.svg?style=flat-square&amp;label=Follow%20%40expo&amp;logo=TWITTER&amp;logoColor=FFFFFF&amp;labelColor=00aced&amp;logoWidth=15&amp;color=lightgray">
  </a>
  <a aria-label="Follow Expo on Medium" href="https://blog.expo.dev/" rel="nofollow">
    <img alt="Medium: exposition" src="https://camo.githubusercontent.com/a4f2c880c7a0cdf5654d18ad9147fd45fe8e519b32a1f64da731393e9ebeb277/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6561726e2532306d6f72652532306f6e2532306f7572253230626c6f672d6c69676874677261792e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/Learn%20more%20on%20our%20blog-lightgray.svg?style=flat-square">
  </a>
</p>
<hr>
<ul dir="auto">
<li><a href="#-documentation"><g-emoji alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png">📚</g-emoji> Documentation</a></li>
<li><a href="#-project-layout"><g-emoji alias="world_map" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png">🗺</g-emoji> Project Layout</a></li>
<li><a href="#-badges"><g-emoji alias="medal_sports" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c5.png">🏅</g-emoji> Badges</a></li>
<li><a href="#-contributing"><g-emoji alias="clap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44f.png">👏</g-emoji> Contributing</a></li>
<li><a href="#-faq"><g-emoji alias="question" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png">❓</g-emoji> FAQ</a></li>
<li><a href="#-the-team"><g-emoji alias="blue_heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f499.png">💙</g-emoji> The Team</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto">Expo is an open-source platform for making universal native apps that run on Android, iOS, and the web. It includes a universal runtime and libraries that let you build native apps by writing React and JavaScript. This repository is where the Expo client software is developed, and includes the client apps, modules, apps, CLI, and more. <a href="https://expo.dev/eas" rel="nofollow">Expo Application Services (EAS)</a> is a platform of hosted services that are deeply integrated with Expo open source tools. EAS helps you build, ship, and iterate on your app as an individual or a team.</p>
<p dir="auto"><a href="https://expo.dev/guidelines" rel="nofollow">Click here to view the Expo Community Guidelines</a>. Thank you for helping keep the Expo community open and welcoming!</p>
<h2 tabindex="-1" dir="auto"><g-emoji alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png">📚</g-emoji> Documentation</h2>
<p dir="auto">Learn about building and deploying universal apps <a aria-label="expo documentation" href="https://docs.expo.dev/" rel="nofollow">in our official docs!</a></p>
<ul dir="auto">
<li><a href="https://docs.expo.dev/" rel="nofollow">Getting Started</a></li>
<li><a href="https://docs.expo.dev/versions/latest/" rel="nofollow">API Reference</a></li>
<li><a href="https://docs.expo.dev/workflow/customizing/" rel="nofollow">Using Custom Native Modules</a></li>
</ul>
<h2 tabindex="-1" dir="auto">🗺 Project Layout</h2>
<ul dir="auto">
<li><a href="https://github.com/expo/expo/blob/main/packages"><code>packages</code></a> All the source code for Expo modules, if you want to edit a library or just see how it works this is where you'll find it.</li>
<li><a href="https://github.com/expo/expo/blob/main/apps"><code>apps</code></a> This is where you can find Expo projects which are linked to the development modules. You'll do most of your testing in here.</li>
<li><a href="https://github.com/expo/expo/blob/main/docs"><code>docs</code></a> The source code for <strong><a href="https://docs.expo.dev/" rel="nofollow">https://docs.expo.dev</a></strong></li>
<li><a href="https://github.com/expo/expo/blob/main/templates"><code>templates</code></a> The template projects you get when you run <code>npx create-expo-app</code></li>
<li><a href="https://github.com/expo/expo/blob/main/react-native-lab"><code>react-native-lab</code></a> This is our fork of <code>react-native</code> used to build Expo Go.</li>
<li><a href="https://github.com/expo/expo/blob/main/guides"><code>guides</code></a> In-depth tutorials for advanced topics like contributing to the client.</li>
<li><a href="https://github.com/expo/expo/blob/main/android"><code>android</code></a> contains the Android project.</li>
<li><a href="https://github.com/expo/expo/blob/main/home"><code>home</code></a> contains the JavaScript source code of the app.</li>
<li><a href="https://github.com/expo/expo/blob/main/ios"><code>ios</code></a> contains the iOS project.</li>
<li><a href="https://github.com/expo/expo/blob/main/ios"><code>ios/Exponent.xcworkspace</code></a> is the Xcode workspace. Always open this instead of <code>Exponent.xcodeproj</code> because the workspace also loads the CocoaPods dependencies.</li>
<li><a href="https://github.com/expo/expo/blob/main/tools"><code>tools</code></a> contains build and configuration tools.</li>
<li><a href="https://github.com/expo/expo/blob/main/template-files"><code>template-files</code></a> contains templates for files that require private keys. They are populated using the keys in <code>template-files/keys.json</code>.</li>
<li><a href="https://github.com/expo/expo/blob/main/template-files/ios/dependencies.json"><code>template-files/ios/dependencies.json</code></a> specifies the CocoaPods dependencies of the app.</li>
</ul>
<h2 tabindex="-1" dir="auto">🏅 Badges</h2>
<p dir="auto">Let everyone know your app can be run instantly in the <em>Expo Go</em> app!
<br></p>
<p dir="auto"><a href="https://expo.dev/client" rel="nofollow"><img src="https://camo.githubusercontent.com/bf6006a5f3eeb3feefa914b78bb3c299e47bc8a33b9839d7c3b16e86584947ac/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f52756e73253230776974682532304578706f253230476f2d3030302e7376673f7374796c653d666c61742d737175617265266c6f676f3d4558504f266c6162656c436f6c6f723d663366336633266c6f676f436f6c6f723d303030" alt="runs with Expo Go" data-canonical-src="https://img.shields.io/badge/Runs%20with%20Expo%20Go-000.svg?style=flat-square&amp;logo=EXPO&amp;labelColor=f3f3f3&amp;logoColor=000"></a></p>
<p dir="auto"><a href="https://expo.dev/client" rel="nofollow"><img src="https://camo.githubusercontent.com/5fa5c16e14a9a4df1b87a973c08de220036673c2d32e7cc3399a34e4d9cff534/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f52756e73253230776974682532304578706f253230476f2d3436333045422e7376673f7374796c653d666c61742d737175617265266c6f676f3d4558504f266c6162656c436f6c6f723d663366336633266c6f676f436f6c6f723d303030" alt="runs with Expo Go" data-canonical-src="https://img.shields.io/badge/Runs%20with%20Expo%20Go-4630EB.svg?style=flat-square&amp;logo=EXPO&amp;labelColor=f3f3f3&amp;logoColor=000"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="[![runs with Expo Go](https://img.shields.io/badge/Runs%20with%20Expo%20Go-000.svg?style=flat-square&amp;logo=EXPO&amp;labelColor=f3f3f3&amp;logoColor=000)](https://expo.dev/client)

[![runs with Expo Go](https://img.shields.io/badge/Runs%20with%20Expo%20Go-4630EB.svg?style=flat-square&amp;logo=EXPO&amp;labelColor=f3f3f3&amp;logoColor=000)](https://expo.dev/client)"><pre><span>[</span><span>![</span>runs with Expo Go<span>]</span><span>(</span><span>https://img.shields.io/badge/Runs%20with%20Expo%20Go-000.svg?style=flat-square&amp;logo=EXPO&amp;labelColor=f3f3f3&amp;logoColor=000</span><span>)]</span><span>(</span><span>https://expo.dev/client</span><span>)</span>

<span>[</span><span>![</span>runs with Expo Go<span>]</span><span>(</span><span>https://img.shields.io/badge/Runs%20with%20Expo%20Go-4630EB.svg?style=flat-square&amp;logo=EXPO&amp;labelColor=f3f3f3&amp;logoColor=000</span><span>)]</span><span>(</span><span>https://expo.dev/client</span><span>)</span></pre></div>
<h2 tabindex="-1" dir="auto"><g-emoji alias="clap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44f.png">👏</g-emoji> Contributing</h2>
<p dir="auto">If you like Expo and want to help make it better then check out our <a href="https://github.com/expo/expo/blob/main/CONTRIBUTING.md">contributing guide</a>! Check out the <a href="https://github.com/expo/expo/tree/main/packages/%40expo/cli">CLI package</a> to work on the Expo CLI.</p>
<h2 tabindex="-1" dir="auto"><g-emoji alias="question" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png">❓</g-emoji> FAQ</h2>
<p dir="auto">If you have questions about Expo and want answers, then check out our <a href="https://docs.expo.dev/versions/latest/introduction/faq/" rel="nofollow">Frequently Asked Questions</a>!</p>
<p dir="auto">If you still have questions you can ask them on our <a href="https://forums.expo.dev/" rel="nofollow">forums</a>, <a href="https://chat.expo.dev/" rel="nofollow">Discord</a> or on Twitter <a href="https://twitter.com/expo" rel="nofollow">@Expo</a>.</p>
<h2 tabindex="-1" dir="auto"><g-emoji alias="blue_heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f499.png">💙</g-emoji> The Team</h2>
<p dir="auto">Curious about who makes Expo? Here are our <a href="https://expo.dev/about" rel="nofollow">team members</a>!</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">The Expo source code is made available under the <a href="https://github.com/expo/expo/blob/main/LICENSE">MIT license</a>. Some of the dependencies are licensed differently, with the BSD license, for example.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/9664363/185428788-d762fd5d-97b3-4f59-8db7-f72405be9677.gif"><img alt="Star the Expo repo on GitHub to support the project" src="https://user-images.githubusercontent.com/9664363/185428788-d762fd5d-97b3-4f59-8db7-f72405be9677.gif" width="50%" data-animated-image=""></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Whom the gods would destroy, they first give real-time analytics (2013) (157 pts)]]></title>
            <link>https://mcfunley.com/whom-the-gods-would-destroy-they-first-give-real-time-analytics</link>
            <guid>36870140</guid>
            <pubDate>Tue, 25 Jul 2023 21:56:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mcfunley.com/whom-the-gods-would-destroy-they-first-give-real-time-analytics">https://mcfunley.com/whom-the-gods-would-destroy-they-first-give-real-time-analytics</a>, See on <a href="https://news.ycombinator.com/item?id=36870140">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <blockquote>
  <p>Homer: There's three ways to do things. The right way, the wrong way, and the <em>Max Power</em> way!</p>
  <p>Bart: Isn't that the wrong way?</p>
  <p>Homer: Yeah. But faster!</p>
  <p>- "Homer to the Max"</p>
</blockquote>

<p>Every few months, I try to talk someone down from building a real-time product analytics system. When I’m lucky, I can get to them early.</p>

<p>The turnaround time for most of the web analysis done at Etsy is at least 24 hours. This a ranking source of grousing. Decreasing this interval is periodically raised as a priority, either by engineers itching for a challenge or by others hoping to make decisions more rapidly. There are companies out there selling instant usage numbers, so why can’t we have them?</p>

<p>Here’s an excerpt from a manifesto demanding the construction of such a system. This was written several years ago by an otherwise brilliant individual, whom I respect. I have made a few omissions for brevity.</p>

<blockquote>
  <p><strong><em>We believe in…</em></strong></p>

  <ol>
    <li><strong>Timeliness</strong>. I want the data to be at most 5 minutes old. So this is a near-real-time system.</li>
    <li><strong>Comprehensiveness</strong>. No sampling. Complete data sets.</li>
    <li><strong>Accuracy</strong> (how precise the data is). Everything should be accurate.</li>
    <li><strong>Accessibility</strong>. Getting to meaningful data in Google Analytics is awful. To start with it’s all 12 - 24 hours old, and this is a huge impediment to insight &amp; action.</li>
    <li><strong>Performance</strong>. Most reports / dashboards should render in under 5 seconds.</li>
    <li><strong>Durability</strong>. Keep all stats for all time. I know this can get rather tough, but it’s just text.</li>
  </ol>
</blockquote>

<p>The 23-year-old programmer inside of me is salivating at the idea of building this. The burned out 27-year-old programmer inside of me is busy writing an email about how all of these demands, taken together, probably violate the <a href="http://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> somehow and also, hey, did you know that accuracy and precision are different?</p>

<p>But the 33-year-old programmer (who has long since beaten those demons into a bloody submission) sees the difficulty as irrelevant at best. Real-time analytics are <em>undesirable</em>. While there are many things wrong with our infrastructure, I would argue that the waiting is not one of those things.</p>

<p>Engineers might find this assertion more puzzling than most. I am sympathetic to this mindset, and I can understand why engineers are predisposed to see instantaneous A/B statistics as self-evidently positive. We monitor everything about our site in real time. Real-time metrics and graphing are the key to deploying 40 times daily with relative impunity. <a href="http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/">Measure anything, measure everything</a>!</p>

<figure>
  <img src="https://mcfunley.com/assets/images/deploy-dash.png" alt="Part of the deploy dashboard at Etsy. We love up-to-the-minute graphs.">
  <figcaption>Part of the deploy dashboard at Etsy. We love up-to-the-minute graphs.</figcaption>
</figure>

<p>This line of thinking is a trap. It’s important to divorce the concepts of operational metrics and product analytics. Confusing <em>how we do things</em> with <em>how we decide which things to do</em> is a fatal mistake.</p>

<p>So what is it that makes product analysis different? Well, there are many ways to screw yourself with real-time analytics. I will endeavor to list a few.</p>

<p>The first and most fundamental way is to disregard statistical significance testing entirely. This is a rookie mistake, but it’s one that’s made all of the time. Let’s say you’re testing a text change for a link on your website. Being an impatient person, you decide to do this over the course of an hour. You observe that 20 people in bucket A clicked, but 30 in bucket B clicked. Satisfied, and eager to move on, you choose bucket B. There are probably thousands of people doing this right now, <em>and they’re getting away with it.</em></p>

<p>This is a mistake because there’s no measurement of how likely it is that the observation (20 clicks vs. 30 clicks) was due to chance. Suppose that we weren’t measuring text on hyperlinks, but instead we were measuring two quarters to see if there was any difference between the two when flipped. As we flip, we could see a large gap between the number of heads received with either quarter. But since we’re talking about quarters, it’s more natural to suspect that that difference might be due to chance. Significance testing lets us ascertain how likely it is that this is the case.</p>

<p>A subtler error is to do significance testing, but to halt the experiment as soon as significance is measured. This is <a href="http://www.evanmiller.org/how-not-to-run-an-ab-test.html">always a bad idea</a>, and the problem is exacerbated by trying to make decisions far too quickly. Funny business with timeframes can coerce most A/B tests into statistical significance.</p>

<figure>
  <img src="https://mcfunley.com/assets/images/real-time-screwed.png" alt="A simulation of flipping two fair coins. In the green regions, the difference in the number of heads is measured to be significant. If we stopped flipping in those regions, we would (incorrectly) decide the coins were different.">
  <figcaption>A simulation of flipping two fair coins. In the green regions, the difference in the number of heads is measured to be significant. If we stopped flipping in those regions, we would (incorrectly) decide the coins were different.</figcaption>
</figure>

<p>Depending on the change that’s being made, making <em>any</em> decision based on a single day of data could be ill-conceived. Even if you think you have plenty of data, it’s not farfetched to imagine that user behavior has its own rhythms. A conspicuous (if basic) example of this is that Etsy sees 30% more orders on Tuesdays than it does on Sundays.</p>

<figure>
  
  <figcaption>Gratuitous infographic courtesy <a href="http://www.thenitpickster.com/">Brendan Sudol</a>.</figcaption>
</figure>

<p>While the sale count itself might not skew a random test, user demographics could be different day over day. Or very likely, you could see a major difference in user behavior immediately upon releasing a change, only to watch it evaporate as users learn to use new functionality. Given all of these concerns, the conservative and reasonable stance is to only consider tests that last a few days or more.</p>

<p>One could certainly have a real-time analytics system without making any of these mistakes. (To be clear, I find this unlikely. Idle hands stoked by a stream of numbers are the devil’s playthings.) But unless the intention is to make decisions with this data, one might wonder what the purpose of such a system could possibly be. Wasting the effort to erect complexity for which there is no use case is perhaps the worst of all of these possible pitfalls.</p>

<p>For all of these reasons I’ve come to view delayed analytics as positive. The turnaround time also imposes a welcome pressure on experimental design. People are more likely to think carefully about how their controls work and how they set up their measurements when there’s no promise of immediate feedback.</p>

<p>Real-time web analytics is a seductive concept. It appeals to our desire for instant gratification. But the truth is that there are very few product decisions that can be made in real time, if there are any at all. Analysis is difficult enough already, without attempting to do it at speed.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“We’ve Changed the Game”: Teamsters Win Historic UPS Contract (198 pts)]]></title>
            <link>https://teamster.org/2023/07/weve-changed-the-game-teamsters-win-historic-ups-contract/</link>
            <guid>36869738</guid>
            <pubDate>Tue, 25 Jul 2023 21:17:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://teamster.org/2023/07/weve-changed-the-game-teamsters-win-historic-ups-contract/">https://teamster.org/2023/07/weve-changed-the-game-teamsters-win-historic-ups-contract/</a>, See on <a href="https://news.ycombinator.com/item?id=36869738">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							<picture>
					
    <img src="https://teamster.org/wp-content/uploads/2023/07/20230703-ChangedGameRedx-IG-1-1264x1264.jpg" srcset="https://teamster.org/wp-content/uploads/2023/07/20230703-ChangedGameRedx-IG-1-406x406.jpg 406w, https://teamster.org/wp-content/uploads/2023/07/20230703-ChangedGameRedx-IG-1-812x812.jpg 812w, https://teamster.org/wp-content/uploads/2023/07/20230703-ChangedGameRedx-IG-1-632x632.jpg 632w, https://teamster.org/wp-content/uploads/2023/07/20230703-ChangedGameRedx-IG-1-1264x1264.jpg 1264w, https://teamster.org/wp-content/uploads/2023/07/20230703-ChangedGameRedx-IG-1-865x865.jpg 865w, https://teamster.org/wp-content/uploads/2023/07/20230703-ChangedGameRedx-IG-1-1730x1730.jpg 1730w" sizes="(max-width: 500px), (max-width: 980px), (min-width: 980px)" alt="20230703-ChangedGameRedx-IG (1)">

				</picture>
						<h3>Deal Results in Higher Wages, More Jobs, Equal Pay, A/C, MLK Day, Part-Time Rewards </h3>
			
	<p><strong>Press Contact: Kara Deniz</strong>&nbsp;Email: kdeniz@teamster.org</p>


<p>(WASHINGTON) – Today, the Teamsters reached the most historic tentative agreement for workers in the history of UPS, protecting and rewarding more than 340,000 UPS Teamsters nationwide. The overwhelmingly lucrative contract raises wages for all workers, creates more full-time jobs, and includes dozens of workplace protections and improvements. The UPS Teamsters National Negotiating Committee unanimously endorsed the five-year tentative agreement.</p>



<p>“Rank-and-file UPS Teamsters sacrificed everything to get this country through a pandemic and enabled UPS to reap record-setting profits. Teamster labor moves America. The union went into this fight committed to winning for our members. We demanded the best contract in the history of UPS, and we got it,” said Teamsters General President Sean M. O’Brien. “UPS has put $30 billion in new money on the table as a direct result of these negotiations. We’ve changed the game, battling it out day and night to make sure our members won an agreement that pays strong wages, rewards their labor, and doesn’t require a single concession. This contract sets a new standard in the labor movement and raises the bar for all workers.”</p>



<p>“UPS came dangerously close to putting itself on strike, but we kept firm on our demands. In my more than 40 years in Louisville representing members at Worldport — the largest UPS hub in the country — I have never seen a national contract that levels the playing field for workers so dramatically as this one. The agreement puts more money in our members’ pockets and establishes a full range of new protections for them on the job,” said Teamsters General Secretary-Treasurer Fred Zuckerman. “We stayed focused on our members and fought like hell to get everything that full-time and part-time UPS Teamsters deserve.”</p>



<p>“Rank-and-file members served on the committee for the first time, so we got to show up every day to support our fellow Teamsters and share their stories,” said Brandy Harris, a part-time UPS Teamster with Local 174 in Seattle and a member of the Teamsters National Negotiating Committee. “Our hard work has paid off — from those members and leaders negotiating for more at the table to my sisters and brothers building a credible strike threat around the country. Our union was organized and we were relentless. We’ve hit every goal that UPS Teamster members wanted and asked for with this agreement. It’s a ‘yes’ vote for the most historic contract we’ve ever had.”</p>



<p>Highlights of the tentative 2023-2028 UPS Teamsters National Master Agreement include:</p>



<ul>
<li>Historic wage increases. Existing full- and part-time UPS Teamsters will get $2.75 more per hour in 2023, and $7.50 more per hour over the length of the contract.</li>
</ul>



<ul>
<li>Existing part-timers will be raised up to no less than $21 per hour immediately, and part-time seniority workers earning more under a market rate adjustment would still receive all new general wage increases.</li>
</ul>



<ul>
<li>General wage increases for part-time workers will be double the amount obtained in the previous UPS Teamsters contract — and existing part-time workers will receive a 48 percent average total wage increase over the next five years.</li>
</ul>



<ul>
<li>Wage increases for full-timers will keep UPS Teamsters the highest paid delivery drivers in the nation, improving their average top rate to $49 per hour.</li>
</ul>



<ul>
<li>Current UPS Teamsters working part-time would receive longevity wage increases of up to $1.50 per hour on top of new hourly raises, compounding their earnings.</li>
</ul>



<ul>
<li>New part-time hires at UPS would start at $21 per hour and advance to $23 per hour.</li>
</ul>



<ul>
<li>All UPS Teamster drivers classified as 22.4s would be reclassified immediately to Regular Package Car Drivers and placed into seniority, ending the unfair two-tier wage system at UPS.</li>
</ul>



<ul>
<li>Safety and health protections, including vehicle air conditioning and cargo ventilation. UPS will equip in-cab A/C in all larger delivery vehicles, sprinter vans, and package cars purchased after Jan. 1, 2024. All cars get two fans and air induction vents in the cargo compartments.</li>
</ul>



<ul>
<li>All UPS Teamsters would receive Martin Luther King Day as a full holiday for the first time.</li>
</ul>



<ul>
<li>No more forced overtime on Teamster drivers’ days off. Drivers would keep one of two workweek schedules and could not be forced into overtime on scheduled off-days.</li>
</ul>



<ul>
<li>UPS Teamster part-timers will have priority to perform all seasonal support work using their own vehicles with a locked-in eight-hour guarantee. For the first time, seasonal work will be contained to five weeks only from November-December.</li>
</ul>



<ul>
<li>The creation of 7,500 new full-time Teamster jobs at UPS and the fulfillment of 22,500 open positions, establishing more opportunities through the life of the agreement for part-timers to transition to full-time work.</li>
</ul>



<ul>
<li>More than 60 total changes and improvements to the National Master Agreement — more than any other time in Teamsters history — and zero concessions from the rank-and-file.</li>
</ul>



<p>On July 31, representatives of the 176 UPS Teamster locals in the U.S. and Puerto Rico will meet to review and recommend the tentative agreement. All UPS rank-and-file members will receive a list of improvements in the contract. Locals will conduct member meetings and Teamsters will have several weeks to vote on the offer electronically. Member voting begins August 3 and concludes August 22.</p>



<p>The UPS Teamsters National Master Agreement is the single largest private-sector collective bargaining agreement in North America. &nbsp;</p>



<p>Founded in 1903, the Teamsters Union represents 1.2 million hardworking people in the U.S., Canada, and Puerto Rico. Visit&nbsp;<a href="https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fu7061146.ct.sendgrid.net%2Fls%2Fclick%3Fupn%3D4tNED-2FM8iDZJQyQ53jATUcsZiLbJRNadTeGF65UTAXn3JDe0AGQx6ooIbLPzmo26VG0H5JqHw1xN7FIF8ufYZ1L5bgH79SQGtMYyST-2F8ZNU06ElIJmQRltt-2F9QYz5SI7Yyf0qbQKzf7HOCqGMJAOChVJDufJBt3g8IGtuNF0L9298R20yA2i5PWCUh49IrpkO0gPMRlsGl-2BKcKAoptXGQ1KFN-2BSBl-2BWr9H7GZmaL3cE9t-2FZ4-2BQ97m0p5XC5HnpOzZ6Q2fP6cJOx39isXs778vSvq3FLqGJhUsYJVGsohezZ5V8BNTZ4CRIm-2FIxtPgFQZytJe-2FO9sFzBrnvGtiG36uLjR-2BLxlYBJbO1SGfxcs4xTUMX0m08zvETsiq5YwA9YmVfU9OtJxfVrZSRvtOVa9-2FCJ3-2F9rX0eO8YoLY8z2K2pSl2RhziSuzFDYCHOB0FpKvuWifNf3czVhcXB6kPCnbrs6vplmePkfLMNozDByUUJFgZnLs9DKF0Ls4S3IsMq8ho-2FLmtK5feFWDjY1jADwLzmjxmxjfOvpDxZ2HM1kve4kuMdhNUpN5dljK4vQ9jIdRXqdOq0uo3LuKVG5-2BohHtJiopPDfIhTSHY8RFd3WbCJV1lDr63TZmvXHMknmrIUsztYC8tD6hlB19gJaOGtV3YBUwZVFXyzAnz8sP9fKDB6FKqEmaOSNiti-2FQNkh3819qdBNSmpistrT92wSt9L63NgQW5-2B534RtnZfKZa5THdIrov9A2ETOBrDjp-2FJcn2fPJhS8NF00T8jz5y3HcX0eK5fjGCHmdKooI8WgY4nt0t42jOA7AUN7d8eIxAHP0liX28SlWEKnZkVO0gLV8rIt87SoZx0V1-2FNgrpkxh7V9LdM3195xJuqTl42oJXP7GQ7SRryfM2aMTnXx7pJQxFobMlLek1sQCmPtfCsmJdl7pjxyMGmZuFvt-2BQKaccpUSWXzNjglDwRqxDAWp-2FdNqf6MCmE-2FjDgGsrDhkAJo2EVdgTchXrlCOBEIkyxpcYhSJNH0eUdzxNArCZOZHTix1rNh55NH9YVSaVUbxbC-2FTJ-2FRvMRAzzV6BeucUMRC-2Bloq6Nz5hOduTdPCTu3pRZ-2ByFaHwxH0A8t1Mxk2A1SYTVT5LAGGS3BLfekQRHmbahdvbKdHPy-2FEED-2Bo3jCRQLUywqFQO2nMwyAFernKgedzeYmrSIP9KLVo8lJTZ5Rmteu84NdC8EWV-2BG59zSvdhCXsIRNlPk0wEonsuVYn7mWJUX1sFqttF0pKsWVeIpbXuBrIhmSpAkQijSBMnZx-2Fa-2F21HtpKpFcj2czk0q-2B8MrIHcszOWDIrF57OF6-2BcmmQB-2FP22Yc4PEbOcbW-2B-2F3xnGFFycgqyzjVNDDeRmLpROt3kTEFDGt0A0QTxdoml-2FaJ4E8dgTDAyu-2BW-2By1wUg452kRHmhZxHyqk-2FbBrDDuB1nxh4BAoikmdNXlhh5RJ6ykjroV-2BsdMKUL0-2BmUrwg88V8E120ZHvEQVvZyB9S3pwfLdIhdGTNhmnXNxrBqYyySpJv-2Brw1MslvFtlWCDbFxOYyPYG6dBx0sG5ccmBZlo9M-2BTCT6-2FotjPaEivgZLAQtm-2F7VONuvL5Ai8aJuLL7jDoqv6-2BZc8qj0xH66lZYLh9DTSa2pmLVF1dH5N7vya2zK7CNYpreiLflImJfo04x-2FrwyOo40HXYKRyP4WPC278f4yxrsGlZ4nE8beqLjR6dzFJinO49OGnjvc2QIT0C4N3eFdOEQNNNnExG152VkR-2FXAz0EAaFdHM0RBYGZN3CEvljDDyLb9iAZMb8MOlXjUxGleNt70pwnbAp7rYh-2Bzpiu1yphp496p3rTe-2FcIGH2UvbqoKS06OsD3A6rRpxZ1D2DK5i1rnRj">Teamster.org</a> to learn more and follow us on Twitter @Teamsters and on Facebook at&nbsp;<a href="https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fu7061146.ct.sendgrid.net%2Fls%2Fclick%3Fupn%3D4tNED-2FM8iDZJQyQ53jATUcsZiLbJRNadTeGF65UTAXn3JDe0AGQx6ooIbLPzmo26VG0H5JqHw1xN7FIF8ufYZ1L5bgH79SQGtMYyST-2F8ZNU06ElIJmQRltt-2F9QYz5SI7Yyf0qbQKzf7HOCqGMJAOChVJDufJBt3g8IGtuNF0L9298R20yA2i5PWCUh49IrpkO0gPMRlsGl-2BKcKAoptXGQ1KFN-2BSBl-2BWr9H7GZmaL3cE9t-2FZ4-2BQ97m0p5XC5HnpOzZ6Q2fP6cJOx39isXs778vSvq3FLqGJhUsYJVGsohezZ5V8BNTZ4CRIm-2FIxtPgFQZytJe-2FO9sFzBrnvGtiG36uLjR-2BLxlYBJbO1SGfxcs4xTWyEJ9BOnTMSstCyecgCZZqMnrCM6Pg57Zkia13F4Xe6zw6zVSn39MsgHcOx7erHrK3-2BA0BzA6SDzt0n-2BYPiz-2BBF9C8jH1KDH9-2FrUHqQc5TytVJ6iFKddrDAiDB2sK-2BYwxwqZgkCXmO7NeSN4yVXltqJ1DXkqGVgLqpc8fxzH0iALZpouOM4ndiVYK3BB3ZqppXTMQoV9YbqBw0MFC1wooW7tlrgnGSz3dbklMnkD3-2BJvqpk9iIV3LJUGU-2Fl2VO42G2KUIFHiwVS2aSdWf3xTSvDs7-2BHZ5BUEPkCsEhS36wUiDb2QZZ26U7Uj-2FJg3Q01-2FhRNxD0U-2FPifn8Zqiy26hAjZF3tOpx8CfS2P40Vg77vtVuBE7oNIF1LO5sK2KCbuLhBkUpyN-2FODZPR84lJ-2FfDqvRxJr-2BcTFITfveyleVpgZiCpBbYopJSp-2BZy-2BJtFbH-2FcVsWUnQTQhlJCA61hO133NYBRj-2BqvZprzIw7SwhZDP4Hx6CV9A2cepvnjukr-2BDk7TjQDCz5xYI7fbsgUcg84jDpIWkCbruKcfUqRMF2Avf0eES7C2YwGGPVX-2BWQ53QLsA9NhpiCLZnhgAB-2B7wolsu6MI38q-2Fr6IBAFYA4o-2BYGpskkgRUu24dupQr1X3syL-2F31Bh5KvKNBVWXfoY9E5pfDpvNCejqMM3ZlIo0LKTvxGyjpNOInogASBIGdq9luoFpbHkd-2BHc-2FYf8LsVif5q68x28i2ZvoKfsNidsZGGglE8N7PzePTEnKLs5Nv-2BLSWDyahf-2FtvwJplng72YhV4NT-2FzF6-2FUacrkjr6owbkAO7pst8e9qI6-2FJI8zu-2FC1MCcYDMv9h-2F-2FiEYrjbA-2B7ND9AyS7k8c5jVQzDm4LZ0N6HtVixCzPUgHqPrPXeDW-2Bj5IbVX3TRWqJiPqsAH9zcsYRSuKjGSgk5P67DQ7MWUnJ2Sd4cqn9D8pBR-2BhQTfdp1o7C9nP-2F909CgFABkZK58dp3WDgFN-2Fa5icvayYXfsgSnRURpZIXD7gXM3NhJcvAlYXO4M9BhTuSInykz3RA5PQvp83SZ1Mrkq2-2FRHfZxM-2B2o0OPlMm4zVhxPThMfs-2B9FosdCRAvCYr3-2F8kMCi7TBTPWVElxMaUnK5Ylj21Xd60UKJjLC6yoFvD5irlWg1P6r3Mg1r6Qx2jr3qoyiLtb06l57SAdNh2yvv9wc77R6Xbq02mwAHrfYUXac-2Fx5YbNshjHy7mmmrp9tihTdhu-2FeFXYP3dhHIV2WfYIxcCIyfxR0ERSSSGJQPnbmsPmeozhIbybhY9wx6nWC2eMrAn2guC286-2Fzqjg6Nz9EGx4AiA-2BPteWYRr7n9nVVS-2B2tmH3qC7KGQ0PC6gLFvKASktvECwSTl4Y6w7SxpNqco8VIRReMaPY-2BDV3uuo4VNMYOLmOHfTjJGpAABmlVFGjFjJ7-2Bp9hUDM6XayIVuEEccapWiMn32x4Wee4R0XoGPHnDL2EYFLmB1el5ujrSZTjGRfGr-2BQNB">Facebook.com/teamsters</a>.</p>


		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A command-line murder mystery (2014) (130 pts)]]></title>
            <link>https://github.com/veltman/clmystery</link>
            <guid>36868978</guid>
            <pubDate>Tue, 25 Jul 2023 20:33:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/veltman/clmystery">https://github.com/veltman/clmystery</a>, See on <a href="https://news.ycombinator.com/item?id=36868978">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">The Command Line Murders</h2>
<div data-snippet-clipboard-copy-content=".OOOOOOOOOOOOOOO @@                                   @@ OOOOOOOOOOOOOOOO.
OOOOOOOOOOOOOOOO @@                                    @@ OOOOOOOOOOOOOOOO
OOOOOOOOOO'''''' @@                                    @@ ```````OOOOOOOOO
OOOOO'' aaa@@@@@@@@@@@@@@@@@@@@&quot;&quot;&quot;                   &quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;@@aaaa `OOOO
OOOOO,&quot;&quot;&quot;&quot;@@@@@@@@@@@@@@&quot;&quot;&quot;&quot;                                     a@&quot;&quot; OOOA
OOOOOOOOOoooooo,                                            |OOoooooOOOOOS
OOOOOOOOOOOOOOOOo,                                          |OOOOOOOOOOOOC
OOOOOOOOOOOOOOOOOO                                         ,|OOOOOOOOOOOOI
OOOOOOOOOOOOOOOOOO @          THE                          |OOOOOOOOOOOOOI
OOOOOOOOOOOOOOOOO'@           COMMAND                      OOOOOOOOOOOOOOb
OOOOOOOOOOOOOOO'a'            LINE                         |OOOOOOOOOOOOOy
OOOOOOOOOOOOOO''              MURDERS                      aa`OOOOOOOOOOOP
OOOOOOOOOOOOOOb,..                                          `@aa``OOOOOOOh
OOOOOOOOOOOOOOOOOOo                                           `@@@aa OOOOo
OOOOOOOOOOOOOOOOOOO|                                             @@@ OOOOe
OOOOOOOOOOOOOOOOOOO@                               aaaaaaa       @@',OOOOn
OOOOOOOOOOOOOOOOOOO@                        aaa@@@@@@@@&quot;&quot;        @@ OOOOOi
OOOOOOOOOO~~ aaaaaa&quot;a                 aaa@@@@@@@@@@&quot;&quot;            @@ OOOOOx
OOOOOO aaaa@&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot; &quot;&quot;            @@@@@@@@@@@@&quot;&quot;               @@@|`OOOO'
OOOOOOOo`@@a                  aa@@  @@@@@@@&quot;&quot;         a@        @@@@ OOOO9
OOOOOOO'  `@@a               @@a@@   @@&quot;&quot;           a@@   a     |@@@ OOOO3
`OOOO'       `@    aa@@       aaa&quot;&quot;&quot;          @a        a@     a@@@',OOOO'"><pre><code>.OOOOOOOOOOOOOOO @@                                   @@ OOOOOOOOOOOOOOOO.
OOOOOOOOOOOOOOOO @@                                    @@ OOOOOOOOOOOOOOOO
OOOOOOOOOO'''''' @@                                    @@ ```````OOOOOOOOO
OOOOO'' aaa@@@@@@@@@@@@@@@@@@@@"""                   """""""""@@aaaa `OOOO
OOOOO,""""@@@@@@@@@@@@@@""""                                     a@"" OOOA
OOOOOOOOOoooooo,                                            |OOoooooOOOOOS
OOOOOOOOOOOOOOOOo,                                          |OOOOOOOOOOOOC
OOOOOOOOOOOOOOOOOO                                         ,|OOOOOOOOOOOOI
OOOOOOOOOOOOOOOOOO @          THE                          |OOOOOOOOOOOOOI
OOOOOOOOOOOOOOOOO'@           COMMAND                      OOOOOOOOOOOOOOb
OOOOOOOOOOOOOOO'a'            LINE                         |OOOOOOOOOOOOOy
OOOOOOOOOOOOOO''              MURDERS                      aa`OOOOOOOOOOOP
OOOOOOOOOOOOOOb,..                                          `@aa``OOOOOOOh
OOOOOOOOOOOOOOOOOOo                                           `@@@aa OOOOo
OOOOOOOOOOOOOOOOOOO|                                             @@@ OOOOe
OOOOOOOOOOOOOOOOOOO@                               aaaaaaa       @@',OOOOn
OOOOOOOOOOOOOOOOOOO@                        aaa@@@@@@@@""        @@ OOOOOi
OOOOOOOOOO~~ aaaaaa"a                 aaa@@@@@@@@@@""            @@ OOOOOx
OOOOOO aaaa@"""""""" ""            @@@@@@@@@@@@""               @@@|`OOOO'
OOOOOOOo`@@a                  aa@@  @@@@@@@""         a@        @@@@ OOOO9
OOOOOOO'  `@@a               @@a@@   @@""           a@@   a     |@@@ OOOO3
`OOOO'       `@    aa@@       aaa"""          @a        a@     a@@@',OOOO'
</code></pre></div>
<p dir="auto">There's been a murder in Terminal City, and TCPD needs your help.</p>
<p dir="auto">To figure out whodunit, you need access to a command line.</p>
<p dir="auto">Once you're ready, clone this repo, or <a href="https://github.com/veltman/clmystery/archive/master.zip">download it as a zip file</a>.</p>
<p dir="auto">Open a Terminal, go to the location of the files, and start by reading the file 'instructions'.</p>
<p dir="auto">One way you can do this is with the command:</p>

<p dir="auto">(<code>cat</code> is a command that will print the contents of the file called <code>instructions</code> for you to read.)</p>
<p dir="auto">To get started on how to use the command line, open cheatsheet.md or cheatsheet.pdf (from the command line, you can type 'nano cheatsheet.md').</p>
<p dir="auto">Don't use a text editor to view any files except these instructions, the cheatsheet, and hints.</p>
<h3 tabindex="-1" dir="auto">Credits</h3>
<p dir="auto">By Noah Veltman<br>
Projects: <a href="http://noahveltman.com/" rel="nofollow">noahveltman.com</a><br>
GitHub: <a href="https://github.com/veltman">veltman</a><br>
Twitter: <a href="https://twitter.com/veltman" rel="nofollow">@veltman</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Treemaps are awesome (301 pts)]]></title>
            <link>https://blog.phronemophobic.com/treemaps-are-awesome.html</link>
            <guid>36868940</guid>
            <pubDate>Tue, 25 Jul 2023 20:31:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.phronemophobic.com/treemaps-are-awesome.html">https://blog.phronemophobic.com/treemaps-are-awesome.html</a>, See on <a href="https://news.ycombinator.com/item?id=36868940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="Why-treemaps">Why treemaps?</h2><p>Treemaps are an underutilized visualization that are capable of generically summarizing data of many shapes and sizes. To date, they've mostly been used for displaying the files consuming all of your disk space, but with a few tweaks, treemaps can be a flexible tool for exploring and navigating messy data blobs.</p><p>Treemaps are space filling. You provide the bounds, and the the treemap algorithm will generate a graphic that uses all of the pixels. This is in contrast to something like pprint, which generates a view of the data that is proportional to the amount of data. Bounding the size of the visual representation has the advantage that treemaps scale gracefully for small to medium sized data.</p><p>Treemaps will use as many pixels as are available to represent the underlying data. In general, more pixels means more clarity. However, the treemap performs well even at relatively small sizes.</p><p><img alt="Sizes Example" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/sizes-example.gif"></p><p>Treemaps are very flexible. They can visualize any data that is tree-like which includes any data that can be represented as JSON or edn.</p><blockquote><p>treemapping is a method for displaying hierarchical data using nested figures, usually rectangles.<br></p></blockquote><p>At its heart, constructing a treemap is straightforward. Given some tree-like data and a rectangle, subdivide the rectangle into smaller rectangles for each of the tree's branches and then recursively apply the same algorithm for each branch.</p><div><pre><span>(</span><span>defn</span> <span>treemap</span> <span>[</span><span>tree-node</span> <span>rect</span><span>]</span>
  <span>(</span><span>if</span> <span>(</span><span>branch?</span> <span>tree-node</span><span>)</span>
    <span>(</span><span>let</span> <span>[</span><span>child-rects</span> <span>(</span><span>subdivide</span> <span>rect</span> <span>(</span><span>children</span> <span>tree-node</span><span>)</span><span>)</span><span>]</span>
      <span>(</span><span>mapcat</span> <span>(</span><span>fn</span> <span>[</span><span>child</span> <span>child-rect</span><span>]</span>
                <span>(</span><span>treemap</span> <span>child</span> <span>child-rect</span><span>)</span><span>)</span>
              <span>(</span><span>children</span> <span>tree-node</span><span>)</span>
              <span>child-rects</span><span>)</span><span>)</span>
    <span>[</span><span>rect</span><span>]</span><span>)</span><span>)</span>
</pre></div><p>The size of each rectangle is proportional to the size of the associated tree node and all of its children. The more descendants a tree node has, the bigger its rectangle. As an additional feature, the function that determines the size of leaves and branches can be parameterized, but for our examples, we will assume all leaves have a size of 1 and the size of a branch is the sum of the leaves under it.</p><p>Here's what the process of subdivision looks like.</p><p><img alt="basic subdivide" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/basic-subdivide.gif"></p><p>You can see that the naive treemap shows some of the structure of the data we're trying to visualize, but many elements of the data's structure aren't revealed in this basic treemap. Next, we'll look at a few tricks for improving our treemaps to capture more elements of our data's structure. The following is by no means an exhaustive list of techniques. In fact, there's tremendous room for experimentation and improvement.</p><h2 id="Improving-the-traditional-treemap">Improving the traditional treemap</h2><p>Treemaps are really good at using all the available pixels, but there's still a lot of work left deciding how best to use the pixels given to us. There are several metrics and aspects that are possible to visualize. Let's consider a few.</p><p><strong>Types:</strong> What types are used in the data?<br><strong>Shape:</strong> Is the data deep, shallow, thin, wide?<br><strong>Cardinality:</strong> How much data is there?<br><strong>Constraints:</strong> What properties must the data have for it to be considered valid?</p><h2 id="Types">Types</h2><p>One of the more obvious improvements is to paint the background of each rectangle with the type of the data it represents. Here's what that looks like:</p><p><img alt="Type Background" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/type-background.png"></p><p>Great, now we can see the types of our tree. However, there's a little snag in our plan. It turns out that for most JSON in the wild, the data is mostly comprised of just strings and numbers. It's really higher level data types that we would be interested in, but if we're interested in summarizing the data, it might be because don't have a schema handy. Automatically inferring data types is something we can work on, but let's move on to other options for now.</p><h2 id="Depth">Depth</h2><p>One of the issues with just showing types is that it doesn't tell us much about the actual structure of the data. Just from the types, we can't tell how deep or how wide the data is. If we're not using the background color to represent the types in the data, we can use it for depth:</p><p><img alt="Depth Background" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/depth-background.png"></p><p>Using the color for depth certainly illuminates whether or not our data structure is deep or wide, but it can still be difficult to decipher the structure of the example data. For example, "Which rectangles share the same parent?"</p><h3 id="Grouping">Grouping</h3><p>One way to visualize which rectangles share the same parent is to add a little padding around each level.</p><p><img alt="Depth Background" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/depth-padding.png"></p><p>Awesome. Just a little spacing helps track the different branches of the tree and see which elements share the same parents. However, there are some limitations with using only spacing to show grouping. How much padding should each level of the hierarchy have? Adding too little padding makes the hierarchies less apparent. Adding too much spacing can waste pixels that could otherwise be more effective. The shape of the data will also influence how much padding is necessary. Determining the amount of padding that works well on various different types of data is still an area that needs work.</p><h3 id="Hierarchy-Lines">Hierarchy Lines</h3><p>Another way to visualize the shape of data is to simply draw lines from parents to their children. We can even change the color of the line to show what type each collection is.</p><p><img alt="Line Depth" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/line-bare-demo.png"></p><p>The main drawback of hierarchy lines is that the lines can overlap and obscure descendant rectangles. We can partially alleviate the overlapping issue by reducing the hierarchy line's opacity near the top of the tree. However, for certain data shapes, the lines can still be an issue. Another way to declutter the graphic while still utilizing hierarchy lines is to allow the user to hover over the graphic and only show the hierarchy line of the element that is currently under the mouse.</p><p>Below is a visualization of the same data as above, but using the background to show depth and only showing the hierarchy lines when hovering.</p><p><img alt="Lines Interactive" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/lines-interactive-shrunk.gif"></p><h2 id="Labels">Labels</h2><p>For small examples it's possible to simply label all of the data.</p><pre><code>{:a [0 1 2 3 4],
 :b [0 1 2 3 4],
 :c [0 1 2 3 4]}
</code></pre><p><img alt="Simple Label Example" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/simple-label-example.png"></p><h2 id="Key-path-labels">Key path labels</h2><p>One common form of nesting is maps containing other maps. We can highlight important structural features of the data if we emphasize the map keys in our treemap.</p><p>Below is a treemap of all the public interns of namespaces on treemap-cljs's classpath that start with <code>clojure.*</code>.</p><p><img alt="Keyed Example" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/keyed-example.png"></p><p>As an additional help the user, we allow the user to hover over the data and show the key path that it would take to traverse the data to that leaf node.</p><p><img alt="Hover Keyed Example" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/hover-keypath-shrunk.gif"></p><p>Not only does showing the key path while hovering help show where the data is situated, we can use the key paths as part of the UI itself. As we hover over the keypath, watch as the area for that subsection of the tree is highlighted in the treemap graphic.</p><p><img alt="Hover Box Example" src="https://blog.phronemophobic.com/treemaps-are-awesome/images/keypath-box-hover-shrunk.gif"></p><h2 id="Comparisons-with-alternatives">Comparisons with alternatives</h2><p>There are several tools that help us to gain an intuition for a particular data representation. Let's compare treemaps with other options to see how treemaps can most effectively be used as part of the data exploration toolset.</p><h3 id="Treemaps">Treemaps</h3><p>Treemaps excel at displaying high level structure that is heirarchical, heterogeneous, and approximately square (ie. the data is about as many layers deep as it is wide). Treemaps struggle with data that is either (wide and shallow) or (thin and deep).</p><h3 id="pprint">pprint</h3><p><code>pprint</code> excels at small to medium size data, especially if the data fits on a single page. Once the data takes more than a page to <code>pprint</code>, then it can obscure the shape and structure of the data.</p><h3 id="Data-Browsers">Data Browsers</h3><p>Data browsers like <a href="https://github.com/cognitect-labs/REBL-distro">rebl</a> excel at scanning through data, but typically only show one level of the data at a time. Many data browsers allow graphical integrations, so hopefully treemaps will be <a href="https://github.com/phronmophobic/treemap-clj">integrated</a> within data browsers to allow for the "big picture" data summarization that treemaps provide.</p><h3 id="Schemas">Schemas</h3><p>Schemas excel at providing an abstract summary of data. Schemas have trouble with deeply nested data, data that are out of sync with the schema, and data that don't have a schema. Additionally, schemas don't usually detail real world usage. They often contain properties that are no longer used or have developed new meaning compared to what the original property name would suggest. Schemas are still very useful and can complement tools that work with concrete instances of data like treemaps, <code>pprint</code>, and data browsers.</p><h2 id="Future-Work">Future Work</h2><p><strong>More sophisticated layout</strong><br>Treemap layout in <code>treemap-clj</code> is fairly naive. The layout only considers one level at a time and only uses simple to heuristics to prefer squarish rectangles (aspect ratios close to 1) over long and thin rectangles.</p><p>Currently, layout of the treemap rectangles is only done one layer at at time. It should be possible to produce better (for various metrics of better) layouts by considering more than one layer of the tree at a time.</p><p><strong>Non rectangular treemaps</strong><br>All <code>treemap-clj</code> layouts subdivide rectangles into smaller rectangles. However, the <a href="https://www.uni-konstanz.de/mmsp/pubsys/publishedFiles/NoBr12a.pdf">literature</a> contains algorithms that subdivide areas into other shapes which could have interesting applications.</p><p><strong>Interactive depth rendering</strong><br>Allowing a user to interactively render a treemap up to X level of depth is likely to be an interesting way of exploring a data structure.</p><p><strong>Alternative coloring schemes</strong><br>Only two uses of color have been presented as part of <code>treemap-clj</code>, depth and data type. Other coloring schemes should be investigated.</p><p><strong>Alternative size functions</strong><br>As noted above, all <code>treemap-clj</code> implementations use a leaf size of 1. Plugging in different sizing functions would allow the user to emphasize different elements of a data structure.</p><p><strong>Use Layout direction to encode more information</strong><br>If you look at other visualization graphics, the directions on the chart typically encode information (eg. a financial chart that goes up and to the right is usually a positive sign). The directions on a treemap don't encode any meaning. It should be possible to place rectangles with certain properties close to either the edges or towards the center to emphasize different qualities of the data.</p><p><strong>Graphic Design</strong><br>I'm bad at graphic design and have probably violated innumerable graphic design principles. Incorporating graphic design expertise would greatly increase clarity and legibility.</p><p><strong>Constraints, schemas and specs</strong><br>Formal data specifications encode a ton of information. Encoding these specifications into the treemap graphic should increase the information density.</p><p><strong>Zooming in and out</strong><br>Just like with geographic maps, it should be possible to zoom in on a part of a treemap to reveal more detail or zoom out to view higher level data features.</p><h2 id="Further-Reading">Further Reading</h2><p>Visualizing business information using generalized treemaps<br><a href="https://pure.tue.nl/ws/files/47041749/631721-1.pdf">https://pure.tue.nl/ws/files/47041749/631721-1.pdf</a></p><p>Visualizing Business Data with Generalized Treemaps<br><a href="https://ieeexplore.ieee.org/document/4015431">https://ieeexplore.ieee.org/document/4015431</a></p><p>Computing Voronoi Treemaps<br><a href="https://www.uni-konstanz.de/mmsp/pubsys/publishedFiles/NoBr12a.pdf">https://www.uni-konstanz.de/mmsp/pubsys/publishedFiles/NoBr12a.pdf</a></p><p>Fast Dynamic Voronoi Treemaps<br><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/isvd.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/isvd.pdf</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Alpine Linux does not make the news (125 pts)]]></title>
            <link>https://drewdevault.com/2023/07/25/Alpine-does-not-make-news.html</link>
            <guid>36867992</guid>
            <pubDate>Tue, 25 Jul 2023 19:24:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://drewdevault.com/2023/07/25/Alpine-does-not-make-news.html">https://drewdevault.com/2023/07/25/Alpine-does-not-make-news.html</a>, See on <a href="https://news.ycombinator.com/item?id=36867992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    <p>My Linux distribution of choice for several years has been <a href="https://alpinelinux.org/">Alpine Linux</a>.
It’s a small, efficient distribution which ships a number of tools I appreciate
for their simplicity, such as musl libc. It has a very nice package manager,
apk, which is fast and maintainable. The development community is professional
and focuses on diligent maintenance of the distribution and little else. Over
the years I have used it, very little of note has happened.</p>
<p>I run Alpine in every context; on my workstation and my laptops but also on
production servers, on bare-metal and in virtual machines, on my RISC-V and ARM
development boards, at times on my phones, and in many other contexts besides.
It has been a boring experience. The system is simply reliable, and the upgrades
go over without issue every other quarter,<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> accompanied by high-quality
release notes. I’m pleased to maintain several dozen packages in the
repositories, and the community is organized such that it is easy for someone
like me to jump in and do the work required to maintain it for my use-cases.</p>
<p>Red Hat has been in the news lately for their moves to monetize the
distribution, moves that I won’t comment on but which have generally raised no
small number of eyebrows, written several headlines, and caused intense
flamewars throughout the internet. I don’t run RHEL or CentOS anywhere, in
production or otherwise, so I just looked curiously on as all of this took place
without calling for any particular action on my part. Generally speaking, Alpine
does not make the news.</p>
<p>And so it has been for years, as various controversies come about and die off,
be it with Red Hat, Ubuntu, Debian, or anything else, I simply keep running “apk
upgrade” every now and then and life goes on uninterrupted. I have high-quality,
up-to-date software on a stable system and suffer from no fuss whatsoever.</p>
<p>The Alpine community is a grassroots set of stakeholders who diligently concern
themselves with the business of maintaining a good Linux distribution. There is
little in the way of centralized governance;<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> for the most part the
distribution is just quietly maintained by the people who use it for the purpose
of ensuring its applicability to their use-cases.</p>
<p>So, Alpine does not make the news. There are no commercial entities which are
trying to monetize it, at least no more than the loosely organized coalition of
commercial entities like SourceHut that depend on Alpine and do their part to
keep it in good working order, alongside various users who have no commercial
purpose for the system. The community is largely in unanimous agreement about
the fundamental purpose of Alpine and the work of the community is focused on
maintaining the project such that this purpose is upheld.</p>
<p>This is a good trait for a Linux distribution to have.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Or more frequently on edge, which I run on my workstation and laptops and
which receives updates shortly after upstream releases for most software. <a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>There’s some. They mostly concern themselves with technical decisions like
whether or not to approve new committers or ports, things like that. <a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>

  </article>
</div><section>
  <h2>
    Articles from blogs I read
    <small>
      Generated by
      <a href="https://git.sr.ht/~sircmpwn/openring">openring</a>
    </small>
  </h2>
  <section>
    
    <div>
      <h4>
        <a href="https://go.dev/blog/survey2023-h2" target="_blank" rel="noopener">Share your feedback about developing with Go</a>
      </h4>
      <p>Help shape the future of Go by sharing your thoughts via the Go Developer Survey</p>
      <p><small>
        via <a href="https://blog.golang.org/feed.atom">The Go Blog</a>
      </small>
      <small>July 25, 2023</small>
    </p></div>
    
    <div>
      <h4>
        <a href="https://emersion.fr/blog/2023/status-update-55/" target="_blank" rel="noopener">Status update, July 2023</a>
      </h4>
      <p>Hi all!
As usual, this month has been rich in Wayland-related activities. Rose has
continued building and upstreaming better frame scheduling infrastructure for
wlroots, you can read more on her blog. I’ve resurrected
an old patch to make wlroots behave bette…</p>
      <p><small>
        via <a href="https://emersion.fr/blog/">emersion</a>
      </small>
      <small>July 19, 2023</small>
    </p></div>
    
    <div>
      <h4>
        <a href="https://100r.co/site/log.html#jun2023" target="_blank" rel="noopener">Summary of changes for June 2023</a>
      </h4>
      <p>
Hey everyone! This is the list of all the changes we've done to our projects and apps during the month of June. We'll also be reporting in our on position in the world, and on our future plans.

Summary Of Changes

  100r.co, added propeller maintenan…</p>
      <p><small>
        via <a href="https://100r.co/">Hundred Rabbits</a>
      </small>
      <small>June 1, 2023</small>
    </p></div>
    
  </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Texas A&M suspended professor accused of criticizing Lt. Gov. Patrick in lecture (213 pts)]]></title>
            <link>https://www.texastribune.org/2023/07/25/texas-a-m-professor-opioids-dan-patrick/</link>
            <guid>36867988</guid>
            <pubDate>Tue, 25 Jul 2023 19:24:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.texastribune.org/2023/07/25/texas-a-m-professor-opioids-dan-patrick/">https://www.texastribune.org/2023/07/25/texas-a-m-professor-opioids-dan-patrick/</a>, See on <a href="https://news.ycombinator.com/item?id=36867988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
    
      <p><em><a href="https://www.texastribune.org/newsletters/the-brief/?utm_medium=website&amp;utm_source=trib-ads-owned&amp;utm_campaign=trib-marketing&amp;utm_term=inline-CTA-brief">Sign up for The Brief</a>, The Texas Tribune’s daily newsletter that keeps readers up to speed on the most essential Texas news.</em>
</p>
<hr>
    
      <p>Joy Alonzo, a respected opioid expert, was in a panic.</p>

    
      <p>The Texas A&amp;M University professor had just returned home from giving a routine lecture on the opioid crisis at the University of Texas Medical Branch when she learned a student had accused her of disparaging Lt. Gov. <a href="https://www.texastribune.org/directory/dan-patrick/">Dan Patrick</a> during the talk.</p>

    
      <p>In the few hours it took to drive from Galveston, the complaint had made its way to her supervisors, and Alonzo’s job was suddenly at risk.</p>

    
      <p>“I am in a ton of trouble. Please call me!” she wrote to Chandler Self, the UTMB professor who invited her to speak.</p>

    
      


    
      <p>Alonzo was right to be afraid. Not only were her supervisors involved, but so was Chancellor John Sharp, a former state comptroller who now holds the highest-ranking position in the Texas A&amp;M University System, which includes 11 public universities and 153,000 students. And Sharp was communicating directly with the lieutenant governor’s office about the incident, promising swift action.</p>

    
      <p>Less than two hours after the lecture ended, Patrick’s chief of staff had sent Sharp a link to Alonzo’s professional bio.</p>

    
      <p>Shortly after, Sharp sent a text directly to the lieutenant governor: “Joy Alonzo has been placed on administrative leave pending investigation re firing her. shud [sic] be finished by end of week.”</p>

    
      <p>The text message was signed “jsharp.”</p>

    
      


    
      <p>For free speech advocates, health experts and students, Texas A&amp;M’s investigation of Alonzo was a shocking demonstration of how quickly university leaders allow politicians to interfere in classroom discussions on topics in which they are not experts — and another example of increasing political involvement from state leaders in how Texas universities are managed.</p>

    
      <p>The revelation comes as Texas A&amp;M is reeling over concerns that the university allowed politically motivated outsiders to derail the hiring of <a href="https://www.texastribune.org/2023/07/11/texas-a-m-kathleen-mcelroy-journalism/">Kathleen McElroy</a>, a Black journalism professor at the University of Texas at Austin, to revive the journalism school at Texas A&amp;M. The subsequent outcry over how Texas A&amp;M handled the situation prompted the <a href="https://www.texastribune.org/2023/07/21/tamu-president-resign-journalism/">university president to resign last week</a>, and the interim dean of arts and sciences stepped down from that role but will remain a professor.</p>

    
      <p>In an email obtained by The Texas Tribune through a public records request, Alonzo told Self the investigation had been kicked off by a student “who has ties to Texas A&amp;M Leadership.”</p>

    
      <p>The Texas A&amp;M system confirmed the series of phone calls and text messages that led to Alonzo’s investigation was kicked off by Texas Land Commissioner Dawn Buckingham, a graduate of UTMB’s medical school. The Tribune confirmed her daughter, a first-year medical student at the time, attended Alonzo’s lecture. Buckingham served six years in the Texas Senate with Patrick, who endorsed her run for land commissioner last year, and she recently attended Sharp’s wedding in May.</p>

    
      


    
      <p>Buckingham declined to comment.</p>

    
      <p>A few hours after Texas A&amp;M started looking into the complaint, course leaders at UTMB sent an email to students in the class saying Alonzo’s comments “about Lieutenant Governor Dan Patrick and his role in the opioid crisis” did not represent the opinion of the university.</p>

    
      <p>The email also included a “formal censure” of Alonzo, although it did not specify what she said that was offensive.</p>

    
      <p>Neither UTMB nor Texas A&amp;M would confirm what Alonzo said that prompted such a reaction, and UTMB students interviewed by the Tribune recalled a vague reference to Patrick’s office but nothing specific.</p>

    
      


    
      <p>UTMB declined to comment for this story, and Alonzo declined to be interviewed.</p>

    
      <p>Ultimately Texas A&amp;M allowed Alonzo to keep her job after an internal investigation could not confirm any wrongdoing.</p>

    
      <p>In a statement, Texas A&amp;M University System spokesperson Laylan Copelin said Sharp’s text to Patrick was a “typical update,” saying it is not unusual for the chancellor to “keep elected officials informed when something at Texas A&amp;M might interest them.”</p>

    
      <p>“It is not unusual to respond to any state official who has concerns about anything occurring at the Texas A&amp;M System,” said Copelin, who said the system followed standard procedure to look into the claim.</p>

    
      


    
      <p>Patrick did not respond to a request for comment.</p>

    
      <p>Adam Steinbaugh, an attorney with the Foundation for Individual Rights and Expression, a nonprofit legal group focused on protecting free speech on college campuses, said “it would be highly inappropriate for a university to conduct an investigation if a faculty member says something critical of a state leader or a government official.”</p>

    
      <p>“That is, I think, a misuse of institutional resources, and it’s one that will have a chilling effect and that has a chilling effect even if you wind up clearing the professor,” Steinbaugh said.</p>

    
      <p>A day after the complaint about Alonzo’s talk, Marcia Ory, a professor at Texas A&amp;M Health and co-chair of the university’s Opioid Task Force with Alonzo, warned about the long-term consequences.</p>

    
      


    
      <p>“The incident in Galveston yesterday is probably an indicator of how sensitive and politically charged this topic is and the need to tread lightly and be aware that anything can be taken out of context,” Ory wrote in an email to Jon Mogford, vice president of Texas A&amp;M Health.</p>

    
      <p>“It’s a shame because all we want is to make people aware of harm-reduction strategies that can save lives, especially among youth and young adults who are especially vulnerable these days,” wrote Ory, who did not respond to a request for comment.</p>

    
      <h3 id="78fe6955-84d0-462c-9b97-36fa55d86a66">An expert with a solid reputation</h3>

    
      <p>Alonzo has spent more than two decades as a pharmacist in Japan, Missouri and elsewhere, and has taught college students in Texas for more than a decade. She now teaches at Texas A&amp;M while working as an ambulatory care pharmacy director at a free health clinic in Bryan.</p>

    
      <p>She has helped bring millions of federal research dollars to the university, and last year Texas A&amp;M’s pharmacy school <a href="https://pharmacy.tamu.edu/awards/index.html#tab-panel-5">named her the early career researcher of the year</a>.</p>

    
      


    
      <p>One of Alonzo’s recent projects focuses on training people to use Narcan, a nasal spray that reverses opioid effects and can save lives in overdose cases. She’s also advised state leaders on other public policies that could improve the fight against opioid overdoses.</p>

    
      <p>Fentanyl, a synthetic opioid often illegally manufactured by Mexican drug cartels, is a growing problem. Between 2019 and 2021, overdose deaths involving fentanyl in the state rose nearly 400%.</p>

    
      <p>This year, Gov. <a href="https://www.texastribune.org/directory/greg-abbott/">Greg Abbott</a> declared cracking down on fentanyl as one of his seven priority issues for the legislative session.</p>

    
      <p>Lawmakers allocated <a href="https://www.texastribune.org/2023/05/27/texas-legislature-budget/">$18 million over the next two years toward providing naloxone</a>, an opioid-reversing drug, to police, schools and community organizations on the front lines of the epidemic. To improve the government’s response to overdose spikes, they also passed laws requiring police and other public entities to report overdoses to a public health agency.</p>

    
      


    
      <p>But instead of backing other recommended strategies to reduce overdose deaths, such as legalizing test strips that can detect the presence of fentanyl in other drugs, lawmakers focused on a more <a href="https://www.texastribune.org/2023/05/16/texas-fentanyl-murder-legislature/">punitive approach</a>, approving laws that increase criminal penalties for providing fentanyl that leads to an overdose death.</p>

    
      <p>Public health experts like Alonzo have largely supported harm-reduction efforts rather than increasing punishments for drug users. As the crisis intensified, Alonzo often received urgent emails from Texas school districts and law enforcement agencies eager for training and naloxone kits. In the past, she estimated she had given away <a href="https://www.texastribune.org/2022/08/03/texas-narcan-opioids/">more than $4.5 million worth of naloxone</a> through her training sessions.</p>

    
      <h3 id="5eb9112d-066a-47c3-a240-7a994e79f4af">Statement of formal censure</h3>

    
      <p>Self, the professor at UTMB, scheduled Alonzo to give the lecture to the first-year medical students months in advance.</p>

    
      <p>“I can’t tell you enough how much the students value this presentation,” Self wrote in October, according to emails obtained through an open records request. “I get feedback all the time from them telling me how important they view this talk. They’ll come up to me even months later to tell me.”</p>

    
      


    
      <p>On March 7, the two started the day with breakfast at the laid-back Mosquito Cafe in Galveston before heading to the lecture, which was mandatory for students to attend.</p>

    
      <p>The lecture was not recorded, but according to presentation slides obtained by the Tribune through an open records request, Alonzo gave students a broad overview of the opioid crisis and the science behind opioids. She walked them through how to prevent opioid deaths, how to recognize an overdose and how to administer naloxone. She even touched on what to do if a police dog was exposed to fentanyl.</p>

    
      <p>The slides show that Alonzo discussed how a lack of infrastructure limits the state’s ability to respond to the crisis, noting that many Texas counties lack a medical examiner; reporting on opioid deaths by emergency rooms is infrequent; and many law enforcement agencies and local health departments don’t track opioid deaths.</p>

    
      <p>This means the U.S. Centers for Disease Control and Prevention considers Texas a nonreporter when it comes to opioid data, which makes it more difficult for researchers to receive grants to tackle the issue. (Alonzo gave her presentation before the Legislature passed new reporting laws this year.)</p>

    
      


    
      <p>The lecture ended around noon. Afterward, students gathered at the front of the class to grab free naloxone kits provided by Alonzo. Some stuck around to ask Alonzo questions.</p>

    
      <p>The course’s instructors gave no indication anything had gone awry.</p>

    
      <p>Alonzo got in her car and started her two-and-a-half-hour journey home.</p>

    
      <p>At 4:22 p.m., as Alonzo was learning that a controversy was brewing, a course coordinator sent an email to the entire class distancing UTMB from comments Alonzo allegedly made about Patrick. The subject line read, “STATEMENT OF FORMAL CENSURE.”</p>

    
      


    
      <p>“The statements made by the guest lecturer do not represent the opinion or position of the University of Texas Medical Branch, nor are they considered as core curriculum content for this course,” the email said.</p>

    
      <p>“UTMB does not support or condone these comments. We take these matters very seriously and wish to express our disapproval of the comment and apologize for harm it may have caused for members of our community,” the email continued. “We hereby issue a formal censure of these statements and will take steps to ensure that such behavior does not happen in the future.”</p>

    
      <p>The email did not specify what comments had led to the censure.</p>

    
      <p>The trouble had started several hours earlier when Buckingham called Patrick to alert him that an A&amp;M professor had made negative comments about him during a guest lecture at UTMB, said Copelin, the A&amp;M system spokesperson. Buckingham then called Jenny Jones, the university system’s vice chancellor for governmental relations.</p>

    
      <p>Copelin said a text message had alerted Buckingham of the comments, but he did not provide information on who sent the text message.</p>

    
      <p>Patrick then called Sharp and Kevin Eltife, the chair of the University of Texas System’s board, Copelin said. The call between Sharp and Patrick was short. Patrick’s chief of staff, Darrell Davila, followed with the text to Sharp that linked to Alonzo’s faculty page. Eltife declined to comment.</p>

    
      <p>Sharp had a staff member look into the complaint and that staff member asked then-A&amp;M President M. Katherine Banks' office to investigate.</p>

    
      <p>Copelin said Sharp’s request went through the chain of command at A&amp;M’s Health Science Center and ended up with Kevin McGinnis, the system’s vice president and chief compliance officer.</p>

    
      <p>At the same time, the government relations team alerted the Health Science Center and the pharmacy school, which are affiliated with Alonzo, Copelin said.</p>

    
      <p>A&amp;M officials received a copy of UTMB’s censure statement and reached out for more information, but UTMB did not cooperate, Copelin said.</p>

    
      <p>“By the close of the day, McGinnis decided to put Alonzo on paid leave and investigate to determine what really happened,” Copelin said in a statement.</p>

    
      <p>As the situation developed, A&amp;M officials updated Patrick and his team.</p>

    
      <p>At 4:43 p.m., just 15 minutes after UTMB sent its official censure letter, Jones alerted Patrick’s deputy chief of staff, Marian Wallace, that the investigation was underway.</p>

    
      <p>“joy alonzo placed on administrative leave pending firing investigation this week js,” read the message from Jones obtained by the Tribune through a public records request.</p>

    
      <p>Copelin said the university’s handling of the complaint against Alonzo followed standard procedure and appropriately updated the relevant lawmakers on the investigation’s progress.</p>

    
      <p>“The investigation into the matter was a reasonable step to take, particularly after UTMB issued a public statement ‘censuring’ one of our faculty members,” he said. “In fact, it would have been irresponsible not to look into it.”</p>

    
      <p>Texas A&amp;M would not answer questions about what specific policy Alonzo may have violated with her comments or provide documents pertaining to the investigation, citing state law that allows a university to withhold such information if a person is cleared of wrongdoing.</p>

    
      <p>The timing of the complaint came as the legislative session was heating up. Universities, including Texas A&amp;M, were making pitches to lawmakers to devote some of the state’s multibillion-dollar surplus to fund special projects.</p>

    
      <p>Alonzo’s predicament also comes as Texas universities are dealing with increasing government involvement in ostensibly independent public universities, particularly at the hand of Patrick, whom Alonzo was accused of criticizing. This year, Texas lawmakers banned diversity, equity and inclusion offices on college campuses, a priority for Patrick. These offices target underrepresented groups on campus to help them succeed, but critics accused them of pushing “woke,” left-leaning ideology on students and faculty.</p>

    
      <p>Patrick also prioritized a bill that would limit certain conversations about race and gender in college classrooms. When professors at UT-Austin publicly reaffirmed their academic freedom to teach critical race theory last year, Patrick pledged to ban tenure in public universities. Ultimately, that proposal was unsuccessful, but faculty say the broad attack on higher education has made Texas a less appealing and more difficult place to work.</p>

    
      <h3 id="726d246a-82c6-4ff6-878e-b272043da1fa">Students scramble to understand what happened</h3>

    
      <p>When students at UTMB received the email hours after the lecture, several started texting each other, trying to figure out what Alonzo had said that was so offensive.</p>

    
      <p>According to one student who asked to remain anonymous for fear of retaliation from the school, some students wondered if it was when Alonzo said that the lieutenant governor’s office was one of the reasons it’s hard for drug users to access certain care for opioid addiction or overdoses.</p>

    
      <p>A second student who also asked to remain anonymous for the same reason said Alonzo made a comment that the lieutenant governor’s office had opposed policies that could have prevented opioid-related deaths, and by doing so had allowed people to die.</p>

    
      <p>A third student who also spoke on the condition of anonymity said Alonzo talked about how policies, like the state’s ban on fentanyl test strips, have a direct impact on the ability to prevent opioid overdoses and deaths. A push to legalize the test strips died earlier this year in the Patrick-led Senate <a href="https://www.texastribune.org/2022/12/01/greg-abbott-fentanyl-strips-opioid-overdose/">despite support from top Republicans</a>, including Abbott.</p>

    
      <p>All of the students interviewed said they felt Alonzo’s comments were accurate and they were not offended by anything in the presentation.</p>

    
      <p>In a statement provided by Copelin, the A&amp;M system spokesperson, Alonzo said “her remarks were mischaracterized and taken out of context,” but she did not confirm exactly what the comments were.</p>

    
      <p>“She added that she had no issue with how the university handled the situation,” Copelin said.</p>

    
      <p>The third student at UTMB said the email from the school was frustrating because it was unclear which comments the university found problematic.</p>

    
      <p>“We’ve been left wondering exactly what it was they objected to,” the student said. “That vagueness just leads to some more self-censorship, since it’s hard to tell what is and isn’t allowed.”</p>

    
      <p>Steinbaugh, an attorney with the legal nonprofit FIRE, said schools have the right to criticize an employee or guest speaker for statements they make, but issuing a formal censure sends a strong and unambiguous message.</p>

    
      <p>“That is a suggestion that if you repeat this language or these criticisms, then you will be subject to disciplinary consequences that go beyond formal censure,” he said. “That is a way to really put an exclamation point on the chilling effect.”</p>

    
      <p>In a statement last week to faculty who were upset about the fallout over the botched hiring of McElroy to the journalism department, Sharp expressed concern about outside influences in the hiring and promotion of faculty, saying it was “never welcome, nor invited.”</p>

    
      <p>Sharp said he only participates in hiring questions over the school’s president and vice chancellors for agriculture and engineering.</p>

    
      <p>“Other than that, I don’t believe it is my place to be part of the hiring process for faculty,” he wrote.</p>

    
      <h3 id="63d5c3e1-348a-4dad-aa96-c99b1f7f5ab1">Fear of a chilling effect on life-saving information</h3>

    
      <p>A few hours after Alonzo reached out to Self about the trouble she was in, she finally heard back. But the tone of the email was notably different from the earlier cordial exchanges.</p>

    
      <p>Self said she did not record the lecture and noted that “all further correspondence will be funneled through our Office of Education.”</p>

    
      <p>Self referred a request for comment by the Tribune to UTMB’s media relations department, which declined to discuss the situation.</p>

    
      <p>Meanwhile, emails obtained through an open records request show that opioid experts and advocates across the state started sending Alonzo letters of support that evening.</p>

    
      <p>“I’ve never seen her to be anything other than professional, knowledgeable, and compassionate,” wrote Kathy Posey, who helped start the <a href="https://www.facebook.com/groups/mocope/">Montgomery County Overdose Prevention Endeavor</a>, an opioid overdose awareness group made up of people whose family members have been addicted to opioids or died from an overdose.</p>

    
      <p>Lucas Hill, a clinical associate professor of pharmacy at the University of Texas at Austin, wrote in his letter that Alonzo was not a divisive educator.</p>

    
      <p>“While I was not present during her guest lecture at the University of Texas Medical Branch this morning, my interactions with Dr. Alonzo gives me great confidence that she engages learners in discussions of controversial topics with the professionalism and restraint described in established principles of academic freedom,” he wrote.</p>

    
      <p>The stakes are high for professors who simultaneously work in their fields and teach, many of whom, like Alonzo, do not have tenure. And it raises concerns that medical experts working on high-stakes issues like the opioid crisis might withhold important, life-saving information out of fear of reprimand or punishment.</p>

    
      <p>“When we’re dealing with basic life-saving interventions, chilling effects can have much more deep consequences,” said Aaron Ferguson, an addiction treatment expert in Austin who works with researchers at public universities to combat opioid overdoses. “People don't feel emboldened to share basic science that could save people’s lives.”</p>

    
      <h3 id="27a2cc16-e71b-4278-ae93-73e19e552669">“Some members of the audience” were offended</h3>

    
      <p>On March 21, two weeks after she was placed on paid leave, Alonzo received an email saying her leave had been lifted.</p>

    
      <p>The following day, pharmacy school Dean George Udeani said in a memo to Alonzo that during the lecture she “related an anecdote and an interaction with a state official.”</p>

    
      <p>“I understand that your comment did not assign blame. However, some members of the audience felt that your anecdote was offensive,” he wrote.</p>

    
      <p>“While it is important to preserve and defend academic freedom and as such be able to discuss and present to students and the public the results of research observations and strategies, you should be mindful of how you present your views,” Udeani said.</p>

    
      <p><i>Disclosure: Texas A&amp;M University, Texas A&amp;M University System, University of Texas at Austin, University of Texas System and Kathleen McElroy have been financial supporters of The Texas Tribune, a nonprofit, nonpartisan news organization that is funded in part by donations from members, foundations and corporate sponsors. Financial supporters play no role in the Tribune's journalism. Find a complete <a href="https://www.texastribune.org/support-us/corporate-sponsors/">list of them here</a>.</i></p>

    
      <hr>

    
      <p><em>Join us for conversations that matter with newly announced speakers at the <a href="https://festival-platform.texastribune.org/?promo=site-story-footer&amp;tr=true" rel="noopener" target="_blank">2023 Texas Tribune Festival</a>, in downtown Austin from Sept. 21-23.</em></p>

    
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PRQL: Pipelined Relational Query Language (473 pts)]]></title>
            <link>https://github.com/PRQL/prql</link>
            <guid>36866861</guid>
            <pubDate>Tue, 25 Jul 2023 18:13:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/PRQL/prql">https://github.com/PRQL/prql</a>, See on <a href="https://news.ycombinator.com/item?id=36866861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">PRQL</h2>

<p dir="auto"><a href="https://prql-lang.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/9d310183f968381c446a43f1225521cfe7755af675ba04d7cc648d1aa3f348a8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f494e54524f2d5745422d626c75653f7374796c653d666f722d7468652d6261646765" alt="Website" data-canonical-src="https://img.shields.io/badge/INTRO-WEB-blue?style=for-the-badge"></a>
<a href="https://prql-lang.org/playground" rel="nofollow"><img src="https://camo.githubusercontent.com/68e68473f42ca8d5d31d7a86e27bd1d2a9b08425a959073425d3163bda127e39/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f494e54524f2d504c415947524f554e442d626c75653f7374796c653d666f722d7468652d6261646765" alt="Playground" data-canonical-src="https://img.shields.io/badge/INTRO-PLAYGROUND-blue?style=for-the-badge"></a>
<a href="https://prql-lang.org/book" rel="nofollow"><img src="https://camo.githubusercontent.com/91acdbb5a1171a4092ebccf530cd2e015b67a8264fc41402d283b9a201a76a29/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f43532d424f4f4b2d626c75653f7374796c653d666f722d7468652d6261646765" alt="Language Docs" data-canonical-src="https://img.shields.io/badge/DOCS-BOOK-blue?style=for-the-badge"></a>
<a href="https://discord.gg/eQcfaCmsNc" rel="nofollow"><img src="https://camo.githubusercontent.com/3d5cec0c7f1816c3d516aa343bba76dd86bbfaa93a3d7c3436e23756b3cf9555/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3933363732383131363731323331363938393f6c6162656c3d646973636f726425323063686174267374796c653d666f722d7468652d6261646765" alt="Discord" data-canonical-src="https://img.shields.io/discord/936728116712316989?label=discord%20chat&amp;style=for-the-badge"></a></p>



<p dir="auto"><a href="https://github.com/PRQL/prql/actions?query=branch%3Amain+workflow%3Atest-all"><img src="https://camo.githubusercontent.com/743fbd692fee4a030f7eb1b37e96f271fa2091fa2a0a7b3f52480be2fb265e12/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f5052514c2f7072716c2f70756c6c2d726571756573742e79616d6c3f6272616e63683d6d61696e266c6f676f3d676974687562267374796c653d666f722d7468652d6261646765" alt="GitHub CI Status" data-canonical-src="https://img.shields.io/github/actions/workflow/status/PRQL/prql/pull-request.yaml?branch=main&amp;logo=github&amp;style=for-the-badge"></a>
<a href="https://github.com/PRQL/prql/graphs/contributors"><img src="https://camo.githubusercontent.com/efe23154c6fb42a3b0c2ce0c1f482b687ab7303567211d6f84d8e10a6cdc42f6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f5052514c2f7072716c3f7374796c653d666f722d7468652d6261646765" alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/PRQL/prql?style=for-the-badge"></a>
<a href="https://github.com/PRQL/prql/stargazers"><img src="https://camo.githubusercontent.com/aa2a20c1221a4e5148eef3102bb622d4bee6cd6b6df32e474f54adaaf016c927/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5052514c2f7072716c3f7374796c653d666f722d7468652d6261646765" alt="Stars" data-canonical-src="https://img.shields.io/github/stars/PRQL/prql?style=for-the-badge"></a></p>
<p dir="auto"><strong>P</strong>ipelined <strong>R</strong>elational <strong>Q</strong>uery <strong>L</strong>anguage, pronounced "Prequel".</p>
<p dir="auto">PRQL is a modern language for transforming data — a simple, powerful, pipelined
SQL replacement. Like SQL, it's readable, explicit and declarative. Unlike SQL,
it forms a logical pipeline of transformations, and supports abstractions such
as variables and functions. It can be used with any database that uses SQL,
since it compiles to SQL.</p>
<p dir="auto">PRQL can be as simple as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from tracks
filter artist == &quot;Bob Marley&quot;                 # Each line transforms the previous result
aggregate {                                   # `aggregate` reduces each column to a value
  plays    = sum plays,
  longest  = max length,
  shortest = min length,                      # Trailing commas are allowed
}"><pre><span>from </span>tracks
<span>filter </span>artist <span>==</span> <span><span>"</span>Bob Marley<span>"</span></span>                 # <span>Each</span> line transforms the previous result
<span>aggregate </span><span><span>{</span></span>                                   # <span>`aggregate`</span> reduces each column to a value
  plays    <span>=</span> sum plays<span><span>,</span></span>
  longest  <span>=</span> max length<span><span>,</span></span>
  shortest <span>=</span> min length<span><span>,</span></span>                      # <span>Trailing</span> commas are allowed
<span><span>}</span></span></pre></div>
<p dir="auto">Here's a fuller example of the language;</p>
<div dir="auto" data-snippet-clipboard-copy-content="from employees
filter start_date > @2021-01-01               # Clear date syntax
derive {                                      # `derive` adds columns / variables
  gross_salary = salary + (tax ?? 0),         # Terse coalesce
  gross_cost = gross_salary + benefits_cost,  # Variables can use other variables
}
filter gross_cost > 0
group {title, country} (                      # `group` runs a pipeline over each group
  aggregate {                                 # `aggregate` reduces each group to a value
    average gross_salary,
    sum_gross_cost = sum gross_cost,          # `=` sets a column name
  }
)
filter sum_gross_cost > 100_000               # `filter` replaces both of SQL's `WHERE` &amp; `HAVING`
derive id = f&quot;{title}_{country}&quot;              # F-strings like Python
derive country_code = s&quot;LEFT(country, 2)&quot;     # S-strings allow using SQL as an escape hatch
sort {sum_gross_cost, -country}               # `-country` means descending order
take 1..20                                    # Range expressions (also valid here as `take 20`)"><pre><span>from </span>employees
<span>filter </span>start_date <span>&gt;</span> @<span>2021</span><span>-</span><span>01</span><span>-</span><span>01</span>               # <span>Clear</span> date syntax
<span>derive </span><span><span>{</span></span>                                      # <span>`derive`</span> adds columns <span>/</span> variables
  gross_salary <span>=</span> salary <span>+</span> <span>(</span>tax <span>??</span> <span>0</span><span>)</span><span><span>,</span></span>         # <span>Terse</span> coalesce
  gross_cost <span>=</span> gross_salary <span>+</span> benefits_cost<span><span>,</span></span>  # <span>Variables</span> can use other variables
<span><span>}</span></span>
<span>filter </span>gross_cost <span>&gt;</span> <span>0</span>
<span>group </span><span><span>{</span></span>title<span><span>,</span></span> country<span><span>}</span></span> <span>(</span>                      # <span>`group`</span> runs a pipeline over each group
  aggregate <span><span>{</span></span>                                 # <span>`aggregate`</span> reduces each group to a value
    average gross_salary<span><span>,</span></span>
    sum_gross_cost <span>=</span> sum gross_cost<span><span>,</span></span>          # `<span>=</span>` sets a column name
  <span><span>}</span></span>
<span>)</span>
<span>filter </span>sum_gross_cost <span>&gt;</span> 100_000               # <span>`filter`</span> replaces both <span>of </span><span>SQL</span>'s <span>`WHERE`</span> <span>&amp;</span> <span>`HAVING`</span>
<span>derive </span>id <span>=</span> f<span><span>"</span>{title}_{country}<span>"</span></span>              # <span>F</span><span>-</span>strings like <span>Python</span>
<span>derive </span>country_code <span>=</span> s<span><span>"</span>LEFT(country, 2)<span>"</span></span>     # <span>S</span><span>-</span>strings allow using <span>SQL</span> <span>as </span>an escape hatch
<span>sort </span><span><span>{</span></span>sum_gross_cost<span><span>,</span></span> <span>-</span>country<span><span>}</span></span>               # `<span>-</span>country` means descending order
<span>take </span><span>1</span><span>..</span><span>20</span>                                    # <span>Range</span> expressions <span>(</span>also valid here <span>as </span>`take <span>20</span>`<span>)</span></pre></div>
<p dir="auto">For more on the language, more examples &amp; comparisons with SQL, visit
<a href="https://prql-lang.org/" rel="nofollow">prql-lang.org</a>. To experiment with PRQL in the browser, check out
<a href="https://prql-lang.org/playground" rel="nofollow">PRQL Playground</a>.</p>
<h2 tabindex="-1" dir="auto">Current Status - April 2023</h2>
<p dir="auto">PRQL is being actively developed by a growing community. It's ready to use by
the intrepid, either with our supported integrations, or within your own tools,
using one of our supported language bindings.</p>
<p dir="auto">PRQL still has some minor bugs and some missing features, and probably is only
ready to be rolled out to non-technical teams for fairly simple queries.</p>
<p dir="auto">Here's our current <a href="https://prql-lang.org/roadmap/" rel="nofollow">Roadmap</a> and our
<a href="https://github.com/PRQL/prql/milestones">Milestones.</a></p>
<p dir="auto">Our immediate focus for the code is on:</p>
<ul dir="auto">
<li>Building out the next few big features, including
<a href="https://github.com/PRQL/prql/pull/1964" data-hovercard-type="pull_request" data-hovercard-url="/PRQL/prql/pull/1964/hovercard">types</a> and
<a href="https://github.com/PRQL/prql/pull/2129" data-hovercard-type="pull_request" data-hovercard-url="/PRQL/prql/pull/2129/hovercard">modules</a>.</li>
<li>Ensuring our supported features feel extremely robust; resolving any
<a href="https://github.com/PRQL/prql/issues?q=is%3Aissue+is%3Aopen+label%3Abug+label%3Apriority">priority bugs</a>.</li>
</ul>
<p dir="auto">We're also spending time thinking about:</p>
<ul dir="auto">
<li>Making it really easy to start using PRQL. We're doing that by building
integrations with tools that folks already use; for example our VS Code
extension &amp; Jupyter integration. If there are tools you're familiar with that
you think would be open to integrating with PRQL, please let us know in an
issue.</li>
<li>Making it easier to contribute to the compiler. We have a wide group of
contributors to the project, but contributions to the compiler itself are
quite concentrated. We're keen to expand this;
<a href="https://github.com/PRQL/prql/issues/1840" data-hovercard-type="issue" data-hovercard-url="/PRQL/prql/issues/1840/hovercard">#1840</a> for feedback.</li>
</ul>
<h2 tabindex="-1" dir="auto">Get involved</h2>
<p dir="auto">To stay in touch with PRQL:</p>
<ul dir="auto">
<li>Follow us on <a href="https://twitter.com/prql_lang" rel="nofollow">Twitter</a></li>
<li>Join us on <a href="https://discord.gg/eQcfaCmsNc" rel="nofollow">Discord</a></li>
<li>Star this repo</li>
<li><a href="https://prql-lang.org/book/contributing/" rel="nofollow">Contribute</a> —&nbsp;join us in building PRQL, through writing code
<a href="https://github.com/PRQL/prql/discussions">(send us your use-cases!)</a>, or
inspiring others to use it.</li>
<li>See the <a href="https://prql-lang.org/book/contributing/development.html" rel="nofollow">development</a> documentation for PRQL. It's easy to get
started — the project can be built in a couple of commands, and we're a really
friendly community!</li>
</ul>
<h2 tabindex="-1" dir="auto">Explore</h2>
<ul dir="auto">
<li><a href="https://prql-lang.org/playground" rel="nofollow">PRQL Playground</a> —&nbsp;experiment with PRQL in the browser.</li>
<li><a href="https://prql-lang.org/book" rel="nofollow">PRQL Book</a> — the language documentation.</li>
<li><a href="https://github.com/prql/dbt-prql">dbt-prql</a> — write PRQL in dbt models.</li>
<li><a href="https://pyprql.readthedocs.io/en/latest/magic_readme.html" rel="nofollow">Jupyter magic</a> —
run PRQL in Jupyter, either against a DB, or a Pandas DataFrame / CSV /
Parquet file through DuckDB.</li>
<li><a href="https://pyprql.readthedocs.io/" rel="nofollow">pyprql Docs</a> — the pyprql documentation, the
Python bindings to PRQL, including Jupyter magic.</li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=prql-lang.prql-vscode" rel="nofollow">PRQL VS Code extension</a></li>
<li><a href="https://www.npmjs.com/package/prql-js" rel="nofollow">prql-js</a> —&nbsp;JavaScript bindings for
PRQL.</li>
</ul>
<h2 tabindex="-1" dir="auto">Repo organization</h2>
<p dir="auto">This repo is composed of:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/PRQL/prql/blob/main/crates/prql-compiler">prql-compiler</a></strong> — the compiler, written in rust,
whose main role is to compile PRQL into SQL. It also includes
<a href="https://github.com/PRQL/prql/blob/main/crates/prqlc">prqlc</a>, the CLI.</li>
<li><strong><a href="https://github.com/PRQL/prql/blob/main/web">web</a></strong> — our web content: the <a href="https://prql-lang.org/book" rel="nofollow">Book</a>,
<a href="https://prql-lang.org/" rel="nofollow">Website</a>, and <a href="https://prql-lang.org/playground" rel="nofollow">Playground</a>.</li>
<li><strong><a href="https://github.com/PRQL/prql/blob/main/bindings">bindings</a></strong> — bindings from various languages to
<code>prql-compiler</code>.</li>
</ul>
<p dir="auto">It also contains our testing / CI infrastructure and development tools. Check
out our <a href="https://prql-lang.org/book/contributing/development.html" rel="nofollow">development docs</a> for more details.</p>
<h2 tabindex="-1" dir="auto">Contributors</h2>
<p dir="auto">Many thanks to those who've made our progress possible:</p>
<p dir="auto"><a href="https://github.com/PRQL/prql/graphs/contributors"><img src="https://camo.githubusercontent.com/c4ec6198e5445cfa52d83958a5d1be5fbd83409a2cdebb3bb7d1ad180cd053a2/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d5052514c2f7072716c" alt="Contributors" data-canonical-src="https://contrib.rocks/image?repo=PRQL/prql"></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hyperlink maximalism (2022) (127 pts)]]></title>
            <link>https://thesephist.com/posts/hyperlink/</link>
            <guid>36866242</guid>
            <pubDate>Tue, 25 Jul 2023 17:40:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thesephist.com/posts/hyperlink/">https://thesephist.com/posts/hyperlink/</a>, See on <a href="https://news.ycombinator.com/item?id=36866242">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        <p>I’m a hyperlink maximalist: <strong>everything should be a hyperlink</strong>, including everything that is hyperlinked by the author, everything that isn’t hyperlinked by the author, and perhaps even the hyperlinks themselves. Words should be hyperlinked, but so should be every interesting phrase, quote, name, proper noun, paragraph, document, and collection of documents I read.</p>
<p>There are two obvious problems with this idea:</p>
<ol>
<li>No author has time to hyperlink infinite permutations of everything they write, and</li>
<li>If everything is hyperlinked, nobody will know which links are useful.</li>
</ol>
<p>But we can solve both of these issues if we simply <strong>begin with today’s lightly hyperlinked documents, and let the reader’s computer generate links on-demand.</strong> When I’m reading something and don’t understand a particular word or want to know more about a quote, <strong>when I select it, my computer should search across everything I’ve read and some small high-quality subset of the Web to bring me 5-10 links</strong> about what I’ve highlighted that are the most relevant to what I’m reading now. Boom. Everything is a hyperlink, and each link reveals to me new and interesting connections between <a href="https://thesephist.com/posts/infinite/">the finite knowledge I have and the infinite expanse of information on the web</a>.</p>
<p>This raises a third issue: How would I, the reader, know which words or ideas are interesting to click on?</p>
<p>That, too, can be solved similarly. <strong>The computer can look at every word on the page, every phrase, name, quote, and section of text, and show me a “map” of the words and ideas behind which lay the most interesting ideas I might want to know about.</strong> In this vision, links are no longer lonesome strands precariously holding together a sparsely connected Web, but a booming choir of connections tightly binding together everything I have read and I will read. From explorers walking across unknown terrain guided only by the occasional blue underlined text, we become master cartographers, with every path and trail between our ideas charted out in front of us.</p>
<p>This is not to say that automatically generated links will replace hyperlinks authors and note-takers like to use today, or even that we should try to replace and deprecate manually-placed hyperlinks. Rather, automatic links can complement manually-annotated links with something that scales faster and easier, so in a world where links can be automatically created, most links will be machine-made.</p>
<p>This vision of a knowledge tool with “everything as a link” really appealed to me when I was building myself a new app for my personal notes earlier this year, so I sought out to prototype a basic tool that would try to achieve some of what I speculated on above: begin with basic, conventional text documents, generate links “on the fly” between my ideas, and visualize a map of such links and connections across my knowledge base.</p>
<p>The result is an app that I named <em>Notation</em>. It’s where my personal notes have lived since the start of the year, and while it’s not very feature-rich, I think it’s an interesting demonstration of some of the ideas of hyperlink maximalism.</p>
<h2 id="_notation_-a-prototype"><em>Notation</em>, a prototype</h2>
<p>At first glance, Notation is just another notes app with nested lists as the basic structure for information. Everything in Notation lives in a single, giant bulleted list, and each bullet can contain sub-bullets that open up when you toggle the bullet by clicking on it or hitting <em>Ctrl + Enter</em>.</p>
<p>Here’s a page of my own notes on <a href="https://numinous.productions/ttft/">Andy and Michael’s excellent essay on tools for thought</a>:</p>
<p><img src="https://thesephist.com/img/notation-highlights.png" alt="A screenshot of Notation, with bulleted lists of text. Some words are highlighted in varying shades of gray."></p>
<p>You’ll notice that some text on the page is highlighted in shades of gray. This layer of highlight is what I call the “heatmap”. It’s a heatmap of connections between notes that exist behind each word, because in Notation, every word and phrase is a link. To access any word or phrase as a link, you simply highlight it, and a popup will show <strong>all the places where I’ve mentioned that idea, sorted so the mentions that share the most similar contexts to my current view are at the top</strong>.</p>
<p><img src="https://thesephist.com/img/notation-demo.gif" alt=""></p>
<p>That’s really all there is to Notation. It lets me treat my notes as if every idea is linked to every other mention of that idea, without ever manually linking anything. The heatmap highlights let me know which words I should try highlighting to see the most relevant and interesting connections between the idea in front of me and the rest of my notes.</p>
<p>These highlights can, as an example, show me when I’ve mentioned a person’s name in many different places.</p>
<p><img src="https://thesephist.com/img/notation-andy-matuschak.png" alt=""></p>
<p>It can also pick up on common phrases like “spaced repetition” or proper nouns like “Quantum Country” without any kind of prior knowledge or training (though both would probably help, the current version doesn’t use either). If these phrases end up being important ideas in my notes, they’ll become more and more highlighted over time.</p>
<p><img src="https://thesephist.com/img/notation-phrases.png" alt=""></p>
<p>It can help me notice connections between ideas in my notes that I wouldn’t have even thought to make myself, even if I were trying to find interesting notes to link together. For example, here, within my list of software without <a href="https://szymonkaliski.com/notes/orientation-in-fractal-software/">fractal interfaces</a>, Notation highlighted the word “spreadsheets” and connected it to how most users of spreadsheets use it for visual organization, not calculation. It’s an important insight – interfaces can have useful spatial layouts without being fractal – and I may have missed it, if I had depended on my own memory.</p>
<p><img src="https://thesephist.com/img/notation-spreadsheets.png" alt=""></p>
<p>In this next instance, rather than finding distant connections, highlights on the phrase “peripheral vision” surfaced all the different authors who have mentioned it, signaling its importance across different streams of work.</p>
<p><img src="https://thesephist.com/img/notation-peripheral-vision.png" alt=""></p>
<p>Lastly, in this screenshot, Notation helps me see that both intelligence and expressiveness in an information medium may be emergent properties of their respective systems.</p>
<p><img src="https://thesephist.com/img/notation-emergent-property.png" alt=""></p>
<p>To produce these highlights and heatmaps, Notation currently uses a very simple algorithm for finding ideas that share similar context: two bullet points are similar if they share many <a href="https://en.wikipedia.org/wiki/N-gram">n-grams</a>. This is so computationally efficient that Notation can currently run everything in-browser, highlighting and generating heatmaps as you type. There are many ways to make this much smarter, such as by using sentence embeddings from language models to determine text similarity, but as a proof-of-concept of the core interface ideas behind highlights and text heatmaps, Notation has already proven quite useful in my personal use.</p>
<h3 id="the-demo">The demo</h3>
<p>If you want to try the interface for yourself, you can find a <a href="https://notation.app/">public deployment of Notation at notation.app</a>, initialized with a small subset of my personal notes from this year. By zooming into any particular page with <em>Control + Shift + Enter</em>, you can start exploring my notes using highlights and heatmaps on your own.</p>
<p>There’s one caveat: Notation was initially created just for myself, so keyboard shortcuts are essential to getting around. Here are the basics:</p>
<pre tabindex="0"><code>=== Notation demo TL;DR ===

Ctrl/Cmd Enter        Expand/collapse bullet
Ctrl/Cmd Shift Enter  Zoom into a bullet

Ctrl/Cmd P            Search
Ctrl/Cmd Shift P      Command bar
Ctrl/Cmd Shift I      New note (top-level bullet)

Ctrl/Cmd ;            Select last word
Ctrl/Cmd '            Select current bullet
</code></pre><h2 id="the-possibilities">The possibilities</h2>
<p>Notation in its current form does one thing pretty well – helping me stumble into connections between my own notes. This has already been so useful to me that I found myself writing down thoughts I don’t even care to remember in Notation, because writing it into Notation will surface connections and links that I wouldn’t have remembered myself. But it should come as no surprise that this basic interface idea can be taken much farther.</p>
<p>The first natural extension of Notation is to expand the search scope of its connection-finding beyond my notes, to things like <a href="https://thesephist.com/posts/monocle/">my web-browsing history or my journals</a>. With a smarter system, a similar interface could even automatically discover and show links from your notes to high-quality articles or online sources that you may not have seen yet, automatically crawling the web on your behalf.</p>
<p>Another direction of exploration may be to use generative language models to automatically recombine and synthesize new ideas from my existing notes. In my past experiments with tools that automatically generated new content, I always found it annoying to have to read and trudge my way through information of uncertain value that I didn’t write. But a heatmap highlight interface similar to Notation’s could make it more practical for AI systems to “brainstorm” large amounts of creative explorations when we aren’t looking, because it would only surface when we clicked on a highlight, letting users discover automatically generated ideas at the right time, when they are most relevant.</p>
<p>Notation’s interface is one small attempt to <a href="https://thesephist.com/posts/nav/">approach and improve thinking as a navigation problem</a>. Highlights and heatmaps drawn by Notation from its understanding of language help us find interesting connections between ideas where we may have remembered none by ourselves, and it turns ideas written down in text form into a kind of a literal “map”. When combined with <a href="https://thesephist.com/posts/latent/">powerful new techniques in NLP</a>, I think interfaces like this can turn computers into powerful creative collaborators in our attempts to understand the world more deeply.</p>

        <hr>
        <p>
            
            ←
            <a href="https://thesephist.com/posts/infinite/"><em>Knowledge tools, finite and infinite</em></a>
            
        </p>
        <p>
            
            <a href="https://thesephist.com/posts/materials/"><em>Design with materials, not features</em></a>
            →
            
        </p>
        <p>
            I share new posts on my <a href="https://thesephist.com/#newsletter">newsletter.</a>
            If you liked this one, you should consider joining the list.
        </p>
        <p>Have a comment or response? You can <a href="https://thesephist.com/#get-in-touch">email me.</a></p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Octox: Unix-like OS in Rust inspired by xv6-riscv (179 pts)]]></title>
            <link>https://github.com/o8vm/octox</link>
            <guid>36865682</guid>
            <pubDate>Tue, 25 Jul 2023 17:10:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/o8vm/octox">https://github.com/o8vm/octox</a>, See on <a href="https://news.ycombinator.com/item?id=36865682">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">octox</h2>
<p dir="auto">octox is a Unix-like operating system inspired by xv6-riscv. octox loosely
  follows the structure and style of xv6, but is implemented in pure Rust.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1aa9584e6c812c231ad6aeb525954b1afff7ac723b40f65f096566a634e00a1f/68747470733a2f2f7668732e636861726d2e73682f7668732d364d51424979416f33447042724152427848784c33352e676966"><img src="https://camo.githubusercontent.com/1aa9584e6c812c231ad6aeb525954b1afff7ac723b40f65f096566a634e00a1f/68747470733a2f2f7668732e636861726d2e73682f7668732d364d51424979416f33447042724152427848784c33352e676966" alt="https://vhs.charm.sh/vhs-6MQBIyAo3DpBrARBxHxL35.gif" data-animated-image="" data-canonical-src="https://vhs.charm.sh/vhs-6MQBIyAo3DpBrARBxHxL35.gif"></a></p>
<ul dir="auto">
  <li>Everything from kernel, userland, mkfs, to build system is written in safe
    Rust as much as possible.</li>
  <li>There are no dependencies on external crates.</li>
  <li>The userland has a library similar to Rust’s std with K&amp;R malloc.</li>
  <li>Multi-core support, buddy allocator as kernel-side memory allocator, file
    system with logging support, etc.</li>
</ul>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<h2 tabindex="-1" dir="auto">Requirements</h2>
<ul dir="auto">
  <li>Install the rust toolchain to have cargo installed by following
    <a href="https://www.rust-lang.org/tools/install" rel="nofollow">this</a> guide.</li>
  <li>Install <code>qemu-system-riscv</code></li>
  <li>(option) Install <code>gdb-multiarch</code></li>
</ul>
<h2 tabindex="-1" dir="auto">Build and Run</h2>
<ul dir="auto">
  <li>Clone this project &amp; enter: <code>git clone ... &amp;&amp; cd octox</code></li>
  <li>Build: <code>cargo build --target riscv64gc-unknown-none-elf</code>.</li>
  <li>Run: <code>cargo run --target riscv64gc-unknown-none-elf</code>, then qemu will boot
    octox. To exit, press <code>Ctrl+a</code> and <code>x</code>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Play with the Shell</h2>
<p dir="auto">A very simple shell is implemented.
  In addition to executing commands, you can only do the following things.</p>
<ul dir="auto">
  <li>Pipe: <code>cat file | head | grep test</code></li>
  <li>Dump processes: <code>Ctrl + P</code></li>
  <li>End of line: <code>Ctrl + D</code></li>
  <li>Redirect output: <code>&gt;</code>, <code>&gt;&gt;</code></li>
</ul>
<h2 tabindex="-1" dir="auto">Development</h2>
<h2 tabindex="-1" dir="auto">Userland Application</h2>
<p dir="auto">The userland comes with a user library called ulib that is similar to Rust’s
  std, so you can use it to develop your favorite commands. If you create a bin
  crate named <code>_command</code> in src/user, the build.rs and mkfs.rs will place a file
  named <code>command</code> in the file system and make it available for use.</p>
<ul dir="auto">
  <li>In src/user/Cargo.toml, define a bin crate with the name of the command you
    want to create with a <code>_</code> prefix
    <div dir="auto" data-snippet-clipboard-copy-content="[[bin]]
name = &quot;_rm&quot;
path = &quot;rm.rs&quot;
    "><pre>[[<span>bin</span>]]
<span>name</span> = <span><span>"</span>_rm<span>"</span></span>
<span>path</span> = <span><span>"</span>rm.rs<span>"</span></span>
    </pre></div>
  </li>
  <li>userland is also no_std, so don’t forget to add <code>#[no_std]</code>. Use ulib to
    develop any command you like. Here is an example of the rm command.
    <div dir="auto" data-snippet-clipboard-copy-content="#![no_std]
use ulib::{env, fs};

fn main() {
    let mut args = env::args().skip(1).peekable();

    if args.peek().is_none() {
        panic!(&quot;Usage: rm files...&quot;)
    }
    for arg in args {
        fs::remove_file(arg).unwrap()
    }
}
    "><pre><span>#!<span>[</span>no_std<span>]</span></span>
<span>use</span> ulib<span>::</span><span>{</span>env<span>,</span> fs<span>}</span><span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> args = env<span>::</span><span>args</span><span>(</span><span>)</span><span>.</span><span>skip</span><span>(</span><span>1</span><span>)</span><span>.</span><span>peekable</span><span>(</span><span>)</span><span>;</span>

    <span>if</span> args<span>.</span><span>peek</span><span>(</span><span>)</span><span>.</span><span>is_none</span><span>(</span><span>)</span> <span>{</span>
        <span>panic</span><span>!</span><span>(</span><span>"Usage: rm files..."</span><span>)</span>
    <span>}</span>
    <span>for</span> arg <span>in</span> args <span>{</span>
        fs<span>::</span><span>remove_file</span><span>(</span>arg<span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span>
    <span>}</span>
<span>}</span>
    </pre></div>
  </li>
  <li>Then, <code>cargo run --target riscv64gc-unknown-none-elf</code> in the root of octox.</li>
  <li>To use <code>Vec</code> and <code>String</code>, etc, do the following:
    <div dir="auto" data-snippet-clipboard-copy-content="extern crate alloc;
use alloc::{string::String, vec::Vec};
    "><pre><span>extern</span> <span>crate</span> alloc<span>;</span>
<span>use</span> alloc<span>::</span><span>{</span>string<span>::</span><span>String</span><span>,</span> vec<span>::</span><span>Vec</span><span>}</span><span>;</span>
    </pre></div>
  </li>
</ul>
<h2 tabindex="-1" dir="auto">Kernel</h2>
<p dir="auto">Developing in src/kernel. Here is an example of adding a system call. If you
  want to add a new system call, you only need to add a definition to the system
  call table in libkernel, and the userland library will be automatically
  generated by build.rs.</p>
<ul dir="auto">
  <li>Add a variant and Syscall Number to <code>enum SysCalls</code> in src/kernel/syscall.rs.
    Here is <code>Dup2</code> as an example:
    <div dir="auto" data-snippet-clipboard-copy-content="pub enum SysCalls {
    Fork = 1,
    ...,
    Dup2 = 23,
    Invalid = 0,
}
    "><pre><span>pub</span> <span>enum</span> <span>SysCalls</span> <span>{</span>
    <span>Fork</span> = <span>1</span><span>,</span>
    ...<span>,</span>
    <span>Dup2</span> = <span>23</span><span>,</span>
    <span>Invalid</span> = <span>0</span><span>,</span>
<span>}</span>
    </pre></div>
  </li>
  <li>Define the function signature of the system call in the <code>TABLE</code> of
    <code>SysCalls</code>. Use the enum type <code>Fn</code> to describe the return type(<code>U</code> (Unit),
    <code>I</code> (Integer), <code>N</code> (never)) and use <code>&amp;str</code> to represent arguments. then,
    define kernel-side implementation as a method on <code>SysCalls</code>. <code>cfg</code> flag is
    used to control the compilation target for kernel and the rest. Here is an
    example of <code>dup2</code>:
    <div dir="auto" data-snippet-clipboard-copy-content="impl SysCalls {
    pub const TABLE: [(fn, &amp;'static str); variant_count::<Self>()] = [
        (Fn::N(Self::Invalid), &quot;&quot;),
        (Fn::I(Self::fork), &quot;()&quot;),
        (Fn::N(Self::exit), &quot;(xstatus: i32)&quot;),
        ...,
        (Fn::I(Self::dup2), &quot;(src: usize, dst: usize)&quot;),
    ];
    pub fn dup2() -> Result<usize> {
        #[cfg(not(all(target_os = &quot;none&quot;, feature = &quot;kernel&quot;)))]
        return Ok(0);
        #[cfg(all(target_os = &quot;none&quot;, feature = &quot;kernel&quot;))]
        {
            let p = Cpus::myproc().unwrap().data_mut();
            let src_fd = argraw(0); let dst_fd = argraw(1);
            if src_fd != dst_fd {
                let mut src = p.ofile.get_mut(src_fd).unwrap()
                    .take().unwrap();
                src.clear_cloexec();
                p.ofile.get_mut(dst_fd)
                    .ok_or(FileDescriptorTooLarge)?.replace(src);
            }
            Ok(dst_fd)
        }
    }
    "><pre><span>impl</span> <span>SysCalls</span> <span>{</span>
    <span>pub</span> <span>const</span> <span>TABLE</span><span>:</span> <span>[</span><span>(</span><span>fn</span><span>,</span> <span>&amp;</span><span>'</span><span>static</span> <span>str</span><span>)</span><span>;</span> <span>variant_count</span><span>::</span><span>&lt;</span><span>Self</span><span>&gt;</span><span>(</span><span>)</span><span>]</span> = <span>[</span>
        <span>(</span><span>Fn</span><span>::</span><span>N</span><span>(</span><span>Self</span><span>::</span><span>Invalid</span><span>)</span><span>,</span> <span>""</span><span>)</span><span>,</span>
        <span>(</span><span>Fn</span><span>::</span><span>I</span><span>(</span><span>Self</span><span>::</span>fork<span>)</span><span>,</span> <span>"()"</span><span>)</span><span>,</span>
        <span>(</span><span>Fn</span><span>::</span><span>N</span><span>(</span><span>Self</span><span>::</span>exit<span>)</span><span>,</span> <span>"(xstatus: i32)"</span><span>)</span><span>,</span>
        ..<span>.</span><span>,</span>
        <span>(</span><span>Fn</span><span>::</span><span>I</span><span>(</span><span>Self</span><span>::</span>dup2<span>)</span><span>,</span> <span>"(src: usize, dst: usize)"</span><span>)</span><span>,</span>
    <span>]</span><span>;</span>
    <span>pub</span> <span>fn</span> <span>dup2</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>usize</span><span>&gt;</span> <span>{</span>
        <span>#<span>[</span>cfg<span>(</span>not<span>(</span>all<span>(</span>target_os = <span>"none"</span><span>,</span> feature = <span>"kernel"</span><span>)</span><span>)</span><span>)</span><span>]</span></span>
        <span>return</span> <span>Ok</span><span>(</span><span>0</span><span>)</span><span>;</span>
        <span>#<span>[</span>cfg<span>(</span>all<span>(</span>target_os = <span>"none"</span><span>,</span> feature = <span>"kernel"</span><span>)</span><span>)</span><span>]</span></span>
        <span>{</span>
            <span>let</span> p = <span>Cpus</span><span>::</span><span>myproc</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>.</span><span>data_mut</span><span>(</span><span>)</span><span>;</span>
            <span>let</span> src_fd = <span>argraw</span><span>(</span><span>0</span><span>)</span><span>;</span> <span>let</span> dst_fd = <span>argraw</span><span>(</span><span>1</span><span>)</span><span>;</span>
            <span>if</span> src_fd != dst_fd <span>{</span>
                <span>let</span> <span>mut</span> src = p<span>.</span><span>ofile</span><span>.</span><span>get_mut</span><span>(</span>src_fd<span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span>
                    <span>.</span><span>take</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span><span>;</span>
                src<span>.</span><span>clear_cloexec</span><span>(</span><span>)</span><span>;</span>
                p<span>.</span><span>ofile</span><span>.</span><span>get_mut</span><span>(</span>dst_fd<span>)</span>
                    <span>.</span><span>ok_or</span><span>(</span><span>FileDescriptorTooLarge</span><span>)</span>?<span>.</span><span>replace</span><span>(</span>src<span>)</span><span>;</span>
            <span>}</span>
            <span>Ok</span><span>(</span>dst_fd<span>)</span>
        <span>}</span>
    <span>}</span><span></span>
    </pre></div>
  </li>
  <li>With just these steps, the dup2 system call is implemented in both kernel and
    userland.</li>
</ul>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">Licensed under either of</p>
<ul dir="auto">
  <li><a href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache License, Version 2.0</a></li>
  <li><a href="http://opensource.org/licenses/MIT" rel="nofollow">MIT license</a></li>
</ul>
<p dir="auto">at your option.</p>
<h2 tabindex="-1" dir="auto">Acknowledgments</h2>
<p dir="auto">octox is inspired by <a href="https://github.com/mit-pdos/xv6-riscv">xv6-riscv</a>.</p>
<p dir="auto">I’m also grateful for the bug reports and discussion about the implementation
  contributed by Takahiro Itazuri and Kuniyuki Iwashima.</p>
<h2 tabindex="-1" dir="auto">Contribution</h2>
<p dir="auto">This is a learning project for me, and I will not be accepting pull requests
  until I consider the implementation complete. However, discussions and advice
  are welcome.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Roundtable (YC S23) – Using AI to Simulate Surveys (101 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36865625</link>
            <guid>36865625</guid>
            <pubDate>Tue, 25 Jul 2023 17:07:12 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36865625">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <tbody><tr id="36865625">
      <td><span></span></td>      <td><center><a id="up_36865625" href="https://news.ycombinator.com/vote?id=36865625&amp;how=up&amp;goto=item%3Fid%3D36865625"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=36865625">Launch HN: Roundtable (YC S23) – Using AI to Simulate Surveys</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_36865625">102 points</span> by <a href="https://news.ycombinator.com/user?id=timshell">timshell</a> <span title="2023-07-25T17:07:12"><a href="https://news.ycombinator.com/item?id=36865625">20 hours ago</a></span> <span id="unv_36865625"></span> | <a href="https://news.ycombinator.com/hide?id=36865625&amp;goto=item%3Fid%3D36865625">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Roundtable%20(YC%20S23)%20%E2%80%93%20Using%20AI%20to%20Simulate%20Surveys&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=36865625&amp;auth=aff46cd2c3b6c9c3d261826fd80906bac185aa35">favorite</a> | <a href="https://news.ycombinator.com/item?id=36865625">86&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Hi HN, we’re Mayank and Matt of Roundtable (<a href="https://roundtable.ai/">https://roundtable.ai/</a>). We use LLMs to produce cheap, yet surprisingly useful, simulations of surveys. Specifically, we train LLMs on standard, curated survey datasets. This approach allows us to essentially build general-purpose models of human behavior and opinion. We combine this with a nice UI that lets users easily visualize and interpret the results.</p><p>Surveys are incredibly important for user and market research, but are expensive and take months to design, run, and analyze. By simulating responses, our users can get results in seconds and make decisions faster. See <a href="https://roundtable.ai/showcase">https://roundtable.ai/showcase</a> for a bunch of examples, and <a href="https://www.loom.com/share/eb6fb27acebe48839dd561cf1546f131" rel="nofollow noreferrer">https://www.loom.com/share/eb6fb27acebe48839dd561cf1546f131</a> for a demo video.</p><p>Our product lets you add questions (e.g. “how old are you”) and conditions (e.g. “is a Hacker News user”) and then see how these affect the survey results. For example, the survey “Are you interested in buying an e-bike?” shows ‘yes’ 28% [1]. But if you narrow it down to people who own a Tesla, ‘yes’ jumps to 52% [2]. Another example: if you survey “where did you learn to code”, the question “how old are you?” makes a dramatic difference—for “45 or older” the answer is 55% “books” [3], but for “younger than 45” it’s 76% “online” [4]. One more: 5% of people answer “legroom” to the question “Which of the following factors is most important for choosing which airline to fly?” [5], and this jumps to 20% when you condition on people over six feet tall [6].</p><p>You wouldn’t think (well, we didn’t think) that such simulated surveys would work very well, but empirically they work a lot better than expected—we have run many surveys in the wild to validate Roundtable's results (e.g. comparing age demographics to U.S. Census data). We’re still trying to figure out why. We believe that LLMs that are pre-trained on the public Internet have internalized a lot of information/correlations about communities (e.g. Tesla drivers, Hacker News, etc.) and can reasonably approximate their behavior. In any case, researchers are seeing the same things that we are. A nice paper by a BYU group [7] discusses extracting sub-population information from GPT/LLMs. A related paper from Microsoft [8] shows how GPT can simulate different human behaviors. It’s an active research topic, and we hope we can get a sense of the theoretical basis relatively soon.</p><p>Because these models are primarily trained on Internet data, they start out skewed towards the demographics of heavy Internet users (e.g., high-income, male). We addressed this by fine-tuning GPT on the GSS (General Social Survey [9] - the gold standard of demographic surveys in the US) so our models emulate a more representative U.S. population.</p><p>We’ve built a transparency feature that shows how similar your survey question is to the training data and thus gives a confidence metric of our accuracy. If you click ‘Investigate Results’, we report the most similar (in terms of cosine distance between LLM embeddings) GSS questions as a way of estimating how much extrapolation / interpolation is going on. This doesn’t quite address the accuracy of the subpopulations / conditioning questions (we are working on this), but we thought we are at a sufficiently advanced point to share what we’ve built with you all.</p><p>We're graduating PhD students from Princeton University in cognitive science and AI. We ran a ton of surveys and behavioral experiments and were often frustrated with the pipeline. We were looking to leave academia, and saw an opportunity in making the survey pipeline better. User and market research is a big market, and many of the tools and methods the industry uses are clunky and slow. Mayank’s PhD work used large datasets and ML for developing interpretable scientific theories, and Matt’s developed complex experimental software to study coordinated group decision-making. We see Roundtable as operating at the intersection of our interests.</p><p>We charge per survey. We are targeting small and mid-market businesses who have market research teams, and ask for a minimum subscription amount. Pricing is at the bottom of our home page.</p><p>We are still in the early stages of building this product, and we’d love for you all to play around with the demo and provide us feedback. Let us know whatever you see - this is our first major endeavor into the private sector from academia, and we’re eager to hear whatever you have to say!</p><p>[1]: <a href="https://roundtable.ai/sandbox/e02e92a9ad20fdd517182788f4ae7e1f96a849c0">https://roundtable.ai/sandbox/e02e92a9ad20fdd517182788f4ae7e...</a></p><p>[2]: <a href="https://roundtable.ai/sandbox/6b4bf8740ad1945b08c0bf584c84c1202a5fec53">https://roundtable.ai/sandbox/6b4bf8740ad1945b08c0bf584c84c1...</a></p><p>[3] <a href="https://roundtable.ai/sandbox/d701556248385d05ce5d26ce7fc776bb4d32fad0">https://roundtable.ai/sandbox/d701556248385d05ce5d26ce7fc776...</a></p><p>[4] <a href="https://roundtable.ai/sandbox/8bd80babad042cf60d500ca28c40f7db413f553a">https://roundtable.ai/sandbox/8bd80babad042cf60d500ca28c40f7...</a></p><p>[5] <a href="https://roundtable.ai/sandbox/0450d499048c089894c34fba514db4042eafb6c0">https://roundtable.ai/sandbox/0450d499048c089894c34fba514db4...</a></p><p>[6] <a href="https://roundtable.ai/sandbox/eeafc6de644632af303896ec19feb69ac4714e24">https://roundtable.ai/sandbox/eeafc6de644632af303896ec19feb6...</a></p><p>[7] <a href="https://arxiv.org/abs/2209.06899" rel="nofollow noreferrer">https://arxiv.org/abs/2209.06899</a></p><p>[8] <a href="https://openreview.net/pdf?id=eYlLlvzngu" rel="nofollow noreferrer">https://openreview.net/pdf?id=eYlLlvzngu</a></p><p>[9] <a href="https://www.norc.org/research/projects/gss.html" rel="nofollow noreferrer">https://www.norc.org/research/projects/gss.html</a></p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td></td></tr>  </tbody></div></div>]]></description>
        </item>
    </channel>
</rss>