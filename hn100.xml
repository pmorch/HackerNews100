<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 24 Dec 2023 11:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Unveiling the big leap in Ruby 3.3's IRB (121 pts)]]></title>
            <link>https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/</link>
            <guid>38750459</guid>
            <pubDate>Sun, 24 Dec 2023 02:15:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/">https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/</a>, See on <a href="https://news.ycombinator.com/item?id=38750459">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In this blog post, we will delve into the major enhancements introduced in Ruby 3.3’s <a href="https://github.com/ruby/irb">IRB</a>,
as well as what’s currently planned for the next year.</p>

<p>In a nutshell, Ruby 3.3’s IRB offers <a href="#advancing-debugging-with-irb">improved debugging capabilities</a>,
an <a href="#enhancing-autocompletion">enhanced autocompletion experience</a>,
and an <a href="#quality-of-life-improvements">overall improved user experience</a>.</p>

<p>It’s worth noting that even if you don’t upgrade to Ruby 3.3 immediately, you can still install the same version of IRB
(<a href="https://github.com/ruby/irb/releases/tag/v1.11.0"><code>v1.11.0</code></a>) in any Ruby 2.7+ projects by adding <code>gem "irb"</code> to its Gemfile.</p>

<h3 id="table-of-contents">Table Of Contents</h3>

<ul>
  <li><a href="#advancing-debugging-with-irb">Advancing Debugging with IRB</a>
    <ul>
      <li><a href="#accessing-irbrdbg-from-bindingbreak--debugger">Accessing <code>irb:rdbg</code> from <code>binding.break</code> / <code>debugger</code></a></li>
    </ul>
  </li>
  <li><a href="#enhancing-autocompletion">Enhancing Autocompletion</a>
    <ul>
      <li><a href="#addressing-completion-dropdown-issues">Addressing Completion Dropdown Issues</a></li>
      <li><a href="#customizing-dropdown-ui-with-relineface">Customizing Dropdown UI with <code>Reline::Face</code></a></li>
      <li><a href="#experimenting-with-more-accurate-completion-using-rbs">Experimenting with More Accurate Completion Using RBS</a></li>
    </ul>
  </li>
  <li><a href="#quality-of-life-improvements">Quality of Life Improvements</a>
    <ul>
      <li><a href="#pager-support">Pager Support</a></li>
      <li><a href="#omitting-return-value-inspection-with-">Omitting Return Value Inspection with <code>;</code></a></li>
      <li><a href="#history-command">History Command</a></li>
      <li><a href="#streamlined-prompt">Streamlined Prompt</a></li>
      <li><a href="#enhanced-show_source-command">Enhanced <code>show_source</code> Command</a></li>
    </ul>
  </li>
  <li><a href="#whats-on-the-horizon-for-ruby-34">What’s on the Horizon for Ruby 3.4?</a>
    <ul>
      <li><a href="#the-help-command-will-print-help-message">The <code>help</code> Command Will Print Help Message</a></li>
      <li><a href="#enhancing-extensibility-command-and-helper-method">Enhancing Extensibility: Command and Helper Method</a></li>
    </ul>
  </li>
  <li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>

<h2 id="advancing-debugging-with-irb">Advancing Debugging with IRB</h2>

<p>In Ruby 3.2’s IRB (<code>v1.6</code>), we introduced a set of shortcut commands such as <code>debug</code> and <code>next</code> that allowed for a swift
transition to the <a href="https://github.com/ruby/debug"><code>debug</code> gem</a>’s session from a <code>binding.irb</code> breakpoint.
(To avoid confusion, I’ll refer to the <code>debug</code> gem’s session as <code>rdbg</code> from here on.)</p>

<p>This was a significant improvement. However, with the switch, users would instantly lose certain IRB features, such as multi-line input:</p>

<div><pre><code>irb<span>(</span>main<span>)</span>:001:0&gt; debug
<span>(</span>ruby<span>)</span> <span>if </span><span>true
eval </span>error: <span>(</span>rdbg<span>)</span>/test.rb:1: syntax error, unexpected end-of-input, expecting <span>`</span><span>then</span><span>' or '</span><span>;</span><span>' or '</span><span>\n</span><span>'
if true
       ^
nil
(rdbg)
</span></code></pre></div>

<p>This year, we’ve taken the user experience to a new level by introducing the <code>irb:rdbg</code> session, which allows users to execute all <code>rdbg</code> commands without exiting the IRB session.</p>

<div><pre><code>irb<span>(</span>main<span>)</span>:001&gt; debug
irb:rdbg<span>(</span>main<span>)</span>:002<span>*</span> <span>if </span><span>true
</span>irb:rdbg<span>(</span>main<span>)</span>:003<span>*</span>   puts <span>"Multi-line input and more!"</span>
irb:rdbg<span>(</span>main<span>)</span>:004&gt; end
Multi-line input and more!
nil
irb:rdbg<span>(</span>main<span>)</span>:005&gt; th
<span># `th` (show all threads) is a example of a command only present in a `rdbg` session</span>
<span>--</span><span>&gt;</span> <span>#0 (sleep)@test.rb:2:in `&lt;main&gt;'</span>
irb:rdbg<span>(</span>main<span>)</span>:006&gt;
</code></pre></div>

<p>This deep integration offers several advantages:</p>

<ul>
  <li>Access to both <a href="https://github.com/ruby/irb?tab=readme-ov-file#commands">IRB</a> and <a href="https://github.com/ruby/debug?tab=readme-ov-file#debug-command-on-the-debug-console">rdbg’s commands</a></li>
  <li>Multi-line input</li>
  <li>Autocompletion</li>
  <li>Symbol aliases to commands, like <code>@</code> (<code>whereami</code>) and <code>$</code> (<code>show_source</code>)</li>
  <li><a href="#pager-support">Pager support</a></li>
</ul>

<p>But most importantly, it offers users a debugging experience equivalent to <code>pry-byebug</code>.</p>

<h3 id="accessing-irbrdbg-from-bindingbreak--debugger">Accessing <code>irb:rdbg</code> from <code>binding.break</code> / <code>debugger</code></h3>

<p>With the <code>debug</code> gem <code>v1.9.0</code>, which will also be part of Ruby 3.3, you can configure it to run <code>irb:rdbg</code> either by:</p>

<ul>
  <li>Setting its <code>irb_console</code> config through the <code>RUBY_DEBUG_IRB_CONSOLE=1</code> environment variable</li>
  <li>Running the <code>irb</code> command in a <code>rdbg</code> session</li>
</ul>

<p>Doing so will give you the same <code>irb:rdbg</code> session in your <code>debug</code> console:</p>

<div><pre><code><span>$ RUBY_DEBUG_IRB_CONSOLE</span><span>=</span>1 bundle <span>exec </span>rdbg test.rb
<span>[</span>1, 1] <span>in </span>test.rb
<span>=&gt;</span>   1| a <span>=</span> 1
<span>=&gt;</span><span>#0    &lt;main&gt; at test.rb:1</span>
irb:rdbg<span>(</span>main<span>)</span>:002&gt;
</code></pre></div>

<h2 id="enhancing-autocompletion">Enhancing Autocompletion</h2>

<h3 id="addressing-completion-dropdown-issues">Addressing Completion Dropdown Issues</h3>

<p>In Ruby 3.1, IRB introduced the autocompletion feature. While it has proven to be a useful feature for many users, it
also came with some issues that were hard to overlook:</p>

<ul>
  <li>The dropdown could push up the prompt when the candidate list is long. This not only distracts users but also pushes up
the output history, forcing users to frequently scroll up and down through the session.</li>
  <li>The dropdown’s colors are not configurable, making text hard to read with some terminal themes.</li>
  <li>Under certain conditions, the dropdown could erase previously rendered content.</li>
</ul>

<p>Here’s a short demonstration of these issues:</p>

<p><img src="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/images/autocompletion-issues.gif" alt="autocompletion issues demo" width="80%"></p>

<p>However, thanks to several fixes on the <a href="https://github.com/ruby/reline">Reline</a> gem (a pure-Ruby replacement of <code>readline</code>),
these issues have been addressed.</p>

<p><img src="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/images/improved-autocompletion.gif" alt="improved autocompletion demo" width="80%"></p>

<p>We are aware that there are other existing issues, such as the lack of debouncing or tab-completion options.
And we will keep improving the feature in future releases.</p>

<h3 id="customizing-dropdown-ui-with-relineface">Customizing Dropdown UI with <code>Reline::Face</code></h3>

<p>With <a href="https://github.com/ruby/reline"><code>Reline</code></a> <code>v0.4.0+</code>, you can use the <code>Reline::Face</code> class to customize the dropdown
UI’s style in your <code>.irbrc</code>.</p>

<p>For instance, this snippet will change the dropdown from its default color scheme to a black-and-white theme:</p>

<div><pre><code><span>Reline</span><span>::</span><span>Face</span><span>.</span><span>config</span><span>(</span><span>:completion_dialog</span><span>)</span> <span>do</span> <span>|</span><span>conf</span><span>|</span>
  <span>conf</span><span>.</span><span>define</span> <span>:default</span><span>,</span> <span>foreground: :white</span><span>,</span> <span>background: :black</span>
  <span>conf</span><span>.</span><span>define</span> <span>:enhanced</span><span>,</span> <span>foreground: :black</span><span>,</span> <span>background: :white</span>
  <span>conf</span><span>.</span><span>define</span> <span>:scrollbar</span><span>,</span> <span>foreground: :white</span><span>,</span> <span>background: :black</span>
<span>end</span>
</code></pre></div>

<p>The result can be seen below:</p>

<figure>
  <img src="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/images/black-and-white-completion-dropdown-with-reline-face.png">
  <figcaption>left: default, right: with the customization</figcaption>
</figure>

<h3 id="experimenting-with-more-accurate-completion-using-rbs">Experimenting with More Accurate Completion Using RBS</h3>

<p>Currently, IRB’s autocompletion is powered by a combination of regular expressions and runtime information from <code>Binding</code>.</p>

<p>This provides a decent result for the first level of completion, but can’t support chained method calls.</p>

<p>For example, when the user types <code>'Ruby'.up</code>, IRB can list <code>upcase</code> and <code>upcase!</code>, because it knows <code>'Ruby'</code> is a <code>String</code>.</p>

<p>But when the user types <code>'Ruby'.upcase.</code>, IRB can’t make another suggestion.
This is because it cannot evaluate <code>'Ruby'.upcase</code> to get its value as it risks causing side-effects
(for example, calling <code>User.all.</code> in a Rails app).</p>

<p>However, there is another way we can get an object’s information without evaluation in Ruby: gradual typing.
Therefore, this year <a href="https://github.com/tompng">@tompng</a> initiated the <a href="https://github.com/ruby/repl_type_completor"><code>repl_type_completor</code></a> project to explore if it’s a feasible replacement for the current regexp-based completor.</p>

<p>To give it a try, you need to add this to your Gemfile:</p>

<div><pre><code><span>gem</span> <span>"repl_type_completor"</span><span>,</span> <span>group: </span><span>[</span><span>:development</span><span>,</span> <span>:test</span><span>]</span>
</code></pre></div>

<p>And to activate it:</p>

<ul>
  <li>Pass the <code>--type-completor</code> flag when starting IRB</li>
  <li>Or add this to your <code>irbrc</code> file:</li>
</ul>

<div><pre><code><span>IRB</span><span>.</span><span>conf</span><span>[</span><span>:COMPLETOR</span><span>]</span> <span>=</span> <span>:type</span> <span># default is :regexp</span>
</code></pre></div>

<p>For more details, please refer to IRB readme’s <a href="https://github.com/ruby/irb?tab=readme-ov-file#type-based-completion">type completion section</a>.</p>

<h2 id="quality-of-life-improvements">Quality of Life Improvements</h2>



<p>This year, IRB has introduced pager support on the output of:</p>

<ul>
  <li>The <code>show_source</code>, <code>ls</code>, and <code>show_cmds</code> commands</li>
  <li>The new <code>history</code> command</li>
  <li>Evaluation result</li>
</ul>

<p>When these outputs’ height exceeds your terminal’s, IRB will now paginate them:</p>

<p><img src="https://railsatscale.com/2023-12-19-irb-for-ruby-3-3/images/pager-demo.gif" alt="pager demo" width="90%"></p>

<p>This offers several benefits:</p>

<ul>
  <li>You can freely scroll up and down with your arrow-up/down keys</li>
  <li>You won’t overscroll to some previous IRB output</li>
  <li>By typing <code>/[search_term]</code>, like <code>/foo</code>, inside a pager, you can easily search any text that contains <code>foo</code>
    <ul>
      <li>This could be extremely useful when inspecting record(s) in a Rails console</li>
    </ul>
  </li>
</ul>

<p>We understand that for some users or under certain conditions, a pager could be more of a hindrance than a help.
Therefore, it’s also possible to disable it with the <code>--no-pager</code> flag, or by adding this to your <code>irbrc</code> file:</p>

<div><pre><code><span>IRB</span><span>.</span><span>conf</span><span>[</span><span>:USE_PAGER</span><span>]</span> <span>=</span> <span>false</span>
</code></pre></div>

<h3 id="omitting-return-value-inspection-with-">Omitting Return Value Inspection with <code>;</code></h3>

<p>Even though paging evaluation result should make long output easier to inspect, sometimes we simply don’t want to see
the return value of certain expressions. For example, the result of <code>users = User.all</code> when it returns hundreds of records.</p>

<p>In such cases, the new IRB allows users to omit the return value by adding a <code>;</code> at the end, like <code>users = User.all;</code>.</p>

<h4 id="example">Example</h4>

<div><pre><code>irb<span>(</span>main<span>)</span>:001&gt; long_string <span>=</span> <span>"foo"</span> <span>*</span> 10000<span>;</span>
irb<span>(</span>main<span>)</span>:002&gt; long_string.size
<span>=&gt;</span> 30000
</code></pre></div>

<h3 id="history-command">History Command</h3>

<p>IRB now has a <code>history</code> command to display all stored input history (which by default is limited to <code>1000</code>).</p>

<p>Since we usually have a long input history, this command also comes with a <code>-g [term]</code> flag for filtering.</p>

<pre><code>irb(main):006&gt; history -g self
1000: history -g self
803: self
769: self
731: watch self @foo
698: break self.inspect
584: self
582: self
</code></pre>

<h3 id="streamlined-prompt">Streamlined Prompt</h3>

<p>As you may have noticed, IRB’s prompt is now a bit shorter:</p>

<h4 id="before">Before</h4>



<h4 id="after">After</h4>



<p>The removed part is the indent level number, which became redundant after IRB implemented a robust auto-indent feature
for multi-line input.</p>

<h3 id="enhanced-show_source-command">Enhanced <code>show_source</code> Command</h3>

<p>The <code>show_source</code> command has always been an essential tool for many IRB users, especially for debugging.
This year it received two enhancements that will make it even more useful:</p>

<ul>
  <li>You can now use <code>-s</code> to get the method’s super definition if it has one</li>
  <li>It can now display private methods too</li>
</ul>

<h4 id="the--s-flag">The <code>-s</code> Flag</h4>

<p>Given this script:</p>

<div><pre><code><span>class</span> <span>Foo</span>
  <span>def</span> <span>foo</span>
    <span>"foo"</span>
  <span>end</span>
<span>end</span>

<span>class</span> <span>Bar</span> <span>&lt;</span> <span>Foo</span>
  <span>def</span> <span>foo</span>
    <span>super</span> <span>+</span> <span>"bar"</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>You can now get both <code>Bar#foo</code> and its super method (<code>Foo#foo</code>)’s definition:</p>

<pre><code>irb(main):001&gt; show_source Bar#foo

From: test.rb:8

  def foo
    super + "bar"
  end

=&gt; nil
irb(main):002&gt; show_source Bar#foo -s

From: test.rb:2

  def foo
    "foo"
  end

=&gt; nil
</code></pre>

<p>You can also stack the flag, like <code>-ss</code>, to walk up the super method chain. And if IRB can’t find the super definition anymore,
it’ll notify you with:</p>

<pre><code>irb(main):003&gt; show_source Bar#foo -ss
Error: Couldn't locate a super definition for Bar#foo
=&gt; nil
</code></pre>

<h2 id="whats-on-the-horizon-for-ruby-34">What’s on the Horizon for Ruby 3.4?</h2>

<h3 id="the-help-command-will-print-help-message">The <code>help</code> Command Will Print Help Message</h3>

<p>When we use a terminal-based application, the first thing we normally do is to type <code>help</code> and learn how to use it.</p>

<p>However, due to historical reasons, IRB’s <code>help</code> command opens up an <code>ri</code> input for Ruby document lookup instead.
This unconventional design can make it hard, especially for new Ruby developers, to learn IRB’s usage.
So we decided to change it in several steps:</p>

<ol>
  <li>In Ruby 3.2, we added <code>show_cmds</code> to print the help message and <code>show_doc</code> command as an alias to the <code>help</code> command.</li>
  <li>In Ruby 3.3, IRB will warn users that <code>help</code> is going to be repurposed to act as <code>show_cmds</code>.</li>
  <li>In early 2024, we plan to release IRB <code>v2.0</code>, with the repurposing of <code>help</code> command being one of the breaking changes.</li>
</ol>

<h3 id="enhancing-extensibility-command-and-helper-method">Enhancing Extensibility: Command and Helper Method</h3>

<p>Several major Ruby web frameworks, such as <a href="https://github.com/rails/rails">Rails</a> and <a href="https://github.com/hanami/hanami">Hanami</a>,
utilize IRB as a platform for their consoles. Additionally, other libraries extend IRB with their custom features via commands.</p>

<p>However, until now, there have been no standard APIs on the IRB side to accommodate new commands or helper methods.
This situation leads to several challenges:</p>

<ul>
  <li>IRB is unable to display these extended features in its help message.</li>
  <li>Refactoring the relevant parts becomes challenging because they are essentially public when private APIs are used directly.</li>
  <li>Projects need to devise their own ways to utilize IRB, often resulting in similar but slightly different solutions.</li>
</ul>

<p>To address these issues, we plan to provide official APIs and relevant documentation to help libraries and applications extend IRB.
Our goal is to transform IRB from not just a great tool, but also into a great platform for other tools.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>The remarkable progress we see on IRB was made possible by the IRB/Reline maintainers team:</p>

<ul>
  <li><a href="https://github.com/tompng">Tomoya Ishida (tompng)</a></li>
  <li><a href="https://github.com/ima1zumi">Mari Imaizumi (ima1zumi)</a></li>
  <li><a href="https://github.com/hasumikin">Hitoshi Hasumi (hasumikin)</a></li>
  <li><a href="https://github.com/st0012">Stan Lo (st0012)</a></li>
</ul>

<p>And also, a big thank you to all the community contributors.</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can Microsoft Flight Simulator help me learn to fly (or make me a better pilot)? (234 pts)]]></title>
            <link>https://aviation.stackexchange.com/questions/738/can-microsoft-flight-simulator-help-me-learn-to-fly-or-make-me-a-better-pilot</link>
            <guid>38750411</guid>
            <pubDate>Sun, 24 Dec 2023 02:08:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aviation.stackexchange.com/questions/738/can-microsoft-flight-simulator-help-me-learn-to-fly-or-make-me-a-better-pilot">https://aviation.stackexchange.com/questions/738/can-microsoft-flight-simulator-help-me-learn-to-fly-or-make-me-a-better-pilot</a>, See on <a href="https://news.ycombinator.com/item?id=38750411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>I believe that there's an element to this question which has not been covered.  This is very much a personal response.</p>

<p>Your question specifically asks whether it can help you to learn how to fly or become a better pilot.  This is actually two questions in one.</p>

<p>Physical and mental limitations not withstanding, I would say that just about anyone can learn how to fly.  But I don't believe that everyone who can learn how to fly could be a good pilot.  Flying and pilotage are very much more than successfully operating a flying machine in order to fly.  It's even a lot more than doing that and following the procedures (ATC, operating in controlled airspace etc) which accompany it.</p>

<p>Being a good pilot requires a certain aptitude and attitude (and I'm not talking about what you see on the AH).  It needs, let's not beat about the bush, a reasonably good IQ and education.</p>

<p>A pilot becomes good when the operation of the machine or the following of the procedures is not enough to to produce a safe, successful conclusion.  A good pilot avoids the traps and pitfalls that catch the unwary and have proved the rule, all to often, that in the ongoing contest between the earth and flimsy machines arriving in other than controlled circumstances, the earth has yet to lose.  </p>

<p>A good pilot takes care of the machine and it's passengers.  A good pilot can deal with the unexpected and make sound decisions to continue a flight or not or perhaps even to not commit aviation at all.  A good pilot has situational awareness which tells them, via sixth sense, that the bizjet calling left base is a potential threat and is already looking by the time the tower calls.</p>

<p>But much more than this, there are some very important elements to flying which a sim can never provide.</p>

<p>There is the emotional response; that thrill, that feeling of privilege, that unquantifiable human response to flying that is so much more than operating the machine.  There is also a set of skills and mental and physical responses without which, it is not possible to be a "good pilot" (IMHO).</p>

<p>Let me give a brief background of where I'm coming from and a concrete example of what I mean.</p>

<p>I have only a couple of hundred hours.  A handful on fixed wing, the rest on helicopters.  I also have about 3500 hours "flying" big tin on VATSIM (if you are serious about PC simming and don't know about VATSIM, Google for it right now!).</p>

<p>In VATSIM, I can operate a 777 (my favourite) very successfully including all of the related procedures which VATSIM does a remarkably good job of simulating.  I can fly a SID, follow my planned route, follow a STAR and do a visual onto 26R at Heathrow without breaking sweat, talking to and complying with ATC all the way. I can deal with an unexpected hold or a last minute change of arrival without fluster. I know this all works because I have also been lucky enough to do this in a "real" sim (737) and I had no problems at all using the automatics and hand flying the machine for the first 500 and last 1000 feet to depart and arrive safely back at Heathrow.</p>

<p>However, when I was learning to fly a helicopter, the reality of it all was very different.  Hovering is like learning to ride a bike.  I would say anyone with reasonable co-ordination can learn to do it.  I can hover all helicopters I've flown with ease.  Without even thinking about it.  Thing is, I can't really tell you how to do it and I have never been able to keep a PC sim helicopter in the air for more than a few minutes without getting into horrible shape.  Try explaining to a child how to ride a bike.  I reckon it's impossible.</p>

<p>When you switch from lurching around the sky with your instructor calling "I have control" every 20 seconds to that magical moment when you are suddenly in a steady hover, there are some things which simply cannot be simulated which your brain needs.  </p>

<p>The beginners mistake is to focus on the ground, I think most people are looking about 15 metres ahead.  You cannot succeed like that.  What you want to do is focus your eyes well into the distance and let your peripheral vision do the work.  It's almost subconscious and I can't even tell you how it all works but you do realise that you are moving over the ground with subtle cues coming in from your periphery.  </p>

<p>The second, and more important set of cues, are those that come through the "seat of your pants".  With a little experience, you just "know" when the helicopter moves, even before your peripheral vision has picked up the movement and you've already put that pressure on the cyclic (and it generally is pressure, not movement) to arrest the movement before the machine has deviated from it's position.  There are always small movements but to an external observer, you are sitting there, in the air, without so much as a ripple.</p>

<p>Subtle sounds are also important.  I can tell you pretty much where the rotor RPM is without looking at the tach and can certainly tell instantly when it's going down.  The second cue is from the tach but by that time, I'm already on the collective to adjust.  The tach just qualifies what I know and shows me that I'm doing the right thing to correct.</p>

<p>I'm sure that the fixed wing guys could provide good parallels.</p>

<p>My experience on sims tells me that you can indeed learn how to operate the machine and prosecute the procedures around it but it cannot help you to fly, nor to be a good (or better) pilot.  Where a sim can help is in teaching you the operation and procedures to the point where your brain is free to concentrate on flying and pilotage and your heart free to enjoy the sensations, because the mechanical stuff has moved into your subconscious and has become muscle memory.</p>

<p>Flying is emotional; it's passionate; it's determination; it's personal conquest; it runs in your veins. It's many things more than a sim can ever provide.</p>

<p>If you don't get "Oh I have slipped the surly bonds", then you are an operator, not a flyer.</p>

<p>[EDIT]
Am I a good pilot?  I'd say I'm average, and striving to be better, but that, in reality, probably describes most pilots, since it is impossible for most to be better than average.  I do recognise that I'm in that statistically dangerous zone where I have enough hours to think I'm good but not enough hours to prove it.  In my experience, ego and cahonas tell many pilots that they <strong>are</strong> good pilots ;)</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[San Francisco’s rent prices have never returned to pre-2020 levels (163 pts)]]></title>
            <link>https://www.sfchronicle.com/realestate/article/sf-rent-prices-18534829.php</link>
            <guid>38750079</guid>
            <pubDate>Sun, 24 Dec 2023 01:14:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sfchronicle.com/realestate/article/sf-rent-prices-18534829.php">https://www.sfchronicle.com/realestate/article/sf-rent-prices-18534829.php</a>, See on <a href="https://news.ycombinator.com/item?id=38750079">Hacker News</a></p>
Couldn't get https://www.sfchronicle.com/realestate/article/sf-rent-prices-18534829.php: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Ask yourself dumb questions and answer them (2020) (152 pts)]]></title>
            <link>https://terrytao.wordpress.com/career-advice/ask-yourself-dumb-questions-and-answer-them/</link>
            <guid>38749473</guid>
            <pubDate>Sat, 23 Dec 2023 23:50:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terrytao.wordpress.com/career-advice/ask-yourself-dumb-questions-and-answer-them/">https://terrytao.wordpress.com/career-advice/ask-yourself-dumb-questions-and-answer-them/</a>, See on <a href="https://news.ycombinator.com/item?id=38749473">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<blockquote>
<p><em>Don’t just read it; fight it! Ask your own questions, look for your own examples, discover your own proofs. Is the hypothesis necessary? Is the converse true? What happens in the classical special case? What about the degenerate cases? Where does the proof use the hypothesis?</em> (<a href="http://en.wikipedia.org/wiki/Paul_Halmos">Paul Halmos</a>, “I want to be a mathematician”)</p>
</blockquote>
<p>When you learn mathematics, whether in books or in lectures, you generally only see the end product – very polished, clever and elegant presentations of a mathematical topic.</p>
<p>However, the process of discovering <em>new</em> mathematics is much messier, full of the pursuit of directions which were naïve, fruitless or uninteresting.</p>
<p>While it is tempting to just ignore all these “failed” lines of inquiry, actually they turn out to be essential to one’s deeper understanding of a topic, and (via the process of elimination) finally zeroing in on the correct way to proceed.</p>
<p>So one should be unafraid to ask “stupid” questions, challenging conventional wisdom on a subject; the answers to these questions will occasionally lead to a surprising conclusion, but more often will simply tell you why the conventional wisdom is there in the first place, which is well worth knowing.</p>
<p>For instance, given a standard lemma in a subject, you can ask what happens if you delete a hypothesis, or attempt to strengthen the conclusion; if a simple result is usually proven by method X, you can ask whether it can be proven by method Y instead; the new proof may be less elegant than the original, or may not work at all, but in either case it tends to illuminate the relative power of methods X and Y, which can be useful when the time comes to prove less standard lemmas.</p>
<p>It’s also acceptable, when listening to a seminar, to ask “dumb” but constructive questions to help clarify some basic issue in the talk (e.g. whether statement X implied statement Y in the argument, or vice versa; whether a terminology introduced by the speaker is related to a very similar sounding terminology that you already knew about; and so forth). If you don’t ask, you might be lost for the remainder of the talk; and usually speakers appreciate the feedback (it shows that at least one audience member is paying attention!) and the opportunity to explain things better, both to you and to the rest of the audience. However, questions which do not immediately enhance the flow of the talk are probably best left to after the end of the talk.</p>

<ul>
<li>Martin Schwartz, “<a href="http://jcs.biologists.org/content/121/11/1771.full?sid=44d5ae18-0f2f-4944-b256-7d35968c9c23">The importance of stupidity in scientific research</a>“, <span>Journal of Cell Science&nbsp;</span><span>2008&nbsp;</span><span>121:&nbsp;</span><span>1771.</span></li>
</ul>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation (274 pts)]]></title>
            <link>https://github.com/cumulo-autumn/StreamDiffusion</link>
            <guid>38749434</guid>
            <pubDate>Sat, 23 Dec 2023 23:42:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cumulo-autumn/StreamDiffusion">https://github.com/cumulo-autumn/StreamDiffusion</a>, See on <a href="https://news.ycombinator.com/item?id=38749434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">StreamDiffusion</h2>
<p dir="auto"><a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/README.md">English</a> | <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/README-ja.md">日本語</a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_07.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_07.gif" width="90%" data-animated-image=""></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_09.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_09.gif" width="90%" data-animated-image=""></a>
</p>
<h2 tabindex="-1" dir="auto">StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation</h2>
<p dir="auto"><strong>Authors:</strong> <a href="https://www.linkedin.com/in/akio-kodaira-1a7b98252/" rel="nofollow">Akio Kodaira</a>, <a href="https://www.chenfengx.com/" rel="nofollow">Chenfeng Xu</a>, Toshiki Hazama, <a href="https://twitter.com/__ramu0e__" rel="nofollow">Takanori Yoshimoto</a>, <a href="https://www.linkedin.com/in/kohei--ohno/" rel="nofollow">Kohei Ohno</a>, <a href="https://me.ddpn.world/" rel="nofollow">Shogo Mitsuhori</a>, <a href="https://twitter.com/toni_nimono" rel="nofollow">Soichi Sugano</a>, <a href="https://twitter.com/hanyingcl" rel="nofollow">Hanying Cho</a>, <a href="https://zhijianliu.com/" rel="nofollow">Zhijian Liu</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=ID9QePIAAAAJ" rel="nofollow">Kurt Keutzer</a></p>
<p dir="auto">StreamDiffusion is an innovative diffusion pipeline designed for real-time interactive generation. It introduces significant performance enhancements to current diffusion-based image generation techniques.</p>
<p dir="auto"><a href="https://arxiv.org/abs/2312.12491" rel="nofollow"><img src="https://camo.githubusercontent.com/47151ae40041cc004f656a042aafc1b5877958bbcc298d169fca2d778994138a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330372e30343732352d6233316231622e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2307.04725-b31b1b.svg"></a>
<a href="https://huggingface.co/papers/2312.12491" rel="nofollow"><img src="https://camo.githubusercontent.com/dbe4a949263f6758ffe4b0757f52aa29772753ceebcca38af41aef80b720f57f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d7061706572732d79656c6c6f77" alt="Hugging Face Papers" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-papers-yellow"></a></p>
<p dir="auto">We sincerely thank <a href="https://twitter.com/AttaQjp" rel="nofollow">Taku Fujimoto</a> and <a href="https://twitter.com/radamar" rel="nofollow">Radamés Ajna</a> and Hugging Face team for their invaluable feedback, courteous support, and insightful discussions.</p>
<h2 tabindex="-1" dir="auto">Key Features</h2>
<ol dir="auto">
<li>
<p dir="auto"><strong>Stream Batch</strong></p>
<ul dir="auto">
<li>Streamlined data processing through efficient batch operations.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Residual Classifier-Free Guidance</strong> - <a href="#residual-cfg-rcfg">Learn More</a></p>
<ul dir="auto">
<li>Improved guidance mechanism that minimizes computational redundancy.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Stochastic Similarity Filter</strong> - <a href="#stochastic-similarity-filter">Learn More</a></p>
<ul dir="auto">
<li>Improves GPU utilization efficiency through advanced filtering techniques.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>IO Queues</strong></p>
<ul dir="auto">
<li>Efficiently manages input and output operations for smoother execution.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Pre-Computation for KV-Caches</strong></p>
<ul dir="auto">
<li>Optimizes caching strategies for accelerated processing.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Model Acceleration Tools</strong></p>
<ul dir="auto">
<li>Utilizes various tools for model optimization and performance boost.</li>
</ul>
</li>
</ol>
<p dir="auto">When images are produced using our proposed StreamDiffusion pipeline in an environment with <strong>GPU: RTX 4090</strong>, <strong>CPU: Core i9-13900K</strong>, and <strong>OS: Ubuntu 22.04.3 LTS</strong>.</p>
<table>
<thead>
<tr>
<th>model</th>
<th>Denoising Step</th>
<th>fps on Txt2Img</th>
<th>fps on Img2Img</th>
</tr>
</thead>
<tbody>
<tr>
<td>SD-turbo</td>
<td>1</td>
<td>106.16</td>
<td>93.897</td>
</tr>
<tr>
<td>LCM-LoRA <br>+<br> KohakuV2</td>
<td>4</td>
<td>38.023</td>
<td>37.133</td>
</tr>
</tbody>
</table>
<p dir="auto">Feel free to explore each feature by following the provided links to learn more about StreamDiffusion's capabilities. If you find it helpful, please consider citing our work:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{kodaira2023streamdiffusion,
      title={StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation},
      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},
      year={2023},
      eprint={2312.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre>@article{kodaira2023streamdiffusion,
      title={StreamDiffusion: A Pipeline-level Solution <span>for</span> Real-time Interactive Generation},
      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},
      year={2023},
      eprint={2312.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</pre></div>
<h2 tabindex="-1" dir="auto">Installation</h2>
<h3 tabindex="-1" dir="auto">Step0: clone this repository</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/cumulo-autumn/StreamDiffusion.git"><pre>git clone https://github.com/cumulo-autumn/StreamDiffusion.git</pre></div>
<h3 tabindex="-1" dir="auto">Step1: Make Environment</h3>
<p dir="auto">You can install StreamDiffusion via pip, conda, or Docker(explanation below).</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n streamdiffusion python=3.10
conda activate streamdiffusion"><pre>conda create -n streamdiffusion python=3.10
conda activate streamdiffusion</pre></div>
<p dir="auto">OR</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m venv .venv
# Windows
.\.venv\Scripts\activate
# Linux
source .venv/bin/activate"><pre>python -m venv .venv
# Windows
.\.venv\Scripts\activate
# Linux
source .venv/bin/activate</pre></div>
<h3 tabindex="-1" dir="auto">Step2: Install PyTorch</h3>
<p dir="auto">Select the appropriate version for your system.</p>
<p dir="auto">CUDA 11.8</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118"><pre>pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118</pre></div>
<p dir="auto">CUDA 12.1</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121"><pre>pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121</pre></div>
<p dir="auto">details: <a href="https://pytorch.org/" rel="nofollow">https://pytorch.org/</a></p>
<h3 tabindex="-1" dir="auto">Step3: Install StreamDiffusion</h3>
<h4 tabindex="-1" dir="auto">For User</h4>
<p dir="auto">Install StreamDiffusion</p>
<div dir="auto" data-snippet-clipboard-copy-content="#for Latest Version (recommended)
pip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]


#or


#for Stable Version
pip install streamdiffusion[tensorrt]"><pre><span><span>#</span>for Latest Version (recommended)</span>
pip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]


<span><span>#</span>or</span>


<span><span>#</span>for Stable Version</span>
pip install streamdiffusion[tensorrt]</pre></div>
<p dir="auto">Install TensorRT extension and pywin32
(※※pywin32 is required only for Windows.)</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m streamdiffusion.tools.install-tensorrt
# If you use Windows, you need to install pywin32 
pip install pywin32"><pre>python -m streamdiffusion.tools.install-tensorrt
<span><span>#</span> If you use Windows, you need to install pywin32 </span>
pip install pywin32</pre></div>
<h4 tabindex="-1" dir="auto">For Developer</h4>
<div dir="auto" data-snippet-clipboard-copy-content="python setup.py develop easy_install streamdiffusion[tensorrt]
python -m streamdiffusion.tools.install-tensorrt"><pre>python setup.py develop easy_install streamdiffusion[tensorrt]
python -m streamdiffusion.tools.install-tensorrt</pre></div>
<h3 tabindex="-1" dir="auto">Docker Installation (TensorRT Ready)</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/cumulo-autumn/StreamDiffusion.git
cd StreamDiffusion
docker build -t stream-diffusion:latest -f Dockerfile .
docker run --gpus all -it -v $(pwd):/home/ubuntu/streamdiffusion stream-diffusion:latest"><pre>git clone https://github.com/cumulo-autumn/StreamDiffusion.git
<span>cd</span> StreamDiffusion
docker build -t stream-diffusion:latest -f Dockerfile <span>.</span>
docker run --gpus all -it -v <span><span>$(</span>pwd<span>)</span></span>:/home/ubuntu/streamdiffusion stream-diffusion:latest</pre></div>
<h2 tabindex="-1" dir="auto">Quick Start</h2>
<p dir="auto">You can try StreamDiffusion in <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/examples"><code>examples</code></a> directory.</p>
<table>
<thead>
<tr>
<th><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_02.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_02.gif" alt="画像3" data-animated-image=""></a></th>
<th><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_03.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_03.gif" alt="画像4" data-animated-image=""></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_04.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_04.gif" alt="画像5" data-animated-image=""></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_05.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_05.gif" alt="画像6" data-animated-image=""></a></td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Real-Time Txt2Img Demo</h2>
<p dir="auto">There is an interactive txt2img demo in <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/demo/realtime-txt2img"><code>demo/realtime-txt2img</code></a> directory!</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_01.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_01.gif" width="100%" data-animated-image=""></a>
</p>
<h2 tabindex="-1" dir="auto">Usage Example</h2>
<p dir="auto">We provide a simple example of how to use StreamDiffusion. For more detailed examples, please refer to <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/examples"><code>examples</code></a> directory.</p>
<h3 tabindex="-1" dir="auto">Image-to-Image</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from diffusers import AutoencoderTiny, StableDiffusionPipeline
from diffusers.utils import load_image

from streamdiffusion import StreamDiffusion
from streamdiffusion.image_utils import postprocess_image

# You can load any models using diffuser's StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(&quot;KBlueLeaf/kohaku-v2.1&quot;).to(
    device=torch.device(&quot;cuda&quot;),
    dtype=torch.float16,
)

# Wrap the pipeline in StreamDiffusion
stream = StreamDiffusion(
    pipe,
    t_index_list=[32, 45],
    torch_dtype=torch.float16,
)

# If the loaded model is not LCM, merge LCM
stream.load_lcm_lora()
stream.fuse_lora()
# Use Tiny VAE for further acceleration
stream.vae = AutoencoderTiny.from_pretrained(&quot;madebyollin/taesd&quot;).to(device=pipe.device, dtype=pipe.dtype)
# Enable acceleration
pipe.enable_xformers_memory_efficient_attention()


prompt = &quot;1girl with dog hair, thick frame glasses&quot;
# Prepare the stream
stream.prepare(prompt)

# Prepare image
init_image = load_image(&quot;assets/img2img_example.png&quot;).resize((512, 512))

# Warmup >= len(t_index_list) x frame_buffer_size
for _ in range(2):
    stream(init_image)

# Run the stream infinitely
while True:
    x_output = stream(init_image)
    postprocess_image(x_output, output_type=&quot;pil&quot;)[0].show()
    input_response = input(&quot;Press Enter to continue or type 'stop' to exit: &quot;)
    if input_response == &quot;stop&quot;:
        break"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>diffusers</span> <span>import</span> <span>AutoencoderTiny</span>, <span>StableDiffusionPipeline</span>
<span>from</span> <span>diffusers</span>.<span>utils</span> <span>import</span> <span>load_image</span>

<span>from</span> <span>streamdiffusion</span> <span>import</span> <span>StreamDiffusion</span>
<span>from</span> <span>streamdiffusion</span>.<span>image_utils</span> <span>import</span> <span>postprocess_image</span>

<span># You can load any models using diffuser's StableDiffusionPipeline</span>
<span>pipe</span> <span>=</span> <span>StableDiffusionPipeline</span>.<span>from_pretrained</span>(<span>"KBlueLeaf/kohaku-v2.1"</span>).<span>to</span>(
    <span>device</span><span>=</span><span>torch</span>.<span>device</span>(<span>"cuda"</span>),
    <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># Wrap the pipeline in StreamDiffusion</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    <span>t_index_list</span><span>=</span>[<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># If the loaded model is not LCM, merge LCM</span>
<span>stream</span>.<span>load_lcm_lora</span>()
<span>stream</span>.<span>fuse_lora</span>()
<span># Use Tiny VAE for further acceleration</span>
<span>stream</span>.<span>vae</span> <span>=</span> <span>AutoencoderTiny</span>.<span>from_pretrained</span>(<span>"madebyollin/taesd"</span>).<span>to</span>(<span>device</span><span>=</span><span>pipe</span>.<span>device</span>, <span>dtype</span><span>=</span><span>pipe</span>.<span>dtype</span>)
<span># Enable acceleration</span>
<span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()


<span>prompt</span> <span>=</span> <span>"1girl with dog hair, thick frame glasses"</span>
<span># Prepare the stream</span>
<span>stream</span>.<span>prepare</span>(<span>prompt</span>)

<span># Prepare image</span>
<span>init_image</span> <span>=</span> <span>load_image</span>(<span>"assets/img2img_example.png"</span>).<span>resize</span>((<span>512</span>, <span>512</span>))

<span># Warmup &gt;= len(t_index_list) x frame_buffer_size</span>
<span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>2</span>):
    <span>stream</span>(<span>init_image</span>)

<span># Run the stream infinitely</span>
<span>while</span> <span>True</span>:
    <span>x_output</span> <span>=</span> <span>stream</span>(<span>init_image</span>)
    <span>postprocess_image</span>(<span>x_output</span>, <span>output_type</span><span>=</span><span>"pil"</span>)[<span>0</span>].<span>show</span>()
    <span>input_response</span> <span>=</span> <span>input</span>(<span>"Press Enter to continue or type 'stop' to exit: "</span>)
    <span>if</span> <span>input_response</span> <span>==</span> <span>"stop"</span>:
        <span>break</span></pre></div>
<h3 tabindex="-1" dir="auto">Text-to-Image</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from diffusers import AutoencoderTiny, StableDiffusionPipeline

from streamdiffusion import StreamDiffusion
from streamdiffusion.image_utils import postprocess_image

# You can load any models using diffuser's StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(&quot;KBlueLeaf/kohaku-v2.1&quot;).to(
    device=torch.device(&quot;cuda&quot;),
    dtype=torch.float16,
)

# Wrap the pipeline in StreamDiffusion
# Requires more long steps (len(t_index_list)) in text2image
# You recommend to use cfg_type=&quot;none&quot; when text2image
stream = StreamDiffusion(
    pipe,
    t_index_list=[0, 16, 32, 45],
    torch_dtype=torch.float16,
    cfg_type=&quot;none&quot;,
)

# If the loaded model is not LCM, merge LCM
stream.load_lcm_lora()
stream.fuse_lora()
# Use Tiny VAE for further acceleration
stream.vae = AutoencoderTiny.from_pretrained(&quot;madebyollin/taesd&quot;).to(device=pipe.device, dtype=pipe.dtype)
# Enable acceleration
pipe.enable_xformers_memory_efficient_attention()


prompt = &quot;1girl with dog hair, thick frame glasses&quot;
# Prepare the stream
stream.prepare(prompt)

# Warmup >= len(t_index_list) x frame_buffer_size
for _ in range(4):
    stream()

# Run the stream infinitely
while True:
    x_output = stream.txt2img()
    postprocess_image(x_output, output_type=&quot;pil&quot;)[0].show()
    input_response = input(&quot;Press Enter to continue or type 'stop' to exit: &quot;)
    if input_response == &quot;stop&quot;:
        break"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>diffusers</span> <span>import</span> <span>AutoencoderTiny</span>, <span>StableDiffusionPipeline</span>

<span>from</span> <span>streamdiffusion</span> <span>import</span> <span>StreamDiffusion</span>
<span>from</span> <span>streamdiffusion</span>.<span>image_utils</span> <span>import</span> <span>postprocess_image</span>

<span># You can load any models using diffuser's StableDiffusionPipeline</span>
<span>pipe</span> <span>=</span> <span>StableDiffusionPipeline</span>.<span>from_pretrained</span>(<span>"KBlueLeaf/kohaku-v2.1"</span>).<span>to</span>(
    <span>device</span><span>=</span><span>torch</span>.<span>device</span>(<span>"cuda"</span>),
    <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># Wrap the pipeline in StreamDiffusion</span>
<span># Requires more long steps (len(t_index_list)) in text2image</span>
<span># You recommend to use cfg_type="none" when text2image</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    <span>t_index_list</span><span>=</span>[<span>0</span>, <span>16</span>, <span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
    <span>cfg_type</span><span>=</span><span>"none"</span>,
)

<span># If the loaded model is not LCM, merge LCM</span>
<span>stream</span>.<span>load_lcm_lora</span>()
<span>stream</span>.<span>fuse_lora</span>()
<span># Use Tiny VAE for further acceleration</span>
<span>stream</span>.<span>vae</span> <span>=</span> <span>AutoencoderTiny</span>.<span>from_pretrained</span>(<span>"madebyollin/taesd"</span>).<span>to</span>(<span>device</span><span>=</span><span>pipe</span>.<span>device</span>, <span>dtype</span><span>=</span><span>pipe</span>.<span>dtype</span>)
<span># Enable acceleration</span>
<span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()


<span>prompt</span> <span>=</span> <span>"1girl with dog hair, thick frame glasses"</span>
<span># Prepare the stream</span>
<span>stream</span>.<span>prepare</span>(<span>prompt</span>)

<span># Warmup &gt;= len(t_index_list) x frame_buffer_size</span>
<span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>4</span>):
    <span>stream</span>()

<span># Run the stream infinitely</span>
<span>while</span> <span>True</span>:
    <span>x_output</span> <span>=</span> <span>stream</span>.<span>txt2img</span>()
    <span>postprocess_image</span>(<span>x_output</span>, <span>output_type</span><span>=</span><span>"pil"</span>)[<span>0</span>].<span>show</span>()
    <span>input_response</span> <span>=</span> <span>input</span>(<span>"Press Enter to continue or type 'stop' to exit: "</span>)
    <span>if</span> <span>input_response</span> <span>==</span> <span>"stop"</span>:
        <span>break</span></pre></div>
<p dir="auto">You can make it faster by using SD-Turbo.</p>
<h3 tabindex="-1" dir="auto">Faster generation</h3>
<p dir="auto">Replace the following code in the above example.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipe.enable_xformers_memory_efficient_attention()"><pre><span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()</pre></div>
<p dir="auto">To</p>
<div dir="auto" data-snippet-clipboard-copy-content="from streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt

stream = accelerate_with_tensorrt(
    stream, &quot;engines&quot;, max_batch_size=2,
)"><pre><span>from</span> <span>streamdiffusion</span>.<span>acceleration</span>.<span>tensorrt</span> <span>import</span> <span>accelerate_with_tensorrt</span>

<span>stream</span> <span>=</span> <span>accelerate_with_tensorrt</span>(
    <span>stream</span>, <span>"engines"</span>, <span>max_batch_size</span><span>=</span><span>2</span>,
)</pre></div>
<p dir="auto">It requires TensorRT extension and time to build the engine, but it will be faster than the above example.</p>
<h2 tabindex="-1" dir="auto">Optionals</h2>
<h3 tabindex="-1" dir="auto">Stochastic Similarity Filter</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_06.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_06.gif" alt="demo" data-animated-image=""></a></p>
<p dir="auto">Stochastic Similarity Filter reduces processing during video input by minimizing conversion operations when there is little change from the previous frame, thereby alleviating GPU processing load, as shown by the red frame in the above GIF. The usage is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="stream = StreamDiffusion(
    pipe,
    [32, 45],
    torch_dtype=torch.float16,
)
stream.enable_similar_image_filter(
    similar_image_filter_threshold,
    similar_image_filter_max_skip_frame,
)"><pre><span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    [<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)
<span>stream</span>.<span>enable_similar_image_filter</span>(
    <span>similar_image_filter_threshold</span>,
    <span>similar_image_filter_max_skip_frame</span>,
)</pre></div>
<p dir="auto">There are the following parameters that can be set as arguments in the function:</p>
<h4 tabindex="-1" dir="auto"><code>similar_image_filter_threshold</code></h4>
<ul dir="auto">
<li>The threshold for similarity between the previous frame and the current frame before the processing is paused.</li>
</ul>
<h4 tabindex="-1" dir="auto"><code>similar_image_filter_max_skip_frame</code></h4>
<ul dir="auto">
<li>The maximum interval during the pause before resuming the conversion.</li>
</ul>
<h3 tabindex="-1" dir="auto">Residual CFG (RCFG)</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/cfg_conparision.png"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/cfg_conparision.png" alt="rcfg"></a></p>
<p dir="auto">RCFG is a method for approximately realizing CFG with competitive computational complexity compared to cases where CFG is not used. It can be specified through the cfg_type argument in the StreamDiffusion. There are two types of RCFG: one with no specified items for negative prompts RCFG Self-Negative and one where negative prompts can be specified RCFG Onetime-Negative. In terms of computational complexity, denoting the complexity without CFG as N and the complexity with a regular CFG as 2N, RCFG Self-Negative can be computed in N steps, while RCFG Onetime-Negative can be computed in N+1 steps.</p>
<p dir="auto">The usage is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# w/0 CFG
cfg_type = &quot;none&quot;
# CFG
cfg_type = &quot;full&quot;
# RCFG Self-Negative
cfg_type = &quot;self&quot;
# RCFG Onetime-Negative
cfg_type = &quot;initialize&quot;
stream = StreamDiffusion(
    pipe,
    [32, 45],
    torch_dtype=torch.float16,
    cfg_type=cfg_type,
)
stream.prepare(
    prompt=&quot;1girl, purple hair&quot;,
    guidance_scale=guidance_scale,
    delta=delta,
)"><pre><span># w/0 CFG</span>
<span>cfg_type</span> <span>=</span> <span>"none"</span>
<span># CFG</span>
<span>cfg_type</span> <span>=</span> <span>"full"</span>
<span># RCFG Self-Negative</span>
<span>cfg_type</span> <span>=</span> <span>"self"</span>
<span># RCFG Onetime-Negative</span>
<span>cfg_type</span> <span>=</span> <span>"initialize"</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    [<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
    <span>cfg_type</span><span>=</span><span>cfg_type</span>,
)
<span>stream</span>.<span>prepare</span>(
    <span>prompt</span><span>=</span><span>"1girl, purple hair"</span>,
    <span>guidance_scale</span><span>=</span><span>guidance_scale</span>,
    <span>delta</span><span>=</span><span>delta</span>,
)</pre></div>
<p dir="auto">The delta has a moderating effect on the effectiveness of RCFG.</p>
<h2 tabindex="-1" dir="auto">Development Team</h2>
<p dir="auto"><a href="https://twitter.com/cumulo_autumn" rel="nofollow">Aki</a>,
<a href="https://twitter.com/AttaQjp" rel="nofollow">Ararat</a>,
<a href="https://twitter.com/Chenfeng_X" rel="nofollow">Chenfeng Xu</a>,
<a href="https://twitter.com/ddPn08" rel="nofollow">ddPn08</a>,
<a href="https://twitter.com/ArtengMimi" rel="nofollow">kizamimi</a>,
<a href="https://twitter.com/__ramu0e__" rel="nofollow">ramune</a>,
<a href="https://twitter.com/hanyingcl" rel="nofollow">teftef</a>,
<a href="https://twitter.com/toni_nimono" rel="nofollow">Tonimono</a>,
<a href="https://twitter.com/IMG_5955" rel="nofollow">Verb</a>,</p>
<p dir="auto">(*alphabetical order)
<br></p>
<h2 tabindex="-1" dir="auto">Acknowledgements</h2>
<p dir="auto">The video and image demos in this GitHub repository were generated using <a href="https://huggingface.co/latent-consistency/lcm-lora-sdv1-5" rel="nofollow">LCM-LoRA</a> + <a href="https://civitai.com/models/136268/kohaku-v2" rel="nofollow">KohakuV2</a> and <a href="https://arxiv.org/abs/2311.17042" rel="nofollow">SD-Turbo</a>.</p>
<p dir="auto">Special thanks to <a href="https://latent-consistency-models.github.io/" rel="nofollow">LCM-LoRA authors</a> for providing the LCM-LoRA and Kohaku BlueLeaf (<a href="https://twitter.com/KBlueleaf" rel="nofollow">@KBlueleaf</a>) for providing the KohakuV2 model and ,to <a href="https://ja.stability.ai/" rel="nofollow">Stability AI</a> for <a href="https://arxiv.org/abs/2311.17042" rel="nofollow">SD-Turbo</a>.</p>
<p dir="auto">KohakuV2 Models can be downloaded from  <a href="https://civitai.com/models/136268/kohaku-v2" rel="nofollow">Civitai</a>  and <a href="https://huggingface.co/KBlueLeaf/kohaku-v2.1" rel="nofollow">Hugging Face</a>.</p>
<p dir="auto">SD-Turbo is also available on <a href="https://huggingface.co/stabilityai/sd-turbo" rel="nofollow">Hugging Face Space</a>.</p>
<h2 tabindex="-1" dir="auto">Contributors</h2>
<a href="https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors">
  <img src="https://camo.githubusercontent.com/850d0f07f10bcc27c428009a76e9837c0546b93021c6caee911f4758fc440e7a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d63756d756c6f2d617574756d6e2f53747265616d446966667573696f6e" data-canonical-src="https://contrib.rocks/image?repo=cumulo-autumn/StreamDiffusion">
</a>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crown shyness (202 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Crown_shyness</link>
            <guid>38749170</guid>
            <pubDate>Sat, 23 Dec 2023 23:00:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Crown_shyness">https://en.wikipedia.org/wiki/Crown_shyness</a>, See on <a href="https://news.ycombinator.com/item?id=38749170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">

<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Dryobalanops_Aromatica_canopy.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Dryobalanops_Aromatica_canopy.jpg/300px-Dryobalanops_Aromatica_canopy.jpg" decoding="async" width="300" height="225" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Dryobalanops_Aromatica_canopy.jpg/450px-Dryobalanops_Aromatica_canopy.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Dryobalanops_Aromatica_canopy.jpg/600px-Dryobalanops_Aromatica_canopy.jpg 2x" data-file-width="1020" data-file-height="765"></a><figcaption>Canopy of <i><a href="https://en.wikipedia.org/wiki/Dryobalanops_aromatica" title="Dryobalanops aromatica">D. aromatica</a></i> at the <a href="https://en.wikipedia.org/wiki/Forest_Research_Institute_Malaysia" title="Forest Research Institute Malaysia">Forest Research Institute Malaysia</a> displaying crown shyness</figcaption></figure>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:River_of_Blue.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/River_of_Blue.jpg/300px-River_of_Blue.jpg" decoding="async" width="300" height="200" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/River_of_Blue.jpg/450px-River_of_Blue.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/River_of_Blue.jpg/600px-River_of_Blue.jpg 2x" data-file-width="2048" data-file-height="1365"></a><figcaption>Trees at <a href="https://en.wikipedia.org/wiki/Plaza_San_Mart%C3%ADn_(Buenos_Aires)" title="Plaza San Martín (Buenos Aires)">Plaza San Martín (Buenos Aires)</a>, Argentina</figcaption></figure>
<p><b>Crown shyness</b> (also <i>canopy disengagement</i>,<sup id="cite_ref-JWG_2008_1-0"><a href="#cite_note-JWG_2008-1">[1]</a></sup> <i>canopy shyness</i>,<sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup> or <i>inter-crown spacing</i><sup id="cite_ref-FEP_1984_3-0"><a href="#cite_note-FEP_1984-3">[3]</a></sup>) is a feature observed in some tree species, in which the <a href="https://en.wikipedia.org/wiki/Crown_(botany)" title="Crown (botany)">crowns</a> of fully stocked trees do not touch each other, instead forming a <a href="https://en.wikipedia.org/wiki/Canopy_(biology)" title="Canopy (biology)">canopy</a> with channel-like gaps.<sup id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup><sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>
This is most prevalent among trees of the same species, but also occurs between trees of different species.<sup id="cite_ref-AJR_1988_6-0"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-KP_1973_7-0"><a href="#cite_note-KP_1973-7">[7]</a></sup> There exist many hypotheses as to why crown shyness is an <a href="https://en.wikipedia.org/wiki/Adaptive_behavior" title="Adaptive behavior">adaptive behavior</a>, and research suggests that it might inhibit spread of <a href="https://en.wikipedia.org/wiki/Leaf_miner" title="Leaf miner">leaf-eating</a> insect <a href="https://en.wikipedia.org/wiki/Larvae" title="Larvae">larvae</a>.<sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Possible_physiological_explanations">Possible physiological explanations</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Crown_shyness&amp;action=edit&amp;section=1" title="Edit section: Possible physiological explanations"><span>edit</span></a><span>]</span></span></h2>
<p>The exact physiological basis of crown shyness is uncertain.<sup id="cite_ref-AJR_1988_6-1"><a href="#cite_note-AJR_1988-6">[6]</a></sup> It has been discussed in scientific literature since the 1920s.<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> The variety of hypotheses and experimental results might suggest that there are multiple mechanisms across different species, an example of <a href="https://en.wikipedia.org/wiki/Convergent_evolution" title="Convergent evolution">convergent evolution</a>.<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (February 2023)">citation needed</span></a></i>]</sup>
</p><p>Some hypotheses contend that the interdigitation of canopy branches leads to "reciprocal pruning" of adjacent trees. Trees in windy areas suffer physical damage as they collide with each other during <a href="https://en.wikipedia.org/wiki/Wind" title="Wind">winds</a>. The abrasions and collisions induce a crown shyness response. Studies suggest that lateral branch growth is largely uninfluenced by neighbours until disturbed by mechanical abrasion.<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup> If the crowns are artificially prevented from colliding in the winds, they gradually fill the canopy gaps.<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup> This  explains instances of crown shyness between branches of the same organism. Proponents of this idea cite that shyness is particularly seen in conditions conducive to this pruning, including windy forests, stands of flexible trees, and early succession forests where branches are flexible and limited in lateral movement.<sup id="cite_ref-AJR_1988_6-2"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-ROL_1980_12-0"><a href="#cite_note-ROL_1980-12">[12]</a></sup> According to this theory, variable flexibility in lateral branches greatly influences the degree of crown shyness.
</p><p>Similarly, some research suggests that constant abrasion at growth nodules disrupts bud tissue such that it is unable to continue with lateral growth. Australian forester M.R. Jacobs, who studied the crown shyness patterns in <a href="https://en.wikipedia.org/wiki/Eucalyptus" title="Eucalyptus">eucalyptus</a> in 1955, believed that the trees' growing tips were sensitive to abrasion, resulting in canopy gaps.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup> Miguel Franco (1986) observed that the branches of <i><a href="https://en.wikipedia.org/wiki/Picea_sitchensis" title="Picea sitchensis">Picea sitchensis</a></i> (Sitka spruce) and <i><a href="https://en.wikipedia.org/wiki/Larix_kaempferi" title="Larix kaempferi">Larix kaempferi</a></i> (Japanese larch) suffered physical damage due to abrasion, which killed the leading shoots.<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup><sup id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup>
</p><p>A prominent hypothesis is that canopy shyness has to do with mutual light sensing by adjacent plants. The photoreceptor-mediated <a href="https://en.wikipedia.org/wiki/Shade_avoidance" title="Shade avoidance">shade avoidance</a> response is a well-documented behavior in a variety of plant species.<sup id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup> Neighbor detection is thought to be a function of several unique photoreceptors. Plants can sense the proximity of neighbors by sensing backscattered <a href="https://en.wikipedia.org/wiki/Far-red" title="Far-red">far-red</a> light, a task widely thought to be accomplished by the activity of the <a href="https://en.wikipedia.org/wiki/Phytochrome" title="Phytochrome">phytochrome</a> photoreceptors.<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup> Many species of plant respond to an increase in far-red light (and, by extension, encroaching neighbors) by directing growth away from the far-red stimulus and by increasing the rate of elongation.<sup id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> Similarly, plants use blue light to induce the shade-avoidance response, likely playing a role in the recognition of neighboring plants,<sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> though this was just starting to be recognised in 1988.<sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup>
</p><p>The characterization of these behaviors might suggest that crown shyness is simply the result of mutual shading based on well-understood shade avoidance responses.<sup id="cite_ref-AJR_1988_6-3"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-F.S.P._Ng_1997_34–37_21-0"><a href="#cite_note-F.S.P._Ng_1997_34–37-21">[21]</a></sup> Malaysian scholar <a href="https://en.wikipedia.org/wiki/Francis_S.P._Ng" title="Francis S.P. Ng">Francis S.P. Ng</a>, who studied <i><a href="https://en.wikipedia.org/wiki/Dryobalanops_aromatica" title="Dryobalanops aromatica">Dryobalanops aromatica</a></i>, suggested that the growing tips were sensitive to light levels and stopped growing when nearing the adjacent foliage due to the induced shade.<sup id="cite_ref-AJR_1988_6-4"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-F.S.P._Ng_1997_34–37_21-1"><a href="#cite_note-F.S.P._Ng_1997_34–37-21">[21]</a></sup>
</p><p>A 2015 study has suggested that <i><a href="https://en.wikipedia.org/wiki/Arabidopsis_thaliana" title="Arabidopsis thaliana">Arabidopsis thaliana</a></i> shows different leaf placement strategies when grown amongst kin and unrelated conspecifics, shading dissimilar neighbors and avoiding kin. This response was shown to be contingent on the proper functioning of multiple photosensory modalities.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup> A 1998 study proposed similar systems of photoreceptor-mediated inhibition of growth as explanations of crown shyness,<sup id="cite_ref-AJR_1988_6-5"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-F.S.P._Ng_1997_34–37_21-2"><a href="#cite_note-F.S.P._Ng_1997_34–37-21">[21]</a></sup> though a causal link between photoreceptors and crown asymmetry had yet to be experimentally proven. This might explain instances of intercrown spacing that are only exhibited between conspecifics.<sup id="cite_ref-AJR_1988_6-6"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-KP_1973_7-1"><a href="#cite_note-KP_1973-7">[7]</a></sup>
</p>
<h2><span id="Species">Species</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Crown_shyness&amp;action=edit&amp;section=2" title="Edit section: Species"><span>edit</span></a><span>]</span></span></h2>
<p>Trees that display crown shyness patterns include:
</p>
<ul><li>Species of <i><a href="https://en.wikipedia.org/wiki/Dryobalanops" title="Dryobalanops">Dryobalanops</a></i>, including <i><a href="https://en.wikipedia.org/wiki/Dryobalanops_lanceolata" title="Dryobalanops lanceolata">Dryobalanops lanceolata</a></i><sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> and <i><a href="https://en.wikipedia.org/wiki/Dryobalanops_aromatica" title="Dryobalanops aromatica">Dryobalanops aromatica</a></i> (kapur)</li>
<li>Some species of <a href="https://en.wikipedia.org/wiki/Eucalypt" title="Eucalypt">eucalypt</a><sup id="cite_ref-RGF_2004_24-0"><a href="#cite_note-RGF_2004-24">[24]</a></sup></li>
<li><i><a href="https://en.wikipedia.org/wiki/Pinus_contorta" title="Pinus contorta">Pinus contorta</a></i> or lodgepole pine<sup id="cite_ref-JWG_2008_1-1"><a href="#cite_note-JWG_2008-1">[1]</a></sup></li>
<li><i><a href="https://en.wikipedia.org/wiki/Avicennia_germinans" title="Avicennia germinans">Avicennia germinans</a></i> or black mangrove<sup id="cite_ref-FEP_1984_3-1"><a href="#cite_note-FEP_1984-3">[3]</a></sup></li>
<li><i><a href="https://en.wikipedia.org/wiki/Schefflera" title="Schefflera">Schefflera</a> pittieri</i><sup id="cite_ref-AJR_1988_6-7"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-ROL_1980_12-1"><a href="#cite_note-ROL_1980-12">[12]</a></sup></li>
<li><i><a href="https://en.wikipedia.org/wiki/Clusia_alata" title="Clusia alata">Clusia alata</a></i><sup id="cite_ref-AJR_1988_6-8"><a href="#cite_note-AJR_1988-6">[6]</a></sup><sup id="cite_ref-ROL_1980_12-2"><a href="#cite_note-ROL_1980-12">[12]</a></sup></li>
<li>K. Paijmans observed crown shyness in a multi-species group of trees, comprising <i><a href="https://en.wikipedia.org/wiki/Celtis_spinosa" title="Celtis spinosa">Celtis spinosa</a></i> and <i><a href="https://en.wikipedia.org/w/index.php?title=Pterocymbium_beccarii&amp;action=edit&amp;redlink=1" title="Pterocymbium beccarii (page does not exist)">Pterocymbium beccarii</a></i><sup id="cite_ref-KP_1973_7-2"><a href="#cite_note-KP_1973-7">[7]</a></sup></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Crown_shyness&amp;action=edit&amp;section=3" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div>
<ol>
<li id="cite_note-JWG_2008-1"><span>^ <a href="#cite_ref-JWG_2008_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-JWG_2008_1-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFGoudiePolssonOtt2008">Goudie, James W.; Polsson, Kenneth R.; Ott, Peter K. (2008). <a rel="nofollow" href="https://books.google.com/books?id=FB3YaZKPoi4C&amp;pg=PA39">"An empirical model of crown shyness for lodgepole pine (Pinus contorta var. latifolia [Engl.] Critch.) in British Columbia"</a>. <i>Forest Ecology and Management</i>. <b>257</b> (1): 321–331. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2Fj.foreco.2008.09.005">10.1016/j.foreco.2008.09.005</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781437926163" title="Special:BookSources/9781437926163"><bdi>9781437926163</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Forest+Ecology+and+Management&amp;rft.atitle=An+empirical+model+of+crown+shyness+for+lodgepole+pine+%28Pinus+contorta+var.+latifolia+%5BEngl.%5D+Critch.%29+in+British+Columbia&amp;rft.volume=257&amp;rft.issue=1&amp;rft.pages=321-331&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1016%2Fj.foreco.2008.09.005&amp;rft.isbn=9781437926163&amp;rft.aulast=Goudie&amp;rft.aufirst=James+W.&amp;rft.au=Polsson%2C+Kenneth+R.&amp;rft.au=Ott%2C+Peter+K.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DFB3YaZKPoi4C%26pg%3DPA39&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><cite id="CITEREFPeter_ThomasJohn_Packham2007">Peter Thomas; John Packham (26 July 2007). <a rel="nofollow" href="https://books.google.com/books?id=0Ntvos9aaC8C&amp;pg=PA12"><i>Ecology of Woodlands and Forests: Description, Dynamics and Diversity</i></a>. Cambridge University Press. p.&nbsp;12. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-83452-0" title="Special:BookSources/978-0-521-83452-0"><bdi>978-0-521-83452-0</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Ecology+of+Woodlands+and+Forests%3A+Description%2C+Dynamics+and+Diversity&amp;rft.pages=12&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2007-07-26&amp;rft.isbn=978-0-521-83452-0&amp;rft.au=Peter+Thomas&amp;rft.au=John+Packham&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D0Ntvos9aaC8C%26pg%3DPA12&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-FEP_1984-3"><span>^ <a href="#cite_ref-FEP_1984_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-FEP_1984_3-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFPutzParkerArchibald1984">Putz, Francis E.; Parker, Geoffrey G.; Archibald, Ruth M. (1984). <a rel="nofollow" href="https://repository.si.edu/bitstream/handle/10088/17792/serc_Putz_etal_1984_AmMidlNat_112_24_28.pdf">"Mechanical Abrasion and Intercrown Spacing"</a> <span>(PDF)</span>. <i>American Midland Naturalist</i>. <b>112</b> (1): 24–28. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2425452">10.2307/2425452</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2425452">2425452</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=American+Midland+Naturalist&amp;rft.atitle=Mechanical+Abrasion+and+Intercrown+Spacing&amp;rft.volume=112&amp;rft.issue=1&amp;rft.pages=24-28&amp;rft.date=1984&amp;rft_id=info%3Adoi%2F10.2307%2F2425452&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2425452%23id-name%3DJSTOR&amp;rft.aulast=Putz&amp;rft.aufirst=Francis+E.&amp;rft.au=Parker%2C+Geoffrey+G.&amp;rft.au=Archibald%2C+Ruth+M.&amp;rft_id=https%3A%2F%2Frepository.si.edu%2Fbitstream%2Fhandle%2F10088%2F17792%2Fserc_Putz_etal_1984_AmMidlNat_112_24_28.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><cite id="CITEREFNorsiha_A._and_Shamsudin2015">Norsiha A. and Shamsudin (2015-04-25). <a rel="nofollow" href="http://www.frim.gov.my/shorea-resinosa-another-jigsaw-puzzle-in-the-sky/">"Shorea resinosa&nbsp;: Another jigsaw puzzle in the sky"</a>. Forest Research Institute Malaysia.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Shorea+resinosa+%3A+Another+jigsaw+puzzle+in+the+sky&amp;rft.pub=Forest+Research+Institute+Malaysia&amp;rft.date=2015-04-25&amp;rft.au=Norsiha+A.+and+Shamsudin&amp;rft_id=http%3A%2F%2Fwww.frim.gov.my%2Fshorea-resinosa-another-jigsaw-puzzle-in-the-sky%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Canadian+Journal+of+Forest+Research&amp;rft.atitle=Crown+shyness+in+lodgepole+pine+stands+of+varying+stand+height%2C+density+and+site+index+in+the+upper+foothills+of+Alberta&amp;rft.volume=36&amp;rft.issue=9&amp;rft.pages=2104-2111&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1139%2Fx06-107&amp;rft.aulast=Fish&amp;rft.aufirst=H&amp;rft.au=Lieffers%2C+VJ&amp;rft.au=Silins%2C+U&amp;rft.au=Hall%2C+RJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-AJR_1988-6"><span>^ <a href="#cite_ref-AJR_1988_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-AJR_1988_6-8"><sup><i><b>i</b></i></sup></a></span> <span><cite id="CITEREFRebertus1988">Rebertus, Alan J (1988). <a rel="nofollow" href="ftp://169.158.189.34/pub/Biotropica/1980s/1988/20-4/Biotropica-1988-20-4-p338.pdf">"Crown shyness in a tropical cloud forest"</a> <span>(PDF)</span>. <i>Biotropica</i>. <b>20</b> (4): 338–339. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2388326">10.2307/2388326</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0006-3606">0006-3606</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2388326">2388326</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biotropica&amp;rft.atitle=Crown+shyness+in+a+tropical+cloud+forest&amp;rft.volume=20&amp;rft.issue=4&amp;rft.pages=338-339&amp;rft.date=1988&amp;rft.issn=0006-3606&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2388326%23id-name%3DJSTOR&amp;rft_id=info%3Adoi%2F10.2307%2F2388326&amp;rft.aulast=Rebertus&amp;rft.aufirst=Alan+J&amp;rft_id=ftp%3A%2F%2F169.158.189.34%2Fpub%2FBiotropica%2F1980s%2F1988%2F20-4%2FBiotropica-1988-20-4-p338.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span><sup><span>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Link_rot" title="Wikipedia:Link rot"><span title="&nbsp;Dead link tagged November 2019">permanent dead link</span></a></i>]</span></sup></span>
</li>
<li id="cite_note-KP_1973-7"><span>^ <a href="#cite_ref-KP_1973_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-KP_1973_7-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-KP_1973_7-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFK._Paijmans1973">K. Paijmans (1973). <a rel="nofollow" href="http://scholarspace.manoa.hawaii.edu/bitstream/handle/10125/802/v27n3-260-268.pdf">"Plant Succession on Pago and Witori Volcanoes, New Britain"</a> <span>(PDF)</span>. <i>Pacific Science</i>. University of Hawaii Press. <b>27</b> (3): 60–268. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0030-8870">0030-8870</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pacific+Science&amp;rft.atitle=Plant+Succession+on+Pago+and+Witori+Volcanoes%2C+New+Britain&amp;rft.volume=27&amp;rft.issue=3&amp;rft.pages=60-268&amp;rft.date=1973&amp;rft.issn=0030-8870&amp;rft.au=K.+Paijmans&amp;rft_id=http%3A%2F%2Fscholarspace.manoa.hawaii.edu%2Fbitstream%2Fhandle%2F10125%2F802%2Fv27n3-260-268.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.zoo.org/document.doc?id=1110">"Tropical Rain Forest"</a>. Woodland Park Zoo. p.&nbsp;37.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Tropical+Rain+Forest&amp;rft.pages=37&amp;rft.pub=Woodland+Park+Zoo&amp;rft_id=http%3A%2F%2Fwww.zoo.org%2Fdocument.doc%3Fid%3D1110&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.for.gov.bc.ca/hfd/library/FIA/2008/FSP_Y083088.pdf">"TASS III: Simulating the management, growth and yield of complex stands"</a> <span>(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=TASS+III%3A+Simulating+the+management%2C+growth+and+yield+of+complex+stands&amp;rft_id=https%3A%2F%2Fwww.for.gov.bc.ca%2Fhfd%2Flibrary%2FFIA%2F2008%2FFSP_Y083088.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite id="CITEREFFranco1986">Franco, M (14 August 1986). "The influences of neighbours on the growth of modular organisms with an example from trees". <i>Philosophical Transactions of the Royal Society of London. B, Biological Sciences</i>. <b>313</b> (1159): 313, 209–225. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1986RSPTB.313..209F">1986RSPTB.313..209F</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1098%2Frstb.1986.0034">10.1098/rstb.1986.0034</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Transactions+of+the+Royal+Society+of+London.+B%2C+Biological+Sciences&amp;rft.atitle=The+influences+of+neighbours+on+the+growth+of+modular+organisms+with+an+example+from+trees&amp;rft.volume=313&amp;rft.issue=1159&amp;rft.pages=313%2C+209-225&amp;rft.date=1986-08-14&amp;rft_id=info%3Adoi%2F10.1098%2Frstb.1986.0034&amp;rft_id=info%3Abibcode%2F1986RSPTB.313..209F&amp;rft.aulast=Franco&amp;rft.aufirst=M&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite id="CITEREFVictor_Lieffers">Victor Lieffers. <a rel="nofollow" href="https://web.archive.org/web/20150925144427/http://www.sfmn.ales.ualberta.ca/en/Publications/~/media/sfmn/Publications/ResearchNotes/Documents/RN_E36_CrownShyness_low.ashx">"Crown shyness in maturing boreal forest stands"</a>. <i>SFM Network Research Note Series</i>. <b>36</b>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1715-0981">1715-0981</a>. Archived from <a rel="nofollow" href="http://www.sfmn.ales.ualberta.ca/en/Publications/~/media/sfmn/Publications/ResearchNotes/Documents/RN_E36_CrownShyness_low.ashx">the original</a> on 2015-09-25<span>. Retrieved <span>2015-08-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SFM+Network+Research+Note+Series&amp;rft.atitle=Crown+shyness+in+maturing+boreal+forest+stands&amp;rft.volume=36&amp;rft.issn=1715-0981&amp;rft.au=Victor+Lieffers&amp;rft_id=http%3A%2F%2Fwww.sfmn.ales.ualberta.ca%2Fen%2FPublications%2F~%2Fmedia%2Fsfmn%2FPublications%2FResearchNotes%2FDocuments%2FRN_E36_CrownShyness_low.ashx&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-ROL_1980-12"><span>^ <a href="#cite_ref-ROL_1980_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ROL_1980_12-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ROL_1980_12-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFLawtonPutz">Lawton, RO; Putz, Francis E. "The vegetation of the Monteverde Cloud Forest Reserve". <i>Brenesia</i>. <b>18</b>: 101–116.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Brenesia&amp;rft.atitle=The+vegetation+of+the+Monteverde+Cloud+Forest+Reserve&amp;rft.volume=18&amp;rft.pages=101-116&amp;rft.aulast=Lawton&amp;rft.aufirst=RO&amp;rft.au=Putz%2C+Francis+E&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFMaxwell_Ralph_Jacobs1955">Maxwell Ralph Jacobs (1955). <a rel="nofollow" href="http://trove.nla.gov.au/version/28637821"><i>Growth Habits of the Eucalypts</i></a>. Forestry and Timber Bureau.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Growth+Habits+of+the+Eucalypts&amp;rft.pub=Forestry+and+Timber+Bureau&amp;rft.date=1955&amp;rft.au=Maxwell+Ralph+Jacobs&amp;rft_id=http%3A%2F%2Ftrove.nla.gov.au%2Fversion%2F28637821&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite id="CITEREFM._Franco1986">M. Franco (14 August 1986). "The Influences of Neighbours on the Growth of Modular Organisms with an Example from Trees". <i>Philosophical Transactions of the Royal Society B</i>. <b>313</b> (1159): 209–225. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1986RSPTB.313..209F">1986RSPTB.313..209F</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1098%2Frstb.1986.0034">10.1098/rstb.1986.0034</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Transactions+of+the+Royal+Society+B&amp;rft.atitle=The+Influences+of+Neighbours+on+the+Growth+of+Modular+Organisms+with+an+Example+from+Trees&amp;rft.volume=313&amp;rft.issue=1159&amp;rft.pages=209-225&amp;rft.date=1986-08-14&amp;rft_id=info%3Adoi%2F10.1098%2Frstb.1986.0034&amp;rft_id=info%3Abibcode%2F1986RSPTB.313..209F&amp;rft.au=M.+Franco&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-15"><span><b><a href="#cite_ref-15">^</a></b></span> <span><cite id="CITEREFWilsonAgnewRoxburgh2019">Wilson, J. Bastow; Agnew, Andrew D.Q.; Roxburgh, Stephen H. (2019). <a rel="nofollow" href="https://www.cambridge.org/core/books/abs/nature-of-plant-communities/interactions-between-species/5B1A7C7580CBFC055BC5FEDB05724301">"2: Interactions between Species"</a>. <i>The Nature of Plant Communities</i>. Cambridge: Cambridge University Press. pp.&nbsp;24–65. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1017%2F9781108612265.004">10.1017/9781108612265.004</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781108612265" title="Special:BookSources/9781108612265"><bdi>9781108612265</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=2%3A+Interactions+between+Species&amp;rft.btitle=The+Nature+of+Plant+Communities&amp;rft.pages=24-65&amp;rft.pub=Cambridge%3A+Cambridge+University+Press&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.1017%2F9781108612265.004&amp;rft.isbn=9781108612265&amp;rft.aulast=Wilson&amp;rft.aufirst=J.+Bastow&amp;rft.au=Agnew%2C+Andrew+D.Q.&amp;rft.au=Roxburgh%2C+Stephen+H.&amp;rft_id=https%3A%2F%2Fwww.cambridge.org%2Fcore%2Fbooks%2Fabs%2Fnature-of-plant-communities%2Finteractions-between-species%2F5B1A7C7580CBFC055BC5FEDB05724301&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-16"><span><b><a href="#cite_ref-16">^</a></b></span> <span><cite id="CITEREFBallaréScopelSánchez1990">Ballaré, CL; Scopel, AL; Sánchez, RA (19 January 1990). "Far-red radiation reflected from adjacent leaves: an early signal of competition in plant canopies". <i>Science</i>. <b>247</b> (4940): 329–32. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1990Sci...247..329B">1990Sci...247..329B</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1126%2Fscience.247.4940.329">10.1126/science.247.4940.329</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/17735851">17735851</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:39622241">39622241</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Far-red+radiation+reflected+from+adjacent+leaves%3A+an+early+signal+of+competition+in+plant+canopies&amp;rft.volume=247&amp;rft.issue=4940&amp;rft.pages=329-32&amp;rft.date=1990-01-19&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.247.4940.329&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A39622241%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F17735851&amp;rft_id=info%3Abibcode%2F1990Sci...247..329B&amp;rft.aulast=Ballar%C3%A9&amp;rft.aufirst=CL&amp;rft.au=Scopel%2C+AL&amp;rft.au=S%C3%A1nchez%2C+RA&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFBallareSanchezScopelCasal1987">Ballare, C. L.; Sanchez, R. A.; Scopel, Ana L.; Casal, J. J.; Ghersa, C. M. (September 1987). "Early detection of neighbour plants by phytochrome perception of spectral changes in reflected sunlight". <i>Plant, Cell and Environment</i>. <b>10</b> (7): 551–557. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1111%2F1365-3040.ep11604091">10.1111/1365-3040.ep11604091</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Plant%2C+Cell+and+Environment&amp;rft.atitle=Early+detection+of+neighbour+plants+by+phytochrome+perception+of+spectral+changes+in+reflected+sunlight.&amp;rft.volume=10&amp;rft.issue=7&amp;rft.pages=551-557&amp;rft.date=1987-09&amp;rft_id=info%3Adoi%2F10.1111%2F1365-3040.ep11604091&amp;rft.aulast=Ballare&amp;rft.aufirst=C.+L.&amp;rft.au=Sanchez%2C+R.+A.&amp;rft.au=Scopel%2C+Ana+L.&amp;rft.au=Casal%2C+J.+J.&amp;rft.au=Ghersa%2C+C.+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-18"><span><b><a href="#cite_ref-18">^</a></b></span> <span><cite id="CITEREFBallaréScopelSánchez1997">Ballaré, CL; Scopel, AL; Sánchez, RA (June 1997). <a rel="nofollow" href="https://doi.org/10.1046%2Fj.1365-3040.1997.d01-112.x">"Foraging for light: photosensory ecology and agricultural implications"</a>. <i>Plant, Cell and Environment</i>. <b>20</b> (6): 820–825. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1046%2Fj.1365-3040.1997.d01-112.x">10.1046/j.1365-3040.1997.d01-112.x</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Plant%2C+Cell+and+Environment&amp;rft.atitle=Foraging+for+light%3A+photosensory+ecology+and+agricultural+implications&amp;rft.volume=20&amp;rft.issue=6&amp;rft.pages=820-825&amp;rft.date=1997-06&amp;rft_id=info%3Adoi%2F10.1046%2Fj.1365-3040.1997.d01-112.x&amp;rft.aulast=Ballar%C3%A9&amp;rft.aufirst=CL&amp;rft.au=Scopel%2C+AL&amp;rft.au=S%C3%A1nchez%2C+RA&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1046%252Fj.1365-3040.1997.d01-112.x&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><cite id="CITEREFJansenGabaGreenberg1998">Jansen, Marcel AK; Gaba, Victor; Greenberg, Bruce M (April 1998). "Higher plants and UV-B radiation: balancing damage, repair and acclimation". <i>Trends in Plant Science</i>. <b>3</b> (4): 131–135. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1016%2FS1360-1385%2898%2901215-1">10.1016/S1360-1385(98)01215-1</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Trends+in+Plant+Science&amp;rft.atitle=Higher+plants+and+UV-B+radiation%3A+balancing+damage%2C+repair+and+acclimation&amp;rft.volume=3&amp;rft.issue=4&amp;rft.pages=131-135&amp;rft.date=1998-04&amp;rft_id=info%3Adoi%2F10.1016%2FS1360-1385%2898%2901215-1&amp;rft.aulast=Jansen&amp;rft.aufirst=Marcel+AK&amp;rft.au=Gaba%2C+Victor&amp;rft.au=Greenberg%2C+Bruce+M&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><cite id="CITEREFChristieReymondPowellBernasconi1998">Christie, JM; Reymond, P; Powell, GK; Bernasconi, P; Raibekas, AA; Liscum, E; Briggs, WR (27 November 1998). "Arabidopsis NPH1: a flavoprotein with the properties of a photoreceptor for phototropism". <i>Science</i>. <b>282</b> (5394): 1698–701. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1998Sci...282.1698C">1998Sci...282.1698C</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1126%2Fscience.282.5394.1698">10.1126/science.282.5394.1698</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/9831559">9831559</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Arabidopsis+NPH1%3A+a+flavoprotein+with+the+properties+of+a+photoreceptor+for+phototropism.&amp;rft.volume=282&amp;rft.issue=5394&amp;rft.pages=1698-701&amp;rft.date=1998-11-27&amp;rft_id=info%3Apmid%2F9831559&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.282.5394.1698&amp;rft_id=info%3Abibcode%2F1998Sci...282.1698C&amp;rft.aulast=Christie&amp;rft.aufirst=JM&amp;rft.au=Reymond%2C+P&amp;rft.au=Powell%2C+GK&amp;rft.au=Bernasconi%2C+P&amp;rft.au=Raibekas%2C+AA&amp;rft.au=Liscum%2C+E&amp;rft.au=Briggs%2C+WR&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-F.S.P._Ng_1997_34–37-21"><span>^ <a href="#cite_ref-F.S.P._Ng_1997_34–37_21-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-F.S.P._Ng_1997_34–37_21-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-F.S.P._Ng_1997_34–37_21-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFF.S.P._Ng1997">F.S.P. Ng (1997). "Shyness in trees". <i>Nature Malaysiana</i>. <b>2</b>: 34–37.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Malaysiana&amp;rft.atitle=Shyness+in+trees&amp;rft.volume=2&amp;rft.pages=34-37&amp;rft.date=1997&amp;rft.au=F.S.P.+Ng&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span><cite id="CITEREFCrepyCasal2015">Crepy, María A.; Casal, Jorge J. (January 2015). "Photoreceptor-mediated kin recognition in plants". <i>New Phytologist</i>. <b>205</b> (1): 329–338. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1111%2Fnph.13040">10.1111/nph.13040</a>. <a href="https://en.wikipedia.org/wiki/Hdl_(identifier)" title="Hdl (identifier)">hdl</a>:<span title="Freely accessible"><a rel="nofollow" href="https://hdl.handle.net/11336%2F37860">11336/37860</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/25264216">25264216</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:28093742">28093742</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+Phytologist&amp;rft.atitle=Photoreceptor-mediated+kin+recognition+in+plants&amp;rft.volume=205&amp;rft.issue=1&amp;rft.pages=329-338&amp;rft.date=2015-01&amp;rft_id=info%3Ahdl%2F11336%2F37860&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A28093742%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F25264216&amp;rft_id=info%3Adoi%2F10.1111%2Fnph.13040&amp;rft.aulast=Crepy&amp;rft.aufirst=Mar%C3%ADa+A.&amp;rft.au=Casal%2C+Jorge+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span><cite id="CITEREFMargaret_LowmanSoubadra_DevyT._Ganesh2013">Margaret Lowman; Soubadra Devy; T. Ganesh (22 June 2013). <a rel="nofollow" href="https://books.google.com/books?id=hVNGAAAAQBAJ&amp;pg=PA34"><i>Treetops at Risk: Challenges of Global Canopy Ecology and Conservation</i></a>. Springer Science &amp; Business Media. p.&nbsp;34. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-4614-7161-5" title="Special:BookSources/978-1-4614-7161-5"><bdi>978-1-4614-7161-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Treetops+at+Risk%3A+Challenges+of+Global+Canopy+Ecology+and+Conservation&amp;rft.pages=34&amp;rft.pub=Springer+Science+%26+Business+Media&amp;rft.date=2013-06-22&amp;rft.isbn=978-1-4614-7161-5&amp;rft.au=Margaret+Lowman&amp;rft.au=Soubadra+Devy&amp;rft.au=T.+Ganesh&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DhVNGAAAAQBAJ%26pg%3DPA34&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
<li id="cite_note-RGF_2004-24"><span><b><a href="#cite_ref-RGF_2004_24-0">^</a></b></span> <span><cite id="CITEREFR._G._Florence2004">R. G. Florence (January 2004). <a rel="nofollow" href="https://books.google.com/books?id=aO1Jgc7d17UC&amp;pg=PA182"><i>Ecology and Silviculture of Eucalypt Forests</i></a>. Csiro Publishing. pp.&nbsp;182–. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-643-09064-4" title="Special:BookSources/978-0-643-09064-4"><bdi>978-0-643-09064-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Ecology+and+Silviculture+of+Eucalypt+Forests&amp;rft.pages=182-&amp;rft.pub=Csiro+Publishing&amp;rft.date=2004-01&amp;rft.isbn=978-0-643-09064-4&amp;rft.au=R.+G.+Florence&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DaO1Jgc7d17UC%26pg%3DPA182&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACrown+shyness"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Crown_shyness&amp;action=edit&amp;section=4" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>
<ul><li><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Commons-logo.svg"><img alt="" src="https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/12px-Commons-logo.svg.png" decoding="async" width="12" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/24px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376"></a></span> Media related to <a href="https://commons.wikimedia.org/wiki/Category:Crown_shyness" title="commons:Category:Crown shyness">Crown shyness</a> at Wikimedia Commons</li></ul>
<!-- 
NewPP limit report
Parsed by mw1352
Cached time: 20231220021903
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.337
Real time usage: 0.454
Preprocessor visited node count: 1818
Post‐expand include size: 54802
Template argument size: 2431
Highest expansion depth: 17
Expensive parser function count: 3
Unstrip recursion depth: 1
Unstrip post‐expand size: 76079
Lua time usage: 0.213
Lua memory usage: 7459340
Number of Wikibase entities loaded: 1
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  405.783      1 -total
 57.55%  233.540      1 Template:Reflist
 39.63%  160.821     16 Template:Cite_journal
 18.37%   74.546      1 Template:Short_description
 10.16%   41.213      1 Template:Cn
  9.25%   37.524      2 Template:Fix
  8.24%   33.424      2 Template:Pagetype
  7.29%   29.586      6 Template:Main_other
  6.82%   27.686      4 Template:Category_handler
  6.68%   27.101      1 Template:SDcat
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:47598638-0!canonical and timestamp 20231220021903 and revision id 1170971299. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NY Governor vetoes ban on noncompete clauses, waters down LLC transparency bill (292 pts)]]></title>
            <link>https://gothamist.com/news/ny-gov-hochul-vetoes-ban-on-noncompete-clauses-waters-down-llc-transparency-bill</link>
            <guid>38749155</guid>
            <pubDate>Sat, 23 Dec 2023 22:59:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gothamist.com/news/ny-gov-hochul-vetoes-ban-on-noncompete-clauses-waters-down-llc-transparency-bill">https://gothamist.com/news/ny-gov-hochul-vetoes-ban-on-noncompete-clauses-waters-down-llc-transparency-bill</a>, See on <a href="https://news.ycombinator.com/item?id=38749155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>New York will create a database that will identify the names of owners of many limited liability companies within the state for the first time. But the public won’t be able to access it, thanks to a legislative amendment secured by Gov. Kathy Hochul’s office.</p><p>The Democratic governor <a href="https://gothamist.com/news/wrongful-deaths-frankensteining-and-more-heres-whats-on-ny-gov-hochuls-desk" rel="noopener" target="_blank">signed 42 bills late Friday and vetoed another 43</a>, part of an end-of-year push to clear her desk of legislation approved by state lawmakers earlier this year.</p><p>Among the bills she signed was <a href="https://gothamist.com/news/llcs-might-soon-have-to-list-their-owners-should-new-yorkers-get-a-look" rel="noopener" target="_blank">the LLC Transparency Act</a>, a measure that will force limited liability companies to list their “beneficial owners” when they create a new company or change the structure of a current one. The bill’s sponsors pushed the measure as a way for New York residents to, for example, find out who their true landlord is if their building is owned by an otherwise nameless LLC.</p><p>But Hochul negotiated an amendment to the bill that will keep that database from the public. Instead, she said, that information will only be maintained for law-enforcement purposes, such as if a district attorney needs to access it for an investigation.</p><p>“The bill as drafted was overly broad, and required changes to ensure it serves the core purpose of exposing unlawful activity while balancing personal privacy,” Hochul wrote in a memo.</p><p>Among the other bills Hochul acted on late Friday:</p><ul><li>Hochul vetoed a <a href="https://gothamist.com/news/future-of-noncompete-agreements-in-ny-in-gov-hochuls-hands" rel="noopener" target="_blank">bill that would have effectively banned noncompete agreements</a>, which employers use to prevent their employees from going to work for a competitor for a period of time after their employment.</li><li>She also vetoed a bill that would have required New York City to install recycling bins at every park, playground and historical site in the five boroughs, arguing it would have placed an unfunded mandate on the city.</li><li>She approved a bill that will move many county- and town-level elections to even-numbered years, much to the chagrin of Republicans.</li></ul><p>Major business interests, including Wall Street firms and the state Business Council, lobbied Hochul to reject the ban on noncompete clauses, arguing they are necessary to protect trade secrets and retain talent. They argued companies would move jobs out of state if New York were to enact the ban.</p><p>Hochul had previously floated a compromise that would allow the ban only to apply to those with salaries under $250,000. But in a statement, bill sponsor Sen. Sean Ryan of Buffalo said Hochul rejected the Legislature’s final offer, which he says would have ceded to the $250,000 cap but indexed it to inflation and exempted all medical workers.</p><p>In her veto message, Hochul said she was trying to find a compromise that protected middle-class and lower-wage workers while allowing businesses “to retain highly compensated talent.” She said she’s “open to future legislation that achieves that right balance.”</p><p>The bill Hochul signed <a href="https://gothamist.com/news/lawmakers-moving-some-ny-elections-to-even-years-could-nyc-be-next" rel="noopener" target="_blank">moving many town- and county-level elections to even years</a> will be phased in over the coming years, eventually aligning them with federal elections in a move supporters say is designed to boost voter turnout.</p><p>Republicans deeply opposed the measure, accusing Democrats of orchestrating the change to benefit from New York’s traditionally heavy Democratic turnout in presidential election years.</p><p>Stephen Acquario, executive director of the state Association of Counties, an organization that lobbies for county governments, criticized Hochul for signing the bill into law.</p><p>“At a time when we should be keeping the divisiveness at the federal and state levels out of our local communities, this bill does the opposite, burying the local issues that impact New Yorkers’ daily lives at the back of exceedingly long ballots,” he said in a statement.</p><p>In a statement Friday afternoon, Hochul said the measure is “about expanding to the ballot box and promoting a more inclusive democracy.”</p><p>While the measure would not apply to New York City (or other city) elections or positions like district attorney —&nbsp;which <a href="https://gothamist.com/news/lawmakers-moving-some-ny-elections-to-even-years-could-nyc-be-next" rel="noopener" target="_blank">are set by the state constitution</a> — Hochul said she would support a constitutional amendment to adjust the election calendar to “to save taxpayer dollars and avoid voter fatigue.” But such a move would be several years away; changing the constitution is a multi-year process, and then any change would have to be gradually phased in after that.</p><p>The governor’s action on the batch of bills leaves just five pieces of legislation to approve or reject by the end of the calendar year.</p><p>That includes a measure known as the Grieving Families Act, which, if signed, would make it easier for family members to seek civil damages for their grief and anguish in wrongful death cases.</p><p>Another pending bill would make changes to the state’s nascent public-campaign-finance system, which good-government advocates oppose.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How does a B-tree make queries fast? (172 pts)]]></title>
            <link>https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html</link>
            <guid>38748433</guid>
            <pubDate>Sat, 23 Dec 2023 21:33:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html">https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html</a>, See on <a href="https://news.ycombinator.com/item?id=38748433">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemprop="articleBody">
        <div> <p><strong>B-tree</strong> is a structure that helps to search through great amounts of data.
It was invented over 40 years ago, yet it is still employed by the majority of modern databases.
Although there are newer index structures, like LSM trees,
<strong>B-tree</strong> is unbeaten when handling most of the database queries.</p>

<p>After reading this post, you will know how <strong>B-tree</strong> organises the data and how it performs search queries.</p>

<h2 id="origins">Origins</h2>

<p>In order to understand <strong>B-tree</strong> let’s focus on <strong>Binary Search Tree (BST)</strong> first.</p>

<p>Wait, isn’t it the same?</p>

<p>What does “B” stand for then?</p>

<p>According to <a href="https://en.wikipedia.org/wiki/B-tree">wikipedia.org</a>, Edward M.&nbsp;McCreight, the inventor of B-tree, once said:</p>

<blockquote>
  <p>“the more you think about what the B in B-trees means, the better you understand B-trees.”</p>
</blockquote>

<p>Confusing <strong>B-tree</strong> with <strong>BST</strong> is a really common misconception.
Anyway, in my opinion, BST is a great starting point for reinventing B-tree.
Let’s start with a simple example of BST:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-basic.webp" alt="Binary Search Tree with three nodes"></p>

<p>The greater number is always on the right, the lower on the left. It may become clearer when we add more numbers.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger.webp" alt="Binary Search Tree with seven nodes"></p>

<p>This tree contains seven numbers, but we need to visit at most three nodes to locate any number.
The following example visualizes searching for 14.
I used SQL to define the query in order to think about this tree as if it were an actual database index.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger-searching.webp" alt="Searching for single node within Binary Search Tree with seven nodes"></p>

<h2 id="hardware">Hardware</h2>

<p>In theory, using Binary Search Tree for running our queries looks fine. Its time complexity (when searching) is \(O(log
n)\), <a href="https://en.wikipedia.org/wiki/B-tree">same as B-tree</a>. However, in practice, this data structure needs to work on actual hardware. An index must be
stored somewhere on your machine.</p>

<p>The computer has three places where the data can be stored:</p>

<ul>
  <li>CPU caches</li>
  <li>RAM (memory)</li>
  <li>Disk (storage)</li>
</ul>

<p>The cache is managed fully by CPUs. Moreover, it is relatively small, usually a few megabytes.
Index may contain gigabytes of data, so it won’t fit there.</p>

<p>Databases vastly use Memory (RAM). It has some great advantages:</p>

<ul>
  <li>assures fast random access (more on that in the next paragraph)</li>
  <li>its size may be pretty big (e.g. AWS RDS cloud service <a href="https://aws.amazon.com/rds/instance-types/">provides instances</a>
with a few terabytes of memory available).</li>
</ul>

<p>Cons? You lose the data when the power supply goes off. Moreover, when compared to the disk, it is pretty expensive.</p>

<p>Finally, the cons of a memory are the pros of a disk storage.
It’s cheap, and data will remain there even if we lose the power.
However, there are no free lunches!
The catch is that we need to be careful about random and sequential access.
Reading from the disk is fast, but only under certain conditions!
I’ll try to explain them simply.</p>

<h3 id="random-and-sequential-access">Random and sequential access</h3>

<p>Memory may be visualized as a line of containers for values, where every container is numbered.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory.webp" alt="Simple memory visualization"></p>

<p>Now let’s assume we want to read data from containers 1, 4, and 6. It requires random access:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory-random-access.webp" alt="Random access visualized on a small chunk of a memory"></p>

<p>And then let’s compare it with reading containers 3, 4, and 5. It may be done sequentially:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory-sequential-access.webp" alt="Sequential access visualized on a small chunk of a memory"></p>

<p>The difference between a “random jump” and a “sequential read” can be explained based on Hard Disk Drive.
It consists of the head and the disk.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/hdd-disk.webp" alt="Hard Disk Drive with cover removed, Public Domain image from https://en.wikipedia.org/wiki/Hard_disk_drive#/media/File:Laptop-hard-drive-exposed.jpg"></p>

<p>“Random jump” requires moving the head to the given place on the disk.
“Sequential read” is simply spinning the disk, allowing the head to read consecutive values.
When reading megabytes of data, the difference between these two types of access is enormous.
Using “sequential reads” lowers the time needed to fetch the data significantly.</p>

<p>Differences in speed between random and sequential access were researched in the article “The Pathologies of Big Data”
by Adam Jacobs, <a href="https://queue.acm.org/detail.cfm?id=1563874">published in Acm Queue</a>.
It revealed a few mind-blowing facts:</p>

<ul>
  <li>Sequential access on HDD may be hundreds of thousands of times faster than random access. 🤯</li>
  <li>It may be faster to read sequentially from the disk than randomly from the memory.</li>
</ul>

<p>Who even uses HDD nowadays?
What about SSD?
This research shows that reading fully sequentially from HDD may be faster than SSD.
However, please note that the article is from 2009 and SSD developed significantly through the last decade,
thus these results are probably outdated.</p>

<p>To sum up, the key takeaway is <strong>to prefer sequential access wherever we can</strong>.
In the next paragraph, I will explain how to apply it to our index structure.</p>

<h2 id="optimizing-a-tree-for-sequential-access">Optimizing a tree for sequential access</h2>

<p>Binary Search Tree may be represented in memory in the same way
as <a href="https://en.wikipedia.org/wiki/Binary_heap">the heap</a>:</p>

<ul>
  <li>parent node position is \(i\)</li>
  <li>left node position is \(2i\)</li>
  <li>right node position is \(2i+1\)</li>
</ul>

<p>That’s how these positions are calculated based on the example (the parent node starts at 1):</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-1.webp" alt="Binary tree representation in the memory—part 1/2"></p>

<p>According to the calculated positions, nodes are aligned into the memory:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-2.webp" alt="Binary tree representation in the memory—part 2/2"></p>

<p>Do you remember the query visualized a few chapters ago?</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger-searching.webp" alt="Searching for single node within Binary Search Tree with seven nodes"></p>

<p>That’s what it looks like on the memory level:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-query.webp" alt="Binary tree representation in the memory - querying"></p>

<p>When performing the query, memory addresses 1, 3, and 6 need to be visited.
Visiting three nodes is not a problem; however, as we store more data, the tree gets higher.
Storing more than one million values requires a tree of height at least 20. It means
that 20 values from different places in memory must be read.
It causes completely random access!</p>

<h3 id="pages">Pages</h3>

<p>While a tree grows in height, random access is causing more and more delay.
The solution to reduce this problem is simple: grow the tree in width rather than in height.
It may be achieved by packing more than one value into a single node.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-in-node.webp" alt="A tree with three values in single node"></p>

<p>It brings us the following benefits:</p>

<ul>
  <li>the tree is shallower (two levels instead of three)</li>
  <li>it still has a lot of space for new values without the need for growing further</li>
</ul>

<p>The query performed on such index looks as follows:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-query.webp" alt="A query performed on a tree with three values in a single node"></p>

<p>Please note that every time we visit a node, we need to load all its values.
In this example, we need to load 4 values (or 6 if the tree is full) in order to reach the one we are looking for.
Below, you will find a visualization of this tree in a memory:</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-memory.webp" alt="A tree with three values in a single node represented in a memory"></p>

<p>Compared to <a href="#optimizing-a-tree-for-sequential-access">the previous example</a> (where the tree grows in height),
this search should be faster.
We need random access only twice (jump to cells 0 and 9) and then sequentially read the rest of values.</p>

<p>This solution works better and better as our database grows. If you want to store one million values, then you need:</p>

<ul>
  <li>Binary Search Tree which has <strong>20</strong> levels</li>
</ul>

<p>OR</p>

<ul>
  <li>3-value node Tree which has <strong>10</strong> levels</li>
</ul>

<p>Values from a single node make a page.
In the example above, each page consists of three values.
A page is a set of values placed on a disk next to each other,
so the database may reach the whole page at once with one sequential read.</p>

<p>And how does it refer to the reality?
<a href="https://www.postgresql.org/docs/current/storage-toast.html#:~:text=PostgreSQL%20uses%20a%20fixed%20page,tuples%20to%20span%20multiple%20pages.">Postgres page size is 8kB</a>.
Let’s assume that 20% is for metadata, so it’s 6kB left.
Half of the page is needed to store
pointers to node’s children, so it gives us 3kB for values.
BIGINT size is 8 bytes, thus we may store ~375 values in a
single page.</p>

<p>Assuming that some pretty big tables in a database have one billion rows,
how many levels in the Postgres tree do we need to store them?
According to the calculations above,
if we create a tree that can handle 375 values in a single node,
it may store <strong>1 billion</strong> values with a tree that has only <strong>four</strong> levels.
Binary Search Tree would require 30 levels for such amount of data.</p>

<p>To sum up, placing multiple values in a single node of the tree helped us to reduce its height, thus using the benefits of sequential access.
Moreover, a B-tree may grow not only in height, but also in width (by using larger pages).</p>

<h2 id="balancing">Balancing</h2>

<p>There are two types of operations in databases: writing and reading.
In the previous section, we addressed the problems with reading the data from the B-tree.
Nonetheless, writing is also a crucial part.
When writing the data to a database, B-tree needs to be constantly updated with new values.</p>

<p>The tree shape depends on the order of values added to the tree.
It’s easily visible in a binary tree.
We may obtain trees with different depths if the values are added in an incorrect order.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-imbalance.webp" alt="Two Binary Trees with shapes depending on the order of inserted values."></p>

<p>When the tree has different depths on different nodes, it is called an unbalanced tree.
There are basically two ways of returning such a tree to a balanced state:</p>

<ol>
  <li>Rebuilding it from the very beginning just by adding the values in the correct order.</li>
  <li>Keeping it balanced all the time, as the new values are added.</li>
</ol>

<p>B-tree implements the second option. A feature that makes the tree balanced all the time is called self-balancing.</p>

<h3 id="self-balancing-algorithm-by-example">Self-balancing algorithm by example</h3>

<p>Building a B-tree can be started simply by creating a single node
and adding new values until there is no free space in it.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-1.webp" alt="Self-balancing, step 1, Add new values until there is a free space in existing nodes."></p>

<p>If there is no space on the corresponding page, it needs to be split.
To perform a split, a „split point” is chosen.
In that case, it will be 12, because it is in the middle.
The „Split point” is a value that will be moved to the upper page.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2a.webp" alt="Self-balancing, step 2a, Splitting the page."></p>

<p>Now, it gets us to an interesting point where there is no upper page.
In such a case, a new one needs to be generated (and it becomes the new root page!).</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2b.webp" alt="Self-balancing, step 2b, Generating a new root page."></p>

<p>And finally, there is some free space in the three, so value 14 may be added.</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2c.webp" alt="Self-balancing, step 2c, Adding the 14 to B-tree."></p>

<p>Following this algorithm, we may constantly add new values to the B-tree, and it will remain balanced all the time!</p>

<p><img src="https://blog.allegro.tech/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-final.webp" alt="Self-balancing, Final state of the B-tree, after adding multiple values."></p>

<p><em>At this point, you may have a valid concern that there is a lot of free space that has no chance to be
filled.
For example, values 14, 15, and 16, are on different pages, so these pages will remain with only one value and two free spaces forever.</em></p>

<p><em>It was caused by the split location choice.
We always split the page in the middle.
But every time we do a split, we may choose any split location we want.</em></p>

<p><em>Postgres has an algorithm that is run every time a split is performed!
Its implementation may be found in the <a href="https://github.com/postgres/postgres/blob/54ccfd65868c013a8c6906bc894bc5ea3640740a/src/backend/access/nbtree/nbtsplitloc.c#L87">_bt_findsplitloc() function in Postgres source code</a>.
Its goal is to leave as little free space as possible.</em></p>

<h2 id="summary">Summary</h2>

<p>In this article, you learned how a B-tree works.
All in all, it may be simply described as a Binary Search Tree with two changes:</p>

<ul>
  <li>every node may contain more than one value</li>
  <li>inserting a new value is followed by a self-balancing algorithm.</li>
</ul>

<p>Although the structures used by modern databases are usually some variants of a B-tree (like B+tree), they are still based on the original conception.
In my opinion, one great strength of a B-tree is the fact that it was designed directly to handle large amounts of data on actual hardware.
It may be the reason why the B-tree has remained with us for such a long time.</p>
</div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art of Electronics (3rd Edition) (300 pts)]]></title>
            <link>https://artofelectronics.net/</link>
            <guid>38748370</guid>
            <pubDate>Sat, 23 Dec 2023 21:23:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://artofelectronics.net/">https://artofelectronics.net/</a>, See on <a href="https://news.ycombinator.com/item?id=38748370">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
								
<h2><a href="https://artofelectronics.net/">About The Book</a></h2>

<p><img src="https://artofelectronics.net/wp-content/uploads/2014/04/book_3rd-137x140.png" alt="About The Book" width="137" height="140"></p>
<blockquote><p>“Wow. Chapter 5 details every circuit artifact that I’ve encountered&nbsp;in the past 30 years in a thorough, pragmatic, and straightforward way.&nbsp;My only ‘twinge’ is that it discloses and explains (in glorious&nbsp;graphical detail and with real part numbers) many topics that I&nbsp;thought were my personal trade secrets. I love the plots. I know that it must take an enormous effort to collate all of the device&nbsp;characteristics. It’s worth the effort. The way the data is&nbsp;presented allows the reader to get terrific perspective on a lot&nbsp;of landscape in a single view. Nice work.” — John Willison, founder, Stanford Research Systems</p></blockquote>
<p><img fetchpriority="high" decoding="async" src="https://artofelectronics.net/wp-content/uploads/2019/11/authors_0630_350w.jpg" alt="" width="350" height="293"></p>

<div>
					<p><span><strong>Counterfeit Warning:</strong> December, 2015 — buyers have reported poor quality copies (confirmed as counterfeit) being sold online at prices too low to be creditable. These are recognizable from their poor bindings and text errors (e.g., missing the ligature “fi”, thus on the author page “Wineld Hill”!). More information <a href="http://artofelectronics.net/the-book/counterfeit-editions/">here</a>. EEVBlog’s Dave Jones gets one in his Mailbag <a href="http://www.eevblog.com/2016/02/03/eevblog-847-mailbag/">here</a>. Note also that the only authorized e-book version is Kindle.</span></p></div><br>
<!--


<table cellspacing="10px" cellpadding="10px" align="center">


<tbody>


<tr>


<td><a href="http://www.cambridge.org/academic/art-electronics-free-sample-chapter" target="_blank" rel="noopener noreferrer">Download a sample chapter from Cambridge Press</a></td>


</tr>


</tbody>


</table>


-->



<table>
<tbody>
<tr>
<td><strong><span color="#666666">– 1220 large format pages</span></strong></td>
<td><strong><span color="#666666">– 80&nbsp;tables listing some 1650 components</span></strong></td>
<td><strong><span color="#666666">– 1470&nbsp;figures and 90 oscilloscope&nbsp;screenshots</span></strong></td>
</tr>
<tr>
<td><strong><span color="#666666">– Extensive practical advice</span></strong></td>
<td><strong><span color="#666666">– Back-of-the-envelope techniques</span></strong></td>
<td><strong><span color="#666666">– Exhaustive&nbsp;index</span></strong></td>
</tr>
</tbody>
</table>

<h4>Where to buy</h4>
<p><strong>Cambridge University Press</strong><span>&nbsp;–&nbsp;<a href="http://www.cambridge.org/us/academic/subjects/physics/electronics-physicists/art-electronics-3rd-edition?format=HB" target="_blank" rel="noopener noreferrer">The Art of Electronics 3rd Edition</a>&nbsp;/&nbsp;<a href="http://www.cambridge.org/us/academic/subjects/physics/electronics-physicists/learning-art-electronics-hands-lab-course">Learning the Art of Electronics 3rd Edition</a><a href="http://www.cambridge.org/us/academic/subjects/physics/electronics-physicists/art-electronics-student-manual?format=PB" target="_blank" rel="noopener noreferrer"><br>
</a></span><strong>Amazon.com</strong><span>&nbsp;–</span><span>&nbsp;</span><span><a href="https://www.amazon.com/The-Art-Electronics-Paul-Horowitz/dp/0521809266?&amp;linkCode=wey&amp;tag=maggicom0e-20" target="_blank" rel="noopener noreferrer">The Art of Electronics 3rd Edition</a>&nbsp;/&nbsp;<a href="https://www.amazon.com/Learning-Art-Electronics-Hands-On-Course/dp/0521177235?linkCode=wey&amp;tag=maggicom0e-20">Learning the Art of Electronics 3rd Edition</a>&nbsp;<a href="https://www.amazon.com/The-Art-Electronics-Student-Manual/dp/0521377099?&amp;linkCode=wey&amp;tag=maggicom0e-20" target="_blank" rel="noopener noreferrer"><br>
</a></span><strong>Adafruit Industries</strong> –<span><i>&nbsp;</i></span><a href="http://www.adafruit.com/products/2356" target="_blank" rel="noopener noreferrer">The Art of Electronics 3rd Edition</a>&nbsp;–&nbsp;<a href="http://www.adafruit.com/products/310" target="_blank" rel="noopener noreferrer">Student Manual to 2nd Edition<br>
</a><strong>Barnes and Noble</strong>&nbsp;–<em>&nbsp;</em><a href="http://www.barnesandnoble.com/w/the-art-of-electronics-paul-horowitz/1116996095?ean=9780521809269" target="_blank" rel="noopener noreferrer">The Art of electronics 3rd Edition</a> /&nbsp;<a href="http://www.barnesandnoble.com/w/learning-the-art-of-electronics-tom-hayes/1122384679" target="_blank" rel="noopener noreferrer">Learning the Art of Electronics 3rd Edition<br>
</a><strong>Amazon.co.uk (UK)</strong> – <a href="http://www.amazon.co.uk/The-Art-Electronics-Paul-Horowitz/dp/0521809266?linkCode=wey&amp;tag=maggicom0e-20" target="_blank" rel="noopener noreferrer">The Art of Electronics 3rd Edition</a>&nbsp;/&nbsp;<a href="http://www.amazon.co.uk/Learning-Art-Electronics-Hands-On-Course/dp/0521177235?linkCode=wey&amp;tag=maggicom0e-20" target="_blank" rel="noopener noreferrer">Learning the Art of Electronics 3rd Edition</a><a href="http://www.barnesandnoble.com/w/art-of-electronics-student-manual-thomas-c-hayes/1100948145?ean=9780521377096" target="_blank" rel="noopener noreferrer"><br>
</a><strong>Foyles (UK)</strong>&nbsp;– <a href="http://www.foyles.co.uk/witem/the-art-of-electronics,paul-horowitz-winfield-hill-9780521809269">The Art of Electronics 3rd Edition</a><br>
<strong>The Book Depository (Worldwide)</strong> – <a href="http://www.bookdepository.com/Art-Electronics-Paul-Horowitz/9780521809269">The Art of Electronics 3rd Edition</a></p>

					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2TB microSD card is on the way early next year (116 pts)]]></title>
            <link>https://overkill.wtf/2tb-microsd-card-on-the-way/</link>
            <guid>38748087</guid>
            <pubDate>Sat, 23 Dec 2023 20:49:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://overkill.wtf/2tb-microsd-card-on-the-way/">https://overkill.wtf/2tb-microsd-card-on-the-way/</a>, See on <a href="https://news.ycombinator.com/item?id=38748087">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<div>
<p>
Japanese storage maker KIOXIA has started production on its highest capacity microSDXC card ever.
</p>
<p>If you're currently <a href="https://overkill.wtf/best-games-under-10-dollars-in-the-steam-winter-sale-2023/" rel="noreferrer">adding countless games to your backlog</a> as a result of the <em>thousands</em> of titles discounted as part of <a href="https://overkill.wtf/steam-winter-sale-2023/" rel="noreferrer">the massive Steam Winter Sale</a>, then you may also find yourself in need of a bigger microSD card for your Steam Deck soon. <em>Thankfully, KIOXIA has a <strong>really</strong> sizeable solution on the way.</em></p><p><strong>The company </strong><a href="https://europe.kioxia.com/en-europe/personal/news/2023/20231220-1.html?ref=overkill.wtf" rel="noreferrer"><strong>just announced</strong></a><strong> that it has created its highest capacity microSDXC memory card ever — <em>coming in at a whopping 2TB</em>.</strong></p><p>This frankly remarkable feat was made possible by "<em>stacking sixteen 1 terabit dies of 3D flash memory</em>" all whilst maintaining the standard 0.8mm thickness.</p><div><p>ℹ️</p><p>16 Terabits = 2 Terabytes</p></div><p>The upcoming 2TB card, dubbed the '<em>EXCERIA PLUS G2</em>' is said to have read speeds of up to 100 MB a second and write speeds of up to 90 MB a second. This 2TB size now hits the <a href="https://www.sdcard.org/developers/sd-standard-overview/capacity-sd-sdhc-sdxc-sduc/?ref=overkill.wtf" rel="noreferrer">upper storage capacity of the defined SDXC standard</a>.</p><p>Japanese storage maker KIOXIA, formerly known under the trusted Toshiba name, has some fine heritage — and can essentially lay claim to having actually <em>invented flash memory</em> thanks to the efforts of <a href="https://en.wikipedia.org/wiki/Fujio_Masuoka?ref=overkill.wtf" rel="noreferrer">engineer Fujio Masuoka</a>. </p><p>All of that is to say that, although you may not be familiar with the modern KIOXIA name (<em>I sure wasn't</em>) —&nbsp;this is a really trusted name in the memory business. </p><p>Jamie Stitt, a marketing manager at KIOXIA Europe said these new "<em>enhanced cards</em>" will likely "<em>become a sought-after option by many</em>", including "<em>on-the-go gamers</em>". Yes sir. 🫡</p><figure><img src="https://overkill.wtf/content/images/2023/12/KIOXIA-EXCERIA-PLUS-G2-microSD-2TB-news.jpg" alt="" loading="lazy" width="660" height="330" srcset="https://overkill.wtf/content/images/size/w600/2023/12/KIOXIA-EXCERIA-PLUS-G2-microSD-2TB-news.jpg 600w, https://overkill.wtf/content/images/2023/12/KIOXIA-EXCERIA-PLUS-G2-microSD-2TB-news.jpg 660w"></figure><p>This card from KIOXIA (<em>and likely the others that will follow from competitors</em>) should all work nice in the likes of the Steam Deck, Lenovo Legion Go, ROG Ally and Nintendo Switch — all of which support the microSDXC standard. </p><p>We don't currently know pricing for this huge new KIOXIA card, but we imagine it will start at a rather meaty price point. </p><p>The EXCERIA PLUS G2 2TB microSD cards are now in mass production, and are <strong>expected to start shipping in the early part of next year</strong>. <em>We'll keep you posted when they do! </em></p><figure><a href="https://overkill.wtf/best-accessories-for-steam-deck-and-asus-rog-ally/"><div><p>The best accessories for Steam Deck and ASUS ROG Ally</p><p>Enhance your handheld gaming experience with the right accessories for your Steam Deck, ROG Ally, or AYANEO 2s. From microSD cards to controllers, we’ve got you covered in this guide.</p><p><img src="https://overkill.wtf/content/images/size/w256h256/2022/08/overkill_icon-1.png" alt=""><span>overkill.wtf</span><span>Kevin Wammer</span></p></div><p><img src="https://overkill.wtf/content/images/2023/06/best-accessories.webp" alt=""></p></a></figure><p>Of course, if you don't want to wait for this big 2TB card to drop, we can vouch for <a href="https://amzn.to/469go7H?ref=overkill.wtf" rel="noreferrer">SanDisk's 1TB option</a>.</p><hr><blockquote><strong>Via:</strong> <a href="https://www.neowin.net/news/kioxia-mass-produces-of-worlds-highest-capacity-2tb-microsdxc-memory-card/?ref=overkill.wtf" rel="noreferrer">Neowin</a></blockquote>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cummins pickup truck engines tricked air quality controls, feds say (161 pts)]]></title>
            <link>https://www.usatoday.com/story/news/nation/2023/12/22/cummins-truck-engines-defeat-devices-feds-say/72013430007/</link>
            <guid>38747747</guid>
            <pubDate>Sat, 23 Dec 2023 20:15:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.usatoday.com/story/news/nation/2023/12/22/cummins-truck-engines-defeat-devices-feds-say/72013430007/">https://www.usatoday.com/story/news/nation/2023/12/22/cummins-truck-engines-defeat-devices-feds-say/72013430007/</a>, See on <a href="https://news.ycombinator.com/item?id=38747747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><article><p>The United States Department of Justice is slamming an Indiana-based engine manufacturing company with a $1.675 billion penalty in a settlement that says the company violated the federal Clean Air Act.</p><partner-banner util-module-path="elements/partner" min-height="600" fluid="" outstream="" momentum=""></partner-banner><p>The department alleges Cummins Inc. installed devices that can bypass emissions sensors on 630,000 RAM pickup truck engines, <a href="https://www.justice.gov/opa/pr/statement-attorney-general-merrick-garland-agreement-principle-cummins-settle-alleged">according to a news release Friday</a>. The whopping financial penalty is the largest ever violation since the law was enacted in 1963 to protect the nation's air quality.</p><p>“The types of devices we allege that Cummins installed in its engines to cheat federal environmental laws have a significant and harmful impact on people’s health and safety," wrote Attorney General Merrick B. Garland. He said Cummins' engines caused excess emissions of nitrogen oxides, which can cause asthma and respiratory infections.</p><p>The company agreed to pay the $1.675 billion fine to the U.S. and the State of California to settle the claims, according to the Department of Justice. The penalty is the second largest environmental penalty in the history of the nation, according to the Department of Justice.</p><partner-banner util-module-path="elements/partner" fluid="" bottom="" lazy="" min-height="390" outstream=""></partner-banner><partner-inline util-module-path="elements/partner" placement="native-article_link" sizes="[[300, 250], [3, 3]]" min-height="250" fluid="" outstream=""></partner-inline><p>The company does not admit wrongdoing and says no one in the company acted in bad faith, said Jon Mills, a spokesperson for Cummins Inc. in an email to USA TODAY.</p><p>"The company has cooperated fully with the relevant regulators, already addressed many of the issues involved, and looks forward to obtaining certainty as it concludes this lengthy matter," reads a news release from the company.</p><partner-inline util-module-path="elements/partner" placement="native-article_link" sizes="[[300, 250], [3, 3]]" min-height="250" fluid="" outstream=""></partner-inline><cta-atoms-container-inline util-module-path="elements/cta"></cta-atoms-container-inline><h2>What is the Department of Justice penalizing Cummins Inc. for?</h2><p>Cummins Inc. allegedly installed defeat devices on the engines of hundreds of thousands of 2013 to 20199 RAM 2500 and 3500 pickup trucks, according to the Department of Justice. The DOJ also says the company installed defeat devices on the engines of 330,000 newer RAM pickup trucks.</p><p>Defeat devices are hardware or software used in vehicles to trick air pollution tests, or bypass emissions controls.</p><partner-banner util-module-path="elements/partner" fluid="" bottom="" lazy="" min-height="600" outstream="" momentum=""></partner-banner><p>The company said it has since recalled those trucks. It has also "initiated a recall of model years 2013 through 2018 RAM 2500 and 3500 trucks and previously accrued a total of $59 million for the estimated costs for executing these and other related recalls," according to a Friday news release from the company.</p><media-image image-set="bestCrop, https://www.gannett-cdn.com/-mm-/524efb2b0124e95e203f794dca9b6b2d5d98feab/c=0-50-580-485/local/-/media/2017/01/15/USATODAY/usatsports/2014-ram-ecodiesel-v6_large.jpg 4:3, https://www.gannett-cdn.com/-mm-/cbb5bbcac65cf5cac6db713e9a01ebc2fdb64774/c=90-0-490-534/local/-/media/2017/01/15/USATODAY/usatsports/2014-ram-ecodiesel-v6_large.jpg 3:4, https://www.gannett-cdn.com/-mm-/f465c41d7b7f7ca23c261feac8e7627066d48822/c=0-104-580-430/local/-/media/2017/01/15/USATODAY/usatsports/2014-ram-ecodiesel-v6_large.jpg 16:9" image-alt="Did FCA's 3.0 liter turbo V6 &quot;EcoDiesel&quot; engine have a defeat device? The Feds are investigating." credit="Fiat&nbsp;Chrysler Automobiles NV" caption="Did FCA's 3.0 liter turbo V6 &quot;EcoDiesel&quot; engine have a defeat device? The Feds are investigating." orientation="horizontal" util-module-path="elements/media"></media-image><h2>Vehicle pollution health effects</h2><p>According to the <a href="https://www.epa.gov/clean-air-act-overview/clean-air-act-text">U.S. Environmental Protection Agency</a>, high emissions of nitrogen oxides, or vehicle pollutions, can get into the air from vehicle emissions and the burning of fuel.</p><p>Those emissions "can irritate airways in the human respiratory system," according to the agency.</p><p>"Such exposures over short periods can aggravate respiratory diseases, particularly asthma, leading to respiratory symptoms (such as coughing, wheezing or difficulty breathing), hospital admissions and visits to emergency rooms," according to the agency. "Longer exposures to elevated concentrations of NO<sub>2</sub>&nbsp;may contribute to the development of asthma and potentially increase susceptibility to respiratory infections."</p><partner-banner util-module-path="elements/partner" fluid="" bottom="" lazy="" min-height="390" outstream=""></partner-banner><h2>What is the Clean Air Act?</h2><p>The Clean Air Act is a federal law that was designed to "protect and improve the nation's air quality and the stratospheric ozone layer," <a href="https://www.epa.gov/clean-air-act-overview/clean-air-act-text">according to the US Environmental Protection Agency</a>.</p><p>Congress first enacted the law in 1963 and several major and minor changes have been made to it since its inception. It's the Environmental Protection Agency's role to uphold the law.</p><p><span><strong>Communities facing air pollution </strong><a href="https://www.usatoday.com/story/news/health/2023/04/07/epa-put-new-rules-chemical-plants-reduce-air-pollution/11620822002/" target="_blank">Could get relief as EPA proposes new rules on chemical plants</a></span></p><p><em>Contact Kayla Jimenez at kjimenez@usatoday.com. Follow her on X, formerly Twitter, at @kaylajjimenez.</em></p><lit-timestamp slot="timestamp" publishdate="2023-12-23 00:52:00 +0000 UTC" updatedate="2023-12-23 00:54:38 +0000 UTC"></lit-timestamp><p><a alt="Post the article to your Facebook Timeline" data-size="large" onclick="fireNavShareAnalytics('facebook');" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
                <path d="M12.6143832,21 L3.99346182,21 C3.44462725,21 3,20.5550968 3,20.006476 L3,3.99345411 C3,3.44469364 3.44469709,3 3.99346182,3 L20.006608,3 C20.5552331,3 21,3.44469364 21,3.99345411 L21,20.006476 C21,20.5551667 20.5551632,21 20.006608,21 L15.4197395,21 L15.4197395,14.029408 L17.7594454,14.029408 L18.1097832,11.3128446 L15.4197395,11.3128446 L15.4197395,9.57849053 C15.4197395,8.79198274 15.6381418,8.25600363 16.7659836,8.25600363 L18.2044917,8.25537504 L18.2044917,5.82565895 C17.9557072,5.79255313 17.1017938,5.71858885 16.108332,5.71858885 C14.0343128,5.71858885 12.6143832,6.98457234 12.6143832,9.30945332 L12.6143832,11.3128446 L10.2686707,11.3128446 L10.2686707,14.029408 L12.6143832,14.029408 L12.6143832,21 L12.6143832,21 Z"></path>
            </svg><span>Facebook</span></a>
<a alt="Tweet about this article" data-size="large" onclick="fireNavShareAnalytics('twitter')" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
                <path d="M21,6.77573131 C20.338616,7.07692308 19.6265188,7.28060672 18.8795563,7.3716143 C19.6423666,6.9035753 20.2276809,6.16143012 20.5034337,5.27735645 C19.7892235,5.71072589 19,6.02600217 18.1568938,6.19501625 C17.4849445,5.45937161 16.5245642,5 15.461701,5 C13.4236661,5 11.770206,6.69555796 11.770206,8.78656555 C11.770206,9.08342362 11.8019017,9.3716143 11.8652932,9.64897075 C8.79609086,9.4907909 6.07554147,7.98483207 4.25303751,5.69122427 C3.93502377,6.2524377 3.75330164,6.9035753 3.75330164,7.59696641 C3.75330164,8.91007584 4.40517697,10.0693391 5.39619651,10.7486457 C4.79186476,10.7302275 4.22134179,10.5579632 3.72266244,10.276273 L3.72266244,10.3228602 C3.72266244,12.1581798 4.9957739,13.6890574 6.68621236,14.035753 C6.37665082,14.1245937 6.05018489,14.1690141 5.71315372,14.1690141 C5.47543582,14.1690141 5.24300053,14.1462622 5.01796091,14.1018418 C5.4881141,15.6056338 6.85103011,16.7009751 8.46751189,16.7302275 C7.20390914,17.7464789 5.61067089,18.3521127 3.88114105,18.3521127 C3.58320127,18.3521127 3.28843106,18.3347779 3,18.3001083 C4.63444268,19.3726977 6.57633386,20 8.66085578,20 C15.4543053,20 19.1679873,14.2307692 19.1679873,9.22643554 C19.1679873,9.06175515 19.1648177,8.89707476 19.1584786,8.73564464 C19.8800845,8.20151679 20.5066033,7.53521127 21,6.77573131"></path>
            </svg><span>Twitter</span></a>
<a alt="Email this article" onclick="fireNavShareAnalytics('email')" rel="noopener" target="_blank"><svg view-box="0 0 24 24">
            <path d="M3,5.8757627 C3,5.39209232 3.39269552,5 3.8926228,5 L20.1073772,5 C20.6003592,5 21,5.40389442 21,5.8757627 L21,18.1242373 C21,18.6079077 20.6073045,19 20.1073772,19 L3.8926228,19 C3.39964084,19 3,18.5961056 3,18.1242373 L3,5.8757627 Z M12,11.09375 L3,6.74107143 L3,8.48214286 L12,12.8348214 L21,8.48214286 L21,6.74107143 L12,11.09375 Z"></path>
        </svg><span>Email</span></a></p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beeper Mini is now open-source (122 pts)]]></title>
            <link>https://github.com/beeper/imessage</link>
            <guid>38747482</guid>
            <pubDate>Sat, 23 Dec 2023 19:49:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/beeper/imessage">https://github.com/beeper/imessage</a>, See on <a href="https://news.ycombinator.com/item?id=38747482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">beeper-imessage</h2>
<p dir="auto">A Matrix-iMessage puppeting bridge.</p>
<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">The bridge works like any other mautrix-go bridge, so the instructions at
<a href="https://docs.mau.fi/bridges/go/setup.html" rel="nofollow">https://docs.mau.fi/bridges/go/setup.html</a> can be applied directly.
You can find precompiled binaries from the GitLab CI at
<a href="https://mau.dev/mautrix/imessagego" rel="nofollow">https://mau.dev/mautrix/imessagego</a>.</p>
<p dir="auto">Additionally, the bridge requires a registration provider running on a <a href="https://github.com/beeper/mac-registration-provider">Mac</a> or
<a href="https://github.com/beeper/phone-registration-provider">jailbroken iPhone</a>, as well as a <a href="https://github.com/beeper/registration-relay">relay server</a> to help the bridge and
registration provider connect to each other.</p>
<p dir="auto">When connecting the bridge to your Beeper account with bbctl, you don't need to
self-host the relay, you only need to run the provider.</p>
<h2 tabindex="-1" dir="auto">Discussion</h2>
<p dir="auto">Matrix room: <a href="https://matrix.to/#/#imessage:maunium.net" rel="nofollow">#imessage:maunium.net</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BBS: The Documentary (2005) (153 pts)]]></title>
            <link>http://www.bbsdocumentary.com/</link>
            <guid>38746221</guid>
            <pubDate>Sat, 23 Dec 2023 17:45:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.bbsdocumentary.com/">http://www.bbsdocumentary.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38746221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td>
   <span face="Arial" color="#DDDDDD" size="+2"><b>
      "A Truly Fascinating Documentary."</b>
   </span></td><td>
   - <span face="Arial" color="#00DD00"><a href="http://www.filmthreat.com/index.php?section=reviews&amp;Id=7870">Film Threat</a>
</span></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In 2023 Organic Maps got its first million users (222 pts)]]></title>
            <link>https://organicmaps.app/news/2023-12-23/in-2023-organic-maps-got-its-first-million-users/</link>
            <guid>38746187</guid>
            <pubDate>Sat, 23 Dec 2023 17:42:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://organicmaps.app/news/2023-12-23/in-2023-organic-maps-got-its-first-million-users/">https://organicmaps.app/news/2023-12-23/in-2023-organic-maps-got-its-first-million-users/</a>, See on <a href="https://news.ycombinator.com/item?id=38746187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto">
<p>In 2023 Organic Maps got its <strong>first million users</strong>. It would not be possible to scale without the help from our beloved community. A million thanks for your energy and time, your <a rel="noopener" target="_blank" href="https://organicmaps.app/donate/">donations</a> and <a rel="noopener" target="_blank" href="https://organicmaps.app/support-us/">support</a> 🙏 ❤️ 🍃</p>
<p>In 2023:</p>
<ul>
<li>New features were added, including GPX import, Ruler tool, Outdoors map style, Background voice directions, Android Auto (in Google's review now), and others (see our <a rel="noopener" target="_blank" href="https://organicmaps.app/news/">news</a> for details)</li>
<li>15 app updates and hotfixes were released</li>
<li>A lot of improvements for the search and routing, map icons and styles, and general usability</li>
<li>4.6 average rating on Google Play from 7,372 users</li>
<li>4.8 rating on the AppStore from 2717 users</li>
<li>1992 commits from 167 contributors <a rel="noopener" target="_blank" href="https://github.com/organicmaps/organicmaps/">on Github</a></li>
<li>960 merged Pull Requests</li>
<li>724 closed issues (almost <a rel="noopener" target="_blank" href="https://github.com/organicmaps/organicmaps/issues/">2000 open issues</a> left to fix/implement 💪)</li>
<li>Total 7710 stars on GitHub</li>
<li>More than a thousand support emails were answered</li>
<li>Almost <a rel="noopener" target="_blank" href="https://wiki.openstreetmap.org/wiki/Editor%5Fusage%5Fstats">9K edits and map contributions</a> in OpenStreetMap</li>
</ul>
<p>It's possible to do even more in 2024 if we get enough <a rel="noopener" target="_blank" href="https://organicmaps.app/donate/">donations</a> to scale the infrastructure, reward our contributors, or hire a full-time working team.
One of our goals is to provide a privacy-focused map alternative to Google and Apple Maps. Another goal is to educate and engage more users in contributing to <a rel="noopener" target="_blank" href="https://openstreetmap.org/">OpenStreetMap</a>. Good map quality means happier hikers, cyclists, drivers, locals, tourists, and explorers. Happier everyone!</p>
<p>Check our <a rel="noopener" target="_blank" href="https://organicmaps.app/donate/">Donations page</a> to sneak-peak the roadmap, and subscribe to our <a rel="noopener" target="_blank" href="https://t.me/OrganicMapsApp">Telegram Channel</a> or to the <a rel="noopener" target="_blank" href="https://omaps.app/matrix">matrix space</a> for updates, or follow OM on <a rel="noopener" target="_blank" href="https://fosstodon.org/@organicmaps">Fosstodon</a>, <a rel="noopener" target="_blank" href="https://mastodon.social/@organicmaps">Mastodon.social</a>, <a rel="noopener" target="_blank" href="https://facebook.com/OrganicMaps">Facebook</a>, <a rel="noopener" target="_blank" href="https://twitter.com/OrganicMapsApp">X (Twitter)</a>, <a rel="noopener" target="_blank" href="https://instagram.com/organicmaps.app/">Instagram</a>, <a rel="noopener" target="_blank" href="https://www.reddit.com/r/organicmaps/">Reddit</a>, <a rel="noopener" target="_blank" href="https://www.linkedin.com/company/organic-maps/">LinkedIn</a>.</p>
<p>And finally, <a rel="noopener" target="_blank" href="https://omaps.app/get">install</a> the December 2023 release! We have prepared several Christmas 🎅 and New Year presents 🎁 for you:</p>
<ul>
<li>Android Auto is in review, you can test it by joining a beta program in <a rel="noopener" target="_blank" href="https://play.google.com/store/apps/details?id=app.organicmaps">Google Play</a> after <a href="https://organicmaps.app/cdn-cgi/l/email-protection#f092958491b09f8297919e99939d918083de918080">telling us</a> your gmail 🤖 🚗 -- kudos to Andrew Shkrob</li>
<li>Outdoors map style for hiking, cycling, and exploring Nature (make sure you've updated maps to the latest version!) 🥾 🏕️ 🚣 🚵 -- a long-time work by Konstantin Pastbin</li>
<li>Search for village addresses without streets in Austria, Czechia, Germany, Poland, Slovakia, and some other countries (the <a rel="noopener" target="_blank" href="https://wiki.openstreetmap.org/wiki/Key:addr:place">addr:place OSM tag</a>, and other search improvements too) -- by Viktor Govako</li>
<li>Type "skiing" or "ski run" to find downhill and Nordic pistes ⛷️ 🏂 🚡</li>
<li>OpenStreetMap map data as of December 13</li>
<li>Fixed several crashes</li>
<li>Multiple KML files can be imported from a single KMZ file, as the first pre-requisite to backup all bookmarks and tracks -- by cyber-toad</li>
<li>A credit card 💳 is displayed for selected places with ATM</li>
<li>A dollar emoji 💲 is displayed where a fee is required -- both by David Martinez</li>
<li>Prettified website links in Place Page</li>
<li>You can open a browser to check photos, reviews, and prices for some hotels. Every booking and every donation contribute to the development of Organic Maps!</li>
</ul>
<p>Android</p>
<ul>
<li>We need your feedback on how to improve Android Auto! <a href="https://organicmaps.app/cdn-cgi/l/email-protection#46242332270629342127282f252b27363568273636">Tell us</a> your gmail address and join the closed beta program on Google Play</li>
<li>Open addresses from Contacts, Calendar, and other apps -- by Roman Tsisyk</li>
<li>"Keep Search History" setting for better privacy -- by Bukkapatnam Sandilya</li>
<li>Connect your phone to TV with Samsung DeX -- by Meenbeese</li>
<li>Fix missing buttons when OM is restarted during navigation</li>
<li>Minor interface fixes and improvements -- by Jean-Baptiste and Kiryl Kaveryn</li>
<li>Fixed wrong voice language for the TTS Test button -- by Gonzalo Pesquero</li>
</ul>
<p>Most iOS fixes by Kiryl Kaveryn</p>
<ul>
<li>Fixed freezing on iOS 13, 14</li>
<li>"Zoom in to see isolines" message does not cover buttons</li>
<li>CarPlay Search button now properly shows search results -- by Fabian Wüthrich</li>
<li>Improved user interface for the Search on iPad</li>
</ul>
<p>Styles -- a lot of fixes by Konstantin Pastbin</p>
<ul>
<li>Any feedback on the Outdoors style is appreciated</li>
<li>Singapore metro icon -- RedAuburn</li>
<li>Hackerspace -- RedAuburn</li>
<li>Fix Porto metro icon -- Matheus Gomes</li>
<li>Fixed residential leisure garden area fills disappearing on some zoom levels</li>
<li>Display house numbers regardless of buildings' sizes</li>
<li>Fine-tuned priorities of many POIs in the main style</li>
<li>Less "gaps" in highways on the World map</li>
<li>Removed very short ferry lines from the World map</li>
</ul>
<p>Translations</p>
<ul>
<li>Improved Chinese, Dutch, Finnish, Polish, Russian translations</li>
<li>Added Dutch, French, Polish, Romanian, and Russian translations for edition Opening Hours instructions</li>
<li>Fixed missing parking translations</li>
</ul>
<p>Linux</p>
<ul>
<li>Show Wikipedia articles in a separate dialog</li>
<li>Touchscreen pinch-zoom support -- by Ferenc-</li>
</ul>
<p>We wish you Organic Christmas and Organic New Year!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quantum computing's reality check (160 pts)]]></title>
            <link>https://spectrum.ieee.org/quantum-computing-skeptics</link>
            <guid>38745970</guid>
            <pubDate>Sat, 23 Dec 2023 17:23:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/quantum-computing-skeptics">https://spectrum.ieee.org/quantum-computing-skeptics</a>, See on <a href="https://news.ycombinator.com/item?id=38745970">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Quantum Computing’s Hard, Cold Reality Check" data-elid="2666638802" data-post-url="https://spectrum.ieee.org/quantum-computing-skeptics" data-authors="Edd Gent" data-page-title="Quantum Computing’s Hard, Cold Reality Check - IEEE Spectrum"><p>The <a data-linked-post="2657574494" href="https://spectrum.ieee.org/quantum-computing-for-dummies" target="_blank">quantum computer revolution</a> may be further off and more limited than many have been led to believe. That’s the message coming from a small but vocal set of prominent skeptics in and around the emerging <a data-linked-post="2657574494" href="https://spectrum.ieee.org/quantum-computing-for-dummies" target="_blank">quantum computing</a> industry.</p><p> Quantum computers <a href="https://spectrum.ieee.org/tag/quantum-computing" target="_blank">have been touted</a> as a solution to a wide range of problems, including <a href="https://www.nature.com/articles/s42254-023-00603-1" target="_blank">financial modeling</a>,  <a href="https://www.nature.com/articles/s41598-023-31765-8" target="_blank">optimizing logistics</a>, and <a href="https://www.mdpi.com/1099-4300/25/2/287" target="_blank">accelerating machine learning</a>. Some of the more <a href="https://quantumzeitgeist.com/ibm-quantum-computer-timeline/" target="_blank">ambitious timelines proposed by quantum computing companies</a> have suggested these machines could be impacting real-world problems in just a handful of years. But there’s growing pushback against what many see as unrealistic expectations for the technology.</p><h3>Meta’s LeCun—Not so fast, qubit</h3><p> Meta’s head of AI research <a href="https://spectrum.ieee.org/yann-lecun-ai" target="_blank">Yann LeCun</a> recently <a href="https://www.cnbc.com/2023/12/03/meta-ai-chief-yann-lecun-skeptical-about-agi-quantum-computing.html" rel="noopener noreferrer" target="_blank">made headlines</a> after pouring cold water on the prospect of quantum computers making a meaningful contribution in the near future. Speaking at a media event celebrating the 10-year anniversary of Meta’s Fundamental AI Research team he said the technology is “a fascinating scientific topic,” but that he was less convinced of “the possibility of actually fabricating quantum computers that are actually useful.”</p><p> While LeCun is not an expert in <a href="https://spectrum.ieee.org/tag/quantum-computing">quantum computing</a>, leading figures in the field are also sounding a note of caution. <a href="https://www.linkedin.com/in/oskar-painter-5388b5a1/" rel="noopener noreferrer" target="_blank">Oskar Painter</a>, head of quantum hardware for <a href="https://spectrum.ieee.org/tag/amazon">Amazon</a> Web Services, says there is a “tremendous amount of hype” in the industry at the minute and “it can be difficult to filter the optimistic from the completely unrealistic.”</p><p> A fundamental challenge for today’s quantum computers is that they are very prone to errors. Some have suggested that these so-called “noisy intermediate-scale quantum” (<a data-linked-post="2650231655" href="https://spectrum.ieee.org/measuring-progress-in-the-noisy-era-of-quantum-computing" target="_blank">NISQ</a>) processors could still be put to useful work. But Painter says there’s growing recognition that this is unlikely and <a data-linked-post="2657518724" href="https://spectrum.ieee.org/quantum-error-correction" target="_blank">quantum error-correction</a> schemes will be key to achieving practical quantum computers.  </p><p>“We found out over the last 10 years that many things that people have proposed don’t work. And then we found some very simple reasons for that.”<br>—Matthias Troyer, <a href="https://spectrum.ieee.org/tag/microsoft">Microsoft</a></p><p>The leading proposal involves spreading information over many physical qubits to create “<a data-linked-post="2657421771" href="https://spectrum.ieee.org/fault-tolerant-quantum-computing-milestone" target="_blank">logical qubits</a>” that are more robust, but this could require as many as 1,000 physical qubits for each logical one. Some have suggested that <a href="https://spectrum.ieee.org/the-case-against-quantum-computing" target="_blank">quantum error correction</a> could even be fundamentally impossible, though that is not a mainstream view. Either way, realizing these schemes at the scale and speeds required remains a distant goal, Painter says. </p><p> “Given the remaining technical challenges in realizing a fault-tolerant quantum computer capable of running billions of gates over thousands of qubits, it is difficult to put a timeline on it, but I would estimate at least a decade out,” he says.</p><h3>Microsoft—Clarity, please</h3><p> The problem isn’t just one of timescales. In May, <a href="https://www.microsoft.com/en-us/research/people/mtroyer/" rel="noopener noreferrer" target="_blank">Matthias Troyer</a>, a technical fellow at Microsoft who leads the company’s quantum computing efforts, co-authored a <a href="https://cacm.acm.org/magazines/2023/5/272276-disentangling-hype-from-practicality-on-realistically-achieving-quantum-advantage/fulltext" rel="noopener noreferrer" target="_blank">paper in </a><a href="https://cacm.acm.org/magazines/2023/5/272276-disentangling-hype-from-practicality-on-realistically-achieving-quantum-advantage/fulltext" rel="noopener noreferrer" target="_blank"><em>Communications of the ACM</em></a> suggesting that the number of applications where quantum computers could provide a meaningful advantage was more limited than some might have you believe.</p><p> “We found out over the last 10 years that many things that people have proposed don’t work,” he says. “And then we found some very simple reasons for that.”</p><p>The main promise of quantum computing is the ability to solve problems far faster than classical computers, but exactly how much faster varies. There are two applications where quantum algorithms appear to provide an exponential speed up, says Troyer. One is factoring large numbers, which could make it possible to <a href="https://spectrum.ieee.org/post-quantum-cryptography-strategy" target="_self">break the public key encryption</a> the internet is built on. The other is simulating quantum systems, which could have applications in chemistry and materials science.<br></p><p>Quantum algorithms have been proposed for a range of other problems including <a href="https://arxiv.org/abs/2302.03711" target="_blank">optimization</a>, <a href="https://academic.oup.com/bioinformatics/article/39/1/btac789/6881079" rel="noopener noreferrer" target="_blank">drug design</a>, and <a href="https://spectrum.ieee.org/computational-fluid-dynamics-quantum-computer" target="_self">fluid dynamics</a>. But touted speedups don’t always pan out—sometimes amounting to a quadratic gain, meaning the time it takes the quantum algorithm to solve a problem is the square root of the time taken by its classical counterpart.</p><p>Troyer says these gains can quickly be wiped out by the massive computational overhead incurred by quantum computers. Operating a qubit is far more complicated than switching a <a href="https://spectrum.ieee.org/tag/transistor">transistor</a> and is therefore orders of magnitude slower. This means that for smaller problems, a classical computer will always be faster, and the point at which the quantum computer gains a lead depends on how quickly the complexity of the classical algorithm scales.</p><p>Operating a qubit is far more complicated than switching a transistor and is therefore orders of magnitude slower. </p><p>Troyer and his colleagues compared a single <a data-linked-post="2665752749" href="https://spectrum.ieee.org/nvidia-qubit" target="_blank">Nvidia A100 GPU</a> against a fictional future fault-tolerant quantum computer with 10,000 “logical qubits” and gates times much faster than today’s devices. Troyer says they found that a quantum algorithm with a quadratic speed up would have to run for centuries, or even millenia, before it could outperform a classical one on problems big enough to be useful.<br></p><p> Another significant barrier is data bandwidth. Qubits’ slow operating speeds fundamentally limit the rate at which you can get classical data in and out of a quantum computer. Even in optimistic future scenarios this is likely to be thousands or millions of times slower than classical computers, says Troyer. That means data-intensive applications like machine learning or searching databases are almost certainly out of reach for the foreseeable future.</p><p> The conclusion, says Troyer, was that quantum computers will only really shine on small-data problems with exponential speed ups. “All the rest is beautiful theory, but will not be practical,” he adds.</p><p> The paper didn’t make much of an impact in the quantum community, says Troyer, but many of Microsoft customers were grateful to get some clarity on realistic applications for quantum computing. He says they’ve seen a number of companies downsize or even shutdown their quantum computing teams, including in the finance and life sciences sectors.</p><h3>Aaronson—Welcome, skeptics</h3><p> These limitations shouldn’t really be a surprise to anyone who has been paying close attention to quantum computing research, says <a href="https://www.cs.utexas.edu/people/faculty-researchers/scott-aaronson" target="_blank">Scott Aaronson</a>, a professor of computer science at the University of Texas at Austin. “There are these claims about how quantum computing will revolutionize machine learning and optimization and finance and all these industries, where I think skepticism was always warranted,” he says. “If people are just now coming around to that, well then, welcome.”</p><p> While he also thinks practical applications are still a long way off, recent progress in the field has actually given him cause for optimism. Earlier this month researchers from quantum computing startup <a href="https://www.quera.com/" rel="noopener noreferrer" target="_blank">QuEra</a> and Harvard demonstrated that they could use a 280 qubit processor to <a href="https://www.newscientist.com/article/2407145-quantum-computer-sets-record-on-path-towards-error-free-calculations/" rel="noopener noreferrer" target="_blank">generate 48 logical qubits</a>–far more than <a href="https://scholar.google.com/scholar?q=quantum+computers+%22logical+qubits%22&amp;hl=en&amp;as_sdt=0,22" target="_blank">previous experiments</a> have managed. “This was definitely the biggest experimental advance maybe for several years,” says Aaronson.</p><p>“When you say quantum is going to solve all the world’s problems, and then it doesn’t, or it doesn’t right now, that creates a little bit of a letdown.”<br>—Yuval Boger, QuEra</p><p><a href="https://www.linkedin.com/in/yuvalboger/" target="_blank">Yuval Boger</a>, chief marketing officer at QuEra, is keen to stress that the experiment was a lab demonstration, but he thinks the results have caused some to reassess their timescales for fault-tolerant quantum computing. At the same time though, he says they have also noticed a trend of companies quietly shifting resources away from quantum computing.</p><p> This has been driven, in part, by growing interest in AI since the advent of large language models, he says. But he agrees that some in the industry have exaggerated the near-term potential of the technology, and says the hype has been a double-edged sword. “It helps get investments and get talented people excited to get into the field,” he says. “But on the other hand, when you say quantum is going to solve all the world’s problems, and then it doesn’t, or it doesn’t right now, that creates a little bit of a letdown.”</p><p> Even in the areas where quantum computers look most promising, the applications could be narrower than initially hoped. In recent years, papers from researchers at <a href="https://arxiv.org/pdf/2009.12472.pdf" rel="noopener noreferrer" target="_blank">scientific software company </a><a href="https://arxiv.org/pdf/2009.12472.pdf" rel="noopener noreferrer" target="_blank">Schrödinger</a> and a <a href="https://arxiv.org/pdf/2208.02199.pdf" rel="noopener noreferrer" target="_blank">multi-institutional team</a> have suggested that only a limited number of problems in quantum chemistry are likely to benefit from quantum speedups.</p><h3>Merck KGaA—Lovely accelerator, sometimes</h3><p> It’s also important to remember that many companies already have mature and productive quantum chemistry workflows that operate on classical hardware, says <a href="https://de.linkedin.com/in/philipp-harbach-579670172" rel="noopener noreferrer" target="_blank">Philipp Harbach</a>, global head of group digital innovation at German pharma giant <a href="https://www.merckgroup.com/en" target="_blank">Merck KGaA</a>, in Darmstadt, Germany (not to be confused with the American company <a href="https://www.merck.com/" target="_blank">Merck</a>).  </p><p> “In the public, the quantum computer was portrayed as if it would enable something not currently achievable, which is inaccurate,” he says. “Primarily, it will accelerate existing processes rather than introducing a completely disruptive new application area. So we are evaluating a difference here.”</p><p> Harbach’s group has been investigating the relevance of quantum computing to Merck’s work for about six years. While NISQ devices may potentially have uses for some certain highly specialized problems, they’ve concluded that quantum computing will not have a significant impact on industry until fault-tolerance is achieved. Even then, how transformative that impact could be really depends on the specific use case and products a company is working on, says Harbach.</p><p> Quantum computers shine at providing accurate solutions to problems that become intractable at larger scales for classical computers. That could be very useful for some applications, such as designing new catalysts, says Harbach. But most of the chemistry problems Merck is interested in involve screening large numbers of candidate molecules very quickly.</p><p> “Most problems in quantum chemistry do not scale exponentially, and approximations are sufficient,” he says. “They are well behaved problems, you just need to make them faster with increased system size.”</p><p> Nonetheless, there can still be cause for optimism, says Microsoft’s Troyer. Even if quantum computers can only tackle a limited palette of problems in areas like chemistry and materials science, the impact could still be game-changing. “We talk about the Stone Age and the Bronze Age, and the Iron Age, and the Silicon Age, so materials have a huge impact on mankind,” he says.</p><p> The goal of airing some skepticism, Troyer says, is not to diminish interest in the field, but to ensure that researchers are focused on the most promising applications of quantum computing with the greatest chance of impact.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta censors pro-Palestinian views on a global scale, report claims (498 pts)]]></title>
            <link>https://www.theguardian.com/technology/2023/dec/21/meta-facebook-instagram-pro-palestine-censorship-human-rights-watch-report</link>
            <guid>38745673</guid>
            <pubDate>Sat, 23 Dec 2023 16:57:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/2023/dec/21/meta-facebook-instagram-pro-palestine-censorship-human-rights-watch-report">https://www.theguardian.com/technology/2023/dec/21/meta-facebook-instagram-pro-palestine-censorship-human-rights-watch-report</a>, See on <a href="https://news.ycombinator.com/item?id=38745673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Meta has engaged in a “systemic and global” censorship of pro-Palestinian content since the outbreak of the <a href="https://www.theguardian.com/world/israel-hamas-war" data-link-name="in body link" data-component="auto-linked-tag">Israel-Gaza war</a> on 7 October, according to a new report from Human Rights Watch (HRW).</p><figure id="a7a8bef0-ae57-4ec3-90d5-a592c8c00f69" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:1,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/technology/2023/dec/19/facebook-moderation-israel-hamas-videos&quot;,&quot;text&quot;:&quot;Meta wrong to remove graphic Israel-Gaza videos, oversight board says&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;elementId&quot;:&quot;a7a8bef0-ae57-4ec3-90d5-a592c8c00f69&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>In a <a href="https://www.hrw.org/report/2023/12/21/metas-broken-promises/systemic-censorship-palestine-content-instagram-and" data-link-name="in body link">scathing 51-page report</a>, the organization documented and reviewed more than a thousand reported instances of Meta removing content and suspending or permanently banning accounts on Facebook and Instagram. The company exhibited “six key patterns of undue censorship” of content in support of Palestine and Palestinians, including the taking down of posts, stories and comments; disabling accounts; restricting users’ ability to interact with others’ posts; and “shadow banning”, where the visibility and reach of a person’s material is significantly reduced, according to HRW.</p><p>Examples it cites include content originating from more than 60 countries, mostly in English, and all in “peaceful support of Palestine, expressed in diverse ways”. Even HRW’s own posts seeking examples of online censorship were flagged as spam, the report said.</p><p>“Censorship of content related to Palestine on <a href="https://www.theguardian.com/technology/instagram" data-link-name="in body link" data-component="auto-linked-tag">Instagram</a> and Facebook is systemic and global [and] Meta’s inconsistent enforcement of its own policies led to the erroneous removal of content about Palestine,” the group said in the report, citing “erroneous implementation, overreliance on automated tools to moderate content, and undue government influence over content removals” as the roots of the problem.</p><p>In a statement to the Guardian, <a href="https://www.theguardian.com/technology/meta" data-link-name="in body link" data-component="auto-linked-tag">Meta</a> acknowledged it makes errors that are “frustrating” for people, but said that “the implication that we deliberately and systemically suppress a particular voice is false. Claiming that 1,000 examples, out of the enormous amount of content posted about the conflict, are proof of ‘systemic censorship’ may make for a good headline, but that doesn’t make the claim any less misleading.</p><p>Meta said it was the only company in the world to have publicly released <a href="https://humanrights.fb.com/wp-content/uploads/2023/09/September-2023-Israel-and-Palestine-HRDD-Meta-Update.pdf" data-link-name="in body link">human rights due diligence</a> on issues related to Israel and Palestine .</p><p>“This report ignores the realities of enforcing our policies globally during a fast-moving, highly polarized and intense conflict, which has led to an increase in content being reported to us. Our policies are designed to give everyone a voice while at the same time keeping our platforms safe,” the company’s statement reads.</p><p>It is the second time this month that Meta has been challenged over accusations that it routinely silences pro-Palestinian content and voices.</p><p>Last week Elizabeth Warren, Democratic senator for Massachusetts, <a href="https://www.theguardian.com/us-news/2023/dec/14/elizabeth-warren-mark-zuckerberg-meta-instagram-censorship" data-link-name="in body link">wrote to Meta’s co-founder and chief executive</a> officer, Mark Zuckerberg, demanding information following hundreds of reports from Instagram users dating back to October that their <a href="https://www.theguardian.com/us-news/2023/dec/14/elizabeth-warren-mark-zuckerberg-meta-instagram-censorship" data-link-name="in body link">content was demoted or removed</a>, and their accounts subjected to shadow banning.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-10">skip past newsletter promotion</a><p id="EmailSignup-skip-link-10" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>On Tuesday, Meta’s oversight board said the company had been wrong <a href="https://www.theguardian.com/technology/2023/dec/19/facebook-moderation-israel-hamas-videos" data-link-name="in body link">to remove two videos of the conflict</a> in particular from Instagram and Facebook. The board said the videos were valuable for “informing the world about human suffering on both sides”. One showed the aftermath of an airstrike near al-Shifa hospital in Gaza via Instagram, the other a woman being taken hostage during the 7 October attack via Facebook. The clips were reinstated.</p><p>Users of Meta’s products have documented what they say is technological bias in favor of pro-Israel content and against pro-Palestinian posts. Instagram’s translation software <a href="https://www.theguardian.com/technology/2023/oct/18/instagram-palestine-posts-censorship-accusations" data-link-name="in body link">replaced “Palestinian” followed by the Arabic phrase “Praise be to Allah” to “Palestinian terrorists” in English</a>. WhatsApp’s AI, when asked to <a href="https://www.theguardian.com/technology/2023/nov/02/whatsapps-ai-palestine-kids-gun-gaza-bias-israel" data-link-name="in body link">generate images of Palestinian boys and girls</a>, created cartoon children with guns, whereas its images Israeli children did not include firearms.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xmas.c, winner of the 1988 International Obfuscated C Code Contest (493 pts)]]></title>
            <link>https://udel.edu/~mm/xmas/</link>
            <guid>38745668</guid>
            <pubDate>Sat, 23 Dec 2023 16:56:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://udel.edu/~mm/xmas/">https://udel.edu/~mm/xmas/</a>, See on <a href="https://news.ycombinator.com/item?id=38745668">Hacker News</a></p>
<div id="readability-page-1" class="page">



 In 1988 an impressive piece of C code named xmas.c
 was a winner in the International Obfuscated C Code Contest.
 After years of being impressed by the program (I first
 saw it around 2000), in November 2008, with some spare time on my hands,
 I decided to rip apart the code and figure out what was going on.

 <h3>The Code</h3>
 <a href="https://udel.edu/~mm/xmas/xmas.c">Click here to download the code.</a>
<xmp>
/*
Least likely to compile successfully: <ian@unipalm.co.uk> Ian Phillipps 

    Ian Phillipps
    Cambridge Consultants Ltd
    Science Park
    Milton Road
    Cambridge CB4 4DW
    England

Compile and run without parameters.

The program is smaller than even the 'compressed' form of its output,
and thus represents a new departure in text compression standards.

The judges thought that this program looked like what you would get
by pounding on the keys of an old typewriter at random.

Copyright (c) 1988, Landon Curt Noll & Larry Bassel.
All Rights Reserved.  Permission for personal, educational or non-profit use
is granted provided this this copyright and notice are included in its entirety
and remains unaltered.  All other uses must receive prior permission in
writing from both Landon Curt Noll and Larry Bassel.
*/

#include <stdio.h>
main(t,_,a)
char
*
a;
{
        return!

0<t?
t<3?

main(-79,-13,a+
main(-87,1-_,
main(-86, 0, a+1 )


+a)):

1,
t<_?
main(t+1, _, a )
:3,

main ( -94, -27+t, a )
&&t == 2 ?_
<13 ?

main ( 2, _+1, "%s %d %d\n" )

:9:16:
t<0?
t<-72?
main( _, t,
"@n'+,#'/*{}w+/w#cdnr/+,{}r/*de}+,/*{*+,/w{%+,/w#q#n+,/#{l,+,/n{n+,/+#n+,/#;\
#q#n+,/+k#;*+,/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l q#'+d'K#!/+k#;\
q#'r}eKK#}w'r}eKK{nl]'/#;#q#n'){)#}w'){){nl]'/+#n';d}rw' i;# ){nl]!/n{n#'; \
r{#w'r nc{nl]'/#{l,+'K {rw' iK{;[{nl]'/w#q#\
\
n'wk nw' iwk{KK{nl]!/w{%'l##w#' i; :{nl]'/*{q#'ld;r'}{nlwb!/*de}'c ;;\
{nl'-{}rw]'/+,}##'*}#nc,',#nw]'/+kd'+e}+;\
#'rdq#w! nr'/ ') }+}{rl#'{n' ')# }'+}##(!!/")
:
t<-50?
_==*a ?
putchar(31[a]):

main(-65,_,a+1)
:
main((*a == '/') + t, _, a + 1 )
:

0&#60;t?

main ( 2, 2 , "%s")
:*a=='/'||

main(0,

main(-61,*a, "!ek;dc i@bK'(q)-[w]*%n+r3#l,{}:\nuwloca-O;m
.vpbks,fxntdCeghiry")

,a+1);}

</xmp>

<h3>The Output</h3>

Amazingly, the output is

<pre>[mm@noise]$ xmas
On the first day of Christmas my true love gave to me
a partridge in a pear tree.

On the second day of Christmas my true love gave to me
two turtle doves
and a partridge in a pear tree.

On the third day of Christmas my true love gave to me
three french hens, two turtle doves
and a partridge in a pear tree.

On the fourth day of Christmas my true love gave to me
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.

On the fifth day of Christmas my true love gave to me
five gold rings;
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.

On the sixth day of Christmas my true love gave to me
six geese a-laying, five gold rings;
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.

On the seventh day of Christmas my true love gave to me
seven swans a-swimming,
six geese a-laying, five gold rings;
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.

On the eigth day of Christmas my true love gave to me
eight maids a-milking, seven swans a-swimming,
six geese a-laying, five gold rings;
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.

On the ninth day of Christmas my true love gave to me
nine ladies dancing, eight maids a-milking, seven swans a-swimming,
six geese a-laying, five gold rings;
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.

On the tenth day of Christmas my true love gave to me
ten lords a-leaping,
nine ladies dancing, eight maids a-milking, seven swans a-swimming,
six geese a-laying, five gold rings;
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.

On the eleventh day of Christmas my true love gave to me
eleven pipers piping, ten lords a-leaping,
nine ladies dancing, eight maids a-milking, seven swans a-swimming,
six geese a-laying, five gold rings;
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.

On the twelfth day of Christmas my true love gave to me
twelve drummers drumming, eleven pipers piping, ten lords a-leaping,
nine ladies dancing, eight maids a-milking, seven swans a-swimming,
six geese a-laying, five gold rings;
four calling birds, three french hens, two turtle doves
and a partridge in a pear tree.
</pre>

<h3>Analysis</h3>

So, how in the world was this accomplished?  First, the code needs to be
rewritten in a more legible manner.  Let's replace all <tt>a ? b : c</tt>
constructs with explicit <tt>if-then-else</tt> blocks.  Additionally, it is
helpful to define character strings whose names will become clear shortly.

<pre>#include &lt;stdio.h&gt;

char *words =
"@n'+,#'/*{}w+/w#cdnr/+,{}r/*de}+,/*{*+,/w{%+,/w#q#n+,/#{l+,/n{n+,/+#n+,/#\
;#q#n+,/+k#;*+,/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l \
q#'+d'K#!/+k#;q#'r}eKK#}w'r}eKK{nl]'/#;#q#n'){)#}w'){){nl]'/+#n';d}rw' i;# \
){nl]!/n{n#'; r{#w'r nc{nl]'/#{l,+'K {rw' iK{;[{nl]'/w#q#n'wk nw' \
iwk{KK{nl]!/w{%'l##w#' i; :{nl]'/*{q#'ld;r'}{nlwb!/*de}'c \
;;{nl'-{}rw]'/+,}##'*}#nc,',#nw]'/+kd'+e}+;#'rdq#w! nr'/ ') }+}{rl#'{n' ')# \
}'+}##(!!/";

char *shift = "!ek;dc i@bK'(q)-[w]*%n+r3#l,{}:\nuwloca-O;m .vpbks,fxntdCeghiry";

main() {
    xmas(1, 0, '\0');
}

xmas(int t, int _, char *a) {

    if (t &lt; -72) {
        return xmas(_, t, words);
    }

    if (t &lt; -50) {
        if (_ == *a) {
            return putchar(a[31]);
        } else {
            return xmas(-65, _, a+1);
        }
    }

    if (t &lt; 0) {
        return xmas((*a == '/')+t, _, a+1);
    }

    if (t == 0) {
        if (*a == '/') {
            return 1;
        } else {
            return xmas(0, xmas(-61, *a, shift), a+1);
        }
    }

    if (t == 1) {
        return xmas(2, 2, "%s");
    }

    if (t == 2)
        xmas(-79, -13, a + xmas(-87, 1-_, a + xmas(-86, 0, a+1)));

    if (t &lt; _)
        xmas(t+1, _, a);

    if (xmas(-94, -27+t, a) &amp;&amp; t == 2) {
        if (_ &lt; 13)
            return xmas(2, _+1, "%s %d %d\n");
        else
            return 9;
    } else {
        return 16;
    }
}
</pre>

Already we can see some hint of what is happening.  The variable <tt>t</tt> has
special significance in controlling the direction of recursion.  The
first conditional, <tt>if (t &lt; -72)</tt>, is just misdirection mostly
for the fun of
it.  It swaps the first two arguments and recurses using the encrypted string
of words for the final argument.  The utility of the conditional is how it
allows nested recursion as seen in the <tt>if (t == 2)</tt> block by ignoring
the third argument.

<p>The second block has more real work going on.  If the character stored
in the variable <tt>_</tt> does not match the first character of the
string <tt>a</tt>, the code recurses in such a way that this same block
of code will again be entered.  That is, <tt>t=-65</tt> forces a return
to the <tt>if (t &lt; -50)</tt> block.  The recursion steps through string
<tt>a</tt>, character by character.  When finally <tt>_ == a</tt>, the
character at <tt>a+31</tt> is printed and returned.

</p><p>Further study shows why this is done.  The string I renamed to
<tt>shift</tt> is really two strings concatenated.  A character found in the
first half of the string is 31 places away from its decoded equivalent.  For
instance, the exclamation point at the first place in the string is followed 31
places later by a newline.  So the string offers the solution to a
substitution cipher.

</p><p>In the code below, comments are added.  The variable <tt>shift</tt> has a
comment under it lining up the decoded (shifted by 31) values of the string.
For example, under the first character <tt>!</tt>, is character in the string
shifted by 31 places, the newline character.  The encoded/decoded halves of
the string are separated at the colon character.

</p><p>The other variable, newly named <tt>words</tt> in the code below, is the
set of encrypted words used to print the Christmas carol lyrics.  Using the
cipher solution string, the words have been decoded in the comments of the
source code below.  Notice that the ordinal numbers and lines of verses are
separated by slash characters.

</p><p>The third conditional, <tt>if (t &lt; 0)</tt> is used to find <tt>|t|</tt> slash
characters and then after <tt>t</tt> recursions, to return <tt>xmas(0, _,
a+1)</tt>, where <tt>a+1</tt> is the string starting at the character after the
slash.  Again, through recursion it simply gets us to the character after 
|t|'th slash.  The next call goes into the conditional block described next.

</p><p>The fourth conditional, <tt>if (t == 0)</tt>, uses recursion with the
2nd conditional block, <tt>t &lt; -50</tt>, to print out the decoded string up
to the next slash and then returns 1.

</p><p>The <tt>t == 1</tt> block is used just a single time at the start to get
the recursion going properly, that is, with <tt>xmas(2, 2, "%s")</tt> where
the string is unimportant because it is never used.

</p><p>The <tt>t == 2</tt> block is used to print "On the <i>[ordinal]</i> day of
Christmas my true love gave to me\n".

</p><p>The final two conditional blocks keep the recursion running in two
directions.  First, the final block counts up to day 12.  The second
to last block, counts down from the current day to print lyrics of the
current verse in reverse order.

</p><pre>#include &lt;stdio.h&gt;

<span color="#0000ff">/*
 * Substitution cipher solution.  Letters up to and including the colon are 
 * shifted by 31 places to the right to find the decoded value.
 */</span>
char *shift = "!ek;dc i@bK'(q)-[w]*%n+r3#l,{}:\nuwloca-O;m .vpbks,fxntdCeghiry";
<span color="#0000ff">         /*  "\nuwloca-O;m .vpbks,fxntdCeghiry"; */</span>

char *words =
"@n'+,#'/*{}w+/w#cdnr/+,{}r/*de}+,/*{*+,/w{%+,/w#q#n+,/#{l+,/n{n+,/+#n+,/#\
;#q#n+,/+k#;*+,/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l \
q#'+d'K#!/+k#;q#'r}eKK#}w'r}eKK{nl]'/#;#q#n'){)#}w'){){nl]'/+#n';d}rw' i;# \
){nl]!/n{n#'; r{#w'r nc{nl]'/#{l,+'K {rw' iK{;[{nl]'/w#q#n'wk nw' \
iwk{KK{nl]!/w{%'l##w#' i; :{nl]'/*{q#'ld;r'}{nlwb!/*de}'c \
;;{nl'-{}rw]'/+,}##'*}#nc,',#nw]'/+kd'+e}+;#'rdq#w! nr'/ ') }+}{rl#'{n' ')# \
}'+}##(!!/";

<span color="#0000ff">/*
  Decoded values of 'words' using the substitution cipher string 'shift':

"@n'+,#'/*{}w+/w#cdnr/+,{}r/*de}+,/*{*+,/w{%+,/w#q#n+,/#{l+,/n{n+,/+#n+,/#\
 <span color="#00ff00">On the /first/second/third/fourth/fifth/sixth/seventh/eigth/ninth/tenth/e </span>

;#q#n+,/+k#;*+,/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l \
<span color="#00ff00">leventh/twelfth/ day of Christmas my true love ga </span>

q#'+d'K#!/+k#;q#'r}eKK#}w'r}eKK{nl]'/#;#q#n'){)#}w'){){nl]'/+#n';d}rw' i;# \
<span color="#00ff00">ve to me0/twelve drummers drumming, /eleven pipers piping, /ten lords a-lea </span>

){nl]!/n{n#'; r{#w'r nc{nl]'/#{l,+'K {rw' iK{;[{nl]'/w#q#n'wk nw' \
<span color="#00ff00">ping,0/nine ladies dancing, /eight maids a-milking, /seven swans a\ </span>

iwk{KK{nl]!/w{%'l##w#' i; :{nl]'/*{q#'ld;r'}{nlwb!/*de}'c \
<span color="#00ff00">-swimming,0/six geese a-laying, /five gold rings;0/four ca
</span>

;;{nl'-{}rw]'/+,}##'*}#nc,',#nw]'/+kd'+e}+;#'rdq#w! nr'/ ') }+}{rl#'{n' ')# \
<span color="#00ff00">lling birds, /three french hens, /two turtle doves0and /a partridge in a pea </span>

}'+}##(!!/";
<span color="#00ff00">r tree.00/ </span>

*/</span>

main() {
    xmas(1, 0, '\0');
}

xmas(int t, int _, char *a) {

    <span color="#0000ff">/* Swap first two args and use new 3rd. */</span>
    if (t &lt; -72) {
        return xmas(_, t, words);
    }

    <span color="#0000ff">/*
     * Loop through a till a==_, then print char at a+31.
     * That is, given character in variable '_' and substitution cipher
     * solution in 'a', decode character '_' and print &amp; return it.
     */</span>
    if (t &lt; -50) {
        if (_ == *a) {
            return putchar(a[31]);
        } else {
            <span color="#0000ff">/* Loop until _ == *a. */</span>
            return xmas(-65, _, a+1);
        }
    }

    <span color="#0000ff">/*
     * Loop until finding -t number of slash characters in a[] and
     * return string starting at character after that slash.
     */</span>
    if (t &lt; 0) {
        return xmas((*a == '/')+t, _, a+1);
    }

    <span color="#0000ff">/*
     * Decode &amp; print word up to next slash character and then return 1.
     */</span>
    if (t == 0) {
        if (*a == '/') {
            return 1;
        } else {
            return xmas(0, xmas(-61, *a, shift), a+1);
        }
    }

    <span color="#0000ff">/*
     * Start off recursion.  Only called once at very start.
     */</span>
    if (t == 1) {
        return xmas(2, 2, "%s");
    }

    if (t == 2)
        xmas(-79, <span color="#0000ff">/* " day of Christmas my true love gave to me\n" */</span>
             -13,
             a + xmas(-87, <span color="#0000ff">/* print which day of Christmas it is */</span>
                      1-_,
                      a + xmas(-86, 0, a+1))); <span color="#0000ff">/* "On the " */</span>

    <span color="#0000ff">/*
     * Recurse, count down days of Christmas
     * from '_' to print lyrics of current verse in reverse day order.
     * I.e., day 12, day 11, etc.
     */</span>
    if (t &lt; _)
        xmas(t+1, _, a);

    <span color="#0000ff">/*
     * Print phrase for current verse.
     * E.g., t == 2: "a partridge in a pear tree\n\n"
     * etc.
     */</span>
    if (xmas(-94, -27+t, a) &amp;&amp; t == 2) {
        if (_ &lt; 13)
            <span color="#0000ff">/* Recurse to next higher day of Christmas.  String not used. */</span>
            return xmas(2, _+1, "%s %d %d\n");
        else
            <span color="#0000ff">/* Just misdirection, return value not used. */</span>
            return 9;
    } else {
        <span color="#0000ff">/* More misdirection! */</span>
        return 16;
    }
}
</pre>

<h2>Simpler</h2>

With new understanding of what's going on, the program can be simplified
by using some iteration and C string library routines.  I have to admit to
preferring recursion myself, but since the analysis has gone this far, let's
not stop now.  Here's what we might see after further simplification:

<pre>#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

char *words =
"@n'+,#'/*{}w+/w#cdnr/+,{}r/*de}+,/*{*+,/w{%+,/w#q#n+,/#{l+,/n{n+,/+#n+,/#\
;#q#n+,/+k#;*+,/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l \
q#'+d'K#!/+k#;q#'r}eKK#}w'r}eKK{nl]'/#;#q#n'){)#}w'){){nl]'/+#n';d}rw' i;# \
){nl]!/n{n#'; r{#w'r nc{nl]'/#{l,+'K {rw' iK{;[{nl]'/w#q#n'wk nw' \
iwk{KK{nl]!/w{%'l##w#' i; :{nl]'/*{q#'ld;r'}{nlwb!/*de}'c \
;;{nl'-{}rw]'/+,}##'*}#nc,',#nw]'/+kd'+e}+;#'rdq#w! nr'/ ') }+}{rl#'{n' ')# \
}'+}##(!!/";

char *shift = "!ek;dc i@bK'(q)-[w]*%n+r3#l,{}:\nuwloca-O;m .vpbks,fxntdCeghiry";

main() {
    xmas(2, 2, "");
}

xmas(int t, int _, char *a) {

    <span color="#0000ff">/*
     * Loop until finding |t| number of slash characters in a[] and
     * return string starting at character after that slash.
     */</span>
    if (t &lt; 0) {
        while (t++ &lt; 0)
            a = 1 + index(a, '/');
        return xmas(0, _, a);
    }

    <span color="#0000ff">/*
     * Decode &amp; print word up to next slash character and return 0 or any int.
     */</span>
    if (t == 0) {
        while (*a != '/')
            putchar(index(shift, *a++)[31]); <span color="#0000ff">/* Decode *a and print it. */</span>
        return 0;
    }

    if (t == 2) {
        <span color="#0000ff">/* 2nd arg is a don't-care since it's unused. */</span>
        xmas(0, 0, words); <span color="#0000ff">/* "On the " */</span>
        xmas(1-_, 0, words); <span color="#0000ff">/* print which day of Christmas it is */</span>
        xmas(-13, 0, words); <span color="#0000ff">/* " my true love gave to me\n" */</span>
    }

    <span color="#0000ff">/*
     * Recurse and count down days of Christmas
     * from '_' down to and including day 2.
     */</span>
    if (t &lt; _)
        xmas(t+1, _, a);

    <span color="#0000ff">/*
     * t==2: "a partridge in a pear tree\n\n"
     * etc.
     */</span>
    xmas(-27+t, 0, words);
    if (t == 2 &amp;&amp; _ &lt; 13)
        return xmas(2, _+1, ""); <span color="#0000ff">/* Next higher day of Christmas. */</span>
}
</pre>

<h2>In Conclusion...</h2>

This could go on till the ultimate simplification of doing nothing more than
printing out the lyrics, but you get the idea.  This is one of the better
obfuscated C programs I've come across because of using the
substitution cipher along with recursion.  That was followed by the addition
of a small bit of unnecessary code and use of random arguments when the
arguments were in fact not made use of.  A very nice little bundle of C
code!

<p>Understanding what's happening is still a long way from writing it.  What a
great example of creativity.

</p><p>Merry Christmas!<br>
Mike Markowski, mike.ab3ap -A- gmail -D- com

<!-- Start of StatCounter Code -->



<!-- End of StatCounter Code -->



</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hbomberguy didn't want to make that 4-hour plagiarism video (152 pts)]]></title>
            <link>https://www.vulture.com/2023/12/hbomberguy-interview-james-somerton-plagiarism.html</link>
            <guid>38745667</guid>
            <pubDate>Sat, 23 Dec 2023 16:56:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vulture.com/2023/12/hbomberguy-interview-james-somerton-plagiarism.html">https://www.vulture.com/2023/12/hbomberguy-interview-james-somerton-plagiarism.html</a>, See on <a href="https://news.ycombinator.com/item?id=38745667">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main">  <article role="main" data-track-type="article-detail" data-uri="www.vulture.com/_components/article/instances/clqgythu1000j0pgqm79jjg6z@published" data-content-channel="Online-Culture" data-crosspost="" data-type="Q&amp;A" data-syndication="original" data-headline="Hbomberguy Didn’t Want to Make That 4-Hour Plagiarism Video" data-authors="Rebecca Alter" data-publish-date="2023-12-22" data-tags="hbomberguy, be gay do crime, youtube, internet culture, lgbtqia+, extremely online, interview, vulture homepage lede" data-issue-date="" data-components-count="37" data-canonical-url="http://www.vulture.com/2023/12/hbomberguy-interview-james-somerton-plagiarism.html">


  
  
  
  <header>
    
  </header>
  <section>
    <div data-editable="content">
      <div>
          <div>
            <picture> <source media="(min-resolution: 192dpi) and (min-width: 1180px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 1180px)" srcset="https://pyxis.nymag.com/v1/imgs/504/912/6d97462cd00decdc4bb37a31788cdd7293-harry-brewis-youtube-video.2x.rhorizontal.w700.jpg 2x" width="700" height="467"> <source media="(min-width: 1180px) " srcset="https://pyxis.nymag.com/v1/imgs/504/912/6d97462cd00decdc4bb37a31788cdd7293-harry-brewis-youtube-video.rhorizontal.w700.jpg" width="700" height="467"> <source media="(min-resolution: 192dpi) and (min-width: 768px), (-webkit-min-device-pixel-ratio: 2) and (min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/504/912/6d97462cd00decdc4bb37a31788cdd7293-harry-brewis-youtube-video.2x.rhorizontal.w700.jpg 2x" width="700" height="467"> <source media="(min-width: 768px)" srcset="https://pyxis.nymag.com/v1/imgs/504/912/6d97462cd00decdc4bb37a31788cdd7293-harry-brewis-youtube-video.rhorizontal.w700.jpg" width="700" height="467"> <source media="(min-resolution: 192dpi), (-webkit-min-device-pixel-ratio: 2)" srcset="https://pyxis.nymag.com/v1/imgs/504/912/6d97462cd00decdc4bb37a31788cdd7293-harry-brewis-youtube-video.2x.rhorizontal.w700.jpg" width="700" height="467"> <img src="https://pyxis.nymag.com/v1/imgs/504/912/6d97462cd00decdc4bb37a31788cdd7293-harry-brewis-youtube-video.rhorizontal.w700.jpg" data-content-img="" width="700" height="467" fetchpriority="high"> </picture>
          </div>
            <div>
              <p>
                  Harry Brewis.
                <span>Photo: Harry Brewis via YouTube</span>
              </p>
            </div>
              </div>
            <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgythu1000i0pgqfmd8n1zw@published" data-word-count="187">On December 2, YouTuber Harry Brewis, better known as hbomberguy, slid in just under the wire and dropped what might be the best, and certainly the most widely talked about, video essay of the year. It is a YouTube video essay about YouTube video essays — specifically, about the prevalence of intellectual theft and laziness in the format, done in the most thoroughly researched and un-lazy way imaginable. “Plagiarism and You(Tube)” is three hours and 51 minutes long. That’s 25 minutes longer than <em>Killers of the Flower Moon. </em>You might think such an inside-baseball topic with such a sober-sounding focus (when’s the last time you’ve had a lecture on plagiarism? College?) would draw an insular, self-selecting crowd; instead, the video has 10 million views and counting, largely because Brewis is really, really, really, really, really good at making videos. Take one of his most famous clips —&nbsp;eviscerating Ben Shapiro while slamming an ax through a wall like a righteous Jack Torrance —&nbsp;for the way he combines researched footage, production design, and above all, humor, full of takedowns toward people and institutions that do, in fact, deserve it.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3vi6001a3j81tr13drlz@published" data-word-count="224">Brewis has been at this game for a while, getting his start as an underpaid editor for a larger content channel before breaking out on his own, offering funny, insightful, and exasperated “measured responses” to YouTube pickup artists, climate-change deniers, and misogynists, and doing video-game and pop-culture analysis. In this video, he addresses the unchecked plagiarism that’s been allowed to proliferate in this space, and the scammy, scummy creators who don’t do much original creating at all while profiting off of fan bases who put their faith in them. The first half of the video focuses on a handful of cases, including one-woman content farm Iilluminaughtii and meme explainer Internet Historian, but it’s the second half that has ignited endless analysis on Twitter and TikTok. Brewis focuses on one creator in particular: James Somerton, a business major who pivoted to doing queer-media analysis in bisexual lighting for over 300,000 subscribers and 3,000 Patreons. Only he wasn’t doing any of the analyzing, instead ripping off entire passages from other, often less famous, queer writers’ work, and in some cases, taking clips and footage from their videos and documentaries and passing them off as his own. These include the written work of Stephen Spinks for defunct regional LGBTQ publication <em>Midlands Zone</em>, <em>The Celluloid Closet </em>author Vito Russo, and <em>Tinker Belles and Evil Queens </em>author Sean Griffin.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3vk7001b3j81x5ket46r@published" data-word-count="118">Part of what makes Brewis’s video so powerful is the way he explains the stakes of this issue: the erasure of queer thinkers’ work, the silencing when they tried to call Somerton out, the manipulation of an audience who trusted him, for his own financial gain; Somerton made a living off of the ad revenue from these videos and encouraged his audience to invest in film productions that never got made. With the help of his producer Kat Lo (because what we <em>will </em>be doing is giving credit), Brewis relates this to questions of art, queer community, and identity itself. He has pledged that ad revenue from his video will be redistributed among the creators Somerton stole from.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3vm2001c3j81a7jqcjhb@published" data-word-count="121">In the wake of this video, Somerton went offline. Other YouTubers published their own videos, both about <a href="https://youtu.be/FkuW7uKG6l8?si=qOuydBZGC8BowqKQ">plagiarism</a> and <a href="https://www.youtube.com/watch?v=A6_LW1PkmnY&amp;pp=ygUidG9kZCBpbiB0aGUgc2hhZG93cyBqYW1lcyBzb21lcnRvbg%3D%3D">Somerton</a>, specifically. A conversation was ignited. On December 21, he posted (and took down and reuploaded) a tearful half-hour apology video, which does begin with him disclosing that he was hospitalized, implying self-harm, in the fallout from the essay. In the rest of the video, he dodges the word “plagiarism,” insists that he did not write any of the more offensive material (something probably disprovable, as he doesn’t show where he <em>did </em>get it from and insists that he’s not throwing his co-writer Nick Herrgott under the bus), and says he will be reinstating his Patreon. The backlash has been … <a href="https://twitter.com/0CEANKNIVES/status/1737702800157499752">considerable</a>.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3vnp001d3j81zzmt2mvl@published" data-word-count="81">We spoke with Brewis in the week after his original video went up, prior to Somerton’s reemergence, going in depth on topics like AI, the moral quandaries of “drama” videos, and why he didn’t even want to have to make this in the first place. “I don’t like starting fights,” he said. “It’s just I feel obligated to at a certain point. I hope this video gave the impression of someone who was dragged through the process somewhat against his will.”</p>

  


  

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3vpn001e3j812i2ltaoz@published" data-word-count="127"><strong>This video has broken out way beyond your usual viewer base; even though you’ve had some other videos do that in the past, none have been quite a moment like this. How does that feel?</strong><br>This video is my <em>Atlas Shrugged </em>in the sense that it’s far too long and inexplicably has crossover appeal despite being far worse than everything else I’ve ever made. I’m kidding, but it’s a shock, because I made this video to talk about an issue that I think affects YouTubers, which is ultimately a small group. I expected it to be popular with other YouTubers at best. I didn’t expect it to be popular with everyone, especially when the second half is largely about an intercommunity problem dealing with a few specific people.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgzjt10003j3b7znmy8ohmf@published" data-word-count="86"><strong>There’s something that feels correct about this video coming out at the same time as George Santos being ousted from Congress. He and James Somerton are spiritually brothers.</strong><br>I love the joke of “people love to say be gay, do crime” until you actually do it. We’ve entered a new era of complicated queer characters. It was difficult to be gay and complicated before, so I appreciate that we live in an era where you can be a gay person and a criminal. That’s a new tack. <strong>&nbsp;</strong></p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3vyk001i3j810rq0j42w@published" data-word-count="152"><strong>You normally don’t make things that could be considered YouTuber drama videos, so how do you feel about this being the video that has broken out?&nbsp;</strong><br>It’s something I’ve always wanted to avoid doing because people are immediately rewarded for it, and then you have a very strong incentive to make worse videos that make your life sadder, because of the income and attention it can bring you. And when I realized I was making a drama video about a guy, I panicked and thought,<em> This isn’t who I want to be. </em>How do I make this morally acceptable to myself before I continue, when obviously I would like to keep all of that money? Then there would be nothing stopping me from doing another one, and keeping that money, and what if I didn’t want to stop? You get to make <em>one</em> of these, and then you stop, or you become evil.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3w4v001k3j81rq3zjdzq@published" data-word-count="217"><strong>What has the reaction generally been like?</strong><br>Within the hyperspecific community of queer people who engage with James and not much else, because he <em>says</em> there’s not much else, the effect has been, I think, really good. I’ve introduced a lot of people to a lot of alternatives. Very few people have come to James’s defense. I was worried that if there were any holes in the video, there would be defenders. I wanted a clean shot. I have had a few people whose usernames ended up in the video [among James’s subscribers] reach out to say, “I’m really embarrassed that it was me that was highlighted.” So we’ve considered going back and maybe adding a blur to those sections of the video. But the people who it was made for have been really positive about it so far. And when the video hit, James immediately panicked and deleted his Discord server so no one could discuss it, and a lot of former Somerton fans have opened their own server to discuss it. My producer has gone in their server to talk to them and ask how they’re feeling about it. Overall, the last thing I wanted to do was make people feel more alone. I want to remind them that there’s a bigger community than they thought.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3w8z001m3j81lp00k5v8@published" data-word-count="205"><strong>Did you see James’s co-writer Nick’s post in the Discord about how he doesn’t do research?&nbsp;</strong><br>I did. That’s a very unfortunate thing for him to have written. He might have been panicking, so he said it in a very inarticulate way. I think what he meant was he was writing based on how he felt about things, so of <em>course </em>he can’t be stealing because he’s writing his authentic feelings. He’s made so many posts <a href="https://twitter.com/NTHerrgott/status/1732547235257213180">severing ties</a> with James, so I think he’s had a chance to think about his place in all this. He was told there were two instances of maybe-plagiarism and that James would fix them, and he believed him because he was his friend, and that’s completely sensible. I don’t blame him. I wish he’d not said those things, though, because it makes me look bad for giving him the benefit of the doubt. When we looked into Nick, one of the things we wanted to know was, <em>Does he know?</em> And <em>how is he, quality-wise, as a writer?</em> And something we left out in the video was James edits out a lot of Nick’s writing. In the video where he steals four minutes about Bob Iger, or Bob Chapek —</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3wcr001o3j81bh8tfyna@published" data-word-count="74"><strong>One of the evil Bobs of Disney.</strong><br>One of the many evil Bobs in the world, all of whom work for Disney, Nick wrote a section connecting the two stolen articles together that <em>almost</em> makes the video make sense. And in the final edit, James cuts immediately to the next chapter. It’s such an awkward and strange cut. I didn’t want to get into that, because it felt like a pointless aside for the video.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3whx001q3j81bd4a51gp@published" data-word-count="120">Another thing we looked at was, we found that Nick is a true writer, in the sense that he has a blog he updated three times and then gave up on, which we’ve all done. The way he would write around issues implies he has no idea what he’s talking about. Like when he was explaining what Orientalism is in this blog, he got his quote explaining what it is from the Wikipedia page for a book about Orientalism. And it wasn’t even a quote from the page itself. It was a quote from a review of the book, cited on the page. You know something’s bad when someone explains something and the quote is attributed to “Wikipedia Orientalism book.”</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3wjv001r3j81oak3c6hu@published" data-word-count="127"><strong>As if you can’t find a free Said PDF on the first page of Google results.&nbsp;</strong><br>It was the Said book. I have a lot of respect for my producer Kat Lo. She’s so much more intelligent than me, and it’s probably the reason why the video ended up good and not being a nightmare. I visited her a couple months ago and out of the corner of my eye saw <em>Orientalism </em>on her shelf. It was such a serendipitous moment. It’s fascinating seeing how easy it is to get away with not knowing anything while being a video essayist. It definitely made me think about how many more people are not plagiarists, but definitely charlatans, and <em>am I one of them and I just don’t know it?</em></p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3wns001t3j81spp0fkhm@published" data-word-count="116">I think people tend to fool themselves. People think they’re good at what they do, and they don’t question their practices. Because James and Nick both thought they were experts on the topics they talked about, they bumbled into so many blind alleys. I mentioned making this video to my friend Todd and he independently started watching James, and fell down all these rabbit holes, and made <a href="https://www.youtube.com/watch?v=A6_LW1PkmnY">a video</a> about him as well. It’s the Dunning-Kruger effect, when you’ve managed to stay at the top of the curve so you think, <em>I </em>must <em>be an expert, </em>and you never do any work that can disconfirm your hypothesis. It’s scary how easy it is to do that.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3wpn001u3j81qon3offy@published" data-word-count="124"><strong>Did you reach out to James at all during the process?&nbsp;</strong><br>I didn’t, mainly because when he’s been tipped off that someone has criticized him in the past, he will just hide the videos and not acknowledge it. I didn’t put the video up early for patrons either, because I didn’t want to tip him off. This was unfortunately a circumstance where I have to report without having his answer. With the other sections, I reached out to people for comment. And I reached out to people he’d stolen from, like Sean Griffin, for example. James claims he got permission in the end to republish Griffin’s video, but no one who’s ever emailed Sean Griffin has got a response. So I find that extremely unlikely.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3wyq001w3j81g1tmf82k@published" data-word-count="208"><strong>Did you hear from anyone who found out that he stole their work through watching your video?</strong><br>Yes. We reached out to a lot of people who James stole from who we trusted not to talk about it. To put it bluntly, I know some journalists in this video who I <em>know</em> would have immediately tweeted about it. So to be careful, there were a few people I reached out to and said, “Your work is being covered in this video,” but I can’t say who, and in fact one of them actually came out and said, “[Hbomberguy] didn’t tell me because he knew I would have talked shit immediately.” I didn’t reach out to Jes Tom, because I wasn’t sure where to reach out correctly because they seem to be traveling a lot, doing a lot of comedy shows, and I didn’t want to intrude. So they found out through the video. They reached out first once it went live, and what’s shocking is that they went on to write for <em>Our Flag Means Death — </em>a show James has made a video about, with plagiarism in it! Not only has he ripped this person off, he’s ripped off other people who review the show they work on.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3x8m001y3j81kr9hvar9@published" data-word-count="118"><strong>How about Somerton’s reaction to the video?</strong><br>We’ve been watching like hawks to see which videos he takes down since this came out. “Society and Queer Horror” was obliterated on impact. Before anyone could have conceivably finished watching the video, it was gone. And a further ten videos have disappeared in the intervening time. It’s been fun; these seem to be ones that we haven’t caught anything in … <em>yet. </em>My one shame is that I didn’t finish this video in time to prevent him from making a video about <em>Utena. </em>That was up for his patrons early and steals from Wikipedia and is bad and everyone hates it. I wish I prevented its existence entirely by being faster.</p>

  

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgzi5pv002e3b7z6xikucpl@published" data-word-count="211"><strong>Is there guilt that comes with doing such a thorough and complete dragging?&nbsp;</strong><br>Yeah, absolutely. I don’t like assuming that I’m going to destroy someone’s life. That feels&nbsp;self-aggrandizing. But making this video, I thought about the damage it would do. I’m not happy. I wish I could make this and then not have to deal with the fact that I did this. I feel very responsible for my actions. As a YouTuber, you hit a certain level of size where it occurs to you that someone can be horrible to you on the internet, and if you acknowledge it, the people who see it might respond to that person and do more harm than you ever received. You have to learn to accept the burden of not being able to fight back against certain people and accusations. There was this sense of,<em> Is James big enough that this is worth talking about?</em> And for a long time, the answer was no, until it kept becoming more clear that he’d profited so much off of people who are less successful than him. That made it more of a real issue. But I do feel guilt. I wish I didn’t have to make it, but ultimately, one person is responsible for making that video.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3xnl00223j814imwdvqn@published" data-word-count="145"><strong>Plus, you bring to light all of these queer writers and creators with way smaller platforms. It’s a greater good.</strong><br>It’s been incredibly invigorating, finding new people whose work I can read. I’m a huge fan now of Bart Bishop, who wrote a review of <em>Aliens</em>. And you can tell why it was discovered that he had been ripped off; people wanted to know where those words came from. They wanted to find the source of the quotes because he was so astute in finding good writing to quote. We can maybe reach out to <em>Midlands Zone</em> Magazine and try and get some kind of archive of their good articles together, because that stuff I read through [that Somerton stole] was fantastic. It means a lot to me because I grew up in the Midlands, thumbing through companies of that magazine when no one was looking.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3xr300243j815baej7oq@published" data-word-count="239"><strong>Was James’s behavior an open secret among YouTubers? How much was unearthed for the first time in your video? I’m thinking of Folding Ideas’ Twitter thread about James’s expensive equipment.&nbsp;</strong><br>There’s a streaming service called Nebula which I’m on, and James wanted to be on it violently. So much so that he was deliberately asking followers on his Discord, “I am once again asking you to email Nebula to insist I get on the platform.” Then he started claiming Nebula didn’t support queer people as a result of not letting him on, and accused them of blacklisting him, which was just not the case. He just saw a podcast by one of Nebula’s creators saying that they blacklist people if they’re really annoying, and then assumed that that must be him because he knows he’s been annoying; he had started a lot of fights with various YouTubers who were queer and on Nebula. When that happened, a lot of people started going, <em>Hold on, what is this guy’s deal? </em>As a result of that, several creators have definitely spoken to each other about him. I wouldn’t call it an open secret, but everyone has a James Somerton story. So when me and Kenny and <a href="https://www.youtube.com/c/FoldingIdeas">Dan</a> were speaking, we would share what we’ve discovered about it. I’d be like, “This thing is stolen too,” and Kenny would be like, “He thinks the Nazis invented health programs” or “Gay porn invented Skype.”</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3xur00263j81lzy51j43@published" data-word-count="113">The thing that really got to me was, for a while, I shot my videos with a Blackmagic Pocket Cinema Camera 4k, and it was far too advanced for YouTube videos, so I don’t use it anymore. And Somerton has a much more expensive version of that camera, and it’s a B-cam that he doesn’t shoot anything with. It’s just a background prop in several of his videos, sitting there unused. And that fills me with a kind of disgust that can’t really be described unless you also do this and know how wasteful that is. So I feel Dan’s pain when he talks about the sheer, vast expense of all of this.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3xwl00273j81wvngy6i2@published" data-word-count="109"><strong>In the video, you say that while YouTube doesn’t do an effective job addressing its plagiarism problem, you believe that if it actually tried, it would make the problem worse. How so?</strong><br>It has to be addressed on the creator level, where artists and creatives discuss what standards we hold each other to, as opposed to there being a formal process. If you plagiarize someone right now, you can just get away with it. No one can fire you. It needs to be a process of people publicly denouncing it when it happens, and being able to talk about it, to let people adjust who they want to watch accordingly.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3y0900293j81801r2lvb@published" data-word-count="120"><strong>If YouTubers have to hold each other accountable and act as their own whistleblowers, where do you think mainstream media is failing in covering this space?</strong><br>Mainstream media doesn’t know how to cover YouTube because it is also a competitor. It’s a different form of media that is actively replacing a lot of people’s consumption habits. So it’s very difficult to address anything happening on YouTube without an underlying message of “here’s what happens when you let a bunch of upstarts start democratizing media.” Mainstream media has had so much trouble keeping itself accountable in so many ways for so long that its inability to address this sort of discourse is not a surprise, and just an extension of that failure.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3y42002b3j81nrsqt394@published" data-word-count="132"><strong>You briefly touch on AI in the video, but I’d like to hear your further thoughts on AI codifying plagiarism, like literally operating on stealing other people’s work.&nbsp;</strong><br>If people were frank about their use of AI and also if we lived in a postcapitalist utopia, AI would be a very interesting experiment. I would love to see what people do with it. People talk about “prompt-mancy,” where they learn how to write the exact line of text to make something really interesting come out of this machine. I wish we could enjoy it on that level without in the back of our minds going, <em>This is going to put so many people out of a job.</em> So many other people’s work was stolen, unpaid, for it to be able to make this.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3ydz002d3j81cevcvv1d@published" data-word-count="295"><strong>In the first half of the video, you talk about Iilluminaughtii lifting from documentaries whole cloth for her own videos. Do you think her audience legitimately doesn’t care, because the version she offers is un-paywalled?&nbsp;</strong><br>Yes and no. Now that people are aware that you can just make a computer write something, I think they’re going to raise their standards. As soon as I started asking myself, “Am I just watching trash a computer spat out?” a lot of YouTubers I used to watch became boring to me. I thought, <em>This is interchangeable. </em>Even if a person wrote it, I hate this. Ninety percent of everything has always been bad. And the fact that we now have to think about what we’re watching and if it’s bad in this very active way, people will be more discerning about what they enjoy. We used to live in a time when there were three channels. If you wanted to watch something, you had to buy it on DVD or you had to go through the effort of stealing it from a torrent site and finding one that wasn’t a virus. But now, it’s convenient to just keep watching anything forever. We hit that stage seamlessly without having a moment of stopping to assess the quality of what we do with that time. It used to be, you’d watch an entire anime, and then you’d have to do work to find what you’d follow it up with. Now, if you have Crunchyroll, there’s 500 million episodes and you can just keep going forever. It’s much harder to stop and reassess. In a way, the badness is so omnipresent now, people will have to actually rethink their practices in a way that is maybe better than what we had before.</p>

  


  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3yio002f3j81u0w6lruq@published" data-word-count="268"><strong>You have 25 extra minutes of material on Iilluminaughtii on your second YouTube channel. What else did you leave on the cutting-room floor?&nbsp;</strong><br>I have a lot more footage of Internet Historian just reading the Mental Floss article [that “Man in Cave” was stolen from] that I just didn’t have time for because it’s an hour long. There’s so <em>much </em>of it. I have so many little mini-tangents about James that I cut out just because I thought if I talk about this, I feel like I’m just being mean. Kat felt very strongly about the misogyny section. Initially, I thought it should be a smaller aside, but as we kept looking, we realized this was a pattern of behavior that needs to be talked about. There’s a lot of other stuff we worried was poisoning the well. At the beginning of that section, I talked about how I’ve always had trouble watching his videos because they’re full of mistakes, and also he used to aggressively promote himself at me. He followed me on Twitter and linked me all of his videos. And then eventually when I didn’t respond, he unfollowed me. It’s very strange, and I decided not to talk about that. Even when you’re dealing with a vile person, if you talk about it wrong, you come across as vindictive, and I genuinely have no vendetta against James. I frankly still don’t believe I know who he is after having consumed so much of his material because there’s so little of him in it. But I wanted to make sure that I gave him a fair shake.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3yod002h3j81m3h5k7pu@published" data-word-count="182"><strong>One of my favorite lines in the video comes toward the end, after going through all of these great queer writers that you discovered through unearthing the work Somerton stole from. You say, “I’m proud of us. Yeah, I guess that’s the word for it.” It’s so clever and sweet. It made me wonder what your scriptwriting process is like for something this long.&nbsp;</strong><br>A lot of the good lines are ad libs. I fell into that line, and it’s so good. I try to make it clear when something is a written joke. A lot of the stuff that happens on the spot comes from having my fake notes, and I get into my emotional space of how I feel about what’s happening, and I just talk. If something funny happens, I feel good that it came out of me and that I didn’t have to plan it. It’s a process of slowly, continuously working at something until the good ideas emerge. I’m proud of that line. It feels so corny, but it came out of me without having to plan it.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3ys6002j3j810wug91bt@published" data-word-count="246"><strong>Toward the end, you expand your focus from this deep dive into this very humane musing about authorship and identity. How do you structure a video this long, and how do you plan for that final big thesis?&nbsp;</strong><br>I made the process of making these discoveries, and the struggle I had even making the video, part of the video. So even if the video went really bad, you’d experience the narrative of me making a bad video and regretting it in real time, and it would be a journey through a nightmare as opposed to just a bad video. It’s annoying. Last year, I deliberately made a video I felt was too long, because I was trying to find what my time limit is. It was about <a href="https://www.youtube.com/watch?v=bgJazjz9ZsA&amp;pp=ygUSaGJvbWJlcmd1eSBkZXVzIGV4"><em>Deus Ex: Human Revolution</em></a>, and the whole joke of the video was just about how this game is fine. It’s three hours, 33 minutes, and 33 seconds. I thought, <em>That’s the limit. People will hate this. I’ll be punished for it and then I’ll know where the line is.</em> Instead, I got 10 million views, and now I don’t know <em>what</em> to do. As long as the video feels authentic, the length dissolves away. I’ve had so many people say that they started watching it out of curiosity, and then they got an hour in before they paused it. Just because it flows. It’s not because I’m a genius at pacing. I just tell people how my experience is going.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3ywe002l3j81zgcbemyy@published" data-word-count="143"><strong>It really goes against the conventional wisdom of the internet giving everyone shortened attention spans.&nbsp;</strong><br>One of the problems with modern discourse about attention spans is that a lot of it is secretly a judgment of literacy. They’re not saying, <em>Everyone has ADHD now and needs to have other windows open at once. </em>What they’re really saying is <em>People don’t want to read. </em>When the honest truth is people might read less books now, but the average person reads more words than ever in human history. People read a book’s length of content on Twitter every day. So people are absolutely ready to spend four hours watching a video or a documentary or a movie that’s that long. We just also have been given the option of watching a thing that’s quite funny and is 20 seconds long. And who would turn that down?</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3z0x002n3j81n94oc159@published" data-word-count="115"><strong>They think there’s a “right” and “wrong” type of media literacy?&nbsp;</strong><br>Especially if you’re in the video-game space. So many games have come out that are effectively books. People like to make fun of visual novels for not being games, but people love them because they do enjoy reading. It’s just that the slight game aspects make it more fun. So yeah, I’m averse to the idea that TikTok&nbsp;is ruining people’s brains. It definitely is having an effect. But as for its effect on literacy, I think you have to be <em>extremely</em> literate to understand what the hell is happening on there. I don’t understand it. And I take that as a mistake on my part.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3zem002p3j81gpvo17mu@published" data-word-count="127"><strong>Are you looking forward to going back to video-game playthroughs after all of this?&nbsp;</strong><br>I’ve not played any games for, like, six months, except <em>Counter Strike. </em>I’m really looking forward to playing that <a href="https://www.youtube.com/watch?v=djS3WrTcQNM&amp;pp=ygUWbGV0aGFsIGNvbXBhbnkgZnVuaGF1cw%3D%3D">new survival horror game everyone loves</a>. It’s funny that for years, when I made the <em>Deus Ex </em>video, I joked that my next review would be of the director’s cut of <em>Deus Ex: Human Revolution. </em>I can’t wait to do that. And I want to finally do a video about the grants program that we ran and what we learned from artists who worked on it. The next video might deal with AI some more; it’s going to be about the modern state of digital creative tools and how Adobe has eaten all of them.</p>

  <p data-editable="text" data-uri="www.vulture.com/_components/clay-paragraph/instances/clqgz3zl4002r3j818wvh70f3@published" data-word-count="95"><strong>What would you like to see come from this video’s success?</strong><br>Toward the end of the video I say, and this was Kat’s suggestion to put this in, so thanks to her for bringing this up — there is definitely value in adapting essays and books. There are so many good words that are yet to make it onto YouTube. What James has done could have easily been an amazing piece of queer activism, if he just was honest about it. And I would like to see more adaptations of people’s writing done with their permission.</p>

  

    </div>

    


          



      <span>Hbomberguy Didn’t Want to Make a 4-Hour Plagiarism Video</span>



  </section>

  
  
</article>

  

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ferret: A Multimodal Large Language Model by Apple (558 pts)]]></title>
            <link>https://github.com/apple/ml-ferret</link>
            <guid>38745348</guid>
            <pubDate>Sat, 23 Dec 2023 16:19:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/apple/ml-ferret">https://github.com/apple/ml-ferret</a>, See on <a href="https://news.ycombinator.com/item?id=38745348">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-ferret/blob/main/figs/ferret_icon.png"><img src="https://github.com/apple/ml-ferret/raw/main/figs/ferret_icon.png" alt="Alt text for the image" width="40" height="45"></a> Ferret: Refer and Ground Anything Anywhere at Any Granularity</h2>
<p dir="auto"><em>An End-to-End MLLM that Accept Any-Form Referring and Ground Anything in Response.</em> [<a href="https://arxiv.org/abs/2310.07704" rel="nofollow">Paper</a>]</p>
<p dir="auto"><a href="https://hxyou.github.io/" rel="nofollow">Haoxuan You*</a>, <a href="https://haotian-zhang.github.io/" rel="nofollow">Haotian Zhang*</a>, <a href="https://zhegan27.github.io/" rel="nofollow">Zhe Gan</a>, <a href="https://scholar.google.com/citations?user=l1hP40AAAAAJ&amp;hl=en" rel="nofollow">Xianzhi Du</a>, <a href="https://zbwglory.github.io/" rel="nofollow">Bowen Zhang</a>, <a href="https://www.cs.cmu.edu/~ziruiw/" rel="nofollow">Zirui Wang</a>, <a href="http://llcao.net/" rel="nofollow">Liangliang Cao</a>, <a href="https://www.ee.columbia.edu/~sfchang/" rel="nofollow">Shih-Fu Chang</a>, <a href="https://sites.google.com/site/yinfeiyang/" rel="nofollow">Yinfei Yang</a>
[*: equal contribution]</p>
<h2 tabindex="-1" dir="auto">Overview</h2>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-ferret/blob/main/figs/ferret_fig_diagram_v2.png"><img src="https://github.com/apple/ml-ferret/raw/main/figs/ferret_fig_diagram_v2.png" width="100%"></a> <br>
    Diagram of Ferret Model.
</p>
<p dir="auto">Key Contributions:</p>
<ul dir="auto">
<li>Ferret Model - <strong>Hybrid Region Representation + Spatial-aware Visual Sampler</strong> enable fine-grained and open-vocabulary referring and grounding in MLLM.</li>
<li>GRIT Dataset (~1.1M) - A <strong>Large-scale, Hierarchical, Robust</strong> ground-and-refer instruction tuning dataset.</li>
<li>Ferret-Bench - A multimodal evaluation benchmark that jointly requires <strong>Referring/Grounding, Semantics, Knowledge, and Reasoning</strong>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Release</h2>
<ul dir="auto">
<li>[12/14] 🔥 We released the <a href="#checkpoints">checkpoints(7B, 13B)</a>.</li>
<li>[10/30] 🔥 We released the code of <strong>FERRET</strong> model and <a href="https://github.com/apple/ml-ferret/blob/main/ferret/eval/ferret_gpt4_data">Ferret-Bench</a>.</li>
</ul>
<p dir="auto"><strong>Usage and License Notices</strong>: The data, and code is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.</p>
<h2 tabindex="-1" dir="auto">Contents</h2>
<ul dir="auto">
<li><a href="#install">Install</a></li>
<li><a href="#train">Train</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#demo">Demo</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Install</h2>
<ol dir="auto">
<li>Clone this repository and navigate to FERRET folder</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/apple/ml-ferret
cd ml-ferret"><pre>git clone https://github.com/apple/ml-ferret
<span>cd</span> ml-ferret</pre></div>
<ol start="2" dir="auto">
<li>Install Package</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n ferret python=3.10 -y
conda activate ferret
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
pip install pycocotools
pip install protobuf==3.20.0"><pre>conda create -n ferret python=3.10 -y
conda activate ferret
pip install --upgrade pip  <span><span>#</span> enable PEP 660 support</span>
pip install -e <span>.</span>
pip install pycocotools
pip install protobuf==3.20.0</pre></div>
<ol start="3" dir="auto">
<li>Install additional packages for training cases</li>
</ol>
<div data-snippet-clipboard-copy-content="pip install ninja
pip install flash-attn --no-build-isolation"><pre><code>pip install ninja
pip install flash-attn --no-build-isolation
</code></pre></div>
<h2 tabindex="-1" dir="auto">Train</h2>
<p dir="auto">FERRET is trained on 8 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the <code>per_device_train_batch_size</code> and increase the <code>gradient_accumulation_steps</code> accordingly. Always keep the global batch size the same: <code>per_device_train_batch_size</code> x <code>gradient_accumulation_steps</code> x <code>num_gpus</code>.</p>
<h3 tabindex="-1" dir="auto">Hyperparameters</h3>
<p dir="auto">We use a similar set of hyperparameters as LLaVA(Vicuna) in finetuning.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Global Batch Size</th>
<th>Learning rate</th>
<th>Epochs</th>
<th>Max length</th>
<th>Weight decay</th>
</tr>
</thead>
<tbody>
<tr>
<td>FERRET-7B</td>
<td>128</td>
<td>2e-5</td>
<td>3</td>
<td>2048</td>
<td>0</td>
</tr>
<tr>
<td>FERRET-13B</td>
<td>128</td>
<td>2e-5</td>
<td>3</td>
<td>2048</td>
<td>0</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">Prepare Vicuna checkpoint and LLaVA's projector</h3>
<p dir="auto">Before you start, prepare our base model Vicuna, which is an instruction-tuned chatbot. Please download its weights following the instructions <a href="https://github.com/lm-sys/FastChat#model-weights">here</a>. Vicuna v1.3 is used in FERRET.</p>
<p dir="auto">Then download LLaVA's first-stage pre-trained projector weight (<a href="https://huggingface.co/liuhaotian/llava-336px-pretrain-vicuna-7b-v1.3" rel="nofollow">7B</a>, <a href="https://huggingface.co/liuhaotian/llava-336px-pretrain-vicuna-13b-v1.3" rel="nofollow">13B</a>).</p>
<h3 tabindex="-1" dir="auto">FERRET Training</h3>
<p dir="auto">The scripts are provided (<a href="https://github.com/apple/ml-ferret/blob/main/experiments/ferret_7b_train.sh">7B</a>, <a href="https://github.com/apple/ml-ferret/blob/main/experiments/ferret_13b_train.sh">13B</a>).</p>
<h2 tabindex="-1" dir="auto">Evaluation</h2>
<p dir="auto">Please see this <a href="https://github.com/apple/ml-ferret/blob/main/EVAL.md">doc</a> for the details.</p>
<h2 tabindex="-1" dir="auto">Checkpoints</h2>
<p dir="auto">We extracted the <code>delta</code> between our pre-trained model and Vicuna. Please first download weights of Vicuna following the <a href="#prepare-vicuna-checkpoint-and-llavas-projector">previous instruction</a>. Then download our prepared offsets of weights: <a href="https://docs-assets.developer.apple.com/ml-research/models/ferret/ferret-7b/ferret-7b-delta.zip" rel="nofollow">7B</a>, <a href="https://docs-assets.developer.apple.com/ml-research/models/ferret/ferret-13b/ferret-13b-delta.zip" rel="nofollow">13B</a> using <code>wget</code> or <code>curl</code>, and unzip the downloaded offsets. Lastly, apply the offset to the Vicuna's weight by running the following script:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# 7B
python3 -m ferret.model.apply_delta \
    --base ./model/vicuna-7b-v1-3 \
    --target ./model/ferret-7b-v1-3 \
    --delta path/to/ferret-7b-delta
# 13B
python3 -m ferret.model.apply_delta \
    --base ./model/vicuna-13b-v1-3 \
    --target ./model/ferret-13b-v1-3 \
    --delta path/to/ferret-13b-delta"><pre><span><span>#</span> 7B</span>
python3 -m ferret.model.apply_delta \
    --base ./model/vicuna-7b-v1-3 \
    --target ./model/ferret-7b-v1-3 \
    --delta path/to/ferret-7b-delta
<span><span>#</span> 13B</span>
python3 -m ferret.model.apply_delta \
    --base ./model/vicuna-13b-v1-3 \
    --target ./model/ferret-13b-v1-3 \
    --delta path/to/ferret-13b-delta</pre></div>
<p dir="auto"><strong>Notices</strong>: Apple's rights in the attached weight differentials are hereby licensed under the CC-BY-NC license. Apple makes no representations with regards to LLaMa or any other third party software, which are subject to their own terms.</p>
<p dir="auto">Please refer to the next section about how to set up a local demo with pre-trained weight.</p>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto">To run our demo, you need to train FERRET and use the checkpoints locally. Gradio web UI is used. Please run the following commands one by one.</p>
<h4 tabindex="-1" dir="auto">Launch a controller</h4>
<div dir="auto" data-snippet-clipboard-copy-content="python -m ferret.serve.controller --host 0.0.0.0 --port 10000"><pre>python -m ferret.serve.controller --host 0.0.0.0 --port 10000</pre></div>
<h4 tabindex="-1" dir="auto">Launch a gradio web server.</h4>
<div dir="auto" data-snippet-clipboard-copy-content="python -m ferret.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --add_region_feature"><pre>python -m ferret.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload --add_region_feature</pre></div>
<h4 tabindex="-1" dir="auto">Launch a model worker</h4>
<p dir="auto">This is the worker that load the ckpt and do the inference on the GPU.  Each worker is responsible for a single model specified in <code>--model-path</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="CUDA_VISIBLE_DEVICES=0 python -m ferret.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/FERRET-13B-v0 --add_region_feature"><pre>CUDA_VISIBLE_DEVICES=0 python -m ferret.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/FERRET-13B-v0 --add_region_feature</pre></div>
<p dir="auto">Wait until the process finishes loading the model and you see "Uvicorn running on ...".  Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-ferret/blob/main/figs/ferret_demo.png"><img src="https://github.com/apple/ml-ferret/raw/main/figs/ferret_demo.png" width="105%"></a> <br>
    Example of Ferret Interactive Demo.
</p>
<h2 tabindex="-1" dir="auto">Citation</h2>
<p dir="auto">If you find Ferret useful, please cite using this BibTeX:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{you2023ferret,
  title={Ferret: Refer and Ground Anything Anywhere at Any Granularity},
  author={You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
  journal={arXiv preprint arXiv:2310.07704},
  year={2023}
}"><pre><span>@article</span>{<span>you2023ferret</span>,
  <span>title</span>=<span><span>{</span>Ferret: Refer and Ground Anything Anywhere at Any Granularity<span>}</span></span>,
  <span>author</span>=<span><span>{</span>You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2310.07704<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2023<span>}</span></span>
}</pre></div>
<h2 tabindex="-1" dir="auto">Acknowledgement</h2>
<ul dir="auto">
<li><a href="https://github.com/haotian-liu/LLaVA">LLaVA</a>: the codebase we built upon.</li>
<li><a href="https://github.com/lm-sys/FastChat">Vicuna</a>: the LLM codebase.</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Otter, Fastest Go in-memory cache based on S3-FIFO algorithm (163 pts)]]></title>
            <link>https://github.com/maypok86/otter</link>
            <guid>38745070</guid>
            <pubDate>Sat, 23 Dec 2023 15:49:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/maypok86/otter">https://github.com/maypok86/otter</a>, See on <a href="https://news.ycombinator.com/item?id=38745070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/logo.png"><img src="https://github.com/maypok86/otter/raw/main/assets/logo.png" width="40%" height="auto"></a>
  </p><h2 tabindex="-1" dir="auto">High performance in-memory cache</h2>

<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/actions/workflows/test.yml/badge.svg"><img src="https://github.com/maypok86/otter/actions/workflows/test.yml/badge.svg"></a>
<a href="https://codecov.io/gh/maypok86/otter" rel="nofollow">
    <img src="https://camo.githubusercontent.com/5ba1f726f8c8ce647e1986aa33709afb35f989a9e7d8666a4882745c8a032523/68747470733a2f2f636f6465636f762e696f2f67682f6d6179706f6b38362f6f747465722f67726170682f62616467652e7376673f746f6b656e3d4730504a464f52384946" data-canonical-src="https://codecov.io/gh/maypok86/otter/graph/badge.svg?token=G0PJFOR8IF">
</a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/af85b5f8cb5222d92129fa14150c46580b174688cd0452ee2c6494395391fa2e/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6d6179706f6b38362f6f74746572"><img src="https://camo.githubusercontent.com/af85b5f8cb5222d92129fa14150c46580b174688cd0452ee2c6494395391fa2e/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6d6179706f6b38362f6f74746572" data-canonical-src="https://goreportcard.com/badge/github.com/maypok86/otter"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/eadbe9d0653062606371bd21eb03c3c9ce28f7ab08328972bd97cadf59724582/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6179706f6b38362f6f74746572"><img src="https://camo.githubusercontent.com/eadbe9d0653062606371bd21eb03c3c9ce28f7ab08328972bd97cadf59724582/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6179706f6b38362f6f74746572" data-canonical-src="https://img.shields.io/github/license/maypok86/otter"></a>
<br>
</p><h2 tabindex="-1" dir="auto">📖 Contents</h2>
<ul dir="auto">
<li><a href="#motivation">Motivation</a></li>
<li><a href="#related-works">Related works</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#usage">Usage</a>
<ul dir="auto">
<li><a href="#requirements">Requirements</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#examples">Examples</a></li>
</ul>
</li>
<li><a href="#benchmarks">Benchmarks</a>
<ul dir="auto">
<li><a href="#performance">Performance</a></li>
<li><a href="#hit-ratio">Hit ratio</a></li>
</ul>
</li>
<li><a href="#contribute">Contribute</a></li>
<li><a href="#license">License</a></li>
</ul>
<h2 tabindex="-1" dir="auto">💡 Motivation <a id="user-content-motivation"></a></h2>
<p dir="auto">I once came across the fact that none of the Golang cache libraries are truly contention-free. All of them are just a standard map with mutex and some eviction policy. Unfortunately, these are not able to reach the speed of caches in other languages (such as <a href="https://github.com/ben-manes/caffeine">Caffeine</a>). For example, the fastest cache from Dgraph labs called <a href="https://github.com/dgraph-io/ristretto">Ristretto</a>, which was faster than competitors by 30% at best (Otter is many times faster) and had a <a href="https://github.com/dgraph-io/ristretto/issues/336" data-hovercard-type="issue" data-hovercard-url="/dgraph-io/ristretto/issues/336/hovercard">disgusting hit ratio</a> even though README says otherwise. This can be a problem in different applications because no one wants to bump the performance of a cache library and its bad hit ratio 🙂. As a result, I wanted to get the fastest, easiest-to-use cache with excellent hit ratio and support from the authors and Otter is designed to correct this unfortunate misunderstanding.</p>
<p dir="auto"><strong>Please leave a ⭐ as motivation if you liked the idea 😄</strong></p>
<h2 tabindex="-1" dir="auto">🗃 Related works <a id="user-content-related-works"></a></h2>
<p dir="auto">Otter is based on the following papers:</p>
<ul dir="auto">
<li><a href="http://web.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-09-1.pdf" rel="nofollow">BP-Wrapper: A Framework Making Any Replacement Algorithms (Almost) Lock Contention Free</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/3600006.3613147" rel="nofollow">FIFO queues are all you need for cache eviction</a></li>
<li><a href="https://dl.acm.org/doi/fullHtml/10.1145/3422575.3422797" rel="nofollow">Bucket-Based Expiration Algorithm: Improving Eviction Efficiency for In-Memory Key-Value Database</a></li>
<li><a href="https://www.usenix.org/system/files/osdi20-yang.pdf" rel="nofollow">A large scale analysis of hundreds of in-memory cache clusters at Twitter</a></li>
</ul>
<h2 tabindex="-1" dir="auto">✨ Features <a id="user-content-features"></a></h2>
<p dir="auto">This library has lots of features such as:</p>
<ul dir="auto">
<li><strong>Simple API</strong>: Just set the parameters you want in the builder and enjoy</li>
<li><strong>Autoconfiguration</strong>: Otter is automatically configured based on the parallelism of your application</li>
<li><strong>Generics</strong>: You can safely use any comparable types as keys and any types as values</li>
<li><strong>TTL</strong>: Expired values will be automatically deleted from the cache</li>
<li><strong>Cost-based eviction</strong>: Otter supports eviction based on the cost of each item</li>
<li><strong>Excellent performance</strong>: Otter is currently the fastest cache library with a huge lead over the <a href="#performance">competition</a></li>
<li><strong>Great hit ratio</strong>: New S3-FIFO algorithm is used, which shows excellent <a href="#hit-ratio">results</a></li>
</ul>
<h2 tabindex="-1" dir="auto">📚 Usage <a id="user-content-usage"></a></h2>
<h3 tabindex="-1" dir="auto">📋 Requirements <a id="user-content-requirements"></a></h3>
<ul dir="auto">
<li>Go 1.18+</li>
</ul>
<h3 tabindex="-1" dir="auto">🛠️ Installation <a id="user-content-installation"></a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="go get -u github.com/maypok86/otter"><pre>go get -u github.com/maypok86/otter</pre></div>
<h3 tabindex="-1" dir="auto">✏️ Examples <a id="user-content-examples"></a></h3>
<p dir="auto"><strong>Builder</strong></p>
<p dir="auto">Otter uses a builder pattern that allows you to conveniently create a cache object with different parameters</p>
<div dir="auto" data-snippet-clipboard-copy-content="package main

import (
    &quot;github.com/maypok86/otter&quot;
)

func main() {
    // NewBuilder creates a builder and sets the future cache capacity to 1000 elements.
    // Returns an error if capacity <= 0.
    builder, err := otter.NewBuilder[string, string](1000)
    if err != nil {
        panic(err)
    }

    // StatsEnabled determines whether statistics should be calculated when the cache is running.
    // By default, statistics calculating is disabled.
    builder.StatsEnabled(true)

    // Cost sets a function to dynamically calculate the weight of a key-value pair.
    // By default this function always returns 1.
    builder.Cost(func(key string, value string) uint32 {
        return uint32(len(value))
    })

    // Build creates a new cache object or
    // returns an error if invalid parameters were passed to the builder.
    cache, err := builder.Build()
    if err != nil {
        panic(err)
    }

    cache.Close()
}"><pre><span>package</span> main

<span>import</span> (
    <span>"github.com/maypok86/otter"</span>
)

<span>func</span> <span>main</span>() {
    <span>// NewBuilder creates a builder and sets the future cache capacity to 1000 elements.</span>
    <span>// Returns an error if capacity &lt;= 0.</span>
    <span>builder</span>, <span>err</span> <span>:=</span> <span>otter</span>.<span>NewBuilder</span>[<span>string</span>, <span>string</span>](<span>1000</span>)
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
        <span>panic</span>(<span>err</span>)
    }

    <span>// StatsEnabled determines whether statistics should be calculated when the cache is running.</span>
    <span>// By default, statistics calculating is disabled.</span>
    <span>builder</span>.<span>StatsEnabled</span>(<span>true</span>)

    <span>// Cost sets a function to dynamically calculate the weight of a key-value pair.</span>
    <span>// By default this function always returns 1.</span>
    <span>builder</span>.<span>Cost</span>(<span>func</span>(<span>key</span> <span>string</span>, <span>value</span> <span>string</span>) <span>uint32</span> {
        <span>return</span> <span>uint32</span>(<span>len</span>(<span>value</span>))
    })

    <span>// Build creates a new cache object or</span>
    <span>// returns an error if invalid parameters were passed to the builder.</span>
    <span>cache</span>, <span>err</span> <span>:=</span> <span>builder</span>.<span>Build</span>()
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
        <span>panic</span>(<span>err</span>)
    }

    <span>cache</span>.<span>Close</span>()
}</pre></div>
<p dir="auto"><strong>Cache</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="package main

import (
    &quot;fmt&quot;
    &quot;time&quot;

    &quot;github.com/maypok86/otter&quot;
)

func main() {
    // create a cache with capacity equal to 10000 elements
    cache, err := otter.MustBuilder[string, string](10_000).Build()
    if err != nil {
        panic(err)
    }

    // set key-value pair with ttl (1 hour) 
    cache.SetWithTTL(&quot;key&quot;, &quot;value&quot;, time.Hour)

    // get value from cache
    value, ok := cache.Get(&quot;key&quot;)
    if !ok {
        panic(&quot;not found key&quot;)
    }
    fmt.Println(value)

    // delete key-value pair from cache
    cache.Delete(&quot;key&quot;)

    // delete data and stop goroutines
    cache.Close()
}"><pre><span>package</span> main

<span>import</span> (
    <span>"fmt"</span>
    <span>"time"</span>

    <span>"github.com/maypok86/otter"</span>
)

<span>func</span> <span>main</span>() {
    <span>// create a cache with capacity equal to 10000 elements</span>
    <span>cache</span>, <span>err</span> <span>:=</span> <span>otter</span>.<span>MustBuilder</span>[<span>string</span>, <span>string</span>](<span>10_000</span>).<span>Build</span>()
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
        <span>panic</span>(<span>err</span>)
    }

    <span>// set key-value pair with ttl (1 hour) </span>
    <span>cache</span>.<span>SetWithTTL</span>(<span>"key"</span>, <span>"value"</span>, <span>time</span>.<span>Hour</span>)

    <span>// get value from cache</span>
    <span>value</span>, <span>ok</span> <span>:=</span> <span>cache</span>.<span>Get</span>(<span>"key"</span>)
    <span>if</span> <span>!</span><span>ok</span> {
        <span>panic</span>(<span>"not found key"</span>)
    }
    <span>fmt</span>.<span>Println</span>(<span>value</span>)

    <span>// delete key-value pair from cache</span>
    <span>cache</span>.<span>Delete</span>(<span>"key"</span>)

    <span>// delete data and stop goroutines</span>
    <span>cache</span>.<span>Close</span>()
}</pre></div>
<h2 tabindex="-1" dir="auto">📊 Benchmarks <a id="user-content-benchmarks"></a></h2>
<p dir="auto">The benchmark code can be found <a href="https://github.com/maypok86/benchmarks">here</a></p>
<h3 tabindex="-1" dir="auto">🚀 Performance <a id="user-content-performance"></a></h3>
<h4 tabindex="-1" dir="auto">Read (100%)</h4>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/reads=100,writes=0.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/reads=100,writes=0.png" alt="reads=100%,writes=0%"></a></p>
<h4 tabindex="-1" dir="auto">Read (75%) / Write (25%)</h4>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/reads=75,writes=25.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/reads=75,writes=25.png" alt="reads=75%,writes=25%"></a></p>
<h4 tabindex="-1" dir="auto">Write (100%)</h4>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/reads=0,writes=100.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/reads=0,writes=100.png" alt="reads=0%,writes=100%"></a></p>
<h3 tabindex="-1" dir="auto">🎯 Hit ratio <a id="user-content-hit-ratio"></a></h3>
<h4 tabindex="-1" dir="auto">Zipf</h4>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/zipf.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/zipf.png" alt="zipf"></a></p>
<h4 tabindex="-1" dir="auto">S3</h4>
<p dir="auto">This trace is described as "disk read accesses initiated by a large commercial search engine in response to various web search requests."</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/s3.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/s3.png" alt="s3"></a></p>
<h4 tabindex="-1" dir="auto">DS1</h4>
<p dir="auto">This trace is described as "a database server running at a commercial site running an ERP application on top of a commercial database."</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/ds1.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/ds1.png" alt="ds1"></a></p>
<h4 tabindex="-1" dir="auto">P3</h4>
<p dir="auto">The trace P3 was collected from workstations running Windows NT by using Vtrace
which captures disk operations through the use of device
filters</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/p3.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/p3.png" alt="p3"></a></p>
<h4 tabindex="-1" dir="auto">P8</h4>
<p dir="auto">The trace P8 was collected from workstations running Windows NT by using Vtrace
which captures disk operations through the use of device
filters</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/p8.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/p8.png" alt="p8"></a></p>
<h4 tabindex="-1" dir="auto">LOOP</h4>
<p dir="auto">This trace demonstrates a looping access pattern.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/loop.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/loop.png" alt="loop"></a></p>
<h4 tabindex="-1" dir="auto">OLTP</h4>
<p dir="auto">This trace is described as "references to a CODASYL database for a one hour period."</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/maypok86/otter/blob/main/assets/results/oltp.png"><img width="60%" src="https://github.com/maypok86/otter/raw/main/assets/results/oltp.png" alt="oltp"></a></p>
<p dir="auto">In summary, we have that S3-FIFO (otter) is inferior to W-TinyLFU (theine) on lfu friendly traces (databases, search, analytics), but has a greater or equal hit ratio on web traces.</p>
<h2 tabindex="-1" dir="auto">👏 Contribute <a id="user-content-contribute"></a></h2>
<p dir="auto">Contributions are welcome as always, before submitting a new PR please make sure to open a new issue so community members can discuss it.
For more information please see <a href="https://github.com/maypok86/otter/blob/main/CONTRIBUTING.md">contribution guidelines</a>.</p>
<p dir="auto">Additionally, you might find existing open issues which can help with improvements.</p>
<p dir="auto">This project follows a standard <a href="https://github.com/maypok86/otter/blob/main/CODE_OF_CONDUCT.md">code of conduct</a> so that you can understand what actions will and will not be tolerated.</p>
<h2 tabindex="-1" dir="auto">📄 License <a id="user-content-license"></a></h2>
<p dir="auto">This project is Apache 2.0 licensed, as found in the <a href="https://github.com/maypok86/otter/blob/main/LICENSE">LICENSE</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bayesians Moving from Defense to Offense (187 pts)]]></title>
            <link>https://statmodeling.stat.columbia.edu/2023/12/23/bayesians-moving-from-defense-to-offense-i-really-think-its-kind-of-irresponsible-now-not-to-use-the-information-from-all-those-thousands-of-medical-trials-that-came-before-is-that-very/</link>
            <guid>38744588</guid>
            <pubDate>Sat, 23 Dec 2023 14:48:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statmodeling.stat.columbia.edu/2023/12/23/bayesians-moving-from-defense-to-offense-i-really-think-its-kind-of-irresponsible-now-not-to-use-the-information-from-all-those-thousands-of-medical-trials-that-came-before-is-that-very/">https://statmodeling.stat.columbia.edu/2023/12/23/bayesians-moving-from-defense-to-offense-i-really-think-its-kind-of-irresponsible-now-not-to-use-the-information-from-all-those-thousands-of-medical-trials-that-came-before-is-that-very/</a>, See on <a href="https://news.ycombinator.com/item?id=38744588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p><img decoding="async" src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2023/12/Screenshot-2023-12-22-at-16.49.49.png" alt="" width="550"></p>
<p>Erik van Zwet, Sander Greenland, Guido Imbens, Simon Schwab, Steve Goodman, and I <a rel="nofollow" href="https://evidence.nejm.org/doi/full/10.1056/EVIDoa2300003">write</a>:</p>
<blockquote><p>We have examined the primary efficacy results of 23,551 randomized clinical trials from the Cochrane Database of Systematic Reviews.</p>
<p>We estimate that the great majority of trials have much lower statistical power for actual effects than the 80 or 90% for the stated effect sizes. Consequently, “statistically significant” estimates tend to seriously overestimate actual treatment effects, “nonsignificant” results often correspond to important effects, and efforts to replicate often fail to achieve “significance” and may even appear to contradict initial results. To address these issues, we reinterpret the P value in terms of a reference population of studies that are, or could have been, in the Cochrane Database.</p>
<p>This leads to an empirical guide for the interpretation of an observed P value from a “typical” clinical trial in terms of the degree of overestimation of the reported effect, the probability of the effect’s sign being wrong, and the predictive power of the trial.</p>
<p>Such an interpretation provides additional insight about the effect under study and can guard medical researchers against naive interpretations of the P value and overoptimistic effect sizes. Because many research fields suffer from low power, our results are also relevant outside the medical domain.</p></blockquote>
<p><a rel="nofollow" href="https://onlinelibrary.wiley.com/doi/10.1002/sim.9992">Also this new paper</a> from Zwet with Lu Tian and Rob Tibshirani:</p>
<blockquote><p>Evaluating a shrinkage estimator for the treatment effect in clinical trials</p>
<p>The main objective of most clinical trials is to estimate the effect of some treatment compared to a control condition. We define the signal-to-noise ratio (SNR) as the ratio of the true treatment effect to the SE of its estimate. In a previous publication in this journal, we estimated the distribution of the SNR among the clinical trials in the Cochrane Database of Systematic Reviews (CDSR). We found that the SNR is often low, which implies that the power against the true effect is also low in many trials. Here we use the fact that the CDSR is a collection of meta-analyses to quantitatively assess the consequences. Among trials that have reached statistical significance we find considerable overoptimism of the usual unbiased estimator and under-coverage of the associated confidence interval. Previously, we have proposed a novel shrinkage estimator to address this “winner’s curse.” We compare the performance of our shrinkage estimator to the usual unbiased estimator in terms of the root mean squared error, the coverage and the bias of the magnitude. We find superior performance of the shrinkage estimator both conditionally and unconditionally on statistical significance.</p></blockquote>
<p>Let me just repeat that last sentence:</p>
<blockquote><p>We find superior performance of the shrinkage estimator both conditionally and unconditionally on statistical significance.</p></blockquote>
<p>From a Bayesian standpoint, this is no surprise.  Bayes is optimal if you average over the prior distribution and can be reasonable if averaging over something close to the prior.  Especially reasonable in comparison to naive unregularized estimates (<a href="https://statmodeling.stat.columbia.edu/2017/07/20/nobel-prize-winning-economist-become-victim-bog-standard-selection-bias/">as here</a>).</p>
<p>Erik summarizes:</p>
<blockquote><p>We’ve determined how much we gain (on average over the Cochrane Database) by using our shrinkage estimator. It turns out to be about a factor 2 more efficient (in terms of the MSE) than the unbiased estimator. That’s roughly like doubling the sample size! We’re using similar methods as our forthcoming paper about meta-analysis with a single trial.</p></blockquote>
<p>People sometimes ask me how I’ve changed as a statistician over the years.  One answer I’ve given is that I’ve gradually become more Bayesian.  I started out as a skeptic, concerned about Bayesian methods at all; then in grad school I started using Bayesian statistics in applications and realizing it could solve some problems for me; when writing BDA and ARM, still having the Bayesian <a href="https://statmodeling.stat.columbia.edu/2021/09/15/the-bayesian-cringe/">cringe</a> and using flat priors as much as possible, or not talking about priors at all; then with <a rel="nofollow" href="http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf">Aleks</a>, <a rel="nofollow" href="http://www.stat.columbia.edu/~gelman/research/published/chung_cov_matrices.pdf">Sophia</a>, and others moving toward weakly informative priors; eventually under the influence of <a href="https://statmodeling.stat.columbia.edu/2021/07/19/default-informative-priors-for-effect-sizes-where-do-they-come-from/">Erik</a> and others trying to use direct prior information.  At this point I’ve pretty much gone full <a href="https://statmodeling.stat.columbia.edu/2013/12/18/memoriam-dennis-lindley/">Lindley</a>.</p>
<p>Just as a comparison to where my colleagues and I are now, check out <a href="https://statmodeling.stat.columbia.edu/2008/01/24/specifying_a_pr/">my response in 2008</a> to a question from Sanjay Kaul about how to specify a prior distribution for a clinical trial.  I wrote:</p>
<blockquote><p>I suppose the best prior distribution would be based on a multilevel model (whether implicit or explicit) based on other, similar experiments. A noninformative prior could be ok but I prefer something weakly informative to avoid your inferences being unduly affected by extremely unrealistic possibilities in the tail of the distribuiton.</p></blockquote>
<p>Nothing wrong with this advice, exactly, but I was still leaning in the direction of noninformativeness in a way that I would not anymore. Sander Greenland <a href="https://statmodeling.stat.columbia.edu/2008/02/05/specifying_a_pr_1/">replied at the time</a> with a recommendation to use direct prior information.  (And, just for fun, <a href="https://statmodeling.stat.columbia.edu/2014/08/11/discussion-sander-greenland-posterior-predictive-checks/">here’s a discussion</a> from 2014 on a topic where Sander and I disagree.)</p>
<p>Erik concludes:</p>
<blockquote><p>I really think it’s kind of irresponsible now <em>not</em> to use the information from all those thousands of medical trials that came before. Is that very radical?</p></blockquote>
<p>That last question reminds me of our paper from 2008, <a rel="nofollow" href="http://www.stat.columbia.edu/~gelman/research/published/radical.pdf">Bayes: Radical, Liberal, or Conservative?</a></p>
<p><strong>P.S.</strong>  Also this:</p>
<p><img decoding="async" src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2023/12/Screenshot-2023-12-22-at-16.59.23.png" alt="" width="550"></p>
<p>You can <a rel="nofollow" href="https://evidence.nejm.org/doi/pdf/10.1056/EVIDoa2300003">click through</a> to see the whole story.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2,300-year-old mosaic made of shells and coral found under Rome (111 pts)]]></title>
            <link>https://www.cnn.com/style/rome-palatine-hill-archaeology-discovery/index.html</link>
            <guid>38744296</guid>
            <pubDate>Sat, 23 Dec 2023 14:07:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/style/rome-palatine-hill-archaeology-discovery/index.html">https://www.cnn.com/style/rome-palatine-hill-archaeology-discovery/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=38744296">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main" data-reorderable="main">  <article data-uri="cms.cnn.com/_components/article/instances/clqdvli4s001h45p326t9fdag@published" role="main" data-unselectable="true">
      
  <section data-tabcontent="Content">
    <main>
        
        
            <div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p><cite>
      <span data-editable="location">Rome, Italy</span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvli4s001g45p3870xdbbw@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      A five-year dig into the side of <a href="https://www.cnn.com/travel/destinations/rome">Rome</a>’s Palatine Hill yielded treasure last week when archaeologists discovered a deluxe banquet room dating from around the first or second century BC, featuring a sizable, intact and brightly colored wall mosaic.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzf00003b6hbpw2o38x@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Estimated to be around 2,300 years old, the work is part of a larger aristocratic mansion, located near the Roman Forum, that has been under excavation since 2018.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzf00013b6hzyg5qnmh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Almost five meters long (16.4 ft) and featuring depictions of vines, lotus leaves, tridents, trumpets, helmets and mythological marine creatures, the mosaic scene was painstakingly created using mother of pearl, shells, corals, shards of precious glass and flecks of marble. The piece is framed by polychrome crystals, spongy travertine, and exotic, ancient Egyptian blue tiles.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzf00023b6howh8r8ie@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      What makes this discovery “unmatched,” said archaeologist Alfonsina Russo, head of the <a href="https://colosseo.it/en/" target="_blank">Colosseum Archaeological Park</a> in charge of the site, is not only the incredible conservation of the mosaic, but its decoration which also features celebratory scenes of naval and land battles likely funded — and won — by an extremely wealthy aristocratic patron who commemorated the victories on their walls.
  </p>

<div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/clqdw2fcp00153b6hfaqmie0q@published" data-name="EAM01574.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6664864864864865" data-original-height="1233" data-original-width="1850" data-url="https://media.cnn.com/api/v1/images/stellar/prod/eam01574.jpg?c=original" data-editable="settings">
       <picture><img src="https://media.cnn.com/api/v1/images/stellar/prod/eam01574.jpg?c=original" alt="Being buried under the earth on Rome's Palatine Hill has protected this fragile mosaic from the elements for centuries." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1233" width="1850" loading="lazy"></picture>
    </div>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzf00033b6hkdcnclbn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      The intricacy of the mosaic’s depictions of victory have surprised the team working on the project. They show a coastal walled town with lookout towers and loggias — which Russo said could be an ideal or a real-life location — sitting atop a cliff designed with pieces of travertine rock. Scenes&nbsp;of sailing ships with raised sails also feature, alongside depictions of mythical sea monsters swallowing enemy fleets.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzf00043b6h7y4mp3gh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Archaeologists are trying to ascertain whether the delicate — and expensive, for the time — coral branches used in the display came from the Mediterranean or the Red Sea&nbsp;(the nearest and most common oceans used by Romans to extract materials). A rare bluish glass paste also featured in the design likely came from the ancient Egyptian city of Alexandria, the team believe.
  </p>

  


  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzf00053b6hqfcw68np@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      “This banquet hall, which measures 25 square meters (270 square foot), is just one space within a ‘domus’ (the latin word for house) spread on several floors,” Russo told CNN in an interview. “In ancient times, when powerful noble families inhabited the Palatine Hill, it was customary to use rich decorative elements as a symbol to show-off opulence and high social rank.”
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzf00063b6h94enyqz7@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      The chamber, deemed a “jewel” by Russo, was an outdoor banquet hall overlooking a garden, likely used during summer to entertain guests.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzf00073b6hi10shhgr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Such an elaborate space would also have been used to impress guests with water games, which were very popular amongst nobility at the time. “We have found lead pipes embedded within the decorated walls, built to carry water inside basins or to make fountains spout to create water games,” said Russo.
  </p>

<div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/clqdw2ln300173b6h6toqhzmo@published" data-name="EAM01707.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6664864864864865" data-original-height="1233" data-original-width="1850" data-url="https://media.cnn.com/api/v1/images/stellar/prod/eam01707.jpg?c=original" data-editable="settings">
       <picture><img src="https://media.cnn.com/api/v1/images/stellar/prod/eam01707.jpg?c=original" alt="Work continues to uncover more of the secrets of the large home, which could have once belonged to a Roman senator." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1233" width="1850" loading="lazy"></picture>
    </div>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzg00083b6h9tan2hc7@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Marco Rossi, professor of Roman antiquities and head of the mosaic lab at Rome’s Università degli Studi di Roma Tre pointed out that these summer banquet rooms were not only somewhere that hosts and guests would go to relax but also used by the mansion owner as a signifier of their wealth and rank.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzg00093b6h6tepmbi7@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      “Mosaics are usually found on floors, but this runs across the entire front wall and has been incredibly well-preserved,” said Rossi of the piece. “It’s not been ruined by the weight of debris — as can happen to some mosaics on the ground — and despite being delicate, it hasn’t so much as chipped across the centuries.”
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzg000a3b6hvdmhzpo5@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      The discovery of an entire wall mosaic is extremely rare, Rossi added, not least because these pieces are more delicate than those for the floor which were designed to be walked upon and withstand pressure.
  </p>

  


  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzg000b3b6h7zzqirly@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      The location of the grand home has also helped the wall’s preservation, scientists believe. Positioned on the side of Rome’s famous Palatine Hill and subsequently covered over by centuries of mud and earth as the land has moved, the structure and treasures within it have been protected from the air and light by layers of ground.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzg000c3b6hbj62hhhh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      While this new discovery still has a lot of secrets to reveal — why the property was abandoned and how long ago, for example — Russo believes there is one mystery archaeologists could perhaps solve: The identity of its owner, likely a Roman senator.
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzg000d3b6h9sfftxvp@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      “The person was so rich they could afford to import such precious elements from across the empire to decorate this mansion,” Russo said. “We have found nothing so far to shed light on their identity, but we believe more research might enable us to pinpoint the noble family.”
  </p>

  <p data-uri="cms.cnn.com/_components/paragraph/instances/clqdvqlzg000e3b6hejg8ojkf@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
      Russo and her team aim to open the space to the public in early January. “We (will) continue to dig the other layers and areas of this evocative place (to try to discover more),” she said. “It is really an incredible display of Roman luxury.”
  </p>

                </div>
    </main>
  </section>
</article>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding every byte in a WASM module (166 pts)]]></title>
            <link>https://danielmangum.com/posts/every-byte-wasm-module/</link>
            <guid>38744168</guid>
            <pubDate>Sat, 23 Dec 2023 13:41:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danielmangum.com/posts/every-byte-wasm-module/">https://danielmangum.com/posts/every-byte-wasm-module/</a>, See on <a href="https://news.ycombinator.com/item?id=38744168">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <p>In <a href="https://danielmangum.com/posts/wasm-wasi-clang-17/">my last post</a>, we
explored how to build <a href="https://webassembly.github.io/spec/core/">WASM</a> modules,
both with and without <a href="https://wasi.dev/">WASI</a> support, using
<a href="https://clang.llvm.org/">Clang</a>. In a <a href="https://www.reddit.com/r/WebAssembly/comments/18m6xjr/comment/ke2f5xy/">comment on
Reddit</a>,
it was mentioned that much of the setup I walked through in that post could be
avoided by just leveraging <a href="https://www.reddit.com/r/WebAssembly/comments/18m6xjr/comment/ke2f5xy/">Zig’s WASI
supprt</a>.
This is a great point, and I would recommend doing the same. The following
command is inarguably simpler than what I described.</p>
<div><pre tabindex="0"><code data-lang="console"><span><span>$ zig cc --target=wasm32-wasi
</span></span></code></pre></div><p>However, there are two reasons why knowing how to use Clang for compilation is
useful. First, and most practical, is that I am working on a codebase that uses
Clang for its compiler toolchain, so leveraging Zig is not currently an option.
Second is that understanding the, admittedly more involved, Clang incantations
taught us a little more about what actually goes into a WASM module, and how
that changes when using WASI. In order to know <em>exactly</em> what is inside a WASM
module, we need to crack it open though. That is what we are going to do today!</p>
<p>As a recap, one of the programs we compiled was a simple <code>add()</code> function, which
accepted two integers and returned their sum.</p>
<p><code>wasm32_args.c</code></p>
<div><pre tabindex="0"><code data-lang="c"><span><span><span>int</span> add(<span>int</span> a, <span>int</span> b) {
</span></span><span><span>  <span>return</span> a+b;
</span></span><span><span>}
</span></span></code></pre></div><p>We compiled it to a WASM module using the following command.</p>
<div><pre tabindex="0"><code data-lang="console"><span><span>$ /usr/lib/llvm-17/bin/clang -target wasm32 -nostdlib -Wl,--no-entry -Wl,--export-all  -o wasm32_args.wasm wasm32_args.c
</span></span></code></pre></div><p>This produced a binary file which can be recognized as a v1 WASM module.</p>
<div><pre tabindex="0"><code data-lang="console"><span><span>$ file wasm32_args.wasm 
</span></span><span><span>wasm32_args.wasm: WebAssembly (wasm) binary module version 0x1 (MVP)
</span></span></code></pre></div><p>We can view the hex contents of the file using <code>xxd</code>.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000000: 0061 736d 0100 0000 010a 0260 0000 6002  .asm.......`..`.
</span></span><span><span>00000010: 7f7f 017f 0303 0200 0105 0301 0002 063f  ...............?
</span></span><span><span>00000020: 0a7f 0141 8088 040b 7f00 4180 080b 7f00  ...A......A.....
</span></span><span><span>00000030: 4180 080b 7f00 4180 080b 7f00 4180 8804  A.....A.....A...
</span></span><span><span>00000040: 0b7f 0041 8008 0b7f 0041 8088 040b 7f00  ...A.....A......
</span></span><span><span>00000050: 4180 8008 0b7f 0041 000b 7f00 4101 0b07  A......A....A...
</span></span><span><span>00000060: a701 0c06 6d65 6d6f 7279 0200 115f 5f77  ....memory...__w
</span></span><span><span>00000070: 6173 6d5f 6361 6c6c 5f63 746f 7273 0000  asm_call_ctors..
</span></span><span><span>00000080: 0361 6464 0001 0c5f 5f64 736f 5f68 616e  .add...__dso_han
</span></span><span><span>00000090: 646c 6503 010a 5f5f 6461 7461 5f65 6e64  dle...__data_end
</span></span><span><span>000000a0: 0302 0b5f 5f73 7461 636b 5f6c 6f77 0303  ...__stack_low..
</span></span><span><span>000000b0: 0c5f 5f73 7461 636b 5f68 6967 6803 040d  .__stack_high...
</span></span><span><span>000000c0: 5f5f 676c 6f62 616c 5f62 6173 6503 050b  __global_base...
</span></span><span><span>000000d0: 5f5f 6865 6170 5f62 6173 6503 060a 5f5f  __heap_base...__
</span></span><span><span>000000e0: 6865 6170 5f65 6e64 0307 0d5f 5f6d 656d  heap_end...__mem
</span></span><span><span>000000f0: 6f72 795f 6261 7365 0308 0c5f 5f74 6162  ory_base...__tab
</span></span><span><span>00000100: 6c65 5f62 6173 6503 090a 4202 0200 0b3d  le_base...B....=
</span></span><span><span>00000110: 0106 7f23 8080 8080 0021 0241 1021 0320  ...#.....!.A.!. 
</span></span><span><span>00000120: 0220 036b 2104 2004 2000 3602 0c20 0420  . .k!. . .6.. . 
</span></span><span><span>00000130: 0136 0208 2004 2802 0c21 0520 0428 0208  .6.. .(..!. .(..
</span></span><span><span>00000140: 2106 2005 2006 6a21 0720 070f 0b00 3404  !. . .j!. ....4.
</span></span><span><span>00000150: 6e61 6d65 0119 0200 115f 5f77 6173 6d5f  name.....__wasm_
</span></span><span><span>00000160: 6361 6c6c 5f63 746f 7273 0103 6164 6407  call_ctors..add.
</span></span><span><span>00000170: 1201 000f 5f5f 7374 6163 6b5f 706f 696e  ....__stack_poin
</span></span><span><span>00000180: 7465 7200 6609 7072 6f64 7563 6572 7301  ter.f.producers.
</span></span><span><span>00000190: 0c70 726f 6365 7373 6564 2d62 7901 0c55  .processed-by..U
</span></span><span><span>000001a0: 6275 6e74 7520 636c 616e 673f 3137 2e30  buntu clang?17.0
</span></span><span><span>000001b0: 2e36 2028 2b2b 3230 3233 3132 3039 3132  .6 (++2023120912
</span></span><span><span>000001c0: 3432 3237 2b36 3030 3937 3038 6234 3336  4227+6009708b436
</span></span><span><span>000001d0: 372d 317e 6578 7031 7e32 3032 3331 3230  7-1~exp1~2023120
</span></span><span><span>000001e0: 3931 3234 3333 362e 3737 2900 2c0f 7461  9124336.77).,.ta
</span></span><span><span>000001f0: 7267 6574 5f66 6561 7475 7265 7302 2b0f  rget_features.+.
</span></span><span><span>00000200: 6d75 7461 626c 652d 676c 6f62 616c 732b  mutable-globals+
</span></span><span><span>00000210: 0873 6967 6e2d 6578 74                   .sign-ext
</span></span></code></pre></div><p>As described in the Binary Format portion of the <a href="https://webassembly.github.io/spec/core/_download/WebAssembly.pdf">WASM
specification</a>,
each module is made up of sections. Each section begins with a 1-byte
identifier.</p>
<table>
<thead>
<tr>
<th>ID (Decimal)</th>
<th>ID (Hex)</th>
<th>Section</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0x00</td>
<td>Custom</td>
</tr>
<tr>
<td>1</td>
<td>0x01</td>
<td>Type</td>
</tr>
<tr>
<td>2</td>
<td>0x02</td>
<td>Import</td>
</tr>
<tr>
<td>3</td>
<td>0x03</td>
<td>Function</td>
</tr>
<tr>
<td>4</td>
<td>0x04</td>
<td>Table</td>
</tr>
<tr>
<td>5</td>
<td>0x05</td>
<td>Memory</td>
</tr>
<tr>
<td>6</td>
<td>0x06</td>
<td>Global</td>
</tr>
<tr>
<td>7</td>
<td>0x07</td>
<td>Export</td>
</tr>
<tr>
<td>8</td>
<td>0x08</td>
<td>Start</td>
</tr>
<tr>
<td>9</td>
<td>0x09</td>
<td>Element</td>
</tr>
<tr>
<td>10</td>
<td>0x0a</td>
<td>Code</td>
</tr>
<tr>
<td>11</td>
<td>0x0b</td>
<td>Data</td>
</tr>
<tr>
<td>12</td>
<td>0x0c</td>
<td>Data Count</td>
</tr>
</tbody>
</table>
<p>Each section must be present at most once, and they must be provided in-order,
with the exception being Custom sections, for which there may be an arbitrary
number and they may be present anywhere in the file. Every section begins with
its identifier, then an <a href="https://en.wikipedia.org/wiki/LEB128">LEB128</a>
variable-length encoded <code>u32</code> size, followed by the contents of the section. In fact,
all integers in a WASM module are encoded using LEB128.</p>
<hr>
<p><strong>Decoding LEB128 Integers</strong></p>
<p>LEB128 can be used to encode signed and unsigned integers of arbitrary length.
We will primarily be focused on <code>u32</code> (unsigned 32-bit) integers today, so we’ll
skip detailing how to decode signed integers. You can find more details on the
previously linked Wikipedia page.</p>
<p>The algorithm for decoding unsigned integers is as follows:</p>
<ol>
<li>Take the least significant (lower) 7 bits of the next byte.</li>
<li>Binary shift the 7 bits to the left by 7 multiplied by the byte number
(initially 0) and bitwise <code>OR</code> with previously decoded bits.</li>
<li>If the most significant bit (i.e. the 8th bit) is a <code>0</code>, stop decoding.
Otherwise, go to step (1).</li>
</ol>
<p>As an example, if we had the byte sequence <code>a6 03</code>, we would decode it using the
following steps.</p>
<p>Take first byte and convert hex to binary.</p>
<p>Take least significant 7 bits.</p>
<p>Shift bits left by 0 (this is the “0th” byte, <code>7*0 = 0</code>) and <code>OR</code> with
previously decoded bits (none decoded yet).</p>
<p>Observe that the 8th bit in <code>0xa6</code> is a <code>1</code>, so continue to the next byte.</p>
<p>Take least significant 7 bits.</p>
<p>Shift bits left by 7 (this is the “1st” byte, <code>7*1 = 7</code>) and <code>OR</code> with
previously decoded bits.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>0000011 -&gt; 0000011 0000000
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="fallback"><span><span>0000000 0100110 | 0000011 0000000 = 0000011 0100110
</span></span></code></pre></div><p>Observe that the 8th bit in <code>0x03</code> is <code>0</code>. We are done. Convert the final result
to decimal.</p>
<hr>
<p>Now that we know how to interpret integers, let’s start breaking down the
sections.</p>
<h2 id="preamble">
  Preamble
  <a href="#preamble">
    
    <span>Link to heading</span>
  </a>
</h2>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000000: 0061 736d 0100 0000 .... .... .... ....  .asm.......`..`.
</span></span></code></pre></div><p>Before the first section is the “preamble”, which is how <code>file</code> was able to
recognize that our binary was a v1 WASM module. The first 4 bytes decode to
<code>\0asm</code>, with the next 4 bytes indicating the version. WASM is
<a href="https://en.wikipedia.org/wiki/Endianness">little-endian</a>, meaning that the
least significant byte is first. Therefore, the version number is 1.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>0100 0000 -&gt; 0000 0001 -&gt; 1
</span></span></code></pre></div><p>The first section begins following the preamble.</p>
<h2 id="type-section">
  Type Section
  <a href="#type-section">
    
    <span>Link to heading</span>
  </a>
</h2>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000000: .... .... .... .... 010a 0260 0000 6002  .asm.......`..`.
</span></span><span><span>00000010: 7f7f 017f .... .... .... .... .... ....  ...............?
</span></span></code></pre></div><p>We can identify the first section as the Type section, as indicated by the first
byte <code>01</code>. Following the identifier is the size, to which we can apply our LEB128
decoding.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>0x0a -&gt; 00001010 -&gt; 0001010 -&gt; 10
</span></span></code></pre></div><p>This informs us that the contents of this section should be 10 bytes in size.
The WASM specification describes the Type section contents as a <code>vec</code> of
<code>functype</code>. <em>Vectors</em> are simply an LEB128 encoded <code>u32</code> length followed by a
sequence of their specified element type. The first byte is <code>02</code>, which by now
you can probably recognize as <code>2</code> without needing to actually perform LEB128
decoding. This means that we should see 2 <code>functype</code> elements next.</p>
<p>Function types are prefixed with <code>0x60</code>, then two <code>vec</code>, one for parameter
types, and one for return types, follows. We have <code>00 00</code> for the first
<code>functype</code>. Remembering that <code>vec</code> are prefixed with length. This is essentially
saying that our first function takes 0 parameters and returns 0 values. We can
use <a href="https://webassembly.github.io/wabt/doc/wasm2wat.1.html">the <code>wasm2wat</code>
tool</a> to verify.</p>
<div><pre tabindex="0"><code data-lang="console"><span><span>$ wasm2wat --enable-annotations wasm32_args.wasm | head -2 | tail -1
</span></span><span><span>  (type (;0;) (func))
</span></span></code></pre></div><p>The next <code>functype</code> does have parameter and result types. There are two
parameters (<code>0x02</code>), each encoded as <code>0x7f</code>, which corresponds to a signed
32-bit integer (<code>i32</code>). There is one return type (<code>0x01</code>), which is also an
<code>i32</code> (<code>0x7f</code>). Though we don’t have symbol information about this function yet,
given that it is the last one defined we can safely assume this is our <code>add()</code>.</p>
<div><pre tabindex="0"><code data-lang="console"><span><span>$ wasm2wat --enable-annotations wasm32_args.wasm | head -3 | tail -1
</span></span><span><span>  (type (;1;) (func (param i32 i32) (result i32)))
</span></span></code></pre></div><h2 id="function-section">
  Function Section
  <a href="#function-section">
    
    <span>Link to heading</span>
  </a>
</h2>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000010: .... .... 0303 0200 01.. .... .... ....  ...............?
</span></span></code></pre></div><p>Next is the Function section (<code>0x03</code>). We do not have an Import section (<code>0x02</code>)
in this module because we did not import any symbols. Because sections must be
provided in-order, we can be certain that no Import section will be provided now
that we have seen the Function section.</p>
<p>The size of this section is <code>3</code> (<code>0x03</code>), and the contents are specified as a
<code>vec</code> of <code>typeidx</code> (type index). A type index is a <code>u32</code>, which will once again
be LEB128 encoded. Our <code>vec</code> begins with <code>0x02</code>, so we should expect two type
indices. The following two bytes, <code>0x00</code> and <code>0x01</code>, correspond to the entries
in our previously detailed Types section.</p>
<p>At this point, we have two functions, each with their own type signature. For
this specific module, the Type and Function sections may seem redundant.
However, consider if we had another function in our module.</p>
<div><pre tabindex="0"><code data-lang="c"><span><span><span>int</span> multiply (<span>int</span> a, <span>int</span> b) {
</span></span><span><span>  <span>return</span> a*b
</span></span><span><span>}
</span></span></code></pre></div><p><code>multiply()</code> has the same function signature as <code>add()</code>, meaning that we would
have only one entry for <code>(i32, i32) i32</code> in the Type section, then two entries
in the Function section that referenced the type signature by the corresponding
index. Compiling the new program with <code>multiply()</code> added results in an
indentical preamble and Type section, but we can see that the Function section
(<code>0x03</code>) now has length <code>4</code> (<code>0x04</code>), with a type index <code>vec</code> of length <code>3</code>
(<code>0x03</code>), and three type index entries (<code>0x00</code>, <code>0x01</code>, <code>0x01</code>) with the latter
two referring to the same type signature.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000000: 0061 736d 0100 0000 010a 0260 0000 6002
</span></span><span><span>00000010: 7f7f 017f 0304 0300 0101 .... .... ....
</span></span></code></pre></div><h2 id="memory-section">
  Memory Section
  <a href="#memory-section">
    
    <span>Link to heading</span>
  </a>
</h2>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000010: .... .... .... .... ..05 0301 0002 ....  ...............?
</span></span></code></pre></div><p>We once again skip a section that is not present in our module (Table with ID
<code>0x04</code>), and move on to Memory (<code>0x05</code>). The Memory section contains a <code>vec</code> of
memories (<code>mem</code>), which are made up of <code>limits</code>. In the current WASM
specification, only one memory may be defined. Our Memory section has size of
<code>3</code> bytes (<code>0x03</code>), and, as expected, the first byte (<code>0x01</code>) specifies that the
length of the <code>vec</code> is <code>1</code>. A <code>limit</code> can include both a maximum and a minimum
size (both LEB128 encoded <code>u32</code>), as indicated by the first byte. In our case,
the first byte is <code>0x00</code>, which means that only a minimum size will be defined,
and the maximum is free to grow to any size. If the first byte was <code>0x01</code>, both
a minimum and a maximum would be defined.</p>
<blockquote>
<p>The <a href="https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md#spec-changes">threads
proposal</a>
extends <code>limits</code> to be allow specifying whether memory is shared or unshared.</p>
</blockquote>
<p>In our module, the minimum memory size is <code>2</code> (<code>0x02</code>).</p>
<h2 id="global-section">
  Global Section
  <a href="#global-section">
    
    <span>Link to heading</span>
  </a>
</h2>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000010: .... .... .... .... .... .... .... 063f  ...............?
</span></span><span><span>00000020: 0a7f 0141 8088 040b 7f00 4180 080b 7f00  ...A......A.....
</span></span><span><span>00000030: 4180 080b 7f00 4180 080b 7f00 4180 8804  A.....A.....A...
</span></span><span><span>00000040: 0b7f 0041 8008 0b7f 0041 8088 040b 7f00  ...A.....A......
</span></span><span><span>00000050: 4180 8008 0b7f 0041 000b 7f00 4101 0b..  A......A....A...
</span></span></code></pre></div><p>The Global section (<code>0x06</code>) is next, with a size of <code>0x3f</code>. With a larger size,
we can quickly check our LEB128 decoding to ensure that we don’t need to
consider subsequent bytes.</p>
<p>The 8th bit is <code>0</code>, so we can simply convert to the decimal value of <code>63</code> for
our size. The section includes a <code>vec</code> of globals (<code>global</code>), where each
<code>global</code> consists of a type (<code>globaltype</code>) and expression (<code>expr</code>). A
<code>globaltype</code> is made up of a value type (<code>valtype</code>) and a 1-byte flag (<code>mut</code>)
indicating whether the value is mutable or not.</p>
<p>An expression is encoded by a sequence of instructions (<code>instr</code>) with a
terminating byte (<code>0x0b</code>) specifying the end of the sequence. The byte following
the section size, <code>0x0a</code>, informs us that 10 globals will be defined in the
<code>vec</code>. We can easily
extract the first one by looking for the first instance of <code>0x0b</code>.</p>
<p>The <code>0x7f</code> should be familiar at this point as an <code>i32</code>, which is the
<code>globaltype</code> of this <code>global</code>. The following byte, <code>0x01</code>, marks it as mutable
(<code>mut</code>).</p>
<p>This is followed by the initialization expression, which includes the first
instruction we have seen. <code>0x41</code> is the opcode for <code>i32.const</code>, which simply
returns a static <code>i32</code> constant, which is specified by the following bytes.
We’ll need to use our LEB128 decoding to interpret it.</p>
<p>Take the first byte.</p>
<p>Take the least significant 7 bits and shift left 0 bits.</p>
<p>The 8th bit is a <code>1</code> so take the next byte.</p>
<p>Take the least significant 7 bits and shift left 7 bits.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>10001000 -&gt; 0001000 0000000
</span></span></code></pre></div><p><code>OR</code> with previously decoded bits.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>0000000 0000000 | 0001000 0000000 = 0001000 0000000
</span></span></code></pre></div><p>The 8th bit is a <code>1</code> so take the next byte.</p>
<p>Take the least significant 7 bits and shift left 14 bits.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000100 -&gt; 0000100 0000000 0000000
</span></span></code></pre></div><p><code>OR</code> with previously decoded bits.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>0000000 0001000 0000000 | 0000100 0000000 0000000 = 0000100 0001000 0000000 
</span></span></code></pre></div><p>The 8th bit is a <code>0</code> so we are done.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>0000100 0001000 0000000 -&gt; 66560
</span></span></code></pre></div><p>We can verify our decoding using <code>wasm2wat</code> again.</p>
<blockquote>
<p>The <code>$__stack_pointer</code> symbol name will be found in a later section.</p>
</blockquote>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>$ wasm2wat --enable-annotations wasm32_args.wasm | head -34 | tail -1
</span></span><span><span>  (global $__stack_pointer (mut i32) (i32.const 66560))
</span></span></code></pre></div><p>The same process can be applied to all globals in the <code>vec</code>, as shown in the
textual representation.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>$ wasm2wat --enable-annotations wasm32_args.wasm | head -43 | tail -10
</span></span><span><span>  (global $__stack_pointer (mut i32) (i32.const 66560))
</span></span><span><span>  (global (;1;) i32 (i32.const 1024))
</span></span><span><span>  (global (;2;) i32 (i32.const 1024))
</span></span><span><span>  (global (;3;) i32 (i32.const 1024))
</span></span><span><span>  (global (;4;) i32 (i32.const 66560))
</span></span><span><span>  (global (;5;) i32 (i32.const 1024))
</span></span><span><span>  (global (;6;) i32 (i32.const 66560))
</span></span><span><span>  (global (;7;) i32 (i32.const 131072))
</span></span><span><span>  (global (;8;) i32 (i32.const 0))
</span></span><span><span>  (global (;9;) i32 (i32.const 1))
</span></span></code></pre></div><h2 id="export-section">
  Export Section
  <a href="#export-section">
    
    <span>Link to heading</span>
  </a>
</h2>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000050: .... .... .... .... .... .... .... ..07  A......A....A...
</span></span><span><span>00000060: a701 0c06 6d65 6d6f 7279 0200 115f 5f77  ....memory...__w
</span></span><span><span>00000070: 6173 6d5f 6361 6c6c 5f63 746f 7273 0000  asm_call_ctors..
</span></span><span><span>00000080: 0361 6464 0001 0c5f 5f64 736f 5f68 616e  .add...__dso_han
</span></span><span><span>00000090: 646c 6503 010a 5f5f 6461 7461 5f65 6e64  dle...__data_end
</span></span><span><span>000000a0: 0302 0b5f 5f73 7461 636b 5f6c 6f77 0303  ...__stack_low..
</span></span><span><span>000000b0: 0c5f 5f73 7461 636b 5f68 6967 6803 040d  .__stack_high...
</span></span><span><span>000000c0: 5f5f 676c 6f62 616c 5f62 6173 6503 050b  __global_base...
</span></span><span><span>000000d0: 5f5f 6865 6170 5f62 6173 6503 060a 5f5f  __heap_base...__
</span></span><span><span>000000e0: 6865 6170 5f65 6e64 0307 0d5f 5f6d 656d  heap_end...__mem
</span></span><span><span>000000f0: 6f72 795f 6261 7365 0308 0c5f 5f74 6162  ory_base...__tab
</span></span><span><span>00000100: 6c65 5f62 6173 6503 09.. .... .... ....  le_base...B....=
</span></span></code></pre></div><p>The Export section (<code>0x07</code>) consists of a <code>vec</code> of <code>export</code>, with each
containing a <code>name</code> and an export description (<code>exportdesc</code>). The <code>name</code> is a
<code>vec</code> of <code>byte</code>, while the <code>exportdesc</code> contains a 1-byte prefix indicating the
type of export, followed by an index to the appropriate section where the export
is defined.</p>
<p>I’ll leave it to the reader to LEB128 decode <code>0xa701</code> as the section size (<code>167</code>
bytes). The first byte following the section size, <code>0x0c</code>, indicates that the
<code>vec</code> will contain 12 exports. The next byte, <code>0x06</code>, is the first byte of the
first export, and thus is defining the length of its name as <code>6</code>. The following
6 bytes can be converted to <a href="https://en.wikipedia.org/wiki/UTF-8">UTF-8
characters</a>.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>6d 65 6d 6f 72 79 -&gt; memory
</span></span></code></pre></div><p>Because we compiled with <code>-Wl,--export-all</code>, all symbols will be exported. In
this case, <code>memory</code> is referring to the first element in the <code>vec</code> in our Memory
section. The export type prefixes are defined as follows.</p>
<table>
<thead>
<tr>
<th>ID (Hex)</th>
<th>Export</th>
</tr>
</thead>
<tbody>
<tr>
<td>0x00</td>
<td>Function</td>
</tr>
<tr>
<td>0x01</td>
<td>Table</td>
</tr>
<tr>
<td>0x02</td>
<td>Memory</td>
</tr>
<tr>
<td>0x03</td>
<td>Global</td>
</tr>
</tbody>
</table>
<p>As expected, the prefix following <code>memory</code> is <code>0x02</code>. The next byte <code>0x00</code>,
specifies that this export corresponds to the first memory in the <code>vec</code>. The
next two exports are our functions. The first is <code>__wasm_call_ctors</code>, which,
following the name definition, has a function prefix (<code>0x00</code>) and an index to
the first function in the Function section <code>vec</code> (<code>0x00</code>).</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>11 5f 5f 77 61 73 6d 5f 63 61 6c 6c 5f 63 74 6f 72 73 -&gt; __wasm_call_ctors
</span></span></code></pre></div><p>The second correponds out our <code>add()</code> function, and refers to the second
(<code>0x01</code>) function (<code>0x00</code>) in the Function section.</p>
<p>The remaining exports are shown in their textual representation below.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>$ wasm2wat --enable-annotations wasm32_args.wasm | head -55 | tail -12
</span></span><span><span>  (export "memory" (memory 0))
</span></span><span><span>  (export "__wasm_call_ctors" (func $__wasm_call_ctors))
</span></span><span><span>  (export "add" (func $add))
</span></span><span><span>  (export "__dso_handle" (global 1))
</span></span><span><span>  (export "__data_end" (global 2))
</span></span><span><span>  (export "__stack_low" (global 3))
</span></span><span><span>  (export "__stack_high" (global 4))
</span></span><span><span>  (export "__global_base" (global 5))
</span></span><span><span>  (export "__heap_base" (global 6))
</span></span><span><span>  (export "__heap_end" (global 7))
</span></span><span><span>  (export "__memory_base" (global 8))
</span></span><span><span>  (export "__table_base" (global 9))
</span></span></code></pre></div><h2 id="code-section">
  Code Section
  <a href="#code-section">
    
    <span>Link to heading</span>
  </a>
</h2>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000100: .... .... .... .... ..0a 4202 0200 0b3d  le_base...B....=
</span></span><span><span>00000110: 0106 7f23 8080 8080 0021 0241 1021 0320  ...#.....!.A.!. 
</span></span><span><span>00000120: 0220 036b 2104 2004 2000 3602 0c20 0420  . .k!. . .6.. . 
</span></span><span><span>00000130: 0136 0208 2004 2802 0c21 0520 0428 0208  .6.. .(..!. .(..
</span></span><span><span>00000140: 2106 2005 2006 6a21 0720 070f 0b.. ....  !. . .j!. ....4.
</span></span></code></pre></div><p>In this module, we don’t have a Start (<code>0x08</code>) or Element (<code>0x09</code>) section, so
next is the Code section (<code>0x0a</code>). The section size is <code>66</code> (LEB128 encoded
<code>0x42</code>), and it consists of the body and local variables of the each function as
a <code>vec</code> of <code>code</code>. Each <code>code</code> element consists of a size (<code>u32</code>), <code>vec</code> of
<code>locals</code>, and an expression (<code>expr</code>) made up of instructions.</p>
<blockquote>
<p>Keep in mind that we didn’t specify any optimizations when compiling this
module (e.g. <code>-O3</code>), so our code section is going to be much longer than it
needs to be. However, the unoptimized function body gives us an opportunity to
explore more WASM instructions.</p>
</blockquote>
<p>The <code>vec</code> has two elements (<code>0x02</code>), which correspond to the two entries in the
Function section. The first function has a size of <code>2</code> (<code>0x02</code>), which we can
immediately know means that it has no locals or instructions. The <code>vec</code> of
<code>locals</code> has a length of <code>0</code> (<code>0x00</code>) and the <code>expr</code>, which is its sequence of
instructions, is a single <code>0x0b</code> (the expression terminating byte).</p>
<p>The next function, which is our <code>add()</code>, has a size of <code>61</code> (<code>0x3d</code>). Its <code>vec</code>
of <code>locals</code> has length <code>1</code> (<code>0x01</code>). Locals are encoded as a <code>u32</code> count and a
value type (<code>valtype</code>). That is, there is a single element in the <code>vec</code> for each
value type for which at least one local exists. Because our <code>vec</code> has length
<code>1</code>, we know that all <code>locals</code> are the same value type. Specifically, there are
<code>6</code> (<code>0x06</code>) locals of type <code>i32</code> (<code>0x7f</code>).</p>
<p>We’ll save a deep dive into the WASM instruction set architecture (ISA) for a
future post, but the key difference from most ISAs you have likely interacted
with is that WASM operates as a stack machine. Values that are to be used as
operands for an instruction must first be pushed onto the stack, before
subsequently being popped off the stack and used to compute a result, which is
then pushed onto the stack.</p>
<p>The first instruction in our <code>add()</code> function is encoded as <code>0x23</code>, which
corresponds to <code>global.get</code>. This instruction takes the index (<code>u32</code>) of a symbol in
the Global section, but you may notice something strange when LEB128 decoding
it.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>80 80 80 80 00 -&gt; 00000000 00000000 00000000 00000000
</span></span></code></pre></div><p>Why are we using the maximum length for LEB128 encoding an unsigned 32-bit
integer? The reason is related to <a href="https://github.com/WebAssembly/tool-conventions/blob/main/Linking.md#merging-global-sections">Global section merging during
linking</a>.
Though we only need one byte to encode index <code>0</code>, if the index of
<code>$__stack_pointer</code> changes, we can now be certain that the <code>global.get</code>
instruction can be updated without changing the position of any other bytes in
the module.</p>
<p>As previously mentioned, WASM is a stack machine, so <code>global.get</code> is going to
push the value of <code>$__stack_pointer</code> onto the stack.</p>
<p>Our next instruction is <code>0x21</code>, which corresponds to <code>local.set</code>. It pops a
value off the top of the stack, then stores it in the local at the index
specified by the supplied immediate, which in this case is <code>2</code> (<code>0x02</code>).
Combining this instruction with the previous one results in storing the value of
<code>$__stack_pointer</code> in the local at index <code>2</code>. Why use <code>2</code> and not <code>0</code>? In
accordance with the WASM specification, <code>2</code> actually refers to the first
declared local, as parameters are referenced as the first locals.</p>
<blockquote>
<p>The parameters of the function are referenced through 0-based local indices in
the function’s body; they are mutable.</p>
</blockquote>
<p>You may already recognize these first few instructions as part of a <a href="https://danielmangum.com/posts/risc-v-bytes-caller-callee-registers/#an-example">function
prologue</a>
in which we are “growing the stack”. The use of a stack when coming from a
language like C can be confusing given that WASM has its own implicit stack.
Also, in this particular example, it is unnecessary to manage a stack, and, as
you’ll see in a moment, compiling with optimization removes these instructions.
Nevertheless, a “shadow stack” is necessary in some real programs, and we’ll
explore some examples in a future post.</p>
<p>Decompiling the full function body results in the following.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>$ wasm2wat --enable-annotations wasm32_args.wasm | head -32 | tail -28
</span></span><span><span>  (func $add (type 1) (param i32 i32) (result i32)
</span></span><span><span>    (local i32 i32 i32 i32 i32 i32)
</span></span><span><span>    global.get $__stack_pointer
</span></span><span><span>    local.set 2
</span></span><span><span>    i32.const 16
</span></span><span><span>    local.set 3
</span></span><span><span>    local.get 2
</span></span><span><span>    local.get 3
</span></span><span><span>    i32.sub
</span></span><span><span>    local.set 4
</span></span><span><span>    local.get 4
</span></span><span><span>    local.get 0
</span></span><span><span>    i32.store offset=12
</span></span><span><span>    local.get 4
</span></span><span><span>    local.get 1
</span></span><span><span>    i32.store offset=8
</span></span><span><span>    local.get 4
</span></span><span><span>    i32.load offset=12
</span></span><span><span>    local.set 5
</span></span><span><span>    local.get 4
</span></span><span><span>    i32.load offset=8
</span></span><span><span>    local.set 6
</span></span><span><span>    local.get 5
</span></span><span><span>    local.get 6
</span></span><span><span>    i32.add
</span></span><span><span>    local.set 7
</span></span><span><span>    local.get 7
</span></span><span><span>    return)
</span></span></code></pre></div><p>The only essential instructions are accesing the parameters (<code>local.get 0</code> and
<code>local.get 1</code>), and the eventual adding of the values with <code>i32.add</code>. This can
be observed by recompiling with ooptimization, then Decompiling.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>clang -target wasm32 -nostdlib -Wl,--no-entry -Wl,--export-all -O3-o wasm32_args_optimized.wasm wasm32_args.c
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="fallback"><span><span>$ wasm2wat --enable-annotations wasm32_args_optimized.wasm | head -8 | tail -4
</span></span><span><span>  (func $add (type 1) (param i32 i32) (result i32)
</span></span><span><span>    local.get 1
</span></span><span><span>    local.get 0
</span></span><span><span>    i32.add)
</span></span></code></pre></div><h2 id="custom-sections">
  Custom Sections
  <a href="#custom-sections">
    
    <span>Link to heading</span>
  </a>
</h2>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>00000140: .... .... .... .... .... .... ..00 3404  !. . .j!. ....4.
</span></span><span><span>00000150: 6e61 6d65 0119 0200 115f 5f77 6173 6d5f  name.....__wasm_
</span></span><span><span>00000160: 6361 6c6c 5f63 746f 7273 0103 6164 6407  call_ctors..add.
</span></span><span><span>00000170: 1201 000f 5f5f 7374 6163 6b5f 706f 696e  ....__stack_poin
</span></span><span><span>00000180: 7465 7200 6609 7072 6f64 7563 6572 7301  ter.f.producers.
</span></span><span><span>00000190: 0c70 726f 6365 7373 6564 2d62 7901 0c55  .processed-by..U
</span></span><span><span>000001a0: 6275 6e74 7520 636c 616e 673f 3137 2e30  buntu clang?17.0
</span></span><span><span>000001b0: 2e36 2028 2b2b 3230 3233 3132 3039 3132  .6 (++2023120912
</span></span><span><span>000001c0: 3432 3237 2b36 3030 3937 3038 6234 3336  4227+6009708b436
</span></span><span><span>000001d0: 372d 317e 6578 7031 7e32 3032 3331 3230  7-1~exp1~2023120
</span></span><span><span>000001e0: 3931 3234 3333 362e 3737 2900 2c0f 7461  9124336.77).,.ta
</span></span><span><span>000001f0: 7267 6574 5f66 6561 7475 7265 7302 2b0f  rget_features.+.
</span></span><span><span>00000200: 6d75 7461 626c 652d 676c 6f62 616c 732b  mutable-globals+
</span></span><span><span>00000210: 0873 6967 6e2d 6578 74                   .sign-ext
</span></span></code></pre></div><p>The remaining bytes make up three Custom sections (<code>0x00</code>), as there is no Data
(<code>0x0b</code>) section or Data Count (<code>0x0c</code>) section in our module. Custom sections
are mostly unstructured, but do begin with the same <code>u32</code> size as other
sections, followed by a <code>name</code>. The <code>0x00</code> byte identifies our first custom
section, and its size is <code>52</code> bytes (<code>0x34</code>). The <code>name</code> encoding starts with
the number of bytes in the <code>name</code>, which in this case is <code>4</code> (<code>0x04</code>). The
following 4 bytes are UTF-8 characters making up the <code>name</code>.</p>
<p>The <code>name</code> of this custom section happens to literally be “name”. It also
happens to be the only Custom section that is defined in the WASM specification
(see Custom Sections in the Appendix). Like other sections, it should only be
included at most once, and it has the additional requirement of occurring after
the Data section. There are three subsections that may be included in the “name”
Custom section, identified by the following IDs.</p>
<table>
<thead>
<tr>
<th>ID (Hex)</th>
<th>Subsection</th>
</tr>
</thead>
<tbody>
<tr>
<td>0x00</td>
<td>Module name</td>
</tr>
<tr>
<td>0x01</td>
<td>Function names</td>
</tr>
<tr>
<td>0x02</td>
<td>Local names</td>
</tr>
</tbody>
</table>
<p>The first subsection is Function names (<code>0x01</code>), and has size of <code>25</code> (<code>0x19</code>).
It consists of a <code>vec</code> of <code>name</code> / <code>index</code> pairs, otherwise known as a “name
map”, which assigns the provided <code>name</code> to the given <code>index</code> in the Function
section. The <code>vec</code> here is of length <code>2</code> (<code>0x02</code>), and first element corresponds
to the first function in the Function section (<code>0x00</code>). The name assigned to the
function is <code>17</code> bytes long (<code>0x11</code>).</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>5f 5f 77 61 73 6d 5f 63 61 6c 6c 5f 63 74 6f 72 73 -&gt; __wasm_call_ctors
</span></span></code></pre></div><p>The following function name entry can be decoded in the same manner, predictably
assigning <code>add</code> to the second function in the Function section (<code>0x01</code>).</p>
<p>The next and final subsection of the “name” Custom section is not actually
standardized in the core WASM specification, but rather part of the <a href="https://github.com/WebAssembly/extended-name-section/blob/main/proposals/extended-name-section/Overview.md#global-names">Extended
Name Section
proposal</a>.
In the proposal, <code>7</code> (<code>0x07</code>) is the index for the Global names subsection. This
subsection also contains a “name map”, providing pairs of a Global section index
and name. The first entry in our Global section (<code>0x00</code>) is being assigned the
name <code>__stack_pointer</code>.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>5f 5f 73 74 61 63 6b 5f 70 6f 69 6e 74 65 72 -&gt; __stack_pointer
</span></span></code></pre></div><p>The next two Custom sections are defined in the <a href="https://github.com/WebAssembly/tool-conventions/">WASM <code>tool-conventions</code>
repository</a>. The first is
<a href="https://github.com/WebAssembly/tool-conventions/blob/c74267a5897c1bdc9aa60adeaf41816387d3cd12/ProducersSection.md">named
<code>producers</code></a>,
and is meant to denote the tools that were used to produce the WASM module. The
second is <a href="https://github.com/WebAssembly/tool-conventions/blob/c74267a5897c1bdc9aa60adeaf41816387d3cd12/Linking.md#target-features-section">named
<code>target_features</code></a>
and must come after the <code>producers</code> Custom secion when included. It describes
what features are used, and whether linking should fail if a given feature is or
is not in the allowed set.</p>
<p>The full decompilation of all three Custom sections is as follows.</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>$ wasm2wat --enable-annotations wasm32_args.wasm | tail -3
</span></span><span><span>  (@custom "name" "\01\19\02\00\11__wasm_call_ctors\01\03add\07\12\01\00\0f__stack_pointer")
</span></span><span><span>  (@custom "producers" "\01\0cprocessed-by\01\0cUbuntu clang?17.0.6 (++20231209124227+6009708b4367-1~exp1~20231209124336.77)")
</span></span><span><span>  (@custom "target_features" "\02+\0fmutable-globals+\08sign-ext"))
</span></span></code></pre></div><blockquote>
<p>The <code>--enable-annotations</code>, which we have been using for all <code>wasm2wat</code>
invocations, is required to include Custom sections in the WAT output.</p>
</blockquote>
<h2 id="final-thoughts">
  Final Thoughts
  <a href="#final-thoughts">
    
    <span>Link to heading</span>
  </a>
</h2>
<p>The WASM module in this post did not include every section that can occur, but
hopefully this breakdown was thorough enough that you feel confident in
dissecting those that were omitted on your own. There are a number of topics,
such as linking, optimization, and shadow stacks, that were mentioned in this
post but were not covered in depth. Check back for future posts where we will go
into greater detail.</p>
<p>As always, if you have feedback, questions, or just want to chat, feel free to
reach out to <code>@hasheddan</code> on any of the platforms listed on the <a href="https://danielmangum.com/">home
page</a>.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["King of the Cannibals": How Sam Altman Took over Silicon Valley (123 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2023/12/23/sam-altman-openai-peter-thiel-silicon-valley/</link>
            <guid>38744021</guid>
            <pubDate>Sat, 23 Dec 2023 13:13:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2023/12/23/sam-altman-openai-peter-thiel-silicon-valley/">https://www.washingtonpost.com/technology/2023/12/23/sam-altman-openai-peter-thiel-silicon-valley/</a>, See on <a href="https://news.ycombinator.com/item?id=38744021">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2023/12/23/sam-altman-openai-peter-thiel-silicon-valley/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Suno AI (282 pts)]]></title>
            <link>https://www.suno.ai/</link>
            <guid>38743719</guid>
            <pubDate>Sat, 23 Dec 2023 12:24:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.suno.ai/">https://www.suno.ai/</a>, See on <a href="https://news.ycombinator.com/item?id=38743719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><header><div><nav></nav><nav><ul><li><a href="https://app.suno.ai/" target="_blank" rel="noopener noreferrer" aria-label="Go to our web app"><span>Make a song</span></a></li></ul></nav></div></header><main></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let me repeat that back to you (109 pts)]]></title>
            <link>https://roughlywritten.substack.com/p/let-me-repeat-that-back-to-you</link>
            <guid>38743718</guid>
            <pubDate>Sat, 23 Dec 2023 12:24:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roughlywritten.substack.com/p/let-me-repeat-that-back-to-you">https://roughlywritten.substack.com/p/let-me-repeat-that-back-to-you</a>, See on <a href="https://news.ycombinator.com/item?id=38743718">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>One of the most effective communications strategies I use is repeating back, in my own words, what was just explained to me.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png" width="1456" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1619455,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3434df27-4167-4866-a818-27fe9d3baf74_1456x816.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Take this conversation:</p><p><strong>Person A:</strong><span> …explains complex technical or product concept…</span></p><p><strong>Person B: </strong><span>“Alright, got it, I’ll go get to work on that”</span></p><p><span>If you’re an engineer reading this, you probably already see the problem. </span><strong>Person A</strong><span> has no way to know if </span><strong>Person B</strong><span> actually understands what they said. They only know that </span><strong>Person B</strong><span> </span><em>thinks</em><span> they understand. In practice, they’ve often misunderstood something important and find out the painful truth later.</span></p><blockquote><p><strong>Person B</strong><span> needs to confirm their understanding by detailing the point back to </span><strong>Person A</strong><span> and getting that confirmation.</span></p></blockquote><p><strong>Person B</strong><span> needs to confirm their understanding by detailing the point back to </span><strong>Person A</strong><span> and getting that confirmation. This is the second phase of the commit.</span></p><p>Treat it like a checkpoint, rather than a continuation of the discussion. Before you continue the discussion by introducing a new idea, or presenting an opinion, stop, repeat, and confirm. Then you can be sure you’re moving forward on the same page.</p><p>This simple strategy will save you so much pain, and the effects can be immediately seen. The first time you repeat something back to someone, only to have them correct you on something you misunderstood…you know it’s working. If you’re like me, you’ll find that you catch something someone misunderstood in the vast majority of cases.</p><p><span>When airline pilots transfer control of the plane from one pilot to another, each must confirm the transfer to the other. The transfer is not complete until communication of control has happened in </span><em>both</em><span> directions.</span></p><p>In other words, this is not a novel idea, just one that we haven’t fully embraced as default in software engineering.</p><p>Only if you let it be. One easy way to cut through any awkwardness is to simply let the other person know what you’re doing:</p><blockquote><p>“Before we move on, let me repeat that back to you confirm that I am understanding everything”</p></blockquote><p>You’ll often find that people start doing this back to you also, when you’re the one giving the initial explanation.</p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>