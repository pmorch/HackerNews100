<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 04 Oct 2024 15:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[AI at Meta: Movie Gen (316 pts)]]></title>
            <link>https://ai.meta.com/research/movie-gen/?_fb_noscript=1</link>
            <guid>41740965</guid>
            <pubDate>Fri, 04 Oct 2024 13:03:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/research/movie-gen/?_fb_noscript=1">https://ai.meta.com/research/movie-gen/?_fb_noscript=1</a>, See on <a href="https://news.ycombinator.com/item?id=41740965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div id="u_0_5_lx" data-scoped-css="fullscreen-hero-scoped-u_0_0_eV"><p>Introducing research for the most advanced media foundation AI models.</p></div><div><div><p>Movie Gen sets a new standard for immersive AI content</p><div><p>Our latest research breakthroughs demonstrate how you can use simple text inputs to produce custom videos and sounds, edit existing videos or transform your personal image into a unique video.</p></div></div><div><p>Text input summary: A girl is running across a beach and holding a kite. She's wearing jean shorts and a yellow t-shirt. The sun is shining down.</p><p>Text input summary: A woman is sitting on the grass of a pumpkin patch. She is wearing a scarf and holding a cup. The background is filled with rows of pumpkins.</p><p>Text input: Thunder cracks loudly, with an orchestral music track.</p></div></div><div><div><p>Generate videos from text</p><div><p>Produce unique videos from text to create a custom masterpiece. Movie Gen creates long high-definition videos at different aspect ratios—the first of its kind in the industry.</p></div></div><div><p>Text input summary: A sloth with pink sunglasses lays on a donut float in a pool. The sloth is holding a tropical drink. The world is tropical. The sunlight casts a shadow.</p><p>Text input summary: The camera is behind a man. The man is shirtless, wearing a green cloth around his waist. He is barefoot. With a fiery object in each hand, he creates wide circular motions. A calm sea is in the background. The atmosphere is mesmerizing, with the fire dance.</p><p>Text input summary: A fluffy koala bear surfs. It has a grey and white coat and a round nose. The surfboard is yellow. The koala bear is holding onto the surfboard with its paws. The koala bear’s facial expression is focused. The sun is shining.</p></div></div><div><p>Edit video with text</p><div><p>Transform existing videos with text inputs. Movie Gen enables precise video editing—from styles and transitions to fine-grained edits.</p></div></div><div><div><p>Produce personalized videos</p><div><p>Upload an image of yourself and transform it into a personalized video. Movie Gen’s cutting-edge model lets you create personalized videos that preserve human identity and motion.</p></div></div><div><p>Text input summary: A man is doing a scientific experiment in a lab with rainbow wallpaper. The man has a serious expression and is wearing glasses. He is wearing a white lab coat with a pen in the pocket. The man pours liquid into a glass beaker and a cloud of white smoke blooms.</p><p>Text input summary: A woman paints a canvas on an easel, in a wood-paneled room. The woman is wearing a white shirt. She has a calm expression as she concentrates on her work. A baby bear cub stands at her feet. The lighting is cool.</p><p>Text input summary: A woman DJ spins records on a rooftop in LA. She is wearing a pink jacket and giant headphones. There is a cheetah next to the woman. The background is a cityscape.</p></div></div><div><div><p>Create sound effects and soundtracks</p><div><p>Use video and text inputs to generate audio for your videos. Movie Gen allows you to create and extend sound effects, background music or entire soundtracks.</p></div></div><div><p>Text input: Rain pours against the cliff and the person, with music playing in the background.</p><p>Text input: Rustling leaves and snapping twigs, with an orchestral music track.</p><p>Text input: ATV engine roars and accelerates, with guitar music.</p><p>Text input: Wheels spinning, and a slamming sound as the skateboard lands on concrete.</p><p>Text input: A beautiful orchestral piece that evokes a sense of wonder.</p><p>Text input: Whistling sounds, followed by a sharp explosion and loud crackling.</p></div></div><div><h2>Learn more about Movie Gen</h2><div><div><p>Download our latest research paper to learn how we’ve set new industry benchmarks on media generation with AI.</p></div><p><a href="https://ai.meta.com/static-resource/movie-gen-research-paper" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external-link&quot;,&quot;creative_detail&quot;:&quot;MovieGenResearchPaperStaticResource_LearnMoreAboutMovieGen_DownloadPaper&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_1u_ex">Download paper</a></p></div><div><div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="AI Generated image of a woman with long, curly hair wearing a dress with the sun behinder her"></p></div><p>BLOG</p><h3>How Meta Movie Gen could usher in a new AI-enabled era for content creators</h3></div><div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="Young man and woman sitting on a sofa looking at two open laptops"></p></div><p>RESPONSIBILITY</p><h3>Building AI around a core set of values to ensure trust and safety</h3></div></div></div></div><div><div id="u_0_1x_ST" data-scoped-css="fullscreen-hero-scoped-u_0_1_xf"><h2>Meta Movie Gen</h2><p>Introducing research for the most advanced media foundation AI models.</p></div><div><div><p>Movie Gen sets a new standard for immersive AI content</p><div><p>Our latest research breakthroughs demonstrate how you can use simple text inputs to produce custom videos and sounds, edit existing videos or transform your personal image into a unique video.</p></div></div><div><p>Text input summary: A girl is running across a beach and holding a kite. She's wearing jean shorts and a yellow t-shirt. The sun is shining down.</p><p>Text input summary: A woman is sitting on the grass of a pumpkin patch. She is wearing a scarf and holding a cup. The background is filled with rows of pumpkins.</p><p>Text input: Thunder cracks loudly, with an orchestral music track.</p><p>Text input: Transform the lantern into a bubble that soars into the air.</p></div></div><div><div><p>Generate videos from text</p><div><p>Produce unique videos from text to create a custom masterpiece. Movie Gen creates long high-definition videos at different aspect ratios—the first of its kind in the industry.</p></div></div><div><p>Text input summary: A sloth with pink sunglasses lays on a donut float in a pool. The sloth is holding a tropical drink. The world is tropical. The sunlight casts a shadow.</p><p>Text input summary: The camera is behind a man. The man is shirtless, wearing a green cloth around his waist. He is barefoot. With a fiery object in each hand, he creates wide circular motions. A calm sea is in the background. The atmosphere is mesmerizing, with the fire dance.</p><p>Text input summary: A fluffy koala bear surfs. It has a grey and white coat and a round nose. The surfboard is yellow. The koala bear is holding onto the surfboard with its paws. The koala bear’s facial expression is focused. The sun is shining.</p><p>Text input summary: A ghost in a white bedsheet faces a mirror. The ghost's reflection can be seen in the mirror. The ghost is in a dusty attic, filled with old beams, cloth-covered furniture. The attic is reflected in the mirror. The light is cool and natural. The ghost dances in front of the mirror.</p><p>Text input summary: A red-faced monkey with white fur is bathing in a natural hot spring. The monkey is playing in the water with a miniature sail ship in front of it, made of wood with a white sail and a small rudder. The hot spring is surrounded by lush greenery, with rocks and trees.</p></div></div><div><p>Edit video with text</p><div><p>Transform existing videos with text inputs. Movie Gen enables precise video editing—from styles and transitions to fine-grained edits.</p></div></div><div><div><p>Produce personalized videos</p><div><p>Upload an image of yourself and transform it into a personalized video. Movie Gen’s cutting-edge model lets you create personalized videos that preserve human identity and motion.</p></div></div><div><p>Text input summary: A man is doing a scientific experiment in a lab with rainbow wallpaper. The man has a serious expression and is wearing glasses. He is wearing a white lab coat with a pen in the pocket. The man pours liquid into a glass beaker and a cloud of white smoke blooms.</p><p>Text input summary: A woman paints a canvas on an easel, in a wood-paneled room. The woman is wearing a white shirt. She has a calm expression as she concentrates on her work. A baby bear cub stands at her feet. The lighting is cool.</p><p>Text input summary: Make a cute selfie video of a man and his dog. The man is wearing a black shirt. The dog is a beagle puppy. The background is a backyard patio, filled with trees. The man has a big smile on his face, as he tries to take the perfect selfie with his dog. The lighting is warm.</p><p>Text input summary: A man sits in the desert, wearing a wide-brimmed hat, a brown coat, and a scarf. The man holds a glass of amber-colored tea. The camera pans from the desert scenery to the person. The lighting is warm, with the sun casting a gentle glow on the scene.</p><p>Text input summary: A cowgirl wearing denim pants is on a white horse in an old western town. A leather belt cinches at her waist. The horse is majestic, with its coat gleaming in the sunlight. The Rocky Mountains are in the background.</p><p>Text input summary: A woman DJ spins records on a rooftop in LA. She is wearing a pink jacket and giant headphones. There is a cheetah next to the woman. The background is a cityscape.</p></div></div><div><div><p>Create sound effects and soundtracks</p><div><p>Use video and text inputs to generate audio for your videos. Movie Gen allows you to create and extend sound effects, background music or entire soundtracks.</p></div></div><div><p>Text input: Rain pours against the cliff and the person, with music playing in the background.</p><p>Text input: Rustling leaves and snapping twigs, with an orchestral music track.</p><p>Text input: ATV engine roars and accelerates, with guitar music.</p><p>Text input: Wheels spinning, and a slamming sound as the skateboard lands on concrete.</p><p>Text input: A beautiful orchestral piece that evokes a sense of wonder.</p><p>Text input: Whistling sounds, followed by a sharp explosion and loud crackling.</p></div></div><div><h2>Learn more about Movie Gen</h2><div><div><p>Download our latest research paper to learn how we’ve set new industry benchmarks on media generation with AI.</p></div><p><a href="https://ai.meta.com/static-resource/movie-gen-research-paper" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external-link&quot;,&quot;creative_detail&quot;:&quot;MovieGenResearchPaperStaticResource_LearnMoreAboutMovieGen_DownloadPaper&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_4d_7u">Download paper</a></p></div><div><div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="AI Generated image of a woman with long, curly hair wearing a dress with the sun behind her"></p></div><p>BLOG</p><h3>How Meta Movie Gen could usher in a new AI-enabled era for content creators</h3></div><div><div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt="Young man and woman sitting on a sofa looking at two open laptops"></p></div><p>RESPONSIBILITY</p><h3>Building AI around a core set of values to ensure trust and safety</h3></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Chebyshev approximation calculator (open source web app) (122 pts)]]></title>
            <link>https://stuffmatic.com/chebyshev/</link>
            <guid>41740568</guid>
            <pubDate>Fri, 04 Oct 2024 12:09:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stuffmatic.com/chebyshev/">https://stuffmatic.com/chebyshev/</a>, See on <a href="https://news.ycombinator.com/item?id=41740568">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[159 employees are leaving Automattic as CEO’s fight with WP Engine escalates (202 pts)]]></title>
            <link>https://techcrunch.com/2024/10/04/159-employees-are-leaving-automattic-as-ceos-fight-with-wp-engine-escalates/</link>
            <guid>41738914</guid>
            <pubDate>Fri, 04 Oct 2024 07:55:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/10/04/159-employees-are-leaving-automattic-as-ceos-fight-with-wp-engine-escalates/">https://techcrunch.com/2024/10/04/159-employees-are-leaving-automattic-as-ceos-fight-with-wp-engine-escalates/</a>, See on <a href="https://news.ycombinator.com/item?id=41738914">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Automattic CEO Matt Mullenweg said on Thursday that 159 employees (roughly 8.4% of staff) <a rel="nofollow" href="https://ma.tt/2024/10/alignment/">accepted a severance package</a> that the company had offered to those who disagreed with his direction of WordPress and his <a href="https://techcrunch.com/2024/10/01/wordpress-vs-wp-engine-drama-explained/">handling of the tussle with web hosting provider WP Engine</a>.</p>

<p>In <a href="https://ma.tt/2024/10/alignment/" target="_blank" rel="noreferrer noopener nofollow">a blog post</a>, Mullenweg said the package offered $30,000 or six months of salary, whichever is higher, but the employees who took it would not be eligible to be re-hired by Automattic. </p>







<p>Nearly 80% of people who took the offer worked in the company’s Ecosystem / WordPress division, and the rest were in Automattic’s Cosmos businesses, consisting of apps like Pocket Casts, Day One, Tumblr and Cloudup.</p>

<p>Mullenweg, who co-created WordPress and is arguably the face of the <a rel="nofollow" href="https://wordpress.org/">open-source project</a>, tried to put a positive spin on the announcement, writing that the company “decided to design the most generous buy-out package possible, we called it an Alignment Offer.”</p>

<p>“HR added some extra details to sweeten the deal; we wanted to make it as enticing as possible,” he wrote, and later on added: “159 people took the offer, 8.4% of the company, the other 91.6% gave up $126M of potential severance to stay!” </p>

<p>“It was an emotional roller coaster of a week. The day you hire someone, you aren’t expecting them to resign or be fired; you’re hoping for a long and mutually beneficial relationship. Every resignation stings a bit,” Mullenweg wrote.</p>

<p>Mullenweg and Automattic have been in a skirmish with WP Engine for almost two weeks now, in which the CEO has called WP Engine a “cancer to WordPress,” accusing it of wrongfully using the WordPress and WooCommerce trademarks, and <a href="https://techcrunch.com/2024/09/25/wordpress-org-bans-wp-engine-blocks-it-from-accessing-its-resources/">banning the company</a> from accessing the open-source WordPress.org resources.</p>


<p>Both WP Engine and Automattic have <a href="https://techcrunch.com/2024/09/23/wp-engine-sends-cease-and-desist-letter-to-automattic-over-mullenwegs-comments/">sent</a> each other <a href="https://techcrunch.com/2024/09/25/legal-ping-pong-in-the-wordpress-world-continues-automattic-now-sends-wp-engine-a-cease-and-desist-letter-alleging-trademark-infringement/" target="_blank" rel="noreferrer noopener">cease-and-desist letters</a>. And WP Engine earlier on Thursday <a href="https://techcrunch.com/2024/10/02/wp-engine-sues-automattic-and-wordpress-co-founder-matt-mullenweg/" target="_blank" rel="noopener">filed a lawsuit against Automattic and Mullenweg</a>, accusing the company and its CEO of “abuse of power,” extortion, and saying the WordPress co-creator has conflicts of interest in handling WordPress as an open-source project.</p>

<p>Automattic has so far called all of WP Engine’s claims meritless. “I stayed up last night reading WP Engine’s Complaint, trying to find any merit anywhere to it. The whole thing is meritless, and we look forward to the federal court’s consideration of their lawsuit,” the company’s legal representative, Neal Katyal, said in a <a rel="nofollow" href="https://automattic.com/2024/10/03/meritless/">blog post</a>.</p>

<p>Over the last few days, <a rel="nofollow" href="https://x.com/jeffr0/status/1841585100665872569">several</a> <a rel="nofollow" href="https://x.com/BoweFrankema/status/1841836017810092059">people</a> on <a rel="nofollow" href="https://x.com/kellie/status/1841210258422972536">X</a> have hinted about a severance offer being circulated among Automattic employees. Mullenweg also <a rel="nofollow" href="https://medium.com/@kelliepeterson/nice-guy-matt-mullenweg-ceo-of-wordpress-com-cries-foul-and-threatens-me-with-legal-action-f116ac57d862">allegedly DM’d a former employee who posted about the offer</a> and accused her of attacking the company and him.</p>







<p>Today, <a rel="nofollow" href="https://x.com/p3ob7o/status/1842078298030956576">some Automattic employees</a> who opted to keep their jobs <a rel="nofollow" href="https://x.com/richard_tabor/status/1842062384820633742">posted messages</a> in support of the company and Mullenweg.</p>

<p><em>You can contact this reporter at im@ivanmehta.com or on Signal: @ivan.42</em><br></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse Engineering and Dismantling Kekz Headphones (123 pts)]]></title>
            <link>https://nv1t.github.io/blog/kekz-headphones/</link>
            <guid>41738552</guid>
            <pubDate>Fri, 04 Oct 2024 06:55:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nv1t.github.io/blog/kekz-headphones/">https://nv1t.github.io/blog/kekz-headphones/</a>, See on <a href="https://news.ycombinator.com/item?id=41738552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Close to a year ago, I stumbled upon the Kekz Headphones, which seemed like an interesting approach on the whole digital audio device space. They claimed to work without any internet connection and all of the content already on the headphones itself. They are On-Ear Headphones, which work by placing a small chip (I call them “Kekz” or “Cookie”) into a little nook on the side and it plays an audio story.
I was intrigued, because there were some speculations going around, how they operate with those “Kekz”-Chips.</p><p>I invite you to join me on a journey into the inner workings of those headphones. We will talk about accessing the encrypted files on the device, breaking the crypto and discovering disclosure of data from customers.</p><h2 id="opening-the-headphones">Opening the headphones</h2><p>I sourced my headphones from “Kleinanzeigen” (something like craigslist, or facebook marketplace) to keep the research costs low and maybe get some cookies with it. I got the wonderful colour red.
<img src="https://nv1t.github.io/blog/img/2024/9fedcf0c1ac98650a8055d6744523e91.png" alt="Headphones opened up, PCBs hanging out, Kekz chips are lying to the left.">
After opening up the headphones, you will have 2 PCBs which are connected by 7 wires. Two speakers and a battery. The chinese lettering in the silk layer is just the colour description of the wires itself. You don’t see any interesting breakout for anything here. The Pin-Row in the middle is for the NFC antenna on the other side of the board. You see two Vias with the label <code>DP</code> and <code>DM</code>, which is on the USB line. This will be interesting at a later stage.</p><p><img src="https://nv1t.github.io/blog/img/2024/9c2962c7adb42ddbf4c88bb9f44b7d7a.jpg" alt="PCB of one of the ears, which has the important chips on it. Description is in the text.">
The first thing that stands out is a Jieli Chip, which appears to be the core component of the entire headset. These chips are mostly used in cheap Bluetooth hardware and are difficult to determine which version is currently running. From a quick search i think this Chip (<code>AC21BP0H733-51C8</code>) is probably some version of the <code>AC6951C</code>.</p><p>The chip on the bottom, <code>TSC9883</code>, is a NFC Reader IC, which I don’t care for.</p><p>It has two infrared proximity sensors to detect the ear and cookie insertion to prevent from reading a cookie more than once and stop when taking off the headphones.</p><p>On the right of the PCB you see an SD cardholder, which has a 32gb SD Card on the inside. The SD Card has a Fat32 Filesystem with 276 directories. There is an update, which ups that to around 369 directories. Each directory has multiple files with the extension <code>kez</code>, which are most likely encrypted.</p><p><img src="https://nv1t.github.io/blog/img/2024/87e3e2c1ef721f3e60733fc2e0e3149e.png" alt="Directory and Filelisting from the SD Card"></p><p>Interestingly, I was looking at the SD Card before and I connected the headphones to the accompanied Windows Application. After the connection, the files were gone and I was kinda puzzled, until I found the following code in the application:</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>public</span> <span>static</span> <span>void</span> HideFolders()
</span></span><span><span>{
</span></span><span><span>	<span>if</span> (Globals.Drive == <span>null</span>)
</span></span><span><span>	{
</span></span><span><span>		<span>throw</span> <span>new</span> Exception(<span>"Drive not set"</span>);
</span></span><span><span>	}
</span></span><span><span>	<span>string</span> drive = Globals.Drive;
</span></span><span><span>	<span>string</span>[] directories = Directory.GetDirectories(drive);
</span></span><span><span>	<span>for</span> (<span>int</span> i = <span>0</span>; i &lt; directories.Length; i++)
</span></span><span><span>	{
</span></span><span><span>		DirectoryInfo directoryInfo = <span>new</span> DirectoryInfo(directories[i]);
</span></span><span><span>		<span>if</span> (<span>int</span>.TryParse(directoryInfo.Name, <span>out</span> <span>var</span> _))
</span></span><span><span>		{
</span></span><span><span>			directoryInfo.Attributes |= FileAttributes.Hidden;
</span></span><span><span>		}
</span></span><span><span>	}
</span></span><span><span>	<span>new</span> FileInfo(Path.Combine(drive, <span>"kekzId.json"</span>)).Attributes |= FileAttributes.Hidden;
</span></span><span><span>}
</span></span></code></pre></div><p>They seem to set the hidden Attribute on the first connect, so the files are not easily discovered.</p><p>We have multiple options to attack this system. We can either try to dump the firmware of the main controller chip and reverse engineer that, or we understand, how a “Kekz” works.</p><p>As we currently have little information about the controller, and we haven’t looked into the NFC Communication yet, let’s first check the cookies themselves and go the easy route.</p><h2 id="you-get-a-cookie-and-you-get-a-cookie">You get a cookie and you get a cookie.</h2><p>I have a <a href="https://proxmark.com/">Proxmark3</a> lying around and if we try to “just read” a Kekz, it will result in an error, as it seems to be locked:</p><pre tabindex="0"><code>[usb] pm3 --&gt; hf mfu info

[=] --- Tag Information --------------------------
[+]       TYPE: NTAG 213 144bytes (NT2H1311G0DU)

[=] --- Tag Counter
[=]        [02]: 00 00 00
[+]             - 00 tearing ( fail )

[=] --- Tag Signature
[=]     Elliptic curve parameters: NID_secp128r1
[=]              TAG IC Signature: 0000000000000000000000000000000000000000000000000000000000000000
[+]        Signature verification ( fail )

[=] --- Tag Version
[=]        Raw bytes: 00 53 04 02 01 00 0F 03
[=]        Vendor ID: 53, Shanghai Feiju Microelectronics Co. Ltd. China
[=]     Product type: NTAG
[=]  Product subtype: 02, 50pF
[=]    Major version: 01
[=]    Minor version: 00
[=]             Size: 0F, (256 &lt;-&gt; 128 bytes)
[=]    Protocol type: 03, ISO14443-3 Compliant
[?] Hint: try `hf mfu pwdgen -r` to get see known pwd gen algo suggestions
[=] ------------------------ Fingerprint -----------------------
[=] Reading tag memory...
[#] Cmd Error: 00
[#] Read block 0 error
[!] ⚠️  Failed reading card
[=] ------------------------------------------------------------

[=] Tag appears to be locked, try using a key to get more info
</code></pre><p>We could either Brute-Force (not sure, if this will result in a locked chip), or we can just sniff the communication between the headset and the cookie. By holding the proxmark near the reader of the headset and inserting a cookie, we will get the whole communication between the reader itself and this cookie, which reveals the authentication Key.</p><p><img src="https://nv1t.github.io/blog/img/2024/a1d514983cf67277c9e02790df905202.png" alt="Trace of the NFC communication with the password for the chips highlighted in a red box">
It then tries to read the 0 block and the 7th block. The 0 block is only the ID of the tag which bears no relevance. If we look into the 7th block and further, though, we can see a string “en002071696263”, if we dump the whole cookie. That is quite interesting.</p><pre tabindex="0"><code>[usb] pm3 --&gt; hf mfu dump -k FFFFFFFF
[+] TYPE: NTAG 213 144bytes (NT2H1311G0DU)
[+] Reading tag memory...

[=] MFU dump file information
[=] -------------------------------------------------------------
[...redacted...]
[=] -------------------------------------------------------------
[=] block#   | data        |lck| ascii
[=] ---------+-------------+---+------
[=]   0/0x00 | 53 BA 20 41 |   | S. A
[=]   1/0x01 | 46 70 00 01 |   | Fp..
[=]   2/0x02 | 37 48 00 00 |   | 7H..
[=]   3/0x03 | E1 10 12 00 | 0 | ....
[=]   4/0x04 | 01 03 A0 0C | 0 | ....
[=]   5/0x05 | 34 03 00 FE | 0 | 4...
[=]   6/0x06 | 00 00 00 00 | 0 | ....
[=]   7/0x07 | 65 6E 30 30 | 0 | en00
[=]   8/0x08 | 32 30 37 31 | 0 | 2071
[=]   9/0x09 | 36 39 36 32 | 0 | 6962
[=]  10/0x0A | 36 33 FE 00 | 0 | 63..
[...redacted...]
[=] ---------------------------------
[=] Using UID as filename
[+] saved 236 bytes to binary file /.proxmark3/files/hf-mfu-53BA2046700001-dump.bin
[+] saved 59 blocks to text file ./.proxmark3/files/hf-mfu-53BA2046700001-dump.eml
[+] saved to json file /.proxmark3/files/hf-mfu-53BA2046700001-dump.json
</code></pre><p>We can check our theory by copying over this string to another cookie, and you will discover, it works. Therefore, <strong>we can clone cookies now.</strong></p><p><img src="https://nv1t.github.io/blog/img/2024/8ca30bafd2e46943f989143f33dab0b8.png" alt="Image of a bunny and cat looking alike next to each other."></p><p>Even if the outside looks different, it plays the same content and is a bunny by heart.</p><p>At this point, it is possible to read this string and build a database to decrypt all content. We just have access to the ones we have already seen.
As those tags are 13.35Mhz, is is also possible to write them by your Phones NFC (you will see this later on)</p><h2 id="what-does-this-string-mean">What does this string mean?</h2><p>We have this weird string <code>en002071696263</code> which has something to do with playing the content and we know the content is probably one of those directories we did see earlier on the SD-card.</p><p>If we begin to delete one directory after the other, we can determine which directory has the desired content inside. For this cookie, the directory is <code>0020</code>. If we look into other cookies we will see the structure is:</p><table><thead><tr><th>Cookie</th><th>String</th></tr></thead><tbody><tr><td>Cookie Crew 1</td><td>en 0020 71696263</td></tr><tr><td>Cookie Crew 1</td><td>en 0020 71696263</td></tr><tr><td>Feuerwehrman Sam</td><td>en 0002 6161777a</td></tr><tr><td>Was ist Was</td><td>en 0031 67766172</td></tr><tr><td>Hotzenplotz</td><td>en 0006 73657463</td></tr></tbody></table><p>I can move those files to a directory <code>4444</code>, the files will be played, but garbage output. This means the <code>0020</code> is important for the decryption phase.
Renaming <code>0020</code> to something else will result in an “unbaked” Chip.
If i move the files to a directory <code>1020</code>, they will be played just fine, after rebranding the chip to <code>en102071696263</code></p><p>We could try more stuff to understand the encryption, but…let us recap</p><ol><li>the four integers number after <code>en</code> is the directory</li><li>they are partially important for the decryption.</li><li>four bytes in the end, we don’t know the purpose, but they are essential for decryption</li></ol><h2 id="moargive-me-moaaaaar">Moar…give me moaaaaar</h2><p>The only crux is, we only can decrypt stuff we have already seen, but i want to have an attack on everything.</p><p>We could brute force the 4 Bytes. Without any further assumption, this would be <code>255**4</code> possibilities, which is way to many.</p><p>But if we look into the last four bytes more closely, we can assume one last thing:
In our examples, the four bytes are hex representation for four small letters from the alphabet.</p><p>With this assumption, we can bring this down to <code>26**4</code>. That sounds more reasonable, but can we attack the crypto further?</p><p>Lucky for us, they published an application which can write a cookie named “Wunderkekz”. This App can encrypt arbitrary MP3 files to the correct <code>kez</code>-Fileformat. And more Lucky for us: it is written in csharp.</p><p>Therefore, we can take a look into the encryption routine (i translated it to python, variable naming directly from the original decompilation):</p><div><pre tabindex="0"><code data-lang="python"><span><span>str_crumb_hex <span>=</span> sys<span>.</span>argv[<span>3</span>] <span># #"E9-F5-33-6B" # Assuming this value based on your previous examples</span>
</span></span><span><span>directory_raw <span>=</span> sys<span>.</span>argv[<span>1</span>]
</span></span><span><span>filename_raw <span>=</span> sys<span>.</span>argv[<span>2</span>]
</span></span><span><span>
</span></span><span><span>directory <span>=</span> bytearray(directory_raw, <span>'ascii'</span>)
</span></span><span><span>filename <span>=</span> bytearray(filename_raw, <span>'ascii'</span>)
</span></span><span><span>array <span>=</span> str_crumb_hex<span>.</span>split(<span>'-'</span>)
</span></span><span><span>b, b2, b3, b4 <span>=</span> [int(value, <span>16</span>) <span>for</span> value <span>in</span> array]
</span></span><span><span>str_crumb_hex_unpacked <span>=</span> bytearray([b, b2, b3, b4])
</span></span><span><span>b5 <span>=</span> (str_crumb_hex_unpacked[<span>0</span>] <span>^</span> directory[<span>0</span>]) <span>&gt;&gt;</span> <span>4</span>
</span></span><span><span>b6 <span>=</span> (str_crumb_hex_unpacked[<span>1</span>] <span>^</span> directory[<span>1</span>]) <span>&gt;&gt;</span> <span>5</span>
</span></span><span><span>b7 <span>=</span> (str_crumb_hex_unpacked[<span>2</span>] <span>^</span> directory[<span>2</span>]) <span>&gt;&gt;</span> <span>3</span>
</span></span><span><span>b8 <span>=</span> (str_crumb_hex_unpacked[<span>3</span>] <span>^</span> directory[<span>3</span>]) <span>&gt;&gt;</span> <span>2</span>
</span></span><span><span>
</span></span><span><span>array3 <span>=</span> bytearray([b5, b6, b7, b8])
</span></span><span><span>
</span></span><span><span>b9 <span>=</span> (filename[<span>0</span>] <span>+</span> filename[<span>1</span>] <span>+</span> filename[<span>2</span>] <span>+</span> filename[<span>3</span>]) <span>%</span> <span>10</span> <span>-</span> <span>1</span>
</span></span><span><span><span>if</span> b9 <span>&gt;=</span> <span>9</span> <span>or</span> b9 <span>&lt;</span> <span>0</span>:
</span></span><span><span>    b9 <span>=</span> <span>6</span>
</span></span><span><span>
</span></span><span><span><span>with</span> open(<span>"</span><span>%s</span><span>/</span><span>%s</span><span>.mp3"</span> <span>%</span> (directory_raw,filename), <span>'rb'</span>) <span>as</span> file_stream:
</span></span><span><span>	array4 <span>=</span> bytearray(file_stream<span>.</span>read())
</span></span><span><span>	array5 <span>=</span> bytearray(len(array4))
</span></span><span><span>	<span>for</span> i <span>in</span> range(len(array4)):
</span></span><span><span>		array4[i] <span>^=</span> array3[i <span>%</span> <span>4</span>]
</span></span><span><span>		array5[i] <span>=</span> ((array4[i] <span>&gt;&gt;</span> (<span>8</span> <span>-</span> b9)) <span>|</span> (array4[i] <span>&lt;&lt;</span> b9)) <span>&amp;</span> <span>0xFF</span>
</span></span><span><span>
</span></span><span><span><span>with</span> open(<span>"</span><span>%s</span><span>/</span><span>%s</span><span>.kez"</span> <span>%</span> (directory_raw,filename),<span>'wb'</span>) <span>as</span> fh:
</span></span><span><span>	fh<span>.</span>write(array5)
</span></span></code></pre></div><ul><li>Opens the MP3 file for reading.</li><li>Reads the entire file into a byte array <code>array4</code>.</li><li>Creates a new byte array <code>array5</code> to hold the encrypted data.</li><li>Iterates through <code>array4</code>, performing the following operations on each byte:<ul><li>XORs the byte with an element of <code>array3</code> (which is some kind of key) (selected in a round-robin fashion).</li><li>Performs a bitwise rotation on the byte, using <code>b9</code> as the shift count, and stores the result in <code>array5</code>.</li></ul></li><li>Closes and disposes of the input file stream.</li></ul><h2 id="attacking-the-crypto">Attacking the Crypto</h2><p>As the shift and 4 Byte XOR key depends on the directory name and unknown 4 Byte Hex Key, we can pre calc a brute-force table which significantly limits the keyspace. We can safely assume the unknown 4-Byte Hex Key to be in a printable state.</p><p>This can be seen in the source code of the Kekz App As well (Source: <code>public static bool GenerateKekzCryptFiles</code>)</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>string</span> s = RandomGenerator.RandomString(<span>4</span>, lowerCase: <span>true</span>);
</span></span><span><span>strCrumbHex = BitConverter.ToString(Encoding.ASCII.GetBytes(s));
</span></span></code></pre></div><p>The following factors reduce the keyspace significantly:</p><ol><li><strong>shifts reduces the keyspace down to 18 bits</strong>:<ul><li><strong><code>b5</code></strong> has 16 possible values (because it’s reduced to 4 bits after the shift).</li><li><strong><code>b6</code></strong> has 8 possible values (because it’s reduced to 3 bits after the shift).</li><li><strong><code>b7</code></strong> has 32 possible values (because it’s reduced to 5 bits after the shift).</li><li><strong><code>b8</code></strong> has 64 possible values (because it’s reduced to 6 bits after the shift).</li></ul></li><li><strong>XOR with the directory</strong>: Each character in the <code>directory</code> string has a limited value range from 48 (ASCII for ‘0’) to 57 (ASCII for ‘9’), or just 10 different possible values for each character.</li><li><strong>Collision</strong>: After the XOR and bit shifts, many different input values may end up producing the same result. The bit shifts cause significant information loss, and many different values could end up mapping to the same <code>b5, b6, b7, b8</code> combination, leading to a large number of collisions. These collisions reduce the number of unique keys generated.</li></ol><p>By writing all possible keys into a Dictionary, we don’t need to sort and uniq an array afterward.
This results in ~56 possible keys to decrypt the content.</p><div><pre tabindex="0"><code data-lang="python"><span><span>characters <span>=</span> <span>"abcdefghijklmnopqrstuvwxyz"</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>keygen</span>(l):
</span></span><span><span>	<span>yield from</span> itertools<span>.</span>product(<span>*</span>([l] <span>*</span> <span>4</span>))
</span></span><span><span>
</span></span><span><span><span>def</span> <span>pre_calc_array3</span>(directory, filename):
</span></span><span><span>	ret <span>=</span> {}
</span></span><span><span>	<span>for</span> x <span>in</span> tqdm(keygen(characters),total<span>=</span>len(characters)<span>**</span><span>4</span>):
</span></span><span><span>		str_crumb_hex <span>=</span> <span>'-'</span><span>.</span>join([hex(ord(i))[<span>2</span>:] <span>for</span> i <span>in</span> x])
</span></span><span><span>		array <span>=</span> str_crumb_hex<span>.</span>split(<span>'-'</span>)
</span></span><span><span>		b, b2, b3, b4 <span>=</span> [int(value, <span>16</span>) <span>for</span> value <span>in</span> array]
</span></span><span><span>		str_crumb_hex_unpacked <span>=</span> bytearray([b, b2, b3, b4])
</span></span><span><span>		b5 <span>=</span> (str_crumb_hex_unpacked[<span>0</span>] <span>^</span> directory[<span>0</span>]) <span>&gt;&gt;</span> <span>4</span>
</span></span><span><span>		b6 <span>=</span> (str_crumb_hex_unpacked[<span>1</span>] <span>^</span> directory[<span>1</span>]) <span>&gt;&gt;</span> <span>5</span>
</span></span><span><span>		b7 <span>=</span> (str_crumb_hex_unpacked[<span>2</span>] <span>^</span> directory[<span>2</span>]) <span>&gt;&gt;</span> <span>3</span>
</span></span><span><span>		b8 <span>=</span> (str_crumb_hex_unpacked[<span>3</span>] <span>^</span> directory[<span>3</span>]) <span>&gt;&gt;</span> <span>2</span>
</span></span><span><span>		array3 <span>=</span> bytearray([b5, b6, b7, b8])
</span></span><span><span>
</span></span><span><span>		ret[<span>f</span><span>"</span><span>{</span>array3[<span>0</span>]<span>}</span><span>,</span><span>{</span>array3[<span>1</span>]<span>}</span><span>,</span><span>{</span>array3[<span>2</span>]<span>}</span><span>,</span><span>{</span>array3[<span>3</span>]<span>}</span><span>"</span>] <span>=</span> array3
</span></span><span><span>	<span>return</span> ret
</span></span></code></pre></div><p>Currently, i take one file, calculate the shift <code>b9</code> and decrypt the file multiple times for every possible key found from the <code>pre_calc_array3</code> method</p><div><pre tabindex="0"><code data-lang="python"><span><span>print(<span>"Starting Brute Force"</span>)
</span></span><span><span><span>for</span> i <span>in</span> tqdm(array3_poss):
</span></span><span><span>	na <span>=</span> i<span>.</span>replace(<span>','</span>,<span>'-'</span>)
</span></span><span><span>	array3 <span>=</span> array3_poss[i]
</span></span><span><span><span>#return True</span>
</span></span><span><span>	<span>with</span> open(<span>"</span><span>%s</span><span>/</span><span>%s</span><span>/</span><span>%s</span><span>.kez"</span> <span>%</span> (location,directory<span>.</span>decode(<span>'utf-8'</span>),filename<span>.</span>decode(<span>'utf-8'</span>)),<span>'rb'</span>) <span>as</span> fh:
</span></span><span><span>		array6 <span>=</span> bytearray(fh<span>.</span>read())
</span></span><span><span>		array4_reversed <span>=</span> bytearray(len(array6))
</span></span><span><span>		<span>for</span> i <span>in</span> range(len(array6)):
</span></span><span><span>			<span># Reverse the bitwise rotation</span>
</span></span><span><span>			array4_reversed[i] <span>=</span> ((array6[i] <span>&lt;&lt;</span> (<span>8</span> <span>-</span> b9)) <span>|</span> (array6[i] <span>&gt;&gt;</span> b9)) <span>&amp;</span> <span>0xFF</span>
</span></span><span><span>			<span># Reverse the XOR operation</span>
</span></span><span><span>			array4_reversed[i] <span>^=</span> array3[i <span>%</span> <span>4</span>]
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>	<span>with</span> tempfile<span>.</span>NamedTemporaryFile(<span>'wb'</span>) <span>as</span> fh:
</span></span><span><span>		fh<span>.</span>write(array4_reversed)
</span></span><span><span>
</span></span><span><span>		returned_output <span>=</span> os<span>.</span>popen(<span>"mpck -q </span><span>%s</span><span>"</span> <span>%</span> (fh<span>.</span>name))<span>.</span>read()
</span></span><span><span>		<span>if</span> <span>": Ok"</span> <span>in</span> returned_output:
</span></span><span><span>			<span>break</span>
</span></span><span><span><span>else</span>:
</span></span><span><span>	<span>return</span> <span>False</span>
</span></span></code></pre></div><p>The Main Problem relies on checking for a valid MP3 files. Because of the shift and the 4 byte xor key you need to check every MP3 Frame, which takes time on larger files.
I currently use an external tool called “<a href="https://github.com/Sjord/checkmate">checkmate</a>”. It has the most robust MP3 validity solution. It basically checks every Frame. (Maybe Fork and implement it in Python? )</p><h2 id="do-you-have-an-app-for-that">Do you have an App for that?</h2><p>I’ve created, for my use and not publication, a small application to read and write the Cookies with my mobile phone. “Kekzmonster” takes in QR Codes, or reads the cookie with NFC and can back up all cookies in my possession. It is not intended to unlock content, which i don’t own.</p><p><img src="https://nv1t.github.io/blog/img/2024/749b63fc6e776f7ce19aad16eca2d7dc.png" alt="Screenshot of my Application Kekzmonster."></p><p>The Encryption/Decryption String can be written to any cookie with this application as well.</p><h2 id="files-without-breaking-the-headphones">Files without breaking the headphones</h2><p>We can now encrypt, decrypt and brute force the cookie content, but you won’t get ny files onto or from the headset on your own, without opening up the headphones and accessing the SD Card. Connecting the headphones to an USB port only charges them and they are listed within Linux as HID Device:</p><pre tabindex="0"><code>Bus 003 Device 012: ID 33f5:0001 Kekz Gmbh kekz headphone
Device Descriptor:
  bLength                18
  bDescriptorType         1
  bcdUSB               1.10
  bDeviceClass            0
  bDeviceSubClass         0
  bDeviceProtocol         0
  bMaxPacketSize0        64
  idVendor           0x33f5
  idProduct          0x0001
  bcdDevice            1.00
  iManufacturer           1 Kekz Gmbh
  iProduct                2 kekz headphone
  iSerial                 3 2021082200001002
  bNumConfigurations      1
  Configuration Descriptor:
    bLength                 9
    bDescriptorType         2
    wTotalLength       0x0022
    bNumInterfaces          1
    bConfigurationValue     1
    iConfiguration          0
    bmAttributes         0x80
      (Bus Powered)
    MaxPower              100mA
    Interface Descriptor:
      bLength                 9
      bDescriptorType         4
      bInterfaceNumber        0
      bAlternateSetting       0
      bNumEndpoints           1
      bInterfaceClass         3 Human Interface Device
      bInterfaceSubClass      0
      bInterfaceProtocol      0
      iInterface              0
        HID Device Descriptor:
          bLength                 9
          bDescriptorType        33
          bcdHID               2.01
          bCountryCode            0 Not supported
          bNumDescriptors         1
          bDescriptorType        34 Report
          wDescriptorLength      27
         Report Descriptors:
           ** UNAVAILABLE **
      Endpoint Descriptor:
        bLength                 7
        bDescriptorType         5
        bEndpointAddress     0x82  EP 2 IN
        bmAttributes            3
          Transfer Type            Interrupt
          Synch Type               None
          Usage Type               Data
        wMaxPacketSize     0x0008  1x 8 bytes
        bInterval               1
Device Status:     0x0000
  (Bus Powered)
</code></pre><p>We talked about the two vias <code>DP</code> and <code>DM</code>. Interestingly, we are not finding any other connection to the chip, no UART, no JTAG, nothing.
Looking into Jie-li, they are pretty weird chips. The documentation and various sources, state they are programmed over the normal USB Data lines. There are two main methods to control them.</p><h2 id="signaling-dpdm-on-usb">Signaling DP/DM on USB</h2><p>The normal way to put the chip into DFU mode would be sending a custom pullup/pulldown over the <code>D+</code> and <code>D-</code>:
<img src="https://nv1t.github.io/blog/img/2024/ad221d82128b7872b20afc5731412ff7.png" alt="">
After this signal, <code>D+</code> and <code>D-</code> gets pulled to Ground for <code>2ms</code> and the device boots up into the DFU Mode with uboot.</p><p>There are special programmer to achieve this, but i’ve seen a post, where somebody build his/her own programmer with an arduino.</p><p>I tried to achieve this with a raspberry pi pico and couldn’t get the Chip into DFU mode. I even tried to use the special programmer for this, but still…no luck.</p><p>In addition, i thought, the windows application does some magic to connect them and read/write content of the headphones. How?! That has to work without any extra hardware and you don’t have such control over the USB data lines from an operating system application.</p><h2 id="hid-communication">HID Communication</h2><p>The other more convenient option for these chips are: they might have special commands over HID which reconnect them in different stages. These are not really documented and can be different for each chip, as it depends on the firmware (i think so, that is what i got in rough translations).</p><h3 id="dfu-mode">DFU Mode</h3><p>Using the Python HID Library, this is effortless. The important thing to know is the <code>dfu_payload</code> which get’s send to the device.</p><div><pre tabindex="0"><code data-lang="python"><span><span>dfu_payload <span>=</span> [<span>0</span>, <span>85</span>, <span>170</span>, <span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>170</span>, <span>85</span>]
</span></span><span><span>
</span></span><span><span>device <span>=</span> hid<span>.</span>device()
</span></span><span><span>
</span></span><span><span>device<span>.</span>open(vendor_id,product_id)
</span></span><span><span>print(<span>f</span><span>"HID: Found Device: </span><span>{</span>device<span>.</span>get_manufacturer_string()<span>}</span><span> </span><span>{</span>device<span>.</span>get_product_string()<span>}</span><span>"</span>)
</span></span><span><span>
</span></span><span><span>data <span>=</span> bytearray(byte_array_left_pad([<span>0</span>, <span>33</span>, <span>9</span>, <span>0</span>, <span>2</span>, <span>1</span>, <span>0</span>, <span>64</span>, <span>0</span>], <span>0</span>, <span>65</span>))
</span></span><span><span>buffer_payload <span>=</span> bytearray(byte_array_left_pad(dfu_payload, <span>0</span>, <span>65</span>))
</span></span><span><span>
</span></span><span><span>device<span>.</span>write(data)
</span></span><span><span>
</span></span><span><span><span>try</span>:
</span></span><span><span>	device<span>.</span>write(buffer_payload)
</span></span><span><span><span>except</span> <span>Exception</span>:
</span></span><span><span>	print(<span>"</span><span>\n</span><span>Communication Error or DFU Success..."</span>)
</span></span></code></pre></div><p>You can now flash new firmware on the chip.</p><h3 id="connecting-to-copy">Connecting to copy</h3><p>The same will go for the connection of the headphones as “normal USB Stick”. The payload is a little bit different:</p><div><pre tabindex="0"><code data-lang="python"><span><span>connect_payload <span>=</span> [<span>0</span>, <span>85</span>, <span>170</span>, <span>3</span>, <span>1</span>, <span>41</span>, <span>40</span>, <span>170</span>, <span>85</span>]
</span></span></code></pre></div><p>After sending this payload, the headphones disconnect and reconnect as a normal USB Stick.
Keep in mind, that if you run or ran the application, the files are with the Windows hidden attribute.</p><p>Success…we can now encrypt custom files, put them into custom directories and write our own cookies.</p><p><strong>We now fully pwn the headphones.</strong></p><h2 id="other-discoveries">Other Discoveries</h2><p>Browsing through the source code of the application and website, i found other things, which are weren’t mentioned before.</p><h2 id="creating-a-list-of-all-public-cookies">Creating a list of all public cookies</h2><p>There are multiple cookies, which are not public yet, but as we know from above, we can decrypt all of them. Unfortunately, we have no idea, what is in those directories.
But, i found a pretty neat trick to generate a list of all already public cookies:</p><p>The Kekz Webshop is/was built upon a WordPress installation. Fortunately for us, the upload’s directory has directory listing enabled. I can download all images ever shown in this webshop.
<img src="https://nv1t.github.io/blog/img/2024/35e598415e4725d4c97b4ce7a5e14b80.png" alt="Directory Listing of the 2024/11 uploads directory of the Kekz store"></p><p>These files are pretty important for us, because you see, every cookie has an ID, for “Raeuber Hotzenplotz”, it is 1-1.0066 and interestingly, the directory is “0006”.
<img src="https://nv1t.github.io/blog/img/2024/b45c09c7774d501681d013c89f0dd13c.png" alt="close Up Picture of a cookie with the ID highlighted with a red box">
If we look into all the images, we can see product images from the packaging as well. This packaging has a barcode with this ID as well.
<img src="https://nv1t.github.io/blog/img/2024/8f2f2a131f5e45e8890c7a60319f7cb8.png" alt="Partial Image of the back packaging of a cookie.">
Therefore, we can automate downloading all images and scanning for barcodes and extracting the directory.</p><p><strong>With this information, we can determine about 1/3 of the content is officially released already in the shop.</strong></p><h2 id="moar-wunderkekze">Moar Wunderkekze</h2><p>Apparently the directories 0990 until 0996 are used for the WunderKekzChips, whereas Green, Orange and Purple are already in circulation.</p><p>Maybe the plan is to add more cookies to the mix.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span><span>return</span> wChip <span>switch</span>
</span></span><span><span>{
</span></span><span><span>	WunderkekzChipEnum.Green =&gt; <span>"0996"</span>,
</span></span><span><span>	WunderkekzChipEnum.Orange =&gt; <span>"0995"</span>,
</span></span><span><span>	WunderkekzChipEnum.Purple =&gt; <span>"0994"</span>,
</span></span><span><span>	WunderkekzChipEnum.NineThree =&gt; <span>"0993"</span>,
</span></span><span><span>	WunderkekzChipEnum.NineTwo =&gt; <span>"0992"</span>,
</span></span><span><span>	WunderkekzChipEnum.NineOne =&gt; <span>"0991"</span>,
</span></span><span><span>	WunderkekzChipEnum.NineZero =&gt; <span>"0990"</span>,
</span></span><span><span>	_ =&gt; <span>"0035"</span>,
</span></span><span><span>};
</span></span></code></pre></div><h2 id="user-data-collection">User Data collection</h2><p>This topic is not so nice.</p><p>While looking through the application, i discovered some not so nice stuff, which wasn’t mentioned in the privacy policy anywhere (<a href="https://web.archive.org/web/20240305082206/https://store.kekz.com/datenschutzerklaerung/">Archive</a>). Point 1.10 about the Kekz App was added at a later stage, after my disclosure emails.</p><h3 id="id3-tags">ID3 Tags</h3><p>ID3 tags are metadata containers used to store information about an MP3 audio file, such as the song’s title, artist, album, and other details. They help media players and libraries organize and display information about the audio files. The ID3 tags are stored within the MP3 file itself.
If you are using an Wunderkekz from the Kekz company, and you use the standard windows application (because there is no other), the ID3 tags are uploaded to an Azure Cosmos database.</p><div><pre tabindex="0"><code data-lang="csharp"><span><span>WunderkekzUploadMetadata wunderkekzUploadMetadata = <span>new</span> WunderkekzUploadMetadata();
</span></span><span><span><span>try</span>
</span></span><span><span>{
</span></span><span><span>	FileInfo fileInfo = <span>new</span> FileInfo(path);
</span></span><span><span>	wunderkekzUploadMetadata.FileName = fileInfo.Name;
</span></span><span><span>	wunderkekzUploadMetadata.FileSize = fileInfo.Length;
</span></span><span><span>	wunderkekzUploadMetadata.EventId = Globals.CurrentEventId;
</span></span><span><span>	<span>using</span> TagLib.File file = TagLib.File.Create(path);
</span></span><span><span>	wunderkekzUploadMetadata.Id3Title = file.Tag.Title;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Artist = file.Tag.FirstPerformer;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Album = file.Tag.Album;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Year = (<span>int</span>)file.Tag.Year;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Track = (<span>int</span>)file.Tag.Track;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Genre = file.Tag.FirstGenre;
</span></span><span><span>	wunderkekzUploadMetadata.Id3Comment = file.Tag.Comment;
</span></span><span><span>	<span>return</span> wunderkekzUploadMetadata;
</span></span><span><span>}
</span></span><span><span><span>catch</span> (Exception ex)
</span></span><span><span>{
</span></span><span><span>	Trace.WriteLine(ex.Message);
</span></span><span><span>	<span>return</span> wunderkekzUploadMetadata;
</span></span><span><span>}
</span></span></code></pre></div><h3 id="geolocation">Geolocation</h3><p>Furthermore, the application tries not only uploading the ID3 Tags, but also geolocation data, which is most likely gathered from Wi-Fi triangulation from windows itself.</p><p>The MainView calls a GeoLocation Service:</p><div><pre tabindex="0"><code data-lang="java"><span><span><span>public</span> async Task<span>&lt;</span>string<span>&gt;</span> <span>GetCurrentLocation</span>()
</span></span><span><span>{
</span></span><span><span>	string strLocation <span>=</span> <span>null</span>;
</span></span><span><span>	<span>try</span>
</span></span><span><span>	{
</span></span><span><span>		Location lastLocation <span>=</span> await Geolocation.<span>Default</span>.<span>GetLastKnownLocationAsync</span>();
</span></span><span><span>		Location location <span>=</span> (await Geolocation.<span>Default</span>.<span>GetLocationAsync</span>()) <span>??</span> lastLocation;
</span></span><span><span>		<span>if</span> (location <span>!=</span> <span>null</span>)
</span></span><span><span>		{
</span></span><span><span>			DefaultInterpolatedStringHandler defaultInterpolatedStringHandler <span>=</span> <span>new</span> DefaultInterpolatedStringHandler(1, 2);
</span></span><span><span>			defaultInterpolatedStringHandler.<span>AppendFormatted</span>(location.<span>Latitude</span>);
</span></span><span><span>			defaultInterpolatedStringHandler.<span>AppendLiteral</span>(<span>":"</span>);
</span></span><span><span>			defaultInterpolatedStringHandler.<span>AppendFormatted</span>(location.<span>Longitude</span>);
</span></span><span><span>			strLocation <span>=</span> (Globals.<span>GeoData</span> <span>=</span> defaultInterpolatedStringHandler.<span>ToStringAndClear</span>());
</span></span><span><span>		}
</span></span><span><span>		<span>return</span> strLocation;
</span></span><span><span>	}
</span></span><span><span>	<span>catch</span> (Exception ex)
</span></span><span><span>	{
</span></span><span><span>		Console.<span>WriteLine</span>(ex.<span>Message</span>);
</span></span><span><span>		<span>return</span> strLocation;
</span></span><span><span>	}
</span></span><span><span>}
</span></span></code></pre></div><p>This is also save within the Cosmos DB as seen in already present locations:</p><div><pre tabindex="0"><code data-lang="json"><span><span>{
</span></span><span><span>  <span>"id"</span>: <span>"29c01f2e-f4cd-4671-9257-a7432b15fc0d"</span>,
</span></span><span><span>  <span>"EventTypeId"</span>: <span>"1"</span>,
</span></span><span><span>  <span>"DeviceGuid"</span>: <span>"3cf4caf5-3f6e-4199-bbf0-ba1abe69a6e9"</span>,
</span></span><span><span>  <span>"WunderkekzId"</span>: <span>"994"</span>,
</span></span><span><span>  <span>"GeoLocation"</span>: <span>"48,xxxxxxxxxxxxxx:11,xxxxxxxxxxxxxx"</span>,
</span></span><span><span>  <span>"UploadedAt"</span>: <span>"2023-10-24T21:03:28.4478194+02:00"</span>,
</span></span><span><span>  <span>"_rid"</span>: <span>"E9MJAL2dw5WRAAAAAAAAAA=="</span>,
</span></span><span><span>  <span>"_self"</span>: <span>"dbs/E9MJAA==/colls/E9MJAL2dw5U=/docs/E9MJAL2dw5WRAAAAAAAAAA==/"</span>,
</span></span><span><span>  <span>"_etag"</span>: <span>"\"3e009e56-0000-0d00-0000-653815010000\""</span>,
</span></span><span><span>  <span>"_attachments"</span>: <span>"attachments/"</span>,
</span></span><span><span>  <span>"_ts"</span>: <span>1698174209</span>
</span></span><span><span>}
</span></span><span><span>{
</span></span><span><span>  <span>"id"</span>: <span>"e3e1482f-1b62-4780-aff1-70593aa79d56"</span>,
</span></span><span><span>  <span>"EventTypeId"</span>: <span>"1"</span>,
</span></span><span><span>  <span>"DeviceGuid"</span>: <span>"a7bc4474-570d-4ece-b1f5-aa3e9a36c1ce"</span>,
</span></span><span><span>  <span>"WunderkekzId"</span>: <span>"995"</span>,
</span></span><span><span>  <span>"GeoLocation"</span>: <span>"48,xxxxxxxxxxxxxx:11,xxxxxxxxxxxxxx"</span>,
</span></span><span><span>  <span>"UploadedAt"</span>: <span>"2023-10-24T21:17:59.1191164+02:00"</span>,
</span></span><span><span>  <span>"_rid"</span>: <span>"E9MJAL2dw5WSAAAAAAAAAA=="</span>,
</span></span><span><span>  <span>"_self"</span>: <span>"dbs/E9MJAA==/colls/E9MJAL2dw5U=/docs/E9MJAL2dw5WSAAAAAAAAAA==/"</span>,
</span></span><span><span>  <span>"_etag"</span>: <span>"\"3e009f58-0000-0d00-0000-653818690000\""</span>,
</span></span><span><span>  <span>"_attachments"</span>: <span>"attachments/"</span>,
</span></span><span><span>  <span>"_ts"</span>: <span>1698175081</span>
</span></span><span><span>}
</span></span></code></pre></div><p>Because the Geolocation data is only cross-referenced with die Device GUID and not the content itself, the enforcement for regional content does not make sense in their newest copy of the privacy policy.</p><h3 id="pii-data">PII Data</h3><p>There is some PII Data involved, but i think those are just test data from some ordering processes. Even the headphones can only be cross-linked to the geolocation data and not the content played.
It could maybe be cross-referenced with the time, but haven’t checked into that, as the main concern is the data being public.</p><h3 id="who-is-special">Who is Special?</h3><p>In addition, the connection string to this Azure cosmos database is accessible within the Decompilation of the application itself and therefore is disclosed.</p><p>So everybody looking into the source code of the application can get information on usage of the headphones, location of some of the headphones and files listened too.</p><p><img src="https://nv1t.github.io/blog/img/2024/ddb71ee1385f009813e51273e8dd6d1d.png" alt="Heatmap of all Geolocation Data in Germany from the headphones usage">
There is some usage in Dublin as well, but only wanted to include the DACH Region.</p><p>Normalizing ID3 Tag Meta Data from various sources is really cumbersome. I tried, but i gave up pretty quickly, because it was just my own curiosity what kids are listening to these days. Let me say that: Bibi Blocksberg, Bibi &amp; Tina, Benjaming Bluemchen, Paw Patrol, Drei Fragezeichen, and various songs, are the all time favourits. (for the english speaking community: except Paw Patrol, are all Children Listening experiences from germany, which exist since 1970 or 1980)</p><h2 id="disclosure">Disclosure</h2><p><strong>19.10.2023</strong>: i reached out to the CTO of Kekz, who told me, he developed the headphones, but is no longer associated with the company. To my knowledge, he forwarded the information to the CEOs.
Never heard back.</p><p><strong>27.02.2024</strong>: i reached out to the CEOs of Kekz (Adin and Carl) with my security concerns. Never heard back.
-&gt; A few weeks later, the privacy policy was changed to include the Kekz-App, therefore i conclude my email was read</p><h2 id="open-questions">Open Questions</h2><ul><li>What is the full functionality of the Jieli-Chip? These chips are strange and challenging to identify. I only guessed which Chip it could be and got lucky with the HID Interface through the Windows Application</li><li>What does the other Jieli-Chip on the other PCB Do?</li><li>A full application to create a custom SD Card with a content manager to not only support the content already present on the Kekz Headphones, but furthermore all non-taken directories.</li><li>Are there more HID commands?</li><li>How good is the Geolocation Data sourced from a Laptop, which is probably triangulated Wifi Signals? Do the 30m-500m from the privacy policy hold up, or can it be narrowed down?</li><li>What is this PII Data in the Azure cosmos database?</li></ul><p>There are most likely more open questions on this one. You are welcome to research further on your own.</p><h2 id="references">References</h2><ul><li><strong>Kekz Information:</strong><ul><li><a href="https://futurezone.at/produkte/kekz-kopfhoerer-im-test-kinder-in-ihrer-eigenen-welt/401978339">https://futurezone.at/produkte/kekz-kopfhoerer-im-test-kinder-in-ihrer-eigenen-welt/401978339</a> (first article i have read about the Headphones)</li><li><a href="https://stadt-bremerhaven.de/kekz-drahtlose-kinderkopfhoerer-nach-dem-tonies-prinzip/">https://stadt-bremerhaven.de/kekz-drahtlose-kinderkopfhoerer-nach-dem-tonies-prinzip/</a> (Kekz article of Caschys blog, with wrong information in the comments on how they operate)</li><li><a href="https://store.kekz.com/haendlersuche/">https://store.kekz.com/haendlersuche/</a> (Kekz - Store search)</li><li><a href="https://apps.microsoft.com/detail/9NXL6Q53G5RX?hl=de-de&amp;gl=DE">https://apps.microsoft.com/detail/9NXL6Q53G5RX?hl=de-de&amp;gl=DE</a> (Kekz application)</li></ul></li><li><strong>Checking MP3s for validity:</strong><ul><li><a href="https://github.com/Sjord/checkmate">https://github.com/Sjord/checkmate</a> (checks every frame, but not python variant exists)</li><li><a href="https://peterextexia.com/blog/verifying-that-an-mp3-file-is-valid-in-python/">https://peterextexia.com/blog/verifying-that-an-mp3-file-is-valid-in-python/</a> (it kinda works, but get’s false positives and because some mp3 decode as valid, this does not work)</li></ul></li><li><strong>Jieli-Chip:</strong><ul><li><a href="https://www.zh-jieli.com/">https://www.zh-jieli.com/</a> (chip production)</li><li><a href="http://www.yunthinker.com/FileUpLoad/DownLoadInfosFile/637729990813300469.pdf">http://www.yunthinker.com/FileUpLoad/DownLoadInfosFile/637729990813300469.pdf</a> (potential Chip)</li><li><a href="https://github.com/christian-kramer/JieLi-AC690X-Familiarization">https://github.com/christian-kramer/JieLi-AC690X-Familiarization</a> (Adventures in figuring out how this incredibly ubiquitous, yet incredibly mysterious integrated circuit works.)</li><li><a href="https://github.com/kagaimiq/jl-uboot-tool/tree/main">https://github.com/kagaimiq/jl-uboot-tool/tree/main</a> (chip and protopcol description)</li><li><a href="https://github.com/kagaimiq/jielie/tree/main">https://github.com/kagaimiq/jielie/tree/main</a> (jielie nice!)</li><li><a href="https://el.jibun.atmarkit.co.jp/thousandiy/2022/09/18_bluetooth_audio_soc.html">https://el.jibun.atmarkit.co.jp/thousandiy/2022/09/18_bluetooth_audio_soc.html</a> (Which Chips are built into cheap Bluetooth Speaker)</li></ul></li><li><strong>Misc:</strong><ul><li><a href="https://www.luther-lawfirm.com/newsroom/blog/detail/reverse-engineering-nach-dem-geschaeftsgeheimnisgesetz-geschgehg-vertragliche-ausschlussmoeglichkeiten#:~:text=a%20GeschGehG%20ist%20das%20Reverse,Gegenst%C3%A4nden%20uneingeschr%C3%A4nkt%20erlaubt">https://www.luther-lawfirm.com/newsroom/blog/detail/reverse-engineering-nach-dem-geschaeftsgeheimnisgesetz-geschgehg-vertragliche-ausschlussmoeglichkeiten#:~:text=a%20GeschGehG%20ist%20das%20Reverse,Gegenst%C3%A4nden%20uneingeschr%C3%A4nkt%20erlaubt</a>. (Reverse Engineering Rechtliche Lage)</li></ul></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Experimental web browser optimized for rabbit-holing (368 pts)]]></title>
            <link>https://szymonkaliski.com/projects/cartographist/</link>
            <guid>41738502</guid>
            <pubDate>Fri, 04 Oct 2024 06:47:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://szymonkaliski.com/projects/cartographist/">https://szymonkaliski.com/projects/cartographist/</a>, See on <a href="https://news.ycombinator.com/item?id=41738502">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Cartographist is an experimental web browser optimized for rabbit-holing.</p>
<ul>
<li>Instead of opening new windows (with <code>cmd</code>-click), Cartographist spawns horizontally scrollable panes.</li>
<li>Instead of forcing you to find things in a linear history, Cartographist shows a tree-structured outline of your browsing:
<img src="https://szymonkaliski.com/projects/cartographist/history.jpg" loading="lazy" width="720" height="280"></li>
<li>Instead of always starting fresh, Cartographist can save, and load "trails" — the exact state of the session you've left — supporting researching topics over long periods of time.</li>
</ul>
<p>For more context about the project, you can check out the longer write-up below, which originally appeared as a part of <a href="https://szymonkaliski.com/newsletter/2022-01-03-q4-2021/"><span>Q4 2021</span></a> newsletter.</p>

<h2>Transcluded from <a href="https://szymonkaliski.com/newsletter/2022-01-03-q4-2021/#cartographist"><span>Cartographist</span></a></h2>
          
<p>During the summer of 2020 I played around with an idea for research-focused web browser, embodying some of the concepts from <a href="https://szymonkaliski.com/notes/browsing-vs-searching/"><span>Browsing vs Searching</span></a> note — <em>browsing</em> being an open-ended divergent activity, and <em>searching</em> understood as information retrieval.</p>

<p><a href="https://twitter.com/szymon_k/status/1289942401318977537" target="_blank"><span>I shared a preview on Twitter</span><span>&nbsp;↗</span></a>, to a surprisingly overwhelming response, but I got distracted with other things and never got back to the project. I still occasionally get requests for sharing this, so here it is: <a href="https://github.com/szymonkaliski/cartographist" target="_blank"><span>szymonkaliski/Cartographist</span><span>&nbsp;↗</span></a>.</p>
<p>The main idea of browsing in panes was inspired by <a href="http://andymatuschak.org/" target="_blank"><span>Andy Matuschak's website layout</span><span>&nbsp;↗</span></a> and some of <a href="http://nateparrott.com/" target="_blank"><span>Nate Parrot's</span><span>&nbsp;↗</span></a> experiments around stacking mobile web browser views next to each other (which I can't seem to find anymore, sorry).
This sort of layout has a long history, starting with <a href="https://en.wikipedia.org/wiki/Miller_columns" target="_blank"><span>Miller columns</span><span>&nbsp;↗</span></a> and the original Smalltalk class browser, and is a great interface for detail-in-context browsing.</p>
<p>As an aside, I also use this technique for navigating code with Vim, where a single shortcut goes to a definition of a function in a new pane:</p>
<p><img src="https://szymonkaliski.com/newsletter/2022-01-03-q4-2021/vim-panes.jpg" loading="lazy" width="3536" height="1790"></p><p>(<a href="https://gtoolkit.com/" target="_blank"><span>Glamorous Toolkit</span><span>&nbsp;↗</span></a> is worth checking out, as it takes the idea of pane browsing to another level)</p>
<p>In theory, I also really like the idea of disk-persisted history which allows for going back to a browsing session after a while, and consciously deciding which "topic" I am in.</p>
<p>Unfortunately, in practice, I don't think having the full history is that useful.
Yes, it's sometimes good to know how you ended up somewhere, but I think what's most valuable about "research" is the synthesis part — grabbing parts of larger wholes, rearranging, recombining, thinking with the material.
A small step in this direction could be persisting scroll position or maybe selection, and making the history editable — allowing users to remove dead ends, add notes, etc.</p>
<p>Additionally, it started to feel that I'm solving this problem on a wrong level.
For example, a good window manager could replace Cartographist almost completely —
I played around with columnar layout in <a href="https://szymonkaliski.com/projects/hhtwm/"><span>HHTWM</span></a> for a bit, but lack of horizontal scrolling makes it not that useful in the end.</p>
<p>Well, let me know if you have any ideas how to make Cartographist better! Is there anything interesting here that I'm not seeing? Could it be useful to you in any way?</p>
<hr>
<p>Code is open sourced: <a href="https://github.com/szymonkaliski/cartographist" target="_blank"><span>szymonkaliski/Cartographist</span><span>&nbsp;↗</span></a>.</p><h2><a href="#backlinks" id="backlinks"><span>Backlinks</span></a></h2><ol><li><a href="https://szymonkaliski.com/newsletter/2022-01-03-q4-2021/#:~:text=I%20shared%20a%20preview,here%20it%20is%3A%20szymonkaliski%2FCartographist.&amp;text=Additionally%2C%20it%20started%20to,useful%20in%20the%20end.&amp;text=Well%2C%20let%20me%20know,you%20in%20any%20way%3F"><span><span>2022-01-03</span><span><span>A Dog, Short Ramble on "Programming", MIDI→CV, and a Rabbit-Holing Web Browser</span><sup><span>3</span></sup></span></span></a></li></ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New research says blue zones can be explained by comically flawed data (141 pts)]]></title>
            <link>https://www.independent.co.uk/life-style/blue-zones-netflix-ignobel-prize-b2622952.html</link>
            <guid>41738434</guid>
            <pubDate>Fri, 04 Oct 2024 06:32:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/life-style/blue-zones-netflix-ignobel-prize-b2622952.html">https://www.independent.co.uk/life-style/blue-zones-netflix-ignobel-prize-b2622952.html</a>, See on <a href="https://news.ycombinator.com/item?id=41738434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><div data-newsletter-key="receiveindylifestyle" data-component="Newsletter" data-loading="lazy" data-theme-name="base"><p><img src="https://static.independent.co.uk/static-assets/images/newsletter/lifestyle/lessons-in-lifestyle.png" loading="lazy" alt="Lessons in Lifestyle"></p><div><div><p><h3 data-nosnippet="">Stay ahead of the curve with our weekly guide to the latest trends, fashion, relationships and more</h3><h3>Stay ahead of the curve with our weekly guide to the latest trends, fashion, relationships and more </h3><h3 data-nosnippet="">Stay ahead of the curve with our weekly guide to the latest trends, fashion, relationships and more </h3></p></div><div role="form" aria-label="Newsletter signup form" id="reg-lite-form"><p><label for="newsletter-checkbox">I would like to be emailed about offers, events and updates from The Independent. Read our&nbsp;<a target="_blank" rel="noreferrer" href="https://www.independent.co.uk/service/privacy-policy-a6184181.html">privacy policy</a></label></p></div></div></div><p><span>T</span>hey were supposed to be real-life fountains of youth. In March 2000 the term <a href="https://www.independent.co.uk/life-style/health-and-families/blue-zones-netflix-diet-documentary-b2408185.html">“Blue Zone”</a> was first used to describe <a href="https://www.independent.co.uk/topic/sardinia">Sardinia</a>, an Italian island that appeared to be home to a statistically improbable number of people living past the age of 100. In the decades since, four more areas have been identified around the globe where locals apparently have an increased chance of becoming a centenarian: <a href="https://www.independent.co.uk/topic/okinawa">Okinawa</a> in Japan; Nicoya in <a href="https://www.independent.co.uk/topic/costa-rica">Costa Rica</a>; <a href="https://www.independent.co.uk/topic/ikaria">Ikaria</a> in Greece and Loma Linda in <a href="https://www.independent.co.uk/topic/california">California</a>. These so-called Blue Zones have inspired countless studies, <a href="https://www.independent.co.uk/arts-entertainment/books/reviews/the-blue-zones-solution-eating-and-living-like-the-world-s-healthiest-people-by-dan-buettner-book-review-bluezone-thinking-for-a-long-and-healthy-existence-10224244.html">cookbooks</a>, travel stories and even their own Netflix documentary series (<a href="https://www.independent.co.uk/life-style/health-and-families/blue-zones-netflix-diet-documentary-b2408185.html">2023’s <em>Live to 100: Secrets of the Blue Zones</em></a>). The trouble is, the outlandish claims about the life-giving properties of these regions just don’t stand up to close scrutiny.</p><p>Last month, Dr Saul Newman of the Oxford Institute of Population Ageing was awarded the Ig Nobel Prize for his work debunking Blue Zones. Newman’s investigation into serious flaws in the data about the world’s oldest people saw him take home an award that has been handed out since 1991 for scientific research that “makes people laugh, and then think.” Newman says that when he looked into the claims about Blue Zones he found a pattern of significant data being routinely ignored if it didn’t fit the desired narrative, and statistical anomalies that could be better explained by administrative errors or cases of pension fraud. “It’s as if you gave the captain of the Titanic nine goes at it and he’s smacked into the iceberg every time,” Newman tells <em>The Independent </em>of the research. “What’s most astounding is that nobody in the academic community seems to have thought it’s ridiculous before this. It’s absurd.”</p><p>Take Sardinia, the original Blue Zone. While it was purported to be home to crowds of centenarians, European Union figures show that the island only ranks around 36-44th for longevity in the continent. Many of those who were supposed to have reached very old age in their Italian idyll turned out to in fact be dead, they just hadn’t been reported as such to the authorities. “Sometimes the mafia is involved, sometimes it’s carers,” says Newman. “There’s a lot of cases in Italy where younger relatives have just kept claiming the pension even though granddad’s out the back in the olive garden.”</p><div><figure><div data-gallery-length="3"><p><img src="https://static.independent.co.uk/2024/10/02/23/GettyImages-1235544411.jpg" srcset="https://static.independent.co.uk/2024/10/02/23/GettyImages-1235544411.jpg?quality=75&amp;width=320&amp;auto=webp 320w, https://static.independent.co.uk/2024/10/02/23/GettyImages-1235544411.jpg?quality=75&amp;width=640&amp;auto=webp 640w" loading="lazy" alt="Natividad Talia Matarrita Fonseca, 93, at home in the supposed ‘Blue Zone’ of Nicoya, Costa Rica"></p></div><figcaption>Natividad Talia Matarrita Fonseca, 93, at home in the supposed ‘Blue Zone’ of Nicoya, Costa Rica<span> <!-- -->(<!-- -->Ezequiel Becerra/AFP via Getty Images<!-- -->)</span></figcaption></figure></div><p>Something similar seems to have been happening in Nicoya in Costa Rica, where around 40-50% of centenarians were found to have misreported their ages, and on the Greek island of Ikaria. In 2015, Germany requested that Greece audit their spending as a condition of their bailout during the financial crisis. Until then, Greece had been paying pensions to around 9,000 centenarians. After the audit, that figure dropped by 72%. “That number comes from the Greek minister who was handing out the pensions,” says Newman. “He’s the guy with the most incentive to minimize the problem of anybody in the world, and he’s saying that 72% are rubbish.”</p><figure><span><svg xmlns="http://www.w3.org/2000/svg" id="7892fd45317565c6" viewBox="0 0 80 47"><path fill="#ec1a2e" d="M21.18 46.99c9.4 0 17.18-7.73 17.18-17.13 0-9.46-7.72-17.12-17.12-17.12A17.2 17.2 0 0 0 3.99 29.86c0 3.74 1.29 7.47 3.48 10.5l-.13.12A23.6 23.6 0 0 1 1.29 24.4c0-12.75 10.36-23.3 23.1-23.3a24 24 0 0 1 11.53 2.89l.57-.96A26 26 0 0 0 24.33 0 24.3 24.3 0 0 0 0 24.4c0 14.09 9.72 22.59 21.18 22.59m41.47 0c9.4 0 17.18-7.73 17.18-17.13 0-9.46-7.72-17.12-17.12-17.12a17.2 17.2 0 0 0-17.25 17.12c0 3.74 1.29 7.47 3.48 10.5l-.13.12a23.6 23.6 0 0 1-6.05-16.08c0-12.75 10.36-23.3 23.1-23.3a24 24 0 0 1 11.53 2.89l.58-.96A26 26 0 0 0 65.8 0a24.33 24.33 0 0 0-24.33 24.4c0 14.09 9.72 22.59 21.18 22.59"></path></svg></span><div><blockquote><p>When they’ve measured Okinawa it doesn’t do any of the things they claim Blue Zones do. Not even close. It’s comedically wrong</p></blockquote></div><p>Dr Saul Newman</p></figure><p>Claims about the Blue Zones have gone far beyond simply saying that people in the regions live longer than average. Author Dan Buettner, who founded the Blue Zones LLC marketing company in 2008, has outlined what he calls the “Power 9” factors prevalent in these areas that contribute to long, healthy lives. These are: exercising by moving naturally, having a purpose, “downshifting” routines to reduce stress, stopping eating when 80% full, eating largely plant-based diets, drinking 1-2 glasses of wine per day, a sense of religious faith or belonging, putting their families first and living in positive social networks.</p><p>However, when Newman looked into whether these characteristics are true in Okinawa, he found a wide gap between the claims and reality. “There’s some extraordinary cognitive dissonance going on,” he says. “The Japanese run one of the largest and longest-running nutritional surveys in the world. It covers 96% of their citizens, and when they’ve measured Okinawa it doesn’t do any of the things they claim Blue Zones do. Not even close. It’s comedically wrong.”</p><p>Take for example the claim made on <a rel="nofollow" target="_blank" href="https://www.bluezones.com/2016/11/power-9/">BlueZones.com</a> that Okinawans are disproportionately filled with “Ikigai”, a sense of purpose in their life. That’s just not true: in fact, Okinawa has the 4th highest suicide rate in Japan. Similarly, claims that the area is particularly religious don’t hold water. “They’re the least religious place in Japan,” says Newman. “93% atheist.”</p><p>How did the idea that Okinawa is a great place to grow old take root in the first place? Newman attributes it in part to the fire-bombing of the area by the US during World War II, which caused the destruction of countless birth certificates and other records. “Within Okinawa, the distribution of centenarians is predicted by who had their Hall of Records blown up,” he says. “You have an occupying army of GIs who don’t really speak Japanese replacing birth certificates because they’ve just blown them sky high. In a town where this has happened, you have more centenarians.”</p><div><figure><div data-gallery-length="3"><p><img src="https://static.independent.co.uk/2024/10/02/23/GettyImages-1366067630.jpg" srcset="https://static.independent.co.uk/2024/10/02/23/GettyImages-1366067630.jpg?quality=75&amp;width=320&amp;auto=webp 320w, https://static.independent.co.uk/2024/10/02/23/GettyImages-1366067630.jpg?quality=75&amp;width=640&amp;auto=webp 640w" loading="lazy" alt="US Marines pass each other going to and from the battlefront during the Battle of Okinawa circa June 1945"></p></div><figcaption>US Marines pass each other going to and from the battlefront during the Battle of Okinawa circa June 1945<span> <!-- -->(<!-- -->Keystone/Hulton Archive/Getty Images<!-- -->)</span></figcaption></figure></div><p>This wouldn’t be the first time that dubious record-keeping has led to false claims of extreme longevity. For decades, the Guinness Book of Records stated that the world’s oldest man was Pierre Joubert, who had lived to the grand old age of 113 years, 124 days. It later turned out that record keepers had conflated a father’s birth date with the death date of his son, who was also named Pierre Joubert. In fact, the elder Joubert had died at the more reasonable age of 65 – a fact that had been written on his death certificate the whole time, if anybody had bothered to check.</p><p>For Newman, claims about the health benefits of the supposed Blue Zone lifestyle lose all credibility when you understand how unsupported the premise is by empirical data. “Firstly, does lying to the public matter?” he asks. “Secondly, the astounding thing is that one of the guidelines is that you should drink every day at twice the NHS heavy drinking guidelines. That is a recipe for alcoholism. It’s mad that it’s being propelled as health advice. If you were a doctor telling your patient to drink every day, you’d get disbarred.”</p><p>The suggestion that daily boozing is a cornerstone of a healthy lifestyle is particularly strange when you consider the fifth Blue Zone, Loma Linda. This small city in Southern California is home to a large community of tee-total Seventh Day Adventists. The area was added to the list in 2008, and then in 2020 Buettner sold Blue Zones LLC to Adventist Health, the health care system of the Seventh Day Adventist Church. “Why are Seventh Day Adventists, who claim this as their core belief, pushing people to drink?” asks Newman. “They’re supposed to be sober. It’s just baffling to me.”</p><p><em>The Independent </em>has approached Blue Zones LLC for comment.</p><p>In the end, it might be time to retire the concept of Blue Zones and write the whole thing off as a collective case of wishful thinking. “People do not want to go jogging,” says Newman. “They don’t want to give up drinking. They don’t want to give up smoking. They want there to be some far flung, exotic island where everything’s okay and if you eat the goji berries you’re gold. It’s a nice dream, and it has always, throughout the entirety of history, sold well. But is it true? I would suggest jogging.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Correcting the record for Continue and PearAI (147 pts)]]></title>
            <link>https://www.ycombinator.com/blog/correcting-the-record/</link>
            <guid>41737326</guid>
            <pubDate>Fri, 04 Oct 2024 02:50:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ycombinator.com/blog/correcting-the-record/">https://www.ycombinator.com/blog/correcting-the-record/</a>, See on <a href="https://news.ycombinator.com/item?id=41737326">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Why does man print "gimme gimme gimme" at 00:30? (2017) (638 pts)]]></title>
            <link>https://unix.stackexchange.com/questions/405783/why-does-man-print-gimme-gimme-gimme-at-0030</link>
            <guid>41736903</guid>
            <pubDate>Fri, 04 Oct 2024 01:25:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unix.stackexchange.com/questions/405783/why-does-man-print-gimme-gimme-gimme-at-0030">https://unix.stackexchange.com/questions/405783/why-does-man-print-gimme-gimme-gimme-at-0030</a>, See on <a href="https://news.ycombinator.com/item?id=41736903">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>This is an easter egg in <code>man</code>. When you run <code>man</code> without specifying the page or with <code>-w</code>, it outputs "gimme gimme gimme" to stderr, but only at 00:30:</p>

<pre><code># date +%T -s "00:30:00"
00:30:00
# man -w
gimme gimme gimme
/usr/local/share/man:/usr/share/man:/usr/man
</code></pre>

<p>The exit code is always 0.</p>

<p>The correct output should always be:</p>

<pre><code># man -w
/usr/local/share/man:/usr/share/man:/usr/man
# echo $?
0
# man
What manual page do you want?
# echo $?
1
</code></pre>

<p>The string "gimme gimme gimme" can be found in RHEL, OpenSUSE, Fedora, Debian and probably more, so it's not really distro specific. You can <code>grep</code> your <code>man</code> binary to verify.</p>

<p><a href="http://git.savannah.nongnu.org/cgit/man-db.git/tree/src/man.c#n4122" rel="noreferrer">This code is responsible for the output</a>, added by <a href="http://git.savannah.nongnu.org/cgit/man-db.git/commit/src/man.c?id=002a6339b1fe8f83f4808022a17e1aa379756d99" rel="noreferrer">this commit</a>:</p>

<pre><code>src/man.c-1167- if (first_arg == argc) {
src/man.c-1168-   /* 
http://twitter.com/#!/marnanel/status/132280557190119424 */
src/man.c-1169-   time_t now = time (NULL);
src/man.c-1170-   struct tm *localnow = localtime (&amp;now);
src/man.c-1171-   if (localnow &amp;&amp;
src/man.c-1172-       localnow-&gt;tm_hour == 0 &amp;&amp; localnow-&gt;tm_min == 30)
src/man.c:1173:     fprintf (stderr, "gimme gimme gimme\n");
</code></pre>

<p>I have contacted RHEL support about this issue.</p>

<p>The string comes from well known <a href="https://www.youtube.com/watch?v=XEjLoHdbVeE&amp;t=1m10s" rel="noreferrer">ABBA song Gimme! Gimme! Gimme! (A Man After Midnight)</a>.</p>

<hr>

<p>The developer of the man-db, Colin Watson, decided that there was enough fun and the story won't get forgotten and <a href="https://git.savannah.gnu.org/cgit/man-db.git/commit/?id=b225d9e76fbb0a6a4539c0992fba88c83f0bd37e" rel="noreferrer">removed the easter egg completely</a>.</p>

<p>Thank you Colin!</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cox slows Internet speeds in entire neighborhoods to punish any heavy users (238 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2020/06/cox-slows-internet-speeds-in-entire-neighborhoods-to-punish-any-heavy-users/</link>
            <guid>41736215</guid>
            <pubDate>Thu, 03 Oct 2024 23:38:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2020/06/cox-slows-internet-speeds-in-entire-neighborhoods-to-punish-any-heavy-users/">https://arstechnica.com/tech-policy/2020/06/cox-slows-internet-speeds-in-entire-neighborhoods-to-punish-any-heavy-users/</a>, See on <a href="https://news.ycombinator.com/item?id=41736215">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="1682027">
  
  <header>
  <div>
    

    

    <p>
      Cox warns customers to lower usage, imposes 10Mbps upload limit on “gigabit” plan.
    </p>

    

    <div>
            <p><a data-pswp-width="2124" data-pswp-height="1411" data-pswp-srcset="https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut.jpg 2124w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-300x199.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-640x425.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-768x510.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-1536x1020.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-2048x1361.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-980x651.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-1440x957.jpg 1440w" data-cropped="false" href="https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut.jpg" target="_blank">
              <img width="2124" height="1411" src="https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut.jpg" alt="A pair of scissors cutting an Ethernet cable." loading="eager" decoding="async" fetchpriority="high" srcset="https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut.jpg 2124w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-300x199.jpg 300w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-640x425.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-768x510.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-1536x1020.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-2048x1361.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-980x651.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2020/06/getty-ethernet-cable-cut-1440x957.jpg 1440w" sizes="(max-width: 2124px) 100vw, 2124px">
            </a></p>
          </div>

    
  </div>
</header>

  

  
      
    
    <div>
                      
                      
          <p>Cox Communications is lowering Internet upload speeds in entire neighborhoods to stop what it considers "excessive usage," in a decision that punishes both heavy Internet users and their neighbors.</p>
<p>Cox, a cable company with about 5.2 million broadband customers in the United States, has been sending notices to some heavy Internet users warning them to use less data and notifying them of neighborhood-wide speed decreases. In the case we will describe in this article, a gigabit customer who was paying $50 extra per month for unlimited data was flagged by Cox because he was using 8TB to 12TB a month.</p>
<p>Cox responded by lowering the upload speeds on the gigabit-download plan from 35Mbps to 10Mbps for the customer's whole neighborhood. Cox confirmed to Ars that it has imposed neighborhood-wide slowdowns in multiple neighborhoods in cases like this one but didn't say how many excessive users are enough to trigger a speed decrease.</p>
<p>Mike, a Cox customer from Gainesville, Florida, pays $150 a month, including $100 for 1Gbps download speeds and 35Mbps upload speeds, and another $50 for "unlimited data" so that he can go over <a href="https://www.cox.com/residential/internet/learn/data-usage.html">Cox's 1TB data cap</a>. Mike told Ars via email that most of his 8TB+ monthly use consists of scheduled device backups and "data sharing via various (encrypted) information-sharing protocols," such as peer-to-peer networks, between 1am and 8am. (We agreed to publish Mike's first name only but reviewed his bills and confirmed the basic details of his account with Cox.)</p>
<p>Generally speaking, data usage for most households declines significantly during those 1am-8am overnight hours, so a robustly built broadband network should be able to handle the traffic. In any case, Mike couldn't use more than 35Mbps for uploads at any given time because that's the limit Cox always imposed on its gigabit-download cable plan. Mike said his household's daytime and evening use is more like a typical Internet user's, with work-from-home activities during the day and streaming video in high-definition during the evening.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>Mike also said his level of Internet usage has been roughly the same for the past four years that he's been using Cox—but it was only in mid-May that the company flagged him for excessive use. This may suggest that Cox is struggling to handle pandemic-level broadband traffic, but Cox says that the vast majority of its network is "performing very well."</p>
<p>Cox provided a little more detail after this story published, saying that the neighborhood-wide slowdowns and disconnection threats sent to individual customers "are two separate initiatives that could cross over in some cases."</p>
<p>(<em>Clarification</em>: Mike's nightly uploads alone couldn't have accounted for more than half of his monthly 8TB+ usage if his upload speeds were capped at 35Mbps; seven hours of nightly uploads at that rate would amount to about 3.3TB per month. The nightly uploads may have accounted for most of his upload usage, however, as his monthly usage of 8TB to 12TB includes both downloads and uploads. Assuming Mike was also downloading heavily during those 1-8am hours, then the overnight usage including both downloads and uploads would account for most of his overall data usage.)</p>
<h2>“Scheduled for termination”</h2>
<p>First, Mike got three calls from Cox including one that left a voicemail saying, "we need to speak with you regarding your Internet usage. Your home is using an extraordinarily high amount of Internet data and adjustments need to be made immediately." The voicemail warned that your "Internet will be scheduled for termination" unless usage reductions are "made within five days," according to Mike.</p>
<p>Mike explained how he responded:</p>
<blockquote><p>Since I work from home, I naturally was very concerned they would pull the plug on me and I'd be unable to work. Immediately calling the number [provided in the voicemail], I was funneled directly to a department for "questions about your recent Internet speed changes," and spoke with a representative there. He went on to explain that their network is overburdened and since I was an above-average user, I was being targeted to lower my usage or else have my account terminated... I tried to explain that my usage is not out of the ordinary for me. My day-time bandwidth usage is paltry (most of my bandwidth consumption is scheduled from 1am-8am), and that Cox should have been upgrading their infrastructure instead of oversubscribing nodes and pocketing the record revenue. I was told if I did not make a substantial decrease in my upload data usage, my service would be terminated.</p></blockquote>
<p>Comments in a <a href="https://www.reddit.com/r/GNV/comments/gkicjg/cox_internet_speeds_lately/fr670cx/">Reddit thread last month</a> confirm that Mike isn't the only Cox customer being warned to cut upload speeds in order to avoid being kicked off the network.&nbsp;Cox didn't tell Mike exactly how much data he'd have to shave off his monthly usage. There was "no magic number or threshold, just an arbitrary amount of decrease, a Cox-deemed 'good effort,'" or his service would be cut off, he said.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Shortly after that phone call, Mike received an email from Cox with the subject line, "Alert: Action required to continue your Internet service." Mike provided Ars with a copy of the email.</p>
<p>"We've recently tried getting in touch with you about your service—your account has been identified as using an extremely high level of bandwidth, which is causing a negative impact on our network and our other customers across your neighborhood," the email said. Mike's "extraordinarily high" upload usage "is negatively impacting Internet service of other customers, which is a violation of our <a href="https://www.cox.com/aboutus/policies/acceptable-use-policy.html">Acceptable Use Policy</a>, the email said. The policy contains a broad prohibition on transmitting amounts of data large enough to disrupt the network, but it doesn't specify an amount.</p>
<p>The real kicker is that Cox's email to Mike said that everyone in his neighborhood will get lower upload speeds until July 15:</p>
<blockquote><p>During these unprecedented times, many people are working and schooling from home, and maintaining connectivity is important. We are working to provide a positive Internet experience for everyone, so we've adjusted our Gigablast upload speeds in your neighborhood from 35Mbps to 10Mbps, now through July 15, 2020. Your download speeds have not changed.</p></blockquote>
<p>Cox's email doesn't specifically state that Mike's usage spurred the decision to impose a neighborhood-wide slowdown, but this is apparently only happening in a small percentage of neighborhoods where Cox has seen heavier use than elsewhere in its network.</p>
<h2>Questions for Cox</h2>
<p>This raises several questions that we asked Cox. We asked the cable company why its network is "unable to handle Mike's uploads in the middle of the night" and whether it has "considered adding capacity to its network instead of forcing unlimited-data customers to use less data." We asked Cox how much data, specifically, customers who pay for unlimited data are actually allowed to use, and "Why isn't Mike allowed to use unlimited data when he is paying for the highest speeds and paying extra for unlimited data?"</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>We also asked why Cox is imposing slowdowns throughout entire neighborhoods instead of only on the people allegedly violating the Acceptable Use Policy and whether the slowdowns are imposed even when only a single customer in a neighborhood is flagged for excessive usage. We also asked how many people in Mike's neighborhood are affected by the upload-speed decrease and whether they will get discounts to reflect their reduced service.</p>
<p>Cox didn't provide as much detail as we were looking for, but it confirmed the neighborhood-wide speed decreases, saying it has "identified a small number of neighborhoods where performance can be improved for all customers in the neighborhood by temporarily increasing or maintaining download speeds and changing upload speeds for some of our service tiers."</p>
<p>Cox defended the temporary 10Mbps upload speed for its gigabit-download plan, saying that "10Mbps is plenty of speed for the vast majority of customers to continue their regular activity and have a positive experience." Of course, customers paying extra for Cox's fastest plan and unlimited data are more likely to be outliers who do need high upstream bandwidth. Unlike fiber-to-the-home service, in which ISPs offer symmetrical upstream and downstream speeds, cable service generally has much lower uploads than downloads. Cox offers symmetrical gigabit speeds in some areas where it has deployed fiber directly to homes but provides slower upload speeds on its cable network. Cable users may eventually get symmetrical upload and download speeds from an <a href="https://arstechnica.com/information-technology/2019/01/envious-of-5g-hype-cable-cos-unveil-potentially-confusing-10g-trademark/">upgrade to DOCSIS</a>, the Data Over Cable Service Interface Specification.</p>
<p>A <a href="https://www.cox.com/aboutus/policies/speeds-and-data-plans.html">Cox webpage</a> that was updated on April 30 says that the gigabit plan's upload speeds are now "10Mbps in limited areas to support consistent service across customers during periods of sustained increased Internet usage."</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>The now-repealed net neutrality rules likely wouldn't have prevented this kind of data slowdown, as the slowdown would presumably fall under an exception for "reasonable network management." But the Obama-era system in which ISPs were regulated as common carriers <a href="https://arstechnica.com/tech-policy/2017/07/how-title-ii-goes-beyond-net-neutrality-to-protect-internet-users-from-isps/">gave more rights to consumers</a> to complain about unreasonable rates and practices, perhaps giving extra impetus to ISPs to upgrade their networks instead of limiting their users. Cox is a private company and thus doesn't report network-upgrade spending publicly, but major ISPs such as <a href="https://arstechnica.com/information-technology/2020/01/ajit-pai-promised-faster-broadband-expansion-comcast-cut-spending-instead/">Comcast</a>, <a href="https://arstechnica.com/information-technology/2020/01/att-slashed-billions-from-network-spending-cut-tens-of-thousands-of-jobs/">AT&amp;T</a>, and <a href="https://arstechnica.com/information-technology/2019/01/charter-will-spend-less-on-cable-network-in-2019-but-charge-customers-more/">Charter</a> have reduced network spending since the FCC <a href="https://arstechnica.com/tech-policy/2017/12/goodbye-net-neutrality-ajit-pais-fcc-votes-to-allow-blocking-and-throttling/">repealed</a> its net neutrality rules and common-carrier regulation. (<strong>Update</strong>: Harold Feld, senior VP of consumer-advocacy group Public Knowledge, <a href="https://twitter.com/haroldfeld/status/1270371055262203906">says</a> that "Cox's&nbsp;going after unspecified 'excess uploaders' would probably have violated the <a href="https://arstechnica.com/tech-policy/2017/12/how-the-net-neutrality-repeal-helps-isps-keep-their-hidden-fees-hidden/">enhanced disclosure rules</a> under the 2015 net neutrality order.")</p>
<p>Cox told Ars that it "will continue to work with anyone who is violating our Acceptable Use Policy with excessive use to help ensure everyone can have a positive Internet experience."</p>
<h2>Cox says network “performing very well”</h2>
<p>Cox told Ars its "network is performing very well overall" during the pandemic, and that out of 28,000 neighborhood nodes across the US, 98 to 99 percent "are performing with adequate capacity even with the tremendous level of increased peak usage." If Cox's 5.2 million paying broadband customers are spread equally across nodes, each node would serve about 185 households.</p>
<p>Cox said that it always "keep[s] a close eye at the individual node level to make sure we don't approach any congestion thresholds and need to make any adjustments. Similar to our normal process, if we see the network reach or exceed utilization thresholds we will accelerate network upgrade plans in the impacted areas. This could include splitting nodes, pulling additional fiber, equipment swaps and/or core network changes, all of which add capacity to the area."</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But those measures apparently aren't enough to handle users like Mike, Cox said:</p>
<blockquote><p>In some instances a number of excessive users, like the customer you referenced, are causing congestion problems in a small number of neighborhoods by utilizing over 100-200 times more upstream bandwidth than the average household. This type of excessive usage is negatively impacting the service of other customers, which is a violation of our Acceptable Use Policy. It is not our desire to terminate anyone's service, but we may need to address excessive usage out of fairness to the rest of our customers, especially during this time when households are even more dependent on a good Internet experience...</p>
<p>In the case of the customer you mentioned, we have communicated with him about our concerns and it appears he has made adjustments to his usage to operate within our Acceptable Use Policy.</p></blockquote>
<p>Mike confirmed to Ars that he has lowered his use by limiting overnight upload speeds to 400kbps, "so that it is always throttled." His usage in the 2.5 weeks since May 22 is 2.1TB, putting him well below his usual monthly pace.</p>
<h2>Pandemic spurs extra broadband use</h2>
<p>Broadband networks have <a href="https://arstechnica.com/information-technology/2020/04/pandemic-hasnt-crushed-broadband-networks-even-rural-areas-are-doing-ok/">mostly held up well</a> during the pandemic. Cable-lobby group NCTA, which represents Cox and other cable companies, <a href="https://www.ncta.com/COVIDdashboard">says</a> that "networks are engineered to provide superior performance throughout the day" and that "provider-backbone networks have significant capacity and show no signs of congestion." Since March 1, NCTA says that peak upstream traffic has risen 26.2 percent and peak download traffic has risen 9.1 percent.</p>
<p>Cox told us that "a small percentage of nodes... were approaching congestion levels prior to" the pandemic, and that "the dramatically increased use in those neighborhoods has pushed [them] beyond the threshold where performance will be impacted. These speed adjustments are temporary while we try to keep as many people as possible connected during the crisis."</p>

          
                  </div>
                    
        
          
    
    <div>
        
        
        
        <div>
          
          
<p>Mike said he suspects Cox is limiting upload speeds "because their network can't handle the increase of residential live video conference streaming" that has happened during the pandemic. Recently, Mike said he has been seeing upload speeds of only 4Mbps to 5Mbps.</p>
<h2>Coming soon: A price increase</h2>
<p>Mike's bill is currently lower than usual because Cox, like other ISPs, is <a href="https://arstechnica.com/information-technology/2020/04/comcast-waives-data-cap-until-at-least-june-30-in-response-to-pandemic/">providing unlimited data</a> to all customers during the pandemic. The waiver of the $50 monthly unlimited-data charge temporarily knocked his bill down to $100, and Cox provided a further $20 discount on his latest bill. Mike's bill doesn't explain the reason for the $20 credit, but it could be because of the new upload data limit.</p>
<p>But Cox is already signaling that Mike is in for a price increase in the near future. Today, Mike told Ars that "I just got an alert after logging in that my one-year introduction rate is now over, so I'll be paying $175 a month for 'unlimited' data once the current data-overage exceptions expire. Yay."</p>
<p><em>This article was updated after publication with additional responses from Cox.</em></p>


          
                  </div>

                  
          


  




  <div>
  <div>
          <p><a href="https://arstechnica.com/author/jon-brodkin/"><img src="https://arstechnica.com/wp-content/uploads/2016/05/j.brodkin-11_2.jpg" alt="Photo of Jon Brodkin"></a></p>
  </div>

  <div>
    

    <p>
      Jon is a Senior IT Reporter for Ars Technica. He covers the telecom industry, Federal Communications Commission rulemakings, broadband consumer affairs, court cases, and government regulation of the tech industry.
    </p>
  </div>
</div>


  


  
              </div>
  </article>


<div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/10/GettyImages-1693802745-768x432.jpg" alt="Listing image for first story in Most Read: Meta smart glasses can be used to dox anyone in seconds, study finds" decoding="async" loading="lazy">
                    <div>
                          
              <p><span>
                <span>1.</span>
                <span>Meta smart glasses can be used to dox anyone in seconds, study finds</span>
              </span>
                      </p></div>
          <a href="https://arstechnica.com/tech-policy/2024/10/harvard-students-make-auto-doxxing-smart-glasses-to-show-need-for-privacy-regs/?itm_source=parsely-api" aria-label="Read Meta smart glasses can be used to dox anyone in seconds, study finds">
          </a>
        </li>
                    <li>
                    <div>
                          
              <p><span>
                <span>2.</span>
                <span>Popular gut probiotic completely craps out in randomized controlled trial</span>
              </span>
                      </p></div>
          <a href="https://arstechnica.com/health/2024/10/popular-gut-probiotic-completely-craps-out-in-randomized-controlled-trial/?itm_source=parsely-api" aria-label="Read Popular gut probiotic completely craps out in randomized controlled trial">
          </a>
        </li>
                    <li>
                    <div>
                          
              <p><span>
                <span>3.</span>
                <span>Microsoft releases Office 2024, the latest buy-once-own-forever version of Office</span>
              </span>
                      </p></div>
          <a href="https://arstechnica.com/gadgets/2024/10/microsoft-releases-office-2024-the-latest-buy-once-own-forever-version-of-office/?itm_source=parsely-api" aria-label="Read Microsoft releases Office 2024, the latest buy-once-own-forever version of Office">
          </a>
        </li>
                    <li>
                    <div>
                          
              <p><span>
                <span>4.</span>
                <span>NASA is working on a plan to replace its space station, but time is running out</span>
              </span>
                      </p></div>
          <a href="https://arstechnica.com/space/2024/10/is-nasas-commercial-space-station-program-doomed/?itm_source=parsely-api" aria-label="Read NASA is working on a plan to replace its space station, but time is running out">
          </a>
        </li>
                    <li>
                    <div>
                          
              <p><span>
                <span>5.</span>
                <span>Amazon will “ramp up” Prime Video ads in 2025</span>
              </span>
                      </p></div>
          <a href="https://arstechnica.com/gadgets/2024/10/amazon-prime-video-is-getting-more-ads-next-year/?itm_source=parsely-api" aria-label="Read Amazon will “ramp up” Prime Video ads in 2025">
          </a>
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Robert Dennard, DRAM Pioneer, has died (299 pts)]]></title>
            <link>https://spectrum.ieee.org/in-memoriam-oct-2024</link>
            <guid>41735544</guid>
            <pubDate>Thu, 03 Oct 2024 22:03:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/in-memoriam-oct-2024">https://spectrum.ieee.org/in-memoriam-oct-2024</a>, See on <a href="https://news.ycombinator.com/item?id=41735544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Robert Dennard, DRAM Pioneer, Dies at 91"><p><strong>Robert Dennard </strong></p><p>Inventor of Dynamic RAM </p><p>Fellow, 91; died 23 April</p><p>In 1967 <a href="https://spectrum.ieee.org/thanks-for-the-memories" target="_self">Dennard</a> invented what is known as DRAM—a type of random-access semiconductor memory that stores each bit of data in a memory cell consisting of a tiny capacitor and <a href="https://spectrum.ieee.org/tag/transistor">transistor</a>. </p><p>Before his invention, RAM required bulky, power-hungry components that were expensive to produce. His breakthrough paved the way for inexpensive, high-density, and commercially available memory.</p><p>The computing pioneer received the 2009 <a href="https://corporate-awards.ieee.org/award/ieee-medal-of-honor/" target="_blank">IEEE Medal of Honor</a> for his innovation.</p><p>Dennard began his career as a staff engineer at <a href="https://www.ibm.com/" rel="noopener noreferrer" target="_blank">IBM</a>’s New York City research lab. He worked on new devices and circuits for logic and memory applications and developed advanced data communication techniques. </p><p>He was transferred in 1961 to IBM’s newly opened <a href="https://research.ibm.com/labs/watson/" rel="noopener noreferrer" target="_blank">Watson Research Center</a>, in Yorktown Heights, N.Y., where he worked on field-effect transistors and integrated circuits, using a six-transistor memory cell for each bit of data, the standard at the time. In 1968 Dennard successfully reduced a RAM cell to a small capacitor and a single field-effect transistor—the device that became known as DRAM. He was granted a <a href="https://patents.google.com/patent/US3387286A/en" rel="noopener noreferrer" target="_blank">U.S. patent</a> for his invention in 1968. </p><p>By the early 1970s, DRAM was standard in virtually all computers. A decade later, the device powered the first PCs—including the <a href="https://spectrum.ieee.org/tag/ibm">IBM</a> 5150—allowing them to perform more complex operations.</p><p>After DRAM’s success, Dennard turned his attention to transistors. In 1972 he developed principles for scaling metal-oxide-semiconductor field-effect transistor (MOSFET) devices, the main building block in low-power and high-performance very-large-scale integration (VLSI) chips. IEEE Fellow <a href="https://spectrum.ieee.org/special-reports/50-years-of-moores-law/" target="_self">Gordon Moore</a> had predicted that the number of transistors on a chip would double every two years, but Dennard proposed that as transistors got smaller, their power consumption would remain nearly constant. He published his theory, now known as <a href="https://en.wikipedia.org/wiki/Dennard_scaling" rel="noopener noreferrer" target="_blank">Dennard scaling</a>, with a set of supporting principles in the 1974 paper “<a href="http://stanford.edu/class/cs114/readings/dennard.pdf" rel="noopener noreferrer" target="_blank">Design of Ion-Implanted MOSFETs With Very Small Physical Dimensions</a>.”</p><p>Dennard was named an <a href="https://www.ibm.com/ibm/ideasfromibm/us/ibm_fellows/" rel="noopener noreferrer" target="_blank">IBM Fellow</a> in 1979. For the next several decades at the company, he worked on metal-oxide-semiconductor technology, refining RAM and developing low-voltage, high-performance circuits. </p><p>During his career, he was honored with dozens of awards including the 2019 <a href="https://corporate-awards.ieee.org/award/ieee-robert-n-noyce-medal/" rel="noopener noreferrer" target="_blank">IEEE Robert N. Noyce Medal</a> and the 2001 <a href="https://corporate-awards.ieee.org/award/ieee-edison-medal/" rel="noopener noreferrer" target="_blank">IEEE Edison Medal</a>. In 1997 Dennard was inducted into the <a href="https://www.invent.org/" rel="noopener noreferrer" target="_blank">National Inventors Hall of Fame</a>.</p><p>He received bachelor’s and master’s degrees in electrical engineering in 1954 and 1956 from <a href="https://www.smu.edu/" rel="noopener noreferrer" target="_blank">Southern Methodist University</a>, in Dallas. Two years later he earned a Ph.D. in electrical engineering from the Carnegie Institute of Technology (now part of <a href="https://www.cmu.edu/" rel="noopener noreferrer" target="_blank">Carnegie Mellon</a>).</p><p><strong>Richard P. Schulz</strong></p><p>Power systems engineer</p><p>Life Fellow, 87; died 29 May</p><p><a href="https://www.donohuefuneralhome.com/obituaries/richard-schulz" rel="noopener noreferrer" target="_blank">Schulz</a> began his career in 1959 as a power systems engineer at <a href="https://www.ge.com/" rel="noopener noreferrer" target="_blank">General Electric</a> in Schenectady, N.Y. He worked there until 1987, when he joined <a href="https://www.aep.com/" rel="noopener noreferrer" target="_blank">American Electric Power</a> in Columbus, Ohio. He retired in 2000 and became a consultant.</p><p>He specialized in power systems analysis. </p><p>Schulz, a member of the <a href="https://ieee-pes.org/" rel="noopener noreferrer" target="_blank">IEEE Power &amp; Energy Society</a>, authored or coauthored <a href="https://ieeexplore.ieee.org/author/37285469200" rel="noopener noreferrer" target="_blank">32 papers</a> on <a href="https://spectrum.ieee.org/tag/power-grid">power grid</a> testing reliability that were featured in IEEE publications.</p><p>He earned a bachelor’s degree in electrical and mechanical engineering in 1959 from <a href="https://www2.lehigh.edu/" rel="noopener noreferrer" target="_blank">Lehigh University</a>, in Bethlehem, Pa., and a master’s degree in 1966 from <a href="https://www.union.edu/" rel="noopener noreferrer" target="_blank">Union College</a>, in Schenectady. </p><p><strong>C. Gordon Bell</strong></p><p>Developer of early PCs</p><p>Fellow, 89; died 17 May</p><p><a href="https://ethw.org/C._Gordon_Bell" rel="noopener noreferrer" target="_blank">Bell</a> helped develop early PCs at <a href="https://en.wikipedia.org/wiki/Digital_Equipment_Corporation" rel="noopener noreferrer" target="_blank">Digital Equipment Corp.</a>, a pioneering computer company in Maynard, Mass. He received the 1992 <a href="https://corporate-awards.ieee.org/award/ieee-john-von-neumann-medal/" rel="noopener noreferrer" target="_blank">IEEE John Von Neumann Medal</a> for “innovative contributions to computer architecture and design.”</p><p>He joined DEC in 1960 and spent his first few years there designing the firm’s minicomputers and time-sharing computers. Time-sharing was a technique that made computing faster and less expensive by allowing multiple users to operate a system concurrently without interfering with each other.</p><p>Bell left DEC in 1966 to join the Carnegie Institute of Technology (now part of <a href="https://www.cmu.edu/" rel="noopener noreferrer" target="_blank">Carnegie Mellon</a>) as a professor of computer science and electrical engineering. </p><p>He returned to DEC in 1972 as vice president of R&amp;D and led the development of the company’s <a href="https://en.wikipedia.org/wiki/VAX" rel="noopener noreferrer" target="_blank">VAX minicomputers</a>. Introduced in 1977, the VAX line featured a 32-bit instruction set architecture and virtual memory. Its user-friendly operating system, known as VMS, enabled file sharing, networking, and other new features. </p><p>In 1979 Bell and <a href="https://en.wikipedia.org/wiki/Ken_Olsen" rel="noopener noreferrer" target="_blank">Ken Olsen</a>, the company’s CEO, founded the <a href="https://tcm.computerhistory.org/decmuseum.html" rel="noopener noreferrer" target="_blank">Digital Computer Museum</a> in Marlborough, Mass. Its collection of artifacts is now part of the <a href="https://computerhistory.org/" rel="noopener noreferrer" target="_blank">Computer History Museum</a> in Mountain View, Calif.</p><p>Bell left DEC in 1983 to found <a href="https://en.wikipedia.org/wiki/Encore_Computer" rel="noopener noreferrer" target="_blank">Encore Computer Co.</a>, a manufacturer in Marlborough. While there, he helped develop one of the first multiple microprocessor computers. Three years later he was appointed the first assistant director of the U.S. National Science Foundation’s <a href="https://new.nsf.gov/cise" rel="noopener noreferrer" target="_blank">Directorate for Computer and Information Science and Engineering</a> and served in that role until 1987.</p><p>After his one-year term with the NSF, Bell moved to California to join <a href="https://en.wikipedia.org/wiki/Stardent_Inc." rel="noopener noreferrer" target="_blank">Ardent Computer Corp.</a> in Sunnyvale as vice president of engineering. He led the development of the first graphics supercomputer there.</p><p>In 1995 he became a researcher at <a href="https://www.microsoft.com/en-us" rel="noopener noreferrer" target="_blank">Microsoft</a>, in Mountain View, where his work focused on telepresence technologies and multimedia. He retired in 2015. </p><p>Bell authored six books on computing and entrepreneurship. He was a Fellow of the <a href="https://www.acm.org/" rel="noopener noreferrer" target="_blank">Association for Computing Machinery</a> and a member of the U.S. <a href="http://www.nae.edu/" rel="noopener noreferrer" target="_blank">National Academy of Engineering</a>.</p><p>He received several awards including the 1991 <a href="https://www.uspto.gov/learning-and-resources/ip-programs-and-awards/national-medal-technology-and-innovation-nmti" rel="noopener noreferrer" target="_blank">National Medal of Technology and Innovation</a> for “his continuing intellectual and industrial achievements in the field of computer design; and for his leading role in establishing cost-effective, powerful computers which serve as a significant tool for engineering, science, and industry.”</p><p>He earned bachelor’s and master’s degrees in electrical engineering from <a href="https://www.mit.edu/" rel="noopener noreferrer" target="_blank">MIT</a> in 1956 and 1957.</p><p><strong>Laveen Nanik Kanal</strong></p><p>Founder and president of LNK</p><p>Life Fellow, 92; died 3 May</p><p><a href="https://lnk.com/" rel="noopener noreferrer" target="_blank">Kanal</a> was a computer science professor for 26 years at the <a href="https://umd.edu/" rel="noopener noreferrer" target="_blank">University of Maryland</a>, in College Park.</p><p>He also founded <a href="https://lnk.com/lnk/" rel="noopener noreferrer" target="_blank">LNK</a> in Germantown, Md., and served as its president for 30 years. The R&amp;D technology company specialized in pattern recognition, image processing, and <a href="https://spectrum.ieee.org/topic/artificial-intelligence/">artificial intelligence</a>.</p><p>Kanal emigrated from India to the United States in 1948 to attend the <a href="https://www.washington.edu/" rel="noopener noreferrer" target="_blank">University of Washington</a> in Seattle, where he earned bachelor’s and master’s degrees in electrical engineering in 1951 and 1953.</p><p>After graduating, Kanal worked for two years as an electronics engineer at <a href="http://www.ge.com/taxonomy/term/1802" rel="noopener noreferrer" target="_blank">Canada General Electric</a> in Toronto. He left in 1955 to join <a href="https://www.gd.com/" rel="noopener noreferrer" target="_blank">General Dynamics</a> in Rochester, N.Y., as manager of its machine intelligence laboratory. After two years, Kanal decided to pursue a Ph.D. at the <a href="https://www.upenn.edu/" rel="noopener noreferrer" target="_blank">University of Pennsylvania</a>, in Philadelphia. He earned his doctorate in electrical engineering in 1960, then joined <a href="https://en.wikipedia.org/wiki/Philco" rel="noopener noreferrer" target="_blank">Philco-Ford Corp.</a>, an electronics company in Willow Grove, Pa., as an engineering manager. He left in 1968 to found LNK. </p><p>In 1970 he changed career paths and became a professor at the University of Maryland, where he played a key role in establishing and expanding the school’s computer science department. He was director of the university’s Machine Intelligence and Pattern Analysis Laboratory (now the <a href="https://ml.umd.edu/" rel="noopener noreferrer" target="_blank">Center for Machine Learning</a>) from 1974 to 1989. He was named professor emeritus in 1996. </p><p>Kanal was a Fellow of the <a href="https://aaai.org/" rel="noopener noreferrer" target="_blank">Association for the Advancement of Artificial Intelligence</a>, the <a href="https://www.aaas.org/" rel="noopener noreferrer" target="_blank">American Association for the Advancement of Science</a>, and the <a href="https://iapr.org/" rel="noopener noreferrer" target="_blank">International Association for Pattern Recognition</a>. In 1992 he received the IAPR’s <a href="https://iapr.org/awards/icpr-awards/king-sun-fu-prize/" rel="noopener noreferrer" target="_blank">King-Sun Fu Prize</a> for contributions to the field of pattern recognition.</p><p><strong>H. Joel Trussell</strong></p><p>Former <em><em>Proceedings of the IEEE </em></em>editor in chief</p><p>Fellow, 79; died 9 April</p><p>Trussell<strong></strong>was a dedicated IEEE volunteer who served as <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5" rel="noopener noreferrer" target="_blank"><em><em>Proceedings of the IEEE</em></em></a> editor in chief from 2013 to 2018. He also was a former member of <a href="https://spectrum.ieee.org/the-institute/" target="_self"><em><em>The</em></em>&nbsp;<em><em>Institute</em></em></a>’s editorial advisory board.</p><p>A prominent researcher in the field of signal and image processing, Trussell spent the first 11 years of his career at <a href="https://www.lanl.gov/" rel="noopener noreferrer" target="_blank">Los Alamos National Laboratory</a>, in Santa Fe, N.M.</p><p>He decided to pursue a career in academia and left Los Alamos to teach at <a href="https://www.hw.ac.uk/" rel="noopener noreferrer" target="_blank">Heriot-Watt University</a>, in Edinburgh. In 1980 he became an electrical and computer and engineering professor at <a href="https://www.ncsu.edu/" rel="noopener noreferrer" target="_blank">North Carolina State University</a>, in Raleigh, where he taught for 40 years, retiring in 2020. </p><p>Trussell’s research interests included estimation theory, color imaging, and signal and image restoration and reconstruction. One of his most significant contributions was introducing new mathematical tools to model and incorporate a priori data—which relies on deductive reasoning to make predictions—to address signal recovery challenges. He also created a robust mathematical framework for the study of color imaging. His research led to partnerships with <a href="https://www.hivelocitymedia.com/companies/ColorSavvy12_3_09.aspx" rel="noopener noreferrer" target="_blank">Color Savvy Systems</a>, a startup that focused on image color measurement and control, as well as <a href="https://www.kodak.com/en/" rel="noopener noreferrer" target="_blank">Eastman Kodak</a> and <a href="https://www.hpe.com/us/en/home.html" rel="noopener noreferrer" target="_blank">Hewlett-Packard</a>. </p><p>During his time at <em><em>Proceedings of the IEEE</em></em>, he edited more than 200 papers. He shared the 1986 <a href="https://signalprocessingsociety.org/community-involvement/awards-submit-award-nomination" rel="noopener noreferrer" target="_blank">IEEE Acoustics, Speech, and Signal Processing Society Senior Paper Award</a> with <a href="https://ieeexplore.ieee.org/author/37703686000" rel="noopener noreferrer" target="_blank">M. Reha Civanlar</a> for “<a href="https://ieeexplore.ieee.org/document/1164297" rel="noopener noreferrer" target="_blank">The Feasible Solution in Signal Restoration</a>,” and the 1993 <a href="https://signalprocessingsociety.org/community-involvement/image-video-and-multidimensional-signal-processing/awards" rel="noopener noreferrer" target="_blank">IEEE Signal Processing Society Paper Award</a> with <a href="https://ieeexplore.ieee.org/author/37273806400" rel="noopener noreferrer" target="_blank">Patrick L. Combettes</a> for “<a href="https://ieeexplore.ieee.org/document/134400" rel="noopener noreferrer" target="_blank">The Use of Noise Properties in Set Theoretic Estimation</a>.”</p><p>Trussell received a bachelor’s degree in applied mathematics in 1967 from <a href="https://gatech.edu/node/1" rel="noopener noreferrer" target="_blank">Georgia Tech</a>. He went on to earn a master’s degree in the same subject the next year from <a href="https://www.fsu.edu/" rel="noopener noreferrer" target="_blank">Florida State University</a>, in Tallahassee. In 1976 he received a Ph.D. in electrical engineering and computer science from the <a href="https://www.unm.edu/" rel="noopener noreferrer" target="_blank">University of New Mexico</a>, in Albuquerque.</p><p><strong>James Theodore Hardin</strong></p><p>Vice president and chief engineer</p><p>Member, 89; died 18 January</p><p><a href="https://www.kygers.com/obituaries/James-Theodore-Hardin?obId=30461661" rel="noopener noreferrer" target="_blank">Hardin</a> was former vice president and chief engineer at <a href="https://www.iactx.com/" rel="noopener noreferrer" target="_blank">Innovative Applications Corp</a>. (IAC), in El Paso, Texas. It specialized in processing and recycling laser toner cartridges.</p><p> In the early 1950s Hardin was an engineer at <a href="https://en.wikipedia.org/wiki/Lockheed_Corporation" rel="noopener noreferrer" target="_blank">Lockheed Corp</a>., an <a href="https://spectrum.ieee.org/topic/aerospace/">aerospace</a> manufacturer, in Calabasas, Calif., where he contributed to the design of <a href="https://www.whitehouse.gov/about-the-white-house/the-grounds/air-force-one/" rel="noopener noreferrer" target="_blank">Air Force One</a>, the airplane used by U.S. presidents. </p><p>He enlisted in the U.S. Navy in 1957 and served on the <a href="https://www.usscoralsea.org/" rel="noopener noreferrer" target="_blank">USS <em><em>Coral Sea</em></em></a> as a nuclear weapons officer. When his active duty ended in 1963, he remained in the <a href="https://www.navyreserve.navy.mil/" rel="noopener noreferrer" target="_blank">Navy Reserve</a> until he retired in the 1980s as a captain. </p><p> In his civilian role, he worked as an engineer and manager at <a href="https://www.prestolite.com/" rel="noopener noreferrer" target="_blank">Prestolite Electric</a>, a global manufacturer and supplier of alternators, starters, and electrical equipment, with headquarters in Novi, Mich. He left in the late 1980s to serve as director of engineering at Septor Electronics Corp., which developed manufacturing equipment in El Paso. In 1992 he left Septor to join IAC.</p><p> Hardin received bachelor’s and master’s degrees in mechanical engineering in 1956 and 1963 from <a href="https://www.bradley.edu/" rel="noopener noreferrer" target="_blank">Bradley University</a>, in Peoria, Ill.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Biggest solar flare since 2017 erupts from sun and Earth is in the firing line (130 pts)]]></title>
            <link>https://www.space.com/most-powerful-solar-flare-this-solar-cycle-x-9-earth-firing-line</link>
            <guid>41734863</guid>
            <pubDate>Thu, 03 Oct 2024 20:52:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/most-powerful-solar-flare-this-solar-cycle-x-9-earth-firing-line">https://www.space.com/most-powerful-solar-flare-this-solar-cycle-x-9-earth-firing-line</a>, See on <a href="https://news.ycombinator.com/item?id=41734863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>

<div id="article-body">
<p>The sun has just unleashed its most powerful solar flare this cycle, a colossal X-class eruption.&nbsp;</p><p>The X9.05 <a data-analytics-id="inline-link" href="https://www.space.com/solar-flares-effects-classification-formation" data-before-rewrite-localise="https://www.space.com/solar-flares-effects-classification-formation"><u>solar flare</u></a> peaked at <a data-analytics-id="inline-link" href="https://www.spaceweatherlive.com/en/solar-activity/solar-flares.html" data-url="https://www.spaceweatherlive.com/en/solar-activity/solar-flares.html" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>8:10 a.m. EDT</u></a> (1210 GMT), triggering shortwave radio blackouts over Africa and Europe, the sunlit portion of Earth at the time of eruption.&nbsp;</p><p>The solar flare emanated from the <a data-analytics-id="inline-link" href="https://www.space.com/sunspots-formation-discovery-observations" data-before-rewrite-localise="https://www.space.com/sunspots-formation-discovery-observations"><u>sunspot</u></a> group AR3842, which has made headlines before. On Oct. 1, the same sunspot region fired off a <a data-analytics-id="inline-link" href="https://www.space.com/sun-monster-solar-flare-x7-video" data-before-rewrite-localise="https://www.space.com/sun-monster-solar-flare-x7-video"><u>powerful X7.1 solar flare</u></a> and unleashed a <a data-analytics-id="inline-link" href="https://www.space.com/coronal-mass-ejections-cme" data-before-rewrite-localise="https://www.space.com/coronal-mass-ejections-cme"><u>coronal mass ejection</u></a> (CME) — a plume of plasma and magnetic field — which is currently barreling toward <a data-analytics-id="inline-link" href="https://www.space.com/54-earth-history-composition-and-atmosphere.html" data-before-rewrite-localise="https://www.space.com/54-earth-history-composition-and-atmosphere.html"><u>Earth</u></a>. That incoming CME is expected to <a data-analytics-id="inline-link" href="https://www.space.com/aurora-alert-northern-lights-far-south-illinois-oregon-g3-geomagnetic-storm-predicted" data-before-rewrite-localise="https://www.space.com/aurora-alert-northern-lights-far-south-illinois-oregon-g3-geomagnetic-storm-predicted"><u>hit Earth between Oct. 3 and Oct. 5</u></a>, possibly triggering widespread auroras.&nbsp;</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-320-80.jpg" alt="x flare eruption on october 3, 2024" srcset="https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/tjMDBHJK24ZtsccRV2ZHj6.jpg"></picture></p></div><figcaption itemprop="caption description"><span>On Oct. 3, the sun released the most powerful solar flare this solar cycle, a colossal X9.05 eruption — and it's heading for Earth.&nbsp; </span><span itemprop="copyrightHolder">(Image credit: NASA / SDO and the AIA, EVE, and HMI science teams / helioviewer.org)</span></figcaption></figure><p>An Earth-directed CME did indeed follow the monster flare, space weather forecaster and meteorologist Sara Housseal <a data-analytics-id="inline-link" href="https://x.com/SNHWx/status/1841909631754330404" target="_blank" data-url="https://x.com/SNHWx/status/1841909631754330404" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">confirmed</a> in a post on X. This could be excellent news for aurora chasers, as CMEs can trigger geomagnetic storms, which in turn can result in dramatically boosted auroral displays.&nbsp;"As of right now, I would anticipate an impact at Earth late 5th — early 6th," Housseal wrote.</p><p><strong>Related:&nbsp;</strong><a data-analytics-id="inline-link" href="https://www.space.com/32601-where-to-see-northern-lights.html" data-before-rewrite-localise="https://www.space.com/32601-where-to-see-northern-lights.html"><strong>Where and when to see the northern lights in 2024</strong></a></p><p>CMEs carry electrically charged particles known as ions, and when these collide with Earth's <a data-analytics-id="inline-link" href="https://www.space.com/earths-magnetic-field-explained" data-before-rewrite-localise="https://www.space.com/earths-magnetic-field-explained"><u>magnetosphere</u></a>, they can trigger geomagnetic storms. During these storms, the ions interact with gases in <a data-analytics-id="inline-link" href="https://www.space.com/17683-earth-atmosphere.html" data-before-rewrite-localise="https://www.space.com/17683-earth-atmosphere.html"><u>Earth's atmosphere</u></a>, releasing energy in the form of light. This phenomenon is recognized as the <a data-analytics-id="inline-link" href="https://www.space.com/15139-northern-lights-auroras-earth-facts-sdcmp.html" data-before-rewrite-localise="https://www.space.com/15139-northern-lights-auroras-earth-facts-sdcmp.html"><u>northern lights</u></a>, or aurora borealis, in the Northern Hemisphere, and as the southern lights, or aurora australis, in the Southern Hemisphere.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-320-80.jpg" alt="graphic detailing the effects of the major x class solar flare and widespread radio blackouts" srcset="https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/i4UNG9ih7EuWsbAvmDAEMR.jpg"></picture></p></div><figcaption itemprop="caption description"><span>Keep an eye on swpc.noaa.gov for the latest updates. </span><span itemprop="copyrightHolder">(Image credit: NOAA Space Weather Prediction Center)</span></figcaption></figure><p>We will have to wait what happens in the following days. <a data-analytics-id="inline-link" href="https://www.space.com/space-weather" data-before-rewrite-localise="https://www.space.com/space-weather"><u>Space weather</u></a> is fickle and unpredictable, so it certainly keeps the forecasters on their toes. One thing we know that did accompany the major X flare was a radio blackout.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-4BAUphb3xjVpBLfwPG5WfB"><section><p>Breaking space news, the latest updates on rocket launches, skywatching events and more!</p></section></div><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-320-80.png" alt="shortwave radio blackout map showing the areas most affected by the radio blackouts, mainly Europe and Africa." srcset="https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/NyBF6dwdXAifNxihXEJv8a.png"></picture></p></div><figcaption itemprop="caption description"><span>Shortwave radio blackout map from NOAA's Space Weather Prediction Center. </span><span itemprop="copyrightHolder">(Image credit: NOAA Space Weather Prediction Center)</span></figcaption></figure><p>The shortwave radio blackouts<a data-analytics-id="inline-link" href="https://x.com/_SpaceWeather_/status/1841815752908628202" data-url="https://x.com/_SpaceWeather_/status/1841815752908628202" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"> experienced over Europe and Africa</a> were a result of the radiation from the solar flare reaching Earth and ionizing the upper atmosphere upon arrival. This ionization creates a denser environment for high-frequency shortwave radio signals, which facilitate long-distance communication, to travel through. As these radio waves pass through ionized (electrically charged) layers, they lose energy due to increased collisions with <a data-analytics-id="inline-link" href="https://www.space.com/electrons-negative-subatomic-particles" data-before-rewrite-localise="https://www.space.com/electrons-negative-subatomic-particles"><u>electrons</u></a>, which can weaken or entirely absorb the radio signals.&nbsp;</p><p>Today's record-breaking solar flare was the most powerful one in this <a data-analytics-id="inline-link" href="https://www.space.com/solar-cycle-frequency-prediction-facts" data-before-rewrite-localise="https://www.space.com/solar-cycle-frequency-prediction-facts"><u>solar cycle</u></a> so far. In fact, it was the most powerful solar flare in over seven years! Back in September 2017, <a data-analytics-id="inline-link" href="https://www.space.com/solar-flare-2017-record-breaking" data-before-rewrite-localise="https://www.space.com/solar-flare-2017-record-breaking"><u>two colossal flares</u></a> measuring &nbsp;X13.3 and X11.8 &nbsp;were reported in the declining phase of the previous solar cycle. (Solar activity waxes and wanes over the course of an 11-year cycle.)</p><p>Solar flares are classified by size into different classes, with X-class flares being the most powerful. M-class flares are 10 times less powerful than X-class, followed by C-class flares, which are 10 times weaker than M-class. B-class flares are 10 times weaker than C-class, and A-class flares are 10 times weaker than B-class and have no noticeable impact on Earth. Each class is further divided by numbers from 1 to 10 (and beyond for X-class flares) to indicate the flare's relative strength.&nbsp;</p><p>If you're interested in tracking space weather and knowing when and where to spot auroras, I suggest downloading a space weather app that provides forecasts based on your location. One option I use is "My Aurora Forecast &amp; Alerts," available for both <a data-analytics-id="inline-link" href="https://go.redirectingat.com/?id=92X1588396&amp;xcust=space_us_6176016681872586921&amp;xs=1&amp;url=https%3A%2F%2Fapps.apple.com%2Fus%2Fapp%2Fmy-aurora-forecast-alerts%2Fid1073082439&amp;sref=https%3A%2F%2Fwww.space.com%2Fmost-powerful-solar-flare-this-solar-cycle-x-9-earth-firing-line" target="_blank" data-url="https://go.redirectingat.com/?id=92X1588396&amp;xcust=space_gb_9898401764288726102&amp;xs=1&amp;url=https%3A%2F%2Fapps.apple.com%2Fus%2Fapp%2Fmy-aurora-forecast-alerts%2Fid1073082439&amp;sref=https%3A%2F%2Fwww.space.com%2Faurora-alert-northern-lights-far-south-illinois-oregon-g3-geomagnetic-storm-predicted" referrerpolicy="no-referrer-when-downgrade" rel="sponsored noopener" data-hl-processed="skimlinks" data-placeholder-url="https://go.redirectingat.com/?id=92X1588396&amp;xcust=hawk-custom-tracking&amp;xs=1&amp;url=https%3A%2F%2Fapps.apple.com%2Fus%2Fapp%2Fmy-aurora-forecast-alerts%2Fid1073082439&amp;sref=https%3A%2F%2Fwww.space.com%2Fmost-powerful-solar-flare-this-solar-cycle-x-9-earth-firing-line" data-google-interstitial="false" data-merchant-name="SkimLinks - apple.com" data-merchant-id="undefined" data-merchant-url="undefined" data-merchant-network="undefined"><u>iOS</u></a> and <a data-analytics-id="inline-link" href="https://play.google.com/store/apps/details?id=com.jrustonapps.myauroraforecast&amp;hl=en_GB" target="_blank" data-url="https://play.google.com/store/apps/details?id=com.jrustonapps.myauroraforecast&amp;hl=en_GB" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>Android</u></a>. However, any similar app should work well. I also use the "Space Weather Live" app, which is <a data-analytics-id="inline-link" href="https://go.redirectingat.com/?id=92X1588396&amp;xcust=space_us_1339075970348571247&amp;xs=1&amp;url=https%3A%2F%2Fapps.apple.com%2Fus%2Fapp%2Fspaceweatherlive%2Fid1435501021&amp;sref=https%3A%2F%2Fwww.space.com%2Fmost-powerful-solar-flare-this-solar-cycle-x-9-earth-firing-line" target="_blank" data-url="https://go.redirectingat.com/?id=92X1588396&amp;xcust=space_gb_3493607590548574249&amp;xs=1&amp;url=https%3A%2F%2Fapps.apple.com%2Fus%2Fapp%2Fspaceweatherlive%2Fid1435501021&amp;sref=https%3A%2F%2Fwww.space.com%2Faurora-alert-northern-lights-far-south-illinois-oregon-g3-geomagnetic-storm-predicted" referrerpolicy="no-referrer-when-downgrade" rel="sponsored noopener" data-hl-processed="skimlinks" data-placeholder-url="https://go.redirectingat.com/?id=92X1588396&amp;xcust=hawk-custom-tracking&amp;xs=1&amp;url=https%3A%2F%2Fapps.apple.com%2Fus%2Fapp%2Fspaceweatherlive%2Fid1435501021&amp;sref=https%3A%2F%2Fwww.space.com%2Fmost-powerful-solar-flare-this-solar-cycle-x-9-earth-firing-line" data-google-interstitial="false" data-merchant-name="SkimLinks - apple.com" data-merchant-id="undefined" data-merchant-url="undefined" data-merchant-network="undefined"><u>available on iOS</u></a> and <a data-analytics-id="inline-link" href="https://play.google.com/store/apps/details?id=com.spaceweatherlive.app&amp;hl=en_GB&amp;pli=1" target="_blank" data-url="https://play.google.com/store/apps/details?id=com.spaceweatherlive.app&amp;hl=en_GB&amp;pli=1" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>Android</u></a>, to get a deeper understanding of whether the current space weather conditions are favorable for aurora sightings.&nbsp;</p>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>
<div id="slice-container-authorBio-4BAUphb3xjVpBLfwPG5WfB"><p>Daisy Dobrijevic joined <a href="https://www.space.com/" data-before-rewrite-localise="https://www.space.com/">Space.com</a> in February 2022 having previously worked for our sister publication <a href="https://www.space.com/43203-all-about-space-free-issue.html" target="_blank" data-before-rewrite-localise="https://www.space.com/43203-all-about-space-free-issue.html">All About Space</a> magazine as a staff writer. Before joining us, Daisy completed an editorial internship with the BBC Sky at Night Magazine and worked at the <a href="https://spacecentre.co.uk/?gclid=Cj0KCQjw3f6HBhDHARIsAD_i3D-mw3nFf8xOLVjxawebnwAOTgxjs-OAi0TTmNvL8gD1MjcujcBiU6QaAlJ1EALw_wcB" target="_blank">National Space Centre</a> in Leicester, U.K., where she enjoyed communicating space science to the public. In 2021, Daisy completed a PhD in plant physiology and also holds a Master's in Environmental Science, she is currently based in Nottingham, U.K. Daisy is passionate about all things space, with a penchant for solar activity and space weather. She has a strong interest in astrotourism and loves nothing more than a good northern lights chase!&nbsp;</p></div>


</section>





<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Dept of Energy announces $1.5B in electric grid improvements (151 pts)]]></title>
            <link>https://www.upi.com/Top_News/US/2024/10/03/energy-department-electric-grid-investment/3631727969828/</link>
            <guid>41734119</guid>
            <pubDate>Thu, 03 Oct 2024 19:34:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.upi.com/Top_News/US/2024/10/03/energy-department-electric-grid-investment/3631727969828/">https://www.upi.com/Top_News/US/2024/10/03/energy-department-electric-grid-investment/3631727969828/</a>, See on <a href="https://news.ycombinator.com/item?id=41734119">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The U.S. Department of Energy Thursday announced a $1.5 billion transmission investment to improve electric grid reliability. Deputy Secretary of Energy David Turk said the investment in expanding the grid will support increased electricity demand. Photo courtesy<a href="https://www.energy.gov/sites/default/files/styles/full_article_width/public/2021-06/DAVID_TURK_PORTRAIT_DSF0477.png?itok=_d4GjCWK"> Energy Department</a></p><article itemprop="articleBody">

<p><span>Oct. 3 (UPI) --</span> The U.S. Department of Energy Thursday announced a $1.5 billion transmission investment to improve electric grid reliability.</p>
<p>The Energy Department<a href="https://www.energy.gov/articles/biden-harris-administration-invests-15-billion-bolster-nations-electricity-grid-and-0" target="_blank"> said the $1.5 billion</a> will fund four transmission projects "that will improve grid reliability and resilience, relieve costly transmission congestion, and open access to affordable energy to millions of Americans across the country."
</p>
<p>"The U.S. transmission network is the backbone of our nation's electricity system. Though our grid has served U.S. energy needs for more than a century, our country's needs are changing," said U.S. Deputy Secretary of Energy David Turk in a statement.</p>
<p>The projects will increase grid capacity and are funded by the Biden-Harris administration's Bipartisan Infrastructure Law.
</p><div><h3>Related</h3>
<ul><li><a href="https://www.upi.com/Top_News/US/2024/07/29/energy-strategic-fuel-reserve-replenishment/2701722278852/" target="_blank" onclick="upi_gtag('Story', 'Related Article', 'US News');" title="U.S. replenishes strategic oil reserve after tapping it for Russia's invasion of Ukraine">U.S. replenishes strategic oil reserve after tapping it for Russia's invasion of Ukraine</a></li>
<li><a href="https://www.upi.com/Top_News/US/2024/03/16/lithium-americas-nevada-project-energy-department/5201710615866/" target="_blank" onclick="upi_gtag('Story', 'Related Article', 'US News');" title="Energy Department grants $2.26B loan for Nevada lithium project">Energy Department grants $2.26B loan for Nevada lithium project</a></li>
<li><a href="https://www.upi.com/Top_News/US/2023/11/03/400-million-for-solar-energy-in-Puerto-Rico/3991698993769/" target="_blank" onclick="upi_gtag('Story', 'Related Article', 'US News');" title="Energy Department announces $400M for solar energy in Puerto Rico">Energy Department announces $400M for solar energy in Puerto Rico</a></li></ul></div>
<p>The projects will enable nearly 1,000 miles of new electric transmission development and 7,100 megawatts of new capacity in Louisiana, Maine, Mississippi, New Mexico, Oklahoma and Texas.</p>
<p>The department said the the projects will also create 9,000 good-paying jobs.</p>
<p>They include the Aroostook Renewable Project in Maine, the Cimarron Link in Oklahoma, Southern Spirit connecting the Texas grid for the first time to southeastern U.S. power markets and Southline in New Mexico.</p>
<p>The Energy Department's National Transmission Planning study released Thursday was meant to be a long-term planning tool.
</p>
<p>It found that a substantial expansion of the transmission system throughout the entire contiguous United States would deliver the biggest grid benefits. That could also save the national electric system between $270 billion to $490 billion through 2050.</p>
<p>"The NTP Study is designed to enhance and encourage interregional planning efforts," the DOE said in a statement. "It does not replace industry planning or identify a specific set of transmission lines that should be built. Rather, the NTP Study identifies potential opportunities for industry planners to consider projects that would benefit customers under a wide range of future scenarios."</p>
			</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Heart of Unix (2018) (139 pts)]]></title>
            <link>https://ericnormand.me/article/the-heart-of-unix</link>
            <guid>41734047</guid>
            <pubDate>Thu, 03 Oct 2024 19:25:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ericnormand.me/article/the-heart-of-unix">https://ericnormand.me/article/the-heart-of-unix</a>, See on <a href="https://news.ycombinator.com/item?id=41734047">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Despite all of its warts, I like working in Linux. I've used it for 15
years and I've never been as productive in another environment. Most
people claim that it's the configurability of Linux that keep the users
coming. That may have attracted me at first, but <strong>what attracts me now
is its programmability.</strong></p>
<p>Let me be very clear. I'm not saying that Linux is great because I can
patch the source code to grep and recompile it. In all my years of Unix,
I've never done anything like that. And I'm not saying that Linux is a
great workstation for programmers because it helps you program better.
Those are topics for another essay.</p>
<h3>Unix is a programmable environment</h3>
<p>I <em>am</em> saying that Unix is a programmable environment. When you interact
with the shell, <strong>you are writing programs to be interpreted</strong>. You can
easily extend the Unix system by writing a shell script, copying it to a
directory in your <code>PATH</code>, and making it executable. Boom. You've got a
new program.</p>
<p>What's more, that program, if it follows certain simple conventions, is
now able to work with other programs. Those conventions are simple, and
they are summed up well by Doug McIlroy, the inventor of Unix pipes:</p>
<blockquote>
<p>This is the Unix philosophy: Write programs that do one thing and do
it well. Write programs to work together. Write programs to handle
text streams, because that is a universal interface.</p>
</blockquote>
<p>If your program <em>reads text lines from standard in and writes text lines
on standard out</em>, it is likely to do well on Unix.</p>
<h3>Programs on your path are like pure functions in the higher-level language called the shell</h3>
<p>Not all programs are so pure. But the vast majority of the programs that
are so typically Unixy are. <code>grep</code>, <code>awk</code>, <code>sed</code>, <code>wc</code>, <code>pr</code>, etc.</p>
<h3>Unix is a multi-lingual environment</h3>
<p>I must have compilers or interpreters for 30 languages on my machine.
Maybe more. All of these languages are invited to the party. They can
all call each other (through the shell). And of course their
<code>stdin</code>/<code>stdouts</code> can be piped together.</p>
<p>You really can use the best tool for the job. I've got Bash scripts, awk
scripts, Python scripts, some Perl scripts. What I program in at the
moment depends on my mood and practical considerations. It is a little
crazy that <strong>I don't have to think about what language something is
written in when I'm at the terminal</strong>.</p>
<h3>Unix provides a universal interface with a universal data structure</h3>
<p>It needs to be stated that there is a reason all of these languages can
work together. There is a standard data structure that programs are
invited to use: text streams. That means sequences of characters. Text
streams are cool because they're simple and flexible. <strong>You can impose a
structure on top of the flat sequence.</strong> For instance, you can break it
into a sequence of sequences of characters by splitting it on a certain
character (like new-line). Then you can split those sequences into
columns. In short, text is flexible.</p>
<h3>Unix is homoiconic</h3>
<p>There's another property that I think is rarely talked about in the
context of Unix. In Lisp, we often are proud that code is data. You can
manipulate code with the same functions that you manipulate other data
structures. This meta-circularity gives you a lot of power.</p>
<p>But this is the same in Unix. <strong>Your programs are text files</strong> and so
can be <code>grep</code>'d and <code>wc</code>'d and anything else if you want to. You can
open up a pipe to Perl and feed it commands, if you like. And this feeds
right back into Unix being programmable.</p>
<h3>Functional + universal data structure + homoiconic = power</h3>
<p>All of this adds up to synergy. When you write a program that follows
the Unix conventions of <code>stdin</code>/<code>stdout</code> with text streams, <strong>it can
work with thousands of programs that are already on your computer</strong>.
What's more, your program has to do less work itself, because so much of
the hard work can be done better by other programs.</p>
<h3>On the file system, hierarchical names point to data objects</h3>
<p>And this synergy extends well beyond just using text streams. I have
this tendency to look to databases as storage solutions for my personal
projects. They have some nice properties, like ACID and SQL.But by using
a database, I'm missing out on joining the Unix ecosystem. <strong>If I use
the file system to store my data---meaning text files in directories---I
can use all of Unix to help me out.</strong> I can use <code>find</code>, <code>grep</code>, <code>head</code>,
<code>tail</code>, etc., just because I chose to use the measly file system instead
of some fancy database.</p>
<h3>Blog example</h3>
<p>A good example of the synergy I'm talking about is the blog you are
reading now. Here's how my blog works:</p>
<p>I store everything on the file system. I have an <code>src/</code> directory with
<code>drafts/</code> <code>posts/</code> <code>pages/</code> and <code>links/</code>. I wrote a Python script
(currently at 183 well-commented lines) that reads <code>src/</code> and spits out
the final product to <code>build/</code>. The Python uses a few libraries, but the
meat of it is done by calling other programs. The rendering of Markdown
to HTML is done by <a href="http://johnmacfarlane.net/pandoc/"><code>pandoc</code></a>, which
happens to be written in Haskell. I also do a call out to the shell to
copy a directory (<code>cp -rp</code>) because I was too lazy to figure out how to
do it in pure Python.</p>
<p>I sync <code>build/</code> to Amazon S3 with a Ruby program called <code>s3sync</code>. I edit
my entries in Emacs. If I need to delete a post, I run <code>rm</code>. If I need
to list my posts, I run <code>ls</code>. If I'd like to change the name of a post,
I use <code>mv</code>.</p>
<p>It may not be the best interface for writing a blog. But notice all of
the stuff I didn't have to write to get started. I'm already writing
posts and publishing them. <strong>Compare that to the reams of PHP and
Javascript it takes to get the same functionality in Wordpress.</strong> That's
the power of small tools working together.</p>
<h3>Unix is old</h3>
<p>Now that I've expressed how great Unix is, allow me to speak about its
numerous shortcomings. I can't say for sure, but I would guess that most
of the shortcomings are due to the long history of <strong>Unix starting on
underpowered machines</strong>.</p>
<p>For instance, the fact that your programs have to be manually stored to
disk using file system operations so that your dynamic shell language
can have access to them seems awfully quaint. But when Unix was
developed, disk space, RAM, and computation were expensive. Everything
was expensive. So the strategy was to <strong>cache your compiler output to
disk</strong> so you wouldn't have to do a costly compile step each time you
ran a program.</p>
<p>If I want to write a new program, even a short one, I have to open up a
text file in Emacs (make sure it's in the path!), write the program,
save it, switch to the terminal, and <code>chmod +x</code> it. Compare that to
Clojure, where you constantly define and redefine functions at the REPL.
Or, if you like, a Smalltalk system where you can open up the editing
menu of <em>anything you can see</em> and change the code which will then be
paged out to disk at a convenient time. Unix clearly has room to grow in
that respect.</p>
<h3>The file system</h3>
<p>The file system is archaic, too. It's reliable, but a little
feature-poor. It's one of the reasons I think first about a database
before remembering the synergy available with the file system. It
doesn't provide any kind of ACID properties. The metadata available is
laughable (permissions, owner/group, date, and filesize?). <strong>A more
modern file system would give a little more oomph</strong> to compete with
other forms of storage.</p>
<h3>The terminal</h3>
<p><strong>The terminal is just old.</strong> It's all text. The editing is
sub-primitive. The help it gives you is the bare minimum. One of its
biggest shortcomings is how opaque it is. It doesn't do much to help you
learn commands. It's not very good with huge dumps on <code>stdout</code>.
Multiline commands? Supported with <code>\</code>. I think we can do better.</p>
<h3>Text streams</h3>
<p>The world of computers has grown up a lot since the early days of Unix.
There has been a Cambrian explosion in the number of file formats. Lots
of them are binary formats. Lots are structured text, like XML or JSON.
Unix can handle those kinds of files, but <strong>it has failed to find a
lever to help the Unix user master them with the same synergy you see
with flat text files</strong>.</p>
<h3>Wrong turns</h3>
<p>Uni
x has a long history. Some of that history was kind, some was unkind.
Most of the development of Unix was just practical people doing their
best with the tools they had.</p>
<p>What's unfortunate is that we now have better tools and we see what
could be done, but to do it would break backwards compatibility. And so
we continue with sub-optimal tools.</p>
<h3>Layering instead of evolving</h3>
<p>One thing I think is unfortunate in the world of Unix today is layering.
<strong>Modern Linux distributions are midden piles of configuration daemons
to manage permissions daemons to give your configuration GUI access to
the configuration daemons.</strong> Or we find ourselves installing a database
to manage a few kilobytes of metadata.</p>
<p>The problem is Unix has not evolved in those areas. The permission
system has changed very little. Modern distributions want to provide a
modern and unintrusive interface to protected resources, so they add a
layer of indirection onto the primitive permissions model instead of
evolving the permission system itself. The Unix permissions system is
solid and has worked for years. Maybe it should stay. But instead of
giving us small programs that do one thing well to let us become masters
of the permissions system, <strong>we get obtuse, opaque daemons</strong> that also
need to be learned.</p>
<p>The file system, though much improved in terms of capacity, stability,
and reliability, still has the same basic features: hierarchical
directories containing files, accessed by name. If you want something
more, you have to add a layer like BerkeleyDB or SQLite. These tools are
great, but I'd like to see a more Unixy solution that allows for the
synergy you get from existing programs made to run with files on the
disk.</p>
<h3>Megacommands</h3>
<p><a href="https://gist.github.com/1091803">Command bloat</a> <a href="https://gist.github.com/665971">is
terrible</a>. Rob Pike and Brian Kernighan
have <a href="http://harmful.cat-v.org/cat-v/">written about this</a>. I'll merely
refer you to their <a href="https://ericnormand.me/files/papers/unix_prog_design.pdf">excellent
paper</a>. The gist is that having n
commands gives yo
u O(n^2^) ways of combining them. <strong>Having fewer,
bigger, "more powerful" programs does not give you this exponential and
synergistic advantage.</strong></p>
<p>If you look at it the right way, all of these little programs that do
one thing are like functions in the higher-level language that is Unix.
We see that languages like Perl and Python have huge numbers of
libraries for doing all sorts of tasks. Those libraries are only
accessible through the programming language they were developed for.
This is a missed opportunity for the languages to interoperate
synergistically with the rest of the Unix ecosystem.</p>
<h3>The road ahead</h3>
<p>I've given a bit of a taste of some of the non-Unixy directions we're
going in. Now I'd like to end with some right directions.</p>
<p>I mentioned before that saving a compiled binary to disk is done to
cache what used to be an expensive operation. With modern hardware, a
short utility C program could be read in, parsed, compiled, and run very
quickly. Probably with no noticeable delay. It's something to consider
when thinking about the division between static programs and dynamic
scripting languages and the role of the compiler.</p>
<h3>Talking to Unix</h3>
<p>Foreign Function Interfaces between programming languages are considered
very difficult to work with because of the semantics mismatch between
any two languages. But Unix provides a universal interface for programs
to <strong>interoperate without the need for FFI</strong>. I hope to see more "sugar"
in languages to take advantage of calling out to other programs for
help. <a href="http://stackoverflow.com/questions/799968/whats-the-difference-between-perls-backticks-system-and-exec">Perl's
backticks</a>
comes to mind.</p>
<p>You might say that this is expensive. Well, yes and no. Yes, there is
much more overhead in reading in who-knows-how-many files to execute
some script on disk than in just calling some library function. I argue,
though, that <strong>the time difference is becoming small enough not to
matter</strong>; and the operating system should evolve to make it more
practical.</p>
<h3>Evolving</h3>
<p><code>stdin</code>/<code>stdout</code></p>
<p>Stdin/stdout with text streams is the closest thing we have to a
universal, language-agnostic interface. It defines a minimal
"constitution" with which programs can interact. <strong>Can this interface be
improved on without destroying it?</strong> I wouldn't doubt it. There are lots
of "data flow" patterns besides input and output. Pub/sub, broadcast,
dispatch, etc., should be explored.</p>
<h3>Text streams, evolved</h3>
<p>Unix was designed for flat text and the existing Unix tools operate on
text. We need new tools to <strong>bring structured text and binary into the
Unix world</strong> to join the party. I don't think this would be hard. I've
written programs that read in JSON and write it out with one JSON object
per line. That lets you grep it to find the one you want, or <code>wc -l</code> it
to count the objects.</p>
<p>Another thing I've been working on is defining a dispatch mechanism for
common operations on files of different types. Take, for instance,
metadata that is stored in a file. An HTML file has a title, sometimes
it has an author (in a meta tag), etc. A JPEG file has metadata in the
EXIF data. <strong>Is there some way we can unify the access of that
metadata?</strong> I think there is and I'm working on it. The same command
would dispatch differently based on mime-type.</p>
<h3>21st Century Terminal</h3>
<p>How can we improve the terminal? I think it's a hard problem but not
impossible. Part of the issue with the terminal is that as X Windows
developed, people started using menus to run monolithic programs instead
of piping things with the shell. So the usefulness of the terminal will
be improved, without changing the terminal itself, by <strong>breaking those
monolithic programs up into composable programs</strong>. For instance, a
program which displays all of the thumbnails of the files listed on
<code>stdin</code> would be much more useful to me than a mouse-oriented file
browser.</p>
<p>The terminal is about text. I don't think that could or should change.
But does it have to be only about text?
<a href="http://acko.net/blog/on-termkit">Explorations</a> are underway.</p>
<h3>The Shell</h3>
<p>The last improvement I want to touch on is the shell language itself.
Bash is ugly. There. I said it. A lot of good work has been done in
programming language design and I'd like to see some of it make its way
to the shell. <strong>What if we take the idea of Unix programs as pure
functions over streams of data a little further?</strong> What about
higher-order functions? Or function transformations? Combinators? <strong>What
if we borrow from object-oriented languages?</strong> Can we have message
passing? What about type-based dispatching?</p>
<h3>Conclusions</h3>
<p>Unix has always been practical and it has proven itself over the years.
It's 40 years old and it's still being used. Furthermore, Unix is the
closest thing to a personal computing experience^1 that is practical
today.</p>
<p>People tend to <a href="https://ericnormand.me/files/papers/LispGoodNewsBadNews.pdf">contrast Unix</a>
with systems like the Lisp Machine and Smalltalk. But I see <strong>more
similarities than differences</strong>: <em>Code as data.</em> <em>Everything is
programmable.</em> <em>Dynamic language prompt.</em> <em>Universal data structure.</em> <em>A
propensity for "dialects" or "distributions".</em> <em>Garbage collection.</em>^2
Unix just made a lot of compromises to make it practical on the limited
hardware that was available.</p>
<p>Unfortunately, those compromises have stuck. A lot of work went into
workarounds and a lot of software has been built on top of those design
decisions. The question is: <strong>where to go from here?</strong></p>
<p>My own personal choice is to go <strong>back to the roots</strong>. Often, when we
want to make a change, we must look to what has worked in the past. What
has brought us this far? What were the things that made Unix special?
Unix was built by individuals all adding their own practical knowledge
and hard work into one cohesive system. Their individual work was
multiplied by the synergy of a common interface. If we want to evolve
Unix (and I do), that common interface---the heart of Unix---is the
place to start.</p>
<hr>
<ol>
<li>
<p>When I say "personal computer", I'm referring to <a href="http://www.mprove.de/diplom/gui/kay72.html">Alan Kay's
vision</a>:</p>
<blockquote>
<p>What then is a personal computer? One would hope that it would be
both a medium for containing and expressing arbitrary symbolic
notions, and also a collection of useful tools for manipulating
these structures, with ways to add new tools to the repertoire.</p>
</blockquote>
</li>
<li>
<p>Unix has a limited form of garbage collection. Short-running
programs (like those executed at the terminal) need not concern
themselves with freeing allocated memory since the OS will free
everything when they exit.</p>
</li>
</ol></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Says It Won't Follow Amazon's Lead with a Return-to-Office Mandate (260 pts)]]></title>
            <link>https://www.entrepreneur.com/business-news/google-recommits-to-hybrid-work-schedule-unlike-amazon/480683</link>
            <guid>41734046</guid>
            <pubDate>Thu, 03 Oct 2024 19:25:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.entrepreneur.com/business-news/google-recommits-to-hybrid-work-schedule-unlike-amazon/480683">https://www.entrepreneur.com/business-news/google-recommits-to-hybrid-work-schedule-unlike-amazon/480683</a>, See on <a href="https://news.ycombinator.com/item?id=41734046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>In a town hall last month, Google brass assured staffers that the company wouldn't follow Amazon's lead and mandate that employees return to the office five days a week, per <i><a href="https://www.businessinsider.com/google-hybrid-work-policy-amazon-rto-employees-workplace-remote-2024-10" rel="follow" target="_self">Business Insider</a></i>.</p><p><b>Related:</b> <a href="https://www.entrepreneur.com/business-news/amazon-ceo-mandates-employees-return-to-office-5-days-a-week/479935" rel="follow" target="_self">Amazon CEO Mandates Employees Work in the Office 5 Days Per Week Starting January</a></p><p>At Google's most recent <a href="https://www.businessinsider.com/google-ai-dodge-tough-questions-tgif-meetings-2024-8" rel="follow" target="_self">"TGIF" (Thank God It's Friday</a>) monthly meeting, where employees submit questions, the most submitted query was reportedly for Google to recommit to its hybrid work policy in light of Amazon's new mandate.</p><p>Last month, Amazon CEO Andy Jassy <a href="http://www.entrepreneur.com/business-news/amazon-ceo-mandates-employees-return-to-office-5-days-a-week/479935" rel="follow" target="_self">announced</a> that all corporate employees are expected to be in the office <a href="https://www.entrepreneur.com/business-news/i-work-at-amazon-i-plan-to-coffee-badge-new-rto-policy/480047" rel="follow" target="_self">five days a week</a> starting in January. Salesforce <a href="https://www.entrepreneur.com/business-news/salesforce-will-require-employees-in-office-5-times-per-week/477422" rel="follow" target="_self">also returned</a> to a mostly five-day-a-week in-office schedule effective October 1.</p><p><b>Related:</b> <a href="https://www.entrepreneur.com/business-news/google-rehires-ai-pioneer-noam-shazeer-in-27-billion-deal/480378" rel="follow" target="_self">Google Is Paying $2.7 Billion to Reportedly Rehire an Early Employee Who Built an AI Chatbot Before ChatGPT</a></p><p>Google's current policy, meanwhile, is for employees to be in the office "at least" three days a week.</p><p>A <a href="https://www.entrepreneur.com/business-news/ex-google-ceo-says-he-misspoke-on-remote-work-ai-rivals/478532" rel="follow" target="_self">Google</a> VP reportedly told employees at the town hall that the current system was working and changes were not expected.</p><p>Although this goes against the trend of large tech companies enforcing stricter RTO rules, Alphabet CEO Sundar Pichai noted the policy will stay flexible as long as staff remains productive during remote days.</p><p>A Google spokesperson confirmed these comments were made but provided no further context, per <a href="https://www.businessinsider.com/google-hybrid-work-policy-amazon-rto-employees-workplace-remote-2024-10" rel="follow" target="_self"><i>Business Insider</i></a>.</p><p><b>Related:</b> <a href="https://www.entrepreneur.com/business-news/samsung-6-day-workweek-for-execs-company-in-emergency-mode/472937" rel="follow" target="_self">Samsung Makes 6-Day Workweeks Mandatory for Executives as the Company Enters 'Emergency Mode'</a></p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Gumroad Didn't Choose Htmx (379 pts)]]></title>
            <link>https://htmx.org/essays/why-gumroad-didnt-choose-htmx/</link>
            <guid>41733625</guid>
            <pubDate>Thu, 03 Oct 2024 18:45:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmx.org/essays/why-gumroad-didnt-choose-htmx/">https://htmx.org/essays/why-gumroad-didnt-choose-htmx/</a>, See on <a href="https://news.ycombinator.com/item?id=41733625">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

  
  
    <address>Sahil Lavingia</address>
    <p><time>September 30, 2024</time></p><p>At Gumroad, we recently embarked on a new project called <a rel="noopener" target="_blank" href="https://helper.ai/">Helper</a>. As the CEO, I was initially quite
optimistic about using <a rel="noopener" target="_blank" href="https://htmx.org/">htmx</a> for this project, even though some team members were less enthusiastic.</p>
<p>My optimism stemmed from previous experiences with React, which often felt like overkill for our needs. I thought htmx
could be a good solution to keep our front-end super light.</p>
<figure>
<a href="https://htmx.org/img/gumroad-red.jpeg" target="_blank">
<img alt="Gumroad Red" src="https://htmx.org/img/gumroad-red.jpeg">
</a> 
<figcaption>Source with htmx - Click Image To View</figcaption>
</figure>
<p>In fact, I shared this sentiment with our team in Slack:</p>
<blockquote>
<p>“https://htmx.org/ may be a way of adding simple interactions to start”</p>
</blockquote>
<p>And initially, it seemed promising! As one of our engineers at Gumroad eloquently put it:</p>
<blockquote>
<p>“HTMX is (officially) a meme to make fun of how overly complicated the JS landscape has gotten - much like tailwind is
just a different syntax for inline CSS, HTMX is a different syntax for inline JS.”</p>
</blockquote>
<p>However, unlike Tailwind, which has found its place in our toolkit, htmx didn’t scale for our purposes and didn’t lead
to the best user experience for our customers–at least for our use case.</p>
<p>Here’s why:</p>
<ol>
<li>
<p><strong>Intuition and Developer Experience</strong>: While it would have been possible to do the right thing in htmx, we found it
much more intuitive and fun to get everything working with Next.js. The development process felt natural with
Next.js, whereas with htmx, it often felt unnatural and forced. For example, when building complex forms with dynamic
validation and conditional fields, we found ourselves writing convoluted server-side logic to handle what would be
straightforward client-side operations in React.</p>
</li>
<li>
<p><strong>UX Limitations</strong>: htmx ended up pushing our app towards a Rails/CRUD approach, which led to a really poor (or at
least, boring and generic) user experience by default. We found ourselves constantly fighting against this tendency,
which was counterproductive. For instance, implementing a drag-and-drop interface for our workflow builder proved to
be a significant challenge with htmx, requiring workarounds that felt clunky compared to the smooth experience we
could achieve with React libraries.</p>
</li>
<li>
<p><strong>AI and Tooling Support</strong>: It’s worth noting that AI tools are intimately familiar with Next.js and not so much with
htmx, due to the lack of open-source training data. This is similar to the issue Rails faces. While not a
dealbreaker, it did impact our development speed and the ease of finding solutions to problems. When we encountered
issues, the wealth of resources available for React/Next.js made troubleshooting much faster.</p>
</li>
<li>
<p><strong>Scalability Concerns</strong>: As our project grew in complexity, we found htmx struggling to keep up with our needs. The
simplicity that initially attracted us began to feel limiting as we tried to implement more sophisticated
interactions and state management. For example, as we added features like real-time collaboration and complex data
visualization, managing state across multiple components became increasingly difficult with htmx’s server-centric
approach.</p>
</li>
<li>
<p><strong>Community and Ecosystem</strong>: The React/Next.js ecosystem is vast and mature, offering solutions to almost any problem
we encountered. With htmx, we often found ourselves reinventing the wheel or compromising on functionality. This
became particularly evident when we needed to integrate third-party services and libraries, which often had React
bindings but no htmx equivalents.</p>
</li>
</ol>
<figure>
<a href="https://htmx.org/img/gumroad-green.jpeg" target="_blank">
<img alt="Gumroad Green" src="https://htmx.org/img/gumroad-green.jpeg">
</a> 
<figcaption>Source with NextJS - Click Image To View</figcaption>
</figure>
<p>Ultimately, we ended up moving to React/Next.js, which has been a really great fit for building the complex UX we’ve
been looking for. We’re happy with this decision–for now. It’s allowed us to move faster, create more engaging user
experiences, and leverage a wealth of existing tools and libraries.</p>
<figure>
<a href="https://htmx.org/img/gumroad-helper-before-after.png" target="_blank">
<img alt="Gumroad Helper Before After" src="https://htmx.org/img/gumroad-helper-before-after.png">
</a> 
<figcaption>Gumroad Helper Before &amp; After - Click Image To View</figcaption>
</figure>
<p>This experience has reinforced a valuable lesson: while it’s important to consider lightweight alternatives, it’s
equally crucial to choose technologies that can grow with your project and support your long-term vision. For Helper,
React and Next.js have proven to be that choice.</p>
<p>Since we’ve moved there, we’ve been able to seriously upgrade our app’s user experience for our core customers.</p>
<ol>
<li>
<p><strong>Drag-and-Drop Functionality</strong>: One of the key features of our workflow builder is the ability to reorder steps
through drag-and-drop. While it’s possible to implement drag-and-drop with htmx, we found that the available
solutions felt clunky and required significant custom JavaScript. In contrast, React ecosystem offers libraries like
react-beautiful-dnd that provide smooth, accessible drag-and-drop with minimal setup.</p>
</li>
<li>
<p><strong>Complex State Management</strong>: Each workflow step has its own set of configurations and conditional logic. As users
edit these, we need to update the UI in real-time to reflect changes and their implications on other steps. With
htmx, this would require numerous server roundtrips or complex client-side state management that goes against htmx’s
server-centric philosophy. React’s state management solutions (like useState or more advanced options like Redux)
made this much more straightforward.</p>
</li>
<li>
<p><strong>Dynamic Form Generation</strong>: The configuration for each step type is different and can change based on user input.
Generating these dynamic forms and handling their state was more intuitive with React’s component model. With htmx,
we found ourselves writing more complex server-side logic to generate and validate these forms.</p>
</li>
<li>
<p><strong>Real-time Collaboration</strong>: While not visible in this screenshot, we implemented features allowing multiple users to
edit a workflow simultaneously. Implementing this with WebSockets and React was relatively straightforward, whereas
with htmx, it would have required more complex server-side logic and custom JavaScript to handle real-time updates.</p>
</li>
<li>
<p><strong>Performance Optimization</strong>: As workflows grew larger and more complex, we needed fine-grained control over
rendering optimizations. React’s virtual DOM and hooks like useMemo and useCallback allowed us to optimize
performance in ways that weren’t as readily available or intuitive with htmx.</p>
</li>
</ol>
<p>It’s important to note that while these challenges aren’t insurmountable with htmx, we found that addressing them often
led us away from htmx’s strengths and towards solutions that felt more natural in a JavaScript-heavy environment. This
realization was a key factor in our decision to switch to React and Next.js.</p>
<p>We acknowledge that htmx may be a great fit for many projects, especially those with simpler interaction models or those
built on top of existing server-rendered applications. Our experience doesn’t invalidate the benefits others have found
in htmx. The key is understanding your project’s specific needs and choosing the tool that best aligns with those
requirements.</p>
<p>In our case, the complex, stateful nature of Helper’s interface made React and Next.js a better fit. However, we
continue to appreciate htmx’s approach and may consider it for future projects where its strengths align better with our
needs.</p>
<p>That said, we’re always open to reevaluating our tech stack as our needs evolve and new technologies emerge. Who knows
what the future might bring?</p>

  <p>
    &lt;/&gt;
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Were RNNs All We Needed? (420 pts)]]></title>
            <link>https://arxiv.org/abs/2410.01201</link>
            <guid>41732853</guid>
            <pubDate>Thu, 03 Oct 2024 17:31:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.01201">https://arxiv.org/abs/2410.01201</a>, See on <a href="https://news.ycombinator.com/item?id=41732853">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.01201">View PDF</a>
    <a href="https://arxiv.org/html/2410.01201v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. In this work, we revisit traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), we show that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, we introduce minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175x faster for a sequence of length 512). Lastly, we show that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Leo Feng [<a href="https://arxiv.org/show-email/59157d94/2410.01201">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 2 Oct 2024 03:06:49 UTC (292 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Most gamers prefer single-player games (160 pts)]]></title>
            <link>https://www.midiaresearch.com/blog/most-gamers-prefer-single-player-games</link>
            <guid>41732763</guid>
            <pubDate>Thu, 03 Oct 2024 17:22:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.midiaresearch.com/blog/most-gamers-prefer-single-player-games">https://www.midiaresearch.com/blog/most-gamers-prefer-single-player-games</a>, See on <a href="https://news.ycombinator.com/item?id=41732763">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                <p>It’s no secret that the bulk of <a href="https://www.midiaresearch.com/blog/in-game-cosmetics-remain-a-huge-opportunity-for-brands-across-music-tv-sports-and-fashion" target="_blank" rel="noreferrer noopener">AAA games market revenues come from in-game purchases</a>, mostly&nbsp;from live-service games. </p>
<p>It’s also impossible to ignore that the live-service market is fiercely competitive. Thanks to the oversaturated attention economy, only so many consumer hours are available. </p>
<p>And live-service games suck up <i>so much</i> attention and engagement. There are still opportunities in the space, but breaking through is extremely challenging.</p>
<p>With every new live-service hit that does manage to carve a spot for itself, there is less space available for new entrants to cast their net. </p>
<p>AAA developers on console and PC are continuing to chase the live-service jackpot, but single player remains the favourite way to play for most (53%) gamers. </p>
<p>There is also less risk, as there are more openings on the calendar for single player to stand out versus live service’s always-on engagement vacuum.</p>
<p><a href="https://www.midiaresearch.com/reports/single-player-is-safer-and-has-more-opportunities-amid-live-service-challenges" target="_blank" rel="noreferrer noopener">MIDiA's latest gaming report</a> looks at the growing viability of(re)allocating resources towards single-player games. </p>
<p>We zoom in on gameplay preferences by platform and age, motivators for trying new games, preferred game themes for puzzle, action-adventure, sports, shooter, and RPG fans, and offer data-backed strategic recommendations. </p>
<p>This article shines the spotlight on some high-level takeaways from the first section of the report. If you’re ready to dive deeper, <a href="https://www.midiaresearch.com/contact" target="_blank" rel="noreferrer noopener">let us know</a>.</p>
<p><b>Several single-player studios were pushed into making live-service games – the trend chasing did NOT pay off</b></p>
<p>Many AAA game makers have been chasing the live-service trend, looking to replicate the success of <i>Fortnite</i>, <i>League of Legends</i>, <i>Roblox</i>, and other success stories. </p>
<p>Even developers that rose to prominence thanks to single-player – with games that helped put their publishers on the map – were pushed to chase the live-service trend.&nbsp; </p>
<p>For many, it was a wild goose chase, and the list of failures is growing. SEGA cancelled Creative Assembly’s <i>Hyenas</i> and PlayStation did the same thing for Naughty Dog’s <i>The Last of Us Online</i> after years of development. </p>
<p>Other revered single-player names in the industry have tried their hand at live services – with dire results: Crystal Dynamics (<i>Marvel’s Avengers</i>), EA’s BioWare (<i>Anthem</i>), Platinum Games (<i>Babylon’s Fall</i>), and Microsoft’s Arkane (<i>Redfall</i>) – the list goes on. &nbsp;</p>
<p>This underlines an undeniable opportunity cost:</p>
<ul>
<li>These studios’ single-player offerings have respectively generated hundreds of millions in revenue</li>
<li>How much revenue and positive consumer sentiment was left on the table because these companies were pushed to make live-service games?</li>
<li>Aggravating things, the market is too delicate for most publishers to take huge risks in an oversaturated space</li>
</ul>
<p>Hitting a few singles and doubles beats trying to hit a home run and striking out.</p>
<p>The timing of live service’s oversaturation is not ideal. Publishers now need to cut costs to adapt to a challenging macroeconomic climate (and to course correct from overextending during the pandemic). </p>
<p><b>There's a way forward for these studios: returning to what has always worked, single-player games </b></p>
<p><a href="https://www.midiaresearch.com/blog/concord-postmortem-what-went-wrong-and-whats-next-for-playstations-live-service-strategy" target="_blank" rel="noreferrer noopener">While new live-service games have floundered</a>, new single-player games have continued to smash records and generate hundreds of millions – or more.&nbsp; And it is not just juggernaut IP like <i>Zelda</i> and <i>Spider-Man</i> proving successful but also new franchises like&nbsp;<i>Elden Ring </i>(25 million copies sold) and <i>Black Myth Wukong&nbsp;</i>(20 million).</p>
<p>While live-service games and in-game purchases dominate spending and attention in the games market, over half of gamers prefer solo play:&nbsp;</p>
<p><img src="https://www.midiaresearch.com/storage/uploads/paragraphs/dbdbaba91de72cd615d8be1a0119cfca" alt="dbdbaba91de72cd615d8be1a0119cfca"></p>
<p>There are some caveats to this opportunity and how best to seize it. Data from the above image, which you can dive deeper into via the <a href="https://www.midiaresearch.com/reports/single-player-is-safer-and-has-more-opportunities-amid-live-service-challenges" target="_blank" rel="noreferrer noopener">full single-player opportunity report</a>, underlines a positive correlation between age and preferring single-player: </p>
<ul>
<li>Beyond younger generations’ higher emphasis on social play, life starts getting busier in the mid-20s</li>
<li>Life commitments make it trickier to play live-service games regularly, and co-ordinating sessions with friends becomes more challenging</li>
<li>To that end, the 25+ gamers are the lowest-hanging fruit to target with single-player games</li>
</ul>
<p><i>Looking for splits by platform, region, or another variable? </i><a href="mailto:rhys@midiaresearch.com?subject=Game%20type%20preferences%20by%20platform%20split"><i>Let us know</i></a><i> – we have the data! </i></p>
<p><b>The one-and-done nature of single-player gels better with the saturated attention economy</b></p>
<p>Younger players prefer PVP, which captures a large swathe of consumer attention and engagement and is enjoyed by all generations. Convincing players – and their friends – to leave for new titles is a huge barrier. </p>
<p>Live-service games are the <i>homes</i> of many highly engaged players. Getting them (and their friends) to relocate permanently to another live-service game is a big ask.</p>
<p>Yet, going 'on holiday’ to a single-player game is more viable. Releasing in quieter periods for the big live-service games, like towards the latter part of the live-service seasons, could be beneficial here. </p>
<p>It is also easier for busier 25+ gamers to dip into a single-player game – perhaps even on a&nbsp;<a href="https://www.midiaresearch.com/blog/steam-is-the-unlikely-winner-of-the-modern-console-wars-for-now" target="_blank" rel="noreferrer noopener">supporting device like the Steam Deck</a>&nbsp;or PlayStation Portal. </p>
<p><b>Single-player games are a safer bet for new games</b></p>
<p>Make no mistake: the costs to make AAA single-player, non-live service games have inflated to astronomic levels. Leaks from Insomniac showed that PlayStation’s AAA flagship games, like <i>Spider-Man 2, </i>have budgets in the hundreds of millions of dollars. But there is a growing <a href="https://www.midiaresearch.com/blog/gamers-arent-finishing-games-and-thats-a-problem" target="_blank" rel="noreferrer noopener">opportunity for AAA studios to make leaner single-player games</a>. </p>
<p>Despite the extra risk, live-service games can be even more expensive. After launch, they also need significant additional costs to run over time (and keep their players coming back to spend). <i>Genshin Impact</i>’s budget was $100 million, with another $200 million <i>per year</i> needed for live-ops costs. </p>
<p>For many AAA publishers making new games, it might be time to course-correct from the red live-service ocean to the bluer single-player one – or at least shift some more resources to solo experiences. </p>
<p>The insights in this article are the tip of the iceberg of the <a href="https://www.midiaresearch.com/reports/single-player-is-safer-and-has-more-opportunities-amid-live-service-challenges">full report</a>, which also looks at what exactly gamers are looking for in new games, what drives them to purchase, low-hanging fruit for publishers, and more. </p>
<p>Want to learn more? <a href="https://www.midiaresearch.com/contact">Reach out to us here</a>.&nbsp;</p>


                                <p>Want the latest entertainment research and insights directly to your inbox? Our newsletter has you covered, <a href="https://www.midiaresearch.com/newsletter?utm_campaign=blog_body_CTA&amp;utm_id=rhys-elliott&amp;utm_medium=social_media&amp;utm_content=most-gamers-prefer-single-player-games">click here to subscribe</a>.</p>
                
                
                                
                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canvas is a new way to write and code with ChatGPT (861 pts)]]></title>
            <link>https://openai.com/index/introducing-canvas/</link>
            <guid>41732634</guid>
            <pubDate>Thu, 03 Oct 2024 17:07:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-canvas/">https://openai.com/index/introducing-canvas/</a>, See on <a href="https://news.ycombinator.com/item?id=41732634">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-canvas/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Why and how we’re migrating many of our servers from Linux to the BSDs (254 pts)]]></title>
            <link>https://it-notes.dragas.net/2024/10/03/i-solve-problems-eurobsdcon/</link>
            <guid>41732415</guid>
            <pubDate>Thu, 03 Oct 2024 16:45:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://it-notes.dragas.net/2024/10/03/i-solve-problems-eurobsdcon/">https://it-notes.dragas.net/2024/10/03/i-solve-problems-eurobsdcon/</a>, See on <a href="https://news.ycombinator.com/item?id=41732415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p><em>This is the text underlying my presentation at EuroBSDCon 2024, on 21 September 2024, in Dublin, Ireland.</em></p><p><em>The slides can be downloaded <a href="https://it-notes.dragas.net/slides/EuroBSDCon2024_Marinelli.pdf" target="_blank" rel="external nofollow noopener noreferrer">here</a></em></p><p><em>The video, not yet separated from the live stream, can <a href="https://www.youtube.com/watch?t=19285&amp;v=u_bdSqqHm58" target="_blank" rel="external nofollow noopener noreferrer">be viewed here</a> - At first, I was a bit tense, then I relaxed.</em></p><p><em>Happy reading!</em></p><p><strong>EuroBSDCon Dublin - 21 September 2024 - <a href="https://events.eurobsdcon.org/2024/talk/LNMLZX/" target="_blank" rel="external nofollow noopener noreferrer">Why (and how) we’re migrating many of our servers from Linux to the BSDs</a></strong></p><p><strong>“I’m Stefano Marinelli, I solve problems.”</strong></p><p>I’m the <a href="https://bsd.cafe/" target="_blank" rel="external nofollow noopener noreferrer">founder and Barista of the BSD Cafe</a>, a community of *BSD enthusiasts.</p><p>I work in my company, called Prodottoinrete - a container of ideas and solutions.</p><p>I’m passionate about technology and computing, and I’ve made my passion my profession. Every morning, when I sit in front of the computer, a new world opens up for me to explore, and I try to share this passion with my clients. Sometimes, I succeed.</p><p>I’ve been a Linux user since 1996, before I turned 17. Back then, I used Fidonet and would read about alternative operating systems. Curiosity got the best of me, and I bought a set of CDs with various Linux distributions at the first opportunity. I tried it, I liked it, but at the time, I didn’t find it advantageous, so I continued using it for secondary tasks while Windows remained my daily driver.</p><p>Things changed in late 1997. I decided to go deeper into Linux and, with the purchase of a new, more powerful computer, I realized that aside from gaming, Linux could be my everyday system. This came in handy when I started university in 1998, where the computer science department was very oriented towards Open Source solutions. I was one of the few students who already understood the concepts of Open Source and knew how to use Linux. I was also one of the few who wasn’t baffled when we found Solaris machines in the lab. After all, there were similarities.</p><p>Over the years, I became one of the administrators of that lab, which was almost entirely Linux-based.</p><p>In 2000, I was fortunate to have <a href="https://en.wikipedia.org/wiki/%C3%96zalp_Babao%C4%9Flu" target="_blank" rel="external nofollow noopener noreferrer">Professor Özalp Babaoğlu</a> as my lecturer, which pushed me to explore other operating systems like the BSDs. However, all I had at the time was an old Compaq laptop (486/25 MHz and 4 MB RAM) and no fast internet connection. So, apart from some theoretical studies, I postponed hands-on experiments until I had better resources. By 2002, thanks to a broadband connection and a new computer, I began exploring BSD systems. I started with FreeBSD, largely thanks to its fantastic handbook. I asked my parents to buy a laser printer so I could “print academic material” — but really, I wanted to print all the documentation I could find, including the BSD handbook. And it was incredibly helpful.</p><p>Before long, FreeBSD became my daily driver — entire nights spent compiling KDE while I slept a meter away from my laptop’s wildly spinning fans—but FreeBSD ran much, much better on that machine than Linux did. Unfortunately, OpenBSD was too slow to be usable with any graphical interface.</p><p>In 2003, my final thesis focused on virtualization on Open Source systems, NetBSD/Xen was one of the best and most efficient solutions I tested.</p><p>Shortly after, I found a job at a company that was just beginning to offer Linux-based solutions, mainly for web and mail servers, but I was criticized because I completed tasks that didn’t require constant interventions — this hurt the company’s billing. That’s when I set my future guidelines:</p><ul><li>I would work for myself, following my own philosophy.</li><li>I would not be tied to any particular vendor. I love exploring and learning, so I would always study solutions in depth and recommend the one I thought best suited for the client.</li><li>I solve problems — I don’t sell boxes.</li><li>I would use and promote Open Source solutions whenever possible.</li><li>I would use a BSD whenever feasible and Linux where a BSD was not suitable.</li></ul><p>In every technological decision, my priority is solving my clients’ specific problems, not selling a predefined solution. <strong>I solve problems.</strong></p><p>I was often told that Open Source systems were “toys for universities” and that the real world ran on something else (mostly meaning Windows). I pressed on. In some cases, I offered to be paid only if the results were achieved, showing how much they’d save compared to traditional licensing costs. It worked, but…</p><ul><li>They accepted Linux, albeit reluctantly, but rejected BSDs because they didn’t know them. In some cases, I managed to convince them. In others, sadly, I didn’t.</li></ul><p>Result: I decided to use the BSDs where clients didn’t have direct access — like email servers or web hosting — while using Linux where they specifically requested it.</p><ul><li>NetBSD/Xen as the virtualization base, <a href="https://it-notes.dragas.net/2023/08/27/that-old-netbsd-server-running-since-2010/" target="_blank" rel="external nofollow noopener noreferrer">with excellent longevity and reliability</a>.</li><li>OpenBSD as network/firewall entry points.</li><li>FreeBSD (especially with the introduction of ZFS) for various services, including backup servers.</li></ul><p>It worked. What surprised clients most was the stability and the reduced need for maintenance. They saw more uptime and were happy.</p><p>Problem: <strong>“If nothing is working, what am I paying you for? If everything’s working, what am I paying you for?”</strong> So, I selected clients and situations that understood that if everything works, it’s because of the work behind the scenes. It’s better to pay for everything to work than to pay to fix problems. The problem to solve, in this case, is not stopping the client’s work. <strong>And I solve problems.</strong></p><p>I managed about 65% *BSD machines and 35% Linux. But as Linux’s popularity grew, so did client demand. It became necessary on multiple occasions to replace or implement Linux instead of a BSD, for specific requests.</p><p>At a certain point, Linux virtualization solutions matured, and many of my hosts transitioned from NetBSD/Xen to OpenNebula (often with MooseFS) and then to Proxmox (often with Ceph). This happened because my clients needed to manage their VM lifecycles autonomously, change configurations, migrate hosts, etc.</p><p>Proxmox showed excellent reliability and stability. The VMs were often Linux-based (especially if the client needed direct management) or FreeBSD, but in smaller quantities. My own infrastructure, however, remained primarily BSD-based.</p><p><em>Over time, I gradually started to reflect and ’take stock’.</em></p><p>No *BSD has ever caused me to lose data to the point of having to restore from backup. On Linux, I’ve lost data with ext4, XFS, and btrfs. The most catastrophic case was with XFS — just a few files, backed up and restored, but the client was highly demanding and didn’t take it well. The largest failure was with btrfs — after a reboot, a 50 TB filesystem (in mirror, for backups) simply stopped working. No more mounting possible. Data was lost, but I had further backups. The client was informed and understood the situation. Within a few days, the server was rebuilt from scratch on FreeBSD with ZFS — since then, I haven’t lost a single bit.</p><p>In 2018, I started introducing Docker and Podman to the developers with specific needs, especially to help them with their development. Many developers began incessantly asking for Docker, unfortunately some of them only wanted to bypass many limitations that traditional setups imposed. Unfortunately, in many cases, those “limitations” were just bad practices — running outdated versions of software and libraries, or not wanting to bother with keeping the stack updated. There are similar problems with other solutions, but at least I can block them. They’re great for quick deployment and increased security, but they aren’t always the best choice. Plus, they’re not the only option, despite what many people think these days.</p><p>Linux has had major development over the past years, but this has shifted towards specific players’ interests (mainly cloud providers) rather than technical reasons. Maybe not in the kernel, but many Linux distributions and communities now seem focused on the constant push to replace “the old with the new” with solid theoretical reasons but sometimes seemingly without practical benefit.</p><p>For me, computing should solve problems and provide opportunities to those who use it. <strong>I solve problems.</strong> Every change or variation will solve one problem but create new ones. It’s crucial to be mindful not to create worse problems than the ones you’re solving, and today’s enterprise world often seems to overlook that.</p><p>It’s common to see software distributed solely via Docker Compose files. Sometimes I use it as an installation tutorial, but I realize they’re just specific pieces precariously glued together (patched files, specific dependency versions, or nothing works).</p><p>The trend is to rush, to simplify deployments as much as possible, sweeping structural problems under the rug. The goal is to “innovate”, not necessarily improve — just as long as it’s “new” or “how everyone does it, nowadays.”</p><p>A massive business has grown around Linux: certifications, training, pentesting, certified platforms — the community is losing decision-making power. This is a far cry from the early days.</p><p><strong>I solve problems.</strong> And these fast-changing technologies risk creating more problems than they solve, at least in certain situations. The workloads I manage often stay up for years, requiring a more stable, upgradable, and consistent approach.</p><p>If a client’s problem is to have an e-commerce site, they don’t really care if it runs on Docker, Podman, a FreeBSD jail, or a Raspberry Pi cluster — as long as their problem is solved. In fact, clients are happy when their solution is stable, upgradable, and secure.</p><p><strong>There isn’t a single solution to all problems, but many solutions for each problem.</strong> My job is to give clients the best solution to solve their specific problem, not the most fashionable one.</p><p>That’s why I decided, several years ago, <em>to reverse the proportion and implement and the BSDs for all possible workloads.</em></p><p>Each *BSD has its characteristics and target audience. Sometimes these targets overlap, but it’s generally not difficult to choose the most appropriate solution.</p><p>The goal of the migration was to create stable, coherent, upgradable, and secure systems.</p><p>By implementing an OpenBSD system, I often don’t need to install any additional packages. When it’s time to upgrade, it’s simple and secure. When a vulnerability emerges, I’m often fortunate to read “OpenBSD is excluded from this issue because it eliminated this risk X years ago…”</p><p>By implementing a NetBSD system, I know I’m installing a system that isn’t in a rush to release new versions and will likely run for years with only a few package updates and security patches, when necessary. And it’s quite rare for a patch to be required.</p><p>By implementing a FreeBSD system, I know I’ll have ZFS at my disposal, a fast and efficient hypervisor like bhyve, and a native, mature jail system that will ensure service and setup separation coherently and precisely. Not as “add-ons” but built into the system itself.</p><p>Moving to the BSDs means, in my experience, reaching systems that are more stable, more easily upgradable, and more consistent in their parts. They don’t chase the hype of the moment, much like the early days of Linux. In the case of FreeBSD, this also means moving to native ZFS and boot environments, giving me greater peace of mind when it comes to upgrades.</p><p>The initial strategy was to migrate what would soon need updates anyway, what would be moved (thus requiring a new setup), and what was causing problems and deserved a deeper dive.</p><p>First, I decided to migrate to FreeBSD the hypervisors not directly accessed by clients, especially on leased servers. The approach was to create a twin machine (to have an objective performance and stability comparison — it wouldn’t make sense to compare it to a different machine), install FreeBSD, bridge it with the production server, install vm-bhyve, and start copying VMs from Proxmox, reconfiguring the main parameters in the configuration file. In some cases, I already used ZFS on Proxmox, so the first part of the migration was a simple zfs-send/receive. In other cases (e.g., when using Ceph), I made an intermediate move by live-migrating the storage to ZFS and then proceeding as in the first case. The first noticeable effect was a reduction in resources used by the host to handle VM traffic — as expected, only FreeBSD’s basic processes and bhyve were running — but at the same time, there was a significant increase in I/O performance, further enhanced by switching the virtual disk driver from virtio to NVMe. This allowed for in-depth testing and revealed that FreeBSD suffers less under heavy I/O (the VMs on Linux tended to block their I/O, an effect I didn’t notice on FreeBSD) and showed significantly lower loads. In short, it handled the load better.</p><p>As an experiment, I decided to migrate two hosts (each with about 10 VMs) of a client — where I had full control—without telling them, over a weekend. By Tuesday, they called me, concerned: they had noticed a massive performance boost and were worried I had upgraded their hardware without approval, thinking that, “given the performance boost,” it would cost them a lot. After explaining what I had done, they asked me to run further tests and progressively continue migrating everything. 20 hosts, all based on (sometimes slightly older) versions of Proxmox. And so I did, but taking advantage of their open-mindedness, I went a step further.</p><p>Many of their VMs handled simple workloads — PHP websites, or Java-based management systems (running on Tomcat), device monitoring software for industrial machinery, etc. The decision to use VMs had been made by their highly competent internal IT staff to separate environments and dependencies. It was a perfect use case for jails. I decided to try, VM by VM, to replicate the setups inside FreeBSD jails. In some cases, for convenience, I managed to run everything directly in Linux jails (with <a href="https://wiki.freebsd.org/Linuxulator" target="_blank" rel="external nofollow noopener noreferrer">Linuxulator</a>); in others, it was impossible, so I recreated the setups in individual jails. They immediately noticed an effect: faster operations. It didn’t surprise me. We avoided double buffering (VM and OS), so all the saved RAM could be used by ZFS for its cache and by the host to run other services. Some VMs remained as VMs (e.g., Zimbra). By the end of the operation, I had drastically reduced the number of VMs, replaced by jails, and consequently, the number of hosts. From 20 down to 11 — with a significant monthly cost saving.</p><p>The road was now clear, and I progressively continued down this path. One of the most interesting anecdotes: a client told me that they used to start an operation before taking a coffee break, around 15 minutes, to find the task almost done by the time they returned. After the migration, they shared that they launched the process, grabbed their things, and the task was already complete. An estimated reduction from about 18 minutes to 6 minutes on average. I didn’t investigate too much, but I suspect a combination of factors, with the predominant one <a href="https://it-notes.dragas.net/2024/06/10/proxmox-vs-freebsd-which-virtualization-host-performs-better/" target="_blank" rel="external nofollow noopener noreferrer">being bhyve’s NVMe driver</a>.</p><p>The main challenge I often face is ideological. Some people are used to thinking that the ideal solution is <strong>X</strong> — and believe that <strong>X</strong> is the only solution for their problems. Often, <strong>X</strong> is the hype of the moment (a few years ago, I fought to convince people that VMware wasn’t necessary and that Proxmox would be a great solution; today, Proxmox is on everyone’s lips — but it’s not the only solution). Often, <strong>X</strong> is a “cloud” cluster with Kubernetes - running WordPress on it. Even for hosting a law firm’s website, which will be updated every five years.</p><p>When I ask, “Okay, but why? Who will manage it? Where will your data really be, and who will safeguard it?”, I get blank faces. They hadn’t considered these questions. No one had even mentioned them. “But everyone I spoke to proposed this type of solution… It’s like at the beginning with Windows, when I proposed *BSD or Linux. Or later with VMware, when I proposed Proxmox. Or now with Kubernetes, when I propose the BSDs.</p><p>But the simplest solutions are the easiest to maintain and manage over time. My experience has taught me that setting something up is often the easiest part. The hardest part is returning to it after 1, 5, 10 years. Keeping it running, updating it, stabilizing it. For many, IT isn’t their business, but a tool to achieve their goals. A Kubernetes cluster is fantastic, but it requires maintenance. Or it’s external — so it’s no longer ours. We’ve lost control of the data. For many, it’s unnecessary to complicate things. And with every additional layer, we’re creating more problems.</p><p>No *BSD system has ever surprised me during an update or a simple reboot. I’ve never encountered, for example, a network interface renaming from <em>enx3e3300c9e14e</em> to <em>enp10s0f0np0</em> on Linux, effectively locking me out of a server. ix0 will remain ix0.</p><p>I’ve never had to recompile ZFS on FreeBSD, only to find that the module wouldn’t load, blocking the filesystem from mounting after reboot.</p><p>Many of the developers I work with have embraced the challenge. Most of them are passionate about technology, and learning a new operating method has been very interesting. Almost all of them, after experiments, were positive and, in fact, began explicitly requesting “jails” instead of Docker hosts. They started using <a href="https://bastillebsd.org/" target="_blank" rel="external nofollow noopener noreferrer">BastilleBSD</a> to clone “template” jails and deploy them. They learned to access ZFS automatic snapshots and recover lost files. They learned to manage the resources at their disposal without repeating the usual mantra of “we need mooooar powaaaar!” every time there’s a problem, a slowdown, or a storage overload. They’ve returned to trying to understand what’s happening, rather than just assembling pieces, libraries, and containers without considering the effects.</p><p>Others, however, struggled, but remained positive nonetheless. And that’s okay. <strong>I solve problems</strong> and can’t force my solutions on everyone.</p><p>In other cases, the challenge wasn’t technical but “commercial.” Often, decision-makers have little to no technical knowledge. Linux sells well. “Cloud” sells even better. A “NetBSD-based solution,” unfortunately, has less commercial appeal today. So, they want what they can sell, without focusing too much on the advantages of alternative solutions.</p><p>Linux today is subject to many compliance requirements — I’m often asked which version of OpenSSH I’m running — and they complain when the version (the latest from OpenBSD) isn’t considered “secure” because it doesn’t match their procedures (e.g., OpenSSH_9.2p1 Debian-2+deb12u3). They don’t understand when I explain that the “Debian” part refers to the Linux distribution, not a release. Those who prepare these documents are often, sadly, unaware of what they’re asking for. They just have a checklist.</p><p>The transition is ongoing, and as I see opportunities, I’m migrating from Linux to the BSDs whenever possible. Today, I can say that <strong>78% of the hypervisors I manage run on FreeBSD, and 66% of the workloads (VPS, jails, hosts, etc.) are running on one of the BSDs</strong> — including solutions like OPNsense. None of the clients have experienced major issues. No one has complained about performance or reliability. All feedback has been positive. Many appreciate moving away from IT monocultures, especially when problems arise — following the recent severe SSH vulnerability, many clients contacted me, worried. It was nice to tell some of them that the exposed SSH service was running OpenBSD’s version, and that OpenBSD wasn’t affected as they had developed a secure mechanism back in 2001. They appreciated it. They want more setups based on OpenBSD.</p><p><strong>I’m Stefano Marinelli, I solve problems. And I love solving problems using BSD systems.</strong></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starlink offering free internet access for 30 days for Hurricane Helene victims (230 pts)]]></title>
            <link>https://www.starlink.com/support/article/58126733-e4d2-db62-b919-9da261a4e096</link>
            <guid>41732335</guid>
            <pubDate>Thu, 03 Oct 2024 16:37:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.starlink.com/support/article/58126733-e4d2-db62-b919-9da261a4e096">https://www.starlink.com/support/article/58126733-e4d2-db62-b919-9da261a4e096</a>, See on <a href="https://news.ycombinator.com/item?id=41732335">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[MikroPhone: A privacy enhanced, simple and featured RISC-V mobile phone (177 pts)]]></title>
            <link>https://mikrophone.net/</link>
            <guid>41731769</guid>
            <pubDate>Thu, 03 Oct 2024 15:41:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mikrophone.net/">https://mikrophone.net/</a>, See on <a href="https://news.ycombinator.com/item?id=41731769">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">


<p>The goal of this project is to develop a privacy enhanced, simple and fully featured mobile phone. For more information about the project see <a href="https://mikrophone.net/about.html">about</a> page.</p>

<h2>Technical specs</h2>

<ul>
<li>Central MCU: SiFive Freedom E310-G002 (RISC-V) microcontroller</li>
<li>Wireless: Espressif ESP32 Wi-Fi + Bluetooth</li>
<li>Cellular modem: Mini PCIe module (SIMCom SIM7600X or Quectel EC-25)</li>
<li>Graphics: BT817 Display controller</li>
<li>Audio: 2xMAX98357A Class D amplifier / PCM1770 headphone amplifier / ICS-43434 MEMS Microphone / MAX9814 microphone amplifier</li>
<li>Storage: SD card (FAT filesystem, AES/Blowfish encryption support)</li>
<li>Power: Lithium battery + charger (BQ25895)</li>
</ul>

<h2>Source</h2>

<p>git <a href="https://git.majstor.org/">repository</a> can be cloned by running the following command:</p>

<pre><code>git clone https://git.majstor.org/mikroPhone
</code></pre>

<p>Repository contains following directories:</p>

<ul>
<li><code>hw/mikroPhone</code>: KiCad project that contains main board schematics and layout</li>
<li><code>hw/display</code>: KiCad project that contains display adapter board schematics and layout</li>
<li><code>hw/prog</code>: KiCad project that contains FT2232H-56Q adapter board schematics and layout</li>
<li><code>3d/mikroPhone.FCStd</code>: FreeCAD project that contains 3d printable phone case</li>
<li><code>fw/fe310</code>: Firmware for Freedom E310 SoC</li>
<li><code>fw/esp32</code>: Firmware for Espressif ESP32 SoC</li>
<li><code>ecp</code>: <a href="https://mikrophone.net/ecp.html">EllipticCP</a> implementation</li>
</ul>

<h2>Current status</h2>

<p><a href="https://mikrophone.net/phone_big.png"><img src="https://mikrophone.net/phone.png" alt="mikroPhone"></a></p>

<ul>
<li>Hardware design of core part completed. 128x68mm prototype board built and tested;</li>
<li>Central MCU OS almost feature complete. Supports basic phone functionality (cellular voice and SMS messaging);</li>
<li>EllipticCP implementation tested for real time voice communication;</li>
<li>Support for app-module in progress. Module envisaged <a href="https://www.toradex.com/computer-on-modules/verdin-arm-family/nxp-imx-8m-plus">i.MX 8M Plus Computer on Module</a>;</li>
<li>3d printable phone case designed in FreeCAD;</li>
</ul>

<p>For more information go to <a href="https://mikrophone.net/history.html">history</a></p>

<p>For building instructions visit <a href="https://mikrophone.net/build.html">how to build my mikroPhone</a>.</p>

<h2>License</h2>

<p>Hardware licensed under the <a href="https://ohwr.org/project/licenses/wikis/cern-ohl-v1.2">CERN OHL v1.2</a></p>

<p>Software licensed under the <a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html">GPLv2</a></p>

<h2>Acknowledgments</h2>

<p>This project is funded through <a href="https://nlnet.nl/entrust">NGI0 Entrust</a>, a fund established by <a href="https://nlnet.nl/">NLnet</a> with financial support from the European Commission's <a href="https://ngi.eu/">Next Generation Internet</a> program. Learn more at the <a href="https://nlnet.nl/mikroPhone">NLnet project page</a>.</p>

<p><a href="https://nlnet.nl/"><img src="https://nlnet.nl/logo/banner.png" alt="NLnet foundation logo"></a>
<a href="https://nlnet.nl/entrust"><img src="https://nlnet.nl/image/logos/NGI0_tag.svg" alt="NGI Zero Logo"></a></p>
</div></div>]]></description>
        </item>
    </channel>
</rss>