<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 10 Nov 2024 17:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Audio Decomposition – open-source seperation of music to constituent instruments (192 pts)]]></title>
            <link>https://matthew-bird.com/blogs/Audio-Decomposition.html</link>
            <guid>42098491</guid>
            <pubDate>Sun, 10 Nov 2024 03:57:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matthew-bird.com/blogs/Audio-Decomposition.html">https://matthew-bird.com/blogs/Audio-Decomposition.html</a>, See on <a href="https://news.ycombinator.com/item?id=42098491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>
                <iframe src="https://www.youtube.com/embed/-i0PSxcoDH0" allowfullscreen=""></iframe>
                <iframe src="https://www.youtube.com/embed/mzPUfs9sYQE" allowfullscreen=""></iframe>
                <iframe src="https://www.youtube.com/embed/Z7D6obv12zk" allowfullscreen=""></iframe>
                <iframe src="https://www.youtube.com/embed/LkDJ9XT-klU" allowfullscreen=""></iframe>
            </p>
            <h4>Demo Vids: <a href="https://youtu.be/-i0PSxcoDH0">https://youtu.be/-i0PSxcoDH0</a>, <a href="https://youtu.be/mzPUfs9sYQE">https://youtu.be/mzPUfs9sYQE</a>, <a href="https://youtu.be/Z7D6obv12zk">https://youtu.be/Z7D6obv12zk</a>, <a href="https://youtu.be/LkDJ9XT-klU">https://youtu.be/LkDJ9XT-klU</a></h4>
            <h4>Github Repository: <a href="https://github.com/mbird1258/Audio-Decomposition">https://github.com/mbird1258/Audio-Decomposition</a></h4>
            
            <h2>Premise</h2>
            <p>My plan for this project was to create a program to turn music to sheet music. It was mainly incentivised by my own desire to turn music to sheet music and the lack (from what I could tell) of open source, simple algorithms to perform audio source separation. </p>

            <h2>Preparation</h2>
            <h3>Instrument data</h3>
            <p>The instrument data all comes from the <a href="https://theremin.music.uiowa.edu/MIS.html">University of Iowa Electronic Music Studios instrument database</a>. With these files, we find the Fourier transform of the entire wave and the envelope of the wave using the same method as documented below. </p>

            <h2>How it works</h2>
            <p>An instrument’s sound wave is mainly characterized by its fourier transform and envelope. Thus, we can use both of these to hopefully get a good idea of which instrument is playing what note. </p>

            <h3>Fourier Transform</h3>
            <p>The program’s first method of splitting music into constituent notes and instruments is by taking the fourier transform of the music file every 0.1 seconds (spectrogram), and adding together our stored fourier transform of each instrument to recreate the fourier transform of the 0.1 second window. The idea was to hopefully perfectly recreate the music at the set time as the fourier transform should represent the music played relatively well. </p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/1.png"></p><h5>Original fourier transform</h5>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/2.png"></p><h5>Constituent instruments</h5>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/3.png"></p><h5>Recreated fourier transform</h5>
            <p>The magnitudes for each instrument are given by solving the following matrix. The matrix is derived from taking the partial derivative of the MSE cost function by frequency(ex. FT value at 5 hz) with respect to each instrument. Each row in the matrix is a different partial derivative. (First is w.r.t cello, second is w.r.t piano, etc.)</p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/4.jpg"></p><h3>Envelope</h3>
            <p>The first step to matching the envelope of the instrument to the sound wave is to obtain the envelope itself. The envelope is the upper bound of a wave, and although there are functions to do this, they seemingly struggle with noise and certain types of sound waves. Thus, since we have to handle many different instruments at different frequencies, we need a more robust solution. </p>
            <p>To get the envelope, the function splits the sound wave into chunks, before taking the max value at each chunk. To further refine the results, the function finds the points where the envelope is below the original sound wave and adds a new point defining the envelope. </p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/5.jpg" ,="">
            <img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/6.jpg" ,="">
            <img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/7.jpg" ,="">
            <img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/8.jpg" ,=""></p><p>The next step is to split the envelope of the wave into its attack, sustain, and release. The attack is the initial noise of the note, the sustain is while the note is held, and the release is when the note stops. For the instrument samples, we can take the first nonzero value of the wave to get the start of the attack. To get the point between the attack and sustain, we get the first point when the function is concave down or decreasing. To get the point between the sustain and release, we get the first point from the end where the function is increasing or concave down. To get the end of the release, we find the first point from the end where the function is nonzero. </p>
            <p>To further classify the wave, we need to take into account the main forms the wave can take. Some instruments, such as the piano, have static decay, in which they mostly follow an exponential decay shape. On the other hand, some instruments, like the violin, can increase or decrease in volume as the note is sustained. In addition to this, some audio samples in the instrument files are held until their sound expires, while others are released early. To differentiate whether the decay is static or dynamic, if the decay factor is &gt;1, or if it deviates from the decay curve by too much, it’s dynamic. To differentiate if the envelope has a release or not(AS or ASR), we look at the average rate of change across the sustain and the release, and if the rate of change of the release is lower then there is no release. </p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/9.jpg"></p><h5>Different types of wave</h5>
            <p>To deal with the music file, we first take the bandpass filter of the signal for each note frequency. With the filtered wave, we iterate through each instrument. For each instrument, we take the cross correlation of the instrument’s attack(normalized) and release(normalized) to find the start and end of each note, and then take the MSE of the instrument wave and the filtered audio to get our cost for the instrument at that time. After this, we multiply the magnitude we found in the fourier transform step and 1/(cost we found in this step) to get our final magnitudes. </p>

            <h2>Display</h2>
            <p>To display the file, we use matplotlib’s scatter plot with - shaped points to display the sheet music. Originally, I wanted to recreate the audio from the magnitudes, but it led to many issues, took a while, and made troubleshooting much harder. I also tried using matplotlib’s imshow plot, but it’s extremely inefficient in this case as most values are 0, and matplotlib needs to redraw every point regardless of if it’s on screen or not every time we pan or zoom the screen. </p>
            <p><img src="https://matthew-bird.com/assets/imgs/Audio%20Decomposition/10.png" ,=""></p><h2>Results</h2>
            <p>Overall, I think it works quite well. You can use it to make recreating sheet music better(as I did <a href="https://www.noteflight.com/scores/view/ffb92ff50357ea0bb2ab4f7ea7b50a9060c7aa1a">here</a> from <a href="https://www.youtube.com/watch?v=bDtT8VZMEHw">this video</a>), especially if you struggle with finding the right pitch or chords, and it doesn’t take too much time to run either. </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Physical Intelligence's first generalist policy AI can finally do your laundry (183 pts)]]></title>
            <link>https://www.physicalintelligence.company/blog/pi0</link>
            <guid>42098236</guid>
            <pubDate>Sun, 10 Nov 2024 02:26:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.physicalintelligence.company/blog/pi0">https://www.physicalintelligence.company/blog/pi0</a>, See on <a href="https://news.ycombinator.com/item?id=42098236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Published</p><p>October 31, 2024</p><p>Email</p><p><span>research@physicalintelligence.company</span><span>Kevin Black, Noah Brown, Danny Driess, Michael Equi, Adnan Esmail, Chelsea Finn, Nick Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Kay Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, Ury Zhilinsky</span></p><p>Paper</p></div>
<video autoplay="" controls="" loop="" muted="" playsinline="" poster="https://www.physicalintelligence.company/images/p0-hero-video-poster.jpg"><source src="https://dnrjl01ydafck.cloudfront.net/v2/upload/PhysicalIntelligence_2min_v2_out.mp4" type="video/mp4"></video>
<div><p>We are living through an AI revolution: the past decade witnessed practically useful AI assistants, AI systems that can generate photorealistic images and videos, and even models that can predict the structure of proteins. But in spite of all these advances, human intelligence dramatically outpaces AI when it comes to the physical world. To paraphrase <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Moravec%27s_paradox">Moravec’s paradox</a>, winning a game of chess or discovering a new drug represent “easy” problems for AI to solve, but folding a shirt or cleaning up a table requires solving some of the most difficult engineering problems ever conceived. To build AI systems that have the kind of physically situated versatility that people possess, we need a new approach — we need to make AI systems embodied so that they can acquire physical intelligence.</p><p>Over the past eight months, we’ve developed a general-purpose robot foundation model that we call <span>π<sub>0</sub></span> (pi-zero). We believe this is a first step toward our long-term goal of developing artificial physical intelligence, so that users can simply ask robots to perform any task they want, just like they can ask large language models (LLMs) and chatbot assistants. Like LLMs, our model is trained on broad and diverse data and can follow various text instructions. Unlike LLMs, it spans images, text, and actions and acquires physical intelligence by training on embodied experience from robots, learning to directly output low-level motor commands via a novel architecture. It can control a variety of different robots, and can either be prompted to carry out the desired task, or fine-tuned to specialize it to challenging application scenarios. An extended article on our work can be found <a href="https://www.physicalintelligence.company/download/pi0.pdf">here</a>.</p></div>

<div><h3>The promise of generalist robot policies</h3><p>Today’s robots are narrow specialists. Industrial robots are programmed for repetitive motions in choreographed settings, repeatedly making the same weld in the same spot on an assembly line or dropping the same item into the same box. Even such simple behaviors require extensive manual engineering, and more complex behaviors in messy real-world environments such as homes are simply infeasible. AI could change that, allowing robots to <em>learn</em> and follow user instructions, so that programming a new behavior is as simple as telling the robot what you want done, and the robot can itself figure out how to adapt its behavior to its environment. But this requires data. Language models and other foundation models mine data from the web, utilizing a significant fraction of all available documents. There is no such treasure trove of robot data, so to enable a robot to learn a new skill, large amounts of data need to be collected <em>with that particular robot and for that particular application</em>.</p><p>If we could train a single <em>generalist</em> robot policy that can perform a wide range of different skills and control a wide range of different robots, we would overcome this challenge: such a model would need only a little bit of data from each robot and each application. Just as a person can learn a new skill quickly by drawing on a lifetime’s worth of experience, such a generalist robot policy could be specialized to new tasks with only modest amounts of data. This would not be the first time that a generalist model beat a specialist at the specialist’s own task: language models have superseded more specialized language processing systems precisely because they can better solve those downstream specialist tasks by drawing on their diverse and general purpose pretraining. In the same way that LLMs provide a <em>foundation model</em> for language, these generalist robot policies will provide a robot foundation model for physical intelligence.</p><p>To get there, we will need to solve major technical challenges. Our first step is <span>π<sub>0</sub></span>, a prototype model that combines large-scale multi-task and multi-robot data collection with a new network architecture to enable the most capable and dexterous generalist robot policy to date. While we believe this is only a small early step toward developing truly general-purpose robot models, we think it represents an exciting step that provides a glimpse of what is to come.</p><h3>A cross-embodiment training mixture</h3></div>
<div><p><span>π<sub>0</sub></span> uses Internet-scale vision-language pre-pretraining, open-source robot
manipulation datasets, and our own datasets consisting of dexterous tasks from
8 distinct robots. The model can then perform a wide variety of tasks, via
either zero-shot prompting or fine-tuning.</p></div>
<p>Our first prototype generalist robot policy is trained on the largest robot
interaction dataset to date. The full training mixture includes both
open-source data and a large and diverse dataset of dexterous tasks that we
collected across 8 distinct robots.</p>
<div><p>Our dataset contains diverse tasks, with each task exhibiting a wide variety of motion primitives, many different objects, and various scenes.</p></div>
<div><p>The tasks in this dataset exercise different dimensions of robot dexterity while covering the range of real tasks that these robots might be asked to perform, from bussing dishes to packing items into envelopes, folding clothing, routing cables, assembling boxes, plugging in power plugs, packing food into to-go boxes, and picking up and throwing out trash. Our goal in selecting these tasks is not to solve any particular application, but to start to provide our model with a general understanding of physical interactions — an initial foundation for physical intelligence.</p><h3>Inheriting Internet-scale semantic understanding</h3><p>Beyond training on many different robots, <span>π<sub>0</sub></span> inherits semantic knowledge and visual understanding from Internet-scale pretraining by starting from a pre-trained vision-language model (VLM). VLMs are trained to model text and images on the web — widely used VLMs include GPT-4V and Gemini. We use a smaller 3 billion parameter VLM as a starting point, and adapt it for real-time dexterous robot control.</p><p>VLMs effectively transfer semantic knowledge from the web, but they are trained to output only discrete language tokens. Dexterous robot manipulation requires <span>π<sub>0</sub></span> to output motor commands at a high frequency, up to 50 times per second. To provide this level of dexterity, we developed a novel method to augment pre-trained VLMs with continuous action outputs via flow matching, a variant of diffusion models. Starting from diverse robot data and a VLM pre-trained on Internet-scale data, we train our vision-language-action flow matching model, which we can then post-train on high-quality robot data to solve a range of downstream tasks.</p></div>
<div><p><img alt="Our vision-language-action model uses a novel flow matching formulation, which augments a vision-language model pre-trained on Internet-scale data with continuous outputs. This enables high-frequency dexterous control, making it particularly well-suited for fine-tuning for complex robot manipulation tasks, such as folding laundry or assembling boxes." loading="lazy" width="569" height="414" decoding="async" data-nimg="1" src="https://www.physicalintelligence.company/images/blog-03.jpg"></p><p>Our vision-language-action model uses a novel flow matching formulation, which augments a vision-language model pre-trained on Internet-scale data with continuous outputs. This enables high-frequency dexterous control, making it particularly well-suited for fine-tuning for complex robot manipulation tasks, such as folding laundry or assembling boxes.</p></div>
<div><h3>Post-training for dexterous manipulation</h3><p>More complex and dexterous tasks may require the model to be fine-tuned to specialize it to downstream challenges. Fine-tuning the model with high-quality data for a challenging task, such as folding laundry, is analogous to the post-training process employed by LLM designers. Pre-training teaches the model about the physical world, while fine-tuning forces it to perform a particular task <em>well</em>. Let’s take a look at some of these tasks.</p></div>
<div><p>After post-training, the robot can unload the dryer, bring the clothes over to the table, and fold the clothes into a stack. The video is uncut, from a single policy operating fully autonomously.</p></div>
<p><strong>Laundry</strong>. We fine-tuned <span>π<sub>0</sub></span> to fold laundry, using either a mobile robot
or a fixed pair of arms. The goal is to get the clothing into a neat stack.
This task is exceptionally difficult for robots (...and some humans): while a
single t-shirt laid flat on the table can sometimes be folded just by repeating
a pre-scripted set of motions, a pile of tangled laundry can be crumpled in
many different ways, so it is not enough to simply move the arms through the
same motion. To our knowledge, no prior robot system has been demonstrated to
perform this task at this level of complexity.</p>

<p>Notably, by training on diverse data, we find that the robot is able to recover
when someone tries to intervene in a variety of different ways.</p>

<p><strong>Table bussing.</strong> We also fine-tuned the model to bus a table. This requires
the robot to pick up dishes and trash on the table, putting any dishes,
cutlery, or cups into a bussing bin, and putting trash into the trash bin. This
task requires the robot to handle a dizzying variety of items. One of the
exciting consequences of training <span>π<sub>0</sub></span> on large and diverse datasets was the
range of emergent strategies that the robot employed: instead of simply
grasping each item in turn, the model could stack multiple dishes to put them
into the bin together, or shake off trash from a plate into the garbage before
placing the plate into the bussing bin.</p>

<p><strong>Assembling a box.</strong> Here, the robot has to take a flattened cardboard box and
build it, folding the sides and then tucking in the flaps. This is very
difficult, because each fold and tuck might fail in unexpected ways, so the
robot needs to watch its progress and adjust as it goes. It also needs to brace
the box with both arms, even using the table, so that the partially folded box
doesn’t come apart.</p>

<div><h3>Evaluating and comparing <span>π<sub>0</sub></span> to prior models</h3><p>We compared <span>π<sub>0</sub></span> to other robot foundation models that have been proposed in the academic literature on our tasks: <a target="_blank" rel="noopener noreferrer" href="https://openvla.github.io/">OpenVLA</a>, a 7B parameter VLA model that uses discretized actions, and <a target="_blank" rel="noopener noreferrer" href="https://octo-models.github.io/">Octo</a>, a 93M parameter model that uses diffusion outputs. These tasks are very difficult compared to those that are typically used in academic experiments — for example, the tasks in the <a target="_blank" rel="noopener noreferrer" href="https://openvla.github.io/">OpenVLA evaluation</a> typically consist of single stage behaviors (e.g., “put eggplant into pot”), whereas our simplest bussing task consisting of sorting multiple objects into either a garbage bin or a bussing bin, and our more complex tasks might require multiple stages, manipulation of deformable objects, and the ability to deploy one of many possible strategies given the current configuration of the environment. These tasks are evaluated according to a scoring rubric that assigns a score of 1.0 for a fully successful completion, with “partial credit” for partially correct execution (e.g., bussing half the objects leads to a score of 0.5). The average scores across 5 zero-shot evaluation tasks are shown below, comparing the full <span>π<sub>0</sub></span> pre-trained model, <span>π<sub>0</sub></span>-small, which is a 470M parameter model that does not use VLM pre-training, <a target="_blank" rel="noopener noreferrer" href="https://openvla.github.io/">OpenVLA</a>, and <a target="_blank" rel="noopener noreferrer" href="https://octo-models.github.io/">Octo</a>. Although OpenVLA and Octo can attain non-zero performance on the easiest of these tasks (“Bussing Easy”), <span>π<sub>0</sub></span> is by far the best-performing model across all of the tasks. The small version, <span>π<sub>0</sub></span>-small, attains the second best performance, but there is more than a 2x improvement in performance from using our full-size architecture with VLM pre-training.</p></div>
<div><div><p><img alt="Zero-shot Performance Comparison Across Tasks" loading="lazy" width="1390" height="700" decoding="async" data-nimg="1" srcset="https://www.physicalintelligence.company/_next/image?url=%2Fimages%2Fp0-zero-shot-comparison.png&amp;w=1920&amp;q=75 1x, https://www.physicalintelligence.company/_next/image?url=%2Fimages%2Fp0-zero-shot-comparison.png&amp;w=3840&amp;q=75 2x" src="https://www.physicalintelligence.company/_next/image?url=%2Fimages%2Fp0-zero-shot-comparison.png&amp;w=3840&amp;q=75"></p></div><p>Average scores for <span>π<sub>0</sub></span>, <span>π<sub>0</sub></span>-small, OpenVLA, and Octo for zero-shot evaluation on 5 test tasks. Across all of the tasks, <span>π<sub>0</sub></span> consistently attains good performance, and outperforms both the small variant and the prior models.</p></div>
<div><p>We include detailed videos from our rigorous empirical evaluation below, with examples of successful and failed episodes for both our zero-shot experiments and the fine-tuning evaluation. Complete results from all experiments can be found in the <a href="https://www.physicalintelligence.company/download/pi0.pdf">full article</a>.</p><h3>Where do we go from here?</h3><p>Our mission at Physical Intelligence is to develop foundation models that can control any robot to perform any task. Our experiments so far show that such models can control a variety of robots and perform tasks that no prior robot learning system has done successfully, such as folding laundry from a hamper or assembling a cardboard box. But generalist robot policies are still in their infancy, and we have a long way to go. The frontiers of robot foundation model research include long-horizon reasoning and planning, autonomous self-improvement, robustness, and safety. We expect that the coming year will see major advances along all of these directions, but the initial results paint a promising picture for the future of robot foundation models: highly capable generalist policies that inherit semantic understanding from Internet-scale pretraining, incorporate data from many different tasks and robot platforms, and enable unprecedented dexterity and physical capability.</p><p>We also think that succeeding at this will require not only new technologies and more data, but a collective effort involving the entire robotics community. We already have collaborations underway with a number of companies and robotics labs, both to refine hardware designs for teleoperation and autonomy, and incorporate data from our partners into our pre-trained models so that we can provide access to models adapted to their specific platforms.</p><p>If you are interested in collaborating, please <a target="_blank" rel="noopener noreferrer" href="mailto:collaborate@physicalintelligence.company">reach out</a>. We are particularly excited to work with companies scaling up data collection with robots deployed for real-world applications, who are looking to collaborate on autonomy.</p><p>We are also hiring! If you'd be interested in <a href="https://www.physicalintelligence.company/join-us">joining us</a> please get in touch.</p><p>For researchers interested in our work, collaborations, or other queries, please write to <a target="_blank" rel="noopener noreferrer" href="mailto:research@physicalintelligence.company">research@physicalintelligence.company</a>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs have reached a point of diminishing returns (126 pts)]]></title>
            <link>https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached</link>
            <guid>42097774</guid>
            <pubDate>Sun, 10 Nov 2024 00:25:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached">https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached</a>, See on <a href="https://news.ycombinator.com/item?id=42097774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg" width="1170" height="1227" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1227,&quot;width&quot;:1170,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243099,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>For years I have been warning that “scaling” — eeking out improvements in AI by adding more data and more compute, without making fundamental architectural changes — would not continue forever. In my most notorious article, in March of 2022, I argued that “</span><a href="https://nautil.us/deep-learning-is-hitting-a-wall-238440/" rel="">deep learning is hitting a wall</a><span>”. Central to the argument was that pure scaling would not solve hallucinations or abstraction; I concluded that “there are serious holes in the scaling argument.” </span></p><p>And I got endless grief for it. Sam Altman implied (without saying my name, but riffing on the images in my then-recent article) I was a “mediocre deep learning skeptic”; Greg Brockman openly mocked the title. Yann LeCun wrote that deep learning wasn’t hitting a wall, and so on. Elon Musk himself made fun of me and the title earlier this year.</p><p><span>The thing is, </span><strong>in the long term, science isn’t majority rule</strong><span>. In the end, the truth generally outs. Alchemy had a good run, but it got replaced by chemistry. The truth is that scaling is running out, and that truth is, at last coming out.</span></p><p>A few days ago, the well-known venture capitalist Marc Andreesen started to spill the beans, saying on a podcast “we're increasing [graphics processing units] at the same rate, we're not getting the intelligent improvements at all out of it” –  which is basically VC-ese for “deep learning is hitting a wall.”</p><p><span>Just a few moments ago, Amir Efrati, editor of the industry trade journal </span><em>The Information</em><span> further confirmed that we have reached a period of diminishing returns, writing on X that “OpenAI's [upcoming] Orion model shows how GPT improvements are slowing down”.  </span></p><p><span>Just as I argued here in April 2024, LLMs </span><a href="https://open.substack.com/pub/garymarcus/p/evidence-that-llms-are-reaching-a?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web" rel="">have reached a point of diminishing returns</a><span>. </span></p><p>§</p><p>The economics are likely to be grim. Sky high valuation of companies like OpenAI and Microsoft are largely based on the notion that LLMs will, with continued scaling, become artificial general intelligence. As I have always warned, that’s just a fantasy. There is no principled solution to hallucinations in systems that traffic only in the statistics of language without explicit representation of facts and explicit tools to reason over those facts.</p><p>LLMs will not disappear, even if improvements diminish, but the economics will likely never make sense: additional training is expensive, the more scaling, the more costly. And, as I have been warning, everyone is landing in more or less the same place, which leaves nobody with a moat. LLMs such as they are, will become a commodity; price wars will keep revenue low. Given the cost of chips, profits will be elusive. When everyone realizes this, the financial bubble may burst quickly; even NVidia might take a hit, when people realize the extent to which its valuation was based on a false premise.</p><p>§</p><p>The sociology here has been perverse too, for a really long time. Many people (especially LeCun, but also a legion of tech influencers who followed his lead) have tried to deplatform me. </p><p>The media has done little to counter the mob psychology; they have mostly listened to people with money, with vested interests, not to scientists. Many of us, including Melanie Mitchell, Subbarao Kambahapati, Emily Bender, Ernie Davis, etc. have been emphasizing for ages that there are principled limits with LLMs. Media (with notable exceptions like Ezra Klein, who gave me a clear platform for skepticism in January 2023) has rarely listened, instead often glorifying the hype of people like Altman and Musk.</p><p>Worse, the US AI policy now, and likely in the next administration, has largely been driven by hype, and the assumption that returns for LLM scaling would not diminish.  And yet here we are at the end of 2024, and even Altman and Andreesen are perhaps starting to see it.</p><p>Meanwhile, precious little investment has been made in other approaches. If LLMs won’t get the US to trustworthy AI, and our adversaries invest in alternative approaches, we could easily be outfoxed. The US has been putting all its AI eggs in the LLM basket, and that may well prove to be an epic, massive mistake.</p><p>§</p><p>In April, when I first saw direct empirical evidence that this moment had come, I wrote (and stand by):</p><blockquote><p>If enthusiasm for GenAI dwindles and market valuations plummet, AI won’t disappear, and LLMs won’t disappear; they will still have their place as tools for statistical approximation.</p><p>But that place may be smaller; it is entirely possible that LLMs on their own will never live up to last year’s wild expectations.</p><p>Reliable, trustworthy AI is surely achievable, but we may need to go back to the drawing board to get there.</p></blockquote><p>I’m glad that the market is finally recognizing that what I’ve been saying is true. Hopefully now we can make real progress.</p><p><em><strong>Gary Marcus</strong><span> has been warning about the foundational limits to traditional neural network approaches since his 2001 book The Algebraic Mind (where he first described hallucinations), and amplified those warnings in Rebooting AI and his most recent book Taming Silicon Valley.</span></em><span> </span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grim Fandango (280 pts)]]></title>
            <link>https://www.filfre.net/2024/11/grim-fandango/</link>
            <guid>42097261</guid>
            <pubDate>Sat, 09 Nov 2024 22:17:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.filfre.net/2024/11/grim-fandango/">https://www.filfre.net/2024/11/grim-fandango/</a>, See on <a href="https://news.ycombinator.com/item?id=42097261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
														<p><a href="https://www.filfre.net/2024/11/grim-fandango/10591938-grim-fandango-windows-front-cover/" rel="attachment wp-att-6154"><img fetchpriority="high" decoding="async" src="https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover-251x300.jpg" alt="" width="377" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover-251x300.jpg 251w, https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover.jpg 670w" sizes="(max-width: 377px) 100vw, 377px"></a></p>
<blockquote><p>My one big regret was <a href="https://www.filfre.net/2024/01/televising-the-revolution">the PlayStation version [of Broken Sword]</a>. No one thought it would sell, so we kept it like the PC version. In hindsight, I think if we had introduced direct control in this game, it would have been enormous.</p>
<p>— Charles Cecil of Revolution Software, speaking from the Department of Be Careful What You Wish For</p>
</blockquote>
<hr>

<p>One day in June of 1995, Tim Schafer came to work at LucasArts and realized that, for the first time in a long time, he didn’t have anything pressing to do. <a href="https://www.filfre.net/2021/07/full-throttle"><em>Full Throttle</em></a>, his biker movie of an adventure game, had been released several weeks before. Now, all of the initial crush of interviews and marketing logistics was behind him. A mountain had been climbed. So, as game designers do, he started to think about what his next Everest should be.</p>
<p>Schafer has told in some detail how he came up with the core ideas behind <em>Grim Fandango</em> over the course of that summer of 1995.</p>
<blockquote><p>The truth is, I had part of the Fandango idea before I did Full Throttle. I wanted to do a game that would feature those little papier-mâché folk-art skeletons from Mexico. I was looking at their simple shapes and how the bones were just painted on the outside, and I thought, “Texture maps! 3D! The bones will be on the outside! It’ll look cool!”</p>
<p>But then I was stuck. I had these skeletons walking around the Land of the Dead. So what? What did they do? Where were they going? What did they want? Who’s the main character? Who’s the villain? The mythology said that the dead walk the dark plane of the underworld known as Mictlān for four years, after which their souls arrive at the ninth plane, the land of eternal rest. Sounds pretty “questy” to me. There you have it: a game.</p>
<p>“Not cool enough,” said Peter Tscale, my lead artist. “A guy walking in a supernatural world? What’s he doing? Supernatural things? It just sounds boring to me.”</p>
<p>So, I revamped the story. Adventure games are all fantasies really, so I had to ask myself, “Who would people want to be in a game? What would people want to do?” And in the Land of the Dead, who would people rather be than Death himself? Being the Grim Reaper is just as cool as being a biker, I decided. And what does the Grim Reaper do? He picks up people who have died and carts them over from the other world. Just like a driver of a taxi or limo.</p>
<p>Okay, so that’s Manny Calavera, our main character. But who’s the bad guy? What’s the plot? I had just seen <a href="https://www.imdb.com/title/tt0071315">Chinatown</a>, and I really liked the whole water-supply/real-estate scam that Noah Cross had going there, so of course I tried to rip that off and have Manny be a real-estate salesman who got caught up in a real-estate scandal. Then he was just like the guys in <a href="https://www.imdb.com/title/tt0104348/">Glengarry Glen Ross</a>, always looking for the good leads. But why would Hector Lemans, my villain, want real estate? Why would anyone? They’re dead! They’re only souls. What do souls in the Land of the Dead want?</p>
<p>They want to get out! They want safe passage out, just like in <a href="https://www.imdb.com/title/tt0034583/">Casablanca</a>! The Land of the Dead is a transitory place, and everybody’s waiting around for their travel papers. So Manny is a travel agent, selling tickets on the big train out of town, and Hector’s stealing the tickets…</p></blockquote>
<div id="attachment_6156"><p><a href="https://www.filfre.net/2024/11/grim-fandango/glottis/" rel="attachment wp-att-6156"><img decoding="async" aria-describedby="caption-attachment-6156" src="https://www.filfre.net/wp-content/uploads/2024/10/glottis-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/glottis-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/glottis.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6156">The missing link between <em>Full Throttle</em> and <em>Grim Fandango</em> is Manny’s chauffeur and mechanic Glottis, a literal speed demon.</p></div>
<p>This, then, became the elevator pitch for <em>Grim Fandango</em>. Begin with the rich folklore surrounding Mexico’s Day of the Dead, a holiday celebrated each year just after Halloween, which combines European Christian myths about death and the afterlife with the older, indigenous ones that still haunt the Aztec ruins of Teopanzolco. Then combine it with classic film noir to wind up with Raymond Chandler in a Latino afterlife. It was nothing if not a strikingly original idea for an adventure game. But there was also one more, almost equally original part of it: to do it in 3D.</p>
<p>To hear Tim Schafer tell the story, the move away from LucasArts’s traditional pixel art and into the realm of points, polygons, and textures was motivated by his desire to deliver a more cinematic experience. By no means does this claim lack credibility; as you can gather by reading what he wrote above, Schafer was and is a passionate film buff, who tends to resort to talking in movie titles when other forms of communication fail him. The environments in previous LucasArts adventure games — even the self-consciously cinematic <em>Full Throttle</em> — could only be shown from the angle the pixel artists had chosen to drawn them from. In this sense, they were like a theatrical play, or a <em>really</em> old movie, from the time before Orson Welles emancipated his camera and let it begin to roam freely through his sets in <a href="https://www.imdb.com/title/tt0033467/"><em>Citizen Kane</em></a>. By using 3D, Schafer could become the Orson Welles of adventure games; he would be able to deliver dramatic angles and closeups as the player’s avatar moved about, would be able to put the player <em>in</em> his world rather than forever forcing her to look down on it from on-high. This is the story he still tells today, and there’s no reason to believe it isn’t true enough, as far as it goes.</p>
<p>Nevertheless, it’s only half of the full story. The other half is a messier, less idealistic tale of process and practical economics.</p>
<p>Reckoned in their cost of production per hour of play time delivered, adventure games stood apart from any other genre in their industry, and not in a good way. Building games entirely out of bespoke, single-use puzzles and assets was <em>expensive</em> in contrast to the more process-intensive genres. As time went on and gamers demanded ever bigger, prettier adventures, in higher resolutions with more colors, this became more and more of a problem. Already in 1995, when adventure games were still selling very well, the production costs that were seemingly inherent to the genre were a cause for concern. And the following year, when the genre <a href="https://www.filfre.net/2022/06/toonstruck-or-a-case-study-in-the-death-of-adventure-games">failed to produce</a> a single million-plus-selling breakout hit for the first time in half a decade, they began to look like an existential threat. At that point, LucasArts’s decision to address the issue proactively in <em>Grim Fandango </em>by switching from pixel art to 3D suddenly seemed a very wise move indeed. For a handful of Silicon Graphics workstations running 3D-modelling software could churn out images far more quickly than an army of pixel artists, at a fraction of the cost per image. If the graphics that resulted lacked some of the quirky, hand-drawn, cartoon-like personality that had marked LucasArts’s earlier adventure games, they made up for that by virtue of their flexibility: a scene could be shown from a different angle just by changing a few parameters instead of having to redraw it from scratch. This really did raise the prospect of making the more immersive games that Tim Schafer desired. But from a bean counter’s point of view, the best thing about it was the cost savings.</p>
<p>And there was one more advantage as well, one that began to seem ever more important as time went on and the market for adventure games running on personal computers continued to soften. Immersive 3D was more or less the default setting of the Sony PlayStation, which had <a href="https://www.filfre.net/2023/12/putting-the-j-in-the-rpg-part-2-playstation-for-the-win">come roaring out of Japan</a> in 1995 to seize the title of the most successful games console of the twentieth century just before the curtain fell on that epoch. In addition to its 3D hardware, the PlayStation sported a CD drive, memory cards for saving state, and a slightly older typical user than the likes of Nintendo and Sega. And yet, although a number of publishers ported their 2D computer-born adventure games to the PlayStation, they never felt entirely at home there, having been designed for a mouse rather than a game controller.<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_1');" onkeypress="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_1');"><sup id="footnote_plugin_tooltip_6147_1_1">[1]</sup></a><span id="footnote_plugin_tooltip_text_6147_1_1">A mouse was available as an accessory for the PlayStation, but it was never very popular.</span></span> A 3D adventure game with a controller-friendly interface might be a very different proposition. If it played its cards right, it would open the door to an installed base of customers five to ten times the size of the extant market for games on personal computers.</p>
<div id="attachment_6149"><p><a href="https://www.filfre.net/2024/11/grim-fandango/manny/" rel="attachment wp-att-6149"><img decoding="async" aria-describedby="caption-attachment-6149" src="https://www.filfre.net/wp-content/uploads/2024/10/manny-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/manny-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/manny.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6149">Working with 3D graphics in the late 1990s required some clever sleight of hand if they weren’t to end up looking terrible. <em>Grim Fandango’</em>s masterstroke was to make all of its characters — like the protagonist Manny Calavera, whom you see above — mere skeletons, whose faces are literally painted onto their skulls. (The characters are shown to speak by manipulating the <a href="https://www.filfre.net/2019/01/life-off-the-grid-part-1-making-ultima-underworld">texture maps</a> that represent their faces, not by manipulating the underlying 3D models themselves.) This approach gave the game a look reminiscent of another of its cinematic inspirations, Tim Burton’s <a href="https://www.imdb.com/title/tt0107688/"><em>The Nightmare Before Christmas</em></a>, whilst conveniently avoiding all of the complications of trying to render pliant flesh. A win-win, as they say. Or, as Tim Schafer said: “Instead of fighting the tech limitations of 3D, you have to embrace them and turn them into a style.”</p></div>
<p>But I’m afraid I’ve gotten slightly ahead of myself. This constellation of ideas, affordances, problems, and solutions was still in a nascent form in November of 1995, when LucasArts hired a young programmer fresh out of university by the name of Bret Mogilefsky. Mogilefsky was a known quantity already, having worked at LucasArts as a tester on and off while he was earning his high-school and university diplomas. Now, he was entrusted with the far more high-profile task of making SCUMM, LucasArts’s venerable adventure engine, safe for 3D.</p>
<p>After struggling for a few months, he concluded that this latest paradigm shift was just too extreme for an engine that had been <a href="https://www.filfre.net/2015/07/a-new-force-in-games-part-3-scumm">created on a Commodore 64</a> circa 1986 and ported and patched from there. He would have to tear SCUMM down so far in order to add 3D functionality that it would be easier and cleaner simply to make a new engine from scratch. He told his superiors this, and they gave him permission to do so — albeit suspecting all the while, Mogilefsky is convinced, that he would eventually realize that game engines are easier envisioned than implemented and come crawling back to SCUMM. By no means was he the first bright spark at LucasArts who thought he could reinvent the adventuring wheel.</p>
<p>But he did prove the first one to call his bosses’ bluff. The engine that he called GrimE (“<em>Grim</em> Engine,” but pronounced like the synonym for “dirt”) used a mixture of pre-rendered and real-time-rendered 3D. The sets in which Manny and his friends and enemies played out their dramas would be the former; the aforementioned actors themselves would be the latter. GrimE was a piebald beast in another sense as well: that of cheerfully appropriating whatever useful code Mogilefsky happened to find lying around the house at LucasArts, most notably from the first-person shooter <a href="https://www.filfre.net/2024/04/jedi-knight-plus-notes-on-an-expanded-universe"><em>Jedi Knight</em></a>.</p>
<p>Like SCUMM before it, GrimE provided relatively non-technical designers like Tim Schafer with a high-level scripting language that they could use themselves to code all of the mechanics of plot and puzzles. Mogilefsky adapted for this task Lua, a new, still fairly obscure programming language out of Brazil. It was an inspired choice. Elegant, learnable, and yet infinitely and easily extendible, Lua has gone on to become a staple language of modern game development, to be found today in such places as the wildly popular Roblox platform.</p>
<p>The most frustrating aspects of GrimE from a development perspective all clustered around the spots where its two approaches to 3D graphics rubbed against one another, producing a good deal of friction in the process. If, for example, Manny was to drink a glass of whiskey, the pre-rendered version of the glass that was part of the background set had to be artfully swapped with its real-time-rendered incarnation as soon as Manny began to interact with it. Getting such actions to look seamless absorbed vastly more time and energy than anyone had expected it to.</p>
<p>In fact, if the bean counters had been asked to pass judgment, they would have had a hard time labeling GrimE a success at all under their metrics. <em>Grim Fandango</em> was in active development for almost three full years, and may have ended up costing as much as $3 million. This was at least two and a half times as much as <em>Full Throttle</em> had cost, and placed it in the same ballpark as <a href="https://www.filfre.net/2024/04/the-curse-of-monkey-island"><em>The Curse of Monkey Island</em></a>, LucasArts’s last and most audiovisually lavish SCUMM adventure, which was released a year before <em>Grim Fandango</em>. Further, despite employing a distinctly console-like control scheme in lieu of pointing and clicking with the mouse, <em>Grim Fandango</em> would never make it to the PlayStation; GrimE ended up being just too demanding to be made to work on such limited hardware.<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_2');" onkeypress="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_2');"><sup id="footnote_plugin_tooltip_6147_1_2">[2]</sup></a><span id="footnote_plugin_tooltip_text_6147_1_2"><em>Escape from Monkey Island</em>, the only other game ever made using GrimE, was ported to the more capable PlayStation 2 in 2001.</span></span></p>
<p>All that aside, though, the new engine remained an impressive technical feat, and did succeed in realizing most of Tim Schafer’s aesthetic goals for it. Even the cost savings it apparently failed to deliver come with some mitigating factors. Making the first game with a new engine is always more expensive than making the ones that follow; there was no reason to conclude that GrimE couldn’t deliver real cost savings on LucasArts’s <em>next</em> adventure game. Then, too, for all that <em>Grim Fandango</em> wound up costing two and a half times as much as <em>Full Throttle</em>, it was also well over two and a half times as long as that game.</p>
<p>“Game production schedules are like flying jumbo jets,” says Tim Schafer. “It’s very intense at the takeoff and landing, but in the middle there’s this long lull.” The landing is the time of crunch, of course, and the crunch on <em>Grim Fandango</em> was protracted and brutal even by the industry’s usual standards, stretching out for months and months of sixteen- and eighteen-hour days. For by the beginning of 1998, the game was way behind schedule and way over budget, facing a marketplace that was growing more and more unkind to the adventure genre in general. This was not a combination to instill patience in the LucasArts executive suite. Schafer’s team did get the game done by the autumn of 1998, as they had been ordered to do in no uncertain terms, but only at a huge cost to their psychological and even physical health.</p>
<p>Bret Mogilefsky remembers coming to Schafer at one point to tell him that he just didn’t think he could go on like this, that he simply had to have a break. He was met with no sympathy whatsoever. To be fair, he probably shouldn’t have expected any. Crunch was considered par for the course in the industry during this era, and LucasArts was among the worst of its practitioners. Long hours spent toiling for ridiculously low wages — Mogilefsky was hired to be the key technical cog in this multi-million-dollar project for a salary of about $30,000 per year — were considered the price you paid for the privilege of working at The <em>Star Wars</em> Company.</p>
<p>Even setting aside the personal toll it took on the people who worked there, crunch did nothing positive for the games themselves. As we’ll see, <em>Grim Fandango</em> shows the scars of crunch most obviously in its dodgy puzzle design. Good puzzles result from a methodical, iterative process of testing and carefully considering the resulting feedback. <em>Grim Fandango</em> did not benefit from such a process, and this lack is all too plainly evident.</p>
<p>But before I continue making some of you very, very mad at me, let me take some time to note the strengths of <em>Grim Fandango</em>, which are every bit as real as its weaknesses. Indeed, if I squint just right, so that my eyes <em>only</em> take in its strengths, I have no problem understanding why it’s to be found on so many lists of “The Best Adventure Games Ever,” sometimes even at the very top.</p>
<p>There’s no denying the stuff that <em>Grim Fandango</em> does well. Its visual aesthetic, which I can best describe as 1930s Art Deco meets Mexican folk art meets 1940s gangster flick, is unforgettable. And it’s married to a script that positively crackles with wit and pathos. Our hero Manny is the rare adventure-game character who can be said to go through an actual character arc, who grows and evolves over the course of his story. The driving force behind the plot is his love for a woman named Meche. But his love isn’t the puppy love that Guybrush Threepwood has for Elaine in the <a href="https://www.filfre.net/2017/03/monkey-island-or-how-ron-gilbert-made-an-adventure-game-that-didnt-suck"><em>Monkey Island</em></a> games; the relationship is more nuanced, more adult, more <em>complicated</em>, and its ultimate resolution is all the more moving for that.</p>
<div id="attachment_6150"><p><a href="https://www.filfre.net/2024/11/grim-fandango/sprout/" rel="attachment wp-att-6150"><img decoding="async" aria-describedby="caption-attachment-6150" src="https://www.filfre.net/wp-content/uploads/2024/10/sprout-300x225.webp" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/sprout-300x225.webp 300w, https://www.filfre.net/wp-content/uploads/2024/10/sprout.webp 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6150">How do you create real stakes in a story where everyone is already dead? The Land of the Death’s equivalent of death is “sprouting,” in which a character is turned into a bunch of flowers and forced to live another life in that form. Why shouldn’t the dead fear life as much as the living fear death?</p></div>
<p>Tim Schafer did not grow up with the Latino traditions that are such an inextricable part of <em>Grim Fandango</em>. Yet the game never feels like the exercise in clueless or condescending cultural tourism it might easily have become. On the contrary, the setting feels full-bodied, lived-in, natural. The cause is greatly aided by a stellar cast of voice actors with just the right accents. The Hollywood veteran Tony Plana, who plays Manny, is particularly good, teasing out exactly the right blend of world-weary cynicism and tarnished romanticism. And Maria Canalas, who plays Meche, is equally perfect in her role. The non-verbal soundtrack by Peter McConnell is likewise superb, a mixture of mariachi music and cool jazz that shouldn’t work but does. Sometimes it soars to the forefront, but more often it tinkles away in the background, setting the mood. You’d only notice it if it was gone — but trust me, then you would <em>really</em> notice.</p>
<p>This is a <em>big</em> game as well as a striking and stylish one — in fact, by most reckonings the biggest adventure that LucasArts ever made. Each of its four acts, which neatly correspond to the four years that the average soul must spend wandering the underworld before going to his or her final rest, is almost big enough to be a self-contained game in its own right. Over the course of <em>Grim Fandango</em>, Manny goes from being a down-on-his-luck Grim Reaper cum travel agent to a nightlife impresario, from the captain of an ocean liner to a prisoner laboring in an underwater mine. The story does arguably peak too early; the second act, an extended homage to <em>Casablanca</em> with Manny in the role of Humphrey Bogart, is so beautifully realized that much of what follows is slightly diminished by the comparison. Be that as it may, though, it doesn’t mean any of what follows is bad.</p>
<div id="attachment_6155"><p><a href="https://www.filfre.net/2024/11/grim-fandango/rubacava/" rel="attachment wp-att-6155"><img decoding="async" aria-describedby="caption-attachment-6155" src="https://www.filfre.net/wp-content/uploads/2024/10/rubacava-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/rubacava-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/rubacava.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6155">The jump cut to Manny’s new life as a bar owner in the port city of Rubacava at the beginning of the second act is to my mind the most breathtaking moment of the game, the one where you first realize how expansive its scope and ambition really are.</p></div>
<p>All told, then, I have no real beef with anyone who chooses to label <em>Grim Fandango</em> an aesthetic masterpiece. If there was an annual award for style in adventure games, this game would have won it easily in 1998, just as Tim Schafer’s <em>Full Throttle</em> would have taken the prize for 1995. Sadly, though, it seems to me that the weaknesses of both games are also the same. In both of their cases, once I move beyond the aesthetics and the storytelling and turn to the gameplay, some of the air starts to leak out of the balloon.</p>
<p>The interactive aspects of <em>Grim Fandango</em> — you know, all that stuff that actually makes it a game — are dogged by two overarching sets of problems. The first is all too typical for the adventure genre: overly convoluted, often nonsensical puzzle design. Tim Schafer was always more intrinsically interested in the worlds, characters, and stories he dreamed up than he was in puzzles. This is fair enough on the face of it; he is very, very good at those things, after all. But it does mean that he needs a capable support network to ensure that his games play as well as they look and read. He had that support for 1993’s <a href="https://www.filfre.net/2019/06/day-of-the-tentacle"><em>Day of the Tentacle</em></a>, largely in the person of his co-designer Dave Grossman; the result was one of the best adventure games LucasArts ever made, a perfect combination of inspired fiction with an equally inspired puzzle framework. Unfortunately, he was left to make <em>Full Throttle</em> on his own, and it showed. Ditto <em>Grim Fandango</em>. For all that he loved movies, the auteur model was not a great fit for Tim Schafer the game designer.</p>
<p><em>Grim Fandango</em> seldom gives you a clear idea of what it is you’re even trying to accomplish. Compare this with <em>The Curse of Monkey Island</em>, the LucasArts adventure just before this one, a game which seemed at the time to herald a renaissance in the studio’s puzzle designs. There, you’re always provided with an explicit set of goals, usually in the form of a literal shopping list. Thus even when the mechanics of the puzzles themselves push the boundaries of real-world logic, you at least have a pretty good sense of where you should be focusing your efforts. Here, you’re mostly left to guess what Tim Schafer would like to have happen to Manny next. You stumble around trying to shake something loose, trying to figure out what you can do and then doing it just because you can. By no means is it lost on me that this sense of confusion arises to a large extent because <em>Grim Fandango</em> is such a character-driven story, one which eschews the mechanistic tic-tac-toe of other adventure-game plots. But recognizing this irony doesn’t make it any less frustrating when you’re wandering around with no clue what the story wants from you.</p>
<p>Compounding the frustrations of the puzzles are the frustrations of the interface. You don’t use the mouse at all; everything is done with the numeric keypad, or, if you’re lucky enough to have one, a console-style controller. (At the time <em>Grim Fandango</em> was released, virtually no one playing games on computers did.) <em>Grim Fandango’</em>s mode of navigation is most reminiscent of the <a href="https://www.filfre.net/2023/11/putting-the-j-in-the-rpg-part-1-dorakue">console-based JRPGs</a> of its era, such as the hugely popular <a href="https://www.filfre.net/2023/12/putting-the-j-in-the-rpg-part-3-playing-final-fantasy-vii-or-old-man-yells-at-jrpg"><em>Final Fantasy VII</em></a>, which sold over 10 million copies on the PlayStation during the late 1990s. Yet in practice it’s far more irritating, because you have to interact with the environment here on a much more granular level. LucasArts themselves referred to their method of steering Manny about as a “tank” interface, a descriptor which turns out to be all too descriptive. It really does feel like you’re driving a bulky, none too agile vehicle through an obstacle course of scenery.</p>
<div id="attachment_6151"><p><a href="https://www.filfre.net/2024/11/grim-fandango/angle/" rel="attachment wp-att-6151"><img decoding="async" aria-describedby="caption-attachment-6151" src="https://www.filfre.net/wp-content/uploads/2024/10/angle-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/angle-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/angle.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6151">Make no mistake: the 3D engine makes possible some truly striking views. But too often the designers prioritize visual aesthetics over playability.</p></div>
<p>In the final reckoning, then, an approach that is fine in a JRPG makes just about every aspect of an old-school, puzzle-solving adventure game — which is what <em>Grim Fandango</em> remains in form and spirit when you strip all of the details of its implementation away — more awkward and less fun. Instead of having hotspots in the environment that light up when you pass a mouse cursor over them, as you do in a SCUMM adventure, you have to watch Manny’s head carefully as you drive him around; when it turns to look in a certain direction, that means there’s something he can interact with there. Needless to say, it’s all too easy to miss a turn of his head, and thereby to miss something vital to your progress through the game.</p>
<div id="attachment_6152"><p><a href="https://www.filfre.net/2024/11/grim-fandango/inventory/" rel="attachment wp-att-6152"><img decoding="async" aria-describedby="caption-attachment-6152" src="https://www.filfre.net/wp-content/uploads/2024/10/inventory-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/inventory-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/inventory.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6152">The inventory system is also fairly excruciating. Instead of being able to bring up a screen showing all of the items Manny is carrying, you have to cycle through them one by one by punching a key or controller button over and over, listening to him drone out their descriptions over and over as you do so. This approach precludes using one inventory object on another one, cutting off a whole avenue of puzzle design.</p></div>
<p>Now, the apologists among you — and this game does have an inordinate number of them — might respond to these complaints of mine by making reference to the old cliché that, for every door that is closed in life (and presumably in games as well), another one is opened. And in theory, the new engine really does open a door to new types of puzzles that are more tactile and embodied, that make you feel more a part of the game’s world. To Tim Schafer’s credit, he does try to include these sorts of puzzles in quite a few places. To our detriment, though, they turn out to be the worst puzzles in the game, relying on finicky positioning and timing and giving no useful feedback when you get those things slightly wrong.</p>
<p>But even when <em>Grim Fandango</em> presents puzzles that could easily have been implemented in SCUMM, they’re made way more annoying than they ought to be by the engine and interface. When you’re reduced to that final adventurer’s gambit of just trying everything on everything, as you most assuredly will be from time to time here, the exercise takes many times longer than it would using SCUMM, what with having to laboriously drive Manny about from place to place.</p>
<p>Taken as a game rather than the movie it often seems more interested in being, <em>Grim Fandango</em> boils down to a lumpy stew of overthought and thoughtlessness. In the former category, there’s an unpleasant ideological quality to its approach, with its prioritization of some hazy ethic of 3D-powered “immersion” and its insistence that no visible interface elements whatsoever can appear onscreen, even when these choices actively damage the player’s experience. This is where Sid Meier can <a href="https://www.filfre.net/2017/03/whats-the-matter-with-covert-action">helpfully step in to remind us</a> that it is the player who is meant to be having the fun in a game, not the designer.</p>
<p>The thoughtlessness comes in the lack of consideration of what <em>kind</em> of game <em>Grim Fandango</em> is meant to be. Like all big-tent gaming genres, the adventure genre subsumes a lot of different styles of game with different priorities. Some adventures are primarily about exploration and puzzle solving. And that’s fine, although one does hope that those games execute their puzzles better than this one does. But <em>Grim Fandango</em> is not primarily about its puzzles; it wants to take you on a ride, to sweep you along on the wings of a compelling story. And boy, does it have a compelling story to share with you. For this reason, it would be best served by streamlined puzzles that don’t get too much in the way of your progress. The ones we have, however, are not only frustrating in themselves but murder on the story’s pacing, undermining what ought to be <em>Grim Fandango’</em>s greatest strengths. A game like this one that is best enjoyed with a walkthrough open on the desk beside it is, in this critic’s view at least, a broken game by definition.</p>
<p>As with so many near-miss games, the really frustrating thing about <em>Grim Fandango</em> is that the worst of its problems could so easily have been fixed with just a bit more testing, a bit more time, and a few more people who were empowered to push back against Tim Schafer’s more dogmatic tendencies. For the 2015 remastered version of the game, Schafer did grudgingly agree to include an alternative point-and-click interface that is more like that of a SCUMM adventure. The results verge on the transformational. By no means does the addition of a mouse cursor remedy all of the infelicities of the puzzle design, but it does make battering your way through them considerably less painful. If my less-than-systematic investigations on YouTube are anything to go by, this so-old-it’s-new-again interface has become by far the most common way to play the game today.</p>
<div id="attachment_6153"><p><a href="https://www.filfre.net/2024/11/grim-fandango/remaster/" rel="attachment wp-att-6153"><img decoding="async" aria-describedby="caption-attachment-6153" src="https://www.filfre.net/wp-content/uploads/2024/10/remaster-300x225.jpg" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/remaster-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2024/10/remaster.jpg 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6153">The <em>Grim Fandango</em> remaster. Note the mouse cursor. The new interface is reportedly implemented entirely in in-engine Lua scripts rather than requiring any re-programming of the GrimE engine itself. This means that it would have been perfectly possible to include as an option in the original release.</p></div>
<p>In other places, the fixes could have been even simpler than revamping the interface. A shocking number of puzzles could have been converted from infuriating to delightful by nothing more than an extra line or two of dialog from Manny or one of the other characters. As it is, too many of the verbal nudges that do exist are too obscure by half and are given only once in passing, as part of conversations that can never be repeated. Hints for Part Four are to be found only in Part One; I defy even an elephant to remember them when the time comes to apply them. All told,<em> Grim Fandango</em> has the distinct odor of a game that no one other than those who were too close to it to see it clearly ever really tried to play before it was put in a box and shoved out the door. There was a time when seeking the feedback of outsiders was a standard part of LucasArts’s adventure-development loop. Alas, that era was long passed by the time of <em>Grim Fandango</em>.</p>
<p>Nonetheless, <em>Grim Fandango</em> was accorded a fairly rapturous reception in the gaming press when it was released in the last week of October in 1998, just in time for Halloween and the Mexican Day of the Dead which follows it on November 1. Its story, characters, and setting were justifiably praised, while the deficiencies of its interface and puzzle design were more often than not relegated to a paragraph or two near the end of the review. This is surprising, but not inexplicable. There was a certain sadness in the trade press — almost a collective guilt — about the diminished prospects of the adventure game in these latter years of the decade. Meanwhile LucasArts was still the beneficiary of a tremendous amount of goodwill, thanks to the many classics they had served up during those earlier, better years for the genre as a whole. <em>Grim Fandango</em> was held up as a sort of standard bearer for the embattled graphic adventure, the ideal mix of tradition and innovation to serve as proof that the genre was still relevant in a <a href="https://www.filfre.net/2023/04/the-next-generation-in-graphics-part-1-three-dimensions-in-software-or-quake-and-its-discontents">post-<em>Quake</em></a>, <a href="https://www.filfre.net/2024/07/starcraft-a-history-in-two-acts">post-<em>Starcraft</em></a> world.</p>
<p>For many years, the standard narrative had it that the unwashed masses of gamers utterly failed to respond to the magazines’ evangelism, that <em>Grim Fandango</em> became an abject failure in the marketplace. In more recent years, Tim Schafer has muddied those waters somewhat by claiming that the game actually sold close to half a million copies. I rather suspect that the truth is somewhere between these two extremes. Sales of a quarter of a million certainly don’t strike me as unreasonable once foreign markets are factored into the equation. Such a figure would have been enough to keep <em>Grim Fandango</em> from losing much if any money, but would have provided LucasArts with little motivation to make any more such boldly original adventure games. And indeed, LucasArts would release only one more adventure game of any stripe in their history. It would use the GrimE engine, but it would otherwise play it about as safe as it possibly could, by being yet another sequel to the venerable but beloved <em>Secret of Monkey Island</em>.</p>
<p>As I was at pains to note earlier, I do see what causes some people to rate <em>Grim Fandango</em> so highly, and I definitely don’t think any less of them for doing so. For my part, though, I’m something of a stickler on some points. To my mind, interactivity is the very quality that separates games from other forms of media, making it hard for me to pronounce a game “good” that botches it. I’ve learned to be deeply suspicious of games whose most committed fans want to talk about everything other than that which you the player actually <em>do</em> in them. The same applies when a game’s creators display the same tendency. Listening to the developers’ commentary tracks in the remastered edition of <em>Grim Fandango </em>(who would have imagined in 1998 that games would someday come with commentary tracks?), I was shocked by how little talk there was about the gameplay. It was all lighting and dialog beats and soundtrack stabs and Z-buffers instead — all of which is really, really important in its place, but none of which can yield a great game on its own. Tellingly, when the subject of puzzle design did come up, it always seemed to be in an off-hand, borderline dismissive way. “I don’t know how players are supposed to figure out this puzzle,” says Tim Schafer outright at one point. Such a statement from your lead designer is never a good sign.</p>
<p>But I won’t belabor the issue any further. Suffice to say that <em>Grim Fandango</em> is doomed to remain a promising might-have-been rather than a classic in my book. As a story and a world, it’s kind of amazing. It’s just a shame that the gameplay part of this game isn’t equally inspired.</p>
<hr>
<p><code> </code><br>
<strong>Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like.</strong></p>
<p><a href="https://www.patreon.com/DigitalAntiquarian" rel="attachment wp-att-5598"><img decoding="async" src="https://www.filfre.net/wp-content/uploads/2023/04/Patreon-300x133-1.png" alt="" width="300" height="133"></a></p>
<hr>

<p><strong>Sources:</strong> The book <em>Grim Fandango: Prima’s Official Strategy Guide</em> by Jo Ashburn. <em>Retro Gamer</em> 31 and 92; <em>Computer Gaming World</em> of November 1997, May 1998, and February 1999; <em>Ultimate PC</em> of August 1998. Plus the commentary track from the 2015 <em>Grim Fandango</em> remaster.</p>
<p>Online sources include&nbsp;<em>The International House of Mojo’</em>s <a href="https://mixnmojo.com/features/sitefeatures/LucasArts-Secret-History-13-Grim-Fandango/1">pages on the game</a>, the self-explanatory <a href="https://grimfandango.network/"><em>Grim Fandango Network</em></a>, <em>Gamespot’</em>s <a href="https://web.archive.org/web/20100201172300/http://www.gamespot.com/pc/adventure/grimfandango/review.html">vintage review of the game</a>, and Daniel Albu’s <a href="https://www.youtube.com/watch?v=_qwAzIYaGUI">YouTube conversation with Bret Mogilefsky</a>.</p>
<p>And a special thank-you to reader Matt Campbell, who shared with me the audio of a talk that Bret Mogilefsky gave at the 2005 Lua Workshop, during which he explained how he used that language in GrimE.</p>
<p><strong>Where to Get It:</strong> A modestly remastered version of <em>Grim Fandango</em> is <a href="https://www.gog.com/en/game/grim_fandango_remastered">available for digital purchase</a> at GOG.com.</p>
							
							
														
													</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You too can write a book (176 pts)]]></title>
            <link>https://parentheticallyspeaking.org/articles/write-a-book/</link>
            <guid>42096915</guid>
            <pubDate>Sat, 09 Nov 2024 21:10:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://parentheticallyspeaking.org/articles/write-a-book/">https://parentheticallyspeaking.org/articles/write-a-book/</a>, See on <a href="https://news.ycombinator.com/item?id=42096915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Thanks to <a href="https://www.neeldhara.com/">Neeldhara Misra</a> for pushing me to write
this post, based on a thread on Twitter.</span></p><p>This article is primarily directed at academics. Its purpose is to
tell you two things:
</p><div><ul><li><p>You <span>can</span> write a book.</p></li><li><p>You probably <span>should</span> write a book.</p></li></ul></div><h3>1<tt>&nbsp;</tt><a name="(part._.You_.Can_.Write_a_.Book)"></a>You Can Write a Book<span><a href="#(part._.You_.Can_.Write_a_.Book)" title="Link to here">🔗</a><span> </span></span></h3><p>Let’s say you’re not using someone else’s textbook, or using it only
loosely. That means you’re going to spend a lot of time organizing
your thoughts. You will probably produce some kind of “lecture
notes”. The delta from there to a book is much smaller than you
imagine.</p><p>Here’s a pro-tip. Back in about 2003/4, I noticed that the quantity of
reading that students would do before class was at most about six
pages; once it got to about eight pages, they wouldn’t read at
all. (These numbers may be much lower now.) But this automatically
bounds how much you have to <span>write</span>!</p><p>In short: let’s say you’re writing up lecture notes. You’re writing
about four to at most six pages per class. Let’s say you have about 30
classes (often many more). You have automatically written about 200
pages without even especially thinking about it. Two hundred pages of
writing is often called a … book. It represents your “take”. So your
take now has a book!</p><h3>2<tt>&nbsp;</tt><a name="(part._.You_.Should_.Write_a_.Book)"></a>You Should Write a Book<span><a href="#(part._.You_.Should_.Write_a_.Book)" title="Link to here">🔗</a><span> </span></span></h3><p>What are the <span>incentives</span> to do this? There are many, but they
may not accrue immediately: they may take time. Think of it as a
long-term investment in yourself.</p><ul><li><p>First, simply: you believe strongly in your view of the world,
and you’re pursuing it with intensity. Right now nobody else is really
able to download your brain. Your book becomes how others can download
it.</p></li><li><p>People at other places might use your book, or at least put it on
reading lists. Even if only one student there reads and internalizes
that supplemental material, that student now carries your ideas with
them. Much more concretely, they could be a PhD applicant.</p><p>I have gotten so many great PhD applicants over the years thanks to my
books! In particular, when they come from a less-well-known university,
this guarantees for me that they have the preparation I need, and that
we have a shared mindset.</p></li><li><p>What’s out there may not be very good.  This is especially an
issue in programming languages, where some of the widely-used texts
are basically hot garbage: basically a broken, 1970s view of the
world. I once wrote a
<a href="https://cs.brown.edu/~sk/Publications/Papers/Published/sk-teach-pl-post-linnaean/">position piece</a> on it
entitled “Teaching Programming Languages in a Post-Linnaean Age”! So
worldview really matters.</p><p>So that’s another incentive. To drive out the bad with good. Do you
use the standard text that everyone else uses in your field? If you’ve
read this far, probably not. Why not? You know (or think you know!)
what should be taught in your field. Who uses a text? Someone less
certain. So it’s not their fault. We need to help them along.</p></li><li><p>Sometimes we have resources—<wbr>like software—<wbr>that many people
don’t know about, but that are especially well suited to education in
our field. For instance, the <span>#lang</span> feature of <a href="https://racket-lang.org/">Racket</a>
is one of the most powerful tools for teaching programming
languages. But most people don’t (yet) know that.</p></li></ul><p>I speak from experience. I have written several books, some solo and
some with co-authors. I did the very thing you are told to not do as a
tenure-track assistant professor: I wrote a quality
undergraduate-level book. I survived (it didn’t hurt my tenure in the
slightest), and I then benefited from it for a long time.</p><h3>3<tt>&nbsp;</tt><a name="(part._.Mechanics)"></a>Mechanics<span><a href="#(part._.Mechanics)" title="Link to here">🔗</a><span> </span></span></h3><p>The one <span>big</span> thing I haven’t said, which drives a lot of this,
is the publishing medium. And oh boy, do I have opinions on this!
I wrote up some of them when I published the first formal edition of
my programming languages book, in an essay entitled
<a href="https://cs.brown.edu/~sk/Memos/Books-as-Software/">Books as Software</a>. Let me summarize/expand.</p><ul><li><p>Don’t go to commercial publishers. They are either just not
ready for the modern world or will basically paywall your
work. Paywalling is the total antithesis of wanting to have impact and
influence and wanting to drive bad books out of the market.</p></li><li><p>Publish it free online. Especially those of us who are
immigrants from poorer countries know what it’s like to not be able to
afford high-quality material. The next you is sitting right now in
Bangalore stuck with a crappy course and crappy book. Be their light.</p></li><li><p>Some people like paper. Upload your file to a print-on-demand
service. Even with a markup, it’ll be a lot cheaper than a book from a
commercial publisher.</p></li><li><p>In a STEM subject, your tenure case is not going to hinge on a
contract from MIT Press. Getting that contract is actually relatively
easy. Impact is hard. Go for the hard part, as you do in your
research. Optimize for it.</p></li><li><p>Put out a new release once a year. Don’t fall for the temptation
of continuous releases. People using your book need to be able to
depend on a fixed version for the semester. They will have references
to pages, sections, etc. Don’t break their build.</p></li><li><p>Publish permalinks. Not everyone can upgrade their course every
year. Yes, it means your old mistakes are on permanent display, and
some people won’t use your latest and greatest. Live with it. Your ego
is not that fragile. You’ll get over it.</p></li><li><p>Make it easy for people to send you corrections. They will (just
as they did for your software). Sometimes you will even get very
insightful and creative ideas. Of course you’ll also get various
dreck. Just as with your software. Because books are software, as my
essay says.</p></li><li><p>Try to provide materials in both PDF and HTML. The reasons
should be obvious. It’s not always easy. I personally prefer to use
<a href="https://docs.racket-lang.org/scribble/index.html">Scribble</a> for this purpose. But I have also used LaTeX and
even Google Docs. The latter two are each terrible in their own way,
but the best tool is the one you use and that lets you get something
done reasonably quickly. There’s always time to revise. Don’t suffer
the paralysis of tool indecision and let that become the reason you
don’t write!</p></li><li><p>You won’t make much money this way. It’s okay, you’re probably
already paid pretty well. And the money you’re not getting is money
the author of the crappy textbook is also not getting! And you’ll get
paid in mindshare, which is infinitely more valuable.</p></li><li><p>You can also do what I did: I published the free PDF on my Web
site. On the print-on-demand site, offer a modestly-priced PDF. I let
readers know where they can get the free version. Therefore, the only
reason to buy the PDF is the equivalent of a tip-har. Most don’t, but
a few do. (You can also use one of the tip-jar services, though they
didn’t provide enough value for me.)</p></li></ul><p>Sure, my revenues have been modest. I view the checks as a little
surprise bonus. Added over time it could probably have bought me a new
bike frame (my unit of measure!), but mostly it’s mostly just
a nice dinner and ice cream. But I’m not doing it for the money. I’m
doing it to spread a worldview and to liberate a field from terrible
books. Both are much more worthwhile to me.</p><p>In short: if you’re even slightly tempted to write that textbook—<wbr>go
for it. I got you, fam.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Visprex – Open-source, in-browser data visualisation tool for CSV files (167 pts)]]></title>
            <link>https://docs.visprex.com/</link>
            <guid>42096837</guid>
            <pubDate>Sat, 09 Nov 2024 20:54:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.visprex.com/">https://docs.visprex.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42096837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  

  
  


<h2 id="_1"><img alt="Visprex" src="https://docs.visprex.com/assets/images/logo.webp#center"></h2>
<h2 id="what-is-visprex">What is Visprex?</h2>
<p>Visprex is a lightweight data visualisation tool that helps you speed up your statistical modelling and analytics workflows. The main high-level features include:</p>
<h3 id="quick">⏱️ Quick</h3>
<ul>
<li>You can visualise your data <strong>in seconds</strong> to quickly build an intuition on your dataset</li>
<li>No need for referring to specific sytax in your statistical analysis software</li>
</ul>
<h3 id="secure">🔒️ Secure</h3>
<ul>
<li>Your data is processed <strong>entirely on your browser</strong>, which means your data won't be sent anywhere</li>
<li>No tracking or analytics software is used for privacy</li>
</ul>
<h3 id="open-source">📖 Open Source</h3>
<ul>
<li>Source code is fully <strong>open source</strong> on GitHub: <a href="https://github.com/visprex/visprex">github.com/visprex/visprex</a></li>
<li>Visprex is also freely available on <a href="https://www.visprex.com/">visprex.com</a></li>
</ul>
<h2 id="who-is-visprex-for">Who is Visprex for?</h2>
<h3 id="students">Students</h3>
<p>Visprex is suitable for students who are starting out in their statistical modelling training.</p>
<p>There's no need for starting your computing environment on your machine or writing tedious visualisation scripts.</p>
<h3 id="data-scientists">Data Scientists</h3>
<p>Visprex is also for data analysts who would like to quickly inspect tabular data for analytical purposes, without worrying about privacy or PII as no data leaves your browser.</p>
<h2 id="quickstart">Quickstart</h2>
<p>You can start visualising your data with just a few clicks by first <a href="https://docs.visprex.com/features/datasets/">loading your dataset</a>.</p>












                
              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NYC Subway Station Layouts (265 pts)]]></title>
            <link>http://www.projectsubwaynyc.com/gallery</link>
            <guid>42096717</guid>
            <pubDate>Sat, 09 Nov 2024 20:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.projectsubwaynyc.com/gallery">http://www.projectsubwaynyc.com/gallery</a>, See on <a href="https://news.ycombinator.com/item?id=42096717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="44" id="block-yui_3_17_2_1_1540856882851_7416"><p>  Inspired and want to support this project? Consider tipping me on the  <a data-preserve-html-node="true" href="http://www.projectsubwaynyc.com/contact/">Contact/Tip</a> page!
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Jaws – a JavaScript to WASM ahead-of-time compiler (248 pts)]]></title>
            <link>https://github.com/drogus/jaws</link>
            <guid>42095879</guid>
            <pubDate>Sat, 09 Nov 2024 18:14:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/drogus/jaws">https://github.com/drogus/jaws</a>, See on <a href="https://news.ycombinator.com/item?id=42095879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Jaws</h2><a id="user-content-jaws" aria-label="Permalink: Jaws" href="#jaws"></a></p>
<p dir="auto">Jaws is a JavaScript to WebAssembly compiler written in Rust. It is similar to <a href="https://github.com/CanadaHonk/porffor">porffor</a> in a way it also results in a standalone WASM binary that can be executed without an interpreter, but it takes a different implementation approach.</p>
<p dir="auto">It's an experimental tool and it's not ready for production. A lot of the language
features and builtin types are missing or incomplete. That said, my goal is to eventually support 100% of the language.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Jaws?</h3><a id="user-content-why-jaws" aria-label="Permalink: Why Jaws?" href="#why-jaws"></a></p>
<p dir="auto">I started this project while working on a stress testing tool called <a href="https://github.com/drogus/crows">Crows</a> that runs WebAssembly scenarios. At the moment it only supports code compiled from Rust to WASM. As much as I love writing Rust, I also know it's not a widely popular language and besides, small tests are often easier to write in interpreted languages. The problem is, running scripting languages on top of WASM is not ideal at the moment. You have to either include an interpreter, which automatically makes the binary at least a few MBs in size and the memory usage even bigger, or use a variation of the language you're targetting (like TinyGo instead of Go, or AssemblyScript instead of TypeScript/JavaScript).</p>
<p dir="auto">I believe that with modern WASM proposals it is possible to implement 100% of JavaScript features without the need to use a compiled interpreter, as WASM runtimes are already interpreters.</p>
<p dir="auto">If you want to see it happen, please consider <a href="https://github.com/sponsors/drogus">sponsoring my work</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What works</h3><a id="user-content-what-works" aria-label="Permalink: What works" href="#what-works"></a></p>
<p dir="auto">As I eventually want to implment 100% of the language, I'm purposefully focused on implementing the semantics first, rather than go for 100% of builtins and grammar as I want to be 100% sure it's doable.</p>
<p dir="auto">I have a list of 4 things that I think are hard to implement and after I implement all of them I will focus on more grammar and builtins. These are:</p>
<ol dir="auto">
<li>Scopes/closures</li>
<li>try/catch</li>
<li>async/await</li>
<li>generators</li>
</ol>
<p dir="auto">The last two are kind of similar as by getting generators working, one essentially has tools to make async await work, but I still wanted to make the distinction. At the moment Jaws can compile code using closures with (mostly) proper scopes support, it allows try/catch and it implements (limited) <code>Promise</code> API and <code>async</code> (but not <code>await</code> yet). For example the following script will print <code>error: foo</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let value = &quot;foo&quot;;
async function foo() {
  throw value;
}

foo().then(
  function () {},
  function (v) {
    console.log(&quot;error&quot;, v);
  },
);"><pre><span>let</span> <span>value</span> <span>=</span> <span>"foo"</span><span>;</span>
<span>async</span> <span>function</span> <span>foo</span><span>(</span><span>)</span> <span>{</span>
  <span>throw</span> <span>value</span><span>;</span>
<span>}</span>

<span>foo</span><span>(</span><span>)</span><span>.</span><span>then</span><span>(</span>
  <span>function</span> <span>(</span><span>)</span> <span>{</span><span>}</span><span>,</span>
  <span>function</span> <span>(</span><span>v</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>"error"</span><span>,</span> <span>v</span><span>)</span><span>;</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span></pre></div>
<p dir="auto">A non exhaustive list of other stuff that should work:</p>
<ul dir="auto">
<li>declaring and assigning: <code>var</code>, <code>let</code>, <code>const</code></li>
<li><code>while</code></li>
<li>string lierals, adding string literals</li>
<li>numbers and basic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>)</li>
<li>booleans and basic boolean operators</li>
<li>array literals</li>
<li>object literals</li>
<li><code>new</code> keyword</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Host requirements</h3><a id="user-content-host-requirements" aria-label="Permalink: Host requirements" href="#host-requirements"></a></p>
<p dir="auto">As Jaws is built with a few relatively recent WASM proposals, the generated binaries are not really portable between runtimes yet. I'm aiming to implement it with WASIp2 in mind, but the only runtime capable of running components and WASIp2, ie. Wasmtime, does not support some other things I use, like parts of the WASM GC proposal or exception handling.</p>
<p dir="auto">In order to make it easier to develop before the runtimes catch up with standardized proposals, I decided to use V8 (through Chromium or Node) with a Javascript polyfill for WASIp2 features that I need. There is a script <code>run.js</code> in the repo that allows to run binaries generated by Jaws. Eventually it should be possible to run them on any runtime implementing WASM GC, exception handling and WASIp2 API.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to use it?</h3><a id="user-content-how-to-use-it" aria-label="Permalink: How to use it?" href="#how-to-use-it"></a></p>
<p dir="auto">Unless you want to contribute you probably shouldn't, but after cloning the repo
you can use an <code>execute.sh</code> script like:</p>
<div data-snippet-clipboard-copy-content="./execute.sh --cargo-run path/to/script.js"><pre><code>./execute.sh --cargo-run path/to/script.js
</code></pre></div>
<p dir="auto">It will generate a WAT file, compile it to a binary and then run using Node.js.</p>
<p dir="auto">It requires Rust's <code>cargo</code>, relatively new version of <code>wasm-tools</code> and Node.js v23.0.0 or newer. Passing <code>--cargo-run</code> will make the script use <code>cargo run</code> command to first compile and then run the project, otherwise it will try to run the release build (so you have to run <code>cargo build --release</code> prior to running <code>./execute.sh</code> without <code>--cargo-run</code> option)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What's next?</h3><a id="user-content-whats-next" aria-label="Permalink: What's next?" href="#whats-next"></a></p>
<p dir="auto">My plan is to finish implementing all of the "hard to implement" features first, so next in line are generators and <code>await</code> keyword support. Ideally I would use the <a href="https://github.com/WebAssembly/stack-switching">stack-switching</a> proposal for both await and generators, but alas it's only in Phase 2 and it has minimal runtime support (I could find some mentions in Chromium development groups, but I couldn't get it to work). In the absence of stack-switching I'm working on using CPS transforms in order to simulate continuations.</p>
<p dir="auto">After that's done, I will be slowly implementing all of the missing pieces, starting with grammar (for loops, switch etc) and then builtin types and APIs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How does it work?</h3><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">The project is essentially translating JavaScript syntax into WASM instructions, leveraging instructions added by WASM GC, exception handling and tail call optimizations proposals. On top of the Rust code that is translating JavaScript code, there is about 3k lines of WAT code with all the plumbing needed to translate JavaScript semantics into WASM.</p>
<p dir="auto">To give an example let's consider scopes and closures. WASM has support for passing function references and for structs and arrays, but it doesn't have the scopes semantics that JavaScript has. Thus, we need to simulate how scopes work, by adding some extra WASM code. Imagine the following JavaScript code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let a = &quot;foo&quot;;

function bar() {
  console.log(a);
}

bar();"><pre><span>let</span> <span>a</span> <span>=</span> <span>"foo"</span><span>;</span>

<span>function</span> <span>bar</span><span>(</span><span>)</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>a</span><span>)</span><span>;</span>
<span>}</span>

<span>bar</span><span>(</span><span>)</span><span>;</span></pre></div>
<p dir="auto">In JavaScript, because a function definition inherits the scope in which it's defined, the <code>bar()</code> function has access to the <code>a</code> variable. Thus, this script should print out the string <code>"foo"</code>. We could translate it to roughly the following pseudo code:</p>
<div data-snippet-clipboard-copy-content="// first we create a global scope, that has no parents
let scope = newScope(null);

// then we set the variable `a` on the scope
declareVariable(scope, &quot;a&quot;, &quot;foo&quot;);

// now we define the  bar function saving a reference to the function
let func = function(parentScope: Scope, arguments: JSArguments, this: Any) -> Any {
  // inside a function declaration we start a new scope, but keeping
  // a reference to the parentScope
  let scope = newScope(parentScope);

  // now we translate console.log call retreiving the variable from the scope
  // this will search for the `a` variable on the current scope and all of the
  // parent scopes
  console.log(retrieve(scope, &quot;a&quot;));
}
// when running a function we have to consider the scope
// in which it was defined
let fObject = createFunctionObject(func, scope);
// and now we also set `bar` on the current scope
declareVariable(scope, &quot;bar&quot;, fObject)

// now we need to fetch the `bar` function from the scop
// and run it
let f = retrieve(scope, &quot;bar&quot;);
call(f);"><pre><code>// first we create a global scope, that has no parents
let scope = newScope(null);

// then we set the variable `a` on the scope
declareVariable(scope, "a", "foo");

// now we define the  bar function saving a reference to the function
let func = function(parentScope: Scope, arguments: JSArguments, this: Any) -&gt; Any {
  // inside a function declaration we start a new scope, but keeping
  // a reference to the parentScope
  let scope = newScope(parentScope);

  // now we translate console.log call retreiving the variable from the scope
  // this will search for the `a` variable on the current scope and all of the
  // parent scopes
  console.log(retrieve(scope, "a"));
}
// when running a function we have to consider the scope
// in which it was defined
let fObject = createFunctionObject(func, scope);
// and now we also set `bar` on the current scope
declareVariable(scope, "bar", fObject)

// now we need to fetch the `bar` function from the scop
// and run it
let f = retrieve(scope, "bar");
call(f);
</code></pre></div>
<p dir="auto">All of the helpers needed to make it work are hand written in WAT format. I have some ideas on how to make it more efficient, but before I can validate all the major features I didn't want to invest too much time into side quests. Writing WAT by hand is not that hard, too, especially when you consider WASM GC.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">License</h3><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The code is licensed under Apache 2.0 license</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Early Cascade Injection: From Windows process creation to stealthy injection (113 pts)]]></title>
            <link>https://www.outflank.nl/blog/2024/10/15/introducing-early-cascade-injection-from-windows-process-creation-to-stealthy-injection/</link>
            <guid>42095752</guid>
            <pubDate>Sat, 09 Nov 2024 17:55:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.outflank.nl/blog/2024/10/15/introducing-early-cascade-injection-from-windows-process-creation-to-stealthy-injection/">https://www.outflank.nl/blog/2024/10/15/introducing-early-cascade-injection-from-windows-process-creation-to-stealthy-injection/</a>, See on <a href="https://news.ycombinator.com/item?id=42095752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="mainEntityOfPage">

<p><em>By <a href="https://www.linkedin.com/in/guido-miggelenbrink-63aa0a166/">Guido Miggelenbrink</a> at Outflank</em></p>



<h2>Introduction</h2>



<p>In this blog post we introduce a novel process injection technique named Early Cascade Injection, explore Windows process creation, and identify how several Endpoint Detection and Response systems (EDRs) initialize their in-process detection capabilities. This new Early Cascade Injection technique targets the user-mode part of process creation and combines elements of the well-known Early Bird APC Injection technique with the recently published EDR-Preloading technique <a href="https://www.malwaretech.com/2024/02/bypassing-edrs-with-edr-preload.html">by Marcus Hutchins</a> <a href="https://www.malwaretech.com/2024/02/bypassing-edrs-with-edr-preload.html">[1]</a>. Unlike Early Bird APC Injection, this new technique avoids queuing cross-process Asynchronous Procedure Calls (APCs), while having minimal remote process interaction. This makes Early Cascade Injection a stealthy process injection technique that is effective against top tier EDRs while avoiding detection.</p>



<p>To provide insights into Early Cascade Injection’s internals, this blog also presents a timeline of the user-mode process creation flow. This overview illustrates how Early Cascade Injection operates and pinpoints the exact moment at which it intervenes in process creation. Furthermore, we compare that to the initialization timing of EDR user-mode detection measures.</p>



<p>Now, let’s dive into the details of Windows process creation, Early Bird APC Injection, and EDR-Preloading. Once we have a solid understanding of these topics, we can proceed to explore Early Cascade Injection.</p>



<h2>Understanding Windows Process Creation</h2>



<h3>Process creation APIs</h3>



<p>In Windows there are various APIs to create a process, such as <code>CreateProcess</code>, <code>CreateProcessAsUser</code>, and <code>CreateProcessWithLogon</code>, as shown in figure 1. Ultimately, all these functions invoke the NAPI <code>NtCreateUserProcess</code> in <code>ntdll.dll</code>. This function is responsible for initiating process creation by switching control to the kernel, where the equally named function <code>NtCreateUserProcess</code> is executed.</p>



<p>Each of these functions include the <code>dwCreationFlags</code> parameter, which controls how the process is created. In this post, we’ll encounter the <code>CREATE_SUSPENDED</code> flag, which instructs the kernel to create the new process’s initial thread in a suspended state <a href="https://learn.microsoft.com/en-us/windows/win32/procthread/process-creation-flags">[2]</a>. The thread remains suspended until the <code>ResumeThread</code> function is called.</p>



<p>Obviously, these functions also have a parameter specifying the path to the application’s image file for which a process is to be created. Refer to the MSDN for the other parameters and flags of these APIs <a href="https://learn.microsoft.com/en-us/windows/win32/procthread/process-creation-flags">[2]</a>.</p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs.png"><img fetchpriority="high" decoding="async" width="1024" height="430" src="https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs-1024x430.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs-1024x430.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs-300x126.png 300w, https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs.png 1435w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 1: Process creation functions (Source: Windows Internals, Part 1)</em></p>



<h3>Kernel-mode and user-mode process creation</h3>



<p>Process creation has two parts: kernel-mode and user-mode. It begins with the kernel-mode part, initiated by <code>NtCreateUserProcess</code>. Once the process’s context and environment are created in kernel-mode, the initial thread of the process completes process creation in user-mode.</p>



<p>The kernel-mode part is responsible for opening the image file of the specified application and mapping it into memory. It then creates process-specific and thread-specific objects, maps the native library <code>ntdll.dll</code> into the process, followed by the creation of the process’s initial thread. If the <code>CREATE_SUSPENDED</code> flag is specified, this thread is created in suspended state, waiting to be resumed before control returns to user-mode for the remainder of the process creation.</p>



<p>The module <code>ntdll.dll</code> is the first DLL loaded into a process and it is the only DLL loaded in kernel-mode, all other modules are loaded in user-mode. Further, <code>ntdll.dll</code> includes the exported function <code>LdrInitializeThunk</code>, which handles the user-mode part of process creation before the application’s main entry point runs. This function is also known as the image loader and the functions related to it in <code>ntdll.dll</code> are prefixed with <code>Ldr</code>.</p>



<p>Returning to the newly created thread: upon resumption of this suspended thread, it starts executing <code>LdrInitializeThunk</code>, the user-mode part of process creation. After, the new process is fully initialized and ready to run the application. The initial thread then begins executing the application’s main entry point.</p>



<blockquote>
<p>Note that using the <code>CREATE_SUSPENDED</code> flag pauses process creation just before the initial thread switches to user-mode to run <code>LdrInitializeThunk</code>. This is particularly interesting because <strong>user-mode malware can interfere with process creation at this point</strong>. Therefore, let’s take a closer look at what happens inside <code>LdrInitializeThunk</code>.</p>
</blockquote>



<h3>User-mode process creation: <code>LdrInitializeThunk</code></h3>



<p><code>LdrInitializeThunk</code> is the first function executed in user-mode, marking the initial point where malware and EDRs can intervene in a process. We will later explore how techniques such as Early Bird APC Injection, EDR-Preloading and Early Cascade Injection interact with <code>LdrInitializeThunk</code>. For now, let’s delve into the details of this function.</p>



<p><code>LdrInitializeThunk</code> is a complex function responsible for the user-mode part of process creation. It handles numerous process initialisation tasks, which are listed and briefly described in the <em>Windows Internals, Part 1</em> book. However, the book does not cover which subordinate functions within <code>LdrInitializeThunk</code> are responsible for these tasks. Hence, to gain a deeper understanding of <code>LdrInitializeThunk</code> and its subordinate functions, we analysed it using x64dbg and IDA Pro.</p>



<p>Based on this analysis, we created a call graph that outlines the sequence of events in the user-mode part of process creation. This timeline includes the functions relevant to this post and thus omits some tasks and associated functions of <code>LdrInitializeThunk</code>. Additionally, please note that this call graph reflects our interpretation and may not be entirely accurate.</p>



<p>The call graph of <code>LdrInitializeThunk</code> is depicted below, followed by a description of the tasks that can be recognized in it. Key functions are highlighted in color.</p>



<p><strong>Call graph <code>LdrInitializeThunk</code>:</strong></p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/FullCallGraphV3.png"><img decoding="async" width="588" height="1024" src="https://www.outflank.nl/wp-content/uploads/2024/10/FullCallGraphV3-588x1024.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/FullCallGraphV3-588x1024.png 588w, https://www.outflank.nl/wp-content/uploads/2024/10/FullCallGraphV3-172x300.png 172w" sizes="(max-width: 588px) 100vw, 588px"></a></figure>



<p>This call graph illustrates the following tasks performed by <code>LdrInitializeThunk</code>:</p>



<ol>
<li>Initialises Loader Lock in the Process Environment Block (<code>PEB</code>).
<ul>
<li>The <code>PEB</code> is a data structure that stores information about the process’s context and environment. Loader Lock will be discussed later.</li>
</ul>
</li>



<li>Sets up the Mutable Read-Only Heap Section (.mrdata).</li>



<li>Creates and inserts the first loader data table entry (<code>LDR_DATA_TABLE_ENTRY</code>) for <code>ntdll.dll</code> into the module database (<code>PEB_LDR_DATA</code>).
<ul>
<li>The module database, stored in the <code>PEB</code>, contains three lists that keep track of the process’s loaded modules: <code>InLoadOrderModuleList</code>, <code>InMemoryOrderModuleList</code>, and <code>InInitializationOrderModuleList</code>. Each entry in these lists includes details such as the module’s base address, entry point, and path.</li>
</ul>
</li>



<li>Initialises the parallel loader.
<ul>
<li>The parallel loader is responsible for loading the application’s imported DLLs concurrently, using a pool of threads. It sets up a <code>LdrpWorkQueue</code> that contains the application’s first order dependencies to be loaded. Then, the parallel threads load these dependencies, recursive ones are added to the work queue. For more information on the Windows parallel loader, refer to this insightful blog by <a href="https://blogs.blackberry.com/en/2017/10/windows-10-parallel-loading-breakdown">[3]</a>.</li>
</ul>
</li>



<li>Creates and inserts the second loader data table entry (<code>LDR_DATA_TABLE_ENTRY</code>) for the application’s executable into the module database (<code>PEB_LDR_DATA</code>).</li>



<li>Loads the initial modules <code>kernel32.dll</code> and <code>kernelbase.dll</code>.
<ul>
<li>These modules are always loaded into every process. Like all other modules, these are also added to the module database (<code>PEB_LDR_DATA</code>).</li>
</ul>
</li>



<li>If enabled, initializes the Shim Engine and parses the Shim Database. <strong>The Shim Engine is by default off.</strong>
<ul>
<li>The Shim Engine applies compatibility fixes (shims) to applications without modifying their code. It intercepts and modifies API calls to address compatibility issues. For more details, refer to <a href="https://techcommunity.microsoft.com/t5/ask-the-performance-team/demystifying-shims-or-using-the-app-compat-toolkit-to-make-your/ba-p/374947">[11]</a>.</li>
</ul>
</li>



<li>If enabled, it uses the parallel loader to load the application’s remaining dependencies into the process; otherwise, DLLs are loaded sequentially.</li>
</ol>



<p>After mapping and initializing all dependencies, <code>LdrInitializeThunk</code> invokes the following functions:</p>



<ol start="9">
<li><code>NtTestAlert</code>: <strong>Empties the APC queue of the calling thread</strong> by switching to kernel-mode, which then calls <code>KiUserApcDispatcher</code> to execute the queued APCs.</li>



<li><code>NtContinue</code>: Sets the initial thread’s execution context to <code>RtlUserThreadStart</code>, which subsequently invokes the main entry point of the application.</li>
</ol>



<p>At the very bottom of the call graph, we see <code>RtlRaiseStatus</code>. This function is normally never executed, as <code>NtContinue</code> redirects the execution flow to the application’s main entry point. However, if the application crashes, <code>RtlRaiseStatus</code> is triggered.</p>



<p>Although the call graph is simplified, it remains complex. Hopefully, it provides some insight into the internals of process creation. To clarify the key points relevant to this blog, we created an abstraction that captures the essential information you need to remember. The red comments describe the operation that the preceding block of functions accomplish.</p>



<p><strong>Abstracted call graph <code>LdrInitializeThunk</code>:</strong></p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/AbstractedCallGraphV4.png"><img decoding="async" width="1024" height="646" src="https://www.outflank.nl/wp-content/uploads/2024/10/AbstractedCallGraphV4-1024x646.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/AbstractedCallGraphV4-1024x646.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/AbstractedCallGraphV4-300x189.png 300w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<blockquote>
<p>In this abstraction of the call graph, we observe that <code>LdrInitializeThunk</code> loads <code>kernel32.dll</code> and <code>kernelbase.dll</code>, then it loads all other dependencies, clears its APC queue (<code>NtTestAlert</code>), and finally begins executing the application’s main entry point (<code>NtContinue</code>). Additionally, we observe that loading a DLL via <code>LdrLoadDll</code> consists of two steps: mapping and initialization.</p>
</blockquote>



<h3>How <code>LdrLoadDll</code> works</h3>



<p>Loading dependencies is a major component of <code>LdrInitializeThunk</code>, which is probably why it is called the image loader. Moreover, the techniques EDR-Preloading and Early Cascade Injection specifically intervene in this <code>LdrLoadDll</code> function during process creation. Therefore, we also briefly cover how <code>LdrLoadDll</code> loads dependencies. After that, we will start exploring the interesting stuff: process injection.</p>



<blockquote>
<p>The most important aspect to remember after this section is that<code>LdrLoadDll</code> loads a DLL in two steps: first it <strong>maps</strong> the DLL’s image file into memory through <code>LdrpFindOrPrepareLoadingModule</code> and second it <strong>initializes</strong> the DLL via <code>LdrpPrepareModuleForExecution</code>. If a DLL has recursive dependencies, these are first mapped into memory and only then each module is initialized. Initialization is performed in reverse order to follow the correct dependency sequence.</p>
</blockquote>



<p>Now, let’s delve into a bit more details and walk through the process of loading <code>kernel32.dll</code>, the first module being loaded during process creation. Try to follow along in the call graph. We start at <code>LdrInitializeThunk</code> which calls <code>LdrpLoadDll</code> to load <code>kernel32.dll</code>. <code>LdrpLoadDll</code> invokes <code>ntdll!LdrpFindOrPrepareLoadingModule</code> for the first step: mapping <code>kernel32.dll</code> into memory. The actual mapping is eventually performed by the nested function <code>NtMapViewOfSection</code>. After, <code>LdrpMapAndSnapDependency</code> checks for dependencies, and since <code>kernel32.dll</code> imports functions from <code>kernelbase.dll</code>, <code>LdrpLoadDependentModule</code> maps <code>kernelbase.dll</code> into memory.</p>



<p>Once the two modules have been mapped into memory, the second part of <code>LdrLoadDll</code> initializes the DLLs, carried out by <code>LdrpPrepareModuleForExecution</code>. This function invokes <code>LdrpCondenseGraph</code> to create a dependency graph, which stores the order in which dependencies must be initialized. Following, <code>LdrpInitializeGraphRecurse</code> processes this graph and for each module in the graph <code>LdrpInitializeNode</code> is called. The first node in the graph is <code>kernelbase.dll</code> which <code>LdrpInitializeNode</code> initialises through <code>LdrpCallInitRoutine</code>. This function calls the entry point of <code>kernelbase.dll</code>. After that, the same is performed to initialize <code>kernel32.dll</code>; hereafter, <code>LdrLoadDll</code> has finished loading <code>kernel32.dll</code>.</p>



<h2>Early Bird APC Injection</h2>



<p>Now that we have a general understanding of process creation, let’s take a closer look at the Early Bird APC Injection technique which was discovered by Cyberbit in 2018 <a href="https://www.cyberbit.com/endpoint-security/new-early-bird-code-injection-technique-discovered/">[4]</a>. This is a well-known and effective process injection method that involves injecting code before the execution of a process’s main entry point. Injecting this early in the process may evade EDR detection measures, including hooks, if these measures are not loaded before APC execution.</p>



<p>The Early Bird APC Injection technique works as follows:</p>



<ol>
<li>Create a target process in suspended state (e.g. <code>CreateProcess</code>);</li>



<li>Allocate writeable memory in the target process (e.g. <code>VirtualAllocEx</code>);</li>



<li>Write malicious code to the allocated memory (e.g. <code>WriteProcessMemory</code>);</li>



<li>Queue an APC to the remote target process, the APC points to the malicious code (e.g. <code>QueueUserAPC</code>);</li>



<li>Resume the target process, upon resumption the APC is executed, running the malicious code (e.g. <code>ResumeThread</code>).</li>
</ol>



<p>As we previously learned, when a process is created in a suspended state (1), execution halts just before the user-mode part of process creation, handled by <code>LdrInitializeThunk</code>. At this point, the payload with malicious code is written into the target process (2, 3). Then, an APC routine pointing to the payload is queued for execution in the suspended thread (4). Last, the suspended thread is resumed (5).</p>



<p>On resumption of the thread, it starts execution at <code>LdrInitializeThunk</code> and one of final tasks of <code>LdrInitializeThunk</code> is emptying the APC queue. Specifically, <code>NtTestAlert</code> is responsible for emptying a threads APC queue by executing the APCs in it. This is when the injected payload runs.</p>



<p>In the past, execution of the payload at this point was early enough to preempt EDR user-mode detection measures, like hooks. However, modern EDR solutions often load their detection measures earlier in the process creation timeline. Nonetheless, we found that a popular EDR still loads its detection measures after <code>NtTestAlert</code>. For this particular EDR, Early Bird APC Injection bypasses the EDR’s user-mode detection measures. Despite Early Bird APC Injection does may no longer evade hooks of modern EDRs, it still remains very useful for injection purposes.</p>



<blockquote>
<p>Early Bird APC remains a valuable injection technique, even though it is less effective as an evasion method against modern EDRs.</p>
</blockquote>



<p>Nevertheless, as mentioned earlier, <strong>Early Bird APC Injection is likely to be detected due to the suspicious cross-process queuing of an APC</strong>. Queuing an APC from one process to another is known as cross-process APC queueing. This behaviour is suspicious and closely monitored by EDRs. It’s difficult to hide cross-process APC queueing, making it a strong indicator for detecting Early Bird APC Injection. In a moment, we will see how Early Cascade Injection performs its injection without cross-process queuing and therefore goes undetected against the EDRs we tested.</p>



<h2>EDR-Preloading</h2>



<p>Early Cascade Injection incorporates elements of EDR-Preloading. Therefore, let’s briefly delve into EDR-Preloading, which was recently introduced in a blog by Marcus Hutchins, who is known for stopping the WannaCry <a href="https://www.malwaretech.com/2024/02/bypassing-edrs-with-edr-preload.html">[1]</a>. His blog inspired my research in this area, thank you for that!</p>



<p>EDR-Preloading is designed to prevent EDRs from loading their user-mode detection measures during process creation. For example, it prevents EDRs from initialising their Hooking DLL, which significantly reduces an EDRs the visibility within the process as the EDR is unable to intercept API calls. Techniques like this are becoming increasingly important as Microsoft gradually restricts third-party access to the kernel, forcing EDR detection measures from kernel-mode to user-mode. It is speculated that kernel restrictions will be pushed further since the Crowdstrike incident took down 8.5 million Windows systems, due to a faulty update in their kernel-level software <a href="https://www.theverge.com/2024/9/12/24242947/microsoft-windows-security-kernel-access-features-crowdstrike">[5]</a>.</p>



<p>How EDR-Preloading works: <strong>it begins by creating a process in suspended state and hijacking the <code>ntdll!AvrfpAPILookupCallbackRoutine</code> callback pointer in <code>ntdll.dll</code></strong>. Hijacking involves assigning the start address of malicious code to the callback pointer and enabling the callback pointer by setting <code>ntdll!AvrfpAPILookupCallbacksEnabled</code> to <code>1</code>. As a result, the callback pointer is executed during the user-mode part of process creation after resuming the suspended process. <strong>Once invoked, the malicious code runs, thereby taking control of the execution flow during process creation.</strong></p>



<p>This malicious code runs very early in the process creation sequence when only <code>ntdll.dll</code> is loaded. Specifically, the callback <code>AvrfpAPILookupCallbackRoutine</code> is triggered in the initialisation part of <code>LdrLoadDll</code>, as illustrated in the call graph. Both the callback pointer (<code>ntdll!AvrfpAPILookupCallbackRoutine</code>) and the boolean variable (<code>ntdll!AvrfpAPILookupCallbacksEnabled</code>) are highlighted in the graph in light green and green. This <code>LdrLoadDll</code> function is executed for the first time during the initialization of <code>kernelbase.dll</code>, the first DLL to be loaded in user-mode. If EDRs load their detection measures after this point, it is possible to prevent them from loading their detection measures. A detailed explanation of this and how to implement it can be found in the EDR-Preloading blog.</p>



<blockquote>
<p>What we found interesting about the EDR-Preloading technique is that it possible to get code execution just by overwriting a callback pointer in the target’s <code>ntdll.dll</code> during process creation. <strong>However, this code execution is highly constrained.</strong></p>
</blockquote>



<h3>Code execution limitations</h3>



<p>The code execution obtained through <code>ntdll!AvrfpAPILookupCallbackRoutine</code> during process initialisation is significantly constrained. These limitations are caused by the <strong>limited number of available dependencies</strong> and the constraints imposed by the <strong>Loader Lock</strong> synchronisation object.</p>



<h4>Limited dependencies</h4>



<p>At the point when the <code>AvrfpAPILookupCallbackRoutine</code> callback is invoked, only <code>ntdll.dll</code> is fully loaded into the process. Consequently, code execution is restricted to the undocumented NTAPI functions within <code>ntdll.dll</code>, significantly limiting the actions that can be performed. The lack of access to other libraries, such as <code>winhttp.dll</code>, complicates the execution of more complex operations, such as communicating with a command and control (C2) server.</p>



<p>Furthermore, due to the presence of Loader Lock, no additional DLLs can be loaded, and no new threads can be created.</p>



<h4>Loader Lock</h4>



<p>The <code>ntdll!AvrfpAPILookupCallbackRoutine</code> callback runs during the initialisation part of <code>LdrLoadDll</code>, under the function <code>LdrpPrepareModuleForExecution</code>. More precisely, the callback is triggered within <code>LdrpInitializeNode</code>, which handles the actual initialisation of a DLL, as previously discussed. During the execution of <code>LdrpInitializeNode</code>, Loader Lock is held to synchronize the loading and unloading of DLLs. The call graph shows that this synchronization is managed by <code>LdrpAcquireLoaderLock</code> and released by <code>LdrpReleaseLoaderLock</code>.</p>



<p><strong>Loader Lock is a critical section object that prevents the loading of additional DLLs and creation of new threads <a href="https://learn.microsoft.com/en-us/windows/win32/sync/critical-section-objects">[8]</a></strong>. Critical sections are a synchronisation mechanism similar to mutexes and semaphores, but they are designed to be more efficient and for single-process synchronisation. For information on critical section objects, refer to the MSDN documentation <a href="https://learn.microsoft.com/en-us/windows/win32/sync/critical-section-objects">[8]</a>.</p>



<p>Loader Lock is acquired each time when a function needs access to the module database (<code>PEB_LDR_DATA</code>), which is involved in tasks such as DLL loading, unloading, and thread creation <a href="https://elliotonsecurity.com/what-is-loader-lock/">[9]</a>. We discussed the module database earlier in step 3 of <code>LdrInitializeThunk</code>‘s tasks. A well-known function that accesses the module database is <code>GetModuleHandle</code>, which retrieves the base address of a DLL and is often used by malware to resolve undocumented NTAPI functions. However, if this function is called while Loader Lock is active, such as during the execution of <code>AvrfpAPILookupCallbackRoutine</code>, a deadlock occurs, causing the process to hang. Similarly, attempting to load additional DLLs via functions like <code>LdrLoadDll</code> results in a deadlock.</p>



<blockquote>
<p>In summary, code execution through the callback pointer <code>AvrfpAPILookupCallbackRoutine</code> during process initialisation is limited to the modules already loaded into the process at that point (<code>ntdll.dll</code>). Additionally, Loader Lock prevents the loading of additional DLLs and the creation of new threads, making it difficult to execute tasks that require access to more modules. Despite these limitations, the EDR-Preloading technique has demonstrated that there are just enough capabilities to prevent EDRs from loading their detection measures.</p>
</blockquote>



<h2>Early Cascade Injection: A new process injection technique</h2>



<p>What we found interesting about the EDR-Preloading technique is that you can get code execution just by overwriting a callback pointer in the target’s <code>ntdll.dll</code> during process creation. However, as we have seen, the code execution obtained through this callback is highly restricted as Loader lock is enabled, making it impractical to run fully functional code, such as an implant with networking capabilities. Therefore, we explored novel and alternative techniques during process creation for process injection. As a results we developed Early Cascade Injection, a novel code injection technique emerged from the restrictions imposed by the Loader Lock.</p>



<h4>An alternative callback pointer: <code>g_pfnSE_DllLoaded</code></h4>



<p>During our search for alternative injection techniques, <strong>we discovered an alternative callback pointer that also allows code execution during the user-mode part of process creation.</strong> This pointer, named <code>g_pfnSE_DllLoaded</code>, is located in the <code>.mrdata</code> section of <code>ntdll.dll</code>. Unlike the <code>AvrfpAPILookupCallbackRoutine</code>, <code>g_pfnSE_DllLoaded</code> does not appear to run under Loader Lock. This can be inferred from the call graph, where it is highlighted in light blue and blue.</p>



<p>While not directly relevant to this blog, it might be interesting to understand what this pointer belongs to. The <code>g_pfnSE_DllLoaded</code> pointer belongs to the Shim Engine, as indicated by its name, the prefix <code>g_pfnSE</code> stands for “global function pointer Shim Engine”. The Shim Engine is a Windows technology responsible for applying compatibility fixes, known as ‘shims,’ without modifying application code. It allows older applications to run on newer versions of Windows, by intercepting and modifying API calls. Although rarely used and disabled by default, the Shim Engine’s implementation is still present in <code>ntdll.dll</code>, along with its pointers, including <code>g_pfnSE_DllLoaded</code>.</p>



<p>Let’s return to the key aspects of <code>g_pfnSE_DllLoaded</code>. <strong>The pointer can be manually enabled by setting the <code>g_ShimsEnabled</code> boolean variable to <code>1</code></strong>, located in the <code>.data</code> section of <code>ntdll.dll</code>. <strong>However, enabling this variable enables all Shim Engine related pointers</strong>, not just <code>g_pfnSE_DllLoaded</code>. Each of these pointers require a valid address, and if any remain uninitialized, the process will crash. This makes it impractical to exploit <code>g_pfnSE_DllLoaded</code> alone without addressing the other pointers.</p>



<p>To overcome this, <strong>we focused specifically on <code>g_pfnSE_DllLoaded</code> as it is the first Shim Engine pointer invoked during process creation.</strong> By targeting this pointer we can execute code before any of the other unassigned pointers and prevent them from executing. This method involves assigning the address of our shellcode to <code>g_pfnSE_DllLoaded</code> and enabling <code>g_ShimsEnabled</code> to activate it. <strong>Upon execution, the shellcode immediately disables <code>g_ShimsEnabled</code>, preventing the remaining Shim Engine pointers from being invoked.</strong> This approach allows use to execute code without causing the process to crash due to uninitialized pointers.</p>



<p>Returning to the call graph, we observe that <code>g_pfnSE_DllLoaded</code> runs in the scope of <code>LdrpSendPostSnapNotifications</code>, which is a subordinate function of <code>LdrpPrepareModuleForExecution</code>. Unlike <code>LdrpPrepareModuleForExecution</code>, we observe that <code>g_pfnSE_DllLoaded</code> does not run under Loader Lock. Instead, a different critical section object is acquired: <code>LdrpDllNotificationLock</code>. This critical section appears to be self-reentrant, suggesting it should not lead to deadlock when loading additional DLLs, although we have not verified.</p>



<p>Despite not operating under Loader Lock, we were unable to run a fully functional shellcode. This is likely due to interrupting the loading process of kernelbase.dll and kernel32.dll. We will work around this in the next section.</p>



<p>Let’s briefly revisit the memory section where <code>g_pfnSE_DllLoaded</code> resides, as this is crucial for leveraging it. <strong><code>g_pfnSE_DllLoaded</code> is located in the <code>.mrdata</code> section, which is writable when a process is created in a suspended state.</strong> Later, during the user-mode part of process initialization, this section is made read-only, as noted in step 2 of <code>LdrInitializeThunk</code>. After this step, modifying its content requires memory protection changes.</p>



<p>Furthermore, <strong>the <code>g_ShimsEnabled</code> boolean is located in the <code>.data</code> section, which remains writable throughout the entire process.</strong> This allows us to enable or disable the <code>g_pfnSE_DllLoaded</code> pointer without modifying memory protections. In contrast, the <code>AvrfpAPILookupCallbacksEnabled</code> boolean, used in EDR-Preloading, resides in the <code>.mrdata</code> section and requires memory protection changes after step 2 of <code>LdrInitializeThunk</code>.</p>



<p>This makes <code>g_pfnSE_DllLoaded</code> preferable over <code>AvrfpAPILookupCallbackRoutine</code>, as it can be disabled without altering memory protections. As a result, the shellcode required to hijack the pointer is smaller, invoked only once, involves fewer API calls, and therefore reduces the risk of detection.</p>



<p>Additionally, <strong>the <code>g_pfnSE_DllLoaded</code> pointer is triggered slightly earlier than <code>AvrfpAPILookupCallbackRoutine</code>, offering earlier control over the process.</strong> Similar to how <code>AvrfpAPILookupCallbackRoutine</code> is leveraged in EDR-Preloading to preempt EDRs, <code>g_pfnSE_DllLoaded</code> can also be used for this purpose, with potentially greater effectiveness due to its earlier execution. As shown in the call graph, <code>g_pfnSE_DllLoaded</code> is executed just before <code>LdrpCallInitRoutine</code>, which initializes a DLL. This timing allows us to disrupt the initialisation of EDR user-mode detection measures implemented as DLL, making them ineffective. For example, it could prevent EDRs from deploying hooks that intercept API calls, significantly reducing an EDRs visibility within a process. While not the focus of this blog, this presents another use case for the pointer.</p>



<blockquote>
<p>In summary, we identified an alternative pointer named <code>g_pfnSE_DllLoaded</code>, located in the <code>.mrdata</code> section of <code>ntdll.dll</code>. This pointer can be enabled via the <code>g_ShimsEnabled</code> boolean, located in the <code>.data</code> section of <code>ntdll.dll</code>. The <code>.mrdata</code> section is writable during suspend state of process creation and the <code>.data</code> section is writable through the entire process, allowing use to hijack this pointer without changing memory protections. Moreover, <code>g_pfnSE_DllLoaded</code> does not operate under Loader Lock, but it is not trivial to execute fully functional shellcode for a unknown reason. Though, we suspect this may be related to a critical section object or because of the interruption during the <code>kernel32.dll</code> and <code>kernelbase.dll</code> loading process.</p>
</blockquote>



<h4>Intra-process APC queueing</h4>



<p>The limitations of code execution through <code>g_pfnSE_DllLoaded</code> made us thinking. We then realized that during the code execution, <strong>we could invoke an execution primitive to run code at a different stage, free from the limitations</strong>. We considered several execution primitives, including <code>NtQueueApcThread</code>, <code>NtCreateThread</code> and various callbacks such as <code>CreateTimerQueueTimer</code>. Eventually, we found that <code>NtQueueApcThread</code> was suitable for our needs and did the job <a href="http://undocumented.ntinternals.net/index.html?page=UserMode%2FUndocumented%20Functions%2FAPC%2FNtQueueApcThread.html">[6]</a>. A comprehensive list of potential callbacks as alternative to <code>NtQueueApcThread</code> can be found in this repository <a href="https://github.com/aahmad097/AlternativeShellcodeExec">[7]</a>.</p>



<p>The use of an execution primitive to move code execution to another point, e.g. via <code>NtQueueApcThread</code>, was inspired by Early Bird APC Injection. Although, Early Bird APC Injection leverages the APC queue for cross-process code execution.</p>



<p><strong>By leveraging the code execution obtained through <code>g_pfnSE_DllLoaded</code>, we can have the initial thread queue an APC on itself.</strong> This allows us to transition to unrestricted execution later in the process creation. We refer to this as intra-process APC queuing. The queued APC routine points to malicious code in the target’s memory, such as an implant.</p>



<p><strong><code>NtQueueApcThread</code> was particular suitable because it is available in <code>ntdll.dll</code> and is not subject to the loader lock since it does not involve DLL operations or thread creation.</strong> This means we don’t have to worry about causing a deadlock when calling this function within the execution scope of <code>g_pfnSE_DllLoaded</code>.</p>



<p>Moreover, <code>NtQueueApcThread</code> allows us to queue an APC early in the process initialization phase, before the APC queue is emptied. As detailed in step 9 of <code>LdrInitializeThunk</code>, one of the final steps involves invoking <code>NtTestAlert</code> to clear the APC queue. <strong>This guarantees the execution of our queued APC.</strong> Furthermore, since <code>NtTestAlert</code> is one of the final functions, we can be certain that all DLLs, including <code>kernel32.dll</code> and <code>kernelbase.dll</code>, have been fully loaded, ensuring that no issues arise from incomplete DLL loading.</p>



<p>To test our idea, we wrote a piece of shellcode that utilizes <code>NtQueueApcThread</code> in <code>ntdll.dll</code> to queue an intra-process APC. We refer to this shellcode as the <strong>payload stub</strong>, which we into the target’s memory. The APC routine passed to <code>NtQueueApcThread</code> points to the address of malicious code that we have written into the target’s memory. This malicious code we refer to as the <strong>payload</strong>. Thus, the payload stub is executed by <code>g_pfnSE_DllLoaded</code>, while the payload is executed through the APC.</p>



<h4>The Early Cascade Injection technique</h4>



<p>By now, the direction of the Early Cascade Injection technique should be clear, as we’ve covered all the key elements and background information. It’s time to bring everything together and formally introduce Early Cascade Injection!</p>



<p><strong>Early Cascade Injection works as follows:</strong> it begins by creating a child process in suspended state. Then it writes a two-part payload into it. Next, the parent locates the pointer <code>g_pfnSE_DllLoaded</code> in the <code>.mrdata</code> section and it locates the <code>g_ShimsEnabled</code> boolean variable in the <code>.data</code> section of <code>ntdll.dll</code>. Following, it assigns the address of the first payload part, the payload stub, to this <code>g_pfnSE_DllLoaded</code> pointer of the new process and enables it by setting <code>g_ShimsEnabled</code> to <code>1</code>. Last, it resumes the suspended process. As a result, the initial thread of the new process executes the payload stub. This payload immediately disables the <code>g_ShimsEnabled</code> by setting it to <code>0</code>, preventing the remaining Shim Engine related pointers from executing. Then, the payload stub queues the second part of the payload as an APC on itself, i.e. the initial thread, using <code>NtQueueApcThread</code>. This APC is triggered near the end of the Windows Image Loader by the <code>NtTestAlert</code> function. As a result, the main payload executes. The main payload could be an implant containing the main functionality that the attackers want to run on the target’s system.</p>



<p>In figure 2, the flow of Early Cascade Injection is depicted as described above.</p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/EarlyCascadeFlowV3.png"><img loading="lazy" decoding="async" width="1024" height="817" src="https://www.outflank.nl/wp-content/uploads/2024/10/EarlyCascadeFlowV3-1024x817.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/EarlyCascadeFlowV3-1024x817.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/EarlyCascadeFlowV3-300x239.png 300w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 2: Flow Early Cascade Injection</em></p>



<p>Early Cascade Injection is a novel injection technique and may serve as alternative to Early Bird APC Injection. The major advantage compared to Early Bird APC Injection is that Early Cascade does not involve a remote execution primitive (cross-process APC queueing). In addition, Early Cascade Injection, unlike Early Bird APC Injection, is currently undocumented and breaks traditional code injection patterns by not queuing an APC across processes. We tested it against multiple EDRs, including top tier ones, and went undetected.</p>



<h5>Key features</h5>



<ul>
<li><strong>No remote execution primitive:</strong> Early Cascade Injection avoids remote execution primitives such as <code>QueueUserAPC</code>. Just like threadless injection methods, it leverages a pointer for execution of the payload, avoiding the need for a remote execution primitive.</li>



<li><strong>Minimal remote process interaction:</strong> Early Cascade Injection only involves remote memory allocation, protection and writing.</li>



<li><strong>Writable <code>.mrdata</code> and <code>.data</code>:</strong> The <code>.mrdata</code> section is writable during the suspended state, allowing modifications without changing memory protections. Also <code>.data</code> is writable during the entire process, allowing enabling/disabling of <code>g_pfnSE_DllLoaded</code> without changing memory protections.</li>



<li><strong>Novel technique:</strong> Due to Early Cascade Injection’s novel approach, its call pattern is less likely to be recognized by security products, reducing the risk of detection.</li>



<li><strong>Undocumented callback:</strong> Early Cascade Injection relies on the undocumented pointer <code>g_pfnSE_DllLoaded</code>, which may change with Windows updates, potentially impacting its reliability.</li>
</ul>



<h2>EDR Detection Measure Loading Mechanism and Timing</h2>



<p>In this final section, we explore how and when EDRs load their user-mode detection measures, such as hooks, during process creation. Understanding the timing of these measures is crucial for developing strategies to preempt and evade them. Preempting means gaining control of the process before these detection measures are in place. For confidentiality, we won’t mention specific EDR names.</p>



<p>To provide a clearer understanding of user-mode detection mechanisms, we will briefly discuss hooks, using them as an example. Besides, user-mode hooks are one of the key detection measures used by EDRs to detect malicious activity. Especially, since Microsoft gradually restricts kernel access, which forces EDRs to shift detection measures to user-mode <a href="https://www.theverge.com/2024/9/12/24242947/microsoft-windows-security-kernel-access-features-crowdstrike">[5]</a>. Microsoft does offer alternatives like Event Tracing for Windows (ETW), however these are not yet widely adopted. This is likely to change in the nearby future.</p>



<p>Hooks allow EDRs to monitor processes in real time by intercepting API calls from within the process. By preventing these hooks from loading, attackers can significantly reduce an EDR’s visibility, thereby increasing the chances of malware evading detection. One effective approach to avoid hooks is by acting before the hooks are fully loaded and take effect. Typically, EDRs place hooks through a hooking DLL during the user-mode part of process creation. In the following section, we will explain how this works in detail.</p>



<p>Before diving into the technical details, it’s essential to understand the role of the kernel driver in EDRs. This driver enables EDRs to register notification callback routines to receive alerts on system events such as process creation or termination, image loading, registry changes, and system shutdown requests. These callbacks gather system information on which EDRs might take future actions. For instance, upon receiving a process creation notification, the EDR can inject its hooking DLL into a new process for monitoring purposes.</p>



<p>The process notification callback is stored in the kernel’s <code>nt!PspCreateProcessNotifyRoutine</code> array, which holds all registered callbacks. When a new process is created, the kernel function <code>nt!PspCallProcessNotifyRoutines</code> iterates through this array, invoking each callback. For more information on the components of EDRs and their interaction with Windows, we recommend the book Evading EDR by Matt Hand.</p>



<p>As a side note, there are evasion tools that can deregister kernel callbacks to prevent EDRs from loading additional security measures <a href="https://br-sn.github.io/Removing-Kernel-Callbacks-Using-Signed-Drivers/">[10]</a>. However, this approach requires access to the kernel, which is typically achieved through the exploitation of a vulnerable kernel driver. By modifying the kernel’s notification callbacks, these tools can block the EDR from loading its user-mode detection measures. However, the required kernel-access for this technique, makes it a complex method for evasion.</p>



<p>Returning to the main point, EDRs use the process creation notification as a trigger to load user-mode detection measures into newly created processes. We analyzed several EDRs to understand how these detection measures are loaded. Based on our findings, we explain the general approach EDRs take to inject their user-mode detection modules.</p>



<p>We observed that when a newly created process resumes from a suspended state, EDRs modify <code>ntdll.dll</code> just before transitioning from kernel to user-mode (<code>LdrInitializeThunk</code>). Specifically, EDRs inject shellcode into the process memory, which contains the logic to load the EDR’s hooking DLL. Additionally, they place a hook in <code>LdrInitializeThunk</code>, redirecting code execution to the injected shellcode. In our analyses of various EDRs, we found that the hooks are specifically placed on <code>LdrLoadDll</code>, <code>LdrpLoadDll</code>, or <code>NtContinue</code> within <code>LdrInitializeThunk</code>. Figure 3 revisits the call graph and highlights these functions. Note, EDRs also load their detection measures using this mechanism for processes that are not created in a suspended state.</p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/EDR-timing-figure.png"><img loading="lazy" decoding="async" width="1024" height="646" src="https://www.outflank.nl/wp-content/uploads/2024/10/EDR-timing-figure-1024x646.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/EDR-timing-figure-1024x646.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/EDR-timing-figure-300x189.png 300w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 3: The red arrows point to the functions that EDRs hook to load their user-mode detection measures</em></p>



<p>As an example, figure 4 shows the hook on <code>LdrLoadDll</code>. The initial bytes of <code>LdrLoadDll</code> are replaced with a jump instruction that points to the injected shellcode. This hooked version of <code>LdrLoadDll</code> is called as a subordinate function of <code>LdrInitializeThunk</code>. When <code>LdrLoadDll</code> executes, the flow of execution is redirected to the injected shellcode.</p>



<p>This shellcode is responsible for loading the EDR’s detection measures. Figure 5 depicts the call stack of the shellcode that loads the EDR’s hooking DLL. In the callstack, we can see that the root function is unbacked, meaning it’s not part of a legitimate module, which indicates it has been injected into the process.</p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/Figure4.png"><img loading="lazy" decoding="async" width="1024" height="143" src="https://www.outflank.nl/wp-content/uploads/2024/10/Figure4-1024x143.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/Figure4-1024x143.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/Figure4-300x42.png 300w, https://www.outflank.nl/wp-content/uploads/2024/10/Figure4.png 1194w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 4: <code>LdrLoadDll</code>‘s initial bytes have been replaced with a jump instruction to the EDR’s shellcode</em></p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/Figure5.png"><img loading="lazy" decoding="async" width="1024" height="87" src="https://www.outflank.nl/wp-content/uploads/2024/10/Figure5-1024x87.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/Figure5-1024x87.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/Figure5-300x25.png 300w, https://www.outflank.nl/wp-content/uploads/2024/10/Figure5.png 1499w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 5: Callstack of the EDR’s shellcode loading it’s hooking DLL</em></p>



<p>The injected shellcode writes the path and name of the hooking DLL into <code>rcx</code> and <code>r9</code> (following the x64 fastcall convention) and then invokes <code>LdrLoadDll</code>. Once the hooking DLL is loaded, its entry point (e.g. <code>DllMain</code>) is executed, which is responsible for initiating the hooks on critical functions. After <code>LdrLoad</code> completes, the shellcode removes the inline hook and resumes the process’s normal execution flow. From this point, the EDR can intercept API calls in real-time and monitor the process.</p>



<p>In the call graph, we can observe the first time <code>LdrLoadDll</code>, <code>LdrpLoadDll</code>, and <code>NtContinue</code> execute. At one of these points, depending on the specific EDR, the EDR’s detection measures (e.g., hooking DLL) are loaded.</p>



<p>For EDRs hooking <code>NtContinue</code>, techniques like Early Bird APC and Early Cascade Injection preempt the EDR’s detection measures. This means that the malicious code (e.g. implant) runs before the detection measures are loaded. In the call graph, we can see that <code>NtTestAlert</code> is executed before <code>NtContinue</code>. Since <code>NtTestAlert</code> empties the APC queue, it ensures that the malicious code runs before the EDR’s detection measures are active.</p>



<p>For EDRs hooking <code>LdrLoadDll</code> and <code>LdrpLoadDll</code> the EDR takes control early in the process, at the loading of <code>kernel32.dll</code>, which is before <code>g_pfnSE_DllLoaded</code>. This allows the EDR to gain control over the process before we do. However, as we can see in the call graph <code>g_pfnSE_DllLoaded</code> provides us with control before the initialization of the EDR, at that point we take control. This means that, despite the EDR taking control first, we can still disrupt the initialization of its detection measures as we can take control before the DLL initializes, preventing the EDR from loading them.</p>



<p>We also observed that most EDRs, through the shellcode, initially load <code>kernel32.dll</code> and <code>kernelbase.dll</code>, following the normal execution flow. Afterward, they load their hooking DLL via <code>LdrLoadDll</code>. Remember that <code>g_pfnSE_DllLoaded</code> is executed during the initialization part of <code>LdrLoadDLL</code>, in this case for <code>kernelbase.dll</code>. That is well-before the EDR’s detection measures are loaded by the shellcode. In theory, at this stage, we can remove the hook on <code>LdrLoadDll</code>, revert to the original code path for loading <code>kernel32.dll</code>, and proceed with execution, bypassing the EDR’s loading process.</p>



<p>There are likely numerous ways to prevent user-mode EDR detection measures using the callback pointers discussed in this blog. We’ve presented one potential approach, which could be integrated into Early Cascade Injection by leveraging the <code>g_pfnSE_DllLoaded</code> callback pointer. This would allow an implant injected via Early Cascade Injection to run more stealthily, further evading EDR detection.</p>



<h2>Conclusion</h2>



<p>In this blog, we explored how a process is created in Windows, focusing on the user-mode part of process creation. We presented a call graph that outlines the key events during process creation. We then examined how Early Bird APC Injection works and interacts with the user-mode part, specifically when the queued APC is executed. After that, we discussed EDR-Preloading, which showed us how we can achieve code execution during process creation simply by overwriting a pointer. This led us to further investigate and discover a new pointer. However, it wasn’t possible to execute fully functional code through it. By combining the APC queuing element of Early Bird APC with the new pointer, inspired by EDR-Preloading, we developed and explained Early Cascade Injection. Finally, we highlighted the key features of this technique. I hope you found the call graph as informative as we did – providing an overview of the process creation, revealing the timing of EDR security measures, and showing how Early Cascade Injection interacts with process creation.</p>



<h2>Implementation in Outflank Security Tooling</h2>



<p>Due to the strong OPSEC properties of this research, we need to prevent misuse and thus will not make the source code of this project public. However, Early Cascade Injection and all other parts of this research are already available for our vetted <a href="https://outflank.nl/ost">Outflank Security Tooling</a> (OST) community. Consider scheduling an <a href="https://www.outflank.nl/demo-request/">expert-led demo</a> to learn more about Early Cascade and the other diverse offerings in OST. Or, if you’re ready to take the next step, request a quote to start the purchase process.</p>







<p>Thank you for reading this blog post, and we hope you learned something new!</p>



<h2>References</h2>



<p>[1] <a href="https://www.malwaretech.com/2024/02/bypassing-edrs-with-edr-preload.html">Bypassing EDRs With EDR-Preloading – Marcus Hutchins</a><br>[2] <a href="https://learn.microsoft.com/en-us/windows/win32/procthread/process-creation-flags">Process Creation Flags – MSDN</a><br>[3] <a href="https://blogs.blackberry.com/en/2017/10/windows-10-parallel-loading-breakdown">Windows 10 Parallel Loading Breakdown – BlackBerry</a><br>[4] <a href="https://www.cyberbit.com/endpoint-security/new-early-bird-code-injection-technique-discovered/">New ‘Early Bird’ Code Injection Technique Discovered – Cyberbit</a><br>[5] <a href="https://www.theverge.com/2024/9/12/24242947/microsoft-windows-security-kernel-access-features-crowdstrike">Microsoft is building new Windows security features to prevent another CrowdStrike incident – The Verge</a><br>[6] <a href="http://undocumented.ntinternals.net/index.html?page=UserMode%2FUndocumented%20Functions%2FAPC%2FNtQueueApcThread.html">NtQueueApcThread – NTAPI Undocumented Functions – NTInternals</a><br>[7] <a href="https://github.com/aahmad097/AlternativeShellcodeExec">AlternativeShellcodeExec – aahmad097</a><br>[8] <a href="https://learn.microsoft.com/en-us/windows/win32/sync/critical-section-objects">Critical Section Objects – MSDN</a><br>[9] <a href="https://elliotonsecurity.com/what-is-loader-lock/">What is Loader Lock? – Elliot Killick</a><br>[10] <a href="https://br-sn.github.io/Removing-Kernel-Callbacks-Using-Signed-Drivers/">Removing Kernel Callbacks Using Signed Drivers – br-sn</a><br>[11] <a href="https://techcommunity.microsoft.com/t5/ask-the-performance-team/demystifying-shims-or-using-the-app-compat-toolkit-to-make-your/ba-p/374947">Demystifying Shims – or – Using the App Compat Toolkit to make your old stuff work with your new stuff – MSDN</a></p>





</div></div>]]></description>
        </item>
    </channel>
</rss>