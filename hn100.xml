<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 12 Apr 2024 13:00:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I Lost Faith in Kagi (161 pts)]]></title>
            <link>https://d-shoot.net/kagi.html</link>
            <guid>40011314</guid>
            <pubDate>Fri, 12 Apr 2024 11:17:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://d-shoot.net/kagi.html">https://d-shoot.net/kagi.html</a>, See on <a href="https://news.ycombinator.com/item?id=40011314">Hacker News</a></p>
<div id="readability-page-1" class="page">
		

		<p>I've posted many times before about paid search engine Kagi but I really wanted, like I have with other sites in the past, to make a post that just outlines why I don't have any faith in it.  Because the majority of my reasons come from being in their Discord, which tons of average users will never really look at, and I suspect a lot of Kagi supporters do not know a lot of this backstory.  This isn't a heavily researched deep dive or anything, I feel like I always need these kind of disclaimers when I'm just posting my thoughts about something backed up with my reasons for thinking it...it's just a quick rundown of why I personally went from a Kagi subscriber to a Kagi Naysayer, so that I can have my reasons in one place that I can link others to.  And I have seen a lot of people recommend Kagi that I personally know would be a lot less interested in doing so if they knew this information, but they don't because it's not readily apparent just from using the site.</p>

		<p>For those unfamiliar to start with, Kagi is a paid search engine that purportedly focuses on privacy.  It's meant to be a no-nonsense platform focused on being the best search it can be.  Frankly, I think at this point it's a very nonsense-focused platform that has an extremely disinterested view of privacy, and I think it's not a particularly sustainable project either.</p>

		<p>First of all, as a project, Kagi stretches itself way too thin.  "Kagi" isn't just Kagi Search, it's also a whole slew of AI tools, a Mac-only web browser called Orion, and right now they are planning on launching an email service as well.  None of these projects are particularly profitable, so it's not a case of one subsidizing the other, and when they announced Kagi Email even their most dedicated userbase (aka the types who hang around in a discord for a search engine) seemed largely disinterested.  For products this niche, I don't think you can afford to be running multiple niche products at once.  Your developers are going to be running far too ragged to keep them all going.  Last I could find Kagi employs about 16 employees, half full time.  Presumably not every single one of those 8 or so full timers are web devs.  That's not a lot of people to throw at this many things.</p>

		<p><a href="https://blog.kagi.com/celebrating-20k">Oh and they own a t-shirt factory.</a></p>

		<p>You see, when Kagi had a funding round and raised about $670k.  This funding came from an investment round including "42 accredited investors, most of whom are actual Kagi users".  Unfortunately unlike Cohost I can't give any kind of huge financial breakdown here, because Kagi's finances are even less transparent.  If there are Kagi financial reports, I haven't seen them, just the occasional Discord comment from Kagi employees.  A while back they raised their prices, which lost them a lot of subscribers, because they were losing money per search at the old prices.  They were actually still losing money per search on the new prices.  They eventually lowered the prices back down a bit (and maybe raised them again?  I've completely lost the plot on their pricing at this point) and have claimed that at 25,000 users they would be breaking even.  I am unsure what happened in all of this for this math to be true, but my point is that they did in fact reach at least 20,000 users, and to celebrate they set up a business entity in Germany (they are currently US based), in order to start a tiny little t-shirt printing company.  And their goal was to print 20,000 t-shirts to give out, FOR FREE, to their first 20,000 users (with users paying only shipping costs).  But I cannot stress enough, they did not just spend money on 20,000 tshirts to give out, they set up a whole new business entity in Germany to run their own t-shirt printing operation, with its own building and warehouse and employee(s? I get the sense it's one guy but I don't know).  And this cost them 1/3 of their $670k funding round.  One, fucking, third.  For t-shirts.  Did I mention that the t-shirts don't even have the Kagi name on them?  Just the Kagi dog mascot, who is at this point the only thing I like about Kagi, to the point where if I wasn't worried people would try to talk to me about Kagi I'd opt for one of the shirts myself.  Great artist, whoever did this.  Terrible financial choice to start a whole t-shirt business to make 20,000 free t-shirts that do not even get your name out there.</p>

		<img src="https://d-shoot.net/img/kagi/breakeven.png" alt="Discord convo, quoting @tkataja saying 'of course I don't know about the financial situation of the company now and then, but', and Vlad replies on 01/28/2024 'we are almost breakeven, at 25k subs we should be'"> <br>
		<img src="https://d-shoot.net/img/kagi/salaries.png" alt="Discord convo, quoting @HKayn saying 'Is Kagi actually operating at a loss atm?', dev Thomas replies on 09/29/2023 'So we price our plans for them to be slightly profitable on a revenue - cost per search basis, but for now there aren't enough of you for that to fully offset R&amp;D expenses (eg. salaries, etc...).  You can help by getting your friends on :)"> <br>
		<img src="https://d-shoot.net/img/kagi/employees.png" alt="Discord convo on 05/23/2023.  Vlad: 'we are not sustainable yet but we are able to pay search and infrastructure cost after revenue from subscriptions.  Since there is a surplus there it is a matter of scaling the number of users now to also pay for the salaries.  That should happen around 20k customers.'  Grooty: 'how many employees do you currently have?' Raymyn: 'Thanks for the response.  Definitely looking forward to the future of Kagi. You guys are definitely the first product I have used in awhile that I am more than willing to throw money at a subscription for.  Hoping you guys hit the 20k mark soon, and grow to 100k, 1 million, or more in the near future!'  Vlad, replying to Grooty: '~16, roughly half of them are full time'"> 

		<p>T-shirt companies aside, there's one other thing about Kagi's finances that was revealed recently in an update stream they did that caught my attention--Kagi was not paying sales tax for two years and they finally have to pay up.  They just...didn't do it.  Didn't think it was important?  I have no idea why.  Their reactions made it sound like they owed previous taxes, not that they just now had to pay them.  They genuinely made it sound like they only just now realized they needed to figure out sales tax.  It's a baffling thing to me and it meant a change in prices for users that some people were not thrilled with.</p>

		<img src="https://d-shoot.net/img/kagi/kagisalestax.png" alt="Kagi slide that says 'Also, sales tax/vat...' with a graph of the world, plus sign, and graph of an unreadable graph, with an equal sign saying 'we need to pay sales taxes'.  Then 'Implementing now - updates in the next few weeks, expect changes in January"> <br>
		<img src="https://d-shoot.net/img/kagi/taxes.png" alt="Discord screenshot from 03/19/2024, Vlad says 'To clarify, this means an end-price increase for affected members (sales tax/VAT will be automatically added on top of Kagi price, if applicable in their country/state) and this is mandated by Kagi becoming large enough to have legal sales tax/VAT obligation.  In addition, Kagi will have to retroactively pay for all sales tax/VAT that we did not collect in the last almost two years.  We have chosen to absorb this on behalf of our customers.'"> 

		<p>But let's say that you can live with their financial issues.  They'll either survive or they won't and you'll just enjoy the ride.  Yet, there's a lot of reasons that I can't just enjoy the ride either.  I think the ride is going to get worse and worse in ways the casual users haven't really noticed yet because of how much of it is in beta or just Ideas That Vlad (the founder of Kagi) Had.  And most of them are surrounding AI.  I have to assume a lot of people don't understand how deep the AI rabbithole for Kagi goes, because I have seen people recommend Kagi to people frustrated with Google's own AI bullshit.  If AI is the thing you are trying to get away from, moving to Kagi is a lateral move at best.</p>

		<p>As it turns out, Kagi was <a href="https://blog.kagi.com/kagi-ai-search">founded originally as an AI company</a>, who later pivoted to search.  And going by their comments in their Discord, AI tools seem to be what they spend most of their time on these days.  They're launching AI features left and right, and they have fully bought into AI being the future of search.  They believe that by embracing AI, that will be the thing that sets their product apart (kinda late now) and get them the kind of userbase that can keep them afloat financially.  They have <a href="https://kagi.com/fastgpt">"FastGPT"</a>, where their focus is having a ChatGPT style service that is focused on being fast, not accurate.  And boy, it sure isn't, I messed around with this for a while and it very confidently gave me a lot of extremely inaccurate information about old sitcoms.  But of course, it's all stuff where if you didn't already know the answer to the question you asked you wouldn't know it was wrong--like when I asked it about the All in the Family episode Cousin Liz, it kept identifying the woman Edith and Archie are talking to in the episode as Cousin Liz, but Cousin Liz is instead Edith's deceased cousin who they were attending a funeral for.  Following a theme I asked it more broadly about homosexuality in All in the Family and it spit out a bunch of text repeatedly saying that Lionel Jefferson was gay.  But I guess it did spit all of this wrong information at me faster than some of the alternatives!  I wrote more about this experience on <a href="https://hackers.town/@lori/110420844338899624">fedi</a>, even though the content warning says ChatGPT this was done with Kagi's FastGPT, I was just trying to keep the thread simple at the time.</p>

		<p>Like most search now Kagi has chosen to include Instant Answers that are AI generated, which means they're often wrong, as well as a <a href="https://kagi.com/summarizer/index.html">"Universal Summarizer"</a> tool, that again is more of the same old AI bullshit.  There's also a beta tool called Kagi Assitant, which I...don't know what's really different about it than the other AI stuff they're doing, I think it has a chatbot mode?  I believe you have to be a subscriber to see this.  It's getting increasingly hard to tell some of these services apart.  There's also another <a href="https://sidekick.kagi.com/">beta feature called Sidekick</a> that puts Kagi AI stuff as a sidebar on your own website.  There was some demo where you could put someone's Twitter handle in and it would give you a summary of who that person was (nightmare shit).  But the developers of Kagi fully believe that this is what search engines should be, a bunch of AI tools so that you don't even need to read primary sources anymore.  If AI is your problem with Google or Bing, Kagi is in no way a solution for you.  Kagi loves AI bullshit and they are going to find more and more ways to use it.  If you check out their Discord and listen to founder Vlad talk about AI tools, it's clear that he will not listen to anyone saying they might be bad in any way.  To the point where he truly believes that <a href="https://twitter.com/vladquant/status/1647017761569402882">AI should be used to remove bias from news articles</a>, and show you which articles are "constructive" or "good" to view.  This is something he legitimately wants to implement and he seems completely oblivious to the fact that this is not something AI can do, that AI spits out exactly as much bias as it is fed in its model.  He's 100% a true believer in AI as unbiased. (Note in the below screenshots: freediver is Vlad's HackerNews account)</p>

		<img src="https://d-shoot.net/img/kagi/personsummary.png" alt="Discord screenshot from 12/05/2022, Vlad says 'now something like this is super useful...we can make the AI read through 50 web pages in mere seconds, after doing 5 searches (that it decides on!), to at the end produce a summary of someone based on just a twitter handle.  Total cost for this is maybe 6-7 cents but it just saved me 10 minutes?' Vlad includes a screenshot of the search for a user named @scottleibrand, which shows a ChatGPT-style generated summary with links to the sites it used, the text is a bit too small for me to transcribe."> <br>

		<img src="https://d-shoot.net/img/kagi/unbiasedreviews.png" alt="Hackernews screenshot, malikNF says ''> Kagi shopping results have no affiliate links and feature the most helpful, unbiased reviews' Wonder how they determine if a review is unbiased or not.  Any attempt at automating this process means someone else will figure a way to game the system over time.' Vlad replies (as freediver) 'Which is why we cannot say how, but please check and let us know if the reviews sound unbiased :)'"> <br>

		<img src="https://d-shoot.net/img/kagi/constructivenews.png" alt="Discord convo from 12/05/23.  Vlad: 'Real balanced news would not discriminate between left and right, but between constructive and destructive.''  ram ram: 'it just so happens the left/right divide correlates strongly with true/false and constructive destructive' the last envoy quoting ram ram: 'I agree' Vlad: 'Interesting, that is implicitly saying that one political option is the source of truth' Ram ram: 'Not at all, it's explicitly saying that one political option is supported by truth.' Vlad: 'but news should not be only about politics?' Ram ram: 'All things are political in nature.' Vlad: 'A coffee shop opening in your town?' Ram ram: 'Business licensing, source of beans, employment practices, building regulation, city ordonances, taxes, food health and safety'"> <br>

		<img src="https://d-shoot.net/img/kagi/constructivenews2.png" alt="Discord convo from 12/05/23.  Vlad: 'line of thought basically comes from here https://www.theguardian.com/media/2013/apr/12/news-is-bad-rolf-dobelli, embed for the Guardian article 'News is bad for you - and giving up reading it will make you happier. News is bad for you, it leads to fear and aggression. It hinders your creativity and makes you sick.  We should stop consuming it, says Rolf Dobelli, who's abstained for years.'  Vlad continues: 'then you ask a question how does a product that makes news constructive to your life (vs irrelevant/destructive)'"> 

		<p>It's honestly impossible to write this without discussing Vlad a little bit.  Vlad is very "my way or the highway", but is the type that will try to appear very measured and calm while completely unwilling to budge.  And he is very, very much the type that believes "not everything is political" and "we don't get into politics".  I won't get into all of the Brave stuff because many people have written about this already, but the <a href="https://kagifeedback.org/d/2808-reconsider-your-partnership-with-brave">support thread</a> about it should give you some ideas.  His personal conception of bias is a guiding factor in a lot of Kagi's decisions but it's frankly ridiculous.  For example, he has stated before that he thinks 3 star reviews on products are "by definition" unbiased, because they must include good and bad points.  Nevermind that a lot of people's reviews of the recent Star Wars films were "good space war stuff but too many minorities in it".  At one point someone <a href="https://kagifeedback.org/d/865-suicide-results-should-probably-have-a-dont-do-that-widget-like-google">suggested the idea that searching for suicide-related terms should bring up a helpline</a>, and he rejected that idea because it would be "biased" (I guess towards not wanting people to kill themselves).  But at the same time, Kagi partners with a service called <a href="https://www.looria.com/">Looria</a> to provide "unbiased reviews" on products in Kagi's shopping page.  Nevermind that unbiased reviews do not exist (there is just a difference between a sincere review and a paid advertisement), but isn't promoting certain products in search at least as biased as telling people to not commit suicide?  You're letting a third party decide what reviews your users should see.</p>

		<p>And Vlad's attitude is also where Kagi's dedication to privacy falls apart for me.  Generally, if someone brings up a security or privacy concern, Vlad's response is either "trust me bro" or "that's not actually important".  He has repeatedly stated that he feels less than 100 people on earth need full anonymity in a search engine (he has never, that I could find, explained where he got this number or idea from).  He believes that email addresses don't count as personally identifiable information, because you can simply use a burner account.  If you say that you wouldn't want Kagi using information from your theoretical Kagi Email Address in your search results, and would rather have a Proton-style privacy focused email?  He says that there's nothing to worry about, Kagi wouldn't do anything bad with your data.  If you bring up "what if Kagi gets sold to someone else?"  He says well, if they sold to someone who did something bad with your data, they'd lose all of their privacy focused customers, so clearly they'd never do that.  Basically anything where you say "I don't want someone to have this data about what I'm doing in a search engine", his reply is "well, we wouldn't do anything with this information."  A lot of questions about what information Kagi collects on people is met with either saying nothing (which isn't true, they connect your account to an email address for payments, since it's a paid service), or saying he isn't sure, or saying it doesn't matter because they won't use it anyway.  Asking what data Stripe collects on them through Kagi, and more importantly what data Stripe sends back TO Kagi, also gets you a vague "I don't know" answer.  He doesn't entertain any discussions about GDPR because he thinks they have nothing that applies anyway.  Questions about what would happen if the government tried to force him to collect information about users are just brushed away with "well we'd simply close the company", although he also notes that he has no problem with criminals being caught through their searches and doesn't want criminals using the platform.</p>

		<img src="https://d-shoot.net/img/kagi/weregood1.png" alt="Discord convo from 02/15/2024, Zack: 'That's not actually the only thing I'm asking.  Was also curious about more of the intricacies.  I would use stripe, I wouldn't opt for bitcoin anyway.'  Vlad: 'the point is we do not really want, need or care about your data.  We just need $10/mo from you to run the service.  How you provide it to use is a technical detail as far as we're concerned.  Zack:  I know that you don't want/need/care.  I was more curious about the actual reality when using stripe, that's all.  I will delve more into the stripe policies another time though!"><br>
		<img src="https://d-shoot.net/img/kagi/weregood2.png" alt="Discord convo.  Heero, replying to Vlad asking 'what is missing?': 'GDPR Data download link. Where to get the data that is collected.' Heero replying to their own post saying 'Something you could do about this false time stamps?' with 'But this is more important for me, cry laughing emoji, it's just annoying when sites cheat like that' Vlad: Kagi does not collect any personal information' Kai: eh, you technically need to provide a download link to a CSV with the email address' Vlad: our payment processor does, and you can ask stripe for that'"><br>
		<img src="https://d-shoot.net/img/kagi/weregood3.png" alt="Discord convo, Heero replying to Vlad saying 'kagi does not collect any personal information: 'You have an account, with mail and billing. This is already personal information in the sense of GDPR, winking emoji.  Heero replying to vlad saying 'Well even email address is not personal information the way kagi does it as you can enter anything', Heero says 'That's actually I think not important' Heero replies to Kai saying 'GDPR is stupid that way' with 'Yes, I agree'.  Vlad replies 'personal information is what you can be identified with as an individual.  No information you submit to kagi is personal information except if you use your real email address to register. So a data download link would download a file that contains your email address.  Heero replies 'Yes, and then you need to provide the data associated with you.  That includes personal preference settings for search optimization I would think, no expert on this though.'"><br>
		<img src="https://d-shoot.net/img/kagi/weregood4.png" alt="Discord convo, Heero replying to Vlad asking 'can you clarify what is sensitive?': 'Sensitive data is private information that, if exposed without permission, can lead to harm or misuse.  Inputting websites that should be preferred or not preferred is highly sensitive.  That shows a bit of a disconnect between data protection and you.  Though I really don't want to blame you, could be more that I actually live in germany and we take such stuff far to serious :D' Vlad: We do not collect/extract this information but the user volunteers it.  As such it is not clear how is it sensitive in terms of privacy protection.' Heero: The thread model here would be something like that: I prefer websites that are pro trans or anti trans, you see the profile and the settings i have set, you see the mail too.'"><br>
		<img src="https://d-shoot.net/img/kagi/weregood5.png" alt="Discord convo, Heero: your not really following GDPR standards.  You don't understand what sensitive data means (to be fair, I don't know if you really do :) ). Even if I can input wrong data, you can't verify that, as such you need to always take it as sensitive, personal, private data.  Also, giving you data volunteerly isnt a thing in GDPR, the source of the data is not important.  So, my concern is, I don't know what you really track, as you don't give me a copy of my data.  I don't know if your obligated to, it's just things I as european citizen expect.' Vlad: (quoting 'your not really following DPR standards') You say these things but you do not really provide evidence for it :) So again, can you clearly articulate what your concern is?  We very well know what personal information is and what GDPR was made for. (quoting 'i don't know what you really track') We have very detailed privacy policy where this is literally written.  Have you read it and still do not know what we track?'  Heero: 'Did I make it clearer know? I really want to stress that I don't have anything against you or kagi :) just trying to be constructive.'"><br>

		<img src="https://d-shoot.net/img/kagi/readingemails.png" alt="Discord convo from 03/31/2024, Gohan: Just the thought of kagi search having the ability to go through my emails would keep me far away from using the email product'  Vlad: 'as I said, it would be opt in, we will need to be able to search email as a product feature anyway' Gohan: That's why I said even having the ability to do so is a massive turn off' Vlad: exposing it in search results is a benefit, not sure why you would think that?' Gohan: 'because if you give any company an inch, they'll take a mile', Vlad: 'not Kagi, I think you may be misunderstanding how would this work and what incentives in play'"><br>

		<img src="https://d-shoot.net/img/kagi/anonymity.png" alt="Discord convo from 07/15/22, Vlad: people who really need anonymity are very rare.  Probably less than a 100 in the entire world.  Definitely not typical Kagi users.  Unless they are criminals, in which case we don't care they don't have full anonymity (nor we want them as customers)'">

		<p><i>I want to note with the above, I'm not a GDPR expert either, I don't know what counts legally and what doesn't, but I strongly disagree with Vlad's handwaving of the issue and his insistence that email addresses aren't PII because you can use a fake one, because that's true for names and almost any other information as well, that logic doesn't hold up for this, and the logic is what I have the issue with.</i></p>		

		<p>Between the absolute blase attitude towards privacy, the 100% dedication to AI being the future of search, and the completely misguided use of the company's limited funds, I honestly can't see Kagi as something I could ever recommend to people.  Is the search good?  I mean...it's not really much better than any other search, it heavily leverages Bing like DDG and the other indie search platforms do, the only real killer feature it has to me is the ability to block domains from your results, which I can currently only do in other search engines via a user script that doesn't help me on mobile.  But what good is filtering out all of the AI generated spamblogs on a search platform that wants to spit more AI generated bullshit at me directly?  Sure I can turn it off, but who's to say that they won't start using my data to fuel their own LLM?  They already have an extremely skewed idea of what counts as PII or not.  They could easily see using people's searches as being "anonymized" and decide they're fine to use, because their primary business isn't search, it's AI.  They just don't want to admit to being an AI company anymore.  Frankly, it's not something I want to pay them to keep developing.  It's something I want less of out in the world.  Do you need to quit using Kagi?  That's up to you.  I'm not really trying to debate anyone into leaving Kagi.  My only interest is to explain why my opinion shifted on it, and to share information that may or may not shift your opinions.  If they don't, it's not something I want to debate people into.  But I think most of the info here is going to be news to a lot of people, and that's the thing.  Most people aren't going to dig into the discord for a project and read what the developers have said about it over time, that's a bonkers thing to do that I did, just do what you will with my findings.  Because I know for a fact, from talking to people, that a lot of Kagi users don't know any of this.</p>
		<hr>

		<a href="https://d-shoot.net/">=&gt; Return to Home</a>
	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tinygrad: Hacked 4090 driver to enable P2P (141 pts)]]></title>
            <link>https://github.com/tinygrad/open-gpu-kernel-modules</link>
            <guid>40010819</guid>
            <pubDate>Fri, 12 Apr 2024 09:27:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tinygrad/open-gpu-kernel-modules">https://github.com/tinygrad/open-gpu-kernel-modules</a>, See on <a href="https://news.ycombinator.com/item?id=40010819">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">NVIDIA Linux Open GPU with P2P support</h2><a id="user-content-nvidia-linux-open-gpu-with-p2p-support" aria-label="Permalink: NVIDIA Linux Open GPU with P2P support" href="#nvidia-linux-open-gpu-with-p2p-support"></a></p>
<p dir="auto">This is a fork of NVIDIA's driver with P2P support added for 4090's.</p>
<p dir="auto"><code>./install.sh</code> to install if that's all you want.</p>
<p dir="auto">You may need to uninstall the driver from DKMS. Your system needs large BAR support and IOMMU off.</p>
<p dir="auto">Not sure all the cache flushes are right, please file issues on here if you find any issues.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto">Normally, P2P on NVIDIA cards uses MAILBOXP2P. This is some hardware interface designed to allow GPUs to transfer memory back in the days of small BAR. It is not present or disabled in hardware on the 4090s, and that's why P2P doesn't work. There <a href="https://forums.developer.nvidia.com/t/standard-nvidia-cuda-tests-fail-with-dual-rtx-4090-linux-box/233202" rel="nofollow">was a bug in early versions</a> of the driver that reported that it did work, and it was actually sending stuff on the PCIe bus. However, because the mailbox hardware wasn't present, these copies wouldn't go to the right place. You could even crash the system by doing something like <code>torch.zeros(10000,10000).cuda().to("cuda:1")</code></p>
<p dir="auto">In some 3090s and all 4090s, NVIDIA added large BAR support.</p>
<div data-snippet-clipboard-copy-content="tiny@tiny14:~$ lspci -s 01:00.0 -v
01:00.0 VGA compatible controller: NVIDIA Corporation AD102 [GeForce RTX 4090] (rev a1) (prog-if 00 [VGA controller])
        Subsystem: Micro-Star International Co., Ltd. [MSI] Device 510b
        Physical Slot: 49
        Flags: bus master, fast devsel, latency 0, IRQ 377
        Memory at b2000000 (32-bit, non-prefetchable) [size=16M]
        Memory at 28800000000 (64-bit, prefetchable) [size=32G]
        Memory at 28400000000 (64-bit, prefetchable) [size=32M]
        I/O ports at 3000 [size=128]
        Expansion ROM at b3000000 [virtual] [disabled] [size=512K]
        Capabilities: <access denied>
        Kernel driver in use: nvidia
        Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia"><pre><code>tiny@tiny14:~$ lspci -s 01:00.0 -v
01:00.0 VGA compatible controller: NVIDIA Corporation AD102 [GeForce RTX 4090] (rev a1) (prog-if 00 [VGA controller])
        Subsystem: Micro-Star International Co., Ltd. [MSI] Device 510b
        Physical Slot: 49
        Flags: bus master, fast devsel, latency 0, IRQ 377
        Memory at b2000000 (32-bit, non-prefetchable) [size=16M]
        Memory at 28800000000 (64-bit, prefetchable) [size=32G]
        Memory at 28400000000 (64-bit, prefetchable) [size=32M]
        I/O ports at 3000 [size=128]
        Expansion ROM at b3000000 [virtual] [disabled] [size=512K]
        Capabilities: &lt;access denied&gt;
        Kernel driver in use: nvidia
        Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia
</code></pre></div>
<p dir="auto">Notice how BAR1 is size 32G. In H100, they also added support for a PCIe mode that uses the BAR directly instead of the mailboxes, called BAR1P2P. So, what happens if we try to enable that on a 4090?</p>
<p dir="auto">We do this by bypassing the HAL and calling a bunch of the GH100 methods directly. Methods like <code>kbusEnableStaticBar1Mapping_GH100</code>, which maps the entire VRAM into BAR1. This mostly just works, but we had to disable the use of that region in the <code>MapAperture</code> function for some reason. Shouldn't matter.</p>
<div data-snippet-clipboard-copy-content="[ 3491.654009] NVRM: kbusEnableStaticBar1Mapping_GH100: Static bar1 mapped offset 0x0 size 0x5e9200000
[ 3491.793389] NVRM: kbusEnableStaticBar1Mapping_GH100: Static bar1 mapped offset 0x0 size 0x5e9200000"><pre><code>[ 3491.654009] NVRM: kbusEnableStaticBar1Mapping_GH100: Static bar1 mapped offset 0x0 size 0x5e9200000
[ 3491.793389] NVRM: kbusEnableStaticBar1Mapping_GH100: Static bar1 mapped offset 0x0 size 0x5e9200000
</code></pre></div>
<p dir="auto">Perfect, we now have the VRAM mapped. However, it's not that easy to get P2P. When you run <code>./simpleP2P</code> from <code>cuda-samples</code>, you get this error.</p>
<div data-snippet-clipboard-copy-content="[ 3742.840689] NVRM: kbusCreateP2PMappingForBar1P2P_GH100: added PCIe BAR1 P2P mapping between GPU2 and GPU3
[ 3742.840762] NVRM: kbusCreateP2PMappingForBar1P2P_GH100: added PCIe BAR1 P2P mapping between GPU3 and GPU2
[ 3742.841089] NVRM: nvAssertFailed: Assertion failed: (shifted >> pField->shift) == value @ field_desc.h:272
[ 3742.841106] NVRM: nvAssertFailed: Assertion failed: (shifted &amp; pField->maskPos) == shifted @ field_desc.h:273
[ 3742.841281] NVRM: nvAssertFailed: Assertion failed: (shifted >> pField->shift) == value @ field_desc.h:272
[ 3742.841292] NVRM: nvAssertFailed: Assertion failed: (shifted &amp; pField->maskPos) == shifted @ field_desc.h:273
[ 3742.865948] NVRM: GPU at PCI:0000:01:00: GPU-49c7a6c9-e3a8-3b48-f0ba-171520d77dd1
[ 3742.865956] NVRM: Xid (PCI:0000:01:00): 31, pid=21804, name=simpleP2P, Ch 00000013, intr 00000000. MMU Fault: ENGINE CE3 HUBCLIENT_CE1 faulted @ 0x7f97_94000000. Fault is of type FAULT_INFO_TYPE_UNSUPPORTED_KIND ACCESS_TYPE_VIRT_WRITE"><pre><code>[ 3742.840689] NVRM: kbusCreateP2PMappingForBar1P2P_GH100: added PCIe BAR1 P2P mapping between GPU2 and GPU3
[ 3742.840762] NVRM: kbusCreateP2PMappingForBar1P2P_GH100: added PCIe BAR1 P2P mapping between GPU3 and GPU2
[ 3742.841089] NVRM: nvAssertFailed: Assertion failed: (shifted &gt;&gt; pField-&gt;shift) == value @ field_desc.h:272
[ 3742.841106] NVRM: nvAssertFailed: Assertion failed: (shifted &amp; pField-&gt;maskPos) == shifted @ field_desc.h:273
[ 3742.841281] NVRM: nvAssertFailed: Assertion failed: (shifted &gt;&gt; pField-&gt;shift) == value @ field_desc.h:272
[ 3742.841292] NVRM: nvAssertFailed: Assertion failed: (shifted &amp; pField-&gt;maskPos) == shifted @ field_desc.h:273
[ 3742.865948] NVRM: GPU at PCI:0000:01:00: GPU-49c7a6c9-e3a8-3b48-f0ba-171520d77dd1
[ 3742.865956] NVRM: Xid (PCI:0000:01:00): 31, pid=21804, name=simpleP2P, Ch 00000013, intr 00000000. MMU Fault: ENGINE CE3 HUBCLIENT_CE1 faulted @ 0x7f97_94000000. Fault is of type FAULT_INFO_TYPE_UNSUPPORTED_KIND ACCESS_TYPE_VIRT_WRITE
</code></pre></div>
<p dir="auto">Failing with an MMU fault. So you dive into this and find that it's using <code>GMMU_APERTURE_PEER</code> as the mapping type. That doesn't seem supported in the 4090. So let's see what types are supported, <code>GMMU_APERTURE_VIDEO</code>,<code>GMMU_APERTURE_SYS_NONCOH</code>, and <code>GMMU_APERTURE_SYS_COH</code>. We don't care about being coherent with the CPU's L2 cache, but it does have to go out the PCIe bus, so we rewrite <code>GMMU_APERTURE_PEER</code> to <code>GMMU_APERTURE_SYS_NONCOH</code>. We also no longer set the peer id that was corrupting the page table.</p>
<div data-snippet-clipboard-copy-content="cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.21GB/s
Preparing host buffer and memcpy to GPU0...
Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...
Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...
Copy data back to host from GPU0 and verify results...
Verification error @ element 1: val = 0.000000, ref = 4.000000
Verification error @ element 2: val = 0.000000, ref = 8.000000"><pre><code>cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.21GB/s
Preparing host buffer and memcpy to GPU0...
Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...
Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...
Copy data back to host from GPU0 and verify results...
Verification error @ element 1: val = 0.000000, ref = 4.000000
Verification error @ element 2: val = 0.000000, ref = 8.000000
</code></pre></div>
<p dir="auto">Progress! <code>./simpleP2P</code> appears to work, however the copy isn't happening. The address is likely wrong. It turns out they have a separate field for the peer address called <code>fldAddrPeer</code>, we change that to <code>fldAddrSysmem</code>. We also print out the addresses and note that the physical BAR address isn't being added properly, they provide a field <code>fabricBaseAddress</code> for <code>GMMU_APERTURE_PEER</code>, we reuse it and put the <code>BAR1</code> base address in there.</p>
<p dir="auto">That's it. Thanks to NVIDIA for writing such a stable driver. And with this, the tinybox green is even better.</p>
<p dir="auto">~ the tiny corp</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Functional</h2><a id="user-content-functional" aria-label="Permalink: Functional" href="#functional"></a></p>
<div data-snippet-clipboard-copy-content="Enabling peer access between GPU0 and GPU1...
Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...
Creating event handles...
cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.44GB/s
Preparing host buffer and memcpy to GPU0...
Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...
Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...
Copy data back to host from GPU0 and verify results...
Disabling peer access...
Shutting down...
Test passed"><pre><code>Enabling peer access between GPU0 and GPU1...
Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...
Creating event handles...
cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.44GB/s
Preparing host buffer and memcpy to GPU0...
Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...
Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...
Copy data back to host from GPU0 and verify results...
Disabling peer access...
Shutting down...
Test passed
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Fast</h2><a id="user-content-fast" aria-label="Permalink: Fast" href="#fast"></a></p>
<div data-snippet-clipboard-copy-content="Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5
     0 919.39  50.11  50.15  51.22  50.59  51.22
     1  50.19 921.29  50.31  51.21  50.62  51.22
     2  50.23  50.55 921.83  51.22  50.39  51.22
     3  50.33  50.65  51.20 920.20  50.43  51.22
     4  50.18  50.68  50.26  51.22 922.30  51.23
     5  50.12  50.09  50.44  51.22  51.21 921.29"><pre><code>Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)
   D\D     0      1      2      3      4      5
     0 919.39  50.11  50.15  51.22  50.59  51.22
     1  50.19 921.29  50.31  51.21  50.62  51.22
     2  50.23  50.55 921.83  51.22  50.39  51.22
     3  50.33  50.65  51.20 920.20  50.43  51.22
     4  50.18  50.68  50.26  51.22 922.30  51.23
     5  50.12  50.09  50.44  51.22  51.21 921.29
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">And NCCL (aka torch) compatible!</h2><a id="user-content-and-nccl-aka-torch-compatible" aria-label="Permalink: And NCCL (aka torch) compatible!" href="#and-nccl-aka-torch-compatible"></a></p>
<div data-snippet-clipboard-copy-content="tiny@tiny14:~/build/nccl-tests/build$ ./all_reduce_perf -g 6
# nThread 1 nGpus 6 minBytes 33554432 maxBytes 33554432 step: 1048576(bytes) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0
#
# Using devices
#  Rank  0 Group  0 Pid  26230 on     tiny14 device  0 [0x01] NVIDIA GeForce RTX 4090
#  Rank  1 Group  0 Pid  26230 on     tiny14 device  1 [0x42] NVIDIA GeForce RTX 4090
#  Rank  2 Group  0 Pid  26230 on     tiny14 device  2 [0x81] NVIDIA GeForce RTX 4090
#  Rank  3 Group  0 Pid  26230 on     tiny14 device  3 [0x82] NVIDIA GeForce RTX 4090
#  Rank  4 Group  0 Pid  26230 on     tiny14 device  4 [0xc1] NVIDIA GeForce RTX 4090
#  Rank  5 Group  0 Pid  26230 on     tiny14 device  5 [0xc2] NVIDIA GeForce RTX 4090
#
#                                                              out-of-place                       in-place
#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
    33554432       8388608     float     sum      -1   2275.1   14.75   24.58      0   2282.5   14.70   24.50      0
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 24.5413
#"><pre><code>tiny@tiny14:~/build/nccl-tests/build$ ./all_reduce_perf -g 6
# nThread 1 nGpus 6 minBytes 33554432 maxBytes 33554432 step: 1048576(bytes) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0
#
# Using devices
#  Rank  0 Group  0 Pid  26230 on     tiny14 device  0 [0x01] NVIDIA GeForce RTX 4090
#  Rank  1 Group  0 Pid  26230 on     tiny14 device  1 [0x42] NVIDIA GeForce RTX 4090
#  Rank  2 Group  0 Pid  26230 on     tiny14 device  2 [0x81] NVIDIA GeForce RTX 4090
#  Rank  3 Group  0 Pid  26230 on     tiny14 device  3 [0x82] NVIDIA GeForce RTX 4090
#  Rank  4 Group  0 Pid  26230 on     tiny14 device  4 [0xc1] NVIDIA GeForce RTX 4090
#  Rank  5 Group  0 Pid  26230 on     tiny14 device  5 [0xc2] NVIDIA GeForce RTX 4090
#
#                                                              out-of-place                       in-place
#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
    33554432       8388608     float     sum      -1   2275.1   14.75   24.58      0   2282.5   14.70   24.50      0
# Out of bounds values : 0 OK
# Avg bus bandwidth    : 24.5413
#
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An open source initiative to share and compare heat pump performance data (243 pts)]]></title>
            <link>https://heatpumpmonitor.org/</link>
            <guid>40010615</guid>
            <pubDate>Fri, 12 Apr 2024 08:53:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://heatpumpmonitor.org/">https://heatpumpmonitor.org/</a>, See on <a href="https://news.ycombinator.com/item?id=40010615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                    <h3 v-if="mode=='user'">My Systems</h3>
                    <h3 v-if="mode=='admin'">Admin Systems</h3>

                    <p v-if="mode=='user'">Add, edit and view systems associated with your account.</p>
                    <p v-if="mode=='admin'">Add, edit and view all systems.</p>
                    
                    <p v-if="mode=='public' &amp;&amp; showContent">Here you can see a variety of installations monitored with OpenEnergyMonitor, and compare detailed statistics to see how performance can vary.</p>
                    <p v-if="mode=='public' &amp;&amp; showContent">If you're monitoring a heat pump with <b>emoncms</b> and the My Heat Pump app, <a href="https://heatpumpmonitor.org//user/login">login</a> to add your details.</p>
                    <p v-if="mode=='public' &amp;&amp; showContent">To join in with discussion of the results, or for support please use the <a href="https://community.openenergymonitor.org/tag/heatpumpmonitor">OpenEnergyMonitor forums.</a></p> 
                    
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Webb – Space Telescope Live. What Is Webb Observing Now? (147 pts)]]></title>
            <link>https://spacetelescopelive.org/webb?obsId=01HTJT20C0STKNZ01KQYGEKBQ1</link>
            <guid>40010221</guid>
            <pubDate>Fri, 12 Apr 2024 07:29:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spacetelescopelive.org/webb?obsId=01HTJT20C0STKNZ01KQYGEKBQ1">https://spacetelescopelive.org/webb?obsId=01HTJT20C0STKNZ01KQYGEKBQ1</a>, See on <a href="https://news.ycombinator.com/item?id=40010221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main data-testid="telescope-view" id="main-content"><div><header><div><div><h2>What is Webb observing?</h2></div><div><p><img alt="Line drawing of the Webb Space Telescope" loading="lazy" width="85" height="68" decoding="async" data-nimg="1" src="https://spacetelescopelive.org/img/webb-vector.svg"></p><div><h2>Target: <span>Not applicable</span></h2><p><strong>Target Category: </strong>Not applicable</p><p><strong>Research Program: </strong>Not applicable</p></div><nav data-testid="prev-next--desktop"></nav></div></div><div><h2>What is Webb observing?</h2><p><strong>Target:  </strong><span>Not applicable</span></p><p><strong>Target Category: </strong>Not applicable</p><p><strong>Research Program: </strong>Not applicable</p></div><div><h3>Observation Status &amp; Details</h3></div><p><span><span>Planned Outage</span><span><span>On Friday, April 12th starting at noon through Sunday, April 14th, Space Telescope Live may be unavailable. We apologize for any inconvenience. </span></span></span></p></header><h3>Sky Map</h3><div data-testid="telescope-view__aladin"><h3>Sky Map Details</h3><p><strong>Background: </strong><span>Two Micron All Sky Survey</span></p><p><strong>Field of View: </strong><span data-testid="fov-label">30 arcminutes</span></p><p><strong>Coordinates:</strong><span data-testid="fov-label">+00 00 00.00 +00 00 00.0</span></p></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A 30-year old Rabbit Telepoint base station at Seven Sisters tube station (178 pts)]]></title>
            <link>https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/</link>
            <guid>40009856</guid>
            <pubDate>Fri, 12 Apr 2024 06:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/">https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/</a>, See on <a href="https://news.ycombinator.com/item?id=40009856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<!-- Article Start -->
					
<p>For over thirty years, a dead rabbit has hung inside Seven Sisters tube station, and thousands of people walk past it every day without noticing.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg.webp 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg.webp 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg.webp 1800w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img fetchpriority="high" decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg" alt="" width="605" height="336" loading="none" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg 1800w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>The dead rabbit is a legacy of an early form of mobile phone technology, albeit one that lasted less than two years before closing down.</p>
<p>We need to jump back to 1989, when the government awarded four licenses to operate Telepoint services, with the aim that their lower costs would offer competition to the country’s two mobile networks – Cellnet (now O2) and Vodafone.</p>
<p>At a time when the two mobile networks had 500,000 customers between them, it was <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0000540%2F19890127%2F238%2F0015">expected</a> these Telepoint phones would have as many as seven million customers by the middle of the 1990s.</p>
<p>Rabbit was created by the Hong-Kong based conglomerate, Hutchison, which didn’t have a license to operate a telepoint service, so they bought one of the four companies that did. However, by the time the company was ready to launch Rabbit to the world, the other three companies — Mercury Callpoint, Ferranti’s Zonephone and BT’s Phonepoint — had already <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0003740%2F19911003%2F011%2F0011">closed down</a>.</p>
<p>Rabbit pushed on and, with a huge marketing blitz, finally started appearing in the shops in May 1992.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg.webp 611w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-358x600.jpg.webp 358w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-768x1288.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-916x1536.jpg.webp 916w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-100x168.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-150x252.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-200x335.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-300x503.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-450x755.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-600x1006.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-900x1509.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg.webp 1052w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg" alt="" width="605" height="1014" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg 611w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-358x600.jpg 358w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-768x1288.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-916x1536.jpg 916w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-100x168.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-150x252.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-200x335.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-300x503.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-450x755.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-600x1006.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-900x1509.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg 1052w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>Despite the fact that it could only make calls within 100 yards of a base station and couldn’t receive calls (you could get a pager telling you to call someone) — in theory, it should have been a success.</p>


<p>At the time, the two mobile networks were for business people and <a href="https://en.wikipedia.org/wiki/Yuppie">Yuppies</a>, and were very expensive to use, so even the restrictive Rabbit phones would have an edge as they were much cheaper to make calls on – typically 50p a minute on a basic cellphone tariff vs 20p a minute on Rabbit. And cheaper monthly rentals.</p>
<p>The other factor is that the Rabbit phones could also be used at home.</p>
<p>Many homes had a <a href="https://www.britishtelephones.com/cordles3.htm">cordless phone</a>, but these domestic analogue radio-based cordless phones were large and frankly pretty poor quality, with calls often dropping out. The Rabbit phones came with a home base station and used early digital phone technologies, so the sound quality was considerably better. And the handset was much smaller than the home cordless phone.</p>
<p>So, when Rabbit hopped into the shops in May 1992, sales were expected to be brisk, and indeed, they were. Initially.</p>
<p>I worked in a shop selling them, and from memory the mobile calling function was only about half the appeal — it was the substantially better home phone function that really appealed to people when they heard about it from friends.</p>
<p>Rabbit launched with base stations on shop fronts and lamposts across Manchester and quickly spread, reaching nationwide coverage by late 1993.</p>
<p>Then it abruptly closed down.</p>
<p>On a day to remember, Friday 5th November, Hutchison Telecom suddenly <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0005278%2F19931107%2F714%2F0050">announced</a> that it was shutting down the Rabbit service. Hutchison Telecom had recently been granted a license to build a full mobile network and wasn’t interested in its old Telepoint based service anymore.</p>


<p>Customers who owned a Rabbit phone were given a refund, and a promise of a discount on Hutchison’s replacement, a GSM based mobile phone network — Orange (now EE).</p>
<p>The bunny had flopped. The future was Orange.</p>
<p>The announcement that Rabbit was closing down sparked something totally unexpected — a surge in sales.</p>
<p>Remember how I mentioned that the phones worked at home as a replacement for the clunky cordless phone — well, that wasn’t dependent on the Rabbit base stations, so the handsets would keep on working in the home after the mobile network was shut down.</p>
<p>And people really liked the Rabbit phone for their homes.</p>
<p>Awkwardly, customers offered a refund had to return the handsets, and many chose not to, while shops were told to return unsold stock to the warehouses, and many chose not to.</p>
<p>Hutchison’s attempt to kill the rabbit dead was struggling for life.</p>
<p>In the end, shops ran out of rabbit stock, and in December 1993, the Rabbit base stations switched off forever.</p>
<p>However, like many items of street furniture, what is installed in haste can take ages to remove at leisure. Unless the location is needed for something else, old signs and clutter can linger on for years, decades even.</p>
<p>So, inside <a href="https://www.ianvisits.co.uk/articles/tag/seven-sisters-station/">Seven Sisters tube station</a>, there’s still a Rabbit base station sitting on the wall, more than 30 years after it last broadcast a radio signal.</p>
<p>If you want to find the dead rabbit, just hop down to Seven Sisters tube station’s High Road entrance ticket hall. You can find it right next to the escalators to the Victoria line.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg.webp 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg.webp 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg.webp 1800w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg" alt="" width="605" height="336" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg 1800w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>There’s also another one, in <a href="https://twitter.com/ianvisits/status/1695383779659948112">rather better condition</a>, <span>in the waiting room on platform 7/8 at Watford Junction station.</span></p>




										


								
					



					

									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a Linux Container Using Namespaces: Part – 1 (2020) (161 pts)]]></title>
            <link>https://www.polarsparc.com/xhtml/Containers-1.html</link>
            <guid>40008841</guid>
            <pubDate>Fri, 12 Apr 2024 02:22:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.polarsparc.com/xhtml/Containers-1.html">https://www.polarsparc.com/xhtml/Containers-1.html</a>, See on <a href="https://news.ycombinator.com/item?id=40008841">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <br>
    
    
    <p>Building a Linux Container using Namespaces :: Part - 1</p>
    <br>
    
    <hr> 
    <p>Overview</p>
    <div id="para-div">
      <p>Ever wondered how Linux <span>Container</span>s worked ???</p>
      <p>Currently, <a href="https://www.polarsparc.com/xhtml/Docker.html" target="_blank"><span>Docker</span></a>
        is one of the most popular and prevalent container implementations.</p>
      <p>Containers run on top of the same Operating System kernel, but isolate the application processes running inside them
        from one another. One of the secret sauces behind containers is <a href="http://man7.org/linux/man-pages/man7/namespaces.7.html" target="_blank"><span>Namespaces</span></a>.</p>
      <p>A <span>Namespace</span> abstracts global system resources, such as, host names, user IDs, group IDs,
        process IDs, network ports, etc., in a way that it appears to the processes (within the namespace) as though they have
        their own isolated instance of the global system resources. One of the primary goals of namespaces is to support the
        implementation of containers (lightweight virtualization).</p>
      <p>Currently, in Linux there are <span>6</span> types of namespaces - <span>IPC</span>,
        <span>Network</span>, <span>Mount</span>, <span>PID</span>,
        <span>User</span>, and <span>UTS</span>.</p>
    </div>
    <div id="para-div">
      <p>The following are brief descriptions for each of the namespaces:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span>IPC</span> :: This namespace isolates certain interprocess communication (IPC) resources,
            namely, Message Queues, Semaphores, and Shared Memory</p>
        </li>
        <li>
          <p><span>Network</span> :: This namespace provides isolation of the system resources associated with
            networking, such as, Network devices, IP addresses, IP routing tables, /proc/net directory, port numbers, and so on</p>
        </li>
        <li>
          <p><span>Mount</span> :: This namespace isolates the set of filesystem mount points seen by a group
            of processes. Processes in different mount namespaces can have different views of the filesystem hierarchy</p>
        </li>
        <li>
          <p><span>PID</span> :: This namespace isolates the process ID number space. This allows processes
            in different PID namespaces to have the same PID</p>
        </li>
        <li>
          <p><span>User</span> :: This namespace isolates the user and group ID number spaces, such that, a
            process's user and group IDs can be different inside and outside the user namespace</p>
        </li>
        <li>
          <p><span>UTS</span> :: This namespace isolates two system identifiers — the hostname and the
            domainname. For containers, the UTS namespaces allows each container to have its own hostname and NIS domain name</p>
        </li>
      </ul>
      <p>For the demonstration in this article, we will be using the <span>unshare</span> Linux command
          as well as implement, build, and execute a simple container using <span>golang</span>.</p>
    </div>
    <p>Installation and Setup</p>
    <p>The installation is on a <span>Ubuntu 18.04 LTS</span> based Linux desktop.</p>
    <div id="para-div">
      <p>We will need two commands <span>newuidmap</span> and <span>newgidmap</span>
        to demonstrate <span>User</span> namespace. For this, we need to install the package
        <span>uidmap</span>.</p>
      <p>To install the package <span>uidmap</span>, execute the following command:</p>
    </div>
    <p>$ sudo apt install -y uidmap</p>
    <div id="para-div">
      <p>Next, we will need the <span>brctl</span> command to create a <span>bridge</span>
        network interface. For this, we need to install the package <span>bridge-utils</span>.</p>
      <p>To install the package <span>bridge-utils</span>, execute the following command:</p>
    </div>
    <p>$ sudo apt install -y bridge-utils</p>
    <div id="para-div">
      <p>To develop, build, and execute the simple container in <span>go</span> programming language, we
        need to install the <span>golang</span> package.</p>
      <p>To check the version of <span>golang</span> available to install, execute the following command:</p>
    </div>
    <p>$ sudo apt-cache policy golang</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>golang:
  Installed: (none)
  Candidate: 2:1.13~1ubuntu1ppa1~bionic
  Version table:
 *** 2:1.13~1ubuntu1ppa1~bionic 500
        500 http://ppa.launchpad.net/hnakamur/golang-1.13/ubuntu bionic/main amd64 Packages
        500 http://ppa.launchpad.net/hnakamur/golang-1.13/ubuntu bionic/main i386 Packages
        100 /var/lib/dpkg/status
     2:1.10~4ubuntu1 500
        500 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages</pre>
    </div>
    <p>To install <span>golang</span>, execute the following command:</p>
    <p>$ sudo apt install -y golang</p>
    <p>The above installation procedure installs <span>golang</span> from the official Ubuntu repository.</p>
    <p>Create a directory for developing, building, and running <span>go</span> programs by executing the
        following commands:</p>
    <div id="cmd-div">
      <p>$ mkdir $HOME/projects/go</p>
      <p>$ export GOPATH=$HOME/projects/go</p>
    </div>
    
    <p>We will need one of the popular <span>go</span> packages on <span>netlink</span>
        for networking.</p>
    <p>To download the <span>go</span> package, execute the following command:</p>
    <p>$ go get github.com/vishvananda/netlink</p>
    <p>Open two <span>Terminal</span> windows - we will refer to them as <span>TA</span>
        and <span>TB</span> respectively. <span>TB</span> is where we will demonstrate the
        simple container.</p>
    <div id="para-div">
      <p>We need to download a minimal root filesystem (<span>rootfs</span>) that will be used as the base
        image for the simple container. For our demonstration, we will choose the latest <span>
        <a href="http://cdimage.ubuntu.com/ubuntu-base/releases/18.04.4/release/ubuntu-base-18.04.4-base-amd64.tar.gz" target="_blank"><span>Ubuntu Base 18.04.4 LTS</span></a></span> at the time of this article.</p>
      <p>We will assume the latest Ubuntu Base is downloaded to the directory <span>$HOME/Downloads</span>.</p>
    </div>
    <p>Hands-on with Namespaces</p>
    <p>UTS Namespace</p>
    <p>The <span>unshare</span> command executes the specified program with the indicated namespace(s)
        isolated from the parent process.</p>
    <p>To display the options for the <span>unshare</span> command, execute the following command in
        <span>TA</span>:</p>
    <p>$ unshare -h</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>Usage:
 unshare [options] [&lt;program&gt; [&lt;argument&gt;...]]

Run a program with some namespaces unshared from the parent.

Options:
 -m, --mount[=&lt;file&gt;]      unshare mounts namespace
 -u, --uts[=&lt;file&gt;]        unshare UTS namespace (hostname etc)
 -i, --ipc[=&lt;file&gt;]        unshare System V IPC namespace
 -n, --net[=&lt;file&gt;]        unshare network namespace
 -p, --pid[=&lt;file&gt;]        unshare pid namespace
 -U, --user[=&lt;file&gt;]       unshare user namespace
 -C, --cgroup[=&lt;file&gt;]     unshare cgroup namespace
 -f, --fork                fork before launching &lt;program&gt;
     --mount-proc[=&lt;dir&gt;]  mount proc filesystem first (implies --mount)
 -r, --map-root-user       map current user to root (implies --user)
     --propagation slave|shared|private|unchanged
                           modify mount propagation in mount namespace
 -s, --setgroups allow|deny  control the setgroups syscall in user namespaces

 -h, --help                display this help
 -V, --version             display version</pre>
    </div>
    <p>Each process (with [PID]) has associated with it a sub-directory <span>/proc/[PID]/ns</span> that
        contains one entry for each of the namespaces.</p>
    <p>To list all the namespaces associated with a process, execute the following command in <span>TA</span>
        :</p>
    <p>$ ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>total 0
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 alice alice 0 Mar  7 20:41 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 user -&gt; 'user:[4026531837]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 uts -&gt; 'uts:[4026531838]'</pre>
    </div>
    <p>To launch a simple container whose host name is isolated from the parent host name, execute the following command in
        <span>TB</span>:</p>
    <p>$ sudo unshare -u /bin/sh</p>
    <div id="para-div">
      <p>The <span>-u</span> option enables the <span>UTS</span> namespace.</p>
      <p>The command prompt will change to a <span>#</span>.</p>
    </div>
    <p>To check the <span>PID</span> of the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># echo $$</p>
    <p>The following would be a typical output:</p>
    
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar  7 12:36 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 user -&gt; 'user:[4026531837]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 uts -&gt; 'uts:[4026533064]'</pre>
    </div>
    <p>Comparing Output.5 to Output.3, we see a change in the <span>uts</span> namespace, which is expected
        and correct.</p>
    <p>To change the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p># hostname leopard</p>
    <p>To display the host name of the parent host, execute the following command in <span>TA</span>:</p>
    <p>$ hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p># hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>This demonstrates to us that we have isolated the host name of the simple container from the parent host name.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span> namespace isolation using the following <span>
        go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.1</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
       panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS,
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>The <span>Command</span> function from the <span>exec</span> package allows one
        to run the specified command (1st parameter) with the supplied arguments (2nd parameter). It returns an instance of
        the <span>Cmd</span> struct.</p>
      <p>One can set the standard input (<span>os.Stdin</span>), the standard output <span>
        os.Stdout</span>, the standard error <span>os.Stderr</span>, and some operating system specific
        attributes on the returned <span>Cmd</span> instance. In this case, we specify the <span>
        syscall.CLONE_NEWUTS</span> OS attribute to indicate the command be run in a new <span>UTS</span>
        namespace.</p>
      <p><span>IMPORTANT</span> : When the <span>main</span> process starts, it internally 
        spawns another <span>main</span> process (with the <span>CLONE</span> argument) in a new
        namespace. It is this spawned <span>main</span> process (running in the new namespace) that is overlayed
        (<span>syscall.Exec</span>) with the shell command by invoking the function <span>
        execContainerShell</span>.</p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/uts</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/uts</p>
      <p>$ cd $GOPATH/uts</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>2020/03/07 12:49:11 Starting process ./main with args: [./main]
2020/03/07 12:49:11 Ready to run command ...
2020/03/07 12:49:11 Starting process ./main with args: [./main CLONE]
2020/03/07 12:49:11 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the <span>UTS</span> namespace using both
        the <span>unshare</span> command and a simple <span>go</span> program.</p>
    <p>User Namespace</p>
    <div id="para-div">
      <p>Let us layer the <span>User</span> namespace on top of the <span>UTS</span> namespace.</p>
      <p>To launch a simple container whose user/group IDs as well as the host name are isolated from the parent namespace,
        execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ sudo unshare -uU /bin/sh</p>
    <div id="para-div">
      <p>The <span>-U</span> option enables the <span>User</span> namespace.</p>
      <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.10</h4>
      <pre>uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup)</pre>
    </div>
    <p>When a <span>User</span> namespace is created, it starts without a mapping for the user/group IDs in
        the new namespace to the parent user/group IDs. The unmapped user/group ID is assigned the default value of the
        overflow user/group ID. The default value for the overflow user ID is read from
        <span>/proc/sys/kernel/overflowuid</span> (which is 65534). Similarly, the default value for the
        overflow group ID is read from <span>/proc/sys/kernel/overflowgid</span> (which is 65534).</p>
    <p>To fix the mapping for the user/group ID to the parent user/group ID, exit the simple container by executing the
        following command in <span>TB</span>:</p>
    <p>$ exit</p>
    <p>To re-launch the simple container with the current effective user/group ID mapped to the superuser user/group ID
        in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>$ sudo unshare -uUr /bin/sh</p>
    <div id="para-div">
      <p>The <span>-r</span> option enables the mapping of the user/group IDs in the new namespace to the
        parent namespace user/group IDs.</p>
      <p>The command prompt will change to a <span>#</span>.</p>
    </div>
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p># id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.11</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.12</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 7 13:09 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 user -&gt; 'user:[4026532892]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 uts -&gt; 'uts:[4026533401]'</pre>
    </div>
    <p>Comparing Output.12 to Output.3, we see a change in both the <span>uts</span> namespace as well as
        the <span>user</span> namespace, which is what is expected and correct.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span> and <span>User</span> namespace isolation
        using the following <span>go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.2</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
        panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
        os.Exit(0)
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWUSER,
        UidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
        GidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>As indicated previously, the <span>Command</span> function returns an instance of the
        <span>Cmd</span> struct.</p>
      <p>In this example, we specify the additional <span>syscall.CLONE_NEWUSER</span> OS attribute
        to indicate the command be run in a new <span>User</span> namespace.</p>
      <p>In addition, we set the user ID map <span>UidMappings</span> as an array of
        <span>syscall.SysProcIDMap</span> struct entries, each consisting of the user ID mapping in
        the container (<span>ContainerID</span>) to the user ID in the host namespace
        (<span>HostID</span>). In this case, we map the <span>root</span> user ID
        <span>0</span> in the container to the <span>root</span> user ID <span>
        0</span> of the host namespace. Similarly, we set the group ID map <span>GidMappings</span></p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/user</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/user</p>
      <p>$ cd $GOPATH/user</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.13</h4>
      <pre>2020/03/07 13:17:02 Starting process ./main with args: [./main]
2020/03/07 13:17:02 Ready to run command ...
2020/03/07 13:17:02 Starting process ./main with args: [./main CLONE]
2020/03/07 13:17:02 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>-&gt; id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.14</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p>-&gt; ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.15</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 13 21:17 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 user -&gt; 'user:[4026532666]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 uts -&gt; 'uts:[4026532723]'</pre>
    </div>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span> and
        <span>User</span> namespaces using both the <span>unshare</span> command and a simple
        <span>go</span> program.</p>
    <p>PID Namespace</p>
    <div id="para-div">
      <p>Let us now layer the <span>PID</span> namespace on top of the <span>User</span> namespace
        and the <span>UTS</span> namespace.</p>
      <p>To launch a simple container whose process IDs as well as the user/group IDs and the host name are isolated from
        the parent namespace, execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ sudo unshare -uUrpf --mount-proc /bin/sh</p>
    <div id="para-div">
      <p>The <span>-p</span> option enables the <span>PID</span> namespace.</p>
      <p>The <span>-f</span> option enables spawning (or forking) of new processes in the new namespace.</p>
      <p>The <span>--mount-proc</span> option mounts the <span>proc</span> filesystem as
        a private mount at <span>/proc</span> in the new namespace. This means the <span>
        /proc</span> pseudo directory only shows information only about processes within that <span>PID</span>
        namespace.</p>
    </div>
    <div id="error-div">
      <h4>ATTENTION</h4>
      <pre><span>Ensure</span> the option <span>-f</span> is *<span>SPECIFIED</span>*. Else will encounter the following error:<p><span>/bin/sh: 4: Cannot fork</span></p></pre>
    </div>
    <p>The command prompt will change to a <span>#</span>.</p>
    <p>To display all the processes in the new namespace, execute the following command in <span>TB</span>:</p>
    <p># ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.17</h4>
      <pre>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   4628   880 pts/1    S    09:08   0:00 /bin/sh
root         6  0.0  0.0  37368  3340 pts/1    R+   09:12   0:00 ps -fu</pre>
    </div>
    <p>To display all the processes in the parent namespace, execute the following command in <span>TA</span>:</p>
    <p>$ ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.18</h4>
      <pre>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
polarsparc  8695  0.0  0.0  22840  5424 pts/1    Ss   08:43   0:00 bash
polarsparc  8681  0.0  0.0  22708  5096 pts/0    Ss   08:43   0:00 bash
polarsparc  9635  0.0  0.0  37368  3364 pts/0    R+   09:12   0:00  \_ ps -fu</pre>
    </div>
    <p>Comparing Output.17 to Output.18, we see the isolation between the new namespace and the parent namespace, which is
        what is expected and correct.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span>, <span>User</span>, and <span>
        PID</span> namespace isolation using the following <span>go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.3</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
        panic(err)
    }
    
    if err := syscall.Mount("proc", "/proc", "proc", 0, ""); err != nil {
        panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
        os.Exit(0)
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWUSER | syscall.CLONE_NEWNS | syscall.CLONE_NEWPID,
        UidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
        GidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>As indicated previously, the <span>Command</span> function returns an instance of the
        <span>Cmd</span> struct.</p>
      <p>In this example, we specify the additional <span>syscall.CLONE_NEWNS</span> and
        <span>syscall.CLONE_NEWPID</span> OS attributes to indicate the command be run in a new
        <span>PID</span> namespace.</p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/pid</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/pid</p>
      <p>$ cd $GOPATH/pid</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.19</h4>
      <pre>2020/03/07 13:38:02 Starting process ./main with args: [./main]
2020/03/07 13:38:02 Ready to run command ...
2020/03/07 13:38:02 Starting process ./main with args: [./main CLONE]
2020/03/07 13:38:02 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>-&gt; id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.21</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To display all the processes in the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.22</h4>
      <pre>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   4628   776 pts/1    S    09:41   0:00 
root         6  0.0  0.0  37368  3400 pts/1    R+   09:41   0:00 ps -fu</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p>-&gt; ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.23</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 14 09:44 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 mnt -&gt; 'mnt:[4026532366]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 pid -&gt; 'pid:[4026532368]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 pid_for_children -&gt; 'pid:[4026532368]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 user -&gt; 'user:[4026532365]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 uts -&gt; 'uts:[4026532367]'</pre>
    </div>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span>,
        <span>User</span>, and <span>PID</span> namespaces using both the
        <span>unshare</span> command and a simple <span>go</span> program.</p>
    <p>Mount Namespace</p>
    <p>We will now setup the minimal Ubuntu Base image for use in the new namespace in the <span>/tmp</span>
        directory.</p>
    <p>To create and copy the base image to a directory in <span>/tmp</span>, execute the following commands
        in <span>TA</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p /tmp/rootfs/.old_root</p>
      <p>$ tar -xvf $HOME/Downloads/ubuntu-base-18.04.4-base-amd64.tar.gz --directory /tmp/rootfs</p>
      <p>cd /tmp</p>
    </div>
    <p>Now let us now layer the <span>Mount</span> namespace on top of the <span>User</span>,
        the <span>UTS</span>, and the <span>PID</span> namespaces.</p>
    <p>To launch a simple container whose mount points as well as the process IDs, the user/group IDs, and the host name
        are isolated from the parent namespace, execute the following command in <span>TB</span>:</p>
    <p>$ sudo unshare -uUrpfm --mount-proc /bin/sh</p>
    <p>The <span>-m</span> option enables the <span>Mount</span> namespace.</p>
    <p>The command prompt will change to a <span>#</span>.</p>
    <p>To list all the mount points in the parent namespace, execute the following command in <span>TA</span>:</p>
    <p>$ cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.24</h4>
      <pre>cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0
cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0
cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0
cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0
cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0
cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0
cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0
cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0
cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0
cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0
cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0
cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0
cgroup /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0
configfs /sys/kernel/config configfs rw,relatime 0 0
debugfs /sys/kernel/debug debugfs rw,relatime 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
/dev/sdb1 /home ext4 rw,relatime,data=ordered 0 0
/dev/sdc1 /home/data ext4 rw,relatime,data=ordered 0 0
fusectl /sys/fs/fuse/connections fusectl rw,relatime 0 0
gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0
hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0
mqueue /dev/mqueue mqueue rw,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0
securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=25,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=28210 0 0
tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0
tmpfs /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0
tmpfs /run tmpfs rw,nosuid,noexec,relatime,size=3293620k,mode=755 0 0
tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=3293616k,mode=700,uid=1000,gid=1000 0 0
tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0
udev /dev devtmpfs rw,nosuid,relatime,size=16402556k,nr_inodes=4100639,mode=755 0 0</pre>
    </div>
    <p>Now, let us list all the mount points in the new namespace by executing the following command in <span>
        TB</span>:</p>
    <p># cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.25</h4>
      <pre>cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0
cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0
cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0
cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0
cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0
cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0
cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0
cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0
cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0
cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0
cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0
cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0
cgroup /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0
configfs /sys/kernel/config configfs rw,relatime 0 0
debugfs /sys/kernel/debug debugfs rw,relatime 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
/dev/sdb1 /home ext4 rw,relatime,data=ordered 0 0
/dev/sdc1 /home/data ext4 rw,relatime,data=ordered 0 0
fusectl /sys/fs/fuse/connections fusectl rw,relatime 0 0
gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0
hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0
mqueue /dev/mqueue mqueue rw,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0
securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=25,pgrp=0,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=28210 0 0
tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0
tmpfs /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0
tmpfs /run tmpfs rw,nosuid,noexec,relatime,size=3293620k,mode=755 0 0
tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=3293616k,mode=700,uid=1000,gid=1000 0 0
tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0
udev /dev devtmpfs rw,nosuid,relatime,size=16402556k,nr_inodes=4100639,mode=755 0 0</pre>
    </div>
    <p>Comparing Output.25 and Output.24, we see the one difference for <span>proc</span>. When a new
        <span>Mount</span> namespace is created, the mount points of the new namespace is a copy of the
        mount points in the parent's namespace.</p>
    <p>We will now demonstrate any changes to the new namespace will not affect the parent namespace.</p>
    <p>To make the mount point <span>/</span> (and its children recursively) to be private to the new
        namespace, execute the following command in <span>TB</span>:</p>
    <p># mount --make-rprivate /</p>
    <p>To recursive bind the mount point <span>rootfs/</span> to <span>rootfs/</span> in the new
        namespace, execute the following command in <span>TB</span>:</p>
    <p># mount --rbind rootfs/ rootfs/</p>
    <p>We need the <span>proc</span> filesystem in the new namespace for making changes to mounts. To mount
        <span>/proc</span> as the proc filesystem <span>proc</span> in the new namespace,
        execute the following command in <span>TB</span>:</p>
    <p># mount -t proc proc rootfs/proc</p>
    <p>Next, we need to make <span>rootfs/</span> the root filesystem in the new namespace and move the parent
        root filesystem to <span>rootfs/.old_root</span> using the <span>pivot_root</span>
        command. To do that, execute the following commands in <span>TB</span>:</p>
    <div id="cmd-div">
      <p># pivot_root rootfs/ rootfs/.old_root</p>
      <p># cd /</p>
    </div>
    <p>To list all the file(s) under <span>/</span> in the parent namespace, execute the following command in
        <span>TA</span>:</p>
    <p>$ ls -l /</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.26</h4>
      <pre>total 96
drwxr-xr-x   2 root root  4096 Mar  1 10:58 bin
drwxr-xr-x   3 root root  4096 Mar 16 21:15 boot
drwxr-xr-x   2 root root  4096 Sep 13  2019 cdrom
drwxr-xr-x  22 root root  4560 Mar 21 06:59 dev
drwxr-xr-x 163 root root 12288 Mar 20 10:01 etc
drwxr-xr-x   5 root root  4096 Sep 13  2019 home
lrwxrwxrwx   1 root root    33 Mar 16 21:15 initrd.img -&gt; boot/initrd.img-4.15.0-91-generic
lrwxrwxrwx   1 root root    33 Feb 17 14:08 initrd.img.old -&gt; boot/initrd.img-4.15.0-88-generic
drwxr-xr-x  25 root root  4096 Mar 16 13:37 lib
drwxr-xr-x   2 root root  4096 Jul 29  2019 lib64
drwx------   2 root root 16384 Sep 13  2019 lost+found
drwxr-xr-x   3 root root  4096 Nov 10 13:00 media
drwxr-xr-x   2 root root  4096 Jul 29  2019 mnt
drwxr-xr-x   7 root root  4096 Mar 13 08:04 opt
dr-xr-xr-x 328 root root     0 Mar 21 06:59 proc
drwx------   9 root root  4096 Feb 23 13:25 root
drwxr-xr-x  36 root root  1140 Mar 21 07:04 run
drwxr-xr-x   2 root root 12288 Mar 16 13:37 sbin
drwxr-xr-x   2 root root  4096 Jul 29  2019 srv
dr-xr-xr-x  13 root root     0 Mar 21 06:59 sys
drwxrwxrwt  20 root root  4096 Mar 21 11:10 tmp
drwxr-xr-x  11 root root  4096 Jul 29  2019 usr
drwxr-xr-x  11 root root  4096 Jul 29  2019 var
lrwxrwxrwx   1 root root    30 Mar 16 21:15 vmlinuz -&gt; boot/vmlinuz-4.15.0-91-generic
lrwxrwxrwx   1 root root    30 Feb 17 14:08 vmlinuz.old -&gt; boot/vmlinuz-4.15.0-88-generic</pre>
    </div>
    <p>To list all the file(s) under <span>/</span> in the new namespace, execute the following command in
        <span>TB</span>:</p>
    <p># ls -l /</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.27</h4>
      <pre>total 72
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 bin
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 boot
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 dev
drwxr-xr-x  29 nobody nogroup 4096 Feb  3 20:24 etc
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 home
drwxr-xr-x   8 nobody nogroup 4096 May 23  2017 lib
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 lib64
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 media
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 mnt
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 opt
dr-xr-xr-x 328 root   root       0 Mar 21 14:10 proc
drwx------   2 nobody nogroup 4096 Feb  3 20:24 root
drwxr-xr-x   4 nobody nogroup 4096 Feb  3 20:23 run
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 sbin
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 srv
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 sys
drwxrwxr-x   2 nobody nogroup 4096 Feb  3 20:24 tmp
drwxr-xr-x  10 nobody nogroup 4096 Feb  3 20:23 usr
drwxr-xr-x  11 nobody nogroup 4096 Feb  3 20:24 var</pre>
    </div>
    <p>Comparing Output.26 and Output.27, we see the root filesystems are totally different.</p>
    <p>To mount <span>/tmp</span> as the temporary filesystem <span>tmpfs</span> in the
        new namespace, execute the following command in <span>TB</span>:</p>
    <p># mount -t tmpfs tmpfs /tmp</p>
    <p>To create a text file <span>/tmp/leopard.txt</span> in the directory <span>/tmp</span>
        of the new namespace, execute the following command in <span>TB</span>:</p>
    <p># echo 'leopard' &gt; /tmp/leopard.txt</p>
    <p>To list the properties of the file <span>/tmp/leopard.txt</span> in the new namespace, execute
        the following command in <span>TB</span>:</p>
    <p># ls -l /tmp/leopard.txt</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.28</h4>
      <pre>-rw-r--r-- 1 root root 7 Mar 14 22:05 /tmp/leopard.txt</pre>
    </div>
    <p>To list the properties of the file <span>/tmp/leopard.txt</span> in the parent namespace, execute
        the following command in <span>TA</span>:</p>
    <p>$ ls -l /tmp/leopard.txt</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.29</h4>
      <pre>ls: cannot access '/tmp/leopard.txt': No such file or directory</pre>
    </div>
    <p>Finally, to completely remove the parent root filesystem <span>rootfs/.old_root</span> from the new
        namespace, execute the following commands in <span>TB</span>:</p>
    <div id="cmd-div">
      <p># mount --make-rprivate /.old_root</p>
      <p># umount -l /.old_root</p>
    </div>
    <p>To list all the mount points in the new namespace by executing the following command in <span>TB</span>
        :</p>
    <p># cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.30</h4>
      <pre>/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
proc /proc proc rw,relatime 0 0
tmpfs /tmp tmpfs rw,relatime 0 0</pre>
    </div>
    <p>To exit the new namespace, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span>,
        <span>User</span>, <span>PID</span>, and <span>Mount</span> namespaces
        using the <span>unshare</span> command.</p>
    <p>References</p>
    
    <br>
    <hr>
    
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DwarFS – Deduplicating Warp-Speed Advanced Read-Only File System (135 pts)]]></title>
            <link>https://github.com/mhx/dwarfs</link>
            <guid>40008755</guid>
            <pubDate>Fri, 12 Apr 2024 02:04:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mhx/dwarfs">https://github.com/mhx/dwarfs</a>, See on <a href="https://news.ycombinator.com/item?id=40008755">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://github.com/mhx/dwarfs/releases/latest"><img src="https://camo.githubusercontent.com/7c14c3f1b59f09fab877e8fd13d3682f78ce4b9318fbe3cc4572e937200d6e56/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6d68782f6477617266733f6c6162656c3d4c617465737425323052656c65617365" alt="Latest Release" data-canonical-src="https://img.shields.io/github/release/mhx/dwarfs?label=Latest%20Release"></a>
<a href="https://github.com/mhx/dwarfs/releases"><img src="https://camo.githubusercontent.com/2b6c35d2fbd0dac827465bda94342ea83a91f3effafab9c1d117eb790cc936e9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d68782f6477617266732f746f74616c2e7376673f26636f6c6f723d453935343230266c6162656c3d546f74616c253230446f776e6c6f616473" alt="Total Downloads" data-canonical-src="https://img.shields.io/github/downloads/mhx/dwarfs/total.svg?&amp;color=E95420&amp;label=Total%20Downloads"></a>
<a href="https://github.com/mhx/dwarfs/actions/workflows/build.yml"><img src="https://github.com/mhx/dwarfs/actions/workflows/build.yml/badge.svg" alt="DwarFS CI Build"></a>
<a href="https://app.travis-ci.com/github/mhx/dwarfs" rel="nofollow"><img src="https://camo.githubusercontent.com/9d83f6eb2696807dc865c941a7b37c7fc27346db3cbb11595be061b977b8866b/68747470733a2f2f6170702e7472617669732d63692e636f6d2f6d68782f6477617266732e7376673f6272616e63683d6d61696e" alt="Build Status" data-canonical-src="https://app.travis-ci.com/mhx/dwarfs.svg?branch=main"></a>
<a href="https://app.codacy.com/gh/mhx/dwarfs/dashboard" rel="nofollow"><img src="https://camo.githubusercontent.com/6d5dac47834dc9a72470dd868d91391d3a808686e1457628e3022dfb61cf92fd/68747470733a2f2f6170702e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3533343839663737373535323438633939396533383035303032363765383839" alt="Codacy Badge" data-canonical-src="https://app.codacy.com/project/badge/Grade/53489f77755248c999e380500267e889"></a>
<a href="https://codecov.io/github/mhx/dwarfs" rel="nofollow"><img src="https://camo.githubusercontent.com/028b266bc1c1b17af19e188abadf5901e33b4b9972e56631c7a76a0dea82d2e5/68747470733a2f2f636f6465636f762e696f2f6769746875622f6d68782f6477617266732f67726170682f62616467652e7376673f746f6b656e3d424b52344133584b4139" alt="codecov" data-canonical-src="https://codecov.io/github/mhx/dwarfs/graph/badge.svg?token=BKR4A3XKA9"></a>
<a href="https://www.bestpractices.dev/projects/8663" rel="nofollow"><img src="https://camo.githubusercontent.com/9a7cc655c21144482581cbbfc96b3a1d20b4dbb174d99ca64ca27a682d2d9aa1/68747470733a2f2f7777772e626573747072616374696365732e6465762f70726f6a656374732f383636332f6261646765" alt="OpenSSF Best Practices" data-canonical-src="https://www.bestpractices.dev/projects/8663/badge"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">DwarFS</h2><a id="user-content-dwarfs" aria-label="Permalink: DwarFS" href="#dwarfs"></a></p>
<p dir="auto">The <strong>D</strong>eduplicating <strong>W</strong>arp-speed <strong>A</strong>dvanced <strong>R</strong>ead-only <strong>F</strong>ile <strong>S</strong>ystem.</p>
<p dir="auto">A fast high compression read-only file system for Linux and Windows.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#history">History</a></li>
<li><a href="#building-and-installing">Building and Installing</a>
<ul dir="auto">
<li><a href="#prebuilt-binaries">Prebuilt Binaries</a></li>
<li><a href="#universal-binaries">Universal Binaries</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#building">Building</a></li>
<li><a href="#installing">Installing</a></li>
<li><a href="#static-builds">Static Builds</a></li>
</ul>
</li>
<li><a href="#usage">Usage</a></li>
<li><a href="#windows-support">Windows Support</a>
<ul dir="auto">
<li><a href="#building-on-windows">Building on Windows</a></li>
</ul>
</li>
<li><a href="#macos-support">macOS Support</a>
<ul dir="auto">
<li><a href="#building-on-macos">Building on macOS</a></li>
</ul>
</li>
<li><a href="#use-cases">Use Cases</a>
<ul dir="auto">
<li><a href="#astrophotography">Astrophotography</a></li>
</ul>
</li>
<li><a href="#dealing-with-bit-rot">Dealing with Bit Rot</a></li>
<li><a href="#extended-attributes">Extended Attributes</a></li>
<li><a href="#comparison">Comparison</a>
<ul dir="auto">
<li><a href="#with-squashfs">With SquashFS</a></li>
<li><a href="#with-squashfs--xz">With SquashFS &amp; xz</a></li>
<li><a href="#with-lrzip">With lrzip</a></li>
<li><a href="#with-zpaq">With zpaq</a></li>
<li><a href="#with-zpaqfranz">With zpaqfranz</a></li>
<li><a href="#with-wimlib">With wimlib</a></li>
<li><a href="#with-cromfs">With Cromfs</a></li>
<li><a href="#with-erofs">With EROFS</a></li>
<li><a href="#with-fuse-archive">With fuse-archive</a></li>
</ul>
</li>
<li><a href="#performance-monitoring">Performance Monitoring</a></li>
<li><a href="#other-obscure-features">Other Obscure Features</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mhx/dwarfs/blob/main/doc/windows.gif?raw=true"><img src="https://github.com/mhx/dwarfs/raw/main/doc/windows.gif?raw=true" alt="Windows Screen Capture" title="DwarFS Windows" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mhx/dwarfs/blob/main/doc/screenshot.gif?raw=true"><img src="https://github.com/mhx/dwarfs/raw/main/doc/screenshot.gif?raw=true" alt="Linux Screen Capture" title="DwarFS Linux" data-animated-image=""></a></p>
<p dir="auto">DwarFS is a read-only file system with a focus on achieving <strong>very
high compression ratios</strong> in particular for very redundant data.</p>
<p dir="auto">This probably doesn't sound very exciting, because if it's redundant,
it <em>should</em> compress well. However, I found that other read-only,
compressed file systems don't do a very good job at making use of
this redundancy. See <a href="#comparison">here</a> for a comparison with other
compressed file systems.</p>
<p dir="auto">DwarFS also <strong>doesn't compromise on speed</strong> and for my use cases I've
found it to be on par with or perform better than SquashFS. For my
primary use case, <strong>DwarFS compression is an order of magnitude better
than SquashFS compression</strong>, it's <strong>6 times faster to build the file
system</strong>, it's typically faster to access files on DwarFS and it uses
less CPU resources.</p>
<p dir="auto">To give you an idea of what DwarFS is capable of, here's a quick comparison
of DwarFS and SquashFS on a set of video files with a total size of 39 GiB.
The twist is that each unique video file has two sibling files with a
different set of audio streams (I didn't make this up, this is
<a href="https://github.com/mhx/dwarfs/discussions/63" data-hovercard-type="discussion" data-hovercard-url="/mhx/dwarfs/discussions/63/hovercard">an actual use case</a>). So
there's redundancy in both the video and audio data, but as the streams
are interleaved and identical blocks are typically very far apart, it's
quite challenging to make use of that redundancy for compression. SquashFS
essentially fails to compress the source data at all, whereas DwarFS is
able to reduce the size by almost a factor of 3, which is close to the
theoretical maximum:</p>
<div data-snippet-clipboard-copy-content="$ du -hs dwarfs-video-test
39G     dwarfs-video-test
$ ls -lh dwarfs-video-test.*fs
-rw-r--r-- 1 mhx users 14G Jul  2 13:01 dwarfs-video-test.dwarfs
-rw-r--r-- 1 mhx users 39G Jul 12 09:41 dwarfs-video-test.squashfs"><pre><code>$ du -hs dwarfs-video-test
39G     dwarfs-video-test
$ ls -lh dwarfs-video-test.*fs
-rw-r--r-- 1 mhx users 14G Jul  2 13:01 dwarfs-video-test.dwarfs
-rw-r--r-- 1 mhx users 39G Jul 12 09:41 dwarfs-video-test.squashfs
</code></pre></div>
<p dir="auto">While this is already impressive, it gets even better. When mounting
the SquashFS image and performing a random-read throughput test using
<a href="https://github.com/axboe/fio/">fio</a>-3.34, both <code>squashfuse</code> and
<code>squashfuse_ll</code> top out at around 230 MiB/s:</p>
<div data-snippet-clipboard-copy-content="$ fio --readonly --rw=randread --name=randread --bs=64k --direct=1 \
      --opendir=mnt --numjobs=4 --ioengine=libaio --iodepth=32 \
      --group_reporting --runtime=60 --time_based
[...]
   READ: bw=230MiB/s (241MB/s), 230MiB/s-230MiB/s (241MB/s-241MB/s), io=13.5GiB (14.5GB), run=60004-60004msec"><pre><code>$ fio --readonly --rw=randread --name=randread --bs=64k --direct=1 \
      --opendir=mnt --numjobs=4 --ioengine=libaio --iodepth=32 \
      --group_reporting --runtime=60 --time_based
[...]
   READ: bw=230MiB/s (241MB/s), 230MiB/s-230MiB/s (241MB/s-241MB/s), io=13.5GiB (14.5GB), run=60004-60004msec
</code></pre></div>
<p dir="auto">DwarFS, however, manages to sustain <strong>random read rates of 20 GiB/s</strong>:</p>
<div data-snippet-clipboard-copy-content="  READ: bw=20.2GiB/s (21.7GB/s), 20.2GiB/s-20.2GiB/s (21.7GB/s-21.7GB/s), io=1212GiB (1301GB), run=60001-60001msec"><pre><code>  READ: bw=20.2GiB/s (21.7GB/s), 20.2GiB/s-20.2GiB/s (21.7GB/s-21.7GB/s), io=1212GiB (1301GB), run=60001-60001msec
</code></pre></div>
<p dir="auto">Distinct features of DwarFS are:</p>
<ul dir="auto">
<li>
<p dir="auto">Clustering of files by similarity using a similarity hash function.
This makes it easier to exploit the redundancy across file boundaries.</p>
</li>
<li>
<p dir="auto">Segmentation analysis across file system blocks in order to reduce
the size of the uncompressed file system. This saves memory when
using the compressed file system and thus potentially allows for
higher cache hit rates as more data can be kept in the cache.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md#categorizers">Categorization framework</a> to categorize
files or even fragments of files and then process individual categories
differently. For example, this allows you to not waste time trying to
compress incompressible files or to compress PCM audio data using FLAC
compression.</p>
</li>
<li>
<p dir="auto">Highly multi-threaded implementation. Both the
<a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md">file system creation tool</a> as well as the
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">FUSE driver</a> are able to make good use of the
many cores of your system.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">I started working on DwarFS in 2013 and my main use case and major
motivation was that I had several hundred different versions of Perl
that were taking up something around 30 gigabytes of disk space, and
I was unwilling to spend more than 10% of my hard drive keeping them
around for when I happened to need them.</p>
<p dir="auto">Up until then, I had been using <a href="https://bisqwit.iki.fi/source/cromfs.html" rel="nofollow">Cromfs</a>
for squeezing them into a manageable size. However, I was getting more
and more annoyed by the time it took to build the filesystem image
and, to make things worse, more often than not it was crashing after
about an hour or so.</p>
<p dir="auto">I had obviously also looked into <a href="https://en.wikipedia.org/wiki/SquashFS" rel="nofollow">SquashFS</a>,
but never got anywhere close to the compression rates of Cromfs.</p>
<p dir="auto">This alone wouldn't have been enough to get me into writing DwarFS,
but at around the same time, I was pretty obsessed with the recent
developments and features of newer C++ standards and really wanted
a C++ hobby project to work on. Also, I've wanted to do something
with <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace" rel="nofollow">FUSE</a>
for quite some time. Last but not least, I had been thinking about
the problem of compressed file systems for a bit and had some ideas
that I definitely wanted to try.</p>
<p dir="auto">The majority of the code was written in 2013, then I did a couple
of cleanups, bugfixes and refactors every once in a while, but I
never really got it to a state where I would feel happy releasing
it. It was too awkward to build with its dependency on Facebook's
(quite awesome) <a href="https://github.com/facebook/folly">folly</a> library
and it didn't have any documentation.</p>
<p dir="auto">Digging out the project again this year, things didn't look as grim
as they used to. Folly now builds with CMake and so I just pulled
it in as a submodule. Most other dependencies can be satisfied
from packages that should be widely available. And I've written
some rudimentary docs as well.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building and Installing</h2><a id="user-content-building-and-installing" aria-label="Permalink: Building and Installing" href="#building-and-installing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prebuilt Binaries</h3><a id="user-content-prebuilt-binaries" aria-label="Permalink: Prebuilt Binaries" href="#prebuilt-binaries"></a></p>
<p dir="auto"><a href="https://github.com/mhx/dwarfs/releases">Each release</a> has pre-built,
statically linked binaries for <code>Linux-x86_64</code>, <code>Linux-aarch64</code> and
<code>Windows-AMD64</code> available for download. These <em>should</em> run without
any dependencies and can be useful especially on older distributions
where you can't easily build the tools from source.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Universal Binaries</h3><a id="user-content-universal-binaries" aria-label="Permalink: Universal Binaries" href="#universal-binaries"></a></p>
<p dir="auto">In addition to the binary tarballs, there's a <strong>universal binary</strong>
available for each architecture. These universal binaries contain
<em>all</em> tools (<code>mkdwarfs</code>, <code>dwarfsck</code>, <code>dwarfsextract</code> and the <code>dwarfs</code>
FUSE driver) in a single executable. These executables are compressed
using <a href="https://github.com/upx/upx">upx</a>, so they are much smaller than
the individual tools combined. However, it also means the binaries need
to be decompressed each time they are run, which can have a signficant
overhead. If that is an issue, you can either stick to the "classic"
individual binaries or you can decompress the universal binary, e.g.:</p>
<div data-snippet-clipboard-copy-content="upx -d dwarfs-universal-0.7.0-Linux-aarch64"><pre><code>upx -d dwarfs-universal-0.7.0-Linux-aarch64
</code></pre></div>
<p dir="auto">The universal binaries can be run through symbolic links named after
the proper tool. e.g.:</p>
<div data-snippet-clipboard-copy-content="$ ln -s dwarfs-universal-0.7.0-Linux-aarch64 mkdwarfs
$ ./mkdwarfs --help"><pre><code>$ ln -s dwarfs-universal-0.7.0-Linux-aarch64 mkdwarfs
$ ./mkdwarfs --help
</code></pre></div>
<p dir="auto">This also works on Windows if the file system supports symbolic links:</p>
<div data-snippet-clipboard-copy-content="> mklink mkdwarfs.exe dwarfs-universal-0.7.0-Windows-AMD64.exe
> .\mkdwarfs.exe --help"><pre><code>&gt; mklink mkdwarfs.exe dwarfs-universal-0.7.0-Windows-AMD64.exe
&gt; .\mkdwarfs.exe --help
</code></pre></div>
<p dir="auto">Alternatively, you can select the tool by passing <code>--tool=&lt;name&gt;</code> as
the first argument on the command line:</p>
<div data-snippet-clipboard-copy-content="> .\dwarfs-universal-0.7.0-Windows-AMD64.exe --tool=mkdwarfs --help"><pre><code>&gt; .\dwarfs-universal-0.7.0-Windows-AMD64.exe --tool=mkdwarfs --help
</code></pre></div>
<p dir="auto">Note that just like the <code>dwarfs.exe</code> Windows binary, the universal
Windows binary depends on the <code>winfsp-x64.dll</code> from the
<a href="https://github.com/winfsp/winfsp">WinFsp</a> project. However, for the
universal binary, the DLL is loaded lazily, so you can still use all
other tools without the DLL.
See the <a href="#windows-support">Windows Support</a> section for more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dependencies</h3><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">DwarFS uses <a href="https://cmake.org/" rel="nofollow">CMake</a> as a build tool.</p>
<p dir="auto">It uses both <a href="https://www.boost.org/" rel="nofollow">Boost</a> and
<a href="https://github.com/facebook/folly">Folly</a>, though the latter is
included as a submodule since very few distributions actually
offer packages for it. Folly itself has a number of dependencies,
so please check <a href="https://github.com/facebook/folly#dependencies">here</a>
for an up-to-date list.</p>
<p dir="auto">It also uses <a href="https://github.com/facebook/fbthrift">Facebook Thrift</a>,
in particular the <code>frozen</code> library, for storing metadata in a highly
space-efficient, memory-mappable and well defined format. It's also
included as a submodule, and we only build the compiler and a very
reduced library that contains just enough for DwarFS to work.</p>
<p dir="auto">Other than that, DwarFS really only depends on FUSE3 and on a set
of compression libraries that Folly already depends on (namely
<a href="https://github.com/lz4/lz4">lz4</a>, <a href="https://github.com/facebook/zstd">zstd</a>
and <a href="https://github.com/kobolabs/liblzma">liblzma</a>).</p>
<p dir="auto">The dependency on <a href="https://github.com/google/googletest">googletest</a>
will be automatically resolved if you build with tests.</p>
<p dir="auto">A good starting point for apt-based systems is probably:</p>
<div data-snippet-clipboard-copy-content="$ apt install \
    gcc \
    g++ \
    clang \
    git \
    ccache \
    ninja-build \
    cmake \
    make \
    bison \
    flex \
    ronn \
    fuse3 \
    pkg-config \
    binutils-dev \
    libacl1-dev \
    libarchive-dev \
    libbenchmark-dev \
    libboost-chrono-dev \
    libboost-context-dev \
    libboost-filesystem-dev \
    libboost-iostreams-dev \
    libboost-program-options-dev \
    libboost-regex-dev \
    libboost-system-dev \
    libboost-thread-dev \
    libbrotli-dev \
    libevent-dev \
    libhowardhinnant-date-dev \
    libjemalloc-dev \
    libdouble-conversion-dev \
    libiberty-dev \
    liblz4-dev \
    liblzma-dev \
    libmagic-dev \
    librange-v3-dev \
    libssl-dev \
    libunwind-dev \
    libdwarf-dev \
    libelf-dev \
    libfmt-dev \
    libfuse3-dev \
    libgoogle-glog-dev \
    libutfcpp-dev \
    libflac++-dev \
    python3-mistletoe"><pre><code>$ apt install \
    gcc \
    g++ \
    clang \
    git \
    ccache \
    ninja-build \
    cmake \
    make \
    bison \
    flex \
    ronn \
    fuse3 \
    pkg-config \
    binutils-dev \
    libacl1-dev \
    libarchive-dev \
    libbenchmark-dev \
    libboost-chrono-dev \
    libboost-context-dev \
    libboost-filesystem-dev \
    libboost-iostreams-dev \
    libboost-program-options-dev \
    libboost-regex-dev \
    libboost-system-dev \
    libboost-thread-dev \
    libbrotli-dev \
    libevent-dev \
    libhowardhinnant-date-dev \
    libjemalloc-dev \
    libdouble-conversion-dev \
    libiberty-dev \
    liblz4-dev \
    liblzma-dev \
    libmagic-dev \
    librange-v3-dev \
    libssl-dev \
    libunwind-dev \
    libdwarf-dev \
    libelf-dev \
    libfmt-dev \
    libfuse3-dev \
    libgoogle-glog-dev \
    libutfcpp-dev \
    libflac++-dev \
    python3-mistletoe
</code></pre></div>
<p dir="auto">Note that when building with <code>gcc</code>, the optimization level will be
set to <code>-O2</code> instead of the CMake default of <code>-O3</code> for release
builds. At least with versions up to <code>gcc-10</code>, the <code>-O3</code> build is
<a href="https://github.com/mhx/dwarfs/issues/14" data-hovercard-type="issue" data-hovercard-url="/mhx/dwarfs/issues/14/hovercard">up to 70% slower</a> than a
build with <code>-O2</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building</h3><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Firstly, either clone the repository...</p>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs
</code></pre></div>
<p dir="auto">...or unpack the release archive:</p>
<div data-snippet-clipboard-copy-content="$ tar xvf dwarfs-x.y.z.tar.bz2
$ cd dwarfs-x.y.z"><pre><code>$ tar xvf dwarfs-x.y.z.tar.bz2
$ cd dwarfs-x.y.z
</code></pre></div>
<p dir="auto">Once all dependencies have been installed, you can build DwarFS
using:</p>
<div data-snippet-clipboard-copy-content="$ mkdir build
$ cd build
$ cmake .. -DWITH_TESTS=1
$ make -j$(nproc)"><pre><code>$ mkdir build
$ cd build
$ cmake .. -DWITH_TESTS=1
$ make -j$(nproc)
</code></pre></div>
<p dir="auto">You can then run tests with:</p>

<p dir="auto">All binaries use <a href="https://github.com/jemalloc/jemalloc">jemalloc</a>
as a memory allocator by default, as it is typically uses much less
system memory compared to the <code>glibc</code> or <code>tcmalloc</code> allocators.
To disable the use of <code>jemalloc</code>, pass <code>-DUSE_JEMALLOC=0</code> on the
<code>cmake</code> command line.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing</h3><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto">Installing is as easy as:</p>

<p dir="auto">Though you don't have to install the tools to play with them.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Static Builds</h3><a id="user-content-static-builds" aria-label="Permalink: Static Builds" href="#static-builds"></a></p>
<p dir="auto">Attempting to build statically linked binaries is highly discouraged
and not officially supported. That being said, here's how to set up
an environment where you <em>might</em> be able to build static binaries.</p>
<p dir="auto">This has been tested with <code>ubuntu-22.04-live-server-amd64.iso</code>. First,
install all the packages listed as dependencies above. Also install:</p>
<div data-snippet-clipboard-copy-content="$ apt install ccache ninja libacl1-dev"><pre><code>$ apt install ccache ninja libacl1-dev
</code></pre></div>
<p dir="auto"><code>ccache</code> and <code>ninja</code> are optional, but help with a speedy compile.</p>
<p dir="auto">Depending on your distibution, you'll need to build and install static
versions of some libraries, e.g. <code>libarchive</code> and <code>libmagic</code> for Ubuntu:</p>
<div data-snippet-clipboard-copy-content="$ wget https://github.com/libarchive/libarchive/releases/download/v3.6.2/libarchive-3.6.2.tar.xz
$ tar xf libarchive-3.6.2.tar.xz &amp;&amp; cd libarchive-3.6.2
$ ./configure --prefix=/opt/static-libs --without-iconv --without-xml2 --without-expat
$ make &amp;&amp; sudo make install"><pre><code>$ wget https://github.com/libarchive/libarchive/releases/download/v3.6.2/libarchive-3.6.2.tar.xz
$ tar xf libarchive-3.6.2.tar.xz &amp;&amp; cd libarchive-3.6.2
$ ./configure --prefix=/opt/static-libs --without-iconv --without-xml2 --without-expat
$ make &amp;&amp; sudo make install
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ wget ftp://ftp.astron.com/pub/file/file-5.44.tar.gz
$ tar xf file-5.44.tar.gz &amp;&amp; cd file-5.44
$ ./configure --prefix=/opt/static-libs --enable-static=yes --enable-shared=no
$ make &amp;&amp; make install"><pre><code>$ wget ftp://ftp.astron.com/pub/file/file-5.44.tar.gz
$ tar xf file-5.44.tar.gz &amp;&amp; cd file-5.44
$ ./configure --prefix=/opt/static-libs --enable-static=yes --enable-shared=no
$ make &amp;&amp; make install
</code></pre></div>
<p dir="auto">That's it! Now you can try building static binaries for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs &amp;&amp; mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=1 -DSTATIC_BUILD_DO_NOT_USE=1 \
           -DSTATIC_BUILD_EXTRA_PREFIX=/opt/static-libs
$ ninja
$ ninja test"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs &amp;&amp; mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=1 -DSTATIC_BUILD_DO_NOT_USE=1 \
           -DSTATIC_BUILD_EXTRA_PREFIX=/opt/static-libs
$ ninja
$ ninja test
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Please check out the manual pages for <a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md">mkdwarfs</a>,
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">dwarfs</a>, <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsck.md">dwarfsck</a> and
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsextract.md">dwarfsextract</a>. You can also access the manual
pages using the <code>--man</code> option to each binary, e.g.:</p>

<p dir="auto">The <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">dwarfs</a> manual page also shows an example for setting
up DwarFS with <a href="https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt" rel="nofollow">overlayfs</a>
in order to create a writable file system mount on top a read-only
DwarFS image.</p>
<p dir="auto">A description of the DwarFS filesystem format can be found in
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs-format.md">dwarfs-format</a>.</p>
<p dir="auto">A high-level overview of the internal operation of <code>mkdwarfs</code> is shown
in <a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs-sequence.svg">this sequence diagram</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Windows Support</h2><a id="user-content-windows-support" aria-label="Permalink: Windows Support" href="#windows-support"></a></p>
<p dir="auto">Support for the Windows operating system is currently experimental.
Having worked pretty much exclusively in a Unix world for the past two
decades, my experience with Windows development is rather limited and
I'd expect there to definitely be bugs and rough edges in the Windows
code.</p>
<p dir="auto">The Windows version of the DwarFS filesystem driver relies on the awesome
<a href="https://github.com/winfsp/winfsp">WinFsp</a> project and its <code>winfsp-x64.dll</code>
must be discoverable by the <code>dwarfs.exe</code> driver.</p>
<p dir="auto">The different tools should behave pretty much the same whether you're
using them on Linux or Windows. The file system images can be copied
between Linux and Windows and images created on one OS should work fine
on the other.</p>
<p dir="auto">There are a few things worth pointing out, though:</p>
<ul dir="auto">
<li>
<p dir="auto">DwarFS supports both hardlinks and symlinks on Windows, just as it
does on Linux. However, creating hardlinks and symlinks seems to
require admin privileges on Windows, so if you want to e.g. extract
a DwarFS image that contains links of some sort, you might run into
errors if you don't have the right privileges.</p>
</li>
<li>
<p dir="auto">Due to a <a href="https://github.com/winfsp/winfsp/issues/454" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/454/hovercard">problem</a> in
WinFsp, symlinks cannot currently point outside of the mounted file
system.  Furthermore, due to another
<a href="https://github.com/winfsp/winfsp/issues/530" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/530/hovercard">problem</a> in WinFsp,
symlinks with a drive letter will appear with a mangled target path.</p>
</li>
<li>
<p dir="auto">The DwarFS driver on Windows correctly reports hardlink counts via
its API, but currently these counts are not correctly propagated
to the Windows file system layer. This is presumably due to a
<a href="https://github.com/winfsp/winfsp/issues/511" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/511/hovercard">problem</a> in WinFsp.</p>
</li>
<li>
<p dir="auto">When mounting a DwarFS image on Windows, the mount point must not
exist. This is different from Linux, where the mount point must
actually exist. Also, it's possible to mount a DwarFS image as a
drive letter, e.g.</p>
<p dir="auto">dwarfs.exe image.dwarfs Z:</p>
</li>
<li>
<p dir="auto">Filter rules for <code>mkdwarfs</code> always require Unix path separators,
regardless of whether it's running on Windows or Linux.</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building on Windows</h3><a id="user-content-building-on-windows" aria-label="Permalink: Building on Windows" href="#building-on-windows"></a></p>
<p dir="auto">Building on Windows is not too complicated thanks to <a href="https://vcpkg.io/" rel="nofollow">vcpkg</a>.
You'll need to install:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://visualstudio.microsoft.com/vs/features/cplusplus/" rel="nofollow">Visual Studio and the MSVC C/C++ compiler</a></p>
</li>
<li>
<p dir="auto"><a href="https://git-scm.com/download/win" rel="nofollow">Git</a></p>
</li>
<li>
<p dir="auto"><a href="https://cmake.org/download/" rel="nofollow">CMake</a></p>
</li>
<li>
<p dir="auto"><a href="https://github.com/ninja-build/ninja/releases">Ninja</a></p>
</li>
<li>
<p dir="auto"><a href="https://github.com/winfsp/winfsp/releases">WinFsp</a></p>
</li>
</ul>
<p dir="auto"><code>WinFsp</code> is expected to be installed in <code>C:\Program Files (x68)\WinFsp</code>;
if it's not, you'll need to set <code>WINFSP_PATH</code> when running CMake via
<code>cmake/win.bat</code>.</p>
<p dir="auto">Now you need to clone <code>vcpkg</code> and <code>dwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="> cd %HOMEPATH%
> mkdir git
> cd git
> git clone https://github.com/Microsoft/vcpkg.git
> git clone https://github.com/mhx/dwarfs"><pre><code>&gt; cd %HOMEPATH%
&gt; mkdir git
&gt; cd git
&gt; git clone https://github.com/Microsoft/vcpkg.git
&gt; git clone https://github.com/mhx/dwarfs
</code></pre></div>
<p dir="auto">Then, bootstrap <code>vcpkg</code>:</p>
<div data-snippet-clipboard-copy-content="> .\vcpkg\bootstrap-vcpkg.bat"><pre><code>&gt; .\vcpkg\bootstrap-vcpkg.bat
</code></pre></div>
<p dir="auto">And build DwarFS:</p>
<div data-snippet-clipboard-copy-content="> cd dwarfs
> mkdir build
> cd build
> ..\cmake\win.bat
> ninja"><pre><code>&gt; cd dwarfs
&gt; mkdir build
&gt; cd build
&gt; ..\cmake\win.bat
&gt; ninja
</code></pre></div>
<p dir="auto">Once that's done, you should be able to run the tests.
Set <code>CTEST_PARALLEL_LEVEL</code> according to the number of CPU cores in
your machine.</p>
<div data-snippet-clipboard-copy-content="> set CTEST_PARALLEL_LEVEL=10
> ninja test"><pre><code>&gt; set CTEST_PARALLEL_LEVEL=10
&gt; ninja test
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">macOS Support</h2><a id="user-content-macos-support" aria-label="Permalink: macOS Support" href="#macos-support"></a></p>
<p dir="auto">Support for the macOS operating system is currently experimental.</p>
<p dir="auto">The macOS version of the DwarFS filesystem driver relies on the awesome
<a href="https://https//osxfuse.github.io/" rel="nofollow">macFUSE</a> project.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building on macOS</h3><a id="user-content-building-on-macos" aria-label="Permalink: Building on macOS" href="#building-on-macos"></a></p>
<p dir="auto">Building on macOS involves a few steps, but should be relatively
straightforward:</p>
<ul dir="auto">
<li>
<p dir="auto">Install <a href="https://brew.sh/" rel="nofollow">Homebrew</a></p>
</li>
<li>
<p dir="auto">Use Homebrew to install the necessary dependencies:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="$ brew install cmake ninja ronn macfuse python3 brotli howard-hinnant-date \
               double-conversion fmt glog libarchive libevent flac openssl \
               pkg-config range-v3 utf8cpp xxhash boost zstd jemalloc"><pre><code>$ brew install cmake ninja ronn macfuse python3 brotli howard-hinnant-date \
               double-conversion fmt glog libarchive libevent flac openssl \
               pkg-config range-v3 utf8cpp xxhash boost zstd jemalloc
</code></pre></div>
<ul dir="auto">
<li>
<p dir="auto">When installing macFUSE for the first time, you'll need to explicitly
allow the sofware in <em>System Preferences</em> / <em>Privacy &amp; Security</em>. It's
quite likely that you'll have to reboot after this.</p>
</li>
<li>
<p dir="auto">Clone the DwarFS repository:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
</code></pre></div>
<ul dir="auto">
<li>Prepare the build by installing the <code>mistletoe</code> python module
in a virtualenv:</li>
</ul>
<div data-snippet-clipboard-copy-content="$ cd dwarfs
$ python3 -m venv @buildenv
$ source ./@buildenv/bin/activate
$ pip3 install mistletoe"><pre><code>$ cd dwarfs
$ python3 -m venv @buildenv
$ source ./@buildenv/bin/activate
$ pip3 install mistletoe
</code></pre></div>
<ul dir="auto">
<li>Build DwarFS and run its tests:</li>
</ul>
<div data-snippet-clipboard-copy-content="$ git checkout v0.9.4
$ git submodule update
$ mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=ON
$ ninja
$ export CTEST_PARALLEL_LEVEL=$(sysctl -n hw.logicalcpu)
$ ninja test"><pre><code>$ git checkout v0.9.4
$ git submodule update
$ mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=ON
$ ninja
$ export CTEST_PARALLEL_LEVEL=$(sysctl -n hw.logicalcpu)
$ ninja test
</code></pre></div>
<ul dir="auto">
<li>Install DwarFS:</li>
</ul>

<p dir="auto">That's it!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use Cases</h2><a id="user-content-use-cases" aria-label="Permalink: Use Cases" href="#use-cases"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Astrophotography</h3><a id="user-content-astrophotography" aria-label="Permalink: Astrophotography" href="#astrophotography"></a></p>
<p dir="auto">Astrophotography can generate huge amounts of raw image data. During a
single night, it's not unlikely to end up with a few dozens of gigabytes
of data. With most dedicated astrophotography cameras, this data ends up
in the form of FITS images. These are usually uncompressed, don't compress
very well with standard compression algorithms, and while there are certain
compressed FITS formats, these aren't widely supported.</p>
<p dir="auto">One of the compression formats (simply called "Rice") compresses reasonably
well and is really fast. However, its implementation for compressed FITS
has a few drawbacks. The most severe drawbacks are that compression isn't
quite as good as it could be for color sensors and sensors with a less than
16 bits of resolution.</p>
<p dir="auto">DwarFS supports the <code>ricepp</code> (Rice++) compression, which builds on the basic
idea of Rice compression, but makes a few enhancements: it compresses color
and low bit depth images significantly better and always searches for the
optimum solution during compression instead of relying on a heuristic.</p>
<p dir="auto">Let's look at an example using 129 images (darks, flats and lights) taken
with an ASI1600MM camera. Each image is 32 MiB, so a total of 4 GiB of data.
Compressing these with the standard <code>fpack</code> tool takes about 16.6 seconds
and yields a total output size of 2.2 GiB:</p>
<div data-snippet-clipboard-copy-content="$ time fpack */*.fit */*/*.fit

user	14.992
system	1.592
total	16.616

$ find . -name '*.fz' -print0 | xargs -0 cat | wc -c
2369943360"><pre><code>$ time fpack */*.fit */*/*.fit

user	14.992
system	1.592
total	16.616

$ find . -name '*.fz' -print0 | xargs -0 cat | wc -c
2369943360
</code></pre></div>
<p dir="auto">However, this leaves you with <code>*.fz</code> files that not every application can
actually read.</p>
<p dir="auto">Using DwarFS, here's what we get:</p>
<div data-snippet-clipboard-copy-content="$ mkdwarfs -i ASI1600 -o asi1600-20.dwarfs -S 20 --categorize
I 08:47:47.459077 scanning &quot;ASI1600&quot;
I 08:47:47.491492 assigning directory and link inodes...
I 08:47:47.491560 waiting for background scanners...
I 08:47:47.675241 scanning CPU time: 1.051s
I 08:47:47.675271 finalizing file inodes...
I 08:47:47.675330 saved 0 B / 3.941 GiB in 0/258 duplicate files
I 08:47:47.675360 assigning device inodes...
I 08:47:47.675371 assigning pipe/socket inodes...
I 08:47:47.675381 building metadata...
I 08:47:47.675393 building blocks...
I 08:47:47.675398 saving names and symlinks...
I 08:47:47.675514 updating name and link indices...
I 08:47:47.675796 waiting for segmenting/blockifying to finish...
I 08:47:50.274285 total ordering CPU time: 616.3us
I 08:47:50.274329 total segmenting CPU time: 1.132s
I 08:47:50.279476 saving chunks...
I 08:47:50.279622 saving directories...
I 08:47:50.279674 saving shared files table...
I 08:47:50.280745 saving names table... [1.047ms]
I 08:47:50.280768 saving symlinks table... [743ns]
I 08:47:50.282031 waiting for compression to finish...
I 08:47:50.823924 compressed 3.941 GiB to 1.201 GiB (ratio=0.304825)
I 08:47:50.824280 compression CPU time: 17.92s
I 08:47:50.824316 filesystem created without errors [3.366s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
5 dirs, 0/0 soft/hard links, 258/258 files, 0 other
original size: 3.941 GiB, hashed: 315.4 KiB (18 files, 0 B/s)
scanned: 3.941 GiB (258 files, 117.1 GiB/s), categorizing: 0 B/s
saved by deduplication: 0 B (0 files), saved by segmenting: 0 B
filesystem: 3.941 GiB in 4037 blocks (4550 chunks, 516/516 fragments, 258 inodes)
compressed filesystem: 4037 blocks/1.201 GiB written"><pre><code>$ mkdwarfs -i ASI1600 -o asi1600-20.dwarfs -S 20 --categorize
I 08:47:47.459077 scanning "ASI1600"
I 08:47:47.491492 assigning directory and link inodes...
I 08:47:47.491560 waiting for background scanners...
I 08:47:47.675241 scanning CPU time: 1.051s
I 08:47:47.675271 finalizing file inodes...
I 08:47:47.675330 saved 0 B / 3.941 GiB in 0/258 duplicate files
I 08:47:47.675360 assigning device inodes...
I 08:47:47.675371 assigning pipe/socket inodes...
I 08:47:47.675381 building metadata...
I 08:47:47.675393 building blocks...
I 08:47:47.675398 saving names and symlinks...
I 08:47:47.675514 updating name and link indices...
I 08:47:47.675796 waiting for segmenting/blockifying to finish...
I 08:47:50.274285 total ordering CPU time: 616.3us
I 08:47:50.274329 total segmenting CPU time: 1.132s
I 08:47:50.279476 saving chunks...
I 08:47:50.279622 saving directories...
I 08:47:50.279674 saving shared files table...
I 08:47:50.280745 saving names table... [1.047ms]
I 08:47:50.280768 saving symlinks table... [743ns]
I 08:47:50.282031 waiting for compression to finish...
I 08:47:50.823924 compressed 3.941 GiB to 1.201 GiB (ratio=0.304825)
I 08:47:50.824280 compression CPU time: 17.92s
I 08:47:50.824316 filesystem created without errors [3.366s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
5 dirs, 0/0 soft/hard links, 258/258 files, 0 other
original size: 3.941 GiB, hashed: 315.4 KiB (18 files, 0 B/s)
scanned: 3.941 GiB (258 files, 117.1 GiB/s), categorizing: 0 B/s
saved by deduplication: 0 B (0 files), saved by segmenting: 0 B
filesystem: 3.941 GiB in 4037 blocks (4550 chunks, 516/516 fragments, 258 inodes)
compressed filesystem: 4037 blocks/1.201 GiB written
</code></pre></div>
<p dir="auto">In less than 3.4 seconds, it compresses the data down to 1.2 GiB, almost
half the size of the <code>fpack</code> output.</p>
<p dir="auto">In addition to saving a lot of disk space, this can also be useful when your
data is stored on a NAS. Here's a comparison of the same set of data accessed
over a 1 Gb/s network connection, first using the uncompressed raw data:</p>
<div data-snippet-clipboard-copy-content="find /mnt/ASI1600 -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 36.0455 s, 117 MB/s"><pre><code>find /mnt/ASI1600 -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 36.0455 s, 117 MB/s
</code></pre></div>
<p dir="auto">And next using a DwarFS image on the same share:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs /mnt/asi1600-20.dwarfs mnt

$ find mnt -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 14.3681 s, 294 MB/s"><pre><code>$ dwarfs /mnt/asi1600-20.dwarfs mnt

$ find mnt -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 14.3681 s, 294 MB/s
</code></pre></div>
<p dir="auto">That's roughly 2.5 times faster. You can very likely see similar results
with slow external hard drives.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dealing with Bit Rot</h2><a id="user-content-dealing-with-bit-rot" aria-label="Permalink: Dealing with Bit Rot" href="#dealing-with-bit-rot"></a></p>
<p dir="auto">Currently, DwarFS has no built-in ability to add recovery information to a
file system image. However, for archival purposes, it's a good idea to have
such recovery infomation in order to be able to repair a damaged image.</p>
<p dir="auto">This is fortunately relatively straightforward using something like
<a href="https://github.com/Parchive/par2cmdline">par2cmdline</a>:</p>
<div data-snippet-clipboard-copy-content="$ par2create -n1 asi1600-20.dwarfs"><pre><code>$ par2create -n1 asi1600-20.dwarfs
</code></pre></div>
<p dir="auto">This will create two additional files that you can place alongside the image
(or on a different storage), as you'll only need them if DwarFS has detected
an issue with the file system image. If there's an issue, you can run</p>
<div data-snippet-clipboard-copy-content="$ par2repair asi1600-20.dwarfs"><pre><code>$ par2repair asi1600-20.dwarfs
</code></pre></div>
<p dir="auto">which will very likely be able to recover the image if less than 5% (that's
the default used by <code>par2create</code>) of the image are damaged.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extended Attributes</h2><a id="user-content-extended-attributes" aria-label="Permalink: Extended Attributes" href="#extended-attributes"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Preserving Extended Attributes in DwarFS Images</h3><a id="user-content-preserving-extended-attributes-in-dwarfs-images" aria-label="Permalink: Preserving Extended Attributes in DwarFS Images" href="#preserving-extended-attributes-in-dwarfs-images"></a></p>
<p dir="auto">Extended attributes are not currently supported. Any extended attributes
stored in the source file system will not currently be preserved when
building a DwarFS image using <code>mkdwarfs</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Extended Attributes exposed by the FUSE Driver</h3><a id="user-content-extended-attributes-exposed-by-the-fuse-driver" aria-label="Permalink: Extended Attributes exposed by the FUSE Driver" href="#extended-attributes-exposed-by-the-fuse-driver"></a></p>
<p dir="auto">That being said, the root inode of a mounted DwarFS image currently exposes
one or two extended attributes on Linux:</p>
<div data-snippet-clipboard-copy-content="$ attr -l mnt
Attribute &quot;dwarfs.driver.pid&quot; has a 4 byte value for mnt
Attribute &quot;dwarfs.driver.perfmon&quot; has a 4849 byte value for mnt"><pre><code>$ attr -l mnt
Attribute "dwarfs.driver.pid" has a 4 byte value for mnt
Attribute "dwarfs.driver.perfmon" has a 4849 byte value for mnt
</code></pre></div>
<p dir="auto">The <code>dwarfs.driver.pid</code> attribute simply contains the PID of the DwarFS
FUSE driver. The <code>dwarfs.driver.perfmon</code> attribute contains the current
results of the <a href="#performance-monitoring">performance monitor</a>.</p>
<p dir="auto">Furthermore, each regular file exposes an attribute <code>dwarfs.inodeinfo</code>
with information about the undelying inode:</p>
<div data-snippet-clipboard-copy-content="$ attr -l &quot;05 Disappear.caf&quot;
Attribute &quot;dwarfs.inodeinfo&quot; has a 448 byte value for 05 Disappear.caf"><pre><code>$ attr -l "05 Disappear.caf"
Attribute "dwarfs.inodeinfo" has a 448 byte value for 05 Disappear.caf
</code></pre></div>
<p dir="auto">The attribute contains a JSON object with information about the
underlying inode:</p>
<div data-snippet-clipboard-copy-content="$ attr -qg dwarfs.inodeinfo &quot;05 Disappear.caf&quot;
{
  &quot;chunks&quot;: [
    {
      &quot;block&quot;: 2,
      &quot;category&quot;: &quot;pcmaudio/metadata&quot;,
      &quot;offset&quot;: 270976,
      &quot;size&quot;: 4096
    },
    {
      &quot;block&quot;: 414,
      &quot;category&quot;: &quot;pcmaudio/waveform&quot;,
      &quot;offset&quot;: 37594368,
      &quot;size&quot;: 29514492
    },
    {
      &quot;block&quot;: 419,
      &quot;category&quot;: &quot;pcmaudio/waveform&quot;,
      &quot;offset&quot;: 0,
      &quot;size&quot;: 29385468
    }
  ],
  &quot;gid&quot;: 100,
  &quot;mode&quot;: 33188,
  &quot;modestring&quot;: &quot;----rw-r--r--&quot;,
  &quot;uid&quot;: 1000
}"><pre><code>$ attr -qg dwarfs.inodeinfo "05 Disappear.caf"
{
  "chunks": [
    {
      "block": 2,
      "category": "pcmaudio/metadata",
      "offset": 270976,
      "size": 4096
    },
    {
      "block": 414,
      "category": "pcmaudio/waveform",
      "offset": 37594368,
      "size": 29514492
    },
    {
      "block": 419,
      "category": "pcmaudio/waveform",
      "offset": 0,
      "size": 29385468
    }
  ],
  "gid": 100,
  "mode": 33188,
  "modestring": "----rw-r--r--",
  "uid": 1000
}
</code></pre></div>
<p dir="auto">This is useful, for example, to check how a particular file is spread
across multiple blocks or which categories have been assigned to the
file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison</h2><a id="user-content-comparison" aria-label="Permalink: Comparison" href="#comparison"></a></p>
<p dir="auto">The SquashFS, <code>xz</code>, <code>lrzip</code>, <code>zpaq</code> and <code>wimlib</code> tests were all done on
an 8 core Intel(R) Xeon(R) E-2286M CPU @ 2.40GHz with 64 GiB of RAM.</p>
<p dir="auto">The Cromfs and EROFS tests were done with an older version of DwarFS
on a 6 core Intel(R) Xeon(R) CPU D-1528 @ 1.90GHz with 64 GiB of RAM.</p>
<p dir="auto">The systems were mostly idle during all of the tests.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With SquashFS</h3><a id="user-content-with-squashfs" aria-label="Permalink: With SquashFS" href="#with-squashfs"></a></p>
<p dir="auto">The source directory contained <strong>1139 different Perl installations</strong>
from 284 distinct releases, a total of 47.65 GiB of data in 1,927,501
files and 330,733 directories. The source directory was freshly
unpacked from a tar archive to an XFS partition on a 970 EVO Plus 2TB
NVME drive, so most of its contents were likely cached.</p>
<p dir="auto">I'm using the same compression type and compression level for
SquashFS that is the default setting for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on perl-install-zstd.squashfs, block size 131072.
[=========================================================/] 2107401/2107401 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 4637597.63 Kbytes (4528.90 Mbytes)
        9.29% of uncompressed filesystem size (49922299.04 Kbytes)
Inode table size 19100802 bytes (18653.13 Kbytes)
        26.06% of uncompressed inode table size (73307702 bytes)
Directory table size 19128340 bytes (18680.02 Kbytes)
        46.28% of uncompressed directory table size (41335540 bytes)
Number of duplicate files found 1780387
Number of inodes 2255794
Number of files 1925061
Number of fragments 28713
Number of symbolic links  0
Number of device nodes 0
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 330733
Number of ids (unique uids + gids) 2
Number of uids 1
        mhx (1000)
Number of gids 1
        users (100)

real    32m54.713s
user    501m46.382s
sys     0m58.528s"><pre><code>$ time mksquashfs install perl-install.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on perl-install-zstd.squashfs, block size 131072.
[=========================================================/] 2107401/2107401 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 4637597.63 Kbytes (4528.90 Mbytes)
        9.29% of uncompressed filesystem size (49922299.04 Kbytes)
Inode table size 19100802 bytes (18653.13 Kbytes)
        26.06% of uncompressed inode table size (73307702 bytes)
Directory table size 19128340 bytes (18680.02 Kbytes)
        46.28% of uncompressed directory table size (41335540 bytes)
Number of duplicate files found 1780387
Number of inodes 2255794
Number of files 1925061
Number of fragments 28713
Number of symbolic links  0
Number of device nodes 0
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 330733
Number of ids (unique uids + gids) 2
Number of uids 1
        mhx (1000)
Number of gids 1
        users (100)

real    32m54.713s
user    501m46.382s
sys     0m58.528s
</code></pre></div>
<p dir="auto">For DwarFS, I'm sticking to the defaults:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install.dwarfs
I 11:33:33.310931 scanning install
I 11:33:39.026712 waiting for background scanners...
I 11:33:50.681305 assigning directory and link inodes...
I 11:33:50.888441 finding duplicate files...
I 11:34:01.120800 saved 28.2 GiB / 47.65 GiB in 1782826/1927501 duplicate files
I 11:34:01.122608 waiting for inode scanners...
I 11:34:12.839065 assigning device inodes...
I 11:34:12.875520 assigning pipe/socket inodes...
I 11:34:12.910431 building metadata...
I 11:34:12.910524 building blocks...
I 11:34:12.910594 saving names and links...
I 11:34:12.910691 bloom filter size: 32 KiB
I 11:34:12.910760 ordering 144675 inodes using nilsimsa similarity...
I 11:34:12.915555 nilsimsa: depth=20000 (1000), limit=255
I 11:34:13.052525 updating name and link indices...
I 11:34:13.276233 pre-sorted index (660176 name, 366179 path lookups) [360.6ms]
I 11:35:44.039375 144675 inodes ordered [91.13s]
I 11:35:44.041427 waiting for segmenting/blockifying to finish...
I 11:37:38.823902 bloom filter reject rate: 96.017% (TPR=0.244%, lookups=4740563665)
I 11:37:38.823963 segmentation matches: good=454708, bad=6819, total=464247
I 11:37:38.824005 segmentation collisions: L1=0.008%, L2=0.000% [2233254 hashes]
I 11:37:38.824038 saving chunks...
I 11:37:38.860939 saving directories...
I 11:37:41.318747 waiting for compression to finish...
I 11:38:56.046809 compressed 47.65 GiB to 430.9 MiB (ratio=0.00883101)
I 11:38:56.304922 filesystem created without errors [323s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
330733 dirs, 0/2440 soft/hard links, 1927501/1927501 files, 0 other
original size: 47.65 GiB, dedupe: 28.2 GiB (1782826 files), segment: 15.19 GiB
filesystem: 4.261 GiB in 273 blocks (319178 chunks, 144675/144675 inodes)
compressed filesystem: 273 blocks/430.9 MiB written [depth: 20000]
█████████████████████████████████████████████████████████████████████████████▏100% |

real    5m23.030s
user    78m7.554s
sys     1m47.968s"><pre><code>$ time mkdwarfs -i install -o perl-install.dwarfs
I 11:33:33.310931 scanning install
I 11:33:39.026712 waiting for background scanners...
I 11:33:50.681305 assigning directory and link inodes...
I 11:33:50.888441 finding duplicate files...
I 11:34:01.120800 saved 28.2 GiB / 47.65 GiB in 1782826/1927501 duplicate files
I 11:34:01.122608 waiting for inode scanners...
I 11:34:12.839065 assigning device inodes...
I 11:34:12.875520 assigning pipe/socket inodes...
I 11:34:12.910431 building metadata...
I 11:34:12.910524 building blocks...
I 11:34:12.910594 saving names and links...
I 11:34:12.910691 bloom filter size: 32 KiB
I 11:34:12.910760 ordering 144675 inodes using nilsimsa similarity...
I 11:34:12.915555 nilsimsa: depth=20000 (1000), limit=255
I 11:34:13.052525 updating name and link indices...
I 11:34:13.276233 pre-sorted index (660176 name, 366179 path lookups) [360.6ms]
I 11:35:44.039375 144675 inodes ordered [91.13s]
I 11:35:44.041427 waiting for segmenting/blockifying to finish...
I 11:37:38.823902 bloom filter reject rate: 96.017% (TPR=0.244%, lookups=4740563665)
I 11:37:38.823963 segmentation matches: good=454708, bad=6819, total=464247
I 11:37:38.824005 segmentation collisions: L1=0.008%, L2=0.000% [2233254 hashes]
I 11:37:38.824038 saving chunks...
I 11:37:38.860939 saving directories...
I 11:37:41.318747 waiting for compression to finish...
I 11:38:56.046809 compressed 47.65 GiB to 430.9 MiB (ratio=0.00883101)
I 11:38:56.304922 filesystem created without errors [323s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
330733 dirs, 0/2440 soft/hard links, 1927501/1927501 files, 0 other
original size: 47.65 GiB, dedupe: 28.2 GiB (1782826 files), segment: 15.19 GiB
filesystem: 4.261 GiB in 273 blocks (319178 chunks, 144675/144675 inodes)
compressed filesystem: 273 blocks/430.9 MiB written [depth: 20000]
█████████████████████████████████████████████████████████████████████████████▏100% |

real    5m23.030s
user    78m7.554s
sys     1m47.968s
</code></pre></div>
<p dir="auto">So in this comparison, <code>mkdwarfs</code> is <strong>more than 6 times faster</strong> than <code>mksquashfs</code>,
both in terms of CPU time and wall clock time.</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install.*fs
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs"><pre><code>$ ll perl-install.*fs
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
</code></pre></div>
<p dir="auto">In terms of compression ratio, the <strong>DwarFS file system is more than 10 times
smaller than the SquashFS file system</strong>. With DwarFS, the content has been
<strong>compressed down to less than 0.9% (!) of its original size</strong>. This compression
ratio only considers the data stored in the individual files, not the actual
disk space used. On the original XFS file system, according to <code>du</code>, the
source folder uses 52 GiB, so <strong>the DwarFS image actually only uses 0.8% of
the original space</strong>.</p>
<p dir="auto">Here's another comparison using <code>lzma</code> compression instead of <code>zstd</code>:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install-lzma.squashfs -comp lzma

real    13m42.825s
user    205m40.851s
sys     3m29.088s"><pre><code>$ time mksquashfs install perl-install-lzma.squashfs -comp lzma

real    13m42.825s
user    205m40.851s
sys     3m29.088s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install-lzma.dwarfs -l9

real    3m43.937s
user    49m45.295s
sys     1m44.550s"><pre><code>$ time mkdwarfs -i install -o perl-install-lzma.dwarfs -l9

real    3m43.937s
user    49m45.295s
sys     1m44.550s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install-lzma.*fs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-lzma.dwarfs
-rw-r--r-- 1 mhx users 3838406656 Mar  3 20:50 perl-install-lzma.squashfs"><pre><code>$ ll perl-install-lzma.*fs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-lzma.dwarfs
-rw-r--r-- 1 mhx users 3838406656 Mar  3 20:50 perl-install-lzma.squashfs
</code></pre></div>
<p dir="auto">It's immediately obvious that the runs are significantly faster and the
resulting images are significantly smaller. Still, <code>mkdwarfs</code> is about
<strong>4 times faster</strong> and produces and image that's <strong>12 times smaller</strong> than
the SquashFS image. The DwarFS image is only 0.6% of the original file size.</p>
<p dir="auto">So, why not use <code>lzma</code> instead of <code>zstd</code> by default? The reason is that <code>lzma</code>
is about an order of magnitude slower to decompress than <code>zstd</code>. If you're
only accessing data on your compressed filesystem occasionally, this might
not be a big deal, but if you use it extensively, <code>zstd</code> will result in
better performance.</p>
<p dir="auto">The comparisons above are not completely fair. <code>mksquashfs</code> by default
uses a block size of 128KiB, whereas <code>mkdwarfs</code> uses 16MiB blocks by default,
or even 64MiB blocks with <code>-l9</code>. When using identical block sizes for both
file systems, the difference, quite expectedly, becomes a lot less dramatic:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install-lzma-1M.squashfs -comp lzma -b 1M

real    15m43.319s
user    139m24.533s
sys     0m45.132s"><pre><code>$ time mksquashfs install perl-install-lzma-1M.squashfs -comp lzma -b 1M

real    15m43.319s
user    139m24.533s
sys     0m45.132s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install-lzma-1M.dwarfs -l9 -S20 -B3

real    4m25.973s
user    52m15.100s
sys     7m41.889s"><pre><code>$ time mkdwarfs -i install -o perl-install-lzma-1M.dwarfs -l9 -S20 -B3

real    4m25.973s
user    52m15.100s
sys     7m41.889s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install*.*fs
-rw-r--r-- 1 mhx users  935953866 Mar 13 12:12 perl-install-lzma-1M.dwarfs
-rw-r--r-- 1 mhx users 3407474688 Mar  3 21:54 perl-install-lzma-1M.squashfs"><pre><code>$ ll perl-install*.*fs
-rw-r--r-- 1 mhx users  935953866 Mar 13 12:12 perl-install-lzma-1M.dwarfs
-rw-r--r-- 1 mhx users 3407474688 Mar  3 21:54 perl-install-lzma-1M.squashfs
</code></pre></div>
<p dir="auto">Even this is <em>still</em> not entirely fair, as it uses a feature (<code>-B3</code>) that allows
DwarFS to reference file chunks from up to two previous filesystem blocks.</p>
<p dir="auto">But the point is that this is really where SquashFS tops out, as it doesn't
support larger block sizes or back-referencing. And as you'll see below, the
larger blocks that DwarFS is using by default don't necessarily negatively
impact performance.</p>
<p dir="auto">DwarFS also features an option to recompress an existing file system with
a different compression algorithm. This can be useful as it allows relatively
fast experimentation with different algorithms and options without requiring
a full rebuild of the file system. For example, recompressing the above file
system with the best possible compression (<code>-l 9</code>):</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs --recompress -i perl-install.dwarfs -o perl-lzma-re.dwarfs -l9
I 20:28:03.246534 filesystem rewrittenwithout errors [148.3s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
filesystem: 4.261 GiB in 273 blocks (0 chunks, 0 inodes)
compressed filesystem: 273/273 blocks/372.7 MiB written
████████████████████████████████████████████████████████████████████▏100% \

real    2m28.279s
user    37m8.825s
sys     0m43.256s"><pre><code>$ time mkdwarfs --recompress -i perl-install.dwarfs -o perl-lzma-re.dwarfs -l9
I 20:28:03.246534 filesystem rewrittenwithout errors [148.3s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
filesystem: 4.261 GiB in 273 blocks (0 chunks, 0 inodes)
compressed filesystem: 273/273 blocks/372.7 MiB written
████████████████████████████████████████████████████████████████████▏100% \

real    2m28.279s
user    37m8.825s
sys     0m43.256s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-*.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 390845518 Mar  4 20:28 perl-lzma-re.dwarfs
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-lzma.dwarfs"><pre><code>$ ll perl-*.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 390845518 Mar  4 20:28 perl-lzma-re.dwarfs
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-lzma.dwarfs
</code></pre></div>
<p dir="auto">Note that while the recompressed filesystem is smaller than the original image,
it is still a lot bigger than the filesystem we previously build with <code>-l9</code>.
The reason is that the recompressed image still uses the same block size, and
the block size cannot be changed by recompressing.</p>
<p dir="auto">In terms of how fast the file system is when using it, a quick test
I've done is to freshly mount the filesystem created above and run
each of the 1139 <code>perl</code> executables to print their version.</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c &quot;umount mnt&quot; -p &quot;umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1&quot; -P procs 5 20 -D 5 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.810 s ±  0.013 s    [User: 1.847 s, System: 0.623 s]
  Range (min … max):    1.788 s …  1.825 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.333 s ±  0.009 s    [User: 1.993 s, System: 0.656 s]
  Range (min … max):    1.321 s …  1.354 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.181 s ±  0.018 s    [User: 2.086 s, System: 0.712 s]
  Range (min … max):    1.165 s …  1.214 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.149 s ±  0.015 s    [User: 2.128 s, System: 0.781 s]
  Range (min … max):    1.136 s …  1.186 s    10 runs"><pre><code>$ hyperfine -c "umount mnt" -p "umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1" -P procs 5 20 -D 5 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.810 s ±  0.013 s    [User: 1.847 s, System: 0.623 s]
  Range (min … max):    1.788 s …  1.825 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.333 s ±  0.009 s    [User: 1.993 s, System: 0.656 s]
  Range (min … max):    1.321 s …  1.354 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.181 s ±  0.018 s    [User: 2.086 s, System: 0.712 s]
  Range (min … max):    1.165 s …  1.214 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.149 s ±  0.015 s    [User: 2.128 s, System: 0.781 s]
  Range (min … max):    1.136 s …  1.186 s    10 runs
</code></pre></div>
<p dir="auto">These timings are for <em>initial</em> runs on a freshly mounted file system,
running 5, 10, 15 and 20 processes in parallel. 1.1 seconds means that
it takes only about 1 millisecond per Perl binary.</p>
<p dir="auto">Following are timings for <em>subsequent</em> runs, both on DwarFS (at <code>mnt</code>)
and the original XFS (at <code>install</code>). DwarFS is around 15% slower here:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -P procs 10 20 -D 10 -w1 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot; &quot;ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     347.0 ms ±   7.2 ms    [User: 1.755 s, System: 0.452 s]
  Range (min … max):   341.3 ms … 365.2 ms    10 runs

Benchmark #2: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     302.5 ms ±   3.3 ms    [User: 1.656 s, System: 0.377 s]
  Range (min … max):   297.1 ms … 308.7 ms    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     342.2 ms ±   4.1 ms    [User: 1.766 s, System: 0.451 s]
  Range (min … max):   336.0 ms … 349.7 ms    10 runs

Benchmark #4: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     302.0 ms ±   3.0 ms    [User: 1.659 s, System: 0.374 s]
  Range (min … max):   297.0 ms … 305.4 ms    10 runs

Summary
  'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'' ran
    1.00 ± 0.01 times faster than 'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null''
    1.13 ± 0.02 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null''
    1.15 ± 0.03 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null''"><pre><code>$ hyperfine -P procs 10 20 -D 10 -w1 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'" "ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     347.0 ms ±   7.2 ms    [User: 1.755 s, System: 0.452 s]
  Range (min … max):   341.3 ms … 365.2 ms    10 runs

Benchmark #2: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     302.5 ms ±   3.3 ms    [User: 1.656 s, System: 0.377 s]
  Range (min … max):   297.1 ms … 308.7 ms    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     342.2 ms ±   4.1 ms    [User: 1.766 s, System: 0.451 s]
  Range (min … max):   336.0 ms … 349.7 ms    10 runs

Benchmark #4: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     302.0 ms ±   3.0 ms    [User: 1.659 s, System: 0.374 s]
  Range (min … max):   297.0 ms … 305.4 ms    10 runs

Summary
  'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'' ran
    1.00 ± 0.01 times faster than 'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null''
    1.13 ± 0.02 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null''
    1.15 ± 0.03 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null''
</code></pre></div>
<p dir="auto">Using the lzma-compressed file system, the metrics for <em>initial</em> runs look
considerably worse (about an order of magnitude):</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c &quot;umount mnt&quot; -p &quot;umount mnt; dwarfs perl-install-lzma.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1&quot; -P procs 5 20 -D 5 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     10.660 s ±  0.057 s    [User: 1.952 s, System: 0.729 s]
  Range (min … max):   10.615 s … 10.811 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.092 s ±  0.021 s    [User: 1.979 s, System: 0.680 s]
  Range (min … max):    9.059 s …  9.126 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.012 s ±  0.188 s    [User: 2.077 s, System: 0.702 s]
  Range (min … max):    8.839 s …  9.277 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.004 s ±  0.298 s    [User: 2.134 s, System: 0.736 s]
  Range (min … max):    8.611 s …  9.555 s    10 runs"><pre><code>$ hyperfine -c "umount mnt" -p "umount mnt; dwarfs perl-install-lzma.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1" -P procs 5 20 -D 5 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     10.660 s ±  0.057 s    [User: 1.952 s, System: 0.729 s]
  Range (min … max):   10.615 s … 10.811 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.092 s ±  0.021 s    [User: 1.979 s, System: 0.680 s]
  Range (min … max):    9.059 s …  9.126 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.012 s ±  0.188 s    [User: 2.077 s, System: 0.702 s]
  Range (min … max):    8.839 s …  9.277 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.004 s ±  0.298 s    [User: 2.134 s, System: 0.736 s]
  Range (min … max):    8.611 s …  9.555 s    10 runs
</code></pre></div>
<p dir="auto">So you might want to consider using <code>zstd</code> instead of <code>lzma</code> if you'd
like to optimize for file system performance. It's also the default
compression used by <code>mkdwarfs</code>.</p>
<p dir="auto">Now here's a comparison with the SquashFS filesystem:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c 'sudo umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs-zstd &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot; -p 'sudo umount mnt; sudo mount -t squashfs perl-install.squashfs mnt; sleep 1' -n squashfs-zstd &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: dwarfs-zstd
  Time (mean ± σ):      1.151 s ±  0.015 s    [User: 2.147 s, System: 0.769 s]
  Range (min … max):    1.118 s …  1.174 s    10 runs

Benchmark #2: squashfs-zstd
  Time (mean ± σ):      6.733 s ±  0.007 s    [User: 3.188 s, System: 17.015 s]
  Range (min … max):    6.721 s …  6.743 s    10 runs

Summary
  'dwarfs-zstd' ran
    5.85 ± 0.08 times faster than 'squashfs-zstd'"><pre><code>$ hyperfine -c 'sudo umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs-zstd "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'" -p 'sudo umount mnt; sudo mount -t squashfs perl-install.squashfs mnt; sleep 1' -n squashfs-zstd "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: dwarfs-zstd
  Time (mean ± σ):      1.151 s ±  0.015 s    [User: 2.147 s, System: 0.769 s]
  Range (min … max):    1.118 s …  1.174 s    10 runs

Benchmark #2: squashfs-zstd
  Time (mean ± σ):      6.733 s ±  0.007 s    [User: 3.188 s, System: 17.015 s]
  Range (min … max):    6.721 s …  6.743 s    10 runs

Summary
  'dwarfs-zstd' ran
    5.85 ± 0.08 times faster than 'squashfs-zstd'
</code></pre></div>
<p dir="auto">So, DwarFS is almost six times faster than SquashFS. But what's more,
SquashFS also uses significantly more CPU power. However, the numbers
shown above for DwarFS obviously don't include the time spent in the
<code>dwarfs</code> process, so I repeated the test outside of hyperfine:</p>
<div data-snippet-clipboard-copy-content="$ time dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4 -f

real    0m4.569s
user    0m2.154s
sys     0m1.846s"><pre><code>$ time dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4 -f

real    0m4.569s
user    0m2.154s
sys     0m1.846s
</code></pre></div>
<p dir="auto">So, in total, DwarFS was using 5.7 seconds of CPU time, whereas
SquashFS was using 20.2 seconds, almost four times as much. Ignore
the 'real' time, this is only how long it took me to unmount the
file system again after mounting it.</p>
<p dir="auto">Another real-life test was to build and test a Perl module with 624
different Perl versions in the compressed file system. The module I've
used, <a href="https://github.com/mhx/Tie-Hash-Indexed">Tie::Hash::Indexed</a>,
has an XS component that requires a C compiler to build. So this really
accesses a lot of different stuff in the file system:</p>
<ul dir="auto">
<li>
<p dir="auto">The <code>perl</code> executables and its shared libraries</p>
</li>
<li>
<p dir="auto">The Perl modules used for writing the Makefile</p>
</li>
<li>
<p dir="auto">Perl's C header files used for building the module</p>
</li>
<li>
<p dir="auto">More Perl modules used for running the tests</p>
</li>
</ul>
<p dir="auto">I wrote a little script to be able to run multiple builds in parallel:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#!/bin/bash
set -eu
perl=$1
dir=$(echo &quot;$perl&quot; | cut -d/ --output-delimiter=- -f5,6)
rsync -a Tie-Hash-Indexed/ $dir/
cd $dir
$1 Makefile.PL >/dev/null 2>&amp;1
make test >/dev/null 2>&amp;1
cd ..
rm -rf $dir
echo $perl"><pre><span><span>#!</span>/bin/bash</span>
<span>set</span> -eu
perl=<span>$1</span>
dir=<span><span>$(</span>echo <span><span>"</span><span>$perl</span><span>"</span></span> <span>|</span> cut -d/ --output-delimiter=- -f5,6<span>)</span></span>
rsync -a Tie-Hash-Indexed/ <span>$dir</span>/
<span>cd</span> <span>$dir</span>
<span>$1</span> Makefile.PL <span>&gt;</span>/dev/null <span>2&gt;&amp;1</span>
make <span>test</span> <span>&gt;</span>/dev/null <span>2&gt;&amp;1</span>
<span>cd</span> ..
rm -rf <span>$dir</span>
<span>echo</span> <span>$perl</span></pre></div>
<p dir="auto">The following command will run up to 16 builds in parallel on the 8 core
Xeon CPU, including debug, optimized and threaded versions of all Perl
releases between 5.10.0 and 5.33.3, a total of 624 <code>perl</code> installations:</p>
<div data-snippet-clipboard-copy-content="$ time ls -1 /tmp/perl/install/*/perl-5.??.?/bin/perl5* | sort -t / -k 8 | xargs -d $'\n' -P 16 -n 1 ./build.sh"><pre><code>$ time ls -1 /tmp/perl/install/*/perl-5.??.?/bin/perl5* | sort -t / -k 8 | xargs -d $'\n' -P 16 -n 1 ./build.sh
</code></pre></div>
<p dir="auto">Tests were done with a cleanly mounted file system to make sure the caches
were empty. <code>ccache</code> was primed to make sure all compiler runs could be
satisfied from the cache. With SquashFS, the timing was:</p>
<div data-snippet-clipboard-copy-content="real    0m52.385s
user    8m10.333s
sys     4m10.056s"><pre><code>real    0m52.385s
user    8m10.333s
sys     4m10.056s
</code></pre></div>
<p dir="auto">And with DwarFS:</p>
<div data-snippet-clipboard-copy-content="real    0m50.469s
user    9m22.597s
sys     1m18.469s"><pre><code>real    0m50.469s
user    9m22.597s
sys     1m18.469s
</code></pre></div>
<p dir="auto">So, frankly, not much of a difference, with DwarFS being just a bit faster.
The <code>dwarfs</code> process itself used:</p>
<div data-snippet-clipboard-copy-content="real    0m56.686s
user    0m18.857s
sys     0m21.058s"><pre><code>real    0m56.686s
user    0m18.857s
sys     0m21.058s
</code></pre></div>
<p dir="auto">So again, DwarFS used less raw CPU power overall, but in terms of wallclock
time, the difference is really marginal.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With SquashFS &amp; xz</h3><a id="user-content-with-squashfs--xz" aria-label="Permalink: With SquashFS &amp; xz" href="#with-squashfs--xz"></a></p>
<p dir="auto">This test uses slightly less pathological input data: the root filesystem of
a recent Raspberry Pi OS release. This file system also contains device inodes,
so in order to preserve those, we pass <code>--with-devices</code> to <code>mkdwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mkdwarfs -i raspbian -o raspbian.dwarfs --with-devices
I 21:30:29.812562 scanning raspbian
I 21:30:29.908984 waiting for background scanners...
I 21:30:30.217446 assigning directory and link inodes...
I 21:30:30.221941 finding duplicate files...
I 21:30:30.288099 saved 31.05 MiB / 1007 MiB in 1617/34582 duplicate files
I 21:30:30.288143 waiting for inode scanners...
I 21:30:31.393710 assigning device inodes...
I 21:30:31.394481 assigning pipe/socket inodes...
I 21:30:31.395196 building metadata...
I 21:30:31.395230 building blocks...
I 21:30:31.395291 saving names and links...
I 21:30:31.395374 ordering 32965 inodes using nilsimsa similarity...
I 21:30:31.396254 nilsimsa: depth=20000 (1000), limit=255
I 21:30:31.407967 pre-sorted index (46431 name, 2206 path lookups) [11.66ms]
I 21:30:31.410089 updating name and link indices...
I 21:30:38.178505 32965 inodes ordered [6.783s]
I 21:30:38.179417 waiting for segmenting/blockifying to finish...
I 21:31:06.248304 saving chunks...
I 21:31:06.251998 saving directories...
I 21:31:06.402559 waiting for compression to finish...
I 21:31:16.425563 compressed 1007 MiB to 287 MiB (ratio=0.285036)
I 21:31:16.464772 filesystem created without errors [46.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
4435 dirs, 5908/0 soft/hard links, 34582/34582 files, 7 other
original size: 1007 MiB, dedupe: 31.05 MiB (1617 files), segment: 47.23 MiB
filesystem: 928.4 MiB in 59 blocks (38890 chunks, 32965/32965 inodes)
compressed filesystem: 59 blocks/287 MiB written [depth: 20000]
████████████████████████████████████████████████████████████████████▏100% |

real    0m46.711s
user    10m39.038s
sys     0m8.123s"><pre><code>$ time sudo mkdwarfs -i raspbian -o raspbian.dwarfs --with-devices
I 21:30:29.812562 scanning raspbian
I 21:30:29.908984 waiting for background scanners...
I 21:30:30.217446 assigning directory and link inodes...
I 21:30:30.221941 finding duplicate files...
I 21:30:30.288099 saved 31.05 MiB / 1007 MiB in 1617/34582 duplicate files
I 21:30:30.288143 waiting for inode scanners...
I 21:30:31.393710 assigning device inodes...
I 21:30:31.394481 assigning pipe/socket inodes...
I 21:30:31.395196 building metadata...
I 21:30:31.395230 building blocks...
I 21:30:31.395291 saving names and links...
I 21:30:31.395374 ordering 32965 inodes using nilsimsa similarity...
I 21:30:31.396254 nilsimsa: depth=20000 (1000), limit=255
I 21:30:31.407967 pre-sorted index (46431 name, 2206 path lookups) [11.66ms]
I 21:30:31.410089 updating name and link indices...
I 21:30:38.178505 32965 inodes ordered [6.783s]
I 21:30:38.179417 waiting for segmenting/blockifying to finish...
I 21:31:06.248304 saving chunks...
I 21:31:06.251998 saving directories...
I 21:31:06.402559 waiting for compression to finish...
I 21:31:16.425563 compressed 1007 MiB to 287 MiB (ratio=0.285036)
I 21:31:16.464772 filesystem created without errors [46.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
4435 dirs, 5908/0 soft/hard links, 34582/34582 files, 7 other
original size: 1007 MiB, dedupe: 31.05 MiB (1617 files), segment: 47.23 MiB
filesystem: 928.4 MiB in 59 blocks (38890 chunks, 32965/32965 inodes)
compressed filesystem: 59 blocks/287 MiB written [depth: 20000]
████████████████████████████████████████████████████████████████████▏100% |

real    0m46.711s
user    10m39.038s
sys     0m8.123s
</code></pre></div>
<p dir="auto">Again, SquashFS uses the same compression options:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mksquashfs raspbian raspbian.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on raspbian.squashfs, block size 131072.
[===============================================================\] 39232/39232 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 371934.50 Kbytes (363.22 Mbytes)
        35.98% of uncompressed filesystem size (1033650.60 Kbytes)
Inode table size 399913 bytes (390.54 Kbytes)
        26.53% of uncompressed inode table size (1507581 bytes)
Directory table size 408749 bytes (399.17 Kbytes)
        42.31% of uncompressed directory table size (966174 bytes)
Number of duplicate files found 1618
Number of inodes 44932
Number of files 34582
Number of fragments 3290
Number of symbolic links  5908
Number of device nodes 7
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 4435
Number of ids (unique uids + gids) 18
Number of uids 5
        root (0)
        mhx (1000)
        unknown (103)
        shutdown (6)
        unknown (106)
Number of gids 15
        root (0)
        unknown (109)
        unknown (42)
        unknown (1000)
        users (100)
        unknown (43)
        tty (5)
        unknown (108)
        unknown (111)
        unknown (110)
        unknown (50)
        mail (12)
        nobody (65534)
        adm (4)
        mem (8)

real    0m50.124s
user    9m41.708s
sys     0m1.727s"><pre><code>$ time sudo mksquashfs raspbian raspbian.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on raspbian.squashfs, block size 131072.
[===============================================================\] 39232/39232 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 371934.50 Kbytes (363.22 Mbytes)
        35.98% of uncompressed filesystem size (1033650.60 Kbytes)
Inode table size 399913 bytes (390.54 Kbytes)
        26.53% of uncompressed inode table size (1507581 bytes)
Directory table size 408749 bytes (399.17 Kbytes)
        42.31% of uncompressed directory table size (966174 bytes)
Number of duplicate files found 1618
Number of inodes 44932
Number of files 34582
Number of fragments 3290
Number of symbolic links  5908
Number of device nodes 7
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 4435
Number of ids (unique uids + gids) 18
Number of uids 5
        root (0)
        mhx (1000)
        unknown (103)
        shutdown (6)
        unknown (106)
Number of gids 15
        root (0)
        unknown (109)
        unknown (42)
        unknown (1000)
        users (100)
        unknown (43)
        tty (5)
        unknown (108)
        unknown (111)
        unknown (110)
        unknown (50)
        mail (12)
        nobody (65534)
        adm (4)
        mem (8)

real    0m50.124s
user    9m41.708s
sys     0m1.727s
</code></pre></div>
<p dir="auto">The difference in speed is almost negligible. SquashFS is just a bit
slower here. In terms of compression, the difference also isn't huge:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian.* *.xz
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
-rw-r--r-- 1 root root  287M Mar  4 21:31 raspbian.dwarfs
-rw-r--r-- 1 root root  364M Mar  4 21:33 raspbian.squashfs"><pre><code>$ ls -lh raspbian.* *.xz
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
-rw-r--r-- 1 root root  287M Mar  4 21:31 raspbian.dwarfs
-rw-r--r-- 1 root root  364M Mar  4 21:33 raspbian.squashfs
</code></pre></div>
<p dir="auto">Interestingly, <code>xz</code> actually can't compress the whole original image
better than DwarFS.</p>
<p dir="auto">We can even again try to increase the DwarFS compression level:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mkdwarfs -i raspbian -o raspbian-9.dwarfs --with-devices -l9

real    0m54.161s
user    8m40.109s
sys     0m7.101s"><pre><code>$ time sudo mkdwarfs -i raspbian -o raspbian-9.dwarfs --with-devices -l9

real    0m54.161s
user    8m40.109s
sys     0m7.101s
</code></pre></div>
<p dir="auto">Now that actually gets the DwarFS image size well below that of the
<code>xz</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian-9.dwarfs *.xz
-rw-r--r-- 1 root root  244M Mar  4 21:36 raspbian-9.dwarfs
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz"><pre><code>$ ls -lh raspbian-9.dwarfs *.xz
-rw-r--r-- 1 root root  244M Mar  4 21:36 raspbian-9.dwarfs
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
</code></pre></div>
<p dir="auto">Even if you actually build a tarball and compress that (instead of
compressing the EXT4 file system itself), <code>xz</code> isn't quite able to
match the DwarFS image size:</p>
<div data-snippet-clipboard-copy-content="$ time sudo tar cf - raspbian | xz -9 -vT 0 >raspbian.tar.xz
  100 %     246.9 MiB / 1,037.2 MiB = 0.238    13 MiB/s       1:18

real    1m18.226s
user    6m35.381s
sys     0m2.205s"><pre><code>$ time sudo tar cf - raspbian | xz -9 -vT 0 &gt;raspbian.tar.xz
  100 %     246.9 MiB / 1,037.2 MiB = 0.238    13 MiB/s       1:18

real    1m18.226s
user    6m35.381s
sys     0m2.205s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz"><pre><code>$ ls -lh raspbian.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
</code></pre></div>
<p dir="auto">DwarFS also comes with the <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsextract.md">dwarfsextract</a> tool
that allows extraction of a filesystem image without the FUSE driver.
So here's a comparison of the extraction speed:</p>
<div data-snippet-clipboard-copy-content="$ time sudo tar xf raspbian.tar.xz -C out1

real    0m12.846s
user    0m12.313s
sys     0m1.616s"><pre><code>$ time sudo tar xf raspbian.tar.xz -C out1

real    0m12.846s
user    0m12.313s
sys     0m1.616s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time sudo dwarfsextract -i raspbian-9.dwarfs -o out2

real    0m3.825s
user    0m13.234s
sys     0m1.382s"><pre><code>$ time sudo dwarfsextract -i raspbian-9.dwarfs -o out2

real    0m3.825s
user    0m13.234s
sys     0m1.382s
</code></pre></div>
<p dir="auto">So, <code>dwarfsextract</code> is almost 4 times faster thanks to using multiple
worker threads for decompression. It's writing about 300 MiB/s in this
example.</p>
<p dir="auto">Another nice feature of <code>dwarfsextract</code> is that it allows you to directly
output data in an archive format, so you could create a tarball from
your image without extracting the files to disk:</p>
<div data-snippet-clipboard-copy-content="$ dwarfsextract -i raspbian-9.dwarfs -f ustar | xz -9 -T0 >raspbian2.tar.xz"><pre><code>$ dwarfsextract -i raspbian-9.dwarfs -f ustar | xz -9 -T0 &gt;raspbian2.tar.xz
</code></pre></div>
<p dir="auto">This has the interesting side-effect that the resulting tarball will
likely be smaller than the one built straight from the directory:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian*.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
-rw-r--r-- 1 mhx users 240M Mar  4 23:52 raspbian2.tar.xz"><pre><code>$ ls -lh raspbian*.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
-rw-r--r-- 1 mhx users 240M Mar  4 23:52 raspbian2.tar.xz
</code></pre></div>
<p dir="auto">That's because <code>dwarfsextract</code> writes files in inode-order, and by
default inodes are ordered by similarity for the best possible
compression.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With lrzip</h3><a id="user-content-with-lrzip" aria-label="Permalink: With lrzip" href="#with-lrzip"></a></p>
<p dir="auto"><a href="https://github.com/ckolivas/lrzip">lrzip</a> is a compression utility
targeted especially at compressing large files. From its description,
it looks like it does something very similar to DwarFS, i.e. it looks
for duplicate segments before passsing the de-duplicated data on to
an <code>lzma</code> compressor.</p>
<p dir="auto">When I first read about <code>lrzip</code>, I was pretty certain it would easily
beat DwarFS. So let's take a look. <code>lrzip</code> operates on a single file,
so it's necessary to first build a tarball:</p>
<div data-snippet-clipboard-copy-content="$ time tar cf perl-install.tar install

real    2m9.568s
user    0m3.757s
sys     0m26.623s"><pre><code>$ time tar cf perl-install.tar install

real    2m9.568s
user    0m3.757s
sys     0m26.623s
</code></pre></div>
<p dir="auto">Now we can run <code>lrzip</code>:</p>
<div data-snippet-clipboard-copy-content="$ time lrzip -vL9 -o perl-install.tar.lrzip perl-install.tar
The following options are in effect for this COMPRESSION.
Threading is ENABLED. Number of CPUs detected: 16
Detected 67106172928 bytes ram
Compression level 9
Nice Value: 19
Show Progress
Verbose
Output Filename Specified: perl-install.tar.lrzip
Temporary Directory set as: ./
Compression mode is: LZMA. LZO Compressibility testing enabled
Heuristically Computed Compression Window: 426 = 42600MB
File size: 52615639040
Will take 2 passes
Beginning rzip pre-processing phase
Beginning rzip pre-processing phase
perl-install.tar - Compression Ratio: 100.378. Average Compression Speed: 14.536MB/s.
Total time: 00:57:32.47

real    57m32.472s
user    81m44.104s
sys     4m50.221s"><pre><code>$ time lrzip -vL9 -o perl-install.tar.lrzip perl-install.tar
The following options are in effect for this COMPRESSION.
Threading is ENABLED. Number of CPUs detected: 16
Detected 67106172928 bytes ram
Compression level 9
Nice Value: 19
Show Progress
Verbose
Output Filename Specified: perl-install.tar.lrzip
Temporary Directory set as: ./
Compression mode is: LZMA. LZO Compressibility testing enabled
Heuristically Computed Compression Window: 426 = 42600MB
File size: 52615639040
Will take 2 passes
Beginning rzip pre-processing phase
Beginning rzip pre-processing phase
perl-install.tar - Compression Ratio: 100.378. Average Compression Speed: 14.536MB/s.
Total time: 00:57:32.47

real    57m32.472s
user    81m44.104s
sys     4m50.221s
</code></pre></div>
<p dir="auto">That definitely took a while. This is about an order of magnitude
slower than <code>mkdwarfs</code> and it barely makes use of the 8 cores.</p>
<div data-snippet-clipboard-copy-content="$ ll -h perl-install.tar.lrzip
-rw-r--r-- 1 mhx users 500M Mar  6 21:16 perl-install.tar.lrzip"><pre><code>$ ll -h perl-install.tar.lrzip
-rw-r--r-- 1 mhx users 500M Mar  6 21:16 perl-install.tar.lrzip
</code></pre></div>
<p dir="auto">This is a surprisingly disappointing result. The archive is 65% larger
than a DwarFS image at <code>-l9</code> that takes less than 4 minutes to build.
Also, you can't just access the files in the <code>.lrzip</code> without fully
unpacking the archive first.</p>
<p dir="auto">That being said, it <em>is</em> better than just using <code>xz</code> on the tarball:</p>
<div data-snippet-clipboard-copy-content="$ time xz -T0 -v9 -c perl-install.tar >perl-install.tar.xz
perl-install.tar (1/1)
  100 %      4,317.0 MiB / 49.0 GiB = 0.086    24 MiB/s      34:55

real    34m55.450s
user    543m50.810s
sys     0m26.533s"><pre><code>$ time xz -T0 -v9 -c perl-install.tar &gt;perl-install.tar.xz
perl-install.tar (1/1)
  100 %      4,317.0 MiB / 49.0 GiB = 0.086    24 MiB/s      34:55

real    34m55.450s
user    543m50.810s
sys     0m26.533s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install.tar.xz -h
-rw-r--r-- 1 mhx users 4.3G Mar  6 22:59 perl-install.tar.xz"><pre><code>$ ll perl-install.tar.xz -h
-rw-r--r-- 1 mhx users 4.3G Mar  6 22:59 perl-install.tar.xz
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">With zpaq</h3><a id="user-content-with-zpaq" aria-label="Permalink: With zpaq" href="#with-zpaq"></a></p>
<p dir="auto"><a href="http://mattmahoney.net/dc/zpaq.html" rel="nofollow">zpaq</a> is a journaling backup
utility and archiver. Again, it appears to share some of the ideas in
DwarFS, like segmentation analysis, but it also adds some features on
top that make it useful for incremental backups. However, it's also
not usable as a file system, so data needs to be extracted before it
can be used.</p>
<p dir="auto">Anyway, how does it fare in terms of speed and compression performance?</p>
<div data-snippet-clipboard-copy-content="$ time zpaq a perl-install.zpaq install -m5"><pre><code>$ time zpaq a perl-install.zpaq install -m5
</code></pre></div>
<p dir="auto">After a few million lines of output that (I think) cannot be turned off:</p>
<div data-snippet-clipboard-copy-content="2258234 +added, 0 -removed.

0.000000 + (51161.953159 -> 8932.000297 -> 490.227707) = 490.227707 MB
2828.082 seconds (all OK)

real    47m8.104s
user    714m44.286s
sys     3m6.751s"><pre><code>2258234 +added, 0 -removed.

0.000000 + (51161.953159 -&gt; 8932.000297 -&gt; 490.227707) = 490.227707 MB
2828.082 seconds (all OK)

real    47m8.104s
user    714m44.286s
sys     3m6.751s
</code></pre></div>
<p dir="auto">So, it's an order of magnitude slower than <code>mkdwarfs</code> and uses 14 times
as much CPU resources as <code>mkdwarfs -l9</code>. The resulting archive it pretty
close in size to the default configuration DwarFS image, but it's more
than 50% bigger than the image produced by <code>mkdwarfs -l9</code>.</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install*.*
-rw-r--r-- 1 mhx users 490227707 Mar  7 01:38 perl-install.zpaq
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs"><pre><code>$ ll perl-install*.*
-rw-r--r-- 1 mhx users 490227707 Mar  7 01:38 perl-install.zpaq
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
</code></pre></div>
<p dir="auto">What's <em>really</em> surprising is how slow it is to extract the <code>zpaq</code>
archive again:</p>
<div data-snippet-clipboard-copy-content="$ time zpaq x perl-install.zpaq
2798.097 seconds (all OK)

real    46m38.117s
user    711m18.734s
sys     3m47.876s"><pre><code>$ time zpaq x perl-install.zpaq
2798.097 seconds (all OK)

real    46m38.117s
user    711m18.734s
sys     3m47.876s
</code></pre></div>
<p dir="auto">That's 700 times slower than extracting the DwarFS image.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With zpaqfranz</h3><a id="user-content-with-zpaqfranz" aria-label="Permalink: With zpaqfranz" href="#with-zpaqfranz"></a></p>
<p dir="auto"><a href="https://github.com/fcorbelli/zpaqfranz">zpaqfranz</a> is a derivative of zpaq.
Much to my delight, it doesn't generate millions of lines of output.
It claims to be multi-threaded and de-duplicating, so definitely worth
taking a look. Like zpaq, it supports incremental backups.</p>
<p dir="auto">We'll use a different input to compare zpaqfranz and DwarFS: The source code
of 670 different releases of the "wine" emulator. That's 73 gigabytes of data
in total, spread across slightly more than 3 million files. It's obviously
highly redundant and should thus be a good data set to compare the tools.
For reference, a <code>.tar.xz</code> of the directory is still 7 GiB in size and a
SquashFS image of the data gets down to around 1.6 GiB. An "optimized"
<code>.tar.xz</code>, where the input files were ordered by similarity, compresses down
to 399 MiB, almost 20 times better than without ordering.</p>
<p dir="auto">Now it's time to try zpaqfranz. The input data is stored on a fast SSD and a
large fraction of it is already in the file system cache from previous runs,
so disk I/O is not a bottleneck.</p>
<div data-snippet-clipboard-copy-content="$ time ./zpaqfranz a winesrc.zpaq winesrc
zpaqfranz v58.8k-JIT-L(2023-08-05)
Creating winesrc.zpaq at offset 0 + 0
Add 2024-01-11 07:25:22 3.117.413     69.632.090.852 (  64.85 GB) 16T (362.904 dirs)
3.480.317 +added, 0 -removed.

0 + (69.632.090.852 -> 8.347.553.798 -> 617.600.892) = 617.600.892 @ 58.38 MB/s

1137.441 seconds (000:18:57) (all OK)

real    18m58.632s
user    11m51.052s
sys     1m3.389s"><pre><code>$ time ./zpaqfranz a winesrc.zpaq winesrc
zpaqfranz v58.8k-JIT-L(2023-08-05)
Creating winesrc.zpaq at offset 0 + 0
Add 2024-01-11 07:25:22 3.117.413     69.632.090.852 (  64.85 GB) 16T (362.904 dirs)
3.480.317 +added, 0 -removed.

0 + (69.632.090.852 -&gt; 8.347.553.798 -&gt; 617.600.892) = 617.600.892 @ 58.38 MB/s

1137.441 seconds (000:18:57) (all OK)

real    18m58.632s
user    11m51.052s
sys     1m3.389s
</code></pre></div>
<p dir="auto">That is considerably faster than the original zpaq, and uses about 60 times
less CPU resources. The output file is 589 MiB, so slightly larger than both
the "optimized" <code>.tar.gz</code> and the zpaq output.</p>
<p dir="auto">How does <code>mkdwarfs</code> do?</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i winesrc -o winesrc.dwarfs -l9
[...]
I 07:55:20.546636 compressed 64.85 GiB to 93.2 MiB (ratio=0.00140344)
I 07:55:20.826699 compression CPU time: 6.726m
I 07:55:20.827338 filesystem created without errors [2.283m]
[...]

real    2m17.100s
user    9m53.633s
sys     2m29.236s"><pre><code>$ time mkdwarfs -i winesrc -o winesrc.dwarfs -l9
[...]
I 07:55:20.546636 compressed 64.85 GiB to 93.2 MiB (ratio=0.00140344)
I 07:55:20.826699 compression CPU time: 6.726m
I 07:55:20.827338 filesystem created without errors [2.283m]
[...]

real    2m17.100s
user    9m53.633s
sys     2m29.236s
</code></pre></div>
<p dir="auto">It uses pretty much the same amount of CPU resources, but finishes more than
8 times faster. The DwarFS output file is more than 6 times smaller.</p>
<p dir="auto">You can actually squeeze a bit more redundancy out of the original data by
tweaking the similarity ordering and switching from lzma to brotli compression,
albeit at a somewhat slower compression speed:</p>
<div data-snippet-clipboard-copy-content="mkdwarfs -i winesrc -o winesrc.dwarfs -l9 -C brotli:quality=11:lgwin=26 --order=nilsimsa:max-cluster-size=200k
[...]
I 08:21:01.138075 compressed 64.85 GiB to 73.52 MiB (ratio=0.00110716)
I 08:21:01.485737 compression CPU time: 36.58m
I 08:21:01.486313 filesystem created without errors [5.501m]
[...]
real    5m30.178s
user    40m59.193s
sys     2m36.234s"><pre><code>mkdwarfs -i winesrc -o winesrc.dwarfs -l9 -C brotli:quality=11:lgwin=26 --order=nilsimsa:max-cluster-size=200k
[...]
I 08:21:01.138075 compressed 64.85 GiB to 73.52 MiB (ratio=0.00110716)
I 08:21:01.485737 compression CPU time: 36.58m
I 08:21:01.486313 filesystem created without errors [5.501m]
[...]
real    5m30.178s
user    40m59.193s
sys     2m36.234s
</code></pre></div>
<p dir="auto">That's almost a 1000x reduction in size.</p>
<p dir="auto">Let's also look at decompression speed:</p>
<div data-snippet-clipboard-copy-content="$ time zpaqfranz x winesrc.zpaq
zpaqfranz v58.8k-JIT-L(2023-08-05)
/home/mhx/winesrc.zpaq:
1 versions, 3.480.317 files, 617.600.892 bytes (588.99 MB)
Extract 69.632.090.852 bytes (64.85 GB) in 3.117.413 files (362.904 folders) / 16 T
        99.18% 00:00:00  (  64.32 GB)=>(  64.85 GB)  548.83 MB/sec

125.636 seconds (000:02:05) (all OK)

real    2m6.968s
user    1m36.177s
sys     1m10.980s"><pre><code>$ time zpaqfranz x winesrc.zpaq
zpaqfranz v58.8k-JIT-L(2023-08-05)
/home/mhx/winesrc.zpaq:
1 versions, 3.480.317 files, 617.600.892 bytes (588.99 MB)
Extract 69.632.090.852 bytes (64.85 GB) in 3.117.413 files (362.904 folders) / 16 T
        99.18% 00:00:00  (  64.32 GB)=&gt;(  64.85 GB)  548.83 MB/sec

125.636 seconds (000:02:05) (all OK)

real    2m6.968s
user    1m36.177s
sys     1m10.980s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time dwarfsextract -i winesrc.dwarfs

real    1m49.182s
user    0m34.667s
sys     1m28.733s"><pre><code>$ time dwarfsextract -i winesrc.dwarfs

real    1m49.182s
user    0m34.667s
sys     1m28.733s
</code></pre></div>
<p dir="auto">Decompression time is pretty much in the same ballpark, with just slightly
shorter times for the DwarFS image.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With wimlib</h3><a id="user-content-with-wimlib" aria-label="Permalink: With wimlib" href="#with-wimlib"></a></p>
<p dir="auto"><a href="https://wimlib.net/" rel="nofollow">wimlib</a> is a really interesting project that is
a lot more mature than DwarFS. While DwarFS at its core has a library
component that could potentially be ported to other operating systems,
wimlib already is available on many platforms. It also seems to have
quite a rich set of features, so it's definitely worth taking a look at.</p>
<p dir="auto">I first tried <code>wimcapture</code> on the perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time wimcapture --unix-data --solid --solid-chunk-size=16M install perl-install.wim
Scanning &quot;install&quot;
47 GiB scanned (1927501 files, 330733 directories)
Using LZMS compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    15m23.310s
user    174m29.274s
sys     0m42.921s"><pre><code>$ time wimcapture --unix-data --solid --solid-chunk-size=16M install perl-install.wim
Scanning "install"
47 GiB scanned (1927501 files, 330733 directories)
Using LZMS compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    15m23.310s
user    174m29.274s
sys     0m42.921s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install.*
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
-rw-r--r-- 1 mhx users 1016981520 Mar  6 21:12 perl-install.wim"><pre><code>$ ll perl-install.*
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
-rw-r--r-- 1 mhx users 1016981520 Mar  6 21:12 perl-install.wim
</code></pre></div>
<p dir="auto">So, wimlib is definitely much better than squashfs, in terms of both
compression ratio and speed. DwarFS is however about 3 times faster to
create the file system and the DwarFS file system less than half the size.
When switching to LZMA compression, the DwarFS file system is more than
3 times smaller (wimlib uses LZMS compression by default).</p>
<p dir="auto">What's a bit surprising is that mounting a <em>wim</em> file takes quite a bit
of time:</p>
<div data-snippet-clipboard-copy-content="$ time wimmount perl-install.wim mnt
[WARNING] Mounting a WIM file containing solid-compressed data; file access may be slow.

real    0m2.038s
user    0m1.764s
sys     0m0.242s"><pre><code>$ time wimmount perl-install.wim mnt
[WARNING] Mounting a WIM file containing solid-compressed data; file access may be slow.

real    0m2.038s
user    0m1.764s
sys     0m0.242s
</code></pre></div>
<p dir="auto">Mounting the DwarFS image takes almost no time in comparison:</p>
<div data-snippet-clipboard-copy-content="$ time git/github/dwarfs/build-clang-11/dwarfs perl-install-default.dwarfs mnt
I 00:23:39.238182 dwarfs (v0.4.0, fuse version 35)

real    0m0.003s
user    0m0.003s
sys     0m0.000s"><pre><code>$ time git/github/dwarfs/build-clang-11/dwarfs perl-install-default.dwarfs mnt
I 00:23:39.238182 dwarfs (v0.4.0, fuse version 35)

real    0m0.003s
user    0m0.003s
sys     0m0.000s
</code></pre></div>
<p dir="auto">That's just because it immediately forks into background by default and
initializes the file system in the background. However, even when
running it in the foreground, initializing the file system takes only
about 60 milliseconds:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs perl-install.dwarfs mnt -f
I 00:25:03.186005 dwarfs (v0.4.0, fuse version 35)
I 00:25:03.248061 file system initialized [60.95ms]"><pre><code>$ dwarfs perl-install.dwarfs mnt -f
I 00:25:03.186005 dwarfs (v0.4.0, fuse version 35)
I 00:25:03.248061 file system initialized [60.95ms]
</code></pre></div>
<p dir="auto">If you actually build the DwarFS file system with uncompressed metadata,
mounting is basically instantaneous:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs perl-install-meta.dwarfs mnt -f
I 00:27:52.667026 dwarfs (v0.4.0, fuse version 35)
I 00:27:52.671066 file system initialized [2.879ms]"><pre><code>$ dwarfs perl-install-meta.dwarfs mnt -f
I 00:27:52.667026 dwarfs (v0.4.0, fuse version 35)
I 00:27:52.671066 file system initialized [2.879ms]
</code></pre></div>
<p dir="auto">I've tried running the benchmark where all 1139 <code>perl</code> executables
print their version with the wimlib image, but after about 10 minutes,
it still hadn't finished the first run (with the DwarFS image, one run
took slightly more than 2 seconds). I then tried the following instead:</p>
<div data-snippet-clipboard-copy-content="$ ls -1 /tmp/perl/install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P1 sh -c 'time $0 -v >/dev/null' 2>&amp;1 | grep ^real
real    0m0.802s
real    0m0.652s
real    0m1.677s
real    0m1.973s
real    0m1.435s
real    0m1.879s
real    0m2.003s
real    0m1.695s
real    0m2.343s
real    0m1.899s
real    0m1.809s
real    0m1.790s
real    0m2.115s"><pre><code>$ ls -1 /tmp/perl/install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P1 sh -c 'time $0 -v &gt;/dev/null' 2&gt;&amp;1 | grep ^real
real    0m0.802s
real    0m0.652s
real    0m1.677s
real    0m1.973s
real    0m1.435s
real    0m1.879s
real    0m2.003s
real    0m1.695s
real    0m2.343s
real    0m1.899s
real    0m1.809s
real    0m1.790s
real    0m2.115s
</code></pre></div>
<p dir="auto">Judging from that, it would have probably taken about half an hour
for a single run, which makes at least the <code>--solid</code> wim image pretty
much unusable for actually working with the file system.</p>
<p dir="auto">The <code>--solid</code> option was suggested to me because it resembles the way
that DwarFS actually organizes data internally. However, judging by the
warning when mounting a solid image, it's probably not ideal when using
the image as a mounted file system. So I tried again without <code>--solid</code>:</p>
<div data-snippet-clipboard-copy-content="$ time wimcapture --unix-data install perl-install-nonsolid.wim
Scanning &quot;install&quot;
47 GiB scanned (1927501 files, 330733 directories)
Using LZX compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    8m39.034s
user    64m58.575s
sys     0m32.003s"><pre><code>$ time wimcapture --unix-data install perl-install-nonsolid.wim
Scanning "install"
47 GiB scanned (1927501 files, 330733 directories)
Using LZX compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    8m39.034s
user    64m58.575s
sys     0m32.003s
</code></pre></div>
<p dir="auto">This is still more than 3 minutes slower than <code>mkdwarfs</code>. However, it
yields an image that's almost 10 times the size of the DwarFS image
and comparable in size to the SquashFS image:</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install-nonsolid.wim -h
-rw-r--r-- 1 mhx users 4.6G Mar  6 23:24 perl-install-nonsolid.wim"><pre><code>$ ll perl-install-nonsolid.wim -h
-rw-r--r-- 1 mhx users 4.6G Mar  6 23:24 perl-install-nonsolid.wim
</code></pre></div>
<p dir="auto">This <em>still</em> takes surprisingly long to mount:</p>
<div data-snippet-clipboard-copy-content="$ time wimmount perl-install-nonsolid.wim mnt

real    0m1.603s
user    0m1.327s
sys     0m0.275s"><pre><code>$ time wimmount perl-install-nonsolid.wim mnt

real    0m1.603s
user    0m1.327s
sys     0m0.275s
</code></pre></div>
<p dir="auto">However, it's really usable as a file system, even though it's about
4-5 times slower than the DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c 'umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot; -p 'umount mnt; wimmount perl-install-nonsolid.wim mnt; sleep 1' -n wimlib &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: dwarfs
  Time (mean ± σ):      1.149 s ±  0.019 s    [User: 2.147 s, System: 0.739 s]
  Range (min … max):    1.122 s …  1.187 s    10 runs

Benchmark #2: wimlib
  Time (mean ± σ):      7.542 s ±  0.069 s    [User: 2.787 s, System: 0.694 s]
  Range (min … max):    7.490 s …  7.732 s    10 runs

Summary
  'dwarfs' ran
    6.56 ± 0.12 times faster than 'wimlib'"><pre><code>$ hyperfine -c 'umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'" -p 'umount mnt; wimmount perl-install-nonsolid.wim mnt; sleep 1' -n wimlib "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: dwarfs
  Time (mean ± σ):      1.149 s ±  0.019 s    [User: 2.147 s, System: 0.739 s]
  Range (min … max):    1.122 s …  1.187 s    10 runs

Benchmark #2: wimlib
  Time (mean ± σ):      7.542 s ±  0.069 s    [User: 2.787 s, System: 0.694 s]
  Range (min … max):    7.490 s …  7.732 s    10 runs

Summary
  'dwarfs' ran
    6.56 ± 0.12 times faster than 'wimlib'
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">With Cromfs</h3><a id="user-content-with-cromfs" aria-label="Permalink: With Cromfs" href="#with-cromfs"></a></p>
<p dir="auto">I used <a href="https://bisqwit.iki.fi/source/cromfs.html" rel="nofollow">Cromfs</a> in the past
for compressed file systems and remember that it did a pretty good job
in terms of compression ratio. But it was never fast. However, I didn't
quite remember just <em>how</em> slow it was until I tried to set up a test.</p>
<p dir="auto">Here's a run on the Perl dataset, with the block size set to 16 MiB to
match the default of DwarFS, and with additional options suggested to
speed up compression:</p>
<div data-snippet-clipboard-copy-content="$ time mkcromfs -f 16777216 -qq -e -r100000 install perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --24bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f3a18259000..0x7f3a1bfffb9c
Root inode spans 0x7f3a205d2948..0x7f3a205d294c
Beginning task for Files and directories: Finding identical blocks
2163608 reuse opportunities found. 561362 unique blocks. Block table will be 79.4% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Blockifying:  0.04% (140017/2724970) idx(siz=80423,del=0) rawin(20.97 MB)rawout(20.97 MB)diff(1956 bytes)
Termination signalled, cleaning up temporaries

real    29m9.634s
user    201m37.816s
sys     2m15.005s"><pre><code>$ time mkcromfs -f 16777216 -qq -e -r100000 install perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --24bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f3a18259000..0x7f3a1bfffb9c
Root inode spans 0x7f3a205d2948..0x7f3a205d294c
Beginning task for Files and directories: Finding identical blocks
2163608 reuse opportunities found. 561362 unique blocks. Block table will be 79.4% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Blockifying:  0.04% (140017/2724970) idx(siz=80423,del=0) rawin(20.97 MB)rawout(20.97 MB)diff(1956 bytes)
Termination signalled, cleaning up temporaries

real    29m9.634s
user    201m37.816s
sys     2m15.005s
</code></pre></div>
<p dir="auto">So, it processed 21 MiB out of 48 GiB in half an hour, using almost
twice as much CPU resources as DwarFS for the <em>whole</em> file system.
At this point I decided it's likely not worth waiting (presumably)
another month (!) for <code>mkcromfs</code> to finish. I double checked that
I didn't accidentally build a debugging version, <code>mkcromfs</code> was
definitely built with <code>-O3</code>.</p>
<p dir="auto">I then tried once more with a smaller version of the Perl dataset.
This only has 20 versions (instead of 1139) of Perl, and obviously
a lot less redundancy:</p>
<div data-snippet-clipboard-copy-content="$ time mkcromfs -f 16777216 -qq -e -r100000 install-small perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --16bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f00e0774000..0x7f00e08410a8
Root inode spans 0x7f00b40048f8..0x7f00b40048fc
Beginning task for Files and directories: Finding identical blocks
25362 reuse opportunities found. 9815 unique blocks. Block table will be 72.1% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Compressing raw rootdir inode (28 bytes)z=982370,del=2) rawin(641.56 MB)rawout(252.72 MB)diff(388.84 MB)
 compressed into 35 bytes
INOTAB pseudo file is 839.85 kB
Inotab inode spans 0x7f00bc036ed8..0x7f00bc036ef4
Beginning task for INOTAB: Finding identical blocks
0 reuse opportunities found. 13 unique blocks. Block table will be 0.0% smaller than without the index search.
Beginning task for INOTAB: Blockifying
mkcromfs: Automatically enabling --packedblocks because it is possible for this filesystem.
Compressing raw inotab inode (52 bytes)
 compressed into 58 bytes
Compressing 9828 block records (4 bytes each, total 39312 bytes)
 compressed into 15890 bytes
Compressing and writing 16 fblocks...

16 fblocks were written: 35.31 MB = 13.90 % of 254.01 MB
Filesystem size: 35.33 MB = 5.50 % of original 642.22 MB
End

real    27m38.833s
user    277m36.208s
sys     11m36.945s"><pre><code>$ time mkcromfs -f 16777216 -qq -e -r100000 install-small perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --16bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f00e0774000..0x7f00e08410a8
Root inode spans 0x7f00b40048f8..0x7f00b40048fc
Beginning task for Files and directories: Finding identical blocks
25362 reuse opportunities found. 9815 unique blocks. Block table will be 72.1% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Compressing raw rootdir inode (28 bytes)z=982370,del=2) rawin(641.56 MB)rawout(252.72 MB)diff(388.84 MB)
 compressed into 35 bytes
INOTAB pseudo file is 839.85 kB
Inotab inode spans 0x7f00bc036ed8..0x7f00bc036ef4
Beginning task for INOTAB: Finding identical blocks
0 reuse opportunities found. 13 unique blocks. Block table will be 0.0% smaller than without the index search.
Beginning task for INOTAB: Blockifying
mkcromfs: Automatically enabling --packedblocks because it is possible for this filesystem.
Compressing raw inotab inode (52 bytes)
 compressed into 58 bytes
Compressing 9828 block records (4 bytes each, total 39312 bytes)
 compressed into 15890 bytes
Compressing and writing 16 fblocks...

16 fblocks were written: 35.31 MB = 13.90 % of 254.01 MB
Filesystem size: 35.33 MB = 5.50 % of original 642.22 MB
End

real    27m38.833s
user    277m36.208s
sys     11m36.945s
</code></pre></div>
<p dir="auto">And repeating the same task with <code>mkdwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small.dwarfs
21:13:38.131724 scanning install-small
21:13:38.320139 waiting for background scanners...
21:13:38.727024 assigning directory and link inodes...
21:13:38.731807 finding duplicate files...
21:13:38.832524 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:13:38.832598 waiting for inode scanners...
21:13:39.619963 assigning device inodes...
21:13:39.620855 assigning pipe/socket inodes...
21:13:39.621356 building metadata...
21:13:39.621453 building blocks...
21:13:39.621472 saving names and links...
21:13:39.621655 ordering 3559 inodes using nilsimsa similarity...
21:13:39.622031 nilsimsa: depth=20000, limit=255
21:13:39.629206 updating name and link indices...
21:13:39.630142 pre-sorted index (3360 name, 2127 path lookups) [8.014ms]
21:13:39.752051 3559 inodes ordered [130.3ms]
21:13:39.752101 waiting for segmenting/blockifying to finish...
21:13:53.250951 saving chunks...
21:13:53.251581 saving directories...
21:13:53.303862 waiting for compression to finish...
21:14:11.073273 compressed 611.8 MiB to 24.01 MiB (ratio=0.0392411)
21:14:11.091099 filesystem created without errors [32.96s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 121.5 MiB
filesystem: 222.5 MiB in 14 blocks (7177 chunks, 3559/3559 inodes)
compressed filesystem: 14 blocks/24.01 MiB written
██████████████████████████████████████████████████████████████████████▏100% \

real    0m33.007s
user    3m43.324s
sys     0m4.015s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small.dwarfs
21:13:38.131724 scanning install-small
21:13:38.320139 waiting for background scanners...
21:13:38.727024 assigning directory and link inodes...
21:13:38.731807 finding duplicate files...
21:13:38.832524 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:13:38.832598 waiting for inode scanners...
21:13:39.619963 assigning device inodes...
21:13:39.620855 assigning pipe/socket inodes...
21:13:39.621356 building metadata...
21:13:39.621453 building blocks...
21:13:39.621472 saving names and links...
21:13:39.621655 ordering 3559 inodes using nilsimsa similarity...
21:13:39.622031 nilsimsa: depth=20000, limit=255
21:13:39.629206 updating name and link indices...
21:13:39.630142 pre-sorted index (3360 name, 2127 path lookups) [8.014ms]
21:13:39.752051 3559 inodes ordered [130.3ms]
21:13:39.752101 waiting for segmenting/blockifying to finish...
21:13:53.250951 saving chunks...
21:13:53.251581 saving directories...
21:13:53.303862 waiting for compression to finish...
21:14:11.073273 compressed 611.8 MiB to 24.01 MiB (ratio=0.0392411)
21:14:11.091099 filesystem created without errors [32.96s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 121.5 MiB
filesystem: 222.5 MiB in 14 blocks (7177 chunks, 3559/3559 inodes)
compressed filesystem: 14 blocks/24.01 MiB written
██████████████████████████████████████████████████████████████████████▏100% \

real    0m33.007s
user    3m43.324s
sys     0m4.015s
</code></pre></div>
<p dir="auto">So, <code>mkdwarfs</code> is about 50 times faster than <code>mkcromfs</code> and uses 75 times
less CPU resources. At the same time, the DwarFS file system is 30% smaller:</p>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs"><pre><code>$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs
</code></pre></div>
<p dir="auto">I noticed that the <code>blockifying</code> step that took ages for the full dataset
with <code>mkcromfs</code> ran substantially faster (in terms of MiB/second) on the
smaller dataset, which makes me wonder if there's some quadratic complexity
behaviour that's slowing down <code>mkcromfs</code>.</p>
<p dir="auto">In order to be completely fair, I also ran <code>mkdwarfs</code> with <code>-l 9</code> to enable
LZMA compression (which is what <code>mkcromfs</code> uses by default):</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small-l9.dwarfs -l 9
21:16:21.874975 scanning install-small
21:16:22.092201 waiting for background scanners...
21:16:22.489470 assigning directory and link inodes...
21:16:22.495216 finding duplicate files...
21:16:22.611221 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:16:22.611314 waiting for inode scanners...
21:16:23.394332 assigning device inodes...
21:16:23.395184 assigning pipe/socket inodes...
21:16:23.395616 building metadata...
21:16:23.395676 building blocks...
21:16:23.395685 saving names and links...
21:16:23.395830 ordering 3559 inodes using nilsimsa similarity...
21:16:23.396097 nilsimsa: depth=50000, limit=255
21:16:23.401042 updating name and link indices...
21:16:23.403127 pre-sorted index (3360 name, 2127 path lookups) [6.936ms]
21:16:23.524914 3559 inodes ordered [129ms]
21:16:23.525006 waiting for segmenting/blockifying to finish...
21:16:33.865023 saving chunks...
21:16:33.865883 saving directories...
21:16:33.900140 waiting for compression to finish...
21:17:10.505779 compressed 611.8 MiB to 17.44 MiB (ratio=0.0284969)
21:17:10.526171 filesystem created without errors [48.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 122.2 MiB
filesystem: 221.8 MiB in 4 blocks (7304 chunks, 3559/3559 inodes)
compressed filesystem: 4 blocks/17.44 MiB written
██████████████████████████████████████████████████████████████████████▏100% /

real    0m48.683s
user    2m24.905s
sys     0m3.292s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small-l9.dwarfs -l 9
21:16:21.874975 scanning install-small
21:16:22.092201 waiting for background scanners...
21:16:22.489470 assigning directory and link inodes...
21:16:22.495216 finding duplicate files...
21:16:22.611221 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:16:22.611314 waiting for inode scanners...
21:16:23.394332 assigning device inodes...
21:16:23.395184 assigning pipe/socket inodes...
21:16:23.395616 building metadata...
21:16:23.395676 building blocks...
21:16:23.395685 saving names and links...
21:16:23.395830 ordering 3559 inodes using nilsimsa similarity...
21:16:23.396097 nilsimsa: depth=50000, limit=255
21:16:23.401042 updating name and link indices...
21:16:23.403127 pre-sorted index (3360 name, 2127 path lookups) [6.936ms]
21:16:23.524914 3559 inodes ordered [129ms]
21:16:23.525006 waiting for segmenting/blockifying to finish...
21:16:33.865023 saving chunks...
21:16:33.865883 saving directories...
21:16:33.900140 waiting for compression to finish...
21:17:10.505779 compressed 611.8 MiB to 17.44 MiB (ratio=0.0284969)
21:17:10.526171 filesystem created without errors [48.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 122.2 MiB
filesystem: 221.8 MiB in 4 blocks (7304 chunks, 3559/3559 inodes)
compressed filesystem: 4 blocks/17.44 MiB written
██████████████████████████████████████████████████████████████████████▏100% /

real    0m48.683s
user    2m24.905s
sys     0m3.292s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small*.*fs
-rw-r--r-- 1 mhx users 18282075 Dec 10 21:17 perl-install-small-l9.dwarfs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs"><pre><code>$ ls -l perl-install-small*.*fs
-rw-r--r-- 1 mhx users 18282075 Dec 10 21:17 perl-install-small-l9.dwarfs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs
</code></pre></div>
<p dir="auto">It takes about 15 seconds longer to build the DwarFS file system with LZMA
compression (this is still 35 times faster than Cromfs), but reduces the
size even further to make it almost half the size of the Cromfs file system.</p>
<p dir="auto">I would have added some benchmarks with the Cromfs FUSE driver, but sadly
it crashed right upon trying to list the directory after mounting.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With EROFS</h3><a id="user-content-with-erofs" aria-label="Permalink: With EROFS" href="#with-erofs"></a></p>
<p dir="auto"><a href="https://github.com/hsiangkao/erofs-utils">EROFS</a> is a new read-only
compressed file system that has recently been added to the Linux kernel.
Its goals are quite different from those of DwarFS, though. It is
designed to be lightweight (which DwarFS is definitely not) and to run
on constrained hardware like embedded devices or smartphones. It only
supports LZ4 compression.</p>
<p dir="auto">I was feeling lucky and decided to run it on the full Perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time mkfs.erofs perl-install.erofs install -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]
^C

real    912m42.601s
user    903m2.777s
sys     1m52.812s"><pre><code>$ time mkfs.erofs perl-install.erofs install -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]
^C

real    912m42.601s
user    903m2.777s
sys     1m52.812s
</code></pre></div>
<p dir="auto">As you can tell, after more than 15 hours I just gave up. In those
15 hours, <code>mkfs.erofs</code> had produced a 13 GiB output file:</p>
<div data-snippet-clipboard-copy-content="$ ll -h perl-install.erofs
-rw-r--r-- 1 mhx users 13G Dec  9 14:42 perl-install.erofs"><pre><code>$ ll -h perl-install.erofs
-rw-r--r-- 1 mhx users 13G Dec  9 14:42 perl-install.erofs
</code></pre></div>
<p dir="auto">I don't think this would have been very useful to compare with DwarFS.</p>
<p dir="auto">Just as for Cromfs, I re-ran with the smaller Perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time mkfs.erofs perl-install-small.erofs install-small -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]

real    0m27.844s
user    0m20.570s
sys     0m1.848s"><pre><code>$ time mkfs.erofs perl-install-small.erofs install-small -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]

real    0m27.844s
user    0m20.570s
sys     0m1.848s
</code></pre></div>
<p dir="auto">That was surprisingly quick, which makes me think that, again, there
might be some accidentally quadratic complexity hiding in <code>mkfs.erofs</code>.
The output file it produced is an order of magnitude larger than the
DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users  26928161 Dec  8 15:05 perl-install-small.dwarfs
-rw-r--r-- 1 mhx users 296488960 Dec  9 14:45 perl-install-small.erofs"><pre><code>$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users  26928161 Dec  8 15:05 perl-install-small.dwarfs
-rw-r--r-- 1 mhx users 296488960 Dec  9 14:45 perl-install-small.erofs
</code></pre></div>
<p dir="auto">Admittedly, this isn't a fair comparison. EROFS has a fixed block size
of 4 KiB, and it uses LZ4 compression. If we tweak DwarFS to the same
parameters, we get:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small-lz4.dwarfs -C lz4hc:level=9 -S 12
21:21:18.136796 scanning install-small
21:21:18.376998 waiting for background scanners...
21:21:18.770703 assigning directory and link inodes...
21:21:18.776422 finding duplicate files...
21:21:18.903505 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:21:18.903621 waiting for inode scanners...
21:21:19.676420 assigning device inodes...
21:21:19.677400 assigning pipe/socket inodes...
21:21:19.678014 building metadata...
21:21:19.678101 building blocks...
21:21:19.678116 saving names and links...
21:21:19.678306 ordering 3559 inodes using nilsimsa similarity...
21:21:19.678566 nilsimsa: depth=20000, limit=255
21:21:19.684227 pre-sorted index (3360 name, 2127 path lookups) [5.592ms]
21:21:19.685550 updating name and link indices...
21:21:19.810401 3559 inodes ordered [132ms]
21:21:19.810519 waiting for segmenting/blockifying to finish...
21:21:26.773913 saving chunks...
21:21:26.776832 saving directories...
21:21:26.821085 waiting for compression to finish...
21:21:27.020929 compressed 611.8 MiB to 140.7 MiB (ratio=0.230025)
21:21:27.036202 filesystem created without errors [8.899s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 0 B
filesystem: 344 MiB in 88073 blocks (91628 chunks, 3559/3559 inodes)
compressed filesystem: 88073 blocks/140.7 MiB written
████████████████████████████████████████████████████████████████▏100% |

real    0m9.075s
user    0m37.718s
sys     0m2.427s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small-lz4.dwarfs -C lz4hc:level=9 -S 12
21:21:18.136796 scanning install-small
21:21:18.376998 waiting for background scanners...
21:21:18.770703 assigning directory and link inodes...
21:21:18.776422 finding duplicate files...
21:21:18.903505 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:21:18.903621 waiting for inode scanners...
21:21:19.676420 assigning device inodes...
21:21:19.677400 assigning pipe/socket inodes...
21:21:19.678014 building metadata...
21:21:19.678101 building blocks...
21:21:19.678116 saving names and links...
21:21:19.678306 ordering 3559 inodes using nilsimsa similarity...
21:21:19.678566 nilsimsa: depth=20000, limit=255
21:21:19.684227 pre-sorted index (3360 name, 2127 path lookups) [5.592ms]
21:21:19.685550 updating name and link indices...
21:21:19.810401 3559 inodes ordered [132ms]
21:21:19.810519 waiting for segmenting/blockifying to finish...
21:21:26.773913 saving chunks...
21:21:26.776832 saving directories...
21:21:26.821085 waiting for compression to finish...
21:21:27.020929 compressed 611.8 MiB to 140.7 MiB (ratio=0.230025)
21:21:27.036202 filesystem created without errors [8.899s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 0 B
filesystem: 344 MiB in 88073 blocks (91628 chunks, 3559/3559 inodes)
compressed filesystem: 88073 blocks/140.7 MiB written
████████████████████████████████████████████████████████████████▏100% |

real    0m9.075s
user    0m37.718s
sys     0m2.427s
</code></pre></div>
<p dir="auto">It finishes in less than half the time and produces an output image
that's half the size of the EROFS image.</p>
<p dir="auto">I'm going to stop the comparison here, as it's pretty obvious that the
domains in which EROFS and DwarFS are being used have extremely little
overlap. DwarFS will likely never be able to run on embedded devices
and EROFS will likely never be able to achieve the compression ratios
of DwarFS.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With fuse-archive</h3><a id="user-content-with-fuse-archive" aria-label="Permalink: With fuse-archive" href="#with-fuse-archive"></a></p>
<p dir="auto">I came across <a href="https://github.com/google/fuse-archive">fuse-archive</a>
while looking for FUSE drivers to mount archives and it seems to be
the most versatile of the alternatives (and the one that actually
compiles out of the box).</p>
<p dir="auto">An interesting test case straight from fuse-archive's README is in
the <a href="https://github.com/google/fuse-archive#performance">Performance</a>
section: an archive with a single huge file full of zeroes. Let's
make the example a bit more extreme and use a 1 GiB file instead of
just 256 MiB:</p>
<div data-snippet-clipboard-copy-content="$ mkdir zerotest
$ truncate --size=1G zerotest/zeroes"><pre><code>$ mkdir zerotest
$ truncate --size=1G zerotest/zeroes
</code></pre></div>
<p dir="auto">Now, we build several different archives and a DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i zerotest -o zerotest.dwarfs -W16 --log-level=warn --progress=none

real    0m7.604s
user    0m7.521s
sys     0m0.083s

$ time zip -9 zerotest.zip zerotest/zeroes
  adding: zerotest/zeroes (deflated 100%)

real    0m4.923s
user    0m4.840s
sys     0m0.080s

$ time 7z a -bb0 -bd zerotest.7z zerotest/zeroes

7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs Intel(R) Xeon(R) E-2286M  CPU @ 2.40GHz (906ED),ASM,AES-NI)

Scanning the drive:
1 file, 1073741824 bytes (1024 MiB)

Creating archive: zerotest.7z

Items to compress: 1

Files read from disk: 1
Archive size: 157819 bytes (155 KiB)
Everything is Ok

real    0m5.535s
user    0m48.281s
sys     0m1.116s

$ time tar --zstd -cf zerotest.tar.zstd zerotest/zeroes

real    0m0.449s
user    0m0.510s
sys     0m0.610s"><pre><code>$ time mkdwarfs -i zerotest -o zerotest.dwarfs -W16 --log-level=warn --progress=none

real    0m7.604s
user    0m7.521s
sys     0m0.083s

$ time zip -9 zerotest.zip zerotest/zeroes
  adding: zerotest/zeroes (deflated 100%)

real    0m4.923s
user    0m4.840s
sys     0m0.080s

$ time 7z a -bb0 -bd zerotest.7z zerotest/zeroes

7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs Intel(R) Xeon(R) E-2286M  CPU @ 2.40GHz (906ED),ASM,AES-NI)

Scanning the drive:
1 file, 1073741824 bytes (1024 MiB)

Creating archive: zerotest.7z

Items to compress: 1

Files read from disk: 1
Archive size: 157819 bytes (155 KiB)
Everything is Ok

real    0m5.535s
user    0m48.281s
sys     0m1.116s

$ time tar --zstd -cf zerotest.tar.zstd zerotest/zeroes

real    0m0.449s
user    0m0.510s
sys     0m0.610s
</code></pre></div>
<p dir="auto">Turns out that <code>tar --zstd</code> is easily winning the compression speed
test. Looking at the file sizes did actually blow my mind just a bit:</p>
<div data-snippet-clipboard-copy-content="$ ll zerotest.* --sort=size
-rw-r--r-- 1 mhx users 1042231 Jul  1 15:24 zerotest.zip
-rw-r--r-- 1 mhx users  157819 Jul  1 15:26 zerotest.7z
-rw-r--r-- 1 mhx users   33762 Jul  1 15:28 zerotest.tar.zstd
-rw-r--r-- 1 mhx users     848 Jul  1 15:23 zerotest.dwarfs"><pre><code>$ ll zerotest.* --sort=size
-rw-r--r-- 1 mhx users 1042231 Jul  1 15:24 zerotest.zip
-rw-r--r-- 1 mhx users  157819 Jul  1 15:26 zerotest.7z
-rw-r--r-- 1 mhx users   33762 Jul  1 15:28 zerotest.tar.zstd
-rw-r--r-- 1 mhx users     848 Jul  1 15:23 zerotest.dwarfs
</code></pre></div>
<p dir="auto">I definitely didn't expect the DwarFS image to be <em>that</em> small.
Dropping the section index would actually save another 100 bytes.
So, if you want to archive lots of zeroes, DwarFS is your friend.</p>
<p dir="auto">Anyway, let's look at how fast and efficiently the zeroes can
be read from the different archives. First, the <code>zip</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
1020117504 bytes (1.0 GB, 973 MiB) copied, 2 s, 510 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.10309 s, 511 MB/s

real    0m2.104s
user    0m0.264s
sys     0m0.486s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
1020117504 bytes (1.0 GB, 973 MiB) copied, 2 s, 510 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.10309 s, 511 MB/s

real    0m2.104s
user    0m0.264s
sys     0m0.486s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 1.8 seconds and mount time
was in the milliseconds.</p>
<p dir="auto">Now, the <code>7z</code> archive:</p>
<div data-snippet-clipboard-copy-content=" $ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
594759168 bytes (595 MB, 567 MiB) copied, 1 s, 595 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.76904 s, 607 MB/s

real    0m1.772s
user    0m0.229s
sys     0m0.572s"><pre><code> $ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
594759168 bytes (595 MB, 567 MiB) copied, 1 s, 595 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.76904 s, 607 MB/s

real    0m1.772s
user    0m0.229s
sys     0m0.572s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 2.9 seconds and mount time
was just over 1.0 seconds.</p>
<p dir="auto">Now, the <code>.tar.zstd</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.799409 s, 1.3 GB/s

real    0m0.801s
user    0m0.262s
sys     0m0.537s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.799409 s, 1.3 GB/s

real    0m0.801s
user    0m0.262s
sys     0m0.537s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 0.53 seconds and mount time
was 0.13 seconds.</p>
<p dir="auto">Last but not least, let's look at DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.753 s, 1.4 GB/s

real    0m0.757s
user    0m0.220s
sys     0m0.534s"><pre><code>$ time dd if=mnt/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.753 s, 1.4 GB/s

real    0m0.757s
user    0m0.220s
sys     0m0.534s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 0.17 seconds and mount time
was less than a millisecond.</p>
<p dir="auto">If we increase the block size for the <code>dd</code> command, we can get
even higher throughput. For fuse-archive with the <code>.tar.zstd</code>:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.318682 s, 3.4 GB/s

real    0m0.323s
user    0m0.005s
sys     0m0.154s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.318682 s, 3.4 GB/s

real    0m0.323s
user    0m0.005s
sys     0m0.154s
</code></pre></div>
<p dir="auto">And for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.172226 s, 6.2 GB/s

real    0m0.176s
user    0m0.020s
sys     0m0.141s"><pre><code>$ time dd if=mnt/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.172226 s, 6.2 GB/s

real    0m0.176s
user    0m0.020s
sys     0m0.141s
</code></pre></div>
<p dir="auto">This is all nice, but what about a more real-life use case?
Let's take the 1.82.0 boost release archives:</p>
<div data-snippet-clipboard-copy-content="$ ll --sort=size boost_1_82_0.*
-rw-r--r-- 1 mhx users 208188085 Apr 10 14:25 boost_1_82_0.zip
-rw-r--r-- 1 mhx users 142580547 Apr 10 14:23 boost_1_82_0.tar.gz
-rw-r--r-- 1 mhx users 121325129 Apr 10 14:23 boost_1_82_0.tar.bz2
-rw-r--r-- 1 mhx users 105901369 Jun 28 12:47 boost_1_82_0.dwarfs
-rw-r--r-- 1 mhx users 103710551 Apr 10 14:25 boost_1_82_0.7z"><pre><code>$ ll --sort=size boost_1_82_0.*
-rw-r--r-- 1 mhx users 208188085 Apr 10 14:25 boost_1_82_0.zip
-rw-r--r-- 1 mhx users 142580547 Apr 10 14:23 boost_1_82_0.tar.gz
-rw-r--r-- 1 mhx users 121325129 Apr 10 14:23 boost_1_82_0.tar.bz2
-rw-r--r-- 1 mhx users 105901369 Jun 28 12:47 boost_1_82_0.dwarfs
-rw-r--r-- 1 mhx users 103710551 Apr 10 14:25 boost_1_82_0.7z
</code></pre></div>
<p dir="auto">Here are the timings for mounting each archive and then using
<code>tar</code> to build another archive from the mountpoint and just counting
the number of bytes in that archive, e.g.:</p>
<div data-snippet-clipboard-copy-content="$ time tar cf - mnt | wc -c
803614720

real    0m4.602s
user    0m0.156s
sys     0m1.123s"><pre><code>$ time tar cf - mnt | wc -c
803614720

real    0m4.602s
user    0m0.156s
sys     0m1.123s
</code></pre></div>
<p dir="auto">Here are the results in terms of wallclock time and FUSE driver
CPU time:</p>
<table>
<thead>
<tr>
<th>Archive</th>
<th>Mount Time</th>
<th><code>tar</code> Wallclock Time</th>
<th>FUSE Driver CPU Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.zip</code></td>
<td>0.458s</td>
<td>5.073s</td>
<td>4.418s</td>
</tr>
<tr>
<td><code>.tar.gz</code></td>
<td>1.391s</td>
<td>3.483s</td>
<td>3.943s</td>
</tr>
<tr>
<td><code>.tar.bz2</code></td>
<td>15.663s</td>
<td>17.942s</td>
<td>32.040s</td>
</tr>
<tr>
<td><code>.7z</code></td>
<td>0.321s</td>
<td>32.554s</td>
<td>31.625s</td>
</tr>
<tr>
<td><code>.dwarfs</code></td>
<td>0.013s</td>
<td>2.974s</td>
<td>1.984s</td>
</tr>
</tbody>
</table>
<p dir="auto">DwarFS easily wins all categories while still compressing the data
almost as well as <code>7z</code>.</p>
<p dir="auto">What about accessing files more randomly?</p>
<div data-snippet-clipboard-copy-content="$ find mnt -type f -print0 | xargs -0 -P32 -n32 cat | dd of=/dev/null status=progress"><pre><code>$ find mnt -type f -print0 | xargs -0 -P32 -n32 cat | dd of=/dev/null status=progress
</code></pre></div>
<p dir="auto">It turns out that fuse-archive grinds to a halt in this case, so I had
to run the test on a subset (the <code>boost</code> subdirectory) of the data.
The <code>.tar.bz2</code> and <code>.7z</code> archives were so slow to read that I stopped
them after a few minutes.</p>
<table>
<thead>
<tr>
<th>Archive</th>
<th>Throughput</th>
<th>Wallclock Time</th>
<th>FUSE Driver CPU Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.zip</code></td>
<td>1.8 MB/s</td>
<td>83.245s</td>
<td>83.669s</td>
</tr>
<tr>
<td><code>.tar.gz</code></td>
<td>1.2 MB/s</td>
<td>121.377s</td>
<td>122.711s</td>
</tr>
<tr>
<td><code>.tar.bz2</code></td>
<td>0.2 MB/s</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>.7z</code></td>
<td>0.3 MB/s</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>.dwarfs</code></td>
<td>598.0 MB/s</td>
<td>0.249s</td>
<td>1.099s</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance Monitoring</h2><a id="user-content-performance-monitoring" aria-label="Permalink: Performance Monitoring" href="#performance-monitoring"></a></p>
<p dir="auto">Both the FUSE driver and <code>dwarfsextract</code> by default have support for
simple performance monitoring. You can build binaries without this
feature (<code>-DENABLE_PERFMON=OFF</code>), but impact should be negligible even
if performance monitoring is enabled at run-time.</p>
<p dir="auto">To enable the performance monitor, you pass a list of components for which
you want to collect latency metrics, e.g.:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs test.dwarfs mnt -f -operfmon=fuse"><pre><code>$ dwarfs test.dwarfs mnt -f -operfmon=fuse
</code></pre></div>
<p dir="auto">When the driver exits, you will see output like this:</p>
<div data-snippet-clipboard-copy-content="[fuse.op_read]
      samples: 45145
      overall: 3.214s
  avg latency: 71.2us
  p50 latency: 131.1us
  p90 latency: 131.1us
  p99 latency: 262.1us

[fuse.op_readdir]
      samples: 2
      overall: 51.31ms
  avg latency: 25.65ms
  p50 latency: 32.77us
  p90 latency: 67.11ms
  p99 latency: 67.11ms

[fuse.op_lookup]
      samples: 16
      overall: 19.98ms
  avg latency: 1.249ms
  p50 latency: 2.097ms
  p90 latency: 4.194ms
  p99 latency: 4.194ms

[fuse.op_init]
      samples: 1
      overall: 199.4us
  avg latency: 199.4us
  p50 latency: 262.1us
  p90 latency: 262.1us
  p99 latency: 262.1us

[fuse.op_open]
      samples: 16
      overall: 122.2us
  avg latency: 7.641us
  p50 latency: 4.096us
  p90 latency: 32.77us
  p99 latency: 32.77us

[fuse.op_getattr]
      samples: 1
      overall: 5.786us
  avg latency: 5.786us
  p50 latency: 8.192us
  p90 latency: 8.192us
  p99 latency: 8.192us"><pre><code>[fuse.op_read]
      samples: 45145
      overall: 3.214s
  avg latency: 71.2us
  p50 latency: 131.1us
  p90 latency: 131.1us
  p99 latency: 262.1us

[fuse.op_readdir]
      samples: 2
      overall: 51.31ms
  avg latency: 25.65ms
  p50 latency: 32.77us
  p90 latency: 67.11ms
  p99 latency: 67.11ms

[fuse.op_lookup]
      samples: 16
      overall: 19.98ms
  avg latency: 1.249ms
  p50 latency: 2.097ms
  p90 latency: 4.194ms
  p99 latency: 4.194ms

[fuse.op_init]
      samples: 1
      overall: 199.4us
  avg latency: 199.4us
  p50 latency: 262.1us
  p90 latency: 262.1us
  p99 latency: 262.1us

[fuse.op_open]
      samples: 16
      overall: 122.2us
  avg latency: 7.641us
  p50 latency: 4.096us
  p90 latency: 32.77us
  p99 latency: 32.77us

[fuse.op_getattr]
      samples: 1
      overall: 5.786us
  avg latency: 5.786us
  p50 latency: 8.192us
  p90 latency: 8.192us
  p99 latency: 8.192us
</code></pre></div>
<p dir="auto">The metrics should be self-explanatory. However, note that the
percentile metrics are logarithmically quantized in order to use
as little resources as possible. As a result, you will only see
values that look an awful lot like powers of two.</p>
<p dir="auto">Currently, the supported components are <code>fuse</code> for the FUSE
operations, <code>filesystem_v2</code> for the DwarFS file system component
and <code>inode_reader_v2</code> for the component that handles all <code>read()</code>
system calls.</p>
<p dir="auto">The FUSE driver also exposes the performance monitor metrics via
an <a href="#extended-attributes">extended attribute</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other Obscure Features</h2><a id="user-content-other-obscure-features" aria-label="Permalink: Other Obscure Features" href="#other-obscure-features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setting Worker Thread CPU Affinity</h3><a id="user-content-setting-worker-thread-cpu-affinity" aria-label="Permalink: Setting Worker Thread CPU Affinity" href="#setting-worker-thread-cpu-affinity"></a></p>
<p dir="auto">This only works on Linux and usually only makes sense if you have CPUs
with different types of cores (e.g. "performance" vs "efficiency" cores)
and are <em>really</em> trying to squeeze the last ounce of speed out of DwarFS.</p>
<p dir="auto">By setting the environment variable <code>DWARFS_WORKER_GROUP_AFFINITY</code>, you
can set the CPU affinity of different worker thread groups, e.g.:</p>
<div data-snippet-clipboard-copy-content="export DWARFS_WORKER_GROUP_AFFINITY=blockify=3:compress=6,7"><pre><code>export DWARFS_WORKER_GROUP_AFFINITY=blockify=3:compress=6,7
</code></pre></div>
<p dir="auto">This will set the affinity of the <code>blockify</code> worker group to CPU 3 and
the affinity of the <code>compress</code> worker group to CPUs 6 and 7.</p>
<p dir="auto">You can use this feature for all tools that use one or more worker thread
groups. For example, the FUSE driver <code>dwarfs</code> and <code>dwarfsextract</code> use a
worker group <code>blkcache</code> that the block cache (i.e. block decompression and
lookup) runs on. <code>mkdwarfs</code> uses a whole array of different worker groups,
namely <code>compress</code> for compression, <code>scanner</code> for scanning, <code>ordering</code> for
input ordering, and <code>blockify</code> for segmenting. <code>blockify</code> is what you would
typically want to run on your "performance" cores.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNS over Wikipedia (182 pts)]]></title>
            <link>https://github.com/aaronjanse/dns-over-wikipedia</link>
            <guid>40008383</guid>
            <pubDate>Fri, 12 Apr 2024 00:52:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/aaronjanse/dns-over-wikipedia">https://github.com/aaronjanse/dns-over-wikipedia</a>, See on <a href="https://news.ycombinator.com/item?id=40008383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DNS over Wikipedia</h2><a id="user-content-dns-over-wikipedia" aria-label="Permalink: DNS over Wikipedia" href="#dns-over-wikipedia"></a></p>
<p dir="auto">Wikipedia keeps track of official URLs for popular websites. With DNS over Wikipedia installed, domains ending with <code>.idk</code> are redirected by searching Wikipedia and extracting the relevant URL from the infobox.</p>
<p dir="auto">Example:</p>
<ol dir="auto">
<li>Type <code>scihub.idk/</code> in browser address bar</li>
<li>Observe redirect to <code>https://sci-hub.tw</code> (at the time of writing)</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/aaronjanse/dns-over-wikipedia/blob/master/demo.gif"><img src="https://github.com/aaronjanse/dns-over-wikipedia/raw/master/demo.gif" width="600" data-animated-image=""></a></p>
<blockquote>
<p dir="auto">Instead of googling for the site, I google for the site's Wikipedia article ("schihub wiki") which usually has an up-to-date link to the site in the sidebar, whereas Google is forced to censor their results.</p>
<p dir="auto">If you Google "Piratebay", the first search result is a fake "thepirate-bay.org" (with a dash) but the Wikipedia article lists the right one.
— <a href="https://news.ycombinator.com/item?id=22414031" rel="nofollow">shpx</a></p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation Options</h2><a id="user-content-installation-options" aria-label="Permalink: Installation Options" href="#installation-options"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://chrome.google.com/webstore/detail/mjmjpfncapfopnommmngnmjalkopljji/" rel="nofollow">Chrome Extension</a></h4><a id="user-content-chrome-extension" aria-label="Permalink: Chrome Extension" href="#chrome-extension"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://addons.mozilla.org/en-US/firefox/addon/dns-over-wikipedia/" rel="nofollow">Firefox Extension</a></h4><a id="user-content-firefox-extension" aria-label="Permalink: Firefox Extension" href="#firefox-extension"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/aaronjanse/dns-over-wikipedia/blob/master/hosts-file">(optional) Rust Redirect Script</a></h4><a id="user-content-optional-rust-redirect-script" aria-label="Permalink: (optional) Rust Redirect Script" href="#optional-rust-redirect-script"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon virtually kills efforts to develop Alexa Skills (123 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/04/amazon-virtually-kills-efforts-to-develop-alexa-skills-disappointing-dozens/</link>
            <guid>40008170</guid>
            <pubDate>Fri, 12 Apr 2024 00:19:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/04/amazon-virtually-kills-efforts-to-develop-alexa-skills-disappointing-dozens/">https://arstechnica.com/gadgets/2024/04/amazon-virtually-kills-efforts-to-develop-alexa-skills-disappointing-dozens/</a>, See on <a href="https://news.ycombinator.com/item?id=40008170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      disincentives    —
</h4>
            
            <h2 itemprop="description">Most devs would need to pay out of pocket to host Alexa apps after June. </h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2020/12/amazon-echo-dot-4-hero-800x450.jpg" alt="amazon echo dot gen 4">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2020/12/amazon-echo-dot-4-hero.jpg" data-height="1080" data-width="1920">Enlarge</a> <span>/</span> The 4th-gen Amazon Echo Dot smart speaker.</p><p>Amazon</p></figcaption>  </figure>

  




<!-- cache hit 723:single/related:5d634d4fdba58a69bfdeca1f6e197a57 --><!-- empty -->
<p>Alexa hasn't worked out the way Amazon originally planned.</p>
<p>There was a time when it thought that Alexa would yield a robust ecosystem of apps, or Alexa Skills, that would make the voice assistant an integral part of users' lives. Amazon envisioned tens of thousands of software developers building valued abilities for Alexa that would grow the voice assistant's popularity—and help Amazon make some money.</p>
<p>But about seven years after launching a rewards program to encourage developers to build Skills, Alexa's most preferred abilities are the basic ones, like checking the weather. And on June 30, Amazon will stop giving out the monthly Amazon Web Services credits that have made it free for third-party developers to build and host Alexa Skills. The company also recently told devs that its <a href="https://techcrunch.com/2017/08/16/amazon-expands-program-that-pays-alexa-developers-for-top-performing-voice-apps/">Alexa Developer Rewards</a> program was ending, virtually disincentivizing third-party devs to build for Alexa.</p>
<h2>Death knell for third-party Alexa apps</h2>
<p>The news has left dozens of Alexa Skills developers wondering if they have a future with Alexa, especially as Amazon preps a generative AI and <a href="https://arstechnica.com/gadgets/2024/01/alexa-is-in-trouble-paid-for-alexa-gives-inaccurate-answers-in-early-demos/">subscription-based version of Alexa</a>. <a href="https://www.youtube.com/watch?v=lKie-vgUGdI">"Dozens" may sound like a dig</a> at Alexa's ecosystem, but it's an estimation based on a podcast from Skills developers <a href="https://developer.amazon.com/en-US/alexa/champions/mark-tucker">Mark Tucker</a> and Allen Firstenberg, who, in a recent <a href="https://www.youtube.com/watch?v=kIVFnP2Z8ZQ">podcast,</a> agreed that "dozens" of third-party devs were contemplating if it's still worthwhile to develop Alexa skills. The casual summary wasn't stated as a hard fact or confirmed by Amazon but, rather, seemed like a rough and quick estimation based on the developers' familiarity with the Skills community. But with such minimal interest and money associated with Skills, dozens isn't an implausible figure either.</p>
<p>Amazon admitted that there's little interest in its Skills incentives programs. Bloomberg reported that "fewer than 1 percent of developers were using the soon-to-end programs," per Amazon spokesperson Lauren Raemhild.</p>                                            
                                                        
<p>"Today, with over 160,000 skills available for customers and a well-established Alexa developer community, these programs have run their course, and we decided to sunset them," she told the publication.</p>
<p>The writing on the wall, though, is that Amazon doesn't have the incentive or money to grow the Alexa app ecosystem it once imagined. Voice assistants largely became <a href="https://arstechnica.com/gadgets/2022/11/amazon-alexa-is-a-colossal-failure-on-pace-to-lose-10-billion-this-year/">money pits</a>, and the Alexa division has endured recent <a href="https://arstechnica.com/gadgets/2023/11/amazon-lays-off-alexa-employees-as-2010s-voice-assistant-boom-gives-way-to-ai/">layoffs</a> as it fights for survival and relevance. Meanwhile, Google Assistant stopped using third-party apps <a href="https://9to5google.com/2022/06/13/google-assistant-voice-apps/">in 2022</a>.</p>
<p>"Many developers are now going to need to make some tough decisions about maintaining existing or creating future experiences on Alexa," Tucker said via a <a href="https://www.linkedin.com/feed/update/urn:li:activity:7182139278531915776/">LinkedIn post</a>.</p>
<h2>Alexa Skills criticized as “useless”</h2>
<p>As of this writing, the top Alexa skills, in order, are: <em>Jeopardy</em>, <em>Are You Smarter Than a 5th Grader?</em>, <em>Who Wants to Be a Millionaire?</em>, and Calm. That's not exactly a futuristic list of must-have technological feats. For years, people have wondered when the "<a href="https://www.gearbrain.com/amazon-alexa-lack-of-skills-2638790700.html">killer app</a>" would come to catapult Alexa's popularity. But now it seems like Alexa's only hope at that killer use case is generative AI (a gamble filled with its <a href="https://arstechnica.com/gadgets/2023/09/amazons-generative-ai-powered-alexa-is-as-big-a-privacy-red-flag-as-old-alexa/">own obstacles</a>).</p>
<p>But like Amazon, third-party developers found it hard to make money off Skills, with a rare few pointing to making <a href="https://www.cnet.com/home/smart-home/amazon-alexa-economy-echo-speaker-google-assistant-siri/">thousands of dollars at most</a> and the vast majority not making anything.</p>
<p>"If you can't make money off it, no one's going to seriously engage," Joseph "Jo" Jaquinta, a developer who had made over 12 Skills, told <a href="https://www.cnet.com/home/smart-home/amazon-alexa-economy-echo-speaker-google-assistant-siri/">CNET</a> in 2017.</p>
<p>By 2018, Amazon had paid developers <a href="https://developer.amazon.com/en-US/blogs/alexa/post/373f6769-9ee7-41bc-9104-25cd1b393e93/alexa-developer-rewards-program-expands-to-skills-for-kid.html">millions</a> to grow Alexa Skills. But by 2020, Amazon reduced the amount of money it paid out to third-party developers, an anonymous source told Bloomberg, The source noted that the apps made by paid developers weren't making the company much money. Come 2024, the most desirable things you can make Alexa do remain basic tasks, like playing a song and apparently trivia games.</p>
<p>Amazon hasn't said it's ending Skills. That would seem premature considering that its Alexa chatbot isn't expected until June. Developers can still make money off Skills with in-app purchases, but the incentive is minimal.</p>
<p>"Developers like you have and will play a critical role in the success of Alexa, and we appreciate your continued engagement," Amazon's notice to devs said, per Bloomberg.</p>
<p>We'll see how "critical" Amazon treats those remaining developers once its generative AI chatbot is ready.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Descent: A classic 3-D first-person shooter (2012) (130 pts)]]></title>
            <link>http://insectoid.budwin.net/dos/descent/descent.html</link>
            <guid>40006697</guid>
            <pubDate>Thu, 11 Apr 2024 20:52:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://insectoid.budwin.net/dos/descent/descent.html">http://insectoid.budwin.net/dos/descent/descent.html</a>, See on <a href="https://news.ycombinator.com/item?id=40006697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="rdescm">
<p>DESCENT is a classic 3-D first-person shooter for DOS that was 
released in 1995, developed by Parallax Software and published by Interplay 
Productions.&nbsp; DESCENT is a contemporary of the other very popular FPS of 
the time, DOOM by id Software, released just over a year before.&nbsp; However, 
it is <em>not</em> based on DOOM; the DESCENT engine works quite differently, 
using 3-D models (rather than sprites) to render enemy robots, and has 
"six degrees of freedom".&nbsp; DESCENT also has very little of the 
blood or gore of DOOM.</p>

<p>There were two games made for the DOS platform, <b>DESCENT</b> (1995) and 
<b>DESCENT II</b> (1996, which was also made for Windows 95), as well a sequel 
for Windows 9x, <b>DESCENT<sup>3</sup></b> (1999).&nbsp; There have been many 
expansions for and re-releases of each; the Game Info pages detail as many of 
those that I know about.</p>

<p>For the moment, the rest of the pages will be concentrating on the first 
mission, <span>DESCENT: First Strike</span>.&nbsp; In the future I 
may make <span>DESCENT II</span> pages; but that remains to be 
seen.</p>

<p><img src="http://insectoid.budwin.net/Images/email.png" alt="Email">Questions or comments about the pages, a specific level, or Descent 
in general may be directed to <b>insectoid</b> <i>(at)</i> <b>budwin</b> 
<i>(dot)</i> <b>net</b>; please put <b>IWP:</b> in the subject line.&nbsp; I do 
appreciate the feedback, folks, both good and bad.&nbsp; However, <em>any spam 
mail will be subject to immediate de-resolution!</em>&nbsp; Requests for copies 
of the full-version games are firmly discouraged and will be treated in like 
manner.</p>

<h2><span><img alt="DESCENT" src="http://insectoid.budwin.net/Images/DOS/Descent/d1-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d1gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and ports to other systems.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1basics.html">Basics</a> – A mini-guide to 
	DESCENT.&nbsp; Under construction.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1levels.html">Walkthrough</a> – A walkthrough of 
	each level.&nbsp; Currently has only a few levels completed.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1robots.html">Robots</a> – Every robot in D1, 
	including the rarely-seen Red Triangle.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1weapons.html">Weapons and Items</a> – All of 
	the power-ups and weapons in D1.</li>
</ul>

<h2><span><img alt="DESCENT II" src="http://insectoid.budwin.net/Images/DOS/Descent/d2-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d2gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and ports to other systems.</li>
</ul>

<h2><span><img alt="DESCENT 3" src="http://insectoid.budwin.net/Images/DOS/Descent/d3-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d3gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and mods.</li>
</ul>

<hr>

<h4>LEGAL STUFF:</h4>

<p>The beveled panels, buttons, and page design are ©2011 
Thomas Keith / Insectoid.<br>

The typeface for most of the banners and the dark grey sidebar buttons are 
TrueType versions of the fonts from DESCENT II; ©1997 Harald Koenigsperger 
(Wild Style GraphX).<br>

The typeface for the game logos is a reproduction of the font used on the discs 
and packaging for all three games; I'm attributing this to Interplay 
Productions.<br>

DESCENT is a registered trademark of Interplay Productions.<br> 

DESCENT, DESCENT II, and all related content and images are ©1995, 1996 
Parallax Software Corporation.<br>

DXX-Rebirth, D1X-Rebirth and D2X-Rebirth are ©Christian Beckhaeuser.<br>

DESCENT<sup>3</sup> and all related content and images are ©1999 Outrage 
Entertainment.<br>

All other copyrights and trademarks are property of their respective owners.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using ClickHouse to scale an events engine (203 pts)]]></title>
            <link>https://github.com/getlago/lago/wiki/Using-Clickhouse-to-scale-an-events-engine</link>
            <guid>40005005</guid>
            <pubDate>Thu, 11 Apr 2024 18:02:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/getlago/lago/wiki/Using-Clickhouse-to-scale-an-events-engine">https://github.com/getlago/lago/wiki/Using-Clickhouse-to-scale-an-events-engine</a>, See on <a href="https://news.ycombinator.com/item?id=40005005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wiki-body" data-view-component="true">
                <p>Like many companies, we had to change our database stack midway while scaling our core product Lago, an open-source usage-based billing platform. As we grew more popular, we began ingesting millions of events every minute. And our rudimentary Postgres-only stack wasn’t cutting it. We were suffering heavy load times, impacting our entire app’s performance.</p>
<p>After some exploration, we decided to use a distributed ClickHouse instance strictly for our streamed events. Our analytics services were now able to directly query ClickHouse, an OLAP database. For all other data needs, we kept Postgres.</p>
<p>The strategy was successful. Since the refactor, we haven’t looked back.</p>
<p>Today, we’re going to explore that decision for a hybrid database stack, and more specifically, why we decided to go with ClickHouse.</p>
<p><h2>OLTP versus OLAP databases</h2><a id="user-content-oltp-versus-olap-databases" aria-label="Permalink: OLTP versus OLAP databases" href="#oltp-versus-olap-databases"></a></p>
<p>Most developers, including junior developers, have experience using OLTP (online transactional processing) databases such as Postgres. As the name implies, OLTP databases are designed for processing <em>transactions</em>. A transaction is one of many different types of instructions that software might invoke to a database. The most common are: (i) read, (ii) insert, (iii) update and (iv) delete.</p>
<p>OLTP databases are typically general-purpose databases. Because they support every type of data processing, they could be used for any data problem <em>within limits</em>. And, even at a large scale, they are fantastic for software that require:</p>
<ul>
<li>
<strong>atomic transactions</strong>, where a set of grouped transactions either all occur or don’t occur at all</li>
<li>
<strong>consistency</strong>, where queries in-between writes and updates are deterministic and predictable</li>
</ul>
<p>For most problems, these are important qualities. For some, they are crucial. A banking application can’t have discrepancies whenever money is transferred between accounts. For those problems, an OLTP database is needed for cents-level accuracy.
Today, we still use Postgres as our <strong>primary</strong> database, configured [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/config/database.yml#L5">via our database.yml file</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/config/database.yml#L5">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/config/database.yml#L5</a>). And given that we use Ruby on Rail’s, [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/schema.rb">our Postgres schema</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/schema.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/schema.rb</a>) is automatically generated by Rail’s [<a href="https://guides.rubyonrails.org/active_record_basics.html" rel="nofollow">Active Record</a>](<a href="https://guides.rubyonrails.org/active_record_basics.html" rel="nofollow">https://guides.rubyonrails.org/active_record_basics.html</a>), an ORM that manages our various models such as [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/charge_spec.rb">charges</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/charge_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/charge_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/credit_note_spec.rb">credit notes</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/credit_note_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/credit_note_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invoice_spec.rb">invoices</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invoice_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invoice_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invite_spec.rb">invites</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invite_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invite_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/fee_spec.rb">fees</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/fee_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/fee_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/coupon_spec.rb">coupons</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/coupon_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/coupon_spec.rb</a>), and [<a href="https://github.com/getlago/lago-api/tree/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models">much, much more</a>](<a href="https://github.com/getlago/lago-api/tree/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models">https://github.com/getlago/lago-api/tree/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models</a>). We write some custom queries given [<a href="https://github.com/getlago/lago/wiki/Is-ORM-still-an-%27anti-pattern%27%3F">the performance limits of the ORM</a>](<a href="https://github.com/getlago/lago/wiki/Is-ORM-still-an-%27anti-pattern%27%3F">https://github.com/getlago/lago/wiki/Is-ORM-still-an-%27anti-pattern%27%3F</a>), but otherwise lean heavily on Active Record for most transactions.</p>
<p>So where do OLAP (online analytical processing) databases like ClickHouse come in? Well, Postgres was designed to be <em>strictly</em> atomic and consistent; two properties that require for data to be fully <strong>ingested</strong> before any query that might process them is run. This creates a problem for tables where entries are ingested in the millions per minutes (e.g. billable events, especially those for infrastructure services like managed servers). Specifically, the issue <em><strong>isn’t</strong></em> ingesting data, but rather simultaneously handling expensive analytical queries without locking up the queue. These data-summarizing problems are where OLAP databases like ClickHouse shine.</p>
<p>OLAP databases are designed for two primary problems—(i) efficiently answering complex read queries with <em><strong>approximate</strong></em> accuracy and (ii) batch processing a large number of write queries. However, OLAP databases are <strong>terrible</strong> for mutating data (where the <strong>entire</strong> database often needs to be re-written) or deleting data.</p>
<p>Different OLAP solutions (e.g. ClickHouse, QuestDB, Druid) have different strengths, and we’ll dive into the specific strain of traits that made ClickHouse a winning solution in the next section. But all OLAP solutions share a common quality—data is stored in an inverted layout relative to OLTP databases like Postgres.
<img width="778" alt="Storage types" src="https://private-user-images.githubusercontent.com/85290767/289697112-9b014845-d4f4-4f54-ac71-bd88af75060c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI4NzMxMDUsIm5iZiI6MTcxMjg3MjgwNSwicGF0aCI6Ii84NTI5MDc2Ny8yODk2OTcxMTItOWIwMTQ4NDUtZDRmNC00ZjU0LWFjNzEtYmQ4OGFmNzUwNjBjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDExVDIyMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWM2ZGQ3Y2E5ODU4MGU2YjAxZGE3OWU0NTdlZWVjN2NkNDRkYjEwNTY1MDYzYzM2NGRlZmI0M2YzY2Q2ZmFhMTEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.HNwTuxD94gYZRubVYuKu0CkxCAKzKPqFo4oekhKT7is" content-type-secured-asset="image/png"></p>
<p>Now, from the user’s standpoint, the table’s columns and rows are still just columns and rows. But, physically in memory, data is scanned column-by-column, not row-by-row.  This makes aggregations—such as adding every value in a certain field—very, very fast, as the relevant data is read sequentially.</p>
<p><h2>Enter ClickHouse, our chosen OLAP solution</h2><a id="user-content-enter-clickhouse-our-chosen-olap-solution" aria-label="Permalink: Enter ClickHouse, our chosen OLAP solution" href="#enter-clickhouse-our-chosen-olap-solution"></a></p>
<p>[<a href="https://clickhouse.com/" rel="nofollow">ClickHouse</a>](<a href="https://clickhouse.com/" rel="nofollow">https://clickhouse.com</a>) is an open-source tool spun out from a closed-source algorithm used by Yandex’s website analytics product. Today, ClickHouse is shepherded by [<a href="https://clickhouse.com/company/our-story" rel="nofollow">ClickHouse Inc</a>](<a href="https://clickhouse.com/company/our-story" rel="nofollow">https://clickhouse.com/company/our-story</a>) with notable contributions by [<a href="https://altinity.com/" rel="nofollow">Altinity</a>](<a href="https://altinity.com/" rel="nofollow">https://altinity.com</a>). To date, it is one of the most successful OLAP databases, both commercially and qualitatively.</p>
<p>ClickHouse has three notable features that make it an analytics powerhouse—(i) dynamic materialized views, (ii) specialized engines, and (iii) vectorized query execution.</p>
<p>To summarize each:</p>
<ul>
<li>
<strong>Dynamic</strong> <strong>Materialized Views.</strong> Materialized Views are query-able views that are generated from raw data in underlying tables. While many databases <strong>do</strong> support materialized views, including Postgres, ClickHouse’s materialized views are dynamic, efficiently refreshing content whenever new content is ingested. These contrasts with ordinary materialized views which are just snapshots of a specific point of time, and are very expensive to refresh.</li>
<li>
<strong>Specialized Engines</strong>. Many databases have a single engine for utilizing hardware to process queries / transactions. ClickHouse, however, has dedicated engines for specific mathematical functions, such as summing or averaging numbers.</li>
<li>
<strong>Vectorized Query Execution</strong>. ClickHouse’s specialized engines leverage vectorized query execution, where the hardware uses multiple units in parallel to achieve a communal result (known as SIMD—Single Instruction, Multiple Data).</li>
</ul>
<p>Combined with its columnar storage, these traits allow ClickHouse to easily sum, average, or generally aggregate database values.</p>
<p>As a caveat, Postgres isn’t <em>entirely</em> incapable of achieving similar results, but only via a bastion of optimizations. For instance, there is a third-party [<a href="https://github.com/citusdata/postgres_vectorization_test">vectorized executor</a>](<a href="https://github.com/citusdata/postgres_vectorization_test">https://github.com/citusdata/postgres_vectorization_test</a>) designed for Postgres that imitates ClickHouse’s native support. There is also [<a href="https://aws.amazon.com/blogs/database/building-fast-refresh-capability-in-amazon-rds-for-postgresql/" rel="nofollow">a Fast Refresh Module</a>](<a href="https://aws.amazon.com/blogs/database/building-fast-refresh-capability-in-amazon-rds-for-postgresql/" rel="nofollow">https://aws.amazon.com/blogs/database/building-fast-refresh-capability-in-amazon-rds-for-postgresql/</a>) that uses Postgres’s log to dynamically update materialized views. Coupled with Postgres triggers, developers could create a ClickHouse-like set-up. But all of these techniques require <em>significant</em> set-up work and additional columns to reach any efficiency that is even comparable to ClickHouse’s.</p>
<p><img width="631" alt="Untitled (2)" src="https://private-user-images.githubusercontent.com/85290767/289697301-27120962-d6a1-46e6-b0bf-fe3ec3a65b0a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI4NzMxMDUsIm5iZiI6MTcxMjg3MjgwNSwicGF0aCI6Ii84NTI5MDc2Ny8yODk2OTczMDEtMjcxMjA5NjItZDZhMS00NmU2LWIwYmYtZmUzZWMzYTY1YjBhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDExVDIyMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWVkNDhkYjRlMGI5NTcwODIzNDcxNTQ1YzZhNmQzMzE0NWJlYWY4ZjAwOTRjNmEyYjM3ZjVjY2Y3NWRiMTNlMDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CGGCPxMP95NweE8DLnqEzLft_Fo_8ppc09C6cOA9m4c" content-type-secured-asset="image/png"></p><p>A relevant <a href="https://posthog.com/blog/clickhouse-vs-postgres" rel="nofollow">meme</a> from my Postgres vs Clickhouse guide for PostHog</p>
<p>Recently, the most interesting rift in the Postgres vs OLAP space is [<a href="https://www.hydra.so/" rel="nofollow">Hydra</a>](<a href="https://www.hydra.so/" rel="nofollow">https://www.hydra.so</a>), an open-source, column-oriented distribution of Postgres that was <em><strong>very</strong></em> recently launched (after our migration to ClickHouse). Had Hydra been available during our decision-making time period, we might’ve made a different choice. However, ClickHouse remains an incredible pick, given its mature product, large community, hardware optimizations, and ease of use side-by-side with Postgres.</p>
<p>Of course, migrating analytics processes to ClickHouse is only half the battle. The next is actually deploying ClickHouse to production—where a few strategies exist.</p>
<p><h2>How we utilize ClickHouse</h2><a id="user-content-how-we-utilize-clickhouse" aria-label="Permalink: How we utilize ClickHouse" href="#how-we-utilize-clickhouse"></a></p>
<p>When discussing our ClickHouse implementation, there are fundamentally two different topics—what we use ClickHouse <em><strong>for</strong></em>, and how our ClickHouse instance is deployed and maintained.</p>
<p><h3>What we query ClickHouse for</h3><a id="user-content-what-we-query-clickhouse-for" aria-label="Permalink: What we query ClickHouse for" href="#what-we-query-clickhouse-for"></a></p>
<p>Our ClickHouse instance [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/models/clickhouse/events_raw.rb#L3">ingests raw billable events</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/models/clickhouse/events_raw.rb#L3">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/models/clickhouse/events_raw.rb#L3</a>) dispatched by our users. While we don’t write our own ClickHouse schema (as it is auto-generated by ActiveRecord), it is written to a file, [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_schema.rb#L4">available in our open-source repository</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_schema.rb#L4">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_schema.rb#L4</a>). Our ClickHouse instance only has two tables—<code>raw_events</code> and <code>raw_events_queue</code>—alongside one materialized view, <code>events_raw_mv</code> . That’s it. We don’t store any of the other “business-critical” data on ClickHouse because they aren’t analytical queries.</p>
<p>In detail, our <code>[raw_events_queue](https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231026124912_create_events_raw_queue.rb)</code> is where events are initially streamed to via [<a href="https://kafka.apache.org/" rel="nofollow">Apache Kafka</a>](<a href="https://kafka.apache.org/" rel="nofollow">https://kafka.apache.org</a>), open-source event streaming software. From it, the <code>[events_raw_mv](https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231030163703_create_events_raw_mv.rb)</code> is generated with ClickHouse’s <code>[cast()](https://clickhouse.com/docs/en/sql-reference/functions/type-conversion-functions)</code> function, which maps the event’s metadata from a JSON blob to a string array. Finally, this materialized view pushes data to the <code>[raw_events](https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231024084411_create_events_raw.rb)</code> table. This is a [<a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree" rel="nofollow">MergeTree</a>](<a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree" rel="nofollow">https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree</a>) table that is apt for a large number of writes.</p>
<p><code>raw_events</code> is what Lago’s general codebase interfaces with via our <code>[ClickHouseStores](https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/events/stores/clickhouse_store.rb#L14)</code> class, which is tapped when [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/billable_metrics/aggregation_factory.rb#L18">aggregating billable metrics</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/billable_metrics/aggregation_factory.rb#L18">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/billable_metrics/aggregation_factory.rb#L18</a>). <code>raw_events</code> uses a tuple of <code>organization_id</code>, <code>external_subscription_id</code>, <code>code</code>, and a timestamp as primary keys; given ClickHouse’s [<a href="https://medium.com/datadenys/how-clickhouse-primary-key-works-and-how-to-choose-it-4aaf3bf4a8b9" rel="nofollow">sophisticated support for primary key tuples</a>](<a href="https://medium.com/datadenys/how-clickhouse-primary-key-works-and-how-to-choose-it-4aaf3bf4a8b9" rel="nofollow">https://medium.com/datadenys/how-clickhouse-primary-key-works-and-how-to-choose-it-4aaf3bf4a8b9</a>), this helps ClickHouse locate rows <strong>very</strong> quickly.</p>
<p><h3>How we deploy ClickHouse</h3><a id="user-content-how-we-deploy-clickhouse" aria-label="Permalink: How we deploy ClickHouse" href="#how-we-deploy-clickhouse"></a></p>
<p>Because ClickHouse is an open-source database, it could be self-hosted on any ordinary Linux server. However, many companies trust managed database solutions because they (i) often reduce overall costs, (ii) make scaling databases easier, and (iii) take care of safe replication/backups.</p>
<p>One of the most popular options is ClickHouse Inc’s ClickHouse Cloud offering, which offers a serverless ClickHouse instance with decoupled compute and storage.</p>
<p>However, we instead opted for Altinity Operator, which deploys and manages ClickHouse in a Kubernetes cluster in our existing cloud offering. We preferred this approach given more flexibility due to custom definitions, efficiency on cost, and ease of maintenance.</p>
<p><h2>Other notable open-source projects that use ClickHouse</h2><a id="user-content-other-notable-open-source-projects-that-use-clickhouse" aria-label="Permalink: Other notable open-source projects that use ClickHouse" href="#other-notable-open-source-projects-that-use-clickhouse"></a></p>
<p>We aren’t the only open-source project that uses ClickHouse; in fact, we aren’t even the only open-source project that migrated from Postgres to ClickHouse. A notable example is [<a href="https://posthog.com/" rel="nofollow">PostHog</a>](<a href="https://posthog.com/" rel="nofollow">https://posthog.com</a>), an open-source analytics suite that switched from [<a href="https://posthog.com/blog/clickhouse-announcement" rel="nofollow">Postgres to ClickHouse</a>](<a href="https://posthog.com/blog/clickhouse-announcement" rel="nofollow">https://posthog.com/blog/clickhouse-announcement</a>) given the sheer amount of web events they were processing per second.</p>
<p>Another great example is Gitlab, which used ClickHouse to store data of streamed events [<a href="https://docs.gitlab.com/ee/architecture/blueprints/clickhouse_usage/" rel="nofollow">in their observability suite</a>](<a href="https://docs.gitlab.com/ee/architecture/blueprints/clickhouse_usage/" rel="nofollow">https://docs.gitlab.com/ee/architecture/blueprints/clickhouse_usage/</a>). In general, it’s common for open-source companies (and closed-source projects alike) to find their general-purpose database like Postgres or mySQL ill-suited as they start to scale.</p>
<p>Even some closed-source solutions, like the HTTP data-streaming product TinyBird, have made [<a href="https://www.tinybird.co/blog-posts/we-launched-an-open-source-clickhouse-knowledge-base" rel="nofollow">open-source contributions to ClickHouse</a>](<a href="https://www.tinybird.co/blog-posts/we-launched-an-open-source-clickhouse-knowledge-base" rel="nofollow">https://www.tinybird.co/blog-posts/we-launched-an-open-source-clickhouse-knowledge-base</a>) given their dependence on it. Slowly, ClickHouse is building the same level of success in the OLAP world as Postgres is achieving in the OLTP space.</p>
<p><h3>Closing Thoughts</h3><a id="user-content-closing-thoughts" aria-label="Permalink: Closing Thoughts" href="#closing-thoughts"></a></p>
<p>Due to the hardware optimizations of inverting table layouts, there is no one-size-fits-all database as applications scale. We ran into that problem fairly early in our journey given the event-heavy nature of our product. However, that doesn’t meant that every team needs to start with an OLTP + OLAP stack—just to be ready for it when the moment arrives.</p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Man creatively sneaks onto Delta flight, but gets caught (136 pts)]]></title>
            <link>https://onemileatatime.com/news/man-creatively-sneaks-onto-delta-flight/</link>
            <guid>40004889</guid>
            <pubDate>Thu, 11 Apr 2024 17:53:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onemileatatime.com/news/man-creatively-sneaks-onto-delta-flight/">https://onemileatatime.com/news/man-creatively-sneaks-onto-delta-flight/</a>, See on <a href="https://news.ycombinator.com/item?id=40004889">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Every so often we hear stories of people sneaking onto flights. Sometimes they’re successful, and sometimes they immediately get caught. A bit over a week ago, someone tried to sneak onto a Delta flight, using <a href="https://www.cbsnews.com/news/unticketed-passenger-removed-delta-flight-salt-lake-city/" target="_blank" rel="noreferrer noopener">a method I’ve never heard of</a>. And he almost got away with it, except for one little problem…</p>
<p>I first wrote about this last week, but wanted to provide an update, as there’s now video footage of this guy’s ploy, and it’s quite interesting to see.</p>
<div id="ez-toc-container">

<nav><ul><li><a href="#how_someone_snuck_onto_a_delta_flight_from_salt_lake_city" title="How someone snuck onto a Delta flight from Salt Lake City">How someone snuck onto a Delta flight from Salt Lake City</a></li><li><a href="#the_guy_deserves_credit_for_creativity" title="The guy deserves credit for creativity">The guy deserves credit for creativity</a></li><li><a href="#bottom_line" title="Bottom line">Bottom line</a></li></ul></nav></div>
<h2 id="h-how-someone-snuck-onto-a-delta-flight-from-salt-lake-city"><span id="how_someone_snuck_onto_a_delta_flight_from_salt_lake_city"></span>How someone snuck onto a Delta flight from Salt Lake City<span></span></h2>
<p>The FBI is investigating a 26-year-old who managed to sneak onto a Delta flight from Salt Lake City (SLC) to Austin (AUS) on Sunday, March 17, 2024.</p>
<p>The man reportedly intended to fly with Southwest to Austin. He had a “buddy pass,” which was presumably given to him by a Southwest employee, to be able to fly with the airline on a space available basis. So he cleared security with that standby pass, though unfortunately the Southwest flight was full, meaning he wouldn’t be able to take the flight.</p>
<p>Desperate to get to Austin, the man then used a different strategy. He went to the gate for the Delta flight to Austin, and used his phone to take pictures of the boarding passes of other passengers without their knowledge. No one noticed this at the time, but rather this was only uncovered using security camera footage after the fact.</p>
<p>He then boarded the aircraft using the barcode on another passenger’s boarding pass. Now, he did sort of think this through — he didn’t simply take the seat associated with that boarding pass, since he knew it would be occupied. Instead, he boarded and then tried to hide in the lavatory. The goal was to let everyone else board, and then take whatever empty seat was left.</p>
<p>The problem is, there were no empty seats on the flight. So when he emerged from the lavatory and the plane began taxiing, flight attendants realized something was wrong, and the plane returned to the gate. The aircraft was met by police, where the traveler “admitted he had made a mistake and was only trying get home.” Police say the man is being held on a federal detainer at the Salt Lake County Metro Jail. The flight ended up departing around 30 minutes late.</p>
<p>Below is a video clip from Good Morning America, showing video footage from inside the terminal. You can see how the man chats up people near him, takes pictures of their boarding passes, and then boards the plane.</p>

<figure></figure>
<h2 id="h-the-guy-deserves-credit-for-creativity"><span id="the_guy_deserves_credit_for_creativity"></span>The guy deserves credit for creativity<span></span></h2>
<p>Let me start by saying that of course you shouldn’t try to sneak onto a plane. Not only because it’s unethical, but because the risk is high, and the reward is low.</p>
<p>Every so often we see stories of people trying to sneak onto planes. In some cases it’s because they genuinely have a reason that they want to go to a certain destination and either can’t or don’t want to pay for a flight, while in other cases it seems to be a game, or mental illness, or both.</p>
<p>I have to at least give this guy credit for sort of thinking this through, and sneaking onto a plane as well as one possibly could:</p>
<ul>
<li>A barcode for an eligible ticket is all that it takes to get onto a domestic flight in the United States, as IDs aren’t generally checked at the gate (but rather only when you check-in and when you go through security)</li>
<li>He made sure that he boarded before the the passenger who had the boarding pass information that he stole; when the other passenger tried to board, the system suggested that this passenger had already boarded, though the gate agent assumed it was a glitch, so allowed that person to board</li>
<li>He was smart for going to the lavatory until boarding was complete, and then hoping to snag whatever seat was empty</li>
<li>He was also of course brilliant for sneaking onto a Delta flight; who wouldn’t want to fly with the airline that gets a revenue premium for its excellence, and which has Airbus A350-1000s on order 😉</li>
</ul>
<p>This plan would have likely worked if it weren’t for every single seat on the aircraft being occupied. So if you are going to sneak onto a plane, this is about as good of a job as one could do. I think his one mistake was not checking the seat map prior to following through with this scheme. When he found out the flight was at capacity, that would’ve certainly been a clue.</p>
<p>I’m also curious how premeditated all of this was, or if he has done it before. He seems to have had a legitimate buddy pass, so he had a real way of getting through security. Did he just come up with this scheme on the spot, or had he thought through the logistics in advance?</p>
<figure><img fetchpriority="high" decoding="async" width="1200" height="938" src="https://cdn.onemileatatime.com/wp-content/uploads/2019/04/Delta-A220-Comfort-1.jpg" alt=""><figcaption>This might have worked if the flight weren’t full</figcaption></figure>
<h2 id="h-bottom-line"><span id="bottom_line"></span>Bottom line<span></span></h2>
<p>A man tried to sneak onto a Delta flight from Salt Lake City to Austin. He had a buddy pass for Southwest, but that flight was full, so he needed to find a new way to travel. So he went to the gate area for a Delta flight, took a picture of someone else’s boarding pass, and then boarded with that. </p>
<p>Once onboard, he headed to the lavatory, where he stayed until boarding was complete. His plan was to take any open seat, though the issue is that the flight was at capacity. Police then met the flight, and the man was arrested.</p>
<p><strong>What do you make of this Delta stowaway scheme?</strong></p>




 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Storm: LLM system that researches a topic and generates full-length wiki article (114 pts)]]></title>
            <link>https://github.com/stanford-oval/storm</link>
            <guid>40004887</guid>
            <pubDate>Thu, 11 Apr 2024 17:53:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/stanford-oval/storm">https://github.com/stanford-oval/storm</a>, See on <a href="https://news.ycombinator.com/item?id=40004887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking</h2><a id="user-content-storm-synthesis-of-topic-outlines-through-retrieval-and-multi-perspective-question-asking" aria-label="Permalink: STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking" href="#storm-synthesis-of-topic-outlines-through-retrieval-and-multi-perspective-question-asking"></a></p>
<p dir="auto">This repository contains the code for our NAACL 2024 paper <a href="https://arxiv.org/abs/2402.14207" rel="nofollow">Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</a> by <a href="https://cs.stanford.edu/~shaoyj" rel="nofollow">Yijia Shao</a>, <a href="https://yucheng-jiang.github.io/" rel="nofollow">Yucheng Jiang</a>, Theodore A. Kanell, Peter Xu, <a href="https://omarkhattab.com/" rel="nofollow">Omar Khattab</a>, and <a href="https://suif.stanford.edu/~lam/" rel="nofollow">Monica S. Lam</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview <a href="https://storm.genie.stanford.edu/" rel="nofollow">(Try STORM now!)</a></h2><a id="user-content-overview-try-storm-now" aria-label="Permalink: Overview (Try STORM now!)" href="#overview-try-storm-now"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/stanford-oval/storm/blob/main/assets/overview.png"><img src="https://github.com/stanford-oval/storm/raw/main/assets/overview.png"></a>
</p>
STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search.
<p dir="auto">While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.</p>
<p dir="auto"><strong>Try out our <a href="https://storm.genie.stanford.edu/" rel="nofollow">live demo</a> to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system 🙏!</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Research Before Writing</h2><a id="user-content-research-before-writing" aria-label="Permalink: Research Before Writing" href="#research-before-writing"></a></p>
<p dir="auto">STORM breaks down generating long articles with citations into two steps:</p>
<ol dir="auto">
<li><strong>Pre-writing stage</strong>: The system conducts Internet-based research to collect references and generates an outline.</li>
<li><strong>Writing stage</strong>: The system uses the outline and references to generate the full-length article with citations.</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/stanford-oval/storm/blob/main/assets/two_stages.jpg"><img src="https://github.com/stanford-oval/storm/raw/main/assets/two_stages.jpg"></a>
</p>
<p dir="auto">STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:</p>
<ol dir="auto">
<li><strong>Perspective-Guided Question Asking</strong>: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.</li>
<li><strong>Simulated Conversation</strong>: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.</li>
</ol>
<p dir="auto">Based on the separation of the two stages, STORM is implemented in a highly modular way (see <a href="https://github.com/stanford-oval/storm/blob/main/src/engine.py">engine.py</a>) using <a href="https://github.com/stanfordnlp/dspy">dspy</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><strong>We view STORM as an example of automated knowledge curation. We are working on enhancing our codebase to increase its extensibility. Stay tuned!</strong></p>
<p dir="auto">Below, we provide a quick start guide to run STORM locally to reproduce our experiments.</p>
<ol dir="auto">
<li>Install the required packages.
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n storm python=3.11
conda activate storm
pip install -r requirements.txt"><pre>conda create -n storm python=3.11
conda activate storm
pip install -r requirements.txt</pre></div>
</li>
<li>Set up OpenAI API key and <a href="https://api.you.com/" rel="nofollow">You.com search API</a> key. Create a file <code>secrets.toml</code> under the root directory and add the following content:
<div dir="auto" data-snippet-clipboard-copy-content="# Set up OpenAI API key.
OPENAI_API_KEY=<your_openai_api_key>
# If you are using the API service provided by OpenAI, include the following line:
OPENAI_API_TYPE=openai
# If you are using the API service provided by Microsoft Azure, include the following lines:
OPENAI_API_TYPE=azure
AZURE_API_BASE=<your_azure_api_base_url>
AZURE_API_VERSION=<your_azure_api_version>
# Set up You.com search API key.
YOU_API_KEY=<your_youcom_api_key>"><pre><span><span>#</span> Set up OpenAI API key.</span>
OPENAI_API_KEY=<span>&lt;</span>your_openai_api_key<span>&gt;</span>
<span><span>#</span> If you are using the API service provided by OpenAI, include the following line:</span>
OPENAI_API_TYPE=openai
<span><span>#</span> If you are using the API service provided by Microsoft Azure, include the following lines:</span>
OPENAI_API_TYPE=azure
AZURE_API_BASE=<span>&lt;</span>your_azure_api_base_url<span>&gt;</span>
AZURE_API_VERSION=<span>&lt;</span>your_azure_api_version<span>&gt;</span>
<span><span>#</span> Set up You.com search API key.</span>
YOU_API_KEY=<span>&lt;</span>your_youcom_api_key<span>&gt;</span></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Paper Experiments</h2><a id="user-content-paper-experiments" aria-label="Permalink: Paper Experiments" href="#paper-experiments"></a></p>
<p dir="auto">The FreshWiki dataset used in our experiments can be found in <a href="https://github.com/stanford-oval/storm/blob/main/FreshWiki">./FreshWiki</a>.</p>
<p dir="auto">Run the following commands under <a href="https://github.com/stanford-oval/storm/blob/main/src">./src</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pre-writing Stage</h3><a id="user-content-pre-writing-stage" aria-label="Permalink: Pre-writing Stage" href="#pre-writing-stage"></a></p>
<p dir="auto">For batch experiment on FreshWiki dataset:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv  --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5"><pre>python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv  --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5</pre></div>
<ul dir="auto">
<li><code>--engine</code> (choices=[<code>gpt-4</code>, <code>gpt-35-turbo</code>]): the LLM engine used for generating the outline</li>
<li><code>--do-research</code>: if True, simulate conversation to research the topic; otherwise, load the results.</li>
<li><code>--max-conv-turn</code>: the maximum number of questions for each information-seeking conversation</li>
<li><code>--max-perspective</code>: the maximum number of perspectives to be considered, each perspective corresponds to an information-seeking conversation.
<ul dir="auto">
<li>STORM also uses a general conversation to collect basic information about the topic. So, the maximum number of QA pairs is <code>max_turn * (max_perspective + 1)</code>. 💡 Reducing <code>max_turn</code> or <code>max_perspective</code> can speed up the process and reduce the cost but may result in less comprehensive outline.</li>
<li>The parameter will not have any effect if <code>--disable-perspective</code> is set (the perspective-driven question asking is disabled).</li>
</ul>
</li>
</ul>
<p dir="auto">To run the experiment on a single topic:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5"><pre>python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5</pre></div>
<ul dir="auto">
<li>The script will ask you to enter the <code>Topic</code> and the <code>Ground truth url</code> that will be excluded. If you do not have any url to exclude, leave that field empty.</li>
</ul>
<p dir="auto">The generated outline will be saved in <code>{output_dir}/{topic}/storm_gen_outline.txt</code> and the collected references will be saved in <code>{output_dir}/{topic}/raw_search_results.json</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Writing Stage</h3><a id="user-content-writing-stage" aria-label="Permalink: Writing Stage" href="#writing-stage"></a></p>
<p dir="auto">For batch experiment on FreshWiki dataset:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate"><pre>python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate</pre></div>
<ul dir="auto">
<li><code>--do-polish-article</code>: if True, polish the article by adding a summarization section and removing duplicate content if <code>--remove-duplicate</code> is set True.</li>
</ul>
<p dir="auto">To run the experiment on a single topic:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate"><pre>python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate</pre></div>
<ul dir="auto">
<li>The script will ask you to enter the <code>Topic</code>. Please enter the same topic as the one used in the pre-writing stage.</li>
</ul>
<p dir="auto">The generated article will be saved in <code>{output_dir}/{topic}/storm_gen_article.txt</code> and the references corresponding to citation index will be saved in <code>{output_dir}/{topic}/url_to_info.json</code>. If <code>--do-polish-article</code> is set, the polished article will be saved in <code>{output_dir}/{topic}/storm_gen_article_polished.txt</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customize the STORM Configurations</h2><a id="user-content-customize-the-storm-configurations" aria-label="Permalink: Customize the STORM Configurations" href="#customize-the-storm-configurations"></a></p>
<p dir="auto">We set up the default LLM configuration in <code>LLMConfigs</code> in <a href="https://github.com/stanford-oval/storm/blob/main/src/modules/utils.py">src/modules/utils.py</a>. You can use <code>set_conv_simulator_lm()</code>,<code>set_question_asker_lm()</code>, <code>set_outline_gen_lm()</code>, <code>set_article_gen_lm()</code>, <code>set_article_polish_lm()</code> to override the default configuration. These functions take in an instance from <code>dspy.dsp.LM</code> or <code>dspy.dsp.HFModel</code>.</p>
<p dir="auto">💡 <strong>For a good practice,</strong></p>
<ul dir="auto">
<li>choose a cheaper/faster model for <code>conv_simulator_lm</code> which is used to split queries, synthesize answers in the conversation.</li>
<li>if you need to conduct the actual writing step, choose a more powerful model for <code>article_gen_lm</code>. Based on our experiments, weak models are bad at generating text with citations.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Automatic Evaluation</h2><a id="user-content-automatic-evaluation" aria-label="Permalink: Automatic Evaluation" href="#automatic-evaluation"></a></p>
<p dir="auto">In our paper, we break down the evaluation into two parts: outline quality and full-length article quality.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Outline Quality</h3><a id="user-content-outline-quality" aria-label="Permalink: Outline Quality" href="#outline-quality"></a></p>
<p dir="auto">We introduce <em>heading soft recall</em> and <em>heading entity recall</em> to evaluate the outline quality. This makes it easier to prototype methods for pre-writing.</p>
<p dir="auto">Run the following command under <a href="https://github.com/stanford-oval/storm/blob/main/eval">./eval</a> to compute the metrics on FreshWiki dataset:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv"><pre>python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Full-length Article Quality</h3><a id="user-content-full-length-article-quality" aria-label="Permalink: Full-length Article Quality" href="#full-length-article-quality"></a></p>
<p dir="auto"><a href="https://github.com/stanford-oval/storm/blob/main/eval/eval_article_quality.py">eval/eval_article_quality.py</a> provides the entry point of evaluating full-length article quality using ROUGE, entity recall, and rubric grading. Run the following command under <code>eval</code> to compute the metrics:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt"><pre>python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use the Metric Yourself</h3><a id="user-content-use-the-metric-yourself" aria-label="Permalink: Use the Metric Yourself" href="#use-the-metric-yourself"></a></p>
<p dir="auto">The similarity-based metrics (i.e., ROUGE, entity recall, and heading entity recall) are implemented in <a href="https://github.com/stanford-oval/storm/blob/main/eval/metrics.py">eval/metrics.py</a>.</p>
<p dir="auto">For rubric grading, we use the <a href="https://huggingface.co/kaist-ai/prometheus-13b-v1.0" rel="nofollow">prometheus-13b-v1.0</a> introduced in <a href="https://arxiv.org/abs/2310.08491" rel="nofollow">this paper</a>. <a href="https://github.com/stanford-oval/storm/blob/main/eval/evaluation_prometheus.py">eval/evaluation_prometheus.py</a> provides the entry point of using the metric.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!</p>
<p dir="auto">Contact person: <a href="mailto:shaoyj@stanford.edu">Yijia Shao</a> and <a href="mailto:yuchengj@stanford.edu">Yucheng Jiang</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">Please cite our paper if you use this code or part of it in your work:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{shao2024assisting,
      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, 
      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},
      year={2024},
      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}
}"><pre><span>@inproceedings</span>{<span>shao2024assisting</span>,
      <span>title</span>=<span><span>{</span>{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>booktitle</span>=<span><span>{</span>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)<span>}</span></span>
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vortex: OpenCL compatible RISC-V GPGPU (139 pts)]]></title>
            <link>https://vortex.cc.gatech.edu/</link>
            <guid>40003868</guid>
            <pubDate>Thu, 11 Apr 2024 16:19:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vortex.cc.gatech.edu/">https://vortex.cc.gatech.edu/</a>, See on <a href="https://news.ycombinator.com/item?id=40003868">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><div><p><h2>Vortex: OpenCL Compatible RISC-V GPGPU</h2><h2>Vortex is an open-source hardware and software project to support GPGPU based on RISC-V ISA extensions. Currently Vortex supports OpenCL and it runs on FPGA. The vortex platform is highly customizable and scalable with a complete open-source compiler, driver and runtime software stack to enable research in GPU architectures.</h2></p></div><section><h2>Recent Publications<!-- --> <a href="https://vortex.cc.gatech.edu/publications/">See All</a></h2><div><div><h4>Skybox: Open-Source Graphic Rendering on Programmable RISC-V GPUs</h4><p>Authors: Blaise Tine, Varun Saxena, Santosh Srivatsan, Joshua R. Simpson, Fadi Alzammar, Liam Cooper, and Hyesoon Kim</p><p><a href="https://dl.acm.org/doi/10.1145/3582016.3582024">See Publication</a></p></div><div><h4>Implementing Hardware Extensions for Multicore RISC-V GPUs</h4><p>Authors: Blaise Tine, Hyesoon Kim</p><p><a href="https://carrv.github.io/2022/papers/CARRV2022_paper_11_Blaise.pdf">See Publication</a></p></div></div></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Readme.txt vs. README.txt (2015) (135 pts)]]></title>
            <link>https://softwareengineering.stackexchange.com/questions/301691/readme-txt-vs-readme-txt</link>
            <guid>40003743</guid>
            <pubDate>Thu, 11 Apr 2024 16:08:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://softwareengineering.stackexchange.com/questions/301691/readme-txt-vs-readme-txt">https://softwareengineering.stackexchange.com/questions/301691/readme-txt-vs-readme-txt</a>, See on <a href="https://news.ycombinator.com/item?id=40003743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>All-uppercase letters stand out and make the file easily visible which makes sense because it is probably the first thing a new user would want to look at.  (Or, at least, <em>should</em> have looked at…) As others have already said, file names starting with a capital letter will be listed before lower-case names in <a href="http://www.catb.org/~esr/jargon/html/A/ASCIIbetical-order.html">ASCIIbetical</a> sorting (<code>LC_COLLATE=C</code>) which helps make the file visible at a first glance.</p>

<p>The <code>README</code> file is part of a bunch of files a user of a free software package would normally expect to find. Others are <code>INSTALL</code> (instructions for building and installing the software), <code>AUTHORS</code> (list of contributors), <code>COPYING</code> (license text), <code>HACKING</code> (how to get started for contributing, maybe including a TODO list of starting points), <code>NEWS</code> (recent changes) or <code>ChangeLog</code> (mostly redundant with version control systems).</p>

<p>This is what the <a href="https://www.gnu.org/prep/standards/html_node/Releases.html#Releases"><em>GNU Coding Standards</em></a> have to say about the <code>README</code> file.</p>

<blockquote>
  <p>The distribution should contain a file named <code>README</code> with a general overview of the package:</p>
  
  <ul>
  <li>the name of the package;</li>
  <li>the version number of the package, or refer to where in the package the version can be found;</li>
  <li>a general description of what the package does;</li>
  <li>a reference to the file <code>INSTALL</code>, which should in turn contain an explanation of the installation procedure;</li>
  <li>a brief explanation of any unusual top-level directories or files, or other hints for readers to find their way around the source;</li>
  <li>a reference to the file which contains the copying conditions. The GNU GPL, if used, should be in a file called <code>COPYING</code>. If the GNU LGPL is used, it should be in a file called <code>COPYING.LESSER</code>.</li>
  </ul>
</blockquote>

<p>Since it is always good to strive for the least surprise of your users, you should follow this convention unless there are compelling reasons for a deviation. In the UNIX world, file name extensions were traditionally used sparingly so the canonical name of the file is <code>README</code> without any suffix. But most users probably would have no troubles understanding that a file named <code>README.txt</code> has the same meaning. If the file is written in <a href="https://daringfireball.net/projects/markdown/"><em>Markdown</em></a>, a file name like <code>README.md</code> might also be reasonable. Avoid using more complicated markup languages like HTML in the <code>README</code> file, however, because it should be convenient to read on a text-only terminal. You can point users to the manual of the software or its on-line documentation, that might be written in a more sophisticated format, for details from the <code>README</code> file.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a new sensor out of 3D printer filament for my PhD (749 pts)]]></title>
            <link>https://paulbupejr.com/developing-the-optigap-sensor-system/</link>
            <guid>40003710</guid>
            <pubDate>Thu, 11 Apr 2024 16:06:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulbupejr.com/developing-the-optigap-sensor-system/">https://paulbupejr.com/developing-the-optigap-sensor-system/</a>, See on <a href="https://news.ycombinator.com/item?id=40003710">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-890">

<div>
<p><span><span>Reading Time: </span> <span>9</span> <span>minutes</span></span></p><p>This article explores the research and development journey behind my new sensor system, <a href="https://ieeexplore.ieee.org/document/10161357" target="_blank" rel="noreferrer noopener">OptiGap</a>, a key component of my PhD research. I’m writing this in a storytelling format to offer insights into my decision-making process and the evolution leading to the final implementation. It should hopefully provide a glimpse into the sometimes-shrouded world of PhD research and may appeal to those curious about the process. For a deeper dive into technical specifics, simulations, and existing research on this subject, my dissertation is <a href="https://ir.library.louisville.edu/etd/4213/" target="_blank" rel="noreferrer noopener">available online here</a>.</p>
<h3>What does it do?</h3>
<p>In very general terms, this sensor is basically a rope that if bent can tell you where along its length you bent it. The fancy term for that is “bend localization.”</p>
<p>OptiGap’s application is mainly within the realm of soft robotics, which typically involves compliant (or ‘squishy’) systems, where the use of<a href="https://paulbupejr.com/autonomous-robot-design/"> traditional sensors</a> is often not practical. The name OptiGap, a fusion of “optical” and “gap,” reflects its core principle of utilizing air gaps within flexible optical light pipes to generate coded patterns essential for bend localization. </p>
<h2>How the OptiGap Sensor System Started</h2>
<p>The idea for OptiGap came about while I was experimenting with light transmission through various light pipes (optical cables) for use as a bend detection sensor. I was initially trying to see how I could effectively “slow down” light through the fiber…a seemingly straightforward task, right?</p>
<p>During this process, I attached a section of clear 3D printer filament (1.75mm TPU) to a piece of tape measure for an experiment and incidentally discovered that when I bent the tape measure (and filament) at the spot where the electrical tape was attached, there was a significant drop in light transmission. I hypothesized that this was because the sticky residue of the electrical tape was causing the filament to stretch, which in turn reduced the light transmission.</p>
<p>To verify this hypothesis, I attached a longer piece of TPU to a tape measure and began bending it at various points to observe how light transmission would change.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/black_tape-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-891&quot;,&quot;imgStyles&quot;:&quot;aspect-ratio:4\/3;object-fit:cover&quot;,&quot;targetWidth&quot;:1920,&quot;targetHeight&quot;:2560,&quot;scaleAttr&quot;:&quot;cover&quot;,&quot;ariaLabel&quot;:&quot;Enlarge image: Tape measure experiment for OptiGap&quot;,&quot;alt&quot;:&quot;Tape measure experiment for OptiGap&quot;}" data-wp-interactive="core/image"><img decoding="async" width="768" height="1024" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-768x1024.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-768x1024.jpg" alt="Tape measure experiment for OptiGap" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-768x1024.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-225x300.jpg 225w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-1152x1536.jpg 1152w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-1536x2048.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-scaled.jpg 1920w" data-sizes="(max-width: 768px) 100vw, 768px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-768x1024.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-225x300.jpg 225w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-1152x1536.jpg 1152w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-1536x2048.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-scaled.jpg 1920w"><figcaption>Tape measure experiment with clear TPU filament.</figcaption></figure></div>
<p>I wrote a small Linux I2C driver for the <a href="https://www.st.com/en/imaging-and-photonics-solutions/vl53l0x.html">VL53L0X </a>ToF sensor to run on a <a href="https://zeromq.org/" target="_blank" rel="noreferrer noopener">Raspberry Pi</a> and push the data to a socket using <a href="https://zeromq.org/">ZeroMQ</a>. I then created a rough GUI in Python to pull the sensor data from the socket and visualize the light transmission data in realtime, shown in the GIF below, which very quickly validated my hypothesis. This validation marked the “Eureka!” moment that sparked the eventual development of the OptiGap sensor.</p>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/tape.gif" alt="Initial OptiGap discovery"><figcaption>My excited face while validating my discovery.</figcaption></figure>
<h2>The OptiGap Realization</h2>
<p>I realized that since I could control where the light was being attenuated, I could use this to encode information about the position of the bend on the sensor. Using electrical tape was not a practical solution, so I started looking for a more reliable and consistent way to create these attenuations. This led me to the idea of cutting the filament and then reattaching it together using a flexible rubber (silicone) sleeve, leaving a small air gap, as shown in the image below.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/tpu_gap.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-895&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1600,&quot;targetHeight&quot;:526,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Proof-of-concept showing a light pipe with an air in a silicone sleeve.&quot;,&quot;alt&quot;:&quot;Proof-of-concept showing a light pipe with an air in a silicone sleeve.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="337" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1024x337.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1024x337.jpg" alt="Proof-of-concept showing a light pipe with an air in a silicone sleeve." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1024x337.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-300x99.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-768x252.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1536x505.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap.jpg 1600w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1024x337.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-300x99.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-768x252.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1536x505.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap.jpg 1600w"><figcaption>Proof-of-concept showing a light pipe with an air in a silicone sleeve.</figcaption></figure>
<p>The main working principle of the air gap is that translation and/or rotation of one light pipe face relative to the other changes the fraction of light transmitted across the gap. The greater the bend angle, the more light escapes across the gap. The resulting change in intensity of the optical signal can then be correlated with known patterns for use as a sensor.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/operating_principle.png&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-896&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1693,&quot;targetHeight&quot;:1046,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: OptiGap operating principle&quot;,&quot;alt&quot;:&quot;OptiGap operating principle&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="633" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1024x633.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1024x633.png" alt="OptiGap operating principle" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1024x633.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-300x185.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-768x474.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1536x949.png 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-825x510.png 825w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle.png 1693w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1024x633.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-300x185.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-768x474.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1536x949.png 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-825x510.png 825w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle.png 1693w"><figcaption>This image is from a COMSOL simulation I made.</figcaption></figure>
<h2>The Big Idea</h2>
<p>I then proceeded to test this idea by creating multiple air gaps in a row and bending the filament to measure the attenuation.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/tpu_tof-1-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large is-style-default&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-898&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1578,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Multiple air gaps along a single TPU lightpipe&quot;,&quot;alt&quot;:&quot;Multiple air gaps along a single TPU lightpipe&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="631" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1024x631.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1024x631.jpg" alt="Multiple air gaps along a single TPU lightpipe" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1024x631.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-300x185.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-768x474.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1536x947.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-2048x1263.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1024x631.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-300x185.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-768x474.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1536x947.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-2048x1263.jpg 2048w"><figcaption>Multiple air gaps along a single TPU lightpipe.</figcaption></figure>
<p>As depicted in the GIF below, the optical intensity decreases at each air gap, with a more noticeable decrease as the bend angle increases. This initial experimentation served as proof of concept, demonstrating the feasibility of the idea. It led to the formulation of my final hypothesis of <strong>utilizing a pattern of these air gaps to encode information regarding the sensor’s bending and employing a naive Bayes classifier on a microcontroller to decode the bend location.</strong></p>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/gaps.gif" alt="Validating the attenuation at the air gaps."><figcaption>Validating the attenuation at the air gaps.</figcaption></figure>
<p>This concept resembles the functionality of a linear encoder. Linear encoders gauge an object’s linear movement, typically comprising a slider rail with a coded scale akin to a measuring ruler and a sensing head that moves across this scale to read it. Linear (absolute) encoders emit a distinct code at each position, ensuring consistent identification of displacement.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/block_diagram.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-900&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:519,&quot;targetHeight&quot;:417,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: OptiGap system overview.&quot;,&quot;alt&quot;:&quot;OptiGap system overview.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="519" height="417" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram.png" alt="OptiGap system overview." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram-300x241.png 300w" data-sizes="(max-width: 519px) 100vw, 519px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram-300x241.png 300w"><figcaption>OptiGap system overview.</figcaption></figure></div>
<p>The OptiGap system, functioning like an absolute encoder, would encode absolute positions using patterns of bend-sensitive air gaps along parallel light pipes, effectively serving as a singular fiber optic sensor.</p>
<h3><strong>Encoding the Bend Location using Inverse Gray Code</strong></h3>
<p>Absolute encoders commonly employ Gray code, a binary system where two successive values differ in only one bit. This property allows for various applications, including error checking. However, Gray code isn’t optimal for the OptiGap sensor system. Here, we aim for consecutive values to differ by the maximum number of bits to facilitate easier differentiation. This necessity gave rise to Inverse Gray code.</p>
<div>
<figure><img decoding="async" width="300" height="180" src="https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-300x180.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-300x180.jpg" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-300x180.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-1024x615.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-768x461.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code.jpg 1176w" data-sizes="(max-width: 300px) 100vw, 300px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-300x180.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-1024x615.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-768x461.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code.jpg 1176w"></figure></div>
<p>Inverse Gray code is a binary code where two successive values differ by the maximum (n-1) number of bits. To implement this, I simply create cuts in the filament wherever there’s a “1” in the Inverse Gray code sequence. This approach can scale to any bit number. For the prototype, I utilized 3 bits, providing 8 possible positions.</p>
<h3>Visualization of the OptiGap Sensor System</h3>
<p>The illustration below depicts the signal patterns of the OptiGap sensor system for each bend position using three fibers. By employing a naive Bayes classifier, the sensor system can discern bend positions based on signal patterns. The third graph represents actual sensor data from the prototype system, utilized for training the classifier on the microcontroller.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/signal_patterns.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-903&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:519,&quot;targetHeight&quot;:448,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: OptiGap bending patterns.&quot;,&quot;alt&quot;:&quot;OptiGap bending patterns.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="519" height="448" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns.png" alt="OptiGap bending patterns." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns-300x259.png 300w" data-sizes="(max-width: 519px) 100vw, 519px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns-300x259.png 300w"><figcaption>OptiGap bending patterns.</figcaption></figure></div>
<h2>The OptiGap Prototype</h2>
<p>I proceeded to construct a prototype of the OptiGap sensor system, utilizing 3 strands of clear TPU 3D printer filament, each featuring a distinct pattern of air gaps. The image below showcases the filament just before cutting, with the cut pattern indicated on a piece of tape.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/making_cuts-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-905&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:389,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Beginning stages of an OptiGap sensor prototype.&quot;,&quot;alt&quot;:&quot;Beginning stages of an OptiGap sensor prototype.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="156" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1024x156.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1024x156.jpg" alt="Beginning stages of an OptiGap sensor prototype." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1024x156.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-300x46.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-768x117.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1536x234.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-2048x312.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1024x156.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-300x46.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-768x117.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1536x234.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-2048x312.jpg 2048w"><figcaption>Beginning stages of an OptiGap sensor prototype.</figcaption></figure>
<p>For the prototype, I employed a commercial 3:1 fiber optic coupler to merge the light from the 3 strands into a single fiber optic cable, resulting in the completion of the sensor prototype, as depicted below.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/first_assembled_prototype-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-906&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:2106,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Assembled sensing head of an OptiGap sensor.&quot;,&quot;alt&quot;:&quot;Assembled sensing head of an OptiGap sensor.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="842" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1024x842.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1024x842.jpg" alt="Assembled sensing head of an OptiGap sensor." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1024x842.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-300x247.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-768x632.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1536x1264.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-2048x1685.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1024x842.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-300x247.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-768x632.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1536x1264.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-2048x1685.jpg 2048w"><figcaption>Assembled sensing head of an OptiGap sensor.</figcaption></figure>
<p>This marked the final phase of validating the hypothesis and operational theory behind the OptiGap sensor.</p>
<h3>Reducing the Physical Size</h3>
<p>The initial prototype proved to be large and bulky, primarily due to the size of the 3D printer filament used. Drawing from previous experience, I recognized that PMMA (plastic) optical fiber offered a smaller and more flexible alternative suitable for this application. Consequently, I assessed 500, 750, and 1000 micron unjacketed PMMA optical fibers from Industrial Fiber Optics, Inc. for the sensor strands, resulting in a significant reduction in sensor size.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/fiber_label-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-907&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1359,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: 500 micron PMMA fiber spool.&quot;,&quot;alt&quot;:&quot;500 micron PMMA fiber spool.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="544" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1024x544.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1024x544.jpg" alt="500 micron PMMA fiber spool." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1024x544.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-300x159.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-768x408.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1536x815.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-2048x1087.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1024x544.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-300x159.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-768x408.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1536x815.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-2048x1087.jpg 2048w"><figcaption>500 micron PMMA fiber spool.</figcaption></figure>
<p>I conducted tests on all three types of fibers to evaluate their light transmission and flexibility. Among them, the 500 micron fiber emerged as the optimal choice overall, although all three exhibited sufficient flexibility for this application.</p>
<h3><strong>Reducing the Optical Transceiver Complexity</strong></h3>
<p>I decided to switch from using the complex VL53L0X ToF sensor to a simple photodiode and IR LED setup to reduce the complexity of the system and to increase modularity. This also allowed me to use a &nbsp;microcontroller to read the sensor data, which was a significant improvement over the initial prototype.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/new_fiber_tx-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-908&quot;,&quot;imgStyles&quot;:&quot;aspect-ratio:4\/3;object-fit:cover&quot;,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1980,&quot;scaleAttr&quot;:&quot;cover&quot;,&quot;ariaLabel&quot;:&quot;Enlarge image: IR LED prototype board.&quot;,&quot;alt&quot;:&quot;IR LED prototype board.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="792" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1024x792.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1024x792.jpg" alt="IR LED prototype board." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1024x792.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-300x232.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-768x594.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1536x1188.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-2048x1584.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1024x792.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-300x232.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-768x594.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1536x1188.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-2048x1584.jpg 2048w"><figcaption>IR LED prototype board with 1000 micron PMMA fiber.</figcaption></figure>
<p>I then created a demo system for the sensor based around an STM32 microcontroller and a photodiode/IR LED setup.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/demo_system.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-909&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:519,&quot;targetHeight&quot;:298,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Full OptiGap demo system using 500 micron PMMA fiber.&quot;,&quot;alt&quot;:&quot;Full OptiGap demo system using 500 micron PMMA fiber.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="519" height="298" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/demo_system.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/demo_system.png" alt="Full OptiGap demo system using 500 micron PMMA fiber." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/demo_system.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/demo_system-300x172.png 300w" data-sizes="(max-width: 519px) 100vw, 519px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/demo_system.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/demo_system-300x172.png 300w"><figcaption>Full OptiGap demo system using 500 micron PMMA fiber.</figcaption></figure></div>
<h2><strong>Realtime Machine Learning on a Microcontroller</strong></h2>
<p>The final stage in developing the OptiGap sensor system involved integrating a naive Bayes classifier onto the STM32 microcontroller to decode the bend location from the sensor data. <strong><em>I opted for a naive Bayes classifier due to its efficiency compared to if-statements or lookup tables, its capability to handle new or previously unseen data, and its potential for increased accuracy by considering relationships between multiple input variables.</em></strong></p>
<p>Implementing the naive Bayes classifier proved to be relatively straightforward. This classifier is a probabilistic model based on applying Bayes’ theorem to determine how a measurement can be assigned to a particular class, with the class representing the bend location in this context. I utilized the <a href="https://www.arm.com/technologies/cmsis">Arm CMSIS-DSP library</a> for the classifier implementation.</p>
<h3>Fitting the Sensor Data</h3>
<p>The initial step in integrating the classifier was to fit the sensor data to a Gaussian distribution for each air gap pattern. To expedite this process, I developed a Python GUI for rapid labeling and fitting of the data using GNB (Gaussian Naive Bayes) from the scikit-learn library.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/ui_2.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-910&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1431,&quot;targetHeight&quot;:632,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Initial data labeling and fitting UI.&quot;,&quot;alt&quot;:&quot;Initial data labeling and fitting UI.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="452" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-1024x452.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-1024x452.jpg" alt="Initial data labeling and fitting UI." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-1024x452.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-300x132.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-768x339.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2.jpg 1431w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-1024x452.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-300x132.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-768x339.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2.jpg 1431w"><figcaption>Initial data labeling and fitting UI.</figcaption></figure>
<p>I later improved this UI to be more general and to allow for more complex data fitting.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/ui.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-911&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1453,&quot;targetHeight&quot;:725,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Improved UI.&quot;,&quot;alt&quot;:&quot;Improved UI.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="511" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/ui-1024x511.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/ui-1024x511.jpg" alt="Improved UI." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/ui-1024x511.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/ui-300x150.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/ui-768x383.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/ui.jpg 1453w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/ui-1024x511.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/ui-300x150.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/ui-768x383.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/ui.jpg 1453w"><figcaption>Improved UI.</figcaption></figure>
<p>The probabilities for each class were computed and saved as a header for use on the microcontroller.</p>
<h3>Filtering the Sensor Data</h3>
<p>To enhance the accuracy of the classifier, I implemented a two-stage filtering process on the STM32 . The initial stage involved a basic moving average filter, followed by a Kalman filter in the second stage. </p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/dsp.jpg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-913&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:554,&quot;targetHeight&quot;:190,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Signal filtering stages.&quot;,&quot;alt&quot;:&quot;Signal filtering stages.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="554" height="190" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/dsp.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/dsp.jpg" alt="Signal filtering stages." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/dsp.jpg 554w, https://paulbupejr.com/wp-content/uploads/2024/04/dsp-300x103.jpg 300w" data-sizes="(max-width: 554px) 100vw, 554px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/dsp.jpg 554w, https://paulbupejr.com/wp-content/uploads/2024/04/dsp-300x103.jpg 300w"><figcaption>Signal filtering stages. Noise reduction relative to input signal.</figcaption></figure></div>

<p>The GIFs provided below illustrate various stages of the OptiGap sensor system, encompassing assembly and the operational demonstration of the final sensor system.</p>
<h4>System Overview</h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/System_Overview.gif" alt=""></figure>
<h4>Assembly of an OptiGap Sensor using TPU Filament</h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Assembly.gif" alt=""></figure>
<h4>Attenuation of Light through the OptiGap Sensor</h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Attenuation.gif" alt=""></figure>
<h4><strong>Fitting of the Sensor Data</strong></h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Training.gif" alt=""></figure>
<h4><strong>Segment Classification using PMMA Optical Fiber</strong></h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Segment_Classification.gif" alt=""></figure>
<h4><strong>Segment Classification using TPU Filament</strong></h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Validation.gif" alt=""></figure>
<h4><strong>Underwater Operation</strong></h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Underwater_Validation.gif" alt=""></figure>
<h2>OptiGap Design Specifications</h2>
<h3>Key Properties &amp; Parameters</h3>
<div>
<figure><img decoding="async" width="1024" height="191" src="https://paulbupejr.com/wp-content/uploads/2024/04/properties-1024x191.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/properties-1024x191.png" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/properties-1024x191.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/properties-300x56.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/properties-768x143.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/properties.png 1176w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/properties-1024x191.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/properties-300x56.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/properties-768x143.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/properties.png 1176w"></figure></div>
<h3>Material Recommendations</h3>
<div>
<figure><img decoding="async" width="1024" height="636" src="https://paulbupejr.com/wp-content/uploads/2024/04/materails-1024x636.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/materails-1024x636.png" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/materails-1024x636.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/materails-300x186.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/materails-768x477.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/materails.png 1174w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/materails-1024x636.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/materails-300x186.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/materails-768x477.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/materails.png 1174w"></figure></div>
<h2>Next Steps</h2>
<p>I’ve made significant progress on the OptiGap system beyond what’s documented here, including its integration into another modular actuation and sensing system I developed called EneGate.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/enegate_optigap-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-922&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:2035,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="814" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1024x814.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1024x814.jpg" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1024x814.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-300x239.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-768x611.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1536x1221.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-2048x1628.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1024x814.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-300x239.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-768x611.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1536x1221.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-2048x1628.jpg 2048w"><figcaption>My EneGate PCB integrating an OptiGap sensor.</figcaption></figure>
<p>This has involved custom PCB design and systems integration, detailed in my dissertation. Additionally, I’ve prototyped miniature PCB versions of the optics to interface with the PCBs for the EneGate system.</p>
<figure>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/daughterboard.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-924&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1600,&quot;targetHeight&quot;:1200,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="768" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" data-id="924" src="https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1024x768.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1024x768.jpg" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1024x768.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-300x225.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-768x576.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1536x1152.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard.jpg 1600w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1024x768.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-300x225.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-768x576.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1536x1152.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard.jpg 1600w"><figcaption>Mini OptiGap PCB</figcaption></figure>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/enegate_optigap_pcb.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-923&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:907,&quot;targetHeight&quot;:599,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="907" height="599" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" data-id="923" src="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb.jpg" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb.jpg 907w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb-300x198.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb-768x507.jpg 768w" data-sizes="(max-width: 907px) 100vw, 907px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb.jpg 907w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb-300x198.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb-768x507.jpg 768w"><figcaption>Another mini OptiGap PCB</figcaption></figure>
</figure>
<p>I’ve also validated OptiGap on a real-world soft robotic system, with full details set to be presented in an upcoming RoboSoft paper titled “<strong><em>Embedded Optical Waveguide Sensors for Dynamic Behavior Monitoring in Twisted-Beam Structures.</em></strong>“</p>
<h3><strong>Commercialization</strong></h3>
<p>There’s an ongoing commercialization aspect to this research as well. Feel free to reach out if you’re interested in further details.</p>
<h2>That’s it for now!</h2>
<p>I don’t want to make this too long so I’ll end here. I hope this provided some insight into the research and development process involved in something like this. If you have any questions or would like to learn more, don’t hesitate to contact me!</p>
</div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenTofu Response to HashiCorp's Cease and Desist Letter (124 pts)]]></title>
            <link>https://opentofu.org/blog/our-response-to-hashicorps-cease-and-desist/</link>
            <guid>40003692</guid>
            <pubDate>Thu, 11 Apr 2024 16:04:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opentofu.org/blog/our-response-to-hashicorps-cease-and-desist/">https://opentofu.org/blog/our-response-to-hashicorps-cease-and-desist/</a>, See on <a href="https://news.ycombinator.com/item?id=40003692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p>On April 3rd, we received a Cease and Desist letter from HashiCorp regarding our implementation of the "removed" block in OpenTofu, claiming copyright infringement on the part of one of our core developers. We were also made aware of an article posted that same day with the same accusations. We have investigated these claims and are publishing the C&amp;D letter, our response and the source code origin document resulting from our investigation.</p>
<p><strong>The OpenTofu team vehemently disagrees with any suggestion that it misappropriated, mis-sourced, or otherwise misused HashiCorp’s BSL code. All such statements have zero basis in facts.</strong></p>
<p>HashiCorp has made claims of copyright infringement in a cease &amp; desist letter. These claims are <strong>completely unsubstantiated</strong>.</p>
<p>The code in question can be clearly shown to have been copied from older code under the MPL-2.0 license. HashiCorp seems to have copied the same code itself when they implemented their version of this feature. All of this is easily visible in our detailed SCO analysis, as well as their own comments which indicate this.</p>

<ul>
<li><a href="https://opentofu.github.io/legal-documents/2024-04-03%20HashiCorp%20C%26D/OpenTofu%20C&amp;D%20-%20Redacted.pdf" target="_blank" rel="noopener noreferrer">HashiCorp's C&amp;D Letter</a></li>
<li><a href="https://opentofu.github.io/legal-documents/2024-04-03%20HashiCorp%20C%26D/OpenTofu%20C&amp;D%20Response%20-%20Redacted.pdf" target="_blank" rel="noopener noreferrer">Our Response</a></li>
<li>Source Code Origin Document: [<a href="https://opentofu.github.io/legal-documents/2024-04-03%20HashiCorp%20C%26D/SCO.html" target="_blank" rel="noopener noreferrer">HTML</a>, <a href="https://opentofu.github.io/legal-documents/2024-04-03%20HashiCorp%20C%26D/SCO.pdf" target="_blank" rel="noopener noreferrer">PDF</a>] <strong>⇐ For the detailed code analysis, see here.</strong></li>
</ul>
<p><em>To prevent further harassment of individual people, we have redacted any personal information from these documents.</em></p>

<p>Despite these events, we have managed to carry out significant development on OpenTofu 1.7, including state encryption, “for_each” implementation for “import” blocks, as well as the all-new <strong>provider-defined functions</strong> supported by the recently released provider plugin protocol.</p>
<p>On that note, we will be releasing a new pre-release version next week, and we are eager to gather feedback from the community.</p>
<p>— The OpenTofu Team</p>
<hr>
<p><small><p><em>The image in this blog post contains code licensed under the BUSL-1.1 by HashiCorp. However, for the purposes of this post we are making non-commercial, transformative fair use under <a href="https://www.govinfo.gov/content/pkg/USCODE-2022-title17/html/USCODE-2022-title17-chap1-sec107.htm" target="_blank" rel="noopener noreferrer">17 U.S. Code § 107</a>. <br> You can read more about fair use on the <a href="https://www.copyright.gov/fair-use/" target="_blank" rel="noopener noreferrer">website of the US Copyright Office</a>.</em></p></small></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New paintings found at Pompeii (171 pts)]]></title>
            <link>https://www.bbc.com/news/science-environment-68777741</link>
            <guid>40003138</guid>
            <pubDate>Thu, 11 Apr 2024 15:19:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/science-environment-68777741">https://www.bbc.com/news/science-environment-68777741</a>, See on <a href="https://news.ycombinator.com/item?id=40003138">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Apple alerts users in 92 nations to mercenary spyware attacks (439 pts)]]></title>
            <link>https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/</link>
            <guid>40002987</guid>
            <pubDate>Thu, 11 Apr 2024 15:06:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/">https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/</a>, See on <a href="https://news.ycombinator.com/item?id=40002987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Apple sent threat notifications to iPhone users in 92 countries on Wednesday, warning them that they may have been targeted by mercenary spyware attacks.</p>
<p>The company said it sent the alerts to individuals in 92 nations at 12 p.m. Pacific Time Wednesday. The notification, which TechCrunch has seen, did not disclose the attackers’ identities or the countries where users received notifications.</p>
<p>“Apple detected that you are being targeted by a mercenary spyware attack that is trying to remotely compromise the iPhone associated with your Apple ID -xxx-,” it wrote in the warning to affected customers.</p>
<p>“This attack is likely targeting you specifically because of who you are or what you do. Although it’s never possible to achieve absolute certainty when detecting such attacks, Apple has high confidence in this warning — please take it seriously,” Apple added in the text.</p>
<p>The iPhone maker sends these kind of notifications <a href="https://techcrunch.com/2023/10/30/indian-opposition-leaders-says-apple-has-warned-them-of-state-sponsored-iphone-attacks/" target="_blank" rel="noopener">multiple times a year</a> and has notified users to such threats in over 150 countries since 2021, per an updated Apple <a href="https://support.apple.com/en-in/102174" target="_blank" rel="noopener">support page</a>.</p>
<p>Apple also sent an identical warning to a number of journalists and politicians in India in October last year. Later, nonprofit advocacy group Amnesty International <a href="https://techcrunch.com/2023/12/27/india-pressed-apple-on-state-sponsored-warnings-report-says/" target="_blank" rel="noopener">reported</a> that it had found Israeli spyware maker NSO Group’s invasive spyware Pegasus on the iPhones of prominent journalists in India. (Users in India are among those who have received Apple’s latest threat notifications, according to people familiar with the matter.)</p>
<p>The spyware alerts arrive at a time when many nations are preparing for elections. In recent months, many tech firms have cautioned about rising state-sponsored efforts to sway certain electoral outcomes. Apple’s alerts, however, did not remark on their timing.</p>
<p>“We are unable to provide more information about what caused us to send you this notification, as that may help mercenary spyware attackers adapt their behavior to evade detection in the future,” Apple told affected customers.</p>
<p>Apple <a href="https://web.archive.org/web/20240101053644/https://support.apple.com/en-in/102174" target="_blank" rel="noopener">previously</a> described the attackers as “state-sponsored” but has replaced all such references with “mercenary spyware attacks.”</p>
<p>The warning to customers adds: “Mercenary spyware attacks, such as those using Pegasus from the NSO Group, are exceptionally rare and vastly more sophisticated than regular cybercriminal activity or consumer malware.”</p>
<p>Apple said it relies solely on “internal threat-intelligence information and investigations to detect such attacks.”</p>
<p>“Although our investigations can never achieve absolute certainty, Apple threat notifications are high-confidence alerts that a user has been individually targeted by a mercenary spyware attack and should be taken very seriously,” it added.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vietnamese property tycoon sentenced to death in $27B fraud case (217 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/apr/11/vietnamese-property-tycoon-sentenced-to-death-in-27bn-case</link>
            <guid>40002851</guid>
            <pubDate>Thu, 11 Apr 2024 14:53:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/apr/11/vietnamese-property-tycoon-sentenced-to-death-in-27bn-case">https://www.theguardian.com/world/2024/apr/11/vietnamese-property-tycoon-sentenced-to-death-in-27bn-case</a>, See on <a href="https://news.ycombinator.com/item?id=40002851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A prominent property tycoon has been sentenced to death for her role in Vietnam’s biggest-ever fraud case.</p><p>Truong My Lan, the chair of the developer Van Thinh Phat, was found guilty of embezzlement, bribery and violations of banking rules on Thursday, in a case that has shocked the country. A total of $12.5bn (£10bn) was embezzled, the equivalent of almost 3% of Vietnamese gross domestic product, but prosecutors said on Thursday the total damages caused by the scam now amounted to $27bn.</p><p>“The defendant’s actions … eroded people’s trust in the leadership of the [Communist] party and state,” read the verdict at the trial in Ho Chi Minh City.</p><p>Lan was found guilty of swindling money from Saigon Commercial Bank (SCB) over a decade. She had been on tried alongside 85 others, including former central bankers and government officials, as well as previous SCB executives.</p><p>The trial is part of a national corruption crackdown led by the secretary general of the Communist party of Vietnam, Nguyễn Phú Trọng. The campaign, which is also known as “Blazing Furnace” and has increased in recent years, has led to the indictment of thousands of people, as well as the <a href="https://www.theguardian.com/world/2024/mar/21/vietnam-president-vo-van-thuong-resignation" data-link-name="in body link">resignation of two presidents</a> and two deputy prime ministers.</p><p>Lan, who was arrested in October 2022, had denied the charges. A relative told Reuters before the verdict that she would appeal. The death sentence is an unusually severe punishment for a corruption case.</p><p>State media reported last week that Lan told the court she had joined the banking industry without sufficient experience and blamed a “lack of understanding of legal matters”. She said she had “thought of death” in desperation, and asked the court for leniency for her husband, a Hong Kong businessman, and niece who were on trial as accomplices.</p><p>The verdicts announced on Thursday followed a five-week trial that has been covered in great detail in Vietnam’s tightly controlled state media.</p><p>Documents related to the trial, kept in 105 boxes, weighed 6 tonnes, according to VN Express, which reported the authorities had installed security cameras and fire safety equipment to protect the evidence ahead of the hearings. More than 1,000 properties belonging to Lan have been seized, and nearly 2,700 individuals were summoned for the trial, which included 200 lawyers.</p><p>Dr Nguyen Khac Giang, a visiting fellow at the Vietnam studies programme at the Iseas-Yusof Ishak Institute, said the case was unprecedented, and a milestone in Vietnam’s anti-corruption crackdown. It was, he said, “the biggest case against a private business, with the biggest number of defendants, the biggest number of evidence and of course, the biggest number of money involved”.</p><p>Although Lan did not directly hold executive power at SCB, she owned 91.5% of the bank’s shares through third parties and shell companies.</p><p>She was accused of setting up fake loan applications to withdraw money from the bank over a period of 11 years, from 2012 to 2022. The loans accounted for 93% of the total credit the bank has issued, according to state media.</p><p>To cover up the fraud, Lan and other SCB bankers were accused of giving state officials $5.2m, the largest bribe recorded in Vietnam. The money was handed over in Styrofoam boxes, Do Thi Nhan, a former chief banking inspector at the State Bank of Vietnam, said during the trial. Nhan said that after realising the boxes contained money, she refused the boxes but that Lan declined to take them back, state media reported.</p><p>Since 2021, thousands of people have been indicted for corruption in the country, in what analysts have described as the most comprehensive anti-corruption effort in the history of the Communist party of Vietnam.</p><p>Last month, the Vietnamese government <a href="https://www.theguardian.com/world/2024/mar/21/vietnam-president-vo-van-thuong-resignation" data-link-name="in body link">announced the resignation of its second president in as many years, Vo Van Thuong</a>, over alleged “violations and flaws” that had “negatively affected public perception, as well as the reputation of the party and the state”. He had been in power for just over a year after his predecessor, <a href="https://www.theguardian.com/world/2023/jan/17/vietnam-president-nguyen-xuan-phuc-quits" data-link-name="in body link">Nguyen Xuan Phuc, was forced out</a> because of corruption scandals involving officials under his control.</p><p>Evaluating public sentiment in Vietnam, a one-party state, is challenging. However, social media comments suggested many were shocked by the scale of the scandal, said Giang. While some welcomed the state crackdown, others questions how corruption on such a huge scale could go unchecked for so long.</p><p>“[The case] might indirectly signal that the state hasn’t really been doing well in terms of managing the system in terms of the increasingly complex market economy and also the state is incapable of controlling their own public officials,” said Giang.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anyone got a contact at OpenAI. They have a spider problem (670 pts)]]></title>
            <link>https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html</link>
            <guid>40001971</guid>
            <pubDate>Thu, 11 Apr 2024 13:34:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html">https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html</a>, See on <a href="https://news.ycombinator.com/item?id=40001971">Hacker News</a></p>
Couldn't get https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>