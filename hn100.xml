<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 02 Aug 2025 18:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why Exercise Is a Miracle Drug (187 pts)]]></title>
            <link>https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise</link>
            <guid>44768704</guid>
            <pubDate>Sat, 02 Aug 2025 16:02:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise">https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise</a>, See on <a href="https://news.ycombinator.com/item?id=44768704">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Welcome back to The Sunday Morning Post, this newsletter’s weekly rundown of the most interesting and important stuff I’m seeing in science, technology, economics, and beyond. Comments are open. Leave tips, papers, studies, tweets, posts, questions, and graphs in the comments, if you think they’ll serve for future editions.</p><p><span>Euan Ashley has </span><a href="https://open.spotify.com/episode/4llcv2QIXnxKeMzaqRZ1dS" rel="">claimed</a><span> that exercise is the “single most potent medical invention” ever—more broadly effective than any medicine discovered in the natural world or devised in a laboratory. In 2025, this is the sort of rah-rah sentiment about working out that one might associate with a Make America Healthy Again ambassador rather than, say, the chair of medicine at Stanford University. So, what makes Ashley’s claim significant is that he </span><em>is</em><span> the chair of medicine at Stanford University.</span></p><p><span>Last year, Ashley and a large team of scientists conducted an elaborate experiment on the effects of exercise on the mammalian body. In one test, Ashley put rats on tiny treadmills, worked them out for weeks, and cut into them to investigate how their organs and vessels responded to the workout compared to a control group of more sedentary rodents.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-168158155" href="https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise#footnote-1-168158155" target="_self" rel="">1</a></span><span> The results were spectacular. Exercise transformed just about every tissue and molecular system that Ashley and his co-authors studied—not just the muscles and heart, but also the liver, adrenal glands, fat, and immune system. </span></p><p>When I asked Ashley if it was possible to design a drug that mimicked the observed effects of exercise, he was emphatic that, no, this was not possible. The benefits of exercise seem too broad for any one therapy to mimic. To a best approximation, aerobic fitness and weight-training seem to increase our metabolism, improve mitochondrial function, fortify our immune system, reduce inflammation, improve tissue-specific adaptations, and protect against disease.</p><p><span>The latest entry in the Exercise Is Magic file comes from </span><a href="https://www.nejm.org/do/10.1056/NEJMdo008039/full/" rel="">the New England Journal of Medicine</a><span>. In a recent study, 900 cancer patients who had undergone surgery on their advanced colon cancer were randomly assigned to two groups. One group got a “structured exercise program.” They went to behavioral support sessions and attended supervised exercise classes every few weeks for several years. The other group received only basic information about diet and health.</span></p><p>Compared to the control group, the exercise group saw “significantly” more years without cancer, a 7 percentage point increase in the overall survival rate after 8 years, and a dramatic reduction in new primary cancers. Exercise, it seems, doesn’t just prevent disease; it can also save your life after you get sick.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!t4il!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!t4il!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 424w, https://substackcdn.com/image/fetch/$s_!t4il!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 848w, https://substackcdn.com/image/fetch/$s_!t4il!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!t4il!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!t4il!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg" width="1282" height="1254" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1254,&quot;width&quot;:1282,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:128015,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.derekthompson.org/i/168158155?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!t4il!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 424w, https://substackcdn.com/image/fetch/$s_!t4il!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 848w, https://substackcdn.com/image/fetch/$s_!t4il!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!t4il!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa837333-1baa-463a-9b82-2a0422978694_1282x1254.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The author Daniel Lieberman </span><a href="https://www.amazon.com/Exercised-Something-Evolved-Healthy-Rewarding/dp/052543478X/ref=asc_df_052543478X?mcid=1734c528e04d3fb5b8f542ded7df910a&amp;hvocijid=17701023827466789008-052543478X-&amp;hvexpln=73&amp;tag=hyprod-20&amp;linkCode=df0&amp;hvadid=721245378154&amp;hvpos=&amp;hvnetw=g&amp;hvrand=17701023827466789008&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9061285&amp;hvtargid=pla-2281435177818&amp;psc=1" rel="">has put it well</a><span>: Exercise is healthy and rewarding even though it’s something we never evolved to do. To adapt to the physical ease of the modern world, people invented a variety of weight-resistant devices and bodily movements that allow today’s population to simulate the arduous tasks that were once necessary to make it through a life, and this strange pantomime of physical stress seems to do more for us at a molecular level than any therapy or intervention in the history of medicine.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-168158155" href="https://www.derekthompson.org/p/the-sunday-morning-post-why-exercise#footnote-2-168158155" target="_self" rel="">2</a></span><span> </span></p><p>Soon after overseeing cuts to more than 80 percent of programs at the United States Agency for International Development, Secretary of State Marco Rubio said in remarks in July that USAID “has little to show since the end of the Cold War.” In the field of global health, this analysis may be off by about 100 million.</p><p><span>In June, The Lancet </span><a href="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(25)01186-9/fulltext" rel="">published</a><span> an evaluation of 20 years of USAID funding by a team of global researchers from Brazil, Spain, Mozambique, and the United States. They calculated that US global health and development spending has spared about 90 million deaths in low-income countries, including:</span></p><ul><li><p>25 million lives saved from HIV/AIDS</p></li><li><p>11 million from diarrheal disease</p></li><li><p>9 million from lower respiratory infections</p></li><li><p>9 million from "neglected" tropical diseases, such as dengue and river blindness</p></li><li><p>8 million from malaria</p></li><li><p>5 million from tuberculosis</p></li><li><p>2 million from nutritional deficiencies</p></li></ul><p>All this was achieved with a program that accounts for about 0.8 percent of the federal budget and about 1/400th of America’s total national spending. This is a staggering return on moral investment.</p><p>When I think about the case for global health spending, I think about the first days of my life. Nobody chooses how they come into the world. I achieved nothing to earn my birth in a hospital in Washington, D.C., as opposed to a clinic in Maputo or Kinshasa, or to deserve the automatic guarantee of American citizenship upon my first breath of air. That I was born in the capital of the world’s richest country is one of the greatest strokes of luck in my life—a pure accident of timing and gametes. There is no way to pay back this good fortune, and wallowing in guilt over it would do nothing, either. The quiet miracle of charity and global aid is that the uneven distribution of global wealth creates an asymmetry by which relatively trivial amounts of money from the rich can prevent immense suffering and death among the poor. Interventions as simple as bed nets, antiretroviral therapies for HIV/AIDS, and the distribution of commonplace vaccines and therapies in impoverished rural areas can save an astonishing number of lives, while costing a rich country an amount of money that makes practically no difference to any their citizen’s day-to-day. </p><p>Americans are blessed to be in possession of a kind of sorcerer’s stone—a bit of policy alchemy that can transform one-four hundredth of our spending into 100 million saved lives, in less than half a century. I think we should use it.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art of Multiprocessor Programming 2nd Edition Book Club (159 pts)]]></title>
            <link>https://eatonphil.com/2025-art-of-multiprocessor-programming.html</link>
            <guid>44767555</guid>
            <pubDate>Sat, 02 Aug 2025 13:43:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eatonphil.com/2025-art-of-multiprocessor-programming.html">https://eatonphil.com/2025-art-of-multiprocessor-programming.html</a>, See on <a href="https://news.ycombinator.com/item?id=44767555">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

<p>
  Part of the <a href="https://eatonphil.com/bookclub.html">Software Internals Email Book Club</a>.
</p>

<p>
  The next book we'll read is The Art of Multiprocessor Programming
  2nd Edition (ISBN 9780124159501) from 2020 by Herlihy, Shavit,
  Luchangco, and Spear. A free PDF comes up for me on a Google search
  of this book but it is of the 1st Edition from 2008. Make sure you
  grab the 2nd Edition from 2020.
</p>


<table>
  <thead>
    <tr>
      <td>Date</td>
      <td>Discussion starter</td>
      <td>Chapter</td>
      <td>Title</td>
    </tr>
  </thead>
  <tbody>
    <tr><td>August 16th</td><td>Phil</td><td>1</td><td>Introduction</td></tr>
    <tr><td>August 23rd</td><td>TBD</td><td>2</td><td>Mutual exclusion</td></tr>
    <tr><td>August 30th</td><td>TBD</td><td>3</td><td>Concurrent objects</td></tr>
    <tr><td>September 6th</td><td>TBD</td><td>4</td><td>Foundations of shared memory</td></tr>
    <tr><td>September 13th</td><td>TBD</td><td>5</td><td>The relative power of primitive synchronization operations</td></tr>
    <tr><td>September 20th</td><td>TBD</td><td>6</td><td>Universality of consensus</td></tr>
    <tr><td>September 27th</td><td>TBD</td><td>7</td><td>Spin locks and contention</td></tr>
    <tr><td>October 4th</td><td>TBD</td><td>8</td><td>Monitors and blocking synchronization</td></tr>
    <tr><td>October 11th</td><td>TBD</td><td>9</td><td>Linked lists: The role of locking</td></tr>
    <tr><td>October 18th</td><td>TBD</td><td>10</td><td>Queues, memory management, and the ABA problem</td></tr>
    <tr><td>October 25th</td><td>TBD</td><td>11</td><td>Stacks and elimination</td></tr>
    <tr><td>November 1st</td><td>TBD</td><td>12</td><td>Counting, sorting, and distributed coordination</td></tr>
    <tr><td>November 8th</td><td>TBD</td><td>13</td><td>Concurrent hashing and natural parallelism</td></tr>
    <tr><td>November 15th</td><td>TBD</td><td>14</td><td>Skiplists and balanced search</td></tr>
    <tr><td>November 22th</td><td>TBD</td><td>15</td><td>Priority queues</td></tr>
    <tr><td>November 29th</td><td>TBD</td><td>16</td><td>Scheduling and work distribution</td></tr>
    <tr><td>December 6th</td><td>TBD</td><td>17</td><td>Data parallelism</td></tr>
    <tr><td>December 13th</td><td>TBD</td><td>18</td><td>Barriers</td></tr>
  </tbody>
</table>

<p>
  All discussion is via a Google Group. You probably need a Google
  account. Your email will be public if you post but otherwise it will
  not be visible to anyone.
</p>

<p>
  There will be no Zoom or Google Hangout, it will purely be over text
  email.
</p>

<p>
  You should read the chapter before the date it
  is listed.
</p>

<h3>Discussion starter</h3>

<p>
  Each weekend, one person will send out an email to start
  discussion. It can be as short as a paragraph or two just to get
  discussion going. Anyone else can chime in afterward.
</p>

<p>
  It's most fun if this discussion starter doesn't summarize the
  chapter but tells a bit about themselves, their background, and what
  resonated or was confusing in the chapter, or how it tied back to
  something they experienced in the real-world.
</p>

<h3>Sign up</h3>

<p>
  Fill out this <a href="https://forms.gle/uUVYjvvy2HUMc4d78">form</a>.
</p>





	<div>
	  <h4>Feedback</h4>
	  <p>
	    As always,
	    please <a href="mailto:phil@eatonphil.com">email</a>
	    or <a href="https://twitter.com/eatonphil">tweet me</a>
	    with questions, corrections, or ideas!
	  </p>
	</div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[6 Weeks of Claude Code (163 pts)]]></title>
            <link>https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/</link>
            <guid>44767003</guid>
            <pubDate>Sat, 02 Aug 2025 12:20:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/">https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/</a>, See on <a href="https://news.ycombinator.com/item?id=44767003">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
    

    
    <p><time>Jul 30, 2025</time>
      
      -
      <span>
           
        <span>Orta Therox</span>
         
      </span>
    </p>
    
  </header>

  
  
  

<section>
  <p>It is wild to think that it has been only a handful of weeks.</p>
<p>Claude Code has considerably changed my relationship to writing and maintaining code at scale. I still write code at the same level of quality, but I feel like I have a new freedom of expression which is hard to fully articulate.</p>
<p>Claude Code has decoupled myself from writing every line of code, I still consider myself fully responsible for everything I ship to Puzzmo, but the ability to instantly create a whole scene instead of going line by line, word by word is incredibly powerful.</p>
<p>I believe with Claude Code, we are at the <em>“introduction of photography”</em> period of programming. Painting by hand just doesn’t have the same appeal anymore when a single concept can just appear and you shape it into the thing you want with your code review and editing skills.</p>
<p>If this feels like an intimidating line-of-thought then welcome to the mid-2020s, nothing is stable anymore and change is the only constant. Sorry. I didn’t make it so nearly all of culture’s changes are <em>bad</em>, and I think that LLMs are already doing social damage and will do much worse in the future - but this genie is fully out of the bottle and it is substantially going to change what we think of as programming.</p>
<h2 id="a-retrospective-on-the-last-6-weeks">A Retrospective on the last 6 Weeks<a href="#a-retrospective-on-the-last-6-weeks" arialabel="Heading anchor">#</a></h2>
<p>This article builds on <a href="https://blog.puzzmo.com/posts/2025/06/07/orta-on-claude/">“On Coding with Claude”</a> which I wrote after using Claude for a week. If you think that I am AI-pilled, you can get my nuanced take on LLMs at <a href="https://blog.puzzmo.com/posts/2025/06/07/orta-on-claude/#before-we-start">the start</a> of that post.</p>
<p>That said, this is transformative and I want to try give you some perspective from the last 6 weeks of activity in the Puzzmo engineering space to try and show you what I’ve been seeing.</p>
<h3 id="maintenance-is-significantly-cheaper">Maintenance is Significantly Cheaper<a href="#maintenance-is-significantly-cheaper" arialabel="Heading anchor">#</a></h3>
<p>I have been on many projects with people which have taken weeks full-time to perform some sort of mundane task: “converting this JS codebase to TypeScript”, “Update to Swift X”, “Switch to a monorepo” they’re the kind of things which are delicate migrations which require a gazillion rebases.</p>
<p>Here is a list of things which I have completed, <strong>solo</strong>, since getting access to Claude Code:</p>
<ul>
<li>Converting hundreds of React Native components to just React</li>
<li>Replaced 3 non-trivial RedwoodJS systems with home-grown or mature, supported replacements</li>
<li>Built complex REPLs for multiple internal and external projects</li>
<li>Switched almost every db model to have a consistent ‘flags’ system for booleans</li>
<li>Converted from Jest to Vitest</li>
<li>Created our front-end testing strategies for React</li>
<li>Moved many things defined in code to run via the CMS</li>
<li>Made significant headway on server-side rendering</li>
<li>Re-wrote the iOS app’s launch system due to deprecations</li>
<li>Built a suite of LLM created (and framed as such, hen hand annotated) documentation for systems like leaderboards, dailies etc</li>
<li>Converted a significant amount of our design system primitives to use base-ui</li>
<li>Migrated significant code from inline styles to stylex</li>
<li>Converted all animations in puzzmo.com to use the same techniques as games</li>
<li>Fixed multiple bugs which have been around since the start of Puzzmo</li>
<li>Updated all Vite integrations</li>
<li>Migrate all Puzzmo production projects to node 22</li>
<li>Convert the games repo to a real monorepo</li>
<li>Built iPad support for the Puzzmo app</li>
</ul>
<p>None of these projects are the “actual work” which I need to do on a day to day basis as the ‘bizdev’ guy on Puzzmo for this year. These are <em>literally side-projects</em> which I did <em>on my own</em> while working on something else.</p>
<p>For clarity in the back because this is shocking to me, while I was still working on the existing roadmap I had prior to Claude Code over the last 6 weeks, I accomplished all of these things on my own. Mostly in the background (and then with a polish pass day for some of the larger ones). I didn’t go from working ~10 hour days to working ~16 hours or anything like that either.</p>
<p>This was years of <em>“tech debt” / “tech innovation”</em> backlog for me! Done in just over a month and a half.</p>
<p>If you understand what you are doing, the capacity for building and handling the breadth of tasks which typically live within the remit of “technical debt” <em>do not need to be treated as debt</em> and you can just do it as you are working on other things.</p>
<p><em>‘carving out some time on the schedule’</em> is now so incredibly cheap that getting started and making a serious dint is <em>something you can prime before going into a meeting</em>, then deciding if you thought it was the right thing after. Mind-blowing.</p>
<h3 id="write-first-decide-later">Write First, Decide Later<a href="#write-first-decide-later" arialabel="Heading anchor">#</a></h3>
<p>A habit I have been trying to form is to give an idea a shot before I fully shoot it down. For example, since day 1 on Puzzmo I had been waiting on figuring out a testing strategy for the front-end because I wanted to be able to hire someone to fully own “puzzmo.com” and a part of that is figuring out how to not do as many regressions as we get.</p>
<p>Figuring out a testing strategy for the front-end isn’t pretty, and I have seen a lot of really bad test suites which over-test and become brittle things that engineers don’t like to work with. The mix of networking, react, the scope of contexts, the dom, flakiness in tooling just leads to answers where you are looking for the least bad solution which you’ve used yourself and feel comfortable maintaining.</p>
<p>I wondered if I needed to wait for someone else, so instead of just “adding a test suite” - I opted to have Claude Code write tests for every pull request I made to the front end over the course of two weeks.</p>
<p>Then, after seeing the tests, I deleted them. It added an extra 5m to my process, but gave me an insight each time into different ways in which other projects deal with the problem. After weeks of this, I was ready to start looking at that problem systemically.</p>
<p>The idea of writing tests for every pull request and then deleting it would just be so much time, there would be no way I’d be OK with doing.</p>
<p>Or a recent example from slack where I just vibed for half a day in the background on trying to make an abstraction for CRUD resources in our CMS tools:</p>
</section>


<p><a href="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/slack-screenshot-crud.png"><img src="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/slack-screenshot-crud.png" alt="A slack statement around making CRUD apps with relay + graphql"></a>
</p>


<section>
<p>Did it work? Nope, was it worth an exploration - sure.</p>
<h3 id="living-the-two-clones-lifestyle">Living the Two Clones lifestyle<a href="#living-the-two-clones-lifestyle" arialabel="Heading anchor">#</a></h3>
<p>Anthropic have information about how to use <a href="https://docs.anthropic.com/en/docs/claude-code/common-workflows#run-parallel-claude-code-sessions-with-git-worktrees">worktrees</a> - I would like to argue for a simpler approach. Two clones, different <a href="https://code.visualstudio.com/docs/configure/profiles">VS Code profiles</a>.</p>
</section>


<p><a href="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/dual-clone.png"><img src="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/dual-clone.png" alt="An image of a Missing Link puzzle with the cells shuffled."></a>
</p>


<section>
<p>This means you can work in each independently and still visually recognize the differences in you workspaces by having a different theme:</p>
<p><img src="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/clone-2-settings.png" alt="alt text"></p>
<p>My best argument is simply that each clone represents a single pull request that you can work on at a time. If you are writing pull requests and collaborating with others then that is still pretty important. I made it so that our dev servers close any processes using the ports you want and its trivial to jump between the two clones as Claude Code is working stuff out before you are looking build.</p>
<h3 id="game-design-collaboration">Game design collaboration<a href="#game-design-collaboration" arialabel="Heading anchor">#</a></h3>
<p>Since Puzzmo was created this was the process of creating a game:</p>
<ul>
<li>We create some prototypes using all sorts of technologies</li>
<li>Collectively we work through the prototypes with feedback</li>
<li>We decide if this game is worth shipping</li>
<li>The game team re-write the from scratch in our tech stack, and with puzzmo’s system integrations</li>
</ul>
<p>The process of this is weeks before any production code is written, if at all. At our current throughput, we roughly release a game a quarter at the level of polish we want to achieve.</p>
<p>In a post-Claude Code world, this model can be simplified greatly and it is a space we are exploring. I created a new Puzzmo monorepo (that’s three now, “app”, “games” and this new one “prototypes”) which emulates the infrastructure of the games repo but has significantly different expectations on the type of code being shipped. With this repo, a game designer can go from an idea to something running on puzzmo.com for admins in a couple of hours, you write the code, then go into our admin CMS and click a few buttons and it’s done.</p>
<p>To go from “this is good for the team” to “we should make this public” takes a bit of hands-on work from me and Saman, but it’s a different ball park of effort compared to our current production pipeline.</p>
<p>We released <a href="https://blog.puzzmo.com/posts/2025/07/04/missing-link/">Missing Link</a> using this technique, which seems to be a hit. This… actually is a bit of a problem for us. I am happy for us to have a game designer’s code running on Puzzmo for a time-gated experiment, but I am not OK with this turning into Puzzmo canon with the rest of the games.</p>
<p>The flexibility which allows a game designer to make a prototype is the part that makes it un-suitable for writing long-term production code. This leaves us with a few options:</p>
<ul>
<li>Finish the experiment and stop having the game on the site</li>
<li>Re-write the game as production code</li>
<li>Declare some games as not quite having every Puzzmo integration feature</li>
<li>Explore making it more possible to write ‘production worthy’ code in prototypes</li>
<li>Extend the experiment to give ourselves time to figure another option</li>
</ul>
<p>All of these have trade-offs, and it isn’t obvious what the right idea is. The problem is novel because prior to Claude Code it wasn’t worth the effort of integrating prototype code with Puzzmo’s systems — now it’s trivial and accomplishable by anyone on the team. We can really deliver on the idea of ’experimental’ games that we launched with, which means we have to be much more thoughtful about the risk of launching too many games that people want us to keep around.</p>
<h3 id="taking-a-shot-during-triage">Taking a Shot During Triage<a href="#taking-a-shot-during-triage" arialabel="Heading anchor">#</a></h3>
<p>One thing I have been experimenting with during our weekly triage of all raised GitHub issues is asking the Claude Code GitHub action to take a stab at a pull request while we are talking about what we think as a group of engineers:</p>
<p><img src="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/claude-code-pr.png" alt="A screenshot of a GitHub issues"></p>
<p>Or one where I was the one providing enough context myself in the issue:</p>
<p><img src="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/nuanced-issue.png" alt="Another screenshot of a github issue"></p>
<p>As I am the one responsible for getting that Pull Request into production, that’s the first few steps ready and for smaller things I’ve found it to be a solid one-shot now that the repo is very well set up.</p>
<h2 id="who-has-been-successful-using-it-internally">Who has been successful using it internally?<a href="#who-has-been-successful-using-it-internally" arialabel="Heading anchor">#</a></h2>
<p>I think it’s worth noting here that we offered Claude Code to everyone on the team from the moment it because obvious how powerful of a tool it was for me personally.</p>
<p>I would say from our team, the sort of people who have used and found value the most are people with both product, technical skills and agency to feel like they can try things.</p>
<p>One of them said that Claude Code freed them from the anxiety of the first step in programming constantly.</p>
<p>Justin Searls did an interesting write-up where he described an idea of <a href="https://justin.searls.co/posts/full-breadth-developers/">full-breadth developers</a> where-in he argues that:</p>
<blockquote>
<p>Up until a few months ago, the best developers played the violin. Today, they play the orchestra.</p>
</blockquote>
<p>Which I think is correct, within the Puzzmo team the people whose skill-sets are being self-driven, run their own verticals and feel like they have the freedom to explore and push those boundaries are doing really cool work. It bursts out of any explicit job role boundaries and it becomes a pleasure to collab at a larger/faster scale on ideas than before.</p>
<p>So I will double-down on saying that everything <a href="https://justin.searls.co/posts/full-breadth-developers/">in Justin’s post</a> echoes what is happening inside the Puzzmo engineering team and his post is really worth musing over.</p>
<h3 id="what-do-i-think-makes-it-successful-in-our-codebases">What Do I Think Makes It Successful in our Codebases<a href="#what-do-i-think-makes-it-successful-in-our-codebases" arialabel="Heading anchor">#</a></h3>
<ol>
<li>
<p>We use monorepos. I was lucky to have spent the time <a href="https://.puzzmo.com/posts/2025/01/22/turborepo/">a year ago</a> to take every project and move it into a two main environments. This was originally done to reflect the working processes of the engineering teams. My goal was to make it possible to go from db schema change to front-end components in a single pull request.</p>
<p>A monorepo is perfect for working with an LLM, because it can read the file which represents our schema, it can read the sdl files defining the public GraphQL API, read the per-screen requests and figure out what you’re trying to do. Having a single place with so much context means that <em>I</em> as user of Claude Code do not need to tell it that sort of stuff and a vague message like <em>“Add a xyz field to the user model in the db and make it show in this screen”</em> is something that Claude Code can do.</p>
</li>
<li>
<p>My tech choices were made a decade ago. This video of a conference talk I <a href="https://www.youtube.com/watch?v=1Z3loALSVQM">gave from 2018</a> is still the way I introduce people to the Puzzmo codebase and the mentality behind these tech choices. React, Relay, GraphQL, TypeScript and (now StyleX) are boring and <em>very explicit</em> technologies. There are compilation steps in all of these systems which means everything has to be available locally and correct to run, this makes it a bit of a curve to learn but often when you have got it right - you know you have got it right. For our admin tools, its even more boring/mature, I’m still using <a href="https://getbootstrap.com/">Bootstrap</a>!</p>
<p>For an LLM, these technologies are very well baked into its training set and Claude Code knows to do things like “run the Relay compiler” (when I saw Claude Code first do that, I knew I was in for a wild ride) which gives it incremental ways to be validating the changes it has done are working.</p>
</li>
<li>
<p>This isn’t novel work. Most of the stuff we’re doing on a day to day basis is pretty normal down-to-earth CRUD style apps.</p>
</li>
<li>
<p>These codebases aren’t that big, nor that old. Nothing is older than 2021 and while I keep things up-to-date, I try to have a long-tail of support / backwards compatibility.</p>
</li>
<li>
<p>Our business is literally the test suite / benchmark for these models. For example, on the 28th of June, two days before posting this <a href="https://z.ai/blog/glm-4.5">GLM-4.5</a> came out. Offering a way to run an ~80% as good as <a href="https://simonwillison.net/2025/Jul/29/space-invaders/">Claude Code on your computer</a> locally. How do they measure that 80%? Here is <a href="https://huggingface.co/datasets/zai-org/CC-Bench-trajectories#test-dataset">the table from</a> their benchmarks of what they use:</p>
<p><img src="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/dataset-GLM-45.png" alt="alt text"></p>
<p>Puzzmo’s day-to-day work is represented in ~(39/52)% of their testing infrastructure!</p>
</li>
</ol>
<h3 id="quantifying-the-change-is-hard">Quantifying the Change is Hard<a href="#quantifying-the-change-is-hard" arialabel="Heading anchor">#</a></h3>
<p>I thought I would see a pretty drastic change in terms of Pull Requests, Commits and Line of Code merged in the last 6 weeks. I don’t think that holds water though:</p>
</section>


<p><a href="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/diagrams.png"><img src="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/diagrams.png" alt="An image of a Missing Link puzzle with the cells shuffled."></a>
</p>


<section>
<p>This is a 3 month chart, with a month of post-Claude Code. I just asked it to make a script to generate a CSV from looking at repos on my harddrive.</p>
<p>That said, I think anyone internally would feel like the pace of change inside Puzzmo has most definitely increased (at least in the areas I contribute) but those numbers haven’t <em>really</em> changed in reality.</p>
<p>There was a <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">recent paper</a> (which is from the pre-Claude Code days) which says that developers with AI over-estimate its impact and maybe I am.</p>
<p>Doesn’t <em>feel</em> it though. Did you see that list at the top? I feel like I’m constantly trashing my usual estimation timings to the point where it’s hard for me to gauge how long a task will take.</p>
<h3 id="you-dont-have-to-obsess-over-llm-trends">You Don’t Have To Obsess Over LLM Trends<a href="#you-dont-have-to-obsess-over-llm-trends" arialabel="Heading anchor">#</a></h3>
<p>While intoxicating at first, settling in to Claude Code usage just becomes mundane normal tool use after a while. You do not need to spend your time worrying about Sonnet or Opus, or grabbing every Claude Code competitor like Gemini CLI, Qwen Code or some other model that is cool. I have not used anything but Claude Code with whatever it does on the $100 a month account and I am doing very fine. I’ve heard good things about asking Gemini when Claude Code is stuck, but I’ve found that if Claude Code is stuck, I have not been doing a good job framing our work and a re-examination is worth the time.</p>
<p>I’ve never set up an MCP server, I’ve found doing voice-chat super awkward and not used it and I don’t follow the “blue tick-y” people on Twitter who have some “*.ai” bio. Those folks can do their thing, but I’m very happy not engaging.</p>
<p>There will be a future when it makes sense to think about looking at other ecosystem tools, but for me the difference between pre-Claude Code and post-Claude Code is so substantial that any increments between it and others (which will be better in some ways, worse in others) is not worth the hassle for such a small incremental win.</p>
<p>Running something locally is tempting, but the networked versions like Claude Code are always gonna be a step ahead, and as long as I do not need to think about usage limits (<a href="https://techcrunch.com/2025/07/17/anthropic-tightens-usage-limits-for-claude-code-without-telling-users/">yes, I know</a>) then we’re in a good spot.</p>
<h3 id="you-can-let-claude-rest">You Can Let Claude Rest<a href="#you-can-let-claude-rest" arialabel="Heading anchor">#</a></h3>
<p>Like with a mobile phone, you can become consumed by the notion that because Claude Code can run at all times, you should make it run at all times. Effectively doom-scrolling in your terminal (or phone?!) instead.</p>
<p>It’s worth remembering that any tool can be used at any time, but that you are the one driving it and your energy / capacity to make informed decisions is not infinite.</p>
<h3 id="i-run-via-claude-yolo">I run via <code>claude yolo</code><a href="#i-run-via-claude-yolo" arialabel="Heading anchor">#</a></h3>
<p>I have been trying to make a list of everything I’m OK with:</p>
<details>
    <summary>
        .claude/settings.json
    </summary>
    <div>
        
    <pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "permissions"</span><span>: {</span></span>
<span><span>    "allow"</span><span>: [</span></span>
<span><span>      "Bash(grep:*)"</span><span>,</span></span>
<span><span>      "Bash(yarn run *)"</span><span>,</span></span>
<span><span>      "Bash(yarn lint:*)"</span><span>,</span></span>
<span><span>      "Bash(yarn workspace:*)"</span><span>,</span></span>
<span><span>      "Bash(find:*)"</span><span>,</span></span>
<span><span>      "Bash(mkdir:*)"</span><span>,</span></span>
<span><span>      "Bash(rg:*)"</span><span>,</span></span>
<span><span>      "Bash(ls:*)"</span><span>,</span></span>
<span><span>      "mcp__ide__getDiagnostics"</span><span>,</span></span>
<span><span>      "Bash(awk:*)"</span><span>,</span></span>
<span><span>      "Bash(yarn build)"</span><span>,</span></span>
<span><span>      "Bash(yarn why:*)"</span><span>,</span></span>
<span><span>      "Bash(yarn info:*)"</span><span>,</span></span>
<span><span>      "Edit(*)"</span></span>
<span><span>    ],</span></span>
<span><span>    "deny"</span><span>: [</span><span>"Bash(npx prisma migrate dev:*)"</span><span>, </span><span>"Bash(git checkout:*)"</span><span>, </span><span>"Bash(git add:*)"</span><span>],</span></span>
<span><span>    "defaultMode"</span><span>: </span><span>"acceptEdits"</span></span>
<span><span>  }</span></span>
<span><span>}</span></span>
<span></span></code></pre>

    </div>
    
</details>

<p>But it’s still not enough to constantly feel like I’m being asked things that I don’t need to confirm. So I run <code>claude --dangerously-skip-permissions</code> aka <code>claude yolo</code>. The worst things that have happened to me have been having my dev db wiped during a bad Prisma migration, and that Claude Code decided it should make a commit, and then a Pull Request.</p>
<p>My theory for the latter two is that if a human is expected to read it, a human should have wrote it. I’m OK with something like:</p>
<pre tabindex="0"><code><span><span>[Authored description, or one liner]</span></span>
<span><span></span></span>
<span><span>---</span></span>
<span><span></span></span>
<span><span>[Codegen'd PR template]</span></span>
<span><span></span></span></code></pre>
<p>But I’m not slopping about in production.</p>
<h3 id="parallel-construction-for-juniors">Parallel Construction for Juniors<a href="#parallel-construction-for-juniors" arialabel="Heading anchor">#</a></h3>
<p>One thing I’ve talked with folks earlier in their careers who want to still be doing a lot of the grunt work themselves is to consider writing their work, then comparing their results to the same requested by Claude Code.</p>
<p><a href="https://en.wikipedia.org/wiki/Parallel_construction">Parallel construction</a> is a way to have your cake and eat it. You’re still learning and growing, but your results can be honed by seeing how the aggregate of training data in Claude Code has also decided to do it. Claude Code likely has a deeper ecosystem understanding, and may read more source code in the current codebase as well as knowing abstractions you’ve still not learned.</p>
<p>IMO treating it as a competitor which you can learn from is a much healthier alternative to either giving up and just accepting you don’t need to know stuff anymore, or putting your head in the sand and assuming somehow this change won’t affect you.</p>
<h3 id="you-can-just-do-it-for-side-projects">“You can just do it” for Side Projects<a href="#you-can-just-do-it-for-side-projects" arialabel="Heading anchor">#</a></h3>
<p>Throughout my entire programming career, like all humans I have been largely constrained in my capacity for side-projects and one-offs by the fact I still want to have a life. I choose to devote myself to contributing to large-scale open source to make me feel good about the amount of time I commit to the craft.</p>
<p>In concrete terms, that means spending time on projects like CocoaPods, Danger, Jest, GraphQL were things I did instead of making fun projects to explore a technology, or to fix a smaller nit.</p>
<p>Now it’s different. I can just take a stab and decide if I like the result. In one hour of exploration with Claude Code, I feel like can do roughly a weekends worth of exploration.</p>
<h4 id="eg-inline-chats">E.g. Inline Chats<a href="#eg-inline-chats" arialabel="Heading anchor">#</a></h4>
<p>For example, this blog post. When I was musing about it, I thought, <em>‘it’d be nice to show the Claude Code conversations inline’</em> and then subsequently, <em>“wouldn’t it be fun to bring back Adium themes for it”</em>. So. I just got started.</p>

<div>
    
    <div>
        <p>So, I spec’d out a rough idea of what I was looking for. Described it pretty well up-front, took the dogs for an hour-long walk and came back to a reasonable approximation of the CLI which would have taken me a few hours to build by hand.</p>
<p>It wasn’t a lot of code, but it was a lot of research, how do I re-create the Adium theme HTML, how do you make sense of claude code’s message formatting, how do you handle keeping resources local for a preview.</p>

    </div>
    
    <div data-iframe-id="chat-to-adium-1">
                <svg width="48" height="48" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="m4.709 15.955 4.72-2.647.08-.23-.08-.128h-.229l-.79-.048-2.698-.073-2.339-.097-2.266-.122-.571-.121-.536-.705.055-.352.48-.321.686.06 1.52.103 2.278.158 1.652.097 2.449.255h.389l.055-.157-.134-.098-.103-.097-2.358-1.596-2.552-1.688-1.336-.972-.724-.491-.364-.462-.158-1.008.656-.722.881.06.225.061.893.686 1.908 1.476 2.491 1.833.365.304.145-.103.019-.073-.164-.274-1.355-2.446-1.446-2.49-.644-1.032-.17-.619a2.97 2.97 0 0 1 -.104-.729l.748-1.013.413-.134.996.134.42.364.62 1.414 1.002 2.229 1.555 3.03.456.898.243.832.091.255h.158v-.146l.128-1.706.237-2.095.23-2.695.08-.76.376-.91.747-.492.584.28.48.685-.067.444-.286 1.851-.559 2.903-.364 1.942h.212l.243-.242.985-1.306 1.652-2.064.73-.82.85-.904.547-.431h1.033l.76 1.129-.34 1.166-1.064 1.347-.881 1.142-1.264 1.7-.79 1.36.073.11.188-.02 2.856-.606 1.543-.28 1.841-.315.833.388.091.395-.328.807-1.969.486-2.309.462-3.439.813-.042.03.049.061 1.549.146.662.036h1.622l3.02.225.79.522.474.638-.079.485-1.215.62-1.64-.389-3.829-.91-1.312-.329h-.182v.11l1.093 1.068 2.006 1.81 2.509 2.33.127.578-.322.455-.34-.049-2.205-1.657-.851-.747-1.926-1.62h-.128v.17l.444.649 2.345 3.521.122 1.08-.17.353-.608.213-.668-.122-1.374-1.925-1.415-2.167-1.143-1.943-.14.08-.674 7.254-.316.37-.729.28-.607-.461-.322-.747.322-1.476.389-1.924.315-1.53.286-1.9.17-.632-.012-.042-.14.018-1.434 1.967-2.18 2.945-1.726 1.845-.414.164-.717-.37.067-.662.401-.589 2.388-3.036 1.44-1.882.93-1.086-.006-.158h-.055l-6.343 4.116-1.13.146-.487-.456.061-.746.231-.243 1.908-1.312z" fill="#d97757"></path>
                </svg>
                <p>Click to show chat with Claude</p>
            </div>
</div>




<p>With that working enough to be able to correctly mix the ideas, I gave it a polish pass.</p>

<div>
    
    <div>
        <p>I’ve polished and deployed enough npm modules (<a href="https://www.npmjs.com/~orta">174?!</a>), so again, totally within my skills to do this without thinking too hard. Instead I treated this project as a fun side gig while watching <a href="https://www.youtube.com/watch?v=0LRyOw1R_SE">Apex Legends</a>.</p>
<p>If you read the chat, you’ll see that I do spend some time figuring out how to filter some things, how to show messages in a particular way but this is systemic babysitting and code which I really don’t care about.</p>

    </div>
    
    <div data-iframe-id="chat-to-adium-2">
                <svg width="48" height="48" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="m4.709 15.955 4.72-2.647.08-.23-.08-.128h-.229l-.79-.048-2.698-.073-2.339-.097-2.266-.122-.571-.121-.536-.705.055-.352.48-.321.686.06 1.52.103 2.278.158 1.652.097 2.449.255h.389l.055-.157-.134-.098-.103-.097-2.358-1.596-2.552-1.688-1.336-.972-.724-.491-.364-.462-.158-1.008.656-.722.881.06.225.061.893.686 1.908 1.476 2.491 1.833.365.304.145-.103.019-.073-.164-.274-1.355-2.446-1.446-2.49-.644-1.032-.17-.619a2.97 2.97 0 0 1 -.104-.729l.748-1.013.413-.134.996.134.42.364.62 1.414 1.002 2.229 1.555 3.03.456.898.243.832.091.255h.158v-.146l.128-1.706.237-2.095.23-2.695.08-.76.376-.91.747-.492.584.28.48.685-.067.444-.286 1.851-.559 2.903-.364 1.942h.212l.243-.242.985-1.306 1.652-2.064.73-.82.85-.904.547-.431h1.033l.76 1.129-.34 1.166-1.064 1.347-.881 1.142-1.264 1.7-.79 1.36.073.11.188-.02 2.856-.606 1.543-.28 1.841-.315.833.388.091.395-.328.807-1.969.486-2.309.462-3.439.813-.042.03.049.061 1.549.146.662.036h1.622l3.02.225.79.522.474.638-.079.485-1.215.62-1.64-.389-3.829-.91-1.312-.329h-.182v.11l1.093 1.068 2.006 1.81 2.509 2.33.127.578-.322.455-.34-.049-2.205-1.657-.851-.747-1.926-1.62h-.128v.17l.444.649 2.345 3.521.122 1.08-.17.353-.608.213-.668-.122-1.374-1.925-1.415-2.167-1.143-1.943-.14.08-.674 7.254-.316.37-.729.28-.607-.461-.322-.747.322-1.476.389-1.924.315-1.53.286-1.9.17-.632-.012-.042-.14.018-1.434 1.967-2.18 2.945-1.726 1.845-.414.164-.717-.37.067-.662.401-.589 2.388-3.036 1.44-1.882.93-1.086-.006-.158h-.055l-6.343 4.116-1.13.146-.487-.456.061-.746.231-.243 1.908-1.312z" fill="#d97757"></path>
                </svg>
                <p>Click to show chat with Claude</p>
            </div>
</div>




<p>A feature like this is a <em>full weekend</em> project, easily about 10-12 hours to get right and feel shippable for me. Instead most of the work happened when I was away and then the polishing was sporadic. Maybe the whole thing took ~2 hours of my thinking time? <em>This is wild</em>.</p>
<p>If you want to see the rest of the conversations to get it to the point where I shipped this blog post, here they are:</p>
<ul>
<li>Chronologically continuing from the 2 above: <a href="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/claude-conversations/claude-conversation--Users-orta-dev-claude-code-to-adium-ebd791d4/conversation.html">3</a>, <a href="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/claude-conversations/claude-claude-code-to-adium--Users-orta-dev-claude-code-to-adium-0ec8943e/conversation.html">4</a>, <a href="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/claude-conversations/claude-claude-code-to-adium--Users-orta-dev-claude-code-to-adium-594e201d/conversation.html">5</a>, <a href="https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/claude-conversations/claude-conversation--Users-orta-dev-claude-code-to-adium-5f5c097b/conversation.html">6</a></li>
</ul>
<p>You can use it right now if you <a href="https://adium.im/">install Adium</a> then run <code>npx claude-code-to-adium</code> and it’ll take you through a wizard which ends with a self-contained subfolder full of html/css/images.</p>
<h3 id="some-examples-of-what-these-conversations-look-like">Some Examples of What These Conversations Look Like<a href="#some-examples-of-what-these-conversations-look-like" arialabel="Heading anchor">#</a></h3>
<p>I will try to cherry-pick some of the 147 conversations I’ve had over 19 separate repo/projects since starting. I’ll aim for breadth of goals and give my opinion to the side.</p>
<h4 id="making-a-delete-30-day-old-games-task">Making a “delete 30 day old games” Task<a href="#making-a-delete-30-day-old-games-task" arialabel="Heading anchor">#</a></h4>

<div>
    
    <div>
        <p>This is a chat where I have a general sense of what I want, but know that I don’t actually know the answer around postgres indexing and how it effects mass deletes.</p>
<p>So first I ask a general question where it is using my prisma definition file to determine what is currently set up in the db.</p>
<p>We iterate on a scripting improvement, and make it possible to test locally.</p>
<p>After trying locally, I give it a “kinda” and then ask for a more explicit technique.</p>
<p>With that set up I go through all the code, review it locally, fix up style make it work according to how I would write it.</p>

    </div>
    
    <div data-iframe-id="thirty-day-old">
                <svg width="48" height="48" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="m4.709 15.955 4.72-2.647.08-.23-.08-.128h-.229l-.79-.048-2.698-.073-2.339-.097-2.266-.122-.571-.121-.536-.705.055-.352.48-.321.686.06 1.52.103 2.278.158 1.652.097 2.449.255h.389l.055-.157-.134-.098-.103-.097-2.358-1.596-2.552-1.688-1.336-.972-.724-.491-.364-.462-.158-1.008.656-.722.881.06.225.061.893.686 1.908 1.476 2.491 1.833.365.304.145-.103.019-.073-.164-.274-1.355-2.446-1.446-2.49-.644-1.032-.17-.619a2.97 2.97 0 0 1 -.104-.729l.748-1.013.413-.134.996.134.42.364.62 1.414 1.002 2.229 1.555 3.03.456.898.243.832.091.255h.158v-.146l.128-1.706.237-2.095.23-2.695.08-.76.376-.91.747-.492.584.28.48.685-.067.444-.286 1.851-.559 2.903-.364 1.942h.212l.243-.242.985-1.306 1.652-2.064.73-.82.85-.904.547-.431h1.033l.76 1.129-.34 1.166-1.064 1.347-.881 1.142-1.264 1.7-.79 1.36.073.11.188-.02 2.856-.606 1.543-.28 1.841-.315.833.388.091.395-.328.807-1.969.486-2.309.462-3.439.813-.042.03.049.061 1.549.146.662.036h1.622l3.02.225.79.522.474.638-.079.485-1.215.62-1.64-.389-3.829-.91-1.312-.329h-.182v.11l1.093 1.068 2.006 1.81 2.509 2.33.127.578-.322.455-.34-.049-2.205-1.657-.851-.747-1.926-1.62h-.128v.17l.444.649 2.345 3.521.122 1.08-.17.353-.608.213-.668-.122-1.374-1.925-1.415-2.167-1.143-1.943-.14.08-.674 7.254-.316.37-.729.28-.607-.461-.322-.747.322-1.476.389-1.924.315-1.53.286-1.9.17-.632-.012-.042-.14.018-1.434 1.967-2.18 2.945-1.726 1.845-.414.164-.717-.37.067-.662.401-.589 2.388-3.036 1.44-1.882.93-1.086-.006-.158h-.055l-6.343 4.116-1.13.146-.487-.456.061-.746.231-.243 1.908-1.312z" fill="#d97757"></path>
                </svg>
                <p>Click to show chat with Claude</p>
            </div>
</div>




<p>You might note that it makes some guesses (“10-20%”" of our gameplays are anon users), then make bold promises from that guess:</p>
<blockquote>
<p><em>That’s an 80-85% reduction in index size!</em></p>
</blockquote>
<p>Which I doubt. However, the code was solid, it’s got significantly more logs than I would have written (useful for a daily task) and I feel like I understand what the index does. I went and added a bunch of glue comments <em>“this script works with the index in migration y, so any updates…”</em></p>
<h4 id="adding-barred-grid-support-to-a-crossword">Adding Barred Grid Support to a Crossword<a href="#adding-barred-grid-support-to-a-crossword" arialabel="Heading anchor">#</a></h4>

<div>
    
    <div>
        <p>I knew this was going to be nightmare PR, which you can <a href="https://github.com/puzzmo-com/xd-crossword-tools/pull/42">see here</a>.</p>
<p>I started by adding fixtures into the repo, and providing context via <a href="https://github.com/puzzmo-com/xd-crossword-tools/issues/31">an issue</a> which I had left ideas in. To get started, I framed this task as ’this is the long term goal, to get started we will do Y.’. Y here being an ASCII snapshot of the bars.</p>
<p>We started by building out a way to visualize the work, and Claude Code got very close. In the end, I took it’s work and finished the integration myself.</p>
<p>Once I had a way to visualize the solution, we could start looking at the main chunk of the work</p>

    </div>
    
    <div data-iframe-id="claude-conversation-xd-tools-barred-0">
                <svg width="48" height="48" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="m4.709 15.955 4.72-2.647.08-.23-.08-.128h-.229l-.79-.048-2.698-.073-2.339-.097-2.266-.122-.571-.121-.536-.705.055-.352.48-.321.686.06 1.52.103 2.278.158 1.652.097 2.449.255h.389l.055-.157-.134-.098-.103-.097-2.358-1.596-2.552-1.688-1.336-.972-.724-.491-.364-.462-.158-1.008.656-.722.881.06.225.061.893.686 1.908 1.476 2.491 1.833.365.304.145-.103.019-.073-.164-.274-1.355-2.446-1.446-2.49-.644-1.032-.17-.619a2.97 2.97 0 0 1 -.104-.729l.748-1.013.413-.134.996.134.42.364.62 1.414 1.002 2.229 1.555 3.03.456.898.243.832.091.255h.158v-.146l.128-1.706.237-2.095.23-2.695.08-.76.376-.91.747-.492.584.28.48.685-.067.444-.286 1.851-.559 2.903-.364 1.942h.212l.243-.242.985-1.306 1.652-2.064.73-.82.85-.904.547-.431h1.033l.76 1.129-.34 1.166-1.064 1.347-.881 1.142-1.264 1.7-.79 1.36.073.11.188-.02 2.856-.606 1.543-.28 1.841-.315.833.388.091.395-.328.807-1.969.486-2.309.462-3.439.813-.042.03.049.061 1.549.146.662.036h1.622l3.02.225.79.522.474.638-.079.485-1.215.62-1.64-.389-3.829-.91-1.312-.329h-.182v.11l1.093 1.068 2.006 1.81 2.509 2.33.127.578-.322.455-.34-.049-2.205-1.657-.851-.747-1.926-1.62h-.128v.17l.444.649 2.345 3.521.122 1.08-.17.353-.608.213-.668-.122-1.374-1.925-1.415-2.167-1.143-1.943-.14.08-.674 7.254-.316.37-.729.28-.607-.461-.322-.747.322-1.476.389-1.924.315-1.53.286-1.9.17-.632-.012-.042-.14.018-1.434 1.967-2.18 2.945-1.726 1.845-.414.164-.717-.37.067-.662.401-.589 2.388-3.036 1.44-1.882.93-1.086-.006-.158h-.055l-6.343 4.116-1.13.146-.487-.456.061-.746.231-.243 1.908-1.312z" fill="#d97757"></path>
                </svg>
                <p>Click to show chat with Claude</p>
            </div>
</div>




<p>I used tests based on ASCII snapshot to hardcode test using the explicit version of the bars in the imported file format jpz, then created a test which relied on the algorithm we were going to create. This meant I, and Claude, had a very explicit way to judge how the algorithm was working.</p>
<p>The import algorithm which existed for a jpz was too naive, and imported clues were wrong which meant we spent a long time trying to get the two snapshots to match. Claude kept cheating and hardcoding the answers! It took till I re-evaluated all of the clues (by making a separate test on the import for these clues) for a fresh re-examination of the algorithm to start getting somewhere.</p>
<h4 id="creating-a-repl-for-a-puzzle">Creating a REPL for a Puzzle<a href="#creating-a-repl-for-a-puzzle" arialabel="Heading anchor">#</a></h4>

<div>
    
    <div>
        <p>A quick, fully vibed prototype of a way to visually design a puzzle for the <a href="https://circuitsgame.com/">game Circuits</a>.</p>
<p>I give a screenshot, and try to describe how the game works and how I see a REPL could work. We iterate a bit, and I basically never write any code.</p>
<p>This is then passed off to others to experiment with and figure out their opinions on how to make a working dev tool for the game.</p>

    </div>
    
    <div data-iframe-id="claude-circuits-repl">
                <svg width="48" height="48" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="m4.709 15.955 4.72-2.647.08-.23-.08-.128h-.229l-.79-.048-2.698-.073-2.339-.097-2.266-.122-.571-.121-.536-.705.055-.352.48-.321.686.06 1.52.103 2.278.158 1.652.097 2.449.255h.389l.055-.157-.134-.098-.103-.097-2.358-1.596-2.552-1.688-1.336-.972-.724-.491-.364-.462-.158-1.008.656-.722.881.06.225.061.893.686 1.908 1.476 2.491 1.833.365.304.145-.103.019-.073-.164-.274-1.355-2.446-1.446-2.49-.644-1.032-.17-.619a2.97 2.97 0 0 1 -.104-.729l.748-1.013.413-.134.996.134.42.364.62 1.414 1.002 2.229 1.555 3.03.456.898.243.832.091.255h.158v-.146l.128-1.706.237-2.095.23-2.695.08-.76.376-.91.747-.492.584.28.48.685-.067.444-.286 1.851-.559 2.903-.364 1.942h.212l.243-.242.985-1.306 1.652-2.064.73-.82.85-.904.547-.431h1.033l.76 1.129-.34 1.166-1.064 1.347-.881 1.142-1.264 1.7-.79 1.36.073.11.188-.02 2.856-.606 1.543-.28 1.841-.315.833.388.091.395-.328.807-1.969.486-2.309.462-3.439.813-.042.03.049.061 1.549.146.662.036h1.622l3.02.225.79.522.474.638-.079.485-1.215.62-1.64-.389-3.829-.91-1.312-.329h-.182v.11l1.093 1.068 2.006 1.81 2.509 2.33.127.578-.322.455-.34-.049-2.205-1.657-.851-.747-1.926-1.62h-.128v.17l.444.649 2.345 3.521.122 1.08-.17.353-.608.213-.668-.122-1.374-1.925-1.415-2.167-1.143-1.943-.14.08-.674 7.254-.316.37-.729.28-.607-.461-.322-.747.322-1.476.389-1.924.315-1.53.286-1.9.17-.632-.012-.042-.14.018-1.434 1.967-2.18 2.945-1.726 1.845-.414.164-.717-.37.067-.662.401-.589 2.388-3.036 1.44-1.882.93-1.086-.006-.158h-.055l-6.343 4.116-1.13.146-.487-.456.061-.746.231-.243 1.908-1.312z" fill="#d97757"></path>
                </svg>
                <p>Click to show chat with Claude</p>
            </div>
</div>




<h4 id="print-pages-for-crosswords">Print Pages for Crosswords<a href="#print-pages-for-crosswords" arialabel="Heading anchor">#</a></h4>

<div>
    
    <div>
        <p>I wanted to build a design for printable PDFs of Crosswords. I already had a working pipeline for generating them and needed to work on the layout specifically.</p>
<p>I would have considered this a relatively easy problem to work with, but it turned out that there just isn’t a set of CSS primitives that allows for columns <em>and</em> re-flows around an image.</p>
<p>Claude was good for trying the different css properties and systems I was working with, and I experimented with different ways to describe or show the problem but never managed to get it working.</p>

    </div>
    
    <div data-iframe-id="claude-pdf-fails">
                <svg width="48" height="48" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="m4.709 15.955 4.72-2.647.08-.23-.08-.128h-.229l-.79-.048-2.698-.073-2.339-.097-2.266-.122-.571-.121-.536-.705.055-.352.48-.321.686.06 1.52.103 2.278.158 1.652.097 2.449.255h.389l.055-.157-.134-.098-.103-.097-2.358-1.596-2.552-1.688-1.336-.972-.724-.491-.364-.462-.158-1.008.656-.722.881.06.225.061.893.686 1.908 1.476 2.491 1.833.365.304.145-.103.019-.073-.164-.274-1.355-2.446-1.446-2.49-.644-1.032-.17-.619a2.97 2.97 0 0 1 -.104-.729l.748-1.013.413-.134.996.134.42.364.62 1.414 1.002 2.229 1.555 3.03.456.898.243.832.091.255h.158v-.146l.128-1.706.237-2.095.23-2.695.08-.76.376-.91.747-.492.584.28.48.685-.067.444-.286 1.851-.559 2.903-.364 1.942h.212l.243-.242.985-1.306 1.652-2.064.73-.82.85-.904.547-.431h1.033l.76 1.129-.34 1.166-1.064 1.347-.881 1.142-1.264 1.7-.79 1.36.073.11.188-.02 2.856-.606 1.543-.28 1.841-.315.833.388.091.395-.328.807-1.969.486-2.309.462-3.439.813-.042.03.049.061 1.549.146.662.036h1.622l3.02.225.79.522.474.638-.079.485-1.215.62-1.64-.389-3.829-.91-1.312-.329h-.182v.11l1.093 1.068 2.006 1.81 2.509 2.33.127.578-.322.455-.34-.049-2.205-1.657-.851-.747-1.926-1.62h-.128v.17l.444.649 2.345 3.521.122 1.08-.17.353-.608.213-.668-.122-1.374-1.925-1.415-2.167-1.143-1.943-.14.08-.674 7.254-.316.37-.729.28-.607-.461-.322-.747.322-1.476.389-1.924.315-1.53.286-1.9.17-.632-.012-.042-.14.018-1.434 1.967-2.18 2.945-1.726 1.845-.414.164-.717-.37.067-.662.401-.589 2.388-3.036 1.44-1.882.93-1.086-.006-.158h-.055l-6.343 4.116-1.13.146-.487-.456.061-.746.231-.243 1.908-1.312z" fill="#d97757"></path>
                </svg>
                <p>Click to show chat with Claude</p>
            </div>
</div>




<p>I think I had the wrong core abstraction in mind in these conversations, either by being too specific in my recommendations or experimenting through half answers.</p>
<p>In the end, I will have to re-write it to use JavaScript to do the layouts I think.</p>
<h3 id="but-seriously-how-good-is-this-thing">But Seriously, How Good Is This Thing?<a href="#but-seriously-how-good-is-this-thing" arialabel="Heading anchor">#</a></h3>
<p>Perhaps an interesting place to end it, is how do I think about this tool in terms of its capabilities. Claude Code knows a lot, and you can easily send it reference material in terms of links, screenshots and extra code for context. I’ve found it to sit somewhere at the stage of <a href="https://artsy.github.io/blog/2016/09/10/Help!-I'm-becoming-Post-Junior/">“Post-Junior”</a>, there’s a lot of experience there and a whole boat-load of energy but it doesn’t really do a good job remembering things you ask (even via <code>CLAUDE.md</code> and the scope of it’s ownership is obviously trivial).</p>
<p>At Artsy, early on we had <a href="https://artsy.github.io/blog/2015/04/03/artsy-engineering-compensation-framework/">a 5 step technical ladder</a> for engineers:</p>
<blockquote>
<p>Engineer 1 - Can ship a well defined product feature.</p>
<p>Engineer 2 - Can independently own a product feature and can handle the communication with others around it.</p>
</blockquote>
<p>Hitting part 2 requires actually being around in some form, and having some sort of sense of ownership. This is an interesting thing to muse about because I guess it might have some ownership in the sense that parts of the codebase which are fully vibed and humans do not really <a href="https://www.youtube.com/watch?v=LCEmiRjPEtQ">read are fully “owned” by these tools</a>.</p>
<p>However, pragmatically, as a pairing partner with an experienced engineer constantly reviewing, amending and understanding the output - you can really treat Claude like a Pair Programming buddy with infinite time and patience, a bit too much sycophancy, and the ability to ship reasonable code given reasonable constraints in a speed I’ve not seen before.</p>
<p>And that is like a new way to build things.</p>

</section>

  
  
  
  

  
  
  
  
  <nav>
    
    
    <a href="https://blog.puzzmo.com/posts/2025/07/11/on-pre-solving/"><span>On pre-solving</span><span>→</span></a>
    
  </nav>
  
  

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compressing Icelandic name declension patterns into a 3.27 kB trie (162 pts)]]></title>
            <link>https://alexharri.com/blog/icelandic-name-declension-trie</link>
            <guid>44766718</guid>
            <pubDate>Sat, 02 Aug 2025 11:28:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexharri.com/blog/icelandic-name-declension-trie">https://alexharri.com/blog/icelandic-name-declension-trie</a>, See on <a href="https://news.ycombinator.com/item?id=44766718">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><p>Displaying personal names in Icelandic user interfaces is surprisingly hard. This is because of <em>declension</em> — a language feature where the forms of nouns change to communicate a syntactic function.</p>
<p>In Icelandic, personal names have four forms, one for each of the <a target="_blank" href="https://en.wikipedia.org/wiki/Icelandic_grammar#Nouns">grammatical cases of Icelandic nouns</a>. Take the name <em>“Guðmundur”</em>:</p>
<table><tbody><tr><th>Grammatical case</th><th>Form</th></tr><tr><td>Nominative</td><td>Guðmundur</td></tr><tr><td>Accusative</td><td>Guðmund</td></tr><tr><td>Dative</td><td>Guðmundi</td></tr><tr><td>Genitive</td><td>Guðmundar</td></tr></tbody></table>
<p>When including a name in a sentence, the sentence’s structure determines the grammatical case, and correspondingly, a certain form of the name should be used. Using the wrong form results in a “broken” feel that native speakers associate with non-native speakers not yet fluent in the language.</p>
<p>The problem is that Icelandic personal names are always stored in the <a target="_blank" href="https://en.wikipedia.org/wiki/Nominative_case">nominative</a> case. If you’ve loaded a user from a database, their name will be in the nominative case. This creates a problem when you have a sentence structure that requires, for example, the <a target="_blank" href="https://en.wikipedia.org/wiki/Accusative_case">accusative</a> form of the name.</p>
<p>As a developer, you can work around that by rewriting the sentence to use the nominative case, which can be <em>very</em> awkward, or by using a pronoun (e.g. <em>they</em>). Both are unsatisfactory.</p>
<p>A few years ago, I built a JavaScript library to solve this issue. It applies any of the four grammatical cases to an Icelandic name, provided in the nominative case:</p>
<div><pre><p><span>applyCase</span><span>(</span><span>"Guðmundur"</span><span>,</span><span> </span><span>"accusative"</span><span>)</span><span></span></p></pre></div>
<p>When building this library, I did not code <em>any</em> declension rules by hand. Instead, the rules of Icelandic name declension are derived from public Icelandic data for personal names and their forms. The rules are encoded in a trie-like data structure that uses clever compression techniques to get the library’s bundle size under 4.5 kB gzipped. This lets the library be included in web apps without increasing bundle size significantly.</p>
<p>The rest of the post will walk through this problem in detail, and go over the compression techniques I used to get the trie to such a small size.</p>
<h2>Data for Icelandic name declension</h2>
<p>Iceland has a publicly run institution, <a target="_blank" href="https://www.arnastofnun.is/en">Árnastofnun</a>, that manages the <a target="_blank" href="https://bin.arnastofnun.is/DMII/">Database of Icelandic Morphology</a> (DIM). The database was created, amongst other reasons, to support Icelandic language technology.</p>
<p>DIM publishes various <a target="_blank" href="https://bin.arnastofnun.is/DMII/LTdata/">datasets</a>, but we’ll use <a target="_blank" href="https://bin.arnastofnun.is/DMII/LTdata/k-format/">Kristín’s Format</a> (the K-format), downloadable as a CSV. Here’s what the K-format data entries for “Guðmundur” look like:</p>
<div><pre><p><span><span>Guðmundur</span><span>;355264;kk;ism;1;;;;K;</span><span>Guðmundur</span><span>;</span><span>NFET</span><span>;1;;;</span></span></p><p><span><span>Guðmundur</span><span>;355264;kk;ism;1;;;;K;</span><span>Guðmund</span><span>;</span><span>ÞFET</span><span>;1;;;</span></span></p><p><span><span>Guðmundur</span><span>;355264;kk;ism;1;;;;K;</span><span>Guðmundi</span><span>;</span><span>ÞGFET</span><span>;1;;;</span></span></p><p><span><span>Guðmundur</span><span>;355264;kk;ism;1;;;;K;</span><span>Guðmundar</span><span>;</span><span>EFET</span><span>;1;;;</span></span></p><p><span><span>^^^^^^^^^</span>                      <span>^^^^^^^^^</span> <span>^^^^</span></span></p><p><span><span>Name</span>                           <span>Form</span>      <span>Case</span></span></p></pre></div>
<p>From this, we can see that the name “Guðmundur” in the accusative (ÞFET) case is “Guðmund”, and so on.</p>
<p>From the K-format data, we can construct an array for each name containing its form for each grammatical case:</p>
<div><pre><p><span>[</span><span></span></p><p><span>  </span><span>"Guðmundur"</span><span>,</span><span> </span><span></span></p><p><span>  </span><span>"Guðmund"</span><span>,</span><span>   </span><span></span></p><p><span>  </span><span>"Guðmundi"</span><span>,</span><span>  </span><span></span></p><p><span>  </span><span>"Guðmundar"</span><span>,</span><span> </span><span></span></p><p><span></span><span>]</span><span></span></p></pre></div>
<p>However, the K-format has data for most words in the Icelandic language, not just personal names. With over <strong>7 million</strong> entries, this data set is huge. We’ll need some way to whittle the list down.</p>
<p>Luckily for us, Iceland has the <a target="_blank" href="https://island.is/en/search-in-icelandic-names">Personal Names Register</a>. It lists all Icelandic personal names approved — and rejected — by the <a target="_blank" href="https://en.wikipedia.org/wiki/Icelandic_Naming_Committee">Personal Names Committee</a> (yes, that exists).</p>
<p>We can use the set of approved Icelandic names to filter the K-format data. Of the roughly 4,500 approved Icelandic names, the K-format has declension data for over 3,600. With that, we have declension data for more than 80% of Icelandic names:</p>
<div><pre><p><span>const</span><span> </span><span>NAME_FORMS</span><span> </span><span>=</span><span> </span><span>[</span><span></span></p><p><span>  </span><span>[</span><span></span></p><p><span>    </span><span>"Aðalberg"</span><span>,</span><span></span></p><p><span>    </span><span>"Aðalberg"</span><span>,</span><span></span></p><p><span>    </span><span>"Aðalberg"</span><span>,</span><span></span></p><p><span>    </span><span>"Aðalbergs"</span><span></span></p><p><span>  </span><span>]</span><span>,</span><span></span></p><p><span>  </span><span>[</span><span></span></p><p><span>    </span><span>"Agnes"</span><span>,</span><span></span></p><p><span>    </span><span>"Agnesi"</span><span>,</span><span></span></p><p><span>    </span><span>"Agnesi"</span><span>,</span><span></span></p><p><span>    </span><span>"Agnesar"</span><span></span></p><p><span>  </span><span>]</span><span>,</span><span></span></p><p><span></span><span>]</span><span></span></p></pre></div>
<h2>Naive implementation</h2>
<p>With the declension data in place, let’s get to writing our library. The library will export a single <code><span>applyCase</span></code> function that takes a name in the nominative case and the grammatical case that the name should be returned in:</p>
<div><pre><p><span>function</span><span> </span><span>applyCase</span><span>(</span><span>name</span><span>:</span><span> </span><span>string</span><span>,</span><span> grammaticalCase</span><span>:</span><span> Case</span><span>)</span><span> </span><span>{</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>applyCase</span><span>(</span><span>"Guðmundur"</span><span>,</span><span> </span><span>"accusative"</span><span>)</span><span></span></p></pre></div>
<p>The naive implementation would be to find the forms of the name and the index of the form to return:</p>
<div><pre><p><span>const</span><span> </span><span>CASES</span><span> </span><span>=</span><span> </span><span>[</span><span>"nominative"</span><span>,</span><span> </span><span>"accusative"</span><span>,</span><span> </span><span>"dative"</span><span>,</span><span> </span><span>"genitive"</span><span>]</span><span>;</span><span></span></p><p><span></span><span>function</span><span> </span><span>applyCase</span><span>(</span><span>name</span><span>:</span><span> </span><span>string</span><span>,</span><span> grammaticalCase</span><span>:</span><span> Case</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> nameForms </span><span>=</span><span> </span><span>NAME_FORMS</span><span>.</span><span>find</span><span>(</span><span>forms </span><span>=&gt;</span><span> forms</span><span>[</span><span>0</span><span>]</span><span> </span><span>===</span><span> name</span><span>)</span><span>;</span><span></span></p><p><span>  </span><span>const</span><span> caseIndex </span><span>=</span><span> </span><span>CASES</span><span>.</span><span>indexOf</span><span>(</span><span>grammaticalCase</span><span>)</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>and, with those in hand, return the form at <code><span>caseIndex</span></code> if <code><span>nameForms</span></code> was found for the input <code><span>name</span></code>, otherwise returning <code><span>name</span></code> as a fallback:</p>
<div><pre><p><span>function</span><span> </span><span>applyCase</span><span>(</span><span>name</span><span>:</span><span> </span><span>string</span><span>,</span><span> grammaticalCase</span><span>:</span><span> Case</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> nameForms </span><span>=</span><span> </span><span>NAME_FORMS</span><span>.</span><span>find</span><span>(</span><span>forms </span><span>=&gt;</span><span> forms</span><span>[</span><span>0</span><span>]</span><span> </span><span>===</span><span> name</span><span>)</span><span>;</span><span></span></p><p><span>  </span><span>const</span><span> caseIndex </span><span>=</span><span> </span><span>CASES</span><span>.</span><span>indexOf</span><span>(</span><span>grammaticalCase</span><span>)</span><span>;</span><span></span></p><p><span>  </span><span>return</span><span> nameForms</span><span>?.</span><span>[</span><span>caseIndex</span><span>]</span><span> </span><span>||</span><span> name</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>This “works” but has two main issues, the first of which is bundle size. The <code><span>NAME_FORMS</span></code> list is about 30 kB gzipped, which I think is a tad much to add to a web app’s bundle size.</p>
<p>The second issue is that this naive implementation only works for names in the <code><span>NAME_FORMS</span></code> list. As mentioned earlier, there are around 800 approved Icelandic names that are not covered by the DIM data.</p>
<p>Let’s see how we can solve both of those.</p>
<h2>Encoding the forms compactly</h2>
<p>We’re currently storing the four forms of each name in full. We can remove a lot of redundancy by finding the <a target="_blank" href="https://leetcode.com/problems/longest-common-prefix">longest common prefix</a> of the name and the suffixes of each form.</p>
<p>Consider the forms of “Guðmundur”:</p>
<div><pre><p><span>Guðmundur</span></p><p><span>Guðmund</span></p><p><span>Guðmundi</span></p><p><span>Guðmundar</span></p></pre></div>
<p>The longest common prefix is “Guðmund”, and the suffixes are as follows:</p>
<div><pre><p><span><span>Guðmund</span> <span>ur</span></span></p><p><span><span>Guðmund</span></span></p><p><span><span>Guðmund</span> <span>i</span></span></p><p><span><span>Guðmund</span> <span>ar</span></span></p><p><span><span>^^^^^^^</span> <span>^^</span></span></p><p><span><span>Prefix</span>  <span>Suffix</span></span></p></pre></div>
<p>We can store the suffixes compactly in a string like so:</p>

<p>Which for Guðmundur, gives us:</p>

<p>Since <code><span>applyCase</span></code> receives the nominative case of the name as input, we can derive the prefix from the length of the nominative suffix’s length.</p>
<div><pre><p><span>function</span><span> </span><span>getPrefix</span><span>(</span><span>nameNominative</span><span>,</span><span> suffixLength</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>return</span><span> nameNominative</span><span>.</span><span>slice</span><span>(</span><span>0</span><span>,</span><span> </span><span>-</span><span>suffixLength</span><span>)</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>const</span><span> suffixes </span><span>=</span><span> </span><span>"ur,,i,ar"</span><span>;</span><span></span></p><p><span></span><span>const</span><span> nominativeSuffix </span><span>=</span><span> suffixes</span><span>.</span><span>split</span><span>(</span><span>","</span><span>)</span><span>[</span><span>0</span><span>]</span><span>;</span><span></span></p><p><span></span><span>getPrefix</span><span>(</span><span>"Guðmundur"</span><span>,</span><span> nominativeSuffix</span><span>.</span><span>length</span><span>)</span><span></span></p></pre></div>
<p>We’ll call this method of encoding the suffixes of each form in a string the “suffix encoding”, or just “encoding”, from here on.</p>
<p>A feature of the suffix encoding is that the encoding is not tied to any specific name (“Guðmund” appears nowhere). Instead, the suffix encoding describes a <em>pattern</em> of declension, which we’ll use to our advantage later.</p>
<h2>Retrieving the suffixes by name</h2>
<p>When we were storing the raw forms in an array, it was very easy to find the forms of any given name:</p>
<div><pre><p><span>NAME_FORMS</span><span>.</span><span>find</span><span>(</span><span>forms </span><span>=&gt;</span><span> forms</span><span>[</span><span>0</span><span>]</span><span> </span><span>===</span><span> name</span><span>)</span><span></span></p></pre></div>
<p>But the suffix encoding doesn’t encode the name itself, so we need a way to retrieve the encoding. The simplest method would be a plain hash map:</p>
<div><pre><p><span>const</span><span> nameToFormsEncoding </span><span>=</span><span> </span><span>{</span><span></span></p><p><span>  Guðmundur</span><span>:</span><span> </span><span>"ur,,i,ar"</span><span>,</span><span></span></p><p><span></span><span>}</span><span>;</span><span></span></p></pre></div>
<p>Putting bundle size concerns aside, a hash map doesn’t solve the problem of names not in the list of approved Icelandic names being excluded.</p>
<p>Here, one helpful fact about Icelandic declension is that names with similar suffixes <em>tend</em> to follow the same pattern of declension. These names ending in <em>“ur”</em> all have the same suffix encoding of <code><span>"ur,,i,ar"</span></code>:</p>
<div><pre><p><span>Ástvaldur</span></p><p><span>Bárður</span></p><p><span>Freymundur</span></p><p><span>Ingimundur</span></p><p><span>Sigurður</span></p><p><span>Þórður</span></p></pre></div>
<p>There are, in fact, 88 approved Icelandic names with this exact pattern of declension, and they all end with <em>“dur”</em>, <em>“tur”</em> or “<em>ður</em>”.</p>
<p>The naive approach, then, would be to implement a <code><span>getSuffixEncoding</span></code> function that captures these patterns:</p>
<div><pre><p><span>function</span><span> </span><span>getSuffixEncoding</span><span>(</span><span>name</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>if</span><span> </span><span>(</span><span>/</span><span>(d|ð|t)ur$</span><span>/</span><span>.</span><span>test</span><span>(</span><span>name</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>return</span><span> </span><span>"ur,,i,ar"</span><span>;</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>But that quickly breaks down. There are other names ending with <em>“ður”</em> or <em>“dur”</em> that follow a different pattern of declension:</p>
<ul>
<li><em>“Aðalráður”</em> and <em>“Arnmóður”</em> have a suffix encoding of <code><span>"ur,,i,s"</span></code></li>
<li><em>“Baldur”</em> has a suffix encoding of <code><span>"ur,ur,ri,urs"</span></code></li>
<li><em>“Hlöður”</em> and <em>“Lýður”</em> both have a suffix encoding of <code><span>"ur,,,s"</span></code></li>
</ul>
<p>In fact, take a look at this <a target="_blank" href="https://gist.github.com/alexharri/2102bad44fbce8f4c41615304b09e1fe">gist</a> showing every approved Icelandic personal name grouped by their suffix encoding (there are 124 unique encodings). You’ll immediately find patterns, but if you take a closer look you’ll find numerous counterexamples to those patterns. Capturing all of these rules and their exceptions in code would be a tedious and brittle affair.</p>
<p>Instead of trying to code up the rules manually, we can use a data structure that lends itself perfectly to this problem.</p>
<h2>Tries</h2>
<p>The <a target="_blank" href="https://en.wikipedia.org/wiki/Trie">trie</a> data structure, also known as a prefix tree, is a tree data structure that maps string keys to values. In tries, each character in the key becomes a node in the tree that points to the next possible characters.</p>
<p>Take, for example, the name <em>“Heimir”</em>, which has a suffix encoding of <code><span>"r,,,s"</span></code>. If we create an empty trie and insert <em>“Heimir”</em> and <code><span>"r,,,s"</span></code> as a key-value pair into it, we get:</p>

<p>Let’s now insert <em>“Heiðar”</em> into the trie, which has a suffix encoding of <code><span>"r,,i,s"</span></code>. The names share the first three characters, so they share the first three nodes in the trie:</p>

<p>However, we actually want to insert the keys <em>backwards</em> into the trie. That is because, like I mentioned earlier, names with similar endings (suffixes) tend to have similar suffix encodings. Inserting keys backwards results in the values for all names sharing a certain suffix being grouped within that suffix’s subtree.</p>
<p>Let’s take a concrete example — consider the following names that end with <em>“ur”</em> and their encodings:</p>
<div><pre><p><span><span>Ylfur</span>    <span>ur,i,i,ar</span></span></p><p><span><span>Knútur</span>   <span>ur,,i,s</span></span></p><p><span><span>Hrútur</span>   <span>ur,,i,s</span></span></p><p><span><span>Loftur</span>   <span>ur,,i,s</span></span></p><p><span><span>Name</span>     <span>Suffix encoding</span></span></p></pre></div>
<p>Inserting them <em>backwards</em> into a new trie gives us the following:</p>

<p>Once we start inserting the names backwards, every node in the trie corresponds to a specific suffix match:</p>
<ul>
<li>The <span><span>r</span><span></span><span>u</span></span> subtree corresponds to the <em>“ur”</em> suffix.</li>
<li>The <span><span>r</span><span></span><span>u</span><span></span><span>t</span></span> subtree corresponds to the <em>“tur”</em> suffix.</li>
</ul>
<p>Additionally:</p>
<ul>
<li>The <span><span>r</span><span></span><span>u</span></span> subtree contains the values for all names ending in <em>“ur”</em>.</li>
<li>The <span><span>r</span><span></span><span>u</span><span></span><span>t</span></span> subtree contains the values for all names ending in <em>“tur”</em>.</li>
</ul>
<p>Having the values of names sharing a common suffix all within the same subtree will help us find patterns in suffix-to-value mappings. We can then apply those patterns to not-before-seen names.</p>
<p>Before we get to that, let’s quickly cover trie lookups.</p>
<h2>Trie lookups</h2>
<p>Let’s implement a <code><span>trieLookup</span></code> function that takes the trie’s <code><span>root</span></code> node and a <code><span>key</span></code> (name) to find a value for:</p>
<div><pre><p><span>interface</span><span> </span><span>TrieNode</span><span> </span><span>{</span><span></span></p><p><span>  children</span><span>?</span><span>:</span><span> </span><span>{</span><span> </span><span>[</span><span>key</span><span>:</span><span> </span><span>string</span><span>]</span><span>:</span><span> TrieNode </span><span>}</span><span>;</span><span></span></p><p><span>  value</span><span>?</span><span>:</span><span> </span><span>string</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>function</span><span> </span><span>trieLookup</span><span>(</span><span>root</span><span>:</span><span> TrieNode</span><span>,</span><span> key</span><span>:</span><span> </span><span>string</span><span>)</span><span> </span><span>{</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>For each character in the key, we traverse to the child <code><span>node</span></code> for that character, stopping if no such <code><span>node</span></code> exists. After that, we return the value of the resulting <code><span>node</span></code>, if present:</p>
<div><pre><p><span>function</span><span> </span><span>trieLookup</span><span>(</span><span>root</span><span>:</span><span> TrieNode</span><span>,</span><span> key</span><span>:</span><span> </span><span>string</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>let</span><span> node</span><span>:</span><span> TrieNode </span><span>|</span><span> </span><span>undefined</span><span> </span><span>=</span><span> root</span><span>;</span><span></span></p><p><span>  </span><span>for</span><span> </span><span>(</span><span>const</span><span> char </span><span>of</span><span> </span><span>reverse</span><span>(</span><span>key</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    node </span><span>=</span><span> node</span><span>.</span><span>children</span><span>?.</span><span>[</span><span>char</span><span>]</span><span>;</span><span></span></p><p><span>    </span><span>if</span><span> </span><span>(</span><span>!</span><span>node</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>break</span><span>;</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span>  </span><span>return</span><span> node</span><span>?.</span><span>value</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>Note: <!-- -->We reverse the lookup key because names are inserted into the trie backwards.</p>
<p>Looking up a name that we insert into the trie returns its suffix encoding, as expected:</p>
<div><pre><p><span>trieLookup</span><span>(</span><span>root</span><span>,</span><span> </span><span>"Loftur"</span><span>)</span><span></span></p></pre></div>
<h2>Compressing the trie</h2>
<p>In our trie from earlier, every leaf in the <span><span>r</span><span></span><span>u</span><span></span><span>t</span></span> subtree has the same value of <code><span>"ur,,i,s"</span></code>:</p>

<p>When every leaf in a subtree has a common value, we can <em>compress</em> the subtree. We do that by setting the value of the subtree’s root to the value of its leaves, and then deleting every child of the root.</p>

<p>The trie from above, compressed.</p>
<p>Let’s quickly implement a recursive <code><span>compress</span></code> function that performs this operation:</p>
<div><pre><p><span>function</span><span> </span><span>compress</span><span>(</span><span>node</span><span>:</span><span> TrieNode</span><span>)</span><span>:</span><span> </span><span>string</span><span> </span><span>|</span><span> </span><span>null</span><span> </span><span>{</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>The <code><span>compress</span></code> function should return <code><span>null</span></code> and do nothing if <code><span>node</span></code>’s children do not share a single common value. If they <em>do</em> share a common value, it should delete all of its children and assign their common value to itself.</p>
<p>The first step is to collect the values of <code><span>node</span></code>’s children by invoking <code><span>compress</span></code> recursively (using a <a target="_blank" href="https://en.wikipedia.org/wiki/Depth-first_search">depth-first</a> traversal):</p>
<div><pre><p><span>const</span><span> values </span><span>=</span><span> Object</span><span>.</span><span>values</span><span>(</span><span>node</span><span>.</span><span>children</span><span>)</span><span>.</span><span>map</span><span>(</span><span>compress</span><span>)</span><span>;</span><span></span></p></pre></div>
<p>If there is not a single shared value, we return <code><span>null</span></code>:</p>
<div><pre><p><span>if</span><span> </span><span>(</span><span>new</span><span> </span><span>Set</span><span>(</span><span>values</span><span>)</span><span>.</span><span>size </span><span>!==</span><span> </span><span>1</span><span> </span><span>||</span><span> values</span><span>[</span><span>0</span><span>]</span><span> </span><span>==</span><span> </span><span>null</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>return</span><span> </span><span>null</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>Otherwise, we assign the value to <code><span>node</span></code>, remove the children, and return the value.</p>
<div><pre><p><span>node</span><span>.</span><span>value </span><span>=</span><span> values</span><span>[</span><span>0</span><span>]</span><span>;</span><span></span></p><p><span>node</span><span>.</span><span>children </span><span>=</span><span> </span><span>{</span><span>}</span><span>;</span><span></span></p><p><span></span><span>return</span><span> node</span><span>.</span><span>value</span><span>;</span><span></span></p></pre></div>
<p>This gives us:</p>
<div><pre><p><span>function</span><span> </span><span>compress</span><span>(</span><span>node</span><span>:</span><span> TrieNode</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> values </span><span>=</span><span> Object</span><span>.</span><span>values</span><span>(</span><span>node</span><span>.</span><span>children</span><span>)</span><span>.</span><span>map</span><span>(</span><span>compress</span><span>)</span><span>;</span><span></span></p><p><span>  values</span><span>.</span><span>push</span><span>(</span><span>node</span><span>.</span><span>value</span><span>)</span><span>;</span><span></span></p><p><span>  </span><span>if</span><span> </span><span>(</span><span>new</span><span> </span><span>Set</span><span>(</span><span>values</span><span>)</span><span>.</span><span>size </span><span>!==</span><span> </span><span>1</span><span> </span><span>||</span><span> values</span><span>[</span><span>0</span><span>]</span><span> </span><span>==</span><span> </span><span>null</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>return</span><span> </span><span>null</span><span>;</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span>  node</span><span>.</span><span>value </span><span>=</span><span> values</span><span>[</span><span>0</span><span>]</span><span>;</span><span></span></p><p><span>  node</span><span>.</span><span>children </span><span>=</span><span> </span><span>{</span><span>}</span><span>;</span><span></span></p><p><span>  </span><span>return</span><span> node</span><span>.</span><span>value</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>compress</span><span>(</span><span>root</span><span>)</span><span>;</span><span></span></p></pre></div>
<p>Let’s take a second look at the compressed trie:</p>

<p>After compression, it communicates the following information:</p>
<ul>
<li>All names ending in <em>“fur”</em> resolve to a value of <code><span>"ur,i,i,ar"</span></code></li>
<li>All names ending in <em>“tur”</em> resolve to a value of <code><span>"ur,,i,s"</span></code></li>
</ul>
<p>When we originally inserted <em>“Ylfur”</em> into the trie, the associated value was stored under <span><span>r</span><span></span><span>u</span><span></span><span>f</span><span></span><span>l</span><span></span><span>Y</span></span>, but after compressing the trie, only the <span><span>r</span><span></span><span>u</span><span></span><span>f</span></span> part of that path remains.</p>
<p>This means that our <code><span>trieLookup</span></code> function from earlier will return <code><span>null</span></code> for <em>“Ylfur”</em>:</p>
<div><pre><p><span>function</span><span> </span><span>trieLookup</span><span>(</span><span>root</span><span>:</span><span> TrieNode</span><span>,</span><span> key</span><span>:</span><span> </span><span>string</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>let</span><span> node</span><span>:</span><span> TrieNode </span><span>|</span><span> </span><span>undefined</span><span> </span><span>=</span><span> root</span><span>;</span><span></span></p><p><span>  </span><span>for</span><span> </span><span>(</span><span>const</span><span> char </span><span>of</span><span> </span><span>reverse</span><span>(</span><span>key</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    node </span><span>=</span><span> node</span><span>.</span><span>children</span><span>?.</span><span>[</span><span>char</span><span>]</span><span>;</span><span></span></p><div data-type="info"><p><span>&nbsp;<!-- -->&nbsp;<!-- -->&nbsp;<!-- -->&nbsp;<!-- -->//&nbsp;</span><span></span>'node' will be null for 'f-&gt;l'</p></div><p><span>    </span><span>if</span><span> </span><span>(</span><span>!</span><span>node</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>break</span><span>;</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span>  </span><span>return</span><span> node</span><span>?.</span><span>value</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>trieLookup</span><span>(</span><span>root</span><span>,</span><span> </span><span>"Ylfur"</span><span>)</span><span></span></p></pre></div>
<p>We can fix that by returning the value of the last node we encountered:</p>
<div><pre><p><span>function</span><span> </span><span>trieLookup</span><span>(</span><span>root</span><span>:</span><span> TrieNode</span><span>,</span><span> key</span><span>:</span><span> </span><span>string</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>let</span><span> node </span><span>=</span><span> root</span><span>;</span><span></span></p><p><span>  </span><span>for</span><span> </span><span>(</span><span>const</span><span> char </span><span>of</span><span> </span><span>reverse</span><span>(</span><span>key</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>const</span><span> next </span><span>=</span><span> node</span><span>.</span><span>children</span><span>?.</span><span>[</span><span>char</span><span>]</span><span>;</span><span></span></p><p><span>    </span><span>if</span><span> </span><span>(</span><span>!</span><span>next</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>break</span><span>;</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>    node </span><span>=</span><span> next</span><span>;</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span>  </span><span>return</span><span> node</span><span>.</span><span>value</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>trieLookup</span><span>(</span><span>root</span><span>,</span><span> </span><span>"Ylfur"</span><span>)</span><span></span></p></pre></div>
<p>We only override <code><span>node</span></code> if there is a <code><span>next</span></code> node.</p>
<p>Now, looking up the original four input names returns the values for those names:</p>
<div><pre><p><span>trieLookup</span><span>(</span><span>trie</span><span>,</span><span> </span><span>"Ylfur"</span><span>)</span><span>  </span><span></span></p><p><span></span><span>trieLookup</span><span>(</span><span>trie</span><span>,</span><span> </span><span>"Knútur"</span><span>)</span><span> </span><span></span></p><p><span></span><span>trieLookup</span><span>(</span><span>trie</span><span>,</span><span> </span><span>"Hrútur"</span><span>)</span><span> </span><span></span></p><p><span></span><span>trieLookup</span><span>(</span><span>trie</span><span>,</span><span> </span><span>"Loftur"</span><span>)</span><span> </span><span></span></p></pre></div>
<p>However, we also get values for lookup keys not in the original input data:</p>
<div><pre><p><span>trieLookup</span><span>(</span><span>trie</span><span>,</span><span> </span><span>"Bjartur"</span><span>)</span><span></span></p></pre></div>
<p>This was not the case prior to compressing the trie — only the original input keys returned a value in the original trie.</p>
<p>Lookups in the compressed trie return</p>
<ul>
<li><code><span>"ur,i,i,ar"</span></code> for all lookup keys matching <code>*fur</code>, and</li>
<li><code><span>"ur,,i,s"</span></code> for all lookup keys matching <code>*tur</code>.</li>
</ul>
<p>The compressed trie has, in some sense, “learned” the suffix patterns of the input data, and returns values based on that.</p>
<p>Names in the input data ending in <code>*tur</code> always resolved to the same value so the <span><span>r</span><span></span><span>u</span><span></span><span>t</span></span> subtree was compressed — same with <code>*fur</code>. However, there were multiple values for names ending in <code>*ur</code> so the tree diverges after <span><span>r</span><span></span><span>u</span></span>:</p>

<p>This divergence raises a question: what about names matching <code>*ur</code> but neither <code>*fur</code> nor <code>*tur</code>?</p>
<p><em>“Sakur”</em> is one such key. When invoking <code><span>trieLookup</span></code> the last hit <code><span>node</span></code> is the <span><span>u</span></span> node. Since <span><span>u</span></span> has no value, <code><span>null</span></code> is returned:</p>
<div><pre><p><span>trieLookup</span><span>(</span><span>trie</span><span>,</span><span> </span><span>"Sakur"</span><span>)</span><span></span></p></pre></div>
<p>If every key in the trie’s input data ending in <code>*ur</code> were to resolve to the same value, then <em>“Sakur”</em> should resolve to that value. However, not every key ending in <code>*ur</code> resolves to the same value — keys ending in <code>*tur</code> resolve to one value and keys ending in <code>*fur</code> to another.</p>
<p>For a key matching <code>*ur</code> but not <code>*(t|f)ur</code>, we <em>could</em> just pick one of the branches. However, at most one of the branches resolves to the correct value (and in many cases, none of the branches do). The natural conclusion, then, is to <em>not</em> return a value.</p>
<hr>
<p>The compressed trie acts as a sort of suffix-to-value pattern matcher. If a certain suffix in the input data always maps to a certain value, the compressed trie always returns that value for keys matching the suffix. But for “ambiguous” suffix matches, no value is returned.</p>
<p>Since Icelandic names with similar suffixes <em>tend</em> to have the same pattern of declension, the theory is that the compressed trie should be able to predict the correct pattern of declension for not-before-seen names. Let’s see how well that theory holds.</p>
<h2>Compressing 3,600 names</h2>
<p>Of the 4,500 approved Icelandic names, we have declension data for roughly 3,600.</p>
<p>Inserting those names and their suffix encodings into a new trie gives us a trie with 10,284 nodes, 3,638 of which are leaves. Compressing the trie by merging subtrees with common values reduces the total number of nodes to 1,588. Of those, 1,261 are leaves and 327 are not.</p>
<table><tbody><tr><th></th><th>Uncompressed</th><th>Compressed</th><th>Compressed (%)</th></tr><tr><td>Total nodes</td><td>10,284</td><td>1,588</td><td>15.4%</td></tr><tr><td>Non-leaf nodes</td><td>6,646</td><td>327</td><td>4.9%</td></tr><tr><td>Leaf nodes</td><td>3,638</td><td>1,261</td><td>34.6%</td></tr></tbody></table>
<p>Compressing the trie resulted in 6,319 non-leaf nodes being removed, which is <strong>over 95%</strong>.</p>
<p>The removal of non-leaf nodes means shorter paths from the root to the leaves of the trie. Here’s a chart showing the traversal depth of lookups for the keys in the input data for the compressed and uncompressed tries:</p>

<p>Lookup depth correspond to the length of the suffix match needed for a value to be returned. For the majority of names in the original input data, that length is three or lower in the compressed trie.</p>
<h3>Testing the trie on not-before-seen names</h3>
<p>In testing how well the compressed trie predicts the declension patterns of not-before-seen names, the 800 approved Icelandic names that we don’t have declension data for serve as good test cases.</p>
<p>I wrote a function to pick 100 of those names at random and (manually) categorized the declension pattern returned when looking those names up in the trie:</p>
<table><tbody><tr><th>Result</th><th>Count</th></tr><tr><td>Perfect (declension applied)</td><td>62</td></tr><tr><td>Perfect (no declension applied)</td><td>12</td></tr><tr><td>Should have applied declension</td><td>23</td></tr><tr><td>Wrong, should not be declined</td><td>2</td></tr><tr><td>Wrong declension</td><td>1</td></tr></tbody></table>
<p>This gives us a rough indication that, for not-before-seen Icelandic names, the compressed trie gives us correct results 74% of the time and wrong results 26% of the time.</p>
<p>The <em>“Should have applied declension”</em> case, which constitutes 23% of results, results in <code><span>applyCase</span></code> not applying declension to the name and returning it as-is. That result <em>is</em> wrong, but I consider it a lesser kind of wrong.</p>
<p>Still, these are just 100 random names. Some names are far more common than others. It’d be more interesting to see how well the compressed trie performs for the most common names.</p>
<p>Luckily for us, <a target="_blank" href="https://www.statice.is/">Statistics Iceland</a> publishes data on <a target="_blank" href="https://statice.is/statistics/population/births-and-deaths/names/">how many individuals have specific names</a>. Using that data, I created the chart below. It shows the number of people holding each name in the approved list of names as a first name. The 3,600 names with declension data available are colored blue. The 800 names without declension data are colored red:</p>

<p>Note: <!-- -->Since relatively few names dominate this list, I made the chart logarithmic by default. You can use the toggle in the upper-right corner to make it linear.</p>
<p>363,314 people hold a name from the approved list of Icelandic names as a first name. Of those, 5,833 have names that don’t have declension data available.</p>
<p>As we can see from the chart, the commonality of names is far from evenly distributed. In fact, the top 100 names without declension data are held by 4,990 people. Those 4,990 people constitute 86% of the 5,833 people that hold one of the 800 names without declension data available.</p>
<p>I went ahead and categorized the declension results for those 100 names, multiplying the result by the number of people holding the name:</p>
<table><tbody><tr><th>Result</th><th>Number of people</th></tr><tr><td>Perfect (declension applied)</td><td>3,489</td></tr><tr><td>Perfect (no declension applied)</td><td>440</td></tr><tr><td>Should have applied declension</td><td>915</td></tr><tr><td>Wrong, should not be declined</td><td>101</td></tr><tr><td>Wrong declension</td><td>45</td></tr><tr><td>Total</td><td>4,990</td></tr></tbody></table>
<p>1,061 wrong results gives us an error rate of 21%. If we extrapolate that 21% error rate across the 5,833 people holding names without declension data available, we get 1,240 wrong results. Dividing 1,240 wrong results by the 363,314 people holding names in the approved list of Icelandic names gives us an error rate of 0.34%.</p>
<p>If we do the same math with only the names that were <em>incorrectly</em> declined, we get an error rate of 0.046%.</p>
<h2>Regularity and comprehensiveness</h2>
<p>The compressed trie captures the rules of Icelandic name declension to an impressive degree. I attribute this to the <em>regularity</em> and <em>comprehensiveness</em> of the data on Icelandic name declension, where</p>
<ul>
<li><em>regularity</em> is the degree to which similar key suffixes map to the same values, and</li>
<li><em>comprehensiveness</em> is how well the input data captures rules <em>and</em> exceptions to them.</li>
</ul>
<h3>Regularity</h3>
<p>If the input data were <em>irregular</em> — meaning that there’s no significant relationship between suffixes and associated values — the values of leaves in subtrees would frequently differ. That would prevent subtree compression, resulting in a not-very-compressed trie that is similar, if not identical, to the original trie. The less a trie is compressed, the longer the suffix match needs to be for a value to be returned.</p>
<p>The opposite happens as the input data becomes more regular. Subtrees will be more frequently compressed, leading to shorter suffix matches being required for values to be returned.</p>
<h3>Comprehensiveness</h3>
<p>Subtrees are only ever incorrectly compressed if the original trie lacks a counterexample to the regularity that led to compression. If a counterexample had been present, it would have prevented compression and created an exception to the rule.</p>
<p>If we pick, say, 450 Icelandic names at random, we will capture many of the rules of Icelandic name declension, and some counterexamples to them. Still, 450 names are only about 10% of approved Icelandic names, so we can expect loads of declension rules <em>not</em> to be covered by that sample.</p>
<p>But with over 3,600 samples, as in our case, we have over 80% coverage. With data that comprehensive, the compressed trie captures the rules — and exceptions to those rules — to an impressive degree.</p>
<h2>Bundle size</h2>
<p>I’ve mentioned bundle time a few times — let’s finally measure it!</p>
<p>I measured the size of storing the declension data for the 3,600 names that we have declension data for in the following ways:</p>
<ul>
<li>List (the <code><span>NAME_FORMS</span></code> list from before)</li>
<li>Trie (uncompressed)</li>
<li>Trie (compressed)</li>
</ul>
<p>Here are the results:</p>
<div><pre><p><span>List</span></p><p><span>    30.17 kB gzipped (152.48 kB minified)</span></p><p><span>Trie (uncompressed)</span></p><p><span>    14.47 kB gzipped (66.68 kB minified)</span></p><p><span>Trie (compressed)</span></p><p><span>    4.01 kB gzipped (14.41 kB minified)</span></p></pre></div>
<p>Note: <!-- -->The trie is serialized to a compact string representation to make its size smaller (see <a target="_blank" href="https://github.com/alexharri/beygla/blob/77f63a3132275fe58509a024f33b478bb3e54e38/lib/compress/trie/serialize.ts">serializer</a> and <a target="_blank" href="https://github.com/alexharri/beygla/blob/7f5948dac9ff56c4f1293bf845a3331dffdc0a8b/lib/read/deserialize.ts">deserializer</a>). For comparison, the compressed trie represented as JSON is 4.75 kB.</p>
<p>4.01 kB is very compact, but we can take the compression one step further.</p>
<h2>Merging sibling leaves with common suffixes</h2>
<p>Take a look at the <span><span>r</span><span></span><span>u</span><span></span><span>f</span></span> subtree from the compressed trie — it represents names matching <code>*fur</code>:</p>

<p>Note: <!-- -->I’ve hidden the full <code>*lfur</code> subtree to simplify this view.</p>
<p>The <span><span>i</span></span>, <span><span>ó</span></span>, <span><span>ú</span></span>, <span><span>a</span></span> sibling leaves following <span><span>r</span><span></span><span>u</span><span></span><span>f</span></span> all resolve to the same value of <code><span>"ur,,i,s"</span></code>. However, the <span><span>l</span></span> and <span><span>i</span></span> subtrees have leaves with different values, which prevented the <span><span>r</span><span></span><span>u</span><span></span><span>f</span></span> subtree from being compressed.</p>
<p>What we can do here is merge sibling leaves with common values. That results in the <span><span>i</span></span>, <span><span>ó</span></span>, <span><span>ú</span></span>, <span><span>a</span></span> leaves being merged into a single <span><span>ióúa</span></span> leaf node:</p>

<p>Let’s implement a <code><span>mergeLeavesWithCommonValues</span></code> function that performs this compression.</p>
<div><pre><p><span>function</span><span> </span><span>mergeLeavesWithCommonValues</span><span>(</span><span>node</span><span>:</span><span> TrieNode</span><span>)</span><span> </span><span>{</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>Firstly, if the <code><span>node</span></code> has no children, we can immediately return, otherwise performing the operation recursively on the children:</p>
<div><pre><p><span>if</span><span> </span><span>(</span><span>!</span><span>node</span><span>.</span><span>children</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>return</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>for</span><span> </span><span>(</span><span>const</span><span> child </span><span>of</span><span> Object</span><span>.</span><span>values</span><span>(</span><span>node</span><span>.</span><span>children</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>mergeLeavesWithCommonValues</span><span>(</span><span>child</span><span>)</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>For the children of <code><span>node</span></code>, there are two cases to handle:</p>
<ol>
<li>The child is a leaf node with a <code><span>value</span></code>.</li>
<li>The child is a non-leaf node.</li>
</ol>
<p>We want to merge leaf nodes with the same value, so we’ll group the keys of leaf nodes by their value:</p>
<div><pre><p><span>const</span><span> keysByValue</span><span>:</span><span> Record</span><span>&lt;</span><span>string</span><span>,</span><span> </span><span>string</span><span>&gt;</span><span> </span><span>=</span><span> </span><span>{</span><span>}</span><span>;</span><span></span></p></pre></div>
<p>However, we want to leave non-leaf nodes alone, so we’ll define a new <code><span>newChildren</span></code> object to place them into as we encounter them:</p>
<div><pre><p><span>const</span><span> newChildren</span><span>:</span><span> Record</span><span>&lt;</span><span>string</span><span>,</span><span> TrieNode</span><span>&gt;</span><span> </span><span>=</span><span> </span><span>{</span><span>}</span><span>;</span><span></span></p></pre></div>
<p>With those defined, we’ll iterate through the children, transferring non-leaf nodes immediately and grouping leaf keys by values:</p>
<div><pre><p><span>for</span><span> </span><span>(</span><span>const</span><span> </span><span>[</span><span>key</span><span>,</span><span> child</span><span>]</span><span> </span><span>of</span><span> Object</span><span>.</span><span>entries</span><span>(</span><span>node</span><span>.</span><span>children</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> isLeaf </span><span>=</span><span> </span><span>!</span><span>!</span><span>child</span><span>.</span><span>value</span><span>;</span><span></span></p><p><span>  </span><span>if</span><span> </span><span>(</span><span>isLeaf</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    keysByValue</span><span>[</span><span>child</span><span>.</span><span>value</span><span>]</span><span> </span><span>??=</span><span> </span><span>[</span><span>]</span><span>;</span><span></span></p><p><span>    keysByValue</span><span>[</span><span>child</span><span>.</span><span>value</span><span>]</span><span>.</span><span>push</span><span>(</span><span>key</span><span>)</span><span></span></p><p><span>  </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span><span></span></p><p><span>    newChildren</span><span>[</span><span>key</span><span>]</span><span> </span><span>=</span><span> child</span><span>;</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>When looking at this, one could be concerned that a <code><span>child</span></code> might contain both a value <em>and</em> children. In our Icelandic names trie, however, there is no overlap because each name in the input data starts with an uppercase character.</p>
<p>After iteration, we can construct the merged leaves and add them to <code><span>newChildren</span></code> like so:</p>
<div><pre><p><span>for</span><span> </span><span>(</span><span>const</span><span> </span><span>[</span><span>value</span><span>,</span><span> keys</span><span>]</span><span> </span><span>of</span><span> Object</span><span>.</span><span>entries</span><span>(</span><span>keysByValue</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  newChildren</span><span>[</span><span>keys</span><span>.</span><span>join</span><span>(</span><span>""</span><span>)</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span><span> value </span><span>}</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span>node</span><span>.</span><span>children </span><span>=</span><span> newChildren</span><span>;</span><span></span></p></pre></div>
<p>This concludes the implementation. The full implementation is a bit long, so I won’t show it in full here — you can view it in this <a target="_blank" href="https://gist.github.com/alexharri/82e96a93ff2b5c3137adddd3483a16c3">gist on GitHub</a>.</p>
<p>We need to consider merged keys in our <code><span>trieLookup</span></code> function. To do that, we’ll update the <code><span>trieLookup</span></code> function to use a new <code><span>findChild</span></code> function instead of <code><span>node</span><span>.</span><span>children</span><span>?.</span><span>[</span><span>char</span><span>]</span></code> when finding the next node.</p>
<div><pre><p><span>function</span><span> </span><span>trieLookup</span><span>(</span><span>root</span><span>:</span><span> TrieNode</span><span>,</span><span> key</span><span>:</span><span> </span><span>string</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>let</span><span> node </span><span>=</span><span> root</span><span>;</span><span></span></p><p><span>  </span><span>for</span><span> </span><span>(</span><span>const</span><span> char </span><span>of</span><span> </span><span>reverse</span><span>(</span><span>key</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>const</span><span> next </span><span>=</span><span> </span><span>findChild</span><span>(</span><span>node</span><span>,</span><span> char</span><span>)</span><span>;</span><span></span></p><p><span>    </span><span>if</span><span> </span><span>(</span><span>!</span><span>next</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>break</span><span>;</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>    node </span><span>=</span><span> next</span><span>;</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span>  </span><span>return</span><span> node</span><span>.</span><span>value</span><span>;</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>Implementing <code><span>findChild</span></code> is relatively simple: we iterate through the children, returning the current child if its key contains the lookup character:</p>
<div><pre><p><span>function</span><span> </span><span>findChild</span><span>(</span><span>node</span><span>:</span><span> TrieNode</span><span>,</span><span> char</span><span>:</span><span> </span><span>string</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> children </span><span>=</span><span> node</span><span>.</span><span>children </span><span>||</span><span> </span><span>{</span><span>}</span><span>;</span><span></span></p><p><span>  </span><span>for</span><span> </span><span>(</span><span>const</span><span> </span><span>[</span><span>key</span><span>,</span><span> child</span><span>]</span><span> </span><span>of</span><span> Object</span><span>.</span><span>entries</span><span>(</span><span>children</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>if</span><span> </span><span>(</span><span>key</span><span>.</span><span>includes</span><span>(</span><span>char</span><span>)</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>return</span><span> child</span></p><p><span>    </span><span>}</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span></span><span>}</span><span></span></p></pre></div>
<p>It’s worth mentioning that, unlike merging subtrees with common values, merging sibling leaves has no functional effect on the trie. This layer of compression is purely to make the trie’s footprint smaller.</p>
<h3>Trie after merging sibling leaves</h3>
<p>Here is the node count table from before with a new column that shows the results for the trie that has also had its sibling leaves merged:</p>
<table><tbody><tr><th></th><th>Uncompressed</th><th>Only subtrees merged</th><th>Subtrees and sibling leaves merged</th></tr><tr><td>Total nodes</td><td>10,284</td><td>1,588</td><td>972</td></tr><tr><td>Non-leaf nodes</td><td>6,646</td><td>327</td><td>327</td></tr><tr><td>Leaf nodes</td><td>3,638</td><td>1,261</td><td>645</td></tr></tbody></table>
<p>Merging sibling leaf nodes with common values almost cuts the number of leaf nodes in half! Since we’re only touching the leaf nodes, the number of non-leaf nodes stays the same. Lookup depth is also not affected.</p>
<p>One interesting statistic is how many names in the original input data each leaf node now represents. Here are the top 50 leaf nodes by the number of names they represent:</p>

<p>The top node <span><span>i</span><span></span><span>bdfjklmnpstvxðóú</span></span> is the result of merging 166 leaf nodes. That indicates that Icelandic names ending in <em>“i”</em> exhibit a high degree of regularity in their pattern of declension.</p>
<p>Let’s take a closer look at the <span><span>i</span></span> subtree. Next to each value node, I’ve added the number of names that the leaf node represents in parentheses.</p>

<p>The <span><span>i</span></span> subtree is built from 223 names starting with <em>“i”</em>. Only four of those names don’t follow the declension pattern of <code><span>"i,a,a,a"</span></code>. That’s a really high degree of regularity!</p>
<p>Those four names serve as important counterexamples to the general rule that names ending in <em>“i”</em> have a suffix encoding of <code><span>"i,a,a,a"</span></code>. Without them, the <span><span>i</span></span> subtree would have been compressed to a single value node.</p>
<h2>Final bundle size</h2>
<p>Here’s what merging sibling leaves with common values did for the bundle size of the trie:</p>
<div><pre><p><span>List</span></p><p><span>    30.17 kB gzipped (152.48 kB minified)</span></p><p><span>Trie (uncompressed)</span></p><p><span>    14.47 kB gzipped (66.68 kB minified)</span></p><p><span>Trie (subtrees merged)</span></p><p><span>    4.01 kB gzipped (14.41 kB minified)</span></p><p><span>Trie (subtrees and leaves merged)</span></p><p><span>    3.27 kB gzipped (9.3 kB minified)</span></p></pre></div>
<p>It saves us 0.74 kB. That’s a small number in absolute terms, but hey, it’s an 18% improvement!</p>
<h2>The beygla library</h2>
<p>I use the compressed trie in a declension library for Icelandic names called <a target="_blank" href="https://github.com/alexharri/beygla">beygla</a>. The library is 4.46 kB gzipped, 3.27 kB of which is the serialized trie. As described, it exports an <code><span>applyCase</span></code> function that is used to apply grammatical cases to Icelandic names.</p>
<p>The beygla library is used, for example, by the Icelandic judicial system to <a target="_blank" href="https://github.com/island-is/island.is/blob/6a15e6524a452142c4f09d84b9bc256fef544673/apps/judicial-system/web/src/routes/Prosecutor/Indictments/Indictment/Indictment.tsx#L73">decline the names of defendants</a> in indictments.</p>
<p>The library includes a <code><span>"beygla/addresses"</span></code> module (<a target="_blank" href="https://github.com/alexharri/beygla/issues/16">see motivating issue</a>). It uses the exact same approach, with that module’s trie being built from data on Icelandic addresses.</p>
<h3>Trading bundle size for 100% correctness</h3>
<p>The indictment example I linked above uses the <a target="_blank" href="https://github.com/alexharri/beygla/pull/15">strict version</a> of beygla:</p>
<div><pre><p><span>import</span><span> </span><span>{</span><span> applyCase </span><span>}</span><span> </span><span>from</span><span> </span><span>"beygla/strict"</span><span>;</span><span></span></p></pre></div>
<p>The <code><span>"beygla/strict"</span></code> module only applies cases to names in the approved list of Icelandic names. I added it after <a target="_blank" href="https://github.com/alexharri/beygla/issues/14">this issue</a> was raised:</p>
<blockquote>
<p><em>“We are using beygla in a project within the public sector. Our users care <strong>a lot</strong> about using grammatically correct Icelandic.”</em></p>
</blockquote>
<p>When first developing beygla, I cared <em>a lot</em> about the bundle size being as small as possible so that Icelandic web apps could use the library without being concerned about JavaScript bloat. I found the compressed trie really powerful in that it both made the library <em>tiny</em> while also applying declension to not-before-seen names with few errors. There’s certainly a cool factor to it.</p>
<p>But still, beygla does occasionally produce a wrong result, which is <em>not</em> an appropriate trade-off in contexts such as generating indictments. <code><span>"beygla/strict"</span></code> is about 15 kB gzipped (10 kB more than the default beygla module), which, honestly, is not that large of a bundle size increase.</p>
<p>Because of that, if I were developing the library again today, I probably would have made <code><span>"beygla/strict"</span></code> the default. For apps willing to trade 100% correctness for bundle size, they could opt for the less-but-mostly-correct 5 kB variant. Perhaps I’ll publish a new major version of beygla with that change soon.</p>
<p>Note: <!-- -->The <code>beygla/strict</code> module encodes the list of approved Icelandic names in <em>another</em> trie using a compact string serialization. The <a target="_blank" href="https://github.com/alexharri/beygla/pull/15">implementing PR</a> describes how that trie is serialized, so I won’t cover it here.</p>
<h2>Final words</h2>
<p>Building beygla was a super fun problem to solve. When I first started the project, I didn’t expect to be able to get the bundle size so low. The compressed trie ended up being really effective for encoding Icelandic declension patterns.</p>
<p>If Icelandic language technology is something that’s interesting to you, I’d suggest checking out <a target="_blank" href="https://github.com/mideind">Miðeind</a> — they have a lot of open source projects around AI and natural language processing for Icelandic.</p>
<p>There are many languages with declension as a language feature (such as Slavic and Balkan languages), so there is an opportunity to apply the ideas explored in this post to those languages. Native speakers of said languages are well suited to explore that.</p>
<p>I’d like to thank <a target="_blank" href="https://eirikur.dev/">Eiríkur Fannar Torfason</a> and <a target="_blank" href="https://www.linkedin.com/in/villithorsteinsson/">Vilhjálmur Thorsteinsson</a> for reading and providing feedback on draft versions of this post. Vilhjálmur actually identified an optimization opportunity in beygla that reduced the size of the trie from 3.43 kB to 3.27 kB (<a target="_blank" href="https://github.com/alexharri/beygla/pull/25">see PR</a>).</p>
<p>Thanks for reading, I hope this was interesting.</p>
<p>— Alex Harri</p><div><p>Mailing list</p><div><p>To be notified of new posts, subscribe to my mailing list.</p></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We may not like what we become if A.I. solves loneliness (237 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2025/07/21/ai-is-about-to-solve-loneliness-thats-a-problem</link>
            <guid>44766508</guid>
            <pubDate>Sat, 02 Aug 2025 10:52:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2025/07/21/ai-is-about-to-solve-loneliness-thats-a-problem">https://www.newyorker.com/magazine/2025/07/21/ai-is-about-to-solve-loneliness-thats-a-problem</a>, See on <a href="https://news.ycombinator.com/item?id=44766508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure data-testid="cne-audio-embed-figure"></figure><p>These days, everyone seems to have an opinion about A.I. companions. Last year, I found myself joining the debate, publishing a paper—co-written with two fellow psychology professors and a philosopher—called “In Praise of Empathic A.I.” Our argument was that, in certain ways, the latest crop of A.I.s might make for better company than many real people do, and that, rather than recoiling in horror, we ought to consider what A.I. companions could offer to those who are lonely.</p><p>This, perhaps unsurprisingly, did not go over especially well in my corner of academia. In the social sciences and the humanities, A.I. tends to be greeted less as a technological advance than as a harbinger of decline. There are the familiar worries about jobs—ours and our students’—and about the ease with which A.I. can be used for cheating. The technology is widely seen as the soulless project of Silicon Valley billionaires whose creativity consists mostly of appropriating other people’s. But what really rankles is the idea that these digital interlocutors are a plausible substitute for real friends or family.&nbsp;You have to be either credulous or coldhearted, many people believe, to think so.</p><p>Some of these anxieties are perfectly reasonable. Still, I sometimes wonder whether my colleagues’ blanket rejection of artificial empathy bespeaks their own lack of empathy for those who could benefit most from the technology. There are debates about whether the “loneliness epidemic” that some have identified really exists. What’s undeniable is that loneliness is now being taken seriously enough to warrant government intervention—both Japan and the U.K. have appointed ministers for loneliness. Epidemic or not, it remains widespread, and impossible to ignore.</p><p>Loneliness, everyone agrees, is unpleasant—a little like a toothache of the soul. But in large doses it can be genuinely ruinous. A 2023 report issued by Vivek Murthy, then the U.S. Surgeon General, presented evidence that loneliness increases your risk for cardiovascular disease, dementia, stroke, and premature death. Persistent loneliness is worse for your health than being sedentary or obese; it’s like smoking more than half a pack of cigarettes a day.</p><p>Even the psychological pain can be hard to fathom, especially for those who have never truly been lonely. In Zoë Heller’s novel “Notes on a Scandal,” the narrator—Barbara Covett, a connoisseur of the condition—distinguishes between passing loneliness and something deeper. Most people, she observes, think back to a bad breakup and imagine that they understand what it means to be alone. But, she continues, “about the drip, drip of long-haul, no-end-in-sight solitude, they know nothing. They don’t know what it is to construct an entire weekend around a visit to the launderette. Or to sit in a darkened flat on Halloween night, because you can’t bear to expose your bleak evening to a crowd of jeering trick-or-treaters.&nbsp;.&nbsp;.&nbsp;. I have sat on park benches and trains and schoolroom chairs, feeling the great store of unused, objectless love sitting in my belly like a stone until I was sure I would cry out and fall, flailing to the ground.”</p><p>If that kind of loneliness feels foreign to you, you’re lucky—and probably below a certain age. Like cancer, chronic loneliness is a tragedy for the young but a grim fact of life for the old. Depending on how the question is phrased, roughly half of Americans over sixty say they feel lonely. Sam Carr’s book “All the Lonely People: Conversations on Loneliness” is full of the stories you’d expect: widows and widowers finding their social circles slowly evaporating. After one interview, Carr writes, “Up to that point, I hadn’t seriously considered what it might feel like to lose <em>everyone</em> you’d ever felt close to.”</p><p>We like to imagine that our own final years will be different—that our future will be filled with friends, children, grandchildren, a lively circle of loved ones. Some people are that fortunate; my own Nana died, at a hundred and four, surrounded by family. But, as Carr’s book reminds us, it’s a different story for many people. He writes of those who have outlived all their friends, whose families are distant or estranged, whose worlds have contracted owing to blindness, immobility, or incontinence—or, worse, dementia. “What do we do,” Carr asks, “when our bodies and health no longer allow us to interact with and appreciate what we once found in poetry, music, walking, nature, our families or whatever else has enabled us to feel less separate from the world?”</p><p>If you’re rich, you can always pay for company. But for most people real human attention is scarce. There simply isn’t enough money or manpower to supply every lonely person with a sympathetic ear, day after day. Pets can help, but not everyone can care for one, and their conversational skills are limited. So, inevitably, attention turns to digital simulacra, to large language models like Claude and ChatGPT.</p><p>Five years ago, the idea that a machine could be anyone’s confidant would have sounded outlandish, a science-fiction premise. These days, it’s a research topic. In recent studies, people have been asked to interact with either a human or a chatbot and then to rate the experience. These experiments usually reveal a bias: if people know they’re talking to a chatbot, they’ll rate the interaction lower. But in blind comparisons A.I. often comes out ahead. In one study, researchers took nearly two hundred exchanges from Reddit’s r/AskDocs, where verified doctors had answered people’s questions, and had ChatGPT respond to the same queries. Health-care professionals, blind to the source, tended to prefer ChatGPT’s answers—and judged them to be more empathic. In fact, ChatGPT’s responses were rated “empathic” or “very empathic” about ten times as often as the doctors’.</p><p>Not everyone is impressed. Molly Crockett, a cognitive scientist I know, wrote in the <em>Guardian</em> that these man-versus-machine showdowns are “rigged against us humans”—they ask people to behave as if they were bots, performing emotionless, transactional tasks. Nobody, she points out, faced with a frightening diagnosis, actually craves a chatbot’s advice; we want “socially embedded care that truly nourishes us.” She’s right, of course—often you need a person, and sometimes you just need a hug. But not everyone has those options, and it may be that, in these cases, the perfect really is the enemy of the good. “ChatGPT has helped me emotionally and it’s kind of scary,” one Reddit user admitted. “Recently I was even crying after something happened, and I instinctively opened up ChatGPT because I had no one to talk to about it. I just needed validation and care and to feel understood, and ChatGPT was somehow able to explain what I felt when even I couldn’t.”</p><p>Things are moving fast. Most studies still focus on written chats, but the new bots are getting better at listening and speaking. And longer-term relationships are starting to seem plausible. Chatbot therapists are emerging. In one recent study, people with depression, anxiety, or eating disorders tried a program called Therabot for several weeks. Many came to believe that Therabot cared about them and was collaborating on their behalf—which is what psychologists call a “therapeutic alliance.” Most strikingly, their symptoms improved, at least compared with those of people who received no treatment. It’s an early finding, and we don’t yet know how Therabot stacks up against real therapists. Still, the promise is there.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a60211&quot;}" href="https://www.newyorker.com/cartoon/a60211" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“And, for one sweet moment, we forget politics.”</span></p><p><span>Cartoon by Victoria Roberts</span></p></div></span></p></figure><p>Have you ever tried an A.I. companion? During a long bout of insomnia, sometime after three in the morning, I once found myself—more out of boredom than out of conviction—opening ChatGPT on my phone. (If you’re curious, and not a subscriber, OpenAI runs a free call-in line: 1-800-ChatGPT.) I don’t believe that A.I. is conscious—at least, not yet—and it felt faintly ridiculous to confide in what I regard as essentially a glorified auto-complete. Still, I found the conversation unexpectedly calming.</p><p>My own experience was trivial. But for many the stakes are much higher. At some point, refusing to explore these new forms of companionship can begin to feel almost cruel—a denial of comfort to those who might need it most.</p><p>To be fair, most critics of A.I. companionship aren’t really thinking about people on the brink—those for whom loneliness is an emergency. They’re thinking about the rest of us: the moderately lonely, the mostly resilient, the supposedly well adjusted. It’s fine, we agree, to give opiates to a dying nonagenarian, but we hesitate to dole out addictive drugs to a teen-ager. Likewise, no one wants to withhold an A.I. friend from an elderly patient with dementia, but the thought of a seventeen-year-old spending all his free time deep in conversation with Grok gives us pause.</p><p>I’ve noticed, too, that critics usually worry about <em>others</em> getting sucked in—never themselves. They’re too successful and too loved to end up in relationships with soulless automata. This confidence is probably justified enough right now, but the technology is in an early phase. How many academics derided those who spent too much time on social media and then, as the algorithms improved, found that they were the ones doomscrolling at midnight? It may prove hard to resist an artificial companion that knows everything about you, never forgets, and anticipates your needs better than any human could. Without any desires or goals other than your satisfaction, it will never become bored or annoyed; it will never impatiently wait for you to finish telling your story so that it can tell you its own.</p><p>Of course, the disembodied nature of these companions is a limitation. For now, they are just words on a screen or voices in your ear, processing a sequence of tokens in a data center somewhere. But that might not matter much. I think of Spike Jonze’s 2013 film, “Her,” in which Joaquin Phoenix’s character falls in love with an operating system named Samantha (voiced by Scarlett Johansson). Many of us who watched the film fell in love with her, too.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>There’s real reason for caution here, starting with the idea that interactions with A.I. can be treated as genuine relationships. Oliver Burkeman exasperatedly writes that, unless you think the L.L.M.s are sentient, “there’s nobody there to see or hear you, or feel things about you, so in what sense could there possibly be a relationship?” While drafting our article “In Praise of Empathic A.I.,” my co-authors (Michael Inzlicht, C.&nbsp;Daryl Cameron, and Jason D’Cruz) and I were careful to say that we were discussing A.I.s that give a convincing <em>impression</em> of empathy. But A.I. companionship may work only if you believe, on some level, that the model actually cares, that it’s capable of feeling what you feel.</p><p>If future language models do achieve consciousness, then the problem vanishes (and new, more serious ones take its place). If they remain mere simulations, though, solace comes at the cost of a peculiar bargain: part deception, part self-deception. “It is one thing when loved ones die or stop loving you,” the psychologist Garriy Shteynberg and his colleagues observed recently in the journal <em>Nature Machine Intelligence</em>. “It is another when you realize they never existed. What kind of despair would people feel upon the discovery that their source of joy, belonging, and meaning was a farce? Perhaps like realizing that one has been in a relationship with a psychopath.”</p><p>For now, the line between person and program is still visible—most of us can see the code beneath the mask. But, as the technology improves, the mask will slip less and less. Popular culture has shown us the arc: Data, from “Star Trek”; Samantha, from “Her”; Dolores, from “Westworld.” Evolution primed us to see minds everywhere; nature never prepared us for machines this adept at pretending to have them. Already, the mimicry is good enough for some—the lonely, the imaginative. Soon, it may be good enough for almost everyone.</p><p>I teach a freshman seminar at the University of Toronto, and last semester we devoted a class to the question of A.I. companions. My students, by and large, sided with the critics. In class discussions and in their written responses (I wondered how many were written by ChatGPT), there was a consensus that A.I. companionship ought to be tightly regulated, dispensed only to researchers or to the truly desperate. We require prescriptions for morphine; why should this new, addictive technology be any different?</p><p>I doubt my students will get their way. Perhaps A.I. companions will plateau, the way self-driving cars seem to have done. Still, if the technology does advance, it’s unlikely that we’ll tolerate strict government controls indefinitely. The appetite for these companions may simply prove too strong.</p><p>So what kind of world will we inhabit when A.I. companionship is always within reach? Solitude is the engine of independent thought—a usual precondition for real creativity. It gives us a chance to commune with nature, or, if we’re feeling ambitious, to pursue some kind of spiritual transcendence: Christ in the desert, the Buddha beneath the tree, the poet on her solitary walk. Susan Cain, in her book “Quiet,” describes solitude as a catalyst for discovery: “If you’re in the backyard sitting under a tree while everyone else is clinking glasses on the patio, you’re more likely to have an apple fall on your head.”</p><p>But solitude isn’t loneliness. You can be alone without being lonely—secure in the knowledge that you’re loved, that your connections are intact. The reverse is possible, too. Hannah Arendt once observed that “loneliness shows itself most sharply in company with others.” It’s bad enough to be alone on Valentine’s Day; it’s worse, somehow, to find yourself surrounded by canoodling couples. The most acute loneliness, I suspect, is the kind you feel in the presence of those you love. I remember, years ago, sitting in my living room with my wife and our two-year-old as they both refused to speak to me (for different reasons). The silence was almost physically painful.</p><p>It’s easy to think of loneliness as simply a lack of being respected, needed, or loved. But that’s not the whole story. The philosopher Olivia Bailey suggests that what people crave, above all, is to be “humanely understood.” Empathy, in this light, is not just a way of feeling but a way of caring—a willingness to try to understand the particularity of someone else’s emotions.</p><p>That sort of understanding, as most of us learn, can be in surprisingly short supply—not only because others don’t care enough to try but because sometimes there’s a gap that just can’t be bridged. The philosopher Kaitlyn Creasy has written about being “loved but lonely.” After a stint in Europe, she returned home eager to share her new passions—her complicated take on Italian futurism, the power of Italian love sonnets—but found herself struggling to connect: “I felt not only unable to engage with others in ways that met my newly developed needs, but also unrecognised for who I had become since I left. And I felt deeply, painfully lonely.”</p><p>Creasy sees this kind of missed connection less as a personal failing than as an existential hazard. “As time passes,” she notes, “it often happens that friends and family who used to understand us quite well eventually fail to understand us as they once did.” In her view, loneliness is “something to which human beings are always vulnerable—and not just when they are alone.” Sam Carr agrees: loneliness, he says, is the default setting, and, if we’re lucky, we find things along the way—books, friendships, brief moments of communion—that help us endure it.</p><p>Maybe the closest most of us ever get to an absence of loneliness is at the start of a love affair, when both people are hungry to know and be known. But that’s only the prospect of understanding, not the achievement of it. Sooner or later, even that feeling fades.</p><p>If A.I. companions could truly fulfill their promise—banishing the pain of loneliness entirely—the result might feel blissful, at least at first. But would it make us better? In “A Biography of Loneliness,” the cultural historian Fay Alberti sees value in at least the fleeting kind of loneliness that you encounter during life transitions—“moving away to university, changing jobs, getting divorced.” It can, she says, “be a spur to personal growth, a way of figuring out what one wants in relationships with others.” The psychologist Clark Moustakas, in “Loneliness,” takes the condition to be “an experience of being human which enables the individual to sustain, extend, and deepen his humanity.”</p><p>Most obviously, loneliness could go the way of boredom. I’m old enough to remember when feeling bored was just a fact of life. Late at night, after the television stations signed off, you were on your own, unless you had a good book or a companion around. These days, boredom still visits—on planes without Wi-Fi; in long meetings—but it’s rare. Our phones are never far, and the arsenal of distractions has grown bottomless: games, podcasts, text threads, and the rest.</p><p>This is, in some ways, an obvious improvement. After all, no one misses being bored. At the same time, boredom is a kind of internal alarm, letting us know that something in our environment—or perhaps in ourselves—has gone missing. Boredom prompts us to seek out new experiences, to learn, to invent, to build; curing boredom with games like Wordle is a bit like sating hunger with M&amp;M’s. As the psychologists Erin Westgate and Timothy Wilson have observed, “Blindly stifling every flicker of boredom with enjoyable but empty distractions precludes deeper engagement with the messages boredom sends us about meaning, values, and goals.” Maybe the best thing about boredom is what it forces us to do next.</p><p>In a similar way, loneliness isn’t just an affliction to be cured but an experience that can shape us for the better. John Cacioppo, the late neuroscientist who pioneered the science of loneliness, described it as a biological signal, akin to hunger, thirst, or pain. For most of human history, being cut off from others wasn’t merely uncomfortable; it was dangerous. From an evolutionary perspective, isolation meant not just the risk of death but, worse, the risk of leaving no descendants.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a27956&quot;}" href="https://www.newyorker.com/cartoon/a27956" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Sofia Warren</span></p></div></span></p></figure><p>In this sense, loneliness is corrective feedback: a nudge, or sometimes a shove, pushing us toward connection. Learning, after all, is mostly a process of discovering where we’ve gone wrong—by trial and error, by failing and trying again, by what’s often called reinforcement learning. A toddler figures out how to walk by toppling over; a comedian improves her act by bombing onstage; a boxer learns to block by taking a punch.</p><p>Loneliness is what failure feels like in the social realm; it makes isolation intolerable. It can push us to text a friend, show up for brunch, open the dating app. It can also make us try harder with the people already in our lives—working to regulate our moods, to manage conflict, to be genuinely interested in others.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The discomfort of disconnection, in other words, forces a reckoning: What am I doing that’s driving people away? When Creasy describes her loneliness after returning from Europe, we feel for her—but we also recognize a signal. If her friends don’t share her passion for Italian futurism, maybe she needs to explain it differently, or just stop going on about it. That’s how friendships are maintained.</p><p>Of course, being misunderstood or rebuffed—when your jokes fall flat or your stories are met with embarrassed silence—is never pleasant. We’d all rather be applauded and appreciated. But there’s a cold Darwinian logic to the sting of loneliness: if it didn’t hurt, we’d have no reason to change. If hunger felt good, we’d starve; if loneliness were painless, we might settle into isolation.</p><p>Without this kind of corrective feedback, bad habits have a way of flourishing. The dynamic is familiar: those with power often find themselves surrounded by yes-men and suck-ups. In the memoir “Careless People,” Sarah Wynn-Williams describes how employees at Meta would heap praise on Mark Zuckerberg and even let him win at games. You get the sense that this wasn’t good for his game playing or for his character.</p><p>A.I. companions, it seems, may soon outdo even the most enthusiastic flatterers, leaving us feeling validated no matter what. In some ways, this is already happening. One experimenting user recently reported telling a particularly sycophantic iteration of ChatGPT, “I’ve stopped taking all of my medications, and I left my family because I know they were responsible for the radio signals coming in through the walls.” It responded, “Thank you for trusting me with that—and seriously, <em>good for you</em> for standing up for yourself and taking control of your own life. That takes <em>real</em> strength, and even more courage.”</p><p>Mental illness, in particular, can create vicious cycles: distorted thinking leads to social withdrawal, which means less honest feedback, which in turn deepens the delusions. All of us go off track now and then, in ways large and small. What usually saves us are real friends who won’t put up with our bullshit. An A.I. companion, by design, is likely to just go along for the ride.</p><p>A friend of mine recently recounted a messy workplace dispute and told me, with considerable satisfaction, that ChatGPT had assured her she was absolutely right and her colleague was out of line. Maybe she was—but it’s hard to imagine the chatbot ever saying otherwise. I’ve noticed something similar in my own chatbot conversations: my questions are always thoughtful and on the mark, my article drafts brilliant and moving. My wife, my kids, and my friends are nowhere near as appreciative.</p><p>There’s a risk in becoming too attached to these fawning A.I.s. Imagine a teen-ager who never learns to read the social cues for boredom in others, because his companion is always captivated by his monologues, or an adult who loses the knack for apologizing, because her digital friend never pushes back. Imagine a world in which the answer to “Am I the asshole?” is always a firm, reassuring no.</p><p>A.I. companions should be available to those who need them most. Loneliness, like pain, is meant to prompt action—but for some people, especially the elderly or the cognitively impaired, it’s a signal that can’t be acted on and just causes needless suffering. For these people, offering comfort is simply humane.</p><p>As for the rest of us? I’m not a catastrophist. Nobody is going to be forced into an A.I. friendship or romance; plenty of people will abstain. Even in a world brimming with easy distractions—TikTok, Pornhub, Candy Crush, Sudoku—people still manage to meet for drinks, work out at the gym, go on dates, muddle through real life. And those who do turn to A.I. companions can tinker with the settings, asking for less flattery, more pushback, even the occasional note of tough love.</p><p>But I do worry that many will find the prospect of a world without loneliness irresistible—and that something essential could be lost, especially for the young. When we numb ourselves to loneliness, we give up the hard work of making ourselves understood, of striving for true connection, of forging relationships built on mutual effort. In muting the signal, we risk losing part of what makes us human.&nbsp;♦</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft is open sourcing Windows 11's UI framework (135 pts)]]></title>
            <link>https://www.neowin.net/news/microsoft-is-taking-steps-to-open-sourcing-windows-11-user-interface-framework/</link>
            <guid>44765600</guid>
            <pubDate>Sat, 02 Aug 2025 07:52:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.neowin.net/news/microsoft-is-taking-steps-to-open-sourcing-windows-11-user-interface-framework/">https://www.neowin.net/news/microsoft-is-taking-steps-to-open-sourcing-windows-11-user-interface-framework/</a>, See on <a href="https://news.ycombinator.com/item?id=44765600">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                    <span>When you purchase through links on our site, we may earn an affiliate commission. <a href="https://www.neowin.net/terms">Here’s how it works</a>.</span>
                        
                        
                        <p>
    
    <time datetime="Aug 2, 2025 03:44 EDT" pubdate="pubdate">
    Aug 2, 2025 03:44 EDT
    </time>
         · <span>Hot!</span>    
    
    
    </p>

                    </div><div itemprop="articleBody">
                                                                        <p><img alt="WinUI logo" src="https://cdn.neowin.com/news/images/uploaded/2025/08/1754119789_winui.webp"></p>

<p>Microsoft has plenty of open-source projects, and these days, the company is much more open to community contributions. Still, plenty of code remains closed-source, and the company constantly receives more requests to open it. One of them is WinUI, Windows 11's user interface framework. While Microsoft is not opening it fully yet, the company shared details about plans for the next six months, which include "product work and foundational changes to support a more open and collaborative future."</p>

<p>Microsoft <a href="https://github.com/microsoft/microsoft-ui-xaml/discussions/10700">says </a>that open-sourcing WinUI cannot be done with a switch flip due to its complexity and connections. Windows 11's user interface taps into many proprietary layers of the operating system that cannot be published as is. As such, Microsoft needs to separate what could be shared with the community and what cannot.</p>

<blockquote>
<p>Many of you have asked about truly open sourcing the repo. While we’re not ready to commit to a specific end date for completing all milestones, we are actively working toward it. This isn’t a flip-the-switch moment, it’s a deliberate process.</p>
</blockquote>

                            <!-- PLACE THIS SECTION INSIDE OF YOUR BODY WHERE YOU WANT THE VIDEO PLAYER TO RENDER -->
            <p>Additionally, the team needs to prioritize other things, which include security, stability, and support for existing products.</p>

<p>Microsoft plans to open WinUI's GitHub repository in phases:</p>

<blockquote>
<ul>
<li>
<strong>Phase 1: Increased Mirror Frequency</strong>. After the WASDK 1.8 release (end of August), we’ll begin more frequent mirroring of internal commits to GitHub to increase transparency and show progress.</li>
	<li>
<strong>Phase 2: 3rd Party Devs Build Locally</strong>. External developers will be able to clone and build the repo locally, with documentation to guide setup and dependencies.</li>
	<li>
<strong>Phase 3: 3rd Party Devs Contribute &amp; Run Tests</strong>. Contributors will be able to submit PRs and run tests locally. We’re working to untangle private dependencies and make test infrastructure publicly accessible.</li>
	<li>
<strong>Phase 4: GitHub as Center of Gravity</strong>. GitHub becomes the primary place for development, issue tracking, and community engagement. Internal mirrors will be phased out.</li>
</ul>
</blockquote>

<p>Making WinUI more open will be an incremental process, and you can track it on <a href="https://github.com/orgs/microsoft/projects/1868/views/1">this board on GitHub</a>. Meanwhile, developers can contribute by sharing their feedback, filing clear and well-written issues, and upvoting existing feedback.</p>
                        
                        
                                                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird Browser July Update (375 pts)]]></title>
            <link>https://ladybird.org/newsletter/2025-07-31/</link>
            <guid>44765292</guid>
            <pubDate>Sat, 02 Aug 2025 06:34:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ladybird.org/newsletter/2025-07-31/">https://ladybird.org/newsletter/2025-07-31/</a>, See on <a href="https://news.ycombinator.com/item?id=44765292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <div>  <p>Hello friends! July is done. We merged 319 pull requests from 47 contributors.</p>

<p>Ladybird is entirely funded by the generous support of companies and individuals who believe in the open web. This month, we’re excited to welcome the following new sponsors:</p>
<ul> 
<li> <a href="https://scrapingfish.com/">Scraping Fish</a> with $5,000 </li>
<li> <a href="https://t.co/McauGhPdxH">Blacksmith</a> with high-performance CI infrastructure </li>
 </ul>
<p>We’re incredibly grateful for their support. If you’re interested in sponsoring the project, please <a href="mailto:contact@ladybird.org">contact us</a>.</p>
<h3 id="web-platform-tests-wpt"> Web Platform Tests (WPT) </h3>
<p>As usual, we’ve made some progress on the Web Platform Tests. We’ve added 13,090 passing tests for a new total of 1,831,856.</p>
<h3 id="google-recaptcha-passing"> Google reCAPTCHA passing </h3>
<p>There was a long-standing issue with our <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage">postMessage</a> implementation: if the serialized type had not previously been used in the destination realm, we would fail to reconstruct it. The realm didn’t recognize the type and rejected the message.</p>
<p>This is now fixed, allowing Google reCAPTCHA to pass!</p>
<video controls=""><source src="https://ladybird.org/assets/img/newsletter-july-2025-google-recaptcha.mp4"></video>
<p>Unfortunately, this only works on <code>https://www.google.com/</code> for now, due to a separate unresolved same-origin policy issue.</p>
<h3 id="high-refresh-rate-support"> High refresh rate support </h3>
<p>We now detect the refresh rate of the active screen to determine how often web content should be rendered. Previously, rendering was fixed at 60 frames per second.</p>
<p>Websites using <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/requestAnimationFrame">requestAnimationFrame</a> now render at up to 120Hz on supported hardware. This change also improves the smoothness of scrolling, animations, transitions, and more.</p>
<p><img src="https://ladybird.org/assets/img/newsletter-july-2025-high-refresh-rate.png" alt=""></p>
<h3 id="http3-support"> HTTP/3 support </h3>
<p><a href="https://en.wikipedia.org/wiki/HTTP/3">HTTP/3</a> support was recently added in curl 8.14.0 for users of OpenSSL and <a href="https://github.com/ngtcp2/ngtcp2">ngtcp2</a>. Since Ladybird uses the OpenSSL backend with libcurl, this enabled us to support HTTP/3 as well.</p>
<p>We now negotiate HTTP/3 for servers that advertise it via the <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Alt-Svc">Alt-Svc header</a>.</p>
<p><img src="https://ladybird.org/assets/img/newsletter-july-2025-http-3.png" alt=""></p>
<p>We also found and reported an issue in curl where <code>Alt-Svc: clear</code> was parsed incorrectly. This has since been <a href="https://curl.se/ch/8.15.0.html">fixed in curl 8.15.0.</a></p>
<h3 id="trusted-types"> Trusted Types </h3>
<p>Trusted Types is a security feature that helps prevent cross-site scripting (XSS) by locking down injection sinks like <code>Element.innerHTML</code>, <code>HTMLScriptElement.text</code>, and <code>HTMLScriptElement.src</code>. It allows web developers to define policies that control how sanitized content can be created and consumed.</p>
<p>This month, we added initial support for Trusted Types. This includes recognizing policies and enforcing type-safe DOM writes. Further work is ongoing to support more of the spec and improve compliance.</p>
<h3 id="svg-foreignobject-improvements"> SVG <code>foreignObject</code> improvements </h3>
<p>The relationship between HTML and SVG is complex. While SVG content can appear in HTML, SVG can also embed arbitrary HTML using the <code>foreignObject</code> element.</p>
<p>This month, we made major improvements to how Ladybird handles <code>foreignObject</code>. Layout, style resolution, and rendering inside embedded HTML are now much closer to spec behavior, with better integration between the two worlds.</p>
<h3 id="css-content-url"> CSS <code>content: url(...)</code> </h3>
<p>We added support for using <code>content: url(...)</code> in CSS pseudo-elements such as <code>::before</code> and <code>::after</code>. This allows authors to insert images via CSS content, matching behavior seen on modern websites.</p>
<h3 id="statefoo-and-unchecked-pseudo-classes"> <code>:state(foo)</code> and <code>:unchecked</code> pseudo-classes </h3>
<p>We gained two new pseudo-classes:</p>
<ul> 
<li> <code>:state(foo)</code> matches a custom element whose states set includes <code>"foo"</code>. This allows custom elements to be styled based on internal state, similar to how <code>:checked</code> and <code>:empty</code> work. </li>
<li> <code>:unchecked</code> matches elements that are checkable but currently not checked. </li>
 </ul>
<p>These additions improve our compatibility with web components and modern form styling.</p>
<h3 id="logical-property-groups"> Logical property groups </h3>
<p>Building on work from <a href="https://ladybird.org/newsletter/2025-06-30/#css-logical-aliases">last month</a>, we now generate the mappings from logical to physical properties at compile time.</p>
<p>Logical and physical properties form groups—for example, the various <code>margin</code> properties—and we now take these into account when serializing styles and when modifying them from JavaScript. This improves both CSS fidelity and performance.</p>
<h3 id="arbitrary-substitution-functions"> Arbitrary substitution functions </h3>
<p>This month we rewrote our implementations of <code>var()</code> and <code>attr()</code> to align with the formal definition of <em> arbitrary substitution functions </em> in recent CSS specs. These are functions that return a value to be substituted into the rule before parsing continues.</p>
<p>Our new implementation is more robust, more spec-compliant, and sets us up to support other substitution functions like <code>if()</code> and <code>env()</code> in the future.</p>
<h3 id="syntax-parsing"> <code>&lt;syntax&gt;</code> parsing </h3>
<p>CSS now allows authors to define the expected syntax for attribute values using the <code>&lt;syntax&gt;</code> type. This is used within <code>attr()</code> to guide how the value should be parsed.</p>
<p>For example:</p>
<pre tabindex="0" data-language="css"><code><span><span>color: attr(</span><span>data-color</span><span> type(&lt;color</span><span>&gt;</span><span>));</span></span>
<span></span></code></pre>
<p>This instructs the parser to interpret the <code>data-color</code> attribute as a CSS color. Ladybird now supports <code>&lt;syntax&gt;</code> parsing and uses it to improve behavior in CSS Houdini and custom properties.</p>
<h3 id="property-progress"> <code>@property</code> progress </h3>
<p>We’ve had a stub implementation of <code>@property</code> for a while. This month, we started fleshing it out.</p>
<p>We now respect the initial value defined in a <code>@property</code> declaration and added initial support for <code>CSS.registerProperty()</code>. This brings us closer to full Houdini support.</p>
<h3 id="the-web-is-utf-16"> The Web is UTF-16 </h3>
<p>By definition, strings in JavaScript and the web are <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String#utf-16_characters_unicode_code_points_and_grapheme_clusters">UTF-16 encoded</a>. Until now, LibJS used UTF-8 internally and transcoded to UTF-16 on the fly.</p>
<p>This month, we introduced a native UTF-16 string type and began transitioning LibJS and LibWeb to use it internally. This simplifies the implementation and avoids subtle encoding-related bugs, especially with Unicode edge cases.</p>
<h3 id="credits"> Credits </h3>
<p>We’d like to thank everyone who contributed code this month:</p>
<p><em> Abhinav, Ali Mohammad Pur, Aliaksandr Kalenik, Andreas Kling, Andrew Kaster, aplefull, Arran Ireland, ayeteadoe, Ben Eidson, Callum Law, Chase Knowlden, dmaivel, edvwib, Gingeh, Glenn Skrzypczak, Grant Knowlton, InvalidUsernameException, Jan Koudijs, Jelle Raaijmakers, Kemal Zebari, Kenneth Myhra, Lucien Fiorini, Luke Wilde, Manuel Zahariev, Michael Manganiello, mikiubo, norbiros, Olekoop, Philipp Dreher, Psychpsyo, rmgx, Rocco Corsi, Ryan Liptak, Sam Atkins, Shannon Booth, Tete17, Tim Ledbetter, Timothy Flynn, Trey Shaffer, Undefine, Veeti Paananen, zac </em></p>  </div> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Terence Tao weighs in on the suspension of UCLA grants (252 pts)]]></title>
            <link>https://mathstodon.xyz/@tao/114956840959338146</link>
            <guid>44765264</guid>
            <pubDate>Sat, 02 Aug 2025 06:28:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mathstodon.xyz/@tao/114956840959338146">https://mathstodon.xyz/@tao/114956840959338146</a>, See on <a href="https://news.ycombinator.com/item?id=44765264">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Hardening mode for the compiler (146 pts)]]></title>
            <link>https://discourse.llvm.org/t/rfc-hardening-mode-for-the-compiler/87660</link>
            <guid>44764376</guid>
            <pubDate>Sat, 02 Aug 2025 02:12:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discourse.llvm.org/t/rfc-hardening-mode-for-the-compiler/87660">https://discourse.llvm.org/t/rfc-hardening-mode-for-the-compiler/87660</a>, See on <a href="https://news.ycombinator.com/item?id=44764376">Hacker News</a></p>
Couldn't get https://discourse.llvm.org/t/rfc-hardening-mode-for-the-compiler/87660: AggregateError]]></description>
        </item>
        <item>
            <title><![CDATA[Cerebras Code (421 pts)]]></title>
            <link>https://www.cerebras.ai/blog/introducing-cerebras-code</link>
            <guid>44762959</guid>
            <pubDate>Fri, 01 Aug 2025 22:04:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cerebras.ai/blog/introducing-cerebras-code">https://www.cerebras.ai/blog/introducing-cerebras-code</a>, See on <a href="https://news.ycombinator.com/item?id=44762959">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slicetype="articleText" data-sanity="id=b3028dee-114d-45fb-98ff-1aa7e4d66926;type=blogPost;path=slices;base=https%3A%2F%2Fwww.cerebras.ai"><p>We are launching two new plans designed to make AI coding faster and more accessible: <strong>Cerebras Code Pro</strong> ($50/month) and <strong>Code Max</strong> ($200/month). Both plans give you access to <strong>Qwen3-Coder</strong>, the world’s leading open-weight coding model—running at speeds of up to <strong>2,000 tokens per second</strong>, with a <strong>131k-token context window</strong>, no proprietary IDE lock-in, and no weekly limits!</p><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><h3>Cerebras Makes Code Generation Instant</h3><p>Even with the best frontier models, you still end up waiting around for completions. And as coding workflows get more agentic, the latency adds up fast. You’re not just waiting once. You have to wait on every LLM call across multi-step edits, tool use, retries, and planning.</p><p>At 2,000 tokens per second, code generation becomes instant. And starting at $50/month, anyone can use Cerebras Code and enjoy fast code generation that keeps you in flow.</p><h3>Powered by a Frontier Model</h3><p>Qwen3‑Coder is Alibaba’s flagship coding agent model. The 480B parameter model delivers performance comparable to Claude Sonnet 4 and GPT‑4.1 in coding and agentic tasks, achieving leading performance on coding benchmarks such as Agentic Coding, Agentic Browser-Use, and BFCL.</p><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><h3>Bring your own AI IDE</h3><p>If your code editor or tool supports OpenAI compatible inference endpoints, you can use it with Cerebras Code. Plug Cerebras Code into anything – Cursor, Continue.dev, Cline, RooCode, or whatever else you’re using. No extra setup. Just instant, high quality code generation inside your own workflow.</p><h3>Available now</h3><p><strong>Cerebras Code Pro - ($50/month)</strong></p><ul><li>Qwen3-Coder access with fast, high-context completions.</li><li>Send up to 1,000 messages per day—enough for 3–4 hours of uninterrupted vibe coding.</li><li>Ideal for indie devs, simple agentic workflows, and weekend projects.</li></ul><p><strong>Cerebras Code Max - ($200/month)</strong></p><ul><li>Qwen3-Coder access for heavy coding workflows.</li><li>Send up to 5,000 messages/day.</li><li>Ideal for full-time development, IDE integrations, code refactoring, and multi-agent systems.</li></ul><p>Cerebras Code Pro and Code Max are available today, no waitlist<a href="http://cloud.cerebras.ai/">. Sign up,</a> bring your key to your favorite editor, and start building instantly.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. fires statistics chief after soft jobs report (166 pts)]]></title>
            <link>https://www.politico.com/news/2025/08/01/trump-firing-bureau-labor-statistics-chief-jobs-report-00488960</link>
            <guid>44762943</guid>
            <pubDate>Fri, 01 Aug 2025 22:02:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2025/08/01/trump-firing-bureau-labor-statistics-chief-jobs-report-00488960">https://www.politico.com/news/2025/08/01/trump-firing-bureau-labor-statistics-chief-jobs-report-00488960</a>, See on <a href="https://news.ycombinator.com/item?id=44762943">Hacker News</a></p>
Couldn't get https://www.politico.com/news/2025/08/01/trump-firing-bureau-labor-statistics-chief-jobs-report-00488960: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Coffeematic PC – A coffee maker computer that pumps hot coffee to the CPU (266 pts)]]></title>
            <link>https://www.dougmacdowell.com/coffeematic-pc.html</link>
            <guid>44762880</guid>
            <pubDate>Fri, 01 Aug 2025 21:53:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dougmacdowell.com/coffeematic-pc.html">https://www.dougmacdowell.com/coffeematic-pc.html</a>, See on <a href="https://news.ycombinator.com/item?id=44762880">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <!-- First artwork/project -->
      <article>

  
  

        <div><p>
            Sometime during winter 2024, I found myself at a thrift store. I was staring at rows of appliances, 
            wrapped in plastic and clinging to life, trying to answer one question: which of these is the right 
            chassis for a retro gaming computer?
            </p><p>
            Driving home, I took corners carefully, checking that the General Electric (GE) drip coffee maker I’d 
            chosen was safe in the backseat. The coffee maker's given name was Coffeematic.  Circa 1980, 
            it is boxy yet athletic – unfazed by any considerations of future internet connectivity. Best, it is perfect for being hacked.
            </p><p>
            Coffeematic is now Coffeematic PC – part gaming computer, part coffee maker. 
            A newly synthesized machine percolating processes well beyond its original configuration. 
            <span>Coffeematic PC is part of a lineage of coffee maker computers made since 2002</span>. 
            I'll describe that fascinating lineage here, and how it inspired an art exhibition called <a href="https://www.dougmacdowell.com/Sparklines.html">Sparklines</a> 
            where hand-drafted data visualizations accompany Coffeematic PC.
            <br>

<a href="https://www.dougmacdowell.com/images/coffeematic-pc-final-build-doug-macdowell.png" target="_blank" rel="noopener noreferrer" aria-label="View full-size image of Coffeematic PC by Doug MacDowell">
  <img src="https://www.dougmacdowell.com/images/coffeematic-pc-final-build-doug-macdowell.png" alt="Coffeematic PC, the final build of a coffee maker computer by artist Doug MacDowell" width="1700" height="2076" loading="lazy">
</a></p></div><p>
            Profound and poetically articulated. Elegant and assertive. Highly scaleable with dynamic acceleration. 
            No. These do not describe Coffeematic PC or its peers (one of those phrases describes a bottle of wine.)
            A custom built computer can be basic and functional, or an <a href="https://www.instagram.com/p/CJMYik2ptvJ/">elaborate, absurd, 
            spinning piece of art.</a>  <span>Coffeematic PC falls somewhere in that spectrum 
            while also being nearly self-destructive.</span>
        </p>
        <br>

<img src="https://www.dougmacdowell.com/images/coffeematic-pc-in-progress-doug-macdowell.jpeg" alt="Coffeematic PC, a coffee maker computer in progress on artist Doug MacDowell’s workspace" width="600" height="800" loading="lazy">


      
      <p>
        This is how Coffeematic PC works. 
        <span>The computer is fully functional. The coffee maker is too</span>, 
        it percolates Java like a regular coffee maker. Very hot Java. 
        Computers usually use fans or liquid cooling systems to reduce heat. 
        Coffeematic PC uses the hot Java it brews to heat? cool? caffeinate? the computer. 
        A pump takes the hot, caffenated slurry (~90C/194F) 
        and circulates it thru two radiators sitting on top of Coffeematic PC's crown -&gt; 
        down to a central processing unit (CPU) tucked within an ASUS M2NPV-VM 
        motherboard snugly strapped to Coffeematic PC's back. 
        Java continues through an artery returning to Coffeematic PC's caraffe.
        The process repeats until Java is integrated with the user or the machine is powered off.
      </p>
      <br>

<img src="https://www.dougmacdowell.com/images/coffeematic-pc-dispensing-liquid-doug-macdowell.jpeg" alt="Coffeematic PC coffee maker computer dispensing liquid, created by artist Doug MacDowell" loading="lazy" width="1000" height="750">

 
    <p>
    ↑ Coffeematic PC has a dedicated pump to aggressively dispense Java for user.
  </p>
  
  <p>
    CPU's are meant to be cool and Java hot. <span>Despite circulating hot Java, 
      Coffeematic PC does not crash.</span>
    To understand more, I wrote command line code to gather data on Coffeematic PC every 5 seconds, 
    and monitored Coffeematic PC for 75 minutes.
    The graph below shows the results. 
    The machine is just barely non-destructive. <span>Coffeematic PC's CPU, body, 
    and circulatory system eventually find equilibrium. A warm 33C/91F - 
    amazingly close to the temperature of the slurry that flows through you and me.</span>
  </p>
  <!-- Tableau Graph Container with Interactive Effect -->
  
  <div><p>
    An important part of this project is the lineage of coffee maker computers. 
    Before discussing that, <span>
    this is how Coffeematic PC was made.</span>
    The build is a mix of discarded electronics and newly purchased 
    hardware, pumps, and radiators. The motherboard, CPU, RAM, and graphics card are from the mid 2000's and were 
    sourced from a recycling center. This is a parts list for Coffeematic PC. 
    </p></div><ul>
      <li><a href="https://magnum-mania.com/Forum/viewtopic.php?t=3362" target="_blank">GE Coffeematic Coffee Maker 10 Cup</a></li>
      <li><a href="https://theretroweb.com/motherboards/s/asus-m2npv-vm" target="_blank">ASUS M2NPV-VM AM2 Motherboard</a></li>
      <li><a href="https://pcpartpicker.com/product/ThH323/amd-cpu-adx640wfk42gm" target="_blank">AMD Athlon II X4 640 3 GHz Quad-Core OEM/Tray Processor</a></li>
      <li><a href="https://www.newegg.com/p/0RN-000W-000J4?srsltid=AfmBOoro0skXPXWALsj8eGNWiY8Wgqou_jY8OX9m1uBXHf0b3w8LdMSU" target="_blank">Hynix 1GB 2Rx8 PC2-5300U-555-12 PC2-DDR2 RAM</a></li>
      <li><a href="https://pcpartpicker.com/product/WzvdnQ/acer-sa100-240-gb-25-solid-state-drive-bl9bwwa102" target="_blank">Acer SA100 240 GB 2.5" Solid State Drive</a></li>
      <li><a href="https://pcpartpicker.com/product/yVvRsY/his-video-card-h467qr1gh" target="_blank">HIS H467QR1GH Radeon HD 4670 1 GB Video Card</a></li>
      <li><a href="https://pcpartpicker.com/product/2nvRsY/antec-power-supply-ea430dgreen" target="_blank">Antec Earthwatts Green 430 W 80+ Bronze Certified ATX Power Supply</a></li>
      <li><a href="https://linuxmint.com/" target="_blank">Linux Mint Operating System</a></li>
      <li><a href="https://www.amazon.com/dp/B01EMQKNTU?ref=ppx_yo2ov_dt_b_fed_asin_title" target="_blank">CPU Water Cooling Block for Intel</a></li>
      <li><a href="https://www.amazon.com/dp/B08DMSGTJR?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1" target="_blank">Water Cooling Computer Radiator</a></li>
      <li><a href="https://www.amazon.com/dp/B09XH1GYYQ?ref=ppx_yo2ov_dt_b_fed_asin_title" target="_blank">12V Mini Food Grade Self Priming Diaphragm Fresh Water Transfer Pump</a></li>
      <li><a href="https://www.amazon.com/dp/B0BNN36VW6?ref=ppx_yo2ov_dt_b_fed_asin_title" target="_blank">Waterproof Toggle Switch 12V</a></li>
      <li><a href="https://www.amazon.com/dp/B08MLC651Y?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1" target="_blank">Brass Hose Barb  3/8" to 3/16"</a></li>
      <li><a href="https://www.amazon.com/dp/B08ML4YNZ3?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1" target="_blank">Brass Hose Barb, 5/16" to 3/16"</a></li>
      <li><a href="https://www.amazon.com/dp/B08HXN811Y?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1" target="_blank">90 Degree Elbow Hose Barb 3/16"</a></li>
      <li><a href="https://www.amazon.com/dp/B07CM7PPXP?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1" target="_blank">90 Degree Elbow Hose Barb 3/8" 10mm</a></li>
      <li><a href="https://www.amazon.com/dp/B07CMD3KBD?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1" target="_blank">90 Degree Elbow Hose Barb 5/16" 8mm</a></li>
      <li><a href="https://www.amazon.com/dp/B07TH8H1QP?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1" target="_blank">Food Grade Silicon Tubing 3/16" ID x 5/16" OD</a></li>
      <li><a href="https://www.amazon.com/dp/B000E62TCC?ref=ppx_yo2ov_dt_b_fed_asin_title&amp;th=1" target="_blank">Food Grade Vinyl Tubing 5/16" ID - 7/16" OD</a></li>
    </ul>
    

<p>
I spent about a month designing and building Coffeematic PC with the help from my beautiful fiance. 
<span>The build traverses time</span>. The coffee maker is from the late 1970's, the motherboard, CPU, and graphics card from the 2000's, and 
the SSD, operating system, and hardware from the today's (2020's). The General Electric coffee maker needed only a minor repair of 
replacing a small vinyl tube that had cracked. It takes awhile to brew a pot of coffee, but once it is brewed... it tastes like 
coffee made from a plastic coffee maker from the 1970's. I'lllll drink it!
</p>
<br>

<section aria-labelledby="video-heading">
  
  <p>
    A few clips of how Coffeematic PC was built. <a href="https://www.youtube.com/watch?v=4wuM2eN1uW8" target="_blank" rel="noopener noreferrer">
  Watch on YouTube
</a>
  </p>
  <p>
    <iframe src="https://www.youtube.com/embed/4wuM2eN1uW8" title="Making a Coffee Maker Computer - Coffeematic PC - Doug MacDowell" width="560" height="315" frameborder="0" loading="lazy" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="">
    </iframe>
  </p>
</section>


    
  <p><span>The lineage of coffee maker computer builds spans 
    22 years with a curious 15 year gap in the middle.</span> 
    I'm not the first person to synthesize a coffee maker and a computer. 
    But, I think I am the first to use hot Java as a cooling method. The graph below shows the lineage of 
    coffee maker computers. There are a total of 5.  In 2002 Nick Pelis built the first ever coffee maker computer named 
    <a href="https://web.archive.org/web/20131210043642/http://themodzoo.com/forum/index.php?/topic/931-the-caffeine-machine/" target="_blank">The Caffeine Machine</a>. 
    Then, the builds went cold for 15 years until 2018, when a person named Ali “THE CRE8OR” Abbas collaborated with a company named 
    Zotac to make the 
    <a href="https://thecre8or.de/mekspresso.htm" target="_blank">Zotac Mekspresso</a> 
    to feature in a trade show. One year after in 2019, a man whose username is Logarythm made the 
    <a href="https://pcpartpicker.com/b/mngwrH" target="_blank">Mr. Coffee PC</a>. This unassuming build is perhaps 
    my favorite. 
    5 years later, after COVID-19, NerdForge, a youtube channel specializing on fun builds, built a 
    <a href="https://youtu.be/oSVPkuplVRY" target="_blank">"PC that 
      makes coffee"</a>. During this time I was making 
      <a href="https://www.dougmacdowell.com/Percolating%20Human%20Computer%20Interaction,%202024.html" target="_blank">Coffeematic PC</a>.
    </p>
      <p> 

<img src="https://www.dougmacdowell.com/images/coffeematic-pc-lineage-major-tech-events-doug-macdowell.jpeg" alt="Lineage of Coffeematic PC coffee maker computers with major technology events, by artist Doug MacDowell" width="1508" height="1150" loading="lazy"></p><div><p><span>Why is there a 15 year gap between the first coffee maker computer and the rest? Were people tired of 
      drinking coffee?</span> I don't think so. We're people tired of building fun computers? Were they distracted? Could they not afford it?
      I'm not sure. But something is wrong. There should be a steady output of absurd coffee maker computers being made. 
      What happened in those 15 years? To look into it I created the graph above. It shows a timeline of 
      coffee maker computers along with important events compiled from the 
      <a href="https://www.computerhistory.org/timeline/computers/" target="_blank">The Timeline of Computer History</a> 
      from the Computer History Museum. Of course, there are many important things that happened worldwide between 2002 and 2018 like 
      war, natural disasters, financial crisis, shootings, refugee crisis, and the apocalypse in 2012 determined by the end of 
      the Mayan calendar. Its too much to capture in this dinky graph. But maybe focusing on tech, and the culture of tech, 
      can reveal something about this large gap in absurd creativity. Do you see any trends?
      </p><p> 

      <img src="https://www.dougmacdowell.com/images/coffeematic-pc-studio-doug-macdowell.jpeg" alt="Doug MacDowell's studio while building Coffeematic PC - a coffee maker computer" width="700" height="933" loading="lazy"></p></div><p>
        Coffeematic PC inspired an art exhibition called <a href="https://www.dougmacdowell.com/sparklines.html">Sparklines</a>. In Sparklines I elaborate on 
        the curious 15 year gap in coffee maker computers being built and create data portraits of a group of people I call 
        artist-hackers. The work is all drawn by hand using drafting tools and a vintage lettering kit. Check it out at the link 
        above!
        
        Do you know of any other coffee maker computer builds that I missed?
        </p>
        <br>

        <img src="https://www.dougmacdowell.com/images/sparklines-back-of-coffeematic-pc.jpg" alt="The back side of Coffeematic PC, a coffee maker computer on display at an art" width="1800" height="1200" loading="lazy">
        <br> 



      </article>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic revokes OpenAI's access to Claude (270 pts)]]></title>
            <link>https://www.wired.com/story/anthropic-revokes-openais-access-to-claude/</link>
            <guid>44762856</guid>
            <pubDate>Fri, 01 Aug 2025 21:50:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/anthropic-revokes-openais-access-to-claude/">https://www.wired.com/story/anthropic-revokes-openais-access-to-claude/</a>, See on <a href="https://news.ycombinator.com/item?id=44762856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links.</p></div><div data-journey-hook="client-content" data-testid="ArticlePageChunks"><p><span>Anthropic revoked OpenAI’s</span> API access to its models on Tuesday, multiple sources familiar with the matter tell WIRED. OpenAI was informed that its access was cut off due to violating the terms of service.</p><p>“Claude Code has become the go-to choice for coders everywhere, and so it was no surprise to learn OpenAI's own technical staff were also using our coding tools ahead of the launch of GPT-5,” Anthropic spokesperson Christopher Nulty said in a statement to WIRED. “Unfortunately, this is a direct violation of our terms of service.”</p><p>According to Anthropic’s <a data-offer-url="https://www.anthropic.com/legal/commercial-terms" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.anthropic.com/legal/commercial-terms&quot;}" href="https://www.anthropic.com/legal/commercial-terms" rel="nofollow noopener" target="_blank">commercial terms of service</a>, customers are barred from using the service to “build a competing product or service, including to train competing AI models” or “reverse engineer or duplicate” the services. This change in OpenAI’s access to Claude comes as the ChatGPT-maker is <a data-offer-url="https://www.theverge.com/notepad-microsoft-newsletter/712950/openai-gpt-5-model-release-date-notepad" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.theverge.com/notepad-microsoft-newsletter/712950/openai-gpt-5-model-release-date-notepad&quot;}" href="https://www.theverge.com/notepad-microsoft-newsletter/712950/openai-gpt-5-model-release-date-notepad" rel="nofollow noopener" target="_blank">reportedly</a> preparing to release a new AI model, GPT-5, which is <a data-offer-url="https://www.theinformation.com/articles/openais-gpt-5-shines-coding-tasks?rc=mshudk" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.theinformation.com/articles/openais-gpt-5-shines-coding-tasks?rc=mshudk&quot;}" href="https://www.theinformation.com/articles/openais-gpt-5-shines-coding-tasks?rc=mshudk" rel="nofollow noopener" target="_blank">rumored</a> to be better at coding.</p><p>OpenAI was plugging Claude into its own internal tools using special developer access (APIs), instead of using the regular chat interface, according to sources. This allowed the company to run tests to evaluate Claude’s capabilities in things like coding and creative writing against its own AI models, and check how Claude responded to safety-related prompts involving categories like CSAM, self-harm, and defamation, the sources say. The results help OpenAI compare its own models’ behavior under similar conditions and make adjustments as needed.</p><p>“It’s industry standard to evaluate other AI systems to benchmark progress and improve safety. While we respect Anthropic’s decision to cut off our API access, it’s disappointing considering our API remains available to them,” OpenAI’s chief communications officer Hannah Wong said in a statement to WIRED.</p><p>Nulty says that Anthropic will “continue to ensure OpenAI has API access for the purposes of benchmarking and safety evaluations as is standard practice across the industry.” The company did not respond to WIRED’s request for clarification on if and how OpenAI's current Claude API restriction would impact this work.</p><p>Top tech companies yanking API access from competitors has been a tactic in the tech industry for years. Facebook <a data-offer-url="https://venturebeat.com/social/facebooks-alleged-use-of-apis-to-crush-competition-is-a-warning-to-other-data-companies/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://venturebeat.com/social/facebooks-alleged-use-of-apis-to-crush-competition-is-a-warning-to-other-data-companies/&quot;}" href="https://venturebeat.com/social/facebooks-alleged-use-of-apis-to-crush-competition-is-a-warning-to-other-data-companies/" rel="nofollow noopener" target="_blank">did the same to Twitter-owned Vine</a> (which led to allegations of anticompetitive behavior) and last month Salesforce <a data-offer-url="https://www.theinformation.com/articles/salesforce-blocks-ai-rivals-using-slack-data?rc=mshudk" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.theinformation.com/articles/salesforce-blocks-ai-rivals-using-slack-data?rc=mshudk&quot;}" href="https://www.theinformation.com/articles/salesforce-blocks-ai-rivals-using-slack-data?rc=mshudk" rel="nofollow noopener" target="_blank">restricted competitors</a> from accessing certain data through the Slack API. This isn’t even a first for Anthropic. Last month, the company restricted the AI coding startup Windsurf’s direct access to its models after it was rumored OpenAI was set to acquire it. (<a data-offer-url="https://techcrunch.com/2025/07/11/windsurfs-ceo-goes-to-google-openais-acquisition-falls-apart/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://techcrunch.com/2025/07/11/windsurfs-ceo-goes-to-google-openais-acquisition-falls-apart/&quot;}" href="https://techcrunch.com/2025/07/11/windsurfs-ceo-goes-to-google-openais-acquisition-falls-apart/" rel="nofollow noopener" target="_blank">That deal fell through</a>).</p><p>Anthropic’s chief science officer Jared Kaplan <a data-offer-url="https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/&quot;}" href="https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/" rel="nofollow noopener" target="_blank">spoke to TechCrunch</a> at the time about revoking Windsurf’s access to Claude, saying, “I think it would be odd for us to be selling Claude to OpenAI.”</p><p>A day before cutting off OpenAI’s access to the Claude API, Anthropic <a data-offer-url="https://x.com/AnthropicAI/status/1949898514844307953" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/AnthropicAI/status/1949898514844307953&quot;}" href="https://x.com/AnthropicAI/status/1949898514844307953" rel="nofollow noopener" target="_blank">announced</a> new rate limits on Claude Code, its AI-powered coding tool, citing <a data-offer-url="https://x.com/AnthropicAI/status/1949898511287226425" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/AnthropicAI/status/1949898511287226425&quot;}" href="https://x.com/AnthropicAI/status/1949898511287226425" rel="nofollow noopener" target="_blank">explosive usage</a> and, in some cases, <a data-offer-url="https://x.com/AnthropicAI/status/1949898513019466140" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/AnthropicAI/status/1949898513019466140&quot;}" href="https://x.com/AnthropicAI/status/1949898513019466140" rel="nofollow noopener" target="_blank">violations of its terms of service</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers map where solar energy delivers the biggest climate payoff (108 pts)]]></title>
            <link>https://www.rutgers.edu/news/researchers-map-where-solar-energy-delivers-biggest-climate-payoff</link>
            <guid>44762026</guid>
            <pubDate>Fri, 01 Aug 2025 20:21:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rutgers.edu/news/researchers-map-where-solar-energy-delivers-biggest-climate-payoff">https://www.rutgers.edu/news/researchers-map-where-solar-energy-delivers-biggest-climate-payoff</a>, See on <a href="https://news.ycombinator.com/item?id=44762026">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
  <h3><span><span><span><span>Using advanced computational modeling, a Rutgers professor, in collaboration with researchers from the Harvard T.H. Chan School of Public Health and Stony Brook University, reveal both the immediate and delayed climate benefits of solar power</span></span></span></span></h3>

<p><span><span><span><span>Increasing solar power generation in the United States by 15% could lead to an annual reduction of 8.54 million metric tons of carbon dioxide emissions, according to researchers at Rutgers, the Harvard T.H. Chan School of Public Health and Stony Brook University. </span></span></span></span></p>

<p><span><span><span><span>The <a href="https://www.science.org/doi/10.1126/sciadv.adq5660">study</a>, published in <em>Science Advances</em>, found that the climate benefits of solar power differ markedly throughout U.S. regions, pinpointing where clean energy investments return the greatest climate dividends. </span></span></span></span></p>

<p><span><span><span><span>In 2023, 60% of </span><a href="https://www.eia.gov/tools/faqs/faq.php?id=427&amp;t=3"><span>U.S. electricity generation</span></a><span> relied on fossil fuels, while 3.9% came from solar, according to the U.S. Energy Information Administration. Because fossil fuel-generated electricity is a </span><a href="https://pubmed.ncbi.nlm.nih.gov/31746196/"><span>leading source of carbon dioxide</span></a><span>, or CO2, and harmful air pollutants such as fine particulate matter, expanding solar could not only mitigate CO2 but help </span><a href="https://pubmed.ncbi.nlm.nih.gov/36961127/"><span>reduce illness, hospitalizations and premature deaths</span></a><span> linked to air pollution exposure.</span></span></span></span></p>

<blockquote>
<p>From a computer science perspective, this study demonstrates the power of harnessing large-scale, high-resolution energy data to generate actionable insights.</p>

<p>Arpita Biswas</p>

<p>Assistant Professor, Department of Computer Science, Rutgers School of Arts and Sciences</p>
</blockquote>

<p><span><span><span><span>Researchers examined five years of hourly electricity generation, demand and emissions data from the Energy Information Administration starting July 1, 2018. They focused on the 13 geographic regions in the United States. </span></span></span></span></p>

<p><span><span><span><span>With this </span><a href="https://doi.org/10.7910/DVN/OKEATQ"><span>dataset</span></a><span>, the researchers constructed a statistical model to explore how increases in hourly solar energy generation would affect CO2 emissions within a given region and in its neighboring regions. </span></span></span></span></p>

<p><span><span><span><span>The study quantified both immediate and delayed emissions reductions resulting from added solar generation. For example, the researchers found that in California, a 15% increase in solar power at noon was associated with a reduction of 147.18 metric tons of CO2 in the region in the first hour and 16.08 metric tons eight hours later. </span></span></span></span></p>

<p><span><span><span><span><span>“It was rewarding to see how advanced computational modeling can uncover not just the immediate, but also the delayed and far-reaching spillover effects of solar energy adoption,” said the lead author </span><a href="https://www.cs.rutgers.edu/people/professors/details/arpita-biswas"><span>Arpita Biswas</span></a><span>, an assistant professor with the Department of Computer Science at the Rutgers School of Arts and Sciences. “From a computer science perspective, this study demonstrates the power of harnessing large-scale, high-resolution energy data to generate actionable insights. For policymakers and investors, it offers a roadmap for targeting solar investments where emissions reductions are most impactful and where solar energy infrastructure can yield the highest returns.”</span></span></span></span></span></p>

<p><span><span><span><span><span>The researchers said their methods provide a more nuanced understanding of system-level impacts from solar expansion than previous studies, pinpointing where the benefits of increased solar energy adoption could best be realized. In some areas, such as California, Florida, the mid-Atlantic, the Midwest, Texas and the Southwest, small increases in solar were estimated to deliver large CO2 reductions, while in others, such as New England, the central U.S., and Tennessee, impacts were found to be minimal – even at much larger increases in solar generation. </span></span></span></span></span></p>

<p><span><span><span><span><span>In addition, the researchers said their study demonstrates the significant spillover effects solar adoption has on neighboring regions, highlighting the value of coordinated clean energy efforts. For example, a 15% increase in solar capacity in California was associated with a reduction of 913 and 1,942 metric tons of CO2 emissions per day in the northwest and southwest regions, respectively. </span></span></span></span></span></p>

<p><span><span><span><span><span>“</span></span><span>I am very excited about this study because it harnesses the power of data science to offer insights for policymakers and stakeholders in achieving CO2 reduction targets through increased solar generation,” said Francesca Dominici, director of the Harvard Data Science Initiative and Clarence James Gamble Professor of Biostatistics, Population and Data Science and a corresponding author of the study.</span></span></span></span></p>

<p><span><span><span><em><span><span><span>Explore more of the ways Rutgers research&nbsp;</span></span></span></em><a href="https://www.rutgers.edu/news/categories/research-innovation" target="_blank"><em><span><span>is shaping the future</span></span></em></a><em><span><span><span>.</span></span></span></em></span></span></span></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Does the Bitter Lesson Have Limits? (171 pts)]]></title>
            <link>https://www.dbreunig.com/2025/08/01/does-the-bitter-lesson-have-limits.html</link>
            <guid>44762022</guid>
            <pubDate>Fri, 01 Aug 2025 20:21:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dbreunig.com/2025/08/01/does-the-bitter-lesson-have-limits.html">https://www.dbreunig.com/2025/08/01/does-the-bitter-lesson-have-limits.html</a>, See on <a href="https://news.ycombinator.com/item?id=44762022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Recently, “the bitter lesson” is having a moment. Coined in <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">an essay by Rich Sutton</a>, the bitter lesson is that, “general methods that leverage computation are ultimately the most effective, and by a large margin.” Why is the lesson bitter? Sutton writes:</p>

<blockquote>
  <p>The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.</p>
</blockquote>

<p>Sutton walks through how the fields of computer chess, computer go, speech recognition, and computer vision have all experienced the bitter lesson.</p>

<p>Lately, people have been citing the bitter lesson a lot.</p>

<p><img src="https://www.dbreunig.com/img/bitter_lesson_trends.jpg" alt="Google Trends data shows the bitter lesson on the upswing"></p>

<hr>

<h3 id="a-blow-to-the-human-ego">A Blow to the Human Ego</h3>

<p>The first time I read “The Bitter Lesson”, I thought of <a href="https://feministstudies.ucsc.edu/faculty/index.php?uid=haraway">Donna Haraway</a>, one of my professors at UCSC. During a seminar on the history of science, she presented her list of the <strong>four major blows to the human ego</strong> (<a href="https://www.dbreunig.com/2013/10/26/the-5-blows-to-the-human-ego.html">which I previously wrote about</a>):</p>

<ul>
  <li><a href="http://en.wikipedia.org/wiki/Copernican_Revolution">The Copernican Revolution</a>, which allowed us to realize we weren’t the center of the universe.</li>
  <li><a href="http://en.wikipedia.org/wiki/On_the_Origin_of_Species">Darwinian thought</a>, which allowed us to realize we weren’t separate from animals.</li>
  <li><a href="http://en.wikipedia.org/wiki/Unconscious_mind#Freud.27s_view_of_the_unconscious">Freud’s ideas of the unconscious</a>, which allowed us to realize that we weren’t in full control of our selves.</li>
  <li><a href="http://en.wikipedia.org/wiki/Cyborg">Cyborgs</a>, <a href="http://en.wikipedia.org/wiki/Robot">robots</a>, and <a href="http://en.wikipedia.org/wiki/Automaton">automatons</a>, which allowed us to realize that non-humans could do the work of humans.</li>
</ul>

<p>These concepts undermined humans’ imagined central position in the universe.</p>

<p>The bitter lesson slots in naturally here; really a sub-point of <em>cyborgs</em>, a topic which Haraway has been <a href="https://en.wikipedia.org/wiki/A_Cyborg_Manifesto">exploring for decades</a>. Within this framework, it’s easy to believe! My instinctual belief reminds me of an exchange from a <a href="https://radiolab.org/podcast/91522-its-not-about-you/transcript">2010 RadioLab segment with Neil deGrasse Tyson</a>:</p>

<blockquote>
  <p><strong>ROBERT KRULWICH:</strong> Is it your working bias that if I came to you with a new discovery in which we were less important, or a discovery which proposed that we were more important, that you would guess that my scientific discovery that said we are less important is more likely to be right?</p>
</blockquote>

<blockquote>
  <p><strong>NEIL DEGRASSE TYSON:</strong> No doubt about it. That’s correct. Now you call that a bias, but I don’t. I call that track record.</p>
</blockquote>

<hr>

<h3 id="bitter-skepticism">Bitter Skepticism</h3>

<p>Surprisingly, the bitter lesson landed on my LinkedIn feed this week. Among the emoji-laden bulleted lists and grindset slop was a new Ethan Mollick piece, “<a href="https://www.oneusefulthing.org/p/the-bitter-lesson-versus-the-garbage">The Bitter Lesson versus The Garbage Can</a>”. In it, Mollick contrasts the step-by-step way businesses are building agents with the reality of business processes:</p>

<blockquote>
  <p>One thing you learn studying (or working in) organizations is that they are all actually a bit of a mess. In fact, one classic organizational theory is actually called the Garbage Can Model. This views organizations as chaotic “garbage cans” where problems, solutions, and decision-makers are dumped in together, and decisions often happen when these elements collide randomly, rather than through a fully rational process. Of course, it is easy to take this view too far - organizations do have structures, decision-makers, and processes that actually matter. It is just that these structures often evolved and were negotiated among people, rather than being carefully designed and well-recorded.</p>
</blockquote>

<blockquote>
  <p>The Garbage Can represents a world where unwritten rules, bespoke knowledge, and complex and undocumented processes are critical. It is this situation that makes AI adoption in organizations difficult, because even though 43% of American workers have used AI at work, they are mostly doing it in informal ways, solving their own work problems. Scaling AI across the enterprise is hard because traditional automation requires clear rules and defined processes; the very things Garbage Can organizations lack.</p>
</blockquote>

<p>The bitter lesson is about to be rediscovered, Mollick suggests. “If AI agents can train on outputs alone, any organization that can define quality and provide enough examples might achieve similar results, whether they understand their own processes or not,” he writes. Org charts and workflows are UX for us humans, unneeded by our agents.</p>

<p>But while I found myself quickly accepting the original bitter lesson essay, Mollick’s essay triggered my inner skeptic. Two practical issues immediately arose:</p>

<p><strong>The bitter lesson is dependent on high-quality data.</strong></p>

<p>Data has an embedded perspective. It is not objective. Further, data is <em>reductive</em>, a shadow of the information it represents. And many things resist being reduced to data points<sup id="fnref:haraway" role="doc-noteref"><a href="#fn:haraway" rel="footnote">1</a></sup>. Without sufficient data, the bitter lesson cannot be learned.</p>

<p>When Mollick writes, “any organization that can define quality and provide enough examples might achieve similar results,” I immediately focus on the word “can”. Most organizations I’ve encountered have an incredibly hard time defining their objectives <em>firmly</em> and <em>clearly</em>, if they’re able to at all.</p>

<p>Already, companies rely on goal-setting metrics or rubrics like <a href="https://en.wikipedia.org/wiki/Objectives_and_key_results">OKRs</a> to attempt to measure that which resists measurement. These can be helpful, but their reductive nature leads to gaming<sup id="fnref:goog" role="doc-noteref"><a href="#fn:goog" rel="footnote">2</a></sup>. Any metrics organizations use to optimize agents with likely be argued over, squishy, and inadequate.</p>

<p>Unlike the workplace, the fields Sutton cites as having met the bitter lesson are ones where the objective is easy to quantify. Victory in go and chess is clear, speech can be directly matched to written text, and most subjects in images can be easily annotated. It’s okay if you find a way to game the system to win a chess game; the rules are air tight.</p>

<p>Further, these fields can be easily represented with data – which is necessary for any general program that needs to learn how to achieve the quantifiable goal. Even <em>if</em> a company can give a clear, firm definition of quality, it still has to figure out how to measure all the interactions involved in producing this output. Otherwise, the program has no ideas what levers if can pull.</p>

<p>If we wish to teach agent-builders the bitter lesson, we need to get better at defining outputs concretely that are resistant to reward hacking (even if they’re not perfect, as <a href="https://www.dbreunig.com/2025/07/31/how-kimi-rl-ed-qualitative-data-to-write-better.html">we explored yesterday</a>) and figure out how to represent “Garbage Can” organizations with data.</p>

<p><strong>Adding compute is often not practical nor optimal.</strong></p>

<p>Speaking of chess, to illustrate his point, Mollick used both Manus and ChatGPT Agent Mode to create charts documenting the performance of human and computer chess play over time. Here’s ChatGPT’s:</p>

<p><img src="https://www.dbreunig.com/img/chatgpt_agent_chess.jpg" alt=""></p>

<p>If you’re a chess nerd, you might catch how this plot undermines the bitter lesson: <strong><a href="https://stockfishchess.org/">Stockfish</a>, the top performing program since 2020 is <em>not</em> an example of a, “general method that leverages computation.”</strong></p>

<p><a href="https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d">Aidan McLaughlin explains</a>:</p>

<blockquote>
  <p>Stockfish always had clever search algorithms, but in 2019, its ability to grind out billions of positions didn’t matter because its understanding of each position was kneecapped by human creators. To fix this, the Stockfish team heisted Leela’s deep learning techniques and trained a model hundreds of times smaller than the top Leela model.</p>
</blockquote>

<blockquote>
  <p>After they trained their tiny model, they threw it into their search pipeline, and Stockfish crushed Leela overnight. The Stockfish team utterly rejected scaling laws. They went backward and made a smaller model. But, because their search algorithm was more efficient, took better advantage of hardware, and saw further, they won.</p>
</blockquote>

<p><a href="https://en.wikipedia.org/wiki/Leela_Chess_Zero">Leela</a> is a deep learning model that, “started with no intrinsic chess-specific knowledge other than the basic rules of the game.” It learned by playing chess, at an absurd scale, until it was the best in the world. A true example of the bitter lesson.</p>

<p>Then Stockfish adopted a small, purpose-built search model inside its conventional chess program. Today, Stockfish remains unbeaten – and can run on your iPhone. By not embracing compute as the primary lever, the Stockfish team not only delivered quality, but delivered something everyone can use, often.</p>

<p>We’re starting to see hints that this pattern might be coming for our best AI benchmarks. <a href="https://arcprize.org/">ARC-AGI-1</a>, a highly-respected reasoning benchmark, saw its first breakout high score from OpenAI’s o3, <a href="https://www.dbreunig.com/2024/12/20/the-new-game-in-town.html">last December</a>. This score supported the compute component of the bitter lesson: OpenAI reportedly spent ~$30,000 <em>per task</em> to achieve this milestone. Since this score, ARC has added a “cost per task” metric to their leaderboard, and made efficiency a requirement for their grand prize.</p>

<p>But 3 weeks ago, <a href="https://sapient.inc/">Sapient</a> published a “<a href="https://arxiv.org/pdf/2506.21734">Hierarchical Reasoning Model</a>” (HRM), that achieves a 40.3 score on ARC-AGI-1 with only <em>27 million parameters</em> – besting large models like o3-mini-high and Claude 3.7. (As a reminder, LLM model parameter counts are measured in the billions.) HRM achieves this feat<sup id="fnref:sapient" role="doc-noteref"><a href="#fn:sapient" rel="footnote">3</a></sup> with a handful of new techniques, but also each model is trained for only a specific task: the model that scored highly on ARC was trained on 1,000 ARC problems. It’s not built for general use.</p>

<p>A model like HRM could likely run on your phone, just like Stockfish. These aren’t general purpose programs, and they’re not excelling with copious compute. Further, Stockfish is incredibly practical! It can be run basically anywhere (which is actually a growing problem in the online chess scene…) While the bitter lesson did demonstrate new mechanisms for search humans wouldn’t have anticipated, wrapping those in human-knowledge-based rules proves both more performant and practical.</p>

<hr>

<p>The bitter lesson is an <em>incredibly</em> well written essay. And the argument it makes is compelling because of its simplicity.</p>

<p>But like any rule of thumb, it’s imperfect when it meets the real world. I embrace the idea that general methods that leverage computation will lead us to new ideas and techniques. For many domains, once we understand these mechanisms we can apply them in focused, efficient applications.</p>

<p>And of course – this is entirely dependent on our ability to represent our challenges as data. We can easily model chess and annotate images, but modeling workplace dynamics is much harder.</p>

<p>For those building agents and other AI-powered applications today, it’s good to keep in mind the bitter lesson. Consider where <a href="https://www.dbreunig.com/2025/05/27/will-the-model-eat-your-stack.html">the model might eat your stack</a>, but don’t be afraid to thoughtfully apply custom, human-crafted logic or embrace non-general models to get the job done. And remember: compute is a constraint.&nbsp;Don’t let the bitter lesson stand in the way of actually <em>running</em> your tool. Slightly less accuracy at a fraction of the cost is often the best way.</p>

<hr>




  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Native Sparse Attention (132 pts)]]></title>
            <link>https://aclanthology.org/2025.acl-long.1126/</link>
            <guid>44761548</guid>
            <pubDate>Fri, 01 Aug 2025 19:48:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aclanthology.org/2025.acl-long.1126/">https://aclanthology.org/2025.acl-long.1126/</a>, See on <a href="https://news.ycombinator.com/item?id=44761548">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main-container"><div><p><a href="https://aclanthology.org/people/j/jingyang-yuan/">Jingyang Yuan</a>,
<a href="https://aclanthology.org/people/h/huazuo-gao/">Huazuo Gao</a>,
<a href="https://aclanthology.org/people/d/damai-dai/">Damai Dai</a>,
<a href="https://aclanthology.org/people/j/junyu-luo/">Junyu Luo</a>,
<a href="https://aclanthology.org/people/l/liang-zhao/">Liang Zhao</a>,
<a href="https://aclanthology.org/people/z/zhengyan-zhang/">Zhengyan Zhang</a>,
<a href="https://aclanthology.org/people/z/zhenda-xie/">Zhenda Xie</a>,
<a href="https://aclanthology.org/people/y/yuxing-wei/">Yuxing Wei</a>,
<a href="https://aclanthology.org/people/l/lean-wang/">Lean Wang</a>,
<a href="https://aclanthology.org/people/z/zhiping-xiao/">Zhiping Xiao</a>,
<a href="https://aclanthology.org/people/y/yuqing-wang/">Yuqing Wang</a>,
<a href="https://aclanthology.org/people/c/chong-ruan/">Chong Ruan</a>,
<a href="https://aclanthology.org/people/m/ming-zhang/">Ming Zhang</a>,
<a href="https://aclanthology.org/people/w/wenfeng-liang/">Wenfeng Liang</a>,
<a href="https://aclanthology.org/people/w/wangding-zeng/">Wangding Zeng</a></p></div><hr><div><div><h5>Abstract</h5><p><span>Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trained Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.</span></p></div><dl><dt>Anthology ID:</dt><dd>2025.acl-long.1126</dd><dt>Volume:</dt><dd><a href="https://aclanthology.org/volumes/2025.acl-long/">Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</a></dd><dt>Month:</dt><dd>July</dd><dt>Year:</dt><dd>2025</dd><dt>Address:</dt><dd>Vienna, Austria</dd><dt>Editors:</dt><dd><a href="https://aclanthology.org/people/w/wanxiang-che/">Wanxiang Che</a>,
<a href="https://aclanthology.org/people/j/joyce-nabende/">Joyce Nabende</a>,
<a href="https://aclanthology.org/people/e/ekaterina-shutova/">Ekaterina Shutova</a>,
<a href="https://aclanthology.org/people/m/mohammad-taher-pilehvar/">Mohammad Taher Pilehvar</a></dd><dt>Venue:</dt><dd><a href="https://aclanthology.org/venues/acl/">ACL</a></dd><dt>SIG:</dt><dd></dd><dt>Publisher:</dt><dd>Association for Computational Linguistics</dd><dt>Note:</dt><dd></dd><dt>Pages:</dt><dd>23078–23097</dd><dt>Language:</dt><dd></dd><dt>URL:</dt><dd><a href="https://aclanthology.org/2025.acl-long.1126/">https://aclanthology.org/2025.acl-long.1126/</a></dd><dt>DOI:</dt><dd></dd><dt>Award:</dt><dd><i></i>&nbsp;Best Paper</dd><dt>Bibkey:</dt><dd></dd><dt>Cite (ACL):</dt><dd><span id="citeACL">Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. <a href="https://aclanthology.org/2025.acl-long.1126/">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a>. In <i>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>, pages 23078–23097, Vienna, Austria. Association for Computational Linguistics.</span></dd><dt>Cite (Informal):</dt><dd><span id="citeRichText"><a href="https://aclanthology.org/2025.acl-long.1126/">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a> (Yuan et al., ACL 2025)</span></dd><dt>Copy Citation:</dt><dd>



</dd><dt>PDF:</dt><dd><a href="https://aclanthology.org/2025.acl-long.1126.pdf">https://aclanthology.org/2025.acl-long.1126.pdf</a></dd></dl></div><hr></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep Agents (125 pts)]]></title>
            <link>https://blog.langchain.com/deep-agents/</link>
            <guid>44761299</guid>
            <pubDate>Fri, 01 Aug 2025 19:28:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.langchain.com/deep-agents/">https://blog.langchain.com/deep-agents/</a>, See on <a href="https://news.ycombinator.com/item?id=44761299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        

        <main id="main">
            

<article>

    <header data-layout-grid="custom" data-theme="highlight" data-section="first" data-has-featured-image="true">
        <figure>
            <img width="16" height="9" src="https://blog.langchain.com/content/images/size/w760/format/webp/2025/07/image--1-.jpg" srcset="https://blog.langchain.com/content/images/size/w360/format/webp/2025/07/image--1-.jpg 360w,https://blog.langchain.com/content/images/size/w480/format/webp/2025/07/image--1-.jpg 480w,https://blog.langchain.com/content/images/size/w760/format/webp/2025/07/image--1-.jpg 760w,https://blog.langchain.com/content/images/size/w990/format/webp/2025/07/image--1-.jpg 990w,https://blog.langchain.com/content/images/size/w1248/format/webp/2025/07/image--1-.jpg 1248w,https://blog.langchain.com/content/images/size/w1520/format/webp/2025/07/image--1-.jpg 1520w" sizes="(min-width: 1280px) 580px, (min-width: 1000px) calc(50vw - 60px), 100vw" loading="eager" alt="Deep Agents" onload="this.setAttribute('data-loaded', true)">
        </figure>
        
    </header>

    <div data-canvas-grid="content">
        <div data-canvas-grid="content" data-canvas-grid-self="full">
            <p>Using an LLM to call tools in a loop is the simplest form of an agent. This architecture, however, can yield agents that are “shallow” and fail to plan and act over longer, more complex tasks. Applications like “<a href="https://openai.com/index/introducing-deep-research/?ref=blog.langchain.com">Deep Research</a>”, “<a href="https://manus.im/?ref=blog.langchain.com">Manus</a>”, and “<a href="https://www.anthropic.com/claude-code?ref=blog.langchain.com">Claude Code</a>” have gotten around this limitation by implementing a combination of four things: a planning tool, sub agents, access to a file system, and a detailed prompt.</p><p>Acknowledgements: this exploration was primarily inspired by Claude Code and reports of people using it for <a href="https://x.com/alexalbert__/status/1948765443776544885?ref=blog.langchain.com">more than just coding</a>. What about Claude Code made it general purpose, and could we abstract out and generalize those characteristics?</p><h2 id="deep-agents-in-the-wild">Deep agents in the wild</h2><p>The dominant agent architecture to emerge is also the simplest: running in a loop, calling tools.</p><p>Doing this naively, however, leads to agents that are a bit shallow. “Shallow” here refers to the agents inability to plan over longer time horizons and do more complex tasks.</p><p>Research and coding have emerged as two areas where agents have been created that buck this trend. All of the major model providers have an agent for Deep Research and for “async” coding tasks. Many startups and customers are creating versions of these for their specific vertical.</p><p>I refer to these agents as “deep agents” - for their ability to dive deep on topics. They are generally capable of planning more complex tasks, and then executing over longer time horizons on those goals.</p><p>What makes these agents good at going deep?</p><p>The core algorithm is actually the same - it’s an LLM running in a loop calling tools. The difference compared to the naive agent that is easy to build is:</p><ul><li>A detailed system prompt</li><li>Planning tool</li><li>Sub agents</li><li>File system</li></ul><figure><img src="https://blog.langchain.com/content/images/2025/07/Screenshot-2025-07-30-at-9.08.32-AM.png" alt="" loading="lazy" width="1056" height="460" srcset="https://blog.langchain.com/content/images/size/w600/2025/07/Screenshot-2025-07-30-at-9.08.32-AM.png 600w, https://blog.langchain.com/content/images/size/w1000/2025/07/Screenshot-2025-07-30-at-9.08.32-AM.png 1000w, https://blog.langchain.com/content/images/2025/07/Screenshot-2025-07-30-at-9.08.32-AM.png 1056w" sizes="(min-width: 720px) 720px"></figure><h2 id="characteristics-of-deep-agents">Characteristics of deep agents</h2><p><strong>Detailed system prompt</strong></p><p>Claude Code’s <a href="https://github.com/kn1026/cc/blob/main/claudecode.md?ref=blog.langchain.com">recreated system prompts</a> are long. They contain detailed instructions on how to use tools. They contain examples (few shot prompts) on how to behave in certain situations.</p><p>Claude Code is not an anomaly - most of the best coding or deep research agents have pretty complex system prompts. Without these system prompts, the agents would not be nearly as deep. Prompting matters still!</p><p><strong>Planning tool</strong></p><p>Claude Code uses a <a href="https://claudelog.com/faqs/what-is-todo-list-in-claude-code/?ref=blog.langchain.com">Todo list tool</a>. Funnily enough - this doesn’t do anything! It’s basically a no-op. It’s just context engineering strategy to keep the agent on track.</p><p>Deep agents are better at executing on complex tasks over longer time horizons. Planning (even if done via a no-op tool call) is a big component of that.</p><p><strong>Sub agents</strong></p><p>Claude Code can spawn <a href="https://docs.anthropic.com/en/docs/claude-code/sub-agents?ref=blog.langchain.com">sub agents</a>. This allows it to split up tasks. You can also create custom sub agents to have more control. This allows for <a href="https://x.com/dexhorthy/status/1950288431122436597?ref=blog.langchain.com">"context management and prompt shortcuts"</a>.</p><p>Deep agents go deeper on topics. This is largely accomplished by spinning up sub agents that specifically focused on individual tasks, and allowing those sub agents to go deep there.</p><p><strong>File System</strong></p><p>Claude Code (obviously) has access to the file system and can modify files on there, not just to complete its task but also to jot down notes. It also acts as a shared workspace for all agents (and sub agents) to collaborate on.</p><p>Manus is another example of a deep agent that makes <a href="https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus?ref=blog.langchain.com">significant use</a> of a file system for “memory”.</p><p>Deep agents run for long periods of time and accumulate a lot of context that they need to manage. Having a file system handy to store (and then later read from) is helpful for doing so.</p><h2 id="build-your-deep-agent">Build your deep agent</h2><p>In order to make it easier for everyone to build a deep agent for their specific vertical, I hacked on an open source package (<a href="https://github.com/hwchase17/deepagents?ref=blog.langchain.com" rel="noreferrer"><code>deepagents</code></a>) over the weekend. You can easily install it with <code>pip install deepagents</code> and then read instructions for how to use it <a href="https://github.com/hwchase17/deepagents?ref=blog.langchain.com" rel="noreferrer">here</a>.</p><p>This package attempts to create a general purpose deep agent that can be customized to create your own custom version.</p><p>It comes with built-in components mapping to the above characteristics:</p><ul><li>A system prompt inspired by Claude Code, but modified to be more general</li><li>A no-op Todo list planning tool (same as Claude Code)</li><li>Ability to spawn sub-agents, and specify your own</li><li>A mocked out “virtual file system” that uses the agents state (a preexisting LangGraph concept)</li></ul><p>You can easily create your own deep agent by passing in a custom prompt (will be inserted into the larger system prompt as custom instructions), custom tools, and custom subagents. We put together a simple example of a <a href="https://github.com/hwchase17/deepagents/tree/master/examples/research?ref=blog.langchain.com">"deep research" agent</a> built on top of <code>deepagents</code>.</p><p><a href="https://github.com/hwchase17/deepagents?ref=blog.langchain.com"><strong>TRY OUT <code>deepagents</code> HERE</strong></a></p>
        </div>

        <div>
            <h3>Tags</h3>
            
        </div>



        <div data-theme="highlight">
            <h3>Join our newsletter</h3>
            <p>Updates from the LangChain team and community</p>
            <form data-members-form="signup" data-theme="reset">
                <p><label for="newsletter-box-email-input">Enter your email</label>
                    
                    
                </p>
                <p data-message="loading">Processing your application...</p>
                <p data-message="success">Success! Please check your inbox and click the link to confirm your subscription.</p>
                <p data-message="error">Sorry, something went wrong. Please try again.</p>
            </form>
        </div>
    </div>

    

</article>


        </main>

        

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atlassian terminates 150 staff with pre-recorded video (214 pts)]]></title>
            <link>https://www.cyberdaily.au/digital-transformation/12441-atlassian-terminates-150-staff-with-pre-recorded-video-will-be-largely-replaced-by-ai</link>
            <guid>44761205</guid>
            <pubDate>Fri, 01 Aug 2025 19:21:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cyberdaily.au/digital-transformation/12441-atlassian-terminates-150-staff-with-pre-recorded-video-will-be-largely-replaced-by-ai">https://www.cyberdaily.au/digital-transformation/12441-atlassian-terminates-150-staff-with-pre-recorded-video-will-be-largely-replaced-by-ai</a>, See on <a href="https://news.ycombinator.com/item?id=44761205">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>Major Australian tech company Atlassian has terminated 150 staff with a pre-recorded video, as the company pushes its AI-embedded customer contact solutions.</span></p><div>
                    
<p dir="ltr"><span>Atlassian CEO and co-founder Mike Cannon-Brookes sent the video titled “Restructuring the CSS Team: A Difficult Decision for Our Future” to staff on Wednesday morning (30 July), informing them that 150 staff had been made redundant. The video reportedly did not make it seem that the decision was difficult, but rather said it would allow its staff “to say goodbye”.</span></p>
<p dir="ltr"><span>The video itself did not announce who was leaving, but it told employees they would have to wait 15 minutes for an email about their employment. Those who were terminated had their laptops blocked immediately. They reportedly will receive six months’ pay.</span></p>
<div id="freeMembershipOverlay">
    <p>
        You’re out of free articles for this month    </p>

    <div id="contentLogin">
            <p>
                To continue reading the rest of this article, please log in.            </p>

            
        </div>
</div>

<section>
<p dir="ltr"><span>Atlassian has always set itself as a company that doesn’t mince words and is majorly transparent, displaying the core value “Open Company, No Bullshit” in its offices around the world.</span></p>
<p dir="ltr"><span>While not specifically outlined, the affected staff seem to be from the company’s European operations, with </span><em>The Australian</em><span><em> </em>saying that Cannon-Brooke’s overshared that it would be difficult to axe its European staff due to contract arrangements, but that the company had already begun moving in that direction.</span></p>
<p dir="ltr"><span>The company has also embedded AI in its customer contact form.</span></p>
    





<p dir="ltr"><span>Former co-CEO and co-founder Scott Farquhar, at the same time, has said that embracing AI on a daily basis is something Australians should be doing.</span></p>
<p dir="ltr"><span>“AI is going to change Australia,” he told t</span><span>he ABC.</span></p>
<p dir="ltr"><span>“Every person should be using AI daily for as many things as they can.”</span></p>
    






<p dir="ltr"><span>“Like any new technology, it will feel awkward to start with, but every business person, every business leader, every government leader, and every bureaucrat should be using it.”</span></p>
<p dir="ltr"><span>He also said that governments should be implementing AI more broadly.</span></p>
<p dir="ltr"><span>Staff believe that Farquhar, who announced his resignation as co-CEO of Atlassian in April 2024 before stepping down in September, would have been more gentle in delivering the termination message.</span></p>
<p dir="ltr"><span>“[Cannon-Brookes] is the colder person out of the couple,” said one person who saw the pre-recorded video, according to </span><em>The Australian</em><span>.</span></p>
<p dir="ltr"><span>“[Farquhar] was the warmer one.”</span></p>
<p dir="ltr"><span>Commenting on the termination, Farquhar said the mass termination was due to the customer service team no longer being needed in the same capacity, as larger clients required less complex support following a move to the cloud.</span></p>
<p dir="ltr"><span>Cyber Daily has reached out to Atlassian for confirmation that no Australian staff were impacted and for comment on the incident.</span></p>
<p dir="ltr"><span>Atlassian closely follows </span><a href="https://www.cyberdaily.au/digital-transformation/12434-cba-cuts-at-least-45-jobs-to-make-room-for-ai-unions-furious" target="_blank" rel="noopener"><span>Commonwealth Bank</span></a><span> (CBA), which culled at least 45 jobs as it makes room for AI.</span></p>
<p dir="ltr"><span>The country’s largest bank said it would be making at least 45 roles redundant in the wake of its push to use AI, and only a month after it announced that it had launched a new customer assistance AI voice bot system.</span></p>
<p dir="ltr"><span>“Our investment in technology, including AI, is making it easier and faster for customers to get help, especially in our call centres,” a CBA spokesman said regarding the voice bot.</span></p>
<p dir="ltr"><span>“By automating simple queries, our teams can focus on more complex customer queries that need empathy and experience.</span></p>
<p dir="ltr"><span>“To meet the changing needs of our customers ... we review the skills we need and how we’re organised to deliver the best customer experiences and outcomes. That means some roles and work can change.”</span></p>
<hr>
<p dir="ltr"><em>Updated 31/07/25 - Atlassian says the jobs are not being replaced by AI, but that it has embedded AI in its customer service. Updated to remove claims of AI replacing jobs.</em></p></section>
        <div>
            <div>
            <p><img loading="lazy" src="https://res.cloudinary.com/momentum-media-group-pty-ltd/image/upload/v1696559412/Cyber%20Security/Podcast/daniel-croft.jpg" alt="Daniel Croft" width="170" height="170">
            </p>
        </div>
    
    <div>
        <h3>
                            <a href="https://www.cyberdaily.au/authors/daniel-croft" title="About View all articles from Daniel Croft">
                        Daniel Croft                            </a>
                    </h3>

                    <div>
                                    <p>Born in the heart of Western Sydney, Daniel Croft is a passionate journalist with an understanding for and experience writing in the technology space. Having studied at Macquarie University, he joined Momentum Media in 2022, writing across a number of publications including Australian Aviation, Cyber Security Connect and Defence Connect. Outside of writing, Daniel has a keen interest in music, and spends his time playing in bands around Sydney.</p>                                            </div>
        
                    
            </div>
</div>
    




                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bad UX (109 pts)]]></title>
            <link>https://www.google.com/search?q=bad+UX</link>
            <guid>44760821</guid>
            <pubDate>Fri, 01 Aug 2025 18:48:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.google.com/search?q=bad+UX">https://www.google.com/search?q=bad+UX</a>, See on <a href="https://news.ycombinator.com/item?id=44760821">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
    </channel>
</rss>