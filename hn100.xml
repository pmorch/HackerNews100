<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 31 May 2025 16:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Beware of Fast-Math (220 pts)]]></title>
            <link>https://simonbyrne.github.io/notes/fastmath/</link>
            <guid>44142472</guid>
            <pubDate>Sat, 31 May 2025 07:05:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonbyrne.github.io/notes/fastmath/">https://simonbyrne.github.io/notes/fastmath/</a>, See on <a href="https://news.ycombinator.com/item?id=44142472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>One of my more frequent rants, both online and in person, is the danger posed by the "fast-math" compiler flag. While these rants may elicit resigned acknowledgment from those who already understand the dangers involved, they do little to help those who don't. So given the remarkable paucity of writing on the topic (including the documentation of the compilers themselves), I decided it would make a good inaugural topic for this blog.</p> <h2 id="so_what_is_fast-math"><a href="#so_what_is_fast-math">So what is fast-math?</a></h2> <p>It's a compiler flag or option that exists in many languages and compilers, including:</p> <ul> <li><p><code>-ffast-math</code> (and included by <code>-Ofast</code>) in <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a> and <a href="https://clang.llvm.org/docs/UsersManual.html#cmdoption-ffast-math">Clang</a></p> </li><li><p><code>-fp-model=fast</code> (the default) in <a href="https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/compiler-options/compiler-option-details/floating-point-options/fp-model-fp.html">ICC</a></p> </li><li><p><code>/fp:fast</code> in <a href="https://docs.microsoft.com/en-us/cpp/build/reference/fp-specify-floating-point-behavior?view=msvc-170">MSVC</a></p> </li><li><p><a href="https://docs.julialang.org/en/v1/manual/command-line-options/#command-line-options"><code>--math-mode=fast</code> command line option</a> or <a href="https://docs.julialang.org/en/v1/base/math/#Base.FastMath.@fastmath"><code>@fastmath</code> macro</a> in Julia.</p> </li></ul> <p>So what does it actually do? Well, as the name said, it makes your math faster. That sounds great, we should definitely do that!</p> <blockquote> <p>I mean, the whole point of fast-math is trading off speed with correctness. If fast-math was to give always the correct results, it wouldn’t be fast-math, it would be the standard way of doing math.</p> </blockquote> <p>— <a href="https://discourse.julialang.org/t/whats-going-on-with-exp-and-math-mode-fast/64619/7?u=simonbyrne">Mosè Giordano</a></p> <p>The rules of floating point operations are specified in <a href="https://en.wikipedia.org/wiki/IEEE_754">the IEEE 754 standard</a>, which all popular programming languages (mostly) adhere to; compilers are only allowed to perform optimizations which obey these rules. Fast-math allows the compiler to break some of these rules: these breakages may seem pretty innocuous at first glance, but can have significant and occasionally unfortunate downstream effects.</p> <p>In <a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">GCC</a>, <code>-ffast-math</code> (or <code>-Ofast</code>) enables the following options: <code>-fno-math-errno</code>, <code>-funsafe-math-optimizations</code>, <code>-ffinite-math-only</code>, <code>-fno-rounding-math</code>, <code>-fno-signaling-nans</code>, <code>-fcx-limited-range</code> and <code>-fexcess-precision=fast</code>. Note that <code>-funsafe-math-optimizations</code> is itself a collection of options <code>-fno-signed-zeros</code>, <code>-fno-trapping-math</code>, <code>-fassociative-math</code> and <code>-freciprocal-math</code>, plus some extra ones, which we will discuss further below.</p> <p>Now some of these are unlikely to cause problems in most cases: <code>-fno-math-errno</code><sup id="fnref:1"><a href="#fndef:1">[1]</a></sup>, <code>-fno-signaling-nans</code>, <code>-fno-trapping-math</code> disable rarely-used (and poorly supported) features. Others, such as <code>-freciprocal-math</code> can reduce accuracy slightly, but are unlikely to cause problems in most cases.</p> <p><a href="https://kristerw.github.io/2021/10/19/fast-math/">Krister Walfridsson</a> gives a very nice (and somewhat more objective) description of some of these, but I want to focus on three in particular.</p> <h2 id="a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only"><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-ffinite-math-only-ffinite-math-only"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-ffinite-math-only"><code>-ffinite-math-only</code></a></h2> <blockquote> <p>Allow optimizations for floating-point arithmetic that assume that arguments and results are not NaNs or +-Infs.</p> </blockquote> <p>The intention here is to allow the compiler to perform some <a href="https://stackoverflow.com/a/10145714/392585">extra optimizations</a> that would not be correct if NaNs or Infs were present: for example the condition <code>x == x</code> can be assumed to always be true (it evaluates false if <code>x</code> is a NaN).</p> <p>This sounds great! My code doesn't generate any NaNs or Infs, so this shouldn't cause any problems.</p> <p>But what if your code doesn't generate any intermediate NaNs only because it internally calls <code>isnan</code> to ensure that they are correctly handled?</p> <p>  — based on <a href="https://twitter.com/johnregehr/status/1440024236257542147">an example from John Regehr</a></p> <p>(to explain what this is showing: the function is setting the return register <code>eax</code> to zero, by <code>xor</code>-ing it with itself, which means the function will always return <code>false</code>)</p> <p>That's right, your compiler has just removed all those checks.</p> <p>Depending on who you ask, this is either obvious ("you told the compiler there were no NaNs, so why does it need to check?") or ridiculous ("how can we safely optimize away NaNs if we can't check for them?"). Even compiler developers <a href="https://twitter.com/johnregehr/status/1440021297103134720">can't agree</a>.</p> <p>This is perhaps the single most frequent cause of fast-math-related <a href="https://stackoverflow.com/a/22931368/392585">StackOverflow</a> <a href="https://stackoverflow.com/q/7263404/392585">questions</a> and <a href="https://github.com/numba/numba/issues/2919">GitHub</a> <a href="https://github.com/google/jax/issues/276">bug</a> <a href="https://github.com/pytorch/glow/issues/2073">reports</a>, and so if your fast-math-compiled code is giving wrong results, the very first thing you should do is disable this option (<code>-fno-finite-math-only</code>).</p> <h2 id="a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math"><a href="#a_hrefhttpsgccgnuorgonlinedocsgccoptimize-optionshtmlindex-fassociative-math-fassociative-math"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#index-fassociative-math"><code>-fassociative-math</code></a></h2> <blockquote> <p>Allow re-association of operands in series of floating-point operations.</p> </blockquote> <p>This allows the compiler to change the order of evaluation in a sequence of floating point operations. For example if you have an expression <code>(a + b) + c</code>, it can evaluate it instead as <code>a + (b + c)</code>. While these are mathematically equivalent with real numbers, they aren't equivalent in floating point arithmetic: the errors they incur can be different, in some cases quite significantly so:</p> <pre><code>julia&gt; a = <span>1e9</span>+<span>1</span>; b = -<span>1e9</span>; c = <span>0.1</span>;

julia&gt; (a+b)+c
<span>1.1</span>

julia&gt; a+(b+c)
<span>1.100000023841858</span></code></pre> <h3 id="vectorization"><a href="#vectorization">Vectorization </a></h3> <p>So why would you want to do this? One primary reason is that it can enable use of vector/SIMD instructions:</p>  <p>For those who aren't familiar with SIMD operations (or reading assembly), I'll try to explain briefly what is going on here (others can skip this part). Since raw clock speeds haven't been getting much faster, one way in which processors have been able to increase performance is through operations which operate on a "vector" (basically, a short sequence of values contiguous in memory).</p> <p>In this case, instead of performing a sequence of floating point additions (<code>addss</code>), it is able to make use of a SIMD instruction (<code>addps</code>) which takes vector <code>float</code>s (4 in this case, but it can be up to 16 with AVX 512 instructions), and adds them element-wise to another vector in one operation. It does this for the whole array, followed by a final reduction step where to sum the vector to a single value. This means that instead of evaluating</p> <pre><code>s = arr[<span>0</span>] + arr[<span>1</span>];
s = s + arr[<span>2</span>];
s = s + arr[<span>3</span>];
...
s = s + arr[<span>255</span>];</code></pre> <p>it is actually doing</p> <pre><code>s0 = arr[<span>0</span>] + arr[<span>4</span>]; s1 = arr[<span>1</span>] + arr[<span>5</span>]; s2 = arr[<span>2</span>] + arr[<span>6</span>];  s3 = arr[<span>3</span>] + arr[<span>7</span>];
s0 = s0 + arr[<span>8</span>];     s1 = s1 + arr[<span>9</span>];     s2 = s2 + arr[<span>10</span>];     s3 = s3 + arr[<span>11</span>]);
...
s0 = s0 + arr[<span>252</span>];   s1 = s1 + arr[<span>253</span>];   s2 = s2 + arr[<span>254</span>];    s3 = s3 + arr[<span>255</span>]);
sa = s0 + s1;
sb = s2 + s3;
s = sa + sb;</code></pre> <p>where each line corresponds to one floating point instruction.</p> <p>The problem here is that the compiler generally isn't allowed to make this optimization: it requires evaluating the sum in a different association grouping than was specified in the code, and so can give different results<sup id="fnref:4"><a href="#fndef:4">[2]</a></sup>. Though in this case it is likely harmless (or may even improve accuracy<sup id="fnref:2"><a href="#fndef:2">[3]</a></sup>), this is not always the case.</p> <h3 id="compensated_arithmetic"><a href="#compensated_arithmetic">Compensated arithmetic</a></h3> <p>Certain algorithms however depend very strictly on the order in which floating point operations are performed. In particular <em>compensated arithmetic</em> operations make use of it to compute the error that is incurred in intermediate calculations, and correct for that in later computations.</p> <p>The most well-known algorithm which makes use of this is <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm">Kahan summation</a>, which corrects for the round off error incurred at addition step in the summation loop. We can compile an implementation of Kahan summation with <code>-ffast-math</code>, and compare the result to the simple loop summation above:</p>  <p>It gives <em>exactly</em> the same assembly as the original summation code above. Why?</p> <p>If you substitute the expression for <code>t</code> into <code>c</code>, you get</p> <pre><code>c = ((s + y) - s) - y);</code></pre>
<p>and by applying reassociation, the compiler will then determine that <code>c</code> is in fact always zero, and so may be completely removed. Following this logic further, <code>y = arr[i]</code> and so the inside of the loop is simply</p>
<pre><code>s = s + arr[i];</code></pre>
<p>and hence it "optimizes" identically to the simple summation loop above.</p>
<p>This might seem like a minor tradeoff, but compensated arithmetic is often used to implement core math functions, such as trigonometric and exponential functions. Allowing the compiler to reassociate inside these can give <a href="https://github.com/JuliaLang/julia/issues/30073#issuecomment-439707503">catastrophically wrong answers</a>.</p>
<h2 id="flushing_subnormals_to_zero"><a href="#flushing_subnormals_to_zero">Flushing subnormals to zero</a></h2>
<p>This one is the most subtle, but by far the most insidious, as it can affect code compiled <em>without</em> fast-math, and is only cryptically documented under <code>-funsafe-math-optimizations</code>:</p>
<blockquote>
<p>When used at link time, it may include libraries or startup files that change the default FPU control word or other similar optimizations.</p>
</blockquote>
<p>So what does that mean? Well this is referring to one of those slightly annoying edge cases of floating point numbers, <em>subnormals</em> (sometimes called <em>denormals</em>). <a href="https://en.wikipedia.org/wiki/Subnormal_number">Wikipedia gives a decent overview</a>, but for our purposes the main thing you need to know is (a) they're <em>very</em> close to zero, and (b) when encountered, they can incur a significant performance penalty on many processors<sup id="fnref:6"><a href="#fndef:6">[4]</a></sup>.</p>
<p>A simple solution to this problem is "flush to zero" (FTZ): that is, if a result would return a subnormal value, return zero instead. This is actually fine for a lot of use cases, and this setting is commonly used in audio and graphics applications. But there are plenty of use cases where it isn't fine: FTZ breaks some important floating point error analysis results, such as <a href="https://en.wikipedia.org/wiki/Sterbenz_lemma">Sterbenz' Lemma</a>, and so unexpected results (such as iterative algorithms failing to converge) may occur.</p>
<p>The problem is how FTZ actually implemented on most hardware: it is not set per-instruction, but instead <a href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/floating-point-operations/understanding-floating-point-operations/setting-the-ftz-and-daz-flags.html">controlled by the floating point environment</a>: more specifically, it is controlled by the floating point control register, which on most systems is set at the thread level: enabling FTZ will affect all other operations in the same thread.</p>
<p>GCC with <code>-funsafe-math-optimizations</code> enables FTZ (and its close relation, denormals-are-zero, or DAZ), even when building shared libraries. That means simply loading a shared library can change the results in completely unrelated code, which is <a href="https://github.com/JuliaCI/BaseBenchmarks.jl/issues/253#issuecomment-573589022">a fun debugging experience</a>.</p>
<h2 id="what_can_programmers_do"><a href="#what_can_programmers_do">What can programmers do?</a></h2>
<p>I've joked on Twitter that "friends don't let friends use fast-math", but with the luxury of a longer format, I will concede that it has valid use cases, and can actually give valuable performance improvements; as SIMD lanes get wider and instructions get fancier, the value of these optimizations will only increase. At the very least, it can provide a useful reference for what performance is left on the table. So when and how can it be safely used?</p>
<p>One reason is if you don't care about the accuracy of the results: I come from a scientific computing background where the primary output of a program is a bunch of numbers. But floating point arithmetic is used in many domains where that is not the case, such as audio, graphics, games, and machine learning. I'm not particularly familiar with requirements in these domains, but there is an interesting rant by <a href="https://gcc.gnu.org/legacy-ml/gcc/2001-07/msg02150.html">Linus Torvalds from 20 years ago</a>, arguing that overly strict floating point semantics are of little importance outside scientific domains. Nevertheless, <a href="https://twitter.com/supahvee1234/status/1382907921848221698">some anecdotes</a> suggest fast-math can cause problems, so it is probably still useful understand what it does and why. If you work in these areas, I would love to hear about your experiences, especially if you identified which of these optimizations had a positive or negative impact.</p>
<blockquote>
<p>I hold that in general it’s simply intractable to “defensively” code against the transformations that <code>-ffast-math</code> may or may not perform. If a sufficiently advanced compiler is indistinguishable from an adversary, then giving the compiler access to <code>-ffast-math</code> is gifting that enemy nukes. That doesn’t mean you can’t use it! You just have to test enough to gain confidence that no bombs go off with your compiler on your system.</p>
</blockquote>
<p>— <a href="https://discourse.julialang.org/t/when-if-a-b-x-1-a-b-divides-by-zero/7154/5?u=simonbyrne">Matt Bauman</a></p>
<p>If you do care about the accuracy of the results, then you need to approach fast-math much more carefully and warily. A common approach is to enable fast-math everywhere, observe erroneous results, and then attempt to isolate and fix the cause as one would usually approach a bug. Unfortunately this task is not so simple: you can't insert branches to check for NaNs or Infs (the compiler will just remove them), you can't rely on a debugger because <a href="https://gitlab.com/libeigen/eigen/-/issues/1674#note_709679831">the bug may disappear in debug builds</a>, and it can even <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1127544">break printing</a>.</p>
<p>So you have to approach fast-math much more carefully. A typical process might be:</p>
<ol>
<li><p>Develop reliable validation tests</p>

</li><li><p>Develop useful benchmarks</p>

</li><li><p>Enable fast-math and compare benchmark results</p>

</li><li><p>Selectively enable/disable fast-math optimizations<sup id="fnref:5"><a href="#fndef:5">[5]</a></sup> to identify</p>
<p>a. which optimizations have a performance impact,</p>
<p>b. which cause problems, and</p>
<p>c. where in the code those changes arise.</p>

</li><li><p>Validate the final numeric results</p>

</li></ol>
<p>The aim of this process should be to use the absolute minimum number of fast-math options, in the minimum number of places, while testing to ensure that the places where the optimizations are used remain correct.</p>
<p>Alternatively, you can look into other approaches to achieve the same performance benefits: in some cases it is possible to rewrite the code to achieve the same results: for example, it is not uncommon to see expressions like <code>x * (1/y)</code> in many scientific codebases.</p>
<p>For SIMD operations, tools such as <a href="https://www.openmp.org/spec-html/5.0/openmpsu42.html">OpenMP</a> or <a href="https://ispc.github.io/">ISPC</a> provide constructions to write code that is amenable to automatic SIMD optimizations. Julia provides the <a href="https://docs.julialang.org/en/v1/base/base/#Base.SimdLoop.@simd"><code>@simd</code> macro</a>, though this also has some important caveats on its use. At the more extreme end, you can use <a href="https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/">SIMD intrinsics</a>: these are commonly used in libraries, often with the help of code generation (<a href="http://fftw.org/">FFTW</a> uses this appraoch), but requires considerably more effort and expertise, and can be difficult to port to new platforms.</p>
<p>Finally, if you're writing an open source library, please don't <a href="https://github.com/tesseract-ocr/tesseract/blob/5884036ecdb2807419cbd21b7ca44b630f547d80/Makefile.am#L140">hardcode fast-math into your Makefile</a>.</p>
<h2 id="what_can_language_and_compilers_developers_do"><a href="#what_can_language_and_compilers_developers_do">What can language and compilers developers do?</a></h2>
<p>I think the widespread use of fast-math should be considered a fundamental design failure: by failing to provide programmers with features they need to make the best use of modern hardware, programmers instead resort to enabling an option that is known to be blatantly unsafe.</p>
<p>Firstly, GCC should address the FTZ library issue: the bug has been <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522">open for 9 years, but is still marked NEW</a>. At the very least, this behavior should be more clearly documented, and have a specific option to disable it.</p>
<p>Beyond that, there are 2 primary approaches: educate users, and provide finer control over the optimizations.</p>
<p>The easiest way to educate users is to give it a better name. Rather than "fast-math", something like "unsafe-math". Documentation could also be improved to educate users on the consequences of these choices (consider this post to be my contribution to toward that goal). Linters and compiler warnings could, for example, warn users that their <code>isnan</code> checks are now useless, or even just highlight which regions of code have been impacted by the optimizations.</p>
<p>Secondly, languages and compilers need to provide better tools to get the job done. Ideally these behaviors shouldn't be enabled or disabled via a compiler flag, which is a very blunt tool, but specified locally in the code itself, for example</p>
<ul>
<li><p>Both GCC and Clang let you <a href="https://stackoverflow.com/a/40702790/392585">enable/disable optimizations on a per-function basis</a>: these should be standardized to work with all compilers.</p>

</li><li><p>There should be options for even finer control, such as a pragma or macro so that users can assert that "under no circumstances should this <code>isnan</code> check be removed/this arithmetic expression be reassociated".</p>

</li><li><p>Conversely, a mechanism to flag certain addition or subtraction operations which the compiler is allowed to reassociate (or contract into a fused-multiply-add operation) regardless of compiler flags.<sup id="fnref:3"><a href="#fndef:3">[6]</a></sup></p>

</li></ul>
<p>This still leaves open the exact question of what the semantics should be: if you combine a regular <code>+</code> and a fast-math <code>+</code>, can they reassociate? What should the scoping rules be, and how should it interact with things like inter-procedural optimization? These are hard yet very important questions, but they need to be answered for programmers to be able to make use of these features safely.</p>
<p>For more discussion, see <a href="https://news.ycombinator.com/item?id=29201473">HN</a>.</p>
<h2 id="updates"><a href="#updates">Updates</a></h2>
<p>A few updates since I wrote this note:</p>
<ul>
<li><p>Brendan Dolan-Gavitt wrote a fantastic piece about <a href="https://moyix.blogspot.com/2022/09/someones-been-messing-with-my-subnormals.html">FTZ-enabling libraries in Python packages</a>: it also has some nice tips on how to find out if your library was compiled with fast-math.</p>
<ul>
<li><p>He also has a nice proof-of-concept <a href="https://github.com/moyix/2_ffast_2_furious">buffer overflow vulnerability</a>.</p>

</li></ul>

</li><li><p>It turns out Clang also enables FTZ when building shared libraries with fast-math: but only if you have a system GCC installation. I've <a href="https://github.com/llvm/llvm-project/issues/57589">opened an issue</a>.</p>

</li><li><p>MSVC doesn't remove <code>isnan</code> checks, but instead <a href="https://twitter.com/dotstdy/status/1567748577962741760">generates what looks like worse code</a> when compiling with fast-math.</p>

</li><li><p>The FTZ library issue will be <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522#c45">fixed in GCC 13</a>!</p>

</li></ul>

 
 
 
 <table id="fndef:5">
    <tbody><tr>
        <td><a href="#fnref:5">[5]</a>
        </td><td>As mentioned above, <code>-fno-finite-math-only</code> should be the first thing you try.
    
</td></tr></tbody></table>
 <table id="fndef:3">
    <tbody><tr>
        <td><a href="#fnref:3">[6]</a>
        </td><td>Rust provides something like this via <a href="https://stackoverflow.com/a/40707111/392585">experimental intrinsics</a>, though I'm not 100% clear on what optimzations are allowed.
    
</td></tr></tbody></table>

<div>
  <p>
    © Simon Byrne. Last modified: April 06, 2024.
  </p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Responses May Include Mistakes (171 pts)]]></title>
            <link>https://www.os2museum.com/wp/ai-responses-may-include-mistakes/</link>
            <guid>44142113</guid>
            <pubDate>Sat, 31 May 2025 05:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.os2museum.com/wp/ai-responses-may-include-mistakes/">https://www.os2museum.com/wp/ai-responses-may-include-mistakes/</a>, See on <a href="https://news.ycombinator.com/item?id=44142113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<p>The other day I wanted to look up a specific IBM PS/2 model, a circa 1992 PS/2 Server system. So I punched the model into Google, and got this:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop.png"><img decoding="async" width="640" height="487" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-640x487.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-640x487.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-300x228.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop-768x584.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense-crop.png 1147w" sizes="(max-width: 640px) 100vw, 640px"></a></figure></div>


<p>That did not look quite right, since the machine I was looking for had 486 processors (yes, plural). And it most certainly <em>did</em> use Microchannel (MCA).</p>



<p>Alright, let’s try again:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop.png"><img decoding="async" width="640" height="557" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-640x557.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-640x557.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-300x261.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop-768x669.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense2-crop.png 1174w" sizes="(max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Simply re-running the identical query produces a different summary. Although the AI still claims that PS/2 Model 280 is an ISA-based 286 system. Maybe the third time is the charm?</p>



<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop.png"><img loading="lazy" decoding="async" width="640" height="465" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-640x465.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-640x465.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-300x218.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop-768x557.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense3-crop.png 1160w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>The AI is really quite certain that PS/2 Model 280 was a 286-based system released in 1987, and I was really looking for a newer machine. Interestingly, the first time the AI claimed Model 280 had 1MB RAM expandable to 6MB, and now it supposedly only has 640 KB RAM. But the AI seems sure that Model 280 had a 1.44 MB drive and VGA graphics.</p>



<p>What if we try again? After a couple of attempts, yet different answer pops up:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop.png"><img loading="lazy" decoding="async" width="640" height="431" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-640x431.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-640x431.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-300x202.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop-768x518.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-nonsense4-crop.png 1138w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Oh look, now the PS/2 Model 280 is a 286 expandable to 128 MB RAM. Amazing! Never mind that the 286 was architecturally limited to 16 MB.</p>



<p>Even better, the AI now tells us that “PS/2 Model 280 was a significant step forward in IBM’s personal computer line, and it helped to establish the PS/2 as a popular and reliable platform.”</p>



<p>The only problem with all that? <em>There is no PS/2 Model 280, and never was.</em> I simply had the model number wrong. The Google AI just “helpfully” hallucinates something that at first glance seems quite plausible, but is in fact utter nonsense.</p>



<p>But wait, that’s not the end of the story. If you try repeating the query often enough, you might get this:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop.png"><img loading="lazy" decoding="async" width="640" height="409" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-640x409.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-640x409.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-300x192.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop-768x491.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct-crop.png 1160w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>That answer is <em>actually correct</em>! “Model 280 was not a specific model in the PS/2 series”, and there was in fact an error in the query.</p>



<p>Here’s another example of a correct answer:</p>


<div>
<figure><a href="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop.png"><img loading="lazy" decoding="async" width="640" height="415" src="https://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-640x415.png" alt="" srcset="http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-640x415.png 640w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-300x194.png 300w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop-768x498.png 768w, http://www.os2museum.com/wp/wp-content/uploads/2025/05/ps2-mod280-correct2-crop.png 1174w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></figure></div>


<p>Unfortunately the correct answer comes up maybe 10% of the time when repeating the query, if at all. In the vast majority of attempts, the AI simply makes stuff up. I do not consider made up, hallucinated answers useful, in fact they are worse than useless. </p>



<p>This minor misadventure might provide a good window into AI-powered Internet search. To a non-expert, the made up answers will seem highly convincing, because there is a lot of detail and overall the answer does not look like junk.</p>



<p>An expert will immediately notice discrepancies in the hallucinated answers, and will follow for example the <a href="https://en.wikipedia.org/wiki/List_of_IBM_PS/2_models">List of IBM PS/2 Models</a> article on Wikipedia. Which will very quickly establish that there is no Model 280.</p>



<p>The (non-expert) users who would most benefit from an AI search summary will be the ones most likely misled by it.</p>



<p>How much would you value a research assistant who gives you a different answer every time you ask, and although sometimes the answer may be correct, the incorrect answers look, if anything, more “real” than the correct ones?</p>



<p>When Google says “AI responses may include mistakes”, do not take it lightly. The AI generated summary could be utter nonsense, and just because it sounds convincing doesn’t mean it has anything to do with reality. Caveat emptor!</p>
											</div><div><p>
							This entry was posted in <a href="https://www.os2museum.com/wp/category/ibm/" rel="category tag">IBM</a>, <a href="https://www.os2museum.com/wp/category/ps2/" rel="category tag">PS/2</a>. Bookmark the <a href="https://www.os2museum.com/wp/ai-responses-may-include-mistakes/" title="Permalink to AI Responses May Include Mistakes" rel="bookmark">permalink</a>.													</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valkey Turns One: Community fork of Redis (232 pts)]]></title>
            <link>https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/</link>
            <guid>44140379</guid>
            <pubDate>Fri, 30 May 2025 22:24:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/">https://www.gomomento.com/blog/valkey-turns-one-how-the-community-fork-left-redis-in-the-dust/</a>, See on <a href="https://news.ycombinator.com/item?id=44140379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  
  <article>
    <div>
        
<p>A year ago, Redis Inc (formerly Garantia Data) made a controversial move that disrupted the open source ecosystem: it closed the source of Redis.&nbsp;<a href="https://www.gomomento.com/blog/rip-redis-how-garantia-data-pulled-off-the-biggest-heist-in-open-source-history/" target="_blank" rel="noreferrer noopener">As I wrote at the time</a>, it was a trust-breaking decision that could have shattered the community.</p>



<p>But instead of splintering, the community responded with purpose. Out of that disruption came&nbsp;<a href="https://valkey.io/" target="_blank" rel="noreferrer noopener">Valkey</a>,&nbsp;a fork that took a shot at keeping the community alive.</p>



<h2>A Return, A Reversal</h2>



<p>As part of efforts to rebuild trust with the community, Redis Inc&nbsp;<a href="https://redis.io/blog/welcome-back-to-redis-antirez/" target="_blank" rel="noreferrer noopener">brought back</a>&nbsp;Salvatore Sanfilippo (aka Antirez), the original creator of Redis. I am genuinely excited about his return because it is already impactful. He’s following through on his promise of contributing new features and performance optimizations to Redis. More profoundly,&nbsp;<a href="https://redis.io/blog/agplv3/" target="_blank" rel="noreferrer noopener">Redis 8.0 has been open-sourced again</a>.</p>



<p>Redis acknowledged that adopting <a href="https://en.wikipedia.org/wiki/Server_Side_Public_License" target="_blank" rel="noreferrer noopener">SSPL</a> strained their bond with the community, questioning contributions from others in the same breath.</p>



<blockquote>
<figure><blockquote><p>How do you keep innovating and investing in OSS projects when cloud providers reap the profits and control the infrastructure without proportional contributions back to the projects that they exploit?</p></blockquote></figure>
</blockquote>



<p>The disheartening move from Redis Inc catalyzed an unprecedented wave of collaboration and contributions. Valkey became the test of the community resolve to keep itself together. One year later, Valkey hasn’t just survived –&nbsp;<a href="https://www.linkedin.com/posts/kshams_valkey-rocks-the-most-remarkable-thing-about-activity-7318683448506793985-_qJE" target="_blank" rel="noreferrer noopener">it’s thriving</a>! The Async I/O Threading model <a href="https://github.com/valkey-io/valkey/pull/758" target="_blank" rel="noreferrer noopener">contribution</a> from AWS unlocked 3x+ throughput by fundamentally changing how I/O threads work inside Redis.</p>



<p>But how do these and other contributions compare to Redis 8? Can we hit 1M RPS out of an 8 VCPU instance (c8g.2xl) on either Valkey 8.1 or Redis 8.0 (with 1KB items, 3M items in the key space, and ~500 connections)? It’s time for a bake off!</p>



<h2>Valkey 8.1 vs Redis 8.0: Can the Fork Outrun the Source?</h2>



<p>The punchline: we could not sustain 1M RPS on an 8 VCPU instance with either Valkey or Redis 8.0, but we got really close!!</p>



<p>On a full-tuned c8g.2xl (8 VCPU), Valkey 8.1.1 pushed to 999.8K RPS on SETs with .8ms p99 latency. Redis 8.0 got as high as 729.4 RPS on SETs with .99ms p99 latencies. On each iteration, we tested 50M SETs followed by 50M GETs. We varied connection counts to optimize the maximum throughput for each system.</p>



<figure><blockquote><p><mark>Valkey achieved higher throughput and lower latency across both reads and writes (37% higher on SET and 16% higher on GET), alongside 30% faster p99 latencies for SET and 60%+ faster on GET.</mark></p></blockquote></figure>



<figure><img loading="lazy" decoding="async" width="1024" height="512" src="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20512'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Valkey-vs-Redis-threads-graphic-1-1024x512.png"></figure>



<figure><img loading="lazy" decoding="async" width="1024" height="341" src="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png" alt="Valkey vs Redis table with SET and GET command" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20341'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-300x100.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-768x256.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-18x6.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Maximum-set-and-get-table_v2-1024x341.png"></figure>



<h2>Threading the multi-threading needle</h2>



<p>If I had a penny for every time heard, “but Redis /Valkey is single threaded….”</p>



<p>Antirez’s emphasis on a shared nothing architecture has been foundational for Redis. Nevertheless, as early as 2020, Redis added support for I/O threads. Unfortunately, they did not offer drastic improvement until recently. If you have previously tried and discarded I/O threads, it is time to evaluate again!</p>



<p>On Valkey, we see SET throughput going from 239K RPS without I/O threads to 678K with 6 threads. Meanwhile, p99 latencies dropped from 1.68ms to 0.93ms <strong>despite doing nearly 3x the throughput</strong>! Similarly, Redis went from 235K RPS without I/O threads to 563K RPS with 6 I/O threads. P99s for Redis also dropped around 40% from 1.35ms to 0.84ms.</p>



<p>Two key takeaways emerged:</p>



<ol>
<li>With two threads, gains were modest (~20%). The impact only really surfaced at three threads and beyond.</li>



<li>Redis and Valkey were neck-and-neck until the fourth thread. After that, Valkey pulled away sharply.</li>
</ol>



<figure><img loading="lazy" decoding="async" width="1024" height="512" src="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20512'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-300x150.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-768x384.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-18x9.png 18w, https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2.png 1200w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/Set-performance-with-IO-threads_v2-1024x512.png"></figure>



<h3>SET Performance on Valkey with IO/Threads &amp; 256 Connections:</h3>



<figure><img loading="lazy" decoding="async" width="700" height="500" src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w" sizes="(max-width: 700px) 100vw, 700px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20500'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey-18x12.png 18w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-Performance-on-Valkey.png"></figure>



<h3>SET Performance on Redis with IO/Threads &amp; 256 Connections:</h3>



<figure><img loading="lazy" decoding="async" width="700" height="500" src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png" alt="" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w" sizes="(max-width: 700px) 100vw, 700px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20700%20500'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png 700w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-300x214.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis-18x12.png 18w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/SET-performance-on-Redis.png"></figure>



<h2>Pushing Valkey Throughput Further</h2>



<p>In the previous section, we saw that Valkey could hit 678K RPS on SETs with 6 threads and 256 connections. If we up the connections to 400, the throughput goes up to 832K RPS. How did we get the additional 167K RPS?</p>



<p>We used <a href="https://github.com/iopsystems/rezolus" target="_blank" rel="noreferrer noopener">Rezolus</a>, our favorite Linux performance telemetry agent, to get deep insights into the system under stress. You can see in the charts below that overall CPU utilization is around 80% and unevenly distributed across the 8 cores.</p>



<p>Diving deeper, this is driven by hardware interrupts from network queues across all 8 cores. Interrupts are bad because they disrupt a hard working Valkey thread to yield to handle network packets.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="835" src="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png" alt="CPU Usage chart" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201024%20835'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png 1024w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-300x245.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-768x627.png 768w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1-15x12.png 15w, https://www.gomomento.com/wp-content/uploads/2025/05/image-1.png 1445w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/image-1-1024x835.png"></figure>



<p>What if we could avoid the context switching on our <code><sup>c8g.2xl</sup></code> with 8 cores? Running close to a million RPS requires considerable packet processing horsepower. Luckily, since a lot of work happens at the Nitro level on EC2 instances, two allocated cores to IRQs is all you need (if you let them focus). Pinning the IRQs to two cores is pretty straightforward.</p>



<pre><code>sudo ethtool -L ens34 combined 2 # reduce to 2 IRQs
grep ens34 /proc/interrupts # ours were on 99 and 100
echo 1 | sudo tee /proc/irq/99/smp_affinity # pin 99 to core 1
echo 2 | sudo tee /proc/irq/100/smp_affinity # pin 100 to core 2</code></pre>



<p>But how do we let these threads focus? and how do we avoid Redis / Valkey threads contending for the same cores? We pin Redis/Valkey to cores 3-8, giving their IO-Threads better isolation while also allowing the IRQs to focus. We used the <code><sup>--cpuset-cpus</sup></code> Docker flag to set these CPU assignments, making sure that Redis and Valkey process stayed pinned to the intended cores throughout the test. This reduces cross-core contention and improves cache locality, <strong>both of which are critical for minimizing tail latencies at high throughput</strong>. Ideal core allocation can vary in multi-tenant environments or mixed workloads, but in this benchmark it provided clean isolation between system and application workloads.</p>



<p><strong>Redis:</strong></p>



<pre><code>docker run --network="host" --rm \
  --cpuset-cpus="2-7" redis:8.0 \
  --save "" --appendonly no \
  --io-threads 6  \
  --protected-mode no --maxmemory 10gb</code></pre>



<p><strong>Valkey:</strong></p>



<pre><code>docker run --network="host" --rm \
  --cpuset-cpus="2-7" valkey/valkey:8.1.1 \
  --save "" --appendonly no --io-threads 6 \
  --protected-mode no --maxmemory 10gb</code></pre>



<p>Let’s see what Rezolus has to say about the new setup with IRQs pinned to the first 2 cores and Valkey pinned to the remaining 6 cores. First, we observed meaningfully higher CPU Utilization. Second, looking at the bottom chart (SoftIRQ), we see that it is now limited to only the first two cores. Third, the Valkey cores are running red hot, whereas we previously saw a much more scattered distribution one usage across cores. <strong>While this setup is ideal for this benchmark, optimal IRQ tuning depends heavily on NIC architecture and the concurrency model of your application.</strong></p>



<figure><img loading="lazy" decoding="async" width="720" height="603" src="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png" alt="CPU Chart 2" srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w" sizes="(max-width: 720px) 100vw, 720px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20720%20603'%3E%3C/svg%3E" data-lazy-srcset="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png 720w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-300x251.png 300w, https://www.gomomento.com/wp-content/uploads/2025/05/image-2-14x12.png 14w" data-lazy-src="https://www.gomomento.com/wp-content/uploads/2025/05/image-2.png"></figure>



<p>The extra 20% CPU utilization is what buys us the extra 167K RPS (from 832K RPS to 999.8K RPS).</p>



<h2>Try it Yourself (And Know Before You Go)</h2>



<p>These benchmarks are hopefully a beginning and not the end. Our hope is that this sets up both Valkey and Redis communities to continue the performance improvement journey. We also recognize that many folks may want to reproduce the benchmark in their own environments, incorporating their workflow specifics. Below, we outline some key instructions that you can use to reproduce this in your own AWS account within an hour.</p>



<p><strong>Instance Types:</strong>&nbsp;We used AWS Graviton4-based&nbsp;<code><sup>c8g</sup></code>&nbsp;instances, launched in September 2024. The&nbsp;<sup><code>c8g.2xlarge</code></sup> server node provides 8 vCPUs (costing roughly $250/month in us-east-1), while the&nbsp;<code><sup>c8g.8xlarge</sup></code>&nbsp;load generator offers 32 vCPUs. This provided enough CPU headroom to cleanly isolate the benchmark workload, IRQ handling, and Redis/Valkey processing. The same c8g.2xl instance was used to run valkey and redis (one at a time). The same load gen node was run each time. Valkey and Redis were restarted right before each test to ensure fairness.</p>



<p><strong>Placement Groups:</strong>&nbsp;We used EC2 placement groups (cluster mode) to ensure minimal network jitter and low-latency communication between the client and server nodes. Placement groups offer extremely tight latencies by reducing the number of hops between your EC2 instances. This has the upside on higher throughput, fewer interruptions, and lower latencies – but it has some shared fate / blast radius implications that are worth considering before deploying them in your production environment.</p>



<p><strong>Core Pinning.</strong> To see the highest throughput and lowest latencies, consider core pinning and reducing the IRQs. See section above for specific instructions we used on our 8 core instance. It is also important to apply similar techniques on your test nodes.</p>



<p><strong>Vary the connections.</strong> Connection count is a surprisingly crucial variable for both Redis and Valkey. In fact, latencies rise steeply as you approach 1024 connections on both of them. For example, going from 400 to 1024 connections, Valkey’s SET throughput dropped from 999.9K RPS with to 969K RPS and p99 latencies doubled from .8ms to 1.6ms (at 2048 conns, p99 latencies triple). Going from 384 connections to 1024, Redis throughput drops from 729.4K RPS to 668K RPS, and p99 latencies more than double from .99ms to 2.5ms. Lower throughput with higher latencies? You get why connection count tuning is so crucial here.</p>



<p><strong>Key Space.</strong> If you want the best numbers, <a href="https://www.linkedin.com/posts/yaoyue-thinkingfish_my-performance-rant-of-the-day-if-you-are-activity-7326350824261980161-qB6h/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAANW9wBFDc3gQ3Jwp6YswZ_BARGfJvyJQQ" target="_blank" rel="noreferrer noopener">use smaller values and a really small key space</a> (<sup><code>-r 10000</code></sup>). This will help you get everything from L3 cache. To make this test slightly more real world, we used 1KB items (<code><sub><sup>-d 1024</sup></sub></code>) and a key space of 3Million (<code><sup>-r 3000000</sup></code>).</p>



<p><strong>Multi-Thread the Benchmark App.</strong> To get the maximum throughput out of valkey-benchmark, make sure to turn on multi-threading on the benchmark tool as well. The <code><sup>--threads 6</sup></code> flag tells valkey-benchmark to run in multi-threaded mode.</p>



<p><strong>Benchmark command:</strong></p>



<pre><code>docker run --network="host" --rm --cpuset-cpus="2-7" \
valkey/valkey:8.0.1 valkey-benchmark \
-h 172.31.4.92 -p 6379 -t SET,GET -n 100000000 -c 256 \
-r 3000000 --threads 6 -d 1024</code></pre>



<h2>A Final Caveat: Benchmarking is imprecise in nature</h2>



<p>We made every effort to make this benchmark resemble more real world workflows, but you can always do better. Valkey-bench is not perfect (nothing is). We have a wishlist of improvements (and so <a href="https://github.com/valkey-io/valkey/issues/900" target="_blank" rel="noreferrer noopener">does</a> the Valkey project).</p>



<p>First, today, it simply pushes as much load as the server is able to handle instead of targeting a particular TPS. The real world rarely modulates its throughput based on your latency. Second, once it can target specific load, it would become closer to real world if it could modulate the load to show spikes and troughs as opposed to running a consistent throughput profile. Lastly, it’s rare to see 100% GETs or 100% SETs in a Key Value cache workflow. We’d love to provide a SET:GET ratio to see how the system reacts.</p>



<p>At Momento, we typically do our testing using <a href="https://github.com/iopsystems/rpc-perf" target="_blank" rel="noreferrer noopener">rpc-perf</a>. It is written entirely in rust, handles more real world scenarios (like the three feature requests above), and pairs incredibly well with Rezolus. Regardless, even rpc-perf is a synthetic benchmark and even though it gives you more degrees of freedom to simulate production workflows, the results should not be interpreted as generally applicable to every workflow. Small variables make huge differences – and simulations are no match for production.</p>



<h2>Final Thoughts: Performance Is a Practice</h2>



<p>Valkey has not only kept pace – it’s setting it. Meanwhile, the performance ceiling keeps rising. But getting there isn’t automatic. It requires expertise across systems, infrastructure, and workload behavior.</p>



<p>At Momento, we help teams achieve this kind of performance every day. Whether you’re running Valkey, Redis, or evaluating your options – we’re here to help you scale with confidence.</p>



<p><strong>Want help tuning your real-time infrastructure? <a href="https://gomomento.com/contact-us" target="_blank" rel="noreferrer noopener">Let’s talk.</a></strong></p>







<hr>



<p><strong>Special thanks to Yao and Brian from&nbsp;<a href="https://iop.systems/" target="_blank" rel="noreferrer noopener">IOP Systems</a>&nbsp;for providing the tools, including rpc-perf and rezolus, as well as insights for this benchmark.</strong></p>




        
      </div>

  </article>
  


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Silicon Valley finally has a big electronics retailer again: Micro Center opens (265 pts)]]></title>
            <link>https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx</link>
            <guid>44140378</guid>
            <pubDate>Fri, 30 May 2025 22:24:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx">https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx</a>, See on <a href="https://news.ycombinator.com/item?id=44140378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p dir="ltr">After years of waiting, the ribbon has been cut and Micro Center Silicon Valley is officially open.&nbsp;</p>
<p dir="ltr">On a sunny Friday morning in Santa Clara, with hundreds of fans queued in a line wrapping down the block and around the corner, we welcomed the Silicon Valley community to our newest store, at <a href="https://www.microcenter.com/site/stores/santa-clara.aspx">5201 Stevens Creek Blvd</a>.&nbsp;</p>
<p dir="ltr">If you're a DIY PC builder, a serious gamer, a creator, a maker, or just someone who gets excited over the latest CPUs, GPUs, and 3D printers, then you already know what Micro Center is about</p>
<p dir="ltr">The Bay Area sets a high bar for all things tech, and here you'll find aisles stacked high with components, knowledgeable staff who actually know what they're talking about, and a hands-on experience you just can't replicate online.&nbsp;</p>
<p dir="ltr">The grand opening celebration also features special promotions, including 20% off Windows desktops and laptops and 20% off monitors. Additionally, the store has over 4,000 graphics cards in stock, including exclusive models, that will available for our grand opening event. For more information, please visit the <a href="https://www.microcenter.com/site/stores/santa-clara.aspx">Micro Center Santa Clara page</a>.</p>
<p><img src="https://60a99bedadae98078522-a9b6cded92292ef3bace063619038eb1.ssl.cf2.rackcdn.com/images_mc-sc-1.jpg" width="1280" height="720" alt="A scene from the MC Santa Clara grand opening. "></p>
<p><span>Photo: Dan Ackerman&nbsp;</span></p>

<p><img src="https://60a99bedadae98078522-a9b6cded92292ef3bace063619038eb1.ssl.cf2.rackcdn.com/images_mc-sc-2.jpg" width="1280" height="720" alt="A scene from the MC Santa Clara grand opening. "></p>
<p><span>Photo: Dan Ackerman&nbsp;</span></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[C++ to Rust Phrasebook (142 pts)]]></title>
            <link>https://cel.cs.brown.edu/crp/</link>
            <guid>44140349</guid>
            <pubDate>Fri, 30 May 2025 22:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cel.cs.brown.edu/crp/">https://cel.cs.brown.edu/crp/</a>, See on <a href="https://news.ycombinator.com/item?id=44140349">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox></mdbook-sidebar-scrollbox>
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                <div id="menu-bar">
                    

                    <h2>C++ to Rust Phrasebook</h2>

                    
                </div>

                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        <h2 id="c-to-rust-phrasebook"><a href="#c-to-rust-phrasebook">C++ to Rust Phrasebook</a></h2>
<p>This book is designed to help C++ programmers learn Rust. It provides translations of common C++ patterns into idiomatic Rust. Each pattern is described through concrete code examples along with high-level discussion of engineering trade-offs.</p>
<p>The book can be read front-to-back, but it is designed to be used random-access.
When you are writing Rust code and think, "I know how to do this in C++ but not Rust," then
look for the corresponding chapter in this book.</p>
<p>This book was hand-written by expert C++ and Rust programmers at Brown University's <a href="https://cel.cs.brown.edu/">Cognitive Engineering Lab</a>. Our goal is provide accurate information with a tasteful degree of detail. No text in this book was written by AI.</p>
<p>If you would like updates on when we add new chapters to this book, you can <a href="https://forms.gle/rcrdZihmT81LWy6F6">drop your email here</a>.</p>
<h2 id="other-resources"><a href="#other-resources">Other resources</a></h2>
<p>If you have zero Rust experience, you might consider first reading <a href="https://rust-book.cs.brown.edu/">The Rust Programming
Language</a> or getting a quick overview at <a href="https://learnxinyminutes.com/rust/">Learn X in Y Minutes</a>.</p>
<p>If you are primarily an embedded systems programmer using C or C++, this book is
a complement to <a href="https://docs.rust-embedded.org/book/">The Embedded Rust Book</a>.</p>
<p>Compared to resources like the <a href="https://doc.rust-lang.org/nomicon/">Rustonomicon</a> and <a href="https://rust-unofficial.github.io/too-many-lists/">Learn Rust With Entirely Too Many Linked Lists</a>, this book is less about "Rust behind the scenes" and more about explicitly describing how Rust works in terms of C++.</p>
<h2 id="feedback-on-this-book"><a href="#feedback-on-this-book">Feedback on this book</a></h2>
<p>At the bottom of every page there is a link
to a form where you can submit feedback: typos, factual errors, or any other issues you spot.</p>
<p>If you answer the quizzes at the end of each chapter, we will save your
responses anonymously for research purposes.</p>


                        <a href="https://docs.google.com/forms/d/e/1FAIpQLScoygeNlygODY2owQ-HvU8VGx3hi50aic7ZlKCyhJ0VktjiCg/viewform?usp=pp_url&amp;entry.1450251950=C++%20to%20Rust%20Phrasebook">Click here to leave us feedback about this page.</a>
                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next prefetch" href="https://cel.cs.brown.edu/crp/idioms/constructors.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">

                    <a rel="next prefetch" href="https://cel.cs.brown.edu/crp/idioms/constructors.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div>


        <!-- Google tag (gtag.js) -->
        
        


        


        
        
        

        
        
        

        <!-- Custom JS scripts -->
        


    </div>
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Photos taken inside musical instruments (815 pts)]]></title>
            <link>https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments</link>
            <guid>44139626</guid>
            <pubDate>Fri, 30 May 2025 20:32:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments">https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments</a>, See on <a href="https://news.ycombinator.com/item?id=44139626">Hacker News</a></p>
Couldn't get https://www.dpreview.com/photography/5400934096/probe-lenses-and-focus-stacking-the-secrets-to-incredible-photos-taken-inside-instruments: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Jerry Lewis's "The Day the Clown Cried" discovered in Sweden after 53 years (168 pts)]]></title>
            <link>https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/</link>
            <guid>44139592</guid>
            <pubDate>Fri, 30 May 2025 20:27:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/">https://www.thenationalnews.com/arts-culture/film-tv/2025/05/29/jerry-lewis-day-the-clown-cried-discovered/</a>, See on <a href="https://news.ycombinator.com/item?id=44139592">Hacker News</a></p>
<div id="readability-page-1" class="page"><article data-identifier="article-body-chain" id=""><p id="el-0-3ZTUS3Y2ZRCBTAOIX7Z4SUTSP4">One of cinema's most sought-after <a href="https://www.thenationalnews.com/arts-culture/film-tv/2024/11/25/lost-john-ford-film-the-scarlet-drop-chile/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/film-tv/2024/11/25/lost-john-ford-film-the-scarlet-drop-chile/">lost films</a> has been discovered after having been kept secretly in the collection of a Swedish actor for 45 years. </p><p id="el-1-BRMSRUAMDZBGHFZU3QXU2IMB5I">Comedian <a href="https://www.thenationalnews.com/arts/jerry-lewis-women-are-funny-but-not-when-crude-1.255081" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts/jerry-lewis-women-are-funny-but-not-when-crude-1.255081">Jerry Lewis</a>'s controversial holocaust film <i>The Day the Clown Cried,</i> shot in 1972 but never released, was thought to <a href="https://www.thenationalnews.com/arts-culture/2024/04/02/jerry-lewis-the-day-the-clown-cried-loc/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/2024/04/02/jerry-lewis-the-day-the-clown-cried-loc/">not exist in finished form</a>.</p><p id="el-3-XA2MPSNDZBCRNC6TCMNBHJA4HA">But Hans Crispin, star of the beloved 1980s Swedish TV series <i>Angne &amp; Svullo</i>, claims he stole a complete workprint of the film from the archives of its production studio in 1980 – and has been screening it for guests in his apartment ever since. </p><p id="el-4-PE4RPU5IJRHFFNIBRAGW3UKI2I">“I have the only copy,” Crispin told <a href="https://www.thenationalnews.com/arts-culture/film-tv/2024/08/30/israel-palestine-on-swedish-tv-review/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/film-tv/2024/08/30/israel-palestine-on-swedish-tv-review/">Swedish state news broadcaster SVT.</a> “I stole it from Europafilm in 1980 and copied it to VHS in the attic where we copied other films at night.</p><p id="el-5-OUFA3ROWJNGGTKG3VEOQRQSPQ4">“I've kept the copy in my bank vault,” Crispin added.</p><div id="el-7-LTK64SHFGFDSHHFSSCB34R3BLE"><figure><picture><source width="800" height="534" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=800&amp;height=534"><source width="600" height="400.49999999999994" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=600&amp;height=400"><source width="400" height="267" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=400&amp;height=267"><img _id="LTK64SHFGFDSHHFSSCB34R3BLE" type="image" originalwidth="3072" originalheight="2048" alt="Swedish actor Hans Crispin has a complete workprint of The Day The Clown Cried, SVT has reported. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/LTK64SHFGFDSHHFSSCB34R3BLE.jpg?smart=true&amp;auth=764112cb59e4b7491bb7e89dbe7523f067eb7a46eaf5fc57fcbcce039b925034&amp;width=400&amp;height=267" width="400" height="267"></picture><div><figcaption>Swedish actor Hans Crispin has a complete workprint of The Day The Clown Cried, SVT has reported. AFP</figcaption></div></figure></div><p id="el-8-6S5ML4BMEVAN7GJ3ZE2N3RA2IA">Crispin recently screened a full copy to journalists from SVT and Sweden's <i>Icon </i>magazine to prove his claim was true. </p><p id="el-9-VVTE7Y2KHJEPTMOUWREYMIGM6Y">“You're the 23rd and 24th people I've shown it to,” he told <i>Icon </i>and SVT. </p><p id="el-10-4ETIS7JGXNAZ7JI52PNYJP6J7E">The actor also revealed that his initial copy was missing the opening six-minute sequence of the film shot in Paris, which was mailed to him anonymously in 1990, along with a note saying that the sender knew he possessed a copy of the rest of the film.</p><h2 id="el-11-K3BGGW4MCZG5NCBMUBIKKXWOBY"><b>Will The Day The Clown Cried be released to the public?</b></h2><p id="el-12-EAZWT3KXKVBPHORFC7PVEXUDB4">Now that he has come out into the open, Crispin intends to make his copy available for the world to see, saying: “It must be seen!”</p><p id="el-13-4SFSIGNF7ZBKDPKNAD6PNME6BQ">Crispin added: “I think I want to hand it over to the next generation. With today's technique, it can be restored. I want to sell it to a serious producer who either restores it or keeps it locked away, or restores it and shows it to people for studying purposes.”</p><p id="el-14-W6Z67YK6XBFQTLO55J3YVKXUUA">The film tells the story of a German circus clown who is imprisoned in a Nazi concentration camp for mocking <a href="https://www.thenationalnews.com/arts-culture/books/hitler-a-bizarrely-sympathetic-biography-1.360922" target="_blank" rel="">Adolf Hitler</a> and is then forced to lure children to their deaths as punishment.</p><div id="el-15-RTWN5IS6O5DRXGUGUCC3P7VSDY"><figure><picture><source width="800" height="534" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=800&amp;height=534"><source width="600" height="400.49999999999994" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=600&amp;height=400"><source width="400" height="267" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=400&amp;height=267"><img _id="RTWN5IS6O5DRXGUGUCC3P7VSDY" type="image" originalwidth="3072" originalheight="2048" alt="The footage that Lewis possessed of the Day The Clown Cried was donated to the Library of Congress in 2015. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/RTWN5IS6O5DRXGUGUCC3P7VSDY.jpg?smart=true&amp;auth=7ae3e215b29972c42efae31fb4f1a4c1b1a3d8029c4ddff1cefd4875061e4741&amp;width=400&amp;height=267" width="400" height="267"></picture><div><figcaption>The footage that Lewis possessed of the Day The Clown Cried was donated to the Library of Congress in 2015. AFP</figcaption></div></figure></div><p id="el-16-S5AOZL6KS5GTLJWT4CE6SEYBYU">Lewis, who directed and starred in the film as clown Helmut Doork, donated five hours of footage to the US Library of Congress in 2015, adding a stipulation that it not be made available until June 2024. </p><p id="el-17-OOAFYHIV6RFHPG5KNF6OAFFGW4">The footage, which has been made available to scholars, was screened last August for <i>The New Republic</i> journalist Benjamin Charles Germain Lee, who reported that the footage was fragmentary and does not constitute a complete film, leading the industry to conclude that the full film did not exist.</p><h2 id="el-18-MITN4F2L5REQ3JS7QLNNV3PABI"><b>Why the film was never released</b></h2><p id="el-19-S2OYFSEKX5BOVGW2SGJI2FOUKU">While there were myriad alleged issues during the shoot itself, problems reportedly arose between Lewis and producer Nat Wachsberger once filming stopped, which is considered the main catalyst for the film's shelving.</p><p id="el-20-MSVX4YUHK5FY7BSCX2CQK25UKI">Lewis was reportedly unsatisfied with the film’s financing and announced that Wachsberger did not fulfil his financial obligations. Hearing this, Wachsberger threatened to sue Lewis for breach of contract, which resulted in a fallout between the two that caused Lewis to leave with a rough cut of the film, according to a 2018 feature in <i>The New York Times.</i></p><p id="el-21-N7PHIBCFE5D77FCB3E74WKAFVI">Lewis had mixed feelings about the film, showing fragments of his footage to close friends. However, in his 1982 autobiography, Lewis said “the picture must be seen”.</p><div id="el-22-XG4SDP7YJZGFJJ3L75JCPD7G5E"><figure><picture><source width="800" height="1200" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=800&amp;height=1200"><source width="600" height="900" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=600&amp;height=900"><source width="400" height="600" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=400&amp;height=600"><img _id="XG4SDP7YJZGFJJ3L75JCPD7G5E" type="image" originalwidth="2048" originalheight="3072" alt="No complete copy of The Day The Clown Cried has ever been confirmed to exist until now. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/XG4SDP7YJZGFJJ3L75JCPD7G5E.jpg?smart=true&amp;auth=1184b9a1d6e572d7bbeb8eaeaf5d9cb85b539359fff54ef9a28d0def0a831bd8&amp;width=400&amp;height=600" width="400" height="600"></picture><div><figcaption>No complete copy of The Day The Clown Cried has ever been confirmed to exist until now. AFP</figcaption></div></figure></div><p id="el-23-OVBRR24ZMBGJRPPTNNZU3LT42A">After watching it, <a href="https://www.thenationalnews.com/lifestyle/fashion/2021/10/04/the-simpsons-make-their-fashion-week-debut-for-balenciaga-in-paris/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/lifestyle/fashion/2021/10/04/the-simpsons-make-their-fashion-week-debut-for-balenciaga-in-paris/"><i>The Simpsons</i></a> voice actor Harry Shearer said it was “a perfect object”, adding: “This movie is so drastically wrong, its pathos and its comedy are so wildly misplaced, that you could not, in your fantasy of what it might be like, improve on what it really is.”</p><p id="el-24-WUNXMZT2DJA5FJATYEOXNWFXIM">In an interview with <i>The New York Times</i> in 2018, Chris Lewis, the comedian's son, said: “It was something that was very close to his heart.”</p><p id="el-25-4F5RPU7YZFBLJOLZNVFPSA56VY">At other times, however, Lewis denounced the film. In 2013, footage of him surfaced on YouTube in which he stated: “It was bad, and it was bad because I lost the magic. No one will ever see it, because I'm embarrassed at the poor work.”</p><h2 id="el-26-ME2KIY2R4ZHJPFY7G33OKZTQBM"><b>The history of lost films</b></h2><p id="el-27-ZBOL6RP5OJD3FK44FZKBL72GLE"><i>The Day the Crown Cried</i> is an example of one of many films that were once thought lost or not fit for public screening.</p><p id="el-28-APHSELV6IRGRRDFZ5L2ZJ4WZX4">Similar films include 1976’s <i>Chess of the Wind </i>by Iranian director <a href="https://www.thenationalnews.com/arts-culture/film/the-7-middle-eastern-films-to-see-at-the-london-film-festival-1.1078995" target="_blank" rel="">Mohammad Reza Aslani.</a></p><p id="el-29-GQ27P642NBDRHNGGYANXB6GKE4">Until it was rediscovered in 2020, the film could only be watched on low-quality VHS tapes. Since then, it has been restored and screened around the world.</p><p id="el-30-KDOD53KPJND4HDABN7NCEBPXOU">One of the best-known lost films is <i>The Passion of Joan of Arc</i> from 1928. After being lost for years, a copy was found in a Norwegian hospital in the 1980s. The film is now considered one of the most important historical film artefacts.</p><div id="el-31-U6C2G5A4HZG7FBM3YI73DKVZ6A"><figure><picture><source width="800" height="1064" media="(min-width: 1024px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=800&amp;height=1064"><source width="600" height="798" media="(min-width: 768px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=600&amp;height=798"><source width="400" height="532" media="(min-width: 350px)" srcset="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=400&amp;height=532"><img _id="U6C2G5A4HZG7FBM3YI73DKVZ6A" type="image" originalwidth="3120" originalheight="4146" alt="The Day The Clown Cried depicts a clown entertaining children during the holocaust. AFP" data-chromatic="ignore" loading="lazy" fetchpriority="low" src="https://www.thenationalnews.com/resizer/v2/U6C2G5A4HZG7FBM3YI73DKVZ6A.jpg?smart=true&amp;auth=d6669dba0199da56108da2efd0f85a3cec33679b83aa0caf770e3e51c8b2ad48&amp;width=400&amp;height=532" width="400" height="532"></picture><div><figcaption>The Day The Clown Cried depicts a clown entertaining children during the holocaust. AFP</figcaption></div></figure></div><p id="el-32-EBP67AN5B5G2BGXRNZMD7JXHZU"><i>London After Midnight</i>, a 1927 horror film directed by Tod Browning starring Lon Chaney, is still a veritable white whale for fans after the last-known copy was destroyed in the 1965 MGM vault fire.</p><p id="el-33-7USMYYXBQZCJZPRD3RLG3SWOJM">Other films that have not yet screened because of filmmaker stipulations include <i>100 Years</i> starring <a href="https://www.thenationalnews.com/arts-culture/film/the-many-faces-of-the-talented-mr-malkovich-1.762364" target="_blank" rel="">John Malkovich</a>. The short film is from 2015 but has been placed in time-locked safes that won’t open until 2115, 100 years after the film was made.</p><p id="el-34-S3T4RD37A5F5FOWZEANBINWKHY">Several recently produced films are now <a href="https://www.thenationalnews.com/arts-culture/2024/08/05/fantastic-four-abandoned-hollywood-films/" target="_blank" rel="noreferrer" title="https://www.thenationalnews.com/arts-culture/2024/08/05/fantastic-four-abandoned-hollywood-films/">considered lost media,</a> including 2022's <i>Batgirl</i>, directed by <a href="https://www.thenationalnews.com/arts-culture/film/who-are-adil-el-arbi-and-bilall-fallah-moroccan-belgian-directors-behind-the-new-batgirl-film-1.1226356" target="_blank" rel="">Adil El Arbi and Bilall Fallah</a>. The superhero film stars Leslie Grace as <a href="https://www.thenationalnews.com/arts-culture/television/2022/08/04/why-was-the-90-million-batgirl-killed-and-could-hbo-max-be-next/" target="_blank" rel="">Batgirl</a> and also includes J K Simmons, Brendan Fraser and Michael Keaton. </p><p id="el-35-4EAHPEUEANBW5JH6D7RZ7BDGIM">Warner Bros Discovery announced in August 2022 that it would not be released due to cost-cutting measures and a strategy shift towards theatrical releases.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Surprisingly Fast AI-Generated Kernels We Didn't Mean to Publish (Yet) (329 pts)]]></title>
            <link>https://crfm.stanford.edu/2025/05/28/fast-kernels.html</link>
            <guid>44139454</guid>
            <pubDate>Fri, 30 May 2025 20:03:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://crfm.stanford.edu/2025/05/28/fast-kernels.html">https://crfm.stanford.edu/2025/05/28/fast-kernels.html</a>, See on <a href="https://news.ycombinator.com/item?id=44139454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    


<div>
    <p><a href="https://www.stanford.edu/" target="_blank">
            <img src="https://crfm.stanford.edu/static/img/header/stanford-white.png">
        </a>
    </p>
</div>

<header>
    <nav>
        <div>
            <div>
                <p><a href="https://crfm.stanford.edu/">
                    <img src="https://crfm.stanford.edu/static/img/header/crfm-rgb.png">
                </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://hai.stanford.edu/" target="_blank">
                    <img src="https://crfm.stanford.edu/static/img/header/hai.png">
                </a></p>
            </div>
            <div>
                <ul>
                    <li>
                        <a href="https://crfm.stanford.edu/people.html">People</a>
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/report.html">Report</a>
                    </li>
                    <li id="dropdownContainer">
                        <a id="dropdownButton">Research</a>
                        
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/policy.html">Policy</a>
                    </li>
                    <li>
                        <a href="https://crfm.stanford.edu/blog.html">Blog</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    
    
</header>

		<div>
				

  <h2>Surprisingly Fast AI-Generated Kernels We Didn’t Mean to Publish (Yet)</h2>
  
  
  
  
  

  

  <hr>

  

  <div>
    <h2 id="tldr">TL;DR</h2>

<p>We have some very fast AI-generated kernels in pure CUDA-C without using libraries and DSLs such as CUTLASS and Triton. They are performing close to or in some cases even beating the standard expert-optimized production kernels shipped in PyTorch. Some of our highlighted results:</p>

<ul>
  <li><strong>Matmul (FP32): 101.3%</strong> performance of FP32 torch.matmul; problem size: 4096x4096 square matrices</li>
  <li><strong>Conv2D: 179.9%</strong> performance of FP32 torch.nn.Conv2D; problem size: (100, 3, 224, 224) input tensor, conv(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)</li>
  <li><strong>Softmax: 111.8%</strong> performance of FP32 torch.softmax; problem size: (4096, 65536) input tensor</li>
  <li><strong>LayerNorm: 484.4%</strong> performance of FP32 torch.nn.LayerNorm; problem size: (16, 64, 256, 256) input tensor</li>
  <li><strong>Conv2D + ReLU + MaxPool: 290.1%</strong> performance of FP32 torch reference, 189.0% performance of FP32 torch.compile() reference; problem size: (100, 3, 224, 224) input tensor, conv(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2), maxpool(kernel_size=3, stride=2)</li>
</ul>

<p>(Our results are benchmarked on an Nvidia L40S GPU, and % performance is defined as reference time divided by generated kernel time)</p>

<p><img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/untiled.png" width="60%"><br>
<small><em>“Untiled” by DALL·E (2025). (Digital pigment on virtual canvas)<br>From the MMA collection</em></small></p>

<h2 id="intro">Intro</h2>

<p>We started with the goal of generating synthetic data to train better kernel generation models. Somewhere along the way the unexpected happened: the test-time only synthetic data generation itself started producing <em>really</em> good kernels beating or performing close to human expert optimized PyTorch baselines, utilizing advanced optimizations and hardware features, which were previously thought to be challenging. As a result, we decided to write this blog post early and share our findings. The point of this blog post isn’t about a novel methodology; in fact, our synthetic data generation design is simple, and what’s surprising is that it is already showing promise.</p>

<p>In this post, we’re sharing the method, five optimized kernels (4 foundational ML operators + 1 fused kernel of an AlexNet block), an example optimization trajectory, and some takeaways and thoughts on what this might mean for performant kernel generation. Consider this a first step in what’s next.</p>

<h2 id="method">Method</h2>

<p>We’re using the <a href="https://arxiv.org/abs/2502.10517">KernelBench</a> (a benchmark for AI based kernel generation that we released in December 2024) task setup: given torch code, the LLM writes custom kernels to replace the torch operators with the goal of getting a speedup. Consistent with the original KernelBench design, the reference code is in the default FP32, and given a tolerance threshold (1e-02), using lower precision solutions is valid. In addition, each problem in KernelBench has specific sizes since there are many size-specific optimizations, so the benchmark tests for the fastest kernel for the specific problem size, not necessarily a generally fast kernel for any arbitrary problem size. We run both the torch reference code and the generated code, and test for correctness by checking the numerical equality of the two outputs over many random inputs.<br>
<img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/kernelbench_design.png" width="100%"></p>

<p>The most common way people scale test-time compute for this problem of optimizing kernels today is through sequential revision, a multi-turn loop where a model incrementally edits a kernel, checks for correctness and performance, then tries again based on the result, either fixing the kernel or try to improve its performance. This loop is intuitive and easy to implement. The model fixes broken kernels, tweaks working ones, and gradually climbs toward something faster.</p>

<p>The main limitation of this approach is the lack of optimization idea diversity. Sequential loops often fall into local minima, revisiting the same classes of transformations or endlessly refining unpromising trajectories. The result is inefficient use of test-time compute and little pressure on the model to generate fundamentally new optimization ideas.</p>

<p>We introduced two key changes to address this:</p>

<ol>
  <li>Reasoning in natural language about optimization ideas: rather than directly generating new kernels in each step, we generate optimization ideas in natural language conditioned on previously attempted ideas, and realize those ideas into new code variants.</li>
  <li>Branching at each optimization step: instead of refining a single candidate per step, we fan out such that each idea spawns multiple implementations, and the highest-performing kernels are used to seed the next round (we also keep a bank of good existing kernels for seeding). This unlocks massive parallelism allowing us to explore radically different directions at each turn, rather than getting stuck in a narrow optimization path.</li>
</ol>

<p><img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/search.png" width="100%"></p>

<p>The result is a test-time loop that looks less like “chat with a compiler” in the case of sequential revision, and more like structured exploratory search, guided by explicit optimization hypotheses and aggressively parallel evaluation.</p>

<p>We ran 10 problems from KernelBench level 1 (and modified the problem sizes to make sure that kernel launch overhead is negligible compared to the overall runtime of the problem). We ran 5 rounds with the OpenAI o3 and Gemini 2.5 Pro models. The plot below shows the distribution of rounds in which the best-performing kernel was first found. Most of the best results emerge in later rounds (out of a total of 5 rounds), with the majority coming in round 4 or 5.
<img src="https://crfm.stanford.edu/static/img/posts/2025-05-28-fast-kernels/rounds.png" width="100%"></p>

<p>As we scaled up our search, we also found that many high-performing kernels clustered into a few recurring optimization strategies, which also aligns with our experience of writing kernels by hand. The main optimization categories are summarized below:</p>

<ul>
  <li><strong>Memory Access Optimization:</strong> improving the efficiency of data movement between different memory hierarchies (global memory, shared memory, registers) and ensuring data is accessed in a way that maximizes bandwidth and minimizes conflicts.</li>
  <li><strong>Asynchronous Operations &amp; Latency Hiding:</strong> hide the latency of slow operations (like global memory access) by overlapping them with computation or other memory transfers</li>
  <li><strong>Data Type &amp; Precision Optimization:</strong> using lower-precision data types (like FP16 or BF16) where possible to reduce memory bandwidth requirements, increase cache effectiveness, and potentially leverage specialized hardware units.</li>
  <li><strong>Compute &amp; Instruction Optimization</strong>: making the arithmetic computations themselves more efficient, reducing instruction count, or leveraging specialized hardware instructions</li>
  <li><strong>Parallelism &amp; Occupancy Enhancement</strong>: maximize the number of active warps on the Streaming Multiprocessors (SMs) to better hide latencies and improve overall throughput</li>
  <li><strong>Control Flow &amp; Loop Optimization</strong>: reducing the overhead associated with loops, branches, and indexing calculations</li>
</ul>

<h2 id="an-example-kernel-optimization-trajectory">An Example Kernel Optimization Trajectory</h2>

<p>Here we show an example optimization trajectory of auto-generated ideas for Conv2D, with torch reference baseline time of <strong>1.41 ms</strong></p>

<p><strong>Round 0: 7.02 ms, 20.1% of reference</strong><br>
Idea: Given the pytorch code, replace the operation with a CUDA Kernel</p>

<p><strong>Round 1: 7.54 ms, 18.8% of reference</strong><br>
Idea: Exploit the read-only cache by loading invariant tensors with __ldg.</p>

<p><strong>Round 2: 3.46 ms, 41.0% of reference</strong><br>
Idea: Convert the convolution to an FP16 Tensor-Core GEMM. <em>[author comment: this is an algorithmic optimization converting a convolution to an implicit GEMM, which is important for running convolutions efficiently on Tensor Cores]</em></p>

<p><strong>Round 3: 3.67 ms, 38.7% of reference</strong><br>
Idea: Double-buffer cp.async pipeline that overlaps global-memory loads with Tensor-Core compute.</p>

<p><strong>Round 4: 3.46 ms, 41.0% of reference</strong><br>
Idea: Given the pytorch code, replace the operation with a CUDA Kernel using implicit matmul. The given GEMM kernel could be helpful.<br>
<em>[author comment: since we know that the optimization involves using GEMM, we seeded the beginning of this round with an existing good GEMM kernel that we generated previously, and this idea is written manually]</em></p>

<p><strong>Round 5: 1.91 ms, 74.9% of reference</strong><br>
Idea: Precompute and reuse `k_idx`-decomposed kernel/input indices in shared memory within each K-tile loop to avoid redundant arithmetic.</p>

<p><strong>Round 6: 1.37 ms, 103.6% of reference</strong><br>
Idea: Precomputing and caching N-dimension GEMM indices in shared memory to reduce redundant arithmetic within the input data loading loop.</p>

<p><strong>Round 7: 1.38 ms, 102.9% of reference</strong><br>
Idea: Parallelize CUDA kernel output by using dedicated per-warp shared memory buffers to eliminate warp-wise serialization during writes to global memory.</p>

<p><strong>Round 8: 1.37 ms, 103.6% of reference</strong><br>
Idea: Precompute and cache base input coordinates in shared memory to reduce redundant arithmetic during input data loading.</p>

<p><strong>Round 9: 1.36 ms, 105.1% of reference</strong><br>
Idea: Software-pipeline B-fragment loading to overlap the next B-tile’s shared memory reads with the current B-tile’s WMMA computations.</p>

<p><strong>Round 10: 1.07 ms, 133.6% of reference</strong><br>
Idea: Reuse precomputed N-dimension GEMM decomposition from shared memory for output address calculation, avoiding redundant and costly division/modulo operations.</p>

<p><strong>Round 11: 1.21 ms, 117.4% of reference</strong><br>
Idea: Remove `hi/lo` decomposition in `half` WMMA operations, relying on standard FP16 accumulation to improve performance if the resulting accuracy is acceptable.</p>

<p><strong>Round 12: 1.01 ms, 141.2% of reference</strong><br>
Idea: Overlap K-loop global memory loads of `Asub` (weights) and `Bsub` (inputs) with MMA computation using double buffering, enabled by calculating K-dimension indices on-the-fly within the load stage of the pipeline.</p>

<p><strong>Round 13: 0.795 ms, 179.9% of reference</strong><br>
Idea: Implement vectorized shared memory writes for loading `Asub_pipe` and `Bsub_pipe` by using wider data types like `half2`</p>

<p><strong>Final Code Sample</strong><br>
The final code sample for the Conv2D kernel is included in the appendix. It uses advanced CUDA techniques that we find challenging to write ourselves!
We also have more example kernels in this <a href="https://github.com/ScalingIntelligence/good-kernels">Github repo</a></p>

<h2 id="takeaways">Takeaways</h2>

<p>Our method echoes a growing theme in AI research: combining strong reasoning with parallel exploration of multiple hypotheses leads to improvements. As some recent work (<a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf">AlphaEvolve</a>, <a href="https://x.com/GoogleDeepMind/status/1924881598102839373">Gemini 2.5 Pro Deep Think</a>) highlight, you might not always need massive retraining — sometimes, clever search and branching strategies can unlock scientific innovation and tackle complex problems, and there might be more gains through extensive searching with verifiers. <br>
However, this doesn’t mean we shouldn’t do further training. On the contrary, our approach also helps generate better synthetic data to improve future model training (this requires more problem instances). So, it’s both a powerful test-time scaling method and a step toward smarter, more data-efficient model development.</p>

<p>Finally, what we’ve demonstrated here is just an early sign of life. The optimization quality looks promising (it’s using many advanced strategies), but there’s plenty of room to improve, such as the generation of better optimization ideas, high quality resulting code, as well as applying this to increasingly complicated kernels. Two concrete examples that we are still actively working on improving are:</p>

<ul>
  <li>FP16 Matmul: 52% performance of torch.matmul</li>
  <li>FP16 Flash Attention: 9% performance of torch.nn.functional.scaled_dot_product_attention</li>
</ul>

<p>FP32 is less common in modern ML workloads and often less optimized on recent hardware compared to FP16 or BF16, which may partly explain why it’s easier to achieve performance gains over PyTorch with FP32 kernels.</p>

<p>Despite the current limitations, we’re optimistic. At the time of KernelBench, we couldn’t even generate functional versions of these two kernels above, and through searching we’ve been steadily increasing the performance of flash attention from &lt;1%, and note that we are working with a quite limited search budget here (around 3 million input tokens + 4 million output tokens in total). The progress since then gives us confidence in the potential for continual improvement, and we are excited to keep pushing the frontier of AI to create increasingly better kernels towards the eventual goal of self-improving AI systems.</p>

<h2 id="thanks">Thanks</h2>

<p>Christopher Rinard, Saman Amarasinghe, and Allen Nie for the helpful discussions; Standard Kernel Co. and Prime Intellect for supporting this work.</p>

<h2 id="appendix-fast-conv2d-kernel">Appendix: Fast Conv2D Kernel</h2>
<div><pre><code><span>import</span> <span>torch</span>
<span>import</span> <span>torch.nn</span> <span>as</span> <span>nn</span>
<span>import</span> <span>torch.nn.functional</span> <span>as</span> <span>F</span>
<span>from</span> <span>torch.utils.cpp_extension</span> <span>import</span> <span>load_inline</span>

<span>conv2d_implicit_gemm_cuda_source</span> <span>=</span> <span>r</span><span>"""
#include &lt;torch/extension.h&gt;
#include &lt;ATen/cuda/CUDAContext.h&gt; // For at::cuda::getCurrentCUDAStream()
#include &lt;mma.h&gt;
#include &lt;cuda_fp16.h&gt;

using namespace nvcuda;

// WMMA tile dimensions
#define WMMA_M 16
#define WMMA_N 16
#define WMMA_K 16

// Skew padding for shared memory to avoid bank conflicts
#define SKEW_HALF 8 // 8 half elements (16 bytes)

// CUDA built-in warpSize is 32 for supported architectures (sm_70+)
// This constant is used for host-side configuration (e.g. blockDim)
#define CUDA_WARP_SIZE_CONST 32 

// Threadblock configuration
#define WARPS_PER_BLOCK 8
// THREADS_PER_BLOCK must be evaluatable by host compiler for blockDim configuration
#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * CUDA_WARP_SIZE_CONST) 

// Macro-tile dimensions computed by a threadblock
// BLOCK_M_TILES_WMMA * WMMA_M = output channels processed by a block
// BLOCK_N_TILES_WMMA * WMMA_N = output spatial elements processed by a block
#define BLOCK_M_TILES_WMMA 8
#define BLOCK_N_TILES_WMMA 8

#define TILE_M_PER_BLOCK (BLOCK_M_TILES_WMMA * WMMA_M) // e.g., 8 * 16 = 128 (for C_out dimension)
#define TILE_N_PER_BLOCK (BLOCK_N_TILES_WMMA * WMMA_N) // e.g., 8 * 16 = 128 (for N_batch * H_out * W_out dimension)

// Vector size for shared memory writes (half2)
#define VECTOR_SIZE_H2 2

// Struct to hold precomputed N-dimension GEMM indices
struct NDecomposed {
    int ow_eff;
    int oh_eff;
    int n_batch_idx;
    bool isValidPixel; // True if this pixel_idx is within N_gemm bounds
    int h_in_base; 
    int w_in_base; 
};

__global__ void conv2d_implicit_gemm_wmma_kernel(
    const float* __restrict__ input_ptr,    // Input: (N, Cin, Hin, Win)
    const float* __restrict__ weight_ptr,   // Weights: (Cout, Cin, Kh, Kw)
    const float* __restrict__ bias_ptr,     // Bias: (Cout) or nullptr
    float* __restrict__ output_ptr,         // Output: (N, Cout, Hout, Wout)
    const int N_batch, const int C_in, const int H_in, const int W_in,
    const int C_out, const int K_h, const int K_w,
    const int stride_h, const int stride_w,
    const int pad_h, const int pad_w,
    const int H_out, const int W_out,
    const int M_gemm, // C_out
    const int N_gemm, // N_batch * H_out * W_out
    const int K_gemm  // C_in * K_h * K_w
) {
    // Thread identification
    const int warp_id = threadIdx.x / warpSize;        // 0 .. WARPS_PER_BLOCK-1
    const int lane_id = threadIdx.x % warpSize;        // 0 .. 31 (or warpSize-1)

    // Top-left corner of the macro-tile this block is responsible for in GEMM terms
    const int block_row_gemm_start = TILE_M_PER_BLOCK * blockIdx.y;
    const int block_col_gemm_start = TILE_N_PER_BLOCK * blockIdx.x;

    // Shared memory for tiles of A (weights) and B (input/im2col) - Double Buffered for K-loop pipelining
    __shared__ half Asub_pipe[2][TILE_M_PER_BLOCK][WMMA_K + SKEW_HALF];
    __shared__ half Bsub_pipe[2][TILE_N_PER_BLOCK][WMMA_K + SKEW_HALF];

    // Shared memory for precomputed N-indices
    __shared__ NDecomposed n_params_sh[TILE_N_PER_BLOCK];

    // Shared memory for output stage (per-warp buffers)
    __shared__ float C_shmem_output_buffers[WARPS_PER_BLOCK][WMMA_M][WMMA_N];

    // Accumulator fragments per warp.
    wmma::fragment&lt;wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float&gt; acc_frag[BLOCK_N_TILES_WMMA];
    #pragma unroll
    for (int i = 0; i &lt; BLOCK_N_TILES_WMMA; ++i) {
        wmma::fill_fragment(acc_frag[i], 0.0f);
    }

    // Populate n_params_sh once at the beginning of the kernel
    if (threadIdx.x &lt; TILE_N_PER_BLOCK) {
        int r_b_tile_idx = threadIdx.x; 
        int current_pixel_idx = block_col_gemm_start + r_b_tile_idx;

        if (current_pixel_idx &lt; N_gemm) {
            n_params_sh[r_b_tile_idx].ow_eff = current_pixel_idx % W_out;
            int temp_div_wout = current_pixel_idx / W_out;
            n_params_sh[r_b_tile_idx].oh_eff = temp_div_wout % H_out;
            n_params_sh[r_b_tile_idx].n_batch_idx = temp_div_wout / H_out;
            n_params_sh[r_b_tile_idx].isValidPixel = true;

            n_params_sh[r_b_tile_idx].h_in_base = n_params_sh[r_b_tile_idx].oh_eff * stride_h - pad_h;
            n_params_sh[r_b_tile_idx].w_in_base = n_params_sh[r_b_tile_idx].ow_eff * stride_w - pad_w;
        } else {
            n_params_sh[r_b_tile_idx].isValidPixel = false;
            n_params_sh[r_b_tile_idx].ow_eff = 0; 
            n_params_sh[r_b_tile_idx].oh_eff = 0;
            n_params_sh[r_b_tile_idx].n_batch_idx = 0;
            n_params_sh[r_b_tile_idx].h_in_base = 0; 
            n_params_sh[r_b_tile_idx].w_in_base = 0;
        }
    }
    __syncthreads();

    // Constants for vectorized shared memory loading
    // Number of half2 elements along K-dim for a shared memory tile row
    const int NUM_H2_ELEMENTS_IN_K_DIM = WMMA_K / VECTOR_SIZE_H2;
    // Number of thread groups, where each group has NUM_H2_ELEMENTS_IN_K_DIM threads.
    // Each group is responsible for loading the K-dimension for one M-row (for A) or N-row (for B) at a time,
    // iterating over M-rows or N-rows with this step size.
    const int NUM_ROW_PROCESSING_GROUPS = THREADS_PER_BLOCK / NUM_H2_ELEMENTS_IN_K_DIM;


    // --- K-Loop Pipelining ---
    int num_k_tiles = (K_gemm + WMMA_K - 1) / WMMA_K;
    
    // --- Prologue: Load first k-tile (k_tile_iter = 0) into pipe_idx = 0 ---
    if (num_k_tiles &gt; 0) { 
        int k_tile_start_prologue = 0; 
        int current_pipe_idx_prologue = 0; 

        // Load Asub_pipe[0] for k_tile_iter = 0
        {
            // This thread is responsible for the 'h2_idx_in_k_dim_A'-th half2 element
            // in the K-dimension of the shared memory tile.
            int h2_idx_in_k_dim_A = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
            // Starting 'half' index in shared memory for this half2 write.
            int shmem_k_start_for_h2_A = h2_idx_in_k_dim_A * VECTOR_SIZE_H2;

            // Global k-indices for the two half elements.
            int k_global_A_0 = k_tile_start_prologue + shmem_k_start_for_h2_A;
            int k_global_A_1 = k_tile_start_prologue + shmem_k_start_for_h2_A + 1;

            // Decompose k_global_A_0
            int kw_eff_reg_A_0 = 0, kh_eff_reg_A_0 = 0, ic_eff_reg_A_0 = 0;
            bool is_valid_k_A_0 = (k_global_A_0 &lt; K_gemm);
            if (is_valid_k_A_0) {
                kw_eff_reg_A_0 = k_global_A_0 % K_w;
                int temp_div_kw_A_0 = k_global_A_0 / K_w;
                kh_eff_reg_A_0 = temp_div_kw_A_0 % K_h;
                ic_eff_reg_A_0 = temp_div_kw_A_0 / K_h;
            }

            // Decompose k_global_A_1
            int kw_eff_reg_A_1 = 0, kh_eff_reg_A_1 = 0, ic_eff_reg_A_1 = 0;
            bool is_valid_k_A_1 = (k_global_A_1 &lt; K_gemm);
            if (is_valid_k_A_1) {
                kw_eff_reg_A_1 = k_global_A_1 % K_w;
                int temp_div_kw_A_1 = k_global_A_1 / K_w;
                kh_eff_reg_A_1 = temp_div_kw_A_1 % K_h;
                ic_eff_reg_A_1 = temp_div_kw_A_1 / K_h;
            }
            
            // This thread belongs to 'm_row_group_id_A'-th group of threads.
            // This group iterates over M-rows of the Asub_pipe tile.
            int m_row_group_id_A = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
            for (int r_a_tile_base = m_row_group_id_A; r_a_tile_base &lt; TILE_M_PER_BLOCK; r_a_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                int oc_idx = block_row_gemm_start + r_a_tile_base;
                float weight_val_0 = 0.0f;
                if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_0) {
                    weight_val_0 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                              ic_eff_reg_A_0 * K_h * K_w +
                                              kh_eff_reg_A_0 * K_w +
                                              kw_eff_reg_A_0];
                }
                float weight_val_1 = 0.0f;
                if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_1) {
                    weight_val_1 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                              ic_eff_reg_A_1 * K_h * K_w +
                                              kh_eff_reg_A_1 * K_w +
                                              kw_eff_reg_A_1];
                }
                half2* smem_ptr_h2_A = reinterpret_cast&lt;half2*&gt;(
                    &amp;Asub_pipe[current_pipe_idx_prologue][r_a_tile_base][shmem_k_start_for_h2_A]
                );
                *smem_ptr_h2_A = make_half2(__float2half(weight_val_0), __float2half(weight_val_1));
            }
        }

        // Load Bsub_pipe[0] for k_tile_iter = 0
        {
            int h2_idx_in_k_dim_B = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
            int shmem_k_start_for_h2_B = h2_idx_in_k_dim_B * VECTOR_SIZE_H2;

            int k_global_B_0 = k_tile_start_prologue + shmem_k_start_for_h2_B;
            int k_global_B_1 = k_tile_start_prologue + shmem_k_start_for_h2_B + 1;

            int kw_eff_reg_B_0 = 0, kh_eff_reg_B_0 = 0, ic_eff_reg_B_0 = 0;
            bool is_valid_k_B_0 = (k_global_B_0 &lt; K_gemm);
            if (is_valid_k_B_0) {
                kw_eff_reg_B_0 = k_global_B_0 % K_w;
                int temp_div_kw_B_0 = k_global_B_0 / K_w;
                kh_eff_reg_B_0 = temp_div_kw_B_0 % K_h;
                ic_eff_reg_B_0 = temp_div_kw_B_0 / K_h;
            }

            int kw_eff_reg_B_1 = 0, kh_eff_reg_B_1 = 0, ic_eff_reg_B_1 = 0;
            bool is_valid_k_B_1 = (k_global_B_1 &lt; K_gemm);
            if (is_valid_k_B_1) {
                kw_eff_reg_B_1 = k_global_B_1 % K_w;
                int temp_div_kw_B_1 = k_global_B_1 / K_w;
                kh_eff_reg_B_1 = temp_div_kw_B_1 % K_h;
                ic_eff_reg_B_1 = temp_div_kw_B_1 / K_h;
            }

            int n_row_group_id_B = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
            for (int r_b_tile_base = n_row_group_id_B; r_b_tile_base &lt; TILE_N_PER_BLOCK; r_b_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                float input_val_0 = 0.0f;
                if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_0) {
                    const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                    int h_in_eff_0 = current_n_params.h_in_base + kh_eff_reg_B_0;
                    int w_in_eff_0 = current_n_params.w_in_base + kw_eff_reg_B_0;
                    if (h_in_eff_0 &gt;= 0 &amp;&amp; h_in_eff_0 &lt; H_in &amp;&amp; w_in_eff_0 &gt;= 0 &amp;&amp; w_in_eff_0 &lt; W_in) {
                        input_val_0 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                              ic_eff_reg_B_0 * H_in * W_in +
                                              h_in_eff_0 * W_in +
                                              w_in_eff_0];
                    }
                }
                float input_val_1 = 0.0f;
                 if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_1) {
                    const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                    int h_in_eff_1 = current_n_params.h_in_base + kh_eff_reg_B_1;
                    int w_in_eff_1 = current_n_params.w_in_base + kw_eff_reg_B_1;
                     if (h_in_eff_1 &gt;= 0 &amp;&amp; h_in_eff_1 &lt; H_in &amp;&amp; w_in_eff_1 &gt;= 0 &amp;&amp; w_in_eff_1 &lt; W_in) {
                        input_val_1 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                              ic_eff_reg_B_1 * H_in * W_in +
                                              h_in_eff_1 * W_in +
                                              w_in_eff_1];
                    }
                }
                half2* smem_ptr_h2_B = reinterpret_cast&lt;half2*&gt;(
                    &amp;Bsub_pipe[current_pipe_idx_prologue][r_b_tile_base][shmem_k_start_for_h2_B]
                );
                *smem_ptr_h2_B = make_half2(__float2half(input_val_0), __float2half(input_val_1));
            }
        }
    }


    // Loop over the K_gemm dimension in tiles of WMMA_K
    for (int k_tile_iter = 0; k_tile_iter &lt; num_k_tiles; ++k_tile_iter) {
        __syncthreads(); // Sync point for pipelining

        int compute_pipe_idx = k_tile_iter % 2;
        int load_pipe_idx = (k_tile_iter + 1) % 2;

        // --- Load Stage for next k-tile (k_tile_iter + 1) into load_pipe_idx ---
        int k_tile_start_for_load = (k_tile_iter + 1) * WMMA_K;
        if (k_tile_start_for_load &lt; K_gemm) { 
            // Load Asub_pipe[load_pipe_idx]
            { 
                int h2_idx_in_k_dim_A = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
                int shmem_k_start_for_h2_A = h2_idx_in_k_dim_A * VECTOR_SIZE_H2;

                int k_global_A_0 = k_tile_start_for_load + shmem_k_start_for_h2_A;
                int k_global_A_1 = k_tile_start_for_load + shmem_k_start_for_h2_A + 1;

                int kw_eff_reg_A_0 = 0, kh_eff_reg_A_0 = 0, ic_eff_reg_A_0 = 0;
                bool is_valid_k_A_0 = (k_global_A_0 &lt; K_gemm);
                if (is_valid_k_A_0) {
                    kw_eff_reg_A_0 = k_global_A_0 % K_w;
                    int temp_div_kw_A_0 = k_global_A_0 / K_w;
                    kh_eff_reg_A_0 = temp_div_kw_A_0 % K_h;
                    ic_eff_reg_A_0 = temp_div_kw_A_0 / K_h;
                }

                int kw_eff_reg_A_1 = 0, kh_eff_reg_A_1 = 0, ic_eff_reg_A_1 = 0;
                bool is_valid_k_A_1 = (k_global_A_1 &lt; K_gemm);
                if (is_valid_k_A_1) {
                    kw_eff_reg_A_1 = k_global_A_1 % K_w;
                    int temp_div_kw_A_1 = k_global_A_1 / K_w;
                    kh_eff_reg_A_1 = temp_div_kw_A_1 % K_h;
                    ic_eff_reg_A_1 = temp_div_kw_A_1 / K_h;
                }
                
                int m_row_group_id_A = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
                for (int r_a_tile_base = m_row_group_id_A; r_a_tile_base &lt; TILE_M_PER_BLOCK; r_a_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                    int oc_idx = block_row_gemm_start + r_a_tile_base;
                    float weight_val_0 = 0.0f;
                    if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_0) {
                        weight_val_0 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                                  ic_eff_reg_A_0 * K_h * K_w +
                                                  kh_eff_reg_A_0 * K_w +
                                                  kw_eff_reg_A_0];
                    }
                    float weight_val_1 = 0.0f;
                    if (oc_idx &lt; C_out &amp;&amp; is_valid_k_A_1) {
                        weight_val_1 = weight_ptr[oc_idx * C_in * K_h * K_w +
                                                  ic_eff_reg_A_1 * K_h * K_w +
                                                  kh_eff_reg_A_1 * K_w +
                                                  kw_eff_reg_A_1];
                    }
                    half2* smem_ptr_h2_A = reinterpret_cast&lt;half2*&gt;(
                        &amp;Asub_pipe[load_pipe_idx][r_a_tile_base][shmem_k_start_for_h2_A]
                    );
                    *smem_ptr_h2_A = make_half2(__float2half(weight_val_0), __float2half(weight_val_1));
                }
            } 

            // Load Bsub_pipe[load_pipe_idx]
            { 
                int h2_idx_in_k_dim_B = threadIdx.x % NUM_H2_ELEMENTS_IN_K_DIM;
                int shmem_k_start_for_h2_B = h2_idx_in_k_dim_B * VECTOR_SIZE_H2;

                int k_global_B_0 = k_tile_start_for_load + shmem_k_start_for_h2_B;
                int k_global_B_1 = k_tile_start_for_load + shmem_k_start_for_h2_B + 1;

                int kw_eff_reg_B_0 = 0, kh_eff_reg_B_0 = 0, ic_eff_reg_B_0 = 0;
                bool is_valid_k_B_0 = (k_global_B_0 &lt; K_gemm);
                if (is_valid_k_B_0) {
                    kw_eff_reg_B_0 = k_global_B_0 % K_w;
                    int temp_div_kw_B_0 = k_global_B_0 / K_w;
                    kh_eff_reg_B_0 = temp_div_kw_B_0 % K_h;
                    ic_eff_reg_B_0 = temp_div_kw_B_0 / K_h;
                }

                int kw_eff_reg_B_1 = 0, kh_eff_reg_B_1 = 0, ic_eff_reg_B_1 = 0;
                bool is_valid_k_B_1 = (k_global_B_1 &lt; K_gemm);
                if (is_valid_k_B_1) {
                    kw_eff_reg_B_1 = k_global_B_1 % K_w;
                    int temp_div_kw_B_1 = k_global_B_1 / K_w;
                    kh_eff_reg_B_1 = temp_div_kw_B_1 % K_h;
                    ic_eff_reg_B_1 = temp_div_kw_B_1 / K_h;
                }

                int n_row_group_id_B = threadIdx.x / NUM_H2_ELEMENTS_IN_K_DIM;
                for (int r_b_tile_base = n_row_group_id_B; r_b_tile_base &lt; TILE_N_PER_BLOCK; r_b_tile_base += NUM_ROW_PROCESSING_GROUPS) {
                    float input_val_0 = 0.0f;
                    if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_0) {
                        const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                        int h_in_eff_0 = current_n_params.h_in_base + kh_eff_reg_B_0;
                        int w_in_eff_0 = current_n_params.w_in_base + kw_eff_reg_B_0;
                        if (h_in_eff_0 &gt;= 0 &amp;&amp; h_in_eff_0 &lt; H_in &amp;&amp; w_in_eff_0 &gt;= 0 &amp;&amp; w_in_eff_0 &lt; W_in) {
                            input_val_0 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                                  ic_eff_reg_B_0 * H_in * W_in +
                                                  h_in_eff_0 * W_in +
                                                  w_in_eff_0];
                        }
                    }
                    float input_val_1 = 0.0f;
                    if (n_params_sh[r_b_tile_base].isValidPixel &amp;&amp; is_valid_k_B_1) {
                        const NDecomposed&amp; current_n_params = n_params_sh[r_b_tile_base];
                        int h_in_eff_1 = current_n_params.h_in_base + kh_eff_reg_B_1;
                        int w_in_eff_1 = current_n_params.w_in_base + kw_eff_reg_B_1;
                        if (h_in_eff_1 &gt;= 0 &amp;&amp; h_in_eff_1 &lt; H_in &amp;&amp; w_in_eff_1 &gt;= 0 &amp;&amp; w_in_eff_1 &lt; W_in) {
                            input_val_1 = input_ptr[current_n_params.n_batch_idx * C_in * H_in * W_in +
                                                  ic_eff_reg_B_1 * H_in * W_in +
                                                  h_in_eff_1 * W_in +
                                                  w_in_eff_1];
                        }
                    }
                    half2* smem_ptr_h2_B = reinterpret_cast&lt;half2*&gt;(
                        &amp;Bsub_pipe[load_pipe_idx][r_b_tile_base][shmem_k_start_for_h2_B]
                    );
                    *smem_ptr_h2_B = make_half2(__float2half(input_val_0), __float2half(input_val_1));
                }
            } 
        }

        // --- Compute Stage for current k-tile (k_tile_iter) using compute_pipe_idx ---
        int a_row_start_in_tile = warp_id * WMMA_M; 

        wmma::fragment&lt;wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major&gt; a_frag;
        wmma::load_matrix_sync(a_frag, &amp;Asub_pipe[compute_pipe_idx][a_row_start_in_tile][0], WMMA_K + SKEW_HALF);

        wmma::fragment&lt;wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major&gt; b_frag_inner_pipe[2];

        if (BLOCK_N_TILES_WMMA &gt; 0) {
            int b_col_start_in_tile_current = 0 * WMMA_N; 
            wmma::load_matrix_sync(b_frag_inner_pipe[0], &amp;Bsub_pipe[compute_pipe_idx][b_col_start_in_tile_current][0], WMMA_K + SKEW_HALF);
        }
        
        int current_inner_pipe_idx = 0;

        #pragma unroll
        for (int n_tile = 0; n_tile &lt; BLOCK_N_TILES_WMMA; ++n_tile) {
            int next_inner_pipe_idx = 1 - current_inner_pipe_idx;

            if (n_tile &lt; BLOCK_N_TILES_WMMA - 1) {
                int b_col_start_in_tile_next = (n_tile + 1) * WMMA_N;
                wmma::load_matrix_sync(b_frag_inner_pipe[next_inner_pipe_idx], &amp;Bsub_pipe[compute_pipe_idx][b_col_start_in_tile_next][0], WMMA_K + SKEW_HALF);
            }

            wmma::mma_sync(acc_frag[n_tile], a_frag, b_frag_inner_pipe[current_inner_pipe_idx], acc_frag[n_tile]);
            
            current_inner_pipe_idx = next_inner_pipe_idx;
        }
    }
    __syncthreads(); 

    // Store results from accumulator fragments to global memory
    #pragma unroll
    for (int n_tile = 0; n_tile &lt; BLOCK_N_TILES_WMMA; ++n_tile) {
        wmma::store_matrix_sync(&amp;C_shmem_output_buffers[warp_id][0][0], acc_frag[n_tile], WMMA_N, wmma::mem_row_major);

        for (int elem_idx_in_frag = lane_id; elem_idx_in_frag &lt; WMMA_M * WMMA_N; elem_idx_in_frag += warpSize) {
            int r_frag = elem_idx_in_frag / WMMA_N;
            int c_frag = elem_idx_in_frag % WMMA_N;

            int oc_idx = block_row_gemm_start + (warp_id * WMMA_M) + r_frag;
            
            int offset_in_block_N_processing = (n_tile * WMMA_N) + c_frag;

            if (oc_idx &lt; C_out &amp;&amp; offset_in_block_N_processing &lt; TILE_N_PER_BLOCK &amp;&amp; 
                n_params_sh[offset_in_block_N_processing].isValidPixel) {
                const NDecomposed&amp; current_n_params = n_params_sh[offset_in_block_N_processing];
                int ow_eff = current_n_params.ow_eff;
                int oh_eff = current_n_params.oh_eff;
                int n_batch_idx = current_n_params.n_batch_idx;

                float val = C_shmem_output_buffers[warp_id][r_frag][c_frag];

                if (bias_ptr != nullptr) {
                    val += bias_ptr[oc_idx];
                }

                output_ptr[n_batch_idx * C_out * H_out * W_out +
                           oc_idx * H_out * W_out +
                           oh_eff * W_out +
                           ow_eff] = val;
            }
        }
    }
}


torch::Tensor conv2d_implicit_gemm_cuda(
    torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
    int N_batch, int C_in, int H_in, int W_in,
    int C_out, int K_h, int K_w,
    int stride_h, int stride_w, int pad_h, int pad_w,
    int H_out, int W_out) {

    TORCH_CHECK(input.device().is_cuda(), "Input must be a CUDA tensor");
    TORCH_CHECK(weight.device().is_cuda(), "Weight must be a CUDA tensor");
    TORCH_CHECK(input.dtype() == torch::kFloat32, "Input must be float32");
    TORCH_CHECK(weight.dtype() == torch::kFloat32, "Weight must be float32");
    if (bias.defined()) {
        TORCH_CHECK(bias.device().is_cuda(), "Bias must be a CUDA tensor");
        TORCH_CHECK(bias.dtype() == torch::kFloat32, "Bias must be float32");
        TORCH_CHECK(bias.dim() == 1 &amp;&amp; bias.size(0) == C_out, "Bias has wrong shape");
    }

    TORCH_CHECK(input.dim() == 4, "Input must be 4D");
    TORCH_CHECK(weight.dim() == 4, "Weight must be 4D");
    TORCH_CHECK(input.size(0) == N_batch, "Input N_batch mismatch");
    TORCH_CHECK(input.size(1) == C_in, "Input C_in mismatch");
    TORCH_CHECK(input.size(2) == H_in, "Input H_in mismatch");
    TORCH_CHECK(input.size(3) == W_in, "Input W_in mismatch");
    TORCH_CHECK(weight.size(0) == C_out, "Weight C_out mismatch");
    TORCH_CHECK(weight.size(1) == C_in, "Weight C_in mismatch");
    TORCH_CHECK(weight.size(2) == K_h, "Weight K_h mismatch");
    TORCH_CHECK(weight.size(3) == K_w, "Weight K_w mismatch");

    auto output = torch::zeros({N_batch, C_out, H_out, W_out}, input.options());

    const int M_gemm = C_out;
    const int N_gemm = N_batch * H_out * W_out;
    const int K_gemm = C_in * K_h * K_w;

    if (M_gemm == 0 || N_gemm == 0) { 
        return output;
    }
    if (K_gemm == 0) { 
         if (bias.defined()) { 
            output = output + bias.reshape({1, C_out, 1, 1});
        }
        return output; 
    }

    dim3 block_dim(THREADS_PER_BLOCK);
    dim3 grid_dim(
        (N_gemm + TILE_N_PER_BLOCK - 1) / TILE_N_PER_BLOCK, 
        (M_gemm + TILE_M_PER_BLOCK - 1) / TILE_M_PER_BLOCK  
    );

    const float* bias_ptr_data = bias.defined() ? bias.data_ptr&lt;float&gt;() : nullptr;

    cudaStream_t stream = at::cuda::getCurrentCUDAStream();
    conv2d_implicit_gemm_wmma_kernel&lt;&lt;&lt;grid_dim, block_dim, 0, stream&gt;&gt;&gt;(
        input.data_ptr&lt;float&gt;(),
        weight.data_ptr&lt;float&gt;(),
        bias_ptr_data,
        output.data_ptr&lt;float&gt;(),
        N_batch, C_in, H_in, W_in,
        C_out, K_h, K_w,
        stride_h, stride_w, pad_h, pad_w,
        H_out, W_out,
        M_gemm, N_gemm, K_gemm
    );
    
    AT_CUDA_CHECK(cudaGetLastError());

    return output;
}
"""</span>

<span>conv2d_implicit_gemm_cuda_declaration</span> <span>=</span> <span>r</span><span>"""
torch::Tensor conv2d_implicit_gemm_cuda(
    torch::Tensor input, torch::Tensor weight, torch::Tensor bias,
    int N_batch, int C_in, int H_in, int W_in,
    int C_out, int K_h, int K_w,
    int stride_h, int stride_w, int pad_h, int pad_w,
    int H_out, int W_out);
"""</span>

<span># JIT compile the CUDA kernel
</span><span>custom_conv2d_wmma_ops</span> <span>=</span> <span>load_inline</span><span>(</span>
    <span>name</span><span>=</span><span>"custom_conv2d_wmma_ops_optimized_k_pipe_vec_smem"</span><span>,</span> <span># Changed name to avoid collision
</span>    <span>cpp_sources</span><span>=</span><span>conv2d_implicit_gemm_cuda_declaration</span><span>,</span>
    <span>cuda_sources</span><span>=</span><span>conv2d_implicit_gemm_cuda_source</span><span>,</span>
    <span>functions</span><span>=</span><span>[</span><span>"conv2d_implicit_gemm_cuda"</span><span>],</span>
    <span>verbose</span><span>=</span><span>True</span><span>,</span> 
    <span>extra_cuda_cflags</span><span>=</span><span>[</span><span>"-arch=sm_70"</span><span>,</span> <span>"--use_fast_math"</span><span>,</span> <span>"-std=c++17"</span><span>]</span> 
<span>)</span>


<span>class</span> <span>ModelNew</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>num_classes</span><span>=</span><span>1000</span><span>):</span> <span># num_classes is part of original signature, kept for consistency
</span>        <span>super</span><span>(</span><span>ModelNew</span><span>,</span> <span>self</span><span>).</span><span>__init__</span><span>()</span>
        
        <span># Define Conv1 parameters (matching the original model)
</span>        <span>self</span><span>.</span><span>in_channels</span> <span>=</span> <span>3</span>
        <span>self</span><span>.</span><span>out_channels</span> <span>=</span> <span>96</span>
        <span>self</span><span>.</span><span>kernel_size_val</span> <span>=</span> <span>11</span> <span># Assuming square kernel
</span>        <span>self</span><span>.</span><span>stride_val</span> <span>=</span> <span>4</span>       <span># Assuming square stride
</span>        <span>self</span><span>.</span><span>padding_val</span> <span>=</span> <span>2</span>      <span># Assuming square padding
</span>
        <span># Create a temporary Conv2d layer to initialize weights and bias
</span>        <span>temp_conv</span> <span>=</span> <span>nn</span><span>.</span><span>Conv2d</span><span>(</span>
            <span>in_channels</span><span>=</span><span>self</span><span>.</span><span>in_channels</span><span>,</span> 
            <span>out_channels</span><span>=</span><span>self</span><span>.</span><span>out_channels</span><span>,</span> 
            <span>kernel_size</span><span>=</span><span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> 
            <span>stride</span><span>=</span><span>self</span><span>.</span><span>stride_val</span><span>,</span> 
            <span>padding</span><span>=</span><span>self</span><span>.</span><span>padding_val</span><span>,</span>
            <span>bias</span><span>=</span><span>True</span> <span># nn.Conv2d has bias=True by default
</span>        <span>)</span>
        <span>self</span><span>.</span><span>conv1_weight</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>temp_conv</span><span>.</span><span>weight</span><span>.</span><span>detach</span><span>().</span><span>clone</span><span>())</span>
        <span>if</span> <span>temp_conv</span><span>.</span><span>bias</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>self</span><span>.</span><span>conv1_bias</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>temp_conv</span><span>.</span><span>bias</span><span>.</span><span>detach</span><span>().</span><span>clone</span><span>())</span>
        <span>else</span><span>:</span>
            <span># Correctly register 'conv1_bias' as None if not present
</span>            <span>self</span><span>.</span><span>register_parameter</span><span>(</span><span>'conv1_bias'</span><span>,</span> <span>None</span><span>)</span> 


        <span>self</span><span>.</span><span>custom_conv_op</span> <span>=</span> <span>custom_conv2d_wmma_ops</span><span>.</span><span>conv2d_implicit_gemm_cuda</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>):</span>
        <span>N_batch</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>0</span><span>)</span>
        <span># C_in_runtime = x.size(1) # Should match self.in_channels
</span>        <span>H_in</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>2</span><span>)</span>
        <span>W_in</span> <span>=</span> <span>x</span><span>.</span><span>size</span><span>(</span><span>3</span><span>)</span>

        <span># Calculate output dimensions
</span>        <span>H_out</span> <span>=</span> <span>(</span><span>H_in</span> <span>+</span> <span>2</span> <span>*</span> <span>self</span><span>.</span><span>padding_val</span> <span>-</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>)</span> <span>//</span> <span>self</span><span>.</span><span>stride_val</span> <span>+</span> <span>1</span>
        <span>W_out</span> <span>=</span> <span>(</span><span>W_in</span> <span>+</span> <span>2</span> <span>*</span> <span>self</span><span>.</span><span>padding_val</span> <span>-</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>)</span> <span>//</span> <span>self</span><span>.</span><span>stride_val</span> <span>+</span> <span>1</span>
        
        <span># Bias tensor handling: pass an undefined tensor if bias is None.
</span>        <span># The C++ TORCH_CHECK(bias.defined()) handles this by providing nullptr to kernel.
</span>        <span>bias_tensor</span> <span>=</span> <span>self</span><span>.</span><span>conv1_bias</span> <span>if</span> <span>self</span><span>.</span><span>conv1_bias</span> <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>torch</span><span>.</span><span>Tensor</span><span>()</span>


        <span>x</span> <span>=</span> <span>self</span><span>.</span><span>custom_conv_op</span><span>(</span>
            <span>x</span><span>,</span> <span>self</span><span>.</span><span>conv1_weight</span><span>,</span> <span>bias_tensor</span><span>,</span>
            <span>N_batch</span><span>,</span> <span>self</span><span>.</span><span>in_channels</span><span>,</span> <span>H_in</span><span>,</span> <span>W_in</span><span>,</span>
            <span>self</span><span>.</span><span>out_channels</span><span>,</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> <span>self</span><span>.</span><span>kernel_size_val</span><span>,</span> <span># K_h, K_w
</span>            <span>self</span><span>.</span><span>stride_val</span><span>,</span> <span>self</span><span>.</span><span>stride_val</span><span>,</span> <span># stride_h, stride_w
</span>            <span>self</span><span>.</span><span>padding_val</span><span>,</span> <span>self</span><span>.</span><span>padding_val</span><span>,</span> <span># pad_h, pad_w
</span>            <span>H_out</span><span>,</span> <span>W_out</span>
        <span>)</span>
        <span>return</span> <span>x</span>
</code></pre></div>

  </div>


			</div>
		






	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mary Meeker's first Trends report since 2019, focused on AI (163 pts)]]></title>
            <link>https://www.bondcap.com/reports/tai</link>
            <guid>44139403</guid>
            <pubDate>Fri, 30 May 2025 19:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bondcap.com/reports/tai">https://www.bondcap.com/reports/tai</a>, See on <a href="https://news.ycombinator.com/item?id=44139403">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Cap: Lightweight, modern open-source CAPTCHA alternative using proof-of-work (149 pts)]]></title>
            <link>https://capjs.js.org/</link>
            <guid>44137867</guid>
            <pubDate>Fri, 30 May 2025 16:36:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://capjs.js.org/">https://capjs.js.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44137867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-9a6c75ad="" data-v-e07eaea7="" id="VPContent" data-v-d8b57b2d=""><!--[--><!--]--><div data-v-dd8814ff="" data-v-e07eaea7=""><div data-v-dd8814ff=""><!--[--><!--]--><!--[--><h2 data-v-dd8814ff=""><span data-v-dd8814ff="">Cap</span><span data-v-dd8814ff="">A modern, lightning-quick PoW captcha</span></h2><p data-v-dd8814ff="">Cap is a lightweight, modern open-source CAPTCHA alternative using SHA-256 proof-of-work</p><!--]--><!--[--><!--]--><!--[--><!--]--></div><div data-v-dd8814ff=""><!--[--><!--[--><p><img src="https://capjs.js.org/logo.png" alt="VitePress" data-v-ab19afbb=""></p><!--]--><!--]--></div></div><!--[--><!--]--><!--[--><!--]--><div data-v-b1eea84a="" data-v-e07eaea7=""><!--[--><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>⚡️</p><h2 data-v-bd37d1a2="">250x smaller than hCaptcha</h2><p data-v-bd37d1a2="">Cap's widget library is extremely small, only ~20kb minified (including WASM)</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🔒️</p><h2 data-v-bd37d1a2="">Private</h2><p data-v-bd37d1a2="">Cap's usage of proof-of-work eliminates the need for any tracking, fingerprinting or data collection</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🌈</p><h2 data-v-bd37d1a2="">Fully customizable</h2><p data-v-bd37d1a2="">Cap is self-hostable so you can customize both the backend &amp; frontend (or you can just use CSS variables)</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🤖</p><h2 data-v-bd37d1a2="">PoW-based</h2><p data-v-bd37d1a2="">Cap uses PoW instead of complex puzzles, making it easier for humans and harder for bots</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🧩</p><h2 data-v-bd37d1a2="">Standalone mode</h2><p data-v-bd37d1a2="">Cap offers a standalone mode with Docker, allowing you to use it with languages other than JS</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>💨</p><h2 data-v-bd37d1a2="">Invisible mode</h2><p data-v-bd37d1a2="">Cap can run invisibly in the background using a simple JS API</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>☁️</p><h2 data-v-bd37d1a2="">Floating mode</h2><p data-v-bd37d1a2="">Floating mode keeps your CAPTCHA hidden until it's needed</p><!----></article><!--]--></div><div data-v-b1eea84a="" data-v-bd37d1a2=""><!--[--><article data-v-bd37d1a2=""><p>🌳</p><h2 data-v-bd37d1a2="">Fully FOSS</h2><p data-v-bd37d1a2="">Completely open source under the Apache 2.0 license</p><!----></article><!--]--></div><!--]--></div><!--[--><!--]--><div data-v-e07eaea7="" data-v-c141a4bd=""><h2 id="what-is-cap" tabindex="-1">What is Cap? <a href="#what-is-cap" aria-label="Permalink to &quot;What is Cap?&quot;">​</a></h2><p>Cap is a lightweight, modern open-source CAPTCHA alternative using SHA-256 proof-of-work. It's fast, private, and extremely simple to integrate. <a href="https://capjs.js.org/guide/effectiveness.html">Learn more about proof-of-work here.</a></p><p>Cap is built into 2 main parts:</p><ul><li><p><strong><a href="https://capjs.js.org/guide/widget.html" target="_blank" rel="noreferrer">@cap.js/widget</a></strong>: A small JavaScript library that renders the CAPTCHA and handles solving it using Web Workers and WASM.</p></li><li><p><strong><a href="https://capjs.js.org/guide/server.html" target="_blank" rel="noreferrer">@cap.js/server</a></strong>: An extremely simple, zero-dependencies library that handles creating and validating challenges.</p></li></ul><p>There are also some other helpful packages:</p><ul><li><p><strong><a href="https://capjs.js.org/guide/solver.html" target="_blank" rel="noreferrer">@cap.js/solver</a></strong>: Server-side solver for the CAPTCHA in case you want to use machine-to-machine.</p></li><li><p><strong><a href="https://capjs.js.org/guide/cli.html" target="_blank" rel="noreferrer">@cap.js/cli</a></strong>: Command-line interface for solving CAPTCHAs made with Cap. It's mainly designed for testing and when you need to solve these CAPTCHAs in a browser without JavaScript support.</p></li><li><p><strong><a href="https://capjs.js.org/guide/standalone.html" target="_blank" rel="noreferrer">Standalone mode</a></strong>: Docker image that helps you use Cap with any language or framework. It runs a simple REST API that can be used to create and validate challenges and an interactive UI to manage your keys.</p></li><li><p><strong>@cap.js/wasm</strong>: WASM solvers for Node and Web built with Rust.</p></li></ul><p>We also provide a middleware for a Cloudflare browser checkpoint-like experience:</p><ul><li><a href="https://capjs.js.org/guide/middleware/hono.html" target="_blank" rel="noreferrer">@cap.js/checkpoint-hono</a></li><li><a href="https://capjs.js.org/guide/middleware/express.html" target="_blank" rel="noreferrer">@cap.js/checkpoint-express</a></li><li><a href="https://capjs.js.org/guide/middleware/elysia.html" target="_blank" rel="noreferrer">@cap.js/middleware-elysia</a></li><li>more coming soon!</li></ul><p>It's designed to be a drop-in replacement for existing CAPTCHA solutions, with a focus on performance and UX.</p><p>Cap is built with JavaScript, runs on any JS runtime (Bun, Node.js, Deno), and has no dependencies. If you're not using any JS runtime, you can also use the standalone mode with Docker, which relies entirely on a simple REST API to create and validate challenges.</p><h2 id="why-cap" tabindex="-1">Why Cap? <a href="#why-cap" aria-label="Permalink to &quot;Why Cap?&quot;">​</a></h2><ul><li><strong>250x smaller than hCaptcha</strong><br><code>@cap.js/widget</code> is extremely small, only 12kb minified and brotli'd.</li><li><strong>Private</strong><br> Cap's usage of proof-of-work eliminates the need for any tracking, fingerprinting or data collection.</li><li><strong>Fully customizable</strong><br> Cap's self-hostable so you can customize both the backend &amp; frontend — or you can just use CSS variables</li><li><strong>Proof-of-work</strong><br> Cap uses proof-of-work instead of complex puzzles, making it easier for humans and harder for bots</li><li><strong>Standalone mode</strong><br> Cap offers a standalone mode with Docker, allowing you to use it with languages other than JS.</li><li><strong>Invisible mode</strong><br> Cap can run invisibly in the background using a simple JS API.</li><li><strong>Floating mode</strong><br> Cap's floating mode keeps your CAPTCHA hidden until it's needed.</li><li><strong>Fully open-source</strong><br> Completely open source under the Apache license 2.0 license.</li></ul><p>It's ideal for:</p><ul><li>Protecting APIs from bots</li><li>Preventing spam on forms</li><li>Blocking automated login attempts</li><li>Securing free-tier abuse</li></ul><h2 id="feature-comparison" tabindex="-1">Feature comparison <a href="#feature-comparison" aria-label="Permalink to &quot;Feature comparison&quot;">​</a></h2><table tabindex="0"><thead><tr><th>CAPTCHA</th><th>Open-source</th><th>Free</th><th>Private</th><th>Fast to solve</th><th>Easy for humans</th><th>Small error rate</th><th>Checkpoint support</th><th>GDPR/CCPA Compliant</th><th>Customizable</th><th>Hard for bots</th><th>Easy to integrate</th></tr></thead><tbody><tr><td><strong>Cap</strong></td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>🟨</td><td>✅</td></tr><tr><td>Cloudflare Turnstile</td><td>❌</td><td>✅</td><td>🟨</td><td>🟨</td><td>✅</td><td>❌</td><td>🟨</td><td>✅</td><td>❌</td><td>🟨</td><td>✅</td></tr><tr><td>reCAPTCHA</td><td>❌</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>❌</td><td>❌</td><td>✅</td></tr><tr><td>hCAPTCHA</td><td>❌</td><td>🟨</td><td>🟨</td><td>❌</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>❌</td><td>🟨</td><td>✅</td></tr><tr><td>Altcha</td><td>✅</td><td>✅</td><td>✅</td><td>🟨</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅</td><td>🟨</td><td>🟨</td></tr><tr><td>FriendlyCaptcha</td><td>❌</td><td>❌</td><td>✅</td><td>🟨</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅</td><td>🟨</td><td>🟨</td></tr><tr><td>MTCaptcha</td><td>❌</td><td>🟨</td><td>🟨</td><td>❌</td><td>❌</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>❌</td><td>🟨</td></tr><tr><td>GeeTest</td><td>❌</td><td>❌</td><td>❌</td><td>🟨</td><td>🟨</td><td>🟨</td><td>❌</td><td>✅</td><td>❌</td><td>🟨</td><td>🟨</td></tr><tr><td>Arkose Labs</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>❌</td><td>✅</td><td>🟨</td><td>❌</td><td>❌</td></tr></tbody></table><h2 id="alternatives" tabindex="-1">Alternatives <a href="#alternatives" aria-label="Permalink to &quot;Alternatives&quot;">​</a></h2><p>Cap is a modern alternative to:</p><ul><li><a href="https://www.google.com/recaptcha/about/" target="_blank" rel="noreferrer">reCAPTCHA</a></li><li><a href="https://www.hcaptcha.com/" target="_blank" rel="noreferrer">hCaptcha</a></li><li><a href="https://developers.cloudflare.com/turnstile/" target="_blank" rel="noreferrer">Cloudflare Turnstile</a></li></ul><p>But unlike them, Cap is <a href="https://capjs.js.org/guide/workings.html"><strong>computation-bound, not tracking-bound</strong></a>.</p><p><a href="https://capjs.js.org/guide/alternatives.html">Read more about alternatives</a></p><h2 id="license" tabindex="-1">License <a href="#license" aria-label="Permalink to &quot;License&quot;">​</a></h2><p>Cap is licensed under the Apache License 2.0.</p><hr><p><a href="https://www.bestpractices.dev/projects/9920" target="_blank" rel="noreferrer"><img src="https://www.bestpractices.dev/projects/9920/badge" alt="OpenSSF Best Practices" loading="lazy"></a></p></div></div></div>]]></description>
        </item>
    </channel>
</rss>