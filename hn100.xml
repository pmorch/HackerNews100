<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 15 Jun 2024 01:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Snowden: "They've gone full mask-off: do not ever trust OpenAI or its products" (112 pts)]]></title>
            <link>https://twitter.com/Snowden/status/1801610725229498403</link>
            <guid>40685644</guid>
            <pubDate>Fri, 14 Jun 2024 22:13:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Snowden/status/1801610725229498403">https://twitter.com/Snowden/status/1801610725229498403</a>, See on <a href="https://news.ycombinator.com/item?id=40685644">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[AI Search: The Bitter-Er Lesson (153 pts)]]></title>
            <link>https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d</link>
            <guid>40683697</guid>
            <pubDate>Fri, 14 Jun 2024 18:47:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d">https://yellow-apartment-148.notion.site/AI-Search-The-Bitter-er-Lesson-44c11acd27294f4495c3de778cd09c8d</a>, See on <a href="https://news.ycombinator.com/item?id=40683697">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft's Recall AI feature is now indefinitely delayed (391 pts)]]></title>
            <link>https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/</link>
            <guid>40683210</guid>
            <pubDate>Fri, 14 Jun 2024 18:03:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/">https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/</a>, See on <a href="https://news.ycombinator.com/item?id=40683210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-index="0" data-js="panel" data-type="wysiwyg-with-aside" data-modular-content="" data-modular-content-collection="">
<p><em><b>Update: June 13, 2024:</b> Today, we are communicating an additional update on the Recall (preview) feature for Copilot+ PCs. Recall will now shift from a preview experience broadly available for Copilot+ PCs on June 18, 2024, to a preview available first in the <a href="https://www.microsoft.com/windowsinsider/">Windows Insider Program (WIP)</a> in the coming weeks. Following receiving feedback on Recall from our Windows Insider Community, as we typically do, we plan to make Recall (preview) available for all Copilot+ PCs coming soon. &nbsp;</em></p>
<p><em>We are adjusting the release model for Recall to leverage the expertise of the Windows Insider community to ensure the experience meets our high standards for quality and security. This decision is rooted in our commitment to providing a trusted, secure and robust experience for all customers and to seek additional feedback prior to making the feature available to all Copilot+ PC users. Additionally, as we shared in our <a href="https://www.microsoft.com/en-us/security/blog/2024/05/03/security-above-all-else-expanding-microsofts-secure-future-initiative/">May 3 blog</a>, security is our top priority at Microsoft, in line with our <a href="https://www.microsoft.com/en-us/microsoft-cloud/resources/secure-future-initiative">Secure Future Initiative (SFI)</a>. This is reflected in additional security protections we are providing for Recall content, including “just in time” decryption protected by <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello-enhanced-sign-in-security">Windows Hello Enhanced Sign-in Security (ESS)</a>, so Recall snapshots will only be decrypted and accessible when the user authenticates. The development of Copilot+ PCs, Recall and Windows will continue to be guided by SFI.&nbsp;</em></p>
<p><em>When Recall (preview) becomes available in the Windows Insider Program, we will publish a blog post with details on how to get the preview. To try Recall (preview) WIP customers will need a Copilot+ PC due to our <a href="https://support.microsoft.com/en-us/topic/copilot-pcs-hardware-requirements-35782169-6eab-4d63-a5c5-c498c3037364">hardware requirements</a>. We look forward to hearing Windows Insider feedback.&nbsp;&nbsp;&nbsp;</em></p>
<p>Today, we are sharing an update on the Recall (preview) feature for Copilot+ PCs, including more information on the set-up experience, privacy controls and additional details on our approach to security.</p>
<p>On May 20, <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">we introduced Copilot+ PCs</a>, our fastest, most intelligent Windows PCs ever. Copilot+ PCs have been reimagined from the inside out to deliver better performance and all new AI experiences to help you be more productive, creative and communicate more effectively. One of the new experiences exclusive to Copilot+ PCs is Recall, a new way to instantly find something you’ve previously seen on your PC. To create an explorable visual timeline, Recall periodically takes a snapshot of what appears on your screen. These images are encrypted, stored and analyzed locally, using on-device AI capabilities to understand their context. When logged into your Copilot+ PC, you can easily <a href="https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c">retrace your steps</a> visually using Recall to find things from apps, websites, images and documents that you’ve seen, operating like your own virtual and completely private “photographic memory.” You are always in control of what’s saved. You can disable saving snapshots, pause temporarily, filter applications and delete your snapshots at any time.</p>
<p>As AI becomes more prevalent, we are rearchitecting Windows to give customers and developers more choice to leverage both the cloud and the power of local processing on the device made possible by the neural processing unit (NPU). This distributed computing model offers choice for both privacy and security. All of this work will continue to be guided by our <a href="https://www.microsoft.com/en-us/microsoft-cloud/resources/secure-future-initiative">Secure Future Initiative (SFI)</a>.</p>
<p>Our team is driven by a relentless desire to empower people through the transformative potential of AI and we see great utility in Recall and the problem it can solve. We also know for people to get the full value out of experiences like Recall, they have to trust it. That’s why we are launching Recall in preview on Copilot+ PCs – to give customers a choice to engage with <span data-contrast="auto">the feature</span><span data-contrast="auto"> early, or not, and to give </span>us an opportunity to learn from the types of <span data-ccp-parastyle="paragraph">real world</span><span data-ccp-parastyle="paragraph"> scenario</span>s customers and the Windows community finds most useful.</p>
<h3><strong>Listening to and acting on customer feedback</strong></h3>
<p>Even before making Recall available to customers, we have heard a clear signal that we can make it easier for people to choose to enable Recall on their Copilot+ PC and improve privacy and security safeguards. With that in mind we are announcing updates that will go into effect before Recall (preview) ships to customers on June 18.</p>
<ul>
<li>First, we are updating the set-up experience of Copilot+ PCs to give people a clearer choice to opt-in to saving snapshots using Recall. If you don’t proactively choose to turn it on, it will be off by default.<br>
<img fetchpriority="high" decoding="async" src="https://blogs.windows.com/wp-content/uploads/prod/sites/2/2024/06/OOBEAsset_1920-1024x649.png" alt="Recall user interface" width="1024" height="649"></li>
</ul>
<ul>
<li>Second, Windows Hello enrollment is required to enable Recall. In addition, proof of presence is also required to view your timeline and search in Recall.<br>
<img decoding="async" src="https://blogs.windows.com/wp-content/uploads/prod/sites/2/2024/06/Hello-1920-1024x581.png" alt="Windows Hello user interface" width="1024" height="581"></li>
</ul>
<ul>
<li>Third, we are adding additional layers of data protection including “just in time” decryption protected by <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello-enhanced-sign-in-security">Windows Hello Enhanced Sign-in Security (ESS)</a> so Recall snapshots will only be decrypted and accessible when the user authenticates. In addition, we encrypted the search index database.</li>
</ul>
<h3><strong>Secure by design and secure by default</strong></h3>
<p>In line with Microsoft’s SFI principles, before the preview release of Recall to customers, we are taking steps to increase data protection. Copilot+ PCs will launch with “just in time” decryption protected by Windows Hello Enhanced Sign-in Security (ESS), so Recall snapshots will only be decrypted and accessible when the user authenticates. This gives an additional layer of protection to Recall data in addition to other default enabled Window Security features like SmartScreen and Defender which use advanced AI techniques to help prevent malware from accessing data like Recall.</p>
<p>We also know the best way to secure information on a PC is to secure the whole PC itself. We want to reinforce what has previously been shared from David Weston, vice president of Enterprise and OS Security, about how <a href="https://www.microsoft.com/en-us/security/blog/2024/05/20/new-windows-11-features-strengthen-security-to-address-evolving-cyberthreat-landscape/">Copilot+ PCs have been designed to be secure by default</a> and share additional details about our security approach. Some notable examples of security enhancements include:</p>
<ul>
<li>All Copilot+ PCs will be Secured-core PCs, bringing advanced security to both commercial and consumer devices. In addition to the layers of protection in Windows 11, Secured-core PCs provide advanced firmware safeguards and dynamic root-of-trust measurement to help protect from chip to cloud.</li>
<li>Microsoft Pluton security processor will be enabled by default on all Copilot+ PCs. Pluton is a chip-to-cloud security technology – designed by Microsoft and built by silicon partners – with <a href="https://www.microsoft.com/security/business/zero-trust">Zero Trust</a> principles at the core. This helps protect credentials, identities, personal data and encryption keys, making them significantly harder to remove from the device, even if a user is tricked into installing malware or an attacker has physical possession of the PC.</li>
<li>All Copilot+ PCs will ship with <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello-enhanced-sign-in-security">Windows Hello Enhanced Sign-in Security (ESS)</a>. This provides more secure biometric sign ins and eliminates the need for a password.</li>
</ul>
<h3><strong>Protecting your privacy on Copilot+ PCs</strong></h3>
<p>In our early internal testing, we have seen different people use Recall in the way that works best for them. Some love the way it makes remembering what they’ve seen across the web so much easier to find than reviewing their browser history. Others like the way it allows them to better review an online course or find a PowerPoint. And people are taking advantage of the controls to exclude apps they don’t want captured in snapshots, from communication apps or Teams calls, or to delete some or all their snapshots. This is why we built Recall with fine-grained controls to allow each person to customize the experience to their comfort level, ensuring your information is protected and that you are in control of when, what and how it is captured.</p>
<ul>
<li><strong>Snapshots are stored locally.</strong> Copilot+ PCs have powerful AI that works on your device itself. No internet or cloud connections are used to store and process snapshots. Recall’s AI processing happens exclusively on your device, and your snapshots are kept safely on your local device only. Your snapshots are yours and they are not used to train the AI on Copilot+ PCs.</li>
<li><strong>Snapshots are not shared.</strong> Recall does not send your snapshots to Microsoft. Snapshots are not shared with any other companies or applications. Recall doesn’t share snapshots with other users who are signed into the same device, and per-user encryption ensures even administrators cannot view other users’ snapshots.</li>
<li><strong>You will know when Recall is saving snapshots.</strong> You’ll see Recall pinned to the taskbar when you reach your desktop. You’ll have a Recall snapshot icon on the system tray letting you know when Windows is saving snapshots.</li>
<li><strong>Digital rights managed or InPrivate browsing snapshots are not saved.</strong> Recall does not save snapshots of digital rights managed content or InPrivate browsing in <a href="https://support.microsoft.com/en-us/windows/privacy-and-control-over-your-recall-experience-d404f672-7647-41e5-886c-a3c59680af15#:~:text=Privacy%20and%20control%20over%20your%20Recall%20experience%201,stays%20local%204%20Built-in%20security%205%20Related%20articles">supported web browsers</a>.</li>
<li><strong>You can pause, filter and delete what’s saved at any time. </strong>You’re always in control of what’s saved as a snapshot. You can disable saving snapshots, pause them temporarily, filter applications and websites from being in snapshots, and delete your snapshots at any time.</li>
<li><strong>Enterprise and customer choice. </strong>For customers using managed work devices, your IT administrator is provided the control to disable the ability to save snapshots. However, your IT administrator cannot enable saving snapshots on your behalf. The choice to enable saving snapshots is solely yours.</li>
</ul>
<h3><strong>Empowering people with experiences they can trust</strong></h3>
<p>We are on a journey to build products and experiences that live up to our company mission to empower people and organizations to achieve more, and are driven by the critical importance of maintaining our customers’ privacy, security and trust. As we always do, we will continue to listen to and learn from our customers, including consumers, developers and enterprises, to evolve our experiences in ways that are meaningful to them.</p>
<p>We are excited for the upcoming launch of Copilot+ PCs on June 18 and for the innovative new features and benefits this entirely new category of PCs will bring. We will continue to build these new capabilities and experiences for our customers by prioritizing privacy, safety and security first. We remain grateful for the vibrant community of customers who continue to share their feedback with us.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The sun's magnetic field is about to flip (241 pts)]]></title>
            <link>https://www.space.com/sun-magnetic-field-flip-solar-maximum-2024</link>
            <guid>40682401</guid>
            <pubDate>Fri, 14 Jun 2024 16:48:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/sun-magnetic-field-flip-solar-maximum-2024">https://www.space.com/sun-magnetic-field-flip-solar-maximum-2024</a>, See on <a href="https://news.ycombinator.com/item?id=40682401">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<p><img src="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-320-80.gif" alt="Image of the sun with a bar magnet graphic rotating in front of it " srcset="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-320-80.gif 320w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-480-80.gif 480w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-650-80.gif 650w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-970-80.gif 970w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-1024-80.gif 1024w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-1200-80.gif 1200w, https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN-1920-80.gif 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN.gif" data-pin-media="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN.gif">
</p>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/JrsCfwuznpj7C6GMLQ4DUN.gif">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span>The sun's magnetic field is about to flip.</span>
<span itemprop="copyrightHolder">(Image credit: Created in Canva by Daisy Dobrijevic)</span>
</figcaption>
</div>

<div id="article-body">
<p>The sun is on the verge of a significant event: a magnetic field reversal.&nbsp;</p><p>This phenomenon happens roughly every 11 years and marks an important stage in the <a data-analytics-id="inline-link" href="https://www.space.com/solar-cycle-frequency-prediction-facts" data-before-rewrite-localise="https://www.space.com/solar-cycle-frequency-prediction-facts"><u>solar cycle</u></a>. The shift in polarity indicates the halfway point of <a data-analytics-id="inline-link" href="https://www.space.com/what-is-solar-maximum-and-when-will-it-happen" data-before-rewrite-localise="https://www.space.com/what-is-solar-maximum-and-when-will-it-happen"><u>solar maximum</u></a>, the height of solar activity, and the beginning of the shift toward solar minimum.&nbsp;</p><p>The last time <a data-analytics-id="inline-link" href="https://www.space.com/58-the-sun-formation-facts-and-characteristics.html" data-before-rewrite-localise="https://www.space.com/58-the-sun-formation-facts-and-characteristics.html"><u>the sun</u></a>'s magnetic field flipped was toward the end of 2013. But what causes this switch in polarity, and is it dangerous? Let's take a deep look at the sun's magnetic field reversal and investigate the effects it could have on <a data-analytics-id="inline-link" href="https://www.space.com/54-earth-history-composition-and-atmosphere.html" data-before-rewrite-localise="https://www.space.com/54-earth-history-composition-and-atmosphere.html"><u>Earth</u></a>.</p><p><strong>Related: </strong><a data-analytics-id="inline-link" href="https://www.space.com/giant-sunspot-ar3664-solar-storms-aurora" data-before-rewrite-localise="https://www.space.com/giant-sunspot-ar3664-solar-storms-aurora"><u>How a giant sunspot unleashed solar storms that spawned global auroras that just dazzled us all</u></a></p><p>To understand the magnetic field's reversal, first, it's important to be familiar with the solar cycle. This approximately 11-year cycle of solar activity is driven by the sun's magnetic field and is indicated by the frequency and intensity of <a data-analytics-id="inline-link" href="https://www.space.com/sunspots-formation-discovery-observations" data-before-rewrite-localise="https://www.space.com/sunspots-formation-discovery-observations"><u>sunspots</u></a> visible on the surface. The height of solar activity during a given solar cycle is known as solar maximum, and current estimates predict it will occur between <a data-analytics-id="inline-link" href="https://www.swpc.noaa.gov/products/solar-cycle-progression#:~:text=The%20Prediction%20Panel%20predicted%20Cycle,November%202024%20and%20March%202026.&amp;text=SWPC%20Space%20Weather%20Operations%20(SWO)%2C%20Daily%20Observations." data-url="https://www.swpc.noaa.gov/products/solar-cycle-progression#:~:text=The%20Prediction%20Panel%20predicted%20Cycle,November%202024%20and%20March%202026.&amp;text=SWPC%20Space%20Weather%20Operations%20(SWO)%2C%20Daily%20Observations." target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>late 2024 and early 2026</u></a>.</p><p>But there is another very important, albeit lesser-known, cycle that encapsulates two 11-year solar cycles. Known as the Hale cycle, this magnetic cycle lasts approximately 22 years, through which the sun's magnetic field reverses and then reverts to its original state, <a data-analytics-id="inline-link" href="https://www.space.com/author/ryan-french" data-before-rewrite-localise="https://www.space.com/author/ryan-french"><u>Ryan French</u></a>, a solar astrophysicist and Space.com contributing writer, told Space.com.&nbsp;</p><p>During solar minimum, the sun's magnetic field is close to a dipole, with one north pole and one south pole, similar to <a data-analytics-id="inline-link" href="https://www.space.com/earths-magnetic-field-explained" data-before-rewrite-localise="https://www.space.com/earths-magnetic-field-explained"><u>Earth's magnetic field</u></a>. But as we shift toward solar maximum, "the sun's magnetic field becomes more complex, without a clear north-south pole separation," French said. By the time solar maximum passes and solar minimum arrives, the sun has returned to a dipole, albeit with a flipped polarity.&nbsp;</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-SQesLi2fRyiUNYk4wTvBaW"><section><p>Sign up to our newsletter for the latest updates on rocket launches, skywatching events and more!</p></section></div><p>The upcoming switch in polarity will be from the northern to southern magnetic field in the Northern Hemisphere and vice versa in the Southern Hemisphere. "This will bring it to a similar magnetic orientation to Earth, which also has its southern-pointing magnetic field in the Northern Hemisphere," French explained.</p><h2 id="what-causes-the-switch-in-polarity-xa0-3">What causes the switch in polarity?&nbsp;</h2><p>The reversal is driven by sunspots, magnetically complex regions of the sun's surface that can spawn significant solar events, such as <a data-analytics-id="inline-link" href="https://www.space.com/solar-flares-effects-classification-formation" data-before-rewrite-localise="https://www.space.com/solar-flares-effects-classification-formation"><u>solar flares</u></a> and <a data-analytics-id="inline-link" href="https://www.space.com/coronal-mass-ejections-cme" data-before-rewrite-localise="https://www.space.com/coronal-mass-ejections-cme"><u>coronal mass ejections</u></a> (CMEs) — large blasts of plasma and magnetic field.</p><a target="_blank" data-url="" href="" data-hl-processed="none"><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-320-80.png.webp 320w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-970-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-1024-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-1200-80.png.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-320-80.png" alt="Diagram of where sunspots are located on the sun at different times during the solar cycle." srcset="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-320-80.png 320w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-970-80.png 970w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-1024-80.png 1024w, https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK-1200-80.png 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/BMbJvDe73XyxoU28b7kCTK.png"></picture></p></div><figcaption itemprop="caption description"><span>During solar maximum a large number of sunspots are visible at mid-latitudes and during solar minimum a very small number (sometimes zero) of sunspots are visible at the equator.&nbsp; </span><span itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure></a><p>As sunspots emerge close to the equator, they will have an orientation matching the old magnetic field, while sunspots forming closer to the poles will have a magnetic field matching the incoming magnetic orientation, French said. This is called Hale's law.&nbsp;</p><p>"The magnetic field from active regions makes its way toward the poles and eventually causes the reversal," solar physicist Todd Hoeksema, director of the Wilcox Solar Observatory at Stanford University, <a data-analytics-id="inline-link" href="https://www.space.com/22310-sun-magnetic-field-flip-mystery.html" data-before-rewrite-localise="https://www.space.com/22310-sun-magnetic-field-flip-mystery.html"><u>previously told Space.com</u></a>.</p><p>But the exact underlying cause of such a flip in polarity remains mysterious. "That gets into the whole [solar] cycle, and wondering what that is," Stanford University solar physicist Phil Scherrer previously told Space.com. "We still don't have a really self-consistent mathematical description of what's happening. And until you can model it, you don't really understand it — it's hard to really understand it."</p><p>It really depends on where the magnetic field comes from. "Are there going to be many sunspots? And are the sunspots going to contribute to the magnetic field of the pole, or are they going to kind of cancel locally?" Hoeksema said. "That question we don't yet know how to answer."</p><h2 id="how-quickly-does-the-switch-occur-xa0-3">How quickly does the switch occur?&nbsp;</h2><p>What we do know is that the solar magnetic field flip is not instantaneous. It's a gradual transition from a dipole to a complex magnetic field, to a reversed dipole over the entire 11-year solar cycle. "In short, there is no specific 'moment' in which the sun's poles flip," French said. "It's not like the Earth, where the flip is measured by the migration of the North/South pole."&nbsp;</p><p>It generally takes a year or two for a complete reversal, but it can vary significantly. For example, the north polar field of Solar Cycle 24, which ended in December 2019, took nearly five years to reverse, according to the <a data-analytics-id="inline-link" href="https://nso.edu/blog/polar-magnetic-field-reversal" target="_blank" data-url="https://nso.edu/blog/polar-magnetic-field-reversal" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>National Solar Observatory</u></a>.</p><p>The magnetic field flip is so gradual, you won't even notice when it happens. And no, however dramatic it might sound, it is not the sign of an impending apocalypse. "The world will not end tomorrow," Scherrer <a data-analytics-id="inline-link" href="https://www.space.com/22289-sun-magnetic-field-flip-earth-effects.html" data-before-rewrite-localise="https://www.space.com/22289-sun-magnetic-field-flip-earth-effects.html"><u>previously told</u></a> Space.com.</p><p>However, we will experience some of the polarity flip's side effects.&nbsp;</p><h2 id="how-does-the-sun-apos-s-magnetic-flip-affect-us-xa0-3">How does the sun's magnetic flip affect us? &nbsp;</h2><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-320-80.jpg" alt="the sun is at the center and a twirling purple sheet extends out far into the solar system. It looks a bit like a dancers skirt rippling and ruffling as they spin." srcset="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/XxzB7nEacRTghJC7Rk8CiT.jpg"></picture></p></div><figcaption itemprop="caption description"><span>Artist's impression of the heliospheric current sheet which becomes more wavy when the sun's magnetic field flips. </span><span itemprop="copyrightHolder">(Image credit: NASA)</span></figcaption></figure><p>There is no doubt that the sun has been incredibly active recently, firing out numerous powerful solar flares and CMEs, triggering strong geomagnetic storms on Earth, which, in turn, have produced some <a data-analytics-id="inline-link" href="https://www.space.com/solar-storms-may-2024-strongest-auroras-500-years" data-before-rewrite-localise="https://www.space.com/solar-storms-may-2024-strongest-auroras-500-years"><u>incredible auroral displays of late</u></a>.&nbsp;</p><p>However, the increased severity of <a data-analytics-id="inline-link" href="https://www.space.com/space-weather" data-before-rewrite-localise="https://www.space.com/space-weather"><u>space weather</u></a> is not the direct cause of the flip in polarity. Rather, these things tend to occur together, Hoeksema told Space.com in 2013.</p><p>Space weather is typically the strongest during solar maximum, when the sun's magnetic field is also the most complex, according to French.&nbsp;</p><p>One side effect of the magnetic field shift is slight but primarily beneficial: It can help shield Earth from galactic <a data-analytics-id="inline-link" href="https://www.space.com/32644-cosmic-rays.html" data-before-rewrite-localise="https://www.space.com/32644-cosmic-rays.html"><u>cosmic rays</u></a> — high-energy subatomic particles that travel at near light speed and can damage spacecraft and harm orbiting astronauts who are outside Earth's protective atmosphere.&nbsp;</p><p>As the sun's magnetic field shifts, the "current sheet" — a sprawling surface that radiates billions of miles outward from the sun's equator — <a data-analytics-id="inline-link" href="https://www.nasa.gov/science-research/heliophysics/the-suns-magnetic-field-is-about-to-flip/" target="_blank" data-url="https://www.nasa.gov/science-research/heliophysics/the-suns-magnetic-field-is-about-to-flip/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>becomes very wavy</u></a>, providing a better barrier against cosmic rays. &nbsp;</p><h2 id="predicting-future-solar-cycle-strengths-xa0-3">Predicting future solar cycle strengths &nbsp;</h2><p>Scientists will be keeping a watchful eye on the sun's magnetic field reversal and seeing how long it takes for it to bounce back into a dipole configuration. If that happens within the next couple of years, the next 11-year cycle will be relatively active, but if the buildup is slow, the cycle will be relatively weak, like the previous Solar Cycle 24.&nbsp;</p>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>
<div id="slice-container-authorBio-SQesLi2fRyiUNYk4wTvBaW"><p>Daisy Dobrijevic joined <a href="https://www.space.com/" data-before-rewrite-localise="https://www.space.com/">Space.com</a> in February 2022 having previously worked for our sister publication <a href="https://www.space.com/43203-all-about-space-free-issue.html" target="_blank" data-before-rewrite-localise="https://www.space.com/43203-all-about-space-free-issue.html">All About Space</a> magazine as a staff writer. Before joining us, Daisy completed an editorial internship with the BBC Sky at Night Magazine and worked at the <a href="https://spacecentre.co.uk/?gclid=Cj0KCQjw3f6HBhDHARIsAD_i3D-mw3nFf8xOLVjxawebnwAOTgxjs-OAi0TTmNvL8gD1MjcujcBiU6QaAlJ1EALw_wcB" target="_blank">National Space Centre</a> in Leicester, U.K., where she enjoyed communicating space science to the public. In 2021, Daisy completed a PhD in plant physiology and also holds a Master's in Environmental Science, she is currently based in Nottingham, U.K. Daisy is passionate about all things space, with a penchant for solar activity and space weather. She has a strong interest in astrotourism and loves nothing more than a good northern lights chase!&nbsp;</p></div>


</section>




<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Effect – Build robust apps in TypeScript (107 pts)]]></title>
            <link>https://effect.website/</link>
            <guid>40682149</guid>
            <pubDate>Fri, 14 Jun 2024 16:20:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://effect.website/">https://effect.website/</a>, See on <a href="https://news.ycombinator.com/item?id=40682149">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2><span>The best way to</span><br><span><span>ship faster</span><span>ship faster</span></span><span>in TypeScript</span></h2><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Maximum Type-safety (incl. error handling)</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Makes your code more composable, reusable and testable</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Extensive library with a rich ecosystem of packages</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Clustering and Workflows (Alpha)</span></li></ul></div><div><h2>Make the hard things easy</h2><p>As your application grows, Effect scales with it - keeping your code simple and maintainable.</p><div><svg viewBox="0 0 577 211" fill="none" xmlns="http://www.w3.org/2000/svg"><rect x="0.5" y="187" width="576" height="1" fill="url(#paint0_linear_280_1304)"></rect><line x1="0.957328" y1="168.502" x2="502.957" y2="125.502" stroke="url(#paint1_linear_280_1304)"></line><path d="M1.5 177C149.455 159.787 424 116 502.5 1" stroke="url(#paint2_linear_280_1304)"></path><defs><linearGradient id="paint0_linear_280_1304" x1="0.499992" y1="187.954" x2="576.5" y2="187.81" gradientUnits="userSpaceOnUse"><stop stop-color="#18181B"></stop><stop offset="0.177083" stop-color="#71717A"></stop><stop offset="1" stop-color="#09090B"></stop></linearGradient><linearGradient id="paint1_linear_280_1304" x1="1" y1="169" x2="503" y2="126" gradientUnits="userSpaceOnUse"><stop stop-color="#3178C6" stop-opacity="0.25"></stop><stop offset="0.515625" stop-color="#3178C6"></stop><stop offset="1" stop-color="#3178C6"></stop></linearGradient><linearGradient id="paint2_linear_280_1304" x1="502.5" y1="0.9998" x2="2.49996" y2="187" gradientUnits="userSpaceOnUse"><stop stop-color="#F97583"></stop><stop offset="0.489583" stop-color="#F97583"></stop><stop offset="1" stop-color="#F97583" stop-opacity="0.25"></stop></linearGradient></defs></svg><p><span><span>Complexity</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11 14"><path d="M9.31328 2.625H9.75078V3.0625V9.1875V9.625H8.87578V9.1875V4.11797L1.96602 11.0277L1.65703 11.3367L1.03906 10.7187L1.34805 10.4098L8.25781 3.5H3.18828H2.75078V2.625H3.18828H9.31328Z"></path></svg></span><span>(Lower is better)</span><span><span>–</span><span>Without Effect</span></span><span><span>–</span><span>With Effect</span></span></p></div><div><div><h3>Without Effect</h3></div><div><h3>With <span>Effect</span><svg viewBox="0 0 149 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M29.8022 24.317C30.2747 24.05 30.4361 23.4582 30.1636 22.9953C29.891 22.5329 29.2873 22.3741 28.8148 22.6411L15.9211 29.9362L3.07463 22.6683C2.60281 22.4012 1.999 22.5594 1.72597 23.0225C1.45347 23.4854 1.61541 24.077 2.08741 24.3441L15.3897 31.8698C15.5053 31.9353 15.6327 31.9771 15.7645 31.9929C15.8963 32.0087 16.0299 31.9981 16.1576 31.9617C16.278 31.9433 16.3941 31.9031 16.5002 31.8431L29.8022 24.317Z" fill="white"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M31.1298 16.6012C31.1974 16.1929 31.0061 15.7675 30.6177 15.5488L16.555 7.63105C16.4443 7.56873 16.3234 7.52682 16.198 7.50732C16.0631 7.46888 15.922 7.45758 15.7827 7.47405C15.6434 7.49053 15.5088 7.53446 15.3865 7.60332L1.32289 15.5214C0.913972 15.7518 0.723686 16.2117 0.824499 16.6391C0.780205 16.9913 0.91787 17.3598 1.32768 17.5916L15.3904 25.5478C15.5127 25.6169 15.6475 25.661 15.7869 25.6776C15.9263 25.6942 16.0675 25.6829 16.2026 25.6445C16.3297 25.6253 16.4522 25.583 16.5642 25.5197L30.6275 17.563C31.0408 17.329 31.1776 16.9562 31.1298 16.6012ZM28.2266 16.5591L15.9459 9.64453L3.67206 16.5554L15.9528 23.5034L28.2266 16.5591Z" fill="white"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M31.3429 10.6097C31.8677 10.3131 32.0476 9.65608 31.7442 9.14178C31.4416 8.62819 30.7712 8.45201 30.2464 8.74854L15.9269 16.8501L1.66063 8.77876C1.13584 8.48152 0.465408 8.65787 0.162793 9.172C-0.14053 9.68541 0.0391253 10.3432 0.564095 10.6397L15.337 18.9976C15.4654 19.0702 15.607 19.1165 15.7534 19.1339C15.8998 19.1514 16.0482 19.1395 16.19 19.0991C16.3236 19.0791 16.4524 19.0347 16.5701 18.9681L31.3429 10.6097Z" fill="white"></path><path d="M2.7403 9.6795L15.8991 1.62024L29.0577 9.67879L15.8989 17.2013L2.7403 9.6795Z" fill="white"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M31.3255 8.49027C31.8513 8.78627 32.0333 9.44279 31.7325 9.95692C31.4307 10.4707 30.7603 10.6474 30.2344 10.3514L15.9128 2.28787L1.64317 10.3224C1.11731 10.6184 0.44688 10.4415 0.145328 9.92794C-0.15587 9.41381 0.0262664 8.75729 0.55159 8.46129L15.325 0.143725C15.4534 0.0713274 15.5949 0.0251207 15.7412 0.00776107C15.8875 -0.00959854 16.0358 0.00223134 16.1775 0.0425706C16.3093 0.0631109 16.4364 0.107185 16.5526 0.172702L31.3255 8.49027Z" fill="white"></path><path d="M109.663 25.038C106.779 25.038 105.211 23.694 105.211 20.53V13.698H103.167V10.478H105.211V7.538L108.991 7.146V10.478H112.071V13.698H108.991V20.334C108.991 21.258 109.439 21.678 110.167 21.678H111.735V25.038H109.663Z" fill="white"></path><path d="M96.4072 25.402C92.2352 25.402 88.9032 21.986 88.9032 17.758C88.9032 13.53 92.1512 10.142 96.6032 10.114C99.2632 10.086 101.419 11.122 102.651 12.83L99.7392 15.21C99.0672 14.202 97.9472 13.558 96.6592 13.558C94.2512 13.558 92.6832 15.49 92.6832 17.758C92.6832 20.026 94.3352 21.958 96.7432 21.958C98.1992 21.958 99.1512 21.23 99.9072 20.222L102.651 22.602C101.279 24.366 99.2632 25.402 96.4072 25.402Z" fill="white"></path><path d="M80.6032 25.402C76.0952 25.402 73.0712 21.986 73.0712 17.786C73.0712 13.586 76.2352 10.114 80.6032 10.114C84.9712 10.114 87.9952 13.586 87.9952 17.786C87.9952 18.206 87.9672 18.654 87.8832 19.13H76.9912C77.4112 20.838 78.6992 22.042 80.6032 22.042C82.2272 22.042 83.4872 21.202 84.1592 20.082L87.0992 22.294C85.9232 24.114 83.4872 25.402 80.6032 25.402ZM76.9912 16.302H84.1872C83.7672 14.678 82.3952 13.362 80.5472 13.362C78.7552 13.362 77.4112 14.566 76.9912 16.302Z" fill="white"></path><path d="M66.328 9.498C66.328 6.558 68.232 4.598 71.172 4.598H72.74V7.818H71.76C70.64 7.818 70.108 8.406 70.108 9.554V10.478H73.076V13.698H70.108V25.038H66.328V13.698H64.284V10.478H66.328V9.498Z" fill="white"></path><path d="M56.9764 9.498C56.9764 6.558 58.8804 4.598 61.8204 4.598H63.3884V7.818H62.4084C61.2884 7.818 60.7564 8.406 60.7564 9.554V10.478H63.7244V13.698H60.7564V25.038H56.9764V13.698H54.9324V10.478H56.9764V9.498Z" fill="white"></path><path d="M41.8917 25.038V5.438H53.3717V8.966H45.8117V13.362H53.2317V16.778H45.8117V21.51H53.4277V25.038H41.8917Z" fill="white"></path></svg></h3></div></div></div><div><h2>The missing standard library for TypeScript</h2><p>TypeScript/JavaScript, the most popular programming language, is still missing a standard library. Effect is filling this gap by providing a solid foundation of data structures, utilities, and abstractions to make building applications easier.
</p><a target="_blank" href="https://2022.stateofjs.com/en-US/opinions/#top_currently_missing_from_js"><span>See 2022 State of JavaScript survey</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11 14"><path d="M9.31328 2.625H9.75078V3.0625V9.1875V9.625H8.87578V9.1875V4.11797L1.96602 11.0277L1.65703 11.3367L1.03906 10.7187L1.34805 10.4098L8.25781 3.5H3.18828H2.75078V2.625H3.18828H9.31328Z"></path></svg></a></div><section><div><div><div><h3>Powerful building blocks</h3></div><p>Every part of the Effect ecosystem is designed to be composable. The Effect primitives can be combined in many different ways to tackle the most complex problems.</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Immutable data structures</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Asynchronous queues &amp; pub-sub</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Configuration &amp; dependency management</span></li></ul><a href="https://effect.website/docs"></a></div><div><div><h3>No more one-off dependencies</h3></div><p>With Effect the batteries are included. Regardless of the application, your package.json will have never been this small.</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Data validation &amp; serialization</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Frameworks for CLI &amp; HTTP applications</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Powerful abstractions for every platform</span></li></ul><a href="https://effect.website/docs"></a></div><div><div><h3>Never try &amp; catch again</h3></div><p>Effect doesn't shy away from errors — it embraces them as a fact of life. Successfully handle failure with the built-in error-handling primitives.</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Type-safe errors as values</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Powerful retry &amp; recovery APIs</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Tools for logging &amp; tracing</span></li></ul><a href="https://effect.website/docs"></a></div></div><hr></section><div><h2>Let's see some<!-- --> <span>example code</span></h2><p>Doing the right thing in TypeScript is hard. Effect makes it easy!<br>We have collated some examples to show you how Effect can be utilized in your next project.</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Effect helps you with handling errors, async code, concurrency, streams and much more.</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Effect provides a unified replacement for many one-off dependencies.</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M256 32a224 224 0 1 1 0 448 224 224 0 1 1 0-448zm0 480A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM363.3 203.3c6.2-6.2 6.2-16.4 0-22.6s-16.4-6.2-22.6 0L224 297.4l-52.7-52.7c-6.2-6.2-16.4-6.2-22.6 0s-6.2 16.4 0 22.6l64 64c6.2 6.2 16.4 6.2 22.6 0l128-128z"></path></svg><span>Effect integrates deeply with your current tech stack.</span></li></ul></div><section><div><h2>Effect gives you new Superpowers</h2><div><div><p><h3>Composable &amp; Reusable</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>Everything in the Effect ecosystem is designed to work together. Building applications with Effect feels like playing with Lego.</p></div><div><p><h3>Maximum Type-Safety</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>Effect leverages the full power of TypeScript to give you confidence in your code. From errors to managing dependencies — if it compiles, it works.</p></div><div><p><h3>Built-In Tracing</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>The built-in support for tracing is first-class. Effect integrates seamlessly with OpenTelemetry to give you full visibility over your application's performance.</p></div><div><p><h3>Built-In Metrics</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>Effect has built-in support for metrics. You can use the provided OpenTelemetry exporter to integrate with a wide range of dashboards.</p></div><div><p><h3>Easy to refactor</h3><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path d="M305 239c9.4 9.4 9.4 24.6 0 33.9L113 465c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l175-175L79 81c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0L305 239z"></path></svg></p><p>Make changes to your app with confidence. Effect's powerful abstractions make it easy to refactor your code without breaking anything.</p></div></div></div><hr></section><div><h2>What Effect users are saying</h2></div><div><h2>Okay, so what's the catch?</h2><div><div><div><h3>Learning curve</h3></div><p>Learning Effect can be quite daunting but it doesn’t take long for you to be productive. It’s similar to learning TypeScript. It’s worth it.</p></div><div><div><h3>Different programming style</h3></div><p>To ensure a high level of type-safety, composabilty and tree-shakability, Effect's programming style might be different from what you're used to.</p></div><div><div><h3>Extensive API surface</h3></div><p>Effect has an API for every situation. While it may take some time to learn them all, a handful of the most common ones will get you a long way.</p></div></div></div><section><div><h2>Frequently asked questions</h2><div><div><h3>Can I incrementally adopt Effect?</h3><p>Yes! Adopting Effect you can start by refactoring small portions of your app, usually the ones with higher complexity, and keep going as you see fit</p></div><div><h3>Does Effect scale?</h3><p>Effect was built for production since the very beginning, we take good care of making everything as performant as possible, by providing you with better ways to deal with concurrency and great observability finding bottlenecks in your program becomes easy</p></div><div><h3>Do I have to know functional programming?</h3><p>No! While Effect makes usage of Functional Programming principles and patterns internally you can be proficient in Effect by simply using it as a smart Promise and forget that there is even a thing called Functional Programming</p></div><div><h3>The library is huge, do I have to know it all?</h3><p>No! Every module in Effect is made with a specific problem in mind that is deemed to be common enough but you don't need to know everything, in fact you can harness 80% of the productivity gain by just learning a few functions and 2-3 core modules.</p></div><div><h3>What's the minimum bundle size?</h3><p>The core of Effect is a runtime system that weights about 15k when compressed and tree-shaken, the rest scales with usage. If you end up using 100k of Effect code there is a good chance your app would have been 1Mb if not using Effect</p></div><div><h3>Any more questions?</h3><p>Feel free to reach out in our Discord community!</p><a rel="noopener noreferrer" target="_blank" href="https://discord.gg/effect-ts"></a></div></div></div><hr></section><div><h2>Join our welcoming community</h2><div><div><p><img alt="Community member" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=640&amp;q=75 640w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=750&amp;q=75 750w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=828&amp;q=75 828w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=1080&amp;q=75 1080w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=1200&amp;q=75 1200w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=1920&amp;q=75 1920w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=2048&amp;q=75 2048w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=3840&amp;q=75 3840w" src="https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2FTz0orXBCLxf9c3QNtdCuXQ6oOXg7fd10UxqFHX9hQYE%2F3iapsSPb9uS-DYbTqyic9HgyipHqRE9z7StXzyKB0lCgyAvf3-6mbEAHsRlvdKSN44qEQvm-GaUhMHVZMXD66pSKqBX01oEXyqMiLwFWJsbqvwMLyficogCM_4CmWkanyNz39E3GfoD-4H-L&amp;w=3840&amp;q=75"></p></div><div><p><img alt="Community member" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=640&amp;q=75 640w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=750&amp;q=75 750w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=828&amp;q=75 828w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=1080&amp;q=75 1080w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=1200&amp;q=75 1200w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=1920&amp;q=75 1920w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=2048&amp;q=75 2048w, https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=3840&amp;q=75 3840w" src="https://effect.website/_next/image?url=https%3A%2F%2Fcdn.discordapp.com%2Fwidget-avatars%2F2EQkX3Z2XRltmRLK5aVBE97flZ7aL9GRVFtmLXk0D58%2FH-ai_N4u1aOnAiPLB1zYmRDPCj1299qiQLHxPnn6OWSDWGJSvs90VQxKO0K0B1blDnA2K6_oxdlr1qJhr6So20TUU-uw-Eb3LFQJSUsI2JR7JWQ0h5DEr0icppbYRfOgIFOE32y5vi6OSQ&amp;w=3840&amp;q=75"></p></div></div></div><div><h2><span>Start to</span><br><span><span>ship faster</span></span><br><span>in TypeScript</span></h2><p>Effect makes it easy to build typed, robust &amp; scalable applications. Check out our friendly documentation to get started.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nemotron-4-340B (101 pts)]]></title>
            <link>https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/</link>
            <guid>40682000</guid>
            <pubDate>Fri, 14 Jun 2024 16:01:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/">https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/</a>, See on <a href="https://news.ycombinator.com/item?id=40682000">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
	<main id="main" role="main">
			<div>
		

<article id="post-72165" data-url="https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/" data-identifier="72165 https://blogs.nvidia.com/?p=72165" data-title="NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models">

	<!-- .entry-header -->

	<!-- META -->
	<!-- .entry-meta -->

	<div>
		<p dir="ltr">NVIDIA today announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry.</p>
<p dir="ltr">High-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access.</p>
<p dir="ltr">Through a uniquely permissive <a href="https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf" target="_blank" rel="noopener">open model license</a>, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.</p>
<p dir="ltr">The Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with <a href="https://github.com/NVIDIA/NeMo" target="_blank" rel="noopener">NVIDIA NeMo</a>, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source <a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener">NVIDIA TensorRT-LLM</a> library.</p>
<p dir="ltr">Nemotron-4 340B can be downloaded now from <a href="https://huggingface.co/collections/nvidia/nemotron-4-340b-666b7ebaf1b3867caf2f1911" target="_blank" rel="noopener">Hugging Face</a>. Developers will soon be able to access the models at <a href="http://ai.nvidia.com/" target="_blank" rel="noopener">ai.nvidia.com</a>, where they’ll be packaged as an <a href="https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/" target="_blank" rel="noopener">NVIDIA NIM</a> microservice with a standard application programming interface that can be deployed anywhere.</p>
<h2 dir="ltr">Navigating Nemotron to Generate Synthetic Data</h2>
<p dir="ltr">LLMs can help developers generate synthetic training data in scenarios where access to large, diverse labeled datasets is limited.</p>
<p dir="ltr">The <a href="https://huggingface.co/nvidia/Nemotron-4-340B-Instruct" target="_blank" rel="noopener">Nemotron-4 340B Instruct</a> model creates diverse synthetic data that mimics the characteristics of real-world data, helping improve data quality to increase the performance and robustness of custom LLMs across various domains.</p>
<p dir="ltr">Then, to boost the quality of the AI-generated data, developers can use the <a href="https://huggingface.co/nvidia/Nemotron-4-340B-Reward" target="_blank" rel="noopener">Nemotron-4 340B Reward</a> model to filter for high-quality responses. Nemotron-4 340B Reward grades responses on five attributes: helpfulness, correctness, coherence, complexity and verbosity. It’s currently first place on the <a target="_blank" href="https://huggingface.co/spaces/allenai/reward-bench">Hugging Face RewardBench leaderboard</a>, created by <a href="https://allenai.org/" target="_blank" rel="noopener">AI2</a>, for evaluating the capabilities, safety and pitfalls of reward models.</p>
<figure id="attachment_72168" aria-describedby="caption-attachment-72168"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-scaled.jpg"><img fetchpriority="high" decoding="async" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-672x378.jpg" alt="nemotron synthetic data generation pipeline diagram" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Synthetic-Data-Generation-Pipeline-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px"></a><figcaption id="caption-attachment-72168">In this synthetic data generation pipeline, (1) the Nemotron-4 340B Instruct model is first used to produce synthetic text-based output. An evaluator model, (2) Nemotron-4 340B Reward, then assesses this generated text — providing feedback that guides iterative improvements and ensures the synthetic data is accurate, relevant and aligned with specific requirements.</figcaption></figure>
<p dir="ltr">Researchers can also create their own instruct or reward models by customizing the <a href="https://huggingface.co/nvidia/Nemotron-4-340B-Base" target="_blank" rel="noopener">Nemotron-4 340B Base</a> model using their proprietary data, combined with the included <a href="https://huggingface.co/datasets/nvidia/HelpSteer2" target="_blank" rel="noopener">HelpSteer2 dataset</a>.</p>
<h2 dir="ltr">Fine-Tuning With NeMo, Optimizing for Inference With TensorRT-LLM</h2>
<p dir="ltr">Using open-source NVIDIA NeMo and NVIDIA TensorRT-LLM, developers can optimize the efficiency of their instruct and reward models to generate synthetic data and to score responses.</p>
<p dir="ltr">All Nemotron-4 340B models are optimized with TensorRT-LLM to take advantage of tensor parallelism, a type of model parallelism in which individual weight matrices are split across multiple GPUs and servers, enabling efficient inference at scale.</p>
<p dir="ltr">Nemotron-4 340B Base, trained on 9 trillion tokens, can be customized using the NeMo framework to adapt to specific use cases or domains. This fine-tuning process benefits from extensive pretraining data and yields more accurate outputs for specific downstream tasks.</p>
<p dir="ltr">A variety of customization methods are available through the NeMo framework, including supervised fine-tuning and parameter-efficient fine-tuning methods such as low-rank adaptation, or LoRA.</p>
<p dir="ltr">To boost model quality, developers can align their models with <a href="https://github.com/NVIDIA/NeMo-Aligner" target="_blank" rel="noopener">NeMo Aligner</a> and datasets annotated by Nemotron-4 340B Reward. Alignment is a key step in training LLMs, where a model’s behavior is fine-tuned using algorithms like reinforcement learning from human feedback (RLHF) to ensure its outputs are safe, accurate, contextually appropriate and consistent with its intended goals.</p>
<p dir="ltr">Businesses seeking enterprise-grade support and security for production environments can also access NeMo and TensorRT-LLM through the cloud-native <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" target="_blank" rel="noopener">NVIDIA AI Enterprise</a> software platform, which provides accelerated and efficient runtimes for generative AI foundation models.</p>
<h2 dir="ltr">Evaluating Model Security and Getting Started</h2>
<p dir="ltr">The Nemotron-4 340B Instruct model underwent extensive safety evaluation, including adversarial tests, and performed well across a wide range of risk indicators. Users should still perform careful evaluation of the model’s outputs to ensure the synthetically generated data is suitable, safe and accurate for their use case.</p>
<p dir="ltr">For more information on model security and safety evaluation, read the model card.</p>
<p dir="ltr">Download Nemotron-4 340B models via&nbsp;<a href="https://huggingface.co/collections/nvidia/nemotron-4-340b-666b7ebaf1b3867caf2f1911" target="_blank" rel="noopener">Hugging Face</a>. For more details, read the <a target="_blank" href="https://research.nvidia.com/publication/2024-06_nemotron-4-340b">research papers on the model</a> and <a href="https://arxiv.org/abs/2406.08673" target="_blank" rel="noopener">dataset</a>.</p>
<p><i>See </i><a href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/" target="_blank" rel="noopener"><i>notice</i></a><i> regarding software product information.</i></p>

		<!-- .entry-footer -->

	</div><!-- .entry-content -->
	
<!-- #secondary -->
	

</article><!-- #post-## -->
			<!-- .navigation -->
					</div>
			</main><!-- #main -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple set to be first Big Tech group to face charges under EU digital law (103 pts)]]></title>
            <link>https://www.ft.com/content/31a996d5-b472-4357-953e-ace078494604</link>
            <guid>40681920</guid>
            <pubDate>Fri, 14 Jun 2024 15:49:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/31a996d5-b472-4357-953e-ace078494604">https://www.ft.com/content/31a996d5-b472-4357-953e-ace078494604</a>, See on <a href="https://news.ycombinator.com/item?id=40681920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="barrier-page">
<div data-component="articleHeaderHeroOffer" data-component-unique-name="CHE-Print"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><p><h2>Try unlimited access<br><strong>Only CHF1 for 4 weeks</strong></h2></p><p>Then CHF85 per month.<br>Complete digital access to quality FT journalism on any device. Cancel anytime during your trial.</p></div></div>
<div id="recommendedOffers-CHE-Print" data-component="recommendedOffers" data-component-unique-name="CHE-Print"><p><h2 data-tiles="" data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-offer-type="premium" data-o-grid-colspan="12" data-tile=""><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw///financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p data-offer-type="premium"><h3>Standard Digital</h3><h3>FT Digital Edition</h3><h3>Premium Digital</h3><h3>Print + Premium Digital</h3><h3>Print</h3><h3>Trial</h3><h3>FT Edit</h3><h3>FT Professional</h3><h3>Weekend Print + Standard Digital</h3><h3>Weekend Print + Premium Digital</h3></p></div><p>Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.</p></div><div data-offer-type="bundle" data-o-grid-colspan="12" data-tile=""><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw///financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_bundle.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p data-offer-type="bundle"><h3>Standard Digital</h3><h3>FT Digital Edition</h3><h3>Premium Digital</h3><h3>Print + Premium Digital</h3><h3>Print</h3><h3>Trial</h3><h3>FT Edit</h3><h3>FT Professional</h3><h3>Weekend Print + Standard Digital</h3><h3>Weekend Print + Premium Digital</h3></p></div><p>Billed Quarterly at CHF379. Complete digital access plus the FT newspaper delivered Monday-Saturday.</p></div><div data-offer-type="" data-o-grid-colspan="12" data-tile=""><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw///financial-times-financial-times.cdn.zephr.com/assets/icons/primary_product_icon_.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p data-offer-type=""><h3>Standard Digital</h3><h3>FT Digital Edition</h3><h3>Premium Digital</h3><h3>Print + Premium Digital</h3><h3>Print</h3><h3>Trial</h3><h3>FT Edit</h3><h3>FT Professional</h3><h3>Weekend Print + Standard Digital</h3><h3>Weekend Print + Premium Digital</h3></p></div></div></div>
<div data-component="subscriptionOptionsV2" data-component-unique-name="UK-Print"><h2>Explore our full range of subscriptions.</h2></div>
<div data-component="whyFT" data-component-unique-name="default"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft">Find out why</a></p></div>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cost of self hosting Llama-3 8B-Instruct (199 pts)]]></title>
            <link>https://blog.lytix.co/posts/self-hosting-llama-3</link>
            <guid>40681784</guid>
            <pubDate>Fri, 14 Jun 2024 15:30:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.lytix.co/posts/self-hosting-llama-3">https://blog.lytix.co/posts/self-hosting-llama-3</a>, See on <a href="https://news.ycombinator.com/item?id=40681784">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><article dir="ltr"><h2>Cost Of Self Hosting Llama-3 8B-Instruct</h2><div><p>Sid Premkumar</p><!-- --><p>,<time datetime="2024-06-13T00:00:00.000Z">Thu Jun 13 2024</time><span>•</span><a href="https://blog.lytix.co/tags/llama-3">llama-3</a></p></div><img src="https://blog.lytix.co/posts/self-hosting-llama-3/cover.webp" alt="Cover Image">
<h2>How much does it cost to self host a LLM?<a href="#how-much-does-it-cost-to-self-host-a-llm" id="how-much-does-it-cost-to-self-host-a-llm" aria-label="Permalink for this section"></a></h2>
<h4>⚡️ TLDR: Assuming 100% utilization of your model <code dir="ltr">Llama-3 8B-Instruct</code> model costs about $17 dollars per 1M tokens when self hosting with EKS, vs ChatGPT with the same workload can offer $1 per 1M tokens. Choosing to self host the hardware can make the cost &lt;$0.01 per 1M token that takes ~5.5 years to break even.<a href="#️-tldr-assuming-100-utilization-of-your-model-llama-3-8b-instruct-model-costs-about-17-dollars-per-1m-tokens-when-self-hosting-with-eks-vs-chatgpt-with-the-same-workload-can-offer-1-per-1m-tokens-choosing-to-self-host-the-hardware-can-make-the-cost-001-per-1m-token-that-takes-55-years-to-break-even" id="️-tldr-assuming-100-utilization-of-your-model-llama-3-8b-instruct-model-costs-about-17-dollars-per-1m-tokens-when-self-hosting-with-eks-vs-chatgpt-with-the-same-workload-can-offer-1-per-1m-tokens-choosing-to-self-host-the-hardware-can-make-the-cost-001-per-1m-token-that-takes-55-years-to-break-even" aria-label="Permalink for this section"></a></h4>
<p>A question we often get is how do you scale with ChatGPT. One thing we wanted to try is to determine the cost of self hosting an open model such as Llama-3.</p>
<br>
<h5>Determining the best hardware<a href="#determining-the-best-hardware" id="determining-the-best-hardware" aria-label="Permalink for this section"></a></h5>
<p><em>Context</em>: All tests were run on a EKS cluster. Each test was allocated the resources of the entire node, nothing else was running on that node other than system pods (prometheus, nvidia-daemon, etc.)</p>
<p>We first wanted to start small, can we run it on a single Nvidia Tesla T4 GPU? I started with AWS’s <code dir="ltr">g4dn.2xlarge</code> instance. Its specs were as follows (<a href="https://aws.amazon.com/ec2/instance-types/g4/" target="_blank" rel="noreferrer">source<span> (opens in a new tab)</span></a>):</p>
<ul>
<li>1 NVidia Tesla T4</li>
<li>32GB Memory.</li>
<li>8 vCPUs</li>
</ul>
<p>The short answer is that running either the 8B param or the 70B param version of Llama 3 did not work on this hardware. We initially tried the 70B param version of Llama 3 and constantly ran into OOM issues.
We then sobered up and tried the 8B param version. Although this time we was able to get a response, generating the response took what felt like ~10 minutes. I checked <code dir="ltr">nvidia-smi</code> and the single GPU was being used, it just wasn’t enough.</p>
<p>For context 8B vs 70B refers to the parameters in the model and generally translate to performance of the model. The advantage of the 8B param is its small size and resource footprint.</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/cuda-oom.png" alt="CUDA OOM">
<p>So I decided to switch to the <code dir="ltr">g4dn.16xlarge</code> instance type. The specs of that were as follows (<a href="https://aws.amazon.com/ec2/instance-types/g4/" target="_blank" rel="noreferrer">source<span> (opens in a new tab)</span></a>):</p>
<ul>
<li>4 NVidia Tesla T4s</li>
<li>192gb memory</li>
<li>48 vCPUs</li>
</ul>
<h5>Initial Implementation<a href="#initial-implementation" id="initial-implementation" aria-label="Permalink for this section"></a></h5>
<p>My initial implementation involved trying to copy and paste the code present on the llama-3 hugging face (<a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" target="_blank" rel="noreferrer">see<span> (opens in a new tab)</span></a>)</p>

<p>These results seemed a lot more promising as the response time lowered from ~10m to sub 10 seconds consistently.</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/inital-implementation-results.png" alt="Initial Implementation Results">
<p>I was able to get a response in a much more reasonable 5-7 seconds. At this point I wanted to start calculating the cost of this request.</p>
<p><code dir="ltr">g5dn.12xlarge</code> costs $3.912 per hour on demand (<a href="https://aws.amazon.com/ec2/instance-types/g5/" target="_blank" rel="noreferrer">see<span> (opens in a new tab)</span></a>).</p>
<p>If we assume a full month of use, that costs:</p>

<p>I had trouble with getting the token count in hugging face so ended up using <a href="https://belladoreai.github.io/llama-tokenizer-js/example-demo/build/" target="_blank" rel="noreferrer">llama-tokenizer-js<span> (opens in a new tab)</span></a> to get an approximation of tokens used.</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/llama-3-tokenizer-js.png" alt="Llama-3 Tokenizer JS">
<p><em><em>Note: This ended up being incorrect way of calculating the tokens used.</em></em></p>
<p>If we look at this result, we assume we used 39 tokens over ~6 seconds. Assuming Im processing tokens 24/7, extrapolating that we get:</p>

<p>With 6.5 tokens per second, over the month I can process:</p>

<p>With 16,848,000 tokens per month, every million tokens costs:</p>

<p>😵Yikes. It is not looking good. Lets compare this to what ChatGPT 3.5 can get us.</p>
<p>Looking at their <a href="https://openai.com/api/pricing/" target="_blank" rel="noreferrer">pricing<span> (opens in a new tab)</span></a>, ChatGPT 3.5 Turbo charges $0.5 per 1M input token, and $1.5 per 1M output token. For sake of simplicity, assuming an average input:output ratio, that means per 1M tokens they charge $1 and thats the number to beat. Far from my $167.17.</p>
<h5>Realization Something Was Wrong<a href="#realization-something-was-wrong" id="realization-something-was-wrong" aria-label="Permalink for this section"></a></h5>
<p>At this point I felt I was doing something wrong. Llama 3 with 8B params is hard to run, but I didn’t feel it should be this hard, especially considering I have 4 GPUs available 🤔.</p>
<p>I decided to try to use vLLM to host an API server instead of attempting to do it myself via hugging face libraries.</p>
<p>This was dead simple and just involved me installing <code dir="ltr">ray</code> and <code dir="ltr">vllm</code> via <code dir="ltr">pip3</code> and then changing my docker entry point to:</p>

<p>Specifically noting that I call <code dir="ltr">—tensor-parallel-size 4</code> to enforce that we use all 4 GPUs.</p>
<p>Using this got <em>significantly</em> better results.</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/significantly-better-results.png" alt="Llama-3 Tokenizer JS">
<p>In this example you can see the query took 2044ms to complete. This was also the time I realized my previous method of calculating token usage was incorrect. vLLM returned the tokens used which was perfect.</p>
<p>Going back to cost, now we can calculate tokens per second:</p>

<p>Again assuming I produce tokens 24/7, this means that I can ingest a total of:</p>

<p>Which means the cost per 1 million tokens would cost me:</p>

<p>Unfortunately this is still not below the value that ChatGPT offers, you’d lose about $17 a day 😭</p>
<h5>An Unconventional Approach<a href="#an-unconventional-approach" id="an-unconventional-approach" aria-label="Permalink for this section"></a></h5>
<p>Instead of using AWS another approach involves self hosting the hardware as well. Even after factoring in energy, this does dramatically lower the price.</p>
<p>Assuming we want to mirror our setup in AWS, we’d need 4xNVidia Tesla T4s. You can buy them for about $700 dollars on eBay</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/ebay-listing.png" alt="Ebay Listing">
<p>Add in $1,000 to setup the rest of the rig and you have a final price of around:</p>

<p>If we calculate energy for this, we get about ~$50 bucks. I determined power consumption by 70W per GPU + 20W overhead:</p>
<img src="https://blog.lytix.co/posts/self-hosting-llama-3/energy-cost.png" alt="Energy Cost">
<p>After factoring in the ~$3,800 fixed cost, you have a monthly cost of ~$50 bucks, lets round up to ~$100 to factor any misc items we might have missed.</p>
<p>Re-calculating our cost per 1M tokens now:</p>

<p>Which is <strong>significantly</strong> cheaper than our ChatGPT costs.</p>
<p>Trying to determine when you’d break even, assuming you want to produce 157,075,200 tokens with ChatGPT, you’re looking at a bill of:</p>

<p>You have a fixed cost of ~$100 a month, which results in a ‘profit’ of $57. To make up your initial server cost of $3,800, you’d need about 66 months or 5.5 years to benefit from this approach.</p>
<p>Although this approach does come with negatives such as having to manage and scale your own hardware, it does seem to be possible to undercut the prices that ChatGPT offer by a significant amount in theory. In pracitice however, you'd have to evaluate how often you are utilizing your LLM, these hypotheticals all assume 100% utilization which is not realistic and would have to be tailord per use case.</p><small><time>2024</time> © lytix.ai.</small></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Japan Passes Law to Allow Third-Party App Stores on the iPhone (128 pts)]]></title>
            <link>https://www.macrumors.com/2024/06/14/japan-passes-law-to-allow-third-party-app-stores/</link>
            <guid>40681375</guid>
            <pubDate>Fri, 14 Jun 2024 14:41:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2024/06/14/japan-passes-law-to-allow-third-party-app-stores/">https://www.macrumors.com/2024/06/14/japan-passes-law-to-allow-third-party-app-stores/</a>, See on <a href="https://news.ycombinator.com/item?id=40681375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2024/06/14/japan-passes-law-to-allow-third-party-app-stores/"><p>New <a href="https://www.jftc.go.jp/file/240612EN3.pdf">legislation in Japan</a> requires Apple to allow third-party app stores and payment providers on the <a href="https://www.macrumors.com/guide/iphone/">iPhone</a>.</p>
<p><img src="https://images.macrumors.com/t/9vxUsPNctk90_yUedrMMhPKdGA8=/400x0/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg?lossy" srcset="https://images.macrumors.com/t/9vxUsPNctk90_yUedrMMhPKdGA8=/400x0/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg?lossy 400w,https://images.macrumors.com/t/gBGXBeSG7RiXuc1kweuddC_OT9M=/800x0/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg?lossy 800w,https://images.macrumors.com/t/5lUXiumA7jjt-2sOhx-q2-SS25M=/1600x0/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg 1600w,https://images.macrumors.com/t/lfOnH7SoaWstF9bMLPrzjm-JL-Y=/2500x0/filters:no_upscale()/article-new/2021/12/apple-japan-new-year-promotion-2022.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="apple japan new year promotion 2022" width="1614" height="944"><br>The Japanese parliament has passed the Act on Promotion of Competition for Specified Smartphone Software, a law that compels Apple to allow access to third-party app stores and payment providers on devices that run iOS. The legislation, which was passed by Japan's upper house and will be enforced following Cabinet approval within the next eighteen months, seeks to curb the dominance of major tech firms like Apple in the smartphone market.</p>
<p>The law requires Apple to make several significant changes to its business practices. The company will have to permit third-party app stores on its devices, just like it does in the EU. App developers will be allowed to use third-party payment services. There are also provisions to allow users to change default settings via new choice screens during setup, such as for selecting a default browser.</p>
<p>Apple will be forbidden from giving its own services preferential treatment in search results without a justifiable reason. The law also prohibits the use of data acquired about competing software to benefit its own apps. Additionally, the law requires that third-party developers have access to the same features as Apple's own apps and services, such as NFC for contactless payments.</p>
<p>Failure to comply with these new regulations could lead to fines amounting to 20 percent of relevant turnover, with the figure increasing to 30 percent for repeat offenses. In a statement to <a href="https://www.theverge.com/2024/6/13/24177599/apple-google-japan-law-third-party-app-stores-competition"><em>The Verge</em></a>, Apple said:</p>
<blockquote><p>The Japanese government made a number of changes to the legislation that will help protect user privacy, data security, innovation, and our intellectual property. We will continue our engagement with the JFTC during the implementation period as we remain concerned about how the law will impact Japanese consumers and the secure and private iPhone experience our users have come to expect.</p></blockquote>
<p>The law is expected to be fully implemented by the end of 2025. <a href="https://www.macrumors.com/guide/epic-games/">Epic Games</a> has already announced plans to bring Fortnite and its game store platform to iOS in Japan by late 2025.</p>
<p>Japan's move follows a trend of international legislative efforts aimed at regulating the dominance of major tech companies. The European Union's Digital Markets Act (DMA) and the UK's Digital Markets, Competition and Consumers Bill are similar initiatives designed to foster competition and prevent monopolistic practices. Various antitrust cases in the United States are also targeting similar issues.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2024/06/11/ios-18-pop-out-button-animation/">iOS 18 Adds Pop-Out Bezel Animation When Pressing iPhone Buttons</a></h3><p>Tuesday June 11, 2024 10:40 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>iOS 18 includes a small but interesting change for the buttons on the iPhone, adding more of a visual element when changing volume, activating the Action button, or locking the screen. When you press an iPhone button in iOS 18, the display bezel bulges outward slightly. This feature is available for the volume buttons, Action button and the power button, and it will also likely be used for...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/ios-18-compatible-with-these-iphone-models/">Revealed: iOS 18 Works With These iPhone Models</a></h3><p>iOS 18 will be compatible with the same iPhone models as iOS 17, according to a post on X today from a private account with a proven track record of sharing build numbers for upcoming iOS updates. iOS 18 will be compatible with the iPhone XR, and hence also the iPhone XS and iPhone XS Max models with the same A12 Bionic chip, but older iPhone models will miss out. Here is the full...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/wwdc-2024-next-generation-carplay-images/">Apple Provides Updated Look at Next-Generation CarPlay at WWDC 2024</a></h3><p>Apple today shared a few WWDC 2024 coding sessions related to its upcoming next-generation CarPlay system ahead of its launch later this year. The sessions include lots of updated next-generation CarPlay images, with one revealing new Vehicle, Media, and Climate apps in action for the first time. MacRumors previously discovered evidence of these apps in the iOS 17.4 beta. Next-generation...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/apple-announces-ios-18-with-new-customization-features-and-more/">Apple Announces iOS 18 With New Customization Features, Redesigned Photos App, and More</a></h3><p>Apple today previewed iOS 18, the next major update to the operating system for the iPhone, with new customization features, a redesigned Photos app, and more. iOS 18 features new customization tools for the Home Screen. App icons now feature Dark Mode and users can tint them with a color to create a unique look. Apps can also now be placed anywhere on the Home Screen freely. The Control...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/macos-sequoia-and-ipados-18-compatibility/">macOS Sequoia and iPadOS 18 Drop Support for These Macs and iPads</a></h3><p>macOS Sequoia is still compatible with several Intel-based Macs, but it does drop support for 2018 and 2019 models of the MacBook Air. macOS Sequoia is compatible with the following Macs, according to Apple: MacBook Pro: 2018 and later MacBook Air: 2020 and later Mac mini: 2018 and later iMac: 2019 and later iMac Pro: 2017 Mac Studio: 2022 and later Mac Pro: 2019 and later The...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/10/wwdc-2024-recap/">Everything Apple Announced at WWDC 2024 in Nine Minutes</a></h3><p>Monday June 10, 2024 7:59 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple crammed an overwhelming number of new features into its WWDC 2024 keynote event, introducing Apple Intelligence, iOS 18, iPadOS 18, macOS Sequoia, visionOS 2, watchOS 11, and tvOS 18. It was hard to keep up with everything that Apple highlighted, so we did a video of all of the new additions you won't want to miss. Subscribe to the MacRumors YouTube channel for more videos. We've also...</p></div><div><h3><a href="https://www.macrumors.com/2024/06/09/ios-18-these-iphones-wont-support-ai/">Massive iPhone Upgrade Coming This Week But These Devices Will Miss Out</a></h3><p>Apple is planning a major AI overhaul in iOS 18, with a feature set it is referring to as "Apple Intelligence." However, these new features will not work on older iPhones, even if they do appear on the new operating system's device compatibility list. Apple's initial AI roadmap for iOS 18 is said to come in two parts: Basic AI features that will be processed on-device, and more advanced...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sleep deprivation disrupts memory (190 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-01732-y</link>
            <guid>40681345</guid>
            <pubDate>Fri, 14 Jun 2024 14:39:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-01732-y">https://www.nature.com/articles/d41586-024-01732-y</a>, See on <a href="https://news.ycombinator.com/item?id=40681345">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_27197870.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_27197870.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="A fluorescence light micrograph of brain hippocampus neurons, shown in green and blue" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_27197870.jpg">
  <figcaption>
   <p><span>Neurons (artificially coloured) in the hippocampus play a part in learning and memory.</span><span>Credit: Cell Applications Inc/Science Photo Library</span></p>
  </figcaption>
 </picture>
</figure><p>A crucial brain signal linked to <a href="https://www.nature.com/articles/d41586-022-02298-3" data-track="click" data-label="https://www.nature.com/articles/d41586-022-02298-3" data-track-category="body text link">long-term memory</a> falters in rats when they are deprived of sleep — which might help to explain why <a href="https://www.nature.com/articles/d41586-023-01849-6" data-track="click" data-label="https://www.nature.com/articles/d41586-023-01849-6" data-track-category="body text link">poor sleep disrupts memory formation</a><sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>. Even a night of normal slumber after a poor night’s sleep <a href="https://www.nature.com/articles/d41586-019-00723-8" data-track="click" data-label="https://www.nature.com/articles/d41586-019-00723-8" data-track-category="body text link">isn’t enough to fix the brain signal</a>.</p><p>These results, published today in <i>Nature</i>, suggest that there is a “critical window for memory processing”, says Loren Frank, a neuroscientist at the University of California, San Francisco, who was not involved with the study. “Once you’ve lost it, you’ve lost it.”</p><p>In time, these findings could lead to targeted <a href="https://www.nature.com/articles/d41586-023-02833-w" data-track="click" data-label="https://www.nature.com/articles/d41586-023-02833-w" data-track-category="body text link">treatments to improve memory</a>, says study co-author Kamran Diba, a computational neuroscientist at the University of Michigan Medical School in Ann Arbor.</p><h2>Firing in lockstep</h2><p>Neurons in the brain seldom act alone; they are highly interconnected and often fire together in a rhythmic or repetitive pattern. One such pattern is the <a href="https://www.nature.com/articles/d41586-021-02122-4" data-track="click" data-label="https://www.nature.com/articles/d41586-021-02122-4" data-track-category="body text link">sharp-wave</a><a href="https://www.nature.com/articles/d41586-021-02122-4" data-track="click" data-label="https://www.nature.com/articles/d41586-021-02122-4" data-track-category="body text link"> ripple</a>, in which a large group of neurons fire with extreme synchrony, then a second large group of neurons does the same and so on, one after the other at a particular tempo. These ripples occur in a brain area called the <a href="https://www.nature.com/articles/d41586-019-02211-5" data-track="click" data-label="https://www.nature.com/articles/d41586-019-02211-5" data-track-category="body text link">hippocampus, which is key to memory formation</a>. The patterns are thought to facilitate communication with the neocortex, where long-term memories are later stored.</p><p>One clue to their function is that some of these ripples are accelerated re-runs of brain-activity patterns that occurred during past events. For example, when an animal visits a particular spot in its cage, <a href="https://www.nature.com/articles/d41586-020-03164-w" data-track="click" data-label="https://www.nature.com/articles/d41586-020-03164-w" data-track-category="body text link">a specific group of neurons in the hippocampus fires in unison</a>, creating a neural representation of that location. Later, these same neurons might participate in sharp-wave ripples — as if they were rapidly replaying snippets of that experience.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-01626-x" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_23192898.jpg"><p>Unpicking the link between smell and memories</p></a>
 </article><p>Previous research<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> found that, when these ripples were disturbed, mice struggled on a memory test. And when the ripples were prolonged, their performance on the same test improved<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>, leading György Buzsáki, a systems neuroscientist at NYU Langone Health in New York City, who has been researching these bursts since the 1980s, to call the ripples a ‘cognitive biomarker’ for memory and learning.</p><p>Researchers also noticed<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup> that sharp-wave ripples tend to occur during deep sleep as well as during waking hours, and that those bursts during slumber seem to be particularly important for transforming short-term knowledge into long-term memories<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>. These links between the ripples, sleep and memory are well-documented, but there have been few studies that have directly manipulated sleep to determine how it affects these ripples, and in turn memory, Diba says.</p><h2>Wake-up call</h2><p>To understand how poor sleep affects memory, Diba and his colleagues recorded hippocampal activity in seven rats as they explored mazes over the course of several weeks. The researchers regularly disrupted the sleep of some of the animals and let others sleep at will.</p><p>To Diba’s surprise, rats that were woken up repeatedly had similar, or even higher, levels of sharp-wave-ripple activity than the rodents that got normal sleep did. But the firing of the ripples was weaker and less organized, showing a marked decrease in repetition of previous firing patterns. After the sleep-deprived animals recovered over the course of two days, re-creation of previous neural patterns rebounded, but never reached levels found in those which had normal sleep.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-024-00930-y" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-01732-y/d41586-024-01732-y_26984650.jpg"><p>Memories are made by breaking DNA — and fixing it</p></a>
 </article><p>This study makes clear that “memories continue to be processed after they’re experienced, and that post-experience processing is really important”, Frank says. He adds that it could explain why cramming before an exam or pulling an all-nighter might be an ineffective strategy.</p><p>It also teaches researchers an important lesson: the content of sharp-wave ripples is more important than its quantity, given that rats that got normal sleep and rats that were sleep-deprived had a similar number of ripples, he says.</p><h2>Ripple effects</h2><p>Buzsáki says that these findings square with data his group published in March<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup> that found that sharp-wave ripples that occur while an animal is awake might help to select which experiences enter long-term memory.</p><p>It’s possible, he says, that the disorganized sharp-wave ripples of sleep-deprived rats don’t allow them to effectively flag experiences for long-term memory. As a result, the animals might be unable to replay the neural firing of those experiences at a later time.</p><p>This means that sleep disruption could be used to prevent memories from entering long-term storage, which could be useful for people who have recently experienced something traumatic, such as those with post-traumatic stress disorder, Buzsáki says.</p>
                </div><div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Giri, B. <i>et al.</i> <i>Nature</i> https://doi.org/10.1038/s41586-024-07538-2 (2024).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-024-07538-2" data-track-item_id="10.1038/s41586-024-07538-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-024-07538-2" aria-label="Article reference 1" data-doi="10.1038/s41586-024-07538-2">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature&amp;doi=10.1038%2Fs41586-024-07538-2&amp;publication_year=2024&amp;author=Giri%2CB.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Jadhav, S. P., Kemere, C., German, P. W. &amp; Frank, L. <i>M. Science</i> <b>336</b>, 1454–1458 (2012).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1126/science.1217230." data-track-item_id="10.1126/science.1217230." data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.1217230." aria-label="Article reference 2" data-doi="10.1126/science.1217230.">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=M.%20Science&amp;doi=10.1126%2Fscience.1217230.&amp;volume=336&amp;pages=1454-1458&amp;publication_year=2012&amp;author=Jadhav%2CS.%20P.&amp;author=Kemere%2CC.&amp;author=German%2CP.%20W.&amp;author=Frank%2CL.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">Fernández-Ruiz, A. <i>et al.</i> <i>Science</i> <b>364</b>, 1082–1086 (2019).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1126/science.aax0758" data-track-item_id="10.1126/science.aax0758" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.aax0758" aria-label="Article reference 3" data-doi="10.1126/science.aax0758">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Science&amp;doi=10.1126%2Fscience.aax0758&amp;volume=364&amp;pages=1082-1086&amp;publication_year=2019&amp;author=Fern%C3%A1ndez-Ruiz%2CA.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="4."><p id="ref-CR4">Eschenko, O., Ramadan, W., Mölle, M., Born, J. &amp; Sara, S. J. <i>Learn. Mem.</i> <b>15</b>, 222–228 (2008).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1101/lm.726008" data-track-item_id="10.1101/lm.726008" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1101%2Flm.726008" aria-label="Article reference 4" data-doi="10.1101/lm.726008">Article</a>&nbsp;
    <a data-track="click||click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18385477" aria-label="PubMed reference 4">PubMed</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Learn.%20Mem.&amp;doi=10.1101%2Flm.726008&amp;volume=15&amp;pages=222-228&amp;publication_year=2008&amp;author=Eschenko%2CO.&amp;author=Ramadan%2CW.&amp;author=M%C3%B6lle%2CM.&amp;author=Born%2CJ.&amp;author=Sara%2CS.%20J.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="5."><p id="ref-CR5">Ramadan, W., Eschenko, O. &amp; Sara, S. J.<i> PLoS ONE</i> <b>4</b>, e6697 (2009).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pone.0006697" data-track-item_id="10.1371/journal.pone.0006697" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pone.0006697" aria-label="Article reference 5" data-doi="10.1371/journal.pone.0006697">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=PLOS%20ONE&amp;doi=10.1371%2Fjournal.pone.0006697&amp;volume=4&amp;publication_year=2009&amp;author=Ramadan%2CW.&amp;author=Eschenko%2CO.&amp;author=Sara%2CS.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="6."><p id="ref-CR6">Yang, W. <i>et al.</i> <i>Science</i> <b>383</b>, 1478–1483 (2024).</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1126/science.adk8261" data-track-item_id="10.1126/science.adk8261" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.adk8261" aria-label="Article reference 6" data-doi="10.1126/science.adk8261">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Science&amp;doi=10.1126%2Fscience.adk8261&amp;volume=383&amp;pages=1478-1483&amp;publication_year=2024&amp;author=Yang%2CW.">
                    Google Scholar</a>&nbsp;
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/d41586-024-01732-y?format=refman&amp;flavour=references">Download references</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[H.264 Is Magic (2016) (134 pts)]]></title>
            <link>https://sidbala.com/h-264-is-magic/</link>
            <guid>40681306</guid>
            <pubDate>Fri, 14 Jun 2024 14:35:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sidbala.com/h-264-is-magic/">https://sidbala.com/h-264-is-magic/</a>, See on <a href="https://news.ycombinator.com/item?id=40681306">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <!--kg-card-begin: markdown--><p>H.264 is a video compression codec standard. It is ubiquitous - internet video, Blu-ray, phones, security cameras, drones, everything. Everything uses H.264 now.</p>
<p>H.264 is a remarkable piece of technology. It is the result of 30+ years of work with one single goal: To reduce the bandwidth required for transmission of full-motion video.</p>
<p>Technically, it is very interesting. This post will give insight into some of the details at a high level - I hope to not bore you too much with the intricacies. Also note that many of the concepts explained here apply to video compression in general, and not just H.264.</p>
<blockquote>
<p>Why even compress anything?</p>
</blockquote>
<p>A simple uncompressed video file will contain an array of 2D buffers containing pixel data for each frame. So it's a 3D (2 spatial dimensions and 1 temporal) array of bytes. Each pixel takes 3 bytes to store - one byte each for the three primary colors (red, green and blue).</p>
<p>1080p @ 60 Hz = 1920x1080x60x3 =&gt; ~<strong>370 MB/sec</strong> of raw data.</p>
<p>This is next to impossible to deal with. A 50GB Blu-ray disk will only hold ~2 mins. You can't move it anywhere fast. Even SSDs have trouble dumping this straight from RAM to Disk[^1].</p>
<p>So yeah. We need compression.</p>
<blockquote>
<p>Why <em>H.264</em> compression?</p>
</blockquote>
<p>Yes, I will answer this. But first let me show you something. Here is the Apple Homepage:</p>
<p><img src="https://sidbala.com/content/images/2016/11/HomePage.png" alt="" loading="lazy"></p>
<p>I captured the screen of this home page and produced two files:</p>
<ul>
<li><a href="https://sidbala.com/content/images/2016/11/outputFrame.png">PNG screenshot of the Apple homepage</a> <strong>1015KB</strong></li>
<li><a href="https://s3-us-west-2.amazonaws.com/sidbala-blog/VideoH264.mp4?ref=sidbala.com">5 Second 60fps H.264 video of the same Apple homepage</a> <strong>175KB</strong></li>
</ul>
<blockquote>
<p>Eh. What? Those file sizes look switched.</p>
</blockquote>
<p>No, they're right. The H.264 video, 300 frames long is 175KB. A single frame of that video in PNG is 1015KB.</p>
<p>It looks like we're storing 300 times the amount of data in the video. But the file size is a fifth. So H.264 would seem to be 1500x as efficient as PNG.</p>
<blockquote>
<p>How is this even possible? All right, what's the trick?</p>
</blockquote>
<p>There are very many tricks! H.264 uses all the tricks you can think of (and tons you can't think of). Let's go through the important ones.</p>
<h6 id="sheddingweight">Shedding weight</h6>
<p>Imagine you're building a car for street racing. You need to go faster. What is the first thing you do? You shed some weight. Your car weighs 3000 lbs. You throw away stuff you don't need. Those back seats? pfft. Chuck those. That subwoofer? Gone. No music for you. Air Conditioning? Yeah, ditch it. Transmission? Ye..no. Wait! We're gonna need that.</p>
<p>You remove everything except the things that matter.</p>
<p>This concept of throwing away bits you don't need to save space is called <strong>lossy</strong> compression. H.264 is a lossy codec - it throws away less important bits and only keeps the important bits.</p>
<p>PNG is a <strong>lossless</strong> codec. It means that nothing is thrown away. Bit for bit, the original source image can be recovered from a PNG encoded image.</p>
<blockquote>
<p>Important bits? How does the algorithm know what bits in my frame are important?</p>
</blockquote>
<p>There are few obvious ways to trim out images. Maybe the top right quadrant is useless all the time. So maybe we can zero out those pixels and discard that quadrant. We would use only 3/4th of the space we need. ~2200 lbs now. Or maybe we can crop out a thick border around the edges of the frame, the important stuff is in the middle anyway. Yes, you could do these. But H.264 doesn't do this.</p>
<blockquote>
<p>What does H.264 actually do?</p>
</blockquote>
<p>H.264, like other lossy image algorithms, discards detail information. Here is a close-up of the original compared with the image post-discard.</p>
<p><img src="https://sidbala.com/content/images/2016/11/CompressedImage-1.jpg" alt="" loading="lazy"></p>
<p>See how the compressed one does not show the holes in the speaker grills in the MacBook Pro? If you don't zoom in, you would even notice the difference. The image on the right weighs in at <strong>7%</strong> the size of the original - and we haven't even compressed the image in the traditional sense. Imagine your car weighed just 200 lbs!</p>
<blockquote>
<p>7% wow! How do you discard detail information like that?</p>
</blockquote>
<p>For this we need a quick math lesson.</p>
<h6 id="informationentropy">Information Entropy</h6>
<p>Now we're getting to the juicy bits! Ha puns! If you took an information theory class, you might remember information entropy. Information entropy is the number of bits required to represent some information. Note that it is not simply the size of some dataset. It is minimum number of bits that must be used to represent all the information contained in a dataset.</p>
<p>For example, if your dataset is the result of a single coin toss, you need 1 bit of entropy. If you have record two coin tosses, you'll need 2 bits. Makes sense?</p>
<p>Suppose you have some strange coin - you've tossed it 10 times, and every time it lands on heads. How would you describe this information to someone? You wouldn't say HHHHHHHHH. You would just say "10 tosses, all heads" - bam! You've just compressed some data! Easy. I saved you hours of mindfuck lectures. This is obviously an oversimplification, but you've transformed some data into another shorter representation of the same information. You've reduced data <strong>redundancy</strong>. The information entropy in this dataset has not changed - you've just converted between representations. This type of encoder is called an <strong>entropy encoder</strong> - it's a general-purpose lossless encoder that works for any type of data.</p>
<h6 id="frequencydomain">Frequency Domain</h6>
<p>Now that you understand information entropy, let's move on to transformations of data. You can represent data in some fundamental units. If you use binary, you have 0 and 1. If you use hex, you have 16 characters. You can easily transform between the two systems. They are essentially equivalent. So far so good? Ok!</p>
<p>Now, some imagination! Imagine you can transform any dataset that varies over space(or time) - something like the brightness value of an image, into a different coordinate space. So instead of x-y coordinates, let's say we have frequency coordinates. freqX and freqY are the axes now. This is called a <strong>frequency domain</strong> representation. There is another mindfuck mathematical theorem[^2] that states that you can do this for any data and you can achieve a perfect lossless transformation as long as freqX and freqY are high enough.</p>
<blockquote>
<p>Okay, but what the freq are freqX and freqY?</p>
</blockquote>
<p>freqX and freqY are some other set of basis units. Just like when we switch from binary to hex, we have a different fundamental unit, we're switching from the familiar X-Y to freqX and freqY. Hex 'A' looks different from binary '1010'. Both mean the same thing, but <strong>look</strong> different. So here is what our image looks like in the frequency domain:</p>
<p><img src="https://sidbala.com/content/images/2016/11/BasicFFT-2.png" alt="" loading="lazy"></p>
<p>The fine grill on that MacBook pro has a high information content in the higher frequency components of that image. Finely varying content = high frequency components. Any sort of gradual variation in the color and brightness - such as gradients are low frequency components of that image. Anything in between falls in between. So fine details = high freq. Gentle gradients = low freq. Makes sense?</p>
<p>In the frequency domain representation, the low frequency components are near the center of that image. The higher frequency components are towards of the edges of the image.</p>
<blockquote>
<p>Okay. Kinda makes sense. But why do all this?</p>
</blockquote>
<p>Because now, you can take that frequency domain image and then mask out the edges - discard information which will contain the information with high frequency components. Now if you convert back to your regular x-y coordinates, you'll find that the resulting image looks similar to the original but has lost some of the fine details. But now, the image only occupies a fraction of the space. By controlling how big your mask is, you can now tune precisely how detailed you want your output images to be.</p>
<p>Here is the close-up of the laptop in the home page again. Except now, there is a circular border mask that's been applied.</p>
<p><img src="https://sidbala.com/content/images/2016/11/QuantizationHorizontalWithMasks-1.jpg" alt="" loading="lazy"></p>
<p>The numbers represent the information entropy of that image as a fraction of the original. Even at 2%, you won't notice the difference unless you're at this zoom level. 2%! - your car now weighs 60 lbs!</p>
<p>So that's how you shed weight. This process in lossy compression is called <strong>quantization</strong>[^3].</p>
<blockquote>
<p>Okay. Impressive, I guess. What else you got?</p>
</blockquote>
<h6 id="chromasubsampling">Chroma Subsampling.</h6>
<p>The human/eye brain system is not very good at resolving finer details in color. It can detect minor variations in brightness very easily but not color. So there must be some way to discard color information to shed even more weight.</p>
<p>In a TV signal, R+G+B color data gets transformed to Y+Cb+Cr. The Y is the luminance (essentially black and white brightness) and the Cb and Cr are the chrominance (color) components. RGB and YCbCr are equivalent in terms of information entropy.</p>
<blockquote>
<p>Why unnecessarily complicate? RGB not good enough for you?</p>
</blockquote>
<p>Back before we had color TV, we only had the Y signal. And when color TVs just started coming along, engineers had to figure out a way to transmit RGB color along with Y. Instead of using two separate data streams, they wisely decided to encode the color information into Cb and Cr and transmit that along with the Y information. That way, BW TVs would only look at the Y component. Color TVs will, in addition, look at the chrominance components and convert to RGB internally.</p>
<p>But check out the trick: the Y component gets encoded at full resolution. The C components only at a quarter resolution. Since the eye/brain is terrible at detecting color variations, you can get away with this. By doing this, you reduce total bandwidth by one half, with very little visual difference. Half! Your car now weighs 30 lbs!</p>
<p>This process of discarding some of the color information is called <strong>Chroma Subsampling</strong>[^4]. While not specific to H.264 and has been around for decades itself, it is used almost universally.</p>
<p>Those are the big weight shedders for lossy compression. Our frames are now tiny - since we discarded most of the detail information and half of the color information.</p>
<blockquote>
<p>Wait. That's it? Can we do something more?</p>
</blockquote>
<p>Yes. Weight shedding is only the first step. So far we're only looking at the spatial domains within a single frame. Now it's time to explore temporal compression - where we look at a group of frames across time.</p>
<h6 id="motioncompensation">Motion compensation</h6>
<p>H.264 is a motion compensation compression standard.</p>
<blockquote>
<p>Motion compensation? What now?</p>
</blockquote>
<p>Imagine you're watching a tennis match. The camera is fixed at a certain angle. The only thing moving is the ball back and forth. How would you encode this information? You do what you always do, right? You have a 3D array of pixels, two dimensions in space and one in time. Right?</p>
<p>Nah. Why would you? Most of the image is the same anyway. The court, the net, the crowds, all are static. The only real action is the ball moving. What if you could just have one static image of everything in the background, and then one moving image of just the ball? Wouldn't that save a lot of space? You see where I am going with this? Get it? See where I am going? Motion estimation?</p>
<p>Lame jokes aside, this is exactly what H.264 does. H.264 splits up the image into macro-blocks - typically 16x16 pixel blocks that it will use for motion estimation. It encodes one static image - typically called an <strong>I-frame</strong>(Intra frame). This is a full frame - containing all the bits it required to construct that frame. And then subsequent frames are either <strong>P-frames</strong>(predicted) or <strong>B-frames</strong>(bi-directionally predicted). P-frames are frames that will encode a motion vector for each of the macro-blocks from the previous frame. So a P-frame has to be constructed by the decoder based on previous frames. It starts with the last I-frame in the video stream and then walks through every subsequent frame - adding up the motion vector deltas as it goes along until it arrives at the current frame.</p>
<p>B-frames are even more interesting, where the prediction happens bi-directionally, both from past frames and from future frames. So you can imagine now why that Apple home page video is so well compressed. Because it's really just three I-frames in which the macro blocks are being panned around.</p>
<p>Let's say you've been playing a video on YouTube. You missed the last few seconds of dialog, so you scrub back a few seconds. Have you noticed that it doesn't instantly start playing from that timecode you just selected. It pauses for a few moments and then plays. It's already buffered those frames from the network, since you just played it, so why that pause?</p>
<blockquote>
<p>Yeah that annoys the shit out of me. Why does it do that?</p>
</blockquote>
<p>Because you've asked the decoder to jump to some arbitrary frame, the decoder has to redo all the calculations - starting from the nearest I-frames and adding up the motion vector deltas to the frame you're on - and this is computationally expensive, and hence the brief pause. Hopefully you'll be less annoyed now, knowing it's actually doing hard work and not just sitting around just to annoy you.</p>
<p>Since you're only encoding motion vectors deltas, this technique is extremely space-efficient for any video with motion, at the cost of some computation.</p>
<p>Now we've covered both spatial and temporal compression! So far we have a shitton of space saved in Quantization. Chroma subsampling further halved the space required. On top of that, we have motion compensation that stores only 3 actual frames for the ~300 that we had in that video.</p>
<blockquote>
<p>Looks pretty good to me. Now what?</p>
</blockquote>
<p>Now we wrap up and seal the deal. We use a traditional lossless entropy encoder. Because why not? Let's just slap that on there for good measure.</p>
<h6 id="entropycoder">Entropy Coder</h6>
<p>The I-frames, after the lossy steps, contain redundant information. The motion vectors for each of the macro blocks in the P and B-frames - there are entire groups of them with the same values - since several macro blocks move by the same amount when the image pans in our test video.</p>
<p>An entropy encoder will take care of this redundancy. And since it is a general purpose lossless encoder, we don't have to worry about what tradeoffs it's making. We can recover all the data that goes in.</p>
<p>And, we're done! At the core of it, this is how video compression codecs like H.264 work. These are its tricks.</p>
<blockquote>
<p>Ok great! But I am curious to know how much our car weighs now.</p>
</blockquote>
<p>The original video was captured at an odd resolution of 1232x1154. If we apply the math here, we get:</p>
<p>5 secs @ 60 fps = 1232x1154x60x3x5 =&gt; <strong>1.2 GB</strong><br>
Compressed video =&gt; <strong>175 KB</strong></p>
<p>If we apply the same ratio to our 3000 lb car, we get <strong>0.4 lbs</strong> as the final weight. 6.5 ounces!</p>
<p><strong>Yeah. It's magic!</strong></p>
<p>Obviously, I am massively oversimplifying several decades of intense research in this field. If you want to know more, the <a href="https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC?ref=sidbala.com">Wikipedia Page</a> is pretty descriptive.</p>
<p>Have comments? Did I get something wrong? Not a fan of the lame jokes? Offended by the swearing? Use <a href="https://news.ycombinator.com/item?id=12871403&amp;ref=sidbala.com"><strong>HackerNews</strong></a> or <a href="https://www.reddit.com/r/programming/comments/5b31gt/h264_is_magic/?ref=sidbala.com"><strong>Reddit</strong></a> for voicing your opinion!</p>
<p>Or hit me up on <a href="https://twitter.com/SidBaIa?ref=sidbala.com"><strong>Twitter</strong></a> or <a href="https://www.linkedin.com/in/sidbalasubramanian?ref=sidbala.com"><strong>LinkedIn</strong></a> if you want to chat.</p>
<p>[^1]<a href="http://www.anandtech.com/show/8747/samsung-ssd-850-evo-review/8?ref=sidbala.com">SSD Benchmarks</a></p>
<p>[^2]<a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem?ref=sidbala.com">Nyquist-Shannon Sampling Theorem</a></p>
<p>[^3][Quantization](<a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)?ref=sidbala.com">https://en.wikipedia.org/wiki/Quantization_(signal_processing)</a></p>
<p>[^4]<a href="https://en.wikipedia.org/wiki/Chroma_subsampling?ref=sidbala.com">Chroma Subsampling</a></p>
<!--kg-card-end: markdown-->
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia Warp: A Python framework for high performance GPU simulation and graphics (318 pts)]]></title>
            <link>https://github.com/NVIDIA/warp</link>
            <guid>40680737</guid>
            <pubDate>Fri, 14 Jun 2024 13:28:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/NVIDIA/warp">https://github.com/NVIDIA/warp</a>, See on <a href="https://news.ycombinator.com/item?id=40680737">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://badge.fury.io/py/warp-lang" rel="nofollow"><img src="https://camo.githubusercontent.com/daa164f830a84b04b64d531b12530928d1bc319b743ce30cc41097d00abc1a14/68747470733a2f2f62616467652e667572792e696f2f70792f776172702d6c616e672e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/warp-lang.svg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/255036a020abd59064bace6a6d943d3a1c4766b84f3da2d1fb63974308bf23e1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f4e56494449412f776172703f6c696e6b3d68747470732533412532462532466769746875622e636f6d2532464e564944494125324677617270253246636f6d6d6974732532466d61696e"><img src="https://camo.githubusercontent.com/255036a020abd59064bace6a6d943d3a1c4766b84f3da2d1fb63974308bf23e1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f4e56494449412f776172703f6c696e6b3d68747470732533412532462532466769746875622e636f6d2532464e564944494125324677617270253246636f6d6d6974732532466d61696e" alt="GitHub commit activity" data-canonical-src="https://img.shields.io/github/commit-activity/m/NVIDIA/warp?link=https%3A%2F%2Fgithub.com%2FNVIDIA%2Fwarp%2Fcommits%2Fmain"></a>
<a href="https://pepy.tech/project/warp-lang" rel="nofollow"><img src="https://camo.githubusercontent.com/df255171f344876357ce0b979019310a9d38d57324ac32ceb4483d1f96c99e60/68747470733a2f2f7374617469632e706570792e746563682f62616467652f776172702d6c616e672f6d6f6e7468" alt="Downloads" data-canonical-src="https://static.pepy.tech/badge/warp-lang/month"></a>
<a href="https://codecov.io/github/NVIDIA/warp" rel="nofollow"><img src="https://camo.githubusercontent.com/a435aebeddbc5c3d5708df6ec5c23570c68b8ae53cc751960cd482683af4c192/68747470733a2f2f636f6465636f762e696f2f6769746875622f4e56494449412f776172702f67726170682f62616467652e7376673f746f6b656e3d374f314b534d37394647" alt="codecov" data-canonical-src="https://codecov.io/github/NVIDIA/warp/graph/badge.svg?token=7O1KSM79FG"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/warp/actions/workflows/build-and-test.yml/badge.svg"><img src="https://github.com/NVIDIA/warp/actions/workflows/build-and-test.yml/badge.svg" alt="GitHub - Build and Test"></a>
<a href="https://discord.com/invite/nvidiaomniverse" rel="nofollow"><img src="https://camo.githubusercontent.com/2fe0be53a1df61baaa90fcf17f265be8b485ec681a3f69467304491a223dd4db/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d2532333538363546322e7376673f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord" data-canonical-src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">NVIDIA Warp</h2><a id="user-content-nvidia-warp" aria-label="Permalink: NVIDIA Warp" href="#nvidia-warp"></a></p>
<p dir="auto">Warp is a Python framework for writing high-performance simulation and graphics code. Warp takes
regular Python functions and JIT compiles them to efficient kernel code that can run on the CPU or GPU.</p>
<p dir="auto">Warp is designed for spatial computing and comes with a rich set of primitives that make it easy to write
programs for physics simulation, perception, robotics, and geometry processing. In addition, Warp kernels
are differentiable and can be used as part of machine-learning pipelines with frameworks such as PyTorch and JAX.</p>
<p dir="auto">Please refer to the project <a href="https://nvidia.github.io/warp/" rel="nofollow">Documentation</a> for API and language reference and <a href="https://github.com/NVIDIA/warp/blob/main/CHANGELOG.md">CHANGELOG.md</a> for release history.</p>
<div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/NVIDIA/warp/raw/main/docs/img/header.jpg"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/header.jpg"></a></p><p dir="auto"><i>A selection of physical simulations computed with Warp</i></p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing</h2><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto">Python version 3.9 or newer is recommended. Warp can run on x86-64 and ARMv8 CPUs on Windows, Linux, and macOS.
GPU support requires a CUDA-capable NVIDIA GPU and driver (minimum GeForce GTX 9xx).</p>
<p dir="auto">The easiest way to install Warp is from <a href="https://pypi.org/project/warp-lang/" rel="nofollow">PyPI</a>:</p>

<p dir="auto">You can also use <code>pip install warp-lang[extras]</code> to install additional dependencies for running examples and USD-related features.</p>
<p dir="auto">The binaries hosted on PyPI are currently built with the CUDA 11.8 runtime.
We provide binaries built with the CUDA 12.5 runtime on the <a href="https://github.com/NVIDIA/warp/releases">GitHub Releases</a> page.
Copy the URL of the appropriate wheel file (<code>warp-lang-{ver}+cu12-py3-none-{platform}.whl</code>) and pass it to
the <code>pip install</code> command, e.g.</p>
<div data-snippet-clipboard-copy-content="pip install https://github.com/NVIDIA/warp/releases/download/v1.2.0/warp_lang-1.2.0+cu12-py3-none-manylinux2014_x86_64.whl"><pre><code>pip install https://github.com/NVIDIA/warp/releases/download/v1.2.0/warp_lang-1.2.0+cu12-py3-none-manylinux2014_x86_64.whl
</code></pre></div>
<p dir="auto">The <code>--force-reinstall</code> option may need to be used to overwrite a previous installation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">An example first program that computes the lengths of random 3D vectors is given below:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import warp as wp
import numpy as np

num_points = 1024

@wp.kernel
def length(points: wp.array(dtype=wp.vec3),
           lengths: wp.array(dtype=float)):

    # thread index
    tid = wp.tid()
    
    # compute distance of each point from origin
    lengths[tid] = wp.length(points[tid])


# allocate an array of 3d points
points = wp.array(np.random.rand(num_points, 3), dtype=wp.vec3)
lengths = wp.zeros(num_points, dtype=float)

# launch kernel
wp.launch(kernel=length,
          dim=len(points),
          inputs=[points, lengths])

print(lengths)"><pre><span>import</span> <span>warp</span> <span>as</span> <span>wp</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>num_points</span> <span>=</span> <span>1024</span>

<span>@<span>wp</span>.<span>kernel</span></span>
<span>def</span> <span>length</span>(<span>points</span>: <span>wp</span>.<span>array</span>(<span>dtype</span><span>=</span><span>wp</span>.<span>vec3</span>),
           <span>lengths</span>: <span>wp</span>.<span>array</span>(<span>dtype</span><span>=</span><span>float</span>)):

    <span># thread index</span>
    <span>tid</span> <span>=</span> <span>wp</span>.<span>tid</span>()
    
    <span># compute distance of each point from origin</span>
    <span>lengths</span>[<span>tid</span>] <span>=</span> <span>wp</span>.<span>length</span>(<span>points</span>[<span>tid</span>])


<span># allocate an array of 3d points</span>
<span>points</span> <span>=</span> <span>wp</span>.<span>array</span>(<span>np</span>.<span>random</span>.<span>rand</span>(<span>num_points</span>, <span>3</span>), <span>dtype</span><span>=</span><span>wp</span>.<span>vec3</span>)
<span>lengths</span> <span>=</span> <span>wp</span>.<span>zeros</span>(<span>num_points</span>, <span>dtype</span><span>=</span><span>float</span>)

<span># launch kernel</span>
<span>wp</span>.<span>launch</span>(<span>kernel</span><span>=</span><span>length</span>,
          <span>dim</span><span>=</span><span>len</span>(<span>points</span>),
          <span>inputs</span><span>=</span>[<span>points</span>, <span>lengths</span>])

<span>print</span>(<span>lengths</span>)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running Examples</h2><a id="user-content-running-examples" aria-label="Permalink: Running Examples" href="#running-examples"></a></p>
<p dir="auto">The <code>examples</code> directory contains a number of scripts that show how to implement different simulation methods using the Warp API. Most examples will generate USD files containing time-sampled animations (stored in the current working directory). Before running examples, users should ensure that the <code>usd-core</code>, <code>matplotlib</code>, and <code>pyglet</code> packages are installed using:</p>
<div data-snippet-clipboard-copy-content="pip install usd-core matplotlib pyglet"><pre><code>pip install usd-core matplotlib pyglet
</code></pre></div>
<p dir="auto">Examples can be run from the command-line as follows:</p>
<div data-snippet-clipboard-copy-content="python -m warp.examples.<example_subdir>.<example>"><pre><code>python -m warp.examples.&lt;example_subdir&gt;.&lt;example&gt;
</code></pre></div>
<p dir="auto">To browse the example source code, you can open the directory where the files are located like this:</p>
<div data-snippet-clipboard-copy-content="python -m warp.examples.browse"><pre><code>python -m warp.examples.browse
</code></pre></div>
<p dir="auto">Most examples can be run on either the CPU or a CUDA-capable device, but a handful require a CUDA-capable device. These are marked at the top of the example script.</p>
<p dir="auto">USD files can be viewed or rendered inside <a href="https://developer.nvidia.com/omniverse" rel="nofollow">NVIDIA Omniverse</a>, Pixar's UsdView, and Blender. Note that Preview in macOS is not recommended as it has limited support for time-sampled animations.</p>
<p dir="auto">Built-in unit tests can be run from the command-line as follows:</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">examples/core</h3><a id="user-content-examplescore" aria-label="Permalink: examples/core" href="#examplescore"></a></p>
<table>
    <tbody>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_dem.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_dem.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_fluid.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_fluid.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_graph_capture.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_graph_capture.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_marching_cubes.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_marching_cubes.png"></a></td>
        </tr>
        <tr>
            <td>dem</td>
            <td>fluid</td>
            <td>graph capture</td>
            <td>marching cubes</td>
        </tr>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_mesh.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_mesh.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_nvdb.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_nvdb.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_raycast.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_raycast.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_raymarch.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_raymarch.png"></a></td>
        </tr>
        <tr>
            <td>mesh</td>
            <td>nvdb</td>
            <td>raycast</td>
            <td>raymarch</td>
        </tr>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_sph.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_sph.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_torch.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_torch.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_wave.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/core_wave.png"></a></td>
            <td></td>
        </tr>
        <tr>
            <td>sph</td>
            <td>torch</td>
            <td>wave</td>
            <td></td>
        </tr>
    </tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto">examples/fem</h3><a id="user-content-examplesfem" aria-label="Permalink: examples/fem" href="#examplesfem"></a></p>
<table>
    <tbody>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_apic_fluid.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/fem_apic_fluid.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_convection_diffusion.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/fem_convection_diffusion.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_diffusion_3d.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/fem_diffusion_3d.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_diffusion.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/fem_diffusion.png"></a></td>
        </tr>
        <tr>
            <td>apic fluid</td>
            <td>convection diffusion</td>
            <td>diffusion 3d</td>
            <td>diffusion</td>
        </tr>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_mixed_elasticity.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/fem_mixed_elasticity.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_navier_stokes.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/fem_navier_stokes.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_stokes_transfer.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/fem_stokes_transfer.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_stokes.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/fem_stokes.png"></a></td>
        </tr>
        <tr>
            <td>mixed elasticity</td>
            <td>navier stokes</td>
            <td>stokes transfer</td>
            <td>stokes</td>
        </tr>
    </tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto">examples/optim</h3><a id="user-content-examplesoptim" aria-label="Permalink: examples/optim" href="#examplesoptim"></a></p>
<table>
    <tbody>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_bounce.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/optim_bounce.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_cloth_throw.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/optim_cloth_throw.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_diffray.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/optim_diffray.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_drone.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/optim_drone.png"></a></td>
        </tr>
        <tr>
            <td>bounce</td>
            <td>cloth throw</td>
            <td>diffray</td>
            <td>drone</td>
        </tr>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_inverse_kinematics.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/optim_inverse_kinematics.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_spring_cage.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/optim_spring_cage.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_trajectory.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/optim_trajectory.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_walker.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/optim_walker.png"></a></td>
        </tr>
        <tr>
            <td>inverse kinematics</td>
            <td>spring cage</td>
            <td>trajectory</td>
            <td>walker</td>
        </tr>
    </tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto">examples/sim</h3><a id="user-content-examplessim" aria-label="Permalink: examples/sim" href="#examplessim"></a></p>
<table>
    <tbody>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_cartpole.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_cartpole.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_cloth.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_cloth.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_granular.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_granular.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_granular_collision_sdf.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_granular_collision_sdf.png"></a></td>
        </tr>
        <tr>
            <td>cartpole</td>
            <td>cloth</td>
            <td>granular</td>
            <td>granular collision sdf</td>
        </tr>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_jacobian_ik.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_jacobian_ik.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_quadruped.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_quadruped.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_rigid_chain.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_rigid_chain.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_rigid_contact.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_rigid_contact.png"></a></td>
        </tr>
        <tr>
            <td>jacobian ik</td>
            <td>quadruped</td>
            <td>rigid chain</td>
            <td>rigid contact</td>
        </tr>
        <tr>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_rigid_force.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_rigid_force.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_rigid_gyroscopic.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_rigid_gyroscopic.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_rigid_soft_contact.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_rigid_soft_contact.png"></a></td>
            <td><a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/sim/example_soft_body.py"><img src="https://github.com/NVIDIA/warp/raw/main/docs/img/examples/sim_soft_body.png"></a></td>
        </tr>
        <tr>
            <td>rigid force</td>
            <td>rigid gyroscopic</td>
            <td>rigid soft contact</td>
            <td>soft body</td>
        </tr>
    </tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">For developers who want to build the library themselves, the following tools are required:</p>
<ul dir="auto">
<li>Microsoft Visual Studio 2019 upwards (Windows)</li>
<li>GCC 9.4 upwards (Linux)</li>
<li>CUDA Toolkit 11.5 or higher</li>
<li><a href="https://git-lfs.github.com/">Git LFS</a> installed</li>
</ul>
<p dir="auto">After cloning the repository, users should run:</p>

<p dir="auto">This will generate the <code>warp.dll</code> / <code>warp.so</code> core library respectively. It will search for the CUDA Toolkit in the default install directory. This path can be overridden by setting the <code>CUDA_PATH</code> environment variable. Alternatively, the path to the CUDA Toolkit can be passed to the build command as <code>--cuda_path="..."</code>. After building, the Warp package should be installed using:</p>

<p dir="auto">This ensures that subsequent modifications to the library will be reflected in the Python package.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Learn More</h2><a id="user-content-learn-more" aria-label="Permalink: Learn More" href="#learn-more"></a></p>
<p dir="auto">Please see the following resources for additional background on Warp:</p>
<ul dir="auto">
<li><a href="https://developer.nvidia.com/warp-python" rel="nofollow">Product Page</a></li>
<li><a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41599" rel="nofollow">GTC 2022 Presentation</a></li>
<li><a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31838" rel="nofollow">GTC 2021 Presentation</a></li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3476117.3483433" rel="nofollow">SIGGRAPH Asia 2021 Differentiable Simulation Course</a></li>
<li><a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s63345/" rel="nofollow">GTC 2024 Presentation</a></li>
</ul>
<p dir="auto">The underlying technology in Warp has been used in a number of research projects at NVIDIA including the following publications:</p>
<ul dir="auto">
<li>Accelerated Policy Learning with Parallel Differentiable Simulation - Xu, J., Makoviychuk, V., Narang, Y., Ramos, F., Matusik, W., Garg, A., &amp; Macklin, M. <a href="https://short-horizon-actor-critic.github.io/" rel="nofollow">(2022)</a></li>
<li>DiSECt: Differentiable Simulator for Robotic Cutting - Heiden, E., Macklin, M., Narang, Y., Fox, D., Garg, A., &amp; Ramos, F <a href="https://github.com/NVlabs/DiSECt">(2021)</a></li>
<li>gradSim: Differentiable Simulation for System Identification and Visuomotor Control - Murthy, J. Krishna, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine et al. <a href="https://gradsim.github.io/" rel="nofollow">(2021)</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Frequently Asked Questions</h2><a id="user-content-frequently-asked-questions" aria-label="Permalink: Frequently Asked Questions" href="#frequently-asked-questions"></a></p>
<p dir="auto">See the <a href="https://nvidia.github.io/warp/faq.html" rel="nofollow">FAQ</a> in the Warp documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Problems, questions, and feature requests can be opened on <a href="https://github.com/NVIDIA/warp/issues">GitHub Issues</a>.</p>
<p dir="auto">The Warp team also monitors the <strong>#warp</strong> channel on the public <a href="https://discord.com/invite/nvidiaomniverse" rel="nofollow">Omniverse Discord</a> server, come chat to us!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Versioning</h2><a id="user-content-versioning" aria-label="Permalink: Versioning" href="#versioning"></a></p>
<p dir="auto">Versions take the format X.Y.Z, similar to <a href="https://devguide.python.org/developer-workflow/development-cycle/#devcycle" rel="nofollow">Python itself</a>:</p>
<ul dir="auto">
<li>Increments in X are reserved for major reworks of the project causing disruptive incompatibility (or reaching the 1.0 milestone).</li>
<li>Increments in Y are for regular releases with a new set of features.</li>
<li>Increments in Z are for bug fixes. In principle there are no new features. Can be omitted if 0 or not relevant.</li>
</ul>
<p dir="auto">This is similar to <a href="https://semver.org/" rel="nofollow">Semantic Versioning</a> but less strict around backward compatibility.
Like with Python, some breaking changes can be present between minor versions if well documented and gradually introduced.</p>
<p dir="auto">Note that prior to 0.11.0 this schema was not strictly adhered to.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Warp is provided under the NVIDIA Software License, please see <a href="https://github.com/NVIDIA/warp/blob/main/LICENSE.md">LICENSE.md</a> for full license text.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions and pull requests from the community are welcome and are taken under the
terms described in the <strong>9. Feedback</strong> section of the <a href="https://github.com/NVIDIA/warp/blob/main/LICENSE.md">license</a>.
<a href="https://github.com/NVIDIA/warp/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> provides additional information on how to open a pull request for Warp.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citing</h2><a id="user-content-citing" aria-label="Permalink: Citing" href="#citing"></a></p>
<p dir="auto">If you use Warp in your research please use the following citation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{warp2022,
title= {Warp: A High-performance Python Framework for GPU Simulation and Graphics},
author = {Miles Macklin},
month = {March},
year = {2022},
note= {NVIDIA GPU Technology Conference (GTC)},
howpublished = {\url{https://github.com/nvidia/warp}}
}"><pre><span>@misc</span>{<span>warp2022</span>,
<span>title</span>= <span><span>{</span>Warp: A High-performance Python Framework for GPU Simulation and Graphics<span>}</span></span>,
<span>author</span> = <span><span>{</span>Miles Macklin<span>}</span></span>,
<span>month</span> = <span><span>{</span>March<span>}</span></span>,
<span>year</span> = <span><span>{</span>2022<span>}</span></span>,
<span>note</span>= <span><span>{</span>NVIDIA GPU Technology Conference (GTC)<span>}</span></span>,
<span>howpublished</span> = <span><span>{</span>\url{https://github.com/nvidia/warp}<span>}</span></span>
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Start Presentations on the Second Slide (303 pts)]]></title>
            <link>https://tidyfirst.substack.com/p/start-presentations-on-the-second</link>
            <guid>40680648</guid>
            <pubDate>Fri, 14 Jun 2024 13:17:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tidyfirst.substack.com/p/start-presentations-on-the-second">https://tidyfirst.substack.com/p/start-presentations-on-the-second</a>, See on <a href="https://news.ycombinator.com/item?id=40680648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><blockquote><p>First published March 2013. It’s the single most effective technique I know to grab an audience’s attention. Also it’s easy to execute—write what you want to write, them switch the first two slides/paragraphs/chapters.</p></blockquote><p>[Edit: Jinghao Yan pointed out my hypocrisy. Fixed.]</p><div><p><span>Technical presos need background but background's not engaging. What's a geeky presenter to do?</span></p><p><span>I've been coaching technical presenters lately, and a couple of concepts come up with almost all of them. I figured I'd write them down so I don't necessarily have to explain them all the time. One is to use specifics and data. I'll write that later. This post explains why to start your presentation on the second slide.</span><br><span> </span><br><span>I stole this technique from Lawrence Block's outstanding Telling Lies for Fun and Profit, a book about writing fiction. He suggests drafting a story the "natural" way, with the first chapter introducing the hero and the second getting the action going, then swapping the two chapters. Now the first chapter starts with a gun pointed at the hero's head. By the end, he is teetering on a cliff about to jump into a crocodile-infested river. Just when the tension reaches a peak, we're introduced to the character but we have reason to want to get to know him.</span><br><span> </span><br><span>Technical presentations need to set some context and then present the problem to be solved. When presenters follow this order, though, the resulting presentation starts with information some listeners already know and other listeners don't have any motivation to try to understand. It's like our adventure story where we're not interested in the color of the hero's hair, at least not until he's about to become a croc-snack.</span><br><span> </span><br><span>For example, consider a presentation on optimizing a virtual machine based on just-in-time compilation (hi @kma). The background information introduces JITing, the basics of performance tuning (Pareto), and the architecture of current machines. Shockingly, applying this information in the usual way, by squeezing hot spots, often results in slower overall performance, while seemingly-benign changes can offer significant improvement.</span><br><span> </span><br><span>Applying the chapter swap technique, slide 1 would show a performance profile with a hotspot, a code change that would seem to offer improvement, and data demonstrating that not only didn't the optimization work, it made the system slower. Now we can have slides about jitting, optimization, and architecture. Listeners who already know the background are willing to sit through it to get to the solution to the mystery. Listeners who haven't covered the background are intrigued enough to concentrate.</span><br><span> </span><br><span>Programmers have a pavlovian engineering response. Pose them a problem and they'll start trying to solve it. Give them a chance to co-engineer along with your presentation by making sure the first bite gets their saliva flowing. Then you can explain the rest of the problem and your brilliant solution knowing that they are there along with you.</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The problem with OpenTelemetry (139 pts)]]></title>
            <link>https://cra.mr/the-problem-with-otel/</link>
            <guid>40680291</guid>
            <pubDate>Fri, 14 Jun 2024 12:36:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cra.mr/the-problem-with-otel/">https://cra.mr/the-problem-with-otel/</a>, See on <a href="https://news.ycombinator.com/item?id=40680291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <p><strong>Edit:</strong> tl;dr <a href="https://docs.rs/tracing/latest/tracing/">tracing</a> should exist in every ecosystem, and be broken out of OpenTelemetry.</p>
<p>I regularly complain about OpenTelemetry, so with an aim to be a less useless contributor, today I’m putting pen to paper. If you’re an implementor, I ask you to read this and take away the personal bias you might have towards your work, and instead look objectively at the feedback being given.</p>
<p>First, some context, if you’re clueless how you ended up here, I started Sentry. Sentry, for obvious reasons, has a stake in this “instrument your application” race. That said, with everything I’ve seen, I cannot say OpenTelemetry is the horse I want to put my money on. That’s where this conversation starts.</p>
<p>In 2015 <a href="https://twitter.com/mitsuhiko">Armin</a> and I built a spec for Distributed Tracing. Its not a hard problem, it just requires an immense amount of coordination and effort. At its core its structured events that carry two GUIDs along with them: a trace ID and a parent event ID. It is just building a tree. The harder part is stabilizing those annotations across an infinitely growing ecosystem of applications, libraries, and various services. Around the same time we were working on our spec, the folks at Lightstep were kicking things into gear. They built an open specification called OpenTracing. The goal, as far as I understand it, was to create a standard approach to instrumentation allowing vendors and customers to avoid rewriting instrumentation.</p>
<p>The goal makes sense to both versions of myself: a leader at Sentry and a developer of software.</p>
<p>Along the way OpenTracing became OpenTelemetry, merging with OpenCensus. I dont have a lot of interest in how that decision was made, or what the hypothetical goals were, but I care about the outcomes of the original goal of an open tracing standard. You see, sometime along the way, OpenTelemetry became a much wider reaching specification beyond the tracing goals. It tries to build specifications and SDKs for not only span annotations, but universal transports for logs and metrics, and who knows what else in the future. It is my personal opinion - not that of Sentry’s - that it has lost its way, and is a classic example of the failure of design by committee. Its an example of a lack of vision, and a lack of leadership.</p>
<p>Now I say that not knowing the people involved in the project, so don’t consider this an attack on the quality of their work, their desire to help the ecosystem, or anything of that matter. The way I look at it is its a bunch of individuals and corporations trying to service their own goals, with some degree of overlap. Many of them are vendors doing whatever they can to achieve PmF, begging the OpenTelemetry gods to feed them more data and customers. A lot of others are the outcasts of the Datadogs, looking to bring their instrumentation to another partner who doesn’t want to middle finger them every quarter when the earnings calls come in.</p>
<p>The problem I see it is that OpenTelemetry doesn’t have an end state that we’d all agree upon, and I personally do not believe strategic alignment nor vision are possible without being able to articulate an end state. When OpenTracing started I could <em>envision</em> what that might have been. I could see the problem that was trying to be solved. Now all I see is a standards committee, for something thats not remotely a standard, and I struggle to ever see becoming one. I stated I wanted this to be more than complaining, so I’m going to articulate a problem, a solution, and why OpenTelemetry is failing to live up to the goal I’ve stated above (which may not be their goal, but certainly reflects what our customers want).</p>
<p>Back in the early 2000s if you wanted to reasonably debug performance issues in production systems you were left with two big tasks:</p>
<ol>
<li>Identifying a vendor or building a centralized APM-like solution</li>
<li>Annotating your code all over the place with span-esque data structures</li>
</ol>
<p>The problem fundamentally came down to the annotations. A lot of them were actually specific to vendor SDKs. None of that instrumentation was portable, well at least not at face value. Realistically a bunch of codemods would easily let you swap out a vendor specific set of annotations with another vendor’s, so it wasn’t as bad as people make it out to be. Fast forward however, and technology has gotten far more complicated. Everyone and their mother is running a shoddy microservice-coupled stack, creating a lot more of a challenge in debugging these production systems. Even outside of performance concerns, something as simple as a stacktrace for an error is often not enough anymore. So we needed something better. We needed the tried and true tracing techniques to become readily available to you and I.</p>
<p>Now from a vendor point of view, nothing really changed. Vendors are still responsible for ensuring that your application is well instrumented. Thats not <em>your</em> problem, at least not entirely. So vendors got together and said, you know what’d be great? We build this set of standards, rope in third parties to implement them, and solve the instrumentation angle once and for all. That’s a great goal, but practically speaking no one has really committed to that. Authors of libraries rarely instrument their code, and the upstream instrumentation (often via monkey patching) looks more or less the same as it has for 20 years.</p>
<p>On top of that, one of the goals is to make it so you, as a developer, have portable instrumentation that goes between vendors. I’m all for this goal, but if you’ve looked at the APIs and specs made available via OpenTelemetry its fairly obvious why adoption is struggling. There are <em>so many concepts</em> you have to master, and even as an experienced developer you’re going to rightly, and quickly, question why some of them are relevant to you. You as a customer almost certainly should only <em>ever</em> need to care about span instrumentation, and in some cases, forwarding baggage (particularly forwarding the trace ID). Everything else is a vendors problem, but the spec is plagued with these concepts that <em>must</em> be considered by <em>anyone</em> implementing span annotations, which is not the vendor.</p>
<p>To take it even further, OpenTelemetry is so far beyond tracing, even though it has yet to achieve traction within that original scope. Its trying to create standards for logging and metrics, neither of which exist in the context of many systems. Logs are just events - which is exactly what a span is, btw - and metrics are just abstractions out of those event properties. That is, you want to know the response time of an API endpoint? You dont rewind 20 years and increment a counter, you instead aggregate the duration of the relevant span segment. Somehow though, Logs and Metrics are still front and center. Feels like BigMonitoring trying to keep relevant personally, but I’m not going to digress on this topic much. What really matters here is one spec has amounted to many specs, so even saying “OpenTelemetry” has no real meaning.</p>
<p>Let’s talk about practicalities. Sentry wants you to have portable instrumentation. Sentry also wants you to have <em>the best data quality possible</em>. The former is pretty easy frankly, so I’m not going to focus on it, but third party instrumentation is where many systems break down. OpenTelemetry, specifically its tracing abstractions, has actually done a really good job in some ecosystems of providing baked-in instrumentation for libraries. Node.js is the one we’re going to focus on today, but its worth noting that while some ecosystems have good implementations of OpenTelemetry, its largely non-existant or sees little adoption.</p>
<p>So going back to our goal of great data quality, Sentry made the decision that we should be <em>compatible</em> with OpenTelemetry. That is, we want to support the effort of great third-party instrumentation, but thats not enough for us. We think there’s inherent value in making <em>everything</em> trace-connected. Yes, spans are great, but you know what else is great? Crash reports, Session replays, and a bunch of other kinds of telemetry that <em>we’ve all yet to realize matters</em>. Those are not spans, and they certainly arent logs or metrics. Our goal is to make that possible, but to do that, we need two things:</p>
<ol>
<li>Trace propagation</li>
<li>Span instrumentation (which is key to propagation)</li>
</ol>
<p>For a couple years now we’ve been experimenting with how to achieve this, how to support OpenTelemetry users, but without becoming yet another generic vendor who just accepts a log drain of junk data. That’s simply not what we want to be, and its not what we hear our 100,000+ customers needing. So we ended up with our current generation JavaScript SDK, which piggybacks on top of OpenTelemetry, ideally giving you the best of both worlds.</p>
<p>What I mean by that is, as a customer, you should be able to do this:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>import</span><span> { startSpan } </span><span>from</span><span> "</span><span>@opentelemetry/sdk</span><span>"</span><span>;</span></span>
<span></span>
<span><span>function</span><span> shittyChatBot</span><span>() {</span></span>
<span><span>    startSpan</span><span>(</span><span>"</span><span>some.operation</span><span>"</span><span>, (</span><span>span</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>        doTheThing</span><span>();</span></span>
<span><span>    });</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<p>That is, you should be able to use a shared specification to instrument your code for portability, and third party vendors should be able to do the same. We’re in favor of that future.  Unfortunately it has not been going super well, and that is why I’m constantly complaining. I hear the complaints of our customers, I see an outcome they want, and I’m struggling to help them get to it.</p>
<p>We as a vendor have what I would simply describe as courage: we have no fear of solving whatever problem gets in our way. We don’t expect others to do our work for us, we control the ball. That means we’re willing to re-invent all instrumentation if thats what it takes, but we’d prefer not to, and we still want customers to have portability in instrumentation.</p>
<p>In particular, we have an extremely well adopted and powerful SDK, and we want our customers to be able to get both the advantages of our SDK - remember we want everything trace connected, not just whatevers in this design committee - but also not be prevented from adopting OpenTelemetry. That means what we actually want is a way to say “hey OpenTelemetry SDK, give us all the current spans in the buffer”. What we don’t want is to be forced to implement some hopeful-standardized transport protocol that may or may not solve our concerns, let alone one that requires us to adopt legacy infrastructure telemetry like logs and metrics.</p>
<p>So to do that we piggyback on the SDK, trying to use their context resolvers, baggage handlers, and most importantly, the span instrumentation code.</p>
<p>Except it doesn’t work for customers. It doesn’t work for the same reasons a lot of other things don’t work: version conflicts, specification incompatibilities, and generally speaking, code that tries to do too much. This headache also comes in regular conversation. “We’re supporting OpenTelemetry” is immediately confusing as we dont do logs, or traceless-metrics.</p>
<p><strong>Alright, enough complaining, here’s what I’d like to see.</strong></p>
<p>First, we have a tracing-focused SDK that is as lightweight as possible. That means it entirely focuses on giving customers and library authors the ability to instrument their code with span annotations. Not metrics. Not logs. Let that be a different spec’s problem. That API probably continues to look similar to today:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>import</span><span> { startSpan } </span><span>from</span><span> "</span><span>tracing</span><span>"</span><span>;</span></span>
<span></span>
<span><span>function</span><span> shittyChatBot</span><span>() {</span></span>
<span><span>    startSpan</span><span>(</span><span>"</span><span>some.operation</span><span>"</span><span>, (</span><span>span</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>        doTheThing</span><span>();</span></span>
<span><span>    });</span></span>
<span><span>}</span></span>
<span></span></code></pre>
<p>Second, that code, as much as possible, does not add to your runtime performance or bundle size <em>unless</em> you’ve opted in to a method of collection. That collection should not be a wire protocol, but an application interface. For example, you should still be able to wire up an OpenTelemetry collector as you do today:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>import</span><span> { newDomainSpecificCollector } </span><span>from</span><span> "</span><span>@opentelemtry/api</span><span>"</span><span>;</span></span>
<span></span>
<span><span>newDomainSpecificCollector</span><span>({</span></span>
<span><span>    endpoint</span><span>:</span><span> "</span><span>http://not.sentry/otel-ingest</span><span>"</span><span>;</span></span>
<span><span>});</span></span>
<span></span></code></pre>
<p>Lastly, all those other non-tracing concerns? Put them whenever you want, but just like the transport protocols, take them out of the core.</p>
<p>If this were to happen, Sentry could then <em>completely ignore</em> the OpenTelemetry wire protocols, collectors, and everything else that frankly doesn’t matter to our customers or our product. That means something would have to be exposed to handle it:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>import</span><span> { onSpanFinish } </span><span>from</span><span> "</span><span>tracing</span><span>"</span><span>;</span></span>
<span><span>import</span><span> { collectSpan } </span><span>from</span><span> "</span><span>@sentry/node</span><span>"</span><span>;</span></span>
<span></span>
<span><span>onSpanFinish.</span><span>addEventListener</span><span>(collectSpan);</span></span>
<span></span></code></pre>
<p>Thats it. Thats my proposal. I’m sure I’m glossing over a bunch of things, but I’m not trying for academic correctness. Consider this feedback to the “OpenTelemetry Head of Product”, from a customer who would love to use and recommend your product, but cannot.</p>
<p>The world I’m describing is a world Sentry would get behind both as an implementor, and as a financier. I’d love to go to our board and tell them we need to alocate $10m in funding to support library authors implementing and maintaining a true standard, one adopted across the board, but today we cannot make that bet. So instead we are continuing to hedge our bet on us having to maintain instrumentation, and the OpenTelemetry committee somehow becoming successful. Maybe thats fine, but it mostly seems like a big distraction that doesnt help our customers.</p>
<p>I want to close with one last point: its ok that people have problems they want to solve, and its ok that they work as a group to solve them, but you’re not going to see adoption of a product if its solving problems that someone doesn’t have. Sentry does not have the problem of “extracting metrics from AWS CloudWatch”, and I would encourage implementors to stop conflating concerns that are not tightly coupled to the problem being addressed.</p>
<p>There are a lot of things OpenTelemetry tries to do, some it does well, but in general is plagued by too many opinions, and too many goals. Maybe bring back OpenTracing?</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Found a 55 Year Old Bug in the First Lunar Lander Game (323 pts)]]></title>
            <link>https://martincmartin.com/2024/06/14/how-i-found-a-55-year-old-bug-in-the-first-lunar-lander-game/</link>
            <guid>40680218</guid>
            <pubDate>Fri, 14 Jun 2024 12:24:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martincmartin.com/2024/06/14/how-i-found-a-55-year-old-bug-in-the-first-lunar-lander-game/">https://martincmartin.com/2024/06/14/how-i-found-a-55-year-old-bug-in-the-first-lunar-lander-game/</a>, See on <a href="https://news.ycombinator.com/item?id=40680218">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<p>Just months after Neil Armstrong’s historic moonwalk, <a href="https://www.cs.brandeis.edu/~storer/">Jim Storer</a>, a Lexington High School student in Massachusetts, wrote the first <a href="https://www.cs.brandeis.edu/~storer/LunarLander/LunarLander.html">Lunar Landing</a> game. By 1973, it <a href="http://www.bitsavers.org/pdf/dec/_Books/101_BASIC_Computer_Games_Mar75.pdf">had become</a> “by far and away the single most popular computer game.” A simple text game, you pilot a moon lander, aiming for a gentle touch down on the moon. All motion is vertical and you decide every 10 simulated seconds how much fuel to burn.</p>



<p>I recently explored the optimal fuel burn schedule to land as gently as possible and with maximum remaining fuel. Surprisingly, the theoretical best strategy didn’t work. The game falsely thinks the lander doesn’t touch down on the surface when in fact it does. Digging in, I was amazed by the sophisticated physics and numerical computing in the game. Eventually I found a bug: a missing “divide by two” that had seemingly gone unnoticed for nearly 55 years.</p>



<h2>Landing with Maximum Fuel</h2>



<p>To use the least fuel while landing, you need to land in the shortest time possible. Initially you maximize your speed by keeping your engine off, then at the last possible second you burn full throttle, reducing your speed to zero just as you touch the surface. The Kerbal Space Program community calls this a “suicide burn”, since getting the timing right is hard and it leaves no room for error.</p>



<p>With some trial and error and a bit of (manual) binary search, you can find the schedule that just barely has you landing.  You burn nothing for 70 seconds, then 164.31426784 lbs/sec for the next 10 seconds, then the max 200 lbs/sec after that:</p>



<figure><img data-attachment-id="656" data-permalink="https://martincmartin.com/2024/06/14/how-i-found-a-55-year-old-bug-in-the-first-lunar-lander-game/lunarlanderoriginallands/" data-orig-file="https://martincmartin.com/wp-content/uploads/2024/06/lunarlanderoriginallands.png" data-orig-size="1120,694" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="LunarLanderOriginalLands" data-image-description="" data-image-caption="" data-medium-file="https://martincmartin.com/wp-content/uploads/2024/06/lunarlanderoriginallands.png?w=300" data-large-file="https://martincmartin.com/wp-content/uploads/2024/06/lunarlanderoriginallands.png?w=640" width="1120" height="694" src="https://martincmartin.com/wp-content/uploads/2024/06/lunarlanderoriginallands.png" alt=""></figure>



<p>The game considers a perfect landing to be less than 1 MPH, but here we land at over 3.5 MPH and are told we “could be better.”  Yet burn even 0.00000001 more lbs/sec, and you miss the surface entirely, ascending at 114 MPH:</p>



<figure><img data-attachment-id="657" data-permalink="https://martincmartin.com/2024/06/14/how-i-found-a-55-year-old-bug-in-the-first-lunar-lander-game/screenshot-2024-06-13-at-1-22-51-pm/" data-orig-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-13-at-1.22.51e280afpm.png" data-orig-size="1112,586" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-06-13 at 1.22.51 PM" data-image-description="" data-image-caption="" data-medium-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-13-at-1.22.51e280afpm.png?w=300" data-large-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-13-at-1.22.51e280afpm.png?w=640" width="1112" height="586" src="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-13-at-1.22.51e280afpm.png" alt=""></figure>



<p>How did we go from a hard landing to not landing at all, without a soft landing in between?</p>



<h2>Physics Simulation: One Smart Kid</h2>



<p>I expected to see the simple Euler integration that’s common in video games even today.  That’s where you compute the forces at the start of each time step, then use <em>F=ma</em> to compute the acceleration, then assume the acceleration is constant over a time step of <img src="https://s0.wp.com/latex.php?latex=%5CDelta+t&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CDelta+t&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CDelta+t&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\Delta t"> seconds:</p>


<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta+v+%3D+a+%5CDelta+t&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CDelta+v+%3D+a+%5CDelta+t&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CDelta+v+%3D+a+%5CDelta+t&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\Delta v = a \Delta t"><br><img src="https://s0.wp.com/latex.php?latex=%5CDelta+x+%3D+v+%5CDelta+t+%2B+%5Cfrac%7B1%7D%7B2%7D+a+%28%5CDelta+t%29%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CDelta+x+%3D+v+%5CDelta+t+%2B+%5Cfrac%7B1%7D%7B2%7D+a+%28%5CDelta+t%29%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CDelta+x+%3D+v+%5CDelta+t+%2B+%5Cfrac%7B1%7D%7B2%7D+a+%28%5CDelta+t%29%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\Delta x = v \Delta t + \frac{1}{2} a (\Delta t)^2"></p>



<p>Because the mass is changing over the timestep, the acceleration will change too, so assuming it’s constant is only approximate.  But surprisingly, Jim used the exact solution, the <a href="https://en.wikipedia.org/wiki/Tsiolkovsky_rocket_equation">Tsiolkovsky rocket equation</a>, with a Taylor series expansion for the logarithm. He also used some algebra to simplify it and reduce the amount of round off error.  Very impressive for a high school senior in 1969.  When I asked him about it:</p>



<blockquote>
<p>“I was skilled at calculus at the time and familiar with concepts like a Taylor series, but also my recollection is that my father, who was a physicist, helped me in the derivation of the equations.” – Jim Storer, personal communication</p>
</blockquote>



<p>The rocket equation is what gives rise to the suicide burn being optimal, and the five terms he uses of the Taylor series, where the argument is at most 0.1212, makes it accurate to over six decimal places. So that’s not the problem we’re looking for.</p>



<h2>Assumptions Go Out The Window When We Hit The Ground</h2>



<p>The rocket equation works well until you hit the ground. In general, collisions between solid objects is a really hard part of dynamics engines, and what separates the great ones from the good ones, as I discovered when contributing to <a href="https://www.ode.org/">Open Dynamics Engine</a> as a postdoc at MIT.</p>



<p>And this is where the Lunar Landing Game faces its biggest challenge. Imagine the lander descending at the start of a 10-second turn but ascending by the end. Simply verifying that it’s above the surface at both points isn’t enough. It might have dipped below the surface somewhere in between. When this happens, the program has to rewind and examine an earlier moment.</p>



<p>An obvious place is to look at the lowest point of the trajectory, where the velocity is zero. For the rocket equation, it turns out, there’s no closed form expression for that lowest point that involves only basic mathematical functions.<sup data-fn="636f2ae6-d817-49bc-a1ca-6c07b4cee9c6"><a href="#636f2ae6-d817-49bc-a1ca-6c07b4cee9c6" id="636f2ae6-d817-49bc-a1ca-6c07b4cee9c6-link">1</a></sup> So instead, we can use the physicists favourite technique, and take only the first few terms of the Taylor series. If you use only the first two terms of the logarithm, the problem simplifies to a quadratic equation and you can use the good old <a href="https://en.wikipedia.org/wiki/Quadratic_formula">quadratic formula</a> from high school. And the approximation should be pretty good over the space of a 10 second turn, accurate to within 0.1% or so.</p>



<p>But that’s not what Jim did. His formula has a square root in the denominator, not the numerator. It also had an error 30 times bigger.</p>



<h2>How To Know When You’ve Hit Rock Bottom</h2>



<figure><img data-attachment-id="626" data-permalink="https://martincmartin.com/?attachment_id=626" data-orig-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-07-at-10.12.51e280afam.png" data-orig-size="1016,986" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2024-06-07-at-10.12.51e280afam" data-image-description="" data-image-caption="" data-medium-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-07-at-10.12.51e280afam.png?w=300" data-large-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-07-at-10.12.51e280afam.png?w=640" width="1016" height="986" src="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-07-at-10.12.51e280afam.png" alt=""></figure>



<p>What could he possibly be doing? I stared at this for a long time, wracking my brain for any other approach to approximate the bottom of the trajectory that would still only use addition, subtraction, multiplication, division and square root. Taking only the first term of the logarithm would give an approximation worse than the quadratic, but wouldn’t involve a square root. Taking a third term and we need to solve a cubic, which in general would need a lot more code and in our case it doesn’t seem to be of any special form that has a simple solution<sup data-fn="809751a5-4c07-42d1-9eb3-892e999bd451"><a href="#809751a5-4c07-42d1-9eb3-892e999bd451" id="809751a5-4c07-42d1-9eb3-892e999bd451-link">2</a></sup>. There are many approximations to <img src="https://s0.wp.com/latex.php?latex=%5Clog%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Clog%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Clog%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\log(x)">, but the non-Taylor ones involve advanced functions like <img src="https://s0.wp.com/latex.php?latex=x%5Ex&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x%5Ex&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x%5Ex&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="x^x"> that are hard to invert.</p>



<p>Until I looked a little more closely at his square root. It’s of the form:</p>


<p><img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B+%5Cleft%28+%5Cdfrac%7Bb%7D%7B2%7D+%5Cright%29+%5E2+-+a+c%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Csqrt%7B+%5Cleft%28+%5Cdfrac%7Bb%7D%7B2%7D+%5Cright%29+%5E2+-+a+c%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Csqrt%7B+%5Cleft%28+%5Cdfrac%7Bb%7D%7B2%7D+%5Cright%29+%5E2+-+a+c%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\sqrt{ \left( \dfrac{b}{2} \right) ^2 - a c} "></p>



<p>Which looks a awful lot like the quadratic formula where we’ve divided by 4 inside the square root. It <em>has to</em> be related. But why is it in the denominator? Did he find a quadratic in <img src="https://s0.wp.com/latex.php?latex=1%2Ft&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1%2Ft&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=1%2Ft&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="1/t"> ? No, because <em>t</em> can be very close to zero, so his formula would need to be approximated over a wide range of very large values, and a quadratic isn’t good for that. Did he make a quadratic approximation to log, <em>then</em> substitute <img src="https://s0.wp.com/latex.php?latex=T%3D1%2Ft&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=T%3D1%2Ft&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=T%3D1%2Ft&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="T=1/t">, solve for <em>T</em>, then substitute back? Playing around with that, I re-discovered an <a href="https://en.wikipedia.org/wiki/Quadratic_formula#Square_root_in_the_denominator">alternate form</a> of the quadratic formula with the square root on the bottom! And indeed, this matches the formula in Jim’s code.</p>



<p>I don’t know why 18 year old Jim was using the alternate form. Perhaps he re-derived the quadratic formula rather than looking it up, and ended up deriving that form. Perhaps he was worried about <a href="https://en.wikipedia.org/wiki/Catastrophic_cancellation">catastrophic cancellation</a> and wanted a form where he’d add positive numbers, rather than subtract them.<sup data-fn="2e932716-93ea-4b84-b03e-a6fb66e4764b"><a href="#2e932716-93ea-4b84-b03e-a6fb66e4764b" id="2e932716-93ea-4b84-b03e-a6fb66e4764b-link">3</a></sup></p>



<h2>Let’s Double Check The Derivation</h2>



<p>But if his formula is equivalent, then why is the approximation error 30 times higher? Deriving the formula ourselves, we get:</p>


<p><img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BLet+%7D+W+%5Cequiv+%5Cdfrac%7B1+-+%5Cfrac%7BMG%7D%7BZK%7D%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Ctext%7BLet+%7D+W+%5Cequiv+%5Cdfrac%7B1+-+%5Cfrac%7BMG%7D%7BZK%7D%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Ctext%7BLet+%7D+W+%5Cequiv+%5Cdfrac%7B1+-+%5Cfrac%7BMG%7D%7BZK%7D%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\text{Let } W \equiv \dfrac{1 - \frac{MG}{ZK}}{2}"></p>
<p><img src="https://s0.wp.com/latex.php?latex=t%3D%5Cdfrac%7BMV%7D%7BZK+%5Cleft%28+W%2B%5Csqrt%7BW%5E2+%2B+%5Cdfrac%7BV%7D%7B2Z%7D%7D+%5Cright%29+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=t%3D%5Cdfrac%7BMV%7D%7BZK+%5Cleft%28+W%2B%5Csqrt%7BW%5E2+%2B+%5Cdfrac%7BV%7D%7B2Z%7D%7D+%5Cright%29+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=t%3D%5Cdfrac%7BMV%7D%7BZK+%5Cleft%28+W%2B%5Csqrt%7BW%5E2+%2B+%5Cdfrac%7BV%7D%7B2Z%7D%7D+%5Cright%29+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="t=\dfrac{MV}{ZK \left( W+\sqrt{W^2 + \dfrac{V}{2Z}} \right) }"></p>



<p>Which is identical to Jim’s code, except … he’s missing the 2 in the denominator inside the square root! It was probably a simple error, either when deriving the formula or typing it into the computer.  After all, the computer algebra system <a href="https://en.wikipedia.org/wiki/Macsyma">MACSYMA</a> had only started a year before, and wouldn’t be available at a high school, so he would have had to do everything using pencil and paper.</p>



<p>With this bug, he consistently <em>under</em>estimates the time to the lowest point. He compensates for this two ways: adding 0.05 sec, and then re-estimating from his new, closer location. And this explains why it misses the time of landing: the first estimate is while the lander is above the surface and still descending, then the second one is after reaching the bottom and ascending again, which takes less than 0.05 sec.</p>



<p>If we add the missing factor of two and remove the 0.05, what happens? Now the best we can do with a suicide burn is:</p>



<figure><img data-attachment-id="659" data-permalink="https://martincmartin.com/2024/06/14/how-i-found-a-55-year-old-bug-in-the-first-lunar-lander-game/screenshot-2024-06-13-at-1-24-32-pm/" data-orig-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-13-at-1.24.32e280afpm.png" data-orig-size="1112,686" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-06-13 at 1.24.32 PM" data-image-description="" data-image-caption="" data-medium-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-13-at-1.24.32e280afpm.png?w=300" data-large-file="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-13-at-1.24.32e280afpm.png?w=640" loading="lazy" width="1112" height="686" src="https://martincmartin.com/wp-content/uploads/2024/06/screenshot-2024-06-13-at-1.24.32e280afpm.png" alt=""></figure>



<p>Our velocity is down to 1.66 MPH, almost three quarters of the way to the perfect landing at 1 MPH.  It’s not perfect because w’re still only using the first two terms of the Taylor series.  Also, once you’ve decided your lowest point is under the surface, you then need to find the time where you first hit the surface, which involves a similar approximation.  Another iteration would help, although with the bug fixed we <em>over</em>estimate the time, so we’d need to go back in time, which might mean we have to pick the other solution to the quadratic. You could simplify that by using only a single term from the Taylor series, and is what’s done in <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton’s method</a>. You could then stop when the magnitude of the velocity is below some threshold, and use the altitude there to decide whether or not you landed.  But this is all more work, and the game was fun to play as it is, so why complicate the code?</p>



<p>It’s also possible to land gently, you just need to end your 14th turn with a low altitude and velocity, and then use low thrust in your 15th turn, landing somewhere after 150 seconds.  It’s just the theoretical full-thrust-on-landing suicide burn, that takes around 148 seconds, that eludes us.</p>



<h2>CAPCOM We’re <a href="https://youtu.be/BHIo6qwJarI?t=45s">Go</a> For Powered Descent</h2>



<p>Overall, this is very impressive work for an 18 year hold high school student in 1969 on a PDP-8.  This was long before Computer Science was taught in high school.  The numerical computing aspects, such as iteratively improving the estimate using Newton’s method or worrying about catastrophic cancellation, weren’t well known back then.  I didn’t learn them until I was studying for a Ph.D. in robotics.</p>



<p>It is also surprising that, as far as I can tell, this bug has been there for almost 55 years and nobody has noticed it before.  That’s probably because, even with the bug, it was a fun game, both difficult and possible to land gently.&nbsp; The quest to not just win, but find the optimal strategy, can certainly lead to trying to understand small discrepancies.  I suspect everybody else was just happy to play the game and have fun.</p>


<ol><li id="636f2ae6-d817-49bc-a1ca-6c07b4cee9c6">It needs the <a href="https://en.wikipedia.org/wiki/Lambert_W_function">Lambert W,</a> the inverse of <img src="https://s0.wp.com/latex.php?latex=x+e%5Ex&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x+e%5Ex&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x+e%5Ex&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="x e^x"> <a href="#636f2ae6-d817-49bc-a1ca-6c07b4cee9c6-link">↩︎</a></li><li id="809751a5-4c07-42d1-9eb3-892e999bd451">For example, <img src="https://s0.wp.com/latex.php?latex=ax%5E2-bx%5E2%2Bbx%2Bd%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=ax%5E2-bx%5E2%2Bbx%2Bd%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=ax%5E2-bx%5E2%2Bbx%2Bd%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="ax^2-bx^2+bx+d=0">, where the 2nd and 3rd coefficients have the same magnitude but different sign, can be factored in the form <img src="https://s0.wp.com/latex.php?latex=a%28x-r%29%5E3+%2B+s+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=a%28x-r%29%5E3+%2B+s+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=a%28x-r%29%5E3+%2B+s+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="a(x-r)^3 + s = 0">, which has solution <img src="https://s0.wp.com/latex.php?latex=x+%3D+%5Csqrt%5B3%5D%7Br-s%2Fa%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=x+%3D+%5Csqrt%5B3%5D%7Br-s%2Fa%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=x+%3D+%5Csqrt%5B3%5D%7Br-s%2Fa%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="x = \sqrt[3]{r-s/a}.">  <a href="#809751a5-4c07-42d1-9eb3-892e999bd451-link">↩︎</a></li><li id="2e932716-93ea-4b84-b03e-a6fb66e4764b">This doesn’t seem likely, as this is a rather advanced topic for a High School senior in 1969.  Plus the alternate form wasn’t well known, and anyway, the errors from cancellation are much smaller than errors from only taking two terms of the Taylor series. <a href="#2e932716-93ea-4b84-b03e-a6fb66e4764b-link">↩︎</a></li></ol>											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Map of forest sounds from around the world (163 pts)]]></title>
            <link>https://timberfestival.org.uk/soundsoftheforest-soundmap/</link>
            <guid>40680107</guid>
            <pubDate>Fri, 14 Jun 2024 12:11:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timberfestival.org.uk/soundsoftheforest-soundmap/">https://timberfestival.org.uk/soundsoftheforest-soundmap/</a>, See on <a href="https://news.ycombinator.com/item?id=40680107">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="project-summary">
    <h2>About / Sounds of the Forest</h2>
    <p>We are collecting the sounds of woodlands and forests from all around the world, creating a growing soundmap bringing together aural tones and textures from the world’s woodlands.</p>
    
    <p>The sounds form an open source library, to be used by anyone to listen to and create from. <a href="https://timberfestival.org.uk/sounds-of-the-forest/the-next-phase/">Selected artists</a> will be responding to the sounds that are gathered, creating music, audio, artwork or something else incredible, to be presented at <a href="https://timberfestival.org.uk/" target="_blank">Timber Festival 2021</a>.</p>
    
    <div>
        
        <h4>Timber</h4>
        <p>An annual three-day festival at the heart of the National Forest, Timber celebrates our connection to trees and woodlands through music, art and ideas, taking its inspiration from the transformed landscape of the National Forest in the Midlands.</p>
        
      <p>Created by <a href="https://wildrumpus.org.uk/" target="_blank">Wild Rumpus</a>, built on top of <a href="https://mapbox.com/" target="_blank">Mapbox GL JS</a>.</p>
    
        <h4>Contributions</h4>
        <p><a href="https://timberfestival.org.uk/sounds-of-the-forest/audio-submission-form/" target="_blank">Find out how you can contribute here.</a></p>

        <h4>Reuse and attribution</h4>
        <p>All audio files found in this project are part of the <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Share Alike agreement.</a></p>

        <h4>Contact</h4>
        <p>For any queries or questions, email <a href="mailto:soundsoftheforest@timberfestival.org.uk">soundsoftheforest@timberfestival.org.uk</a></p>
        
        <p>Sounds of the Forest is supported using public funding by Arts Council England. <a href="https://timberfestival.org.uk/sounds-of-the-forest/the-next-phase/">The second phase of the project is supported by PRS Foundation’s Open Fund for Organisations</a>.</p>
    
    </div>
    
    <p><a id="hide">CLOSE</a>
    
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[POSIX.1-2024 is published (143 pts)]]></title>
            <link>https://ieeexplore.ieee.org/document/10555529</link>
            <guid>40679809</guid>
            <pubDate>Fri, 14 Jun 2024 11:37:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ieeexplore.ieee.org/document/10555529">https://ieeexplore.ieee.org/document/10555529</a>, See on <a href="https://news.ycombinator.com/item?id=40679809">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="LayoutWrapper">
						











						
						








<meta name="cToken" content="eyJhbGciOiJIUzUxMiIsInppcCI6IkRFRiJ9.eNqqVkosKFCyUoooyMkvSlXSUcosLgZyK2Dc1AqgrKG5oYWxhYWlgTFQPrEEJmBmZGBcCwAAAP__.fubf6p8I9y_QNEKVkXBsEMlNDKuRlqyBU_P6b_Oe6oJ9JAEv50JTT_bmWyEoYC5ulg7raOmjfK_TEi7WcDiRbA">



<!-- XPL-21560-Added as part of Universal CASA-Dev -->



<!--- This is a picture popup embed for mobilew view.  -->












<div>
		<xpl-root>
			
		</xpl-root>
	</div>

						




<div id="xploreFooter">
		<div>
			<div>
				<h3>IEEE Account</h3>
				<ul>
					<li><a href="https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Change Username/Password</a></li>
					<li><a href="https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Update Address</a></li>
				</ul>
			</div>
			<div>
				<h3>Purchase Details</h3>
				<ul>
					<li><a href="https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a></li>
					<li><a href="https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Order History</a></li>
					<li><a href="https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp">View Purchased Documents</a></li>
				</ul>
			</div>
			<div>
				<h3>Profile Information</h3>
				<ul>
					<li><a href="https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a></li>
					<li><a href="https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a></li>
					<li><a href="https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Technical Interests</a></li>
				</ul>
			</div>
			<div>
				<h3>Need Help?</h3>
				<ul>
					<li><strong>US &amp; Canada:</strong> +1 800 678 4333</li>
					<li><strong>Worldwide: </strong> +1 732 981 0060<br>
					</li>
					<li><a href="https://ieeexplore.ieee.org/xpl/contact">Contact &amp; Support</a></li>
				</ul>
			</div>
		</div>
		<div>
						<ul>
							<li><a href="https://ieeexplore.ieee.org/Xplorehelp/about-ieee-xplore.html">About IEEE <em>Xplore</em></a></li>
							<li><a href="https://ieeexplore.ieee.org/xpl/contact">Contact Us</a></li>
							<li><a href="https://ieeexplore.ieee.org/Xplorehelp/Help_start.html" target="blank">Help</a></li>
							<li><a href="https://ieeexplore.ieee.org/Xplorehelp/accessibility-statement.html" target="blank">Accessibility</a></li> 
							<li><a href="https://ieeexplore.ieee.org/Xplorehelp/Help_Terms_of_Use.html" target="_blank">Terms of Use</a></li>
							<li><a href="http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html">Nondiscrimination Policy</a></li>
							<li><a href="https://ieeexplore.ieee.org/xpl/sitemap.jsp">Sitemap</a></li>
							<li><a href="http://www.ieee.org/about/help/security_privacy.html" target="blank">Privacy &amp; Opting Out of Cookies</a></li>
						</ul>
						<p>
							A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.<br>© Copyright 2024 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
						</p>
					</div>
	</div>

						<!-- BEGIN: tealium in v2/common/template.jsp. We need to include tealiumAnalytics.js here since Angular 2+ app load if you load after commnon.js then tealium value will not be available in angular 2+ app  -->
						






		<!-- BEGIN: TealiumAnalytics.jsp -->
		
		
		
		
		
		
		
		
		
		
		
		
			
				
			
			
			
			
		
		
		
		
		
		

			


			

			
			


		
 		
		<!-- END: TealiumAnalytics.jsp -->
			 

						<!-- END: tealium in v2/common/template.jsp -->
						






	
	






















	
	


<!-- START OF Angular bundle assets -->




<!-- END OF Angular bundle assets -->

<!-- Usabilla Combicode for IEEE-->
<!-- Begin Usabilla for Websites embed code -->

<!-- end usabilla live embed code -->




<!-- START: ZoomInfo JS beacon integration XPL-27850-->
	
<!-- END: ZoomInfo JS beacon integration-->



<!-- START: Hum JS beacon integration XPL-27979-->

<!-- END: Hum JS beacon integration-->





	









<g:compress>








		
		
			
				
					
					
								
					
								
			
				
					
						








 



					
					
								
			
				
					
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
		
	








</g:compress>








		
			
					
			
					
			
				
					








					
			
					
			
					
			
					
			
					
			
		
		
	




	<!--Begin Optional Configuration-->
	
	
	
	
	
	


	
	
	

	<!--End Optional Configuration-->


<!-- Removed due to network issues when loading in China -->
<!-- <script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#pubid=ra-5005a435228f9245" async="async"></script>-->

<!-- Load Mathjax and process the document for Mathjax characters -->


<!-- <script type="text/javascript" src="/xploreAssets/MathJax-274/MathJax.js?config=default"></script> -->






						







					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brain-Health Benefits of Weightlifting (102 pts)]]></title>
            <link>https://www.psychologytoday.com/us/blog/the-modern-brain/202402/the-surprising-benefits-of-weightlifting-for-brain-health</link>
            <guid>40679742</guid>
            <pubDate>Fri, 14 Jun 2024 11:26:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psychologytoday.com/us/blog/the-modern-brain/202402/the-surprising-benefits-of-weightlifting-for-brain-health">https://www.psychologytoday.com/us/blog/the-modern-brain/202402/the-surprising-benefits-of-weightlifting-for-brain-health</a>, See on <a href="https://news.ycombinator.com/item?id=40679742">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div>
<p><img alt="Andrea Piacquadio/Pexels" height="213" src="https://cdn2.psychologytoday.com/assets/styles/article_inline_half_caption/public/field_blog_entry_images/2024-02/pexels-andrea-piacquadio-3757376.jpg?itok=syv30W6P" title="Andrea Piacquadio/Pexels" width="320"></p>

<p>Andrea Piacquadio/Pexels</p>
</div>
<p>In the conversation about how to protect and improve brain health, exercise is always at the top of the list. This is because consistent research shows that people who move their bodies are at lower risk for brain issues like <a href="https://www.psychologytoday.com/us/basics/depression" title="Psychology Today looks at depression" hreflang="en-US">depression</a> and Alzheimer’s disease. By and large, the focus of this conversation is on walking, jogging, and potentially lower-impact movements like tai chi and yoga. In fact, aerobic exercises like these are more widely prescribed by clinicians. But research shows us we need to consider weightlifting more strongly for our brain health.</p>

<p>In this article, we’ll be discussing some of the science backing the idea that weightlifting, or resistance training, can have specific and marked benefits to brain health. To get us started, let’s just explore a few of the studied benefits of weightlifting for brain health.</p>
<ol><li>People who engage in resistance training tend to have better brain health.</li>
<li>Healthy people who start weightlifting can <a href="https://pubmed.ncbi.nlm.nih.gov/20101012/">improve</a> their brain function, and this benefit is likely to be more pronounced in the elderly.</li>
<li>People with <a href="https://www.psychologytoday.com/us/basics/mild-cognitive-impairment" title="Psychology Today looks at cognitive decline" hreflang="en-US">cognitive decline</a> had better <a href="https://www.psychologytoday.com/us/basics/cognition" title="Psychology Today looks at cognition" hreflang="en-US">cognition</a> after starting resistance training.</li>
<li>Resistance training may help <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10745562/">decrease</a> depressive symptoms.</li>
</ol>

<p>This is notable stuff. And to be clear, there is <em>no pharmaceutical</em><strong> </strong>currently on the market that can achieve this combination of benefits. And of course, there’s lots of other research showing the benefit of weight training for almost every other chronic health issue (for example, a recent <a href="https://bjsm.bmj.com/content/56/13/755">meta-analysis</a> in the <em>British Journal of Sports Medicine</em> concluded that “muscle-strengthening activities were inversely associated with the risk of all-cause mortality and major non-communicable diseases including CVD, total cancer, diabetes and lung cancer.”</p>

<p>Personally, I believe this data is more than sufficient to recommend that everyone at least consider adding weights to their exercise routine. But what’s at play here as it relates specifically to the brain benefits? There are at least two major pathways at play.</p>
<p><strong>1. Weight training helps regulate metabolic balance. </strong>Several years ago, researchers at Brown University proposed that Alzheimer’s <a href="https://www.psychologytoday.com/us/basics/dementia" title="Psychology Today looks at dementia" hreflang="en-US">dementia</a> could be considered “type 3 diabetes,” due to a series of metabolic alterations seen in the brains of people with this condition. Namely, research suggests that as we age, and especially in people with Alzheimer’s disease, there is a drop-off in the brain’s ability to access and use glucose. Additional work shows that people with metabolic dysfunction are at higher risk for developing dementia and depression.</p>

<p>This is serious stuff. <a href="https://www.sciencedirect.com/science/article/abs/pii/S1751991821001911">A 2022 meta-analysis</a> found the odds for depression in people with type 2 diabetes to be 77 percent higher than those with normal blood sugar. Similarly, a recent <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7746331/">Mendelian analysis</a> found that having a higher fasting blood sugar was linked to a higher risk for Alzheimer’s disease.</p>

<p>Mechanistically, there are two major connections between brain health and blood sugar worthy of consideration. First, high variability in blood sugar levels and prolonged high blood sugar may both be damaging to the brain through a host of pathways including inflammation. Second, long-term imbalances in blood sugar may come alongside an issue called <em>insulin resistance</em>, which means that our cells (potentially including our brain cells) have more trouble getting the glucose they need.</p>

<p>What do we know about the specifics of the link between blood sugar balance and resistance training?</p>
<ul><li><a href="https://www.sciencedirect.com/science/article/abs/pii/S1056872720303664">A single bout</a> of resistance training has been shown to lower glucose and insulin levels for up to 24 and 18 hours, respectively.</li>
<li>Resistance training helps lower 3-month blood sugar (HbA1c) measurements in those at risk for developing type 2 diabetes based on a <a href="https://sportsmedicine-open.springeropen.com/articles/10.1186/s40798-021-00321-x">2021 meta-analysis</a>.</li>
<li>Resistance training has been found to improve blood sugar regulation and insulin requirements in <a href="https://www.psychologytoday.com/us/basics/pregnancy" title="Psychology Today looks at pregnant" hreflang="en-US">pregnant</a> women with gestational diabetes in a <a href="https://www.sciencedirect.com/science/article/pii/S0266613820302114">2020 meta-analysis</a>.</li>
<li>Resistance training improves insulin sensitivity in the elderly based on a <a href="https://www.sciencedirect.com/science/article/pii/S1728869X21000307">2021</a> meta-analysis published in the <em><a href="https://www.sciencedirect.com/journal/journal-of-exercise-science-and-fitness" title="Go to Journal of Exercise Science &amp; Fitness on ScienceDirect">Journal of Exercise Science and Fitness</a></em>.</li>
</ul>

<p><strong>2. Weight training helps promote immune balance. </strong>In the wake of the pandemic, more people than ever appreciate the importance of immune health to overall wellness. In addition, there’s been a groundswell of interest around the idea of chronic inflammation as a central driver of disease. One of the most important breakthroughs in research around the benefit of weight training concerns the immune system. Specifically, it relates to molecules called myokines, which are tiny signals produced by muscles that can enter and impact the brain.</p>

<p>While the research around myokines is still pretty new, the overall idea is simple: resistance training alters levels of a host of chemicals produced by the muscles that may have beneficial effects on the brain. Some of the best-studied myokines include brain-derived neurotrophic factor (BDNF), insulin-like growth factor 1 (IGF-1), interleukin 6 (IL-6), and irisin. Many of these and other myokines are believed to directly and indirectly affect the brain’s immune system for the better. Notably, BDNF is strongly tied to the process of <em>neuroplasticity</em>, and is known to help strengthen connections between our brain cells and even help us generate new ones.</p>



  
<div>
      <p>Cognition Essential Reads</p>
        
  </div>

<p>In a more straightforward sense, resistance training has been linked to better immune balance and lower inflammation. For example, people with more muscle mass <a href="https://www.sciencedirect.com/science/article/pii/S1568163720303202">tend</a> to have lower levels of inflammation. Similarly, stronger grip and knee extension abilities correlate with lower levels of inflammatory markers.</p>

<h2>The Best Way to Capitalize on the Benefits of Resistance Training</h2>
<p>Like everything else in long-term health promotion, brain health strategies should be designed with the goal of sustainability; that means avoiding injury and creating habits that you’ll enjoy for months to years. Here are some great tips on resistance training for better overall and brain health:</p>

<ul><li>Start light and build your way up. While heavy weights can be great, you can get an excellent workout with lighter weights. Similarly, you don’t have to lift for hours to get brain benefits. Even doing a few minutes of resistance training a day may benefit you.</li>
<li>Consider getting help: If you haven’t done much weight training, consider working with a trainer to learn the ropes and establish a routine that fits your needs</li>
<li>Classes can be great motivators. There’s robust data suggesting that spending time with others boosts the brain, so combining this with weights could be an excellent double benefit to your cognition. This could be anything from a session at your local gym to following along with an online instructor to CrossFit.</li>
<li>Consider resistance bands. Resistance bands are an excellent way to do resistance training anywhere and to customize it to your needs. They’ve been <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6383082/">shown</a> to be effective at building muscle, and have been <a href="https://agsjournals.onlinelibrary.wiley.com/doi/abs/10.1111/jgs.14526">linked</a> to better mood.</li>
<li>Listen to your body! Having personally injured myself while weight training a number of times, I can’t <a href="https://www.psychologytoday.com/us/basics/stress" title="Psychology Today looks at stress" hreflang="en-US">stress</a> enough the importance of paying <a href="https://www.psychologytoday.com/us/basics/attention" title="Psychology Today looks at attention" hreflang="en-US">attention</a> to your body while you lift and modifying your routine accordingly. If something hurts in a bad way, listen!</li>
<li>Don’t skip the legs! Your legs contain the largest muscles in your body. If you’re looking to get the biggest effect of resistance training, you definitely want to engage your legs. Leg presses, squats, and leg extensions/curls are some of my favorites. </li>
<li>Consider the nutritional aspects. <a href="https://nutritionandmetabolism.biomedcentral.com/articles/10.1186/s12986-016-0124-8">Research</a> shows that getting adequate protein is <a href="https://www.sciencedirect.com/science/article/abs/pii/S1525861012001788">key</a> to muscle growth. If you’re new to resistance training, you may need to up your protein intake. Ideal protein intake varies by person and <a href="https://www.psychologytoday.com/us/basics/motivation" title="Psychology Today looks at goals" hreflang="en-US">goals</a>, but some data suggests that we also should be consuming more protein as we age to offset age-related muscle loss. Additionally, there’s good research supporting the role of creatine supplementation for brain and muscle health at around 3-5 grams a day.</li>
</ul>

<p><strong><a href="https://www.psychologytoday.com/us/basics/social-media" title="Psychology Today looks at Facebook" hreflang="en-US">Facebook</a> image: LarsZ/Shutterstock</strong></p>
<p><strong>LinkedIn image: BAZA Production/Shutterstock</strong></p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FAA investigating how counterfeit titanium got into Boeing and Airbus jets (146 pts)]]></title>
            <link>https://www.nytimes.com/2024/06/14/us/politics/boeing-airbus-titanium-faa.html</link>
            <guid>40679599</guid>
            <pubDate>Fri, 14 Jun 2024 11:00:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/06/14/us/politics/boeing-airbus-titanium-faa.html">https://www.nytimes.com/2024/06/14/us/politics/boeing-airbus-titanium-faa.html</a>, See on <a href="https://news.ycombinator.com/item?id=40679599">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/06/14/us/politics/boeing-airbus-titanium-faa.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[40 out of 60 German climate greening endavours fraudulent (161 pts)]]></title>
            <link>https://www.fr.de/politik/warnungen-milliarden-an-konzerne-gezahlt-betrugsverdacht-beim-klimaschutz-trotz-zr-93122965.html</link>
            <guid>40677926</guid>
            <pubDate>Fri, 14 Jun 2024 05:59:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fr.de/politik/warnungen-milliarden-an-konzerne-gezahlt-betrugsverdacht-beim-klimaschutz-trotz-zr-93122965.html">https://www.fr.de/politik/warnungen-milliarden-an-konzerne-gezahlt-betrugsverdacht-beim-klimaschutz-trotz-zr-93122965.html</a>, See on <a href="https://news.ycombinator.com/item?id=40677926">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><ol data-k5a-pos="west_breadcrumb"><li><a href="https://www.fr.de/" data-id-ec="{&quot;shn&quot;:&quot;Story&quot;,&quot;sht&quot;:&quot;Container&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Breadcrumb&quot;,&quot;displayType&quot;:&quot;Breadcrumb Textlink&quot;,&quot;type&quot;:&quot;BREADCRUMB_LINK&quot;,&quot;value&quot;:&quot;Startseite&quot;,&quot;position&quot;:&quot;1&quot;}}" data-k5a-pos="1_startseite">Startseite</a></li><li><a href="https://www.fr.de/politik/" data-id-ec="{&quot;shn&quot;:&quot;Story&quot;,&quot;sht&quot;:&quot;Container&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Breadcrumb&quot;,&quot;displayType&quot;:&quot;Breadcrumb Textlink&quot;,&quot;type&quot;:&quot;BREADCRUMB_LINK&quot;,&quot;value&quot;:&quot;Politik&quot;,&quot;position&quot;:&quot;2&quot;}}" data-k5a-pos="2_politik">Politik</a></li></ol><p><span><span>Stand: </span><time datetime="2024-06-14 04:51">14.06.2024, 04:51 Uhr</time></span></p><p><a href="#id-Comments" data-id-ec="{&quot;shn&quot;:&quot;Story&quot;,&quot;sht&quot;:&quot;Container&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;InteractionBar&quot;,&quot;displayType&quot;:&quot;InteractionBar Comment-Action&quot;,&quot;type&quot;:&quot;COMMENT_ACTION&quot;,&quot;value&quot;:&quot;Kommentieren&quot;,&quot;position&quot;:&quot;1&quot;,&quot;storyElementPosition&quot;:&quot;5&quot;,&quot;storyElementCount&quot;:&quot;43&quot;}}" data-k5a-pos="west_commentAction"><span>Kommentare</span></a><a href="https://www.fr.de/sso/login?redirect_uri=https://www.fr.de/politik/warnungen-milliarden-an-konzerne-gezahlt-betrugsverdacht-beim-klimaschutz-trotz-zr-93122965.html&amp;source=uid_ssowidget_login_web" data-id-ec="{&quot;shn&quot;:&quot;Story&quot;,&quot;sht&quot;:&quot;Container&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;InteractionBar&quot;,&quot;displayType&quot;:&quot;InteractionBar Print-Action&quot;,&quot;type&quot;:&quot;PRINT_ACTION&quot;,&quot;value&quot;:&quot;Drucken&quot;,&quot;position&quot;:&quot;2&quot;,&quot;storyElementPosition&quot;:&quot;5&quot;,&quot;storyElementCount&quot;:&quot;43&quot;}}" data-k5a-pos="west_printAction" data-iduid-sso-widget-source-prefix="west_pdf-print"><i></i><span>Drucken</span></a></p><p>Öl-Konzerne betrügen vermutlich deutsche Verbraucher mit nicht existenten Anlagen. Auch, weil Deutschland keine Kontrolle hatte? Der Umweltausschuss nimmt Stellung.</p><p><strong>Update vom 12. Juni, 16.21 Uhr:</strong> Schwere Vorwürfe gegen das Umweltministerium des Bundes: Viel zu fahrlässig habe man chinesische Projekte zum Klimaschutz unterstützt. Dabei seien Millionen an Geldern geflossen, die aus der sogenannten THG-Quote stammten.  </p><p>„Seit August 2023 gibt es erhebliche Betrugsvorwürfe gegen Klimaschutzprojekte in China, welche zur Anrechnung auf die Treibhausgasminderungsquote genutzt werden“, sagte Daniel Rinkert (SPD) in einem Plenum des Umweltausschusses am Mittwoch. „In der heutigen Sitzung wurde das Thema ausführlich diskutiert und Vorwürfe ausgeräumt.“</p><p>Die Betrugsanschuldigungen bei den Upstream-Emission-Reduction-Projekten – kurz UER-Projekte – seien laut Parlamentarier erheblich und verlangten eine konsequente Aufklärung. Laut Umweltbundesamt stehen aktuell 40 von 60 Projekten in China unter einem Betrugsverdacht. „Das ist Wirtschaftskriminalität im großen Umfang“, sagte Rinkert. „Es ist mehr als offensichtlich, dass bei diesen Projekten keine Kontrolle auf anspruchsvollem Niveau sichergestellt werden kann. Daher wird die Förderung dieser Projekte auch durch die Bundesregierung Ende 2024 eingestellt. Die entsprechende Verordnung wurde am 8. Juni im Kabinett verabschiedet.“</p><h2>Umweltministerium will mit Auswärtigem Amt deutsche Kontrollen in China durchdrücken</h2><p>Das Bundesumweltministerium und das Umweltbundesamt hätten damit „sehr deutlich gemacht“, dass die zuständigen Stellen in Deutschland frühzeitig eine „vollumfängliche Aufklärung“ eingeleitet haben. „In aufwendigen Prüfungsschritten wird seit den ersten aufkommenden Betrugsvorwürfen jedes einzelne Projekt untersucht und überprüft“, betonte Rinkert.&nbsp; Die in diesem Zusammenhang immer wieder vorgebrachten Vorwürfe der Tatenlosigkeit der zuständigen Stellen hätten sich damit als haltlos herausgestellt. „Die Union sollte die Vorwürfe daher nicht wiederholen und Nebelkerzen werfen. Sie sollte vielmehr die Bundesregierung bei der Aufklärung unterstützen“, sagte Rinkert.</p><p>Gemeinsam mit dem Auswärtigen Amt dränge das Umweltministerium nun die chinesische Regierung dazu, eine unabhängige deutsche Kontrollmission in China zuzulassen. Nur dadurch könne ein vollumfänglicher Einblick in die Situation vor Ort gewonnen werden. „Für dieses Vorgehen gibt es die volle Unterstützung der SPD-Bundestagsfraktion“, sagte Rinkert zum Schluss.</p><h2>Millionen-Betrug in China: Haben Konzerne die deutschen Klimaschutzmaßnahmen unterwandert?</h2><p><strong>Erstmeldung vom 11. Juni: </strong>Berlin – Der Image-Schaden für den deutschen Klimaschutz dürfte fast schwerer wiegen, als die verlorenen Milliarden an Euros. Nagelneue Ölförderungsanlagen in China sollen angeblich CO₂ einsparen. Gebaut von internationalen Großkonzernen zur „grüneren“ Gewinnung von Öl. Die Bauvorhaben ermöglicht jeder, der in Deutschland tankt. Denn als Nutzer:in von fossilen Brennstoffen zahlt man beim Tanken Klima-Abgaben über die „Treibhausgasminderungsquote“ – kurz THG-Quote. Das Problem: Wahrscheinlich gibt es diese neuen und sauberen Anlagen in <a href="https://www.merkur.de/politik/china/" title="China" data-id-ec="{&quot;sht&quot;:&quot;Container&quot;,&quot;shn&quot;:&quot;Story&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Paragraph&quot;,&quot;displayType&quot;:&quot;Paragraph InlineLink&quot;,&quot;type&quot;:&quot;INLINE_LINK&quot;,&quot;value&quot;:&quot;China&quot;,&quot;variant&quot;:&quot;standardLink&quot;,&quot;sponsored&quot;:false,&quot;storyElementPosition&quot;:18,&quot;storyElementCount&quot;:43}}">China</a> gar nicht, wie Recherchen von <em>ZDF frontal</em> ergaben. Und anscheinend ist es auch nicht möglich für das Umweltministerium des Bundes, den Betrugsvorwürfen effektiv nachzugehen, wie die <em>Welt</em> berichtet.</p><p>Doch von Anfang an: Verstrickt in den Skandal sind anscheinend mehrere Ölmultis, die die Anlagen angeblich gebaut haben. Die meisten dieser Projekte sollen auf den Öl- und Gasfeldern der Provinz Xinjang entstanden sein. Für die Konzerne ein lukratives Geschäft mit dem Handel von Umwelt-Zertifikaten. Denn für jede eingesparte Tonne Kohlendioxid konnten die Unternehmen bis zu 400 Euro in Deutschland kassieren. Viele Projekte waren nach Recherchen von „ZDF Frontal“ aber nur vorgetäuscht. </p><figure><p><img src="https://www.fr.de/assets/images/34/795/34795011-eigentlich-sollten-mit-der-umlage-oel-und-gasbohrungen-vor-allem-in-china-finanziert-werden-die-umweltfreundlicher-als-die-gewoehnliche-foerderung-OBBG.jpg" loading="lazy" srcset="https://www.fr.de/assets/images/34/795/34795011-eigentlich-sollten-mit-der-umlage-oel-und-gasbohrungen-vor-allem-in-china-finanziert-werden-die-umweltfreundlicher-als-die-gewoehnliche-foerderung-OBb9.jpg 448w,https://www.fr.de/assets/images/34/795/34795011-eigentlich-sollten-mit-der-umlage-oel-und-gasbohrungen-vor-allem-in-china-finanziert-werden-die-umweltfreundlicher-als-die-gewoehnliche-foerderung-OB73.jpg 768w,https://www.fr.de/assets/images/34/795/34795011-eigentlich-sollten-mit-der-umlage-oel-und-gasbohrungen-vor-allem-in-china-finanziert-werden-die-umweltfreundlicher-als-die-gewoehnliche-foerderung-OBBG.jpg 1100w,https://www.fr.de/assets/images/34/795/34795011-eigentlich-sollten-mit-der-umlage-oel-und-gasbohrungen-vor-allem-in-china-finanziert-werden-die-umweltfreundlicher-als-die-gewoehnliche-foerderung-OB7d.jpg 1408w,https://www.fr.de/assets/images/34/795/34795011-eigentlich-sollten-mit-der-umlage-oel-und-gasbohrungen-vor-allem-in-china-finanziert-werden-die-umweltfreundlicher-als-die-gewoehnliche-foerderung-OBPH.jpg 1600w" sizes="(min-width: 769px) 768px, 100vw" height="733" width="1100" alt="Eigentlich sollten mit der Umlage Öl- und Gasbohrungen vor allem in China finanziert werden, die umweltfreundlicher als die gewöhnliche Förderung sind. Das scheint in vielen Fällen nicht der Fall gewesen zu sein."></p><figcaption>Eigentlich sollten mit der Umlage Öl- und Gasbohrungen vor allem in China finanziert werden, die umweltfreundlicher als die gewöhnliche Förderung sind. Das scheint in vielen Fällen nicht der Fall gewesen zu sein. (Symbolbild)   ©&nbsp;Xiao Yijiu/dpa</figcaption></figure><h2>Betrugsverdacht: Chinesischer Konzern meldet sich bei deutschen Behörden</h2><p>Dabei waren die Gefahren Geld nach China zu zahlen für solche Projekte dem Bundesumweltministerium eigentlich bekannt. Zuletzt im April meldete sich ein chinesischer Öl- und Gaskonzern von selbst bei dem von Steffi Lemke (Grüne) geführten Umweltministerium und erklärte deutlich, dass von Betrugsfällen auszugehen ist. „Wir vermuten, dass es eine hohe Wahrscheinlichkeit gibt, dass Dokumente gefälscht wurden und wir bitten dringend, dass Ihre Behörde dazu ermittelt“, teilte der chinesische Konzern dem Ministerium mit. Dieses wimmelte wohl ab, wie die <em>Welt</em> berichtet. Deutsche Prüfstellen haben anscheinend einige Daten der Anlagen des chinesischen Unternehmens geändert und ohne dessen Zustimmung verwendet. Das Ziel war, möglichst hohe CO₂-Einsparungen in Deutschland geltend zu machen. Allein in diesem offenbar gefälschten Projekt ging es laut <em>ZDF</em> um eine Million Tonnen Kohlendioxid, die angeblich eingespart wurden – der Marktwert: etwa 180 Millionen Euro.</p><p>Ein weiteres Beispiel: Deutsche Autofahrerinnen und Autofahrer zahlten mit ihrer Klima-Abgabe vermutlich rund 80 Millionen Euro für ein<a href="https://www.fr.de/thema/klima-sti162637/" title="Klima" data-id-ec="{&quot;sht&quot;:&quot;Container&quot;,&quot;shn&quot;:&quot;Story&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Paragraph&quot;,&quot;displayType&quot;:&quot;Paragraph InlineLink&quot;,&quot;type&quot;:&quot;INLINE_LINK&quot;,&quot;value&quot;:&quot; Klimaschutzprojekt&quot;,&quot;variant&quot;:&quot;standardLink&quot;,&quot;sponsored&quot;:false,&quot;storyElementPosition&quot;:25,&quot;storyElementCount&quot;:43}}"> Klimaschutzprojekt</a> in der chinesischen Provinz der unterdrückten Uiguren. Am Ende stand dort laut <em>Welt</em> lediglich ein Hühnerstall. Recherchen deutscher Bioenergie-Unternehmen bestätigten den Vorfall.</p><p>Es existieren wohl mindestens 60 weitere und ähnlich gelagerte Verdachtsfälle in China. Der Schaden könnte sich auf mehr als 4,5 Milliarden Euro belaufen, wie das <em>Hauptstadtbüro Bioenergie</em> schätzt. Beim Hauptstadtbüro Bioenergie handelt es sich um eine Vertretung der Branche in Berlin.</p><p>Der Kern des vermutlichen Betrugs liegt in den Klimaschutzauflagen für Öl-Konzerne. Sie sind gesetzlich dazu verpflichtet, die von etwa Benzin und Diesel verursachten Emissionen jährlich zu senken. Die Kennzahlen dafür legt die Bundesregierung fest. Dabei reicht es nicht mehr, biologische Anteile in die Kraftstoffe zu mischen. Da so die Quoten nicht mehr erfüllt werden können.</p><h2>Behörden können zur Kontrolle der Klimaschutz-Projekte nicht in China einreisen</h2><p>Ein anderer vom Gesetzgeber vorgesehener Weg zur Einhaltung der THG-Quote ist die Möglichkeit, als Alternative im Ausland CO₂-Sparmaßnahmen zu finanzieren. Bei der Öl- und Gasförderung gibt es verschiedene technische Möglichkeiten, die Emissionen zu drücken. Mit der Reduzierung dieser sogenannten „Upstream-Emissionen“ – kurz UER – dürfen Konzerne seit 2020 ein Fünftel ihrer THG-Quoten erfüllen. Um die UER zu bestimmen, wird eine mögliche Menge an Emissionen der „grünen Anlage“ mit dem Wert verglichen, der entstünde, wenn die Förderungsanlagen nicht mit einem Projekt zur Minderung der Abgase gebaut worden wäre. Der Wert wird dabei über genormte Grundsätze nach ISO 14065 ermittelt. </p><p>Ölkonzerne können aber auch Zertifikate kaufen, die für eine Klimaschutz-Investition in eine Raffinerie oder andere Förderanlage stehen. Die Ausgaben für alle Vorhaben werden auf den Kraftstoffpreis an der Tankstelle umgelegt.</p><p>Das <a href="https://www.fr.de/politik/tbl-china-klimapolitik-treibhausgas-emissionen-co2-klimawandel-cop-28-entwicklungsland-zr-92405505.html" data-id-ec="{&quot;sht&quot;:&quot;Container&quot;,&quot;shn&quot;:&quot;Story&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Paragraph&quot;,&quot;displayType&quot;:&quot;Paragraph InlineLink&quot;,&quot;type&quot;:&quot;INLINE_LINK&quot;,&quot;value&quot;:&quot;Umweltbundesamt und die Deutsche Emissionshandelsstelle genehmigten &quot;,&quot;variant&quot;:&quot;standardLink&quot;,&quot;sponsored&quot;:false,&quot;storyElementPosition&quot;:33,&quot;storyElementCount&quot;:43}}">Umweltbundesamt und die Deutsche Emissionshandelsstelle genehmigten </a>75 dieser UER-Projekte – fast ausschließlich in China. Und das, obwohl weitere Hinweise dafür sprachen, besser nicht dort zu investieren. Denn China lässt unabhängige Kontrollen im eigenen Land nicht zu. Peking verweigert entsprechenden Prüfer:innen die Einreise.</p><p>„Das Ausmaß des Betrugs am Klimaschutz ist verheerend. Mehr als 7,6 Millionen Tonnen angeblicher CO₂-Einsparung hat es real nie gegeben. Das können wir jetzt nicht einfach mit einem Achselzucken abtun“, sagte Sandra Rostek dazu in einem Statement. Sie ist Leiterin des Hauptstadtbüros Bioenergie. „Hinweisgeber aus der Branche wurden von den Behörden abgewimmelt – noch vor wenigen Wochen wurden offenkundig gefälschte Projekte durchgewunken. Wir hoffen, dass vor dem Hintergrund der Recherche-Ergebnisse des <em>ZDF</em> die verantwortlichen Behörden endlich aufwachen.“</p><p>Was Rostek mit dem „Durchwinken“ meint, ist die anscheinend unzureichende Prüfung der Bauvorhaben durch das Umweltbundesamt und die Deutsche Emissionshandelsstelle. Über Satellitenbilder wäre einfach zu erkennen gewesen, dass einige der eingereichten chinesischen Vorhaben schon vor dem eigentlichen Baustart existiert haben. Dazu würden die Anlagen in keiner Weise den Vorgaben als UER-Projekt gerecht, wie das <em>ZDF</em> herausfand.</p><figure><p><img src="https://www.fr.de/assets/images/34/795/34795078-in-ihrem-ministerium-soll-es-massive-fehler-in-der-zulassung-von-emissions-projekten-gegeben-haben-bundesumweltministerin-steffi-lemke-1JNwnsullkBG.jpg" loading="lazy" srcset="https://www.fr.de/assets/images/34/795/34795078-in-ihrem-ministerium-soll-es-massive-fehler-in-der-zulassung-von-emissions-projekten-gegeben-haben-bundesumweltministerin-steffi-lemke-1JNwnsullkb9.jpg 448w,https://www.fr.de/assets/images/34/795/34795078-in-ihrem-ministerium-soll-es-massive-fehler-in-der-zulassung-von-emissions-projekten-gegeben-haben-bundesumweltministerin-steffi-lemke-1JNwnsullk73.jpg 768w,https://www.fr.de/assets/images/34/795/34795078-in-ihrem-ministerium-soll-es-massive-fehler-in-der-zulassung-von-emissions-projekten-gegeben-haben-bundesumweltministerin-steffi-lemke-1JNwnsullkBG.jpg 1100w,https://www.fr.de/assets/images/34/795/34795078-in-ihrem-ministerium-soll-es-massive-fehler-in-der-zulassung-von-emissions-projekten-gegeben-haben-bundesumweltministerin-steffi-lemke-1JNwnsullk7d.jpg 1408w,https://www.fr.de/assets/images/34/795/34795078-in-ihrem-ministerium-soll-es-massive-fehler-in-der-zulassung-von-emissions-projekten-gegeben-haben-bundesumweltministerin-steffi-lemke-1JNwnsullkPH.jpg 1600w" sizes="(min-width: 769px) 768px, 100vw" height="733" width="1100" alt="In ihrem Ministerium soll es massive Fehler in der Zulassung von Emissions-Projekten gegeben haben: Bundesumweltministerin Steffi Lemke"></p><figcaption>In ihrem Ministerium soll es massive Fehler in der Zulassung von Emissions-Projekten gegeben haben: Bundesumweltministerin Steffi Lemke (Grüne). ©&nbsp;Kay Nietfeld/dpa</figcaption></figure><h2>Möglicher Milliarden-Betrug: Verbraucherschützer, Opposition und Verbände üben Kritik</h2><p>„Durch <a href="https://www.op-online.de/offenbach/sieben-gasversorger-aus-der-region-nach-recherche-des-netzwerks-correctiv-unter-druck-93048646.html" data-id-ec="{&quot;sht&quot;:&quot;Container&quot;,&quot;shn&quot;:&quot;Story&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Paragraph&quot;,&quot;displayType&quot;:&quot;Paragraph InlineLink&quot;,&quot;type&quot;:&quot;INLINE_LINK&quot;,&quot;value&quot;:&quot;fehlerhafte Zertifizierungen und schlampige Kontrollen&quot;,&quot;variant&quot;:&quot;standardLink&quot;,&quot;sponsored&quot;:false,&quot;storyElementPosition&quot;:39,&quot;storyElementCount&quot;:43}}">fehlerhafte Zertifizierungen und schlampige Kontrollen</a> deutscher Behörden ist nicht nur ein horrender finanzieller Schaden entstanden, sondern auch ein massiver Vertrauensverlust in Klimaschutzprojekte im Ausland“, kritisiert Anja Weisgerber, klimapolitische Sprecherin der Union den Vorfall im Bundestag.</p><p>Die THG-Quoten an sich sind dadurch laut <em>ADAC </em><a href="https://www.fr.de/wirtschaft/ablasshandel-fuers-klima-greenwashing-co2-kompensation-93020231.html" data-id-ec="{&quot;sht&quot;:&quot;Container&quot;,&quot;shn&quot;:&quot;Story&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Paragraph&quot;,&quot;displayType&quot;:&quot;Paragraph InlineLink&quot;,&quot;type&quot;:&quot;INLINE_LINK&quot;,&quot;value&quot;:&quot;kaum aussagekräftig&quot;,&quot;variant&quot;:&quot;standardLink&quot;,&quot;sponsored&quot;:false,&quot;storyElementPosition&quot;:41,&quot;storyElementCount&quot;:43}}">kaum aussagekräftig</a>. Manipulierte Reduzierungen durch gefälschte Projekte wie in China sorgen dafür, dass THG-Quoten finanziell günstiger erreicht werden könnten, als durch legal erreichte THG-Quoten. „Damit erzielen Elektrofahrzeughalter geringere Erlöse beim THG-Bonus und das Förderinstrument für Elektromobilität wird geschwächt“, schreibt der Autoclub dazu. Denn Fahrerinnen und Fahrer von E-Autos können die von ihnen nicht ausgestoßenen CO₂-Abgase <a href="https://www.rosenheim24.de/auto/mit-dem-auto-geld-verdienen-so-beantragt-ihr-die-thg-praemie-92639943.html" data-id-ec="{&quot;sht&quot;:&quot;Container&quot;,&quot;shn&quot;:&quot;Story&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Paragraph&quot;,&quot;displayType&quot;:&quot;Paragraph InlineLink&quot;,&quot;type&quot;:&quot;INLINE_LINK&quot;,&quot;value&quot;:&quot;als Zertifikate an Zwischenhändler verkaufen&quot;,&quot;variant&quot;:&quot;standardLink&quot;,&quot;sponsored&quot;:false,&quot;storyElementPosition&quot;:41,&quot;storyElementCount&quot;:43}}">als Zertifikate an Zwischenhändler verkaufen</a>.</p><p>Dazu seien die Zuständigkeiten zwischen <a href="https://www.fr.de/politik/eu-europaeische-union-mitgliedstaaten-laender-gruendung-von-der-leyen-ziele-91385429.html" data-id-ec="{&quot;sht&quot;:&quot;Container&quot;,&quot;shn&quot;:&quot;Story&quot;,&quot;p&quot;:{&quot;containerType&quot;:&quot;Paragraph&quot;,&quot;displayType&quot;:&quot;Paragraph InlineLink&quot;,&quot;type&quot;:&quot;INLINE_LINK&quot;,&quot;value&quot;:&quot;EU&quot;,&quot;variant&quot;:&quot;standardLink&quot;,&quot;sponsored&quot;:false,&quot;storyElementPosition&quot;:42,&quot;storyElementCount&quot;:43}}">EU</a>-Kommission und Mitgliedstaat unklar geregelt, es fehle an belastbaren Kontrollmechanismen für Zertifizierungssysteme und Zertifizierer und die Klärung von Hinweisen auf fehlerhafte Zertifizierung dauere zu lange.</p><p>Verbände, die Politik und der Verbraucherschutz fordern von der Bundesregierung, etwas zu unternehmen. Immerhin: Die Bundesregierung arbeitete nun mit Frankreich und den Niederlanden zusammen, um das Thema auf die Tagesordnung des EU-Minister:innen-Rates zu bringen und die Kommission zum Handeln zu bewegen. <em>(ske)</em></p><span></span><div><h3>Auch interessant</h3></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A look at Apple's technical approach to AI including core model performance etc. (184 pts)]]></title>
            <link>https://www.interconnects.ai/p/apple-intelligence</link>
            <guid>40677810</guid>
            <pubDate>Fri, 14 Jun 2024 05:36:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.interconnects.ai/p/apple-intelligence">https://www.interconnects.ai/p/apple-intelligence</a>, See on <a href="https://news.ycombinator.com/item?id=40677810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>By being quiet in the race for the biggest and most rad foundation models, many people assumed that Apple didn’t have anything to contribute to the AI race. After their new announcements, many in the heart of the AI labs still will argue this, forecasting that </span><a href="https://situational-awareness.ai/from-agi-to-superintelligence/#The_power_of_superintelligence" rel="">a marginal lead on language model performance has an effectively infinite expected value</a><span>. Apple is betting that AI follows paths paved by previous technological revolutions: incremental progress to transformational results.</span></p><p><span>With a large swath of new features, many of which are built into a new multi-model AI system called </span><a href="https://www.apple.com/apple-intelligence/" rel="">Apple Intelligence</a><span>, Apple has demonstrated how meaningful AI interactions can be built into every corner of our digital life — connecting apps and making tasks easier. By executing this well, Apple can guarantee that AI is a force multiplier that keeps even people using their devices longer, rather than pushing them to other upstart companies or competitive products.</span></p><p><span>Apple’s presentation rang very different than most AI keynotes we’ve seen in the last few years. While OpenAI and Google are trying to </span><em>prove</em><span> that they are the best at AI, Apple leaned into a narrative of what else we can do with AI. Apple’s large suite of new AI features coming this fall across all their devices, enabling automation, information retrieval, and generation in a privacy-conscious way will be the first time that many people meaningfully interact with AI.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png" width="1456" height="910" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:910,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1597691,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c3529b-5f53-405f-b5ca-decf769fe061_3456x2160.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>This is the latest chapter that the story of the internet is unfolding at the junction between Apple and Meta. Two companies, who should really be compliments, consistently try to one-up and out-position each other. A few months ago with the launch of Llama 3, Meta launched </span><a href="https://ai.meta.com/meta-ai/" rel="">Meta AI</a><span>, which is generally available and rolling out to Meta’s 3 billion users. Meta AI is much more general. It is trying to be a chatbot, entertainment, and an information store. Its competition includes ChatGPT and all the other places people can go for this information. Meta AI should really be integrated next to ChatGPT into operating system features, but this is very unlikely.</span></p><p>Apple’s new features will be rolled out on its newest devices (e.g. iPhone 15 Pro and later, M1 Macs and later), which eventually can reach all of their 2 billion users as well. It presents a very different view of what AI is to the general public. Apple is framing what they built as being fundamentally different from ChatGPT, Claude, Meta AI, Gemini results, etc., and it is probably right.</p><p><span>While the many features they announced are interesting, and I will be upgrading my phone this cycle to try them out, they have an extremely high variance on how well they’ll land in the final product (e.g. how </span><a href="https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/" rel="">Microsoft Recall needed substantial updates post announcement</a><span> due to security issues). Photo generation, decent on-device generation, better Siri, textual Siri, webpage summaries, etc. all will serve different audiences. Today, beyond the focus on how transformative it is for billions of people to have access to AI rather than hundreds of millions, I want to dig into the interesting technical details Apple has shared with us.</span></p><p><span>All the evidence in WWDC and related communications, such as a </span><a href="https://machinelearning.apple.com/research/introducing-apple-foundation-models" rel="">foundation model blog post</a><span>, showcase that Apple is right where it needs to be with AI on the technical side. Given the amount of AI products that are announced and never delivered, passing the bar on technical capabilities via clear communications with the ML community is important for independent monitoring of the performance of top labs (i.e. what I, and many investors, ML nerds, etc. all try to do). I split this summary into three core categories: 1) </span><strong>Core model performance</strong><span>, 2) </span><strong>adapter and on-device strategy</strong><span>, and 3) </span><strong>alignment strategies</strong><span>.</span></p><p><span>The general approach that Apple is taking is highlighted in this </span><a href="https://x.com/MaxWinebach/status/1800277157135909005" rel="">great 6-minute video from the WWDC keynote</a><span>. In summary, Apple foundation models focus on personalization (alignment strategies), performance (core models), and size (on-device strategies). Only the last one is orthogonal from other developers out there, but it surely added substantial constraints that inspired the other two. </span></p><p>There are a lot of details in the blog post about their models, covering everything from safety to summarization to email completion. At the end of the day, most of these evaluations are extremely opaque, so I’m leaning on a lot of trust in the company's reputation and general context to make predictions on the actual quality of their models.</p><p>The most central figure to my analysis is the Apple human evaluation versus other top language models. This evaluation likely favors Apple by at least a few percentage points. The things they’re training these models for are explicitly different than what ChatGPT is — otherwise, why would Apple have the option to send any prompts to other services? These models are about things that iPhone users already could use AI for. Regardless, the scores are still solid enough to make a strong case.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png" width="1456" height="629" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:629,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:212746,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65e4c4f7-1cc6-442e-bfac-af1d43d66c25_2434x1052.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>We can see that the on-device model, which is about 3 billion parameters, performs very well versus other models out there. Models like </span><a href="https://www.interconnects.ai/p/gemma-google-ships-it" rel="">Gemma</a><span> and </span><a href="https://www.interconnects.ai/p/phi-3-and-arctic-llms" rel="">Phi 3</a><span> are more research or marketing artifacts than models central to future business strategy, so Apple really ought to be crushing models here.</span></p><p>Apple’s Server Model is where things get a little more interesting. The models they’re comparing to, except GPT-4, mostly are considered B-tier by the community. This looks like 2 years after the release of GPT-4, Apple has approximately trained an original GPT-4 level model. This is likely not their ceiling, but the sweet spot they found with respect to their costs — again, the hardest tasks will be offloaded. Beating models like DBRX and Mixtral 8x22B is not trivial, yet it is where Apple should be. To beat the current GPT-4-Turbo, Apple likely needed to invest way more money than would be worth it (e.g. Meta’s GenAI / Metaverse level spending). It was reported that Apple “used a host of licensed content to train and fine-tune their models” like many big tech peers.</p><p><span>There are lots of noteworthy privacy features in the handling of the server model — </span><a href="https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web" rel="">secure enclave</a><span>, secure boot, encryption, no persistent storage, etc. — the combination of which Apple is going to make public so security researchers can verify their claims of privacy. Apple is serving this model entirely on its own infrastructure, which is likely the only way this level of privacy could be achieved (so Google may be able to do something similar if they want to too).</span></p><p>I wasn’t expecting to write about this, but Apple buried a revealing paragraph about fine-tuning techniques. in their foundation model blog post (emphasis mine):</p><blockquote><p><span>We find that data quality is essential to model success, so we utilize a hybrid data strategy in our training pipeline, incorporating both human-annotated and synthetic data, and conduct thorough data curation and filtering procedures. We have developed two novel algorithms in post-training: (1) a </span><strong>rejection sampling fine-tuning algorithm with teacher committee</strong><span>, and (2) a reinforcement learning from human feedback </span><strong>(RLHF) algorithm with mirror descent policy optimization</strong><span> and a </span><strong>leave-one-out advantage estimator</strong><span>. We find that these two algorithms lead to significant improvement in the model’s instruction-following quality.</span></p></blockquote><p>The bookends of this paragraph are pretty standard — use all the quality data you can get (human and synthetic) and the right algorithm, and you’ll get great performance out of your model. The algorithmic details are interesting because they are so specific with respect to existing literature.</p><p><span>First, rejection sampling with the teacher committee looks like an extension of the rejection sampling approach that was used for Llama 2, Llama 3, Qwen 2, and more models. </span><a href="https://arxiv.org/abs/2309.06657" rel="">Rejection sampling</a><span> is a simple idea to generate completions from your model, rank them by a reward model (trained separately), and then use the language modeling loss again to fine-tune your model on the top texts. The new thing introduced is a “teacher committee,” which is very likely to be building on the extensive work that has been appearing around using </span><a href="https://arxiv.org/abs/2312.09244" rel="">reward model ensembles</a><span> (or </span><a href="https://arxiv.org/abs/2401.12187" rel="">merging</a><span>) to </span><a href="https://arxiv.org/abs/2401.16635" rel="">improve performance</a><span>. Ensembles, in principle, improve performance by giving a more robust and or calibrated reward signal.</span></p><p><span>The second algorithm is far more interesting (and technical, so bear with me). They put a direct reference to an older RL algorithm that never really got a ton of traction (e.g. it isn’t implemented in most popular RL libraries) and </span><a href="https://arxiv.org/abs/2005.09814" rel="">mirror descent policy optimization</a><span> (MDPO). This algorithm is similar to </span><a href="https://arxiv.org/abs/1707.06347" rel="">proximal policy optimization</a><span> (PPO), the algorithm you hear about a lot when people discuss RLHF, but solves the problem in a different way. PPO, and its predecessor </span><a href="https://arxiv.org/abs/1502.05477" rel="">TRPO</a><span>, optimize what they call a “surrogate objective” that looks like maximizing a value with respect to a constraint on a “trust region.” Instead of a constraint, MDPO looks like a penalty (and there are many other details). </span><a href="https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo" rel="">Leave-one-out error</a><span> is likely an implementation detail of that algorithm that is grounded broadly in ML theory.</span></p><p>The MDPO offers some great intuitions on how the algorithms relate to each other, and how parts of them may impact current RLHF approaches. Let’s start with a comparison of PPO and TRPO, from the paper (additions mine):</p><blockquote><p>PPO takes a more relaxed approach [than TRPO] and updates its policies by solving an unconstrained optimization problem in which the ratio of the new to old policies is clipped to remain bounded.</p></blockquote><p>This, though, can result in some numerical issues in practice.</p><blockquote><p><span>This phenomenon, which has been reported in (</span><a href="https://arxiv.org/abs/1903.07940" rel="">Wang et al., 2019</a><span>) and (</span><a href="https://arxiv.org/abs/2005.12729" rel="">Engstrom et al., 2020</a><span>), shows that clipping in PPO does not prevent the policy ratios to go out of bound, but it only reduces its probability. This means that despite using clipping, PPO does not guarantee that the trust-region constraint is always satisfied. In fact, recent results, including those in (Engstrom et al., 2020) and our experiments in Section 5.3, show that most of the improved performance exhibited by PPO is due to code-level optimization techniques, such as learning rate annealing, observation and reward normalization, and in particular, the use of generalized advantage estimation (GAE).</span></p></blockquote><p>This is essentially saying that PPO works due to implementation tricks and not exactly due to its designed objective, which MDPO tries to fix. The key differences between MDPO and PPO are that:</p><ul><li><p>MDPO updates are on the “entire dataset” rather than batches. With this, MDPO can be run in an on-policy and an off-policy manner.</p></li><li><p>MDPO uses a different scheduling of KL distance (which may not exist in the RLHF version relative to the original paper).</p></li><li><p><span>MDPO uses opposite direction of </span><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="">KL</a><span>. The Kullback–Leibler (KL) divergence is a directional measure of distance between two distributions that is not symmetric. PPO does KL distance the old policy to the new policy and MDPO the opposite. Intuitively, this is a change between information gain and loss of the policy, where one direction penalizes the loss of high-probability tokens and the other penalizes the creation of new behaviors. MDPO, being the latter, could potentially explore better.</span></p></li></ul><p>The key takeaway is that seeing new algorithms for RLHF is exciting because it is very unlikely that the current PPO implementations are actually optimal for our final goals, so this will encourage more experimentation across the industry.</p><p><span>Realistically, all the details aside, Apple is most likely to innovate here due to their deep company culture of creating </span><em>personal</em><span> products. I’m excited to keep tabs on what they build, as true personalization is a large blindspot in the RLHF literature and discourse.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.interconnects.ai/p/apple-intelligence?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.interconnects.ai/p/apple-intelligence?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>Apple has done a ton of things to put all of this together on their devices. They figured out how to train great models that use just the right amount of memory with quantization, how to train many adapters that work with different apps or styles, how to get fast latency speeds, and much more they didn’t talk about. This is very serious ML system engineering of a different flavor than large models and large request count handling.</p><p><span>This on-device stage relies on a lot of groundwork that Apple has built in the past, such as </span><a href="https://developer.apple.com/documentation/appintents" rel="">App Intents</a><span>, which is a standard way for apps to expose their functionality to automation apps and Siri. Apple has successfully trained a small language model to speak the same functional language. In the WWDC keynote, Apple explained that they focused on fine-tuning specific models for specific tasks, which is likely the 12 categories of applications they will be supporting when this launches.</span></p><p>Apple fine-tuned a batch of adapters for the small model that are loaded into memory when the prompt is detected to need them. What I’m not sure about is how the multi-app sequences showcased in the keynote are handled. When Apple Intelligence is asked “Can I make it to my daughter’s play in time,” the example Craig used, are each of the adapters loaded sequentially, or are they applied and mixed together? This sort of technical magic I don’t expect to get an answer to, but represents a serious investment, especially with the high-quality bar that Apple has.</p><p><span>It’s extremely validating for the local model community that a model that takes only 1 GB of memory to load when quantized, and another 3.2GB to run inference for can do so much. As Apple expands the memory footprint of their devices to accommodate a larger AI narrative, they have easy performance to gain by just dropping the quantization levels a bit. For more on this area, check out </span><a href="https://buttondown.email/ainews/archive/ainews-talaria-apples-new-mlops-superweapon-4066/" rel="">the summary on the AI News newsletter</a><span> that explains a bit more about the paper Apple published in this area, </span><a href="https://machinelearning.apple.com/research/talaria?utm_source=ainews&amp;utm_medium=email&amp;utm_campaign=ainews-talaria-apples-new-mlops-superweapon-4066" rel="">Talaria</a><span>.</span></p><p>When you put the Apple Intelligence system together, you end up with this slide from the keynote.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png" width="1430" height="894" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:894,&quot;width&quot;:1430,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:565933,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b9e9bc-e612-4d2f-bfe5-e814d237e2fe_1430x894.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>They left out one detail, predictably, which is the handling of the external language models that Apple hopes to commoditize by letting the user choose.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png" width="1430" height="894" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:894,&quot;width&quot;:1430,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:530584,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa04c86eb-a152-4174-8cc6-b386f3e394a3_1430x894.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>I’m not the first one to say this, but Apple crushed this presentation (and </span><a href="https://www.forbes.com/sites/dereksaul/2024/06/11/apple-stock-hits-all-time-high-minting-200-billion-ai-rally/" rel="">the markets agree</a><span>). More important is </span><a href="https://www.hyperdimensional.co/i/145548687/what-it-means-two-conflicting-visions-of-ais-future" rel="">what this means for the future and narratives of AI</a><span>. I’ve been saying for a long time that parties outside of the existential risk community need better narratives around the future of AI’s development. Apple's succeeding, especially given that </span><a href="https://www.interconnects.ai/p/llama-3-and-scaling-open-llms" rel="">Llama 3 was branded as an “open-source AGI,”</a><span> is a great addition to a simple and calibrated discourse of AI. Being able to say “Yes, super powerful AI can be folded into many compelling stories around the future of humanity, but it is also a technology that we use today,” will go a long way in policy circles.</span></p><p><strong>Housekeeping</strong></p><ul><li><p><span>Audio of this post is available (soon) in </span><a href="https://podcast.interconnects.ai/" rel="">podcast</a><span> form or on </span><a href="https://www.youtube.com/@interconnects" rel="">YouTube</a><span>.</span></p></li><li><p><span>My real podcast is at </span><a href="http://retortai.com/" rel="">retortai.com</a><span>.</span></p></li><li><p><em>Paid subscriber Discord access in email footer.</em></p></li><li><p><span>Referrals → paid sub: Use the </span><a href="https://www.interconnects.ai/leaderboard" rel="">Interconnects Leaderboard</a><span>.</span></p></li><li><p><span>Student discounts in </span><a href="https://www.interconnects.ai/about" rel="">About page</a><span>.</span></p></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft to delay release of Recall AI feature on security concerns (150 pts)]]></title>
            <link>https://www.reuters.com/technology/artificial-intelligence/microsoft-delay-release-recall-ai-feature-security-concerns-2024-06-14/</link>
            <guid>40677424</guid>
            <pubDate>Fri, 14 Jun 2024 04:14:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/artificial-intelligence/microsoft-delay-release-recall-ai-feature-security-concerns-2024-06-14/">https://www.reuters.com/technology/artificial-intelligence/microsoft-delay-release-recall-ai-feature-security-concerns-2024-06-14/</a>, See on <a href="https://news.ycombinator.com/item?id=40677424">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/artificial-intelligence/microsoft-delay-release-recall-ai-feature-security-concerns-2024-06-14/: Error: Request failed with status code 401]]></description>
        </item>
    </channel>
</rss>